{"home.repos.pwc.inspect_result.liuwei1206_LEBERT.None.to_json.convert": [[10, 24], ["os.path.join", "os.path.join", "os.path.join", "os.path.join", "function.format_convert.BMES_to_json"], "function", ["home.repos.pwc.inspect_result.liuwei1206_LEBERT.function.format_convert.BMES_to_json"], ["def", "convert", "(", "task_name", ",", "mode", "=", "\"train\"", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        task_name: NER/weibo, NER/note4, POS/CTB5 and so on\n        mode: train, dev, or test\n    Return:\n        json format dataset\n    \"\"\"", "\n", "in_file", "=", "os", ".", "path", ".", "join", "(", "task_name", ",", "mode", "+", "\".char.bmes\"", ")", "\n", "in_file", "=", "os", ".", "path", ".", "join", "(", "\"data/dataset\"", ",", "in_file", ")", "\n", "out_file", "=", "os", ".", "path", ".", "join", "(", "task_name", ",", "mode", "+", "\".json\"", ")", "\n", "out_file", "=", "os", ".", "path", ".", "join", "(", "\"data/dataset\"", ",", "out_file", ")", "\n", "\n", "BMES_to_json", "(", "in_file", ",", "out_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.None.Trainer.set_seed": [[82, 87], ["random.seed", "numpy.random.seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all"], "function", ["None"], ["def", "set_seed", "(", "seed", ")", ":", "\n", "    ", "random", ".", "seed", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "torch", ".", "cuda", ".", "manual_seed_all", "(", "seed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.None.Trainer.get_dataloader": [[89, 125], ["print", "torch.utils.data.dataloader.DataLoader", "len", "module.sampler.SequentialDistributedSampler", "module.sampler.SequentialDistributedSampler", "torch.utils.data.sampler.SequentialSampler", "torch.utils.data.sampler.SequentialSampler"], "function", ["None"], ["", "def", "get_dataloader", "(", "dataset", ",", "args", ",", "mode", "=", "'train'", ")", ":", "\n", "    ", "\"\"\"\n    generator datasetloader for training.\n    Note that: for training, we need random sampler, same to shuffle\n               for eval or predict, we need sequence sampler, same to no shuffle\n    Args:\n        dataset:\n        args:\n        mode: train or non-train\n    \"\"\"", "\n", "print", "(", "\"Dataset length: \"", ",", "len", "(", "dataset", ")", ")", "\n", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "        ", "if", "mode", "==", "'train'", ":", "\n", "            ", "sampler", "=", "SequentialDistributedSampler", "(", "dataset", ",", "do_shuffle", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "sampler", "=", "SequentialDistributedSampler", "(", "dataset", ")", "\n", "", "", "else", ":", "\n", "        ", "if", "mode", "==", "'train'", ":", "\n", "            ", "sampler", "=", "SequentialSampler", "(", "dataset", ")", "\n", "", "else", ":", "\n", "            ", "sampler", "=", "SequentialSampler", "(", "dataset", ")", "\n", "", "", "if", "mode", "==", "'train'", ":", "\n", "        ", "batch_size", "=", "args", ".", "per_gpu_train_batch_size", "\n", "", "else", ":", "\n", "        ", "batch_size", "=", "args", ".", "per_gpu_eval_batch_size", "\n", "\n", "", "data_loader", "=", "DataLoader", "(", "\n", "dataset", "=", "dataset", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "shuffle", "=", "False", ",", "\n", "num_workers", "=", "0", ",", "\n", "pin_memory", "=", "True", ",", "\n", "sampler", "=", "sampler", "\n", ")", "\n", "\n", "return", "data_loader", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.None.Trainer.get_optimizer": [[126, 152], ["transformers.optimization.AdamW", "transformers.optimization.get_linear_schedule_with_warmup", "model.named_parameters", "model.named_parameters", "any", "any"], "function", ["None"], ["", "def", "get_optimizer", "(", "model", ",", "args", ",", "num_training_steps", ")", ":", "\n", "    ", "\"\"\"\n    Setup the optimizer and the learning rate scheduler\n\n    we provide a reasonable default that works well\n    If you want to use something else, you can pass a tuple in the Trainer's init,\n    or override this method in a subclass.\n    \"\"\"", "\n", "no_bigger", "=", "[", "\"word_embedding\"", ",", "\"attn_w\"", ",", "\"word_transform\"", ",", "\"word_word_weight\"", ",", "\"hidden2tag\"", ",", "\n", "\"lstm\"", ",", "\"crf\"", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "\n", "\"params\"", ":", "[", "p", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_bigger", ")", "]", ",", "\n", "\"weight_decay\"", ":", "args", ".", "weight_decay", ",", "\n", "}", ",", "\n", "{", "\n", "\"params\"", ":", "[", "p", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_bigger", ")", "]", ",", "\n", "\"lr\"", ":", "0.0001", "\n", "}", "\n", "]", "\n", "optimizer", "=", "AdamW", "(", "optimizer_grouped_parameters", ",", "lr", "=", "args", ".", "learning_rate", ",", "eps", "=", "args", ".", "adam_epsilon", ")", "\n", "scheduler", "=", "get_linear_schedule_with_warmup", "(", "\n", "optimizer", ",", "num_warmup_steps", "=", "args", ".", "warmup_steps", ",", "num_training_steps", "=", "num_training_steps", "\n", ")", "\n", "\n", "return", "optimizer", ",", "scheduler", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.None.Trainer.print_log": [[153, 181], ["print", "logs.items", "tb_writer.flush", "iterator.write", "logger.info", "isinstance", "tb_writer.add_scalar", "logger.warning", "type"], "function", ["None"], ["", "def", "print_log", "(", "logs", ",", "epoch", ",", "global_step", ",", "eval_type", ",", "tb_writer", ",", "iterator", "=", "None", ")", ":", "\n", "    ", "if", "epoch", "is", "not", "None", ":", "\n", "        ", "logs", "[", "'epoch'", "]", "=", "epoch", "\n", "", "if", "global_step", "is", "None", ":", "\n", "        ", "global_step", "=", "0", "\n", "", "if", "eval_type", "in", "[", "\"Dev\"", ",", "\"Test\"", "]", ":", "\n", "        ", "print", "(", "\"#############  %s's result  #############\"", "%", "(", "eval_type", ")", ")", "\n", "", "if", "tb_writer", ":", "\n", "        ", "for", "k", ",", "v", "in", "logs", ".", "items", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "v", ",", "(", "int", ",", "float", ")", ")", ":", "\n", "                ", "tb_writer", ".", "add_scalar", "(", "k", ",", "v", ",", "global_step", ")", "\n", "", "else", ":", "\n", "                ", "logger", ".", "warning", "(", "\n", "\"Trainer is attempting to log a value of \"", "\n", "'\"%s\" of type %s for key \"%s\" as a scalar. '", "\n", "\"This invocation of Tensorboard's writer.add_scalar() \"", "\n", "\"is incorrect so we dropped this attribute.\"", ",", "\n", "v", ",", "\n", "type", "(", "v", ")", ",", "\n", "k", ",", "\n", ")", "\n", "", "", "tb_writer", ".", "flush", "(", ")", "\n", "\n", "", "output", "=", "{", "**", "logs", ",", "**", "{", "\"step\"", ":", "global_step", "}", "}", "\n", "if", "iterator", "is", "not", "None", ":", "\n", "        ", "iterator", ".", "write", "(", "output", ")", "\n", "", "else", ":", "\n", "        ", "logger", ".", "info", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.None.Trainer.train": [[182, 402], ["Trainer.get_dataloader", "Trainer.get_optimizer", "torch.nn.parallel.DistributedDataParallel.zero_grad", "tqdm.auto.trange", "os.path.join", "os.makedirs", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "print", "logger.info", "int", "apex.amp.initialize", "os.path.isfile", "os.path.isfile", "optimizer.load_state_dict", "scheduler.load_state_dict", "torch.nn.parallel.DistributedDataParallel.cuda", "torch.nn.parallel.DistributedDataParallel", "torch.nn.parallel.DistributedDataParallel", "torch.nn.parallel.DistributedDataParallel", "torch.nn.parallel.DistributedDataParallel", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "int", "tqdm.auto.tqdm", "enumerate", "os.path.join", "os.makedirs", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.nn.parallel.DistributedDataParallel.state_dict", "os.path.join", "optimizer.state_dict", "os.path.join", "scheduler.state_dict", "os.path.join", "os.path.join", "os.path.join", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "len", "int", "torch.nn.parallel.DistributedDataParallel.load_state_dict", "logger.info", "logger.info", "logger.info", "logger.info", "isinstance", "isinstance", "get_dataloader.sampler.set_epoch", "torch.nn.parallel.DistributedDataParallel.train", "tuple", "loss.mean.item", "torch.nn.parallel.DistributedDataParallel.state_dict", "os.path.join", "optimizer.state_dict", "os.path.join", "scheduler.state_dict", "os.path.join", "Trainer.evaluate", "Trainer.evaluate", "tqdm.auto.trange.close", "os.path.join", "os.path.join", "torch.load", "torch.load", "torch.load", "torch.load", "logger.info", "torch.nn.parallel.DistributedDataParallel.", "loss.mean.mean", "scaler.scale().backward", "scheduler.step", "torch.nn.parallel.DistributedDataParallel.zero_grad", "tqdm.auto.tqdm.close", "Trainer.print_log", "Trainer.print_log", "len", "len", "[].split", "len", "len", "os.path.join", "t.to", "autocast", "torch.nn.parallel.DistributedDataParallel.", "loss.mean.backward", "len", "scaler.unscale_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "scaler.step", "scaler.update", "optimizer.step", "os.path.join", "os.makedirs", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "scaler.scale", "apex.amp.scale_loss", "scaled_loss.backward", "torch.nn.parallel.DistributedDataParallel.parameters", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "Trainer.print_log", "torch.nn.parallel.DistributedDataParallel.state_dict", "os.path.join", "optimizer.state_dict", "os.path.join", "scheduler.state_dict", "os.path.join", "Trainer.evaluate", "Trainer.evaluate", "apex.amp.master_params", "torch.nn.parallel.DistributedDataParallel.parameters", "packaging.version.parse", "packaging.version.parse", "scheduler.get_last_lr", "scheduler.get_lr", "Trainer.print_log", "Trainer.print_log", "model_path.split"], "function", ["home.repos.pwc.inspect_result.liuwei1206_LEBERT.None.Trainer.get_dataloader", "home.repos.pwc.inspect_result.liuwei1206_LEBERT.None.Trainer.get_optimizer", "home.repos.pwc.inspect_result.liuwei1206_LEBERT.None.Trainer.train", "home.repos.pwc.inspect_result.liuwei1206_LEBERT.None.Trainer.evaluate", "home.repos.pwc.inspect_result.liuwei1206_LEBERT.None.Trainer.evaluate", "home.repos.pwc.inspect_result.liuwei1206_LEBERT.None.Trainer.print_log", "home.repos.pwc.inspect_result.liuwei1206_LEBERT.None.Trainer.print_log", "home.repos.pwc.inspect_result.liuwei1206_LEBERT.None.Trainer.print_log", "home.repos.pwc.inspect_result.liuwei1206_LEBERT.None.Trainer.evaluate", "home.repos.pwc.inspect_result.liuwei1206_LEBERT.None.Trainer.evaluate", "home.repos.pwc.inspect_result.liuwei1206_LEBERT.None.Trainer.print_log", "home.repos.pwc.inspect_result.liuwei1206_LEBERT.None.Trainer.print_log"], ["", "", "def", "train", "(", "model", ",", "args", ",", "train_dataset", ",", "dev_dataset", ",", "test_dataset", ",", "label_vocab", ",", "tb_writer", ",", "model_path", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    train the model\n    \"\"\"", "\n", "## 1.prepare data", "\n", "train_dataloader", "=", "get_dataloader", "(", "train_dataset", ",", "args", ",", "mode", "=", "'train'", ")", "\n", "if", "args", ".", "max_steps", ">", "0", ":", "\n", "        ", "t_total", "=", "args", ".", "max_steps", "\n", "num_train_epochs", "=", "(", "\n", "args", ".", "max_steps", "//", "(", "len", "(", "train_dataloader", ")", "//", "args", ".", "gradient_accumulation_steps", ")", "+", "1", "\n", ")", "\n", "", "else", ":", "\n", "        ", "t_total", "=", "int", "(", "len", "(", "train_dataloader", ")", "//", "args", ".", "gradient_accumulation_steps", "*", "args", ".", "num_train_epochs", ")", "\n", "num_train_epochs", "=", "args", ".", "num_train_epochs", "\n", "\n", "## 2.optimizer and model", "\n", "", "optimizer", ",", "scheduler", "=", "get_optimizer", "(", "model", ",", "args", ",", "t_total", ")", "\n", "\n", "if", "args", ".", "fp16", "and", "_use_apex", ":", "\n", "        ", "model", ",", "optimizer", "=", "amp", ".", "initialize", "(", "model", ",", "optimizer", ",", "opt_level", "=", "args", ".", "fp16_opt_level", ")", "\n", "\n", "# Check if saved optimizer or scheduler states exist", "\n", "", "if", "(", "model_path", "is", "not", "None", "\n", "and", "os", ".", "path", ".", "isfile", "(", "os", ".", "path", ".", "join", "(", "model_path", ",", "\"optimizer.pt\"", ")", ")", "\n", "and", "os", ".", "path", ".", "isfile", "(", "os", ".", "path", ".", "join", "(", "model_path", ",", "\"scheduler.pt\"", ")", ")", "\n", ")", ":", "\n", "        ", "optimizer", ".", "load_state_dict", "(", "\n", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "model_path", ",", "\"optimizer.pt\"", ")", ",", "map_location", "=", "args", ".", "device", ")", "\n", ")", "\n", "scheduler", ".", "load_state_dict", "(", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "model_path", ",", "\"scheduler.pt\"", ")", ")", ")", "\n", "\n", "", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "        ", "model", "=", "model", ".", "cuda", "(", ")", "\n", "model", "=", "torch", ".", "nn", ".", "parallel", ".", "DistributedDataParallel", "(", "\n", "model", ",", "\n", "device_ids", "=", "[", "args", ".", "local_rank", "]", ",", "\n", "output_device", "=", "args", ".", "local_rank", ",", "\n", "find_unused_parameters", "=", "True", "\n", ")", "\n", "\n", "## 3.begin train", "\n", "", "total_train_batch_size", "=", "args", ".", "per_gpu_train_batch_size", "*", "args", ".", "gradient_accumulation_steps", "\n", "if", "args", ".", "local_rank", "==", "0", "or", "args", ".", "local_rank", "==", "-", "1", ":", "\n", "        ", "logger", ".", "info", "(", "\"***** Running training *****\"", ")", "\n", "logger", ".", "info", "(", "\"  Num examples = %d\"", ",", "len", "(", "train_dataloader", ".", "dataset", ")", ")", "\n", "logger", ".", "info", "(", "\"  Num Epochs = %d\"", ",", "num_train_epochs", ")", "\n", "logger", ".", "info", "(", "\"  Instantaneous batch size per device = %d\"", ",", "args", ".", "per_gpu_train_batch_size", ")", "\n", "logger", ".", "info", "(", "\"  Total train batch size (w. parallel, distributed & accumulation) = %d\"", ",", "total_train_batch_size", ")", "\n", "logger", ".", "info", "(", "\"  Gradient Accumulation steps = %d\"", ",", "args", ".", "gradient_accumulation_steps", ")", "\n", "logger", ".", "info", "(", "\"  Total optimization steps = %d\"", ",", "t_total", ")", "\n", "\n", "", "global_step", "=", "0", "\n", "epoch", "=", "0", "\n", "epochs_trained", "=", "0", "\n", "steps_trained_in_current_epoch", "=", "0", "\n", "if", "model_path", "is", "not", "None", ":", "# load checkpoint and continue training", "\n", "        ", "try", ":", "\n", "            ", "global_step", "=", "int", "(", "model_path", ".", "split", "(", "\"-\"", ")", "[", "-", "1", "]", ".", "split", "(", "\"/\"", ")", "[", "0", "]", ")", "\n", "epochs_trained", "=", "global_step", "//", "(", "len", "(", "train_dataloader", ")", "//", "args", ".", "gradient_accumulation_steps", ")", "\n", "steps_trained_in_current_epoch", "=", "global_step", "%", "(", "\n", "len", "(", "train_dataloader", ")", "//", "args", ".", "gradient_accumulation_steps", "\n", ")", "\n", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "model_path", ",", "\"pytorch_model.bin\"", ")", ")", ")", "\n", "logger", ".", "info", "(", "\"  Continuing training from checkpoint, will skip to saved global_step\"", ")", "\n", "logger", ".", "info", "(", "\"  Continuing training from epoch %d\"", ",", "epochs_trained", ")", "\n", "logger", ".", "info", "(", "\"  Continuing training from global step %d\"", ",", "global_step", ")", "\n", "logger", ".", "info", "(", "\"  Will skip the first %d steps in the first epoch\"", ",", "steps_trained_in_current_epoch", ")", "\n", "", "except", "ValueError", ":", "\n", "            ", "global_step", "=", "0", "\n", "logger", ".", "info", "(", "\"  Starting fine-tuning.\"", ")", "\n", "\n", "", "", "tr_loss", "=", "0.0", "\n", "logging_loss", "=", "0.0", "\n", "model", ".", "zero_grad", "(", ")", "\n", "train_iterator", "=", "trange", "(", "epochs_trained", ",", "int", "(", "num_train_epochs", ")", ",", "desc", "=", "\"Epoch\"", ")", "\n", "for", "epoch", "in", "train_iterator", ":", "\n", "        ", "if", "isinstance", "(", "train_dataloader", ",", "DataLoader", ")", "and", "isinstance", "(", "train_dataloader", ".", "sampler", ",", "DistributedSampler", ")", ":", "\n", "            ", "train_dataloader", ".", "sampler", ".", "set_epoch", "(", "epoch", ")", "\n", "\n", "", "epoch_iterator", "=", "tqdm", "(", "train_dataloader", ",", "desc", "=", "\"Iteration\"", ")", "\n", "\n", "for", "step", ",", "batch", "in", "enumerate", "(", "epoch_iterator", ")", ":", "\n", "            ", "if", "steps_trained_in_current_epoch", ">", "0", ":", "\n", "# Skip past any already trained steps if resuming training", "\n", "                ", "steps_trained_in_current_epoch", "-=", "1", "\n", "continue", "\n", "", "model", ".", "train", "(", ")", "\n", "\n", "# new batch data: [input_ids, token_type_ids, attention_mask, matched_word_ids,", "\n", "# matched_word_mask, boundary_ids, labels", "\n", "batch_data", "=", "(", "batch", "[", "0", "]", ",", "batch", "[", "2", "]", ",", "batch", "[", "1", "]", ",", "batch", "[", "3", "]", ",", "batch", "[", "4", "]", ",", "batch", "[", "5", "]", ",", "batch", "[", "6", "]", ")", "\n", "new_batch", "=", "batch_data", "\n", "batch", "=", "tuple", "(", "t", ".", "to", "(", "args", ".", "device", ")", "for", "t", "in", "new_batch", ")", "\n", "inputs", "=", "{", "\"input_ids\"", ":", "batch", "[", "0", "]", ",", "\"attention_mask\"", ":", "batch", "[", "1", "]", ",", "\"token_type_ids\"", ":", "batch", "[", "2", "]", ",", "\n", "\"matched_word_ids\"", ":", "batch", "[", "3", "]", ",", "\"matched_word_mask\"", ":", "batch", "[", "4", "]", ",", "\n", "\"boundary_ids\"", ":", "batch", "[", "5", "]", ",", "\"labels\"", ":", "batch", "[", "6", "]", ",", "\"flag\"", ":", "\"Train\"", "}", "\n", "batch_data", "=", "None", "\n", "new_batch", "=", "None", "\n", "\n", "if", "args", ".", "fp16", "and", "_use_native_amp", ":", "\n", "                ", "with", "autocast", "(", ")", ":", "\n", "                    ", "outputs", "=", "model", "(", "**", "inputs", ")", "\n", "loss", "=", "outputs", "[", "0", "]", "\n", "", "", "else", ":", "\n", "                ", "outputs", "=", "model", "(", "**", "inputs", ")", "\n", "loss", "=", "outputs", "[", "0", "]", "\n", "\n", "", "if", "args", ".", "n_gpu", ">", "1", ":", "\n", "                ", "loss", "=", "loss", ".", "mean", "(", ")", "\n", "", "if", "args", ".", "gradient_accumulation_steps", ">", "1", ":", "\n", "                ", "loss", "=", "loss", "/", "args", ".", "gradient_accumulation_steps", "\n", "\n", "", "if", "args", ".", "fp16", "and", "_use_native_amp", ":", "\n", "                ", "scaler", ".", "scale", "(", "loss", ")", ".", "backward", "(", ")", "\n", "", "elif", "args", ".", "fp16", "and", "_use_apex", ":", "\n", "                ", "with", "amp", ".", "scale_loss", "(", "loss", ",", "optimizer", ")", "as", "scaled_loss", ":", "\n", "                    ", "scaled_loss", ".", "backward", "(", ")", "\n", "", "", "else", ":", "\n", "                ", "loss", ".", "backward", "(", ")", "\n", "\n", "", "tr_loss", "+=", "loss", ".", "item", "(", ")", "\n", "\n", "## update gradient", "\n", "if", "(", "step", "+", "1", ")", "%", "args", ".", "gradient_accumulation_steps", "==", "0", "or", "(", "(", "step", "+", "1", ")", "==", "len", "(", "epoch_iterator", ")", ")", ":", "\n", "                ", "if", "args", ".", "fp16", "and", "_use_native_amp", ":", "\n", "                    ", "scaler", ".", "unscale_", "(", "optimizer", ")", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "model", ".", "parameters", "(", ")", ",", "args", ".", "max_grad_norm", ")", "\n", "", "elif", "args", ".", "fp16", "and", "_use_apex", ":", "\n", "                    ", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "amp", ".", "master_params", "(", "optimizer", ")", ",", "args", ".", "max_grad_norm", ")", "\n", "", "else", ":", "\n", "                    ", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "model", ".", "parameters", "(", ")", ",", "args", ".", "max_grad_norm", ")", "\n", "\n", "", "if", "args", ".", "fp16", "and", "_use_native_amp", ":", "\n", "                    ", "scaler", ".", "step", "(", "optimizer", ")", "\n", "scaler", ".", "update", "(", ")", "\n", "", "else", ":", "\n", "                    ", "optimizer", ".", "step", "(", ")", "\n", "\n", "", "scheduler", ".", "step", "(", ")", "\n", "model", ".", "zero_grad", "(", ")", "\n", "global_step", "+=", "1", "\n", "\n", "## logger and evaluate", "\n", "if", "(", "args", ".", "logging_steps", ">", "0", "and", "global_step", "%", "args", ".", "logging_steps", "==", "0", ")", ":", "\n", "                    ", "logs", "=", "{", "}", "\n", "logs", "[", "\"loss\"", "]", "=", "(", "tr_loss", "-", "logging_loss", ")", "/", "args", ".", "logging_steps", "\n", "# backward compatibility for pytorch schedulers", "\n", "logs", "[", "\"learning_rate\"", "]", "=", "(", "\n", "scheduler", ".", "get_last_lr", "(", ")", "[", "0", "]", "\n", "if", "version", ".", "parse", "(", "torch", ".", "__version__", ")", ">=", "version", ".", "parse", "(", "\"1.4\"", ")", "\n", "else", "scheduler", ".", "get_lr", "(", ")", "[", "0", "]", "\n", ")", "\n", "logging_loss", "=", "tr_loss", "\n", "if", "args", ".", "local_rank", "==", "0", "or", "args", ".", "local_rank", "==", "-", "1", ":", "\n", "                        ", "print_log", "(", "logs", ",", "epoch", ",", "global_step", ",", "\"\"", ",", "tb_writer", ")", "\n", "\n", "## save checkpoint", "\n", "", "", "if", "False", "and", "args", ".", "save_steps", ">", "0", "and", "global_step", "%", "args", ".", "save_steps", "==", "0", "and", "(", "args", ".", "local_rank", "==", "0", "or", "args", ".", "local_rank", "==", "-", "1", ")", ":", "\n", "                    ", "output_dir", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "f\"{PREFIX_CHECKPOINT_DIR}-{global_step}\"", ")", "\n", "os", ".", "makedirs", "(", "output_dir", ",", "exist_ok", "=", "True", ")", "\n", "\n", "torch", ".", "save", "(", "model", ".", "state_dict", "(", ")", ",", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"pytorch_model.bin\"", ")", ")", "\n", "torch", ".", "save", "(", "optimizer", ".", "state_dict", "(", ")", ",", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"optimizer.pt\"", ")", ")", "\n", "torch", ".", "save", "(", "scheduler", ".", "state_dict", "(", ")", ",", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"scheduler.pt\"", ")", ")", "\n", "\n", "if", "False", "and", "args", ".", "evaluate_during_training", ":", "\n", "# for dev", "\n", "                        ", "metrics", ",", "_", "=", "evaluate", "(", "\n", "model", ",", "args", ",", "dev_dataset", ",", "label_vocab", ",", "global_step", ",", "description", "=", "\"Dev\"", ")", "\n", "if", "args", ".", "local_rank", "==", "0", "or", "args", ".", "local_rank", "==", "-", "1", ":", "\n", "                            ", "print_log", "(", "metrics", ",", "epoch", ",", "global_step", ",", "\"Dev\"", ",", "tb_writer", ")", "\n", "\n", "# for test", "\n", "", "metrics", ",", "_", "=", "evaluate", "(", "\n", "model", ",", "args", ",", "test_dataset", ",", "label_vocab", ",", "global_step", ",", "description", "=", "\"Test\"", ")", "\n", "if", "args", ".", "local_rank", "==", "0", "or", "args", ".", "local_rank", "==", "-", "1", ":", "\n", "                            ", "print_log", "(", "metrics", ",", "epoch", ",", "global_step", ",", "\"Test\"", ",", "tb_writer", ")", "\n", "\n", "", "", "", "", "if", "args", ".", "max_steps", ">", "0", "and", "global_step", ">", "args", ".", "max_steps", ":", "\n", "                ", "epoch_iterator", ".", "close", "(", ")", "\n", "break", "\n", "\n", "# save after each epoch", "\n", "", "", "output_dir", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "f\"{PREFIX_CHECKPOINT_DIR}-{global_step}\"", ")", "\n", "os", ".", "makedirs", "(", "output_dir", ",", "exist_ok", "=", "True", ")", "\n", "\n", "torch", ".", "save", "(", "model", ".", "state_dict", "(", ")", ",", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"pytorch_model.bin\"", ")", ")", "\n", "torch", ".", "save", "(", "optimizer", ".", "state_dict", "(", ")", ",", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"optimizer.pt\"", ")", ")", "\n", "torch", ".", "save", "(", "scheduler", ".", "state_dict", "(", ")", ",", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"scheduler.pt\"", ")", ")", "\n", "\n", "# evaluate after each epoch", "\n", "if", "args", ".", "evaluate_during_training", ":", "\n", "# for dev", "\n", "            ", "metrics", ",", "_", "=", "evaluate", "(", "model", ",", "args", ",", "dev_dataset", ",", "label_vocab", ",", "global_step", ",", "description", "=", "\"Dev\"", ",", "write_file", "=", "True", ")", "\n", "if", "args", ".", "local_rank", "==", "0", "or", "args", ".", "local_rank", "==", "-", "1", ":", "\n", "                ", "print_log", "(", "metrics", ",", "epoch", ",", "global_step", ",", "\"Dev\"", ",", "tb_writer", ")", "\n", "\n", "# for test", "\n", "", "metrics", ",", "_", "=", "evaluate", "(", "model", ",", "args", ",", "test_dataset", ",", "label_vocab", ",", "global_step", ",", "description", "=", "\"Test\"", ",", "write_file", "=", "True", ")", "\n", "if", "args", ".", "local_rank", "==", "0", "or", "args", ".", "local_rank", "==", "-", "1", ":", "\n", "                ", "print_log", "(", "metrics", ",", "epoch", ",", "global_step", ",", "\"Test\"", ",", "tb_writer", ")", "\n", "\n", "", "", "if", "args", ".", "max_steps", ">", "0", "and", "global_step", ">", "args", ".", "max_steps", ":", "\n", "            ", "train_iterator", ".", "close", "(", ")", "\n", "break", "\n", "\n", "# save the last one", "\n", "", "", "output_dir", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "f\"{PREFIX_CHECKPOINT_DIR}-{global_step}\"", ")", "\n", "os", ".", "makedirs", "(", "output_dir", ",", "exist_ok", "=", "True", ")", "\n", "torch", ".", "save", "(", "model", ".", "state_dict", "(", ")", ",", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"pytorch_model.bin\"", ")", ")", "\n", "\n", "# model.save_pretrained(os.path.join(output_dir, \"pytorch-model.bin\"))", "\n", "torch", ".", "save", "(", "optimizer", ".", "state_dict", "(", ")", ",", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"optimizer.pt\"", ")", ")", "\n", "torch", ".", "save", "(", "scheduler", ".", "state_dict", "(", ")", ",", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"scheduler.pt\"", ")", ")", "\n", "\n", "print", "(", "\"global_step: \"", ",", "global_step", ")", "\n", "logger", ".", "info", "(", "\"\\n\\nTraining completed. Do not forget to share your model on huggingface.co/models =)\\n\\n\"", ")", "\n", "return", "global_step", ",", "tr_loss", "/", "global_step", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.None.Trainer.evaluate": [[403, 478], ["Trainer.get_dataloader", "torch.nn.parallel.DistributedDataParallel.eval", "tqdm.auto.tqdm", "function.metrics.seq_f1_with_mask", "torch.nn.parallel.DistributedDataParallel.cuda", "torch.nn.parallel.DistributedDataParallel", "torch.nn.parallel.DistributedDataParallel", "torch.nn.parallel.DistributedDataParallel", "torch.nn.parallel.DistributedDataParallel", "logger.info", "logger.info", "logger.info", "tuple", "batch[].detach().cpu().numpy", "batch[].detach().cpu().numpy", "preds.detach().cpu().numpy", "batch[].detach().cpu().numpy", "os.path.join", "transformers.tokenization_bert.BertTokenizer.from_pretrained", "function.utils.save_preds_for_seq_labelling", "len", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.nn.parallel.DistributedDataParallel.", "numpy.append", "numpy.append", "numpy.append", "numpy.append", "t.to", "batch[].detach().cpu", "batch[].detach().cpu", "preds.detach().cpu", "batch[].detach().cpu", "str", "batch[].detach", "batch[].detach", "preds.detach", "batch[].detach"], "function", ["home.repos.pwc.inspect_result.liuwei1206_LEBERT.None.Trainer.get_dataloader", "home.repos.pwc.inspect_result.liuwei1206_LEBERT.function.metrics.seq_f1_with_mask", "home.repos.pwc.inspect_result.liuwei1206_LEBERT.function.utils.save_preds_for_seq_labelling"], ["", "def", "evaluate", "(", "model", ",", "args", ",", "dataset", ",", "label_vocab", ",", "global_step", ",", "description", "=", "\"dev\"", ",", "write_file", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    evaluate the model's performance\n    \"\"\"", "\n", "dataloader", "=", "get_dataloader", "(", "dataset", ",", "args", ",", "mode", "=", "'dev'", ")", "\n", "if", "(", "not", "args", ".", "do_train", ")", "and", "(", "not", "args", ".", "no_cuda", ")", "and", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "        ", "model", "=", "model", ".", "cuda", "(", ")", "\n", "model", "=", "torch", ".", "nn", ".", "parallel", ".", "DistributedDataParallel", "(", "\n", "model", ",", "\n", "device_ids", "=", "[", "args", ".", "local_rank", "]", ",", "\n", "output_device", "=", "args", ".", "local_rank", ",", "\n", "find_unused_parameters", "=", "True", "\n", ")", "\n", "\n", "", "batch_size", "=", "dataloader", ".", "batch_size", "\n", "if", "args", ".", "local_rank", "==", "0", "or", "args", ".", "local_rank", "==", "-", "1", ":", "\n", "        ", "logger", ".", "info", "(", "\"***** Running %s *****\"", ",", "description", ")", "\n", "logger", ".", "info", "(", "\"  Num examples = %d\"", ",", "len", "(", "dataloader", ".", "dataset", ")", ")", "\n", "logger", ".", "info", "(", "\"  Batch size = %d\"", ",", "batch_size", ")", "\n", "", "eval_losses", "=", "[", "]", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "all_input_ids", "=", "None", "\n", "all_label_ids", "=", "None", "\n", "all_predict_ids", "=", "None", "\n", "all_attention_mask", "=", "None", "\n", "\n", "for", "batch", "in", "tqdm", "(", "dataloader", ",", "desc", "=", "description", ")", ":", "\n", "# new batch data: [input_ids, token_type_ids, attention_mask, matched_word_ids,", "\n", "# matched_word_mask, boundary_ids, labels", "\n", "        ", "batch_data", "=", "(", "batch", "[", "0", "]", ",", "batch", "[", "2", "]", ",", "batch", "[", "1", "]", ",", "batch", "[", "3", "]", ",", "batch", "[", "4", "]", ",", "batch", "[", "5", "]", ",", "batch", "[", "6", "]", ")", "\n", "new_batch", "=", "batch_data", "\n", "batch", "=", "tuple", "(", "t", ".", "to", "(", "args", ".", "device", ")", "for", "t", "in", "new_batch", ")", "\n", "inputs", "=", "{", "\"input_ids\"", ":", "batch", "[", "0", "]", ",", "\"attention_mask\"", ":", "batch", "[", "1", "]", ",", "\"token_type_ids\"", ":", "batch", "[", "2", "]", ",", "\n", "\"matched_word_ids\"", ":", "batch", "[", "3", "]", ",", "\"matched_word_mask\"", ":", "batch", "[", "4", "]", ",", "\n", "\"boundary_ids\"", ":", "batch", "[", "5", "]", ",", "\"labels\"", ":", "batch", "[", "6", "]", ",", "\"flag\"", ":", "\"Predict\"", "}", "\n", "batch_data", "=", "None", "\n", "new_batch", "=", "None", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "outputs", "=", "model", "(", "**", "inputs", ")", "\n", "preds", "=", "outputs", "[", "0", "]", "\n", "\n", "", "input_ids", "=", "batch", "[", "0", "]", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "label_ids", "=", "batch", "[", "6", "]", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "pred_ids", "=", "preds", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "attention_mask", "=", "batch", "[", "1", "]", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "if", "all_label_ids", "is", "None", ":", "\n", "            ", "all_input_ids", "=", "input_ids", "\n", "all_label_ids", "=", "label_ids", "\n", "all_predict_ids", "=", "pred_ids", "\n", "all_attention_mask", "=", "attention_mask", "\n", "", "else", ":", "\n", "            ", "all_input_ids", "=", "np", ".", "append", "(", "all_input_ids", ",", "input_ids", ",", "axis", "=", "0", ")", "\n", "all_label_ids", "=", "np", ".", "append", "(", "all_label_ids", ",", "label_ids", ",", "axis", "=", "0", ")", "\n", "all_predict_ids", "=", "np", ".", "append", "(", "all_predict_ids", ",", "pred_ids", ",", "axis", "=", "0", ")", "\n", "all_attention_mask", "=", "np", ".", "append", "(", "all_attention_mask", ",", "attention_mask", ",", "axis", "=", "0", ")", "\n", "\n", "## calculate metrics", "\n", "", "", "acc", ",", "p", ",", "r", ",", "f1", ",", "all_true_labels", ",", "all_pred_labels", "=", "seq_f1_with_mask", "(", "\n", "all_label_ids", ",", "all_predict_ids", ",", "all_attention_mask", ",", "label_vocab", ")", "\n", "metrics", "=", "{", "}", "\n", "metrics", "[", "'acc'", "]", "=", "acc", "\n", "metrics", "[", "'p'", "]", "=", "p", "\n", "metrics", "[", "'r'", "]", "=", "r", "\n", "metrics", "[", "'f1'", "]", "=", "f1", "\n", "\n", "## write labels into file", "\n", "if", "write_file", ":", "\n", "        ", "file_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"{}-{}-{}.txt\"", ".", "format", "(", "args", ".", "model_type", ",", "description", ",", "str", "(", "global_step", ")", ")", ")", "\n", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "args", ".", "vocab_file", ")", "\n", "save_preds_for_seq_labelling", "(", "all_input_ids", ",", "tokenizer", ",", "all_true_labels", ",", "all_pred_labels", ",", "file_path", ")", "\n", "\n", "", "return", "metrics", ",", "(", "all_true_labels", ",", "all_pred_labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.None.Trainer.main": [[480, 594], ["wcbert_parser.get_argparse().parse_args", "logger.info", "logger.info", "SummaryWriter", "Trainer.set_seed", "function.preprocess.build_lexicon_tree_from_vocabs", "os.path.join", "os.path.join", "function.preprocess.get_corpus_matched_word_from_lexicon_tree", "feature.vocab.ItemVocabArray", "feature.vocab.ItemVocabFile", "transformers.tokenization_bert.BertTokenizer.from_pretrained", "function.utils.build_pretrained_embedding_for_corpus", "transformers.configuration_bert.BertConfig.from_pretrained", "feature.vocab.ItemVocabFile.get_item_size", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.set_device", "torch.cuda.set_device", "torch.cuda.set_device", "torch.cuda.set_device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.distributed.init_process_group", "torch.distributed.init_process_group", "torch.distributed.init_process_group", "torch.distributed.init_process_group", "torch.device", "torch.device", "torch.device", "torch.device", "bool", "os.path.join", "os.path.join", "open", "enumerate", "wcbert_modeling.WCBertCRFForTokenClassification.from_pretrained", "BertWordLSTMCRFForTokenClassification.from_pretrained.cuda", "feature.task_dataset.TaskDataset", "feature.task_dataset.TaskDataset", "feature.task_dataset.TaskDataset", "Trainer.train", "logger.info", "feature.task_dataset.TaskDataset", "Trainer.evaluate", "print", "logger.info", "feature.task_dataset.TaskDataset", "Trainer.evaluate", "print", "wcbert_parser.get_argparse", "f.write", "wcbert_modeling.BertWordLSTMCRFForTokenClassification.from_pretrained", "[].split", "[].split", "feature.vocab.ItemVocabFile.get_item_size", "feature.vocab.ItemVocabFile.get_item_size", "get_argparse().parse_args.model_name_or_path.split", "get_argparse().parse_args.model_name_or_path.split"], "function", ["home.repos.pwc.inspect_result.liuwei1206_LEBERT.None.Trainer.set_seed", "home.repos.pwc.inspect_result.liuwei1206_LEBERT.function.preprocess.build_lexicon_tree_from_vocabs", "home.repos.pwc.inspect_result.liuwei1206_LEBERT.function.preprocess.get_corpus_matched_word_from_lexicon_tree", "home.repos.pwc.inspect_result.liuwei1206_LEBERT.function.utils.build_pretrained_embedding_for_corpus", "home.repos.pwc.inspect_result.liuwei1206_LEBERT.feature.vocab.ItemVocabArray.get_item_size", "home.repos.pwc.inspect_result.liuwei1206_LEBERT.None.Trainer.train", "home.repos.pwc.inspect_result.liuwei1206_LEBERT.None.Trainer.evaluate", "home.repos.pwc.inspect_result.liuwei1206_LEBERT.None.Trainer.evaluate", "home.repos.pwc.inspect_result.liuwei1206_LEBERT.None.wcbert_parser.get_argparse", "home.repos.pwc.inspect_result.liuwei1206_LEBERT.feature.vocab.ItemVocabArray.get_item_size", "home.repos.pwc.inspect_result.liuwei1206_LEBERT.feature.vocab.ItemVocabArray.get_item_size"], ["", "def", "main", "(", ")", ":", "\n", "    ", "args", "=", "get_argparse", "(", ")", ".", "parse_args", "(", ")", "\n", "args", ".", "no_cuda", "=", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", "\n", "\n", "########### for multi-gpu training ##############", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "        ", "args", ".", "n_gpu", "=", "1", "\n", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "local_rank", ")", "\n", "device", "=", "torch", ".", "device", "(", "'cuda'", ",", "args", ".", "local_rank", ")", "\n", "torch", ".", "distributed", ".", "init_process_group", "(", "backend", "=", "'nccl'", ",", "init_method", "=", "'env://'", ")", "\n", "", "else", ":", "\n", "        ", "device", "=", "torch", ".", "device", "(", "\"cpu\"", ")", "\n", "args", ".", "n_gpu", "=", "0", "\n", "#################################################", "\n", "\n", "", "args", ".", "device", "=", "device", "\n", "logger", ".", "info", "(", "\n", "\"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\"", ",", "\n", "args", ".", "local_rank", ",", "\n", "args", ".", "device", ",", "\n", "args", ".", "n_gpu", ",", "\n", "bool", "(", "args", ".", "local_rank", "!=", "-", "1", ")", ",", "\n", "args", ".", "fp16", ",", "\n", ")", "\n", "logger", ".", "info", "(", "\"Training/evaluation parameters %s\"", ",", "args", ")", "\n", "tb_writer", "=", "SummaryWriter", "(", "log_dir", "=", "args", ".", "logging_dir", ")", "\n", "set_seed", "(", "args", ".", "seed", ")", "\n", "\n", "## 1.prepare data", "\n", "# a. lexicon tree", "\n", "lexicon_tree", "=", "build_lexicon_tree_from_vocabs", "(", "[", "args", ".", "word_vocab_file", "]", ",", "scan_nums", "=", "[", "args", ".", "max_scan_num", "]", ")", "\n", "embed_lexicon_tree", "=", "lexicon_tree", "\n", "\n", "# b. word vocab, label vocab", "\n", "train_data_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "\"train.json\"", ")", "\n", "# if only has test_set no dev_set, such as msra NER", "\n", "if", "\"msra\"", "in", "args", ".", "data_dir", ":", "\n", "        ", "dev_data_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "\"test.json\"", ")", "\n", "", "else", ":", "\n", "        ", "dev_data_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "\"dev.json\"", ")", "\n", "", "test_data_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "\"test.json\"", ")", "\n", "data_files", "=", "[", "train_data_file", ",", "dev_data_file", ",", "test_data_file", "]", "\n", "matched_words", "=", "get_corpus_matched_word_from_lexicon_tree", "(", "data_files", ",", "embed_lexicon_tree", ")", "\n", "word_vocab", "=", "ItemVocabArray", "(", "items_array", "=", "matched_words", ",", "is_word", "=", "True", ",", "has_default", "=", "False", ",", "unk_num", "=", "5", ")", "\n", "label_vocab", "=", "ItemVocabFile", "(", "files", "=", "[", "args", ".", "label_file", "]", ",", "is_word", "=", "False", ")", "\n", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "args", ".", "vocab_file", ")", "\n", "\n", "with", "open", "(", "\"word_vocab.txt\"", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "        ", "for", "idx", ",", "word", "in", "enumerate", "(", "word_vocab", ".", "idx2item", ")", ":", "\n", "            ", "f", ".", "write", "(", "\"%d\\t%s\\n\"", "%", "(", "idx", ",", "word", ")", ")", "\n", "\n", "# c. prepare embeddinggit", "\n", "", "", "pretrained_word_embedding", ",", "embed_dim", "=", "build_pretrained_embedding_for_corpus", "(", "\n", "embedding_path", "=", "args", ".", "word_embedding", ",", "\n", "word_vocab", "=", "word_vocab", ",", "\n", "embed_dim", "=", "args", ".", "word_embed_dim", ",", "\n", "max_scan_num", "=", "args", ".", "max_scan_num", ",", "\n", "saved_corpus_embedding_dir", "=", "args", ".", "saved_embedding_dir", ",", "\n", ")", "\n", "\n", "# d. define model", "\n", "config", "=", "BertConfig", ".", "from_pretrained", "(", "args", ".", "config_name", ")", "\n", "if", "args", ".", "model_type", "==", "\"WCBertCRF_Token\"", ":", "\n", "        ", "model", "=", "WCBertCRFForTokenClassification", ".", "from_pretrained", "(", "\n", "args", ".", "model_name_or_path", ",", "config", "=", "config", ",", "\n", "pretrained_embeddings", "=", "pretrained_word_embedding", ",", "\n", "num_labels", "=", "label_vocab", ".", "get_item_size", "(", ")", ")", "\n", "", "elif", "args", ".", "model_type", "==", "\"BertWordLSTMCRF_Token\"", ":", "\n", "        ", "model", "=", "BertWordLSTMCRFForTokenClassification", ".", "from_pretrained", "(", "\n", "args", ".", "model_name_or_path", ",", "config", "=", "config", ",", "\n", "pretrained_embeddings", "=", "pretrained_word_embedding", ",", "\n", "num_labels", "=", "label_vocab", ".", "get_item_size", "(", ")", "\n", ")", "\n", "\n", "", "if", "not", "args", ".", "no_cuda", ":", "\n", "        ", "model", "=", "model", ".", "cuda", "(", ")", "\n", "", "args", ".", "label_size", "=", "label_vocab", ".", "get_item_size", "(", ")", "\n", "dataset_params", "=", "{", "\n", "'tokenizer'", ":", "tokenizer", ",", "\n", "'word_vocab'", ":", "word_vocab", ",", "\n", "'label_vocab'", ":", "label_vocab", ",", "\n", "'lexicon_tree'", ":", "lexicon_tree", ",", "\n", "'max_seq_length'", ":", "args", ".", "max_seq_length", ",", "\n", "'max_scan_num'", ":", "args", ".", "max_scan_num", ",", "\n", "'max_word_num'", ":", "args", ".", "max_word_num", ",", "\n", "'default_label'", ":", "args", ".", "default_label", ",", "\n", "}", "\n", "\n", "if", "args", ".", "do_train", ":", "\n", "        ", "train_dataset", "=", "TaskDataset", "(", "train_data_file", ",", "params", "=", "dataset_params", ",", "do_shuffle", "=", "args", ".", "do_shuffle", ")", "\n", "dev_dataset", "=", "TaskDataset", "(", "dev_data_file", ",", "params", "=", "dataset_params", ",", "do_shuffle", "=", "False", ")", "\n", "test_dataset", "=", "TaskDataset", "(", "test_data_file", ",", "params", "=", "dataset_params", ",", "do_shuffle", "=", "False", ")", "\n", "args", ".", "model_name_or_path", "=", "None", "\n", "train", "(", "model", ",", "args", ",", "train_dataset", ",", "dev_dataset", ",", "test_dataset", ",", "label_vocab", ",", "tb_writer", ")", "\n", "\n", "", "if", "args", ".", "do_eval", ":", "\n", "        ", "logger", ".", "info", "(", "\"*** Dev Evaluate ***\"", ")", "\n", "dev_dataset", "=", "TaskDataset", "(", "dev_data_file", ",", "params", "=", "dataset_params", ",", "do_shuffle", "=", "False", ")", "\n", "global_steps", "=", "args", ".", "model_name_or_path", ".", "split", "(", "\"/\"", ")", "[", "-", "2", "]", ".", "split", "(", "\"-\"", ")", "[", "-", "1", "]", "\n", "eval_output", ",", "_", "=", "evaluate", "(", "model", ",", "args", ",", "dev_dataset", ",", "label_vocab", ",", "global_steps", ",", "\"dev\"", ",", "write_file", "=", "True", ")", "\n", "eval_output", "[", "\"global_steps\"", "]", "=", "global_steps", "\n", "print", "(", "\"Dev Result: acc: %.4f, p: %.4f, r: %.4f, f1: %.4f\\n\"", "%", "\n", "(", "eval_output", "[", "'acc'", "]", ",", "eval_output", "[", "'p'", "]", ",", "eval_output", "[", "'r'", "]", ",", "eval_output", "[", "'f1'", "]", ")", ")", "\n", "\n", "\n", "# return eval_output", "\n", "", "if", "args", ".", "do_predict", ":", "\n", "        ", "logger", ".", "info", "(", "\"*** Test Evaluate ***\"", ")", "\n", "test_dataset", "=", "TaskDataset", "(", "test_data_file", ",", "params", "=", "dataset_params", ",", "do_shuffle", "=", "False", ")", "\n", "global_steps", "=", "args", ".", "model_name_or_path", ".", "split", "(", "\"/\"", ")", "[", "-", "2", "]", ".", "split", "(", "\"-\"", ")", "[", "-", "1", "]", "\n", "eval_output", ",", "_", "=", "evaluate", "(", "model", ",", "args", ",", "test_dataset", ",", "label_vocab", ",", "global_steps", ",", "\"test\"", ",", "write_file", "=", "True", ")", "\n", "eval_output", "[", "\"global_steps\"", "]", "=", "global_steps", "\n", "print", "(", "\"Test Result: acc: %.4f, p: %.4f, r: %.4f, f1: %.4f\\n\"", "%", "\n", "(", "eval_output", "[", "'acc'", "]", ",", "eval_output", "[", "'p'", "]", ",", "eval_output", "[", "'r'", "]", ",", "eval_output", "[", "'f1'", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.None.wcbert_modeling.BertEmbeddings.__init__": [[42, 56], ["torch.nn.Module.__init__", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "BertLayerNorm", "torch.nn.Dropout", "torch.nn.Dropout", "wcbert_modeling.BertEmbeddings.register_buffer", "torch.arange().expand", "torch.arange().expand", "torch.arange().expand", "torch.arange().expand", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.liuwei1206_LEBERT.feature.vocab.ItemVocabArray.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "word_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "vocab_size", ",", "config", ".", "hidden_size", ",", "padding_idx", "=", "config", ".", "pad_token_id", ")", "\n", "self", ".", "position_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "max_position_embeddings", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "token_type_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "type_vocab_size", ",", "config", ".", "hidden_size", ")", "\n", "\n", "# self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load", "\n", "# any TensorFlow checkpoint file", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "# position_ids (1, len position emb) is contiguous in memory and exported when serialized", "\n", "self", ".", "register_buffer", "(", "\"position_ids\"", ",", "torch", ".", "arange", "(", "config", ".", "max_position_embeddings", ")", ".", "expand", "(", "(", "1", ",", "-", "1", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.None.wcbert_modeling.BertEmbeddings.forward": [[57, 87], ["wcbert_modeling.BertEmbeddings.position_embeddings", "wcbert_modeling.BertEmbeddings.token_type_embeddings", "wcbert_modeling.BertEmbeddings.LayerNorm", "wcbert_modeling.BertEmbeddings.dropout", "input_ids.size", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "wcbert_modeling.BertEmbeddings.word_embeddings", "wcbert_modeling.BertEmbeddings.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", "=", "None", ",", "token_type_ids", "=", "None", ",", "position_ids", "=", "None", ",", "boundary_ids", "=", "None", ",", "inputs_embeds", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        here we add a boundary information\n        boundary_ids: [batch_size, seq_length, boundary_size]\n        boundary_mask: filter some boubdary information\n        \"\"\"", "\n", "if", "input_ids", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "", "else", ":", "\n", "            ", "input_shape", "=", "inputs_embeds", ".", "size", "(", ")", "[", ":", "-", "1", "]", "\n", "\n", "", "seq_length", "=", "input_shape", "[", "1", "]", "\n", "\n", "if", "position_ids", "is", "None", ":", "\n", "            ", "position_ids", "=", "self", ".", "position_ids", "[", ":", ",", ":", "seq_length", "]", "\n", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros", "(", "input_shape", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "self", ".", "position_ids", ".", "device", ")", "\n", "\n", "", "if", "inputs_embeds", "is", "None", ":", "\n", "            ", "inputs_embeds", "=", "self", ".", "word_embeddings", "(", "input_ids", ")", "\n", "\n", "", "position_embeddings", "=", "self", ".", "position_embeddings", "(", "position_ids", ")", "\n", "token_type_embeddings", "=", "self", ".", "token_type_embeddings", "(", "token_type_ids", ")", "\n", "\n", "embeddings", "=", "inputs_embeds", "+", "position_embeddings", "+", "token_type_embeddings", "\n", "\n", "embeddings", "=", "self", ".", "LayerNorm", "(", "embeddings", ")", "\n", "embeddings", "=", "self", ".", "dropout", "(", "embeddings", ")", "\n", "return", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.None.wcbert_modeling.BertLayer.__init__": [[93, 120], ["torch.nn.Module.__init__", "transformers.modeling_bert.BertAttention", "transformers.modeling_bert.BertIntermediate", "transformers.modeling_bert.BertOutput", "transformers.modeling_bert.BertAttention", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Tanh", "torch.nn.Tanh", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.nn.Parameter", "torch.nn.Parameter", "wcbert_modeling.BertLayer.attn_W.data.normal_", "torch.nn.LayerNorm", "torch.nn.LayerNorm"], "methods", ["home.repos.pwc.inspect_result.liuwei1206_LEBERT.feature.vocab.ItemVocabArray.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "has_word_attn", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "chunk_size_feed_forward", "=", "config", ".", "chunk_size_feed_forward", "\n", "self", ".", "seq_len_dim", "=", "1", "\n", "self", ".", "attention", "=", "BertAttention", "(", "config", ")", "\n", "self", ".", "is_decoder", "=", "config", ".", "is_decoder", "\n", "self", ".", "add_cross_attention", "=", "config", ".", "add_cross_attention", "\n", "if", "self", ".", "add_cross_attention", ":", "\n", "            ", "assert", "self", ".", "is_decoder", ",", "f\"{self} should be used as a decoder model if cross attention is added\"", "\n", "self", ".", "crossattention", "=", "BertAttention", "(", "config", ")", "\n", "\n", "## here we add a attention for matched word", "\n", "", "self", ".", "has_word_attn", "=", "has_word_attn", "\n", "if", "self", ".", "has_word_attn", ":", "\n", "            ", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "act", "=", "nn", ".", "Tanh", "(", ")", "\n", "\n", "self", ".", "word_transform", "=", "nn", ".", "Linear", "(", "config", ".", "word_embed_dim", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "word_word_weight", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "attn_W", "=", "torch", ".", "zeros", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "attn_W", "=", "nn", ".", "Parameter", "(", "attn_W", ")", "\n", "self", ".", "attn_W", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "config", ".", "initializer_range", ")", "\n", "self", ".", "fuse_layernorm", "=", "nn", ".", "LayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "\n", "", "self", ".", "intermediate", "=", "BertIntermediate", "(", "config", ")", "\n", "self", ".", "output", "=", "BertOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.None.wcbert_modeling.BertLayer.forward": [[121, 196], ["wcbert_modeling.BertLayer.attention", "transformers.modeling_utils.apply_chunking_to_forward", "hasattr", "wcbert_modeling.BertLayer.crossattention", "wcbert_modeling.BertLayer.word_transform", "wcbert_modeling.BertLayer.act", "wcbert_modeling.BertLayer.word_word_weight", "wcbert_modeling.BertLayer.dropout", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "alpha.unsqueeze.unsqueeze.squeeze", "alpha.unsqueeze.unsqueeze.unsqueeze", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "wcbert_modeling.BertLayer.dropout", "wcbert_modeling.BertLayer.fuse_layernorm", "wcbert_modeling.BertLayer.unsqueeze", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.nn.Softmax", "torch.nn.Softmax", "torch.nn.Softmax", "torch.nn.Softmax", "input_word_mask.float"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "None", ",", "\n", "input_word_embeddings", "=", "None", ",", "\n", "input_word_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "output_attentions", "=", "False", "\n", ")", ":", "\n", "        ", "\"\"\"\n        code refer to: https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py\n        N: batch_size\n        L: seq length\n        W: word size\n        D: word_embedding dim\n        Args:\n            input_word_embedding: [N, L, W, D]\n            input_word_mask: [N, L, W]\n        \"\"\"", "\n", "## 1.character contextual representation", "\n", "self_attention_outputs", "=", "self", ".", "attention", "(", "\n", "hidden_states", ",", "\n", "attention_mask", ",", "\n", "head_mask", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", ")", "\n", "attention_output", "=", "self_attention_outputs", "[", "0", "]", "# this is the contextual representation", "\n", "outputs", "=", "self_attention_outputs", "[", "1", ":", "]", "# add self attentions if we output attention weights", "\n", "\n", "# decode need join attention from the outputs", "\n", "if", "self", ".", "is_decoder", "and", "encoder_hidden_states", "is", "not", "None", ":", "\n", "            ", "assert", "hasattr", "(", "\n", "self", ",", "\"crossattention\"", "\n", ")", ",", "f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers by setting `config.add_cross_attention=True`\"", "\n", "cross_attention_outputs", "=", "self", ".", "crossattention", "(", "\n", "attention_output", ",", "\n", "attention_mask", ",", "\n", "head_mask", ",", "\n", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", ",", "\n", "output_attentions", ",", "\n", ")", "\n", "attention_output", "=", "cross_attention_outputs", "[", "0", "]", "\n", "outputs", "=", "outputs", "+", "cross_attention_outputs", "[", "1", ":", "]", "# add cross attentions if we output attention weights", "\n", "\n", "", "layer_output", "=", "apply_chunking_to_forward", "(", "\n", "self", ".", "feed_forward_chunk", ",", "self", ".", "chunk_size_feed_forward", ",", "self", ".", "seq_len_dim", ",", "attention_output", "\n", ")", "\n", "\n", "if", "self", ".", "has_word_attn", ":", "\n", "            ", "assert", "input_word_mask", "is", "not", "None", "\n", "\n", "# transform", "\n", "word_outputs", "=", "self", ".", "word_transform", "(", "input_word_embeddings", ")", "# [N, L, W, D]", "\n", "word_outputs", "=", "self", ".", "act", "(", "word_outputs", ")", "\n", "word_outputs", "=", "self", ".", "word_word_weight", "(", "word_outputs", ")", "\n", "word_outputs", "=", "self", ".", "dropout", "(", "word_outputs", ")", "\n", "\n", "# attention_output = attention_output.unsqueeze(2) # [N, L, D] -> [N, L, 1, D]", "\n", "alpha", "=", "torch", ".", "matmul", "(", "layer_output", ".", "unsqueeze", "(", "2", ")", ",", "self", ".", "attn_W", ")", "# [N, L, 1, D]", "\n", "alpha", "=", "torch", ".", "matmul", "(", "alpha", ",", "torch", ".", "transpose", "(", "word_outputs", ",", "2", ",", "3", ")", ")", "# [N, L, 1, W]", "\n", "alpha", "=", "alpha", ".", "squeeze", "(", ")", "# [N, L, W]", "\n", "alpha", "=", "alpha", "+", "(", "1", "-", "input_word_mask", ".", "float", "(", ")", ")", "*", "(", "-", "10000.0", ")", "\n", "alpha", "=", "torch", ".", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "(", "alpha", ")", "# [N, L, W]", "\n", "alpha", "=", "alpha", ".", "unsqueeze", "(", "-", "1", ")", "# [N, L, W, 1]", "\n", "weighted_word_embedding", "=", "torch", ".", "sum", "(", "word_outputs", "*", "alpha", ",", "dim", "=", "2", ")", "# [N, L, D]", "\n", "layer_output", "=", "layer_output", "+", "weighted_word_embedding", "\n", "\n", "layer_output", "=", "self", ".", "dropout", "(", "layer_output", ")", "\n", "layer_output", "=", "self", ".", "fuse_layernorm", "(", "layer_output", ")", "\n", "\n", "", "outputs", "=", "(", "layer_output", ",", ")", "+", "outputs", "\n", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.None.wcbert_modeling.BertLayer.feed_forward_chunk": [[197, 201], ["wcbert_modeling.BertLayer.intermediate", "wcbert_modeling.BertLayer.output"], "methods", ["None"], ["", "def", "feed_forward_chunk", "(", "self", ",", "attention_output", ")", ":", "\n", "        ", "intermediate_output", "=", "self", ".", "intermediate", "(", "attention_output", ")", "\n", "layer_output", "=", "self", ".", "output", "(", "intermediate_output", ",", "attention_output", ")", "\n", "return", "layer_output", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.None.wcbert_modeling.BertEncoder.__init__": [[204, 217], ["torch.nn.Module.__init__", "range", "torch.nn.ModuleList", "torch.nn.ModuleList", "total_layers.append", "total_layers.append", "wcbert_modeling.BertLayer", "wcbert_modeling.BertLayer"], "methods", ["home.repos.pwc.inspect_result.liuwei1206_LEBERT.feature.vocab.ItemVocabArray.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "config", "=", "config", "\n", "self", ".", "add_layers", "=", "config", ".", "add_layers", "\n", "\n", "total_layers", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "config", ".", "num_hidden_layers", ")", ":", "\n", "            ", "if", "i", "in", "self", ".", "add_layers", ":", "\n", "                ", "total_layers", ".", "append", "(", "BertLayer", "(", "config", ",", "True", ")", ")", "\n", "", "else", ":", "\n", "                ", "total_layers", ".", "append", "(", "BertLayer", "(", "config", ",", "False", ")", ")", "\n", "\n", "", "", "self", ".", "layer", "=", "nn", ".", "ModuleList", "(", "total_layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.None.wcbert_modeling.BertEncoder.forward": [[218, 280], ["enumerate", "tuple", "getattr", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "layer_module", "wcbert_modeling.BertEncoder.forward.create_custom_forward"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "None", ",", "\n", "input_word_embeddings", "=", "None", ",", "\n", "input_word_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "output_attentions", "=", "False", ",", "\n", "output_hidden_states", "=", "False", ",", "\n", "return_dict", "=", "False", ",", "\n", ")", ":", "\n", "        ", "all_hidden_states", "=", "(", ")", "if", "output_hidden_states", "else", "None", "\n", "all_attentions", "=", "(", ")", "if", "output_attentions", "else", "None", "\n", "# print(\"Layer 0: \\n\")", "\n", "# print(hidden_states)", "\n", "for", "i", ",", "layer_module", "in", "enumerate", "(", "self", ".", "layer", ")", ":", "\n", "            ", "if", "output_hidden_states", ":", "\n", "                ", "all_hidden_states", "=", "all_hidden_states", "+", "(", "hidden_states", ",", ")", "\n", "\n", "", "layer_head_mask", "=", "head_mask", "[", "i", "]", "if", "head_mask", "is", "not", "None", "else", "None", "\n", "\n", "if", "getattr", "(", "self", ".", "config", ",", "\"gradient_checkpointing\"", ",", "False", ")", ":", "\n", "\n", "                ", "def", "create_custom_forward", "(", "module", ")", ":", "\n", "                    ", "def", "custom_forward", "(", "*", "inputs", ")", ":", "\n", "                        ", "return", "module", "(", "*", "inputs", ",", "output_attentions", ")", "\n", "\n", "", "return", "custom_forward", "\n", "\n", "", "layer_outputs", "=", "torch", ".", "utils", ".", "checkpoint", ".", "checkpoint", "(", "\n", "create_custom_forward", "(", "layer_module", ")", ",", "\n", "hidden_states", ",", "\n", "attention_mask", ",", "\n", "input_word_embeddings", ",", "\n", "input_word_mask", ",", "\n", "layer_head_mask", ",", "\n", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", ",", "\n", ")", "\n", "", "else", ":", "\n", "                ", "layer_outputs", "=", "layer_module", "(", "\n", "hidden_states", ",", "\n", "attention_mask", ",", "\n", "input_word_embeddings", ",", "\n", "input_word_mask", ",", "\n", "layer_head_mask", ",", "\n", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", ",", "\n", "output_attentions", ",", "\n", ")", "\n", "", "hidden_states", "=", "layer_outputs", "[", "0", "]", "\n", "# print(\"Layer %d: \\n\"%(i+1))", "\n", "# print(hidden_states)", "\n", "if", "output_attentions", ":", "\n", "                ", "all_attentions", "=", "all_attentions", "+", "(", "layer_outputs", "[", "1", "]", ",", ")", "\n", "\n", "", "", "if", "output_hidden_states", ":", "\n", "            ", "all_hidden_states", "=", "all_hidden_states", "+", "(", "hidden_states", ",", ")", "\n", "\n", "", "return", "tuple", "(", "v", "for", "v", "in", "[", "hidden_states", ",", "all_hidden_states", ",", "all_attentions", "]", "if", "v", "is", "not", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.None.wcbert_modeling.BertPooler.__init__": [[283, 287], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Tanh", "torch.nn.Tanh"], "methods", ["home.repos.pwc.inspect_result.liuwei1206_LEBERT.feature.vocab.ItemVocabArray.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "activation", "=", "nn", ".", "Tanh", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.None.wcbert_modeling.BertPooler.forward": [[288, 295], ["wcbert_modeling.BertPooler.dense", "wcbert_modeling.BertPooler.activation"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "# We \"pool\" the model by simply taking the hidden state corresponding", "\n", "# to the first token.", "\n", "        ", "first_token_tensor", "=", "hidden_states", "[", ":", ",", "0", "]", "\n", "pooled_output", "=", "self", ".", "dense", "(", "first_token_tensor", ")", "\n", "pooled_output", "=", "self", ".", "activation", "(", "pooled_output", ")", "\n", "return", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.None.wcbert_modeling.BertPreTrainedModel._init_weights": [[306, 317], ["isinstance", "module.weight.data.normal_", "isinstance", "isinstance", "module.bias.data.zero_", "module.bias.data.zero_", "module.weight.data.fill_"], "methods", ["None"], ["def", "_init_weights", "(", "self", ",", "module", ")", ":", "\n", "        ", "\"\"\" Initialize the weights \"\"\"", "\n", "if", "isinstance", "(", "module", ",", "(", "nn", ".", "Linear", ",", "nn", ".", "Embedding", ",", "nn", ".", "Parameter", ")", ")", ":", "\n", "# Slightly different from the TF version which uses truncated_normal for initialization", "\n", "# cf https://github.com/pytorch/pytorch/pull/5617", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "self", ".", "config", ".", "initializer_range", ")", "\n", "", "elif", "isinstance", "(", "module", ",", "nn", ".", "LayerNorm", ")", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "module", ".", "weight", ".", "data", ".", "fill_", "(", "1.0", ")", "\n", "", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", "and", "module", ".", "bias", "is", "not", "None", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.None.wcbert_modeling.WCBertModel.__init__": [[320, 328], ["transformers.modeling_utils.PreTrainedModel.__init__", "wcbert_modeling.BertEmbeddings", "wcbert_modeling.BertEncoder", "wcbert_modeling.WCBertModel.init_weights", "wcbert_modeling.BertPooler"], "methods", ["home.repos.pwc.inspect_result.liuwei1206_LEBERT.feature.vocab.ItemVocabArray.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "add_pooling_layer", "=", "True", ")", ":", "\n", "        ", "super", "(", "WCBertModel", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "\n", "self", ".", "embeddings", "=", "BertEmbeddings", "(", "config", ")", "\n", "self", ".", "encoder", "=", "BertEncoder", "(", "config", ")", "\n", "self", ".", "pooler", "=", "BertPooler", "(", "config", ")", "if", "add_pooling_layer", "else", "None", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.None.wcbert_modeling.WCBertModel.get_input_embeddings": [[329, 331], ["None"], "methods", ["None"], ["", "def", "get_input_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "embeddings", ".", "word_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.None.wcbert_modeling.WCBertModel.set_input_embeddings": [[332, 334], ["None"], "methods", ["None"], ["", "def", "set_input_embeddings", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "embeddings", ".", "word_embeddings", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.None.wcbert_modeling.WCBertModel._prune_heads": [[335, 342], ["heads_to_prune.items", "wcbert_modeling.WCBertModel.encoder.layer[].attention.prune_heads"], "methods", ["None"], ["", "def", "_prune_heads", "(", "self", ",", "heads_to_prune", ")", ":", "\n", "        ", "\"\"\"Prunes heads of the model.\n        heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n        See base class PreTrainedModel\n        \"\"\"", "\n", "for", "layer", ",", "heads", "in", "heads_to_prune", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "encoder", ".", "layer", "[", "layer", "]", ".", "attention", ".", "prune_heads", "(", "heads", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.None.wcbert_modeling.WCBertModel.forward": [[343, 457], ["wcbert_modeling.WCBertModel.get_extended_attention_mask", "wcbert_modeling.WCBertModel.get_head_mask", "wcbert_modeling.WCBertModel.embeddings", "wcbert_modeling.WCBertModel.encoder", "wcbert_modeling.WCBertModel.pooler", "BaseModelOutputWithPooling", "ValueError", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "encoder_hidden_states.size", "wcbert_modeling.WCBertModel.invert_attention_mask", "input_ids.size", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "ValueError", "inputs_embeds.size"], "methods", ["None"], ["", "", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "matched_word_embeddings", "=", "None", ",", "\n", "matched_word_mask", "=", "None", ",", "\n", "boundary_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention\n            if the model is configured as a decoder.\n        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Mask to avoid performing attention on the padding token indices of the encoder input. This mask\n            is used in the cross-attention if the model is configured as a decoder.\n            Mask values selected in ``[0, 1]``:\n            ``1`` for tokens that are NOT MASKED, ``0`` for MASKED tokens.\n\n        batch_size: N\n        seq_length: L\n        dim: D\n        word_num: W\n        boundary_num: B\n\n\n        Args:\n            input_ids: [N, L]\n            attention_mask: [N, L]\n            boundary_ids: [N, L, B]\n            boundary_mask: [N, L, B]\n            matched_word_embeddings: [B, L, W, D]\n            matched_word_mask: [B, L, W]\n        \"\"\"", "\n", "output_attentions", "=", "output_attentions", "if", "output_attentions", "is", "not", "None", "else", "self", ".", "config", ".", "output_attentions", "\n", "output_hidden_states", "=", "(", "\n", "output_hidden_states", "if", "output_hidden_states", "is", "not", "None", "else", "self", ".", "config", ".", "output_hidden_states", "\n", ")", "\n", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "if", "input_ids", "is", "not", "None", "and", "inputs_embeds", "is", "not", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"You cannot specify both input_ids and inputs_embeds at the same time\"", ")", "\n", "", "elif", "input_ids", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "", "elif", "inputs_embeds", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "inputs_embeds", ".", "size", "(", ")", "[", ":", "-", "1", "]", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"You have to specify either input_ids or inputs_embeds\"", ")", "\n", "\n", "", "device", "=", "input_ids", ".", "device", "if", "input_ids", "is", "not", "None", "else", "inputs_embeds", ".", "device", "\n", "\n", "if", "attention_mask", "is", "None", ":", "\n", "            ", "attention_mask", "=", "torch", ".", "ones", "(", "input_shape", ",", "device", "=", "device", ")", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros", "(", "input_shape", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", "\n", "\n", "# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]", "\n", "# ourselves in which case we just need to make it broadcastable to all heads.", "\n", "", "extended_attention_mask", ":", "torch", ".", "Tensor", "=", "self", ".", "get_extended_attention_mask", "(", "attention_mask", ",", "input_shape", ",", "device", ")", "\n", "\n", "# If a 2D or 3D attention mask is provided for the cross-attention", "\n", "# we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]", "\n", "if", "self", ".", "config", ".", "is_decoder", "and", "encoder_hidden_states", "is", "not", "None", ":", "\n", "            ", "encoder_batch_size", ",", "encoder_sequence_length", ",", "_", "=", "encoder_hidden_states", ".", "size", "(", ")", "\n", "encoder_hidden_shape", "=", "(", "encoder_batch_size", ",", "encoder_sequence_length", ")", "\n", "if", "encoder_attention_mask", "is", "None", ":", "\n", "                ", "encoder_attention_mask", "=", "torch", ".", "ones", "(", "encoder_hidden_shape", ",", "device", "=", "device", ")", "\n", "", "encoder_extended_attention_mask", "=", "self", ".", "invert_attention_mask", "(", "encoder_attention_mask", ")", "\n", "", "else", ":", "\n", "            ", "encoder_extended_attention_mask", "=", "None", "\n", "\n", "# Prepare head mask if needed", "\n", "# 1.0 in head_mask indicate we keep the head", "\n", "# attention_probs has shape bsz x n_heads x N x N", "\n", "# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]", "\n", "# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]", "\n", "", "head_mask", "=", "self", ".", "get_head_mask", "(", "head_mask", ",", "self", ".", "config", ".", "num_hidden_layers", ")", "\n", "\n", "embedding_output", "=", "self", ".", "embeddings", "(", "\n", "input_ids", "=", "input_ids", ",", "position_ids", "=", "position_ids", ",", "token_type_ids", "=", "token_type_ids", ",", "\n", "boundary_ids", "=", "boundary_ids", ",", "inputs_embeds", "=", "inputs_embeds", ",", "\n", ")", "\n", "\n", "encoder_outputs", "=", "self", ".", "encoder", "(", "\n", "embedding_output", ",", "\n", "attention_mask", "=", "extended_attention_mask", ",", "\n", "input_word_embeddings", "=", "matched_word_embeddings", ",", "\n", "input_word_mask", "=", "matched_word_mask", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "encoder_hidden_states", "=", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", "=", "encoder_extended_attention_mask", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", ")", "\n", "sequence_output", "=", "encoder_outputs", "[", "0", "]", "\n", "pooled_output", "=", "self", ".", "pooler", "(", "sequence_output", ")", "\n", "\n", "if", "not", "return_dict", ":", "\n", "            ", "return", "(", "sequence_output", ",", "pooled_output", ")", "+", "encoder_outputs", "[", "1", ":", "]", "\n", "\n", "", "return", "BaseModelOutputWithPooling", "(", "\n", "last_hidden_state", "=", "sequence_output", ",", "\n", "pooler_output", "=", "pooled_output", ",", "\n", "hidden_states", "=", "encoder_outputs", ".", "hidden_states", ",", "\n", "attentions", "=", "encoder_outputs", ".", "attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.None.wcbert_modeling.WCBertCRFForTokenClassification.__init__": [[460, 477], ["transformers.modeling_utils.PreTrainedModel.__init__", "torch.nn.Embedding", "torch.nn.Embedding", "wcbert_modeling.WCBertModel", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "module.crf.CRF", "wcbert_modeling.WCBertCRFForTokenClassification.init_weights", "wcbert_modeling.WCBertCRFForTokenClassification.word_embeddings.weight.data.copy_", "print", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy"], "methods", ["home.repos.pwc.inspect_result.liuwei1206_LEBERT.feature.vocab.ItemVocabArray.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "pretrained_embeddings", ",", "num_labels", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "\n", "word_vocab_size", "=", "pretrained_embeddings", ".", "shape", "[", "0", "]", "\n", "embed_dim", "=", "pretrained_embeddings", ".", "shape", "[", "1", "]", "\n", "self", ".", "word_embeddings", "=", "nn", ".", "Embedding", "(", "word_vocab_size", ",", "embed_dim", ")", "\n", "self", ".", "bert", "=", "WCBertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "HP_dropout", ")", "\n", "self", ".", "num_labels", "=", "num_labels", "\n", "self", ".", "hidden2tag", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "num_labels", "+", "2", ")", "\n", "self", ".", "crf", "=", "CRF", "(", "num_labels", ",", "torch", ".", "cuda", ".", "is_available", "(", ")", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n", "## init the embedding", "\n", "self", ".", "word_embeddings", ".", "weight", ".", "data", ".", "copy_", "(", "torch", ".", "from_numpy", "(", "pretrained_embeddings", ")", ")", "\n", "print", "(", "\"Load pretrained embedding from file.........\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.None.wcbert_modeling.WCBertCRFForTokenClassification.forward": [[478, 511], ["wcbert_modeling.WCBertCRFForTokenClassification.word_embeddings", "wcbert_modeling.WCBertCRFForTokenClassification.bert", "wcbert_modeling.WCBertCRFForTokenClassification.dropout", "wcbert_modeling.WCBertCRFForTokenClassification.hidden2tag", "wcbert_modeling.WCBertCRFForTokenClassification.crf.neg_log_likelihood_loss", "wcbert_modeling.WCBertCRFForTokenClassification.crf._viterbi_decode", "wcbert_modeling.WCBertCRFForTokenClassification.crf._viterbi_decode"], "methods", ["home.repos.pwc.inspect_result.liuwei1206_LEBERT.module.crf.CRF.neg_log_likelihood_loss", "home.repos.pwc.inspect_result.liuwei1206_LEBERT.module.crf.CRF._viterbi_decode", "home.repos.pwc.inspect_result.liuwei1206_LEBERT.module.crf.CRF._viterbi_decode"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "matched_word_ids", "=", "None", ",", "\n", "matched_word_mask", "=", "None", ",", "\n", "boundary_ids", "=", "None", ",", "\n", "labels", "=", "None", ",", "\n", "flag", "=", "\"Train\"", "\n", ")", ":", "\n", "        ", "matched_word_embeddings", "=", "self", ".", "word_embeddings", "(", "matched_word_ids", ")", "\n", "outputs", "=", "self", ".", "bert", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "matched_word_embeddings", "=", "matched_word_embeddings", ",", "\n", "matched_word_mask", "=", "matched_word_mask", ",", "\n", "boundary_ids", "=", "boundary_ids", "\n", ")", "\n", "\n", "sequence_output", "=", "outputs", "[", "0", "]", "\n", "sequence_output", "=", "self", ".", "dropout", "(", "sequence_output", ")", "\n", "logits", "=", "self", ".", "hidden2tag", "(", "sequence_output", ")", "\n", "\n", "if", "flag", "==", "'Train'", ":", "\n", "            ", "assert", "labels", "is", "not", "None", "\n", "loss", "=", "self", ".", "crf", ".", "neg_log_likelihood_loss", "(", "logits", ",", "attention_mask", ",", "labels", ")", "\n", "_", ",", "preds", "=", "self", ".", "crf", ".", "_viterbi_decode", "(", "logits", ",", "attention_mask", ")", "\n", "return", "(", "loss", ",", "preds", ")", "\n", "", "elif", "flag", "==", "'Predict'", ":", "\n", "            ", "_", ",", "preds", "=", "self", ".", "crf", ".", "_viterbi_decode", "(", "logits", ",", "attention_mask", ")", "\n", "return", "(", "preds", ",", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.None.wcbert_modeling.BertWordLSTMCRFForTokenClassification.__init__": [[519, 547], ["transformers.modeling_utils.PreTrainedModel.__init__", "torch.nn.Embedding", "torch.nn.Embedding", "transformers.modeling_bert.BertModel", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Tanh", "torch.nn.Tanh", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "module.bilstm.BiLSTM", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.nn.Parameter", "torch.nn.Parameter", "wcbert_modeling.BertWordLSTMCRFForTokenClassification.attn_W.data.normal_", "torch.nn.Linear", "torch.nn.Linear", "module.crf.CRF", "wcbert_modeling.BertWordLSTMCRFForTokenClassification.init_weights", "wcbert_modeling.BertWordLSTMCRFForTokenClassification.word_embeddings.weight.data.copy_", "print", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy"], "methods", ["home.repos.pwc.inspect_result.liuwei1206_LEBERT.feature.vocab.ItemVocabArray.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "pretrained_embeddings", ",", "num_labels", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "\n", "word_vocab_size", "=", "pretrained_embeddings", ".", "shape", "[", "0", "]", "\n", "embed_dim", "=", "pretrained_embeddings", ".", "shape", "[", "1", "]", "\n", "self", ".", "word_embeddings", "=", "nn", ".", "Embedding", "(", "word_vocab_size", ",", "embed_dim", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "HP_dropout", ")", "\n", "\n", "self", ".", "act", "=", "nn", ".", "Tanh", "(", ")", "\n", "self", ".", "word_transform", "=", "nn", ".", "Linear", "(", "config", ".", "word_embed_dim", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "word_word_weight", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "bilstm", "=", "BiLSTM", "(", "config", ".", "hidden_size", "*", "2", ",", "config", ".", "lstm_size", ",", "config", ".", "HP_dropout", ")", "\n", "\n", "\n", "attn_W", "=", "torch", ".", "zeros", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "attn_W", "=", "nn", ".", "Parameter", "(", "attn_W", ")", "\n", "self", ".", "attn_W", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "config", ".", "initializer_range", ")", "\n", "\n", "self", ".", "num_labels", "=", "num_labels", "\n", "self", ".", "hidden2tag", "=", "nn", ".", "Linear", "(", "config", ".", "lstm_size", "*", "2", ",", "num_labels", "+", "2", ")", "\n", "self", ".", "crf", "=", "CRF", "(", "num_labels", ",", "torch", ".", "cuda", ".", "is_available", "(", ")", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n", "## init the embedding", "\n", "self", ".", "word_embeddings", ".", "weight", ".", "data", ".", "copy_", "(", "torch", ".", "from_numpy", "(", "pretrained_embeddings", ")", ")", "\n", "print", "(", "\"Load pretrained embedding from file.........\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.None.wcbert_modeling.BertWordLSTMCRFForTokenClassification.forward": [[548, 596], ["wcbert_modeling.BertWordLSTMCRFForTokenClassification.word_embeddings", "wcbert_modeling.BertWordLSTMCRFForTokenClassification.bert", "wcbert_modeling.BertWordLSTMCRFForTokenClassification.word_transform", "wcbert_modeling.BertWordLSTMCRFForTokenClassification.act", "wcbert_modeling.BertWordLSTMCRFForTokenClassification.word_word_weight", "wcbert_modeling.BertWordLSTMCRFForTokenClassification.dropout", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "alpha.unsqueeze.unsqueeze.squeeze", "alpha.unsqueeze.unsqueeze.unsqueeze", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "wcbert_modeling.BertWordLSTMCRFForTokenClassification.dropout", "wcbert_modeling.BertWordLSTMCRFForTokenClassification.bilstm", "wcbert_modeling.BertWordLSTMCRFForTokenClassification.hidden2tag", "wcbert_modeling.BertWordLSTMCRFForTokenClassification.unsqueeze", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.nn.Softmax", "torch.nn.Softmax", "torch.nn.Softmax", "torch.nn.Softmax", "wcbert_modeling.BertWordLSTMCRFForTokenClassification.crf.neg_log_likelihood_loss", "wcbert_modeling.BertWordLSTMCRFForTokenClassification.crf._viterbi_decode", "wcbert_modeling.BertWordLSTMCRFForTokenClassification.crf._viterbi_decode", "matched_word_mask.float"], "methods", ["home.repos.pwc.inspect_result.liuwei1206_LEBERT.module.crf.CRF.neg_log_likelihood_loss", "home.repos.pwc.inspect_result.liuwei1206_LEBERT.module.crf.CRF._viterbi_decode", "home.repos.pwc.inspect_result.liuwei1206_LEBERT.module.crf.CRF._viterbi_decode"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "matched_word_ids", "=", "None", ",", "\n", "matched_word_mask", "=", "None", ",", "\n", "boundary_ids", "=", "None", ",", "\n", "labels", "=", "None", ",", "\n", "flag", "=", "\"Train\"", "\n", ")", ":", "\n", "        ", "matched_word_embeddings", "=", "self", ".", "word_embeddings", "(", "matched_word_ids", ")", "\n", "\n", "outputs", "=", "self", ".", "bert", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "token_type_ids", "=", "token_type_ids", "\n", ")", "\n", "sequence_output", "=", "outputs", "[", "0", "]", "\n", "\n", "matched_word_embeddings", "=", "self", ".", "word_transform", "(", "matched_word_embeddings", ")", "\n", "matched_word_embeddings", "=", "self", ".", "act", "(", "matched_word_embeddings", ")", "\n", "matched_word_embeddings", "=", "self", ".", "word_word_weight", "(", "matched_word_embeddings", ")", "\n", "matched_word_embeddings", "=", "self", ".", "dropout", "(", "matched_word_embeddings", ")", "\n", "\n", "alpha", "=", "torch", ".", "matmul", "(", "sequence_output", ".", "unsqueeze", "(", "2", ")", ",", "self", ".", "attn_W", ")", "# [N, L, 1, D]", "\n", "alpha", "=", "torch", ".", "matmul", "(", "alpha", ",", "torch", ".", "transpose", "(", "matched_word_embeddings", ",", "2", ",", "3", ")", ")", "# [N, L, 1, W]", "\n", "alpha", "=", "alpha", ".", "squeeze", "(", ")", "# [N, L, W]", "\n", "alpha", "=", "alpha", "+", "(", "1", "-", "matched_word_mask", ".", "float", "(", ")", ")", "*", "(", "-", "2", "**", "31", "+", "1", ")", "\n", "alpha", "=", "torch", ".", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "(", "alpha", ")", "# [N, L, W]", "\n", "alpha", "=", "alpha", ".", "unsqueeze", "(", "-", "1", ")", "# [N, L, W, 1]", "\n", "matched_word_embeddings", "=", "torch", ".", "sum", "(", "matched_word_embeddings", "*", "alpha", ",", "dim", "=", "2", ")", "# [N, L, D]", "\n", "\n", "## concat the embedding [B, L, N, D], [B, L, N]", "\n", "sequence_output", "=", "torch", ".", "cat", "(", "(", "sequence_output", ",", "matched_word_embeddings", ")", ",", "dim", "=", "-", "1", ")", "\n", "\n", "sequence_output", "=", "self", ".", "dropout", "(", "sequence_output", ")", "\n", "lstm_output", "=", "self", ".", "bilstm", "(", "sequence_output", ",", "attention_mask", ")", "\n", "logits", "=", "self", ".", "hidden2tag", "(", "lstm_output", ")", "\n", "\n", "if", "flag", "==", "'Train'", ":", "\n", "            ", "assert", "labels", "is", "not", "None", "\n", "loss", "=", "self", ".", "crf", ".", "neg_log_likelihood_loss", "(", "logits", ",", "attention_mask", ",", "labels", ")", "\n", "_", ",", "preds", "=", "self", ".", "crf", ".", "_viterbi_decode", "(", "logits", ",", "attention_mask", ")", "\n", "return", "(", "loss", ",", "preds", ")", "\n", "", "elif", "flag", "==", "'Predict'", ":", "\n", "            ", "_", ",", "preds", "=", "self", ".", "crf", ".", "_viterbi_decode", "(", "logits", ",", "attention_mask", ")", "\n", "return", "(", "preds", ",", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.None.wcbert_parser.get_argparse": [[7, 71], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument"], "function", ["None"], ["def", "get_argparse", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--data_dir\"", ",", "default", "=", "\"data/dataset/NER\"", ",", "type", "=", "str", ",", "help", "=", "\"The input data dir\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_dir\"", ",", "default", "=", "\"data/result\"", ",", "type", "=", "str", ",", "help", "=", "\"the output dir\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--overwrite_cache\"", ",", "default", "=", "True", ",", "help", "=", "\"overwrite the cache or not\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--logging_dir\"", ",", "default", "=", "'data/log'", ",", "type", "=", "str", ",", "help", "=", "\"the dir for log\"", ")", "\n", "\n", "## for from_pretrained parameters", "\n", "parser", ".", "add_argument", "(", "\"--model_name_or_path\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "help", "=", "\"the pretrained bert path\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--model_type\"", ",", "default", "=", "\"Bert_Token\"", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Bert_Token, BertCRF_Token, BertBiLSTMCRF_Token, WCBert_Token, WC....\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--config_name\"", ",", "default", "=", "\"data/berts/bert/config.json\"", ",", "type", "=", "str", ",", "help", "=", "\"the config of define model\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--vocab_file\"", ",", "default", "=", "\"data/berts/bert/vocab.txt\"", ",", "type", "=", "str", ",", "help", "=", "\"the vocab file for bert\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--word_vocab_file\"", ",", "default", "=", "\"data/vocab/final_vocab.txt\"", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "\"--label_file\"", ",", "default", "=", "\"data/dataset/NER/label.txt\"", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "\"--default_label\"", ",", "default", "=", "'O'", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "\"--word_embedding\"", ",", "default", "=", "\"data/embedding/word_embedding.txt\"", ",", "\n", "help", "=", "\"the embedding file path\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--saved_embedding_dir\"", ",", "default", "=", "\"data/embedding\"", ",", "type", "=", "str", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--do_train\"", ",", "default", "=", "False", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"Whether to run training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_eval\"", ",", "default", "=", "False", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"Whether to do evaluation\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_predict\"", ",", "default", "=", "False", ",", "action", "=", "\"store_true\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--evaluate_during_training\"", ",", "default", "=", "False", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"Whether do evuation during training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_seq_length\"", ",", "default", "=", "48", ",", "type", "=", "int", ",", "help", "=", "\"the max length of input sequence\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--per_gpu_train_batch_size\"", ",", "default", "=", "4", ",", "type", "=", "int", ",", "help", "=", "\"the training batch size\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--per_gpu_eval_batch_size\"", ",", "default", "=", "4", ",", "type", "=", "int", ",", "help", "=", "\"the eval batch size\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_train_epochs\"", ",", "default", "=", "2", ",", "type", "=", "int", ",", "help", "=", "\"training epoch, only work when max_step==-1\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--learning_rate\"", ",", "default", "=", "1e-4", ",", "type", "=", "float", ",", "help", "=", "\"The initial learning rate for Adam\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--weight_decay\"", ",", "default", "=", "0.0", ",", "type", "=", "float", ",", "help", "=", "\"the weight of L2 normalization\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--adam_epsilon\"", ",", "default", "=", "1e-8", ",", "type", "=", "float", ",", "help", "=", "\"Epsilon for Adam optimizer\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--sgd_momentum\"", ",", "default", "=", "0.9", ",", "type", "=", "float", ",", "help", "=", "\"momentum value for SGD\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_grad_norm\"", ",", "default", "=", "1.0", ",", "type", "=", "float", ",", "help", "=", "\"max clip gradient?\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_steps\"", ",", "default", "=", "-", "1", ",", "type", "=", "int", ",", "help", "=", "\"the total number of training steps\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--warmup_steps\"", ",", "default", "=", "95", ",", "type", "=", "int", ",", "help", "=", "\"the number of warmup steps\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--save_steps\"", ",", "default", "=", "800", ",", "type", "=", "int", ",", "help", "=", "\"How often to save the model chekcpoint\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--save_total_limit\"", ",", "default", "=", "50", ",", "type", "=", "int", ",", "help", "=", "\"the total number of saved checkpoints\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--seed\"", ",", "default", "=", "106524", ",", "type", "=", "int", ",", "help", "=", "\"the seed used to initiate parameters\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--logging_steps\"", ",", "default", "=", "4", ",", "type", "=", "int", ",", "help", "=", "\"Log every X updates steps\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_shuffle\"", ",", "default", "=", "True", ",", "type", "=", "bool", ",", "help", "=", "\"do shuffle for each piece dataset or not\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--word_embed_dim\"", ",", "default", "=", "200", ",", "type", "=", "int", ",", "help", "=", "\"the dimension of item embedding\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_scan_num\"", ",", "default", "=", "10000", ",", "type", "=", "int", ",", "help", "=", "\"The boundary of data files\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_word_num\"", ",", "default", "=", "5", ",", "type", "=", "int", ")", "\n", "\n", "## machine parameter", "\n", "parser", ".", "add_argument", "(", "\"--no_cuda\"", ",", "default", "=", "False", ",", "help", "=", "\"Do not use CUDA even it is available\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--fp16\"", ",", "default", "=", "False", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"Whether use fp16 to old_train\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--fp16_opt_level\"", ",", "default", "=", "\"O1\"", ",", "type", "=", "str", ",", "\n", "help", "=", "\"level selected in ['O0', 'O1', 'O2', 'O3']\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--gradient_accumulation_steps\"", ",", "default", "=", "1", ",", "type", "=", "int", ",", "\n", "help", "=", "\"Number of updates steps to accumulatate before performing update\"", ")", "\n", "\n", "# for distribute training", "\n", "parser", ".", "add_argument", "(", "\"--nodes\"", ",", "default", "=", "1", ",", "type", "=", "int", ",", "\n", "help", "=", "\"the total number of nodes(machines) we are going to use\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--n_gpu\"", ",", "default", "=", "1", ",", "type", "=", "int", ",", "\n", "help", "=", "\"ranking within the nodes\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--local_rank\"", ",", "default", "=", "-", "1", ",", "type", "=", "int", ",", "\n", "help", "=", "\"the rank of current node within all nodes, goes from 0 to args.nodes-1\"", ")", "\n", "\n", "\n", "return", "parser", "", "", ""]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.module.lexicon_tree.TrieNode.__init__": [[9, 12], ["collections.defaultdict"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "children", "=", "collections", ".", "defaultdict", "(", "TrieNode", ")", "\n", "self", ".", "is_word", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.module.lexicon_tree.Trie.__init__": [[22, 29], ["lexicon_tree.TrieNode"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "use_single", "=", "True", ")", ":", "\n", "        ", "self", ".", "root", "=", "TrieNode", "(", ")", "\n", "self", ".", "max_depth", "=", "0", "\n", "if", "use_single", ":", "\n", "            ", "self", ".", "min_len", "=", "0", "\n", "", "else", ":", "\n", "            ", "self", ".", "min_len", "=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.module.lexicon_tree.Trie.insert": [[30, 39], ["None"], "methods", ["None"], ["", "", "def", "insert", "(", "self", ",", "word", ")", ":", "\n", "        ", "current", "=", "self", ".", "root", "\n", "deep", "=", "0", "\n", "for", "letter", "in", "word", ":", "\n", "            ", "current", "=", "current", ".", "children", "[", "letter", "]", "\n", "deep", "+=", "1", "\n", "", "current", ".", "is_word", "=", "True", "\n", "if", "deep", ">", "self", ".", "max_depth", ":", "\n", "            ", "self", ".", "max_depth", "=", "deep", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.module.lexicon_tree.Trie.search": [[40, 48], ["current.children.get.children.get.children.get"], "methods", ["None"], ["", "", "def", "search", "(", "self", ",", "word", ")", ":", "\n", "        ", "current", "=", "self", ".", "root", "\n", "for", "letter", "in", "word", ":", "\n", "            ", "current", "=", "current", ".", "children", ".", "get", "(", "letter", ")", "\n", "\n", "if", "current", "is", "None", ":", "\n", "                ", "return", "False", "\n", "", "", "return", "current", ".", "is_word", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.module.lexicon_tree.Trie.enumerateMatch": [[49, 66], ["len", "lexicon_tree.Trie.search", "matched.insert", "len", "len", "space.join"], "methods", ["home.repos.pwc.inspect_result.liuwei1206_LEBERT.module.lexicon_tree.Trie.search", "home.repos.pwc.inspect_result.liuwei1206_LEBERT.module.lexicon_tree.Trie.insert"], ["", "def", "enumerateMatch", "(", "self", ",", "str", ",", "space", "=", "\"\"", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            str: \u9700\u8981\u5339\u914d\u7684\u8bcd\n        Return:\n            \u8fd4\u56de\u5339\u914d\u7684\u8bcd, \u5982\u679c\u5b58\u5728\u591a\u5b57\u8bcd\uff0c\u5219\u4f1a\u7b5b\u53bb\u5355\u5b57\u8bcd\n        \"\"\"", "\n", "matched", "=", "[", "]", "\n", "while", "len", "(", "str", ")", ">", "self", ".", "min_len", ":", "\n", "            ", "if", "self", ".", "search", "(", "str", ")", ":", "\n", "                ", "matched", ".", "insert", "(", "0", ",", "space", ".", "join", "(", "str", "[", ":", "]", ")", ")", "# \u77ed\u7684\u8bcd\u603b\u662f\u5728\u6700\u524d\u9762", "\n", "", "del", "str", "[", "-", "1", "]", "\n", "\n", "", "if", "len", "(", "matched", ")", ">", "1", "and", "len", "(", "matched", "[", "0", "]", ")", "==", "1", ":", "# filter single character word", "\n", "            ", "matched", "=", "matched", "[", "1", ":", "]", "\n", "\n", "", "return", "matched", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.module.crf.CRF.__init__": [[30, 42], ["torch.Module.__init__", "print", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "init_transitions.cuda.cuda.cuda"], "methods", ["home.repos.pwc.inspect_result.liuwei1206_LEBERT.feature.vocab.ItemVocabArray.__init__"], ["    ", "def", "__init__", "(", "self", ",", "tagset_size", ",", "gpu", ")", ":", "\n", "        ", "super", "(", "CRF", ",", "self", ")", ".", "__init__", "(", ")", "\n", "print", "(", "\"build batched crf...\"", ")", "\n", "self", ".", "gpu", "=", "gpu", "\n", "self", ".", "average_batch", "=", "False", "\n", "self", ".", "tagset_size", "=", "tagset_size", "\n", "# # We add 2 here, because of START_TAG and STOP_TAG", "\n", "# # transitions (f_tag_size, t_tag_size), transition value from f_tag to t_tag", "\n", "init_transitions", "=", "torch", ".", "zeros", "(", "self", ".", "tagset_size", "+", "2", ",", "self", ".", "tagset_size", "+", "2", ")", "\n", "if", "self", ".", "gpu", ":", "\n", "            ", "init_transitions", "=", "init_transitions", ".", "cuda", "(", ")", "\n", "", "self", ".", "transitions", "=", "nn", ".", "Parameter", "(", "init_transitions", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.module.crf.CRF._calculate_PZ": [[43, 79], ["feats.transpose().contiguous().view().expand.transpose().contiguous().view().expand.size", "feats.transpose().contiguous().view().expand.transpose().contiguous().view().expand.size", "feats.transpose().contiguous().view().expand.transpose().contiguous().view().expand.size", "mask.bool().transpose().contiguous.bool().transpose().contiguous.bool().transpose().contiguous", "feats.transpose().contiguous().view().expand.transpose().contiguous().view().expand.transpose().contiguous().view().expand", "scores.view.view.view", "enumerate", "next", "inivalues[].clone().view", "crf.log_sum_exp", "crf.CRF.transitions.view().expand", "crf.log_sum_exp", "mask_idx.contiguous().view.contiguous().view.view().expand", "log_sum_exp.masked_select", "mask_idx.contiguous().view.contiguous().view.contiguous().view", "inivalues[].clone().view.masked_scatter_", "crf.CRF.transitions.view().expand", "inivalues[].clone().view.contiguous().view().expand", "final_partition.sum", "mask.bool().transpose().contiguous.bool().transpose().contiguous.bool().transpose", "feats.transpose().contiguous().view().expand.transpose().contiguous().view().expand.transpose().contiguous().view", "inivalues[].clone", "inivalues[].clone().view.contiguous().view().expand", "crf.CRF.transitions.view", "mask_idx.contiguous().view.contiguous().view.view", "mask_idx.contiguous().view.contiguous().view.contiguous", "crf.CRF.transitions.view", "inivalues[].clone().view.contiguous().view", "mask.bool().transpose().contiguous.bool().transpose().contiguous.bool", "feats.transpose().contiguous().view().expand.transpose().contiguous().view().expand.transpose().contiguous", "inivalues[].clone().view.contiguous().view", "inivalues[].clone().view.contiguous", "feats.transpose().contiguous().view().expand.transpose().contiguous().view().expand.transpose", "inivalues[].clone().view.contiguous"], "methods", ["home.repos.pwc.inspect_result.liuwei1206_LEBERT.module.crf.log_sum_exp", "home.repos.pwc.inspect_result.liuwei1206_LEBERT.module.crf.log_sum_exp"], ["", "def", "_calculate_PZ", "(", "self", ",", "feats", ",", "mask", ")", ":", "\n", "        ", "\"\"\"\n            input:\n                feats: (batch, seq_len, self.tag_size+2)\n                masks: (batch, seq_len)\n        \"\"\"", "\n", "batch_size", "=", "feats", ".", "size", "(", "0", ")", "\n", "seq_len", "=", "feats", ".", "size", "(", "1", ")", "\n", "tag_size", "=", "feats", ".", "size", "(", "2", ")", "\n", "assert", "(", "tag_size", "==", "self", ".", "tagset_size", "+", "2", ")", "\n", "mask", "=", "mask", ".", "bool", "(", ")", ".", "transpose", "(", "1", ",", "0", ")", ".", "contiguous", "(", ")", "\n", "ins_num", "=", "seq_len", "*", "batch_size", "\n", "feats", "=", "feats", ".", "transpose", "(", "1", ",", "0", ")", ".", "contiguous", "(", ")", ".", "view", "(", "ins_num", ",", "1", ",", "tag_size", ")", ".", "expand", "(", "ins_num", ",", "tag_size", ",", "tag_size", ")", "\n", "scores", "=", "feats", "+", "self", ".", "transitions", ".", "view", "(", "1", ",", "tag_size", ",", "tag_size", ")", ".", "expand", "(", "ins_num", ",", "tag_size", ",", "tag_size", ")", "\n", "scores", "=", "scores", ".", "view", "(", "seq_len", ",", "batch_size", ",", "tag_size", ",", "tag_size", ")", "\n", "seq_iter", "=", "enumerate", "(", "scores", ")", "\n", "_", ",", "inivalues", "=", "next", "(", "seq_iter", ")", "\n", "partition", "=", "inivalues", "[", ":", ",", "START_TAG", ",", ":", "]", ".", "clone", "(", ")", ".", "view", "(", "batch_size", ",", "tag_size", ",", "1", ")", "# bat_size * to_target_size", "\n", "\n", "## add start score (from start to all tag, duplicate to batch_size)", "\n", "# partition = partition + self.transitions[START_TAG,:].view(1, tag_size, 1).expand(batch_size, tag_size, 1)", "\n", "# iter over last scores", "\n", "for", "idx", ",", "cur_values", "in", "seq_iter", ":", "\n", "            ", "cur_values", "=", "cur_values", "+", "partition", ".", "contiguous", "(", ")", ".", "view", "(", "batch_size", ",", "tag_size", ",", "1", ")", ".", "expand", "(", "batch_size", ",", "tag_size", ",", "tag_size", ")", "\n", "cur_partition", "=", "log_sum_exp", "(", "cur_values", ",", "tag_size", ")", "\n", "mask_idx", "=", "mask", "[", "idx", ",", ":", "]", "\n", "mask_idx", "=", "mask_idx", ".", "view", "(", "batch_size", ",", "1", ")", ".", "expand", "(", "batch_size", ",", "tag_size", ")", "\n", "masked_cur_partition", "=", "cur_partition", ".", "masked_select", "(", "mask_idx", ")", "\n", "mask_idx", "=", "mask_idx", ".", "contiguous", "(", ")", ".", "view", "(", "batch_size", ",", "tag_size", ",", "1", ")", "\n", "partition", ".", "masked_scatter_", "(", "mask_idx", ",", "masked_cur_partition", ")", "\n", "\n", "", "cur_values", "=", "self", ".", "transitions", ".", "view", "(", "1", ",", "tag_size", ",", "tag_size", ")", ".", "expand", "(", "batch_size", ",", "tag_size", ",", "tag_size", ")", "+", "partition", ".", "contiguous", "(", ")", ".", "view", "(", "batch_size", ",", "tag_size", ",", "1", ")", ".", "expand", "(", "batch_size", ",", "tag_size", ",", "tag_size", ")", "\n", "cur_partition", "=", "log_sum_exp", "(", "cur_values", ",", "tag_size", ")", "\n", "final_partition", "=", "cur_partition", "[", ":", ",", "STOP_TAG", "]", "\n", "return", "final_partition", ".", "sum", "(", ")", ",", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.module.crf.CRF._viterbi_decode": [[80, 147], ["feats.transpose().contiguous().view().expand.transpose().contiguous().view().expand.size", "feats.transpose().contiguous().view().expand.transpose().contiguous().view().expand.size", "feats.transpose().contiguous().view().expand.transpose().contiguous().view().expand.size", "torch.sum().view().long", "torch.sum().view().long", "torch.sum().view().long", "torch.sum().view().long", "torch.sum().view().long", "torch.sum().view().long", "torch.sum().view().long", "torch.sum().view().long", "torch.sum().view().long", "torch.sum().view().long", "torch.sum().view().long", "torch.sum().view().long", "torch.sum().view().long", "torch.sum().view().long", "torch.sum().view().long", "torch.sum().view().long", "mask.transpose().contiguous.transpose().contiguous.transpose().contiguous", "feats.transpose().contiguous().view().expand.transpose().contiguous().view().expand.transpose().contiguous().view().expand", "scores.view.view.view", "enumerate", "list", "list", "next", "inivalues[].clone().view", "torch.cat().view().transpose().contiguous.append", "torch.cat().view().transpose().contiguous.append", "torch.cat().view().transpose().contiguous.append", "torch.cat().view().transpose().contiguous.append", "torch.cat().view().transpose().contiguous", "torch.cat().view().transpose().contiguous", "torch.cat().view().transpose().contiguous", "torch.cat().view().transpose().contiguous", "torch.cat().view().transpose().contiguous", "torch.cat().view().transpose().contiguous", "torch.cat().view().transpose().contiguous", "torch.cat().view().transpose().contiguous", "torch.cat().view().transpose().contiguous", "torch.cat().view().transpose().contiguous", "torch.cat().view().transpose().contiguous", "torch.cat().view().transpose().contiguous", "torch.cat().view().transpose().contiguous", "torch.cat().view().transpose().contiguous", "torch.cat().view().transpose().contiguous", "torch.cat().view().transpose().contiguous", "torch.gather().view", "torch.gather().view", "torch.gather().view", "torch.gather().view", "torch.gather().view", "torch.gather().view", "torch.gather().view", "torch.gather().view", "torch.gather().view", "torch.gather().view", "torch.gather().view", "torch.gather().view", "torch.gather().view", "torch.gather().view", "torch.gather().view", "torch.gather().view", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "back_points.transpose().contiguous.transpose().contiguous.append", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.gather.contiguous().view().expand", "torch.gather.contiguous().view().expand", "torch.gather.contiguous().view().expand", "torch.gather.contiguous().view().expand", "back_points.transpose().contiguous.transpose().contiguous.transpose().contiguous", "back_points.transpose().contiguous.transpose().contiguous.scatter_", "back_points.transpose().contiguous.transpose().contiguous.transpose().contiguous", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "range", "decode_idx.cuda.cuda.long().transpose", "crf.CRF.transitions.view().expand", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "partition.view.view.view", "torch.cat().view().transpose().contiguous.append", "torch.cat().view().transpose().contiguous.append", "torch.cat().view().transpose().contiguous.append", "torch.cat().view().transpose().contiguous.append", "cur_bp.masked_fill_", "back_points.transpose().contiguous.transpose().contiguous.append", "torch.sum().view().long.view().expand", "torch.sum().view().long.view().expand", "torch.sum().view().long.view().expand", "torch.sum().view().long.view().expand", "torch.gather().view.expand", "torch.gather().view.expand", "torch.gather().view.expand", "torch.gather().view.expand", "crf.CRF.transitions.view().expand", "pad_zero.cuda.cuda.cuda", "decode_idx.cuda.cuda.cuda", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather.data.view", "torch.gather.data.view", "torch.gather.data.view", "torch.gather.data.view", "torch.sum().view", "torch.sum().view", "torch.sum().view", "torch.sum().view", "torch.sum().view", "torch.sum().view", "torch.sum().view", "torch.sum().view", "torch.sum().view", "torch.sum().view", "torch.sum().view", "torch.sum().view", "torch.sum().view", "torch.sum().view", "torch.sum().view", "torch.sum().view", "mask.transpose().contiguous.transpose().contiguous.transpose", "feats.transpose().contiguous().view().expand.transpose().contiguous().view().expand.transpose().contiguous().view", "inivalues[].clone", "partition.view.view.contiguous().view().expand", "mask[].view().expand", "torch.cat().view().transpose", "torch.cat().view().transpose", "torch.cat().view().transpose", "torch.cat().view().transpose", "torch.cat().view().transpose", "torch.cat().view().transpose", "torch.cat().view().transpose", "torch.cat().view().transpose", "torch.cat().view().transpose", "torch.cat().view().transpose", "torch.cat().view().transpose", "torch.cat().view().transpose", "torch.cat().view().transpose", "torch.cat().view().transpose", "torch.cat().view().transpose", "torch.cat().view().transpose", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.gather.contiguous().view", "torch.gather.contiguous().view", "torch.gather.contiguous().view", "torch.gather.contiguous().view", "back_points.transpose().contiguous.transpose().contiguous.transpose", "back_points.transpose().contiguous.transpose().contiguous.transpose", "len", "torch.gather.contiguous().view", "torch.gather.contiguous().view", "torch.gather.contiguous().view", "torch.gather.contiguous().view", "decode_idx.cuda.cuda.long", "crf.CRF.transitions.view", "mask.transpose().contiguous.transpose().contiguous.long", "partition.view.view.size", "partition.view.view.size", "torch.sum().view().long.view", "torch.sum().view().long.view", "torch.sum().view().long.view", "torch.sum().view().long.view", "crf.CRF.transitions.view", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "feats.transpose().contiguous().view().expand.transpose().contiguous().view().expand.transpose().contiguous", "partition.view.view.contiguous().view", "mask[].view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.gather.contiguous", "torch.gather.contiguous", "torch.gather.contiguous", "torch.gather.contiguous", "torch.gather.contiguous", "torch.gather.contiguous", "torch.gather.contiguous", "torch.gather.contiguous", "mask.transpose().contiguous.transpose().contiguous.long", "feats.transpose().contiguous().view().expand.transpose().contiguous().view().expand.transpose", "partition.view.view.contiguous", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "_viterbi_decode", "(", "self", ",", "feats", ",", "mask", ")", ":", "\n", "        ", "\"\"\"\n            input:\n                feats: (batch, seq_len, self.tag_size+2)\n                mask: (batch, seq_len)\n            output:\n                decode_idx: (batch, seq_len) decoded sequence\n                path_score: (batch, 1) corresponding score for each sequence (to be implementated)\n        \"\"\"", "\n", "batch_size", "=", "feats", ".", "size", "(", "0", ")", "\n", "seq_len", "=", "feats", ".", "size", "(", "1", ")", "\n", "tag_size", "=", "feats", ".", "size", "(", "2", ")", "\n", "assert", "(", "tag_size", "==", "self", ".", "tagset_size", "+", "2", ")", "\n", "length_mask", "=", "torch", ".", "sum", "(", "mask", ".", "long", "(", ")", ",", "dim", "=", "1", ")", ".", "view", "(", "batch_size", ",", "1", ")", ".", "long", "(", ")", "\n", "mask", "=", "mask", ".", "transpose", "(", "1", ",", "0", ")", ".", "contiguous", "(", ")", "\n", "ins_num", "=", "seq_len", "*", "batch_size", "\n", "feats", "=", "feats", ".", "transpose", "(", "1", ",", "0", ")", ".", "contiguous", "(", ")", ".", "view", "(", "ins_num", ",", "1", ",", "tag_size", ")", ".", "expand", "(", "ins_num", ",", "tag_size", ",", "tag_size", ")", "\n", "scores", "=", "feats", "+", "self", ".", "transitions", ".", "view", "(", "1", ",", "tag_size", ",", "tag_size", ")", ".", "expand", "(", "ins_num", ",", "tag_size", ",", "tag_size", ")", "\n", "scores", "=", "scores", ".", "view", "(", "seq_len", ",", "batch_size", ",", "tag_size", ",", "tag_size", ")", "\n", "\n", "seq_iter", "=", "enumerate", "(", "scores", ")", "\n", "back_points", "=", "list", "(", ")", "\n", "partition_history", "=", "list", "(", ")", "\n", "mask", "=", "(", "1", "-", "mask", ".", "long", "(", ")", ")", ".", "bool", "(", ")", "\n", "_", ",", "inivalues", "=", "next", "(", "seq_iter", ")", "\n", "partition", "=", "inivalues", "[", ":", ",", "START_TAG", ",", ":", "]", ".", "clone", "(", ")", ".", "view", "(", "batch_size", ",", "tag_size", ",", "1", ")", "# bat_size * to_target_size", "\n", "partition_history", ".", "append", "(", "partition", ")", "\n", "for", "idx", ",", "cur_values", "in", "seq_iter", ":", "\n", "            ", "cur_values", "=", "cur_values", "+", "partition", ".", "contiguous", "(", ")", ".", "view", "(", "batch_size", ",", "tag_size", ",", "1", ")", ".", "expand", "(", "batch_size", ",", "tag_size", ",", "tag_size", ")", "\n", "partition", ",", "cur_bp", "=", "torch", ".", "max", "(", "cur_values", ",", "1", ")", "\n", "\n", "partition", "=", "partition", ".", "view", "(", "partition", ".", "size", "(", ")", "[", "0", "]", ",", "partition", ".", "size", "(", ")", "[", "1", "]", ",", "1", ")", "\n", "\n", "partition_history", ".", "append", "(", "partition", ")", "\n", "cur_bp", ".", "masked_fill_", "(", "mask", "[", "idx", "]", ".", "view", "(", "batch_size", ",", "1", ")", ".", "expand", "(", "batch_size", ",", "tag_size", ")", ",", "0", ")", "\n", "back_points", ".", "append", "(", "cur_bp", ")", "\n", "### add score to final STOP_TAG", "\n", "", "partition_history", "=", "torch", ".", "cat", "(", "partition_history", ",", "0", ")", ".", "view", "(", "seq_len", ",", "batch_size", ",", "-", "1", ")", ".", "transpose", "(", "1", ",", "0", ")", ".", "contiguous", "(", ")", "\n", "last_position", "=", "length_mask", ".", "view", "(", "batch_size", ",", "1", ",", "1", ")", ".", "expand", "(", "batch_size", ",", "1", ",", "tag_size", ")", "-", "1", "\n", "last_partition", "=", "torch", ".", "gather", "(", "partition_history", ",", "1", ",", "last_position", ")", ".", "view", "(", "batch_size", ",", "tag_size", ",", "1", ")", "\n", "last_values", "=", "last_partition", ".", "expand", "(", "batch_size", ",", "tag_size", ",", "tag_size", ")", "+", "self", ".", "transitions", ".", "view", "(", "1", ",", "tag_size", ",", "tag_size", ")", ".", "expand", "(", "batch_size", ",", "tag_size", ",", "tag_size", ")", "\n", "_", ",", "last_bp", "=", "torch", ".", "max", "(", "last_values", ",", "1", ")", "\n", "pad_zero", "=", "torch", ".", "zeros", "(", "batch_size", ",", "tag_size", ")", ".", "long", "(", ")", "\n", "if", "self", ".", "gpu", ":", "\n", "            ", "pad_zero", "=", "pad_zero", ".", "cuda", "(", ")", "\n", "", "back_points", ".", "append", "(", "pad_zero", ")", "\n", "back_points", "=", "torch", ".", "cat", "(", "back_points", ")", ".", "view", "(", "seq_len", ",", "batch_size", ",", "tag_size", ")", "\n", "\n", "## select end ids in STOP_TAG", "\n", "pointer", "=", "last_bp", "[", ":", ",", "STOP_TAG", "]", "\n", "insert_last", "=", "pointer", ".", "contiguous", "(", ")", ".", "view", "(", "batch_size", ",", "1", ",", "1", ")", ".", "expand", "(", "batch_size", ",", "1", ",", "tag_size", ")", "\n", "back_points", "=", "back_points", ".", "transpose", "(", "1", ",", "0", ")", ".", "contiguous", "(", ")", "\n", "back_points", ".", "scatter_", "(", "1", ",", "last_position", ",", "insert_last", ")", "\n", "back_points", "=", "back_points", ".", "transpose", "(", "1", ",", "0", ")", ".", "contiguous", "(", ")", "\n", "# decode_idx = autograd.Variable(torch.LongTensor(seq_len, batch_size))", "\n", "decode_idx", "=", "torch", ".", "zeros", "(", "seq_len", ",", "batch_size", ")", "\n", "if", "self", ".", "gpu", ":", "\n", "            ", "decode_idx", "=", "decode_idx", ".", "cuda", "(", ")", "\n", "", "decode_idx", "[", "-", "1", "]", "=", "pointer", ".", "data", "\n", "for", "idx", "in", "range", "(", "len", "(", "back_points", ")", "-", "2", ",", "-", "1", ",", "-", "1", ")", ":", "\n", "            ", "pointer", "=", "torch", ".", "gather", "(", "back_points", "[", "idx", "]", ",", "1", ",", "pointer", ".", "contiguous", "(", ")", ".", "view", "(", "batch_size", ",", "1", ")", ")", "\n", "decode_idx", "[", "idx", "]", "=", "pointer", ".", "data", ".", "view", "(", "-", "1", ")", "\n", "", "path_score", "=", "None", "\n", "decode_idx", "=", "decode_idx", ".", "long", "(", ")", ".", "transpose", "(", "1", ",", "0", ")", "\n", "return", "path_score", ",", "decode_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.module.crf.CRF.forward": [[148, 151], ["crf.CRF._viterbi_decode"], "methods", ["home.repos.pwc.inspect_result.liuwei1206_LEBERT.module.crf.CRF._viterbi_decode"], ["", "def", "forward", "(", "self", ",", "feats", ")", ":", "\n", "        ", "path_score", ",", "best_path", "=", "self", ".", "_viterbi_decode", "(", "feats", ")", "\n", "return", "path_score", ",", "best_path", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.module.crf.CRF._score_sentence": [[152, 187], ["scores.size", "scores.size", "scores.size", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "range", "crf.CRF.transitions[].contiguous().view().expand", "torch.sum().view().long", "torch.sum().view().long", "torch.sum().view().long", "torch.sum().view().long", "torch.sum().view().long", "torch.sum().view().long", "torch.sum().view().long", "torch.sum().view().long", "torch.sum().view().long", "torch.sum().view().long", "torch.sum().view().long", "torch.sum().view().long", "torch.sum().view().long", "torch.sum().view().long", "torch.sum().view().long", "torch.sum().view().long", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "new_tags.cuda.cuda.transpose().contiguous().view", "torch.gather().view", "torch.gather().view", "torch.gather().view", "torch.gather().view", "torch.gather().view", "torch.gather().view", "torch.gather().view", "torch.gather().view", "torch.gather().view", "torch.gather().view", "torch.gather().view", "torch.gather().view", "torch.gather().view", "torch.gather().view", "torch.gather().view", "torch.gather().view", "tg_energy.masked_select.masked_select.masked_select", "new_tags.cuda.cuda.cuda", "mask.bool().transpose", "tg_energy.masked_select.masked_select.sum", "torch.gather.sum", "torch.gather.sum", "torch.gather.sum", "torch.gather.sum", "crf.CRF.transitions[].contiguous().view", "torch.sum().view", "torch.sum().view", "torch.sum().view", "torch.sum().view", "torch.sum().view", "torch.sum().view", "torch.sum().view", "torch.sum().view", "torch.sum().view", "torch.sum().view", "torch.sum().view", "torch.sum().view", "torch.sum().view", "torch.sum().view", "torch.sum().view", "torch.sum().view", "new_tags.cuda.cuda.transpose().contiguous", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "scores.view", "new_tags.cuda.cuda.long", "mask.bool", "crf.CRF.transitions[].contiguous", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "new_tags.cuda.cuda.transpose", "mask.long"], "methods", ["None"], ["", "def", "_score_sentence", "(", "self", ",", "scores", ",", "mask", ",", "tags", ")", ":", "\n", "        ", "\"\"\"\n            input:\n                scores: variable (seq_len, batch, tag_size, tag_size)\n                mask: (batch, seq_len)\n                tags: tensor  (batch, seq_len)\n            output:\n                score: sum of score for gold sequences within whole batch\n        \"\"\"", "\n", "# Gives the score of a provided tag sequence", "\n", "batch_size", "=", "scores", ".", "size", "(", "1", ")", "\n", "seq_len", "=", "scores", ".", "size", "(", "0", ")", "\n", "tag_size", "=", "scores", ".", "size", "(", "2", ")", "\n", "# new_tags = autograd.Variable(torch.LongTensor(batch_size, seq_len))", "\n", "new_tags", "=", "torch", ".", "zeros", "(", "batch_size", ",", "seq_len", ")", "\n", "if", "self", ".", "gpu", ":", "\n", "            ", "new_tags", "=", "new_tags", ".", "cuda", "(", ")", "\n", "", "for", "idx", "in", "range", "(", "seq_len", ")", ":", "\n", "            ", "if", "idx", "==", "0", ":", "\n", "                ", "new_tags", "[", ":", ",", "0", "]", "=", "(", "tag_size", "-", "2", ")", "*", "tag_size", "+", "tags", "[", ":", ",", "0", "]", "\n", "", "else", ":", "\n", "                ", "new_tags", "[", ":", ",", "idx", "]", "=", "tags", "[", ":", ",", "idx", "-", "1", "]", "*", "tag_size", "+", "tags", "[", ":", ",", "idx", "]", "\n", "\n", "## transition for label to STOP_TAG", "\n", "", "", "end_transition", "=", "self", ".", "transitions", "[", ":", ",", "STOP_TAG", "]", ".", "contiguous", "(", ")", ".", "view", "(", "1", ",", "tag_size", ")", ".", "expand", "(", "batch_size", ",", "tag_size", ")", "\n", "length_mask", "=", "torch", ".", "sum", "(", "mask", ".", "long", "(", ")", ",", "dim", "=", "1", ")", ".", "view", "(", "batch_size", ",", "1", ")", ".", "long", "(", ")", "\n", "end_ids", "=", "torch", ".", "gather", "(", "tags", ",", "1", ",", "length_mask", "-", "1", ")", "\n", "end_energy", "=", "torch", ".", "gather", "(", "end_transition", ",", "1", ",", "end_ids", ")", "\n", "\n", "new_tags", "=", "new_tags", ".", "transpose", "(", "1", ",", "0", ")", ".", "contiguous", "(", ")", ".", "view", "(", "seq_len", ",", "batch_size", ",", "1", ")", "\n", "tg_energy", "=", "torch", ".", "gather", "(", "scores", ".", "view", "(", "seq_len", ",", "batch_size", ",", "-", "1", ")", ",", "2", ",", "new_tags", ".", "long", "(", ")", ")", ".", "view", "(", "seq_len", ",", "batch_size", ")", "# seq_len * bat_size", "\n", "tg_energy", "=", "tg_energy", ".", "masked_select", "(", "mask", ".", "bool", "(", ")", ".", "transpose", "(", "1", ",", "0", ")", ")", "\n", "\n", "gold_score", "=", "tg_energy", ".", "sum", "(", ")", "+", "end_energy", ".", "sum", "(", ")", "\n", "return", "gold_score", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.module.crf.CRF.neg_log_likelihood_loss": [[188, 198], ["feats.size", "crf.CRF._calculate_PZ", "crf.CRF._score_sentence"], "methods", ["home.repos.pwc.inspect_result.liuwei1206_LEBERT.module.crf.CRF._calculate_PZ", "home.repos.pwc.inspect_result.liuwei1206_LEBERT.module.crf.CRF._score_sentence"], ["", "def", "neg_log_likelihood_loss", "(", "self", ",", "feats", ",", "mask", ",", "tags", ")", ":", "\n", "# nonegative log likelihood", "\n", "        ", "batch_size", "=", "feats", ".", "size", "(", "0", ")", "\n", "forward_score", ",", "scores", "=", "self", ".", "_calculate_PZ", "(", "feats", ",", "mask", ")", "\n", "gold_score", "=", "self", ".", "_score_sentence", "(", "scores", ",", "mask", ",", "tags", ")", "\n", "\n", "if", "self", ".", "average_batch", ":", "\n", "            ", "return", "(", "forward_score", "-", "gold_score", ")", "/", "batch_size", "\n", "", "else", ":", "\n", "            ", "return", "forward_score", "-", "gold_score", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.module.crf.log_sum_exp": [[16, 28], ["torch.max", "torch.max", "torch.max", "torch.max", "torch.gather().view", "torch.gather().view", "torch.gather().view", "torch.gather().view", "torch.gather().view.view", "torch.log().view", "torch.log().view", "torch.log().view", "torch.log().view", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "idx.view", "torch.log", "torch.log", "torch.log", "torch.log", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.gather().view.expand_as"], "function", ["None"], ["def", "log_sum_exp", "(", "vec", ",", "m_size", ")", ":", "\n", "    ", "\"\"\"\n    calculate log of exp sum\n    args:\n        vec (batch_size, vanishing_dim, hidden_dim) : input tensor\n        m_size : hidden_dim\n    return:\n        batch_size, hidden_dim\n    \"\"\"", "\n", "_", ",", "idx", "=", "torch", ".", "max", "(", "vec", ",", "1", ")", "# B * 1 * M", "\n", "max_score", "=", "torch", ".", "gather", "(", "vec", ",", "1", ",", "idx", ".", "view", "(", "-", "1", ",", "1", ",", "m_size", ")", ")", ".", "view", "(", "-", "1", ",", "1", ",", "m_size", ")", "# B * M", "\n", "return", "max_score", ".", "view", "(", "-", "1", ",", "m_size", ")", "+", "torch", ".", "log", "(", "torch", ".", "sum", "(", "torch", ".", "exp", "(", "vec", "-", "max_score", ".", "expand_as", "(", "vec", ")", ")", ",", "1", ")", ")", ".", "view", "(", "-", "1", ",", "m_size", ")", "# B * M", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.module.sampler.SequentialDistributedSampler.__init__": [[23, 38], ["int", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "math.ceil", "torch.distributed.is_available", "torch.distributed.is_available", "torch.distributed.is_available", "torch.distributed.is_available", "RuntimeError", "torch.distributed.is_available", "torch.distributed.is_available", "torch.distributed.is_available", "torch.distributed.is_available", "RuntimeError", "len"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "dataset", ",", "num_replicas", "=", "None", ",", "rank", "=", "None", ",", "do_shuffle", "=", "False", ")", ":", "\n", "        ", "if", "num_replicas", "is", "None", ":", "\n", "            ", "if", "not", "torch", ".", "distributed", ".", "is_available", "(", ")", ":", "\n", "                ", "raise", "RuntimeError", "(", "\"Requires distributed package to be available\"", ")", "\n", "", "num_replicas", "=", "torch", ".", "distributed", ".", "get_world_size", "(", ")", "\n", "", "if", "rank", "is", "None", ":", "\n", "            ", "if", "not", "torch", ".", "distributed", ".", "is_available", "(", ")", ":", "\n", "                ", "raise", "RuntimeError", "(", "\"Requires distributed package to be available\"", ")", "\n", "", "rank", "=", "torch", ".", "distributed", ".", "get_rank", "(", ")", "\n", "", "self", ".", "dataset", "=", "dataset", "\n", "self", ".", "num_replicas", "=", "num_replicas", "\n", "self", ".", "rank", "=", "rank", "\n", "self", ".", "num_samples", "=", "int", "(", "math", ".", "ceil", "(", "len", "(", "self", ".", "dataset", ")", "*", "1.0", "/", "self", ".", "num_replicas", ")", ")", "\n", "self", ".", "total_size", "=", "self", ".", "num_samples", "*", "self", ".", "num_replicas", "\n", "self", ".", "do_shuffle", "=", "do_shuffle", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.module.sampler.SequentialDistributedSampler.__iter__": [[39, 50], ["list", "range", "len", "random.shuffle", "iter", "iter"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "start_pos", "=", "self", ".", "rank", "*", "self", ".", "num_samples", "\n", "end_pos", "=", "(", "self", ".", "rank", "+", "1", ")", "*", "self", ".", "num_samples", "\n", "indices", "=", "list", "(", "range", "(", "start_pos", ",", "end_pos", ")", ")", "\n", "\n", "assert", "len", "(", "indices", ")", "==", "self", ".", "num_samples", "\n", "if", "self", ".", "do_shuffle", ":", "\n", "            ", "random", ".", "shuffle", "(", "indices", ")", "\n", "return", "iter", "(", "indices", ")", "\n", "", "else", ":", "\n", "            ", "return", "iter", "(", "indices", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.module.sampler.SequentialDistributedSampler.__len__": [[51, 53], ["None"], "methods", ["None"], ["", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "num_samples", "", "", "", ""]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.module.bilstm.BiLSTM.__init__": [[14, 20], ["torch.Module.__init__", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.Dropout", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.liuwei1206_LEBERT.feature.vocab.ItemVocabArray.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input_size", ",", "hidden_size", ",", "dropout", ")", ":", "\n", "        ", "super", "(", "BiLSTM", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "f_lstm", "=", "nn", ".", "LSTM", "(", "input_size", ",", "hidden_size", ",", "num_layers", "=", "1", ",", "batch_first", "=", "True", ",", "bidirectional", "=", "False", ")", "\n", "self", ".", "b_lstm", "=", "nn", ".", "LSTM", "(", "input_size", ",", "hidden_size", ",", "num_layers", "=", "1", ",", "batch_first", "=", "True", ",", "bidirectional", "=", "False", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.module.bilstm.BiLSTM.forward": [[21, 38], ["torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "list", "bilstm.BiLSTM.f_lstm", "bilstm.BiLSTM.b_lstm", "bilstm.BiLSTM.dropout", "bilstm.BiLSTM.dropout", "function.utils.reverse_padded_sequence", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "map"], "methods", ["home.repos.pwc.inspect_result.liuwei1206_LEBERT.function.utils.reverse_padded_sequence"], ["", "def", "forward", "(", "self", ",", "inputs", ",", "attention_mask", ")", ":", "\n", "        ", "batch_length", "=", "torch", ".", "sum", "(", "attention_mask", ",", "dim", "=", "-", "1", ")", "# [batch]", "\n", "batch_length", "=", "list", "(", "map", "(", "int", ",", "batch_length", ")", ")", "\n", "\n", "f_lstm_output", ",", "_", "=", "self", ".", "f_lstm", "(", "inputs", ")", "\n", "b_lstm_output", ",", "_", "=", "self", ".", "b_lstm", "(", "inputs", ")", "\n", "\n", "f_lstm_output", "=", "self", ".", "dropout", "(", "f_lstm_output", ")", "\n", "b_lstm_output", "=", "self", ".", "dropout", "(", "b_lstm_output", ")", "\n", "\n", "# reverse", "\n", "b_lstm_output", "=", "reverse_padded_sequence", "(", "b_lstm_output", ",", "batch_length", ")", "\n", "\n", "# concat", "\n", "lstm_output", "=", "torch", ".", "cat", "(", "(", "f_lstm_output", ",", "b_lstm_output", ")", ",", "dim", "=", "-", "1", ")", "\n", "\n", "return", "lstm_output", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.feature.task_dataset.TaskDataset.__init__": [[18, 50], ["file.split", "os.path.join", "task_dataset.TaskDataset.init_np_dataset", "file_items[].split"], "methods", ["home.repos.pwc.inspect_result.liuwei1206_LEBERT.feature.task_dataset.TaskDataset.init_np_dataset"], ["    ", "def", "__init__", "(", "self", ",", "file", ",", "params", ",", "do_shuffle", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            file: data file\n            params: 1.vocab.txt, tokenizer\n                    2.word_vocab\n                    3.label_vocab\n                    4.max_word_num\n                    5.max_scan_num\n                    6.max_seq_length\n\n        \"\"\"", "\n", "self", ".", "max_word_num", "=", "params", "[", "'max_word_num'", "]", "\n", "self", ".", "tokenizer", "=", "params", "[", "'tokenizer'", "]", "\n", "self", ".", "label_vocab", "=", "params", "[", "'label_vocab'", "]", "\n", "self", ".", "word_vocab", "=", "params", "[", "'word_vocab'", "]", "\n", "self", ".", "lexicon_tree", "=", "params", "[", "'lexicon_tree'", "]", "\n", "self", ".", "max_scan_num", "=", "params", "[", "'max_scan_num'", "]", "\n", "self", ".", "max_seq_length", "=", "params", "[", "'max_seq_length'", "]", "\n", "self", ".", "default_label", "=", "params", "[", "'default_label'", "]", "\n", "self", ".", "do_shuffle", "=", "do_shuffle", "\n", "\n", "self", ".", "file", "=", "file", "\n", "file_items", "=", "file", ".", "split", "(", "\"/\"", ")", "\n", "data_dir", "=", "\"/\"", ".", "join", "(", "file_items", "[", ":", "-", "1", "]", ")", "\n", "\n", "file_name", "=", "\"saved_maxword_{}_maxseq_{}_\"", ".", "format", "(", "self", ".", "max_word_num", ",", "self", ".", "max_seq_length", ")", "+", "file_items", "[", "-", "1", "]", ".", "split", "(", "'.'", ")", "[", "0", "]", "+", "\"_{}.npz\"", ".", "format", "(", "self", ".", "max_scan_num", ")", "\n", "saved_np_file", "=", "os", ".", "path", ".", "join", "(", "data_dir", ",", "file_name", ")", "\n", "self", ".", "np_file", "=", "saved_np_file", "\n", "\n", "self", ".", "init_np_dataset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.feature.task_dataset.TaskDataset.init_np_dataset": [[51, 187], ["os.path.exists", "list", "print", "print", "print", "range", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.savez", "range", "random.shuffle", "numpy.load", "task_dataset.TaskDataset.tokenizer.convert_ids_to_tokens", "print", "print", "open", "f.readlines", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "task_dataset.TaskDataset.word_vocab.convert_ids_to_items", "line.strip.strip.strip", "json.loads", "text.insert", "label.insert", "text.append", "label.append", "task_dataset.TaskDataset.tokenizer.convert_tokens_to_ids", "task_dataset.TaskDataset.label_vocab.convert_items_to_ids", "numpy.zeros", "numpy.ones", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "function.preprocess.sent_to_matched_words_boundaries", "len", "range", "numpy.array.append", "numpy.array.append", "numpy.array.append", "numpy.array.append", "numpy.array.append", "numpy.array.append", "numpy.array.append", "len", "len", "len", "numpy.zeros", "task_dataset.TaskDataset.word_vocab.convert_items_to_ids", "print", "print", "print", "range", "print", "print", "print", "len", "len", "len", "len", "task_dataset.TaskDataset.tokenizer.convert_ids_to_tokens", "print", "print", "len", "len", "len", "task_dataset.TaskDataset.word_vocab.convert_ids_to_items"], "methods", ["home.repos.pwc.inspect_result.liuwei1206_LEBERT.feature.vocab.ItemVocabArray.convert_ids_to_items", "home.repos.pwc.inspect_result.liuwei1206_LEBERT.module.lexicon_tree.Trie.insert", "home.repos.pwc.inspect_result.liuwei1206_LEBERT.module.lexicon_tree.Trie.insert", "home.repos.pwc.inspect_result.liuwei1206_LEBERT.feature.vocab.ItemVocabArray.convert_items_to_ids", "home.repos.pwc.inspect_result.liuwei1206_LEBERT.function.preprocess.sent_to_matched_words_boundaries", "home.repos.pwc.inspect_result.liuwei1206_LEBERT.feature.vocab.ItemVocabArray.convert_items_to_ids", "home.repos.pwc.inspect_result.liuwei1206_LEBERT.feature.vocab.ItemVocabArray.convert_ids_to_items"], ["", "def", "init_np_dataset", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        generate np file, accumulate the read speed.\n        we need\n            2. tokenizer\n            3. word_vocab\n            4. label vocab\n            5. max_scan_num\n            6, max_word_num\n        \"\"\"", "\n", "print_flag", "=", "True", "\n", "if", "os", ".", "path", ".", "exists", "(", "self", ".", "np_file", ")", ":", "\n", "            ", "with", "np", ".", "load", "(", "self", ".", "np_file", ")", "as", "dataset", ":", "\n", "                ", "self", ".", "input_ids", "=", "dataset", "[", "\"input_ids\"", "]", "\n", "self", ".", "segment_ids", "=", "dataset", "[", "\"segment_ids\"", "]", "\n", "self", ".", "attention_mask", "=", "dataset", "[", "\"attention_mask\"", "]", "\n", "self", ".", "input_matched_word_ids", "=", "dataset", "[", "\"input_matched_word_ids\"", "]", "\n", "self", ".", "input_matched_word_mask", "=", "dataset", "[", "\"input_matched_word_mask\"", "]", "\n", "self", ".", "input_boundary_ids", "=", "dataset", "[", "\"input_boundary_ids\"", "]", "\n", "self", ".", "labels", "=", "dataset", "[", "\"labels\"", "]", "\n", "", "print", "(", "\"\u6838\u5bf9%s\u4e2did\u548c\u8bcd\u662f\u5426\u5339\u914d: \"", "%", "(", "self", ".", "file", ")", ")", "\n", "print", "(", "self", ".", "input_ids", "[", "0", "]", "[", ":", "10", "]", ")", "\n", "print", "(", "self", ".", "tokenizer", ".", "convert_ids_to_tokens", "(", "self", ".", "input_ids", "[", "0", "]", "[", ":", "10", "]", ")", ")", "\n", "for", "idx", "in", "range", "(", "10", ")", ":", "\n", "                ", "print", "(", "self", ".", "input_matched_word_ids", "[", "0", "]", "[", "idx", "]", ")", "\n", "print", "(", "self", ".", "word_vocab", ".", "convert_ids_to_items", "(", "self", ".", "input_matched_word_ids", "[", "0", "]", "[", "idx", "]", ")", ")", "\n", "\n", "", "", "else", ":", "\n", "            ", "all_input_ids", "=", "[", "]", "\n", "all_segment_ids", "=", "[", "]", "\n", "all_attention_mask", "=", "[", "]", "\n", "all_input_matched_word_ids", "=", "[", "]", "\n", "all_input_matched_word_mask", "=", "[", "]", "\n", "all_input_boundary_ids", "=", "[", "]", "\n", "all_labels", "=", "[", "]", "\n", "\n", "with", "open", "(", "self", ".", "file", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "                ", "lines", "=", "f", ".", "readlines", "(", ")", "\n", "for", "line", "in", "lines", ":", "\n", "                    ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "line", ":", "\n", "                        ", "sample", "=", "json", ".", "loads", "(", "line", ")", "\n", "text", "=", "sample", "[", "'text'", "]", "\n", "label", "=", "sample", "[", "'label'", "]", "\n", "if", "len", "(", "text", ")", ">", "self", ".", "max_seq_length", "-", "2", ":", "\n", "                            ", "text", "=", "text", "[", ":", "self", ".", "max_seq_length", "-", "2", "]", "\n", "label", "=", "label", "[", ":", "self", ".", "max_seq_length", "-", "2", "]", "\n", "", "text", ".", "insert", "(", "0", ",", "'[CLS]'", ")", "\n", "label", ".", "insert", "(", "0", ",", "self", ".", "default_label", ")", "\n", "text", ".", "append", "(", "'[SEP]'", ")", "\n", "label", ".", "append", "(", "self", ".", "default_label", ")", "\n", "\n", "token_ids", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "text", ")", "\n", "label_ids", "=", "self", ".", "label_vocab", ".", "convert_items_to_ids", "(", "label", ")", "\n", "\n", "input_ids", "=", "np", ".", "zeros", "(", "self", ".", "max_seq_length", ",", "dtype", "=", "np", ".", "int", ")", "\n", "segment_ids", "=", "np", ".", "ones", "(", "self", ".", "max_seq_length", ",", "dtype", "=", "np", ".", "int", ")", "\n", "attention_mask", "=", "np", ".", "zeros", "(", "self", ".", "max_seq_length", ",", "dtype", "=", "np", ".", "int", ")", "\n", "matched_word_ids", "=", "np", ".", "zeros", "(", "(", "self", ".", "max_seq_length", ",", "self", ".", "max_word_num", ")", ",", "dtype", "=", "np", ".", "int", ")", "\n", "matched_word_mask", "=", "np", ".", "zeros", "(", "(", "self", ".", "max_seq_length", ",", "self", ".", "max_word_num", ")", ",", "dtype", "=", "np", ".", "int", ")", "\n", "boundary_ids", "=", "np", ".", "zeros", "(", "self", ".", "max_seq_length", ",", "dtype", "=", "np", ".", "int", ")", "\n", "if", "len", "(", "label", ")", "==", "len", "(", "text", ")", ":", "\n", "                            ", "np_label", "=", "np", ".", "zeros", "(", "self", ".", "max_seq_length", ",", "dtype", "=", "np", ".", "int", ")", "\n", "np_label", "[", ":", "len", "(", "label_ids", ")", "]", "=", "label_ids", "\n", "", "else", ":", "\n", "                            ", "np_label", "=", "label_ids", "\n", "\n", "# token_ids, segment_ids, attention_mask", "\n", "", "input_ids", "[", ":", "len", "(", "token_ids", ")", "]", "=", "token_ids", "\n", "segment_ids", "[", ":", "len", "(", "token_ids", ")", "]", "=", "0", "\n", "attention_mask", "[", ":", "len", "(", "token_ids", ")", "]", "=", "1", "\n", "\n", "# matched word, boubdary", "\n", "matched_words", ",", "sent_boundaries", "=", "sent_to_matched_words_boundaries", "(", "text", ",", "self", ".", "lexicon_tree", ",", "self", ".", "max_word_num", ")", "\n", "sent_length", "=", "len", "(", "text", ")", "\n", "boundary_ids", "[", ":", "len", "(", "sent_boundaries", ")", "]", "=", "sent_boundaries", "\n", "for", "idy", "in", "range", "(", "sent_length", ")", ":", "\n", "                            ", "now_words", "=", "matched_words", "[", "idy", "]", "\n", "now_word_ids", "=", "self", ".", "word_vocab", ".", "convert_items_to_ids", "(", "now_words", ")", "\n", "matched_word_ids", "[", "idy", "]", "[", ":", "len", "(", "now_word_ids", ")", "]", "=", "now_word_ids", "\n", "matched_word_mask", "[", "idy", "]", "[", ":", "len", "(", "now_word_ids", ")", "]", "=", "1", "\n", "\n", "", "if", "print_flag", ":", "\n", "                            ", "print", "(", "\"\u6838\u5bf9%s\u4e2did\u548c\u8bcd\u662f\u5426\u5339\u914d: \"", "%", "(", "self", ".", "file", ")", ")", "\n", "print", "(", "input_ids", "[", ":", "10", "]", ")", "\n", "print", "(", "self", ".", "tokenizer", ".", "convert_ids_to_tokens", "(", "input_ids", "[", ":", "10", "]", ")", ")", "\n", "for", "idx", "in", "range", "(", "10", ")", ":", "\n", "                                ", "print", "(", "matched_word_ids", "[", "idx", "]", ")", "\n", "print", "(", "self", ".", "word_vocab", ".", "convert_ids_to_items", "(", "matched_word_ids", "[", "idx", "]", ")", ")", "\n", "\n", "", "print", "(", "matched_words", ")", "\n", "print", "(", "matched_words", "[", ":", "10", "]", ")", "\n", "print", "(", "matched_word_ids", "[", ":", "10", "]", ")", "\n", "print_flag", "=", "False", "\n", "\n", "", "all_input_ids", ".", "append", "(", "input_ids", ")", "\n", "all_segment_ids", ".", "append", "(", "segment_ids", ")", "\n", "all_attention_mask", ".", "append", "(", "attention_mask", ")", "\n", "all_input_matched_word_ids", ".", "append", "(", "matched_word_ids", ")", "\n", "all_input_matched_word_mask", ".", "append", "(", "matched_word_mask", ")", "\n", "all_input_boundary_ids", ".", "append", "(", "boundary_ids", ")", "\n", "all_labels", ".", "append", "(", "np_label", ")", "\n", "\n", "", "", "", "assert", "len", "(", "all_input_ids", ")", "==", "len", "(", "all_segment_ids", ")", ",", "(", "len", "(", "all_input_ids", ")", ",", "len", "(", "all_segment_ids", ")", ")", "\n", "assert", "len", "(", "all_input_ids", ")", "==", "len", "(", "all_attention_mask", ")", ",", "(", "len", "(", "all_input_ids", ")", ",", "len", "(", "all_attention_mask", ")", ")", "\n", "assert", "len", "(", "all_input_ids", ")", "==", "len", "(", "all_input_matched_word_ids", ")", ",", "(", "len", "(", "all_input_ids", ")", ",", "len", "(", "all_input_matched_word_ids", ")", ")", "\n", "assert", "len", "(", "all_input_ids", ")", "==", "len", "(", "all_input_matched_word_mask", ")", ",", "(", "len", "(", "all_input_ids", ")", ",", "len", "(", "all_input_matched_word_mask", ")", ")", "\n", "assert", "len", "(", "all_input_ids", ")", "==", "len", "(", "all_input_boundary_ids", ")", ",", "(", "len", "(", "all_input_ids", ")", ",", "len", "(", "all_input_boundary_ids", ")", ")", "\n", "assert", "len", "(", "all_input_ids", ")", "==", "len", "(", "all_labels", ")", ",", "(", "len", "(", "all_input_ids", ")", ",", "len", "(", "all_labels", ")", ")", "\n", "\n", "all_input_ids", "=", "np", ".", "array", "(", "all_input_ids", ")", "\n", "all_segment_ids", "=", "np", ".", "array", "(", "all_segment_ids", ")", "\n", "all_attention_mask", "=", "np", ".", "array", "(", "all_attention_mask", ")", "\n", "all_input_matched_word_ids", "=", "np", ".", "array", "(", "all_input_matched_word_ids", ")", "\n", "all_input_matched_word_mask", "=", "np", ".", "array", "(", "all_input_matched_word_mask", ")", "\n", "all_input_boundary_ids", "=", "np", ".", "array", "(", "all_input_boundary_ids", ")", "\n", "all_labels", "=", "np", ".", "array", "(", "all_labels", ")", "\n", "np", ".", "savez", "(", "\n", "self", ".", "np_file", ",", "input_ids", "=", "all_input_ids", ",", "segment_ids", "=", "all_segment_ids", ",", "attention_mask", "=", "all_attention_mask", ",", "\n", "input_matched_word_ids", "=", "all_input_matched_word_ids", ",", "input_matched_word_mask", "=", "all_input_matched_word_mask", ",", "\n", "input_boundary_ids", "=", "all_input_boundary_ids", ",", "labels", "=", "all_labels", "\n", ")", "\n", "\n", "self", ".", "input_ids", "=", "all_input_ids", "\n", "self", ".", "segment_ids", "=", "all_segment_ids", "\n", "self", ".", "attention_mask", "=", "all_attention_mask", "\n", "self", ".", "input_matched_word_ids", "=", "all_input_matched_word_ids", "\n", "self", ".", "input_matched_word_mask", "=", "all_input_matched_word_mask", "\n", "self", ".", "input_boundary_ids", "=", "all_input_boundary_ids", "\n", "self", ".", "labels", "=", "all_labels", "\n", "\n", "", "self", ".", "total_size", "=", "self", ".", "input_ids", ".", "shape", "[", "0", "]", "\n", "self", ".", "indexes", "=", "list", "(", "range", "(", "self", ".", "total_size", ")", ")", "\n", "if", "self", ".", "do_shuffle", ":", "\n", "            ", "random", ".", "shuffle", "(", "self", ".", "indexes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.feature.task_dataset.TaskDataset.__len__": [[188, 190], ["None"], "methods", ["None"], ["", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "total_size", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.feature.task_dataset.TaskDataset.__getitem__": [[191, 201], ["torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "index", "=", "self", ".", "indexes", "[", "index", "]", "\n", "return", "(", "\n", "torch", ".", "tensor", "(", "self", ".", "input_ids", "[", "index", "]", ")", ",", "\n", "torch", ".", "tensor", "(", "self", ".", "segment_ids", "[", "index", "]", ")", ",", "\n", "torch", ".", "tensor", "(", "self", ".", "attention_mask", "[", "index", "]", ")", ",", "\n", "torch", ".", "tensor", "(", "self", ".", "input_matched_word_ids", "[", "index", "]", ")", ",", "\n", "torch", ".", "tensor", "(", "self", ".", "input_matched_word_mask", "[", "index", "]", ")", ",", "\n", "torch", ".", "tensor", "(", "self", ".", "input_boundary_ids", "[", "index", "]", ")", ",", "\n", "torch", ".", "tensor", "(", "self", ".", "labels", "[", "index", "]", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.feature.vocab.ItemVocabFile.__init__": [[25, 45], ["vocab.ItemVocabFile.init_vocab", "vocab.ItemVocabFile.idx2item.append", "vocab.ItemVocabFile.idx2item.append", "range", "vocab.ItemVocabFile.idx2item.append"], "methods", ["home.repos.pwc.inspect_result.liuwei1206_LEBERT.feature.vocab.ItemVocabArray.init_vocab"], ["def", "__init__", "(", "self", ",", "files", ",", "is_word", "=", "False", ",", "has_default", "=", "False", ",", "unk_num", "=", "0", ")", ":", "\n", "        ", "self", ".", "files", "=", "files", "\n", "self", ".", "item2idx", "=", "{", "}", "\n", "self", ".", "idx2item", "=", "[", "]", "\n", "self", ".", "item_size", "=", "0", "\n", "self", ".", "is_word", "=", "is_word", "\n", "if", "not", "has_default", "and", "self", ".", "is_word", ":", "\n", "            ", "self", ".", "item2idx", "[", "'<pad>'", "]", "=", "self", ".", "item_size", "\n", "self", ".", "idx2item", ".", "append", "(", "'<pad>'", ")", "\n", "self", ".", "item_size", "+=", "1", "\n", "self", ".", "item2idx", "[", "'<unk>'", "]", "=", "self", ".", "item_size", "\n", "self", ".", "idx2item", ".", "append", "(", "'<unk>'", ")", "\n", "self", ".", "item_size", "+=", "1", "\n", "# for unk words", "\n", "for", "i", "in", "range", "(", "unk_num", ")", ":", "\n", "                ", "self", ".", "item2idx", "[", "'<unk>{}'", ".", "format", "(", "i", "+", "1", ")", "]", "=", "self", ".", "item_size", "\n", "self", ".", "idx2item", ".", "append", "(", "'<unk>{}'", ".", "format", "(", "i", "+", "1", ")", ")", "\n", "self", ".", "item_size", "+=", "1", "\n", "\n", "", "", "self", ".", "init_vocab", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.feature.vocab.ItemVocabFile.init_vocab": [[46, 59], ["open", "f.readlines", "line.strip.strip.strip", "line.strip.strip.split", "items[].strip", "vocab.ItemVocabFile.idx2item.append"], "methods", ["None"], ["", "def", "init_vocab", "(", "self", ")", ":", "\n", "        ", "for", "file", "in", "self", ".", "files", ":", "\n", "            ", "with", "open", "(", "file", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "                ", "lines", "=", "f", ".", "readlines", "(", ")", "\n", "for", "line", "in", "lines", ":", "\n", "                    ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "not", "line", ":", "\n", "                        ", "continue", "\n", "", "items", "=", "line", ".", "split", "(", ")", "\n", "item", "=", "items", "[", "0", "]", ".", "strip", "(", ")", "\n", "self", ".", "item2idx", "[", "item", "]", "=", "self", ".", "item_size", "\n", "self", ".", "idx2item", ".", "append", "(", "item", ")", "\n", "self", ".", "item_size", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.feature.vocab.ItemVocabFile.get_item_size": [[60, 62], ["None"], "methods", ["None"], ["", "", "", "", "def", "get_item_size", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "item_size", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.feature.vocab.ItemVocabFile.convert_item_to_id": [[63, 76], ["print", "print", "KeyError", "str", "len"], "methods", ["None"], ["", "def", "convert_item_to_id", "(", "self", ",", "item", ")", ":", "\n", "        ", "if", "item", "in", "self", ".", "item2idx", ":", "\n", "            ", "return", "self", ".", "item2idx", "[", "item", "]", "\n", "", "elif", "self", ".", "is_word", ":", "\n", "            ", "unk", "=", "\"<unk>\"", "+", "str", "(", "len", "(", "item", ")", ")", "\n", "if", "unk", "in", "self", ".", "item2idx", ":", "\n", "                ", "return", "self", ".", "item2idx", "[", "unk", "]", "\n", "", "else", ":", "\n", "                ", "return", "self", ".", "item2idx", "[", "'<unk>'", "]", "\n", "", "", "else", ":", "\n", "            ", "print", "(", "\"Label does not exist!!!!\"", ")", "\n", "print", "(", "item", ")", "\n", "raise", "KeyError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.feature.vocab.ItemVocabFile.convert_items_to_ids": [[77, 79], ["vocab.ItemVocabFile.convert_item_to_id"], "methods", ["home.repos.pwc.inspect_result.liuwei1206_LEBERT.feature.vocab.ItemVocabArray.convert_item_to_id"], ["", "", "def", "convert_items_to_ids", "(", "self", ",", "items", ")", ":", "\n", "        ", "return", "[", "self", ".", "convert_item_to_id", "(", "item", ")", "for", "item", "in", "items", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.feature.vocab.ItemVocabFile.convert_id_to_item": [[80, 82], ["None"], "methods", ["None"], ["", "def", "convert_id_to_item", "(", "self", ",", "id", ")", ":", "\n", "        ", "return", "self", ".", "idx2item", "[", "id", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.feature.vocab.ItemVocabFile.convert_ids_to_items": [[83, 85], ["vocab.ItemVocabFile.convert_id_to_item"], "methods", ["home.repos.pwc.inspect_result.liuwei1206_LEBERT.feature.vocab.ItemVocabArray.convert_id_to_item"], ["", "def", "convert_ids_to_items", "(", "self", ",", "ids", ")", ":", "\n", "        ", "return", "[", "self", ".", "convert_id_to_item", "(", "id", ")", "for", "id", "in", "ids", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.feature.vocab.ItemVocabArray.__init__": [[93, 113], ["vocab.ItemVocabArray.init_vocab", "vocab.ItemVocabArray.idx2item.append", "vocab.ItemVocabArray.idx2item.append", "range", "vocab.ItemVocabArray.idx2item.append"], "methods", ["home.repos.pwc.inspect_result.liuwei1206_LEBERT.feature.vocab.ItemVocabArray.init_vocab"], ["def", "__init__", "(", "self", ",", "items_array", ",", "is_word", "=", "False", ",", "has_default", "=", "False", ",", "unk_num", "=", "0", ")", ":", "\n", "        ", "self", ".", "items_array", "=", "items_array", "\n", "self", ".", "item2idx", "=", "{", "}", "\n", "self", ".", "idx2item", "=", "[", "]", "\n", "self", ".", "item_size", "=", "0", "\n", "self", ".", "is_word", "=", "is_word", "\n", "if", "not", "has_default", "and", "self", ".", "is_word", ":", "\n", "            ", "self", ".", "item2idx", "[", "'<pad>'", "]", "=", "self", ".", "item_size", "\n", "self", ".", "idx2item", ".", "append", "(", "'<pad>'", ")", "\n", "self", ".", "item_size", "+=", "1", "\n", "self", ".", "item2idx", "[", "'<unk>'", "]", "=", "self", ".", "item_size", "\n", "self", ".", "idx2item", ".", "append", "(", "'<unk>'", ")", "\n", "self", ".", "item_size", "+=", "1", "\n", "# for unk words", "\n", "for", "i", "in", "range", "(", "1", ",", "unk_num", "+", "1", ")", ":", "\n", "                ", "self", ".", "item2idx", "[", "'<unk>{}'", ".", "format", "(", "i", "+", "1", ")", "]", "=", "self", ".", "item_size", "\n", "self", ".", "idx2item", ".", "append", "(", "'<unk>{}'", ".", "format", "(", "i", "+", "1", ")", ")", "\n", "self", ".", "item_size", "+=", "1", "\n", "\n", "", "", "self", ".", "init_vocab", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.feature.vocab.ItemVocabArray.init_vocab": [[114, 119], ["vocab.ItemVocabArray.idx2item.append"], "methods", ["None"], ["", "def", "init_vocab", "(", "self", ")", ":", "\n", "        ", "for", "item", "in", "self", ".", "items_array", ":", "\n", "            ", "self", ".", "item2idx", "[", "item", "]", "=", "self", ".", "item_size", "\n", "self", ".", "idx2item", ".", "append", "(", "item", ")", "\n", "self", ".", "item_size", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.feature.vocab.ItemVocabArray.get_item_size": [[120, 122], ["None"], "methods", ["None"], ["", "", "def", "get_item_size", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "item_size", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.feature.vocab.ItemVocabArray.convert_item_to_id": [[123, 136], ["print", "print", "KeyError", "str", "len"], "methods", ["None"], ["", "def", "convert_item_to_id", "(", "self", ",", "item", ")", ":", "\n", "        ", "if", "item", "in", "self", ".", "item2idx", ":", "\n", "            ", "return", "self", ".", "item2idx", "[", "item", "]", "\n", "", "elif", "self", ".", "is_word", ":", "\n", "            ", "unk", "=", "\"<unk>\"", "+", "str", "(", "len", "(", "item", ")", ")", "\n", "if", "unk", "in", "self", ".", "item2idx", ":", "\n", "                ", "return", "self", ".", "item2idx", "[", "unk", "]", "\n", "", "else", ":", "\n", "                ", "return", "self", ".", "item2idx", "[", "'<unk>'", "]", "\n", "", "", "else", ":", "\n", "            ", "print", "(", "\"Label does not exist!!!!\"", ")", "\n", "print", "(", "item", ")", "\n", "raise", "KeyError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.feature.vocab.ItemVocabArray.convert_items_to_ids": [[137, 139], ["vocab.ItemVocabArray.convert_item_to_id"], "methods", ["home.repos.pwc.inspect_result.liuwei1206_LEBERT.feature.vocab.ItemVocabArray.convert_item_to_id"], ["", "", "def", "convert_items_to_ids", "(", "self", ",", "items", ")", ":", "\n", "        ", "return", "[", "self", ".", "convert_item_to_id", "(", "item", ")", "for", "item", "in", "items", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.feature.vocab.ItemVocabArray.convert_id_to_item": [[140, 142], ["None"], "methods", ["None"], ["", "def", "convert_id_to_item", "(", "self", ",", "id", ")", ":", "\n", "        ", "return", "self", ".", "idx2item", "[", "id", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.feature.vocab.ItemVocabArray.convert_ids_to_items": [[143, 145], ["vocab.ItemVocabArray.convert_id_to_item"], "methods", ["home.repos.pwc.inspect_result.liuwei1206_LEBERT.feature.vocab.ItemVocabArray.convert_id_to_item"], ["", "def", "convert_ids_to_items", "(", "self", ",", "ids", ")", ":", "\n", "        ", "return", "[", "self", ".", "convert_id_to_item", "(", "id", ")", "for", "id", "in", "ids", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.function.format_convert.BMES_to_json": [[14, 50], ["open", "f.readlines", "len", "tqdm.trange", "open", "line.strip.strip", "f.write", "texts.append", "line.strip.split", "words.append", "labels.append", "len", "len", "len", "len", "json.dumps"], "function", ["None"], ["def", "BMES_to_json", "(", "bmes_file", ",", "json_file", ")", ":", "\n", "    ", "\"\"\"\n    convert bmes format file to json file, json file has two key, including text and label\n    Args:\n        bmes_file:\n        json_file:\n    :return:\n    \"\"\"", "\n", "texts", "=", "[", "]", "\n", "with", "open", "(", "bmes_file", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "lines", "=", "f", ".", "readlines", "(", ")", "\n", "total_line_num", "=", "len", "(", "lines", ")", "\n", "line_iter", "=", "trange", "(", "total_line_num", ")", "\n", "words", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "for", "idx", "in", "line_iter", ":", "\n", "            ", "line", "=", "lines", "[", "idx", "]", "\n", "line", "=", "line", ".", "strip", "(", ")", "\n", "\n", "if", "not", "line", ":", "\n", "                ", "assert", "len", "(", "words", ")", "==", "len", "(", "labels", ")", ",", "(", "len", "(", "words", ")", ",", "len", "(", "labels", ")", ")", "\n", "sample", "=", "{", "}", "\n", "sample", "[", "'text'", "]", "=", "words", "\n", "sample", "[", "'label'", "]", "=", "labels", "\n", "texts", ".", "append", "(", "json", ".", "dumps", "(", "sample", ",", "ensure_ascii", "=", "False", ")", ")", "\n", "\n", "words", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "", "else", ":", "\n", "                ", "items", "=", "line", ".", "split", "(", ")", "\n", "words", ".", "append", "(", "items", "[", "0", "]", ")", "\n", "labels", ".", "append", "(", "items", "[", "1", "]", ")", "\n", "\n", "", "", "", "with", "open", "(", "json_file", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "for", "text", "in", "texts", ":", "\n", "            ", "f", ".", "write", "(", "\"%s\\n\"", "%", "(", "text", ")", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.function.metrics.seq_f1_with_mask": [[8, 48], ["len", "range", "seqeval.metrics.accuracy_score", "seqeval.metrics.precision_score", "seqeval.metrics.recall_score", "seqeval.metrics.f1_score", "len", "len", "len", "len", "len", "len", "len", "len", "numpy.sum", "range", "true_labels.append", "pred_labels.append", "len", "len", "len", "len", "len", "len", "len", "len", "tmp_true.append", "tmp_pred.append", "label_vocab.convert_id_to_item().replace", "label_vocab.convert_id_to_item().replace", "label_vocab.convert_id_to_item", "label_vocab.convert_id_to_item"], "function", ["home.repos.pwc.inspect_result.liuwei1206_LEBERT.feature.vocab.ItemVocabArray.convert_id_to_item", "home.repos.pwc.inspect_result.liuwei1206_LEBERT.feature.vocab.ItemVocabArray.convert_id_to_item"], ["def", "seq_f1_with_mask", "(", "all_true_labels", ",", "all_pred_labels", ",", "all_label_mask", ",", "label_vocab", ")", ":", "\n", "    ", "\"\"\"\n    For Chinese, since label is given to each character, do not exists subtoken,\n    so we can evaluate in character level directly, extra processing\n\n    Args:\n        all_true_labels: true label ids\n        all_pred_labels: predict label ids\n        all_label_mask: the valid of each position\n        label_vocab: from id to labels\n    \"\"\"", "\n", "assert", "len", "(", "all_true_labels", ")", "==", "len", "(", "all_pred_labels", ")", ",", "(", "len", "(", "all_true_labels", ")", ",", "len", "(", "all_pred_labels", ")", ")", "\n", "assert", "len", "(", "all_true_labels", ")", "==", "len", "(", "all_label_mask", ")", ",", "(", "len", "(", "all_true_labels", ")", ",", "len", "(", "all_label_mask", ")", ")", "\n", "\n", "true_labels", "=", "[", "]", "\n", "pred_labels", "=", "[", "]", "\n", "\n", "sample_num", "=", "len", "(", "all_true_labels", ")", "\n", "for", "i", "in", "range", "(", "sample_num", ")", ":", "\n", "        ", "tmp_true", "=", "[", "]", "\n", "tmp_pred", "=", "[", "]", "\n", "\n", "assert", "len", "(", "all_true_labels", "[", "i", "]", ")", "==", "len", "(", "all_pred_labels", "[", "i", "]", ")", ",", "(", "len", "(", "all_true_labels", "[", "i", "]", ")", ",", "len", "(", "all_pred_labels", "[", "i", "]", ")", ")", "\n", "assert", "len", "(", "all_true_labels", "[", "i", "]", ")", "==", "len", "(", "all_label_mask", "[", "i", "]", ")", ",", "(", "len", "(", "all_true_labels", "[", "i", "]", ")", ",", "len", "(", "all_label_mask", "[", "i", "]", ")", ")", "\n", "\n", "real_seq_length", "=", "np", ".", "sum", "(", "all_label_mask", "[", "i", "]", ")", "\n", "for", "j", "in", "range", "(", "1", ",", "real_seq_length", "-", "1", ")", ":", "# remove the label of [CLS] and [SEP]", "\n", "            ", "if", "all_label_mask", "[", "i", "]", "[", "j", "]", "==", "1", ":", "\n", "                ", "tmp_true", ".", "append", "(", "label_vocab", ".", "convert_id_to_item", "(", "all_true_labels", "[", "i", "]", "[", "j", "]", ")", ".", "replace", "(", "\"M-\"", ",", "\"I-\"", ")", ")", "\n", "tmp_pred", ".", "append", "(", "label_vocab", ".", "convert_id_to_item", "(", "all_pred_labels", "[", "i", "]", "[", "j", "]", ")", ".", "replace", "(", "\"M-\"", ",", "\"I-\"", ")", ")", "\n", "\n", "", "", "true_labels", ".", "append", "(", "tmp_true", ")", "\n", "pred_labels", ".", "append", "(", "tmp_pred", ")", "\n", "\n", "", "acc", "=", "accuracy_score", "(", "true_labels", ",", "pred_labels", ")", "\n", "p", "=", "precision_score", "(", "true_labels", ",", "pred_labels", ")", "\n", "r", "=", "recall_score", "(", "true_labels", ",", "pred_labels", ")", "\n", "f1", "=", "f1_score", "(", "true_labels", ",", "pred_labels", ")", "\n", "\n", "return", "acc", ",", "p", ",", "r", ",", "f1", ",", "true_labels", ",", "pred_labels", "\n", "", ""]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.function.preprocess.sent_to_matched_words_boundaries": [[12, 88], ["len", "range", "lexicon_tree.enumerateMatch", "len", "len", "len", "len", "range", "range", "sent_boundaries[].append", "len", "print", "new_sent_boundaries.append", "len", "len", "sent_words[].extend", "len", "new_sent_boundaries.append", "len", "len", "len", "sent_words[].extend", "sent_boundaries[].append", "range", "sent_words[].append", "len", "sum", "new_sent_boundaries.append", "len", "sent_boundaries[].append", "sent_words[].append", "sent_boundaries[].append", "len", "new_sent_boundaries.append", "print", "print", "new_sent_boundaries.append", "len", "sent_boundaries[].append"], "function", ["home.repos.pwc.inspect_result.liuwei1206_LEBERT.module.lexicon_tree.Trie.enumerateMatch"], ["def", "sent_to_matched_words_boundaries", "(", "sent", ",", "lexicon_tree", ",", "max_word_num", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    \u8f93\u5165\u4e00\u4e2a\u53e5\u5b50\u548c\u8bcd\u5178\u6811, \u8fd4\u56de\u53e5\u5b50\u4e2d\u6bcf\u4e2a\u5b57\u6240\u5c5e\u7684\u5339\u914d\u8bcd, \u4ee5\u53ca\u8be5\u5b57\u7684\u8bcd\u8fb9\u754c\n    \u5b57\u53ef\u80fd\u5c5e\u4e8e\u4ee5\u4e0b\u51e0\u79cd\u8fb9\u754c:\n        B-: \u8bcd\u7684\u5f00\u59cb, 0\n        M-: \u8bcd\u7684\u4e2d\u95f4, 1\n        E-: \u8bcd\u7684\u7ed3\u5c3e, 2\n        S-: \u5355\u5b57\u8bcd, 3\n        BM-: \u65e2\u662f\u67d0\u4e2a\u8bcd\u7684\u5f00\u59cb, \u53c8\u662f\u67d0\u4e2a\u8bcd\u4e2d\u95f4, 4\n        BE-: \u65e2\u662f\u67d0\u4e2a\u8bcd\u5f00\u59cb\uff0c\u53c8\u662f\u67d0\u4e2a\u8bcd\u7ed3\u5c3e, 5\n        ME-: \u65e2\u662f\u67d0\u4e2a\u8bcd\u7684\u4e2d\u95f4\uff0c\u53c8\u662f\u67d0\u4e2a\u8bcd\u7ed3\u5c3e, 6\n        BME-: \u8bcd\u7684\u5f00\u59cb\u3001\u8bcd\u7684\u4e2d\u95f4\u548c\u8bcd\u7684\u7ed3\u5c3e, 7\n\n    Args:\n        sent: \u8f93\u5165\u7684\u53e5\u5b50, \u4e00\u4e2a\u5b57\u7684\u6570\u7ec4\n        lexicon_tree: \u8bcd\u5178\u6811\n        max_word_num: \u6700\u591a\u5339\u914d\u7684\u8bcd\u7684\u6570\u91cf\n    Args:\n        sent_words: \u53e5\u5b50\u4e2d\u6bcf\u4e2a\u5b57\u5f52\u5c5e\u7684\u8bcd\u7ec4\n        sent_boundaries: \u53e5\u5b50\u4e2d\u6bcf\u4e2a\u5b57\u6240\u5c5e\u7684\u8fb9\u754c\u7c7b\u578b\n    \"\"\"", "\n", "sent_length", "=", "len", "(", "sent", ")", "\n", "sent_words", "=", "[", "[", "]", "for", "_", "in", "range", "(", "sent_length", ")", "]", "\n", "sent_boundaries", "=", "[", "[", "]", "for", "_", "in", "range", "(", "sent_length", ")", "]", "# each char has a boundary", "\n", "\n", "for", "idx", "in", "range", "(", "sent_length", ")", ":", "\n", "        ", "sub_sent", "=", "sent", "[", "idx", ":", "idx", "+", "lexicon_tree", ".", "max_depth", "]", "# speed using max depth", "\n", "words", "=", "lexicon_tree", ".", "enumerateMatch", "(", "sub_sent", ")", "\n", "\n", "if", "len", "(", "words", ")", "==", "0", "and", "len", "(", "sent_boundaries", "[", "idx", "]", ")", "==", "0", ":", "\n", "            ", "sent_boundaries", "[", "idx", "]", ".", "append", "(", "3", ")", "# S-", "\n", "", "else", ":", "\n", "            ", "if", "len", "(", "words", ")", "==", "1", "and", "len", "(", "words", "[", "0", "]", ")", "==", "1", ":", "# single character word", "\n", "                ", "if", "len", "(", "sent_words", "[", "idx", "]", ")", "==", "0", ":", "\n", "                    ", "sent_words", "[", "idx", "]", ".", "extend", "(", "words", ")", "\n", "sent_boundaries", "[", "idx", "]", ".", "append", "(", "3", ")", "# S-", "\n", "", "", "else", ":", "\n", "                ", "if", "max_word_num", ":", "\n", "                    ", "need_num", "=", "max_word_num", "-", "len", "(", "sent_words", "[", "idx", "]", ")", "\n", "words", "=", "words", "[", ":", "need_num", "]", "\n", "", "sent_words", "[", "idx", "]", ".", "extend", "(", "words", ")", "\n", "for", "word", "in", "words", ":", "\n", "                    ", "if", "0", "not", "in", "sent_boundaries", "[", "idx", "]", ":", "\n", "                        ", "sent_boundaries", "[", "idx", "]", ".", "append", "(", "0", ")", "# S-", "\n", "", "start_pos", "=", "idx", "+", "1", "\n", "end_pos", "=", "idx", "+", "len", "(", "word", ")", "-", "1", "\n", "for", "tmp_j", "in", "range", "(", "start_pos", ",", "end_pos", ")", ":", "\n", "                        ", "if", "1", "not", "in", "sent_boundaries", "[", "tmp_j", "]", ":", "\n", "                            ", "sent_boundaries", "[", "tmp_j", "]", ".", "append", "(", "1", ")", "# M-", "\n", "", "sent_words", "[", "tmp_j", "]", ".", "append", "(", "word", ")", "\n", "", "if", "2", "not", "in", "sent_boundaries", "[", "end_pos", "]", ":", "\n", "                        ", "sent_boundaries", "[", "end_pos", "]", ".", "append", "(", "2", ")", "# E-", "\n", "", "sent_words", "[", "end_pos", "]", ".", "append", "(", "word", ")", "\n", "\n", "", "", "", "", "assert", "len", "(", "sent_words", ")", "==", "len", "(", "sent_boundaries", ")", "\n", "\n", "new_sent_boundaries", "=", "[", "]", "\n", "idx", "=", "0", "\n", "for", "boundary", "in", "sent_boundaries", ":", "\n", "        ", "if", "len", "(", "boundary", ")", "==", "0", ":", "\n", "            ", "print", "(", "\"Error\"", ")", "\n", "new_sent_boundaries", ".", "append", "(", "0", ")", "\n", "", "elif", "len", "(", "boundary", ")", "==", "1", ":", "\n", "            ", "new_sent_boundaries", ".", "append", "(", "boundary", "[", "0", "]", ")", "\n", "", "elif", "len", "(", "boundary", ")", "==", "2", ":", "\n", "            ", "total_num", "=", "sum", "(", "boundary", ")", "\n", "new_sent_boundaries", ".", "append", "(", "3", "+", "total_num", ")", "\n", "", "elif", "len", "(", "boundary", ")", "==", "3", ":", "\n", "            ", "new_sent_boundaries", ".", "append", "(", "7", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "boundary", ")", "\n", "print", "(", "\"Error\"", ")", "\n", "new_sent_boundaries", ".", "append", "(", "8", ")", "\n", "", "", "assert", "len", "(", "sent_words", ")", "==", "len", "(", "new_sent_boundaries", ")", "\n", "\n", "return", "sent_words", ",", "new_sent_boundaries", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.function.preprocess.sent_to_distinct_matched_words": [[89, 124], ["len", "range", "lexicon_tree.enumerateMatch", "range", "range", "len", "len", "len", "len", "[].append", "[].append", "range", "[].append", "[].append"], "function", ["home.repos.pwc.inspect_result.liuwei1206_LEBERT.module.lexicon_tree.Trie.enumerateMatch"], ["", "def", "sent_to_distinct_matched_words", "(", "sent", ",", "lexicon_tree", ")", ":", "\n", "    ", "\"\"\"\n    \u5f97\u5230\u53e5\u5b50\u7684\u5339\u914d\u8bcd, \u5e76\u8fdb\u884c\u5206\u7ec4, \u6309\u7167BMES\u8fdb\u884c\u5206\u7ec4\n    Args:\n        sent: \u4e00\u4e2a\u5b57\u7684\u6570\u7ec4\n        lexicon_tree: \u8bcd\u6c47\u8868\u6811\n        max_word_num: \u6700\u5927\u8bcd\u6570\n    \"\"\"", "\n", "sent_length", "=", "len", "(", "sent", ")", "\n", "sent_words", "=", "[", "[", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "]", "for", "_", "in", "range", "(", "sent_length", ")", "]", "# \u6bcf\u4e2a\u5b57\u90fd\u6709\u5bf9\u5e94BMES", "\n", "sent_group_mask", "=", "[", "[", "0", ",", "0", ",", "0", ",", "0", "]", "for", "_", "in", "range", "(", "sent_length", ")", "]", "\n", "\n", "for", "idx", "in", "range", "(", "sent_length", ")", ":", "\n", "        ", "sub_sent", "=", "sent", "[", "idx", ":", "idx", "+", "lexicon_tree", ".", "max_depth", "]", "\n", "words", "=", "lexicon_tree", ".", "enumerateMatch", "(", "sub_sent", ")", "\n", "if", "len", "(", "words", ")", "==", "0", ":", "\n", "            ", "continue", "\n", "", "else", ":", "\n", "            ", "for", "word", "in", "words", ":", "\n", "                ", "word_length", "=", "len", "(", "word", ")", "\n", "if", "word_length", "==", "1", ":", "\n", "                    ", "sent_words", "[", "idx", "]", "[", "3", "]", ".", "append", "(", "word", ")", "\n", "sent_group_mask", "[", "idx", "]", "[", "3", "]", "=", "1", "\n", "", "else", ":", "\n", "                    ", "sent_words", "[", "idx", "]", "[", "0", "]", ".", "append", "(", "word", ")", "# begin", "\n", "sent_group_mask", "[", "idx", "]", "[", "0", "]", "=", "1", "\n", "for", "pos", "in", "range", "(", "1", ",", "word_length", "-", "1", ")", ":", "\n", "                        ", "sent_words", "[", "idx", "+", "pos", "]", "[", "1", "]", ".", "append", "(", "word", ")", "# middle", "\n", "", "sent_words", "[", "idx", "+", "word_length", "-", "1", "]", "[", "2", "]", ".", "append", "(", "word", ")", "# end", "\n", "", "", "", "if", "len", "(", "sent_words", "[", "idx", "]", "[", "1", "]", ")", ">", "0", ":", "\n", "            ", "sent_group_mask", "[", "idx", "]", "[", "1", "]", "=", "1", "\n", "", "if", "len", "(", "sent_words", "[", "idx", "]", "[", "2", "]", ")", ">", "0", ":", "\n", "            ", "sent_group_mask", "[", "idx", "]", "[", "2", "]", "=", "1", "\n", "\n", "", "", "return", "sent_words", ",", "sent_group_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.function.preprocess.sent_to_matched_words": [[126, 154], ["len", "range", "lexicon_tree.enumerateMatch", "range", "len", "sent_words[].extend", "len", "len", "len", "sent_words[].extend", "range", "sent_words[].append", "len", "sent_words[].append", "len"], "function", ["home.repos.pwc.inspect_result.liuwei1206_LEBERT.module.lexicon_tree.Trie.enumerateMatch"], ["", "def", "sent_to_matched_words", "(", "sent", ",", "lexicon_tree", ",", "max_word_num", "=", "None", ")", ":", "\n", "    ", "\"\"\"same to sent_to_matched_words_boundaries, but only return words\"\"\"", "\n", "sent_length", "=", "len", "(", "sent", ")", "\n", "sent_words", "=", "[", "[", "]", "for", "_", "in", "range", "(", "sent_length", ")", "]", "\n", "\n", "for", "idx", "in", "range", "(", "sent_length", ")", ":", "\n", "        ", "sub_sent", "=", "sent", "[", "idx", ":", "idx", "+", "lexicon_tree", ".", "max_depth", "]", "# speed using max depth", "\n", "words", "=", "lexicon_tree", ".", "enumerateMatch", "(", "sub_sent", ")", "\n", "\n", "if", "len", "(", "words", ")", "==", "0", ":", "\n", "            ", "continue", "\n", "", "else", ":", "\n", "            ", "if", "len", "(", "words", ")", "==", "1", "and", "len", "(", "words", "[", "0", "]", ")", "==", "1", ":", "# single character word", "\n", "                ", "if", "len", "(", "sent_words", "[", "idx", "]", ")", "==", "0", ":", "\n", "                    ", "sent_words", "[", "idx", "]", ".", "extend", "(", "words", ")", "\n", "", "", "else", ":", "\n", "                ", "if", "max_word_num", ":", "\n", "                    ", "need_num", "=", "max_word_num", "-", "len", "(", "sent_words", "[", "idx", "]", ")", "\n", "words", "=", "words", "[", ":", "need_num", "]", "\n", "", "sent_words", "[", "idx", "]", ".", "extend", "(", "words", ")", "\n", "for", "word", "in", "words", ":", "\n", "                    ", "start_pos", "=", "idx", "+", "1", "\n", "end_pos", "=", "idx", "+", "len", "(", "word", ")", "-", "1", "\n", "for", "tmp_j", "in", "range", "(", "start_pos", ",", "end_pos", ")", ":", "\n", "                        ", "sent_words", "[", "tmp_j", "]", ".", "append", "(", "word", ")", "\n", "", "sent_words", "[", "end_pos", "]", ".", "append", "(", "word", ")", "\n", "\n", "", "", "", "", "return", "sent_words", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.function.preprocess.sent_to_matched_words_set": [[155, 168], ["len", "set", "range", "list", "sorted", "lexicon_tree.enumerateMatch", "range", "sorted.add"], "function", ["home.repos.pwc.inspect_result.liuwei1206_LEBERT.module.lexicon_tree.Trie.enumerateMatch"], ["", "def", "sent_to_matched_words_set", "(", "sent", ",", "lexicon_tree", ",", "max_word_num", "=", "None", ")", ":", "\n", "    ", "\"\"\"return matched words set\"\"\"", "\n", "sent_length", "=", "len", "(", "sent", ")", "\n", "sent_words", "=", "[", "[", "]", "for", "_", "in", "range", "(", "sent_length", ")", "]", "\n", "matched_words_set", "=", "set", "(", ")", "\n", "for", "idx", "in", "range", "(", "sent_length", ")", ":", "\n", "        ", "sub_sent", "=", "sent", "[", "idx", ":", "idx", "+", "lexicon_tree", ".", "max_depth", "]", "# speed using max depth", "\n", "words", "=", "lexicon_tree", ".", "enumerateMatch", "(", "sub_sent", ")", "\n", "\n", "_", "=", "[", "matched_words_set", ".", "add", "(", "word", ")", "for", "word", "in", "words", "]", "\n", "", "matched_words_set", "=", "list", "(", "matched_words_set", ")", "\n", "matched_words_set", "=", "sorted", "(", "matched_words_set", ")", "\n", "return", "matched_words_set", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.function.preprocess.get_corpus_matched_word_from_vocab_files": [[170, 210], ["set", "zip", "list", "sorted", "module.lexicon_tree.Trie", "preprocess.get_corpus_matched_word_from_lexicon_tree", "len", "module.lexicon_tree.Trie.insert", "open", "f.readlines", "len", "tqdm.trange", "min", "line.strip.strip", "line.strip.split", "items[].strip", "sorted.add"], "function", ["home.repos.pwc.inspect_result.liuwei1206_LEBERT.function.preprocess.get_corpus_matched_word_from_lexicon_tree", "home.repos.pwc.inspect_result.liuwei1206_LEBERT.module.lexicon_tree.Trie.insert"], ["", "def", "get_corpus_matched_word_from_vocab_files", "(", "files", ",", "vocab_files", ",", "scan_nums", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    the corpus's matched words from vocab files\n    Args:\n        files: input data files\n        vocab_files: input vocab files\n        scan_num: -1 total,\n    Returns:\n        total_matched_words:\n        lexicon_tree:\n    \"\"\"", "\n", "# 1.\u83b7\u53d6\u8bcd\u6c47\u8868", "\n", "vocabs", "=", "set", "(", ")", "\n", "if", "scan_nums", "is", "None", ":", "\n", "        ", "length", "=", "len", "(", "vocab_files", ")", "\n", "scan_nums", "=", "[", "-", "1", "]", "*", "length", "\n", "\n", "", "for", "file", ",", "need_num", "in", "zip", "(", "vocab_files", ",", "scan_nums", ")", ":", "\n", "        ", "with", "open", "(", "file", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "            ", "lines", "=", "f", ".", "readlines", "(", ")", "\n", "total_line_num", "=", "len", "(", "lines", ")", "\n", "if", "need_num", ">=", "0", ":", "\n", "                ", "total_line_num", "=", "min", "(", "total_line_num", ",", "need_num", ")", "\n", "\n", "", "line_iter", "=", "trange", "(", "total_line_num", ")", "\n", "for", "idx", "in", "line_iter", ":", "\n", "                ", "line", "=", "lines", "[", "idx", "]", "\n", "line", "=", "line", ".", "strip", "(", ")", "\n", "items", "=", "line", ".", "split", "(", ")", "\n", "word", "=", "items", "[", "0", "]", ".", "strip", "(", ")", "\n", "vocabs", ".", "add", "(", "word", ")", "\n", "", "", "", "vocabs", "=", "list", "(", "vocabs", ")", "\n", "vocabs", "=", "sorted", "(", "vocabs", ")", "\n", "# 2.\u5efa\u7acb\u8bcd\u5178\u6811", "\n", "lexicon_tree", "=", "Trie", "(", ")", "\n", "for", "word", "in", "vocabs", ":", "\n", "        ", "lexicon_tree", ".", "insert", "(", "word", ")", "\n", "\n", "", "total_matched_words", "=", "get_corpus_matched_word_from_lexicon_tree", "(", "files", ",", "lexicon_tree", ")", "\n", "return", "total_matched_words", ",", "lexicon_tree", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.function.preprocess.get_corpus_matched_word_from_lexicon_tree": [[212, 250], ["set", "list", "sorted", "open", "open", "f.readlines", "len", "tqdm.trange", "f.write", "line.strip.strip", "json.loads", "preprocess.sent_to_matched_words_set", "sorted.add"], "function", ["home.repos.pwc.inspect_result.liuwei1206_LEBERT.function.preprocess.sent_to_matched_words_set"], ["", "def", "get_corpus_matched_word_from_lexicon_tree", "(", "files", ",", "lexicon_tree", ")", ":", "\n", "    ", "\"\"\"\n    \u6570\u636e\u7c7b\u578b\u7edf\u4e00\u4e3ajson\u683c\u5f0f, {'text': , 'label': }\n    Args:\n        files: corpus data files\n        lexicon_tree: built lexicon tree\n\n    Return:\n        total_matched_words: all found matched words\n    \"\"\"", "\n", "total_matched_words", "=", "set", "(", ")", "\n", "for", "file", "in", "files", ":", "\n", "        ", "with", "open", "(", "file", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "            ", "lines", "=", "f", ".", "readlines", "(", ")", "\n", "total_line_num", "=", "len", "(", "lines", ")", "\n", "line_iter", "=", "trange", "(", "total_line_num", ")", "\n", "for", "idx", "in", "line_iter", ":", "\n", "                ", "line", "=", "lines", "[", "idx", "]", "\n", "line", "=", "line", ".", "strip", "(", ")", "\n", "\n", "sample", "=", "json", ".", "loads", "(", "line", ")", "\n", "if", "'text'", "in", "sample", ":", "\n", "                    ", "text", "=", "sample", "[", "'text'", "]", "\n", "", "elif", "'text_a'", "in", "sample", "and", "'text_b'", "in", "sample", ":", "\n", "                    ", "text_a", "=", "sample", "[", "'text_a'", "]", "\n", "text_b", "=", "sample", "[", "'text_b'", "]", "\n", "text", "=", "text_a", "+", "[", "\"[SEP]\"", "]", "+", "text_b", "\n", "", "sent", "=", "[", "ch", "for", "ch", "in", "text", "]", "\n", "sent_matched_words", "=", "sent_to_matched_words_set", "(", "sent", ",", "lexicon_tree", ")", "\n", "_", "=", "[", "total_matched_words", ".", "add", "(", "word", ")", "for", "word", "in", "sent_matched_words", "]", "\n", "\n", "", "", "", "total_matched_words", "=", "list", "(", "total_matched_words", ")", "\n", "total_matched_words", "=", "sorted", "(", "total_matched_words", ")", "\n", "with", "open", "(", "\"matched_word.txt\"", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "        ", "for", "word", "in", "total_matched_words", ":", "\n", "            ", "f", ".", "write", "(", "\"%s\\n\"", "%", "(", "word", ")", ")", "\n", "\n", "", "", "return", "total_matched_words", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.function.preprocess.insert_seg_vocab_to_lexicon_tree": [[251, 292], ["set", "set", "list", "sorted", "print", "open", "f.readlines", "len", "tqdm.trange", "open", "f.readlines", "len", "tqdm.trange", "len", "lexicon_tree.insert", "line.strip.strip", "line.strip.strip", "set.add", "set.add"], "function", ["home.repos.pwc.inspect_result.liuwei1206_LEBERT.module.lexicon_tree.Trie.insert"], ["", "def", "insert_seg_vocab_to_lexicon_tree", "(", "seg_vocab", ",", "word_vocab", ",", "lexicon_tree", ")", ":", "\n", "    ", "\"\"\"\n    \u901a\u8fc7\u67e5\u627eseg_vocab\u548cword_vocab\u7684\u91cd\u5408\u8bcd, \u5c06\u91cd\u5408\u8bcd\u63d2\u5165\u5230lexicon_tree\u91cc\u9762\n    Args:\n        seg_vocab: seg_vocab\u4e2d\u7684\u8bcd\u6587\u4ef6\n        word_vocab: \u5168\u91cf\u7684\u8bcd\u6587\u4ef6\n        lexicon_tree:\n    \"\"\"", "\n", "seg_words", "=", "set", "(", ")", "\n", "whole_words", "=", "set", "(", ")", "\n", "with", "open", "(", "seg_vocab", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "lines", "=", "f", ".", "readlines", "(", ")", "\n", "total_line_num", "=", "len", "(", "lines", ")", "\n", "line_iter", "=", "trange", "(", "total_line_num", ")", "\n", "\n", "for", "idx", "in", "line_iter", ":", "\n", "            ", "line", "=", "lines", "[", "idx", "]", "\n", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "line", ":", "\n", "                ", "seg_words", ".", "add", "(", "line", ")", "\n", "\n", "", "", "", "with", "open", "(", "word_vocab", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "lines", "=", "f", ".", "readlines", "(", ")", "\n", "total_line_num", "=", "len", "(", "lines", ")", "\n", "line_iter", "=", "trange", "(", "total_line_num", ")", "\n", "\n", "for", "idx", "in", "line_iter", ":", "\n", "            ", "line", "=", "lines", "[", "idx", "]", "\n", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "line", ":", "\n", "                ", "whole_words", ".", "add", "(", "line", ")", "\n", "\n", "", "", "", "overleap_words", "=", "seg_words", "&", "whole_words", "\n", "overleap_words", "=", "list", "(", "overleap_words", ")", "\n", "overleap_words", "=", "sorted", "(", "overleap_words", ")", "\n", "print", "(", "\"Overleap words number is: \\n\"", ",", "len", "(", "overleap_words", ")", ")", "\n", "\n", "for", "word", "in", "overleap_words", ":", "\n", "        ", "lexicon_tree", ".", "insert", "(", "word", ")", "\n", "\n", "", "return", "lexicon_tree", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.function.preprocess.build_lexicon_tree_from_vocabs": [[294, 324], ["print", "set", "zip", "list", "sorted", "module.lexicon_tree.Trie", "len", "module.lexicon_tree.Trie.insert", "open", "f.readlines", "len", "tqdm.trange", "min", "line.strip.strip", "line.strip.split", "items[].strip", "sorted.add"], "function", ["home.repos.pwc.inspect_result.liuwei1206_LEBERT.module.lexicon_tree.Trie.insert"], ["", "def", "build_lexicon_tree_from_vocabs", "(", "vocab_files", ",", "scan_nums", "=", "None", ")", ":", "\n", "# 1.\u83b7\u53d6\u8bcd\u6c47\u8868", "\n", "    ", "print", "(", "vocab_files", ")", "\n", "vocabs", "=", "set", "(", ")", "\n", "if", "scan_nums", "is", "None", ":", "\n", "        ", "length", "=", "len", "(", "vocab_files", ")", "\n", "scan_nums", "=", "[", "-", "1", "]", "*", "length", "\n", "\n", "", "for", "file", ",", "need_num", "in", "zip", "(", "vocab_files", ",", "scan_nums", ")", ":", "\n", "        ", "with", "open", "(", "file", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "            ", "lines", "=", "f", ".", "readlines", "(", ")", "\n", "total_line_num", "=", "len", "(", "lines", ")", "\n", "if", "need_num", ">=", "0", ":", "\n", "                ", "total_line_num", "=", "min", "(", "total_line_num", ",", "need_num", ")", "\n", "\n", "", "line_iter", "=", "trange", "(", "total_line_num", ")", "\n", "for", "idx", "in", "line_iter", ":", "\n", "                ", "line", "=", "lines", "[", "idx", "]", "\n", "line", "=", "line", ".", "strip", "(", ")", "\n", "items", "=", "line", ".", "split", "(", ")", "\n", "word", "=", "items", "[", "0", "]", ".", "strip", "(", ")", "\n", "vocabs", ".", "add", "(", "word", ")", "\n", "", "", "", "vocabs", "=", "list", "(", "vocabs", ")", "\n", "vocabs", "=", "sorted", "(", "vocabs", ")", "\n", "# 2.\u5efa\u7acb\u8bcd\u5178\u6811", "\n", "lexicon_tree", "=", "Trie", "(", ")", "\n", "for", "word", "in", "vocabs", ":", "\n", "        ", "lexicon_tree", ".", "insert", "(", "word", ")", "\n", "\n", "", "return", "lexicon_tree", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.function.preprocess.get_all_labels_from_corpus": [[326, 351], ["open", "open", "f.readlines", "f.write", "line.strip.strip", "json.loads", "isinstance", "labels.append", "labels.append"], "function", ["None"], ["", "def", "get_all_labels_from_corpus", "(", "files", ",", "label_file", ",", "defalut_label", "=", "'O'", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        files: data files\n        label_file:\n    \"\"\"", "\n", "labels", "=", "[", "defalut_label", "]", "\n", "for", "file", "in", "files", ":", "\n", "        ", "with", "open", "(", "file", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "            ", "lines", "=", "f", ".", "readlines", "(", ")", "\n", "for", "line", "in", "lines", ":", "\n", "                ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "line", ":", "\n", "                    ", "sample", "=", "json", ".", "loads", "(", "line", ")", "\n", "label", "=", "sample", "[", "'label'", "]", "\n", "if", "isinstance", "(", "label", ",", "list", ")", ":", "\n", "                        ", "for", "l", "in", "label", ":", "\n", "                            ", "if", "l", "not", "in", "labels", ":", "\n", "                                ", "labels", ".", "append", "(", "l", ")", "\n", "", "", "", "else", ":", "\n", "                        ", "labels", ".", "append", "(", "label", ")", "\n", "\n", "", "", "", "", "", "with", "open", "(", "label_file", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "for", "label", "in", "labels", ":", "\n", "            ", "f", ".", "write", "(", "\"%s\\n\"", "%", "(", "label", ")", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.function.utils.load_pretrain_embed": [[13, 55], ["dict", "open", "f.readlines", "min", "tqdm.trange", "len", "len", "line.strip.strip", "line.strip.split", "len", "int", "len", "numpy.empty", "len", "print", "numpy.empty", "print"], "function", ["None"], ["def", "load_pretrain_embed", "(", "embedding_path", ",", "max_scan_num", "=", "1000000", ",", "add_seg_vocab", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    \u4ecepretrained word embedding\u4e2d\u8bfb\u53d6\u524dmax_scan_num\u7684\u8bcd\u5411\u91cf\n    Args:\n        embedding_path: \u8bcd\u5411\u91cf\u8def\u5f84\n        max_scan_num: \u6700\u591a\u8bfb\u591a\u5c11\n    \"\"\"", "\n", "## \u5982\u679c\u662f\u4f7f\u7528add_seg_vocab, \u5219\u5168\u5c40\u904d\u5386", "\n", "if", "add_seg_vocab", ":", "\n", "        ", "max_scan_num", "=", "-", "1", "\n", "\n", "", "embed_dict", "=", "dict", "(", ")", "\n", "embed_dim", "=", "-", "1", "\n", "with", "open", "(", "embedding_path", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "lines", "=", "f", ".", "readlines", "(", ")", "\n", "if", "max_scan_num", "==", "-", "1", ":", "\n", "            ", "max_scan_num", "=", "len", "(", "lines", ")", "\n", "", "max_scan_num", "=", "min", "(", "max_scan_num", ",", "len", "(", "lines", ")", ")", "\n", "line_iter", "=", "trange", "(", "max_scan_num", ")", "\n", "for", "idx", "in", "line_iter", ":", "\n", "            ", "line", "=", "lines", "[", "idx", "]", "\n", "line", "=", "line", ".", "strip", "(", ")", "\n", "items", "=", "line", ".", "split", "(", ")", "\n", "if", "len", "(", "items", ")", "==", "2", ":", "\n", "                ", "embed_dim", "=", "int", "(", "items", "[", "1", "]", ")", "\n", "continue", "\n", "", "elif", "len", "(", "items", ")", "==", "201", ":", "\n", "                ", "token", "=", "items", "[", "0", "]", "\n", "embedd", "=", "np", ".", "empty", "(", "[", "1", ",", "embed_dim", "]", ")", "\n", "embedd", "[", ":", "]", "=", "items", "[", "1", ":", "]", "\n", "embed_dict", "[", "token", "]", "=", "embedd", "\n", "", "elif", "len", "(", "items", ")", ">", "201", ":", "\n", "                ", "print", "(", "\"++++longer than 201+++++, line is: %s\\n\"", "%", "(", "line", ")", ")", "\n", "token", "=", "items", "[", "0", ":", "-", "200", "]", "\n", "token", "=", "\"\"", ".", "join", "(", "token", ")", "\n", "embedd", "=", "np", ".", "empty", "(", "[", "1", ",", "embed_dim", "]", ")", "\n", "embedd", "[", ":", "]", "=", "items", "[", "-", "200", ":", "]", "\n", "embed_dict", "[", "token", "]", "=", "embedd", "\n", "", "else", ":", "\n", "                ", "print", "(", "\"-------error word-------, line is: %s\\n\"", "%", "(", "line", ")", ")", "\n", "\n", "", "", "", "return", "embed_dict", ",", "embed_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.function.utils.build_pretrained_embedding_for_corpus": [[57, 106], ["os.path.join", "os.path.exists", "dict", "numpy.sqrt", "numpy.empty", "enumerate", "len", "print", "utils.load_pretrain_embed", "open", "pickle.dump", "open", "pickle.load", "numpy.random.uniform"], "function", ["home.repos.pwc.inspect_result.liuwei1206_LEBERT.function.utils.load_pretrain_embed"], ["", "def", "build_pretrained_embedding_for_corpus", "(", "\n", "embedding_path", ",", "\n", "word_vocab", ",", "\n", "embed_dim", "=", "200", ",", "\n", "max_scan_num", "=", "1000000", ",", "\n", "saved_corpus_embedding_dir", "=", "None", ",", "\n", "add_seg_vocab", "=", "False", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        embedding_path: \u9884\u8bad\u7ec3\u7684word embedding\u8def\u5f84\n        word_vocab: corpus\u7684word vocab\n        embed_dim: \u7ef4\u5ea6\n        max_scan_num: \u6700\u5927\u6d4f\u89c8\u591a\u5927\u6570\u91cf\u7684\u8bcd\u8868\n        saved_corpus_embedding_dir: \u8fd9\u4e2acorpus\u5bf9\u5e94\u7684embedding\u4fdd\u5b58\u8def\u5f84\n    \"\"\"", "\n", "saved_corpus_embedding_file", "=", "os", ".", "path", ".", "join", "(", "saved_corpus_embedding_dir", ",", "'saved_word_embedding_{}.pkl'", ".", "format", "(", "max_scan_num", ")", ")", "\n", "\n", "if", "os", ".", "path", ".", "exists", "(", "saved_corpus_embedding_file", ")", ":", "\n", "        ", "with", "open", "(", "saved_corpus_embedding_file", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "pretrained_emb", "=", "pickle", ".", "load", "(", "f", ")", "\n", "", "return", "pretrained_emb", ",", "embed_dim", "\n", "\n", "", "embed_dict", "=", "dict", "(", ")", "\n", "if", "embedding_path", "is", "not", "None", ":", "\n", "        ", "embed_dict", ",", "embed_dim", "=", "load_pretrain_embed", "(", "embedding_path", ",", "max_scan_num", "=", "max_scan_num", ",", "add_seg_vocab", "=", "add_seg_vocab", ")", "\n", "\n", "", "scale", "=", "np", ".", "sqrt", "(", "3.0", "/", "embed_dim", ")", "\n", "pretrained_emb", "=", "np", ".", "empty", "(", "[", "word_vocab", ".", "item_size", ",", "embed_dim", "]", ")", "\n", "\n", "matched", "=", "0", "\n", "not_matched", "=", "0", "\n", "\n", "for", "idx", ",", "word", "in", "enumerate", "(", "word_vocab", ".", "idx2item", ")", ":", "\n", "        ", "if", "word", "in", "embed_dict", ":", "\n", "            ", "pretrained_emb", "[", "idx", ",", ":", "]", "=", "embed_dict", "[", "word", "]", "\n", "matched", "+=", "1", "\n", "", "else", ":", "\n", "            ", "pretrained_emb", "[", "idx", ",", ":", "]", "=", "np", ".", "random", ".", "uniform", "(", "-", "scale", ",", "scale", ",", "[", "1", ",", "embed_dim", "]", ")", "\n", "not_matched", "+=", "1", "\n", "\n", "", "", "pretrained_size", "=", "len", "(", "embed_dict", ")", "\n", "print", "(", "\"Embedding:\\n     pretrain word:%s, prefect match:%s, oov:%s, oov%%:%s\"", "%", "(", "\n", "pretrained_size", ",", "matched", ",", "not_matched", ",", "(", "not_matched", "+", "0.", ")", "/", "word_vocab", ".", "item_size", ")", ")", "\n", "\n", "with", "open", "(", "saved_corpus_embedding_file", ",", "'wb'", ")", "as", "f", ":", "\n", "        ", "pickle", ".", "dump", "(", "pretrained_emb", ",", "f", ",", "protocol", "=", "4", ")", "\n", "\n", "", "return", "pretrained_emb", ",", "embed_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.function.utils.reverse_padded_sequence": [[107, 138], ["torch.LongTensor().transpose", "range", "ind.cuda.expand_as", "torch.gather", "inputs.transpose.transpose", "inputs.transpose.size", "inputs.transpose.size", "len", "ValueError", "inputs.transpose.dim", "ind.cuda.unsqueeze", "ind.cuda.cuda", "reversed_inputs.transpose.transpose", "list", "list", "torch.LongTensor", "reversed", "range", "range"], "function", ["None"], ["", "def", "reverse_padded_sequence", "(", "inputs", ",", "lengths", ",", "batch_first", "=", "True", ")", ":", "\n", "    ", "\"\"\"Reverses sequences according to their lengths.\n    Inputs should have size ``T x B x *`` if ``batch_first`` is False, or\n    ``B x T x *`` if True. T is the length of the longest sequence (or larger),\n    B is the batch size, and * is any number of dimensions (including 0).\n    Arguments:\n        inputs (Variable): padded batch of variable length sequences.\n        lengths (list[int]): list of sequence lengths\n        batch_first (bool, optional): if True, inputs should be B x T x *.\n    Returns:\n        A Variable with the same size as inputs, but with each sequence\n        reversed according to its length.\n    \"\"\"", "\n", "if", "batch_first", ":", "\n", "        ", "inputs", "=", "inputs", ".", "transpose", "(", "0", ",", "1", ")", "\n", "", "max_length", ",", "batch_size", "=", "inputs", ".", "size", "(", "0", ")", ",", "inputs", ".", "size", "(", "1", ")", "\n", "if", "len", "(", "lengths", ")", "!=", "batch_size", ":", "\n", "        ", "raise", "ValueError", "(", "\"inputs is incompatible with lengths.\"", ")", "\n", "", "ind", "=", "[", "list", "(", "reversed", "(", "range", "(", "0", ",", "length", ")", ")", ")", "+", "list", "(", "range", "(", "length", ",", "max_length", ")", ")", "\n", "for", "length", "in", "lengths", "]", "\n", "ind", "=", "torch", ".", "LongTensor", "(", "ind", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "for", "dim", "in", "range", "(", "2", ",", "inputs", ".", "dim", "(", ")", ")", ":", "\n", "        ", "ind", "=", "ind", ".", "unsqueeze", "(", "dim", ")", "\n", "", "ind", "=", "ind", ".", "expand_as", "(", "inputs", ")", "\n", "if", "inputs", ".", "is_cuda", ":", "\n", "        ", "ind", "=", "ind", ".", "cuda", "(", ")", "\n", "", "reversed_inputs", "=", "torch", ".", "gather", "(", "inputs", ",", "0", ",", "ind", ")", "\n", "if", "batch_first", ":", "\n", "        ", "reversed_inputs", "=", "reversed_inputs", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "", "return", "reversed_inputs", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.function.utils.random_embedding": [[140, 146], ["numpy.empty", "numpy.sqrt", "range", "numpy.random.uniform"], "function", ["None"], ["", "def", "random_embedding", "(", "self", ",", "vocab_size", ",", "embedding_dim", ")", ":", "\n", "    ", "pretrain_emb", "=", "np", ".", "empty", "(", "[", "vocab_size", ",", "embedding_dim", "]", ")", "\n", "scale", "=", "np", ".", "sqrt", "(", "3.0", "/", "embedding_dim", ")", "\n", "for", "index", "in", "range", "(", "vocab_size", ")", ":", "\n", "        ", "pretrain_emb", "[", "index", ",", ":", "]", "=", "np", ".", "random", ".", "uniform", "(", "-", "scale", ",", "scale", ",", "[", "1", ",", "embedding_dim", "]", ")", "\n", "", "return", "pretrain_emb", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.function.utils.gather_indexes": [[148, 174], ["sequence_tensor.size", "sequence_tensor.size", "sequence_tensor.size", "torch.tensor", "whole_seq_length.to.to", "torch.cumsum", "flat_offsets.unsqueeze.unsqueeze", "flat_positions.contiguous().view.contiguous().view", "sequence_tensor.contiguous().view", "sequence_tensor.contiguous().view.index_select", "output_tensor.contiguous().view.contiguous().view", "flat_positions.contiguous().view.contiguous", "sequence_tensor.contiguous", "output_tensor.contiguous().view.contiguous", "range"], "function", ["None"], ["", "def", "gather_indexes", "(", "sequence_tensor", ",", "positions", ")", ":", "\n", "    ", "\"\"\"\n    gather specific tensor based on the positions\n    Args:\n        sequence_tensor: [B, L, D]\n        positions: [B, P]\n    \"\"\"", "\n", "batch_size", "=", "sequence_tensor", ".", "size", "(", "0", ")", "\n", "seq_length", "=", "sequence_tensor", ".", "size", "(", "1", ")", "\n", "dim", "=", "sequence_tensor", ".", "size", "(", "2", ")", "\n", "\n", "whole_seq_length", "=", "torch", ".", "tensor", "(", "[", "seq_length", "for", "_", "in", "range", "(", "batch_size", ")", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "whole_seq_length", "=", "whole_seq_length", ".", "to", "(", "sequence_tensor", ".", "device", ")", "\n", "\n", "flat_offsets", "=", "torch", ".", "cumsum", "(", "whole_seq_length", ",", "dim", "=", "-", "1", ")", "\n", "flat_offsets", "=", "flat_offsets", "-", "whole_seq_length", "# [B]", "\n", "flat_offsets", "=", "flat_offsets", ".", "unsqueeze", "(", "-", "1", ")", "# [B, 1]", "\n", "flat_positions", "=", "positions", "+", "flat_offsets", "# [B, P]", "\n", "flat_positions", "=", "flat_positions", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", "\n", "flat_sequence_tensor", "=", "sequence_tensor", ".", "contiguous", "(", ")", ".", "view", "(", "batch_size", "*", "seq_length", ",", "-", "1", ")", "# [B * L, D]", "\n", "\n", "# output_tensor = flat_sequence_tensor[flat_positions]", "\n", "output_tensor", "=", "flat_sequence_tensor", ".", "index_select", "(", "0", ",", "flat_positions", ")", "\n", "output_tensor", "=", "output_tensor", ".", "contiguous", "(", ")", ".", "view", "(", "batch_size", ",", "-", "1", ")", "\n", "\n", "return", "output_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.liuwei1206_LEBERT.function.utils.save_preds_for_seq_labelling": [[175, 203], ["open", "zip", "tokenizer.convert_ids_to_tokens", "len", "zip", "f.write", "len", "len", "len", "len", "len", "len", "len", "len", "f.write", "f.write"], "function", ["None"], ["", "def", "save_preds_for_seq_labelling", "(", "token_ids", ",", "tokenizer", ",", "true_labels", ",", "pred_labels", ",", "file", ")", ":", "\n", "    ", "\"\"\"\n    save sequence labelling result into files\n    Args:\n        token_ids:\n        tokenizer:\n        true_labels:\n        pred_labels:\n        file:\n    \"\"\"", "\n", "error_num", "=", "1", "\n", "with", "open", "(", "file", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "for", "w_ids", ",", "t_labels", ",", "p_labels", "in", "zip", "(", "token_ids", ",", "true_labels", ",", "pred_labels", ")", ":", "\n", "            ", "tokens", "=", "tokenizer", ".", "convert_ids_to_tokens", "(", "w_ids", ")", "\n", "token_num", "=", "len", "(", "t_labels", ")", "\n", "tokens", "=", "tokens", "[", "1", ":", "token_num", "+", "1", "]", "\n", "\n", "assert", "len", "(", "tokens", ")", "==", "len", "(", "t_labels", ")", ",", "(", "len", "(", "tokens", ")", ",", "len", "(", "t_labels", ")", ")", "\n", "assert", "len", "(", "tokens", ")", "==", "len", "(", "p_labels", ")", ",", "(", "len", "(", "tokens", ")", ",", "len", "(", "p_labels", ")", ")", "\n", "\n", "for", "w", ",", "t", ",", "p", "in", "zip", "(", "tokens", ",", "t_labels", ",", "p_labels", ")", ":", "\n", "                ", "if", "t", "==", "p", ":", "\n", "                    ", "f", ".", "write", "(", "\"%s\\t%s\\t%s\\n\"", "%", "(", "w", ",", "t", ",", "p", ")", ")", "\n", "", "else", ":", "\n", "                    ", "f", ".", "write", "(", "\"%s\\t%s\\t%s\\t%d\\n\"", "%", "(", "w", ",", "t", ",", "p", ",", "error_num", ")", ")", "\n", "error_num", "+=", "1", "\n", "\n", "", "", "f", ".", "write", "(", "\"\\n\"", ")", "", "", "", "", ""]]}