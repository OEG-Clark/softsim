{"home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.verif.Verif.__init__": [[58, 67], ["None"], "methods", ["None"], ["", "", "def", "__init__", "(", "self", ",", "\n", "posts", ":", "Dict", "[", "str", ",", "Post", "]", ",", "\n", "post_embeddings", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", "hparams", ":", "'Verif.Hyperparameters'", ",", "\n", "device", ":", "torch", ".", "device", ")", ":", "\n", "        ", "self", ".", "_posts", "=", "posts", "\n", "self", ".", "_post_embeddings", "=", "post_embeddings", "\n", "self", ".", "_hparams", "=", "hparams", "\n", "self", ".", "_device", "=", "device", "\n", "\n"]], "home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.verif.Verif.build_datasets": [[143, 197], ["print", "verif.Verif.Dataset", "verif.Verif.Dataset", "verif.Verif.Dataset", "verif.Verif.calc_stats_for_aux_feature", "len", "len", "len", "dataset.min_max_scale_aux_feature", "dataset.standard_scale_aux_feature", "ValueError"], "methods", ["home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.util.DatasetHelper.calc_stats_for_aux_feature", "home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.util.DatasetHelper.min_max_scale_aux_feature", "home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.util.DatasetHelper.standard_scale_aux_feature"], ["", "", "", "def", "build_datasets", "(", "self", ",", "\n", "train_instances", ":", "Iterable", "[", "VerifInstance", "]", ",", "\n", "dev_instances", ":", "Optional", "[", "Iterable", "[", "VerifInstance", "]", "]", ",", "\n", "test_instances", ":", "Optional", "[", "Iterable", "[", "VerifInstance", "]", "]", ",", "\n", "sdqc_estimates", ":", "Dict", "[", "str", ",", "Tuple", "[", "SdqcInstance", ".", "Label", ",", "\n", "Dict", "[", "SdqcInstance", ".", "Label", ",", "\n", "float", "]", "]", "]", ")", "->", "(", "'Verif.Dataset'", ",", "\n", "Optional", "[", "'Verif.Dataset'", "]", ",", "\n", "Optional", "[", "'Verif.Dataset'", "]", ")", ":", "\n", "        ", "print", "(", "'Number of instances: train={:d}, dev={:d}, test={:d}'", "\n", ".", "format", "(", "len", "(", "train_instances", ")", ",", "\n", "len", "(", "dev_instances", "or", "[", "]", ")", ",", "\n", "len", "(", "test_instances", "or", "[", "]", ")", ")", ")", "\n", "\n", "train_dataset", "=", "self", ".", "Dataset", "(", "\n", "train_instances", ",", "self", ".", "_posts", ",", "self", ".", "_post_embeddings", ",", "\n", "sdqc_estimates", ",", "self", ".", "_hparams", ",", "self", ".", "_device", ")", "\n", "\n", "dev_dataset", "=", "None", "\n", "if", "dev_instances", ":", "\n", "            ", "dev_dataset", "=", "self", ".", "Dataset", "(", "\n", "dev_instances", ",", "self", ".", "_posts", ",", "self", ".", "_post_embeddings", ",", "\n", "sdqc_estimates", ",", "self", ".", "_hparams", ",", "self", ".", "_device", ")", "\n", "\n", "", "test_dataset", "=", "None", "\n", "if", "test_instances", ":", "\n", "            ", "test_dataset", "=", "self", ".", "Dataset", "(", "\n", "test_instances", ",", "self", ".", "_posts", ",", "self", ".", "_post_embeddings", ",", "\n", "sdqc_estimates", ",", "self", ".", "_hparams", ",", "self", ".", "_device", ")", "\n", "\n", "", "def", "filter_func", "(", "post_id", ":", "str", ")", "->", "bool", ":", "\n", "            ", "return", "self", ".", "_posts", "[", "post_id", "]", ".", "platform", "==", "Post", ".", "Platform", ".", "twitter", "\n", "\n", "", "for", "index", "in", "self", ".", "_hparams", ".", "input_scaling_features", ":", "\n", "            ", "min", ",", "max", ",", "mean", ",", "std", "=", "train_dataset", ".", "calc_stats_for_aux_feature", "(", "index", ",", "filter_func", ")", "\n", "\n", "for", "dataset", "in", "(", "train_dataset", ",", "dev_dataset", ",", "test_dataset", ")", ":", "\n", "                ", "if", "not", "dataset", ":", "\n", "                    ", "continue", "\n", "\n", "", "if", "self", ".", "_hparams", ".", "input_scaling_mode", "==", "ScalingMode", ".", "none", ":", "\n", "                    ", "pass", "\n", "", "elif", "self", ".", "_hparams", ".", "input_scaling_mode", "==", "ScalingMode", ".", "min_max", ":", "\n", "                    ", "dataset", ".", "min_max_scale_aux_feature", "(", "\n", "index", ",", "min", ",", "max", ",", "filter_func", ")", "\n", "", "elif", "self", ".", "_hparams", ".", "input_scaling_mode", "==", "ScalingMode", ".", "standard", ":", "\n", "                    ", "dataset", ".", "standard_scale_aux_feature", "(", "\n", "index", ",", "mean", ",", "std", ",", "filter_func", ")", "\n", "", "else", ":", "\n", "                    ", "raise", "ValueError", "(", "'Unimplemented enum variant.'", ")", "\n", "\n", "", "", "", "return", "train_dataset", ",", "dev_dataset", ",", "test_dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.verif.Verif.train": [[234, 316], ["verif.Verif.Model().to", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.optim.Adam", "torch.optim.Adam", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "range", "verif.Verif.parameters", "enumerate", "verif.Verif.Model", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "tqdm.tqdm.tqdm", "torch.optim.Adam.zero_grad", "verif.Verif.train", "verif.Verif.", "torch.nn.CrossEntropyLoss.", "nn.CrossEntropyLoss.backward", "torch.optim.Adam.step", "losses.append", "numpy.concatenate.append", "numpy.concatenate.append", "numpy.concatenate.append", "tqdm.tqdm.tqdm.close", "numpy.concatenate", "numpy.concatenate", "numpy.concatenate", "numpy.maximum", "numpy.mean", "sklearn.metrics.accuracy_score", "sklearn.metrics.f1_score", "src.util.rmse_score", "print", "verif.Verif.eval", "print", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.softmax().max", "torch.softmax().max", "nn.CrossEntropyLoss.item", "batch[].data.cpu().numpy", "batch_prediction.data.cpu().numpy", "batch_prediction_prob.data.cpu().numpy", "tqdm.tqdm.tqdm.set_postfix", "tqdm.tqdm.tqdm.update", "len", "len", "torch.softmax", "torch.softmax", "batch[].data.cpu", "batch_prediction.data.cpu", "batch_prediction_prob.data.cpu", "str", "nn.CrossEntropyLoss.item"], "methods", ["home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.sdqc.Sdqc.train", "home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.util.rmse_score", "home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.sdqc.Sdqc.eval"], ["", "", "def", "train", "(", "self", ",", "\n", "train_dataset", ":", "'Verif.Dataset'", ",", "\n", "dev_dataset", ":", "Optional", "[", "'Verif.Dataset'", "]", "=", "None", ",", "\n", "print_progress", ":", "bool", "=", "False", ")", "->", "'Verif.Model'", ":", "\n", "        ", "model", "=", "self", ".", "Model", "(", "self", ".", "_hparams", ")", ".", "to", "(", "self", ".", "_device", ")", "\n", "\n", "criterion", "=", "nn", ".", "CrossEntropyLoss", "(", "\n", "weight", "=", "torch", ".", "tensor", "(", "self", ".", "_hparams", ".", "class_weights", ",", "\n", "dtype", "=", "torch", ".", "float32", ",", "\n", "device", "=", "self", ".", "_device", ")", ")", "\n", "optimizer", "=", "optim", ".", "Adam", "(", "model", ".", "parameters", "(", ")", ",", "\n", "lr", "=", "self", ".", "_hparams", ".", "learning_rate", ",", "\n", "weight_decay", "=", "self", ".", "_hparams", ".", "weight_decay", ")", "\n", "\n", "train_loader", "=", "DataLoader", "(", "\n", "train_dataset", ",", "batch_size", "=", "self", ".", "_hparams", ".", "batch_size", ",", "shuffle", "=", "True", ")", "\n", "\n", "for", "epoch_no", "in", "range", "(", "1", ",", "self", ".", "_hparams", ".", "num_epochs", "+", "1", ")", ":", "\n", "            ", "losses", ",", "labels", ",", "predictions", ",", "prediction_probs", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "\n", "progress_bar", "=", "None", "\n", "if", "print_progress", ":", "\n", "                ", "progress_bar", "=", "tqdm", "(", "total", "=", "(", "len", "(", "train_loader", ")", ")", ",", "\n", "unit", "=", "'batch'", ",", "\n", "desc", "=", "'Epoch: {:{}d}/{:d}'", ".", "format", "(", "\n", "epoch_no", ",", "\n", "len", "(", "str", "(", "self", ".", "_hparams", ".", "num_epochs", ")", ")", ",", "\n", "self", ".", "_hparams", ".", "num_epochs", ")", ")", "\n", "\n", "", "for", "batch_no", ",", "batch", "in", "enumerate", "(", "train_loader", ")", ":", "\n", "                ", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "model", ".", "train", "(", ")", "\n", "batch_logits", "=", "model", "(", "batch", "[", "'features'", "]", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                    ", "batch_prediction_prob", ",", "batch_prediction", "=", "F", ".", "softmax", "(", "batch_logits", ",", "dim", "=", "1", ")", ".", "max", "(", "dim", "=", "1", ")", "\n", "\n", "", "loss", "=", "criterion", "(", "batch_logits", ",", "batch", "[", "'label'", "]", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "losses", ".", "append", "(", "loss", ".", "item", "(", ")", ")", "\n", "labels", ".", "append", "(", "batch", "[", "'label'", "]", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "predictions", ".", "append", "(", "batch_prediction", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "prediction_probs", ".", "append", "(", "\n", "batch_prediction_prob", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "\n", "if", "progress_bar", ":", "\n", "                    ", "progress_bar", ".", "set_postfix", "(", "{", "\n", "'loss'", ":", "'{:.2e}'", ".", "format", "(", "loss", ".", "item", "(", ")", ")", ",", "\n", "}", ")", "\n", "progress_bar", ".", "update", "(", "1", ")", "\n", "\n", "", "", "if", "progress_bar", ":", "\n", "                ", "progress_bar", ".", "close", "(", ")", "\n", "\n", "labels", "=", "np", ".", "concatenate", "(", "labels", ")", "\n", "predictions", "=", "np", ".", "concatenate", "(", "predictions", ")", "\n", "prediction_probs", "=", "np", ".", "concatenate", "(", "prediction_probs", ")", "\n", "\n", "confidences", "=", "np", ".", "maximum", "(", "0.5", ",", "prediction_probs", ")", "\n", "confidences", "[", "\n", "predictions", "==", "VerifInstance", ".", "Label", ".", "unverified", ".", "value", "]", "=", "0", "\n", "\n", "epoch_loss", "=", "np", ".", "mean", "(", "losses", ")", "\n", "epoch_acc", "=", "accuracy_score", "(", "labels", ",", "predictions", ")", "\n", "epoch_f1", "=", "f1_score", "(", "labels", ",", "predictions", ",", "average", "=", "'macro'", ")", "\n", "epoch_rmse", "=", "rmse_score", "(", "labels", ",", "predictions", ",", "confidences", ")", "\n", "\n", "print", "(", "'  Loss={:.2e}  Accuracy={:.2%}  F1-score={:.2%}  '", "\n", "'RMSE={:.4f}'", ".", "format", "(", "epoch_loss", ",", "epoch_acc", ",", "epoch_f1", ",", "\n", "epoch_rmse", ")", ")", "\n", "\n", "", "if", "print_progress", "and", "dev_dataset", "and", "(", "epoch_no", "==", "self", ".", "_hparams", ".", "num_epochs", "\n", "or", "not", "epoch_no", "%", "EVAL_DEV_EVERY_N_EPOCH", ")", ":", "\n", "                ", "dev_acc", ",", "dev_f1", ",", "dev_rmse", ",", "_", "=", "self", ".", "eval", "(", "model", ",", "dev_dataset", ")", "\n", "print", "(", "'  Validation:    Accuracy={:.2%}  F1-score={:.2%}  '", "\n", "'RMSE={:.4f}'", ".", "format", "(", "dev_acc", ",", "dev_f1", ",", "dev_rmse", ")", ")", "\n", "\n", "", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.verif.Verif.eval": [[317, 351], ["numpy.concatenate", "numpy.concatenate", "numpy.concatenate", "numpy.maximum", "sklearn.metrics.accuracy_score", "sklearn.metrics.f1_score", "src.util.rmse_score", "sklearn.metrics.classification_report", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "model.eval", "model", "torch.softmax().max", "torch.softmax().max", "numpy.concatenate.append", "numpy.concatenate.append", "numpy.concatenate.append", "range", "batch[].data.cpu().numpy", "batch_predictions.data.cpu().numpy", "batch_prediction_probs.data.cpu().numpy", "len", "torch.softmax", "torch.softmax", "batch[].data.cpu", "batch_predictions.data.cpu", "batch_prediction_probs.data.cpu"], "methods", ["home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.util.rmse_score", "home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.sdqc.Sdqc.eval"], ["", "def", "eval", "(", "self", ",", "model", ":", "'Verif.Model'", ",", "dataset", ":", "'Verif.Dataset'", ")", "->", "(", "float", ",", "float", ",", "float", ",", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "float", "]", "]", ")", ":", "\n", "        ", "labels", ",", "predictions", ",", "prediction_probs", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "data_loader", "=", "DataLoader", "(", "\n", "dataset", ",", "batch_size", "=", "self", ".", "_hparams", ".", "batch_size", ")", "\n", "for", "batch", "in", "data_loader", ":", "\n", "                ", "model", ".", "eval", "(", ")", "\n", "batch_logits", "=", "model", "(", "batch", "[", "'features'", "]", ")", "\n", "batch_prediction_probs", ",", "batch_predictions", "=", "F", ".", "softmax", "(", "batch_logits", ",", "dim", "=", "1", ")", ".", "max", "(", "dim", "=", "1", ")", "\n", "\n", "labels", ".", "append", "(", "batch", "[", "'label'", "]", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "predictions", ".", "append", "(", "batch_predictions", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "prediction_probs", ".", "append", "(", "\n", "batch_prediction_probs", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "\n", "", "", "labels", "=", "np", ".", "concatenate", "(", "labels", ")", "\n", "predictions", "=", "np", ".", "concatenate", "(", "predictions", ")", "\n", "prediction_probs", "=", "np", ".", "concatenate", "(", "prediction_probs", ")", "\n", "\n", "confidences", "=", "np", ".", "maximum", "(", "0.5", ",", "prediction_probs", ")", "\n", "confidences", "[", "predictions", "==", "VerifInstance", ".", "Label", ".", "unverified", ".", "value", "]", "=", "0", "\n", "\n", "acc", "=", "accuracy_score", "(", "labels", ",", "predictions", ")", "\n", "f1", "=", "f1_score", "(", "labels", ",", "predictions", ",", "average", "=", "'macro'", ")", "\n", "rmse", "=", "rmse_score", "(", "labels", ",", "predictions", ",", "confidences", ")", "\n", "report", "=", "classification_report", "(", "\n", "labels", ",", "predictions", ",", "output_dict", "=", "True", ",", "\n", "labels", "=", "range", "(", "len", "(", "VerifInstance", ".", "Label", ")", ")", ",", "\n", "target_names", "=", "[", "label", ".", "name", "for", "label", "in", "VerifInstance", ".", "Label", "]", ")", "\n", "\n", "return", "acc", ",", "f1", ",", "rmse", ",", "report", "\n", "\n"]], "home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.verif.Verif.predict": [[352, 391], ["verif.Verif.Dataset", "src.dataset.VerifInstance", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "model.eval", "model", "torch.softmax().max", "torch.softmax().max", "batch_predictions.data.cpu().numpy.data.cpu().numpy.data.cpu().numpy", "batch_prediction_probs.data.cpu().numpy.data.cpu().numpy.data.cpu().numpy", "numpy.maximum", "zip", "torch.softmax", "torch.softmax", "batch_predictions.data.cpu().numpy.data.cpu().numpy.data.cpu", "batch_prediction_probs.data.cpu().numpy.data.cpu().numpy.data.cpu", "src.dataset.VerifInstance.Label", "confidence.item", "prediction.item"], "methods", ["home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.sdqc.Sdqc.eval"], ["", "def", "predict", "(", "self", ",", "\n", "model", ":", "'Verif.Model'", ",", "\n", "post_ids", ":", "Iterable", "[", "str", "]", ",", "\n", "sdqc_estimates", ":", "Dict", "[", "str", ",", "Tuple", "[", "SdqcInstance", ".", "Label", ",", "\n", "Dict", "[", "SdqcInstance", ".", "Label", ",", "\n", "float", "]", "]", "]", ")", "->", "Dict", "[", "str", ",", "Tuple", "[", "VerifInstance", ".", "Label", ",", "float", "]", "]", ":", "\n", "        ", "instances", "=", "[", "VerifInstance", "(", "post_id", ")", "for", "post_id", "in", "post_ids", "]", "\n", "dataset", "=", "self", ".", "Dataset", "(", "\n", "instances", ",", "self", ".", "_posts", ",", "self", ".", "_post_embeddings", ",", "sdqc_estimates", ",", "\n", "self", ".", "_hparams", ",", "self", ".", "_device", ")", "\n", "\n", "results", "=", "{", "}", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "data_loader", "=", "DataLoader", "(", "dataset", ",", "\n", "batch_size", "=", "self", ".", "_hparams", ".", "batch_size", ")", "\n", "for", "batch", "in", "data_loader", ":", "\n", "                ", "model", ".", "eval", "(", ")", "\n", "batch_logits", "=", "model", "(", "batch", "[", "'features'", "]", ")", "\n", "batch_prediction_probs", ",", "batch_predictions", "=", "F", ".", "softmax", "(", "batch_logits", ",", "dim", "=", "1", ")", ".", "max", "(", "dim", "=", "1", ")", "\n", "\n", "batch_predictions", "=", "batch_predictions", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "batch_prediction_probs", "=", "batch_prediction_probs", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "batch_confidences", "=", "np", ".", "maximum", "(", "0.5", ",", "batch_prediction_probs", ")", "\n", "batch_confidences", "[", "batch_predictions", "\n", "==", "VerifInstance", ".", "Label", ".", "unverified", ".", "value", "]", "=", "0", "\n", "batch_predictions", "[", "batch_predictions", "\n", "==", "VerifInstance", ".", "Label", ".", "unverified", ".", "value", "]", "=", "VerifInstance", ".", "Label", ".", "true", ".", "value", "\n", "\n", "for", "post_id", ",", "prediction", ",", "confidence", "in", "zip", "(", "\n", "batch", "[", "'post_id'", "]", ",", "batch_predictions", ",", "batch_confidences", ")", ":", "\n", "                    ", "results", "[", "post_id", "]", "=", "(", "VerifInstance", ".", "Label", "(", "prediction", ".", "item", "(", ")", ")", ",", "\n", "confidence", ".", "item", "(", ")", ")", "\n", "\n", "", "", "", "return", "results", "\n", "", "", ""]], "home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.dataset.Post.__init__": [[110, 140], ["TWEET_TOKENIZER.tokenize", "REDDIT_TOKENIZER.tokenize", "ValueError"], "methods", ["None"], ["", "def", "__init__", "(", "self", ",", "\n", "id", ":", "str", ",", "\n", "text", ":", "str", ",", "\n", "depth", ":", "int", ",", "\n", "platform", ":", "Platform", ",", "\n", "has_media", ":", "bool", ",", "\n", "source_id", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "topic", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "user_verified", ":", "Optional", "[", "bool", "]", "=", "None", ",", "\n", "followers_count", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "friends_count", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "upvote_ratio", ":", "Optional", "[", "float", "]", "=", "None", ")", ":", "\n", "        ", "self", ".", "id", "=", "id", "\n", "\n", "if", "platform", "==", "self", ".", "Platform", ".", "twitter", ":", "\n", "            ", "self", ".", "text", ":", "List", "[", "str", "]", "=", "TWEET_TOKENIZER", ".", "tokenize", "(", "text", ")", "\n", "", "elif", "platform", "==", "self", ".", "Platform", ".", "reddit", ":", "\n", "            ", "self", ".", "text", ":", "List", "[", "str", "]", "=", "REDDIT_TOKENIZER", ".", "tokenize", "(", "text", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", ")", "\n", "\n", "", "self", ".", "depth", "=", "depth", "\n", "self", ".", "platform", "=", "platform", "\n", "self", ".", "has_media", "=", "has_media", "\n", "self", ".", "source_id", "=", "source_id", "or", "self", ".", "id", "\n", "self", ".", "topic", "=", "topic", "\n", "self", ".", "user_verified", "=", "user_verified", "\n", "self", ".", "followers_count", "=", "followers_count", "\n", "self", ".", "friends_count", "=", "friends_count", "\n", "self", ".", "upvote_ratio", "=", "upvote_ratio", "\n", "\n"]], "home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.dataset.Post.has_source_depth": [[141, 145], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "has_source_depth", "(", "self", ")", "->", "bool", ":", "\n", "        ", "\"\"\"Whether the post is the source of a thread.\"\"\"", "\n", "return", "self", ".", "depth", "==", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.dataset.Post.has_reply_depth": [[146, 150], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "has_reply_depth", "(", "self", ")", "->", "bool", ":", "\n", "        ", "\"\"\"Whether the post is a reply to the source of a thread.\"\"\"", "\n", "return", "self", ".", "depth", "==", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.dataset.Post.has_nested_depth": [[151, 155], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "has_nested_depth", "(", "self", ")", "->", "bool", ":", "\n", "        ", "\"\"\"Whether the post is neither source nor reply to a thread's source.\"\"\"", "\n", "return", "self", ".", "depth", ">=", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.dataset.Post.url": [[156, 167], ["ValueError"], "methods", ["None"], ["", "@", "property", "\n", "def", "url", "(", "self", ")", "->", "str", ":", "\n", "        ", "\"\"\"Url of the post (useful for debugging).\"\"\"", "\n", "if", "self", ".", "platform", "==", "self", ".", "Platform", ".", "twitter", ":", "\n", "            ", "return", "'https://twitter.com/statuses/{}'", ".", "format", "(", "self", ".", "id", ")", "\n", "", "elif", "self", ".", "platform", "==", "self", ".", "Platform", ".", "reddit", ":", "\n", "            ", "if", "self", ".", "source_id", "==", "self", ".", "id", ":", "\n", "                ", "return", "'https://reddit.com//comments/{}'", ".", "format", "(", "self", ".", "id", ")", "\n", "", "return", "'https://reddit.com//comments/{}//{}'", ".", "format", "(", "self", ".", "source_id", ",", "\n", "self", ".", "id", ")", "\n", "", "raise", "ValueError", "(", "'Invalid post source value, must be either Twitter or '", "\n", "'Reddit.'", ")", "\n"]], "home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.dataset.Post.__repr__": [[169, 171], ["vars"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", "->", "str", ":", "\n", "        ", "return", "'Post {}'", ".", "format", "(", "vars", "(", "self", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.dataset.Post.load_from_twitter_dict": [[172, 201], ["dataset.Post"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "load_from_twitter_dict", "(", "cls", ",", "\n", "twitter_dict", ":", "Dict", ",", "\n", "post_depths", ":", "Dict", "[", "str", ",", "int", "]", ",", "\n", "source_id", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "topic", ":", "Optional", "[", "str", "]", "=", "None", ")", "->", "'Post'", ":", "\n", "        ", "\"\"\"Creates a `Post` instance from a JSON dict of a Twitter post.\n\n        Args:\n            twitter_dict: The JSON dict.\n            post_depths: A map that gives the depth of the post by it's ID.\n            source_id: The ID of the thread's source post. `None` if this post\n                is itself the source post.\n            topic: The rumor topic the posts is labelled to belong to.\n\n        Returns:\n            The created `Post` instance.\n        \"\"\"", "\n", "id", "=", "twitter_dict", "[", "'id_str'", "]", "\n", "return", "Post", "(", "id", "=", "id", ",", "\n", "text", "=", "twitter_dict", "[", "'text'", "]", ",", "\n", "depth", "=", "post_depths", "[", "id", "]", ",", "\n", "platform", "=", "cls", ".", "Platform", ".", "twitter", ",", "\n", "has_media", "=", "'media'", "in", "twitter_dict", "[", "'entities'", "]", ",", "\n", "source_id", "=", "source_id", ",", "\n", "topic", "=", "topic", ",", "\n", "user_verified", "=", "twitter_dict", "[", "'user'", "]", "[", "'verified'", "]", ",", "\n", "followers_count", "=", "twitter_dict", "[", "'user'", "]", "[", "'followers_count'", "]", ",", "\n", "friends_count", "=", "twitter_dict", "[", "'user'", "]", "[", "'friends_count'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.dataset.Post.load_from_reddit_dict": [[202, 236], ["dataset.Post", "isinstance", "data.get", "data.get", "data.get", "data[].startswith"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "load_from_reddit_dict", "(", "cls", ",", "\n", "reddit_dict", ":", "Dict", ",", "\n", "post_depths", ":", "Dict", "[", "str", ",", "int", "]", ",", "\n", "source_id", ":", "Optional", "[", "str", "]", "=", "None", ")", "->", "'Post'", ":", "\n", "        ", "\"\"\"Creates a `Post` instance from a JSON dict of a Reddit post.\n\n        There are labels for some deleted Reddit posts (all classified as\n        \"comment\"). For these posts only the ID is available. The text is set\n        to be empty. See:\n        https://groups.google.com/forum/#!msg/rumoureval/-6XzTDhWirk/eSc31xFOFQAJ\n\n        Args:\n            reddit_dict: The JSON dict.\n            post_depths: A map that gives the depth of the post by it's ID.\n            source_id: The ID of the thread's source post. `None` if this post\n                is itself the source post.\n\n        Returns:\n            The created `Post` instance.\n        \"\"\"", "\n", "data", "=", "reddit_dict", "[", "'data'", "]", "\n", "if", "'children'", "in", "data", "and", "isinstance", "(", "data", "[", "'children'", "]", "[", "0", "]", ",", "dict", ")", ":", "\n", "            ", "data", "=", "data", "[", "'children'", "]", "[", "0", "]", "[", "'data'", "]", "\n", "\n", "", "id", "=", "data", "[", "'id'", "]", "\n", "return", "Post", "(", "id", "=", "id", ",", "\n", "text", "=", "data", ".", "get", "(", "'title'", ")", "or", "data", ".", "get", "(", "'body'", ")", "or", "''", ",", "\n", "depth", "=", "post_depths", "[", "id", "]", ",", "\n", "platform", "=", "cls", ".", "Platform", ".", "reddit", ",", "\n", "has_media", "=", "(", "'domain'", "in", "data", "\n", "and", "not", "data", "[", "'domain'", "]", ".", "startswith", "(", "'self.'", ")", ")", ",", "\n", "source_id", "=", "source_id", ",", "\n", "upvote_ratio", "=", "data", ".", "get", "(", "'upvote_ratio'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.dataset.SdqcInstance.__init__": [[438, 441], ["None"], "methods", ["None"], ["", "def", "__init__", "(", "self", ",", "post_id", ":", "str", ",", "label", ":", "Optional", "[", "Label", "]", "=", "None", ")", ":", "\n", "        ", "self", ".", "post_id", "=", "post_id", "\n", "self", ".", "label", "=", "label", "\n", "\n"]], "home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.dataset.SdqcInstance.__repr__": [[442, 444], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "'SDQC ({}, {})'", ".", "format", "(", "self", ".", "post_id", ",", "self", ".", "label", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.dataset.VerifInstance.__init__": [[491, 494], ["None"], "methods", ["None"], ["", "def", "__init__", "(", "self", ",", "post_id", ":", "str", ",", "label", ":", "Optional", "[", "Label", "]", "=", "None", ")", ":", "\n", "        ", "self", ".", "post_id", "=", "post_id", "\n", "self", ".", "label", "=", "label", "\n", "\n"]], "home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.dataset.VerifInstance.__str__": [[495, 497], ["print"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "print", "(", "'Verif ({}, {})'", ".", "format", "(", "self", ".", "post_id", ",", "self", ".", "label", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.dataset.check_for_required_external_data_files": [[54, 65], ["required_file.exists", "sys.exit"], "function", ["None"], ["def", "check_for_required_external_data_files", "(", ")", "->", "None", ":", "\n", "    ", "\"\"\"Checks whether all required external data files are present.\n\n    If not, will print a message to stderr and exit.\n    \"\"\"", "\n", "for", "required_file", "in", "[", "ELMO_WEIGHTS_FILE", ",", "ELMO_OPTIONS_FILE", ",", "\n", "TRAINING_DATA_ARCHIVE_FILE", ",", "TEST_DATA_ARCHIVE_FILE", ",", "\n", "EVALUATION_SCRIPT_FILE", "]", ":", "\n", "        ", "if", "not", "required_file", ".", "exists", "(", ")", ":", "\n", "            ", "exit", "(", "'Required file \"{}\" is not present. See the README on how to '", "\n", "'obtain it.'", ".", "format", "(", "required_file", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.dataset.load_posts": [[238, 419], ["print", "time.time", "zipfile.ZipFile", "dataset.load_posts.get_archive_directory_structure"], "function", ["None"], ["", "", "def", "load_posts", "(", ")", "->", "Dict", "[", "str", ",", "Post", "]", ":", "\n", "    ", "\"\"\"Loads all Twitter and Reddit posts into a dictionary.\n\n    Since the dataset is very small, we just load the whole dataset into RAM.\n\n    Returns:\n        A dictionary mapping post IDs to their respective posts.\n    \"\"\"", "\n", "\n", "def", "get_archive_directory_structure", "(", "archive", ":", "ZipFile", ")", "->", "Dict", ":", "\n", "        ", "\"\"\"Parses a ZipFile's list of files into a hierarchical representation.\n\n        We need to do this because ZipFile just gives us a list of all files in\n        contains and doesn't provide any methods to check which files lie in a\n        specific subdirectory.\n\n        Args:\n            archive: The archive to parse.\n\n        Returns:\n            A nested dictionary. Keys of this dictionary are either file names\n            which point to their full path in the archive or directory names\n            which again point to a nested dictionary that contains their\n            contents.\n\n        Example:\n            If the archive would contain the following files::\n\n                ['foo.txt',\n                 'bar/bar.log',\n                 'bar/baz.out',\n                 'bar/boogy/text.out']\n\n            This would be transformed into the following hierarchical form::\n\n                {\n                    'foo.txt': 'foo.txt',\n                    'bar': {\n                        'bar.log': 'bar/bar.log',\n                        'baz.out': 'bar/baz.out',\n                        'boogy': {\n                            'text.out': 'bar/boogy/text.out'\n                        }\n                    }\n                }\n        \"\"\"", "\n", "result", "=", "{", "}", "\n", "for", "file", "in", "archive", ".", "namelist", "(", ")", ":", "\n", "# Skip directories in archive.", "\n", "            ", "if", "file", ".", "endswith", "(", "'/'", ")", ":", "\n", "                ", "continue", "\n", "\n", "", "d", "=", "result", "\n", "path", "=", "file", ".", "split", "(", "'/'", ")", "[", "1", ":", "]", "# [1:] to skip top-level directory.", "\n", "for", "p", "in", "path", "[", ":", "-", "1", "]", ":", "# [:-1] to skip filename", "\n", "                ", "if", "p", "not", "in", "d", ":", "\n", "                    ", "d", "[", "p", "]", "=", "{", "}", "\n", "", "d", "=", "d", "[", "p", "]", "\n", "", "d", "[", "path", "[", "-", "1", "]", "]", "=", "file", "\n", "", "return", "result", "\n", "\n", "", "def", "calc_post_depths_from_thread_structure", "(", "thread_structure", ":", "Dict", ")", "->", "Dict", "[", "str", ",", "int", "]", ":", "\n", "        ", "\"\"\"Calculates the nested depth of each post in a thread.\n\n        We determine post depth from the provided `structure.json` files in the\n        dataset because this is easier than following the chain of a post's\n        parents to the source post of a thread.\n\n        Args:\n            thread_structure: The parsed JSON dict from one of the dataset's\n                `structure.json` files.\n\n        Returns:\n            A dictionary mapping post IDs to their nested depth. The source\n            post of a thread always has depth `0`, first level replies `1`, etc.\n\n        Example:\n            If the `thread_structure` would look like the following::\n\n                {\n                    'foo': {\n                        'bar': [],\n                        'baz': {\n                            'boogy': []\n                        },\n                        'qux': []\n                    }\n                }\n\n            The parsed post depths would be::\n\n                {\n                    'foo': 0,\n                    'bar': 1,\n                    'baz': 1,\n                    'boogy': 2,\n                    'qux': 1\n                }\n        \"\"\"", "\n", "post_depths", "=", "{", "}", "\n", "\n", "def", "walk", "(", "thread", ":", "Dict", ",", "depth", ":", "int", ")", "->", "None", ":", "\n", "            ", "for", "post_id", ",", "subthread", "in", "thread", ".", "items", "(", ")", ":", "\n", "                ", "post_depths", "[", "post_id", "]", "=", "depth", "\n", "if", "isinstance", "(", "subthread", ",", "Dict", ")", ":", "\n", "                    ", "walk", "(", "subthread", ",", "depth", "+", "1", ")", "\n", "\n", "", "", "", "walk", "(", "thread_structure", ",", "0", ")", "\n", "return", "post_depths", "\n", "\n", "", "print", "(", "'Loading posts...'", ")", "\n", "time_before", "=", "time", "(", ")", "\n", "\n", "training_data_archive", "=", "ZipFile", "(", "TRAINING_DATA_ARCHIVE_FILE", ")", "\n", "training_data_contents", "=", "get_archive_directory_structure", "(", "\n", "training_data_archive", ")", "\n", "twitter_english", "=", "training_data_contents", "[", "'twitter-english'", "]", "\n", "reddit_training_data", "=", "training_data_contents", "[", "'reddit-training-data'", "]", "\n", "reddit_dev_data", "=", "training_data_contents", "[", "'reddit-dev-data'", "]", "\n", "\n", "test_data_archive", "=", "ZipFile", "(", "TEST_DATA_ARCHIVE_FILE", ")", "\n", "test_data_contents", "=", "get_archive_directory_structure", "(", "test_data_archive", ")", "\n", "twitter_en_test_data", "=", "test_data_contents", "[", "'twitter-en-test-data'", "]", "\n", "reddit_test_data", "=", "test_data_contents", "[", "'reddit-test-data'", "]", "\n", "\n", "posts", ":", "Dict", "[", "str", ",", "Post", "]", "=", "{", "}", "\n", "\n", "# -- Load Twitter posts ----------------------------------------------------", "\n", "for", "archive", ",", "topics", "in", "[", "(", "training_data_archive", ",", "twitter_english", ".", "items", "(", ")", ")", ",", "\n", "(", "test_data_archive", ",", "twitter_en_test_data", ".", "items", "(", ")", ")", "]", ":", "\n", "        ", "for", "topic", ",", "threads", "in", "topics", ":", "\n", "            ", "for", "thread", "in", "threads", ".", "values", "(", ")", ":", "\n", "                ", "post_depths", "=", "calc_post_depths_from_thread_structure", "(", "\n", "json", ".", "loads", "(", "archive", ".", "read", "(", "thread", "[", "'structure.json'", "]", ")", ")", ")", "\n", "\n", "source_post", "=", "Post", ".", "load_from_twitter_dict", "(", "\n", "json", ".", "loads", "(", "archive", ".", "read", "(", "\n", "next", "(", "iter", "(", "thread", "[", "'source-tweet'", "]", ".", "values", "(", ")", ")", ")", ")", ")", ",", "\n", "post_depths", ",", "\n", "topic", "=", "topic", ")", "\n", "posts", "[", "source_post", ".", "id", "]", "=", "source_post", "\n", "\n", "for", "reply", "in", "thread", ".", "get", "(", "'replies'", ",", "{", "}", ")", ".", "values", "(", ")", ":", "\n", "                    ", "reply_post", "=", "Post", ".", "load_from_twitter_dict", "(", "\n", "json", ".", "loads", "(", "archive", ".", "read", "(", "reply", ")", ")", ",", "\n", "post_depths", ",", "\n", "source_id", "=", "source_post", ".", "id", ",", "\n", "topic", "=", "topic", ")", "\n", "posts", "[", "reply_post", ".", "id", "]", "=", "reply_post", "\n", "\n", "# -- Load Reddit posts. ----------------------------------------------------", "\n", "", "", "", "", "for", "archive", ",", "threads", "in", "[", "(", "training_data_archive", ",", "\n", "chain", "(", "reddit_training_data", ".", "values", "(", ")", ",", "\n", "reddit_dev_data", ".", "values", "(", ")", ")", ")", ",", "\n", "(", "test_data_archive", ",", "reddit_test_data", ".", "values", "(", ")", ")", "]", ":", "\n", "        ", "for", "thread", "in", "threads", ":", "\n", "            ", "post_depths", "=", "calc_post_depths_from_thread_structure", "(", "\n", "json", ".", "loads", "(", "archive", ".", "read", "(", "thread", "[", "'structure.json'", "]", ")", ")", ")", "\n", "\n", "source_post", "=", "Post", ".", "load_from_reddit_dict", "(", "\n", "json", ".", "loads", "(", "archive", ".", "read", "(", "\n", "next", "(", "iter", "(", "thread", "[", "'source-tweet'", "]", ".", "values", "(", ")", ")", ")", ")", ")", ",", "\n", "post_depths", ")", "\n", "posts", "[", "source_post", ".", "id", "]", "=", "source_post", "\n", "\n", "for", "reply", "in", "thread", ".", "get", "(", "'replies'", ",", "{", "}", ")", ".", "values", "(", ")", ":", "\n", "                ", "reply_post", "=", "Post", ".", "load_from_reddit_dict", "(", "\n", "json", ".", "loads", "(", "archive", ".", "read", "(", "reply", ")", ")", ",", "\n", "post_depths", ",", "\n", "source_id", "=", "source_post", ".", "id", ")", "\n", "posts", "[", "reply_post", ".", "id", "]", "=", "reply_post", "\n", "\n", "", "", "", "print", "(", "'  Number of posts: {:d} (Reddit={:d}, Twitter={:d})'", ".", "format", "(", "\n", "len", "(", "posts", ")", ",", "\n", "sum", "(", "1", "for", "p", "in", "posts", ".", "values", "(", ")", "if", "p", ".", "platform", "==", "Post", ".", "Platform", ".", "reddit", ")", ",", "\n", "sum", "(", "1", "for", "p", "in", "posts", ".", "values", "(", ")", "if", "p", ".", "platform", "==", "Post", ".", "Platform", ".", "twitter", ")", ")", ")", "\n", "time_after", "=", "time", "(", ")", "\n", "print", "(", "'  Took {:.2f}s.'", ".", "format", "(", "time_after", "-", "time_before", ")", ")", "\n", "\n", "return", "posts", "\n", "\n"]], "home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.dataset.load_sdcq_instances": [[446, 474], ["zipfile.ZipFile", "dataset.load_sdcq_instances.load_from_json_dict"], "function", ["None"], ["", "", "def", "load_sdcq_instances", "(", ")", "->", "(", "List", "[", "SdqcInstance", "]", ",", "\n", "List", "[", "SdqcInstance", "]", ",", "\n", "Optional", "[", "List", "[", "SdqcInstance", "]", "]", ")", ":", "\n", "    ", "\"\"\"Load SDQC (RumorEval Task A) training, dev, and test datasets.\n\n    Returns:\n        A tuple containing lists of SDQC instances. The first element is the\n        training dataset, the second the dev, and the third the test, if it is\n        available, otherwise `None`.\n    \"\"\"", "\n", "\n", "def", "load_from_json_dict", "(", "json_dict", ":", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "str", "]", "]", ")", "->", "List", "[", "SdqcInstance", "]", ":", "\n", "        ", "return", "[", "SdqcInstance", "(", "post_id", ",", "SdqcInstance", ".", "Label", "[", "label", "]", ")", "\n", "for", "post_id", ",", "label", "in", "json_dict", "[", "'subtaskaenglish'", "]", ".", "items", "(", ")", "]", "\n", "\n", "", "training_data_archive", "=", "ZipFile", "(", "TRAINING_DATA_ARCHIVE_FILE", ")", "\n", "train", "=", "load_from_json_dict", "(", "json", ".", "loads", "(", "training_data_archive", ".", "read", "(", "\n", "'rumoureval-2019-training-data/train-key.json'", ")", ")", ")", "\n", "dev", "=", "load_from_json_dict", "(", "json", ".", "loads", "(", "training_data_archive", ".", "read", "(", "\n", "'rumoureval-2019-training-data/dev-key.json'", ")", ")", ")", "\n", "test", "=", "None", "\n", "\n", "if", "EVALUATION_DATA_FILE", ".", "exists", "(", ")", ":", "\n", "        ", "with", "EVALUATION_DATA_FILE", ".", "open", "(", "'rb'", ")", "as", "fin", ":", "\n", "            ", "test", "=", "load_from_json_dict", "(", "json", ".", "loads", "(", "fin", ".", "read", "(", ")", ")", ")", "\n", "\n", "", "", "return", "train", ",", "dev", ",", "test", "\n", "\n"]], "home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.dataset.load_verif_instances": [[499, 527], ["zipfile.ZipFile", "dataset.load_sdcq_instances.load_from_json_dict"], "function", ["None"], ["", "", "def", "load_verif_instances", "(", ")", "->", "(", "List", "[", "VerifInstance", "]", ",", "\n", "List", "[", "VerifInstance", "]", ",", "\n", "Optional", "[", "List", "[", "VerifInstance", "]", "]", ")", ":", "\n", "    ", "\"\"\"Load Verification (RumorEval Task B) training, dev, and test datasets.\n\n    Returns:\n        A tuple containing lists of Verification instances. The first element is\n        the training dataset, the second the dev, and the third the test, if it\n        is available, otherwise `None`.\n    \"\"\"", "\n", "\n", "def", "load_from_json_dict", "(", "json_dict", ":", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "str", "]", "]", ")", "->", "List", "[", "VerifInstance", "]", ":", "\n", "        ", "return", "[", "VerifInstance", "(", "post_id", ",", "VerifInstance", ".", "Label", "[", "label", "]", ")", "\n", "for", "post_id", ",", "label", "in", "json_dict", "[", "'subtaskbenglish'", "]", ".", "items", "(", ")", "]", "\n", "\n", "", "training_data_archive", "=", "ZipFile", "(", "TRAINING_DATA_ARCHIVE_FILE", ")", "\n", "train", "=", "load_from_json_dict", "(", "json", ".", "loads", "(", "training_data_archive", ".", "read", "(", "\n", "'rumoureval-2019-training-data/train-key.json'", ")", ")", ")", "\n", "dev", "=", "load_from_json_dict", "(", "json", ".", "loads", "(", "training_data_archive", ".", "read", "(", "\n", "'rumoureval-2019-training-data/dev-key.json'", ")", ")", ")", "\n", "test", "=", "None", "\n", "\n", "if", "EVALUATION_DATA_FILE", ".", "exists", "(", ")", ":", "\n", "        ", "with", "EVALUATION_DATA_FILE", ".", "open", "(", "'rb'", ")", "as", "fin", ":", "\n", "            ", "test", "=", "load_from_json_dict", "(", "json", ".", "loads", "(", "fin", ".", "read", "(", ")", ")", ")", "\n", "\n", "", "", "return", "train", ",", "dev", ",", "test", "\n", "", ""]], "home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.sdqc.Sdqc.__init__": [[67, 76], ["None"], "methods", ["None"], ["", "", "def", "__init__", "(", "self", ",", "\n", "posts", ":", "Dict", "[", "str", ",", "Post", "]", ",", "\n", "post_embeddings", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", "hparams", ":", "'Sdqc.Hyperparameters'", ",", "\n", "device", ":", "torch", ".", "device", ")", ":", "\n", "        ", "self", ".", "_posts", "=", "posts", "\n", "self", ".", "_post_embeddings", "=", "post_embeddings", "\n", "self", ".", "_hparams", "=", "hparams", "\n", "self", ".", "_device", "=", "device", "\n", "\n"]], "home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.sdqc.Sdqc.build_datasets": [[111, 164], ["print", "sdqc.Sdqc.Dataset", "sdqc.Sdqc.Dataset", "sdqc.Sdqc.Dataset", "sdqc.Sdqc.calc_stats_for_aux_feature", "len", "len", "len", "dataset.min_max_scale_aux_feature", "dataset.standard_scale_aux_feature", "ValueError"], "methods", ["home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.util.DatasetHelper.calc_stats_for_aux_feature", "home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.util.DatasetHelper.min_max_scale_aux_feature", "home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.util.DatasetHelper.standard_scale_aux_feature"], ["", "", "", "def", "build_datasets", "(", "self", ",", "\n", "train_instances", ":", "Iterable", "[", "SdqcInstance", "]", ",", "\n", "dev_instances", ":", "Optional", "[", "Iterable", "[", "SdqcInstance", "]", "]", ",", "\n", "test_instances", ":", "Optional", "[", "Iterable", "[", "SdqcInstance", "]", "]", ")", "->", "(", "'Sdqc.Dataset'", ",", "\n", "Optional", "[", "'Sdqc.Dataset'", "]", ",", "\n", "Optional", "[", "'Sdqc.Dataset'", "]", ")", ":", "\n", "        ", "print", "(", "'Number of instances: train={:d}, dev={:d}, test={:d}'", "\n", ".", "format", "(", "len", "(", "train_instances", ")", ",", "\n", "len", "(", "dev_instances", "or", "[", "]", ")", ",", "\n", "len", "(", "test_instances", "or", "[", "]", ")", ")", ")", "\n", "\n", "train_dataset", "=", "self", ".", "Dataset", "(", "\n", "train_instances", ",", "self", ".", "_posts", ",", "self", ".", "_post_embeddings", ",", "self", ".", "_hparams", ",", "\n", "self", ".", "_device", ")", "\n", "\n", "dev_dataset", "=", "None", "\n", "if", "dev_instances", ":", "\n", "            ", "dev_dataset", "=", "self", ".", "Dataset", "(", "\n", "dev_instances", ",", "self", ".", "_posts", ",", "self", ".", "_post_embeddings", ",", "\n", "self", ".", "_hparams", ",", "self", ".", "_device", ")", "\n", "\n", "", "test_dataset", "=", "None", "\n", "if", "test_instances", ":", "\n", "            ", "test_dataset", "=", "self", ".", "Dataset", "(", "\n", "test_instances", ",", "self", ".", "_posts", ",", "self", ".", "_post_embeddings", ",", "\n", "self", ".", "_hparams", ",", "self", ".", "_device", ")", "\n", "\n", "", "def", "filter_func", "(", "post_id", ":", "str", ")", "->", "bool", ":", "\n", "            ", "return", "self", ".", "_posts", "[", "post_id", "]", ".", "platform", "==", "Post", ".", "Platform", ".", "twitter", "\n", "\n", "", "for", "index", "in", "self", ".", "_hparams", ".", "input_aux_scaling_features", ":", "\n", "            ", "min", ",", "max", ",", "mean", ",", "std", "=", "train_dataset", ".", "calc_stats_for_aux_feature", "(", "index", ",", "filter_func", ")", "\n", "\n", "for", "dataset", "in", "(", "train_dataset", ",", "dev_dataset", ",", "test_dataset", ")", ":", "\n", "                ", "if", "not", "dataset", ":", "\n", "                    ", "continue", "\n", "\n", "", "if", "self", ".", "_hparams", ".", "input_aux_scaling_mode", "==", "ScalingMode", ".", "none", ":", "\n", "                    ", "pass", "\n", "", "elif", "(", "self", ".", "_hparams", ".", "input_aux_scaling_mode", "\n", "==", "ScalingMode", ".", "min_max", ")", ":", "\n", "                    ", "dataset", ".", "min_max_scale_aux_feature", "(", "\n", "index", ",", "min", ",", "max", ",", "filter_func", ")", "\n", "", "elif", "(", "self", ".", "_hparams", ".", "input_aux_scaling_mode", "\n", "==", "ScalingMode", ".", "standard", ")", ":", "\n", "                    ", "dataset", ".", "standard_scale_aux_feature", "(", "\n", "index", ",", "mean", ",", "std", ",", "filter_func", ")", "\n", "", "else", ":", "\n", "                    ", "raise", "ValueError", "(", "'Unimplemented enum variant.'", ")", "\n", "\n", "", "", "", "return", "train_dataset", ",", "dev_dataset", ",", "test_dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.sdqc.Sdqc.train": [[255, 327], ["sdqc.Sdqc.Model().to", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.optim.Adam", "torch.optim.Adam", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "range", "sdqc.Sdqc.parameters", "enumerate", "sdqc.Sdqc.Model", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "tqdm.tqdm.tqdm", "torch.optim.Adam.zero_grad", "sdqc.Sdqc.train", "sdqc.Sdqc.", "torch.nn.CrossEntropyLoss.", "nn.CrossEntropyLoss.backward", "torch.optim.Adam.step", "losses.append", "numpy.concatenate.append", "numpy.concatenate.append", "tqdm.tqdm.tqdm.close", "numpy.concatenate", "numpy.concatenate", "numpy.mean", "sklearn.metrics.accuracy_score", "sklearn.metrics.f1_score", "print", "sdqc.Sdqc.eval", "print", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "nn.CrossEntropyLoss.item", "batch[].data.cpu().numpy", "torch.argmax.data.cpu().numpy", "torch.argmax.data.cpu().numpy", "tqdm.tqdm.tqdm.set_postfix", "tqdm.tqdm.tqdm.update", "len", "len", "batch[].data.cpu", "torch.argmax.data.cpu", "torch.argmax.data.cpu", "str", "nn.CrossEntropyLoss.item"], "methods", ["home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.sdqc.Sdqc.train", "home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.sdqc.Sdqc.eval"], ["", "", "def", "train", "(", "self", ",", "\n", "train_dataset", ":", "'Sdqc.Dataset'", ",", "\n", "dev_dataset", ":", "Optional", "[", "'Sdqc.Dataset'", "]", "=", "None", ",", "\n", "print_progress", ":", "bool", "=", "False", ")", "->", "'Sdqc.Model'", ":", "\n", "        ", "model", "=", "self", ".", "Model", "(", "self", ".", "_hparams", ")", ".", "to", "(", "self", ".", "_device", ")", "\n", "\n", "criterion", "=", "nn", ".", "CrossEntropyLoss", "(", "\n", "weight", "=", "torch", ".", "tensor", "(", "self", ".", "_hparams", ".", "class_weights", ",", "\n", "dtype", "=", "torch", ".", "float32", ",", "\n", "device", "=", "self", ".", "_device", ")", ")", "\n", "optimizer", "=", "optim", ".", "Adam", "(", "model", ".", "parameters", "(", ")", ",", "\n", "lr", "=", "self", ".", "_hparams", ".", "learning_rate", ",", "\n", "weight_decay", "=", "self", ".", "_hparams", ".", "weight_decay", ")", "\n", "\n", "train_loader", "=", "DataLoader", "(", "\n", "train_dataset", ",", "batch_size", "=", "self", ".", "_hparams", ".", "batch_size", ",", "shuffle", "=", "True", ")", "\n", "\n", "for", "epoch_no", "in", "range", "(", "1", ",", "self", ".", "_hparams", ".", "num_epochs", "+", "1", ")", ":", "\n", "            ", "losses", ",", "labels", ",", "predictions", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "\n", "progress_bar", "=", "None", "\n", "if", "print_progress", ":", "\n", "                ", "progress_bar", "=", "tqdm", "(", "total", "=", "(", "len", "(", "train_loader", ")", ")", ",", "\n", "unit", "=", "'batch'", ",", "\n", "desc", "=", "'Epoch: {:{}d}/{:d}'", ".", "format", "(", "\n", "epoch_no", ",", "\n", "len", "(", "str", "(", "self", ".", "_hparams", ".", "num_epochs", ")", ")", ",", "\n", "self", ".", "_hparams", ".", "num_epochs", ")", ")", "\n", "\n", "", "for", "batch_no", ",", "batch", "in", "enumerate", "(", "train_loader", ")", ":", "\n", "                ", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "model", ".", "train", "(", ")", "\n", "batch_logits", "=", "model", "(", "batch", "[", "'emb'", "]", ",", "batch", "[", "'features'", "]", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                    ", "batch_prediction", "=", "torch", ".", "argmax", "(", "batch_logits", ",", "dim", "=", "1", ")", "\n", "\n", "", "loss", "=", "criterion", "(", "batch_logits", ",", "batch", "[", "'label'", "]", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "losses", ".", "append", "(", "loss", ".", "item", "(", ")", ")", "\n", "labels", ".", "append", "(", "batch", "[", "'label'", "]", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "predictions", ".", "append", "(", "batch_prediction", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "\n", "if", "progress_bar", ":", "\n", "                    ", "progress_bar", ".", "set_postfix", "(", "{", "\n", "'loss'", ":", "'{:.2e}'", ".", "format", "(", "loss", ".", "item", "(", ")", ")", ",", "\n", "}", ")", "\n", "progress_bar", ".", "update", "(", "1", ")", "\n", "\n", "", "", "if", "progress_bar", ":", "\n", "                ", "progress_bar", ".", "close", "(", ")", "\n", "\n", "labels", "=", "np", ".", "concatenate", "(", "labels", ")", "\n", "predictions", "=", "np", ".", "concatenate", "(", "predictions", ")", "\n", "\n", "epoch_loss", "=", "np", ".", "mean", "(", "losses", ")", "\n", "epoch_acc", "=", "accuracy_score", "(", "labels", ",", "predictions", ")", "\n", "epoch_f1", "=", "f1_score", "(", "labels", ",", "predictions", ",", "average", "=", "'macro'", ")", "\n", "\n", "print", "(", "'  Loss={:.2e}  Accuracy={:.2%}  F1-score={:.2%}'", "\n", ".", "format", "(", "epoch_loss", ",", "epoch_acc", ",", "epoch_f1", ")", ")", "\n", "\n", "", "if", "print_progress", "and", "dev_dataset", "and", "(", "epoch_no", "==", "self", ".", "_hparams", ".", "num_epochs", "\n", "or", "not", "epoch_no", "%", "EVAL_DEV_EVERY_N_EPOCH", ")", ":", "\n", "                ", "dev_acc", ",", "dev_f1", ",", "_", "=", "self", ".", "eval", "(", "model", ",", "dev_dataset", ")", "\n", "print", "(", "'  Validation:    Accuracy={:.2%}  F1-score={:.2%}'", "\n", ".", "format", "(", "dev_acc", ",", "dev_f1", ")", ")", "\n", "\n", "", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.sdqc.Sdqc.eval": [[328, 354], ["numpy.concatenate", "numpy.concatenate", "sklearn.metrics.accuracy_score", "sklearn.metrics.f1_score", "sklearn.metrics.classification_report", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "model.eval", "model", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "numpy.concatenate.append", "numpy.concatenate.append", "range", "batch[].data.cpu().numpy", "torch.argmax.data.cpu().numpy", "torch.argmax.data.cpu().numpy", "len", "batch[].data.cpu", "torch.argmax.data.cpu", "torch.argmax.data.cpu"], "methods", ["home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.sdqc.Sdqc.eval"], ["", "def", "eval", "(", "self", ",", "model", ":", "'Sdqc.Model'", ",", "dataset", ":", "'Sdqc.Dataset'", ")", "->", "(", "float", ",", "float", ",", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "float", "]", "]", ")", ":", "\n", "        ", "labels", ",", "predictions", "=", "[", "]", ",", "[", "]", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "data_loader", "=", "DataLoader", "(", "\n", "dataset", ",", "batch_size", "=", "self", ".", "_hparams", ".", "batch_size", ")", "\n", "for", "batch", "in", "data_loader", ":", "\n", "                ", "model", ".", "eval", "(", ")", "\n", "batch_logits", "=", "model", "(", "batch", "[", "'emb'", "]", ",", "batch", "[", "'features'", "]", ")", "\n", "batch_prediction", "=", "torch", ".", "argmax", "(", "batch_logits", ",", "dim", "=", "1", ")", "\n", "\n", "labels", ".", "append", "(", "batch", "[", "'label'", "]", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "predictions", ".", "append", "(", "batch_prediction", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "\n", "", "", "labels", "=", "np", ".", "concatenate", "(", "labels", ")", "\n", "predictions", "=", "np", ".", "concatenate", "(", "predictions", ")", "\n", "\n", "acc", "=", "accuracy_score", "(", "labels", ",", "predictions", ")", "\n", "f1", "=", "f1_score", "(", "labels", ",", "predictions", ",", "average", "=", "'macro'", ")", "\n", "report", "=", "classification_report", "(", "\n", "labels", ",", "predictions", ",", "output_dict", "=", "True", ",", "\n", "labels", "=", "range", "(", "len", "(", "SdqcInstance", ".", "Label", ")", ")", ",", "\n", "target_names", "=", "[", "label", ".", "name", "for", "label", "in", "SdqcInstance", ".", "Label", "]", ")", "\n", "\n", "return", "acc", ",", "f1", ",", "report", "\n", "\n"]], "home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.sdqc.Sdqc.predict": [[355, 379], ["sdqc.Sdqc.Dataset", "src.dataset.SdqcInstance", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "model.eval", "model", "torch.softmax", "torch.softmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "zip", "src.dataset.SdqcInstance.Label", "dict", "prediction.item", "zip", "probs.tolist"], "methods", ["home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.sdqc.Sdqc.eval"], ["", "def", "predict", "(", "self", ",", "model", ":", "'Sdqc.Model'", ",", "post_ids", ":", "Iterable", "[", "str", "]", ")", "->", "Dict", "[", "str", ",", "Tuple", "[", "SdqcInstance", ".", "Label", ",", "\n", "Dict", "[", "SdqcInstance", ".", "Label", ",", "float", "]", "]", "]", ":", "\n", "        ", "instances", "=", "[", "SdqcInstance", "(", "post_id", ")", "for", "post_id", "in", "post_ids", "]", "\n", "dataset", "=", "self", ".", "Dataset", "(", "instances", ",", "self", ".", "_posts", ",", "self", ".", "_post_embeddings", ",", "\n", "self", ".", "_hparams", ",", "self", ".", "_device", ")", "\n", "\n", "results", "=", "{", "}", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "data_loader", "=", "DataLoader", "(", "dataset", ",", "\n", "batch_size", "=", "self", ".", "_hparams", ".", "batch_size", ")", "\n", "for", "batch", "in", "data_loader", ":", "\n", "                ", "model", ".", "eval", "(", ")", "\n", "batch_logits", "=", "model", "(", "batch", "[", "'emb'", "]", ",", "batch", "[", "'features'", "]", ")", "\n", "batch_probs", "=", "F", ".", "softmax", "(", "batch_logits", ",", "dim", "=", "1", ")", "\n", "batch_prediction", "=", "torch", ".", "argmax", "(", "batch_logits", ",", "dim", "=", "1", ")", "\n", "\n", "for", "post_id", ",", "prediction", ",", "probs", "in", "zip", "(", "\n", "batch", "[", "'post_id'", "]", ",", "batch_prediction", ",", "batch_probs", ")", ":", "\n", "                    ", "results", "[", "post_id", "]", "=", "(", "SdqcInstance", ".", "Label", "(", "prediction", ".", "item", "(", ")", ")", ",", "\n", "dict", "(", "zip", "(", "SdqcInstance", ".", "Label", ",", "probs", ".", "tolist", "(", ")", ")", ")", ")", "\n", "\n", "", "", "", "return", "results", "\n", "", "", ""]], "home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.util.DatasetHelper.__init__": [[44, 48], ["torch.utils.data.Dataset.__init__"], "methods", ["home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.util.DatasetHelper.__init__"], ["    ", "def", "__init__", "(", "self", ",", "post_embeddings", ":", "Dict", "[", "str", ",", "torch", ".", "tensor", "]", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_dataset", "=", "[", "]", "\n", "self", ".", "_post_embeddings", "=", "post_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.util.DatasetHelper.calc_shared_features": [[49, 70], ["numpy.array", "util.DatasetHelper._post_embeddings[].mean", "util.DatasetHelper._post_embeddings[].mean", "torch.cosine_similarity().cpu().numpy", "torch.cosine_similarity().cpu().numpy", "torch.cosine_similarity().cpu", "torch.cosine_similarity().cpu", "torch.cosine_similarity", "torch.cosine_similarity"], "methods", ["None"], ["", "def", "calc_shared_features", "(", "self", ",", "post", ":", "Post", ")", "->", "(", "np", ".", "ndarray", ",", "np", ".", "ndarray", ",", "np", ".", "ndarray", ")", ":", "\n", "        ", "post_platform", "=", "[", "post", ".", "platform", "==", "Post", ".", "Platform", ".", "twitter", ",", "\n", "post", ".", "platform", "==", "Post", ".", "Platform", ".", "reddit", "]", "\n", "\n", "post_author", "=", "[", "0", ",", "0", ",", "0", ",", "0", ",", "0", "]", "\n", "if", "post", ".", "platform", "==", "Post", ".", "Platform", ".", "twitter", ":", "\n", "            ", "post_author", "=", "[", "post", ".", "user_verified", ",", "\n", "not", "post", ".", "user_verified", ",", "\n", "post", ".", "followers_count", ",", "\n", "post", ".", "friends_count", ",", "\n", "post", ".", "followers_count", "/", "(", "post", ".", "friends_count", "+", "1e-8", ")", "]", "\n", "\n", "", "post_similarity_to_source", "=", "np", ".", "array", "(", "1", ")", "\n", "if", "not", "post", ".", "has_source_depth", ":", "\n", "            ", "post_emb_mean", "=", "self", ".", "_post_embeddings", "[", "post", ".", "id", "]", ".", "mean", "(", "dim", "=", "1", ")", "\n", "source_emb_mean", "=", "self", ".", "_post_embeddings", "[", "post", ".", "source_id", "]", ".", "mean", "(", "dim", "=", "1", ")", "\n", "post_similarity_to_source", "=", "F", ".", "cosine_similarity", "(", "\n", "post_emb_mean", ",", "source_emb_mean", ",", "dim", "=", "0", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "", "return", "post_platform", ",", "post_author", ",", "post_similarity_to_source", "\n", "\n"]], "home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.util.DatasetHelper.__len__": [[71, 73], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "len", "(", "self", ".", "_dataset", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.util.DatasetHelper.__getitem__": [[74, 76], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ":", "int", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "return", "self", ".", "_dataset", "[", "index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.util.DatasetHelper.calc_stats_for_aux_feature": [[77, 93], ["numpy.array", "numpy.array.min", "numpy.array.max", "numpy.array.mean", "numpy.array.std", "[].item", "util.DatasetHelper.calc_stats_for_aux_feature.filter_func"], "methods", ["None"], ["", "def", "calc_stats_for_aux_feature", "(", "self", ",", "\n", "index", ":", "int", ",", "\n", "filter_func", ":", "Optional", "[", "\n", "Callable", "[", "[", "str", "]", ",", "bool", "]", "]", "=", "None", ")", "->", "(", "float", ",", "float", ",", "float", ",", "float", ")", ":", "\n", "        ", "if", "not", "filter_func", ":", "\n", "            ", "def", "filter_func", "(", "_post_id", ":", "str", ")", "->", "bool", ":", "\n", "                ", "return", "True", "\n", "\n", "", "", "feature_values", "=", "np", ".", "array", "(", "[", "post", "[", "'features'", "]", "[", "index", "]", ".", "item", "(", ")", "\n", "for", "post", "in", "self", ".", "_dataset", "\n", "if", "filter_func", "(", "post", "[", "'post_id'", "]", ")", "]", ")", "\n", "return", "(", "feature_values", ".", "min", "(", ")", ",", "\n", "feature_values", ".", "max", "(", ")", ",", "\n", "feature_values", ".", "mean", "(", ")", ",", "\n", "feature_values", ".", "std", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.util.DatasetHelper.min_max_scale_aux_feature": [[94, 109], ["util.DatasetHelper.calc_stats_for_aux_feature.filter_func"], "methods", ["None"], ["", "def", "min_max_scale_aux_feature", "(", "self", ",", "\n", "index", ":", "int", ",", "\n", "min", ":", "float", ",", "\n", "max", ":", "float", ",", "\n", "filter_func", ":", "Optional", "[", "\n", "Callable", "[", "[", "str", "]", ",", "bool", "]", "]", "=", "None", ")", "->", "None", ":", "\n", "        ", "if", "not", "filter_func", ":", "\n", "            ", "def", "filter_func", "(", "_post_id", ":", "str", ")", "->", "bool", ":", "\n", "                ", "return", "True", "\n", "\n", "", "", "for", "post", "in", "self", ".", "_dataset", ":", "\n", "            ", "if", "filter_func", "(", "post", "[", "'post_id'", "]", ")", ":", "\n", "                ", "value", "=", "post", "[", "'features'", "]", "[", "index", "]", "\n", "post", "[", "'features'", "]", "[", "index", "]", "=", "(", "value", "-", "min", ")", "/", "(", "max", "-", "min", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.util.DatasetHelper.standard_scale_aux_feature": [[110, 125], ["util.DatasetHelper.calc_stats_for_aux_feature.filter_func"], "methods", ["None"], ["", "", "", "def", "standard_scale_aux_feature", "(", "self", ",", "\n", "index", ":", "int", ",", "\n", "mean", ":", "float", ",", "\n", "std", ":", "float", ",", "\n", "filter_func", ":", "Optional", "[", "\n", "Callable", "[", "[", "str", "]", ",", "bool", "]", "]", "=", "None", ")", "->", "None", ":", "\n", "        ", "if", "not", "filter_func", ":", "\n", "            ", "def", "filter_func", "(", "_post_id", ":", "str", ")", "->", "bool", ":", "\n", "                ", "return", "True", "\n", "\n", "", "", "for", "post", "in", "self", ".", "_dataset", ":", "\n", "            ", "if", "filter_func", "(", "post", "[", "'post_id'", "]", ")", ":", "\n", "                ", "value", "=", "post", "[", "'features'", "]", "[", "index", "]", "\n", "post", "[", "'features'", "]", "[", "index", "]", "=", "(", "value", "-", "mean", ")", "/", "std", "\n", "\n"]], "home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.util.calculate_post_elmo_embeddings": [[127, 205], ["print", "time.time", "allennlp.modules.elmo.Elmo().to", "Elmo().to.eval", "enumerate", "time.time", "print", "posts.values", "batch_ids.append", "batch_texts.append", "allennlp.modules.elmo.Elmo", "allennlp.modules.elmo.batch_to_ids().to", "batch_embeddings.split.split", "zip", "range", "post_embedding.squeeze_", "post_embedding.transpose_", "len", "allennlp.modules.elmo.batch_to_ids", "range", "Elmo().to."], "function", ["home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.sdqc.Sdqc.eval"], ["", "", "", "", "def", "calculate_post_elmo_embeddings", "(", "posts", ":", "Dict", "[", "str", ",", "Post", "]", ",", "\n", "max_sentence_length", ":", "int", ",", "\n", "batch_size", ":", "int", ",", "\n", "scalar_mix_parameters", ":", "List", "[", "float", "]", ",", "\n", "device", ":", "torch", ".", "device", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "    ", "\"\"\"Calculate ELMo embeddings of all posts in the dataset.\n\n    Calculating these embeddings one time before training the actual models\n    allows for extremely fast training later. The downsides are that we can't\n    propagate gradients through the embeddings, but fine-tuning these would\n    probably lead to be overfitting, since our dataset is very small.\n    Additionally, we also can't learn the scalar_mix_parameters, but since\n    training is so much faster, adjusting these by hand should be sufficient.\n\n    Since we are going to load the entire dataset into GPU memory later anyways,\n    we keep the embeddings in GPU memory here already.\n\n    Args:\n        posts: A dictionary mapping post IDs to their respective posts. Load\n            this with `src.dataset.load_posts()`.\n        max_sentence_length: Number of tokens after which sentences will be\n            truncated.\n        batch_size: Batch size for calculating the ELMo embeddings.\n        scalar_mix_parameters: Parameters for mixing the different ELMo layers.\n            See the paper for details on this.\n        device: Device to execute on.\n\n    Returns:\n        A dictionary mapping post IDs to their respective ELMo embedding in a\n        PyTorch tensor. Each tensor will have shape\n        `(num_elmo_dimensions, max_sentence_length)`.\n    \"\"\"", "\n", "\n", "print", "(", "'Calculating post embeddings...'", ")", "\n", "time_before", "=", "time", "(", ")", "\n", "\n", "elmo", "=", "Elmo", "(", "ELMO_OPTIONS_FILE", ",", "\n", "ELMO_WEIGHTS_FILE", ",", "\n", "num_output_representations", "=", "1", ",", "\n", "dropout", "=", "0", ",", "\n", "requires_grad", "=", "False", ",", "\n", "do_layer_norm", "=", "False", ",", "\n", "scalar_mix_parameters", "=", "scalar_mix_parameters", ")", ".", "to", "(", "device", ")", "\n", "elmo", ".", "eval", "(", ")", "\n", "\n", "post_embeddings", "=", "{", "}", "\n", "batch_ids", "=", "[", "]", "\n", "# Add a dummy sentence with max_sentence_length to each batch to enforce", "\n", "# that each batch of embeddings has the same shape. `batch_to_id()` and", "\n", "# `elmo()` take care of zero padding shorter sentences for us.", "\n", "batch_texts", "=", "[", "[", "''", "for", "_", "in", "range", "(", "max_sentence_length", ")", "]", "]", "\n", "for", "i", ",", "post", "in", "enumerate", "(", "posts", ".", "values", "(", ")", ")", ":", "\n", "        ", "batch_ids", ".", "append", "(", "post", ".", "id", ")", "\n", "batch_texts", ".", "append", "(", "post", ".", "text", "[", ":", "max_sentence_length", "]", ")", "\n", "\n", "if", "not", "i", "%", "batch_size", "or", "i", "==", "len", "(", "posts", ")", "-", "1", ":", "\n", "            ", "batch_character_ids", "=", "batch_to_ids", "(", "batch_texts", ")", ".", "to", "(", "device", ")", "\n", "batch_texts", "=", "[", "[", "''", "for", "_", "in", "range", "(", "max_sentence_length", ")", "]", "]", "\n", "\n", "# - [0] to select first output representation (there is only one", "\n", "#   because of `num_output_representations=1` at `elmo` creation.", "\n", "# - [1:] to ignore dummy sentence added at the start.", "\n", "batch_embeddings", "=", "elmo", "(", "batch_character_ids", ")", "[", "'elmo_representations'", "]", "[", "0", "]", "[", "1", ":", "]", "\n", "batch_embeddings", "=", "batch_embeddings", ".", "split", "(", "split_size", "=", "1", ",", "dim", "=", "0", ")", "\n", "del", "batch_character_ids", "# Free up memory sooner.", "\n", "\n", "for", "post_id", ",", "post_embedding", "in", "zip", "(", "batch_ids", ",", "batch_embeddings", ")", ":", "\n", "                ", "post_embedding", ".", "squeeze_", "(", "dim", "=", "0", ")", "\n", "post_embedding", ".", "transpose_", "(", "0", ",", "1", ")", "\n", "post_embeddings", "[", "post_id", "]", "=", "post_embedding", "\n", "", "batch_ids", "=", "[", "]", "\n", "\n", "", "", "time_after", "=", "time", "(", ")", "\n", "print", "(", "'  Took {:.2f}s.'", ".", "format", "(", "time_after", "-", "time_before", ")", ")", "\n", "\n", "return", "post_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.util.generate_folds_for_k_fold_cross_validation": [[207, 236], ["collections.defaultdict", "posts.values", "list", "random.shuffle", "posts_per_discriminator[].add", "list.values", "set", "enumerate", "folds[].update", "range", "ValueError", "len", "len"], "function", ["None"], ["", "def", "generate_folds_for_k_fold_cross_validation", "(", "posts", ":", "Dict", "[", "str", ",", "Post", "]", ",", "\n", "num_folds", ":", "int", ")", "->", "List", "[", "Set", "[", "str", "]", "]", ":", "\n", "    ", "posts_per_discriminator", "=", "defaultdict", "(", "set", ")", "\n", "for", "post", "in", "posts", ".", "values", "(", ")", ":", "\n", "        ", "if", "post", ".", "platform", "==", "Post", ".", "Platform", ".", "twitter", ":", "\n", "            ", "discriminator", "=", "post", ".", "topic", "\n", "", "elif", "post", ".", "platform", "==", "Post", ".", "Platform", ".", "reddit", ":", "\n", "            ", "discriminator", "=", "post", ".", "source_id", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "'Unimplemented enum variant.'", ")", "\n", "", "posts_per_discriminator", "[", "discriminator", "]", ".", "add", "(", "post", ".", "id", ")", "\n", "", "posts_per_discriminator", "=", "list", "(", "posts_per_discriminator", ".", "values", "(", ")", ")", "\n", "shuffle", "(", "posts_per_discriminator", ")", "\n", "\n", "folds", "=", "[", "set", "(", ")", "for", "_", "in", "range", "(", "num_folds", ")", "]", "\n", "for", "post_ids", "in", "posts_per_discriminator", ":", "\n", "# Find fold with fewest elements", "\n", "        ", "index", "=", "None", "\n", "num_elements", "=", "maxsize", "\n", "for", "i", ",", "fold", "in", "enumerate", "(", "folds", ")", ":", "\n", "            ", "if", "num_elements", ">", "len", "(", "fold", ")", ":", "\n", "                ", "num_elements", "=", "len", "(", "fold", ")", "\n", "index", "=", "i", "\n", "\n", "# Add post to that fold", "\n", "", "", "folds", "[", "index", "]", ".", "update", "(", "post_ids", ")", "\n", "\n", "", "return", "folds", "\n", "\n"]], "home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.util.arrange_folds_for_k_fold_cross_validation": [[238, 245], ["set", "itertools.chain.from_iterable", "enumerate"], "function", ["None"], ["", "def", "arrange_folds_for_k_fold_cross_validation", "(", "folds", ":", "List", "[", "Set", "[", "str", "]", "]", ",", "\n", "index", ":", "int", ")", "->", "(", "Set", "[", "str", "]", ",", "Set", "[", "str", "]", ")", ":", "\n", "    ", "train_post_ids", "=", "set", "(", "chain", ".", "from_iterable", "(", "\n", "fold", "for", "i", ",", "fold", "in", "enumerate", "(", "folds", ")", "if", "i", "!=", "index", ")", ")", "\n", "test_post_ids", "=", "folds", "[", "index", "]", "\n", "return", "train_post_ids", ",", "test_post_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.util.filter_instances": [[247, 259], ["random.shuffle", "random.shuffle"], "function", ["None"], ["", "def", "filter_instances", "(", "train_post_ids", ":", "Set", "[", "str", "]", ",", "\n", "test_post_ids", ":", "Set", "[", "str", "]", ",", "\n", "instances", ":", "Iterable", "[", "Union", "[", "SdqcInstance", ",", "VerifInstance", "]", "]", ")", "->", "(", "List", "[", "Union", "[", "SdqcInstance", ",", "VerifInstance", "]", "]", ",", "\n", "List", "[", "Union", "[", "SdqcInstance", ",", "VerifInstance", "]", "]", ")", ":", "\n", "    ", "train_instances", "=", "[", "i", "for", "i", "in", "instances", "if", "i", ".", "post_id", "in", "train_post_ids", "]", "\n", "test_instances", "=", "[", "i", "for", "i", "in", "instances", "if", "i", ".", "post_id", "in", "test_post_ids", "]", "\n", "\n", "shuffle", "(", "train_instances", ")", "\n", "shuffle", "(", "test_instances", ")", "\n", "\n", "return", "train_instances", ",", "test_instances", "\n", "\n"]], "home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.util.rmse_score": [[261, 275], ["zip", "math.sqrt", "len"], "function", ["None"], ["", "def", "rmse_score", "(", "labels", ",", "predictions", ",", "confidences", ")", ":", "\n", "    ", "rmse", "=", "0", "\n", "for", "label", ",", "prediction", ",", "confidence", "in", "zip", "(", "labels", ",", "predictions", ",", "confidences", ")", ":", "\n", "        ", "if", "label", "==", "prediction", "and", "(", "label", "==", "VerifInstance", ".", "Label", ".", "true", ".", "value", "\n", "or", "label", "==", "VerifInstance", ".", "Label", ".", "false", ".", "value", ")", ":", "\n", "            ", "rmse", "+=", "(", "1", "-", "confidence", ")", "**", "2", "\n", "", "elif", "label", "==", "VerifInstance", ".", "Label", ".", "unverified", ".", "value", ":", "\n", "            ", "rmse", "+=", "confidence", "**", "2", "\n", "", "else", ":", "\n", "            ", "rmse", "+=", "1", "\n", "", "", "rmse", "=", "sqrt", "(", "rmse", "/", "len", "(", "labels", ")", ")", "\n", "return", "rmse", "\n", "\n"]], "home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.util.display_results": [[277, 323], ["print", "print", "util.display_results.display_report"], "function", ["None"], ["", "def", "display_results", "(", "sdqc_accs", ":", "Iterable", "[", "float", "]", ",", "\n", "sdqc_f1s", ":", "Iterable", "[", "float", "]", ",", "\n", "sdqc_reports", ":", "Iterable", "[", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "float", "]", "]", "]", ",", "\n", "verif_accs", ":", "Iterable", "[", "float", "]", ",", "\n", "verif_f1s", ":", "Iterable", "[", "float", "]", ",", "\n", "verif_rmses", ":", "Iterable", "[", "float", "]", ",", "\n", "verif_reports", ":", "Iterable", "[", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "float", "]", "]", "]", ")", ":", "\n", "    ", "def", "display_report", "(", "reports", ":", "Iterable", "[", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "float", "]", "]", "]", ")", ":", "\n", "        ", "report_lists", "=", "defaultdict", "(", "lambda", ":", "defaultdict", "(", "list", ")", ")", "\n", "for", "report", "in", "reports", ":", "\n", "            ", "for", "outer_key", ",", "inner_report", "in", "report", ".", "items", "(", ")", ":", "\n", "                ", "if", "outer_key", "==", "'accuracy'", ":", "\n", "                    ", "report_lists", "[", "outer_key", "]", "[", "'accuracy'", "]", ".", "append", "(", "inner_report", ")", "\n", "", "else", ":", "\n", "                    ", "for", "inner_key", ",", "value", "in", "inner_report", ".", "items", "(", ")", ":", "\n", "                        ", "report_lists", "[", "outer_key", "]", "[", "inner_key", "]", ".", "append", "(", "value", ")", "\n", "\n", "", "", "", "", "report_stats", "=", "{", "}", "\n", "for", "outer_key", ",", "inner_report", "in", "report_lists", ".", "items", "(", ")", ":", "\n", "            ", "report_stats", "[", "outer_key", "]", "=", "{", "}", "\n", "for", "inner_key", ",", "values", "in", "inner_report", ".", "items", "(", ")", ":", "\n", "                ", "report_stats", "[", "outer_key", "]", "[", "inner_key", "]", "=", "'{:.1%}\u00b1{:.1%}'", ".", "format", "(", "\n", "np", ".", "mean", "(", "values", ")", ",", "np", ".", "std", "(", "values", ")", ")", "\n", "\n", "", "", "pprint", "(", "report_stats", ")", "\n", "\n", "", "sdqc_acc", "=", "(", "np", ".", "mean", "(", "sdqc_accs", ")", ",", "np", ".", "std", "(", "sdqc_accs", ")", ")", "\n", "sdqc_f1", "=", "(", "np", ".", "mean", "(", "sdqc_f1s", ")", ",", "np", ".", "std", "(", "sdqc_f1s", ")", ")", "\n", "print", "(", "'Task A: SDQC'", ")", "\n", "print", "(", "'  Accuracy: {:.1%}\u00b1{:.1%}'", "\n", "'  F1-Score: {:.1%}\u00b1{:.1%}'", "\n", ".", "format", "(", "sdqc_acc", "[", "0", "]", ",", "sdqc_acc", "[", "1", "]", ",", "\n", "sdqc_f1", "[", "0", "]", ",", "sdqc_f1", "[", "1", "]", ")", ")", "\n", "display_report", "(", "sdqc_reports", ")", "\n", "\n", "verif_acc", "=", "(", "np", ".", "mean", "(", "verif_accs", ")", ",", "np", ".", "std", "(", "verif_accs", ")", ")", "\n", "verif_f1", "=", "(", "np", ".", "mean", "(", "verif_f1s", ")", ",", "np", ".", "std", "(", "verif_f1s", ")", ")", "\n", "verif_rmse", "=", "(", "np", ".", "mean", "(", "verif_rmses", ")", ",", "np", ".", "std", "(", "verif_rmses", ")", ")", "\n", "print", "(", "'Task B: Verification'", ")", "\n", "print", "(", "'  Accuracy: {:.1%}\u00b1{:.1%}'", "\n", "'  F1-Score: {:.1%}\u00b1{:.1%}'", "\n", "'  RMSE: {:.3f}\u00b1{:.3f}'", "\n", ".", "format", "(", "verif_acc", "[", "0", "]", ",", "verif_acc", "[", "1", "]", ",", "\n", "verif_f1", "[", "0", "]", ",", "verif_f1", "[", "1", "]", ",", "\n", "verif_rmse", "[", "0", "]", ",", "verif_rmse", "[", "1", "]", ")", ")", "\n", "display_report", "(", "verif_reports", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lschmelzeisen_clearumor.src.util.write_answers_json": [[325, 348], ["collections.OrderedDict", "collections.OrderedDict", "collections.OrderedDict", "path.open", "json.dump"], "function", ["None"], ["", "def", "write_answers_json", "(", "\n", "path", ":", "Path", ",", "\n", "sdqc_instances", ":", "List", "[", "SdqcInstance", "]", ",", "\n", "verif_instances", ":", "List", "[", "SdqcInstance", "]", ",", "\n", "sdqc_estimates", ":", "Dict", "[", "str", ",", "Tuple", "[", "SdqcInstance", ".", "Label", ",", "\n", "Dict", "[", "SdqcInstance", ".", "Label", ",", "float", "]", "]", "]", ",", "\n", "verif_estimates", ":", "Dict", "[", "str", ",", "Tuple", "[", "VerifInstance", ".", "Label", ",", "float", "]", "]", ")", ":", "\n", "    ", "sdqc_answers", "=", "OrderedDict", "(", ")", "\n", "for", "instance", "in", "sdqc_instances", ":", "\n", "        ", "answer", "=", "sdqc_estimates", "[", "instance", ".", "post_id", "]", "\n", "sdqc_answers", "[", "instance", ".", "post_id", "]", "=", "answer", "[", "0", "]", ".", "name", "\n", "\n", "", "verif_answers", "=", "OrderedDict", "(", ")", "\n", "for", "instance", "in", "verif_instances", ":", "\n", "        ", "answer", "=", "verif_estimates", "[", "instance", ".", "post_id", "]", "\n", "verif_answers", "[", "instance", ".", "post_id", "]", "=", "(", "answer", "[", "0", "]", ".", "name", ",", "answer", "[", "1", "]", ")", "\n", "\n", "", "answers", "=", "OrderedDict", "(", ")", "\n", "answers", "[", "'subtaskaenglish'", "]", "=", "sdqc_answers", "\n", "answers", "[", "'subtaskbenglish'", "]", "=", "verif_answers", "\n", "\n", "with", "path", ".", "open", "(", "'w'", ",", "encoding", "=", "'UTF-8'", ")", "as", "fout", ":", "\n", "        ", "json", ".", "dump", "(", "answers", ",", "fout", ",", "indent", "=", "2", ")", "\n", "", "", ""]]}