{"home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.second_model.Model.__init__": [[16, 54], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "config", ",", "label2id_sent", ",", "label2id_tok", ")", ":", "\n", "        ", "self", ".", "config", "=", "config", "\n", "self", ".", "label2id_sent", "=", "label2id_sent", "\n", "self", ".", "label2id_tok", "=", "label2id_tok", "\n", "\n", "self", ".", "UNK", "=", "\"<unk>\"", "\n", "self", ".", "CUNK", "=", "\"<cunk>\"", "\n", "self", ".", "word2id", "=", "None", "\n", "self", ".", "char2id", "=", "None", "\n", "self", ".", "singletons", "=", "None", "\n", "self", ".", "num_heads", "=", "None", "\n", "\n", "self", ".", "word_ids", "=", "None", "\n", "self", ".", "char_ids", "=", "None", "\n", "self", ".", "sentence_lengths", "=", "None", "\n", "self", ".", "word_lengths", "=", "None", "\n", "\n", "self", ".", "sentence_labels", "=", "None", "\n", "self", ".", "word_labels", "=", "None", "\n", "\n", "self", ".", "word_embeddings", "=", "None", "\n", "self", ".", "char_embeddings", "=", "None", "\n", "\n", "self", ".", "word_objective_weights", "=", "None", "\n", "self", ".", "sentence_objective_weights", "=", "None", "\n", "\n", "self", ".", "learning_rate", "=", "None", "\n", "self", ".", "loss", "=", "None", "\n", "self", ".", "initializer", "=", "None", "\n", "self", ".", "is_training", "=", "None", "\n", "self", ".", "session", "=", "None", "\n", "self", ".", "saver", "=", "None", "\n", "self", ".", "train_op", "=", "None", "\n", "\n", "self", ".", "sentence_predictions", "=", "None", "\n", "self", ".", "sentence_probabilities", "=", "None", "\n", "self", ".", "token_predictions", "=", "None", "\n", "self", ".", "token_probabilities", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.second_model.Model.build_vocabs": [[55, 113], ["list", "collections.Counter", "collections.Counter", "collections.OrderedDict", "collections.Counter.most_common", "collections.OrderedDict", "collections.Counter.most_common", "set", "print", "print", "print", "collections.OrderedDict", "collections.Counter.update", "len", "open", "str", "str", "str", "re.sub.lower", "re.sub", "len", "line.strip().split", "embedding_vocab.add", "len", "len", "len", "len", "len", "re.sub.lower", "re.sub", "line.strip"], "methods", ["None"], ["", "def", "build_vocabs", "(", "self", ",", "data_train", ",", "data_dev", ",", "data_test", ",", "embedding_path", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Builds the vocabulary based on the the data and embeddings info.\n        \"\"\"", "\n", "data_source", "=", "list", "(", "data_train", ")", "\n", "if", "self", ".", "config", "[", "\"vocab_include_devtest\"", "]", ":", "\n", "            ", "if", "data_dev", "is", "not", "None", ":", "\n", "                ", "data_source", "+=", "data_dev", "\n", "", "if", "data_test", "is", "not", "None", ":", "\n", "                ", "data_source", "+=", "data_test", "\n", "\n", "", "", "char_counter", "=", "collections", ".", "Counter", "(", ")", "\n", "word_counter", "=", "collections", ".", "Counter", "(", ")", "\n", "for", "sentence", "in", "data_source", ":", "\n", "            ", "for", "token", "in", "sentence", ".", "tokens", ":", "\n", "                ", "char_counter", ".", "update", "(", "token", ".", "value", ")", "\n", "w", "=", "token", ".", "value", "\n", "if", "self", ".", "config", "[", "\"lowercase\"", "]", ":", "\n", "                    ", "w", "=", "w", ".", "lower", "(", ")", "\n", "", "if", "self", ".", "config", "[", "\"replace_digits\"", "]", ":", "\n", "                    ", "w", "=", "re", ".", "sub", "(", "r'\\d'", ",", "'0'", ",", "w", ")", "\n", "", "word_counter", "[", "w", "]", "+=", "1", "\n", "\n", "", "", "self", ".", "char2id", "=", "collections", ".", "OrderedDict", "(", "[", "(", "self", ".", "CUNK", ",", "0", ")", "]", ")", "\n", "for", "char", ",", "count", "in", "char_counter", ".", "most_common", "(", ")", ":", "\n", "            ", "if", "char", "not", "in", "self", ".", "char2id", ":", "\n", "                ", "self", ".", "char2id", "[", "char", "]", "=", "len", "(", "self", ".", "char2id", ")", "\n", "\n", "", "", "self", ".", "word2id", "=", "collections", ".", "OrderedDict", "(", "[", "(", "self", ".", "UNK", ",", "0", ")", "]", ")", "\n", "for", "word", ",", "count", "in", "word_counter", ".", "most_common", "(", ")", ":", "\n", "            ", "if", "self", ".", "config", "[", "\"min_word_freq\"", "]", "<=", "0", "or", "count", ">=", "self", ".", "config", "[", "\"min_word_freq\"", "]", ":", "\n", "                ", "if", "word", "not", "in", "self", ".", "word2id", ":", "\n", "                    ", "self", ".", "word2id", "[", "word", "]", "=", "len", "(", "self", ".", "word2id", ")", "\n", "\n", "", "", "", "self", ".", "singletons", "=", "set", "(", "[", "word", "for", "word", "in", "word_counter", "if", "word_counter", "[", "word", "]", "==", "1", "]", ")", "\n", "\n", "if", "embedding_path", "and", "self", ".", "config", "[", "\"vocab_only_embedded\"", "]", ":", "\n", "            ", "embedding_vocab", "=", "{", "self", ".", "UNK", "}", "\n", "with", "open", "(", "embedding_path", ")", "as", "f", ":", "\n", "                ", "for", "line", "in", "f", ":", "\n", "                    ", "line_parts", "=", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "if", "len", "(", "line_parts", ")", "<=", "2", ":", "\n", "                        ", "continue", "\n", "", "w", "=", "line_parts", "[", "0", "]", "\n", "if", "self", ".", "config", "[", "\"lowercase\"", "]", ":", "\n", "                        ", "w", "=", "w", ".", "lower", "(", ")", "\n", "", "if", "self", ".", "config", "[", "\"replace_digits\"", "]", ":", "\n", "                        ", "w", "=", "re", ".", "sub", "(", "r'\\d'", ",", "'0'", ",", "w", ")", "\n", "", "embedding_vocab", ".", "add", "(", "w", ")", "\n", "", "", "word2id_revised", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "for", "word", "in", "self", ".", "word2id", ":", "\n", "                ", "if", "word", "in", "embedding_vocab", "and", "word", "not", "in", "word2id_revised", ":", "\n", "                    ", "word2id_revised", "[", "word", "]", "=", "len", "(", "word2id_revised", ")", "\n", "", "", "self", ".", "word2id", "=", "word2id_revised", "\n", "\n", "", "print", "(", "\"Total number of words: \"", "+", "str", "(", "len", "(", "self", ".", "word2id", ")", ")", ")", "\n", "print", "(", "\"Total number of chars: \"", "+", "str", "(", "len", "(", "self", ".", "char2id", ")", ")", ")", "\n", "print", "(", "\"Total number of singletons: \"", "+", "str", "(", "len", "(", "self", ".", "singletons", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.second_model.Model.construct_network": [[114, 507], ["tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.zeros_initializer", "tensorflow.get_variable", "tensorflow.nn.embedding_lookup", "tensorflow.nn.rnn_cell.LSTMCell", "tensorflow.nn.rnn_cell.LSTMCell", "tensorflow.concat", "tensorflow.concat", "tensorflow.tile", "tensorflow.nn.softmax", "tensorflow.where", "tensorflow.argmax", "tensorflow.nn.softmax", "tensorflow.argmax", "tensorflow.reduce_max", "tensorflow.one_hot", "second_model.Model.construct_optimizer", "print", "tensorflow.random_normal_initializer", "tensorflow.nn.dropout", "tensorflow.control_dependencies", "tensorflow.nn.bidirectional_dynamic_rnn", "tensorflow.nn.dropout", "tensorflow.nn.dropout", "tensorflow.nn.dropout", "tensorflow.layers.dense", "tensorflow.layers.dense", "tensorflow.layers.dense", "tensorflow.zeros_like", "tensorflow.nn.sparse_softmax_cross_entropy_with_logits", "tensorflow.where", "tensorflow.cast", "modules.label_smoothing", "tensorflow.nn.softmax", "tensorflow.gather", "tensorflow.squeeze", "tensorflow.gather", "tensorflow.reduce_max", "tensorflow.glorot_uniform_initializer", "tensorflow.variable_scope", "tensorflow.control_dependencies", "tensorflow.get_variable", "tensorflow.nn.embedding_lookup", "tensorflow.shape", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.nn.rnn_cell.LSTMCell", "tensorflow.nn.rnn_cell.LSTMCell", "tensorflow.nn.bidirectional_dynamic_rnn", "tensorflow.concat", "tensorflow.reshape", "tensorflow.layers.dense", "tensorflow.variable_scope", "range", "tensorflow.stack", "tensorflow.stack", "tensorflow.expand_dims", "tensorflow.sequence_mask", "tensorflow.zeros_like", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "len", "len", "len", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "len", "len", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.squeeze", "tensorflow.reduce_sum", "second_model.Model.construct_lm_cost", "second_model.Model.construct_lm_cost", "tensorflow.glorot_normal_initializer", "len", "tensorflow.layers.dense", "tensorflow.concat", "tensorflow.cast", "tensorflow.cast", "tensorflow.assert_equal", "tensorflow.cast", "tensorflow.cast", "tensorflow.convert_to_tensor", "tensorflow.convert_to_tensor", "len", "len", "len", "tensorflow.layers.dense", "tensorflow.layers.dense", "tensorflow.layers.dense", "tensorflow.reshape", "token_scores_list.append", "tensorflow.where", "tensorflow.reduce_sum", "tensorflow.layers.dense", "tensorflow.reshape", "sentence_scores_list.append", "len", "len", "tensorflow.sequence_mask", "len", "tensorflow.cast", "len", "tensorflow.gather", "tensorflow.reduce_max", "tensorflow.concat", "ValueError", "len", "tensorflow.gather", "tensorflow.reduce_max", "tensorflow.concat", "ValueError", "tensorflow.assert_equal", "second_model.Model.construct_lm_cost", "second_model.Model.construct_lm_cost", "ValueError", "tensorflow.reduce_max", "tensorflow.exp", "tensorflow.sequence_mask", "tensorflow.zeros_like", "tensorflow.reduce_sum", "tensorflow.layers.dense", "len", "tensorflow.gather", "tensorflow.gather", "tensorflow.reduce_max", "tensorflow.concat", "tensorflow.layers.dense", "tensorflow.nn.sparse_softmax_cross_entropy_with_logits", "tensorflow.gather", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.gather", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.square", "tensorflow.reduce_max", "len", "tensorflow.shape", "tensorflow.shape", "tensorflow.sigmoid", "tensorflow.square", "tensorflow.square", "tensorflow.square", "tensorflow.cast", "tensorflow.square", "tensorflow.shape", "tensorflow.shape", "tensorflow.shape", "ValueError", "list", "len", "tensorflow.cast", "list", "list", "tensorflow.equal", "tensorflow.ones_like", "tensorflow.shape", "range", "range", "range", "tensorflow.ones_like", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.construct_optimizer", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.label_smoothing", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.construct_lm_cost", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.construct_lm_cost", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.construct_lm_cost", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.construct_lm_cost"], ["", "def", "construct_network", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Constructs a variant of the multi-head attention labeller (MHAL)\n        that does not use keys, queries and values, but only a simple form\n        of additive attention, as proposed by Yang et al. (2016).\n        \"\"\"", "\n", "self", ".", "word_ids", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "None", ",", "None", "]", ",", "name", "=", "\"word_ids\"", ")", "\n", "self", ".", "char_ids", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "None", ",", "None", ",", "None", "]", ",", "name", "=", "\"char_ids\"", ")", "\n", "self", ".", "sentence_lengths", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "None", "]", ",", "name", "=", "\"sentence_lengths\"", ")", "\n", "self", ".", "word_lengths", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "None", ",", "None", "]", ",", "name", "=", "\"word_lengths\"", ")", "\n", "self", ".", "sentence_labels", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "None", "]", ",", "name", "=", "\"sentence_labels\"", ")", "\n", "self", ".", "word_labels", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "None", ",", "None", "]", ",", "name", "=", "\"word_labels\"", ")", "\n", "\n", "self", ".", "word_objective_weights", "=", "tf", ".", "placeholder", "(", "\n", "tf", ".", "float32", ",", "[", "None", ",", "None", "]", ",", "name", "=", "\"word_objective_weights\"", ")", "\n", "self", ".", "sentence_objective_weights", "=", "tf", ".", "placeholder", "(", "\n", "tf", ".", "float32", ",", "[", "None", "]", ",", "name", "=", "\"sentence_objective_weights\"", ")", "\n", "\n", "self", ".", "learning_rate", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "name", "=", "\"learning_rate\"", ")", "\n", "self", ".", "is_training", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "name", "=", "\"is_training\"", ")", "\n", "self", ".", "loss", "=", "0.0", "\n", "\n", "if", "self", ".", "config", "[", "\"initializer\"", "]", "==", "\"normal\"", ":", "\n", "            ", "self", ".", "initializer", "=", "tf", ".", "random_normal_initializer", "(", "stddev", "=", "0.1", ")", "\n", "", "elif", "self", ".", "config", "[", "\"initializer\"", "]", "==", "\"glorot\"", ":", "\n", "            ", "self", ".", "initializer", "=", "tf", ".", "glorot_uniform_initializer", "(", ")", "\n", "", "elif", "self", ".", "config", "[", "\"initializer\"", "]", "==", "\"xavier\"", ":", "\n", "            ", "self", ".", "initializer", "=", "tf", ".", "glorot_normal_initializer", "(", ")", "\n", "\n", "", "zeros_initializer", "=", "tf", ".", "zeros_initializer", "(", ")", "\n", "\n", "self", ".", "word_embeddings", "=", "tf", ".", "get_variable", "(", "\n", "name", "=", "\"word_embeddings\"", ",", "\n", "shape", "=", "[", "len", "(", "self", ".", "word2id", ")", ",", "self", ".", "config", "[", "\"word_embedding_size\"", "]", "]", ",", "\n", "initializer", "=", "(", "zeros_initializer", "if", "self", ".", "config", "[", "\"emb_initial_zero\"", "]", "else", "self", ".", "initializer", ")", ",", "\n", "trainable", "=", "(", "True", "if", "self", ".", "config", "[", "\"train_embeddings\"", "]", "else", "False", ")", ")", "\n", "word_input_tensor", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "self", ".", "word_embeddings", ",", "self", ".", "word_ids", ")", "\n", "\n", "if", "self", ".", "config", "[", "\"char_embedding_size\"", "]", ">", "0", "and", "self", ".", "config", "[", "\"char_recurrent_size\"", "]", ">", "0", ":", "\n", "            ", "with", "tf", ".", "variable_scope", "(", "\"chars\"", ")", ",", "tf", ".", "control_dependencies", "(", "\n", "[", "tf", ".", "assert_equal", "(", "tf", ".", "shape", "(", "self", ".", "char_ids", ")", "[", "2", "]", ",", "\n", "tf", ".", "reduce_max", "(", "self", ".", "word_lengths", ")", ",", "\n", "message", "=", "\"Char dimensions don't match\"", ")", "]", ")", ":", "\n", "                ", "self", ".", "char_embeddings", "=", "tf", ".", "get_variable", "(", "\n", "name", "=", "\"char_embeddings\"", ",", "\n", "shape", "=", "[", "len", "(", "self", ".", "char2id", ")", ",", "self", ".", "config", "[", "\"char_embedding_size\"", "]", "]", ",", "\n", "initializer", "=", "self", ".", "initializer", ",", "\n", "trainable", "=", "True", ")", "\n", "char_input_tensor", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "self", ".", "char_embeddings", ",", "self", ".", "char_ids", ")", "\n", "\n", "char_input_tensor_shape", "=", "tf", ".", "shape", "(", "char_input_tensor", ")", "\n", "char_input_tensor", "=", "tf", ".", "reshape", "(", "\n", "char_input_tensor", ",", "\n", "shape", "=", "[", "char_input_tensor_shape", "[", "0", "]", "\n", "*", "char_input_tensor_shape", "[", "1", "]", ",", "\n", "char_input_tensor_shape", "[", "2", "]", ",", "\n", "self", ".", "config", "[", "\"char_embedding_size\"", "]", "]", ")", "\n", "_word_lengths", "=", "tf", ".", "reshape", "(", "\n", "self", ".", "word_lengths", ",", "shape", "=", "[", "char_input_tensor_shape", "[", "0", "]", "\n", "*", "char_input_tensor_shape", "[", "1", "]", "]", ")", "\n", "\n", "char_lstm_cell_fw", "=", "tf", ".", "nn", ".", "rnn_cell", ".", "LSTMCell", "(", "\n", "self", ".", "config", "[", "\"char_recurrent_size\"", "]", ",", "\n", "use_peepholes", "=", "self", ".", "config", "[", "\"lstm_use_peepholes\"", "]", ",", "\n", "state_is_tuple", "=", "True", ",", "\n", "initializer", "=", "self", ".", "initializer", ",", "\n", "reuse", "=", "False", ")", "\n", "char_lstm_cell_bw", "=", "tf", ".", "nn", ".", "rnn_cell", ".", "LSTMCell", "(", "\n", "self", ".", "config", "[", "\"char_recurrent_size\"", "]", ",", "\n", "use_peepholes", "=", "self", ".", "config", "[", "\"lstm_use_peepholes\"", "]", ",", "\n", "state_is_tuple", "=", "True", ",", "\n", "initializer", "=", "self", ".", "initializer", ",", "\n", "reuse", "=", "False", ")", "\n", "\n", "# Concatenate the final forward and the backward character contexts", "\n", "# to obtain a compact character representation for each word.", "\n", "_", ",", "(", "(", "_", ",", "char_output_fw", ")", ",", "(", "_", ",", "char_output_bw", ")", ")", "=", "tf", ".", "nn", ".", "bidirectional_dynamic_rnn", "(", "\n", "cell_fw", "=", "char_lstm_cell_fw", ",", "cell_bw", "=", "char_lstm_cell_bw", ",", "inputs", "=", "char_input_tensor", ",", "\n", "sequence_length", "=", "_word_lengths", ",", "dtype", "=", "tf", ".", "float32", ",", "time_major", "=", "False", ")", "\n", "\n", "char_output_tensor", "=", "tf", ".", "concat", "(", "[", "char_output_fw", ",", "char_output_bw", "]", ",", "axis", "=", "-", "1", ")", "\n", "char_output_tensor", "=", "tf", ".", "reshape", "(", "\n", "char_output_tensor", ",", "\n", "shape", "=", "[", "char_input_tensor_shape", "[", "0", "]", ",", "char_input_tensor_shape", "[", "1", "]", ",", "\n", "2", "*", "self", ".", "config", "[", "\"char_recurrent_size\"", "]", "]", ")", "\n", "\n", "# Include a char-based language modelling loss, LMc.", "\n", "if", "self", ".", "config", "[", "\"lm_cost_char_gamma\"", "]", ">", "0.0", ":", "\n", "                    ", "self", ".", "loss", "+=", "self", ".", "config", "[", "\"lm_cost_char_gamma\"", "]", "*", "self", ".", "construct_lm_cost", "(", "\n", "input_tensor_fw", "=", "char_output_tensor", ",", "\n", "input_tensor_bw", "=", "char_output_tensor", ",", "\n", "sentence_lengths", "=", "self", ".", "sentence_lengths", ",", "\n", "target_ids", "=", "self", ".", "word_ids", ",", "\n", "lm_cost_type", "=", "\"separate\"", ",", "\n", "name", "=", "\"lm_cost_char_separate\"", ")", "\n", "\n", "", "if", "self", ".", "config", "[", "\"lm_cost_joint_char_gamma\"", "]", ">", "0.0", ":", "\n", "                    ", "self", ".", "loss", "+=", "self", ".", "config", "[", "\"lm_cost_joint_char_gamma\"", "]", "*", "self", ".", "construct_lm_cost", "(", "\n", "input_tensor_fw", "=", "char_output_tensor", ",", "\n", "input_tensor_bw", "=", "char_output_tensor", ",", "\n", "sentence_lengths", "=", "self", ".", "sentence_lengths", ",", "\n", "target_ids", "=", "self", ".", "word_ids", ",", "\n", "lm_cost_type", "=", "\"joint\"", ",", "\n", "name", "=", "\"lm_cost_char_joint\"", ")", "\n", "\n", "", "if", "self", ".", "config", "[", "\"char_hidden_layer_size\"", "]", ">", "0", ":", "\n", "                    ", "char_output_tensor", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", "=", "char_output_tensor", ",", "units", "=", "self", ".", "config", "[", "\"char_hidden_layer_size\"", "]", ",", "\n", "activation", "=", "tf", ".", "tanh", ",", "kernel_initializer", "=", "self", ".", "initializer", ")", "\n", "\n", "", "if", "self", ".", "config", "[", "\"char_integration_method\"", "]", "==", "\"concat\"", ":", "\n", "                    ", "word_input_tensor", "=", "tf", ".", "concat", "(", "[", "word_input_tensor", ",", "char_output_tensor", "]", ",", "axis", "=", "-", "1", ")", "\n", "", "elif", "self", ".", "config", "[", "\"char_integration_method\"", "]", "==", "\"none\"", ":", "\n", "                    ", "word_input_tensor", "=", "word_input_tensor", "\n", "", "else", ":", "\n", "                    ", "raise", "ValueError", "(", "\"Unknown char integration method\"", ")", "\n", "\n", "", "", "", "if", "self", ".", "config", "[", "\"dropout_input\"", "]", ">", "0.0", ":", "\n", "            ", "dropout_input", "=", "(", "self", ".", "config", "[", "\"dropout_input\"", "]", "*", "tf", ".", "cast", "(", "self", ".", "is_training", ",", "tf", ".", "float32", ")", "\n", "+", "(", "1.0", "-", "tf", ".", "cast", "(", "self", ".", "is_training", ",", "tf", ".", "float32", ")", ")", ")", "\n", "word_input_tensor", "=", "tf", ".", "nn", ".", "dropout", "(", "\n", "word_input_tensor", ",", "dropout_input", ",", "name", "=", "\"dropout_word\"", ")", "\n", "\n", "", "word_lstm_cell_fw", "=", "tf", ".", "nn", ".", "rnn_cell", ".", "LSTMCell", "(", "\n", "self", ".", "config", "[", "\"word_recurrent_size\"", "]", ",", "\n", "use_peepholes", "=", "self", ".", "config", "[", "\"lstm_use_peepholes\"", "]", ",", "\n", "state_is_tuple", "=", "True", ",", "\n", "initializer", "=", "self", ".", "initializer", ",", "\n", "reuse", "=", "False", ")", "\n", "word_lstm_cell_bw", "=", "tf", ".", "nn", ".", "rnn_cell", ".", "LSTMCell", "(", "\n", "self", ".", "config", "[", "\"word_recurrent_size\"", "]", ",", "\n", "use_peepholes", "=", "self", ".", "config", "[", "\"lstm_use_peepholes\"", "]", ",", "\n", "state_is_tuple", "=", "True", ",", "\n", "initializer", "=", "self", ".", "initializer", ",", "\n", "reuse", "=", "False", ")", "\n", "\n", "with", "tf", ".", "control_dependencies", "(", "\n", "[", "tf", ".", "assert_equal", "(", "\n", "tf", ".", "shape", "(", "self", ".", "word_ids", ")", "[", "1", "]", ",", "\n", "tf", ".", "reduce_max", "(", "self", ".", "sentence_lengths", ")", ",", "\n", "message", "=", "\"Sentence dimensions don't match\"", ")", "]", ")", ":", "\n", "            ", "(", "lstm_outputs_fw", ",", "lstm_outputs_bw", ")", ",", "(", "(", "_", ",", "lstm_output_fw", ")", ",", "(", "_", ",", "lstm_output_bw", ")", ")", "=", "tf", ".", "nn", ".", "bidirectional_dynamic_rnn", "(", "\n", "cell_fw", "=", "word_lstm_cell_fw", ",", "cell_bw", "=", "word_lstm_cell_bw", ",", "inputs", "=", "word_input_tensor", ",", "\n", "sequence_length", "=", "self", ".", "sentence_lengths", ",", "dtype", "=", "tf", ".", "float32", ",", "time_major", "=", "False", ")", "\n", "\n", "", "lstm_output_states", "=", "tf", ".", "concat", "(", "[", "lstm_output_fw", ",", "lstm_output_bw", "]", ",", "axis", "=", "-", "1", ")", "\n", "\n", "if", "self", ".", "config", "[", "\"dropout_word_lstm\"", "]", ">", "0.0", ":", "\n", "            ", "dropout_word_lstm", "=", "(", "self", ".", "config", "[", "\"dropout_word_lstm\"", "]", "*", "tf", ".", "cast", "(", "self", ".", "is_training", ",", "tf", ".", "float32", ")", "\n", "+", "(", "1.0", "-", "tf", ".", "cast", "(", "self", ".", "is_training", ",", "tf", ".", "float32", ")", ")", ")", "\n", "lstm_outputs_fw", "=", "tf", ".", "nn", ".", "dropout", "(", "\n", "lstm_outputs_fw", ",", "dropout_word_lstm", ",", "\n", "noise_shape", "=", "tf", ".", "convert_to_tensor", "(", "\n", "[", "tf", ".", "shape", "(", "self", ".", "word_ids", ")", "[", "0", "]", ",", "1", ",", "self", ".", "config", "[", "\"word_recurrent_size\"", "]", "]", ",", "dtype", "=", "tf", ".", "int32", ")", ")", "\n", "lstm_outputs_bw", "=", "tf", ".", "nn", ".", "dropout", "(", "\n", "lstm_outputs_bw", ",", "dropout_word_lstm", ",", "\n", "noise_shape", "=", "tf", ".", "convert_to_tensor", "(", "\n", "[", "tf", ".", "shape", "(", "self", ".", "word_ids", ")", "[", "0", "]", ",", "1", ",", "self", ".", "config", "[", "\"word_recurrent_size\"", "]", "]", ",", "dtype", "=", "tf", ".", "int32", ")", ")", "\n", "lstm_output_states", "=", "tf", ".", "nn", ".", "dropout", "(", "lstm_output_states", ",", "dropout_word_lstm", ")", "\n", "\n", "# The forward and backward states are concatenated at every token position.", "\n", "", "lstm_outputs_states", "=", "tf", ".", "concat", "(", "[", "lstm_outputs_fw", ",", "lstm_outputs_bw", "]", ",", "axis", "=", "-", "1", ")", "\n", "\n", "if", "self", ".", "config", "[", "\"whidden_layer_size\"", "]", ">", "0", ":", "\n", "            ", "lstm_outputs_states", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "lstm_outputs_states", ",", "self", ".", "config", "[", "\"whidden_layer_size\"", "]", ",", "\n", "activation", "=", "tf", ".", "tanh", ",", "kernel_initializer", "=", "self", ".", "initializer", ")", "\n", "\n", "", "if", "self", ".", "config", "[", "\"model_type\"", "]", "==", "\"last\"", ":", "\n", "            ", "processed_tensor", "=", "lstm_output_states", "\n", "token_scores", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "lstm_outputs_states", ",", "units", "=", "len", "(", "self", ".", "label2id_tok", ")", ",", "\n", "kernel_initializer", "=", "self", ".", "initializer", ",", "\n", "name", "=", "\"token_scores_last_lstm_outputs_ff\"", ")", "\n", "if", "self", ".", "config", "[", "\"hidden_layer_size\"", "]", ">", "0", ":", "\n", "                ", "processed_tensor", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "processed_tensor", ",", "units", "=", "self", ".", "config", "[", "\"hidden_layer_size\"", "]", ",", "\n", "activation", "=", "tf", ".", "tanh", ",", "kernel_initializer", "=", "self", ".", "initializer", ")", "\n", "", "sentence_scores", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "processed_tensor", ",", "units", "=", "len", "(", "self", ".", "label2id_sent", ")", ",", "\n", "kernel_initializer", "=", "self", ".", "initializer", ",", "\n", "name", "=", "\"sentence_scores_last_lstm_outputs_ff\"", ")", "\n", "", "else", ":", "\n", "            ", "with", "tf", ".", "variable_scope", "(", "\"attention\"", ")", ":", "\n", "                ", "token_scores_list", "=", "[", "]", "\n", "sentence_scores_list", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "label2id_tok", ")", ")", ":", "\n", "                    ", "keys", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "lstm_outputs_states", ",", "units", "=", "self", ".", "config", "[", "\"attention_evidence_size\"", "]", ",", "\n", "activation", "=", "tf", ".", "tanh", ",", "kernel_initializer", "=", "self", ".", "initializer", ")", "\n", "values", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "lstm_outputs_states", ",", "units", "=", "self", ".", "config", "[", "\"attention_evidence_size\"", "]", ",", "\n", "activation", "=", "tf", ".", "tanh", ",", "kernel_initializer", "=", "self", ".", "initializer", ")", "\n", "\n", "token_scores_head", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "keys", ",", "units", "=", "1", ",", "kernel_initializer", "=", "self", ".", "initializer", ")", "# [B, M, 1]", "\n", "token_scores_head", "=", "tf", ".", "reshape", "(", "\n", "token_scores_head", ",", "shape", "=", "tf", ".", "shape", "(", "self", ".", "word_ids", ")", ")", "# [B, M]", "\n", "token_scores_list", ".", "append", "(", "token_scores_head", ")", "\n", "\n", "if", "self", ".", "config", "[", "\"attention_activation\"", "]", "==", "\"sharp\"", ":", "\n", "                        ", "attention_weights_unnormalized", "=", "tf", ".", "exp", "(", "token_scores_head", ")", "\n", "", "elif", "self", ".", "config", "[", "\"attention_activation\"", "]", "==", "\"soft\"", ":", "\n", "                        ", "attention_weights_unnormalized", "=", "tf", ".", "sigmoid", "(", "token_scores_head", ")", "\n", "", "elif", "self", ".", "config", "[", "\"attention_activation\"", "]", "==", "\"linear\"", ":", "\n", "                        ", "attention_weights_unnormalized", "=", "token_scores_head", "\n", "", "else", ":", "\n", "                        ", "raise", "ValueError", "(", "\"Unknown/unsupported token scoring method: %s\"", "\n", "%", "self", ".", "config", "[", "\"attention_activation\"", "]", ")", "\n", "", "attention_weights_unnormalized", "=", "tf", ".", "where", "(", "\n", "tf", ".", "sequence_mask", "(", "self", ".", "sentence_lengths", ")", ",", "\n", "attention_weights_unnormalized", ",", "\n", "tf", ".", "zeros_like", "(", "attention_weights_unnormalized", ")", ")", "\n", "attention_weights", "=", "attention_weights_unnormalized", "/", "tf", ".", "reduce_sum", "(", "\n", "attention_weights_unnormalized", ",", "axis", "=", "1", ",", "keep_dims", "=", "True", ")", "# [B, M]", "\n", "processed_tensor", "=", "tf", ".", "reduce_sum", "(", "\n", "values", "*", "attention_weights", "[", ":", ",", ":", ",", "numpy", ".", "newaxis", "]", ",", "axis", "=", "1", ")", "# [B, E]", "\n", "\n", "if", "self", ".", "config", "[", "\"hidden_layer_size\"", "]", ">", "0", ":", "\n", "                        ", "processed_tensor", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "processed_tensor", ",", "units", "=", "self", ".", "config", "[", "\"hidden_layer_size\"", "]", ",", "\n", "activation", "=", "tf", ".", "tanh", ",", "kernel_initializer", "=", "self", ".", "initializer", ")", "\n", "\n", "", "sentence_score_head", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "processed_tensor", ",", "units", "=", "1", ",", "\n", "kernel_initializer", "=", "self", ".", "initializer", ",", "\n", "name", "=", "\"output_ff_head_%d\"", "%", "i", ")", "# [B, 1]", "\n", "sentence_score_head", "=", "tf", ".", "reshape", "(", "\n", "sentence_score_head", ",", "shape", "=", "[", "tf", ".", "shape", "(", "processed_tensor", ")", "[", "0", "]", "]", ")", "# [B]", "\n", "sentence_scores_list", ".", "append", "(", "sentence_score_head", ")", "\n", "\n", "", "token_scores", "=", "tf", ".", "stack", "(", "token_scores_list", ",", "axis", "=", "-", "1", ")", "# [B, M, H]", "\n", "all_sentence_scores", "=", "tf", ".", "stack", "(", "sentence_scores_list", ",", "axis", "=", "-", "1", ")", "# [B, H]", "\n", "\n", "if", "len", "(", "self", ".", "label2id_tok", ")", "!=", "len", "(", "self", ".", "label2id_sent", ")", ":", "\n", "                    ", "if", "len", "(", "self", ".", "label2id_sent", ")", "==", "2", ":", "\n", "                        ", "default_sentence_score", "=", "tf", ".", "gather", "(", "\n", "all_sentence_scores", ",", "indices", "=", "[", "0", "]", ",", "axis", "=", "1", ")", "# [B, 1]", "\n", "maximum_non_default_sentence_score", "=", "tf", ".", "gather", "(", "\n", "all_sentence_scores", ",", "indices", "=", "list", "(", "\n", "range", "(", "1", ",", "len", "(", "self", ".", "label2id_tok", ")", ")", ")", ",", "axis", "=", "1", ")", "# [B, num_heads-1]", "\n", "maximum_non_default_sentence_score", "=", "tf", ".", "reduce_max", "(", "\n", "maximum_non_default_sentence_score", ",", "axis", "=", "1", ",", "keep_dims", "=", "True", ")", "# [B, 1]", "\n", "sentence_scores", "=", "tf", ".", "concat", "(", "\n", "[", "default_sentence_score", ",", "maximum_non_default_sentence_score", "]", ",", "\n", "axis", "=", "-", "1", ",", "name", "=", "\"sentence_scores_concatenation\"", ")", "# [B, 2]", "\n", "", "else", ":", "\n", "                        ", "sentence_scores", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "all_sentence_scores", ",", "units", "=", "len", "(", "self", ".", "label2id_sent", ")", ",", "\n", "kernel_initializer", "=", "self", ".", "initializer", ")", "# [B, num_sent_labels]", "\n", "", "", "else", ":", "\n", "                    ", "sentence_scores", "=", "all_sentence_scores", "\n", "\n", "# Mask the token scores that do not fall in the range of the true sentence length.", "\n", "# Do this for each head (change shape from [B, M] to [B, M, num_heads]).", "\n", "", "", "", "tiled_sentence_lengths", "=", "tf", ".", "tile", "(", "\n", "input", "=", "tf", ".", "expand_dims", "(", "\n", "tf", ".", "sequence_mask", "(", "self", ".", "sentence_lengths", ")", ",", "axis", "=", "-", "1", ")", ",", "\n", "multiples", "=", "[", "1", ",", "1", ",", "len", "(", "self", ".", "label2id_tok", ")", "]", ")", "\n", "self", ".", "token_probabilities", "=", "tf", ".", "nn", ".", "softmax", "(", "token_scores", ",", "axis", "=", "-", "1", ")", "\n", "self", ".", "token_probabilities", "=", "tf", ".", "where", "(", "\n", "tiled_sentence_lengths", ",", "\n", "self", ".", "token_probabilities", ",", "\n", "tf", ".", "zeros_like", "(", "self", ".", "token_probabilities", ")", ")", "\n", "self", ".", "token_predictions", "=", "tf", ".", "argmax", "(", "self", ".", "token_probabilities", ",", "axis", "=", "2", ")", "\n", "\n", "self", ".", "sentence_probabilities", "=", "tf", ".", "nn", ".", "softmax", "(", "sentence_scores", ")", "\n", "self", ".", "sentence_predictions", "=", "tf", ".", "argmax", "(", "self", ".", "sentence_probabilities", ",", "axis", "=", "1", ")", "\n", "\n", "if", "self", ".", "config", "[", "\"word_objective_weight\"", "]", ">", "0", ":", "\n", "            ", "word_objective_loss", "=", "tf", ".", "nn", ".", "sparse_softmax_cross_entropy_with_logits", "(", "\n", "logits", "=", "token_scores", ",", "labels", "=", "tf", ".", "cast", "(", "self", ".", "word_labels", ",", "tf", ".", "int32", ")", ")", "\n", "word_objective_loss", "=", "tf", ".", "where", "(", "\n", "tf", ".", "sequence_mask", "(", "self", ".", "sentence_lengths", ")", ",", "\n", "word_objective_loss", ",", "\n", "tf", ".", "zeros_like", "(", "word_objective_loss", ")", ")", "\n", "self", ".", "loss", "+=", "self", ".", "config", "[", "\"word_objective_weight\"", "]", "*", "tf", ".", "reduce_sum", "(", "\n", "self", ".", "word_objective_weights", "*", "word_objective_loss", ")", "\n", "\n", "", "if", "self", ".", "config", "[", "\"sentence_objective_weight\"", "]", ">", "0", ":", "\n", "            ", "self", ".", "loss", "+=", "self", ".", "config", "[", "\"sentence_objective_weight\"", "]", "*", "tf", ".", "reduce_sum", "(", "\n", "self", ".", "sentence_objective_weights", "*", "\n", "tf", ".", "nn", ".", "sparse_softmax_cross_entropy_with_logits", "(", "\n", "logits", "=", "sentence_scores", ",", "labels", "=", "tf", ".", "cast", "(", "self", ".", "sentence_labels", ",", "tf", ".", "int32", ")", ")", ")", "\n", "\n", "", "max_over_token_heads", "=", "tf", ".", "reduce_max", "(", "self", ".", "token_probabilities", ",", "axis", "=", "1", ")", "# [B, H]", "\n", "one_hot_sentence_labels", "=", "tf", ".", "one_hot", "(", "\n", "tf", ".", "cast", "(", "self", ".", "sentence_labels", ",", "tf", ".", "int32", ")", ",", "\n", "depth", "=", "len", "(", "self", ".", "label2id_sent", ")", ")", "\n", "if", "self", ".", "config", "[", "\"enable_label_smoothing\"", "]", ":", "\n", "            ", "one_hot_sentence_labels_smoothed", "=", "label_smoothing", "(", "\n", "one_hot_sentence_labels", ",", "epsilon", "=", "self", ".", "config", "[", "\"smoothing_epsilon\"", "]", ")", "\n", "", "else", ":", "\n", "            ", "one_hot_sentence_labels_smoothed", "=", "one_hot_sentence_labels", "\n", "\n", "# At least one token has a label corresponding to the true sentence label.", "\n", "# This loss also pushes the maximums over the other heads towards 0 (but smoothed).", "\n", "", "if", "self", ".", "config", "[", "\"type1_attention_objective_weight\"", "]", ">", "0", ":", "\n", "            ", "this_max_over_token_heads", "=", "max_over_token_heads", "\n", "if", "len", "(", "self", ".", "label2id_tok", ")", "!=", "len", "(", "self", ".", "label2id_sent", ")", ":", "\n", "                ", "if", "len", "(", "self", ".", "label2id_sent", ")", "==", "2", ":", "\n", "                    ", "max_default_head", "=", "tf", ".", "gather", "(", "\n", "max_over_token_heads", ",", "indices", "=", "[", "0", "]", ",", "axis", "=", "-", "1", ")", "# [B, 1]", "\n", "max_non_default_head", "=", "tf", ".", "reduce_max", "(", "tf", ".", "gather", "(", "\n", "max_over_token_heads", ",", "indices", "=", "list", "(", "\n", "range", "(", "1", ",", "len", "(", "self", ".", "label2id_tok", ")", ")", ")", ",", "axis", "=", "-", "1", ")", ",", "\n", "axis", "=", "1", ",", "keep_dims", "=", "True", ")", "# [B, 1]", "\n", "this_max_over_token_heads", "=", "tf", ".", "concat", "(", "\n", "[", "max_default_head", ",", "max_non_default_head", "]", ",", "axis", "=", "-", "1", ")", "# [B, 2]", "\n", "", "else", ":", "\n", "                    ", "raise", "ValueError", "(", "\n", "\"Unsupported attention loss for num_heads != num_sent_lables \"", "\n", "\"and num_sentence_labels != 2.\"", ")", "\n", "", "", "self", ".", "loss", "+=", "self", ".", "config", "[", "\"type1_attention_objective_weight\"", "]", "*", "(", "\n", "tf", ".", "reduce_sum", "(", "self", ".", "sentence_objective_weights", "*", "tf", ".", "reduce_sum", "(", "tf", ".", "square", "(", "\n", "this_max_over_token_heads", "-", "one_hot_sentence_labels_smoothed", ")", ",", "axis", "=", "-", "1", ")", ")", ")", "\n", "\n", "# The predicted distribution over the token labels (heads) should be similar to the", "\n", "# predicted distribution over the sentence representations.", "\n", "", "if", "self", ".", "config", "[", "\"type2_attention_objective_weight\"", "]", ">", "0", ":", "\n", "            ", "all_sentence_scores_probabilities", "=", "tf", ".", "nn", ".", "softmax", "(", "all_sentence_scores", ")", "# [B, H]", "\n", "self", ".", "loss", "+=", "self", ".", "config", "[", "\"type2_attention_objective_weight\"", "]", "*", "(", "\n", "tf", ".", "reduce_sum", "(", "self", ".", "sentence_objective_weights", "*", "tf", ".", "reduce_sum", "(", "tf", ".", "square", "(", "\n", "max_over_token_heads", "-", "all_sentence_scores_probabilities", ")", ",", "axis", "=", "-", "1", ")", ")", ")", "\n", "\n", "# At least one token has a label corresponding to the true sentence label.", "\n", "", "if", "self", ".", "config", "[", "\"type3_attention_objective_weight\"", "]", ">", "0", ":", "\n", "            ", "this_max_over_token_heads", "=", "max_over_token_heads", "\n", "if", "len", "(", "self", ".", "label2id_tok", ")", "!=", "len", "(", "self", ".", "label2id_sent", ")", ":", "\n", "                ", "if", "len", "(", "self", ".", "label2id_sent", ")", "==", "2", ":", "\n", "                    ", "max_default_head", "=", "tf", ".", "gather", "(", "\n", "max_over_token_heads", ",", "indices", "=", "[", "0", "]", ",", "axis", "=", "-", "1", ")", "# [B, 1]", "\n", "max_non_default_head", "=", "tf", ".", "reduce_max", "(", "tf", ".", "gather", "(", "\n", "max_over_token_heads", ",", "indices", "=", "list", "(", "\n", "range", "(", "1", ",", "len", "(", "self", ".", "label2id_tok", ")", ")", ")", ",", "axis", "=", "-", "1", ")", ",", "\n", "axis", "=", "1", ",", "keep_dims", "=", "True", ")", "# [B, 1]", "\n", "this_max_over_token_heads", "=", "tf", ".", "concat", "(", "\n", "[", "max_default_head", ",", "max_non_default_head", "]", ",", "axis", "=", "-", "1", ")", "# [B, 2]", "\n", "", "else", ":", "\n", "                    ", "raise", "ValueError", "(", "\n", "\"Unsupported attention loss for num_heads != num_sent_lables \"", "\n", "\"and num_sentence_labels != 2.\"", ")", "\n", "", "", "self", ".", "loss", "+=", "self", ".", "config", "[", "\"type3_attention_objective_weight\"", "]", "*", "(", "\n", "tf", ".", "reduce_sum", "(", "self", ".", "sentence_objective_weights", "*", "tf", ".", "reduce_sum", "(", "tf", ".", "square", "(", "\n", "(", "this_max_over_token_heads", "*", "one_hot_sentence_labels", ")", "\n", "-", "one_hot_sentence_labels_smoothed", ")", ",", "axis", "=", "-", "1", ")", ")", ")", "\n", "\n", "# A sentence that has a default label, should only contain tokens labeled as default.", "\n", "", "if", "self", ".", "config", "[", "\"type4_attention_objective_weight\"", "]", ">", "0", ":", "\n", "            ", "default_head", "=", "tf", ".", "gather", "(", "self", ".", "token_probabilities", ",", "indices", "=", "[", "0", "]", ",", "axis", "=", "-", "1", ")", "# [B, M, 1]", "\n", "default_head", "=", "tf", ".", "squeeze", "(", "default_head", ",", "axis", "=", "-", "1", ")", "# [B, M]", "\n", "self", ".", "loss", "+=", "self", ".", "config", "[", "\"type4_attention_objective_weight\"", "]", "*", "(", "\n", "tf", ".", "reduce_sum", "(", "self", ".", "sentence_objective_weights", "*", "tf", ".", "cast", "(", "\n", "tf", ".", "equal", "(", "self", ".", "sentence_labels", ",", "0.0", ")", ",", "tf", ".", "float32", ")", "*", "tf", ".", "reduce_sum", "(", "\n", "tf", ".", "square", "(", "default_head", "-", "tf", ".", "ones_like", "(", "default_head", ")", ")", ",", "axis", "=", "-", "1", ")", ")", ")", "\n", "\n", "# Every sentence has at least one default label.", "\n", "", "if", "self", ".", "config", "[", "\"type5_attention_objective_weight\"", "]", ">", "0", ":", "\n", "            ", "default_head", "=", "tf", ".", "gather", "(", "self", ".", "token_probabilities", ",", "indices", "=", "[", "0", "]", ",", "axis", "=", "-", "1", ")", "# [B, M, 1]", "\n", "max_default_head", "=", "tf", ".", "reduce_max", "(", "tf", ".", "squeeze", "(", "default_head", ",", "axis", "=", "-", "1", ")", ",", "axis", "=", "-", "1", ")", "# [B]", "\n", "self", ".", "loss", "+=", "self", ".", "config", "[", "\"type5_attention_objective_weight\"", "]", "*", "(", "\n", "tf", ".", "reduce_sum", "(", "self", ".", "sentence_objective_weights", "*", "tf", ".", "square", "(", "\n", "max_default_head", "-", "tf", ".", "ones_like", "(", "max_default_head", ")", ")", ")", ")", "\n", "\n", "# Include a word-based language modelling loss, LMw.", "\n", "", "if", "self", ".", "config", "[", "\"lm_cost_lstm_gamma\"", "]", ">", "0.0", ":", "\n", "            ", "self", ".", "loss", "+=", "self", ".", "config", "[", "\"lm_cost_lstm_gamma\"", "]", "*", "self", ".", "construct_lm_cost", "(", "\n", "input_tensor_fw", "=", "lstm_outputs_fw", ",", "\n", "input_tensor_bw", "=", "lstm_outputs_bw", ",", "\n", "sentence_lengths", "=", "self", ".", "sentence_lengths", ",", "\n", "target_ids", "=", "self", ".", "word_ids", ",", "\n", "lm_cost_type", "=", "\"separate\"", ",", "\n", "name", "=", "\"lm_cost_lstm_separate\"", ")", "\n", "\n", "", "if", "self", ".", "config", "[", "\"lm_cost_joint_lstm_gamma\"", "]", ">", "0.0", ":", "\n", "            ", "self", ".", "loss", "+=", "self", ".", "config", "[", "\"lm_cost_joint_lstm_gamma\"", "]", "*", "self", ".", "construct_lm_cost", "(", "\n", "input_tensor_fw", "=", "lstm_outputs_fw", ",", "\n", "input_tensor_bw", "=", "lstm_outputs_bw", ",", "\n", "sentence_lengths", "=", "self", ".", "sentence_lengths", ",", "\n", "target_ids", "=", "self", ".", "word_ids", ",", "\n", "lm_cost_type", "=", "\"joint\"", ",", "\n", "name", "=", "\"lm_cost_lstm_joint\"", ")", "\n", "\n", "", "self", ".", "train_op", "=", "self", ".", "construct_optimizer", "(", "\n", "opt_strategy", "=", "self", ".", "config", "[", "\"opt_strategy\"", "]", ",", "\n", "loss", "=", "self", ".", "loss", ",", "\n", "learning_rate", "=", "self", ".", "learning_rate", ",", "\n", "clip", "=", "self", ".", "config", "[", "\"clip\"", "]", ")", "\n", "print", "(", "\"Notwork built.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.second_model.Model.construct_lm_cost": [[508, 554], ["tensorflow.variable_scope", "min", "tensorflow.where", "len", "tensorflow.greater_equal", "second_model.Model._construct_lm_cost", "second_model.Model._construct_lm_cost", "tensorflow.sequence_mask", "tensorflow.sequence_mask", "tensorflow.concat", "second_model.Model._construct_lm_cost", "ValueError", "tensorflow.zeros_like", "tensorflow.sequence_mask", "tensorflow.shape", "tensorflow.shape", "tensorflow.shape"], "methods", ["home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model._construct_lm_cost", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model._construct_lm_cost", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model._construct_lm_cost"], ["", "def", "construct_lm_cost", "(", "\n", "self", ",", "input_tensor_fw", ",", "input_tensor_bw", ",", "\n", "sentence_lengths", ",", "target_ids", ",", "lm_cost_type", ",", "name", ")", ":", "\n", "        ", "\"\"\"\n        Constructs the char/word-based language modelling objective.\n        \"\"\"", "\n", "with", "tf", ".", "variable_scope", "(", "name", ")", ":", "\n", "            ", "lm_cost_max_vocab_size", "=", "min", "(", "\n", "len", "(", "self", ".", "word2id", ")", ",", "self", ".", "config", "[", "\"lm_cost_max_vocab_size\"", "]", ")", "\n", "target_ids", "=", "tf", ".", "where", "(", "\n", "tf", ".", "greater_equal", "(", "target_ids", ",", "lm_cost_max_vocab_size", "-", "1", ")", ",", "\n", "x", "=", "(", "lm_cost_max_vocab_size", "-", "1", ")", "+", "tf", ".", "zeros_like", "(", "target_ids", ")", ",", "\n", "y", "=", "target_ids", ")", "\n", "cost", "=", "0.0", "\n", "if", "lm_cost_type", "==", "\"separate\"", ":", "\n", "                ", "lm_cost_fw_mask", "=", "tf", ".", "sequence_mask", "(", "\n", "sentence_lengths", ",", "maxlen", "=", "tf", ".", "shape", "(", "target_ids", ")", "[", "1", "]", ")", "[", ":", ",", "1", ":", "]", "\n", "lm_cost_bw_mask", "=", "tf", ".", "sequence_mask", "(", "\n", "sentence_lengths", ",", "maxlen", "=", "tf", ".", "shape", "(", "target_ids", ")", "[", "1", "]", ")", "[", ":", ",", ":", "-", "1", "]", "\n", "lm_cost_fw", "=", "self", ".", "_construct_lm_cost", "(", "\n", "input_tensor_fw", "[", ":", ",", ":", "-", "1", ",", ":", "]", ",", "\n", "lm_cost_max_vocab_size", ",", "\n", "lm_cost_fw_mask", ",", "\n", "target_ids", "[", ":", ",", "1", ":", "]", ",", "\n", "name", "=", "name", "+", "\"_fw\"", ")", "\n", "lm_cost_bw", "=", "self", ".", "_construct_lm_cost", "(", "\n", "input_tensor_bw", "[", ":", ",", "1", ":", ",", ":", "]", ",", "\n", "lm_cost_max_vocab_size", ",", "\n", "lm_cost_bw_mask", ",", "\n", "target_ids", "[", ":", ",", ":", "-", "1", "]", ",", "\n", "name", "=", "name", "+", "\"_bw\"", ")", "\n", "cost", "+=", "lm_cost_fw", "+", "lm_cost_bw", "\n", "", "elif", "lm_cost_type", "==", "\"joint\"", ":", "\n", "                ", "joint_input_tensor", "=", "tf", ".", "concat", "(", "\n", "[", "input_tensor_fw", "[", ":", ",", ":", "-", "2", ",", ":", "]", ",", "input_tensor_bw", "[", ":", ",", "2", ":", ",", ":", "]", "]", ",", "axis", "=", "-", "1", ")", "\n", "lm_cost_mask", "=", "tf", ".", "sequence_mask", "(", "\n", "sentence_lengths", ",", "maxlen", "=", "tf", ".", "shape", "(", "target_ids", ")", "[", "1", "]", ")", "[", ":", ",", "1", ":", "-", "1", "]", "\n", "cost", "+=", "self", ".", "_construct_lm_cost", "(", "\n", "joint_input_tensor", ",", "\n", "lm_cost_max_vocab_size", ",", "\n", "lm_cost_mask", ",", "\n", "target_ids", "[", ":", ",", "1", ":", "-", "1", "]", ",", "\n", "name", "=", "name", "+", "\"_joint\"", ")", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"Unknown lm_cost_type: %s.\"", "%", "lm_cost_type", ")", "\n", "", "return", "cost", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.second_model.Model._construct_lm_cost": [[555, 569], ["tensorflow.variable_scope", "tensorflow.layers.dense", "tensorflow.layers.dense", "tensorflow.nn.sparse_softmax_cross_entropy_with_logits", "tensorflow.where", "tensorflow.reduce_sum", "tensorflow.zeros_like"], "methods", ["None"], ["", "", "def", "_construct_lm_cost", "(", "\n", "self", ",", "input_tensor", ",", "lm_cost_max_vocab_size", ",", "\n", "lm_cost_mask", ",", "target_ids", ",", "name", ")", ":", "\n", "        ", "with", "tf", ".", "variable_scope", "(", "name", ")", ":", "\n", "            ", "lm_cost_hidden_layer", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", "=", "input_tensor", ",", "units", "=", "self", ".", "config", "[", "\"lm_cost_hidden_layer_size\"", "]", ",", "\n", "activation", "=", "tf", ".", "tanh", ",", "kernel_initializer", "=", "self", ".", "initializer", ")", "\n", "lm_cost_output", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", "=", "lm_cost_hidden_layer", ",", "units", "=", "lm_cost_max_vocab_size", ",", "\n", "kernel_initializer", "=", "self", ".", "initializer", ")", "\n", "lm_cost_loss", "=", "tf", ".", "nn", ".", "sparse_softmax_cross_entropy_with_logits", "(", "\n", "logits", "=", "lm_cost_output", ",", "labels", "=", "target_ids", ")", "\n", "lm_cost_loss", "=", "tf", ".", "where", "(", "lm_cost_mask", ",", "lm_cost_loss", ",", "tf", ".", "zeros_like", "(", "lm_cost_loss", ")", ")", "\n", "return", "tf", ".", "reduce_sum", "(", "lm_cost_loss", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.second_model.Model.construct_optimizer": [[570, 591], ["tensorflow.train.AdadeltaOptimizer", "zip", "tensorflow.clip_by_global_norm", "tensorflow.train.GradientDescentOptimizer.apply_gradients", "tensorflow.train.GradientDescentOptimizer.minimize", "tensorflow.train.AdamOptimizer", "zip", "tensorflow.train.GradientDescentOptimizer", "ValueError", "tensorflow.train.GradientDescentOptimizer.compute_gradients"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "construct_optimizer", "(", "opt_strategy", ",", "loss", ",", "learning_rate", ",", "clip", ")", ":", "\n", "        ", "\"\"\"\n        Applies an optimization strategy to minimize the loss.\n        \"\"\"", "\n", "if", "opt_strategy", "==", "\"adadelta\"", ":", "\n", "            ", "optimizer", "=", "tf", ".", "train", ".", "AdadeltaOptimizer", "(", "learning_rate", "=", "learning_rate", ")", "\n", "", "elif", "opt_strategy", "==", "\"adam\"", ":", "\n", "            ", "optimizer", "=", "tf", ".", "train", ".", "AdamOptimizer", "(", "learning_rate", "=", "learning_rate", ")", "\n", "", "elif", "opt_strategy", "==", "\"sgd\"", ":", "\n", "            ", "optimizer", "=", "tf", ".", "train", ".", "GradientDescentOptimizer", "(", "learning_rate", "=", "learning_rate", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unknown optimisation strategy: %s.\"", "%", "opt_strategy", ")", "\n", "\n", "", "if", "clip", ">", "0.0", ":", "\n", "            ", "grads", ",", "vs", "=", "zip", "(", "*", "optimizer", ".", "compute_gradients", "(", "loss", ")", ")", "\n", "grads", ",", "gnorm", "=", "tf", ".", "clip_by_global_norm", "(", "grads", ",", "clip", ")", "\n", "train_op", "=", "optimizer", ".", "apply_gradients", "(", "zip", "(", "grads", ",", "vs", ")", ")", "\n", "", "else", ":", "\n", "            ", "train_op", "=", "optimizer", ".", "minimize", "(", "loss", ")", "\n", "", "return", "train_op", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.second_model.Model.preload_word_embeddings": [[592, 616], ["set", "second_model.Model.session.run", "second_model.Model.session.run", "print", "open", "second_model.Model.word_embeddings.assign", "line.strip().split", "len", "len", "re.sub.lower", "re.sub", "numpy.array", "set.add", "line.strip"], "methods", ["None"], ["", "def", "preload_word_embeddings", "(", "self", ",", "embedding_path", ")", ":", "\n", "        ", "\"\"\"\n        Load the word embeddings in advance to get a feel\n        of the proportion of singletons in the dataset.\n        \"\"\"", "\n", "loaded_embeddings", "=", "set", "(", ")", "\n", "embedding_matrix", "=", "self", ".", "session", ".", "run", "(", "self", ".", "word_embeddings", ")", "\n", "with", "open", "(", "embedding_path", ")", "as", "f", ":", "\n", "            ", "for", "line", "in", "f", ":", "\n", "                ", "line_parts", "=", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "if", "len", "(", "line_parts", ")", "<=", "2", ":", "\n", "                    ", "continue", "\n", "", "w", "=", "line_parts", "[", "0", "]", "\n", "if", "self", ".", "config", "[", "\"lowercase\"", "]", ":", "\n", "                    ", "w", "=", "w", ".", "lower", "(", ")", "\n", "", "if", "self", ".", "config", "[", "\"replace_digits\"", "]", ":", "\n", "                    ", "w", "=", "re", ".", "sub", "(", "r'\\d'", ",", "'0'", ",", "w", ")", "\n", "", "if", "w", "in", "self", ".", "word2id", "and", "w", "not", "in", "loaded_embeddings", ":", "\n", "                    ", "word_id", "=", "self", ".", "word2id", "[", "w", "]", "\n", "embedding", "=", "numpy", ".", "array", "(", "line_parts", "[", "1", ":", "]", ")", "\n", "embedding_matrix", "[", "word_id", "]", "=", "embedding", "\n", "loaded_embeddings", ".", "add", "(", "w", ")", "\n", "", "", "", "self", ".", "session", ".", "run", "(", "self", ".", "word_embeddings", ".", "assign", "(", "embedding_matrix", ")", ")", "\n", "print", "(", "\"No. of pre-loaded embeddings: %d.\"", "%", "len", "(", "loaded_embeddings", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.second_model.Model.translate2id": [[617, 640], ["re.sub.lower", "re.sub", "numpy.random.uniform", "ValueError"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "translate2id", "(", "\n", "token", ",", "token2id", ",", "unk_token", "=", "None", ",", "lowercase", "=", "False", ",", "\n", "replace_digits", "=", "False", ",", "singletons", "=", "None", ",", "singletons_prob", "=", "0.0", ")", ":", "\n", "        ", "\"\"\"\n        Maps each token/character to its index.\n        \"\"\"", "\n", "if", "lowercase", ":", "\n", "            ", "token", "=", "token", ".", "lower", "(", ")", "\n", "", "if", "replace_digits", ":", "\n", "            ", "token", "=", "re", ".", "sub", "(", "r'\\d'", ",", "'0'", ",", "token", ")", "\n", "\n", "", "if", "singletons", "and", "token", "in", "singletons", "and", "token", "in", "token2id", "and", "unk_token", "and", "numpy", ".", "random", ".", "uniform", "(", ")", "<", "singletons_prob", ":", "\n", "            ", "token_id", "=", "token2id", "[", "unk_token", "]", "\n", "", "elif", "token", "in", "token2id", ":", "\n", "            ", "token_id", "=", "token2id", "[", "token", "]", "\n", "", "elif", "unk_token", ":", "\n", "            ", "token_id", "=", "token2id", "[", "unk_token", "]", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unable to handle value, no UNK token: %s.\"", "%", "token", ")", "\n", "", "return", "token_id", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.second_model.Model.create_input_dictionary_for_batch": [[641, 715], ["numpy.array", "numpy.array.max", "numpy.array().max", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "enumerate", "min", "len", "len", "enumerate", "len", "numpy.array", "len", "len", "len", "len", "len", "second_model.Model.translate2id", "len", "range", "min", "second_model.Model.translate2id", "numpy.array().max", "len", "numpy.array", "len"], "methods", ["home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.translate2id", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.translate2id"], ["", "def", "create_input_dictionary_for_batch", "(", "self", ",", "batch", ",", "is_training", ",", "learning_rate", ")", ":", "\n", "        ", "\"\"\"\n        Creates the dictionary fed to the the TF model.\n        \"\"\"", "\n", "sentence_lengths", "=", "numpy", ".", "array", "(", "[", "len", "(", "sentence", ".", "tokens", ")", "for", "sentence", "in", "batch", "]", ")", "\n", "max_sentence_length", "=", "sentence_lengths", ".", "max", "(", ")", "\n", "max_word_length", "=", "numpy", ".", "array", "(", "\n", "[", "numpy", ".", "array", "(", "[", "len", "(", "token", ".", "value", ")", "for", "token", "in", "sentence", ".", "tokens", "]", ")", ".", "max", "(", ")", "\n", "for", "sentence", "in", "batch", "]", ")", ".", "max", "(", ")", "\n", "\n", "if", "0", "<", "self", ".", "config", "[", "\"allowed_word_length\"", "]", "<", "max_word_length", ":", "\n", "            ", "max_word_length", "=", "min", "(", "max_word_length", ",", "self", ".", "config", "[", "\"allowed_word_length\"", "]", ")", "\n", "\n", "", "word_ids", "=", "numpy", ".", "zeros", "(", "\n", "(", "len", "(", "batch", ")", ",", "max_sentence_length", ")", ",", "dtype", "=", "numpy", ".", "int32", ")", "\n", "char_ids", "=", "numpy", ".", "zeros", "(", "\n", "(", "len", "(", "batch", ")", ",", "max_sentence_length", ",", "max_word_length", ")", ",", "dtype", "=", "numpy", ".", "int32", ")", "\n", "word_lengths", "=", "numpy", ".", "zeros", "(", "\n", "(", "len", "(", "batch", ")", ",", "max_sentence_length", ")", ",", "dtype", "=", "numpy", ".", "int32", ")", "\n", "word_labels", "=", "numpy", ".", "zeros", "(", "\n", "(", "len", "(", "batch", ")", ",", "max_sentence_length", ")", ",", "dtype", "=", "numpy", ".", "float32", ")", "\n", "sentence_labels", "=", "numpy", ".", "zeros", "(", "\n", "(", "len", "(", "batch", ")", ")", ",", "dtype", "=", "numpy", ".", "float32", ")", "\n", "word_objective_weights", "=", "numpy", ".", "zeros", "(", "\n", "(", "len", "(", "batch", ")", ",", "max_sentence_length", ")", ",", "dtype", "=", "numpy", ".", "float32", ")", "\n", "sentence_objective_weights", "=", "numpy", ".", "zeros", "(", "(", "len", "(", "batch", ")", ")", ",", "dtype", "=", "numpy", ".", "float32", ")", "\n", "\n", "# A proportion of the singletons are assigned to UNK (do this just for training).", "\n", "singletons", "=", "self", ".", "singletons", "if", "is_training", "else", "None", "\n", "singletons_prob", "=", "self", ".", "config", "[", "\"singletons_prob\"", "]", "if", "is_training", "else", "0.0", "\n", "\n", "for", "i", ",", "sentence", "in", "enumerate", "(", "batch", ")", ":", "\n", "            ", "sentence_labels", "[", "i", "]", "=", "sentence", ".", "label_sent", "\n", "\n", "if", "sentence_labels", "[", "i", "]", "!=", "0", ":", "\n", "                ", "if", "self", ".", "config", "[", "\"sentence_objective_weights_non_default\"", "]", ">", "0.0", ":", "\n", "                    ", "sentence_objective_weights", "[", "i", "]", "=", "self", ".", "config", "[", "\n", "\"sentence_objective_weights_non_default\"", "]", "\n", "", "else", ":", "\n", "                    ", "sentence_objective_weights", "[", "i", "]", "=", "1.0", "\n", "", "", "else", ":", "\n", "                ", "sentence_objective_weights", "[", "i", "]", "=", "1.0", "\n", "\n", "", "for", "j", ",", "token", "in", "enumerate", "(", "sentence", ".", "tokens", ")", ":", "\n", "                ", "word_ids", "[", "i", "]", "[", "j", "]", "=", "self", ".", "translate2id", "(", "\n", "token", "=", "token", ".", "value", ",", "\n", "token2id", "=", "self", ".", "word2id", ",", "\n", "unk_token", "=", "self", ".", "UNK", ",", "\n", "lowercase", "=", "self", ".", "config", "[", "\"lowercase\"", "]", ",", "\n", "replace_digits", "=", "self", ".", "config", "[", "\"replace_digits\"", "]", ",", "\n", "singletons", "=", "singletons", ",", "\n", "singletons_prob", "=", "singletons_prob", ")", "\n", "word_labels", "[", "i", "]", "[", "j", "]", "=", "token", ".", "label_tok", "\n", "word_lengths", "[", "i", "]", "[", "j", "]", "=", "len", "(", "token", ".", "value", ")", "\n", "for", "k", "in", "range", "(", "min", "(", "len", "(", "token", ".", "value", ")", ",", "max_word_length", ")", ")", ":", "\n", "                    ", "char_ids", "[", "i", "]", "[", "j", "]", "[", "k", "]", "=", "self", ".", "translate2id", "(", "\n", "token", "=", "token", ".", "value", "[", "k", "]", ",", "\n", "token2id", "=", "self", ".", "char2id", ",", "\n", "unk_token", "=", "self", ".", "CUNK", ")", "\n", "", "if", "token", ".", "enable_supervision", "is", "True", ":", "\n", "                    ", "word_objective_weights", "[", "i", "]", "[", "j", "]", "=", "1.0", "\n", "\n", "", "", "", "input_dictionary", "=", "{", "\n", "self", ".", "word_ids", ":", "word_ids", ",", "\n", "self", ".", "char_ids", ":", "char_ids", ",", "\n", "self", ".", "sentence_lengths", ":", "sentence_lengths", ",", "\n", "self", ".", "word_lengths", ":", "word_lengths", ",", "\n", "self", ".", "sentence_labels", ":", "sentence_labels", ",", "\n", "self", ".", "word_labels", ":", "word_labels", ",", "\n", "self", ".", "word_objective_weights", ":", "word_objective_weights", ",", "\n", "self", ".", "sentence_objective_weights", ":", "sentence_objective_weights", ",", "\n", "self", ".", "learning_rate", ":", "learning_rate", ",", "\n", "self", ".", "is_training", ":", "is_training", "}", "\n", "return", "input_dictionary", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.second_model.Model.process_batch": [[716, 731], ["second_model.Model.create_input_dictionary_for_batch", "second_model.Model.session.run"], "methods", ["home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.create_input_dictionary_for_batch"], ["", "def", "process_batch", "(", "self", ",", "batch", ",", "is_training", ",", "learning_rate", ")", ":", "\n", "        ", "\"\"\"\n        Processes a batch of sentences.\n        :param batch: a set of sentences of size \"max_batch_size\".\n        :param is_training: whether the current batch is a training instance or not.\n        :param learning_rate: the pace at which learning should be performed.\n        :return: the cost, the sentence predictions, the sentence label distribution,\n        the token predictions and the token label distribution.\n        \"\"\"", "\n", "feed_dict", "=", "self", ".", "create_input_dictionary_for_batch", "(", "batch", ",", "is_training", ",", "learning_rate", ")", "\n", "cost", ",", "sentence_pred", ",", "sentence_prob", ",", "token_pred", ",", "token_prob", "=", "self", ".", "session", ".", "run", "(", "\n", "[", "self", ".", "loss", ",", "self", ".", "sentence_predictions", ",", "self", ".", "sentence_probabilities", ",", "\n", "self", ".", "token_predictions", ",", "self", ".", "token_probabilities", "]", "+", "\n", "(", "[", "self", ".", "train_op", "]", "if", "is_training", "else", "[", "]", ")", ",", "feed_dict", "=", "feed_dict", ")", "[", ":", "5", "]", "\n", "return", "cost", ",", "sentence_pred", ",", "sentence_prob", ",", "token_pred", ",", "token_prob", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.second_model.Model.initialize_session": [[732, 744], ["tensorflow.set_random_seed", "tensorflow.ConfigProto", "tensorflow.Session", "second_model.Model.session.run", "tensorflow.train.Saver", "tensorflow.global_variables_initializer"], "methods", ["None"], ["", "def", "initialize_session", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Initializes a tensorflow session and sets the random seed.\n        \"\"\"", "\n", "tf", ".", "set_random_seed", "(", "self", ".", "config", "[", "\"random_seed\"", "]", ")", "\n", "session_config", "=", "tf", ".", "ConfigProto", "(", ")", "\n", "session_config", ".", "gpu_options", ".", "allow_growth", "=", "self", ".", "config", "[", "\"tf_allow_growth\"", "]", "\n", "session_config", ".", "gpu_options", ".", "per_process_gpu_memory_fraction", "=", "self", ".", "config", "[", "\n", "\"tf_per_process_gpu_memory_fraction\"", "]", "\n", "self", ".", "session", "=", "tf", ".", "Session", "(", "config", "=", "session_config", ")", "\n", "self", ".", "session", ".", "run", "(", "tf", ".", "global_variables_initializer", "(", ")", ")", "\n", "self", ".", "saver", "=", "tf", ".", "train", ".", "Saver", "(", "max_to_keep", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.second_model.Model.get_parameter_count": [[745, 758], ["tensorflow.trainable_variables", "variable.get_shape"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_parameter_count", "(", ")", ":", "\n", "        ", "\"\"\"\n        Counts the total number of parameters.\n        \"\"\"", "\n", "total_parameters", "=", "0", "\n", "for", "variable", "in", "tf", ".", "trainable_variables", "(", ")", ":", "\n", "            ", "shape", "=", "variable", ".", "get_shape", "(", ")", "\n", "variable_parameters", "=", "1", "\n", "for", "dim", "in", "shape", ":", "\n", "                ", "variable_parameters", "*=", "dim", ".", "value", "\n", "", "total_parameters", "+=", "variable_parameters", "\n", "", "return", "total_parameters", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.second_model.Model.get_parameter_count_without_word_embeddings": [[759, 768], ["second_model.Model.word_embeddings.get_shape", "second_model.Model.get_parameter_count"], "methods", ["home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.get_parameter_count"], ["", "def", "get_parameter_count_without_word_embeddings", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Counts the number of parameters without those introduced by word embeddings.\n        \"\"\"", "\n", "shape", "=", "self", ".", "word_embeddings", ".", "get_shape", "(", ")", "\n", "variable_parameters", "=", "1", "\n", "for", "dim", "in", "shape", ":", "\n", "            ", "variable_parameters", "*=", "dim", ".", "value", "\n", "", "return", "self", ".", "get_parameter_count", "(", ")", "-", "variable_parameters", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.second_model.Model.save": [[769, 791], ["dict", "tensorflow.global_variables", "second_model.Model.session.run", "open", "pickle.dump"], "methods", ["None"], ["", "def", "save", "(", "self", ",", "filename", ")", ":", "\n", "        ", "\"\"\"\n        Saves a trained model to the path in filename.\n        \"\"\"", "\n", "dump", "=", "dict", "(", ")", "\n", "dump", "[", "\"config\"", "]", "=", "self", ".", "config", "\n", "dump", "[", "\"label2id_sent\"", "]", "=", "self", ".", "label2id_sent", "\n", "dump", "[", "\"label2id_tok\"", "]", "=", "self", ".", "label2id_tok", "\n", "dump", "[", "\"UNK\"", "]", "=", "self", ".", "UNK", "\n", "dump", "[", "\"CUNK\"", "]", "=", "self", ".", "CUNK", "\n", "dump", "[", "\"word2id\"", "]", "=", "self", ".", "word2id", "\n", "dump", "[", "\"char2id\"", "]", "=", "self", ".", "char2id", "\n", "dump", "[", "\"singletons\"", "]", "=", "self", ".", "singletons", "\n", "\n", "dump", "[", "\"params\"", "]", "=", "{", "}", "\n", "for", "variable", "in", "tf", ".", "global_variables", "(", ")", ":", "\n", "            ", "assert", "(", "\n", "variable", ".", "name", "not", "in", "dump", "[", "\"params\"", "]", ")", ",", "\"Error: variable with this name already exists: %s.\"", "%", "variable", ".", "name", "\n", "dump", "[", "\"params\"", "]", "[", "variable", ".", "name", "]", "=", "self", ".", "session", ".", "run", "(", "variable", ")", "\n", "", "with", "open", "(", "filename", ",", "'wb'", ")", "as", "f", ":", "\n", "            ", "pickle", ".", "dump", "(", "dump", ",", "f", ",", "protocol", "=", "pickle", ".", "HIGHEST_PROTOCOL", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.second_model.Model.load": [[792, 819], ["open", "pickle.load", "second_model.Model", "second_model.Model.construct_network", "second_model.Model.initialize_session", "second_model.Model.load_params"], "methods", ["home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.load", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.construct_network", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.initialize_session", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.load_params"], ["", "", "@", "staticmethod", "\n", "def", "load", "(", "filename", ",", "new_config", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Loads a pre-trained MHAL model.\n        \"\"\"", "\n", "with", "open", "(", "filename", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "dump", "=", "pickle", ".", "load", "(", "f", ")", "\n", "dump", "[", "\"config\"", "]", "[", "\"save\"", "]", "=", "None", "\n", "\n", "# Use the saved config, except for values that are present in the new config.", "\n", "if", "new_config", ":", "\n", "                ", "for", "key", "in", "new_config", ":", "\n", "                    ", "dump", "[", "\"config\"", "]", "[", "key", "]", "=", "new_config", "[", "key", "]", "\n", "\n", "", "", "labeler", "=", "Model", "(", "dump", "[", "\"config\"", "]", ",", "dump", "[", "\"label2id_sent\"", "]", ",", "dump", "[", "\"label2id_tok\"", "]", ")", "\n", "labeler", ".", "UNK", "=", "dump", "[", "\"UNK\"", "]", "\n", "labeler", ".", "CUNK", "=", "dump", "[", "\"CUNK\"", "]", "\n", "labeler", ".", "word2id", "=", "dump", "[", "\"word2id\"", "]", "\n", "labeler", ".", "char2id", "=", "dump", "[", "\"char2id\"", "]", "\n", "labeler", ".", "singletons", "=", "dump", "[", "\"singletons\"", "]", "\n", "\n", "labeler", ".", "construct_network", "(", ")", "\n", "labeler", ".", "initialize_session", "(", ")", "\n", "\n", "labeler", ".", "load_params", "(", "filename", ")", "\n", "\n", "return", "labeler", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.second_model.Model.load_params": [[820, 836], ["open", "pickle.load", "tensorflow.global_variables", "numpy.asarray", "second_model.Model.session.run", "variable.assign", "str", "str"], "methods", ["home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.load"], ["", "", "def", "load_params", "(", "self", ",", "filename", ")", ":", "\n", "        ", "\"\"\"\n        Loads the parameters of a trained model.\n        \"\"\"", "\n", "with", "open", "(", "filename", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "dump", "=", "pickle", ".", "load", "(", "f", ")", "\n", "\n", "for", "variable", "in", "tf", ".", "global_variables", "(", ")", ":", "\n", "                ", "assert", "(", "variable", ".", "name", "in", "dump", "[", "\"params\"", "]", ")", ",", "\"Variable not in dump: %s.\"", "%", "variable", ".", "name", "\n", "assert", "(", "variable", ".", "shape", "==", "dump", "[", "\"params\"", "]", "[", "variable", ".", "name", "]", ".", "shape", ")", ",", "\"Variable shape not as expected: %s, of shape %s. %s\"", "%", "(", "\n", "variable", ".", "name", ",", "str", "(", "variable", ".", "shape", ")", ",", "\n", "str", "(", "dump", "[", "\"params\"", "]", "[", "variable", ".", "name", "]", ".", "shape", ")", ")", "\n", "value", "=", "numpy", ".", "asarray", "(", "dump", "[", "\"params\"", "]", "[", "variable", ".", "name", "]", ")", "\n", "self", ".", "session", ".", "run", "(", "variable", ".", "assign", "(", "value", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.variants.Model.__init__": [[16, 55], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "config", ",", "label2id_sent", ",", "label2id_tok", ")", ":", "\n", "        ", "self", ".", "config", "=", "config", "\n", "self", ".", "label2id_sent", "=", "label2id_sent", "\n", "self", ".", "label2id_tok", "=", "label2id_tok", "\n", "\n", "self", ".", "UNK", "=", "\"<unk>\"", "\n", "self", ".", "CUNK", "=", "\"<cunk>\"", "\n", "self", ".", "word2id", "=", "None", "\n", "self", ".", "char2id", "=", "None", "\n", "self", ".", "singletons", "=", "None", "\n", "self", ".", "num_heads", "=", "None", "\n", "\n", "self", ".", "word_ids", "=", "None", "\n", "self", ".", "char_ids", "=", "None", "\n", "self", ".", "sentence_lengths", "=", "None", "\n", "self", ".", "word_lengths", "=", "None", "\n", "\n", "self", ".", "sentence_labels", "=", "None", "\n", "self", ".", "word_labels", "=", "None", "\n", "\n", "self", ".", "word_embeddings", "=", "None", "\n", "self", ".", "char_embeddings", "=", "None", "\n", "\n", "self", ".", "word_objective_weights", "=", "None", "\n", "self", ".", "learning_rate", "=", "None", "\n", "self", ".", "loss", "=", "None", "\n", "self", ".", "initializer", "=", "None", "\n", "self", ".", "is_training", "=", "None", "\n", "self", ".", "session", "=", "None", "\n", "self", ".", "saver", "=", "None", "\n", "self", ".", "train_op", "=", "None", "\n", "\n", "self", ".", "token_scores", "=", "None", "\n", "self", ".", "sentence_scores", "=", "None", "\n", "self", ".", "token_predictions", "=", "None", "\n", "self", ".", "sentence_predictions", "=", "None", "\n", "self", ".", "token_probabilities", "=", "None", "\n", "self", ".", "sentence_probabilities", "=", "None", "\n", "self", ".", "attention_weights", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.variants.Model.build_vocabs": [[56, 114], ["list", "collections.Counter", "collections.Counter", "collections.OrderedDict", "collections.Counter.most_common", "collections.OrderedDict", "collections.Counter.most_common", "set", "print", "print", "print", "collections.OrderedDict", "collections.Counter.update", "len", "open", "str", "str", "str", "re.sub.lower", "re.sub", "len", "line.strip().split", "embedding_vocab.add", "len", "len", "len", "len", "len", "re.sub.lower", "re.sub", "line.strip"], "methods", ["None"], ["", "def", "build_vocabs", "(", "self", ",", "data_train", ",", "data_dev", ",", "data_test", ",", "embedding_path", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Builds the vocabulary based on the the data and embeddings info.\n        \"\"\"", "\n", "data_source", "=", "list", "(", "data_train", ")", "\n", "if", "self", ".", "config", "[", "\"vocab_include_devtest\"", "]", ":", "\n", "            ", "if", "data_dev", "is", "not", "None", ":", "\n", "                ", "data_source", "+=", "data_dev", "\n", "", "if", "data_test", "is", "not", "None", ":", "\n", "                ", "data_source", "+=", "data_test", "\n", "\n", "", "", "char_counter", "=", "collections", ".", "Counter", "(", ")", "\n", "word_counter", "=", "collections", ".", "Counter", "(", ")", "\n", "for", "sentence", "in", "data_source", ":", "\n", "            ", "for", "token", "in", "sentence", ".", "tokens", ":", "\n", "                ", "char_counter", ".", "update", "(", "token", ".", "value", ")", "\n", "w", "=", "token", ".", "value", "\n", "if", "self", ".", "config", "[", "\"lowercase\"", "]", ":", "\n", "                    ", "w", "=", "w", ".", "lower", "(", ")", "\n", "", "if", "self", ".", "config", "[", "\"replace_digits\"", "]", ":", "\n", "                    ", "w", "=", "re", ".", "sub", "(", "r'\\d'", ",", "'0'", ",", "w", ")", "\n", "", "word_counter", "[", "w", "]", "+=", "1", "\n", "\n", "", "", "self", ".", "char2id", "=", "collections", ".", "OrderedDict", "(", "[", "(", "self", ".", "CUNK", ",", "0", ")", "]", ")", "\n", "for", "char", ",", "count", "in", "char_counter", ".", "most_common", "(", ")", ":", "\n", "            ", "if", "char", "not", "in", "self", ".", "char2id", ":", "\n", "                ", "self", ".", "char2id", "[", "char", "]", "=", "len", "(", "self", ".", "char2id", ")", "\n", "\n", "", "", "self", ".", "word2id", "=", "collections", ".", "OrderedDict", "(", "[", "(", "self", ".", "UNK", ",", "0", ")", "]", ")", "\n", "for", "word", ",", "count", "in", "word_counter", ".", "most_common", "(", ")", ":", "\n", "            ", "if", "self", ".", "config", "[", "\"min_word_freq\"", "]", "<=", "0", "or", "count", ">=", "self", ".", "config", "[", "\"min_word_freq\"", "]", ":", "\n", "                ", "if", "word", "not", "in", "self", ".", "word2id", ":", "\n", "                    ", "self", ".", "word2id", "[", "word", "]", "=", "len", "(", "self", ".", "word2id", ")", "\n", "\n", "", "", "", "self", ".", "singletons", "=", "set", "(", "[", "word", "for", "word", "in", "word_counter", "if", "word_counter", "[", "word", "]", "==", "1", "]", ")", "\n", "\n", "if", "embedding_path", "and", "self", ".", "config", "[", "\"vocab_only_embedded\"", "]", ":", "\n", "            ", "embedding_vocab", "=", "{", "self", ".", "UNK", "}", "\n", "with", "open", "(", "embedding_path", ")", "as", "f", ":", "\n", "                ", "for", "line", "in", "f", ":", "\n", "                    ", "line_parts", "=", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "if", "len", "(", "line_parts", ")", "<=", "2", ":", "\n", "                        ", "continue", "\n", "", "w", "=", "line_parts", "[", "0", "]", "\n", "if", "self", ".", "config", "[", "\"lowercase\"", "]", ":", "\n", "                        ", "w", "=", "w", ".", "lower", "(", ")", "\n", "", "if", "self", ".", "config", "[", "\"replace_digits\"", "]", ":", "\n", "                        ", "w", "=", "re", ".", "sub", "(", "r'\\d'", ",", "'0'", ",", "w", ")", "\n", "", "embedding_vocab", ".", "add", "(", "w", ")", "\n", "", "", "word2id_revised", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "for", "word", "in", "self", ".", "word2id", ":", "\n", "                ", "if", "word", "in", "embedding_vocab", "and", "word", "not", "in", "word2id_revised", ":", "\n", "                    ", "word2id_revised", "[", "word", "]", "=", "len", "(", "word2id_revised", ")", "\n", "", "", "self", ".", "word2id", "=", "word2id_revised", "\n", "\n", "", "print", "(", "\"Total number of words: \"", "+", "str", "(", "len", "(", "self", ".", "word2id", ")", ")", ")", "\n", "print", "(", "\"Total number of chars: \"", "+", "str", "(", "len", "(", "self", ".", "char2id", ")", ")", ")", "\n", "print", "(", "\"Total number of singletons: \"", "+", "str", "(", "len", "(", "self", ".", "singletons", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.variants.Model.construct_network": [[115, 603], ["tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.zeros_initializer", "tensorflow.get_variable", "tensorflow.nn.embedding_lookup", "tensorflow.nn.rnn_cell.LSTMCell", "tensorflow.nn.rnn_cell.LSTMCell", "tensorflow.concat", "tensorflow.concat", "variants.Model.construct_optimizer", "print", "tensorflow.random_normal_initializer", "tensorflow.nn.dropout", "tensorflow.control_dependencies", "tensorflow.nn.bidirectional_dynamic_rnn", "tensorflow.nn.dropout", "tensorflow.nn.dropout", "tensorflow.nn.dropout", "len", "tensorflow.layers.dense", "modules.single_head_attention_binary_labels", "tensorflow.square", "tensorflow.where", "tensorflow.square", "tensorflow.tile", "tensorflow.where", "tensorflow.glorot_uniform_initializer", "tensorflow.variable_scope", "tensorflow.control_dependencies", "tensorflow.get_variable", "tensorflow.nn.embedding_lookup", "tensorflow.shape", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.nn.rnn_cell.LSTMCell", "tensorflow.nn.rnn_cell.LSTMCell", "tensorflow.nn.bidirectional_dynamic_rnn", "tensorflow.concat", "tensorflow.reshape", "ValueError", "tensorflow.sequence_mask", "tensorflow.zeros_like", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "modules.baseline_lstm_last_contexts", "tensorflow.nn.sparse_softmax_cross_entropy_with_logits", "tensorflow.zeros_like", "modules.compute_attention_loss", "modules.compute_gap_distance_loss", "variants.Model.construct_lm_cost", "variants.Model.construct_lm_cost", "tensorflow.glorot_normal_initializer", "len", "tensorflow.layers.dense", "tensorflow.concat", "tensorflow.cast", "tensorflow.cast", "tensorflow.assert_equal", "tensorflow.cast", "tensorflow.cast", "tensorflow.convert_to_tensor", "tensorflow.convert_to_tensor", "ceil", "modules.single_head_attention_multiple_labels", "tensorflow.nn.sparse_softmax_cross_entropy_with_logits", "tensorflow.where", "ValueError", "tensorflow.reduce_sum", "tensorflow.expand_dims", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.assert_equal", "variants.Model.construct_lm_cost", "variants.Model.construct_lm_cost", "ValueError", "tensorflow.reduce_max", "len", "len", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "len", "len", "modules.multi_head_attention_with_scores_from_shared_heads", "tensorflow.sequence_mask", "tensorflow.zeros_like", "tensorflow.reduce_sum", "tensorflow.cast", "tensorflow.sequence_mask", "len", "len", "len", "len", "len", "tensorflow.reduce_max", "len", "tensorflow.shape", "len", "len", "tensorflow.square", "tensorflow.square", "len", "len", "modules.multi_head_attention_with_scores_from_separate_heads", "tensorflow.cast", "tensorflow.shape", "tensorflow.shape", "tensorflow.shape", "len", "len", "modules.single_head_attention_multiple_transformations", "tensorflow.reduce_max", "tensorflow.reduce_min", "len", "len", "modules.variant_1", "tensorflow.where", "tensorflow.where", "len", "len", "modules.variant_2", "tensorflow.sequence_mask", "tensorflow.sequence_mask", "len", "len", "modules.variant_3", "tensorflow.zeros_like", "tensorflow.zeros_like", "len", "len", "modules.variant_4", "len", "len", "modules.variant_5", "len", "len", "modules.variant_6", "ValueError", "len", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.construct_optimizer", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.single_head_attention_binary_labels", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.baseline_lstm_last_contexts", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.compute_attention_loss", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.compute_gap_distance_loss", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.construct_lm_cost", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.construct_lm_cost", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.single_head_attention_multiple_labels", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.construct_lm_cost", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.construct_lm_cost", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.multi_head_attention_with_scores_from_shared_heads", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.multi_head_attention_with_scores_from_separate_heads", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.single_head_attention_multiple_transformations", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.variant_1", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.variant_2", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.variant_3", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.variant_4", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.variant_5", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.variant_6"], ["", "def", "construct_network", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Constructs a certain variant of the multi-head attention labeller (MHAL).\n        \"\"\"", "\n", "self", ".", "word_ids", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "None", ",", "None", "]", ",", "name", "=", "\"word_ids\"", ")", "\n", "self", ".", "char_ids", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "None", ",", "None", ",", "None", "]", ",", "name", "=", "\"char_ids\"", ")", "\n", "self", ".", "sentence_lengths", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "None", "]", ",", "name", "=", "\"sentence_lengths\"", ")", "\n", "self", ".", "word_lengths", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "None", ",", "None", "]", ",", "name", "=", "\"word_lengths\"", ")", "\n", "self", ".", "sentence_labels", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "None", "]", ",", "name", "=", "\"sentence_labels\"", ")", "\n", "self", ".", "word_labels", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "None", ",", "None", "]", ",", "name", "=", "\"word_labels\"", ")", "\n", "\n", "self", ".", "word_objective_weights", "=", "tf", ".", "placeholder", "(", "\n", "tf", ".", "float32", ",", "[", "None", ",", "None", "]", ",", "name", "=", "\"word_objective_weights\"", ")", "\n", "\n", "self", ".", "learning_rate", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "name", "=", "\"learning_rate\"", ")", "\n", "self", ".", "is_training", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "name", "=", "\"is_training\"", ")", "\n", "self", ".", "loss", "=", "0.0", "\n", "\n", "if", "self", ".", "config", "[", "\"initializer\"", "]", "==", "\"normal\"", ":", "\n", "            ", "self", ".", "initializer", "=", "tf", ".", "random_normal_initializer", "(", "stddev", "=", "0.1", ")", "\n", "", "elif", "self", ".", "config", "[", "\"initializer\"", "]", "==", "\"glorot\"", ":", "\n", "            ", "self", ".", "initializer", "=", "tf", ".", "glorot_uniform_initializer", "(", ")", "\n", "", "elif", "self", ".", "config", "[", "\"initializer\"", "]", "==", "\"xavier\"", ":", "\n", "            ", "self", ".", "initializer", "=", "tf", ".", "glorot_normal_initializer", "(", ")", "\n", "\n", "", "zeros_initializer", "=", "tf", ".", "zeros_initializer", "(", ")", "\n", "\n", "self", ".", "word_embeddings", "=", "tf", ".", "get_variable", "(", "\n", "name", "=", "\"word_embeddings\"", ",", "\n", "shape", "=", "[", "len", "(", "self", ".", "word2id", ")", ",", "self", ".", "config", "[", "\"word_embedding_size\"", "]", "]", ",", "\n", "initializer", "=", "(", "zeros_initializer", "if", "self", ".", "config", "[", "\"emb_initial_zero\"", "]", "else", "self", ".", "initializer", ")", ",", "\n", "trainable", "=", "(", "True", "if", "self", ".", "config", "[", "\"train_embeddings\"", "]", "else", "False", ")", ")", "\n", "word_input_tensor", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "self", ".", "word_embeddings", ",", "self", ".", "word_ids", ")", "\n", "\n", "if", "self", ".", "config", "[", "\"char_embedding_size\"", "]", ">", "0", "and", "self", ".", "config", "[", "\"char_recurrent_size\"", "]", ">", "0", ":", "\n", "            ", "with", "tf", ".", "variable_scope", "(", "\"chars\"", ")", ",", "tf", ".", "control_dependencies", "(", "\n", "[", "tf", ".", "assert_equal", "(", "tf", ".", "shape", "(", "self", ".", "char_ids", ")", "[", "2", "]", ",", "\n", "tf", ".", "reduce_max", "(", "self", ".", "word_lengths", ")", ",", "\n", "message", "=", "\"Char dimensions don't match\"", ")", "]", ")", ":", "\n", "                ", "self", ".", "char_embeddings", "=", "tf", ".", "get_variable", "(", "\n", "name", "=", "\"char_embeddings\"", ",", "\n", "shape", "=", "[", "len", "(", "self", ".", "char2id", ")", ",", "self", ".", "config", "[", "\"char_embedding_size\"", "]", "]", ",", "\n", "initializer", "=", "self", ".", "initializer", ",", "\n", "trainable", "=", "True", ")", "\n", "char_input_tensor", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "self", ".", "char_embeddings", ",", "self", ".", "char_ids", ")", "\n", "\n", "char_input_tensor_shape", "=", "tf", ".", "shape", "(", "char_input_tensor", ")", "\n", "char_input_tensor", "=", "tf", ".", "reshape", "(", "\n", "char_input_tensor", ",", "\n", "shape", "=", "[", "char_input_tensor_shape", "[", "0", "]", "\n", "*", "char_input_tensor_shape", "[", "1", "]", ",", "\n", "char_input_tensor_shape", "[", "2", "]", ",", "\n", "self", ".", "config", "[", "\"char_embedding_size\"", "]", "]", ")", "\n", "_word_lengths", "=", "tf", ".", "reshape", "(", "\n", "self", ".", "word_lengths", ",", "shape", "=", "[", "char_input_tensor_shape", "[", "0", "]", "\n", "*", "char_input_tensor_shape", "[", "1", "]", "]", ")", "\n", "\n", "char_lstm_cell_fw", "=", "tf", ".", "nn", ".", "rnn_cell", ".", "LSTMCell", "(", "\n", "self", ".", "config", "[", "\"char_recurrent_size\"", "]", ",", "\n", "use_peepholes", "=", "self", ".", "config", "[", "\"lstm_use_peepholes\"", "]", ",", "\n", "state_is_tuple", "=", "True", ",", "\n", "initializer", "=", "self", ".", "initializer", ",", "\n", "reuse", "=", "False", ")", "\n", "char_lstm_cell_bw", "=", "tf", ".", "nn", ".", "rnn_cell", ".", "LSTMCell", "(", "\n", "self", ".", "config", "[", "\"char_recurrent_size\"", "]", ",", "\n", "use_peepholes", "=", "self", ".", "config", "[", "\"lstm_use_peepholes\"", "]", ",", "\n", "state_is_tuple", "=", "True", ",", "\n", "initializer", "=", "self", ".", "initializer", ",", "\n", "reuse", "=", "False", ")", "\n", "\n", "# Concatenate the final forward and the backward character contexts", "\n", "# to obtain a compact character representation for each word.", "\n", "_", ",", "(", "(", "_", ",", "char_output_fw", ")", ",", "(", "_", ",", "char_output_bw", ")", ")", "=", "tf", ".", "nn", ".", "bidirectional_dynamic_rnn", "(", "\n", "cell_fw", "=", "char_lstm_cell_fw", ",", "cell_bw", "=", "char_lstm_cell_bw", ",", "inputs", "=", "char_input_tensor", ",", "\n", "sequence_length", "=", "_word_lengths", ",", "dtype", "=", "tf", ".", "float32", ",", "time_major", "=", "False", ")", "\n", "\n", "char_output_tensor", "=", "tf", ".", "concat", "(", "[", "char_output_fw", ",", "char_output_bw", "]", ",", "axis", "=", "-", "1", ")", "\n", "char_output_tensor", "=", "tf", ".", "reshape", "(", "\n", "char_output_tensor", ",", "\n", "shape", "=", "[", "char_input_tensor_shape", "[", "0", "]", ",", "char_input_tensor_shape", "[", "1", "]", ",", "\n", "2", "*", "self", ".", "config", "[", "\"char_recurrent_size\"", "]", "]", ")", "\n", "\n", "# Include a char-based language modelling loss, LMc.", "\n", "if", "self", ".", "config", "[", "\"lm_cost_char_gamma\"", "]", ">", "0.0", ":", "\n", "                    ", "self", ".", "loss", "+=", "self", ".", "config", "[", "\"lm_cost_char_gamma\"", "]", "*", "self", ".", "construct_lm_cost", "(", "\n", "input_tensor_fw", "=", "char_output_tensor", ",", "\n", "input_tensor_bw", "=", "char_output_tensor", ",", "\n", "sentence_lengths", "=", "self", ".", "sentence_lengths", ",", "\n", "target_ids", "=", "self", ".", "word_ids", ",", "\n", "lm_cost_type", "=", "\"separate\"", ",", "\n", "name", "=", "\"lm_cost_char_separate\"", ")", "\n", "\n", "", "if", "self", ".", "config", "[", "\"lm_cost_joint_char_gamma\"", "]", ">", "0.0", ":", "\n", "                    ", "self", ".", "loss", "+=", "self", ".", "config", "[", "\"lm_cost_joint_char_gamma\"", "]", "*", "self", ".", "construct_lm_cost", "(", "\n", "input_tensor_fw", "=", "char_output_tensor", ",", "\n", "input_tensor_bw", "=", "char_output_tensor", ",", "\n", "sentence_lengths", "=", "self", ".", "sentence_lengths", ",", "\n", "target_ids", "=", "self", ".", "word_ids", ",", "\n", "lm_cost_type", "=", "\"joint\"", ",", "\n", "name", "=", "\"lm_cost_char_joint\"", ")", "\n", "\n", "", "if", "self", ".", "config", "[", "\"char_hidden_layer_size\"", "]", ">", "0", ":", "\n", "                    ", "char_output_tensor", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", "=", "char_output_tensor", ",", "units", "=", "self", ".", "config", "[", "\"char_hidden_layer_size\"", "]", ",", "\n", "activation", "=", "tf", ".", "tanh", ",", "kernel_initializer", "=", "self", ".", "initializer", ")", "\n", "\n", "", "if", "self", ".", "config", "[", "\"char_integration_method\"", "]", "==", "\"concat\"", ":", "\n", "                    ", "word_input_tensor", "=", "tf", ".", "concat", "(", "[", "word_input_tensor", ",", "char_output_tensor", "]", ",", "axis", "=", "-", "1", ")", "\n", "", "elif", "self", ".", "config", "[", "\"char_integration_method\"", "]", "==", "\"none\"", ":", "\n", "                    ", "word_input_tensor", "=", "word_input_tensor", "\n", "", "else", ":", "\n", "                    ", "raise", "ValueError", "(", "\"Unknown char integration method\"", ")", "\n", "\n", "", "", "", "if", "self", ".", "config", "[", "\"dropout_input\"", "]", ">", "0.0", ":", "\n", "            ", "dropout_input", "=", "(", "self", ".", "config", "[", "\"dropout_input\"", "]", "*", "tf", ".", "cast", "(", "self", ".", "is_training", ",", "tf", ".", "float32", ")", "\n", "+", "(", "1.0", "-", "tf", ".", "cast", "(", "self", ".", "is_training", ",", "tf", ".", "float32", ")", ")", ")", "\n", "word_input_tensor", "=", "tf", ".", "nn", ".", "dropout", "(", "\n", "word_input_tensor", ",", "dropout_input", ",", "name", "=", "\"dropout_word\"", ")", "\n", "\n", "", "word_lstm_cell_fw", "=", "tf", ".", "nn", ".", "rnn_cell", ".", "LSTMCell", "(", "\n", "self", ".", "config", "[", "\"word_recurrent_size\"", "]", ",", "\n", "use_peepholes", "=", "self", ".", "config", "[", "\"lstm_use_peepholes\"", "]", ",", "\n", "state_is_tuple", "=", "True", ",", "\n", "initializer", "=", "self", ".", "initializer", ",", "\n", "reuse", "=", "False", ")", "\n", "word_lstm_cell_bw", "=", "tf", ".", "nn", ".", "rnn_cell", ".", "LSTMCell", "(", "\n", "self", ".", "config", "[", "\"word_recurrent_size\"", "]", ",", "\n", "use_peepholes", "=", "self", ".", "config", "[", "\"lstm_use_peepholes\"", "]", ",", "\n", "state_is_tuple", "=", "True", ",", "\n", "initializer", "=", "self", ".", "initializer", ",", "\n", "reuse", "=", "False", ")", "\n", "\n", "with", "tf", ".", "control_dependencies", "(", "\n", "[", "tf", ".", "assert_equal", "(", "\n", "tf", ".", "shape", "(", "self", ".", "word_ids", ")", "[", "1", "]", ",", "\n", "tf", ".", "reduce_max", "(", "self", ".", "sentence_lengths", ")", ",", "\n", "message", "=", "\"Sentence dimensions don't match\"", ")", "]", ")", ":", "\n", "            ", "(", "lstm_outputs_fw", ",", "lstm_outputs_bw", ")", ",", "(", "(", "_", ",", "lstm_output_fw", ")", ",", "(", "_", ",", "lstm_output_bw", ")", ")", "=", "tf", ".", "nn", ".", "bidirectional_dynamic_rnn", "(", "\n", "cell_fw", "=", "word_lstm_cell_fw", ",", "cell_bw", "=", "word_lstm_cell_bw", ",", "inputs", "=", "word_input_tensor", ",", "\n", "sequence_length", "=", "self", ".", "sentence_lengths", ",", "dtype", "=", "tf", ".", "float32", ",", "time_major", "=", "False", ")", "\n", "\n", "", "lstm_output_states", "=", "tf", ".", "concat", "(", "[", "lstm_output_fw", ",", "lstm_output_bw", "]", ",", "-", "1", ")", "\n", "\n", "if", "self", ".", "config", "[", "\"dropout_word_lstm\"", "]", ">", "0.0", ":", "\n", "            ", "dropout_word_lstm", "=", "(", "self", ".", "config", "[", "\"dropout_word_lstm\"", "]", "*", "tf", ".", "cast", "(", "self", ".", "is_training", ",", "tf", ".", "float32", ")", "\n", "+", "(", "1.0", "-", "tf", ".", "cast", "(", "self", ".", "is_training", ",", "tf", ".", "float32", ")", ")", ")", "\n", "lstm_outputs_fw", "=", "tf", ".", "nn", ".", "dropout", "(", "\n", "lstm_outputs_fw", ",", "dropout_word_lstm", ",", "\n", "noise_shape", "=", "tf", ".", "convert_to_tensor", "(", "\n", "[", "tf", ".", "shape", "(", "self", ".", "word_ids", ")", "[", "0", "]", ",", "1", ",", "self", ".", "config", "[", "\"word_recurrent_size\"", "]", "]", ",", "dtype", "=", "tf", ".", "int32", ")", ")", "\n", "lstm_outputs_bw", "=", "tf", ".", "nn", ".", "dropout", "(", "\n", "lstm_outputs_bw", ",", "dropout_word_lstm", ",", "\n", "noise_shape", "=", "tf", ".", "convert_to_tensor", "(", "\n", "[", "tf", ".", "shape", "(", "self", ".", "word_ids", ")", "[", "0", "]", ",", "1", ",", "self", ".", "config", "[", "\"word_recurrent_size\"", "]", "]", ",", "dtype", "=", "tf", ".", "int32", ")", ")", "\n", "lstm_output_states", "=", "tf", ".", "nn", ".", "dropout", "(", "lstm_output_states", ",", "dropout_word_lstm", ")", "\n", "\n", "# The forward and backward states are concatenated at every token position.", "\n", "", "lstm_outputs_states", "=", "tf", ".", "concat", "(", "[", "lstm_outputs_fw", ",", "lstm_outputs_bw", "]", ",", "-", "1", ")", "# [B, M, 2 * emb_size]", "\n", "\n", "if", "self", ".", "config", "[", "\"whidden_layer_size\"", "]", ">", "0", ":", "\n", "            ", "lstm_output_units", "=", "self", ".", "config", "[", "\"whidden_layer_size\"", "]", "\n", "num_heads", "=", "len", "(", "self", ".", "label2id_tok", ")", "\n", "\n", "# Make the number of units a multiple of num_heads.", "\n", "if", "lstm_output_units", "%", "num_heads", "!=", "0", ":", "\n", "                ", "lstm_output_units", "=", "ceil", "(", "lstm_output_units", "/", "num_heads", ")", "*", "num_heads", "\n", "\n", "", "lstm_outputs", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", "=", "lstm_outputs_states", ",", "units", "=", "lstm_output_units", ",", "\n", "activation", "=", "tf", ".", "tanh", ",", "kernel_initializer", "=", "self", ".", "initializer", ")", "# [B, M, lstm_output_units]", "\n", "", "else", ":", "\n", "            ", "lstm_outputs", "=", "lstm_outputs_states", "\n", "\n", "", "if", "self", ".", "config", "[", "\"model_type\"", "]", "==", "\"single_head_attention_binary_labels\"", ":", "\n", "            ", "if", "not", "(", "len", "(", "self", ".", "label2id_tok", ")", "==", "2", "and", "len", "(", "self", ".", "label2id_sent", ")", "==", "2", ")", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "\"The model_type you selected (%s) is only available for \"", "\n", "\"binary labels! Currently, the no. sentence_labels = %d and \"", "\n", "\"the no. token_labels = %d. Consider changing the model type.\"", "\n", "%", "(", "self", ".", "config", "[", "\"model_type\"", "]", ",", "\n", "len", "(", "self", ".", "label2id_sent", ")", ",", "len", "(", "self", ".", "label2id_tok", ")", ")", ")", "\n", "\n", "", "self", ".", "sentence_scores", ",", "self", ".", "sentence_predictions", ",", "self", ".", "token_scores", ",", "self", ".", "token_predictions", "=", "single_head_attention_binary_labels", "(", "\n", "inputs", "=", "lstm_outputs", ",", "\n", "initializer", "=", "self", ".", "initializer", ",", "\n", "attention_size", "=", "self", ".", "config", "[", "\"attention_evidence_size\"", "]", ",", "\n", "sentence_lengths", "=", "self", ".", "sentence_lengths", ",", "\n", "hidden_units", "=", "self", ".", "config", "[", "\"hidden_layer_size\"", "]", ")", "\n", "\n", "# Include a token-level loss (for sequence labelling).", "\n", "word_objective_loss", "=", "tf", ".", "square", "(", "self", ".", "token_scores", "-", "self", ".", "word_labels", ")", "\n", "word_objective_loss", "=", "tf", ".", "where", "(", "\n", "tf", ".", "sequence_mask", "(", "self", ".", "sentence_lengths", ")", ",", "\n", "word_objective_loss", ",", "tf", ".", "zeros_like", "(", "word_objective_loss", ")", ")", "\n", "self", ".", "loss", "+=", "self", ".", "config", "[", "\"word_objective_weight\"", "]", "*", "tf", ".", "reduce_sum", "(", "\n", "self", ".", "word_objective_weights", "*", "word_objective_loss", ")", "\n", "\n", "# Include a sentence-level loss (for sentence classification).", "\n", "sentence_objective_loss", "=", "tf", ".", "square", "(", "self", ".", "sentence_scores", "-", "self", ".", "sentence_labels", ")", "\n", "self", ".", "loss", "+=", "self", ".", "config", "[", "\"sentence_objective_weight\"", "]", "*", "tf", ".", "reduce_sum", "(", "sentence_objective_loss", ")", "\n", "\n", "# Include an attention-level loss for wiring the two hierarchical levels.", "\n", "if", "self", ".", "config", "[", "\"attention_objective_weight\"", "]", ">", "0.0", ":", "\n", "                ", "self", ".", "loss", "+=", "self", ".", "config", "[", "\"attention_objective_weight\"", "]", "*", "(", "\n", "tf", ".", "reduce_sum", "(", "\n", "tf", ".", "square", "(", "\n", "tf", ".", "reduce_max", "(", "\n", "tf", ".", "where", "(", "\n", "tf", ".", "sequence_mask", "(", "self", ".", "sentence_lengths", ")", ",", "\n", "self", ".", "token_scores", ",", "\n", "tf", ".", "zeros_like", "(", "self", ".", "token_scores", ")", "-", "1e6", ")", ",", "\n", "axis", "=", "-", "1", ")", "-", "self", ".", "sentence_labels", ")", ")", "\n", "+", "\n", "tf", ".", "reduce_sum", "(", "\n", "tf", ".", "square", "(", "\n", "tf", ".", "reduce_min", "(", "\n", "tf", ".", "where", "(", "\n", "tf", ".", "sequence_mask", "(", "self", ".", "sentence_lengths", ")", ",", "\n", "self", ".", "token_scores", ",", "\n", "tf", ".", "zeros_like", "(", "self", ".", "token_scores", ")", "+", "1e6", ")", ",", "\n", "axis", "=", "-", "1", ")", "-", "0.0", ")", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "scoring_activation", "=", "None", "\n", "if", "\"scoring_activation\"", "in", "self", ".", "config", "and", "self", ".", "config", "[", "\"scoring_activation\"", "]", ":", "\n", "                ", "if", "self", ".", "config", "[", "\"scoring_activation\"", "]", "==", "\"tanh\"", ":", "\n", "                    ", "scoring_activation", "=", "tf", ".", "tanh", "\n", "", "elif", "self", ".", "config", "[", "\"scoring_activation\"", "]", "==", "\"sigmoid\"", ":", "\n", "                    ", "scoring_activation", "=", "tf", ".", "sigmoid", "\n", "", "elif", "self", ".", "config", "[", "\"scoring_activation\"", "]", "==", "\"relu\"", ":", "\n", "                    ", "scoring_activation", "=", "tf", ".", "nn", ".", "relu", "\n", "", "elif", "self", ".", "config", "[", "\"scoring_activation\"", "]", "==", "\"softmax\"", ":", "\n", "                    ", "scoring_activation", "=", "tf", ".", "nn", ".", "softmax", "\n", "\n", "", "", "if", "\"baseline_lstm_last_contexts\"", "in", "self", ".", "config", "[", "\"model_type\"", "]", ":", "\n", "                ", "self", ".", "sentence_scores", ",", "self", ".", "sentence_predictions", ",", "self", ".", "token_scores", ",", "self", ".", "token_predictions", ",", "self", ".", "token_probabilities", ",", "self", ".", "sentence_probabilities", ",", "self", ".", "attention_weights", "=", "baseline_lstm_last_contexts", "(", "\n", "last_token_contexts", "=", "lstm_outputs_states", ",", "\n", "last_context", "=", "lstm_output_states", ",", "\n", "initializer", "=", "self", ".", "initializer", ",", "\n", "scoring_activation", "=", "scoring_activation", ",", "\n", "sentence_lengths", "=", "self", ".", "sentence_lengths", ",", "\n", "hidden_units", "=", "self", ".", "config", "[", "\"hidden_layer_size\"", "]", ",", "\n", "num_sentence_labels", "=", "len", "(", "self", ".", "label2id_sent", ")", ",", "\n", "num_token_labels", "=", "len", "(", "self", ".", "label2id_tok", ")", ")", "\n", "", "elif", "self", ".", "config", "[", "\"model_type\"", "]", "==", "\"single_head_attention_multiple_labels\"", ":", "\n", "                ", "self", ".", "sentence_scores", ",", "self", ".", "sentence_predictions", ",", "self", ".", "token_scores", ",", "self", ".", "token_predictions", ",", "self", ".", "token_probabilities", ",", "self", ".", "sentence_probabilities", ",", "self", ".", "attention_weights", "=", "single_head_attention_multiple_labels", "(", "\n", "inputs", "=", "lstm_outputs", ",", "\n", "initializer", "=", "self", ".", "initializer", ",", "\n", "attention_activation", "=", "self", ".", "config", "[", "\"attention_activation\"", "]", ",", "\n", "attention_size", "=", "self", ".", "config", "[", "\"attention_evidence_size\"", "]", ",", "\n", "sentence_lengths", "=", "self", ".", "sentence_lengths", ",", "\n", "hidden_units", "=", "self", ".", "config", "[", "\"hidden_layer_size\"", "]", ",", "\n", "num_sentence_labels", "=", "len", "(", "self", ".", "label2id_sent", ")", ",", "\n", "num_token_labels", "=", "len", "(", "self", ".", "label2id_tok", ")", ")", "\n", "", "elif", "self", ".", "config", "[", "\"model_type\"", "]", "==", "\"multi_head_attention_with_scores_from_shared_heads\"", ":", "\n", "                ", "self", ".", "sentence_scores", ",", "self", ".", "sentence_predictions", ",", "self", ".", "token_scores", ",", "self", ".", "token_predictions", ",", "self", ".", "token_probabilities", ",", "self", ".", "sentence_probabilities", ",", "self", ".", "attention_weights", "=", "multi_head_attention_with_scores_from_shared_heads", "(", "\n", "inputs", "=", "lstm_outputs", ",", "\n", "initializer", "=", "self", ".", "initializer", ",", "\n", "attention_activation", "=", "self", ".", "config", "[", "\"attention_activation\"", "]", ",", "\n", "hidden_units", "=", "self", ".", "config", "[", "\"hidden_layer_size\"", "]", ",", "\n", "num_sentence_labels", "=", "len", "(", "self", ".", "label2id_sent", ")", ",", "\n", "num_heads", "=", "len", "(", "self", ".", "label2id_tok", ")", ",", "\n", "is_training", "=", "self", ".", "is_training", ",", "\n", "dropout", "=", "self", ".", "config", "[", "\"dropout_attention\"", "]", ",", "\n", "sentence_lengths", "=", "self", ".", "sentence_lengths", ",", "\n", "use_residual_connection", "=", "self", ".", "config", "[", "\"residual_connection\"", "]", ",", "\n", "token_scoring_method", "=", "self", ".", "config", "[", "\"token_scoring_method\"", "]", ")", "\n", "", "elif", "self", ".", "config", "[", "\"model_type\"", "]", "==", "\"multi_head_attention_with_scores_from_separate_heads\"", ":", "\n", "                ", "self", ".", "sentence_scores", ",", "self", ".", "sentence_predictions", ",", "self", ".", "token_scores", ",", "self", ".", "token_predictions", ",", "self", ".", "token_probabilities", ",", "self", ".", "sentence_probabilities", ",", "self", ".", "attention_weights", "=", "multi_head_attention_with_scores_from_separate_heads", "(", "\n", "inputs", "=", "lstm_outputs", ",", "\n", "initializer", "=", "self", ".", "initializer", ",", "\n", "attention_activation", "=", "self", ".", "config", "[", "\"attention_activation\"", "]", ",", "\n", "num_sentence_labels", "=", "len", "(", "self", ".", "label2id_sent", ")", ",", "\n", "num_heads", "=", "len", "(", "self", ".", "label2id_tok", ")", ",", "\n", "is_training", "=", "self", ".", "is_training", ",", "\n", "dropout", "=", "self", ".", "config", "[", "\"dropout_attention\"", "]", ",", "\n", "sentence_lengths", "=", "self", ".", "sentence_lengths", ",", "\n", "normalize_sentence", "=", "self", ".", "config", "[", "\"normalize_sentence\"", "]", ",", "\n", "token_scoring_method", "=", "self", ".", "config", "[", "\"token_scoring_method\"", "]", ",", "\n", "scoring_activation", "=", "scoring_activation", ",", "\n", "separate_heads", "=", "self", ".", "config", "[", "\"separate_heads\"", "]", ")", "\n", "", "elif", "self", ".", "config", "[", "\"model_type\"", "]", "==", "\"single_head_attention_multiple_transformations\"", ":", "\n", "                ", "self", ".", "sentence_scores", ",", "self", ".", "sentence_predictions", ",", "self", ".", "token_scores", ",", "self", ".", "token_predictions", ",", "self", ".", "token_probabilities", ",", "self", ".", "sentence_probabilities", ",", "self", ".", "attention_weights", "=", "single_head_attention_multiple_transformations", "(", "\n", "inputs", "=", "lstm_outputs", ",", "\n", "initializer", "=", "self", ".", "initializer", ",", "\n", "attention_activation", "=", "self", ".", "config", "[", "\"attention_activation\"", "]", ",", "\n", "num_sentence_labels", "=", "len", "(", "self", ".", "label2id_sent", ")", ",", "\n", "num_heads", "=", "len", "(", "self", ".", "label2id_tok", ")", ",", "\n", "sentence_lengths", "=", "self", ".", "sentence_lengths", ",", "\n", "token_scoring_method", "=", "self", ".", "config", "[", "\"token_scoring_method\"", "]", ",", "\n", "scoring_activation", "=", "scoring_activation", ",", "\n", "how_to_compute_attention", "=", "self", ".", "config", "[", "\"how_to_compute_attention\"", "]", ",", "\n", "separate_heads", "=", "self", ".", "config", "[", "\"separate_heads\"", "]", ")", "\n", "", "elif", "self", ".", "config", "[", "\"model_type\"", "]", "==", "\"variant_1\"", ":", "\n", "                ", "self", ".", "sentence_scores", ",", "self", ".", "sentence_predictions", ",", "self", ".", "token_scores", ",", "self", ".", "token_predictions", ",", "self", ".", "token_probabilities", ",", "self", ".", "sentence_probabilities", ",", "self", ".", "attention_weights", "=", "variant_1", "(", "\n", "inputs", "=", "lstm_outputs", ",", "\n", "initializer", "=", "self", ".", "initializer", ",", "\n", "attention_activation", "=", "self", ".", "config", "[", "\"attention_activation\"", "]", ",", "\n", "num_sentence_labels", "=", "len", "(", "self", ".", "label2id_sent", ")", ",", "\n", "num_heads", "=", "len", "(", "self", ".", "label2id_tok", ")", ",", "\n", "hidden_units", "=", "self", ".", "config", "[", "\"hidden_layer_size\"", "]", ",", "\n", "sentence_lengths", "=", "self", ".", "sentence_lengths", ",", "\n", "scoring_activation", "=", "scoring_activation", ",", "\n", "token_scoring_method", "=", "self", ".", "config", "[", "\"token_scoring_method\"", "]", ",", "\n", "use_inputs_instead_values", "=", "self", ".", "config", "[", "\"use_inputs_instead_values\"", "]", ",", "\n", "separate_heads", "=", "self", ".", "config", "[", "\"separate_heads\"", "]", ")", "\n", "", "elif", "self", ".", "config", "[", "\"model_type\"", "]", "==", "\"variant_2\"", ":", "\n", "                ", "self", ".", "sentence_scores", ",", "self", ".", "sentence_predictions", ",", "self", ".", "token_scores", ",", "self", ".", "token_predictions", ",", "self", ".", "token_probabilities", ",", "self", ".", "sentence_probabilities", ",", "self", ".", "attention_weights", "=", "variant_2", "(", "\n", "inputs", "=", "lstm_outputs", ",", "\n", "initializer", "=", "self", ".", "initializer", ",", "\n", "attention_activation", "=", "self", ".", "config", "[", "\"attention_activation\"", "]", ",", "\n", "num_sentence_labels", "=", "len", "(", "self", ".", "label2id_sent", ")", ",", "\n", "num_heads", "=", "len", "(", "self", ".", "label2id_tok", ")", ",", "\n", "hidden_units", "=", "self", ".", "config", "[", "\"hidden_layer_size\"", "]", ",", "\n", "sentence_lengths", "=", "self", ".", "sentence_lengths", ",", "\n", "scoring_activation", "=", "scoring_activation", ",", "\n", "use_inputs_instead_values", "=", "self", ".", "config", "[", "\"use_inputs_instead_values\"", "]", ",", "\n", "separate_heads", "=", "self", ".", "config", "[", "\"separate_heads\"", "]", ")", "\n", "", "elif", "self", ".", "config", "[", "\"model_type\"", "]", "==", "\"variant_3\"", ":", "\n", "                ", "self", ".", "sentence_scores", ",", "self", ".", "sentence_predictions", ",", "self", ".", "token_scores", ",", "self", ".", "token_predictions", ",", "self", ".", "token_probabilities", ",", "self", ".", "sentence_probabilities", ",", "self", ".", "attention_weights", "=", "variant_3", "(", "\n", "inputs", "=", "lstm_outputs", ",", "\n", "initializer", "=", "self", ".", "initializer", ",", "\n", "attention_activation", "=", "self", ".", "config", "[", "\"attention_activation\"", "]", ",", "\n", "num_sentence_labels", "=", "len", "(", "self", ".", "label2id_sent", ")", ",", "\n", "num_heads", "=", "len", "(", "self", ".", "label2id_tok", ")", ",", "\n", "attention_size", "=", "self", ".", "config", "[", "\"attention_evidence_size\"", "]", ",", "\n", "sentence_lengths", "=", "self", ".", "sentence_lengths", ",", "\n", "scoring_activation", "=", "scoring_activation", ",", "\n", "separate_heads", "=", "self", ".", "config", "[", "\"separate_heads\"", "]", ")", "\n", "", "elif", "self", ".", "config", "[", "\"model_type\"", "]", "==", "\"variant_4\"", ":", "\n", "                ", "self", ".", "sentence_scores", ",", "self", ".", "sentence_predictions", ",", "self", ".", "token_scores", ",", "self", ".", "token_predictions", ",", "self", ".", "token_probabilities", ",", "self", ".", "sentence_probabilities", ",", "self", ".", "attention_weights", "=", "variant_4", "(", "\n", "inputs", "=", "lstm_outputs", ",", "\n", "initializer", "=", "self", ".", "initializer", ",", "\n", "attention_activation", "=", "self", ".", "config", "[", "\"attention_activation\"", "]", ",", "\n", "num_sentence_labels", "=", "len", "(", "self", ".", "label2id_sent", ")", ",", "\n", "num_heads", "=", "len", "(", "self", ".", "label2id_tok", ")", ",", "\n", "hidden_units", "=", "self", ".", "config", "[", "\"hidden_layer_size\"", "]", ",", "\n", "sentence_lengths", "=", "self", ".", "sentence_lengths", ",", "\n", "scoring_activation", "=", "scoring_activation", ",", "\n", "token_scoring_method", "=", "self", ".", "config", "[", "\"token_scoring_method\"", "]", ",", "\n", "use_inputs_instead_values", "=", "self", ".", "config", "[", "\"use_inputs_instead_values\"", "]", ",", "\n", "separate_heads", "=", "self", ".", "config", "[", "\"separate_heads\"", "]", ")", "\n", "", "elif", "self", ".", "config", "[", "\"model_type\"", "]", "==", "\"variant_5\"", ":", "\n", "                ", "self", ".", "sentence_scores", ",", "self", ".", "sentence_predictions", ",", "self", ".", "token_scores", ",", "self", ".", "token_predictions", ",", "self", ".", "token_probabilities", ",", "self", ".", "sentence_probabilities", ",", "self", ".", "attention_weights", "=", "variant_5", "(", "\n", "inputs", "=", "lstm_outputs", ",", "\n", "initializer", "=", "self", ".", "initializer", ",", "\n", "attention_activation", "=", "self", ".", "config", "[", "\"attention_activation\"", "]", ",", "\n", "num_sentence_labels", "=", "len", "(", "self", ".", "label2id_sent", ")", ",", "\n", "num_heads", "=", "len", "(", "self", ".", "label2id_tok", ")", ",", "\n", "hidden_units", "=", "self", ".", "config", "[", "\"hidden_layer_size\"", "]", ",", "\n", "sentence_lengths", "=", "self", ".", "sentence_lengths", ",", "\n", "scoring_activation", "=", "scoring_activation", ",", "\n", "token_scoring_method", "=", "self", ".", "config", "[", "\"token_scoring_method\"", "]", ",", "\n", "use_inputs_instead_values", "=", "self", ".", "config", "[", "\"use_inputs_instead_values\"", "]", ",", "\n", "separate_heads", "=", "self", ".", "config", "[", "\"separate_heads\"", "]", ")", "\n", "", "elif", "self", ".", "config", "[", "\"model_type\"", "]", "==", "\"variant_6\"", ":", "\n", "                ", "self", ".", "sentence_scores", ",", "self", ".", "sentence_predictions", ",", "self", ".", "token_scores", ",", "self", ".", "token_predictions", ",", "self", ".", "token_probabilities", ",", "self", ".", "sentence_probabilities", ",", "self", ".", "attention_weights", "=", "variant_6", "(", "\n", "inputs", "=", "lstm_outputs", ",", "\n", "initializer", "=", "self", ".", "initializer", ",", "\n", "attention_activation", "=", "self", ".", "config", "[", "\"attention_activation\"", "]", ",", "\n", "num_sentence_labels", "=", "len", "(", "self", ".", "label2id_sent", ")", ",", "\n", "num_heads", "=", "len", "(", "self", ".", "label2id_tok", ")", ",", "\n", "hidden_units", "=", "self", ".", "config", "[", "\"hidden_layer_size\"", "]", ",", "\n", "scoring_activation", "=", "scoring_activation", ",", "\n", "token_scoring_method", "=", "self", ".", "config", "[", "\"token_scoring_method\"", "]", ",", "\n", "separate_heads", "=", "self", ".", "config", "[", "\"separate_heads\"", "]", ")", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"Unknown/unsupported model type: %s\"", "\n", "%", "self", ".", "config", "[", "\"model_type\"", "]", ")", "\n", "\n", "# Include a token-level loss (for sequence labelling).", "\n", "", "if", "self", ".", "config", "[", "\"word_objective_weight\"", "]", ">", "0", ":", "\n", "                ", "if", "self", ".", "config", "[", "\"token_labels_available\"", "]", ":", "\n", "                    ", "word_objective_loss", "=", "tf", ".", "nn", ".", "sparse_softmax_cross_entropy_with_logits", "(", "\n", "logits", "=", "self", ".", "token_scores", ",", "labels", "=", "tf", ".", "cast", "(", "self", ".", "word_labels", ",", "tf", ".", "int32", ")", ")", "\n", "word_objective_loss", "=", "tf", ".", "where", "(", "\n", "tf", ".", "sequence_mask", "(", "self", ".", "sentence_lengths", ")", ",", "\n", "word_objective_loss", ",", "tf", ".", "zeros_like", "(", "word_objective_loss", ")", ")", "\n", "self", ".", "loss", "+=", "self", ".", "config", "[", "\"word_objective_weight\"", "]", "*", "tf", ".", "reduce_sum", "(", "\n", "self", ".", "word_objective_weights", "*", "word_objective_loss", ")", "\n", "", "else", ":", "\n", "                    ", "raise", "ValueError", "(", "\n", "\"No token labels available! You cannot supervise on the token-level\"", "\n", "\" so please change \\\"word_objective_weight\\\" to 0\"", "\n", "\" or provide token-annotated files.\"", ")", "\n", "\n", "# Include a sentence-level loss (for sentence classification).", "\n", "", "", "if", "self", ".", "config", "[", "\"sentence_objective_weight\"", "]", ">", "0", ":", "\n", "                ", "sentence_objective_loss", "=", "tf", ".", "nn", ".", "sparse_softmax_cross_entropy_with_logits", "(", "\n", "logits", "=", "self", ".", "sentence_scores", ",", "labels", "=", "tf", ".", "cast", "(", "self", ".", "sentence_labels", ",", "tf", ".", "int32", ")", ")", "\n", "self", ".", "loss", "+=", "self", ".", "config", "[", "\"sentence_objective_weight\"", "]", "*", "tf", ".", "reduce_sum", "(", "sentence_objective_loss", ")", "\n", "\n", "# Mask the token scores that do not fall in the range of the true sentence length.", "\n", "# Do this for each head (change shape from [B, M] to [B, M, num_heads]).", "\n", "", "tiled_sentence_lengths", "=", "tf", ".", "tile", "(", "\n", "input", "=", "tf", ".", "expand_dims", "(", "\n", "tf", ".", "sequence_mask", "(", "self", ".", "sentence_lengths", ")", ",", "axis", "=", "-", "1", ")", ",", "\n", "multiples", "=", "[", "1", ",", "1", ",", "len", "(", "self", ".", "label2id_tok", ")", "]", ")", "\n", "\n", "self", ".", "token_probabilities", "=", "tf", ".", "where", "(", "\n", "tiled_sentence_lengths", ",", "\n", "self", ".", "token_probabilities", ",", "\n", "tf", ".", "zeros_like", "(", "self", ".", "token_probabilities", ")", ")", "\n", "\n", "if", "self", ".", "config", "[", "\"attention_objective_weight\"", "]", ">", "0.0", ":", "\n", "                ", "attention_loss", "=", "compute_attention_loss", "(", "\n", "self", ".", "token_probabilities", ",", "\n", "self", ".", "sentence_labels", ",", "\n", "num_sent_labels", "=", "len", "(", "self", ".", "label2id_sent", ")", ",", "\n", "num_tok_labels", "=", "len", "(", "self", ".", "label2id_tok", ")", ",", "\n", "approach", "=", "self", ".", "config", "[", "\"aux_loss_approach\"", "]", ",", "\n", "compute_pairwise", "=", "self", ".", "config", "[", "\"compute_pairwise_attention\"", "]", ")", "\n", "self", ".", "loss", "+=", "self", ".", "config", "[", "\"attention_objective_weight\"", "]", "*", "tf", ".", "reduce_sum", "(", "attention_loss", ")", "\n", "\n", "# Apply a gap-distance loss.", "\n", "", "if", "self", ".", "config", "[", "\"gap_objective_weight\"", "]", ">", "0.0", ":", "\n", "                ", "gap_distance_loss", "=", "compute_gap_distance_loss", "(", "\n", "self", ".", "token_probabilities", ",", "\n", "self", ".", "sentence_labels", ",", "\n", "num_sent_labels", "=", "len", "(", "self", ".", "label2id_sent", ")", ",", "\n", "num_tok_labels", "=", "len", "(", "self", ".", "label2id_tok", ")", ",", "\n", "minimum_gap_distance", "=", "self", ".", "config", "[", "\"minimum_gap_distance\"", "]", ",", "\n", "approach", "=", "self", ".", "config", "[", "\"aux_loss_approach\"", "]", ",", "\n", "type_distance", "=", "self", ".", "config", "[", "\"type_distance\"", "]", ")", "\n", "self", ".", "loss", "+=", "self", ".", "config", "[", "\"gap_objective_weight\"", "]", "*", "tf", ".", "reduce_sum", "(", "gap_distance_loss", ")", "\n", "\n", "# Include a word-based language modelling loss, LMw.", "\n", "", "", "if", "self", ".", "config", "[", "\"lm_cost_lstm_gamma\"", "]", ">", "0.0", ":", "\n", "            ", "self", ".", "loss", "+=", "self", ".", "config", "[", "\"lm_cost_lstm_gamma\"", "]", "*", "self", ".", "construct_lm_cost", "(", "\n", "input_tensor_fw", "=", "lstm_outputs_fw", ",", "\n", "input_tensor_bw", "=", "lstm_outputs_bw", ",", "\n", "sentence_lengths", "=", "self", ".", "sentence_lengths", ",", "\n", "target_ids", "=", "self", ".", "word_ids", ",", "\n", "lm_cost_type", "=", "\"separate\"", ",", "\n", "name", "=", "\"lm_cost_lstm_separate\"", ")", "\n", "\n", "", "if", "self", ".", "config", "[", "\"lm_cost_joint_lstm_gamma\"", "]", ">", "0.0", ":", "\n", "            ", "self", ".", "loss", "+=", "self", ".", "config", "[", "\"lm_cost_joint_lstm_gamma\"", "]", "*", "self", ".", "construct_lm_cost", "(", "\n", "input_tensor_fw", "=", "lstm_outputs_fw", ",", "\n", "input_tensor_bw", "=", "lstm_outputs_bw", ",", "\n", "sentence_lengths", "=", "self", ".", "sentence_lengths", ",", "\n", "target_ids", "=", "self", ".", "word_ids", ",", "\n", "lm_cost_type", "=", "\"joint\"", ",", "\n", "name", "=", "\"lm_cost_lstm_joint\"", ")", "\n", "\n", "", "self", ".", "train_op", "=", "self", ".", "construct_optimizer", "(", "\n", "opt_strategy", "=", "self", ".", "config", "[", "\"opt_strategy\"", "]", ",", "\n", "loss", "=", "self", ".", "loss", ",", "\n", "learning_rate", "=", "self", ".", "learning_rate", ",", "\n", "clip", "=", "self", ".", "config", "[", "\"clip\"", "]", ")", "\n", "print", "(", "\"Notwork built.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.variants.Model.construct_lm_cost": [[604, 650], ["tensorflow.variable_scope", "min", "tensorflow.where", "len", "tensorflow.greater_equal", "variants.Model._construct_lm_cost", "variants.Model._construct_lm_cost", "tensorflow.sequence_mask", "tensorflow.sequence_mask", "tensorflow.concat", "variants.Model._construct_lm_cost", "ValueError", "tensorflow.zeros_like", "tensorflow.sequence_mask", "tensorflow.shape", "tensorflow.shape", "tensorflow.shape"], "methods", ["home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model._construct_lm_cost", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model._construct_lm_cost", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model._construct_lm_cost"], ["", "def", "construct_lm_cost", "(", "\n", "self", ",", "input_tensor_fw", ",", "input_tensor_bw", ",", "\n", "sentence_lengths", ",", "target_ids", ",", "lm_cost_type", ",", "name", ")", ":", "\n", "        ", "\"\"\"\n        Constructs the char/word-based language modelling objective.\n        \"\"\"", "\n", "with", "tf", ".", "variable_scope", "(", "name", ")", ":", "\n", "            ", "lm_cost_max_vocab_size", "=", "min", "(", "\n", "len", "(", "self", ".", "word2id", ")", ",", "self", ".", "config", "[", "\"lm_cost_max_vocab_size\"", "]", ")", "\n", "target_ids", "=", "tf", ".", "where", "(", "\n", "tf", ".", "greater_equal", "(", "target_ids", ",", "lm_cost_max_vocab_size", "-", "1", ")", ",", "\n", "x", "=", "(", "lm_cost_max_vocab_size", "-", "1", ")", "+", "tf", ".", "zeros_like", "(", "target_ids", ")", ",", "\n", "y", "=", "target_ids", ")", "\n", "cost", "=", "0.0", "\n", "if", "lm_cost_type", "==", "\"separate\"", ":", "\n", "                ", "lm_cost_fw_mask", "=", "tf", ".", "sequence_mask", "(", "\n", "sentence_lengths", ",", "maxlen", "=", "tf", ".", "shape", "(", "target_ids", ")", "[", "1", "]", ")", "[", ":", ",", "1", ":", "]", "\n", "lm_cost_bw_mask", "=", "tf", ".", "sequence_mask", "(", "\n", "sentence_lengths", ",", "maxlen", "=", "tf", ".", "shape", "(", "target_ids", ")", "[", "1", "]", ")", "[", ":", ",", ":", "-", "1", "]", "\n", "lm_cost_fw", "=", "self", ".", "_construct_lm_cost", "(", "\n", "input_tensor_fw", "[", ":", ",", ":", "-", "1", ",", ":", "]", ",", "\n", "lm_cost_max_vocab_size", ",", "\n", "lm_cost_fw_mask", ",", "\n", "target_ids", "[", ":", ",", "1", ":", "]", ",", "\n", "name", "=", "name", "+", "\"_fw\"", ")", "\n", "lm_cost_bw", "=", "self", ".", "_construct_lm_cost", "(", "\n", "input_tensor_bw", "[", ":", ",", "1", ":", ",", ":", "]", ",", "\n", "lm_cost_max_vocab_size", ",", "\n", "lm_cost_bw_mask", ",", "\n", "target_ids", "[", ":", ",", ":", "-", "1", "]", ",", "\n", "name", "=", "name", "+", "\"_bw\"", ")", "\n", "cost", "+=", "lm_cost_fw", "+", "lm_cost_bw", "\n", "", "elif", "lm_cost_type", "==", "\"joint\"", ":", "\n", "                ", "joint_input_tensor", "=", "tf", ".", "concat", "(", "\n", "[", "input_tensor_fw", "[", ":", ",", ":", "-", "2", ",", ":", "]", ",", "input_tensor_bw", "[", ":", ",", "2", ":", ",", ":", "]", "]", ",", "axis", "=", "-", "1", ")", "\n", "lm_cost_mask", "=", "tf", ".", "sequence_mask", "(", "\n", "sentence_lengths", ",", "maxlen", "=", "tf", ".", "shape", "(", "target_ids", ")", "[", "1", "]", ")", "[", ":", ",", "1", ":", "-", "1", "]", "\n", "cost", "+=", "self", ".", "_construct_lm_cost", "(", "\n", "joint_input_tensor", ",", "\n", "lm_cost_max_vocab_size", ",", "\n", "lm_cost_mask", ",", "\n", "target_ids", "[", ":", ",", "1", ":", "-", "1", "]", ",", "\n", "name", "=", "name", "+", "\"_joint\"", ")", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"Unknown lm_cost_type: %s.\"", "%", "lm_cost_type", ")", "\n", "", "return", "cost", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.variants.Model._construct_lm_cost": [[651, 665], ["tensorflow.variable_scope", "tensorflow.layers.dense", "tensorflow.layers.dense", "tensorflow.nn.sparse_softmax_cross_entropy_with_logits", "tensorflow.where", "tensorflow.reduce_sum", "tensorflow.zeros_like"], "methods", ["None"], ["", "", "def", "_construct_lm_cost", "(", "\n", "self", ",", "input_tensor", ",", "lm_cost_max_vocab_size", ",", "\n", "lm_cost_mask", ",", "target_ids", ",", "name", ")", ":", "\n", "        ", "with", "tf", ".", "variable_scope", "(", "name", ")", ":", "\n", "            ", "lm_cost_hidden_layer", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", "=", "input_tensor", ",", "units", "=", "self", ".", "config", "[", "\"lm_cost_hidden_layer_size\"", "]", ",", "\n", "activation", "=", "tf", ".", "tanh", ",", "kernel_initializer", "=", "self", ".", "initializer", ")", "\n", "lm_cost_output", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", "=", "lm_cost_hidden_layer", ",", "units", "=", "lm_cost_max_vocab_size", ",", "\n", "kernel_initializer", "=", "self", ".", "initializer", ")", "\n", "lm_cost_loss", "=", "tf", ".", "nn", ".", "sparse_softmax_cross_entropy_with_logits", "(", "\n", "logits", "=", "lm_cost_output", ",", "labels", "=", "target_ids", ")", "\n", "lm_cost_loss", "=", "tf", ".", "where", "(", "lm_cost_mask", ",", "lm_cost_loss", ",", "tf", ".", "zeros_like", "(", "lm_cost_loss", ")", ")", "\n", "return", "tf", ".", "reduce_sum", "(", "lm_cost_loss", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.variants.Model.construct_optimizer": [[666, 687], ["tensorflow.train.AdadeltaOptimizer", "zip", "tensorflow.clip_by_global_norm", "tensorflow.train.GradientDescentOptimizer.apply_gradients", "tensorflow.train.GradientDescentOptimizer.minimize", "tensorflow.train.AdamOptimizer", "zip", "tensorflow.train.GradientDescentOptimizer", "ValueError", "tensorflow.train.GradientDescentOptimizer.compute_gradients"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "construct_optimizer", "(", "opt_strategy", ",", "loss", ",", "learning_rate", ",", "clip", ")", ":", "\n", "        ", "\"\"\"\n        Applies an optimization strategy to minimize the loss.\n        \"\"\"", "\n", "if", "opt_strategy", "==", "\"adadelta\"", ":", "\n", "            ", "optimizer", "=", "tf", ".", "train", ".", "AdadeltaOptimizer", "(", "learning_rate", "=", "learning_rate", ")", "\n", "", "elif", "opt_strategy", "==", "\"adam\"", ":", "\n", "            ", "optimizer", "=", "tf", ".", "train", ".", "AdamOptimizer", "(", "learning_rate", "=", "learning_rate", ")", "\n", "", "elif", "opt_strategy", "==", "\"sgd\"", ":", "\n", "            ", "optimizer", "=", "tf", ".", "train", ".", "GradientDescentOptimizer", "(", "learning_rate", "=", "learning_rate", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unknown optimisation strategy: %s.\"", "%", "opt_strategy", ")", "\n", "\n", "", "if", "clip", ">", "0.0", ":", "\n", "            ", "grads", ",", "vs", "=", "zip", "(", "*", "optimizer", ".", "compute_gradients", "(", "loss", ")", ")", "\n", "grads", ",", "gnorm", "=", "tf", ".", "clip_by_global_norm", "(", "grads", ",", "clip", ")", "\n", "train_op", "=", "optimizer", ".", "apply_gradients", "(", "zip", "(", "grads", ",", "vs", ")", ")", "\n", "", "else", ":", "\n", "            ", "train_op", "=", "optimizer", ".", "minimize", "(", "loss", ")", "\n", "", "return", "train_op", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.variants.Model.preload_word_embeddings": [[688, 712], ["set", "variants.Model.session.run", "variants.Model.session.run", "print", "open", "variants.Model.word_embeddings.assign", "line.strip().split", "len", "len", "re.sub.lower", "re.sub", "numpy.array", "set.add", "line.strip"], "methods", ["None"], ["", "def", "preload_word_embeddings", "(", "self", ",", "embedding_path", ")", ":", "\n", "        ", "\"\"\"\n        Load the word embeddings in advance to get a feel\n        of the proportion of singletons in the dataset.\n        \"\"\"", "\n", "loaded_embeddings", "=", "set", "(", ")", "\n", "embedding_matrix", "=", "self", ".", "session", ".", "run", "(", "self", ".", "word_embeddings", ")", "\n", "with", "open", "(", "embedding_path", ")", "as", "f", ":", "\n", "            ", "for", "line", "in", "f", ":", "\n", "                ", "line_parts", "=", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "if", "len", "(", "line_parts", ")", "<=", "2", ":", "\n", "                    ", "continue", "\n", "", "w", "=", "line_parts", "[", "0", "]", "\n", "if", "self", ".", "config", "[", "\"lowercase\"", "]", ":", "\n", "                    ", "w", "=", "w", ".", "lower", "(", ")", "\n", "", "if", "self", ".", "config", "[", "\"replace_digits\"", "]", ":", "\n", "                    ", "w", "=", "re", ".", "sub", "(", "r'\\d'", ",", "'0'", ",", "w", ")", "\n", "", "if", "w", "in", "self", ".", "word2id", "and", "w", "not", "in", "loaded_embeddings", ":", "\n", "                    ", "word_id", "=", "self", ".", "word2id", "[", "w", "]", "\n", "embedding", "=", "numpy", ".", "array", "(", "line_parts", "[", "1", ":", "]", ")", "\n", "embedding_matrix", "[", "word_id", "]", "=", "embedding", "\n", "loaded_embeddings", ".", "add", "(", "w", ")", "\n", "", "", "", "self", ".", "session", ".", "run", "(", "self", ".", "word_embeddings", ".", "assign", "(", "embedding_matrix", ")", ")", "\n", "print", "(", "\"No. of pre-loaded embeddings: %d.\"", "%", "len", "(", "loaded_embeddings", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.variants.Model.translate2id": [[713, 736], ["re.sub.lower", "re.sub", "numpy.random.uniform", "ValueError"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "translate2id", "(", "\n", "token", ",", "token2id", ",", "unk_token", "=", "None", ",", "lowercase", "=", "False", ",", "\n", "replace_digits", "=", "False", ",", "singletons", "=", "None", ",", "singletons_prob", "=", "0.0", ")", ":", "\n", "        ", "\"\"\"\n        Maps each token/character to its index.\n        \"\"\"", "\n", "if", "lowercase", ":", "\n", "            ", "token", "=", "token", ".", "lower", "(", ")", "\n", "", "if", "replace_digits", ":", "\n", "            ", "token", "=", "re", ".", "sub", "(", "r'\\d'", ",", "'0'", ",", "token", ")", "\n", "\n", "", "if", "singletons", "and", "token", "in", "singletons", "and", "token", "in", "token2id", "and", "unk_token", "and", "numpy", ".", "random", ".", "uniform", "(", ")", "<", "singletons_prob", ":", "\n", "            ", "token_id", "=", "token2id", "[", "unk_token", "]", "\n", "", "elif", "token", "in", "token2id", ":", "\n", "            ", "token_id", "=", "token2id", "[", "token", "]", "\n", "", "elif", "unk_token", ":", "\n", "            ", "token_id", "=", "token2id", "[", "unk_token", "]", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unable to handle value, no UNK token: %s.\"", "%", "token", ")", "\n", "", "return", "token_id", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.variants.Model.create_input_dictionary_for_batch": [[737, 810], ["numpy.array", "numpy.array.max", "numpy.array().max", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "enumerate", "min", "len", "len", "enumerate", "len", "numpy.array", "len", "len", "len", "len", "len", "variants.Model.translate2id", "len", "range", "min", "variants.Model.translate2id", "numpy.array().max", "len", "numpy.array", "len"], "methods", ["home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.translate2id", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.translate2id"], ["", "def", "create_input_dictionary_for_batch", "(", "self", ",", "batch", ",", "is_training", ",", "learning_rate", ")", ":", "\n", "        ", "\"\"\"\n        Creates the dictionary fed to the the TF model.\n        \"\"\"", "\n", "sentence_lengths", "=", "numpy", ".", "array", "(", "[", "len", "(", "sentence", ".", "tokens", ")", "for", "sentence", "in", "batch", "]", ")", "\n", "max_sentence_length", "=", "sentence_lengths", ".", "max", "(", ")", "\n", "max_word_length", "=", "numpy", ".", "array", "(", "\n", "[", "numpy", ".", "array", "(", "[", "len", "(", "token", ".", "value", ")", "for", "token", "in", "sentence", ".", "tokens", "]", ")", ".", "max", "(", ")", "\n", "for", "sentence", "in", "batch", "]", ")", ".", "max", "(", ")", "\n", "\n", "if", "0", "<", "self", ".", "config", "[", "\"allowed_word_length\"", "]", "<", "max_word_length", ":", "\n", "            ", "max_word_length", "=", "min", "(", "max_word_length", ",", "self", ".", "config", "[", "\"allowed_word_length\"", "]", ")", "\n", "\n", "", "word_ids", "=", "numpy", ".", "zeros", "(", "\n", "(", "len", "(", "batch", ")", ",", "max_sentence_length", ")", ",", "dtype", "=", "numpy", ".", "int32", ")", "\n", "char_ids", "=", "numpy", ".", "zeros", "(", "\n", "(", "len", "(", "batch", ")", ",", "max_sentence_length", ",", "max_word_length", ")", ",", "dtype", "=", "numpy", ".", "int32", ")", "\n", "word_lengths", "=", "numpy", ".", "zeros", "(", "\n", "(", "len", "(", "batch", ")", ",", "max_sentence_length", ")", ",", "dtype", "=", "numpy", ".", "int32", ")", "\n", "word_labels", "=", "numpy", ".", "zeros", "(", "\n", "(", "len", "(", "batch", ")", ",", "max_sentence_length", ")", ",", "dtype", "=", "numpy", ".", "float32", ")", "\n", "sentence_labels", "=", "numpy", ".", "zeros", "(", "\n", "(", "len", "(", "batch", ")", ")", ",", "dtype", "=", "numpy", ".", "float32", ")", "\n", "word_objective_weights", "=", "numpy", ".", "zeros", "(", "\n", "(", "len", "(", "batch", ")", ",", "max_sentence_length", ")", ",", "dtype", "=", "numpy", ".", "float32", ")", "\n", "sentence_objective_weights", "=", "numpy", ".", "zeros", "(", "(", "len", "(", "batch", ")", ")", ",", "dtype", "=", "numpy", ".", "float32", ")", "\n", "\n", "# A proportion of the singletons are assigned to UNK (do this just for training).", "\n", "singletons", "=", "self", ".", "singletons", "if", "is_training", "else", "None", "\n", "singletons_prob", "=", "self", ".", "config", "[", "\"singletons_prob\"", "]", "if", "is_training", "else", "0.0", "\n", "\n", "for", "i", ",", "sentence", "in", "enumerate", "(", "batch", ")", ":", "\n", "            ", "sentence_labels", "[", "i", "]", "=", "sentence", ".", "label_sent", "\n", "\n", "if", "sentence_labels", "[", "i", "]", "!=", "0", ":", "\n", "                ", "if", "self", ".", "config", "[", "\"sentence_objective_weights_non_default\"", "]", ">", "0.0", ":", "\n", "                    ", "sentence_objective_weights", "[", "i", "]", "=", "self", ".", "config", "[", "\n", "\"sentence_objective_weights_non_default\"", "]", "\n", "", "else", ":", "\n", "                    ", "sentence_objective_weights", "[", "i", "]", "=", "1.0", "\n", "", "", "else", ":", "\n", "                ", "sentence_objective_weights", "[", "i", "]", "=", "1.0", "\n", "\n", "", "for", "j", ",", "token", "in", "enumerate", "(", "sentence", ".", "tokens", ")", ":", "\n", "                ", "word_ids", "[", "i", "]", "[", "j", "]", "=", "self", ".", "translate2id", "(", "\n", "token", "=", "token", ".", "value", ",", "\n", "token2id", "=", "self", ".", "word2id", ",", "\n", "unk_token", "=", "self", ".", "UNK", ",", "\n", "lowercase", "=", "self", ".", "config", "[", "\"lowercase\"", "]", ",", "\n", "replace_digits", "=", "self", ".", "config", "[", "\"replace_digits\"", "]", ",", "\n", "singletons", "=", "singletons", ",", "\n", "singletons_prob", "=", "singletons_prob", ")", "\n", "word_labels", "[", "i", "]", "[", "j", "]", "=", "token", ".", "label_tok", "\n", "word_lengths", "[", "i", "]", "[", "j", "]", "=", "len", "(", "token", ".", "value", ")", "\n", "for", "k", "in", "range", "(", "min", "(", "len", "(", "token", ".", "value", ")", ",", "max_word_length", ")", ")", ":", "\n", "                    ", "char_ids", "[", "i", "]", "[", "j", "]", "[", "k", "]", "=", "self", ".", "translate2id", "(", "\n", "token", "=", "token", ".", "value", "[", "k", "]", ",", "\n", "token2id", "=", "self", ".", "char2id", ",", "\n", "unk_token", "=", "self", ".", "CUNK", ")", "\n", "", "if", "token", ".", "enable_supervision", "is", "True", ":", "\n", "                    ", "word_objective_weights", "[", "i", "]", "[", "j", "]", "=", "1.0", "\n", "\n", "", "", "", "input_dictionary", "=", "{", "\n", "self", ".", "word_ids", ":", "word_ids", ",", "\n", "self", ".", "char_ids", ":", "char_ids", ",", "\n", "self", ".", "sentence_lengths", ":", "sentence_lengths", ",", "\n", "self", ".", "word_lengths", ":", "word_lengths", ",", "\n", "self", ".", "sentence_labels", ":", "sentence_labels", ",", "\n", "self", ".", "word_labels", ":", "word_labels", ",", "\n", "self", ".", "word_objective_weights", ":", "word_objective_weights", ",", "\n", "self", ".", "learning_rate", ":", "learning_rate", ",", "\n", "self", ".", "is_training", ":", "is_training", "}", "\n", "return", "input_dictionary", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.variants.Model.process_batch": [[811, 826], ["variants.Model.create_input_dictionary_for_batch", "variants.Model.session.run"], "methods", ["home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.create_input_dictionary_for_batch"], ["", "def", "process_batch", "(", "self", ",", "batch", ",", "is_training", ",", "learning_rate", ")", ":", "\n", "        ", "\"\"\"\n        Processes a batch of sentences.\n        :param batch: a set of sentences of size \"max_batch_size\".\n        :param is_training: whether the current batch is a training instance or not.\n        :param learning_rate: the pace at which learning should be performed.\n        :return: the cost, the sentence predictions, the sentence label distribution,\n        the token predictions and the token label distribution.\n        \"\"\"", "\n", "feed_dict", "=", "self", ".", "create_input_dictionary_for_batch", "(", "batch", ",", "is_training", ",", "learning_rate", ")", "\n", "cost", ",", "sentence_pred", ",", "sentence_prob", ",", "token_pred", ",", "token_prob", "=", "self", ".", "session", ".", "run", "(", "\n", "[", "self", ".", "loss", ",", "self", ".", "sentence_predictions", ",", "self", ".", "sentence_probabilities", ",", "\n", "self", ".", "token_predictions", ",", "self", ".", "token_probabilities", "]", "+", "\n", "(", "[", "self", ".", "train_op", "]", "if", "is_training", "else", "[", "]", ")", ",", "feed_dict", "=", "feed_dict", ")", "[", ":", "5", "]", "\n", "return", "cost", ",", "sentence_pred", ",", "sentence_prob", ",", "token_pred", ",", "token_prob", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.variants.Model.initialize_session": [[827, 839], ["tensorflow.set_random_seed", "tensorflow.ConfigProto", "tensorflow.Session", "variants.Model.session.run", "tensorflow.train.Saver", "tensorflow.global_variables_initializer"], "methods", ["None"], ["", "def", "initialize_session", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Initializes a tensorflow session and sets the random seed.\n        \"\"\"", "\n", "tf", ".", "set_random_seed", "(", "self", ".", "config", "[", "\"random_seed\"", "]", ")", "\n", "session_config", "=", "tf", ".", "ConfigProto", "(", ")", "\n", "session_config", ".", "gpu_options", ".", "allow_growth", "=", "self", ".", "config", "[", "\"tf_allow_growth\"", "]", "\n", "session_config", ".", "gpu_options", ".", "per_process_gpu_memory_fraction", "=", "self", ".", "config", "[", "\n", "\"tf_per_process_gpu_memory_fraction\"", "]", "\n", "self", ".", "session", "=", "tf", ".", "Session", "(", "config", "=", "session_config", ")", "\n", "self", ".", "session", ".", "run", "(", "tf", ".", "global_variables_initializer", "(", ")", ")", "\n", "self", ".", "saver", "=", "tf", ".", "train", ".", "Saver", "(", "max_to_keep", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.variants.Model.get_parameter_count": [[840, 853], ["tensorflow.trainable_variables", "variable.get_shape"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_parameter_count", "(", ")", ":", "\n", "        ", "\"\"\"\n        Counts the total number of parameters.\n        \"\"\"", "\n", "total_parameters", "=", "0", "\n", "for", "variable", "in", "tf", ".", "trainable_variables", "(", ")", ":", "\n", "            ", "shape", "=", "variable", ".", "get_shape", "(", ")", "\n", "variable_parameters", "=", "1", "\n", "for", "dim", "in", "shape", ":", "\n", "                ", "variable_parameters", "*=", "dim", ".", "value", "\n", "", "total_parameters", "+=", "variable_parameters", "\n", "", "return", "total_parameters", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.variants.Model.get_parameter_count_without_word_embeddings": [[854, 863], ["variants.Model.word_embeddings.get_shape", "variants.Model.get_parameter_count"], "methods", ["home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.get_parameter_count"], ["", "def", "get_parameter_count_without_word_embeddings", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Counts the number of parameters without those introduced by word embeddings.\n        \"\"\"", "\n", "shape", "=", "self", ".", "word_embeddings", ".", "get_shape", "(", ")", "\n", "variable_parameters", "=", "1", "\n", "for", "dim", "in", "shape", ":", "\n", "            ", "variable_parameters", "*=", "dim", ".", "value", "\n", "", "return", "self", ".", "get_parameter_count", "(", ")", "-", "variable_parameters", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.variants.Model.save": [[864, 886], ["dict", "tensorflow.global_variables", "variants.Model.session.run", "open", "pickle.dump"], "methods", ["None"], ["", "def", "save", "(", "self", ",", "filename", ")", ":", "\n", "        ", "\"\"\"\n        Saves a trained model to the path in filename.\n        \"\"\"", "\n", "dump", "=", "dict", "(", ")", "\n", "dump", "[", "\"config\"", "]", "=", "self", ".", "config", "\n", "dump", "[", "\"label2id_sent\"", "]", "=", "self", ".", "label2id_sent", "\n", "dump", "[", "\"label2id_tok\"", "]", "=", "self", ".", "label2id_tok", "\n", "dump", "[", "\"UNK\"", "]", "=", "self", ".", "UNK", "\n", "dump", "[", "\"CUNK\"", "]", "=", "self", ".", "CUNK", "\n", "dump", "[", "\"word2id\"", "]", "=", "self", ".", "word2id", "\n", "dump", "[", "\"char2id\"", "]", "=", "self", ".", "char2id", "\n", "dump", "[", "\"singletons\"", "]", "=", "self", ".", "singletons", "\n", "\n", "dump", "[", "\"params\"", "]", "=", "{", "}", "\n", "for", "variable", "in", "tf", ".", "global_variables", "(", ")", ":", "\n", "            ", "assert", "(", "\n", "variable", ".", "name", "not", "in", "dump", "[", "\"params\"", "]", ")", ",", "\"Error: variable with this name already exists: %s.\"", "%", "variable", ".", "name", "\n", "dump", "[", "\"params\"", "]", "[", "variable", ".", "name", "]", "=", "self", ".", "session", ".", "run", "(", "variable", ")", "\n", "", "with", "open", "(", "filename", ",", "'wb'", ")", "as", "f", ":", "\n", "            ", "pickle", ".", "dump", "(", "dump", ",", "f", ",", "protocol", "=", "pickle", ".", "HIGHEST_PROTOCOL", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.variants.Model.load": [[887, 914], ["open", "pickle.load", "variants.Model", "variants.Model.construct_network", "variants.Model.initialize_session", "variants.Model.load_params"], "methods", ["home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.load", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.construct_network", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.initialize_session", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.load_params"], ["", "", "@", "staticmethod", "\n", "def", "load", "(", "filename", ",", "new_config", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Loads a pre-trained MHAL model.\n        \"\"\"", "\n", "with", "open", "(", "filename", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "dump", "=", "pickle", ".", "load", "(", "f", ")", "\n", "dump", "[", "\"config\"", "]", "[", "\"save\"", "]", "=", "None", "\n", "\n", "# Use the saved config, except for values that are present in the new config.", "\n", "if", "new_config", ":", "\n", "                ", "for", "key", "in", "new_config", ":", "\n", "                    ", "dump", "[", "\"config\"", "]", "[", "key", "]", "=", "new_config", "[", "key", "]", "\n", "\n", "", "", "labeler", "=", "Model", "(", "dump", "[", "\"config\"", "]", ",", "dump", "[", "\"label2id_sent\"", "]", ",", "dump", "[", "\"label2id_tok\"", "]", ")", "\n", "labeler", ".", "UNK", "=", "dump", "[", "\"UNK\"", "]", "\n", "labeler", ".", "CUNK", "=", "dump", "[", "\"CUNK\"", "]", "\n", "labeler", ".", "word2id", "=", "dump", "[", "\"word2id\"", "]", "\n", "labeler", ".", "char2id", "=", "dump", "[", "\"char2id\"", "]", "\n", "labeler", ".", "singletons", "=", "dump", "[", "\"singletons\"", "]", "\n", "\n", "labeler", ".", "construct_network", "(", ")", "\n", "labeler", ".", "initialize_session", "(", ")", "\n", "\n", "labeler", ".", "load_params", "(", "filename", ")", "\n", "\n", "return", "labeler", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.variants.Model.load_params": [[915, 931], ["open", "pickle.load", "tensorflow.global_variables", "numpy.asarray", "variants.Model.session.run", "variable.assign", "str", "str"], "methods", ["home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.load"], ["", "", "def", "load_params", "(", "self", ",", "filename", ")", ":", "\n", "        ", "\"\"\"\n        Loads the parameters of a trained model.\n        \"\"\"", "\n", "with", "open", "(", "filename", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "dump", "=", "pickle", ".", "load", "(", "f", ")", "\n", "\n", "for", "variable", "in", "tf", ".", "global_variables", "(", ")", ":", "\n", "                ", "assert", "(", "variable", ".", "name", "in", "dump", "[", "\"params\"", "]", ")", ",", "\"Variable not in dump: %s.\"", "%", "variable", ".", "name", "\n", "assert", "(", "variable", ".", "shape", "==", "dump", "[", "\"params\"", "]", "[", "variable", ".", "name", "]", ".", "shape", ")", ",", "\"Variable shape not as expected: %s, of shape %s. %s\"", "%", "(", "\n", "variable", ".", "name", ",", "str", "(", "variable", ".", "shape", ")", ",", "\n", "str", "(", "dump", "[", "\"params\"", "]", "[", "variable", ".", "name", "]", ".", "shape", ")", ")", "\n", "value", "=", "numpy", ".", "asarray", "(", "dump", "[", "\"params\"", "]", "[", "variable", ".", "name", "]", ")", "\n", "self", ".", "session", ".", "run", "(", "variable", ".", "assign", "(", "value", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.conlleval.EvalCounts.__init__": [[29, 40], ["collections.defaultdict", "collections.defaultdict", "collections.defaultdict"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "correct_chunk", "=", "0", "# number of correctly identified chunks", "\n", "self", ".", "correct_tags", "=", "0", "# number of correct chunk tags", "\n", "self", ".", "found_correct", "=", "0", "# number of chunks in corpus", "\n", "self", ".", "found_guessed", "=", "0", "# number of identified chunks", "\n", "self", ".", "token_counter", "=", "0", "# token counter (ignores sentence breaks)", "\n", "\n", "# counts by type", "\n", "self", ".", "t_correct_chunk", "=", "defaultdict", "(", "int", ")", "\n", "self", ".", "t_found_correct", "=", "defaultdict", "(", "int", ")", "\n", "self", ".", "t_found_guessed", "=", "defaultdict", "(", "int", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.conlleval.parse_args": [[42, 57], ["argparse.ArgumentParser", "arg", "arg", "arg", "arg", "argparse.ArgumentParser.parse_args"], "function", ["home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.conlleval.parse_args"], ["", "", "def", "parse_args", "(", "argv", ")", ":", "\n", "    ", "import", "argparse", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "description", "=", "'evaluate tagging results using CoNLL criteria'", ",", "\n", "formatter_class", "=", "argparse", ".", "ArgumentDefaultsHelpFormatter", "\n", ")", "\n", "arg", "=", "parser", ".", "add_argument", "\n", "arg", "(", "'-b'", ",", "'--boundary'", ",", "metavar", "=", "'STR'", ",", "default", "=", "'-X-'", ",", "\n", "help", "=", "'sentence boundary'", ")", "\n", "arg", "(", "'-d'", ",", "'--delimiter'", ",", "metavar", "=", "'CHAR'", ",", "default", "=", "ANY_SPACE", ",", "\n", "help", "=", "'character delimiting items in input'", ")", "\n", "arg", "(", "'-o'", ",", "'--otag'", ",", "metavar", "=", "'CHAR'", ",", "default", "=", "'O'", ",", "\n", "help", "=", "'alternative outside tag'", ")", "\n", "arg", "(", "'file'", ",", "nargs", "=", "'?'", ",", "default", "=", "None", ")", "\n", "return", "parser", ".", "parse_args", "(", "argv", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.conlleval.parse_tag": [[59, 62], ["re.match", "re.match.groups"], "function", ["None"], ["", "def", "parse_tag", "(", "t", ")", ":", "\n", "    ", "m", "=", "re", ".", "match", "(", "r'^([^-]*)-(.*)$'", ",", "t", ")", "\n", "return", "m", ".", "groups", "(", ")", "if", "m", "else", "(", "t", ",", "''", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.conlleval.evaluate": [[64, 143], ["conlleval.EvalCounts", "conlleval.parse_args", "line.rstrip.rstrip", "conlleval.parse_tag", "conlleval.parse_tag", "line.split.pop", "conlleval.end_of_chunk", "conlleval.end_of_chunk", "conlleval.start_of_chunk", "conlleval.start_of_chunk", "line.rstrip.split", "line.rstrip.split", "len", "len", "conlleval.FormatError", "line.split.pop", "line.split.pop", "conlleval.FormatError", "len", "len", "len", "len", "sys.stdin"], "function", ["home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.conlleval.parse_args", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.conlleval.parse_tag", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.conlleval.parse_tag", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.conlleval.end_of_chunk", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.conlleval.end_of_chunk", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.conlleval.start_of_chunk", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.conlleval.start_of_chunk"], ["", "def", "evaluate", "(", "iterable", ",", "options", "=", "None", ")", ":", "\n", "    ", "if", "options", "is", "None", ":", "\n", "        ", "options", "=", "parse_args", "(", "[", "]", ")", "# use defaults", "\n", "\n", "", "counts", "=", "EvalCounts", "(", ")", "\n", "num_features", "=", "None", "# number of features per line", "\n", "in_correct", "=", "False", "# currently processed chunks is correct until now", "\n", "last_correct", "=", "'O'", "# previous chunk tag in corpus", "\n", "last_correct_type", "=", "''", "# type of previously identified chunk tag", "\n", "last_guessed", "=", "'O'", "# previously identified chunk tag", "\n", "last_guessed_type", "=", "''", "# type of previous chunk tag in corpus", "\n", "\n", "for", "line", "in", "iterable", ":", "\n", "        ", "line", "=", "line", ".", "rstrip", "(", "'\\r\\n'", ")", "\n", "\n", "if", "options", ".", "delimiter", "==", "ANY_SPACE", ":", "\n", "            ", "features", "=", "line", ".", "split", "(", ")", "\n", "", "else", ":", "\n", "            ", "features", "=", "line", ".", "split", "(", "options", ".", "delimiter", ")", "\n", "\n", "", "if", "num_features", "is", "None", ":", "\n", "            ", "num_features", "=", "len", "(", "features", ")", "\n", "", "elif", "num_features", "!=", "len", "(", "features", ")", "and", "len", "(", "features", ")", "!=", "0", ":", "\n", "            ", "raise", "FormatError", "(", "'unexpected number of features: %d (%d)'", "%", "\n", "(", "len", "(", "features", ")", ",", "num_features", ")", ")", "\n", "\n", "", "if", "len", "(", "features", ")", "==", "0", "or", "features", "[", "0", "]", "==", "options", ".", "boundary", ":", "\n", "            ", "features", "=", "[", "options", ".", "boundary", ",", "'O'", ",", "'O'", "]", "\n", "", "if", "len", "(", "features", ")", "<", "3", ":", "\n", "            ", "raise", "FormatError", "(", "'unexpected number of features in line %s'", "%", "line", ")", "\n", "\n", "", "guessed", ",", "guessed_type", "=", "parse_tag", "(", "features", ".", "pop", "(", ")", ")", "\n", "correct", ",", "correct_type", "=", "parse_tag", "(", "features", ".", "pop", "(", ")", ")", "\n", "first_item", "=", "features", ".", "pop", "(", "0", ")", "\n", "\n", "if", "first_item", "==", "options", ".", "boundary", ":", "\n", "            ", "guessed", "=", "'O'", "\n", "\n", "", "end_correct", "=", "end_of_chunk", "(", "last_correct", ",", "correct", ",", "\n", "last_correct_type", ",", "correct_type", ")", "\n", "end_guessed", "=", "end_of_chunk", "(", "last_guessed", ",", "guessed", ",", "\n", "last_guessed_type", ",", "guessed_type", ")", "\n", "start_correct", "=", "start_of_chunk", "(", "last_correct", ",", "correct", ",", "\n", "last_correct_type", ",", "correct_type", ")", "\n", "start_guessed", "=", "start_of_chunk", "(", "last_guessed", ",", "guessed", ",", "\n", "last_guessed_type", ",", "guessed_type", ")", "\n", "\n", "if", "in_correct", ":", "\n", "            ", "if", "end_correct", "and", "end_guessed", "and", "last_guessed_type", "==", "last_correct_type", ":", "\n", "                ", "in_correct", "=", "False", "\n", "counts", ".", "correct_chunk", "+=", "1", "\n", "counts", ".", "t_correct_chunk", "[", "last_correct_type", "]", "+=", "1", "\n", "", "elif", "end_correct", "!=", "end_guessed", "or", "guessed_type", "!=", "correct_type", ":", "\n", "                ", "in_correct", "=", "False", "\n", "\n", "", "", "if", "start_correct", "and", "start_guessed", "and", "guessed_type", "==", "correct_type", ":", "\n", "            ", "in_correct", "=", "True", "\n", "\n", "", "if", "start_correct", ":", "\n", "            ", "counts", ".", "found_correct", "+=", "1", "\n", "counts", ".", "t_found_correct", "[", "correct_type", "]", "+=", "1", "\n", "", "if", "start_guessed", ":", "\n", "            ", "counts", ".", "found_guessed", "+=", "1", "\n", "counts", ".", "t_found_guessed", "[", "guessed_type", "]", "+=", "1", "\n", "", "if", "first_item", "!=", "options", ".", "boundary", ":", "\n", "            ", "if", "correct", "==", "guessed", "and", "guessed_type", "==", "correct_type", ":", "\n", "                ", "counts", ".", "correct_tags", "+=", "1", "\n", "", "counts", ".", "token_counter", "+=", "1", "\n", "\n", "", "last_guessed", "=", "guessed", "\n", "last_correct", "=", "correct", "\n", "last_guessed_type", "=", "guessed_type", "\n", "last_correct_type", "=", "correct_type", "\n", "\n", "", "if", "in_correct", ":", "\n", "        ", "counts", ".", "correct_chunk", "+=", "1", "\n", "counts", ".", "t_correct_chunk", "[", "last_correct_type", "]", "+=", "1", "\n", "\n", "", "return", "counts", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.conlleval.uniq": [[145, 148], ["set", "set.add"], "function", ["None"], ["", "def", "uniq", "(", "iterable", ")", ":", "\n", "    ", "seen", "=", "set", "(", ")", "\n", "return", "[", "i", "for", "i", "in", "iterable", "if", "not", "(", "i", "in", "seen", "or", "seen", ".", "add", "(", "i", ")", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.conlleval.calculate_metrics": [[150, 156], ["Metrics"], "function", ["None"], ["", "def", "calculate_metrics", "(", "correct", ",", "guessed", ",", "total", ")", ":", "\n", "    ", "tp", ",", "fp", ",", "fn", "=", "correct", ",", "guessed", "-", "correct", ",", "total", "-", "correct", "\n", "p", "=", "0", "if", "tp", "+", "fp", "==", "0", "else", "1.", "*", "tp", "/", "(", "tp", "+", "fp", ")", "\n", "r", "=", "0", "if", "tp", "+", "fn", "==", "0", "else", "1.", "*", "tp", "/", "(", "tp", "+", "fn", ")", "\n", "f", "=", "0", "if", "p", "+", "r", "==", "0", "else", "2", "*", "p", "*", "r", "/", "(", "p", "+", "r", ")", "\n", "return", "Metrics", "(", "tp", ",", "fp", ",", "fn", ",", "p", ",", "r", ",", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.conlleval.metrics": [[158, 169], ["conlleval.calculate_metrics", "conlleval.uniq", "conlleval.calculate_metrics", "list", "list", "c.t_found_correct.keys", "c.t_found_guessed.keys"], "function", ["home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.evaluator.Evaluator.calculate_metrics", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.conlleval.uniq", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.evaluator.Evaluator.calculate_metrics"], ["", "def", "metrics", "(", "counts", ")", ":", "\n", "    ", "c", "=", "counts", "\n", "overall", "=", "calculate_metrics", "(", "\n", "c", ".", "correct_chunk", ",", "c", ".", "found_guessed", ",", "c", ".", "found_correct", "\n", ")", "\n", "by_type", "=", "{", "}", "\n", "for", "t", "in", "uniq", "(", "list", "(", "c", ".", "t_found_correct", ".", "keys", "(", ")", ")", "+", "list", "(", "c", ".", "t_found_guessed", ".", "keys", "(", ")", ")", ")", ":", "\n", "        ", "by_type", "[", "t", "]", "=", "calculate_metrics", "(", "\n", "c", ".", "t_correct_chunk", "[", "t", "]", ",", "c", ".", "t_found_guessed", "[", "t", "]", ",", "c", ".", "t_found_correct", "[", "t", "]", "\n", ")", "\n", "", "return", "overall", ",", "by_type", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.conlleval.report": [[171, 195], ["conlleval.metrics", "out.write", "out.write", "sorted", "out.write", "out.write", "out.write", "out.write", "by_type.items", "out.write", "out.write", "out.write", "out.write"], "function", ["home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.conlleval.metrics", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Writer.write", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Writer.write", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Writer.write", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Writer.write", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Writer.write", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Writer.write", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Writer.write", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Writer.write", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Writer.write", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Writer.write"], ["", "def", "report", "(", "counts", ",", "out", "=", "None", ")", ":", "\n", "    ", "if", "out", "is", "None", ":", "\n", "        ", "out", "=", "sys", ".", "stdout", "\n", "\n", "", "overall", ",", "by_type", "=", "metrics", "(", "counts", ")", "\n", "\n", "c", "=", "counts", "\n", "out", ".", "write", "(", "'processed %d tokens with %d phrases; '", "%", "\n", "(", "c", ".", "token_counter", ",", "c", ".", "found_correct", ")", ")", "\n", "out", ".", "write", "(", "'found: %d phrases; correct: %d.\\n'", "%", "\n", "(", "c", ".", "found_guessed", ",", "c", ".", "correct_chunk", ")", ")", "\n", "\n", "if", "c", ".", "token_counter", ">", "0", ":", "\n", "        ", "out", ".", "write", "(", "'accuracy: %6.2f%%; '", "%", "\n", "(", "100.", "*", "c", ".", "correct_tags", "/", "c", ".", "token_counter", ")", ")", "\n", "out", ".", "write", "(", "'precision: %6.2f%%; '", "%", "(", "100.", "*", "overall", ".", "prec", ")", ")", "\n", "out", ".", "write", "(", "'recall: %6.2f%%; '", "%", "(", "100.", "*", "overall", ".", "rec", ")", ")", "\n", "out", ".", "write", "(", "'FB1: %6.2f\\n'", "%", "(", "100.", "*", "overall", ".", "fscore", ")", ")", "\n", "\n", "", "for", "i", ",", "m", "in", "sorted", "(", "by_type", ".", "items", "(", ")", ")", ":", "\n", "        ", "out", ".", "write", "(", "'%17s: '", "%", "i", ")", "\n", "out", ".", "write", "(", "'precision: %6.2f%%; '", "%", "(", "100.", "*", "m", ".", "prec", ")", ")", "\n", "out", ".", "write", "(", "'recall: %6.2f%%; '", "%", "(", "100.", "*", "m", ".", "rec", ")", ")", "\n", "out", ".", "write", "(", "'FB1: %6.2f  %d\\n'", "%", "(", "100.", "*", "m", ".", "fscore", ",", "c", ".", "t_found_guessed", "[", "i", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.conlleval.end_of_chunk": [[197, 230], ["None"], "function", ["None"], ["", "", "def", "end_of_chunk", "(", "prev_tag", ",", "tag", ",", "prev_type", ",", "type_", ")", ":", "\n", "# check if a chunk ended between the previous and current word", "\n", "# arguments: previous and current chunk tags, previous and current types", "\n", "    ", "chunk_end", "=", "False", "\n", "\n", "if", "prev_tag", "==", "'E'", ":", "\n", "        ", "chunk_end", "=", "True", "\n", "", "if", "prev_tag", "==", "'S'", ":", "\n", "        ", "chunk_end", "=", "True", "\n", "\n", "", "if", "prev_tag", "==", "'B'", "and", "tag", "==", "'B'", ":", "\n", "        ", "chunk_end", "=", "True", "\n", "", "if", "prev_tag", "==", "'B'", "and", "tag", "==", "'S'", ":", "\n", "        ", "chunk_end", "=", "True", "\n", "", "if", "prev_tag", "==", "'B'", "and", "tag", "==", "'O'", ":", "\n", "        ", "chunk_end", "=", "True", "\n", "", "if", "prev_tag", "==", "'I'", "and", "tag", "==", "'B'", ":", "\n", "        ", "chunk_end", "=", "True", "\n", "", "if", "prev_tag", "==", "'I'", "and", "tag", "==", "'S'", ":", "\n", "        ", "chunk_end", "=", "True", "\n", "", "if", "prev_tag", "==", "'I'", "and", "tag", "==", "'O'", ":", "\n", "        ", "chunk_end", "=", "True", "\n", "\n", "", "if", "prev_tag", "!=", "'O'", "and", "prev_tag", "!=", "'.'", "and", "prev_type", "!=", "type_", ":", "\n", "        ", "chunk_end", "=", "True", "\n", "\n", "# these chunks are assumed to have length 1", "\n", "", "if", "prev_tag", "==", "']'", ":", "\n", "        ", "chunk_end", "=", "True", "\n", "", "if", "prev_tag", "==", "'['", ":", "\n", "        ", "chunk_end", "=", "True", "\n", "\n", "", "return", "chunk_end", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.conlleval.start_of_chunk": [[232, 265], ["None"], "function", ["None"], ["", "def", "start_of_chunk", "(", "prev_tag", ",", "tag", ",", "prev_type", ",", "type_", ")", ":", "\n", "# check if a chunk started between the previous and current word", "\n", "# arguments: previous and current chunk tags, previous and current types", "\n", "    ", "chunk_start", "=", "False", "\n", "\n", "if", "tag", "==", "'B'", ":", "\n", "        ", "chunk_start", "=", "True", "\n", "", "if", "tag", "==", "'S'", ":", "\n", "        ", "chunk_start", "=", "True", "\n", "\n", "", "if", "prev_tag", "==", "'E'", "and", "tag", "==", "'E'", ":", "\n", "        ", "chunk_start", "=", "True", "\n", "", "if", "prev_tag", "==", "'E'", "and", "tag", "==", "'I'", ":", "\n", "        ", "chunk_start", "=", "True", "\n", "", "if", "prev_tag", "==", "'S'", "and", "tag", "==", "'E'", ":", "\n", "        ", "chunk_start", "=", "True", "\n", "", "if", "prev_tag", "==", "'S'", "and", "tag", "==", "'I'", ":", "\n", "        ", "chunk_start", "=", "True", "\n", "", "if", "prev_tag", "==", "'O'", "and", "tag", "==", "'E'", ":", "\n", "        ", "chunk_start", "=", "True", "\n", "", "if", "prev_tag", "==", "'O'", "and", "tag", "==", "'I'", ":", "\n", "        ", "chunk_start", "=", "True", "\n", "\n", "", "if", "tag", "!=", "'O'", "and", "tag", "!=", "'.'", "and", "prev_type", "!=", "type_", ":", "\n", "        ", "chunk_start", "=", "True", "\n", "\n", "# these chunks are assumed to have length 1", "\n", "", "if", "tag", "==", "'['", ":", "\n", "        ", "chunk_start", "=", "True", "\n", "", "if", "tag", "==", "']'", ":", "\n", "        ", "chunk_start", "=", "True", "\n", "\n", "", "return", "chunk_start", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.conlleval.main": [[267, 276], ["conlleval.parse_args", "conlleval.report", "conlleval.evaluate", "open", "conlleval.evaluate"], "function", ["home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.conlleval.parse_args", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.conlleval.report", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.conlleval.evaluate", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.conlleval.evaluate"], ["", "def", "main", "(", "argv", ")", ":", "\n", "    ", "args", "=", "parse_args", "(", "argv", "[", "1", ":", "]", ")", "\n", "\n", "if", "args", ".", "file", "is", "None", ":", "\n", "        ", "counts", "=", "evaluate", "(", "sys", ".", "stdin", ",", "args", ")", "\n", "", "else", ":", "\n", "        ", "with", "open", "(", "args", ".", "file", ")", "as", "f", ":", "\n", "            ", "counts", "=", "evaluate", "(", "f", ",", "args", ")", "\n", "", "", "report", "(", "counts", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.layer_normalization": [[5, 22], ["tensorflow.variable_scope", "tensorflow.nn.moments", "tensorflow.get_variable", "tensorflow.get_variable", "layer.get_shape", "tensorflow.zeros_initializer", "tensorflow.ones_initializer"], "function", ["None"], ["def", "layer_normalization", "(", "layer", ",", "epsilon", "=", "1e-8", ")", ":", "\n", "    ", "\"\"\"\n    Implements layer normalization.\n    :param layer: has 2-dimensional, the first dimension is the batch_size\n    :param epsilon: a small number to avoid numerical issues, such as zero division.\n    :return: normalized tensor, of the same shape as the input\n    \"\"\"", "\n", "with", "tf", ".", "variable_scope", "(", "\"layer_norm\"", ")", ":", "\n", "        ", "params_shape", "=", "layer", ".", "get_shape", "(", ")", "[", "-", "1", ":", "]", "\n", "mean", ",", "variance", "=", "tf", ".", "nn", ".", "moments", "(", "layer", ",", "[", "-", "1", "]", ",", "keep_dims", "=", "True", ")", "\n", "beta", "=", "tf", ".", "get_variable", "(", "\n", "name", "=", "\"beta\"", ",", "shape", "=", "params_shape", ",", "initializer", "=", "tf", ".", "zeros_initializer", "(", ")", ",", "trainable", "=", "True", ")", "\n", "gamma", "=", "tf", ".", "get_variable", "(", "\n", "name", "=", "\"gamma\"", ",", "shape", "=", "params_shape", ",", "initializer", "=", "tf", ".", "ones_initializer", "(", ")", ",", "trainable", "=", "True", ")", "\n", "normalized", "=", "(", "layer", "-", "mean", ")", "/", "(", "(", "variance", "+", "epsilon", ")", "**", "0.5", ")", "\n", "outputs", "=", "gamma", "*", "normalized", "+", "beta", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.division_masking": [[24, 41], ["tensorflow.sign", "tensorflow.tile", "tensorflow.where", "tensorflow.reduce_sum", "tensorflow.equal", "tensorflow.zeros_like", "tensorflow.div", "tensorflow.reduce_sum"], "function", ["None"], ["", "def", "division_masking", "(", "inputs", ",", "axis", ",", "multiplies", ")", ":", "\n", "    ", "\"\"\"\n    Masking used when dividing one element by the sum on a certain axis.\n    Division by 0 is not possible -- all values will be -infinity, instead.\n    :param inputs: the input needed to be divided\n    :param axis: axis on which to perform the reduced sum\n    :param multiplies: the shape to be used when tiling the division masks.\n    :return: the correct normalized inputs (with -infinity for divisions by 0).\n    \"\"\"", "\n", "division_masks", "=", "tf", ".", "sign", "(", "tf", ".", "reduce_sum", "(", "inputs", ",", "axis", "=", "axis", ",", "keep_dims", "=", "True", ")", ")", "\n", "division_masks", "=", "tf", ".", "tile", "(", "division_masks", ",", "multiples", "=", "multiplies", ")", "\n", "divided_inputs", "=", "tf", ".", "where", "(", "\n", "tf", ".", "equal", "(", "division_masks", ",", "0", ")", ",", "\n", "tf", ".", "zeros_like", "(", "inputs", ")", ",", "\n", "# tf.ones_like(inputs) * (-2 ** 32 + 1.0),", "\n", "tf", ".", "div", "(", "inputs", ",", "tf", ".", "reduce_sum", "(", "inputs", ",", "axis", "=", "axis", ",", "keep_dims", "=", "True", ")", ")", ")", "\n", "return", "divided_inputs", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.label_smoothing": [[43, 54], ["labels.get_shape().as_list", "labels.get_shape"], "function", ["None"], ["", "def", "label_smoothing", "(", "labels", ",", "epsilon", "=", "0.1", ")", ":", "\n", "    ", "\"\"\"\n    Implements label smoothing. This prevents the model from becoming\n    over-confident about its predictions and thus, less prone to overfitting.\n    Label smoothing regularizes the model and makes it more adaptable.\n    :param labels: 3D tensor with the last dimension as the number of labels\n    :param epsilon: smoothing rate\n    :return: smoothed labels\n    \"\"\"", "\n", "num_labels", "=", "labels", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "-", "1", "]", "\n", "return", "(", "(", "1", "-", "epsilon", ")", "*", "labels", ")", "+", "(", "epsilon", "/", "num_labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.mask": [[56, 79], ["tensorflow.sign", "tensorflow.expand_dims", "tensorflow.tile", "tensorflow.where", "tensorflow.reduce_sum", "tensorflow.ones_like", "tensorflow.equal", "tensorflow.sign", "tensorflow.expand_dims", "tensorflow.tile", "ValueError", "tensorflow.abs", "tensorflow.reduce_sum", "tensorflow.shape", "tensorflow.abs", "tensorflow.shape"], "function", ["None"], ["", "def", "mask", "(", "inputs", ",", "queries", "=", "None", ",", "keys", "=", "None", ",", "mask_type", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Generates masks and apply them to 3D inputs.\n    inputs: 3D tensor. [B, M, M]\n    queries: 3D tensor. [B, M, E]\n    keys: 3D tensor. [B, M, E]\n    \"\"\"", "\n", "padding_num", "=", "-", "2", "**", "32", "+", "1", "\n", "if", "\"key\"", "in", "mask_type", ":", "\n", "        ", "masks", "=", "tf", ".", "sign", "(", "tf", ".", "reduce_sum", "(", "tf", ".", "abs", "(", "keys", ")", ",", "axis", "=", "-", "1", ")", ")", "# [B, M]", "\n", "masks", "=", "tf", ".", "expand_dims", "(", "masks", ",", "axis", "=", "1", ")", "# [B, 1, M]", "\n", "masks", "=", "tf", ".", "tile", "(", "masks", ",", "[", "1", ",", "tf", ".", "shape", "(", "queries", ")", "[", "1", "]", ",", "1", "]", ")", "# [B, M, M]", "\n", "paddings", "=", "tf", ".", "ones_like", "(", "inputs", ")", "*", "padding_num", "\n", "outputs", "=", "tf", ".", "where", "(", "tf", ".", "equal", "(", "masks", ",", "0", ")", ",", "paddings", ",", "inputs", ")", "# [B, M, M]", "\n", "", "elif", "\"query\"", "in", "mask_type", ":", "\n", "        ", "masks", "=", "tf", ".", "sign", "(", "tf", ".", "reduce_sum", "(", "tf", ".", "abs", "(", "queries", ")", ",", "axis", "=", "-", "1", ")", ")", "# [B, M]", "\n", "masks", "=", "tf", ".", "expand_dims", "(", "masks", ",", "axis", "=", "-", "1", ")", "# [B, M, 1]", "\n", "masks", "=", "tf", ".", "tile", "(", "masks", ",", "[", "1", ",", "1", ",", "tf", ".", "shape", "(", "keys", ")", "[", "1", "]", "]", ")", "# [B, M, M]", "\n", "outputs", "=", "inputs", "*", "masks", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Unknown mask type: %s. You need to choose \"", "\n", "\"between \\\"keys\\\" and \\\"query\\\".\"", "%", "mask_type", ")", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.mask_2": [[81, 104], ["tensorflow.sign", "tensorflow.expand_dims", "tensorflow.tile", "tensorflow.where", "tensorflow.reduce_sum", "tensorflow.ones_like", "tensorflow.equal", "tensorflow.sign", "tensorflow.expand_dims", "tensorflow.tile", "ValueError", "tensorflow.abs", "tensorflow.reduce_sum", "tensorflow.shape", "tensorflow.abs", "tensorflow.shape"], "function", ["None"], ["", "def", "mask_2", "(", "inputs", ",", "queries", "=", "None", ",", "keys", "=", "None", ",", "mask_type", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Generates masks and apply them to 4D inputs.\n    inputs: 3D tensor. [H, B, M, M]\n    queries: 3D tensor. [H, B, M, E]\n    keys: 3D tensor. [H, B, M, E]\n    \"\"\"", "\n", "padding_num", "=", "-", "2", "**", "32", "+", "1", "\n", "if", "\"key\"", "in", "mask_type", ":", "\n", "        ", "masks", "=", "tf", ".", "sign", "(", "tf", ".", "reduce_sum", "(", "tf", ".", "abs", "(", "keys", ")", ",", "axis", "=", "-", "1", ")", ")", "# [H, B, M]", "\n", "masks", "=", "tf", ".", "expand_dims", "(", "masks", ",", "axis", "=", "2", ")", "# [H, B, 1, M]", "\n", "masks", "=", "tf", ".", "tile", "(", "masks", ",", "[", "1", ",", "1", ",", "tf", ".", "shape", "(", "queries", ")", "[", "2", "]", ",", "1", "]", ")", "# [H, B, M, M]", "\n", "paddings", "=", "tf", ".", "ones_like", "(", "inputs", ")", "*", "padding_num", "\n", "outputs", "=", "tf", ".", "where", "(", "tf", ".", "equal", "(", "masks", ",", "0", ")", ",", "paddings", ",", "inputs", ")", "# [H, B, M, M]", "\n", "", "elif", "\"query\"", "in", "mask_type", ":", "\n", "        ", "masks", "=", "tf", ".", "sign", "(", "tf", ".", "reduce_sum", "(", "tf", ".", "abs", "(", "queries", ")", ",", "axis", "=", "-", "1", ")", ")", "# [H, B, M]", "\n", "masks", "=", "tf", ".", "expand_dims", "(", "masks", ",", "axis", "=", "-", "1", ")", "# [H, B, M, 1]", "\n", "masks", "=", "tf", ".", "tile", "(", "masks", ",", "[", "1", ",", "1", ",", "1", ",", "tf", ".", "shape", "(", "keys", ")", "[", "2", "]", "]", ")", "# [H, B, M, M]", "\n", "outputs", "=", "inputs", "*", "masks", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Unknown mask type: %s. You need to choose \"", "\n", "\"between \\\"keys\\\" and \\\"query\\\".\"", "%", "mask_type", ")", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.cosine_distance_loss": [[106, 138], ["tensorflow.variable_scope", "tensorflow.nn.l2_normalize", "list", "tensorflow.matmul", "tensorflow.ones_like", "tensorflow.matrix_band_part", "tensorflow.matrix_band_part", "tensorflow.cast", "tensorflow.boolean_mask", "range", "tensorflow.transpose", "tensorflow.reduce_mean", "tensorflow.reduce_mean", "len", "tensorflow.math.abs", "inputs.get_shape().as_list", "inputs.get_shape"], "function", ["None"], ["", "def", "cosine_distance_loss", "(", "inputs", ",", "take_abs", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Computes the cosine pairwise distance loss between the input heads.\n    :param inputs: expects tensor with its last two dimensions [*, H, E],\n    where H = num heads and E = arbitrary vector dimension.\n    :param take_abs: take the absolute value of the cosine similarity; this\n    has the effect of switching from [-1, 1] to [0, 1], with the minimum at 0,\n    i.e. when the vectors are orthogonal, which is what we want.\n    However, this might not be differentiable at 0.\n    :return: loss of the cosine distance between any 2 pairs of head vectors.\n    \"\"\"", "\n", "with", "tf", ".", "variable_scope", "(", "\"cosine_distance_loss\"", ")", ":", "\n", "# Calculate the cosine similarity and cosine distance.", "\n", "# The goal is to maximize the cosine distance.", "\n", "        ", "normalized_inputs", "=", "tf", ".", "nn", ".", "l2_normalize", "(", "inputs", ",", "axis", "=", "-", "1", ")", "\n", "permutation", "=", "list", "(", "range", "(", "len", "(", "inputs", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", ")", ")", ")", "\n", "permutation", "[", "-", "1", "]", ",", "permutation", "[", "-", "2", "]", "=", "permutation", "[", "-", "2", "]", ",", "permutation", "[", "-", "1", "]", "\n", "cos_similarity", "=", "tf", ".", "matmul", "(", "\n", "normalized_inputs", ",", "tf", ".", "transpose", "(", "normalized_inputs", ",", "permutation", ")", ")", "\n", "\n", "# Mask the lower diagonal matrix.", "\n", "ones", "=", "tf", ".", "ones_like", "(", "cos_similarity", ")", "\n", "mask_upper", "=", "tf", ".", "matrix_band_part", "(", "ones", ",", "0", ",", "-", "1", ")", "# upper triangular part", "\n", "mask_diagonal", "=", "tf", ".", "matrix_band_part", "(", "ones", ",", "0", ",", "0", ")", "# diagonal", "\n", "mask_matrix", "=", "tf", ".", "cast", "(", "mask_upper", "-", "mask_diagonal", ",", "dtype", "=", "tf", ".", "bool", ")", "\n", "\n", "upper_triangular_flat", "=", "tf", ".", "boolean_mask", "(", "cos_similarity", ",", "mask_matrix", ")", "\n", "\n", "if", "take_abs", ":", "\n", "            ", "return", "tf", ".", "reduce_mean", "(", "tf", ".", "math", ".", "abs", "(", "upper_triangular_flat", ")", ")", "\n", "", "else", ":", "\n", "            ", "return", "tf", ".", "reduce_mean", "(", "upper_triangular_flat", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.single_head_attention_binary_labels": [[140, 202], ["tensorflow.variable_scope", "tensorflow.layers.dense", "tensorflow.layers.dense", "tensorflow.squeeze", "tensorflow.sigmoid", "tensorflow.where", "tensorflow.cast", "tensorflow.where", "tensorflow.reduce_sum", "tensorflow.layers.dense", "tensorflow.reshape", "tensorflow.where", "tensorflow.greater_equal", "tensorflow.ones_like", "tensorflow.zeros_like", "tensorflow.where", "tensorflow.sequence_mask", "tensorflow.zeros_like", "tensorflow.reduce_sum", "tensorflow.expand_dims", "tensorflow.layers.dense", "tensorflow.greater_equal", "tensorflow.ones_like", "tensorflow.zeros_like", "tensorflow.sequence_mask", "tensorflow.zeros_like", "tensorflow.shape"], "function", ["None"], ["", "", "", "def", "single_head_attention_binary_labels", "(", "\n", "inputs", ",", "\n", "initializer", ",", "\n", "attention_size", ",", "\n", "sentence_lengths", ",", "\n", "hidden_units", ")", ":", "\n", "    ", "\"\"\"\n    Computes single-head attention (just normal, vanilla, soft attention).\n    :param inputs: 3D floats of shape [B, M, E]\n    :param initializer: type of initializer (best if Glorot or Xavier)\n    :param attention_size: number of units to use for the attention evidence\n    :param sentence_lengths: 2D ints of shape [B, M]\n    :param hidden_units: number of units to use for the processed sent tensor\n    :return sentence_scores: result of the attention * input; floats of shape [B]\n    :return sentence_predictions: predicted labels for each sentence in the batch; ints of shape [B]\n    :return token_scores: result of the un-normalized attention weights; floats of shape [B, M]\n    :return token_predictions: predicted labels for each token in each sentence; ints of shape [B, M]\n    \"\"\"", "\n", "with", "tf", ".", "variable_scope", "(", "\"single_head_attention_binary_labels\"", ")", ":", "\n", "        ", "attention_evidence", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", "=", "inputs", ",", "units", "=", "attention_size", ",", "\n", "activation", "=", "tf", ".", "tanh", ",", "kernel_initializer", "=", "initializer", ")", "# [B, M, attention_size]", "\n", "attention_weights", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", "=", "attention_evidence", ",", "units", "=", "1", ",", "\n", "kernel_initializer", "=", "initializer", ")", "# [B, M, 1]", "\n", "attention_weights", "=", "tf", ".", "squeeze", "(", "attention_weights", ",", "axis", "=", "-", "1", ")", "# [B, M]", "\n", "attention_weights", "=", "tf", ".", "sigmoid", "(", "attention_weights", ")", "\n", "\n", "token_scores", "=", "attention_weights", "\n", "token_predictions", "=", "tf", ".", "where", "(", "\n", "tf", ".", "greater_equal", "(", "token_scores", ",", "0.5", ")", ",", "\n", "tf", ".", "ones_like", "(", "token_scores", ")", ",", "\n", "tf", ".", "zeros_like", "(", "token_scores", ")", ")", "\n", "token_predictions", "=", "tf", ".", "cast", "(", "tf", ".", "where", "(", "\n", "tf", ".", "sequence_mask", "(", "sentence_lengths", ")", ",", "\n", "token_predictions", ",", "\n", "tf", ".", "zeros_like", "(", "token_predictions", ")", "-", "1e6", ")", ",", "tf", ".", "int32", ")", "\n", "\n", "attention_weights", "=", "tf", ".", "where", "(", "\n", "tf", ".", "sequence_mask", "(", "sentence_lengths", ")", ",", "\n", "attention_weights", ",", "tf", ".", "zeros_like", "(", "attention_weights", ")", ")", "\n", "attention_weights", "=", "attention_weights", "/", "tf", ".", "reduce_sum", "(", "\n", "attention_weights", ",", "axis", "=", "1", ",", "keep_dims", "=", "True", ")", "# [B, M]", "\n", "product", "=", "inputs", "*", "tf", ".", "expand_dims", "(", "attention_weights", ",", "axis", "=", "-", "1", ")", "# [B, M, E]", "\n", "processed_tensor", "=", "tf", ".", "reduce_sum", "(", "product", ",", "axis", "=", "1", ")", "# [B, E]", "\n", "\n", "if", "hidden_units", ">", "0", ":", "\n", "            ", "processed_tensor", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", "=", "processed_tensor", ",", "units", "=", "hidden_units", ",", "\n", "activation", "=", "tf", ".", "tanh", ",", "kernel_initializer", "=", "initializer", ")", "# [B, hidden_units]", "\n", "\n", "", "sentence_scores", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", "=", "processed_tensor", ",", "units", "=", "1", ",", "\n", "activation", "=", "tf", ".", "sigmoid", ",", "kernel_initializer", "=", "initializer", ",", "\n", "name", "=", "\"output_sent_single_head_ff\"", ")", "# [B, 1]", "\n", "sentence_scores", "=", "tf", ".", "reshape", "(", "\n", "sentence_scores", ",", "shape", "=", "[", "tf", ".", "shape", "(", "processed_tensor", ")", "[", "0", "]", "]", ")", "# [B]", "\n", "sentence_predictions", "=", "tf", ".", "where", "(", "\n", "tf", ".", "greater_equal", "(", "sentence_scores", ",", "0.5", ")", ",", "\n", "tf", ".", "ones_like", "(", "sentence_scores", ",", "dtype", "=", "tf", ".", "int32", ")", ",", "\n", "tf", ".", "zeros_like", "(", "sentence_scores", ",", "dtype", "=", "tf", ".", "int32", ")", ")", "# [B]", "\n", "return", "sentence_scores", ",", "sentence_predictions", ",", "token_scores", ",", "token_predictions", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.baseline_lstm_last_contexts": [[204, 268], ["tensorflow.variable_scope", "tensorflow.layers.dense", "tensorflow.nn.softmax", "tensorflow.argmax", "tensorflow.layers.dense", "tensorflow.tile", "tensorflow.where", "tensorflow.nn.softmax", "tensorflow.argmax", "tensorflow.zeros_like", "tensorflow.layers.dense", "tensorflow.layers.dense", "tensorflow.zeros_like", "tensorflow.expand_dims", "tensorflow.sequence_mask"], "function", ["None"], ["", "", "def", "baseline_lstm_last_contexts", "(", "\n", "last_token_contexts", ",", "\n", "last_context", ",", "\n", "initializer", ",", "\n", "scoring_activation", ",", "\n", "sentence_lengths", ",", "\n", "hidden_units", ",", "\n", "num_sentence_labels", ",", "\n", "num_token_labels", ")", ":", "\n", "    ", "\"\"\"\n    Computes token and sentence scores/predictions solely from the last LSTM contexts.\n    vectors that the Bi-LSTM has produced. Works for flexible no. of labels.\n    :param last_token_contexts: the (concatenated) Bi-LSTM outputs per-token.\n    :param last_context: the (concatenated) Bi-LSTM final state.\n    :param initializer: type of initializer (best if Glorot or Xavier)\n    :param scoring_activation: used in computing the sentence scores from the token scores (per-head)\n    :param sentence_lengths: 2D ints of shape [B, M]\n    :param hidden_units: number of units to use for the processed sentence tensor\n    :param num_sentence_labels: number of unique sentence labels\n    :param num_token_labels: number of unique token labels\n    :return sentence_scores: 2D floats of shape [B, num_sentence_labels]\n    :return sentence_predictions: predicted labels for each sentence in the batch; ints of shape [B]\n    :return token_scores: 3D floats of shape [B, M, num_token_labels]\n    :return token_predictions: predicted labels for each token in each sentence; ints of shape [B, M]\n    :return: attention weights will be a tensor of zeros of shape [B, M, num_token_labels].\n    \"\"\"", "\n", "with", "tf", ".", "variable_scope", "(", "\"baseline_lstm_last_contexts\"", ")", ":", "\n", "        ", "if", "hidden_units", ">", "0", ":", "\n", "            ", "processed_tensor", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "last_context", ",", "units", "=", "hidden_units", ",", "\n", "activation", "=", "tf", ".", "tanh", ",", "kernel_initializer", "=", "initializer", ")", "\n", "token_scores", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "last_token_contexts", ",", "units", "=", "hidden_units", ",", "\n", "activation", "=", "tf", ".", "tanh", ",", "kernel_initializer", "=", "initializer", ")", "\n", "", "else", ":", "\n", "            ", "processed_tensor", "=", "last_context", "\n", "token_scores", "=", "last_token_contexts", "\n", "\n", "", "sentence_scores", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "processed_tensor", ",", "units", "=", "num_sentence_labels", ",", "\n", "activation", "=", "scoring_activation", ",", "kernel_initializer", "=", "initializer", ",", "\n", "name", "=", "\"sentence_scores_lstm_ff\"", ")", "# [B, num_sentence_labels]", "\n", "sentence_probabilities", "=", "tf", ".", "nn", ".", "softmax", "(", "sentence_scores", ",", "axis", "=", "-", "1", ")", "\n", "sentence_predictions", "=", "tf", ".", "argmax", "(", "sentence_probabilities", ",", "axis", "=", "-", "1", ")", "# [B]", "\n", "\n", "token_scores", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "token_scores", ",", "units", "=", "num_token_labels", ",", "\n", "activation", "=", "scoring_activation", ",", "kernel_initializer", "=", "initializer", ",", "\n", "name", "=", "\"token_scores_lstm_ff\"", ")", "# [B, M, num_token_labels]", "\n", "\n", "masked_sentence_lengths", "=", "tf", ".", "tile", "(", "\n", "input", "=", "tf", ".", "expand_dims", "(", "\n", "tf", ".", "sequence_mask", "(", "sentence_lengths", ")", ",", "axis", "=", "-", "1", ")", ",", "\n", "multiples", "=", "[", "1", ",", "1", ",", "num_token_labels", "]", ")", "\n", "token_scores", "=", "tf", ".", "where", "(", "\n", "masked_sentence_lengths", ",", "\n", "token_scores", ",", "\n", "tf", ".", "zeros_like", "(", "token_scores", ")", ")", "# [B, M, num_token_labels]", "\n", "token_probabilities", "=", "tf", ".", "nn", ".", "softmax", "(", "token_scores", ",", "axis", "=", "-", "1", ")", "\n", "token_predictions", "=", "tf", ".", "argmax", "(", "token_probabilities", ",", "axis", "=", "-", "1", ")", "\n", "attention_weights", "=", "tf", ".", "zeros_like", "(", "token_scores", ")", "\n", "\n", "return", "sentence_scores", ",", "sentence_predictions", ",", "token_scores", ",", "token_predictions", ",", "token_probabilities", ",", "sentence_probabilities", ",", "attention_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.single_head_attention_multiple_labels": [[270, 353], ["tensorflow.variable_scope", "tensorflow.layers.dense", "tensorflow.layers.dense", "tensorflow.squeeze", "tensorflow.where", "tensorflow.layers.dense", "tensorflow.nn.softmax", "tensorflow.argmax", "tensorflow.reduce_sum", "tensorflow.layers.dense", "tensorflow.nn.softmax", "tensorflow.argmax", "tensorflow.nn.sigmoid", "tensorflow.sequence_mask", "tensorflow.zeros_like", "tensorflow.expand_dims", "tensorflow.layers.dense", "tensorflow.math.exp", "tensorflow.reduce_sum", "tensorflow.expand_dims", "tensorflow.nn.softmax", "ValueError"], "function", ["None"], ["", "", "def", "single_head_attention_multiple_labels", "(", "\n", "inputs", ",", "\n", "initializer", ",", "\n", "attention_activation", ",", "\n", "attention_size", ",", "\n", "sentence_lengths", ",", "\n", "hidden_units", ",", "\n", "num_sentence_labels", ",", "\n", "num_token_labels", ")", ":", "\n", "    ", "\"\"\"\n    Computes single-head attention, but adapt it (naively) to make it work for multiple labels.\n    :param inputs: 3D floats of shape [B, M, E]\n    :param initializer: type of initializer (best if Glorot or Xavier)\n    :param attention_activation: type of attention activation (soft, sharp, linear, etc)\n    :param attention_size: number of units to use for the attention evidence\n    :param sentence_lengths: 2D ints of shape [B, M]\n    :param hidden_units: number of units to use for the processed sent tensor\n    :param num_sentence_labels: number of unique sentence labels\n    :param num_token_labels: number of unique token labels\n    :return sentence_scores: 2D floats of shape [B, num_sentence_labels]\n    :return sentence_predictions: predicted labels for each sentence in the batch; ints of shape [B]\n    :return token_scores: 3D floats of shape [B, M, num_token_labels]\n    :return token_predictions: predicted labels for each token in each sentence; ints of shape [B, M]\n    \"\"\"", "\n", "with", "tf", ".", "variable_scope", "(", "\"SHA_multiple_labels\"", ")", ":", "\n", "        ", "attention_evidence", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", "=", "inputs", ",", "units", "=", "attention_size", ",", "\n", "activation", "=", "tf", ".", "tanh", ",", "kernel_initializer", "=", "initializer", ")", "# [B, M, attention_size]", "\n", "\n", "attention_evidence", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", "=", "attention_evidence", ",", "units", "=", "1", ",", "\n", "kernel_initializer", "=", "initializer", ")", "# [B, M, 1]", "\n", "attention_evidence", "=", "tf", ".", "squeeze", "(", "attention_evidence", ",", "axis", "=", "-", "1", ")", "# [B, M]", "\n", "\n", "# Apply a non-linear layer to obtain (un-normalized) attention weights.", "\n", "if", "attention_activation", "==", "\"soft\"", ":", "\n", "            ", "attention_weights", "=", "tf", ".", "nn", ".", "sigmoid", "(", "attention_evidence", ")", "\n", "", "elif", "attention_activation", "==", "\"sharp\"", ":", "\n", "            ", "attention_weights", "=", "tf", ".", "math", ".", "exp", "(", "attention_evidence", ")", "\n", "", "elif", "attention_activation", "==", "\"linear\"", ":", "\n", "            ", "attention_weights", "=", "attention_evidence", "\n", "", "elif", "attention_activation", "==", "\"softmax\"", ":", "\n", "            ", "attention_weights", "=", "tf", ".", "nn", ".", "softmax", "(", "attention_evidence", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unknown/unsupported activation for attention activation: %s.\"", "\n", "%", "attention_activation", ")", "\n", "\n", "# Mask attention weights.", "\n", "", "attention_weights", "=", "tf", ".", "where", "(", "\n", "tf", ".", "sequence_mask", "(", "sentence_lengths", ")", ",", "\n", "attention_weights", ",", "tf", ".", "zeros_like", "(", "attention_weights", ")", ")", "\n", "attention_weights_unnormalized", "=", "attention_weights", "\n", "\n", "# Normalize attention weights.", "\n", "if", "attention_activation", "!=", "\"softmax\"", ":", "\n", "            ", "attention_weights", "=", "attention_weights", "/", "tf", ".", "reduce_sum", "(", "\n", "attention_weights", ",", "axis", "=", "-", "1", ",", "keep_dims", "=", "True", ")", "# [B, M]", "\n", "\n", "", "token_scores", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", "=", "tf", ".", "expand_dims", "(", "attention_weights_unnormalized", ",", "-", "1", ")", ",", "\n", "units", "=", "num_token_labels", ",", "\n", "kernel_initializer", "=", "initializer", ",", "\n", "name", "=", "\"output_single_head_token_scores_ff\"", ")", "# [B, M, num_token_labels]", "\n", "token_probabilities", "=", "tf", ".", "nn", ".", "softmax", "(", "token_scores", ")", "\n", "token_predictions", "=", "tf", ".", "argmax", "(", "token_probabilities", ",", "\n", "axis", "=", "2", ",", "output_type", "=", "tf", ".", "int32", ")", "# [B, M]", "\n", "\n", "product", "=", "inputs", "*", "tf", ".", "expand_dims", "(", "attention_weights", ",", "axis", "=", "-", "1", ")", "# [B, M, E]", "\n", "processed_tensor", "=", "tf", ".", "reduce_sum", "(", "product", ",", "axis", "=", "1", ")", "# [B, E]", "\n", "\n", "if", "hidden_units", ">", "0", ":", "\n", "            ", "processed_tensor", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", "=", "processed_tensor", ",", "units", "=", "hidden_units", ",", "\n", "activation", "=", "tf", ".", "tanh", ",", "kernel_initializer", "=", "initializer", ")", "# [B, hidden_units]", "\n", "\n", "", "sentence_scores", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", "=", "processed_tensor", ",", "units", "=", "num_sentence_labels", ",", "\n", "kernel_initializer", "=", "initializer", ",", "\n", "name", "=", "\"output_multi_sent_specified_scores_ff\"", ")", "# [B, num_unique_sent_labels]", "\n", "sentence_probabilities", "=", "tf", ".", "nn", ".", "softmax", "(", "sentence_scores", ",", "axis", "=", "-", "1", ")", "\n", "sentence_predictions", "=", "tf", ".", "argmax", "(", "sentence_probabilities", ",", "axis", "=", "-", "1", ")", "# [B]", "\n", "return", "sentence_scores", ",", "sentence_predictions", ",", "token_scores", ",", "token_predictions", ",", "token_probabilities", ",", "sentence_probabilities", ",", "attention_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.multi_head_attention_with_scores_from_shared_heads": [[355, 524], ["tensorflow.variable_scope", "tensorflow.layers.dense", "tensorflow.layers.dense", "tensorflow.layers.dense", "tensorflow.tile", "tensorflow.where", "tensorflow.where", "tensorflow.where", "tensorflow.concat", "tensorflow.concat", "tensorflow.concat", "tensorflow.matmul", "tensorflow.math.divide", "modules.mask", "modules.mask", "modules.mask", "tensorflow.matmul", "tensorflow.concat", "tensorflow.reduce_sum", "tensorflow.layers.dense", "tensorflow.nn.softmax", "tensorflow.argmax", "tensorflow.concat", "tensorflow.nn.softmax", "tensorflow.argmax", "tensorflow.concat", "tf.layers.dense.get_shape().as_list", "tensorflow.layers.dense", "tensorflow.zeros_like", "tensorflow.zeros_like", "tensorflow.zeros_like", "tensorflow.split", "tensorflow.split", "tensorflow.split", "tensorflow.transpose", "tensorflow.constant", "tensorflow.nn.sigmoid", "tensorflow.reduce_sum", "tensorflow.nn.dropout", "tensorflow.split", "modules.layer_normalization", "tensorflow.layers.dense", "tensorflow.expand_dims", "tensorflow.split", "tensorflow.split", "math.ceil", "tensorflow.expand_dims", "tensorflow.math.exp", "tensorflow.reduce_sum", "tensorflow.expand_dims", "tensorflow.expand_dims", "tf.layers.dense.get_shape", "tensorflow.sequence_mask", "tensorflow.cast", "tensorflow.cast", "tensorflow.reduce_max", "tensorflow.expand_dims", "tensorflow.nn.softmax", "ValueError", "tensorflow.reduce_mean", "tensorflow.expand_dims", "ValueError", "tensorflow.reduce_logsumexp"], "function", ["home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.mask", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.mask", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.mask", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.layer_normalization"], ["", "", "def", "multi_head_attention_with_scores_from_shared_heads", "(", "\n", "inputs", ",", "\n", "initializer", ",", "\n", "attention_activation", ",", "\n", "hidden_units", ",", "\n", "num_sentence_labels", ",", "\n", "num_heads", ",", "\n", "is_training", ",", "\n", "dropout", ",", "\n", "sentence_lengths", ",", "\n", "use_residual_connection", ",", "\n", "token_scoring_method", ")", ":", "\n", "    ", "\"\"\"\n    Computes multi-head attention (mainly inspired by the transformer architecture).\n    This method does not take into account any masking at any level.\n    All the masking will be performed before computing a primary/secondary loss.\n    :param inputs: 3D floats of shape [B, M, E]\n    :param initializer: type of initializer (best if Glorot or Xavier)\n    :param attention_activation: type of attention activation (linear, softmax or sigmoid)\n    :param hidden_units: number of units to use for the processed sent tensor\n    :param num_sentence_labels: number of unique sentence labels\n    :param num_heads: number of unique token labels\n    :param is_training: if set to True, the current phase is a training one (rather than testing)\n    :param dropout: the keep_probs value for the dropout\n    :param sentence_lengths: the true sentence lengths, used for masking\n    :param use_residual_connection: if set to True, a residual connection is added to the inputs\n    :param token_scoring_method: can be either max, sum or avg\n    :return sentence_scores: 2D floats of shape [B, num_sentence_labels]\n    :return sentence_predictions: predicted labels for each sentence in the batch; ints of shape [B]\n    :return token_scores: 3D floats of shape [B, M, num_heads]\n    :return token_predictions: predicted labels for each token in each sentence; ints of shape [B, M]\n    :return token_probabilities: the token scores normalized across the axis\n    \"\"\"", "\n", "with", "tf", ".", "variable_scope", "(", "\"MHA_sentence_scores_from_shared_heads\"", ")", ":", "\n", "        ", "num_units", "=", "inputs", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "-", "1", "]", "\n", "if", "num_units", "%", "num_heads", "!=", "0", ":", "\n", "            ", "num_units", "=", "ceil", "(", "num_units", "/", "num_heads", ")", "*", "num_heads", "\n", "inputs", "=", "tf", ".", "layers", ".", "dense", "(", "inputs", ",", "num_units", ")", "# [B, M, num_units]", "\n", "\n", "# Project to get the queries, keys, and values.", "\n", "", "queries", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", ",", "num_units", ",", "activation", "=", "tf", ".", "tanh", ",", "\n", "kernel_initializer", "=", "initializer", ")", "# [B, M, num_units]", "\n", "keys", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", ",", "num_units", ",", "activation", "=", "tf", ".", "tanh", ",", "\n", "kernel_initializer", "=", "initializer", ")", "# [B, M, num_units]", "\n", "values", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", ",", "num_units", ",", "activation", "=", "tf", ".", "tanh", ",", "\n", "kernel_initializer", "=", "initializer", ")", "# [B, M, num_units]", "\n", "\n", "# Mask out the keys, queries and values: replace with 0 all the token", "\n", "# positions between the true and the maximum sentence length.", "\n", "multiplication_mask", "=", "tf", ".", "tile", "(", "\n", "input", "=", "tf", ".", "expand_dims", "(", "tf", ".", "sequence_mask", "(", "sentence_lengths", ")", ",", "axis", "=", "-", "1", ")", ",", "\n", "multiples", "=", "[", "1", ",", "1", ",", "num_units", "]", ")", "# [B, M, num_units]", "\n", "queries", "=", "tf", ".", "where", "(", "multiplication_mask", ",", "queries", ",", "tf", ".", "zeros_like", "(", "queries", ")", ")", "\n", "keys", "=", "tf", ".", "where", "(", "multiplication_mask", ",", "keys", ",", "tf", ".", "zeros_like", "(", "keys", ")", ")", "\n", "values", "=", "tf", ".", "where", "(", "multiplication_mask", ",", "values", ",", "tf", ".", "zeros_like", "(", "values", ")", ")", "\n", "\n", "# Split and concat as many projections as the number of heads.", "\n", "queries", "=", "tf", ".", "concat", "(", "\n", "tf", ".", "split", "(", "queries", ",", "num_heads", ",", "axis", "=", "2", ")", ",", "\n", "axis", "=", "0", ")", "# [B*num_heads, M, num_units/num_heads]", "\n", "keys", "=", "tf", ".", "concat", "(", "\n", "tf", ".", "split", "(", "keys", ",", "num_heads", ",", "axis", "=", "2", ")", ",", "\n", "axis", "=", "0", ")", "# [B*num_heads, M, num_units/num_heads]", "\n", "values", "=", "tf", ".", "concat", "(", "\n", "tf", ".", "split", "(", "values", ",", "num_heads", ",", "axis", "=", "2", ")", ",", "\n", "axis", "=", "0", ")", "# [B*num_heads, M, num_units/num_heads]", "\n", "\n", "# Transpose multiplication and scale", "\n", "attention_evidence", "=", "tf", ".", "matmul", "(", "\n", "queries", ",", "tf", ".", "transpose", "(", "keys", ",", "[", "0", ",", "2", ",", "1", "]", ")", ")", "# [B*num_heads, M, M]", "\n", "attention_evidence", "=", "tf", ".", "math", ".", "divide", "(", "\n", "attention_evidence", ",", "tf", ".", "constant", "(", "num_units", "**", "0.5", ")", ")", "\n", "\n", "# Mask columns (with values of -infinity), based on rows that have 0 sum.", "\n", "attention_evidence_masked", "=", "mask", "(", "\n", "attention_evidence", ",", "queries", ",", "keys", ",", "mask_type", "=", "\"key\"", ")", "\n", "\n", "# Apply a non-linear layer to obtain (un-normalized) attention weights.", "\n", "if", "attention_activation", "==", "\"soft\"", ":", "\n", "            ", "attention_weights", "=", "tf", ".", "nn", ".", "sigmoid", "(", "attention_evidence_masked", ")", "\n", "", "elif", "attention_activation", "==", "\"sharp\"", ":", "\n", "            ", "attention_weights", "=", "tf", ".", "math", ".", "exp", "(", "attention_evidence_masked", ")", "\n", "", "elif", "attention_activation", "==", "\"linear\"", ":", "\n", "            ", "attention_weights", "=", "attention_evidence_masked", "\n", "", "elif", "attention_activation", "==", "\"softmax\"", ":", "\n", "            ", "attention_weights", "=", "tf", ".", "nn", ".", "softmax", "(", "attention_evidence_masked", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unknown/unsupported attention activation: %s.\"", "\n", "%", "attention_activation", ")", "\n", "\n", "", "attention_weights_unnormalized", "=", "attention_weights", "\n", "\n", "# Normalize attention weights.", "\n", "if", "attention_activation", "!=", "\"softmax\"", ":", "\n", "            ", "attention_weights", "/=", "tf", ".", "reduce_sum", "(", "\n", "attention_weights", ",", "axis", "=", "-", "1", ",", "keep_dims", "=", "True", ")", "\n", "\n", "# Mask rows (with values of 0), based on columns that have 0 sum.", "\n", "", "attention_weights", "=", "mask", "(", "\n", "attention_weights", ",", "queries", ",", "keys", ",", "mask_type", "=", "\"query\"", ")", "\n", "attention_weights_unnormalized", "=", "mask", "(", "\n", "attention_weights_unnormalized", ",", "queries", ",", "keys", ",", "mask_type", "=", "\"query\"", ")", "\n", "\n", "# Apply a dropout layer on the attention weights.", "\n", "if", "dropout", ">", "0.0", ":", "\n", "            ", "dropout_attention", "=", "(", "dropout", "*", "tf", ".", "cast", "(", "is_training", ",", "tf", ".", "float32", ")", "\n", "+", "(", "1.0", "-", "tf", ".", "cast", "(", "is_training", ",", "tf", ".", "float32", ")", ")", ")", "\n", "attention_weights", "=", "tf", ".", "nn", ".", "dropout", "(", "\n", "attention_weights", ",", "dropout_attention", ",", "\n", "name", "=", "\"dropout_attention_weights\"", ")", "# [B*num_heads, M, M]", "\n", "\n", "# [B*num_heads, M, num_units/num_heads]", "\n", "", "product", "=", "tf", ".", "matmul", "(", "attention_weights", ",", "values", ")", "\n", "product", "=", "tf", ".", "concat", "(", "\n", "tf", ".", "split", "(", "product", ",", "num_heads", ")", ",", "axis", "=", "2", ")", "# [B, M, num_units]", "\n", "\n", "# Add a residual connection, followed by layer normalization.", "\n", "if", "use_residual_connection", ":", "\n", "            ", "product", "+=", "inputs", "\n", "product", "=", "layer_normalization", "(", "product", ")", "# [B, M, num_units]", "\n", "\n", "", "processed_tensor", "=", "tf", ".", "reduce_sum", "(", "product", ",", "axis", "=", "1", ")", "# [B, num_units]", "\n", "\n", "if", "hidden_units", ">", "0", ":", "\n", "            ", "processed_tensor", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", "=", "processed_tensor", ",", "units", "=", "hidden_units", ",", "\n", "activation", "=", "tf", ".", "tanh", ",", "kernel_initializer", "=", "initializer", ")", "# [B, hidden_units]", "\n", "\n", "", "sentence_scores", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", "=", "processed_tensor", ",", "units", "=", "num_sentence_labels", ",", "\n", "kernel_initializer", "=", "initializer", ",", "\n", "name", "=", "\"output_sent_specified_scores_ff\"", ")", "# [B, num_unique_sent_labels]", "\n", "sentence_probabilities", "=", "tf", ".", "nn", ".", "softmax", "(", "sentence_scores", ")", "\n", "sentence_predictions", "=", "tf", ".", "argmax", "(", "sentence_probabilities", ",", "axis", "=", "1", ")", "# [B]", "\n", "\n", "# Obtain token scores from the attention weights.", "\n", "# The token scores will have shape [B*num_heads, M, 1].", "\n", "if", "token_scoring_method", "==", "\"sum\"", ":", "\n", "            ", "token_scores", "=", "tf", ".", "expand_dims", "(", "tf", ".", "reduce_sum", "(", "\n", "attention_weights_unnormalized", ",", "axis", "=", "1", ")", ",", "axis", "=", "2", ")", "\n", "", "elif", "token_scoring_method", "==", "\"max\"", ":", "\n", "            ", "token_scores", "=", "tf", ".", "expand_dims", "(", "tf", ".", "reduce_max", "(", "\n", "attention_weights_unnormalized", ",", "axis", "=", "1", ")", ",", "axis", "=", "2", ")", "\n", "", "elif", "token_scoring_method", "==", "\"avg\"", ":", "\n", "            ", "token_scores", "=", "tf", ".", "expand_dims", "(", "tf", ".", "reduce_mean", "(", "\n", "attention_weights_unnormalized", ",", "axis", "=", "1", ")", ",", "axis", "=", "2", ")", "\n", "", "elif", "token_scoring_method", "==", "\"logsumexp\"", ":", "\n", "            ", "token_scores", "=", "tf", ".", "expand_dims", "(", "tf", ".", "reduce_logsumexp", "(", "\n", "attention_weights_unnormalized", ",", "axis", "=", "1", ")", ",", "axis", "=", "2", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unknown/unsupported token scoring method: %s\"", "\n", "%", "token_scoring_method", ")", "\n", "\n", "", "token_scores", "=", "tf", ".", "concat", "(", "\n", "tf", ".", "split", "(", "token_scores", ",", "num_heads", ")", ",", "axis", "=", "2", ")", "# [B, M, num_heads]", "\n", "token_probabilities", "=", "tf", ".", "nn", ".", "softmax", "(", "token_scores", ")", "\n", "token_predictions", "=", "tf", ".", "argmax", "(", "\n", "token_probabilities", ",", "axis", "=", "2", ",", "output_type", "=", "tf", ".", "int32", ")", "# [B, M]", "\n", "\n", "attention_weights", "=", "tf", ".", "concat", "(", "\n", "tf", ".", "split", "(", "tf", ".", "expand_dims", "(", "attention_weights", ",", "axis", "=", "-", "1", ")", ",", "num_heads", ")", ",", "\n", "axis", "=", "-", "1", ")", "# [B, M, M, num_heads]", "\n", "\n", "return", "sentence_scores", ",", "sentence_predictions", ",", "token_scores", ",", "token_predictions", ",", "token_probabilities", ",", "sentence_probabilities", ",", "attention_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.multi_head_attention_with_scores_from_separate_heads": [[526, 732], ["tensorflow.variable_scope", "tensorflow.layers.dense", "tensorflow.layers.dense", "tensorflow.layers.dense", "tensorflow.tile", "tensorflow.where", "tensorflow.where", "tensorflow.where", "tensorflow.concat", "tensorflow.concat", "tensorflow.matmul", "tensorflow.math.divide", "modules.mask", "modules.mask", "tensorflow.concat", "modules.division_masking", "tensorflow.nn.softmax", "tensorflow.argmax", "tensorflow.matmul", "tensorflow.nn.softmax", "tensorflow.argmax", "tensorflow.transpose", "tensorflow.transpose", "tensorflow.concat", "tf.layers.dense.get_shape().as_list", "tensorflow.layers.dense", "tensorflow.zeros_like", "tensorflow.zeros_like", "tensorflow.zeros_like", "tensorflow.split", "tensorflow.split", "tensorflow.transpose", "tensorflow.constant", "tensorflow.nn.sigmoid", "tensorflow.reduce_sum", "tensorflow.nn.dropout", "tensorflow.reduce_sum", "tensorflow.split", "modules.layer_normalization", "tensorflow.gather", "tensorflow.gather", "tensorflow.layers.dense", "tensorflow.squeeze", "tensorflow.layers.dense", "tensorflow.reduce_mean", "tensorflow.concat", "tensorflow.layers.dense", "tensorflow.reduce_sum", "tensorflow.split", "math.ceil", "tensorflow.expand_dims", "tensorflow.math.exp", "tensorflow.reduce_max", "tensorflow.expand_dims", "tf.layers.dense.get_shape", "tensorflow.sequence_mask", "tensorflow.cast", "tensorflow.cast", "tensorflow.reduce_mean", "list", "tensorflow.nn.softmax", "ValueError", "tensorflow.reduce_logsumexp", "ValueError", "tensorflow.shape", "range"], "function", ["home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.mask", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.mask", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.division_masking", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.layer_normalization"], ["", "", "def", "multi_head_attention_with_scores_from_separate_heads", "(", "\n", "inputs", ",", "\n", "initializer", ",", "\n", "attention_activation", ",", "\n", "num_sentence_labels", ",", "\n", "num_heads", ",", "\n", "is_training", ",", "\n", "dropout", ",", "\n", "sentence_lengths", ",", "\n", "normalize_sentence", ",", "\n", "token_scoring_method", ",", "\n", "scoring_activation", "=", "None", ",", "\n", "separate_heads", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    Computes multi-head attention (mainly inspired by the transformer architecture).\n    This version of the implementation applies masking at several levels:\n        * first, the keys, queries and values so that the matrix multiplications\n          are performed only between meaningful positions\n        * second, the attention evidence values of 0 should be replaced with -infinity\n          so that when applying a non-linear layer, the resulted value is very close to 0.\n        * third, when obtaining the token probabilities (by normalizing across the scores),\n          division masking is performed (a value of 0 should be attributed to all 0 sums).\n    The masking performed before computing a primary/secondary loss is preserved.\n    :param inputs: 3D floats of shape [B, M, E]\n    :param initializer: type of initializer (best if Glorot or Xavier)\n    :param attention_activation: type of attention activation (linear, softmax or sigmoid)\n    :param num_sentence_labels: number of unique sentence labels\n    :param num_heads: number of unique token labels\n    :param is_training: if set to True, the current phase is a training one (rather than testing)\n    :param dropout: the keep_probs value for the dropout\n    :param sentence_lengths: the true sentence lengths, used for masking\n    :param normalize_sentence: if set to True, the last weighted sentence layer is normalized\n    :param token_scoring_method: can be either max, sum or avg\n    :param scoring_activation: used in computing the sentence scores from the token scores (per-head)\n    :param separate_heads: boolean value; when set to False, all heads\n    are used to obtain the sentence scores; when set to True, the default and non-default heads\n    from the token scores are used to obtain the sentence scores.\n    :return sentence_scores: 2D floats of shape [B, num_sentence_labels]\n    :return sentence_predictions: predicted labels for each sentence in the batch; ints of shape [B]\n    :return token_scores: 3D floats of shape [B, M, num_heads]\n    :return token_predictions: predicted labels for each token in each sentence; ints of shape [B, M]\n    \"\"\"", "\n", "with", "tf", ".", "variable_scope", "(", "\"MHA_sentence_scores_from_separate_heads\"", ")", ":", "\n", "        ", "num_units", "=", "inputs", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "-", "1", "]", "\n", "if", "num_units", "%", "num_heads", "!=", "0", ":", "\n", "            ", "num_units", "=", "ceil", "(", "num_units", "/", "num_heads", ")", "*", "num_heads", "\n", "inputs", "=", "tf", ".", "layers", ".", "dense", "(", "inputs", ",", "num_units", ")", "# [B, M, num_units]", "\n", "\n", "# Project to get the queries, keys, and values.", "\n", "", "queries", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", ",", "num_units", ",", "activation", "=", "tf", ".", "tanh", ",", "\n", "kernel_initializer", "=", "initializer", ")", "# [B, M, num_units]", "\n", "keys", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", ",", "num_units", ",", "activation", "=", "tf", ".", "tanh", ",", "\n", "kernel_initializer", "=", "initializer", ")", "# [B, M, num_units]", "\n", "values", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", ",", "num_units", ",", "activation", "=", "tf", ".", "tanh", ",", "\n", "kernel_initializer", "=", "initializer", ")", "# [B, M, num_units]", "\n", "\n", "# Mask out the keys, queries and values: replace with 0 all the token", "\n", "# positions between the true and the maximum sentence length.", "\n", "multiplication_mask", "=", "tf", ".", "tile", "(", "\n", "input", "=", "tf", ".", "expand_dims", "(", "tf", ".", "sequence_mask", "(", "sentence_lengths", ")", ",", "axis", "=", "-", "1", ")", ",", "\n", "multiples", "=", "[", "1", ",", "1", ",", "num_units", "]", ")", "# [B, M, num_units]", "\n", "queries", "=", "tf", ".", "where", "(", "multiplication_mask", ",", "queries", ",", "tf", ".", "zeros_like", "(", "queries", ")", ")", "\n", "keys", "=", "tf", ".", "where", "(", "multiplication_mask", ",", "keys", ",", "tf", ".", "zeros_like", "(", "keys", ")", ")", "\n", "values", "=", "tf", ".", "where", "(", "multiplication_mask", ",", "values", ",", "tf", ".", "zeros_like", "(", "values", ")", ")", "\n", "\n", "# Split and concat as many projections as the number of heads.", "\n", "queries", "=", "tf", ".", "concat", "(", "\n", "tf", ".", "split", "(", "queries", ",", "num_heads", ",", "axis", "=", "2", ")", ",", "\n", "axis", "=", "0", ")", "# [B*num_heads, M, num_units/num_heads]", "\n", "keys", "=", "tf", ".", "concat", "(", "\n", "tf", ".", "split", "(", "keys", ",", "num_heads", ",", "axis", "=", "2", ")", ",", "\n", "axis", "=", "0", ")", "# [B*num_heads, M, num_units/num_heads]", "\n", "\n", "# Transpose multiplication and scale", "\n", "attention_evidence", "=", "tf", ".", "matmul", "(", "\n", "queries", ",", "tf", ".", "transpose", "(", "keys", ",", "[", "0", ",", "2", ",", "1", "]", ")", ")", "# [B*num_heads, M, M]", "\n", "attention_evidence", "=", "tf", ".", "math", ".", "divide", "(", "\n", "attention_evidence", ",", "tf", ".", "constant", "(", "num_units", "**", "0.5", ")", ")", "\n", "\n", "# Mask columns (with values of -infinity), based on rows that have 0 sum.", "\n", "attention_evidence_masked", "=", "mask", "(", "\n", "attention_evidence", ",", "queries", ",", "keys", ",", "mask_type", "=", "\"key\"", ")", "\n", "\n", "# Apply a non-linear layer to obtain (un-normalized) attention weights.", "\n", "if", "attention_activation", "==", "\"soft\"", ":", "\n", "            ", "attention_weights", "=", "tf", ".", "nn", ".", "sigmoid", "(", "attention_evidence_masked", ")", "\n", "", "elif", "attention_activation", "==", "\"sharp\"", ":", "\n", "            ", "attention_weights", "=", "tf", ".", "math", ".", "exp", "(", "attention_evidence_masked", ")", "\n", "", "elif", "attention_activation", "==", "\"linear\"", ":", "\n", "            ", "attention_weights", "=", "attention_evidence_masked", "\n", "", "elif", "attention_activation", "==", "\"softmax\"", ":", "\n", "            ", "attention_weights", "=", "tf", ".", "nn", ".", "softmax", "(", "attention_evidence_masked", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unknown/unsupported attention activation: %s.\"", "\n", "%", "attention_activation", ")", "\n", "\n", "# Normalize attention weights.", "\n", "", "if", "attention_activation", "!=", "\"softmax\"", ":", "\n", "            ", "attention_weights", "/=", "tf", ".", "reduce_sum", "(", "\n", "attention_weights", ",", "axis", "=", "-", "1", ",", "keep_dims", "=", "True", ")", "\n", "\n", "# Mask rows (with values of 0), based on columns that have 0 sum.", "\n", "", "attention_weights", "=", "mask", "(", "\n", "attention_weights", ",", "queries", ",", "keys", ",", "mask_type", "=", "\"query\"", ")", "\n", "\n", "# Apply a dropout layer on the attention weights.", "\n", "if", "dropout", ">", "0.0", ":", "\n", "            ", "dropout_attention", "=", "(", "dropout", "*", "tf", ".", "cast", "(", "is_training", ",", "tf", ".", "float32", ")", "\n", "+", "(", "1.0", "-", "tf", ".", "cast", "(", "is_training", ",", "tf", ".", "float32", ")", ")", ")", "\n", "attention_weights", "=", "tf", ".", "nn", ".", "dropout", "(", "\n", "attention_weights", ",", "dropout_attention", ",", "\n", "name", "=", "\"dropout_attention_weights\"", ")", "# [B*num_heads, M, M]", "\n", "\n", "# Obtain the token scores from the attention weights.", "\n", "# The token_scores below will have shape [B*num_heads, 1, M].", "\n", "", "if", "token_scoring_method", "==", "\"sum\"", ":", "\n", "            ", "token_scores", "=", "tf", ".", "reduce_sum", "(", "\n", "attention_weights", ",", "axis", "=", "1", ",", "keep_dims", "=", "True", ")", "\n", "", "elif", "token_scoring_method", "==", "\"max\"", ":", "\n", "            ", "token_scores", "=", "tf", ".", "reduce_max", "(", "\n", "attention_weights", ",", "axis", "=", "1", ",", "keep_dims", "=", "True", ")", "\n", "", "elif", "token_scoring_method", "==", "\"avg\"", ":", "\n", "            ", "token_scores", "=", "tf", ".", "reduce_mean", "(", "\n", "attention_weights", ",", "axis", "=", "1", ",", "keep_dims", "=", "True", ")", "\n", "", "elif", "token_scoring_method", "==", "\"logsumexp\"", ":", "\n", "            ", "token_scores", "=", "tf", ".", "reduce_logsumexp", "(", "\n", "attention_weights", ",", "axis", "=", "1", ",", "keep_dims", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unknown/unsupported token scoring method: %s\"", "\n", "%", "token_scoring_method", ")", "\n", "\n", "", "token_scores", "=", "tf", ".", "concat", "(", "\n", "tf", ".", "split", "(", "token_scores", ",", "num_heads", ")", ",", "\n", "axis", "=", "1", ")", "# [B, num_heads, M]", "\n", "token_scores_normalized", "=", "division_masking", "(", "\n", "inputs", "=", "token_scores", ",", "axis", "=", "-", "1", ",", "\n", "multiplies", "=", "[", "1", ",", "1", ",", "tf", ".", "shape", "(", "token_scores", ")", "[", "-", "1", "]", "]", ")", "# [B, num_heads, M]", "\n", "token_probabilities", "=", "tf", ".", "nn", ".", "softmax", "(", "token_scores", ",", "axis", "=", "1", ")", "\n", "token_predictions", "=", "tf", ".", "argmax", "(", "\n", "token_probabilities", ",", "axis", "=", "1", ",", "output_type", "=", "tf", ".", "int32", ")", "# [B, M]", "\n", "\n", "# Obtain a weighted sum between the inputs and the attention weights.", "\n", "# [B, num_heads, num_units]", "\n", "weighted_sum_representation", "=", "tf", ".", "matmul", "(", "token_scores_normalized", ",", "values", ")", "\n", "\n", "if", "normalize_sentence", ":", "\n", "            ", "weighted_sum_representation", "=", "layer_normalization", "(", "weighted_sum_representation", ")", "\n", "\n", "", "if", "separate_heads", ":", "\n", "# Get the sentence representations corresponding to the default head.", "\n", "            ", "default_head", "=", "tf", ".", "gather", "(", "\n", "weighted_sum_representation", ",", "\n", "indices", "=", "[", "0", "]", ",", "axis", "=", "1", ")", "# [B, 1, num_units]", "\n", "\n", "# Get the sentence representations corresponding to the default head.", "\n", "non_default_heads", "=", "tf", ".", "gather", "(", "\n", "weighted_sum_representation", ",", "\n", "indices", "=", "list", "(", "range", "(", "1", ",", "num_heads", ")", ")", ",", "axis", "=", "1", ")", "# [B, num_heads-1, num_units]", "\n", "\n", "# Project onto one unit, corresponding to", "\n", "# the default sentence label score.", "\n", "sentence_default_scores", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "default_head", ",", "units", "=", "1", ",", "\n", "activation", "=", "scoring_activation", ",", "kernel_initializer", "=", "initializer", ",", "\n", "name", "=", "\"sentence_default_scores_ff\"", ")", "# [B, 1, 1]", "\n", "sentence_default_scores", "=", "tf", ".", "squeeze", "(", "\n", "sentence_default_scores", ",", "axis", "=", "-", "1", ")", "# [B, 1]", "\n", "\n", "# Project onto (num_sentence_labels-1) units, corresponding to", "\n", "# the non-default sentence label scores.", "\n", "sentence_non_default_scores", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "non_default_heads", ",", "units", "=", "num_sentence_labels", "-", "1", ",", "\n", "activation", "=", "scoring_activation", ",", "kernel_initializer", "=", "initializer", ",", "\n", "name", "=", "\"sentence_non_default_scores_ff\"", ")", "# [B, num_heads-1, num_sentence_labels-1]", "\n", "\n", "sentence_non_default_scores", "=", "tf", ".", "reduce_mean", "(", "\n", "sentence_non_default_scores", ",", "axis", "=", "1", ")", "# [B, num_sent_labels-1]", "\n", "\n", "sentence_scores", "=", "tf", ".", "concat", "(", "\n", "[", "sentence_default_scores", ",", "sentence_non_default_scores", "]", ",", "\n", "axis", "=", "-", "1", ",", "name", "=", "\"sentence_scores_concatenation\"", ")", "# [B, num_sent_labels]", "\n", "", "else", ":", "\n", "            ", "processed_tensor", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", "=", "weighted_sum_representation", ",", "units", "=", "num_sentence_labels", ",", "\n", "activation", "=", "scoring_activation", ",", "kernel_initializer", "=", "initializer", ",", "\n", "name", "=", "\"sentence_scores_ff\"", ")", "# [B, num_heads, num_unique_sent_labels]", "\n", "sentence_scores", "=", "tf", ".", "reduce_sum", "(", "\n", "processed_tensor", ",", "axis", "=", "1", ")", "# [B, num_sent_labels]", "\n", "\n", "", "sentence_probabilities", "=", "tf", ".", "nn", ".", "softmax", "(", "sentence_scores", ")", "\n", "sentence_predictions", "=", "tf", ".", "argmax", "(", "sentence_probabilities", ",", "axis", "=", "1", ")", "# [B]", "\n", "\n", "# Get token scores and probabilities of shape # [B, M, num_heads].", "\n", "token_scores", "=", "tf", ".", "transpose", "(", "token_scores", ",", "[", "0", ",", "2", ",", "1", "]", ")", "\n", "token_probabilities", "=", "tf", ".", "transpose", "(", "token_probabilities", ",", "[", "0", ",", "2", ",", "1", "]", ")", "\n", "\n", "attention_weights", "=", "tf", ".", "concat", "(", "\n", "tf", ".", "split", "(", "tf", ".", "expand_dims", "(", "attention_weights", ",", "axis", "=", "-", "1", ")", ",", "num_heads", ")", ",", "\n", "axis", "=", "-", "1", ")", "# [B, M, M, num_heads]", "\n", "\n", "return", "sentence_scores", ",", "sentence_predictions", ",", "token_scores", ",", "token_predictions", ",", "token_probabilities", ",", "sentence_probabilities", ",", "attention_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.compute_scores_from_additive_attention": [[734, 795], ["tensorflow.variable_scope", "tensorflow.layers.dense", "tensorflow.layers.dense", "tensorflow.squeeze", "tensorflow.where", "tensorflow.reduce_sum", "tensorflow.layers.dense", "tensorflow.squeeze", "tensorflow.nn.sigmoid", "tensorflow.sequence_mask", "tensorflow.zeros_like", "tensorflow.reduce_sum", "tensorflow.expand_dims", "tensorflow.layers.dense", "tensorflow.exp", "ValueError"], "function", ["None"], ["", "", "def", "compute_scores_from_additive_attention", "(", "\n", "inputs", ",", "\n", "initializer", ",", "\n", "attention_activation", ",", "\n", "sentence_lengths", ",", "\n", "attention_size", "=", "50", ",", "\n", "hidden_units", "=", "50", ")", ":", "\n", "    ", "\"\"\"\n    Computes token and sentence scores from a single-head additive attention mechanism.\n    :param inputs: 3D floats of shape [B, M, E]\n    :param initializer: type of initializer (best if Glorot or Xavier)\n    :param attention_activation: type of attention activation (linear, softmax or sigmoid)\n    :param sentence_lengths: 2D ints of shape [B, M]\n    :param attention_size: number of units to use for the attention evidence\n    :param hidden_units: number of units to use for the processed sent tensor\n    :return sentence_scores: result of the attention * input; floats of shape [B]\n    :return token_scores: result of the un-normalized attention weights; floats of shape [B, M]\n    :return attention_weights: 2D floats of shape [B, M] of normalized token_scores\n    \"\"\"", "\n", "with", "tf", ".", "variable_scope", "(", "\"compute_classic_single_head_attention\"", ")", ":", "\n", "        ", "attention_evidence", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", "=", "inputs", ",", "units", "=", "attention_size", ",", "\n", "activation", "=", "tf", ".", "tanh", ",", "kernel_initializer", "=", "initializer", ")", "# [B, M, attention_size]", "\n", "attention_weights", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", "=", "attention_evidence", ",", "units", "=", "1", ",", "\n", "kernel_initializer", "=", "initializer", ")", "# [B, M, 1]", "\n", "attention_weights", "=", "tf", ".", "squeeze", "(", "attention_weights", ",", "axis", "=", "-", "1", ")", "# [B, M]", "\n", "\n", "# Obtain the un-normalized attention weights.", "\n", "if", "attention_activation", "==", "\"soft\"", ":", "\n", "            ", "attention_weights", "=", "tf", ".", "nn", ".", "sigmoid", "(", "attention_weights", ")", "\n", "", "elif", "attention_activation", "==", "\"sharp\"", ":", "\n", "            ", "attention_weights", "=", "tf", ".", "exp", "(", "attention_weights", ")", "\n", "", "elif", "attention_activation", "==", "\"linear\"", ":", "\n", "            ", "attention_weights", "=", "attention_weights", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unknown/unsupported attention activation: %s\"", "\n", "%", "attention_activation", ")", "\n", "\n", "", "attention_weights", "=", "tf", ".", "where", "(", "\n", "tf", ".", "sequence_mask", "(", "sentence_lengths", ")", ",", "\n", "attention_weights", ",", "tf", ".", "zeros_like", "(", "attention_weights", ")", ")", "\n", "token_scores", "=", "attention_weights", "# [B, M]", "\n", "\n", "# Obtain the normalized attention weights (they will also be sentence weights).", "\n", "attention_weights", "=", "attention_weights", "/", "tf", ".", "reduce_sum", "(", "\n", "attention_weights", ",", "axis", "=", "1", ",", "keep_dims", "=", "True", ")", "# [B, M]", "\n", "product", "=", "inputs", "*", "tf", ".", "expand_dims", "(", "attention_weights", ",", "axis", "=", "-", "1", ")", "# [B, M, num_units]", "\n", "processed_tensor", "=", "tf", ".", "reduce_sum", "(", "product", ",", "axis", "=", "1", ")", "# [B, E]", "\n", "\n", "if", "hidden_units", ">", "0", ":", "\n", "            ", "processed_tensor", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", "=", "processed_tensor", ",", "units", "=", "hidden_units", ",", "\n", "activation", "=", "tf", ".", "tanh", ",", "kernel_initializer", "=", "initializer", ")", "# [B, hidden_units]", "\n", "\n", "", "sentence_scores", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", "=", "processed_tensor", ",", "units", "=", "1", ",", "\n", "activation", "=", "tf", ".", "sigmoid", ",", "kernel_initializer", "=", "initializer", ",", "\n", "name", "=", "\"output_sent_single_head_ff\"", ")", "# [B, 1]", "\n", "sentence_scores", "=", "tf", ".", "squeeze", "(", "sentence_scores", ",", "axis", "=", "-", "1", ")", "\n", "return", "sentence_scores", ",", "token_scores", ",", "attention_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.compute_scores_from_scaled_dot_product_attention": [[797, 898], ["tensorflow.variable_scope", "tensorflow.layers.dense", "tensorflow.layers.dense", "tensorflow.tile", "tensorflow.where", "tensorflow.where", "tensorflow.matmul", "tensorflow.math.divide", "modules.mask", "tensorflow.reduce_sum", "modules.mask", "modules.mask", "modules.division_masking", "tensorflow.reduce_sum", "tensorflow.layers.dense", "tensorflow.squeeze", "inputs.get_shape().as_list", "tensorflow.zeros_like", "tensorflow.zeros_like", "tensorflow.transpose", "tensorflow.constant", "tensorflow.nn.sigmoid", "tensorflow.reduce_sum", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.exp", "ValueError", "tensorflow.reduce_max", "inputs.get_shape", "tensorflow.sequence_mask", "tensorflow.reduce_mean", "tensorflow.reduce_logsumexp", "ValueError", "tensorflow.shape"], "function", ["home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.mask", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.mask", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.mask", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.division_masking"], ["", "", "def", "compute_scores_from_scaled_dot_product_attention", "(", "\n", "inputs", ",", "\n", "initializer", ",", "\n", "attention_activation", ",", "\n", "sentence_lengths", ",", "\n", "token_scoring_method", ")", ":", "\n", "    ", "\"\"\"\n    Computes token and sentence scores from a single-head scaled dot product attention mechanism.\n    :param inputs: 3D floats of shape [B, M, E]\n    :param initializer: type of initializer (best with Glorot or Xavier)\n    :param attention_activation: type of attention activation: sharp (exp) or soft (sigmoid)\n    :param sentence_lengths: 2D ints of shape [B, M]\n    :param token_scoring_method: can be either max, sum or avg\n    :return sentence_scores: 2D floats of shape [B, num_sentence_labels]\n    :return token_scores: 2D floats of shape [B, M]\n    :return token_probabilities: 2D floats of shape [B, M] of normalized token_scores\n    \"\"\"", "\n", "with", "tf", ".", "variable_scope", "(", "\"compute_transformer_single_head_attention\"", ")", ":", "\n", "        ", "num_units", "=", "inputs", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "-", "1", "]", "\n", "\n", "# Project to get the queries, keys, and values, all of them of shape [B, M, num_units].", "\n", "queries", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", ",", "num_units", ",", "activation", "=", "tf", ".", "tanh", ",", "\n", "kernel_initializer", "=", "initializer", ")", "\n", "keys", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", ",", "num_units", ",", "activation", "=", "tf", ".", "tanh", ",", "\n", "kernel_initializer", "=", "initializer", ")", "\n", "\n", "# Mask out the keys, queries and values: replace with 0 all the token", "\n", "# positions between the true and the maximum sentence length.", "\n", "multiplication_mask", "=", "tf", ".", "tile", "(", "\n", "input", "=", "tf", ".", "expand_dims", "(", "tf", ".", "sequence_mask", "(", "sentence_lengths", ")", ",", "axis", "=", "-", "1", ")", ",", "\n", "multiples", "=", "[", "1", ",", "1", ",", "num_units", "]", ")", "# [B, M, num_units]", "\n", "queries", "=", "tf", ".", "where", "(", "multiplication_mask", ",", "queries", ",", "tf", ".", "zeros_like", "(", "queries", ")", ")", "\n", "keys", "=", "tf", ".", "where", "(", "multiplication_mask", ",", "keys", ",", "tf", ".", "zeros_like", "(", "keys", ")", ")", "\n", "\n", "# Scaled dot-product attention.", "\n", "attention_evidence", "=", "tf", ".", "matmul", "(", "\n", "queries", ",", "tf", ".", "transpose", "(", "keys", ",", "[", "0", ",", "2", ",", "1", "]", ")", ")", "# [B, M, M]", "\n", "attention_evidence", "=", "tf", ".", "math", ".", "divide", "(", "\n", "attention_evidence", ",", "tf", ".", "constant", "(", "num_units", "**", "0.5", ")", ")", "\n", "\n", "# Mask columns (with values of -infinity), based on rows that have 0 sum.", "\n", "attention_evidence_masked", "=", "mask", "(", "\n", "attention_evidence", ",", "queries", ",", "keys", ",", "mask_type", "=", "\"key\"", ")", "\n", "\n", "# Obtain the un-normalized attention weights.", "\n", "if", "attention_activation", "==", "\"soft\"", ":", "\n", "            ", "attention_weights", "=", "tf", ".", "nn", ".", "sigmoid", "(", "attention_evidence_masked", ")", "\n", "", "elif", "attention_activation", "==", "\"sharp\"", ":", "\n", "            ", "attention_weights", "=", "tf", ".", "exp", "(", "attention_evidence_masked", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unknown/unsupported activation for attention: %s\"", "\n", "%", "attention_activation", ")", "\n", "", "attention_weights_unnormalized", "=", "attention_weights", "\n", "\n", "# Normalize attention weights.", "\n", "attention_weights", "/=", "tf", ".", "reduce_sum", "(", "\n", "attention_weights", ",", "axis", "=", "-", "1", ",", "keep_dims", "=", "True", ")", "# [B, M, M]", "\n", "\n", "# Mask rows (with values of 0), based on columns that have 0 sum.", "\n", "attention_weights", "=", "mask", "(", "\n", "attention_weights", ",", "queries", ",", "keys", ",", "mask_type", "=", "\"query\"", ")", "\n", "attention_weights_unnormalized", "=", "mask", "(", "\n", "attention_weights_unnormalized", ",", "queries", ",", "keys", ",", "mask_type", "=", "\"query\"", ")", "\n", "\n", "# Obtain the token scores from the attention weights.", "\n", "# The token_scores below will have shape [B, M].", "\n", "if", "token_scoring_method", "==", "\"sum\"", ":", "\n", "            ", "token_scores", "=", "tf", ".", "reduce_sum", "(", "\n", "attention_weights_unnormalized", ",", "axis", "=", "1", ")", "\n", "", "elif", "token_scoring_method", "==", "\"max\"", ":", "\n", "            ", "token_scores", "=", "tf", ".", "reduce_max", "(", "\n", "attention_weights_unnormalized", ",", "axis", "=", "1", ")", "\n", "", "elif", "token_scoring_method", "==", "\"avg\"", ":", "\n", "            ", "token_scores", "=", "tf", ".", "reduce_mean", "(", "\n", "attention_weights_unnormalized", ",", "axis", "=", "1", ")", "\n", "", "elif", "token_scoring_method", "==", "\"logsumexp\"", ":", "\n", "            ", "token_scores", "=", "tf", ".", "reduce_logsumexp", "(", "\n", "attention_weights_unnormalized", ",", "axis", "=", "1", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unknown/unsupported token scoring method: %s\"", "\n", "%", "token_scoring_method", ")", "\n", "\n", "", "token_scores_normalized", "=", "division_masking", "(", "\n", "inputs", "=", "token_scores", ",", "axis", "=", "-", "1", ",", "\n", "multiplies", "=", "[", "1", ",", "tf", ".", "shape", "(", "token_scores", ")", "[", "1", "]", "]", ")", "# [B, M]", "\n", "\n", "# Sentence scores as a weighted sum between the inputs and the attention weights.", "\n", "# weighted_sum_representation = tf.matmul(attention_weights, inputs)", "\n", "weighted_sum_representation", "=", "inputs", "*", "tf", ".", "expand_dims", "(", "\n", "token_scores_normalized", ",", "axis", "=", "-", "1", ")", "# [B, M, num_units]", "\n", "\n", "processed_tensor", "=", "tf", ".", "reduce_sum", "(", "\n", "weighted_sum_representation", ",", "axis", "=", "1", ")", "# [B, num_units]", "\n", "sentence_scores", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", "=", "processed_tensor", ",", "units", "=", "1", ",", "\n", "activation", "=", "tf", ".", "sigmoid", ",", "kernel_initializer", "=", "initializer", ",", "\n", "name", "=", "\"sentence_scores_from_scaled_dot_product_ff\"", ")", "# [B, 1]", "\n", "sentence_scores", "=", "tf", ".", "squeeze", "(", "sentence_scores", ",", "axis", "=", "-", "1", ")", "# [B]", "\n", "return", "sentence_scores", ",", "token_scores", ",", "attention_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.single_head_attention_multiple_transformations": [[900, 993], ["tensorflow.variable_scope", "range", "tensorflow.stack", "tensorflow.nn.softmax", "tensorflow.argmax", "tensorflow.stack", "tensorflow.nn.softmax", "tensorflow.argmax", "tensorflow.stack", "tensorflow.layers.dense", "tensorflow.layers.dense", "tensorflow.concat", "tensorflow.layers.dense", "tensorflow.variable_scope", "sentence_scores_per_head.append", "token_scores_per_head.append", "attention_weights_per_head.append", "modules.compute_scores_from_additive_attention", "tensorflow.expand_dims", "modules.compute_scores_from_scaled_dot_product_attention", "ValueError"], "function", ["home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.compute_scores_from_additive_attention", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.compute_scores_from_scaled_dot_product_attention"], ["", "", "def", "single_head_attention_multiple_transformations", "(", "\n", "inputs", ",", "\n", "initializer", ",", "\n", "attention_activation", ",", "\n", "num_sentence_labels", ",", "\n", "num_heads", ",", "\n", "sentence_lengths", ",", "\n", "token_scoring_method", ",", "\n", "scoring_activation", "=", "None", ",", "\n", "how_to_compute_attention", "=", "\"dot\"", ",", "\n", "separate_heads", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    Computes token and sentence scores using a single-head attention mechanism,\n    which can either be additive (mainly inspired by the single-head binary-label\n    method above, as in Rei and Sogaard paper https://arxiv.org/pdf/1811.05949.pdf)\n    or a scaled-dot product version (inspired by the transformer, but with just one head).\n    Then, use these scores to obtain predictions at both granularities.\n    :param inputs: 3D floats of shape [B, M, E]\n    :param initializer: type of initializer (best if Glorot or Xavier)\n    :param attention_activation\n    :param num_sentence_labels: number of unique sentence labels\n    :param num_heads: number of unique token labels\n    :param sentence_lengths: the true sentence lengths, used for masking\n    :param token_scoring_method\n    :param scoring_activation: activation used for scoring, default is None.\n    :param how_to_compute_attention: compute attention in the classic way (Marek) or as in transformer\n    :param separate_heads: boolean value; when set to False, all heads\n    are used to obtain the sentence scores; when set to True, the default and non-default heads\n    from the token scores are used to obtain the sentence scores.\n    :return sentence_scores: 2D floats of shape [B, num_sentence_labels]\n    :return sentence_predictions: predicted labels for each sentence in the batch; ints of shape [B]\n    :return token_scores: 3D floats of shape [B, M, num_heads]\n    :return token_predictions: predicted labels for each token in each sentence; ints of shape [B, M]\n    \"\"\"", "\n", "with", "tf", ".", "variable_scope", "(", "\"transformer_single_heads_multi_attention\"", ")", ":", "\n", "        ", "token_scores_per_head", "=", "[", "]", "\n", "sentence_scores_per_head", "=", "[", "]", "\n", "attention_weights_per_head", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_heads", ")", ":", "\n", "            ", "with", "tf", ".", "variable_scope", "(", "\"num_head_{}\"", ".", "format", "(", "i", ")", ",", "reuse", "=", "tf", ".", "AUTO_REUSE", ")", ":", "\n", "                ", "if", "how_to_compute_attention", "==", "\"additive\"", ":", "\n", "                    ", "sentence_scores_head_i", ",", "token_scores_head_i", ",", "attention_weights_head_i", "=", "compute_scores_from_additive_attention", "(", "\n", "inputs", "=", "inputs", ",", "initializer", "=", "initializer", ",", "\n", "attention_activation", "=", "attention_activation", ",", "\n", "sentence_lengths", "=", "sentence_lengths", ")", "\n", "", "elif", "how_to_compute_attention", "==", "\"dot\"", ":", "\n", "                    ", "sentence_scores_head_i", ",", "token_scores_head_i", ",", "attention_weights_head_i", "=", "compute_scores_from_scaled_dot_product_attention", "(", "\n", "inputs", "=", "inputs", ",", "initializer", "=", "initializer", ",", "\n", "attention_activation", "=", "attention_activation", ",", "\n", "sentence_lengths", "=", "sentence_lengths", ",", "\n", "token_scoring_method", "=", "token_scoring_method", ")", "\n", "", "else", ":", "\n", "                    ", "raise", "ValueError", "(", "\"Unknown/unsupported way of computing the attention: %s\"", "\n", "%", "how_to_compute_attention", ")", "\n", "", "sentence_scores_per_head", ".", "append", "(", "sentence_scores_head_i", ")", "\n", "token_scores_per_head", ".", "append", "(", "token_scores_head_i", ")", "\n", "attention_weights_per_head", ".", "append", "(", "attention_weights_head_i", ")", "\n", "\n", "", "", "sentence_scores", "=", "tf", ".", "stack", "(", "sentence_scores_per_head", ",", "axis", "=", "-", "1", ")", "# [B, num_heads]", "\n", "\n", "if", "separate_heads", ":", "\n", "            ", "sentence_default_score", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", "=", "tf", ".", "expand_dims", "(", "sentence_scores", "[", ":", ",", "0", "]", ",", "axis", "=", "-", "1", ")", ",", "units", "=", "1", ",", "\n", "activation", "=", "scoring_activation", ",", "kernel_initializer", "=", "initializer", ",", "\n", "name", "=", "\"ff_non_default_sentence_scores\"", ")", "\n", "sentence_non_default_scores", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", "=", "sentence_scores", "[", ":", ",", "1", ":", "]", ",", "units", "=", "num_sentence_labels", "-", "1", ",", "\n", "activation", "=", "scoring_activation", ",", "kernel_initializer", "=", "initializer", ",", "\n", "name", "=", "\"ff_default_sentence_scores\"", ")", "\n", "sentence_scores", "=", "tf", ".", "concat", "(", "\n", "[", "sentence_default_score", ",", "sentence_non_default_scores", "]", ",", "\n", "axis", "=", "-", "1", ",", "name", "=", "\"sentence_scores_concatenation\"", ")", "\n", "", "else", ":", "\n", "            ", "sentence_scores", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", "=", "sentence_scores", ",", "units", "=", "num_sentence_labels", ",", "\n", "activation", "=", "scoring_activation", ",", "kernel_initializer", "=", "initializer", ",", "\n", "name", "=", "\"ff_sentence_scores\"", ")", "# [B, num_sentence_labels]", "\n", "\n", "", "sentence_probabilities", "=", "tf", ".", "nn", ".", "softmax", "(", "sentence_scores", ")", "\n", "sentence_predictions", "=", "tf", ".", "argmax", "(", "sentence_probabilities", ",", "axis", "=", "1", ")", "# [B]", "\n", "\n", "token_scores", "=", "tf", ".", "stack", "(", "token_scores_per_head", ",", "axis", "=", "-", "1", ")", "# [B, M, num_heads]", "\n", "token_probabilities", "=", "tf", ".", "nn", ".", "softmax", "(", "token_scores", ",", "axis", "=", "-", "1", ")", "# [B, M, num_heads]", "\n", "token_predictions", "=", "tf", ".", "argmax", "(", "token_probabilities", ",", "axis", "=", "-", "1", ")", "# [B, M]", "\n", "\n", "# Will be of shape [B, M, H] if an additive attention was used, or", "\n", "# of shape [B, M, M, H] if a scaled-dot product attention was used.", "\n", "attention_weights", "=", "tf", ".", "stack", "(", "attention_weights_per_head", ",", "axis", "=", "-", "1", ")", "\n", "\n", "return", "sentence_scores", ",", "sentence_predictions", ",", "token_scores", ",", "token_predictions", ",", "token_probabilities", ",", "sentence_probabilities", ",", "attention_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.variant_1": [[995, 1167], ["tensorflow.variable_scope", "tensorflow.layers.dense", "tensorflow.layers.dense", "tensorflow.layers.dense", "tensorflow.tile", "tensorflow.where", "tensorflow.where", "tensorflow.concat", "tensorflow.concat", "tensorflow.concat", "tensorflow.concat", "tensorflow.matmul", "tensorflow.math.divide", "modules.mask", "tensorflow.reduce_sum", "modules.mask", "modules.mask", "tensorflow.reduce_sum", "tensorflow.layers.dense", "tensorflow.layers.dense", "tensorflow.concat", "tensorflow.nn.softmax", "tensorflow.argmax", "tensorflow.expand_dims", "tensorflow.concat", "tensorflow.nn.softmax", "tensorflow.argmax", "tensorflow.concat", "tf.layers.dense.get_shape().as_list", "tensorflow.layers.dense", "tensorflow.zeros_like", "tensorflow.zeros_like", "tensorflow.split", "tensorflow.split", "tensorflow.split", "tensorflow.split", "tensorflow.transpose", "tensorflow.constant", "tensorflow.nn.sigmoid", "tensorflow.matmul", "tensorflow.matmul", "tensorflow.split", "tensorflow.layers.dense", "tensorflow.reduce_sum", "tensorflow.split", "tensorflow.split", "math.ceil", "tensorflow.expand_dims", "tensorflow.math.exp", "tensorflow.gather", "tensorflow.gather", "tensorflow.layers.dense", "tensorflow.layers.dense", "tensorflow.concat", "tensorflow.reduce_max", "tensorflow.expand_dims", "tf.layers.dense.get_shape", "tensorflow.sequence_mask", "ValueError", "tensorflow.reduce_mean", "list", "tensorflow.reduce_logsumexp", "ValueError", "range"], "function", ["home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.mask", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.mask", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.mask"], ["", "", "def", "variant_1", "(", "\n", "inputs", ",", "\n", "initializer", ",", "\n", "attention_activation", ",", "\n", "num_sentence_labels", ",", "\n", "num_heads", ",", "\n", "hidden_units", ",", "\n", "sentence_lengths", ",", "\n", "scoring_activation", "=", "None", ",", "\n", "token_scoring_method", "=", "\"max\"", ",", "\n", "use_inputs_instead_values", "=", "False", ",", "\n", "separate_heads", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    Variant 1 of the multi-head attention to obtain sentence and token scores and predictions.\n    \"\"\"", "\n", "with", "tf", ".", "variable_scope", "(", "\"variant_1\"", ")", ":", "\n", "        ", "num_units", "=", "inputs", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "-", "1", "]", "\n", "if", "num_units", "%", "num_heads", "!=", "0", ":", "\n", "            ", "num_units", "=", "ceil", "(", "num_units", "/", "num_heads", ")", "*", "num_heads", "\n", "inputs", "=", "tf", ".", "layers", ".", "dense", "(", "inputs", ",", "num_units", ")", "# [B, M, num_units]", "\n", "\n", "# Project to get the queries, keys, and values.", "\n", "", "queries", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", ",", "num_units", ",", "activation", "=", "tf", ".", "tanh", ",", "\n", "kernel_initializer", "=", "initializer", ")", "# [B, M, num_units]", "\n", "keys", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", ",", "num_units", ",", "activation", "=", "tf", ".", "tanh", ",", "\n", "kernel_initializer", "=", "initializer", ")", "# [B, M, num_units]", "\n", "values", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", ",", "num_units", ",", "activation", "=", "tf", ".", "tanh", ",", "\n", "kernel_initializer", "=", "initializer", ")", "# [B, M, num_units]", "\n", "\n", "# Mask out the keys, queries and values: replace with 0 all the token", "\n", "# positions between the true and the maximum sentence length.", "\n", "multiplication_mask", "=", "tf", ".", "tile", "(", "\n", "input", "=", "tf", ".", "expand_dims", "(", "tf", ".", "sequence_mask", "(", "sentence_lengths", ")", ",", "axis", "=", "-", "1", ")", ",", "\n", "multiples", "=", "[", "1", ",", "1", ",", "num_units", "]", ")", "# [B, M, num_units]", "\n", "queries", "=", "tf", ".", "where", "(", "multiplication_mask", ",", "queries", ",", "tf", ".", "zeros_like", "(", "queries", ")", ")", "\n", "keys", "=", "tf", ".", "where", "(", "multiplication_mask", ",", "keys", ",", "tf", ".", "zeros_like", "(", "keys", ")", ")", "\n", "\n", "# Split and concat as many projections as the number of heads.", "\n", "queries", "=", "tf", ".", "concat", "(", "\n", "tf", ".", "split", "(", "queries", ",", "num_heads", ",", "axis", "=", "2", ")", ",", "\n", "axis", "=", "0", ")", "# [B*num_heads, M, num_units/num_heads]", "\n", "keys", "=", "tf", ".", "concat", "(", "\n", "tf", ".", "split", "(", "keys", ",", "num_heads", ",", "axis", "=", "2", ")", ",", "\n", "axis", "=", "0", ")", "# [B*num_heads, M, num_units/num_heads]", "\n", "values", "=", "tf", ".", "concat", "(", "\n", "tf", ".", "split", "(", "values", ",", "num_heads", ",", "axis", "=", "2", ")", ",", "\n", "axis", "=", "0", ")", "# [B*num_heads, M, num_units/num_heads]", "\n", "inputs", "=", "tf", ".", "concat", "(", "\n", "tf", ".", "split", "(", "inputs", ",", "num_heads", ",", "axis", "=", "2", ")", ",", "\n", "axis", "=", "0", ")", "# [B*num_heads, M, num_units/num_heads]", "\n", "\n", "# Transpose multiplication and scale", "\n", "attention_evidence", "=", "tf", ".", "matmul", "(", "\n", "queries", ",", "tf", ".", "transpose", "(", "keys", ",", "[", "0", ",", "2", ",", "1", "]", ")", ")", "# [B*num_heads, M, M]", "\n", "attention_evidence", "=", "tf", ".", "math", ".", "divide", "(", "\n", "attention_evidence", ",", "tf", ".", "constant", "(", "num_units", "**", "0.5", ")", ")", "\n", "\n", "# Mask columns (with values of -infinity), based on rows that have 0 sum.", "\n", "attention_evidence_masked", "=", "mask", "(", "\n", "attention_evidence", ",", "queries", ",", "keys", ",", "mask_type", "=", "\"key\"", ")", "\n", "\n", "# Apply a non-linear layer to obtain (un-normalized) attention weights.", "\n", "if", "attention_activation", "==", "\"soft\"", ":", "\n", "            ", "attention_weights", "=", "tf", ".", "nn", ".", "sigmoid", "(", "attention_evidence_masked", ")", "\n", "", "elif", "attention_activation", "==", "\"sharp\"", ":", "\n", "            ", "attention_weights", "=", "tf", ".", "math", ".", "exp", "(", "attention_evidence_masked", ")", "\n", "", "elif", "attention_activation", "==", "\"linear\"", ":", "\n", "            ", "attention_weights", "=", "attention_evidence_masked", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unknown/unsupported attention activation: %s.\"", "\n", "%", "attention_activation", ")", "\n", "\n", "", "attention_weights_unnormalized", "=", "attention_weights", "\n", "\n", "# Normalize attention weights.", "\n", "attention_weights", "/=", "tf", ".", "reduce_sum", "(", "\n", "attention_weights", ",", "axis", "=", "-", "1", ",", "keep_dims", "=", "True", ")", "\n", "\n", "# Mask rows (with values of 0), based on columns that have 0 sum.", "\n", "attention_weights", "=", "mask", "(", "\n", "attention_weights", ",", "queries", ",", "keys", ",", "mask_type", "=", "\"query\"", ")", "\n", "attention_weights_unnormalized", "=", "mask", "(", "\n", "attention_weights_unnormalized", ",", "queries", ",", "keys", ",", "mask_type", "=", "\"query\"", ")", "\n", "\n", "# [B*num_heads, M, num_units/num_heads]", "\n", "if", "use_inputs_instead_values", ":", "\n", "            ", "product", "=", "tf", ".", "matmul", "(", "attention_weights", ",", "inputs", ")", "\n", "", "else", ":", "\n", "            ", "product", "=", "tf", ".", "matmul", "(", "attention_weights", ",", "values", ")", "\n", "\n", "", "product", "=", "tf", ".", "reduce_sum", "(", "product", ",", "axis", "=", "1", ")", "# [B*num_heads, num_units/num_heads]", "\n", "\n", "product", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", "=", "product", ",", "units", "=", "hidden_units", ",", "\n", "activation", "=", "tf", ".", "tanh", ",", "kernel_initializer", "=", "initializer", ")", "# [B*num_heads, hidden_units]", "\n", "\n", "processed_tensor", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", "=", "product", ",", "units", "=", "1", ",", "\n", "kernel_initializer", "=", "initializer", ")", "# [B*num_heads, 1]", "\n", "\n", "processed_tensor", "=", "tf", ".", "concat", "(", "\n", "tf", ".", "split", "(", "processed_tensor", ",", "num_heads", ")", ",", "axis", "=", "1", ")", "# [B, num_heads]", "\n", "\n", "if", "separate_heads", ":", "\n", "            ", "if", "num_sentence_labels", "==", "num_heads", ":", "\n", "                ", "sentence_scores", "=", "processed_tensor", "\n", "", "else", ":", "\n", "# Get the sentence representations corresponding to the default head.", "\n", "                ", "default_head", "=", "tf", ".", "gather", "(", "\n", "processed_tensor", ",", "\n", "indices", "=", "[", "0", "]", ",", "axis", "=", "-", "1", ")", "# [B, 1]", "\n", "\n", "# Get the sentence representations corresponding to the non-default head.", "\n", "non_default_heads", "=", "tf", ".", "gather", "(", "\n", "processed_tensor", ",", "\n", "indices", "=", "list", "(", "range", "(", "1", ",", "num_heads", ")", ")", ",", "axis", "=", "-", "1", ")", "# [B, num_heads-1]", "\n", "\n", "# Project onto one unit, corresponding to the default sentence label score.", "\n", "sentence_default_scores", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "default_head", ",", "units", "=", "1", ",", "\n", "activation", "=", "scoring_activation", ",", "kernel_initializer", "=", "initializer", ",", "\n", "name", "=", "\"sentence_default_scores_ff\"", ")", "# [B, 1]", "\n", "\n", "# Project onto (num_sentence_labels-1) units, corresponding to", "\n", "# the non-default sentence label scores.", "\n", "sentence_non_default_scores", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "non_default_heads", ",", "units", "=", "num_sentence_labels", "-", "1", ",", "\n", "activation", "=", "scoring_activation", ",", "kernel_initializer", "=", "initializer", ",", "\n", "name", "=", "\"sentence_non_default_scores_ff\"", ")", "# [B, num_sentence_labels-1]", "\n", "\n", "sentence_scores", "=", "tf", ".", "concat", "(", "\n", "[", "sentence_default_scores", ",", "sentence_non_default_scores", "]", ",", "\n", "axis", "=", "-", "1", ",", "name", "=", "\"sentence_scores_concatenation\"", ")", "# [B, num_sent_labels]", "\n", "", "", "else", ":", "\n", "            ", "sentence_scores", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", "=", "processed_tensor", ",", "units", "=", "num_sentence_labels", ",", "\n", "activation", "=", "scoring_activation", ",", "kernel_initializer", "=", "initializer", ",", "\n", "name", "=", "\"output_sent_specified_scores_ff\"", ")", "# [B, num_sent_labels]", "\n", "\n", "", "sentence_probabilities", "=", "tf", ".", "nn", ".", "softmax", "(", "sentence_scores", ")", "\n", "sentence_predictions", "=", "tf", ".", "argmax", "(", "sentence_probabilities", ",", "axis", "=", "1", ")", "# [B]", "\n", "\n", "# Obtain token scores from attention weights. Shape is [B*num_heads, M].", "\n", "if", "token_scoring_method", "==", "\"sum\"", ":", "\n", "            ", "token_scores", "=", "tf", ".", "reduce_sum", "(", "attention_weights_unnormalized", ",", "axis", "=", "1", ")", "\n", "", "elif", "token_scoring_method", "==", "\"max\"", ":", "\n", "            ", "token_scores", "=", "tf", ".", "reduce_max", "(", "attention_weights_unnormalized", ",", "axis", "=", "1", ")", "\n", "", "elif", "token_scoring_method", "==", "\"avg\"", ":", "\n", "            ", "token_scores", "=", "tf", ".", "reduce_mean", "(", "attention_weights_unnormalized", ",", "axis", "=", "1", ")", "\n", "", "elif", "token_scoring_method", "==", "\"logsumexp\"", ":", "\n", "            ", "token_scores", "=", "tf", ".", "reduce_logsumexp", "(", "attention_weights_unnormalized", ",", "axis", "=", "1", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unknown/unsupported token scoring method: %s\"", "\n", "%", "token_scoring_method", ")", "\n", "\n", "", "token_scores", "=", "tf", ".", "expand_dims", "(", "token_scores", ",", "axis", "=", "2", ")", "# [B*num_heads, M, 1]", "\n", "token_scores", "=", "tf", ".", "concat", "(", "\n", "tf", ".", "split", "(", "token_scores", ",", "num_heads", ")", ",", "axis", "=", "2", ")", "# [B, M, num_heads]", "\n", "token_probabilities", "=", "tf", ".", "nn", ".", "softmax", "(", "token_scores", ")", "\n", "token_predictions", "=", "tf", ".", "argmax", "(", "\n", "token_probabilities", ",", "axis", "=", "2", ",", "output_type", "=", "tf", ".", "int32", ")", "# [B, M]", "\n", "\n", "attention_weights", "=", "tf", ".", "concat", "(", "\n", "tf", ".", "split", "(", "tf", ".", "expand_dims", "(", "attention_weights", ",", "axis", "=", "-", "1", ")", ",", "num_heads", ")", ",", "\n", "axis", "=", "-", "1", ")", "# [B, M, M, num_heads]", "\n", "\n", "return", "sentence_scores", ",", "sentence_predictions", ",", "token_scores", ",", "token_predictions", ",", "token_probabilities", ",", "sentence_probabilities", ",", "attention_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.variant_2": [[1169, 1336], ["tensorflow.variable_scope", "tensorflow.layers.dense", "tensorflow.layers.dense", "tensorflow.layers.dense", "tensorflow.tile", "tensorflow.where", "tensorflow.concat", "tensorflow.reduce_sum", "tensorflow.concat", "tensorflow.concat", "tensorflow.concat", "tensorflow.matmul", "tensorflow.math.divide", "modules.mask", "tensorflow.reduce_sum", "modules.mask", "modules.mask", "tensorflow.transpose", "tensorflow.reduce_sum", "tensorflow.layers.dense", "tensorflow.layers.dense", "tensorflow.concat", "tensorflow.nn.softmax", "tensorflow.argmax", "tensorflow.transpose", "tensorflow.concat", "tensorflow.nn.softmax", "tensorflow.argmax", "tensorflow.concat", "tf.layers.dense.get_shape().as_list", "tensorflow.layers.dense", "tensorflow.zeros_like", "tensorflow.split", "tensorflow.split", "tensorflow.split", "tensorflow.split", "tensorflow.transpose", "tensorflow.constant", "tensorflow.nn.sigmoid", "tensorflow.split", "tensorflow.layers.dense", "tensorflow.split", "tensorflow.split", "math.ceil", "tensorflow.expand_dims", "tensorflow.math.exp", "tensorflow.gather", "tensorflow.gather", "tensorflow.layers.dense", "tensorflow.layers.dense", "tensorflow.concat", "tensorflow.transpose", "tf.layers.dense.get_shape", "tensorflow.sequence_mask", "ValueError", "list", "range"], "function", ["home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.mask", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.mask", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.mask"], ["", "", "def", "variant_2", "(", "\n", "inputs", ",", "\n", "initializer", ",", "\n", "attention_activation", ",", "\n", "num_sentence_labels", ",", "\n", "num_heads", ",", "\n", "hidden_units", ",", "\n", "sentence_lengths", ",", "\n", "scoring_activation", "=", "None", ",", "\n", "use_inputs_instead_values", "=", "False", ",", "\n", "separate_heads", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    Variant 2 of the multi-head attention to obtain sentence and token scores and predictions.\n    \"\"\"", "\n", "with", "tf", ".", "variable_scope", "(", "\"variant_2\"", ")", ":", "\n", "        ", "num_units", "=", "inputs", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "-", "1", "]", "\n", "if", "num_units", "%", "num_heads", "!=", "0", ":", "\n", "            ", "num_units", "=", "ceil", "(", "num_units", "/", "num_heads", ")", "*", "num_heads", "\n", "inputs", "=", "tf", ".", "layers", ".", "dense", "(", "inputs", ",", "num_units", ")", "# [B, M, num_units]", "\n", "\n", "# Project to get the queries, keys, and values.", "\n", "", "queries", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", ",", "num_units", ",", "activation", "=", "tf", ".", "tanh", ",", "\n", "kernel_initializer", "=", "initializer", ")", "# [B, M, num_units]", "\n", "keys", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", ",", "num_units", ",", "activation", "=", "tf", ".", "tanh", ",", "\n", "kernel_initializer", "=", "initializer", ")", "# [B, M, num_units]", "\n", "values", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", ",", "num_units", ",", "activation", "=", "tf", ".", "tanh", ",", "\n", "kernel_initializer", "=", "initializer", ")", "# [B, M, num_units]", "\n", "\n", "# Mask out the keys, queries and values: replace with 0 all the token", "\n", "# positions between the true and the maximum sentence length.", "\n", "multiplication_mask", "=", "tf", ".", "tile", "(", "\n", "input", "=", "tf", ".", "expand_dims", "(", "tf", ".", "sequence_mask", "(", "sentence_lengths", ")", ",", "axis", "=", "-", "1", ")", ",", "\n", "multiples", "=", "[", "1", ",", "1", ",", "num_units", "]", ")", "# [B, M, num_units]", "\n", "keys", "=", "tf", ".", "where", "(", "multiplication_mask", ",", "keys", ",", "tf", ".", "zeros_like", "(", "keys", ")", ")", "\n", "\n", "# Split and concat as many projections as the number of heads.", "\n", "queries", "=", "tf", ".", "concat", "(", "\n", "tf", ".", "split", "(", "queries", ",", "num_heads", ",", "axis", "=", "2", ")", ",", "\n", "axis", "=", "0", ")", "# [B*num_heads, M, num_units/num_heads]", "\n", "\n", "# [B*num_heads, 1, num_units/num_heads]", "\n", "queries", "=", "tf", ".", "reduce_sum", "(", "queries", ",", "axis", "=", "1", ",", "keep_dims", "=", "True", ")", "\n", "\n", "keys", "=", "tf", ".", "concat", "(", "\n", "tf", ".", "split", "(", "keys", ",", "num_heads", ",", "axis", "=", "2", ")", ",", "\n", "axis", "=", "0", ")", "# [B*num_heads, M, num_units/num_heads]", "\n", "values", "=", "tf", ".", "concat", "(", "\n", "tf", ".", "split", "(", "values", ",", "num_heads", ",", "axis", "=", "2", ")", ",", "\n", "axis", "=", "0", ")", "# [B*num_heads, M, num_units/num_heads]", "\n", "inputs", "=", "tf", ".", "concat", "(", "\n", "tf", ".", "split", "(", "inputs", ",", "num_heads", ",", "axis", "=", "2", ")", ",", "\n", "axis", "=", "0", ")", "# [B*num_heads, M, num_units/num_heads]", "\n", "\n", "# Transpose multiplication and scale", "\n", "attention_evidence", "=", "tf", ".", "matmul", "(", "\n", "queries", ",", "tf", ".", "transpose", "(", "keys", ",", "[", "0", ",", "2", ",", "1", "]", ")", ")", "# [B*num_heads, 1, M]", "\n", "attention_evidence", "=", "tf", ".", "math", ".", "divide", "(", "\n", "attention_evidence", ",", "tf", ".", "constant", "(", "num_units", "**", "0.5", ")", ")", "\n", "\n", "# Mask columns (with values of -infinity), based on rows that have 0 sum.", "\n", "attention_evidence_masked", "=", "mask", "(", "\n", "attention_evidence", ",", "queries", ",", "keys", ",", "mask_type", "=", "\"key\"", ")", "\n", "\n", "# Apply a non-linear layer to obtain (un-normalized) attention weights.", "\n", "if", "attention_activation", "==", "\"soft\"", ":", "\n", "            ", "attention_weights", "=", "tf", ".", "nn", ".", "sigmoid", "(", "attention_evidence_masked", ")", "\n", "", "elif", "attention_activation", "==", "\"sharp\"", ":", "\n", "            ", "attention_weights", "=", "tf", ".", "math", ".", "exp", "(", "attention_evidence_masked", ")", "\n", "", "elif", "attention_activation", "==", "\"linear\"", ":", "\n", "            ", "attention_weights", "=", "attention_evidence_masked", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unknown/unsupported attention activation: %s.\"", "\n", "%", "attention_activation", ")", "\n", "\n", "", "attention_weights_unnormalized", "=", "attention_weights", "\n", "\n", "# Normalize attention weights.", "\n", "attention_weights", "/=", "tf", ".", "reduce_sum", "(", "\n", "attention_weights", ",", "axis", "=", "-", "1", ",", "keep_dims", "=", "True", ")", "\n", "\n", "# Mask rows (with values of 0), based on columns that have 0 sum.", "\n", "attention_weights", "=", "mask", "(", "\n", "attention_weights", ",", "queries", ",", "keys", ",", "mask_type", "=", "\"query\"", ")", "\n", "attention_weights_unnormalized", "=", "mask", "(", "\n", "attention_weights_unnormalized", ",", "queries", ",", "keys", ",", "mask_type", "=", "\"query\"", ")", "\n", "\n", "# Transpose attention weights.", "\n", "attention_weights", "=", "tf", ".", "transpose", "(", "\n", "attention_weights", ",", "[", "0", ",", "2", ",", "1", "]", ")", "# [B*num_heads, M, 1]", "\n", "\n", "# [B*num_heads, M, num_units/num_heads]", "\n", "if", "use_inputs_instead_values", ":", "\n", "            ", "product", "=", "inputs", "*", "attention_weights", "\n", "", "else", ":", "\n", "            ", "product", "=", "values", "*", "attention_weights", "\n", "\n", "", "product", "=", "tf", ".", "reduce_sum", "(", "product", ",", "axis", "=", "1", ")", "# [B*num_heads, num_units/num_heads]", "\n", "\n", "product", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", "=", "product", ",", "units", "=", "hidden_units", ",", "\n", "activation", "=", "tf", ".", "tanh", ",", "kernel_initializer", "=", "initializer", ")", "# [B*num_heads, hidden_units]", "\n", "\n", "processed_tensor", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", "=", "product", ",", "units", "=", "1", ",", "\n", "kernel_initializer", "=", "initializer", ")", "# [B*num_heads, 1]", "\n", "\n", "processed_tensor", "=", "tf", ".", "concat", "(", "\n", "tf", ".", "split", "(", "processed_tensor", ",", "num_heads", ")", ",", "axis", "=", "1", ")", "# [B, num_heads]", "\n", "\n", "if", "separate_heads", ":", "\n", "            ", "if", "num_sentence_labels", "==", "num_heads", ":", "\n", "                ", "sentence_scores", "=", "processed_tensor", "\n", "", "else", ":", "\n", "# Get the sentence representations corresponding to the default head.", "\n", "                ", "default_head", "=", "tf", ".", "gather", "(", "\n", "processed_tensor", ",", "\n", "indices", "=", "[", "0", "]", ",", "axis", "=", "-", "1", ")", "# [B, 1]", "\n", "\n", "# Get the sentence representations corresponding to the non-default head.", "\n", "non_default_heads", "=", "tf", ".", "gather", "(", "\n", "processed_tensor", ",", "\n", "indices", "=", "list", "(", "range", "(", "1", ",", "num_heads", ")", ")", ",", "axis", "=", "-", "1", ")", "# [B, num_heads-1]", "\n", "\n", "# Project onto one unit, corresponding to the default sentence label score.", "\n", "sentence_default_scores", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "default_head", ",", "units", "=", "1", ",", "\n", "activation", "=", "scoring_activation", ",", "kernel_initializer", "=", "initializer", ",", "\n", "name", "=", "\"sentence_default_scores_ff\"", ")", "# [B, 1]", "\n", "\n", "# Project onto (num_sentence_labels-1) units, corresponding to", "\n", "# the non-default sentence label scores.", "\n", "sentence_non_default_scores", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "non_default_heads", ",", "units", "=", "num_sentence_labels", "-", "1", ",", "\n", "activation", "=", "scoring_activation", ",", "kernel_initializer", "=", "initializer", ",", "\n", "name", "=", "\"sentence_non_default_scores_ff\"", ")", "# [B, num_sentence_labels-1]", "\n", "\n", "sentence_scores", "=", "tf", ".", "concat", "(", "\n", "[", "sentence_default_scores", ",", "sentence_non_default_scores", "]", ",", "\n", "axis", "=", "-", "1", ",", "name", "=", "\"sentence_scores_concatenation\"", ")", "# [B, num_sent_labels]", "\n", "", "", "else", ":", "\n", "            ", "sentence_scores", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", "=", "processed_tensor", ",", "units", "=", "num_sentence_labels", ",", "\n", "activation", "=", "scoring_activation", ",", "kernel_initializer", "=", "initializer", ",", "\n", "name", "=", "\"output_sent_specified_scores_ff\"", ")", "# [B, num_sent_labels]", "\n", "\n", "", "sentence_probabilities", "=", "tf", ".", "nn", ".", "softmax", "(", "sentence_scores", ")", "\n", "sentence_predictions", "=", "tf", ".", "argmax", "(", "sentence_probabilities", ",", "axis", "=", "1", ")", "# [B]", "\n", "\n", "# Obtain token scores from attention weights.", "\n", "token_scores", "=", "tf", ".", "transpose", "(", "\n", "attention_weights_unnormalized", ",", "[", "0", ",", "2", ",", "1", "]", ")", "# [num_heads*B, M, 1]", "\n", "token_scores", "=", "tf", ".", "concat", "(", "\n", "tf", ".", "split", "(", "token_scores", ",", "num_heads", ")", ",", "axis", "=", "2", ")", "# [B, M, num_heads]", "\n", "token_probabilities", "=", "tf", ".", "nn", ".", "softmax", "(", "token_scores", ")", "\n", "token_predictions", "=", "tf", ".", "argmax", "(", "\n", "token_probabilities", ",", "axis", "=", "2", ",", "output_type", "=", "tf", ".", "int32", ")", "# [B, M]", "\n", "\n", "attention_weights", "=", "tf", ".", "concat", "(", "\n", "tf", ".", "split", "(", "tf", ".", "transpose", "(", "attention_weights", ",", "[", "0", ",", "2", ",", "1", "]", ")", ",", "num_heads", ")", ",", "\n", "axis", "=", "-", "1", ")", "# [B, M, num_heads]", "\n", "\n", "return", "sentence_scores", ",", "sentence_predictions", ",", "token_scores", ",", "token_predictions", ",", "token_probabilities", ",", "sentence_probabilities", ",", "attention_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.variant_3": [[1338, 1454], ["tensorflow.variable_scope", "tensorflow.Variable", "tensorflow.Variable", "tensorflow.Variable", "tensorflow.tanh", "tensorflow.tensordot", "tensorflow.tile", "tensorflow.where", "tensorflow.transpose", "tensorflow.tile", "tensorflow.reduce_sum", "tensorflow.squeeze", "tensorflow.nn.softmax", "tensorflow.argmax", "tensorflow.nn.softmax", "tensorflow.argmax", "tf.layers.dense.get_shape().as_list", "tensorflow.layers.dense", "tensorflow.random_normal", "tensorflow.random_normal", "tensorflow.random_normal", "tensorflow.nn.sigmoid", "tensorflow.zeros_like", "tensorflow.reduce_sum", "tensorflow.expand_dims", "tensorflow.layers.dense", "tensorflow.layers.dense", "math.ceil", "tensorflow.tensordot", "tensorflow.math.exp", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.gather", "tensorflow.gather", "tensorflow.layers.dense", "tensorflow.layers.dense", "tensorflow.concat", "tf.layers.dense.get_shape", "ValueError", "tensorflow.sequence_mask", "list", "range"], "function", ["None"], ["", "", "def", "variant_3", "(", "\n", "inputs", ",", "\n", "initializer", ",", "\n", "attention_activation", ",", "\n", "num_sentence_labels", ",", "\n", "num_heads", ",", "\n", "attention_size", ",", "\n", "sentence_lengths", ",", "\n", "scoring_activation", "=", "None", ",", "\n", "separate_heads", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    Variant 3 of the multi-head attention to obtain sentence and token scores and predictions.\n    \"\"\"", "\n", "with", "tf", ".", "variable_scope", "(", "\"variant_3\"", ")", ":", "\n", "        ", "num_units", "=", "inputs", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "-", "1", "]", "\n", "if", "num_units", "%", "num_heads", "!=", "0", ":", "\n", "            ", "num_units", "=", "ceil", "(", "num_units", "/", "num_heads", ")", "*", "num_heads", "\n", "inputs", "=", "tf", ".", "layers", ".", "dense", "(", "inputs", ",", "num_units", ")", "# [B, M, num_units]", "\n", "\n", "# Trainable parameters", "\n", "", "w_omega", "=", "tf", ".", "Variable", "(", "\n", "tf", ".", "random_normal", "(", "[", "num_heads", ",", "num_units", ",", "attention_size", "]", ",", "\n", "stddev", "=", "0.1", ")", ")", "# [num_heads, num_units, A]", "\n", "b_omega", "=", "tf", ".", "Variable", "(", "tf", ".", "random_normal", "(", "[", "attention_size", "]", ",", "stddev", "=", "0.1", ")", ")", "\n", "u_omega", "=", "tf", ".", "Variable", "(", "tf", ".", "random_normal", "(", "[", "attention_size", "]", ",", "stddev", "=", "0.1", ")", ")", "\n", "\n", "# Computing the attention score, of shape [B, M, H, A].", "\n", "attention_evidence", "=", "tf", ".", "tanh", "(", "tf", ".", "tensordot", "(", "inputs", ",", "w_omega", ",", "axes", "=", "[", "[", "2", "]", ",", "[", "1", "]", "]", ")", "+", "b_omega", ")", "\n", "attention_evidence", "=", "tf", ".", "tensordot", "(", "\n", "attention_evidence", ",", "u_omega", ",", "axes", "=", "[", "[", "-", "1", "]", ",", "[", "0", "]", "]", ",", "\n", "name", "=", "'attention_evidence_score'", ")", "# [B, M, H]", "\n", "\n", "# Apply a non-linear layer to obtain (un-normalized) attention weights.", "\n", "if", "attention_activation", "==", "\"soft\"", ":", "\n", "            ", "attention_weights_unnormalized", "=", "tf", ".", "nn", ".", "sigmoid", "(", "attention_evidence", ")", "\n", "", "elif", "attention_activation", "==", "\"sharp\"", ":", "\n", "            ", "attention_weights_unnormalized", "=", "tf", ".", "math", ".", "exp", "(", "attention_evidence", ")", "\n", "", "elif", "attention_activation", "==", "\"linear\"", ":", "\n", "            ", "attention_weights_unnormalized", "=", "attention_evidence", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unknown/unsupported attention activation: %s.\"", "\n", "%", "attention_activation", ")", "\n", "\n", "", "tiled_sentence_lengths", "=", "tf", ".", "tile", "(", "\n", "input", "=", "tf", ".", "expand_dims", "(", "\n", "tf", ".", "sequence_mask", "(", "sentence_lengths", ")", ",", "axis", "=", "-", "1", ")", ",", "\n", "multiples", "=", "[", "1", ",", "1", ",", "num_heads", "]", ")", "\n", "\n", "attention_weights_unnormalized", "=", "tf", ".", "where", "(", "\n", "tiled_sentence_lengths", ",", "\n", "attention_weights_unnormalized", ",", "\n", "tf", ".", "zeros_like", "(", "attention_weights_unnormalized", ")", ")", "\n", "\n", "attention_weights", "=", "attention_weights_unnormalized", "/", "tf", ".", "reduce_sum", "(", "\n", "attention_weights_unnormalized", ",", "axis", "=", "1", ",", "keep_dims", "=", "True", ")", "# [B, M, H]", "\n", "\n", "# Prepare alphas and input.", "\n", "attention_weights", "=", "tf", ".", "transpose", "(", "attention_weights", ",", "[", "0", ",", "2", ",", "1", "]", ")", "# [B, H, M, 1]", "\n", "inputs", "=", "tf", ".", "tile", "(", "\n", "input", "=", "tf", ".", "expand_dims", "(", "inputs", ",", "axis", "=", "1", ")", ",", "\n", "multiples", "=", "[", "1", ",", "num_heads", ",", "1", ",", "1", "]", ")", "# [B, H, M, E]", "\n", "\n", "product", "=", "inputs", "*", "tf", ".", "expand_dims", "(", "attention_weights", ",", "axis", "=", "-", "1", ")", "# [B, H, M, E]", "\n", "output", "=", "tf", ".", "reduce_sum", "(", "product", ",", "axis", "=", "2", ")", "# [B, H, E]", "\n", "\n", "processed_tensor", "=", "tf", ".", "squeeze", "(", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", "=", "output", ",", "units", "=", "1", ",", "\n", "kernel_initializer", "=", "initializer", ")", ",", "axis", "=", "-", "1", ")", "# [B, num_heads]", "\n", "\n", "if", "separate_heads", ":", "\n", "            ", "if", "num_sentence_labels", "==", "num_heads", ":", "\n", "                ", "sentence_scores", "=", "processed_tensor", "\n", "", "else", ":", "\n", "# Get the sentence representations corresponding to the default head.", "\n", "                ", "default_head", "=", "tf", ".", "gather", "(", "\n", "processed_tensor", ",", "\n", "indices", "=", "[", "0", "]", ",", "axis", "=", "-", "1", ")", "# [B, 1]", "\n", "\n", "# Get the sentence representations corresponding to the non-default head.", "\n", "non_default_heads", "=", "tf", ".", "gather", "(", "\n", "processed_tensor", ",", "\n", "indices", "=", "list", "(", "range", "(", "1", ",", "num_heads", ")", ")", ",", "axis", "=", "-", "1", ")", "# [B, num_heads-1]", "\n", "\n", "# Project onto one unit, corresponding to the default sentence label score.", "\n", "sentence_default_scores", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "default_head", ",", "units", "=", "1", ",", "\n", "activation", "=", "scoring_activation", ",", "kernel_initializer", "=", "initializer", ",", "\n", "name", "=", "\"sentence_default_scores_ff\"", ")", "# [B, 1]", "\n", "\n", "# Project onto (num_sentence_labels-1) units, corresponding to", "\n", "# the non-default sentence label scores.", "\n", "sentence_non_default_scores", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "non_default_heads", ",", "units", "=", "num_sentence_labels", "-", "1", ",", "\n", "activation", "=", "scoring_activation", ",", "kernel_initializer", "=", "initializer", ",", "\n", "name", "=", "\"sentence_non_default_scores_ff\"", ")", "# [B, num_sentence_labels-1]", "\n", "\n", "sentence_scores", "=", "tf", ".", "concat", "(", "\n", "[", "sentence_default_scores", ",", "sentence_non_default_scores", "]", ",", "\n", "axis", "=", "-", "1", ",", "name", "=", "\"sentence_scores_concatenation\"", ")", "# [B, num_sent_labels]", "\n", "", "", "else", ":", "\n", "            ", "sentence_scores", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", "=", "processed_tensor", ",", "units", "=", "num_sentence_labels", ",", "\n", "activation", "=", "scoring_activation", ",", "kernel_initializer", "=", "initializer", ",", "\n", "name", "=", "\"output_sent_specified_scores_ff\"", ")", "# [B, num_sent_labels]", "\n", "\n", "", "sentence_probabilities", "=", "tf", ".", "nn", ".", "softmax", "(", "sentence_scores", ")", "\n", "sentence_predictions", "=", "tf", ".", "argmax", "(", "sentence_probabilities", ",", "axis", "=", "1", ")", "# [B]", "\n", "\n", "token_scores", "=", "attention_weights_unnormalized", "# [B, M, num_heads]", "\n", "token_probabilities", "=", "tf", ".", "nn", ".", "softmax", "(", "token_scores", ")", "\n", "token_predictions", "=", "tf", ".", "argmax", "(", "\n", "token_probabilities", ",", "axis", "=", "2", ",", "output_type", "=", "tf", ".", "int32", ")", "# [B, M]", "\n", "\n", "return", "sentence_scores", ",", "sentence_predictions", ",", "token_scores", ",", "token_predictions", ",", "token_probabilities", ",", "sentence_probabilities", ",", "attention_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.variant_4": [[1456, 1627], ["tensorflow.variable_scope", "tensorflow.layers.dense", "tensorflow.layers.dense", "tensorflow.layers.dense", "tensorflow.tile", "tensorflow.where", "tensorflow.where", "tensorflow.where", "tensorflow.concat", "tensorflow.concat", "tensorflow.concat", "tensorflow.concat", "tensorflow.matmul", "tensorflow.math.divide", "modules.mask", "modules.mask", "tensorflow.concat", "tensorflow.nn.softmax", "tensorflow.argmax", "tensorflow.layers.dense", "tensorflow.layers.dense", "tensorflow.concat", "tensorflow.nn.softmax", "tensorflow.argmax", "tensorflow.concat", "tf.layers.dense.get_shape().as_list", "tensorflow.layers.dense", "tensorflow.zeros_like", "tensorflow.zeros_like", "tensorflow.zeros_like", "tensorflow.split", "tensorflow.split", "tensorflow.split", "tensorflow.split", "tensorflow.transpose", "tensorflow.constant", "tensorflow.nn.sigmoid", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.split", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.split", "tensorflow.layers.dense", "tensorflow.split", "math.ceil", "tensorflow.expand_dims", "tensorflow.math.exp", "tensorflow.reduce_max", "tensorflow.expand_dims", "tensorflow.gather", "tensorflow.gather", "tensorflow.layers.dense", "tensorflow.layers.dense", "tensorflow.concat", "tensorflow.expand_dims", "tf.layers.dense.get_shape", "tensorflow.sequence_mask", "ValueError", "tensorflow.reduce_mean", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.reduce_logsumexp", "ValueError", "list", "range"], "function", ["home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.mask", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.mask"], ["", "", "def", "variant_4", "(", "\n", "inputs", ",", "\n", "initializer", ",", "\n", "attention_activation", ",", "\n", "num_sentence_labels", ",", "\n", "num_heads", ",", "\n", "hidden_units", ",", "\n", "sentence_lengths", ",", "\n", "scoring_activation", "=", "None", ",", "\n", "token_scoring_method", "=", "\"max\"", ",", "\n", "use_inputs_instead_values", "=", "False", ",", "\n", "separate_heads", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    Variant 4 of the multi-head attention to obtain sentence and token scores and predictions.\n    \"\"\"", "\n", "with", "tf", ".", "variable_scope", "(", "\"variant_4\"", ")", ":", "\n", "        ", "num_units", "=", "inputs", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "-", "1", "]", "\n", "if", "num_units", "%", "num_heads", "!=", "0", ":", "\n", "            ", "num_units", "=", "ceil", "(", "num_units", "/", "num_heads", ")", "*", "num_heads", "\n", "inputs", "=", "tf", ".", "layers", ".", "dense", "(", "inputs", ",", "num_units", ")", "# [B, M, num_units]", "\n", "\n", "# Project to get the queries, keys, and values.", "\n", "", "queries", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", ",", "num_units", ",", "activation", "=", "tf", ".", "tanh", ",", "\n", "kernel_initializer", "=", "initializer", ")", "# [B, M, num_units]", "\n", "keys", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", ",", "num_units", ",", "activation", "=", "tf", ".", "tanh", ",", "\n", "kernel_initializer", "=", "initializer", ")", "# [B, M, num_units]", "\n", "values", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", ",", "num_units", ",", "activation", "=", "tf", ".", "tanh", ",", "\n", "kernel_initializer", "=", "initializer", ")", "# [B, M, num_units]", "\n", "\n", "# Mask out the keys, queries and values: replace with 0 all the token", "\n", "# positions between the true and the maximum sentence length.", "\n", "multiplication_mask", "=", "tf", ".", "tile", "(", "\n", "input", "=", "tf", ".", "expand_dims", "(", "tf", ".", "sequence_mask", "(", "sentence_lengths", ")", ",", "axis", "=", "-", "1", ")", ",", "\n", "multiples", "=", "[", "1", ",", "1", ",", "num_units", "]", ")", "# [B, M, num_units]", "\n", "queries", "=", "tf", ".", "where", "(", "multiplication_mask", ",", "queries", ",", "tf", ".", "zeros_like", "(", "queries", ")", ")", "\n", "keys", "=", "tf", ".", "where", "(", "multiplication_mask", ",", "keys", ",", "tf", ".", "zeros_like", "(", "keys", ")", ")", "\n", "values", "=", "tf", ".", "where", "(", "multiplication_mask", ",", "values", ",", "tf", ".", "zeros_like", "(", "values", ")", ")", "\n", "\n", "# Split and concat as many projections as the number of heads.", "\n", "queries", "=", "tf", ".", "concat", "(", "\n", "tf", ".", "split", "(", "queries", ",", "num_heads", ",", "axis", "=", "2", ")", ",", "\n", "axis", "=", "0", ")", "# [B*num_heads, M, num_units/num_heads]", "\n", "keys", "=", "tf", ".", "concat", "(", "\n", "tf", ".", "split", "(", "keys", ",", "num_heads", ",", "axis", "=", "2", ")", ",", "\n", "axis", "=", "0", ")", "# [B*num_heads, M, num_units/num_heads]", "\n", "values", "=", "tf", ".", "concat", "(", "\n", "tf", ".", "split", "(", "values", ",", "num_heads", ",", "axis", "=", "2", ")", ",", "\n", "axis", "=", "0", ")", "# [B*num_heads, M, num_units/num_heads]", "\n", "inputs", "=", "tf", ".", "concat", "(", "\n", "tf", ".", "split", "(", "inputs", ",", "num_heads", ",", "axis", "=", "2", ")", ",", "\n", "axis", "=", "0", ")", "# [B*num_heads, M, num_units/num_heads]", "\n", "\n", "# Transpose multiplication and scale", "\n", "attention_evidence", "=", "tf", ".", "matmul", "(", "\n", "queries", ",", "tf", ".", "transpose", "(", "keys", ",", "[", "0", ",", "2", ",", "1", "]", ")", ")", "# [B*num_heads, M, M]", "\n", "attention_evidence", "=", "tf", ".", "math", ".", "divide", "(", "\n", "attention_evidence", ",", "tf", ".", "constant", "(", "num_units", "**", "0.5", ")", ")", "\n", "\n", "# Mask columns (with values of -infinity), based on rows that have 0 sum.", "\n", "attention_evidence_masked", "=", "mask", "(", "\n", "attention_evidence", ",", "queries", ",", "keys", ",", "mask_type", "=", "\"key\"", ")", "\n", "\n", "# Apply a non-linear layer to obtain (un-normalized) attention weights.", "\n", "if", "attention_activation", "==", "\"soft\"", ":", "\n", "            ", "attention_weights_unnormalized", "=", "tf", ".", "nn", ".", "sigmoid", "(", "attention_evidence_masked", ")", "\n", "", "elif", "attention_activation", "==", "\"sharp\"", ":", "\n", "            ", "attention_weights_unnormalized", "=", "tf", ".", "math", ".", "exp", "(", "attention_evidence_masked", ")", "\n", "", "elif", "attention_activation", "==", "\"linear\"", ":", "\n", "            ", "attention_weights_unnormalized", "=", "attention_evidence_masked", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unknown/unsupported attention activation: %s.\"", "\n", "%", "attention_activation", ")", "\n", "\n", "", "attention_weights_unnormalized", "=", "mask", "(", "# [B*num_heads, M, M]", "\n", "attention_weights_unnormalized", ",", "queries", ",", "keys", ",", "mask_type", "=", "\"query\"", ")", "\n", "\n", "# Obtain token scores from attention weights. Shape is [B*num_heads, M].", "\n", "if", "token_scoring_method", "==", "\"sum\"", ":", "\n", "            ", "attention_weights_unnormalized", "=", "tf", ".", "reduce_sum", "(", "\n", "attention_weights_unnormalized", ",", "axis", "=", "1", ")", "\n", "", "elif", "token_scoring_method", "==", "\"max\"", ":", "\n", "            ", "attention_weights_unnormalized", "=", "tf", ".", "reduce_max", "(", "\n", "attention_weights_unnormalized", ",", "axis", "=", "1", ")", "\n", "", "elif", "token_scoring_method", "==", "\"avg\"", ":", "\n", "            ", "attention_weights_unnormalized", "=", "tf", ".", "reduce_mean", "(", "\n", "attention_weights_unnormalized", ",", "axis", "=", "1", ")", "\n", "", "elif", "token_scoring_method", "==", "\"logsumexp\"", ":", "\n", "            ", "attention_weights_unnormalized", "=", "tf", ".", "reduce_logsumexp", "(", "\n", "attention_weights_unnormalized", ",", "axis", "=", "1", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unknown/unsupported token scoring method: %s\"", "\n", "%", "token_scoring_method", ")", "\n", "\n", "# Normalize to obtain attention weights.", "\n", "", "attention_weights", "=", "attention_weights_unnormalized", "/", "tf", ".", "reduce_sum", "(", "\n", "attention_weights_unnormalized", ",", "axis", "=", "1", ",", "keep_dims", "=", "True", ")", "\n", "\n", "token_scores", "=", "tf", ".", "concat", "(", "\n", "tf", ".", "split", "(", "tf", ".", "expand_dims", "(", "attention_weights_unnormalized", ",", "axis", "=", "2", ")", ",", "num_heads", ")", ",", "\n", "axis", "=", "2", ")", "# [B, M, num_heads]", "\n", "token_probabilities", "=", "tf", ".", "nn", ".", "softmax", "(", "token_scores", ")", "\n", "token_predictions", "=", "tf", ".", "argmax", "(", "\n", "token_probabilities", ",", "axis", "=", "2", ",", "output_type", "=", "tf", ".", "int32", ")", "# [B, M]", "\n", "\n", "if", "use_inputs_instead_values", ":", "\n", "            ", "product", "=", "tf", ".", "reduce_sum", "(", "inputs", "*", "tf", ".", "expand_dims", "(", "attention_weights", ",", "axis", "=", "-", "1", ")", ",", "\n", "axis", "=", "1", ")", "# [B*num_heads, num_units/num_heads]", "\n", "", "else", ":", "\n", "            ", "product", "=", "tf", ".", "reduce_sum", "(", "values", "*", "tf", ".", "expand_dims", "(", "attention_weights", ",", "axis", "=", "-", "1", ")", ",", "\n", "axis", "=", "1", ")", "# [B*num_heads, num_units/num_heads]", "\n", "\n", "", "product", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", "=", "product", ",", "units", "=", "hidden_units", ",", "\n", "activation", "=", "tf", ".", "tanh", ",", "kernel_initializer", "=", "initializer", ")", "# [B*num_heads, hidden_units]", "\n", "\n", "processed_tensor", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", "=", "product", ",", "units", "=", "1", ",", "\n", "kernel_initializer", "=", "initializer", ")", "# [B*num_heads, 1]", "\n", "\n", "processed_tensor", "=", "tf", ".", "concat", "(", "\n", "tf", ".", "split", "(", "processed_tensor", ",", "num_heads", ")", ",", "axis", "=", "1", ")", "# [B, num_heads]", "\n", "\n", "if", "separate_heads", ":", "\n", "            ", "if", "num_sentence_labels", "==", "num_heads", ":", "\n", "                ", "sentence_scores", "=", "processed_tensor", "\n", "", "else", ":", "\n", "# Get the sentence representations corresponding to the default head.", "\n", "                ", "default_head", "=", "tf", ".", "gather", "(", "\n", "processed_tensor", ",", "\n", "indices", "=", "[", "0", "]", ",", "axis", "=", "-", "1", ")", "# [B, 1]", "\n", "\n", "# Get the sentence representations corresponding to the non-default head.", "\n", "non_default_heads", "=", "tf", ".", "gather", "(", "\n", "processed_tensor", ",", "\n", "indices", "=", "list", "(", "range", "(", "1", ",", "num_heads", ")", ")", ",", "axis", "=", "-", "1", ")", "# [B, num_heads-1]", "\n", "\n", "# Project onto one unit, corresponding to the default sentence label score.", "\n", "sentence_default_scores", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "default_head", ",", "units", "=", "1", ",", "\n", "activation", "=", "scoring_activation", ",", "kernel_initializer", "=", "initializer", ",", "\n", "name", "=", "\"sentence_default_scores_ff\"", ")", "# [B, 1]", "\n", "\n", "# Project onto (num_sentence_labels-1) units, corresponding to", "\n", "# the non-default sentence label scores.", "\n", "sentence_non_default_scores", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "non_default_heads", ",", "units", "=", "num_sentence_labels", "-", "1", ",", "\n", "activation", "=", "scoring_activation", ",", "kernel_initializer", "=", "initializer", ",", "\n", "name", "=", "\"sentence_non_default_scores_ff\"", ")", "# [B, num_sentence_labels-1]", "\n", "\n", "sentence_scores", "=", "tf", ".", "concat", "(", "\n", "[", "sentence_default_scores", ",", "sentence_non_default_scores", "]", ",", "\n", "axis", "=", "-", "1", ",", "name", "=", "\"sentence_scores_concatenation\"", ")", "# [B, num_sent_labels]", "\n", "", "", "else", ":", "\n", "            ", "sentence_scores", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", "=", "processed_tensor", ",", "units", "=", "num_sentence_labels", ",", "\n", "activation", "=", "scoring_activation", ",", "kernel_initializer", "=", "initializer", ",", "\n", "name", "=", "\"output_sent_specified_scores_ff\"", ")", "# [B, num_sent_labels]", "\n", "\n", "", "sentence_probabilities", "=", "tf", ".", "nn", ".", "softmax", "(", "sentence_scores", ")", "\n", "sentence_predictions", "=", "tf", ".", "argmax", "(", "sentence_probabilities", ",", "axis", "=", "1", ")", "# [B]", "\n", "\n", "attention_weights", "=", "tf", ".", "concat", "(", "\n", "tf", ".", "split", "(", "tf", ".", "expand_dims", "(", "attention_weights", ",", "axis", "=", "-", "1", ")", ",", "num_heads", ")", ",", "\n", "axis", "=", "-", "1", ")", "# [B, M, num_heads]", "\n", "\n", "return", "sentence_scores", ",", "sentence_predictions", ",", "token_scores", ",", "token_predictions", ",", "token_probabilities", ",", "sentence_probabilities", ",", "attention_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.variant_5": [[1629, 1801], ["tensorflow.variable_scope", "tensorflow.layers.dense", "tensorflow.layers.dense", "tensorflow.layers.dense", "tensorflow.tile", "tensorflow.where", "tensorflow.where", "tensorflow.where", "tensorflow.concat", "tensorflow.concat", "tensorflow.concat", "tensorflow.concat", "tensorflow.matmul", "tensorflow.math.divide", "tensorflow.tile", "tensorflow.where", "tensorflow.concat", "tensorflow.nn.softmax", "tensorflow.argmax", "tensorflow.layers.dense", "tensorflow.layers.dense", "tensorflow.concat", "tensorflow.nn.softmax", "tensorflow.argmax", "tensorflow.concat", "tf.layers.dense.get_shape().as_list", "tensorflow.layers.dense", "tensorflow.zeros_like", "tensorflow.zeros_like", "tensorflow.zeros_like", "tensorflow.split", "tensorflow.split", "tensorflow.split", "tensorflow.split", "tensorflow.transpose", "tensorflow.constant", "tensorflow.reduce_sum", "tensorflow.nn.sigmoid", "tensorflow.zeros_like", "tensorflow.reduce_sum", "tensorflow.split", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.split", "tensorflow.layers.dense", "tensorflow.split", "math.ceil", "tensorflow.expand_dims", "tensorflow.reduce_max", "tensorflow.math.exp", "tensorflow.sequence_mask", "tensorflow.expand_dims", "tensorflow.gather", "tensorflow.gather", "tensorflow.layers.dense", "tensorflow.layers.dense", "tensorflow.concat", "tensorflow.expand_dims", "tf.layers.dense.get_shape", "tensorflow.sequence_mask", "tensorflow.reduce_mean", "ValueError", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.reduce_logsumexp", "ValueError", "list", "range"], "function", ["None"], ["", "", "def", "variant_5", "(", "\n", "inputs", ",", "\n", "initializer", ",", "\n", "attention_activation", ",", "\n", "num_sentence_labels", ",", "\n", "num_heads", ",", "\n", "hidden_units", ",", "\n", "sentence_lengths", ",", "\n", "scoring_activation", "=", "None", ",", "\n", "token_scoring_method", "=", "\"max\"", ",", "\n", "use_inputs_instead_values", "=", "False", ",", "\n", "separate_heads", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    Variant 5 of the multi-head attention to obtain sentence and token scores and predictions.\n    \"\"\"", "\n", "with", "tf", ".", "variable_scope", "(", "\"variant_5\"", ")", ":", "\n", "        ", "num_units", "=", "inputs", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "-", "1", "]", "\n", "if", "num_units", "%", "num_heads", "!=", "0", ":", "\n", "            ", "num_units", "=", "ceil", "(", "num_units", "/", "num_heads", ")", "*", "num_heads", "\n", "inputs", "=", "tf", ".", "layers", ".", "dense", "(", "inputs", ",", "num_units", ")", "# [B, M, num_units]", "\n", "\n", "# Project to get the queries, keys, and values.", "\n", "", "queries", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", ",", "num_units", ",", "activation", "=", "tf", ".", "tanh", ",", "\n", "kernel_initializer", "=", "initializer", ")", "# [B, M, num_units]", "\n", "keys", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", ",", "num_units", ",", "activation", "=", "tf", ".", "tanh", ",", "\n", "kernel_initializer", "=", "initializer", ")", "# [B, M, num_units]", "\n", "values", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", ",", "num_units", ",", "activation", "=", "tf", ".", "tanh", ",", "\n", "kernel_initializer", "=", "initializer", ")", "# [B, M, num_units]", "\n", "\n", "# Mask out the keys, queries and values: replace with 0 all the token", "\n", "# positions between the true and the maximum sentence length.", "\n", "multiplication_mask", "=", "tf", ".", "tile", "(", "\n", "input", "=", "tf", ".", "expand_dims", "(", "tf", ".", "sequence_mask", "(", "sentence_lengths", ")", ",", "axis", "=", "-", "1", ")", ",", "\n", "multiples", "=", "[", "1", ",", "1", ",", "num_units", "]", ")", "# [B, M, num_units]", "\n", "queries", "=", "tf", ".", "where", "(", "multiplication_mask", ",", "queries", ",", "tf", ".", "zeros_like", "(", "queries", ")", ")", "\n", "keys", "=", "tf", ".", "where", "(", "multiplication_mask", ",", "keys", ",", "tf", ".", "zeros_like", "(", "keys", ")", ")", "\n", "values", "=", "tf", ".", "where", "(", "multiplication_mask", ",", "values", ",", "tf", ".", "zeros_like", "(", "values", ")", ")", "\n", "\n", "# Split and concat as many projections as the number of heads.", "\n", "queries", "=", "tf", ".", "concat", "(", "\n", "tf", ".", "split", "(", "queries", ",", "num_heads", ",", "axis", "=", "2", ")", ",", "\n", "axis", "=", "0", ")", "# [B*num_heads, M, num_units/num_heads]", "\n", "keys", "=", "tf", ".", "concat", "(", "\n", "tf", ".", "split", "(", "keys", ",", "num_heads", ",", "axis", "=", "2", ")", ",", "\n", "axis", "=", "0", ")", "# [B*num_heads, M, num_units/num_heads]", "\n", "values", "=", "tf", ".", "concat", "(", "\n", "tf", ".", "split", "(", "values", ",", "num_heads", ",", "axis", "=", "2", ")", ",", "\n", "axis", "=", "0", ")", "# [B*num_heads, M, num_units/num_heads]", "\n", "inputs", "=", "tf", ".", "concat", "(", "\n", "tf", ".", "split", "(", "inputs", ",", "num_heads", ",", "axis", "=", "2", ")", ",", "\n", "axis", "=", "0", ")", "# [B*num_heads, M, num_units/num_heads]", "\n", "\n", "# Transpose multiplication and scale", "\n", "attention_evidence", "=", "tf", ".", "matmul", "(", "\n", "queries", ",", "tf", ".", "transpose", "(", "keys", ",", "[", "0", ",", "2", ",", "1", "]", ")", ")", "# [B*num_heads, M, M]", "\n", "attention_evidence", "=", "tf", ".", "math", ".", "divide", "(", "\n", "attention_evidence", ",", "tf", ".", "constant", "(", "num_units", "**", "0.5", ")", ")", "\n", "\n", "# Obtain token scores from attention weights. Shape is [B*num_heads, M].", "\n", "if", "token_scoring_method", "==", "\"sum\"", ":", "\n", "            ", "attention_evidence", "=", "tf", ".", "reduce_sum", "(", "\n", "attention_evidence", ",", "axis", "=", "1", ")", "\n", "", "elif", "token_scoring_method", "==", "\"max\"", ":", "\n", "            ", "attention_evidence", "=", "tf", ".", "reduce_max", "(", "\n", "attention_evidence", ",", "axis", "=", "1", ")", "\n", "", "elif", "token_scoring_method", "==", "\"avg\"", ":", "\n", "            ", "attention_evidence", "=", "tf", ".", "reduce_mean", "(", "\n", "attention_evidence", ",", "axis", "=", "1", ")", "\n", "", "elif", "token_scoring_method", "==", "\"logsumexp\"", ":", "\n", "            ", "attention_evidence", "=", "tf", ".", "reduce_logsumexp", "(", "\n", "attention_evidence", ",", "axis", "=", "1", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unknown/unsupported token scoring method: %s\"", "\n", "%", "token_scoring_method", ")", "\n", "\n", "# Apply a non-linear layer to obtain un-normalized attention weights.", "\n", "", "if", "attention_activation", "==", "\"soft\"", ":", "\n", "            ", "attention_weights_unnormalized", "=", "tf", ".", "nn", ".", "sigmoid", "(", "attention_evidence", ")", "\n", "", "elif", "attention_activation", "==", "\"sharp\"", ":", "\n", "            ", "attention_weights_unnormalized", "=", "tf", ".", "math", ".", "exp", "(", "attention_evidence", ")", "\n", "", "elif", "attention_activation", "==", "\"linear\"", ":", "\n", "            ", "attention_weights_unnormalized", "=", "attention_evidence", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unknown/unsupported attention activation: %s.\"", "\n", "%", "attention_activation", ")", "\n", "\n", "", "tiled_sentence_lengths", "=", "tf", ".", "tile", "(", "\n", "input", "=", "tf", ".", "sequence_mask", "(", "sentence_lengths", ")", ",", "multiples", "=", "[", "num_heads", ",", "1", "]", ")", "\n", "\n", "attention_weights_unnormalized", "=", "tf", ".", "where", "(", "\n", "tiled_sentence_lengths", ",", "\n", "attention_weights_unnormalized", ",", "\n", "tf", ".", "zeros_like", "(", "attention_weights_unnormalized", ")", ")", "\n", "\n", "# Normalize to obtain attention weights of shape [B*num_heads, M].", "\n", "attention_weights", "=", "attention_weights_unnormalized", "/", "tf", ".", "reduce_sum", "(", "\n", "attention_weights_unnormalized", ",", "axis", "=", "1", ",", "keep_dims", "=", "True", ")", "\n", "\n", "token_scores", "=", "tf", ".", "concat", "(", "\n", "tf", ".", "split", "(", "tf", ".", "expand_dims", "(", "attention_weights_unnormalized", ",", "axis", "=", "2", ")", ",", "num_heads", ")", ",", "\n", "axis", "=", "2", ")", "# [B, M, num_heads]", "\n", "token_probabilities", "=", "tf", ".", "nn", ".", "softmax", "(", "token_scores", ")", "\n", "token_predictions", "=", "tf", ".", "argmax", "(", "\n", "token_probabilities", ",", "axis", "=", "2", ",", "output_type", "=", "tf", ".", "int32", ")", "# [B, M]", "\n", "\n", "if", "use_inputs_instead_values", ":", "\n", "            ", "product", "=", "tf", ".", "reduce_sum", "(", "inputs", "*", "tf", ".", "expand_dims", "(", "attention_weights", ",", "axis", "=", "-", "1", ")", ",", "\n", "axis", "=", "1", ")", "# [B*num_heads, num_units/num_heads]", "\n", "", "else", ":", "\n", "            ", "product", "=", "tf", ".", "reduce_sum", "(", "values", "*", "tf", ".", "expand_dims", "(", "attention_weights", ",", "axis", "=", "-", "1", ")", ",", "\n", "axis", "=", "1", ")", "# [B*num_heads, num_units/num_heads]", "\n", "\n", "", "product", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", "=", "product", ",", "units", "=", "hidden_units", ",", "\n", "activation", "=", "tf", ".", "tanh", ",", "kernel_initializer", "=", "initializer", ")", "# [B*num_heads, hidden_units]", "\n", "\n", "processed_tensor", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", "=", "product", ",", "units", "=", "1", ",", "\n", "kernel_initializer", "=", "initializer", ")", "# [B*num_heads, 1]", "\n", "\n", "processed_tensor", "=", "tf", ".", "concat", "(", "\n", "tf", ".", "split", "(", "processed_tensor", ",", "num_heads", ")", ",", "axis", "=", "1", ")", "# [B, num_heads]", "\n", "\n", "if", "separate_heads", ":", "\n", "            ", "if", "num_sentence_labels", "==", "num_heads", ":", "\n", "                ", "sentence_scores", "=", "processed_tensor", "\n", "", "else", ":", "\n", "# Get the sentence representations corresponding to the default head.", "\n", "                ", "default_head", "=", "tf", ".", "gather", "(", "\n", "processed_tensor", ",", "\n", "indices", "=", "[", "0", "]", ",", "axis", "=", "-", "1", ")", "# [B, 1]", "\n", "\n", "# Get the sentence representations corresponding to the non-default head.", "\n", "non_default_heads", "=", "tf", ".", "gather", "(", "\n", "processed_tensor", ",", "\n", "indices", "=", "list", "(", "range", "(", "1", ",", "num_heads", ")", ")", ",", "axis", "=", "-", "1", ")", "# [B, num_heads-1]", "\n", "\n", "# Project onto one unit, corresponding to the default sentence label score.", "\n", "sentence_default_scores", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "default_head", ",", "units", "=", "1", ",", "\n", "activation", "=", "scoring_activation", ",", "kernel_initializer", "=", "initializer", ",", "\n", "name", "=", "\"sentence_default_scores_ff\"", ")", "# [B, 1]", "\n", "\n", "# Project onto (num_sentence_labels-1) units, corresponding to", "\n", "# the non-default sentence label scores.", "\n", "sentence_non_default_scores", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "non_default_heads", ",", "units", "=", "num_sentence_labels", "-", "1", ",", "\n", "activation", "=", "scoring_activation", ",", "kernel_initializer", "=", "initializer", ",", "\n", "name", "=", "\"sentence_non_default_scores_ff\"", ")", "# [B, num_sentence_labels-1]", "\n", "\n", "sentence_scores", "=", "tf", ".", "concat", "(", "\n", "[", "sentence_default_scores", ",", "sentence_non_default_scores", "]", ",", "\n", "axis", "=", "-", "1", ",", "name", "=", "\"sentence_scores_concatenation\"", ")", "# [B, num_sent_labels]", "\n", "", "", "else", ":", "\n", "            ", "sentence_scores", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", "=", "processed_tensor", ",", "units", "=", "num_sentence_labels", ",", "\n", "activation", "=", "scoring_activation", ",", "kernel_initializer", "=", "initializer", ",", "\n", "name", "=", "\"output_sent_specified_scores_ff\"", ")", "# [B, num_sent_labels]", "\n", "\n", "", "sentence_probabilities", "=", "tf", ".", "nn", ".", "softmax", "(", "sentence_scores", ")", "\n", "sentence_predictions", "=", "tf", ".", "argmax", "(", "sentence_probabilities", ",", "axis", "=", "1", ")", "# [B]", "\n", "\n", "attention_weights", "=", "tf", ".", "concat", "(", "\n", "tf", ".", "split", "(", "tf", ".", "expand_dims", "(", "attention_weights", ",", "axis", "=", "-", "1", ")", ",", "num_heads", ")", ",", "\n", "axis", "=", "-", "1", ")", "# [B, M, num_heads]", "\n", "\n", "return", "sentence_scores", ",", "sentence_predictions", ",", "token_scores", ",", "token_predictions", ",", "token_probabilities", ",", "sentence_probabilities", ",", "attention_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.variant_6": [[1803, 1954], ["tensorflow.variable_scope", "range", "tensorflow.stack", "tensorflow.stack", "tensorflow.stack", "tensorflow.matmul", "tensorflow.math.divide", "modules.mask_2", "tensorflow.reduce_sum", "modules.mask_2", "modules.mask_2", "tensorflow.matmul", "tensorflow.reduce_sum", "tensorflow.layers.dense", "tensorflow.layers.dense", "tensorflow.transpose", "tensorflow.nn.softmax", "tensorflow.argmax", "tensorflow.transpose", "tensorflow.nn.softmax", "tensorflow.argmax", "tensorflow.transpose", "inputs.get_shape().as_list", "tensorflow.transpose", "tensorflow.constant", "tensorflow.nn.sigmoid", "tensorflow.squeeze", "tensorflow.layers.dense", "tensorflow.reduce_sum", "tensorflow.variable_scope", "tensorflow.layers.dense", "tensorflow.layers.dense", "tensorflow.layers.dense", "keys_list.append", "queries_list.append", "values_list.append", "tensorflow.math.exp", "tensorflow.gather", "tensorflow.gather", "tensorflow.layers.dense", "tensorflow.layers.dense", "tensorflow.concat", "tensorflow.reduce_max", "inputs.get_shape", "ValueError", "tensorflow.reduce_mean", "tensorflow.contrib.layers.l2_regularizer", "list", "tensorflow.reduce_logsumexp", "ValueError", "range"], "function", ["home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.mask_2", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.mask_2", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.mask_2"], ["", "", "def", "variant_6", "(", "\n", "inputs", ",", "\n", "initializer", ",", "\n", "attention_activation", ",", "\n", "num_sentence_labels", ",", "\n", "num_heads", ",", "\n", "hidden_units", ",", "\n", "scoring_activation", "=", "None", ",", "\n", "token_scoring_method", "=", "\"max\"", ",", "\n", "separate_heads", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    Variant 6 of the multi-head attention to obtain sentence and token scores and predictions.\n    \"\"\"", "\n", "with", "tf", ".", "variable_scope", "(", "\"variant_6\"", ")", ":", "\n", "        ", "num_units", "=", "inputs", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "-", "1", "]", "\n", "keys_list", "=", "[", "]", "\n", "queries_list", "=", "[", "]", "\n", "values_list", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "num_heads", ")", ":", "\n", "            ", "with", "tf", ".", "variable_scope", "(", "\"num_head_{}\"", ".", "format", "(", "i", ")", ",", "reuse", "=", "tf", ".", "AUTO_REUSE", ")", ":", "\n", "                ", "keys_this_head", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", ",", "num_units", ",", "activation", "=", "tf", ".", "tanh", ",", "\n", "kernel_initializer", "=", "initializer", ")", "# [B, M, num_units]", "\n", "queries_this_head", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", ",", "num_units", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ",", "\n", "kernel_regularizer", "=", "tf", ".", "contrib", ".", "layers", ".", "l2_regularizer", "(", "scale", "=", "0.7", ")", ",", "\n", "kernel_initializer", "=", "initializer", ")", "# [B, M, num_units]", "\n", "values_this_head", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", ",", "num_units", ",", "activation", "=", "tf", ".", "tanh", ",", "\n", "kernel_initializer", "=", "initializer", ")", "# [B, M, num_units]", "\n", "\n", "keys_list", ".", "append", "(", "keys_this_head", ")", "\n", "queries_list", ".", "append", "(", "queries_this_head", ")", "\n", "values_list", ".", "append", "(", "values_this_head", ")", "\n", "\n", "", "", "keys", "=", "tf", ".", "stack", "(", "keys_list", ")", "# [num_heads, B, M, num_units]", "\n", "queries", "=", "tf", ".", "stack", "(", "queries_list", ")", "# [num_heads, B, M, num_units]", "\n", "values", "=", "tf", ".", "stack", "(", "values_list", ")", "# [num_heads, B, M, num_units]", "\n", "\n", "# Transpose multiplication and scale", "\n", "attention_evidence", "=", "tf", ".", "matmul", "(", "\n", "queries", ",", "tf", ".", "transpose", "(", "keys", ",", "[", "0", ",", "1", ",", "3", ",", "2", "]", ")", ")", "# [num_heads, B, M, M]", "\n", "attention_evidence", "=", "tf", ".", "math", ".", "divide", "(", "\n", "attention_evidence", ",", "tf", ".", "constant", "(", "num_units", "**", "0.5", ")", ")", "\n", "\n", "# Mask columns (with values of -infinity), based on rows that have 0 sum.", "\n", "attention_evidence_masked", "=", "mask_2", "(", "\n", "attention_evidence", ",", "queries", ",", "keys", ",", "mask_type", "=", "\"key\"", ")", "\n", "\n", "# Apply a non-linear layer to obtain (un-normalized) attention weights.", "\n", "if", "attention_activation", "==", "\"soft\"", ":", "\n", "            ", "attention_weights", "=", "tf", ".", "nn", ".", "sigmoid", "(", "attention_evidence_masked", ")", "\n", "", "elif", "attention_activation", "==", "\"sharp\"", ":", "\n", "            ", "attention_weights", "=", "tf", ".", "math", ".", "exp", "(", "attention_evidence_masked", ")", "\n", "", "elif", "attention_activation", "==", "\"linear\"", ":", "\n", "            ", "attention_weights", "=", "attention_evidence_masked", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unknown/unsupported attention activation: %s.\"", "\n", "%", "attention_activation", ")", "\n", "\n", "", "attention_weights_unnormalized", "=", "attention_weights", "\n", "\n", "# Normalize attention weights.", "\n", "attention_weights", "/=", "tf", ".", "reduce_sum", "(", "\n", "attention_weights", ",", "axis", "=", "-", "1", ",", "keep_dims", "=", "True", ")", "\n", "\n", "# Mask rows (with values of 0), based on columns that have 0 sum.", "\n", "attention_weights", "=", "mask_2", "(", "\n", "attention_weights", ",", "queries", ",", "keys", ",", "mask_type", "=", "\"query\"", ")", "\n", "attention_weights_unnormalized", "=", "mask_2", "(", "\n", "attention_weights_unnormalized", ",", "queries", ",", "keys", ",", "mask_type", "=", "\"query\"", ")", "\n", "\n", "# [num_heads, B, M, num_units]", "\n", "product", "=", "tf", ".", "matmul", "(", "attention_weights", ",", "values", ")", "\n", "\n", "product", "=", "tf", ".", "reduce_sum", "(", "product", ",", "axis", "=", "2", ")", "# [num_heads, B, num_units]", "\n", "\n", "product", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", "=", "product", ",", "units", "=", "hidden_units", ",", "\n", "activation", "=", "tf", ".", "tanh", ",", "kernel_initializer", "=", "initializer", ")", "# [num_heads, B, hidden_units]", "\n", "\n", "processed_tensor", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", "=", "product", ",", "units", "=", "1", ",", "\n", "kernel_initializer", "=", "initializer", ")", "# [num_heads, B, 1]", "\n", "\n", "processed_tensor", "=", "tf", ".", "transpose", "(", "\n", "tf", ".", "squeeze", "(", "processed_tensor", ",", "axis", "=", "-", "1", ")", ",", "[", "1", ",", "0", "]", ")", "# [B, num_heads]", "\n", "\n", "if", "separate_heads", ":", "\n", "            ", "if", "num_sentence_labels", "==", "num_heads", ":", "\n", "                ", "sentence_scores", "=", "processed_tensor", "\n", "", "else", ":", "\n", "# Get the sentence representations corresponding to the default head.", "\n", "                ", "default_head", "=", "tf", ".", "gather", "(", "\n", "processed_tensor", ",", "\n", "indices", "=", "[", "0", "]", ",", "axis", "=", "-", "1", ")", "# [B, 1]", "\n", "\n", "# Get the sentence representations corresponding to the non-default head.", "\n", "non_default_heads", "=", "tf", ".", "gather", "(", "\n", "processed_tensor", ",", "\n", "indices", "=", "list", "(", "range", "(", "1", ",", "num_heads", ")", ")", ",", "axis", "=", "-", "1", ")", "# [B, num_heads-1]", "\n", "\n", "# Project onto one unit, corresponding to the default sentence label score.", "\n", "sentence_default_scores", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "default_head", ",", "units", "=", "1", ",", "\n", "activation", "=", "scoring_activation", ",", "kernel_initializer", "=", "initializer", ",", "\n", "name", "=", "\"sentence_default_scores_ff\"", ")", "# [B, 1]", "\n", "\n", "# Project onto (num_sentence_labels-1) units, corresponding to", "\n", "# the non-default sentence label scores.", "\n", "sentence_non_default_scores", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "non_default_heads", ",", "units", "=", "num_sentence_labels", "-", "1", ",", "\n", "activation", "=", "scoring_activation", ",", "kernel_initializer", "=", "initializer", ",", "\n", "name", "=", "\"sentence_non_default_scores_ff\"", ")", "# [B, num_sentence_labels-1]", "\n", "\n", "sentence_scores", "=", "tf", ".", "concat", "(", "\n", "[", "sentence_default_scores", ",", "sentence_non_default_scores", "]", ",", "\n", "axis", "=", "-", "1", ",", "name", "=", "\"sentence_scores_concatenation\"", ")", "# [B, num_sent_labels]", "\n", "", "", "else", ":", "\n", "            ", "sentence_scores", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", "=", "processed_tensor", ",", "units", "=", "num_sentence_labels", ",", "\n", "activation", "=", "scoring_activation", ",", "kernel_initializer", "=", "initializer", ",", "\n", "name", "=", "\"output_sent_specified_scores_ff\"", ")", "# [B, num_sent_labels]", "\n", "\n", "", "sentence_probabilities", "=", "tf", ".", "nn", ".", "softmax", "(", "sentence_scores", ")", "\n", "sentence_predictions", "=", "tf", ".", "argmax", "(", "sentence_probabilities", ",", "axis", "=", "1", ")", "# [B]", "\n", "\n", "# Obtain token scores from attention weights. Shape is [num_heads, B, M].", "\n", "if", "token_scoring_method", "==", "\"sum\"", ":", "\n", "            ", "token_scores", "=", "tf", ".", "reduce_sum", "(", "attention_weights_unnormalized", ",", "axis", "=", "2", ")", "\n", "", "elif", "token_scoring_method", "==", "\"max\"", ":", "\n", "            ", "token_scores", "=", "tf", ".", "reduce_max", "(", "attention_weights_unnormalized", ",", "axis", "=", "2", ")", "\n", "", "elif", "token_scoring_method", "==", "\"avg\"", ":", "\n", "            ", "token_scores", "=", "tf", ".", "reduce_mean", "(", "attention_weights_unnormalized", ",", "axis", "=", "2", ")", "\n", "", "elif", "token_scoring_method", "==", "\"logsumexp\"", ":", "\n", "            ", "token_scores", "=", "tf", ".", "reduce_logsumexp", "(", "attention_weights_unnormalized", ",", "axis", "=", "2", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unknown/unsupported token scoring method: %s\"", "\n", "%", "token_scoring_method", ")", "\n", "\n", "", "token_scores", "=", "tf", ".", "transpose", "(", "token_scores", ",", "[", "1", ",", "2", ",", "0", "]", ")", "# [B, M, num_heads]", "\n", "token_probabilities", "=", "tf", ".", "nn", ".", "softmax", "(", "token_scores", ")", "\n", "token_predictions", "=", "tf", ".", "argmax", "(", "\n", "token_probabilities", ",", "axis", "=", "2", ",", "output_type", "=", "tf", ".", "int32", ")", "# [B, M]", "\n", "\n", "attention_weights", "=", "tf", ".", "transpose", "(", "attention_weights", ",", "[", "1", ",", "2", ",", "3", ",", "0", "]", ")", "# [B, M, M, num_heads]", "\n", "\n", "return", "sentence_scores", ",", "sentence_predictions", ",", "token_scores", ",", "token_predictions", ",", "token_probabilities", ",", "sentence_probabilities", ",", "attention_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.get_token_representative_values": [[1956, 1976], ["tensorflow.reduce_max", "tensorflow.reduce_max", "tensorflow.reduce_logsumexp", "ValueError"], "function", ["None"], ["", "", "def", "get_token_representative_values", "(", "token_probabilities", ",", "approach", ")", ":", "\n", "    ", "\"\"\"\n    Obtains the token probabilities representative for each head across the sentence.\n    :param token_probabilities: the softmaxed token scores.\n    :param approach: how to get the representations (max, avg, log).\n    :return: token_representative_values of shape [batch_size, num_heads].\n    \"\"\"", "\n", "if", "\"max\"", "in", "approach", ":", "\n", "        ", "token_representative_values", "=", "tf", ".", "reduce_max", "(", "\n", "token_probabilities", ",", "axis", "=", "1", ")", "\n", "", "elif", "\"avg\"", "in", "approach", ":", "\n", "        ", "token_representative_values", "=", "tf", ".", "reduce_max", "(", "\n", "token_probabilities", ",", "axis", "=", "1", ")", "\n", "", "elif", "\"log\"", "in", "approach", ":", "\n", "        ", "token_representative_values", "=", "tf", ".", "reduce_logsumexp", "(", "\n", "token_probabilities", ",", "axis", "=", "1", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Unknown approach for getting \"", "\n", "\"token representative values: %s.\"", "%", "approach", ")", "\n", "", "return", "token_representative_values", "# [B, num_heads]", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.get_one_hot_of_token_labels_length": [[1978, 2008], ["tensorflow.one_hot", "tensorflow.cast", "tensorflow.gather", "tensorflow.gather", "tensorflow.tile", "tensorflow.concat"], "function", ["None"], ["", "def", "get_one_hot_of_token_labels_length", "(", "\n", "sentence_labels", ",", "num_sent_labels", ",", "num_tok_labels", ")", ":", "\n", "    ", "\"\"\"\n    Obtains one-hot sentence representations.\n    :param sentence_labels: ground truth sentence labels.\n    :param num_sent_labels: total number of unique sentence labels.\n    :param num_tok_labels: total number of unique token labels.\n    :return: one hot sentence labels, corresponding to the token labels.\n    \"\"\"", "\n", "one_hot_sentence_labels", "=", "tf", ".", "one_hot", "(", "\n", "tf", ".", "cast", "(", "sentence_labels", ",", "tf", ".", "int64", ")", ",", "\n", "depth", "=", "num_sent_labels", ")", "\n", "\n", "if", "num_sent_labels", "==", "2", "and", "num_sent_labels", "!=", "num_tok_labels", ":", "\n", "# Get the default and non-default sentence labels.", "\n", "        ", "default_sentence_labels", "=", "tf", ".", "gather", "(", "\n", "one_hot_sentence_labels", ",", "indices", "=", "[", "0", "]", ",", "axis", "=", "-", "1", ")", "# [B x 1]", "\n", "non_default_sentence_labels", "=", "tf", ".", "gather", "(", "\n", "one_hot_sentence_labels", ",", "indices", "=", "[", "1", "]", ",", "axis", "=", "-", "1", ")", "# [B x 1]", "\n", "\n", "# Tile the non-default one (num_tok_labels - 1) times.", "\n", "tiled_non_default_sentence_labels", "=", "tf", ".", "tile", "(", "\n", "input", "=", "non_default_sentence_labels", ",", "\n", "multiples", "=", "[", "1", ",", "num_tok_labels", "-", "1", "]", ")", "\n", "\n", "# Get one-hot sentence labels of shape [B, num_tok_labels].", "\n", "one_hot_sentence_labels", "=", "tf", ".", "concat", "(", "\n", "[", "default_sentence_labels", ",", "tiled_non_default_sentence_labels", "]", ",", "\n", "axis", "=", "-", "1", ",", "name", "=", "\"one_hot_sentence_labels_concatenation\"", ")", "\n", "", "return", "one_hot_sentence_labels", "# [B, num_tok_labels]", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.compute_attention_loss": [[2010, 2053], ["modules.get_token_representative_values", "modules.get_one_hot_of_token_labels_length", "ValueError", "tensorflow.losses.mean_pairwise_squared_error", "tensorflow.square", "modules.label_smoothing", "modules.label_smoothing"], "function", ["home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.get_token_representative_values", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.get_one_hot_of_token_labels_length", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.label_smoothing", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.label_smoothing"], ["", "def", "compute_attention_loss", "(", "\n", "token_probabilities", ",", "sentence_labels", ",", "\n", "num_sent_labels", ",", "num_tok_labels", ",", "\n", "approach", ",", "compute_pairwise", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Attention-level loss -- currently, implementation possible only in two cases:\n      1. The number of sentence labels is equal to the number of token labels.\n         In this case, the attention loss is computed element-wise (for each label).\n      2. The number of sentence labels is 2, while the number of tokens is arbitrary.\n         In this case, two scores are computed from the token scores:\n              * one corresponding to the default label\n              * one corresponding to the rest of labels (non-default labels)\n    :param token_probabilities: 3D tensor, shape [B, M, num_tok_labels]\n                                that are normalized across heads (last axis).\n    :param sentence_labels: 2D tensor, shape [B, num_labels_tok]\n    :param num_sent_labels: number of unique sentence labels.\n    :param num_tok_labels: number of unique token labels.\n    :param approach: method to extract token representation values.\n    :param compute_pairwise: whether to compute the loss pairwise or not.\n    :return: a number representing the sum over attention losses computed.\n    \"\"\"", "\n", "if", "num_sent_labels", "==", "num_tok_labels", "or", "num_sent_labels", "==", "2", ":", "\n", "# Compute the token representations based on the approach selected.", "\n", "        ", "token_representative_values", "=", "get_token_representative_values", "(", "\n", "token_probabilities", ",", "approach", ")", "# [B, num_heads]", "\n", "\n", "one_hot_sentence_labels", "=", "get_one_hot_of_token_labels_length", "(", "\n", "sentence_labels", ",", "num_sent_labels", ",", "num_tok_labels", ")", "\n", "if", "compute_pairwise", ":", "\n", "            ", "attention_loss", "=", "tf", ".", "losses", ".", "mean_pairwise_squared_error", "(", "\n", "labels", "=", "label_smoothing", "(", "one_hot_sentence_labels", ",", "epsilon", "=", "0.15", ")", ",", "\n", "predictions", "=", "token_representative_values", ",", "weights", "=", "1.15", ")", "\n", "", "else", ":", "\n", "            ", "attention_loss", "=", "tf", ".", "square", "(", "\n", "token_representative_values", "-", "\n", "label_smoothing", "(", "one_hot_sentence_labels", ",", "epsilon", "=", "0.15", ")", ")", "\n", "", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"You have different number of token labels (%d) and \"", "\n", "\"sentence labels (%d, which is non-binary). \"", "\n", "\"We don't support attention loss for such a case!\"", "\n", "%", "(", "num_tok_labels", ",", "num_sent_labels", ")", ")", "\n", "", "return", "attention_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.compute_gap_distance_loss": [[2055, 2161], ["modules.get_token_representative_values", "modules.get_one_hot_of_token_labels_length", "tensorflow.multiply", "tensorflow.squeeze", "tensorflow.squeeze", "tensorflow.squeeze", "tensorflow.squeeze", "tensorflow.stack", "tensorflow.stack", "tensorflow.where", "ValueError", "tensorflow.cast", "tensorflow.gather", "tensorflow.gather", "tensorflow.reduce_max", "tensorflow.reduce_max", "tensorflow.equal", "tensorflow.math.maximum", "tensorflow.gather", "tensorflow.gather", "tensorflow.cast", "tensorflow.ones_like", "tensorflow.math.subtract", "tensorflow.reduce_sum", "tensorflow.add", "tensorflow.subtract", "tensorflow.math.abs", "tensorflow.square", "tensorflow.multiply", "tensorflow.multiply", "tensorflow.subtract", "tensorflow.maximum", "tensorflow.add", "tensorflow.ones_like", "tensorflow.math.exp", "tensorflow.add", "range", "range", "tensorflow.math.maximum", "tensorflow.math.minimum", "tensorflow.math.maximum", "tensorflow.math.abs"], "function", ["home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.get_token_representative_values", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.get_one_hot_of_token_labels_length"], ["", "def", "compute_gap_distance_loss", "(", "\n", "token_probabilities", ",", "sentence_labels", ",", "\n", "num_sent_labels", ",", "num_tok_labels", ",", "\n", "minimum_gap_distance", ",", "approach", ",", "\n", "type_distance", ")", ":", "\n", "    ", "\"\"\"\n    Gap-distance loss: the intuition is that the gap between the default\n    and non-default scores should be wider than a certain threshold.\n    :param token_probabilities: 3D tensor, shape [B, M, num_tok_labels]\n                                that are normalized across heads (last axis).\n    :param sentence_labels: 2D tensor, shape [B, num_labels_tok]\n    :param num_sent_labels: number of unique sentence labels.\n    :param num_tok_labels: number of unique token labels.\n    :param minimum_gap_distance: the minimum distance gap imposed between\n    scores corresponding tot he default or non-default gold sentence label.\n    :param approach: method to extract token representation values.\n    :param type_distance: type of gap distance loss that you want.\n    :return: a number representing the sum over gap-distance losses.\n    \"\"\"", "\n", "if", "num_sent_labels", "==", "num_tok_labels", "or", "num_sent_labels", "==", "2", ":", "\n", "# Compute the token representations based on the approach selected.", "\n", "        ", "token_representative_values", "=", "get_token_representative_values", "(", "\n", "token_probabilities", ",", "approach", ")", "# [B, num_heads]", "\n", "\n", "one_hot_sentence_labels", "=", "get_one_hot_of_token_labels_length", "(", "\n", "sentence_labels", ",", "num_sent_labels", ",", "num_tok_labels", ")", "\n", "valid_tokens", "=", "tf", ".", "multiply", "(", "\n", "tf", ".", "cast", "(", "one_hot_sentence_labels", ",", "tf", ".", "float32", ")", ",", "\n", "token_representative_values", ")", "# [B, num_tok_labels]", "\n", "\n", "tokens_default_head_correct", "=", "tf", ".", "squeeze", "(", "tf", ".", "gather", "(", "\n", "valid_tokens", ",", "indices", "=", "[", "0", "]", ",", "axis", "=", "-", "1", ")", ",", "axis", "=", "-", "1", ")", "# [B]", "\n", "tokens_default_head_incorrect", "=", "tf", ".", "squeeze", "(", "tf", ".", "gather", "(", "\n", "token_representative_values", ",", "indices", "=", "[", "0", "]", ",", "axis", "=", "-", "1", ")", ",", "axis", "=", "-", "1", ")", "# [B]", "\n", "\n", "tokens_non_default_head_correct", "=", "tf", ".", "squeeze", "(", "\n", "tf", ".", "reduce_max", "(", "tf", ".", "gather", "(", "\n", "valid_tokens", ",", "\n", "indices", "=", "[", "[", "i", "]", "for", "i", "in", "range", "(", "1", ",", "num_tok_labels", ")", "]", ",", "\n", "axis", "=", "-", "1", ")", ",", "axis", "=", "1", ")", ",", "axis", "=", "-", "1", ")", "\n", "tokens_non_default_head_incorrect", "=", "tf", ".", "squeeze", "(", "\n", "tf", ".", "reduce_max", "(", "tf", ".", "gather", "(", "\n", "token_representative_values", ",", "\n", "indices", "=", "[", "[", "i", "]", "for", "i", "in", "range", "(", "1", ",", "num_tok_labels", ")", "]", ",", "\n", "axis", "=", "-", "1", ")", ",", "axis", "=", "1", ")", ",", "axis", "=", "-", "1", ")", "\n", "\n", "heads_correct", "=", "tf", ".", "stack", "(", "\n", "[", "tokens_default_head_correct", ",", "tokens_non_default_head_correct", "]", ",", "\n", "axis", "=", "-", "1", ")", "# [B, 2]", "\n", "heads_incorrect", "=", "tf", ".", "stack", "(", "\n", "[", "tokens_default_head_incorrect", ",", "tokens_non_default_head_incorrect", "]", ",", "\n", "axis", "=", "-", "1", ")", "# [B, 2]", "\n", "y_heads", "=", "tf", ".", "where", "(", "\n", "tf", ".", "equal", "(", "tf", ".", "cast", "(", "tokens_non_default_head_correct", ",", "tf", ".", "int32", ")", ",", "0", ")", ",", "\n", "one_hot_sentence_labels", ",", "\n", "tf", ".", "ones_like", "(", "one_hot_sentence_labels", ")", "-", "one_hot_sentence_labels", ")", "\n", "\n", "\"\"\"\n        heads_correct = tf.where(\n            tf.equal(tf.cast(tokens_non_default_head, tf.int32), 0),\n            tokens_default_head,\n            tokens_non_default_head)\n\n        heads_incorrect = tf.where(\n            tf.equal(tf.cast(tokens_default_head, tf.int32), 0),\n            tokens_default_head,\n            tokens_non_default_head)\n        \"\"\"", "\n", "\n", "if", "type_distance", "==", "\"distance_only\"", ":", "\n", "# loss = max(0.0, threshold - |correct - incorrect|).", "\n", "            ", "gap_loss", "=", "tf", ".", "math", ".", "maximum", "(", "\n", "0.0", ",", "\n", "tf", ".", "math", ".", "subtract", "(", "\n", "minimum_gap_distance", ",", "\n", "tf", ".", "math", ".", "abs", "(", "tf", ".", "subtract", "(", "\n", "tokens_default_head_incorrect", ",", "\n", "tokens_non_default_head_incorrect", ")", ")", ")", ")", "\n", "", "elif", "type_distance", "==", "\"contrastive\"", ":", "\n", "            ", "squared_euclidean_distance", "=", "tf", ".", "reduce_sum", "(", "\n", "tf", ".", "square", "(", "heads_correct", "-", "heads_incorrect", ")", ")", "\n", "# loss = y * dist + (1 - y) * max(0.0, threshold - d).", "\n", "gap_loss", "=", "tf", ".", "add", "(", "\n", "tf", ".", "multiply", "(", "tf", ".", "ones_like", "(", "y_heads", ")", "-", "y_heads", ",", "\n", "squared_euclidean_distance", ")", ",", "\n", "tf", ".", "multiply", "(", "y_heads", ",", "\n", "tf", ".", "maximum", "(", "0.0", ",", "\n", "minimum_gap_distance", "-", "squared_euclidean_distance", ")", ")", ")", "\n", "", "else", ":", "\n", "# loss =", "\n", "# [exp(max(0.0, threshold - |correct - incorrect|))", "\n", "#   * (1.0 + max(correct, incorrect) - x_correct)", "\n", "#   * (1.0 + incorrect - min(correct, incorrect))] - 1.0", "\n", "            ", "gap_loss", "=", "tf", ".", "subtract", "(", "\n", "tf", ".", "math", ".", "exp", "(", "tf", ".", "math", ".", "maximum", "(", "\n", "0.0", ",", "minimum_gap_distance", "-", "tf", ".", "math", ".", "abs", "(", "heads_correct", "-", "heads_incorrect", ")", ")", ")", "\n", "*", "tf", ".", "add", "(", "1.0", ",", "tf", ".", "math", ".", "maximum", "(", "heads_correct", ",", "heads_incorrect", ")", "-", "heads_correct", ")", "\n", "*", "tf", ".", "add", "(", "1.0", ",", "heads_incorrect", "-", "tf", ".", "math", ".", "minimum", "(", "heads_correct", ",", "heads_incorrect", ")", ")", ",", "\n", "1.0", ")", "\n", "", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"You have different number of token labels (%d) and \"", "\n", "\"sentence labels (%d, which is non-binary). \"", "\n", "\"We don't support attention loss for such a case!\"", "\n", "%", "(", "num_tok_labels", ",", "num_sent_labels", ")", ")", "\n", "", "return", "gap_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.disable_tokens.add_another_column": [[5, 26], ["open", "open", "line_tok_orig.strip", "line_tok.split.split", "write_file.write", "len", "write_file.write"], "function", ["home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Writer.write", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Writer.write"], ["def", "add_another_column", "(", "dataset", ",", "extension", ")", ":", "\n", "    ", "\"\"\"\n    The original dataset file has multiple columns, the first one being\n    the token and the last one the label. This method builds another file\n    containing these as well as an additional middle column, representing\n    the supervision status of a certain token, which can be \"on\" or \"off\".\n    To start, all tokens are disabled. Later, we gradually increase the\n    proportion of token-annotated sentences for which supervision is on.\n    \"\"\"", "\n", "path2", "=", "dataset", "+", "\".column_added\"", "+", "extension", "\n", "num_sent", "=", "0", "\n", "with", "open", "(", "dataset", "+", "extension", ")", "as", "read_file", ",", "open", "(", "path2", ",", "'w'", ")", "as", "write_file", ":", "\n", "        ", "for", "line_tok_orig", "in", "read_file", ":", "\n", "            ", "line_tok", "=", "line_tok_orig", ".", "strip", "(", ")", "\n", "if", "len", "(", "line_tok", ")", "==", "0", ":", "\n", "                ", "num_sent", "+=", "1", "\n", "write_file", ".", "write", "(", "line_tok_orig", ")", "\n", "continue", "\n", "", "line_tok", "=", "line_tok", ".", "split", "(", ")", "\n", "write_file", ".", "write", "(", "line_tok", "[", "0", "]", "+", "\"\\t\"", "+", "\"off\"", "+", "\"\\t\"", "+", "line_tok", "[", "-", "1", "]", "+", "\"\\n\"", ")", "\n", "", "", "return", "path2", ",", "num_sent", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.disable_tokens.convert_labels": [[28, 75], ["open", "read_file.read().split", "len", "lines[].strip", "lines[].strip.split", "random.random", "open", "write_file.write", "len", "len", "read_file.read", "len", "lines[].strip", "len", "lines[].strip.split", "lines[].strip"], "function", ["home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Writer.write"], ["", "def", "convert_labels", "(", "read_dataset", ",", "write_dataset", ",", "percent", ",", "no_sentences_to_enable", ")", ":", "\n", "    ", "\"\"\"\n    Takes a file and randomly enables no_sentences_to_enable of them.\n    :param read_dataset: file to read from\n    :param write_dataset: file to write to\n    :param percent: percent to enable (used to calculate the probability of enabling)\n    :param no_sentences_to_enable: how many sentences should be enabled\n    :return: sentences enabled\n    \"\"\"", "\n", "sentences_enabled", "=", "0", "\n", "write_file_str", "=", "\"\"", "\n", "with", "open", "(", "read_dataset", ")", "as", "read_file", ":", "\n", "        ", "lines", "=", "read_file", ".", "read", "(", ")", ".", "split", "(", "\"\\n\"", ")", "\n", "\n", "", "line_index", "=", "0", "\n", "while", "line_index", "<", "len", "(", "lines", ")", ":", "\n", "        ", "line", "=", "lines", "[", "line_index", "]", ".", "strip", "(", ")", "\n", "if", "len", "(", "line", ")", "==", "0", ":", "\n", "            ", "write_file_str", "+=", "\"\\n\"", "\n", "line_index", "+=", "1", "\n", "continue", "\n", "\n", "", "line_tok", "=", "line", ".", "split", "(", ")", "\n", "assert", "len", "(", "line_tok", ")", ">", "2", ",", "\"Line tok shouldn't be empty!\"", "\n", "prob", "=", "random", ".", "random", "(", ")", "\n", "conf", "=", "(", "percent", "+", "3", ")", "/", "100.0", "\n", "\n", "if", "conf", "<", "1", "and", "(", "prob", ">", "conf", "or", "line_tok", "[", "1", "]", "==", "\"on\"", "\n", "or", "sentences_enabled", ">=", "no_sentences_to_enable", ")", ":", "\n", "            ", "while", "len", "(", "line", ")", "!=", "0", ":", "\n", "                ", "write_file_str", "+=", "line", "+", "\"\\n\"", "\n", "line_index", "+=", "1", "\n", "line", "=", "lines", "[", "line_index", "]", ".", "strip", "(", ")", "\n", "", "", "else", ":", "\n", "            ", "sentences_enabled", "+=", "1", "\n", "while", "len", "(", "line", ")", "!=", "0", ":", "\n", "                ", "line_tok", "=", "line", ".", "split", "(", ")", "\n", "write_file_str", "+=", "(", "line_tok", "[", "0", "]", "+", "\"\\t\"", "+", "\"on\"", "+", "\"\\t\"", "\n", "+", "line_tok", "[", "-", "1", "]", "+", "\"\\t\"", "+", "\"\\n\"", ")", "\n", "line_index", "+=", "1", "\n", "line", "=", "lines", "[", "line_index", "]", ".", "strip", "(", ")", "\n", "", "", "write_file_str", "+=", "\"\\n\"", "\n", "line_index", "+=", "1", "\n", "\n", "", "with", "open", "(", "write_dataset", ",", "'w'", ")", "as", "write_file", ":", "\n", "        ", "write_file", ".", "write", "(", "write_file_str", ")", "\n", "", "return", "sentences_enabled", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Token.__init__": [[34, 44], ["experiment.Token.unique_labels_tok.add", "experiment.Token.labels_tok_dict.keys"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "value", ",", "label", ",", "enable_supervision", ")", ":", "\n", "        ", "self", ".", "value", "=", "value", "\n", "self", ".", "label_tok", "=", "label", "\n", "self", ".", "enable_supervision", "=", "True", "\n", "if", "\"off\"", "in", "enable_supervision", ":", "\n", "            ", "self", ".", "enable_supervision", "=", "False", "\n", "", "self", ".", "unique_labels_tok", ".", "add", "(", "label", ")", "\n", "if", "label", "not", "in", "self", ".", "labels_tok_dict", ".", "keys", "(", ")", ":", "\n", "            ", "self", ".", "labels_tok_dict", "[", "label", "]", "=", "0", "\n", "", "self", ".", "labels_tok_dict", "[", "label", "]", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Sentence.__init__": [[57, 60], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "tokens", "=", "[", "]", "\n", "self", ".", "label_sent", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Sentence.add_token": [[61, 79], ["experiment.Sentence.set_label", "experiment.Token", "experiment.Sentence.tokens.append"], "methods", ["home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Sentence.set_label"], ["", "def", "add_token", "(", "self", ",", "value", ",", "label", ",", "enable_supervision", ",", "\n", "sentence_label_type", ",", "default_label", ")", ":", "\n", "        ", "\"\"\"\n        Adds a token with the specified value, label and state to the list of tokens.\n        If the token value is \"sent_label\" then, instead of adding a token, it sets\n        the sentence label (needing a sentence_label_type and a default_label).\n        :param value: str, the token value (i.e. what's the actual word, precisely)\n        :param label: str, the label of the current token\n        :param enable_supervision: str, whether to allow supervision or not\n        :param sentence_label_type: str, type of sentence label assignment to expect\n        (binary, majority, specified). Should be set by \"sentence_label\" in config.\n        :param default_label: str, the default label, set by \"default_label\" in config.\n        \"\"\"", "\n", "if", "value", "==", "\"sent_label\"", ":", "\n", "            ", "self", ".", "set_label", "(", "sentence_label_type", ",", "default_label", ",", "label", ")", "\n", "", "else", ":", "\n", "            ", "token", "=", "Token", "(", "value", ",", "label", ",", "enable_supervision", ")", "\n", "self", ".", "tokens", ".", "append", "(", "token", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Sentence.set_label": [[80, 112], ["experiment.Sentence.unique_labels_sent.add", "experiment.Sentence.labels_sent_dict.keys", "ValueError", "sum", "collections.Counter().most_common", "collections.Counter"], "methods", ["None"], ["", "", "def", "set_label", "(", "self", ",", "sentence_label_type", ",", "default_label", ",", "label", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Sets the label of the sentence, according to \"sentence_label_type\"\n        specified in config, which can be \"specified\", \"majority\", or \"binary\".\n        The \"default_label\" is also needed to infer the binary labels.\n        :param sentence_label_type: str\n        :param default_label: str\n        :param label: str\n        \"\"\"", "\n", "if", "sentence_label_type", "==", "\"specified\"", ":", "\n", "            ", "assert", "label", "is", "not", "None", "or", "self", ".", "label_sent", "is", "not", "None", ",", "\"Sentence label missing!\"", "\n", "if", "label", "is", "not", "None", ":", "\n", "                ", "self", ".", "label_sent", "=", "label", "\n", "", "", "elif", "label", "is", "None", "and", "sentence_label_type", "==", "\"majority\"", ":", "\n", "            ", "majority_label", "=", "Counter", "(", "\n", "[", "token", ".", "label_tok", "for", "token", "in", "self", ".", "tokens", "]", ")", ".", "most_common", "(", ")", "[", "0", "]", "[", "0", "]", "\n", "if", "majority_label", "is", "not", "None", ":", "\n", "                ", "self", ".", "label_sent", "=", "majority_label", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"Majority label is None! Sentence tokens: \"", ",", "self", ".", "tokens", ")", "\n", "", "", "elif", "label", "is", "None", "and", "sentence_label_type", "==", "\"binary\"", ":", "\n", "            ", "non_default_token_labels", "=", "sum", "(", "\n", "[", "0", "if", "token", ".", "label_tok", "==", "default_label", "else", "1", "for", "token", "in", "self", ".", "tokens", "]", ")", "\n", "if", "non_default_token_labels", ">", "0", ":", "\n", "                ", "self", ".", "label_sent", "=", "\"1\"", "# non-default_label", "\n", "", "else", ":", "\n", "                ", "self", ".", "label_sent", "=", "\"0\"", "# default_label", "\n", "", "", "if", "self", ".", "label_sent", "is", "not", "None", ":", "\n", "            ", "self", ".", "unique_labels_sent", ".", "add", "(", "self", ".", "label_sent", ")", "\n", "if", "self", ".", "label_sent", "not", "in", "self", ".", "labels_sent_dict", ".", "keys", "(", ")", ":", "\n", "                ", "self", ".", "labels_sent_dict", "[", "self", ".", "label_sent", "]", "=", "0", "\n", "", "self", ".", "labels_sent_dict", "[", "self", ".", "label_sent", "]", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Sentence.print_sentence": [[113, 129], ["print", "to_print.append", "len"], "methods", ["None"], ["", "", "def", "print_sentence", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Prints a sentence in this format: \"sent_label: tok_i(label_i, is_supervision_enabled_i)\".\n        :rtype: int, representing the number of tokens enabled in this sentence\n        \"\"\"", "\n", "to_print", "=", "[", "]", "\n", "num_tokens_enabled", "=", "0", "\n", "for", "token", "in", "self", ".", "tokens", ":", "\n", "            ", "to_print", ".", "append", "(", "\"%s (%s, %s)\"", "%", "(", "token", ".", "value", ",", "token", ".", "label_tok", ",", "token", ".", "enable_supervision", ")", ")", "\n", "if", "token", ".", "enable_supervision", ":", "\n", "                ", "num_tokens_enabled", "+=", "1", "\n", "", "", "print", "(", "\"sent %s: %s\\n\"", "%", "(", "self", ".", "label_sent", ",", "\" \"", ".", "join", "(", "to_print", ")", ")", ")", "\n", "if", "self", ".", "tokens", "[", "0", "]", ".", "enable_supervision", ":", "\n", "            ", "assert", "num_tokens_enabled", "==", "len", "(", "self", ".", "tokens", ")", ",", "\"Number of tokens enabled does not equal the number of tokens in the sentence!\"", "\n", "", "return", "num_tokens_enabled", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Experiment.__init__": [[136, 140], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "config", "=", "None", "\n", "self", ".", "label2id_sent", "=", "None", "\n", "self", ".", "label2id_tok", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Experiment.read_input_files": [[141, 195], ["experiment.Sentence", "file_paths.strip().split", "file_paths.strip", "open", "line.strip.strip.strip", "len", "experiment.Sentence", "len", "line.strip.strip.split", "len", "experiment.Sentence.add_token", "experiment.Sentence.set_label", "sentences.append", "len", "experiment.Sentence", "len", "len", "len", "len", "len", "experiment.Sentence.set_label", "sentences.append", "len", "len"], "methods", ["home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Sentence.add_token", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Sentence.set_label", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Sentence.set_label"], ["", "def", "read_input_files", "(", "self", ",", "file_paths", ",", "max_sentence_length", "=", "-", "1", ")", ":", "\n", "        ", "\"\"\"\n        Reads input files in whitespace-separated format.\n        Splits file_paths on comma, reading from multiple files.\n        Expects one token per line: first column = value, last column = label.\n        If the sentence label is already specified in the input file, it needs to have:\n        first column = \"sent_label\" and config[\"sentence_label\"] = specified.\n        If the sentence label is not specified, it will be inferred from the data\n        depending on the value of config[\"sentence_label\"]. Can be set to majority or binary.\n        :type file_paths: str\n        :type max_sentence_length: int\n        :rtype: list of Sentence objects\n        \"\"\"", "\n", "sentences", "=", "[", "]", "\n", "line_length", "=", "None", "\n", "sentence", "=", "Sentence", "(", ")", "\n", "\n", "for", "file_path", "in", "file_paths", ".", "strip", "(", ")", ".", "split", "(", "\",\"", ")", ":", "\n", "            ", "with", "open", "(", "file_path", ")", "as", "f", ":", "\n", "                ", "for", "line", "in", "f", ":", "\n", "                    ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "len", "(", "line", ")", ">", "0", ":", "\n", "                        ", "line_parts", "=", "line", ".", "split", "(", ")", "\n", "assert", "len", "(", "line_parts", ")", ">=", "2", ",", "\"Line parts less than 2: %s\\n\"", "%", "line", "\n", "assert", "len", "(", "line_parts", ")", "==", "line_length", "or", "line_length", "is", "None", ",", "\"Inconsistent line parts: expected %d, but got %d for line %s.\"", "%", "(", "\n", "len", "(", "line_parts", ")", ",", "line_length", ",", "line", ")", "\n", "line_length", "=", "len", "(", "line_parts", ")", "\n", "\n", "# The first element on the line is the token value, while the last is the token label.", "\n", "# If there is a penultimate column whose value is either \"on\" or \"off\", it indicates", "\n", "# whether supervision on this token is enabled or not. If there is no such element,", "\n", "# we implicitly assume that supervision is possible and turn it on.", "\n", "sentence", ".", "add_token", "(", "\n", "value", "=", "line_parts", "[", "0", "]", ",", "label", "=", "line_parts", "[", "-", "1", "]", ",", "\n", "enable_supervision", "=", "line_parts", "[", "-", "2", "]", "if", "len", "(", "line_parts", ")", ">", "2", "else", "\"on\"", ",", "\n", "sentence_label_type", "=", "self", ".", "config", "[", "\"sentence_label\"", "]", ",", "\n", "default_label", "=", "self", ".", "config", "[", "\"default_label\"", "]", ")", "\n", "", "elif", "len", "(", "line", ")", "==", "0", "and", "len", "(", "sentence", ".", "tokens", ")", ">", "0", ":", "\n", "                        ", "if", "max_sentence_length", "<=", "0", "or", "len", "(", "sentence", ".", "tokens", ")", "<=", "max_sentence_length", ":", "\n", "                            ", "sentence", ".", "set_label", "(", "\n", "sentence_label_type", "=", "self", ".", "config", "[", "\"sentence_label\"", "]", ",", "\n", "default_label", "=", "self", ".", "config", "[", "\"default_label\"", "]", ")", "\n", "sentences", ".", "append", "(", "sentence", ")", "\n", "", "sentence", "=", "Sentence", "(", ")", "\n", "", "", "if", "len", "(", "sentence", ".", "tokens", ")", ">", "0", ":", "\n", "                    ", "if", "max_sentence_length", "<=", "0", "or", "len", "(", "sentence", ".", "tokens", ")", "<=", "max_sentence_length", ":", "\n", "                        ", "sentence", ".", "set_label", "(", "\n", "sentence_label_type", "=", "self", ".", "config", "[", "\"sentence_label\"", "]", ",", "\n", "default_label", "=", "self", ".", "config", "[", "\"default_label\"", "]", ")", "\n", "sentences", ".", "append", "(", "sentence", ")", "\n", "", "sentence", "=", "Sentence", "(", ")", "\n", "", "", "", "return", "sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Experiment.create_labels_mapping": [[196, 212], ["sorted", "sorted", "list", "list", "unique_labels.difference", "enumerate", "enumerate"], "methods", ["None"], ["", "def", "create_labels_mapping", "(", "self", ",", "unique_labels", ")", ":", "\n", "        ", "\"\"\"\n        Maps a list of U unique labels to an index in [0, U).\n        The default label (if it exists) will receive index 0.\n        All other labels get the index corresponding to their natural order.\n        :type unique_labels: set\n        :rtype: dict\n        \"\"\"", "\n", "if", "self", ".", "config", "[", "\"default_label\"", "]", "and", "self", ".", "config", "[", "\"default_label\"", "]", "in", "unique_labels", ":", "\n", "            ", "sorted_labels", "=", "sorted", "(", "list", "(", "unique_labels", ".", "difference", "(", "self", ".", "config", "[", "\"default_label\"", "]", ")", ")", ")", "\n", "label2id", "=", "{", "label", ":", "index", "+", "1", "for", "index", ",", "label", "in", "enumerate", "(", "sorted_labels", ")", "}", "\n", "label2id", "[", "self", ".", "config", "[", "\"default_label\"", "]", "]", "=", "0", "\n", "", "else", ":", "\n", "            ", "sorted_labels", "=", "sorted", "(", "list", "(", "unique_labels", ")", ")", "\n", "label2id", "=", "{", "label", ":", "index", "for", "index", ",", "label", "in", "enumerate", "(", "sorted_labels", ")", "}", "\n", "", "return", "label2id", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Experiment.convert_labels": [[213, 231], ["print", "print", "print"], "methods", ["None"], ["", "def", "convert_labels", "(", "self", ",", "data", ")", ":", "\n", "        ", "\"\"\"\n        Converts each sentence and token label to its corresponding index.\n        :type data: list[Sentence]\n        :rtype: list[Sentence]\n        \"\"\"", "\n", "for", "sentence", "in", "data", ":", "\n", "            ", "current_label_sent", "=", "sentence", ".", "label_sent", "\n", "try", ":", "\n", "                ", "sentence", ".", "label_sent", "=", "self", ".", "label2id_sent", "[", "current_label_sent", "]", "\n", "", "except", "KeyError", ":", "\n", "                ", "print", "(", "\"Key error for \"", ",", "current_label_sent", ")", "\n", "print", "(", "\"Sentence: \"", ",", "[", "token", ".", "value", "for", "token", "in", "sentence", ".", "tokens", "]", ")", "\n", "print", "(", "\"Label to id\"", ",", "self", ".", "label2id_sent", ")", "\n", "", "for", "token", "in", "sentence", ".", "tokens", ":", "\n", "                ", "current_label_tok", "=", "token", ".", "label_tok", "\n", "token", ".", "label_tok", "=", "self", ".", "label2id_tok", "[", "current_label_tok", "]", "\n", "", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Experiment.parse_config": [[232, 254], ["configparser.ConfigParser", "configparser.ConfigParser.read", "collections.OrderedDict", "configparser.ConfigParser.items", "len", "value.lower", "configparser.ConfigParser.getboolean", "value.isdigit", "value.strip", "configparser.ConfigParser.getint", "experiment.Experiment.is_float", "configparser.ConfigParser.getfloat", "configparser.ConfigParser.get"], "methods", ["home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Experiment.is_float"], ["", "def", "parse_config", "(", "self", ",", "config_section", ",", "config_path", ")", ":", "\n", "        ", "\"\"\"\n        Reads the configuration file, guessing the correct data type for each value.\n        :type config_section: str\n        :type config_path: str\n        :rtype: dict\n        \"\"\"", "\n", "config_parser", "=", "configparser", ".", "ConfigParser", "(", "allow_no_value", "=", "True", ")", "\n", "config_parser", ".", "read", "(", "config_path", ")", "\n", "config", "=", "OrderedDict", "(", ")", "\n", "for", "key", ",", "value", "in", "config_parser", ".", "items", "(", "config_section", ")", ":", "\n", "            ", "if", "value", "is", "None", "or", "len", "(", "value", ".", "strip", "(", ")", ")", "==", "0", ":", "\n", "                ", "config", "[", "key", "]", "=", "None", "\n", "", "elif", "value", ".", "lower", "(", ")", "in", "[", "\"true\"", ",", "\"false\"", "]", ":", "\n", "                ", "config", "[", "key", "]", "=", "config_parser", ".", "getboolean", "(", "config_section", ",", "key", ")", "\n", "", "elif", "value", ".", "isdigit", "(", ")", ":", "\n", "                ", "config", "[", "key", "]", "=", "config_parser", ".", "getint", "(", "config_section", ",", "key", ")", "\n", "", "elif", "self", ".", "is_float", "(", "value", ")", ":", "\n", "                ", "config", "[", "key", "]", "=", "config_parser", ".", "getfloat", "(", "config_section", ",", "key", ")", "\n", "", "else", ":", "\n", "                ", "config", "[", "key", "]", "=", "config_parser", ".", "get", "(", "config_section", ",", "key", ")", "\n", "", "", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Experiment.is_float": [[255, 267], ["float"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "is_float", "(", "value", ")", ":", "\n", "        ", "\"\"\"\n        Checks if value is of type float.\n        :type value: any type\n        :rtype: bool\n        \"\"\"", "\n", "try", ":", "\n", "            ", "float", "(", "value", ")", "\n", "return", "True", "\n", "", "except", "ValueError", ":", "\n", "            ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Experiment.create_batches_of_sentence_ids": [[268, 314], ["collections.OrderedDict", "range", "range", "len", "len", "sentence_ids_by_length[].append", "range", "len", "current_batch.append", "len", "batches_of_sentence_ids.append", "int", "len", "batches_of_sentence_ids.append", "len", "len", "batches_of_sentence_ids.append", "len", "len"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "create_batches_of_sentence_ids", "(", "sentences", ",", "batch_equal_size", ",", "max_batch_size", ")", ":", "\n", "        ", "\"\"\"\n        Creates batches of sentence ids. A positive max_batch_size determines\n        the maximum number of sentences in each batch. A negative max_batch_size\n        dynamically creates the batches such that each batch contains\n        abs(max_batch_size) words. Returns a list of lists with sentences ids.\n        :type sentences: List[Sentence]\n        :type batch_equal_size: bool\n        :type max_batch_size: int\n        :rtype: List[List[int]]\n        \"\"\"", "\n", "batches_of_sentence_ids", "=", "[", "]", "\n", "if", "batch_equal_size", ":", "\n", "            ", "sentence_ids_by_length", "=", "OrderedDict", "(", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "sentences", ")", ")", ":", "\n", "                ", "length", "=", "len", "(", "sentences", "[", "i", "]", ".", "tokens", ")", "\n", "if", "length", "not", "in", "sentence_ids_by_length", ":", "\n", "                    ", "sentence_ids_by_length", "[", "length", "]", "=", "[", "]", "\n", "", "sentence_ids_by_length", "[", "length", "]", ".", "append", "(", "i", ")", "\n", "\n", "", "for", "sentence_length", "in", "sentence_ids_by_length", ":", "\n", "                ", "if", "max_batch_size", ">", "0", ":", "\n", "                    ", "batch_size", "=", "max_batch_size", "\n", "", "else", ":", "\n", "                    ", "batch_size", "=", "int", "(", "(", "-", "1", "*", "max_batch_size", ")", "/", "sentence_length", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "0", ",", "len", "(", "sentence_ids_by_length", "[", "sentence_length", "]", ")", ",", "batch_size", ")", ":", "\n", "                    ", "batches_of_sentence_ids", ".", "append", "(", "\n", "sentence_ids_by_length", "[", "sentence_length", "]", "[", "i", ":", "i", "+", "batch_size", "]", ")", "\n", "", "", "", "else", ":", "\n", "            ", "current_batch", "=", "[", "]", "\n", "max_sentence_length", "=", "0", "\n", "for", "i", "in", "range", "(", "len", "(", "sentences", ")", ")", ":", "\n", "                ", "current_batch", ".", "append", "(", "i", ")", "\n", "if", "len", "(", "sentences", "[", "i", "]", ".", "tokens", ")", ">", "max_sentence_length", ":", "\n", "                    ", "max_sentence_length", "=", "len", "(", "sentences", "[", "i", "]", ".", "tokens", ")", "\n", "", "if", "(", "(", "0", "<", "max_batch_size", "<=", "len", "(", "current_batch", ")", ")", "\n", "or", "(", "max_batch_size", "<=", "0", "\n", "and", "len", "(", "current_batch", ")", "*", "max_sentence_length", ">=", "(", "-", "1", "*", "max_batch_size", ")", ")", ")", ":", "\n", "                    ", "batches_of_sentence_ids", ".", "append", "(", "current_batch", ")", "\n", "current_batch", "=", "[", "]", "\n", "max_sentence_length", "=", "0", "\n", "", "", "if", "len", "(", "current_batch", ")", ">", "0", ":", "\n", "                ", "batches_of_sentence_ids", ".", "append", "(", "current_batch", ")", "\n", "", "", "return", "batches_of_sentence_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Experiment.process_sentences": [[315, 380], ["evaluator.Evaluator.Evaluator", "experiment.Experiment.create_batches_of_sentence_ids", "evaluator.Evaluator.Evaluator.get_results", "evaluator.Evaluator.Evaluator.get_results_nice_print", "random.shuffle", "model.process_batch", "evaluator.Evaluator.Evaluator.append_data", "print", "visualize.plot_predictions", "all_batches.append", "all_sentence_probs.append", "all_token_probs.append", "zip", "[].split", "gc.collect", "visualize.plot_token_scores", "str", "len", "len", "experiment.Experiment.config[].split"], "methods", ["home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Experiment.create_batches_of_sentence_ids", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.evaluator.Evaluator.get_results", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.evaluator.Evaluator.get_results_nice_print", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.process_batch", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.evaluator.Evaluator.append_data", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.visualize.plot_predictions", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.visualize.plot_token_scores"], ["", "def", "process_sentences", "(", "self", ",", "sentences", ",", "model", ",", "is_training", ",", "learning_rate", ",", "name", ")", ":", "\n", "        ", "\"\"\"\n        Obtains predictions and returns the evaluation metrics.\n        :type sentences: List[Sentence]\n        :type model: Model\n        :type is_training: bool\n        :type learning_rate: float\n        :type name: str\n        :rtype: List[floats]\n        \"\"\"", "\n", "evaluator", "=", "Evaluator", "(", "self", ".", "label2id_sent", ",", "self", ".", "label2id_tok", ",", "\n", "self", ".", "config", "[", "\"conll03_eval\"", "]", ")", "\n", "\n", "batches_of_sentence_ids", "=", "self", ".", "create_batches_of_sentence_ids", "(", "\n", "sentences", ",", "self", ".", "config", "[", "\"batch_equal_size\"", "]", ",", "self", ".", "config", "[", "\"max_batch_size\"", "]", ")", "\n", "\n", "if", "is_training", ":", "\n", "            ", "random", ".", "shuffle", "(", "batches_of_sentence_ids", ")", "\n", "\n", "", "all_batches", ",", "all_sentence_probs", ",", "all_token_probs", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "\n", "for", "batch_of_sentence_ids", "in", "batches_of_sentence_ids", ":", "\n", "            ", "batch", "=", "[", "sentences", "[", "i", "]", "for", "i", "in", "batch_of_sentence_ids", "]", "\n", "\n", "cost", ",", "sentence_pred", ",", "sentence_probs", ",", "token_pred", ",", "token_probs", "=", "model", ".", "process_batch", "(", "batch", ",", "is_training", ",", "learning_rate", ")", "\n", "evaluator", ".", "append_data", "(", "cost", ",", "batch", ",", "sentence_pred", ",", "token_pred", ")", "\n", "\n", "if", "\"test\"", "in", "name", "and", "self", ".", "config", "[", "\"plot_predictions_html\"", "]", ":", "\n", "                ", "all_batches", ".", "append", "(", "batch", ")", "\n", "all_sentence_probs", ".", "append", "(", "sentence_probs", ")", "\n", "all_token_probs", ".", "append", "(", "token_probs", ")", "\n", "\n", "# Plot the token scores for each sentence in the batch.", "\n", "", "if", "\"test\"", "in", "name", "and", "self", ".", "config", "[", "\"plot_token_scores\"", "]", ":", "\n", "                ", "for", "sentence", ",", "token_proba_per_sentence", ",", "sent_pred", "in", "zip", "(", "batch", ",", "token_probs", ",", "sentence_pred", ")", ":", "\n", "                    ", "if", "sentence", ".", "label_sent", "!=", "0", "and", "sentence", ".", "label_sent", "==", "sent_pred", "and", "len", "(", "sentence", ".", "tokens", ")", ">", "5", ":", "\n", "                        ", "visualize", ".", "plot_token_scores", "(", "\n", "token_probs", "=", "token_proba_per_sentence", ",", "\n", "sentence", "=", "sentence", ",", "\n", "id2label_tok", "=", "evaluator", ".", "id2label_tok", ",", "\n", "plot_name", "=", "self", ".", "config", "[", "\"path_plot_token_scores\"", "]", ")", "\n", "\n", "", "", "", "while", "self", ".", "config", "[", "\"garbage_collection\"", "]", "and", "gc", ".", "collect", "(", ")", ">", "0", ":", "\n", "                ", "pass", "\n", "\n", "", "", "results", "=", "evaluator", ".", "get_results", "(", "\n", "name", "=", "name", ",", "token_labels_available", "=", "self", ".", "config", "[", "\"token_labels_available\"", "]", ")", "\n", "\n", "for", "key", "in", "results", ":", "\n", "            ", "print", "(", "\"%s_%s: %s\"", "%", "(", "name", ",", "key", ",", "str", "(", "results", "[", "key", "]", ")", ")", ")", "\n", "", "evaluator", ".", "get_results_nice_print", "(", "\n", "name", "=", "name", ",", "token_labels_available", "=", "self", ".", "config", "[", "\"token_labels_available\"", "]", ")", "\n", "\n", "# Create html visualizations based on the test set predictions.", "\n", "if", "\"test\"", "in", "name", "and", "self", ".", "config", "[", "\"plot_predictions_html\"", "]", ":", "\n", "            ", "save_name", "=", "(", "self", ".", "config", "[", "\"to_write_filename\"", "]", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", ")", ".", "split", "(", "\".\"", ")", "[", "0", "]", "\n", "visualize", ".", "plot_predictions", "(", "\n", "all_sentences", "=", "all_batches", ",", "\n", "all_sentence_probs", "=", "all_sentence_probs", ",", "\n", "all_token_probs", "=", "all_token_probs", ",", "\n", "id2label_tok", "=", "evaluator", ".", "id2label_tok", ",", "\n", "html_name", "=", "self", ".", "config", "[", "\"path_plot_predictions_html\"", "]", "+", "\"/%s\"", "%", "save_name", ",", "\n", "sent_binary", "=", "len", "(", "self", ".", "label2id_sent", ")", "==", "2", ")", "\n", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Experiment.run_baseline": [[381, 465], ["print", "print", "experiment.Experiment.create_labels_mapping", "experiment.Experiment.create_labels_mapping", "print", "print", "pandas.DataFrame.to_csv", "experiment.Experiment.config[].strip().split", "experiment.Experiment.config[].strip().split", "len", "experiment.Experiment.read_input_files", "collections.Counter().most_common", "collections.Counter().most_common", "experiment.Experiment.read_input_files", "experiment.Experiment.convert_labels", "evaluator.Evaluator", "evaluator.Evaluator.append_data", "evaluator.Evaluator.get_results", "evaluator.Evaluator.get_results_nice_print", "pandas.DataFrame.append", "evaluator.Evaluator", "evaluator.Evaluator.append_data", "evaluator.Evaluator.get_results", "evaluator.Evaluator.get_results_nice_print", "pandas.DataFrame.append", "experiment.Experiment.config[].strip", "experiment.Experiment.config[].strip", "len", "majority_pred_tok.append", "str", "print", "pandas.DataFrame", "random_pred_sent.append", "random_pred_tok.append", "str", "print", "collections.Counter", "collections.Counter", "random.randint", "experiment.Experiment.config[].split", "len", "evaluator.Evaluator.get_results.keys", "random.randint", "str", "len", "range", "str", "len", "len"], "methods", ["home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Experiment.create_labels_mapping", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Experiment.create_labels_mapping", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Experiment.read_input_files", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Experiment.read_input_files", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Experiment.convert_labels", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.evaluator.Evaluator.append_data", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.evaluator.Evaluator.get_results", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.evaluator.Evaluator.get_results_nice_print", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.evaluator.Evaluator.append_data", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.evaluator.Evaluator.get_results", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.evaluator.Evaluator.get_results_nice_print"], ["", "def", "run_baseline", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Runs majority and random baselines.\n        \"\"\"", "\n", "if", "self", ".", "config", "[", "\"path_train\"", "]", "and", "len", "(", "self", ".", "config", "[", "\"path_train\"", "]", ")", ">", "0", ":", "\n", "            ", "data_train", "=", "[", "]", "\n", "for", "path_train", "in", "self", ".", "config", "[", "\"path_train\"", "]", ".", "strip", "(", ")", ".", "split", "(", "\":\"", ")", ":", "\n", "                ", "data_train", "+=", "self", ".", "read_input_files", "(", "\n", "file_paths", "=", "path_train", ",", "\n", "max_sentence_length", "=", "self", ".", "config", "[", "\"max_train_sent_length\"", "]", ")", "\n", "\n", "", "", "majority_sentence_label", "=", "Counter", "(", "Sentence", ".", "labels_sent_dict", ")", ".", "most_common", "(", "1", ")", "[", "0", "]", "[", "0", "]", "\n", "majority_token_label", "=", "Counter", "(", "Token", ".", "labels_tok_dict", ")", ".", "most_common", "(", "1", ")", "[", "0", "]", "[", "0", "]", "\n", "\n", "print", "(", "\"Most common sentence label (as in the train set) = \"", ",", "majority_sentence_label", ")", "\n", "print", "(", "\"Most common token label (as in the train set) = \"", ",", "majority_token_label", ")", "\n", "\n", "self", ".", "label2id_sent", "=", "self", ".", "create_labels_mapping", "(", "Sentence", ".", "unique_labels_sent", ")", "\n", "self", ".", "label2id_tok", "=", "self", ".", "create_labels_mapping", "(", "Token", ".", "unique_labels_tok", ")", "\n", "print", "(", "\"Sentence labels to id: \"", ",", "self", ".", "label2id_sent", ")", "\n", "print", "(", "\"Token labels to id: \"", ",", "self", ".", "label2id_tok", ")", "\n", "\n", "df_results", "=", "None", "\n", "\n", "if", "self", ".", "config", "[", "\"path_test\"", "]", "is", "not", "None", ":", "\n", "            ", "i", "=", "0", "\n", "for", "path_test", "in", "self", ".", "config", "[", "\"path_test\"", "]", ".", "strip", "(", ")", ".", "split", "(", "\":\"", ")", ":", "\n", "                ", "data_test", "=", "self", ".", "read_input_files", "(", "path_test", ")", "\n", "data_test", "=", "self", ".", "convert_labels", "(", "data_test", ")", "\n", "\n", "# Majority baseline.", "\n", "majority_pred_sent", "=", "[", "self", ".", "label2id_sent", "[", "majority_sentence_label", "]", "]", "*", "len", "(", "data_test", ")", "\n", "majority_pred_tok", "=", "[", "]", "\n", "for", "sentence", "in", "data_test", ":", "\n", "                    ", "majority_pred_tok", ".", "append", "(", "\n", "[", "self", ".", "label2id_tok", "[", "majority_token_label", "]", "]", "*", "len", "(", "sentence", ".", "tokens", ")", ")", "\n", "\n", "", "majority_evaluator", "=", "Evaluator", "(", "\n", "self", ".", "label2id_sent", ",", "self", ".", "label2id_tok", ",", "self", ".", "config", "[", "\"conll03_eval\"", "]", ")", "\n", "majority_evaluator", ".", "append_data", "(", "\n", "0.0", ",", "data_test", ",", "majority_pred_sent", ",", "majority_pred_tok", ")", "\n", "\n", "name", "=", "\"majority_test\"", "+", "str", "(", "i", ")", "\n", "results", "=", "majority_evaluator", ".", "get_results", "(", "\n", "name", "=", "name", ",", "token_labels_available", "=", "self", ".", "config", "[", "\"token_labels_available\"", "]", ")", "\n", "\n", "for", "key", "in", "results", ":", "\n", "                    ", "print", "(", "\"%s_%s: %s\"", "%", "(", "name", ",", "key", ",", "str", "(", "results", "[", "key", "]", ")", ")", ")", "\n", "", "majority_evaluator", ".", "get_results_nice_print", "(", "\n", "name", "=", "name", ",", "token_labels_available", "=", "self", ".", "config", "[", "\"token_labels_available\"", "]", ")", "\n", "\n", "if", "df_results", "is", "None", ":", "\n", "                    ", "df_results", "=", "pd", ".", "DataFrame", "(", "columns", "=", "results", ".", "keys", "(", ")", ")", "\n", "", "df_results", "=", "df_results", ".", "append", "(", "results", ",", "ignore_index", "=", "True", ")", "\n", "\n", "# Random baseline.", "\n", "random_pred_sent", "=", "[", "]", "\n", "random_pred_tok", "=", "[", "]", "\n", "for", "sentence", "in", "data_test", ":", "\n", "                    ", "random_pred_sent", ".", "append", "(", "random", ".", "randint", "(", "0", ",", "len", "(", "self", ".", "label2id_sent", ")", "-", "1", ")", ")", "\n", "random_pred_tok", ".", "append", "(", "\n", "[", "random", ".", "randint", "(", "0", ",", "len", "(", "self", ".", "label2id_tok", ")", "-", "1", ")", "\n", "for", "_", "in", "range", "(", "len", "(", "sentence", ".", "tokens", ")", ")", "]", ")", "\n", "\n", "", "random_evaluator", "=", "Evaluator", "(", "\n", "self", ".", "label2id_sent", ",", "self", ".", "label2id_tok", ",", "self", ".", "config", "[", "\"conll03_eval\"", "]", ")", "\n", "random_evaluator", ".", "append_data", "(", "\n", "0.0", ",", "data_test", ",", "random_pred_sent", ",", "random_pred_tok", ")", "\n", "\n", "name", "=", "\"rand_test\"", "+", "str", "(", "i", ")", "\n", "results", "=", "random_evaluator", ".", "get_results", "(", "\n", "name", "=", "name", ",", "token_labels_available", "=", "self", ".", "config", "[", "\"token_labels_available\"", "]", ")", "\n", "\n", "for", "key", "in", "results", ":", "\n", "                    ", "print", "(", "\"%s_%s: %s\"", "%", "(", "name", ",", "key", ",", "str", "(", "results", "[", "key", "]", ")", ")", ")", "\n", "", "random_evaluator", ".", "get_results_nice_print", "(", "\n", "name", "=", "name", ",", "token_labels_available", "=", "self", ".", "config", "[", "\"token_labels_available\"", "]", ")", "\n", "\n", "df_results", "=", "df_results", ".", "append", "(", "results", ",", "ignore_index", "=", "True", ")", "\n", "i", "+=", "1", "\n", "\n", "# Save data frame with all the training and testing results", "\n", "", "", "df_results", ".", "to_csv", "(", "\"\"", ".", "join", "(", "self", ".", "config", "[", "\"to_write_filename\"", "]", ".", "split", "(", "\".\"", ")", "[", ":", "-", "1", "]", ")", "\n", "+", "\"_df_results.txt\"", ",", "index", "=", "False", ",", "sep", "=", "\"\\t\"", ",", "encoding", "=", "\"utf-8\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Experiment.run_experiment": [[466, 668], ["experiment.Experiment.parse_config", "experiment.initialize_writer", "random.randint", "print", "experiment.Experiment.config.items", "experiment.Experiment.create_labels_mapping", "experiment.Experiment.create_labels_mapping", "print", "print", "model.Model.Model", "model.Model.Model.build_vocabs", "model.Model.Model.construct_network", "model.Model.Model.initialize_session", "print", "print", "experiment.Experiment.config[].split", "range", "df_results.append.append.to_csv", "model.Model.Model.load", "print", "print", "print", "random.seed", "numpy.random.seed", "print", "experiment.Experiment.run_baseline", "experiment.Experiment.config[].strip().split", "experiment.Experiment.config[].strip().split", "experiment.Experiment.config[].strip().split", "experiment.Experiment.convert_labels", "experiment.Experiment.convert_labels", "experiment.Experiment.convert_labels", "model.Model.Model.preload_word_embeddings", "ValueError", "type", "len", "sum", "float", "print", "print", "random.shuffle", "experiment.Experiment.process_sentences", "df_results.append.append.append", "model.Model.Model.saver.restore", "os.remove", "os.remove", "os.remove", "os.remove", "model.Model.Model.save", "enumerate", "enumerate", "len", "experiment.Experiment.read_input_files", "len", "experiment.Experiment.read_input_files", "len", "experiment.Experiment.read_input_files", "model.Model.Model.get_parameter_count", "model.Model.Model.get_parameter_count_without_word_embeddings", "float", "len", "len", "len", "sum", "pandas.DataFrame", "experiment.Experiment.process_sentences", "df_results.append.append.append", "sum", "print", "print", "len", "experiment.Experiment.config[].strip().split", "experiment.Experiment.read_input_files", "experiment.Experiment.convert_labels", "experiment.Experiment.process_sentences", "df_results.append.append.append", "experiment.config[].strip().split", "experiment.read_input_files", "experiment.convert_labels", "experiment.process_sentences", "int", "str", "experiment.Experiment.config[].strip", "experiment.Experiment.config[].strip", "experiment.Experiment.config[].strip", "experiment.Experiment.config[].split", "zip", "math.isnan", "math.isinf", "ValueError", "model.Model.Model.saver.save", "gc.collect", "time.time", "str", "len", "experiment.Experiment.keys", "experiment.Experiment.config[].strip", "experiment.Experiment.config[].split", "experiment.config[].strip", "model_selector_to_ratio.items", "str", "str", "os.path.basename"], "methods", ["home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Experiment.parse_config", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.initialize_writer", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Experiment.create_labels_mapping", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Experiment.create_labels_mapping", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.build_vocabs", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.construct_network", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.initialize_session", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.load", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Experiment.run_baseline", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Experiment.convert_labels", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Experiment.convert_labels", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Experiment.convert_labels", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.preload_word_embeddings", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Experiment.process_sentences", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.save", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Experiment.read_input_files", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Experiment.read_input_files", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Experiment.read_input_files", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.get_parameter_count", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.get_parameter_count_without_word_embeddings", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Experiment.process_sentences", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Experiment.read_input_files", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Experiment.convert_labels", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Experiment.process_sentences", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Experiment.read_input_files", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Experiment.convert_labels", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Experiment.process_sentences", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.save"], ["", "def", "run_experiment", "(", "self", ",", "config_path", ")", ":", "\n", "        ", "\"\"\"\n        Runs an experiment with MHAL.\n        :type config_path: str\n        \"\"\"", "\n", "self", ".", "config", "=", "self", ".", "parse_config", "(", "\"config\"", ",", "config_path", ")", "\n", "\n", "# If you already have a pre-trained model that you just want to test/visualize, set", "\n", "# \"load_pretrained_model\" to True and add the path to the saved model in \"save\".", "\n", "if", "self", ".", "config", "[", "\"load_pretrained_model\"", "]", ":", "\n", "            ", "model_filename", "=", "experiment", ".", "config", "[", "\"save\"", "]", "\n", "loaded_model", "=", "Model", ".", "load", "(", "model_filename", ")", "\n", "print", "(", "\"Loaded model from %s\"", "%", "model_filename", ")", "\n", "\n", "experiment", ".", "label2id_sent", "=", "loaded_model", ".", "label2id_sent", "\n", "experiment", ".", "label2id_tok", "=", "loaded_model", ".", "label2id_tok", "\n", "print", "(", "\"Sentence labels to id: \"", ",", "experiment", ".", "label2id_sent", ")", "\n", "print", "(", "\"Token labels to id: \"", ",", "experiment", ".", "label2id_tok", ")", "\n", "\n", "if", "experiment", ".", "config", "[", "\"path_test\"", "]", ":", "\n", "                ", "for", "d", ",", "path_data_test", "in", "enumerate", "(", "experiment", ".", "config", "[", "\"path_test\"", "]", ".", "strip", "(", ")", ".", "split", "(", "\":\"", ")", ")", ":", "\n", "                    ", "data_test_loaded", "=", "experiment", ".", "read_input_files", "(", "path_data_test", ")", "\n", "data_test_loaded", "=", "experiment", ".", "convert_labels", "(", "data_test_loaded", ")", "\n", "experiment", ".", "process_sentences", "(", "\n", "data_test_loaded", ",", "loaded_model", ",", "is_training", "=", "False", ",", "\n", "learning_rate", "=", "0.0", ",", "name", "=", "\"test\"", "+", "str", "(", "d", ")", ")", "\n", "", "", "return", "\n", "\n", "# Train and test a new model.", "\n", "\n", "", "initialize_writer", "(", "self", ".", "config", "[", "\"to_write_filename\"", "]", ")", "\n", "i_rand", "=", "random", ".", "randint", "(", "1", ",", "10000", ")", "\n", "print", "(", "\"i_rand = \"", ",", "i_rand", ")", "\n", "temp_model_path", "=", "\"models/temp_model_%d\"", "%", "(", "\n", "int", "(", "time", ".", "time", "(", ")", ")", "+", "i_rand", ")", "+", "\".model\"", "\n", "\n", "if", "\"random_seed\"", "in", "self", ".", "config", ":", "\n", "            ", "random", ".", "seed", "(", "self", ".", "config", "[", "\"random_seed\"", "]", ")", "\n", "np", ".", "random", ".", "seed", "(", "self", ".", "config", "[", "\"random_seed\"", "]", ")", "\n", "\n", "", "for", "key", ",", "val", "in", "self", ".", "config", ".", "items", "(", ")", ":", "\n", "            ", "print", "(", "str", "(", "key", ")", "+", "\" = \"", "+", "str", "(", "val", ")", ")", "\n", "\n", "# Run majority and random baselines.", "\n", "", "if", "\"baseline\"", "in", "self", ".", "config", "[", "\"model_type\"", "]", ":", "\n", "            ", "self", ".", "run_baseline", "(", ")", "\n", "return", "\n", "\n", "", "data_train", ",", "data_dev", ",", "data_test", "=", "None", ",", "None", ",", "None", "\n", "\n", "if", "self", ".", "config", "[", "\"path_train\"", "]", "and", "len", "(", "self", ".", "config", "[", "\"path_train\"", "]", ")", ">", "0", ":", "\n", "            ", "data_train", "=", "[", "]", "\n", "for", "path_train", "in", "self", ".", "config", "[", "\"path_train\"", "]", ".", "strip", "(", ")", ".", "split", "(", "\":\"", ")", ":", "\n", "                ", "data_train", "+=", "self", ".", "read_input_files", "(", "\n", "file_paths", "=", "path_train", ",", "\n", "max_sentence_length", "=", "self", ".", "config", "[", "\"max_train_sent_length\"", "]", ")", "\n", "\n", "", "", "if", "self", ".", "config", "[", "\"path_dev\"", "]", "and", "len", "(", "self", ".", "config", "[", "\"path_dev\"", "]", ")", ">", "0", ":", "\n", "            ", "data_dev", "=", "[", "]", "\n", "for", "path_dev", "in", "self", ".", "config", "[", "\"path_dev\"", "]", ".", "strip", "(", ")", ".", "split", "(", "\":\"", ")", ":", "\n", "                ", "data_dev", "+=", "self", ".", "read_input_files", "(", "file_paths", "=", "path_dev", ")", "\n", "\n", "", "", "if", "self", ".", "config", "[", "\"path_test\"", "]", "and", "len", "(", "self", ".", "config", "[", "\"path_test\"", "]", ")", ">", "0", ":", "\n", "            ", "data_test", "=", "[", "]", "\n", "for", "path_test", "in", "self", ".", "config", "[", "\"path_test\"", "]", ".", "strip", "(", ")", ".", "split", "(", "\":\"", ")", ":", "\n", "                ", "data_test", "+=", "self", ".", "read_input_files", "(", "file_paths", "=", "path_test", ")", "\n", "\n", "", "", "self", ".", "label2id_sent", "=", "self", ".", "create_labels_mapping", "(", "Sentence", ".", "unique_labels_sent", ")", "\n", "self", ".", "label2id_tok", "=", "self", ".", "create_labels_mapping", "(", "Token", ".", "unique_labels_tok", ")", "\n", "print", "(", "\"Sentence labels to id: \"", ",", "self", ".", "label2id_sent", ")", "\n", "print", "(", "\"Token labels to id: \"", ",", "self", ".", "label2id_tok", ")", "\n", "\n", "data_train", "=", "self", ".", "convert_labels", "(", "data_train", ")", "if", "data_train", "else", "None", "\n", "data_dev", "=", "self", ".", "convert_labels", "(", "data_dev", ")", "if", "data_dev", "else", "None", "\n", "data_test", "=", "self", ".", "convert_labels", "(", "data_test", ")", "if", "data_test", "else", "None", "\n", "\n", "data_train", "=", "data_train", "[", ":", "50", "]", "\n", "data_dev", "=", "data_dev", "[", ":", "50", "]", "\n", "data_test", "=", "data_test", "[", ":", "50", "]", "\n", "\n", "model", "=", "Model", "(", "self", ".", "config", ",", "self", ".", "label2id_sent", ",", "self", ".", "label2id_tok", ")", "\n", "model", ".", "build_vocabs", "(", "data_train", ",", "data_dev", ",", "data_test", ",", "\n", "embedding_path", "=", "self", ".", "config", "[", "\"preload_vectors\"", "]", ")", "\n", "model", ".", "construct_network", "(", ")", "\n", "model", ".", "initialize_session", "(", ")", "\n", "\n", "if", "self", ".", "config", "[", "\"preload_vectors\"", "]", ":", "\n", "            ", "model", ".", "preload_word_embeddings", "(", "self", ".", "config", "[", "\"preload_vectors\"", "]", ")", "\n", "\n", "", "print", "(", "\"Parameter count: %d.\"", "\n", "%", "model", ".", "get_parameter_count", "(", ")", ")", "\n", "print", "(", "\"Parameter count without word embeddings: %d.\"", "\n", "%", "model", ".", "get_parameter_count_without_word_embeddings", "(", ")", ")", "\n", "\n", "if", "data_train", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"No training set provided!\"", ")", "\n", "\n", "", "model_selector_splits", "=", "self", ".", "config", "[", "\"model_selector\"", "]", ".", "split", "(", "\":\"", ")", "\n", "if", "type", "(", "self", ".", "config", "[", "\"model_selector_ratio\"", "]", ")", "==", "str", ":", "\n", "            ", "model_selector_ratios_splits", "=", "[", "\n", "float", "(", "val", ")", "for", "val", "in", "self", ".", "config", "[", "\"model_selector_ratio\"", "]", ".", "split", "(", "\":\"", ")", "]", "\n", "", "else", ":", "\n", "            ", "model_selector_ratios_splits", "=", "[", "self", ".", "config", "[", "\"model_selector_ratio\"", "]", "]", "\n", "", "model_selector_type", "=", "model_selector_splits", "[", "-", "1", "]", "\n", "model_selector_values", "=", "model_selector_splits", "[", ":", "-", "1", "]", "\n", "assert", "(", "len", "(", "model_selector_values", ")", "==", "len", "(", "model_selector_ratios_splits", ")", "\n", "or", "len", "(", "model_selector_ratios_splits", ")", "==", "1", ")", ",", "\"Model selector values and ratios don't match!\"", "\n", "\n", "# Each model_selector_value contributes in proportion to its", "\n", "# corresponding (normalized) weight value. If just one ratio is specified,", "\n", "# all model_selector_values receive equal weight.", "\n", "if", "len", "(", "model_selector_ratios_splits", ")", "==", "1", ":", "\n", "            ", "normalized_ratio", "=", "model_selector_ratios_splits", "[", "0", "]", "/", "sum", "(", "\n", "model_selector_ratios_splits", "*", "len", "(", "model_selector_values", ")", ")", "\n", "model_selector_to_ratio", "=", "{", "value", ":", "normalized_ratio", "for", "value", "in", "model_selector_values", "}", "\n", "", "else", ":", "\n", "            ", "sum_ratios", "=", "sum", "(", "model_selector_ratios_splits", ")", "\n", "normalized_ratios", "=", "[", "ratio", "/", "sum_ratios", "for", "ratio", "in", "model_selector_ratios_splits", "]", "\n", "model_selector_to_ratio", "=", "{", "value", ":", "ratio", "for", "value", ",", "ratio", "in", "\n", "zip", "(", "model_selector_values", ",", "normalized_ratios", ")", "}", "\n", "\n", "", "best_selector_value", "=", "0.0", "\n", "if", "model_selector_type", "==", "\"low\"", ":", "\n", "            ", "best_selector_value", "=", "float", "(", "\"inf\"", ")", "\n", "", "best_epoch", "=", "-", "1", "\n", "learning_rate", "=", "self", ".", "config", "[", "\"learning_rate\"", "]", "\n", "\n", "df_results", "=", "None", "\n", "\n", "for", "epoch", "in", "range", "(", "self", ".", "config", "[", "\"epochs\"", "]", ")", ":", "\n", "            ", "print", "(", "\"EPOCH: %d\"", "%", "epoch", ")", "\n", "print", "(", "\"Learning rate: %f\"", "%", "learning_rate", ")", "\n", "random", ".", "shuffle", "(", "data_train", ")", "\n", "\n", "results_train", "=", "self", ".", "process_sentences", "(", "\n", "data_train", ",", "model", ",", "is_training", "=", "True", ",", "\n", "learning_rate", "=", "learning_rate", ",", "name", "=", "\"train_epoch%d\"", "%", "epoch", ")", "\n", "\n", "if", "df_results", "is", "None", ":", "\n", "                ", "df_results", "=", "pd", ".", "DataFrame", "(", "columns", "=", "results_train", ".", "keys", "(", ")", ")", "\n", "", "df_results", "=", "df_results", ".", "append", "(", "results_train", ",", "ignore_index", "=", "True", ")", "\n", "\n", "if", "data_dev", ":", "\n", "                ", "results_dev", "=", "self", ".", "process_sentences", "(", "\n", "data_dev", ",", "model", ",", "is_training", "=", "False", ",", "\n", "learning_rate", "=", "0.0", ",", "name", "=", "\"dev_epoch%d\"", "%", "epoch", ")", "\n", "\n", "df_results", "=", "df_results", ".", "append", "(", "results_dev", ",", "ignore_index", "=", "True", ")", "\n", "\n", "if", "math", ".", "isnan", "(", "results_dev", "[", "\"cost_sum\"", "]", ")", "or", "math", ".", "isinf", "(", "results_dev", "[", "\"cost_sum\"", "]", ")", ":", "\n", "                    ", "raise", "ValueError", "(", "\"Cost is NaN or Inf!\"", ")", "\n", "\n", "", "results_dev_for_model_selector", "=", "sum", "(", "[", "\n", "results_dev", "[", "model_selector", "]", "*", "ratio", "\n", "for", "model_selector", ",", "ratio", "in", "model_selector_to_ratio", ".", "items", "(", ")", "]", ")", "\n", "\n", "if", "(", "epoch", "==", "0", "\n", "or", "(", "model_selector_type", "==", "\"high\"", "\n", "and", "results_dev_for_model_selector", ">", "best_selector_value", ")", "\n", "or", "(", "model_selector_type", "==", "\"low\"", "\n", "and", "results_dev_for_model_selector", "<", "best_selector_value", ")", ")", ":", "\n", "                    ", "best_epoch", "=", "epoch", "\n", "best_selector_value", "=", "results_dev_for_model_selector", "\n", "model", ".", "saver", ".", "save", "(", "sess", "=", "model", ".", "session", ",", "save_path", "=", "temp_model_path", ",", "\n", "latest_filename", "=", "os", ".", "path", ".", "basename", "(", "temp_model_path", ")", "+", "\".checkpoint\"", ")", "\n", "\n", "", "print", "(", "\"Best epoch: %d\"", "%", "best_epoch", ")", "\n", "print", "(", "\"*\"", "*", "50", "+", "\"\\n\"", ")", "\n", "\n", "if", "0", "<", "self", ".", "config", "[", "\"stop_if_no_improvement_for_epochs\"", "]", "<=", "epoch", "-", "best_epoch", ":", "\n", "                    ", "break", "\n", "\n", "", "if", "epoch", "-", "best_epoch", ">", "3", ":", "\n", "                    ", "learning_rate", "*=", "self", ".", "config", "[", "\"learning_rate_decay\"", "]", "\n", "\n", "", "", "while", "self", ".", "config", "[", "\"garbage_collection\"", "]", "and", "gc", ".", "collect", "(", ")", ">", "0", ":", "\n", "                ", "pass", "\n", "\n", "", "", "if", "data_dev", "and", "best_epoch", ">=", "0", ":", "\n", "            ", "model", ".", "saver", ".", "restore", "(", "model", ".", "session", ",", "temp_model_path", ")", "\n", "os", ".", "remove", "(", "temp_model_path", "+", "\".checkpoint\"", ")", "\n", "os", ".", "remove", "(", "temp_model_path", "+", "\".data-00000-of-00001\"", ")", "\n", "os", ".", "remove", "(", "temp_model_path", "+", "\".index\"", ")", "\n", "os", ".", "remove", "(", "temp_model_path", "+", "\".meta\"", ")", "\n", "\n", "", "if", "self", ".", "config", "[", "\"save\"", "]", "is", "not", "None", "and", "len", "(", "self", ".", "config", "[", "\"save\"", "]", ")", ">", "0", ":", "\n", "            ", "model", ".", "save", "(", "self", ".", "config", "[", "\"save\"", "]", ")", "\n", "\n", "", "if", "self", ".", "config", "[", "\"path_test\"", "]", "is", "not", "None", ":", "\n", "            ", "for", "i", ",", "path_test", "in", "enumerate", "(", "self", ".", "config", "[", "\"path_test\"", "]", ".", "strip", "(", ")", ".", "split", "(", "\":\"", ")", ")", ":", "\n", "                ", "data_test", "=", "self", ".", "read_input_files", "(", "path_test", ")", "\n", "data_test", "=", "self", ".", "convert_labels", "(", "data_test", ")", "\n", "data_test", "=", "data_test", "[", ":", "50", "]", "\n", "results_test", "=", "self", ".", "process_sentences", "(", "\n", "data_test", ",", "model", ",", "is_training", "=", "False", ",", "\n", "learning_rate", "=", "0.0", ",", "name", "=", "\"test\"", "+", "str", "(", "i", ")", ")", "\n", "df_results", "=", "df_results", ".", "append", "(", "results_test", ",", "ignore_index", "=", "True", ")", "\n", "\n", "# Save all the training and testing results in csv format.", "\n", "", "", "df_results", ".", "to_csv", "(", "\"\"", ".", "join", "(", "self", ".", "config", "[", "\"to_write_filename\"", "]", ".", "split", "(", "\".\"", ")", "[", ":", "-", "1", "]", ")", "\n", "+", "\"_df_results.txt\"", ",", "index", "=", "False", ",", "sep", "=", "\"\\t\"", ",", "encoding", "=", "\"utf-8\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Writer.__init__": [[674, 676], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "*", "writers", ")", ":", "\n", "        ", "self", ".", "writers", "=", "writers", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Writer.write": [[677, 680], ["w.write"], "methods", ["home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Writer.write"], ["", "def", "write", "(", "self", ",", "text", ")", ":", "\n", "        ", "for", "w", "in", "self", ".", "writers", ":", "\n", "            ", "w", ".", "write", "(", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Writer.flush": [[681, 683], ["None"], "methods", ["None"], ["", "", "def", "flush", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.initialize_writer": [[685, 692], ["open", "experiment.Writer"], "function", ["None"], ["", "", "def", "initialize_writer", "(", "to_write_filename", ")", ":", "\n", "    ", "\"\"\"\n    Method to initialize my writer class.\n    :param to_write_filename: path to write the file to.\n    \"\"\"", "\n", "file_out", "=", "open", "(", "to_write_filename", ",", "\"wt\"", ")", "\n", "sys", ".", "stdout", "=", "Writer", "(", "sys", ".", "stdout", ",", "file_out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.__init__": [[15, 53], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "config", ",", "label2id_sent", ",", "label2id_tok", ")", ":", "\n", "        ", "self", ".", "config", "=", "config", "\n", "self", ".", "label2id_sent", "=", "label2id_sent", "\n", "self", ".", "label2id_tok", "=", "label2id_tok", "\n", "\n", "self", ".", "UNK", "=", "\"<unk>\"", "\n", "self", ".", "CUNK", "=", "\"<cunk>\"", "\n", "self", ".", "word2id", "=", "None", "\n", "self", ".", "char2id", "=", "None", "\n", "self", ".", "singletons", "=", "None", "\n", "self", ".", "num_heads", "=", "None", "\n", "\n", "self", ".", "word_ids", "=", "None", "\n", "self", ".", "char_ids", "=", "None", "\n", "self", ".", "sentence_lengths", "=", "None", "\n", "self", ".", "word_lengths", "=", "None", "\n", "\n", "self", ".", "sentence_labels", "=", "None", "\n", "self", ".", "word_labels", "=", "None", "\n", "\n", "self", ".", "word_embeddings", "=", "None", "\n", "self", ".", "char_embeddings", "=", "None", "\n", "\n", "self", ".", "word_objective_weights", "=", "None", "\n", "self", ".", "sentence_objective_weights", "=", "None", "\n", "\n", "self", ".", "learning_rate", "=", "None", "\n", "self", ".", "loss", "=", "None", "\n", "self", ".", "initializer", "=", "None", "\n", "self", ".", "is_training", "=", "None", "\n", "self", ".", "session", "=", "None", "\n", "self", ".", "saver", "=", "None", "\n", "self", ".", "train_op", "=", "None", "\n", "\n", "self", ".", "sentence_predictions", "=", "None", "\n", "self", ".", "sentence_probabilities", "=", "None", "\n", "self", ".", "token_predictions", "=", "None", "\n", "self", ".", "token_probabilities", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.build_vocabs": [[54, 112], ["list", "collections.Counter", "collections.Counter", "collections.OrderedDict", "collections.Counter.most_common", "collections.OrderedDict", "collections.Counter.most_common", "set", "print", "print", "print", "collections.OrderedDict", "collections.Counter.update", "len", "open", "len", "len", "len", "re.sub.lower", "re.sub", "len", "line.strip().split", "embedding_vocab.add", "len", "len", "re.sub.lower", "re.sub", "line.strip"], "methods", ["None"], ["", "def", "build_vocabs", "(", "self", ",", "data_train", ",", "data_dev", ",", "data_test", ",", "embedding_path", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Builds the vocabulary based on the the data and embeddings info.\n        \"\"\"", "\n", "data_source", "=", "list", "(", "data_train", ")", "\n", "if", "self", ".", "config", "[", "\"vocab_include_devtest\"", "]", ":", "\n", "            ", "if", "data_dev", "is", "not", "None", ":", "\n", "                ", "data_source", "+=", "data_dev", "\n", "", "if", "data_test", "is", "not", "None", ":", "\n", "                ", "data_source", "+=", "data_test", "\n", "\n", "", "", "char_counter", "=", "collections", ".", "Counter", "(", ")", "\n", "word_counter", "=", "collections", ".", "Counter", "(", ")", "\n", "for", "sentence", "in", "data_source", ":", "\n", "            ", "for", "token", "in", "sentence", ".", "tokens", ":", "\n", "                ", "char_counter", ".", "update", "(", "token", ".", "value", ")", "\n", "w", "=", "token", ".", "value", "\n", "if", "self", ".", "config", "[", "\"lowercase\"", "]", ":", "\n", "                    ", "w", "=", "w", ".", "lower", "(", ")", "\n", "", "if", "self", ".", "config", "[", "\"replace_digits\"", "]", ":", "\n", "                    ", "w", "=", "re", ".", "sub", "(", "r'\\d'", ",", "'0'", ",", "w", ")", "\n", "", "word_counter", "[", "w", "]", "+=", "1", "\n", "\n", "", "", "self", ".", "char2id", "=", "collections", ".", "OrderedDict", "(", "[", "(", "self", ".", "CUNK", ",", "0", ")", "]", ")", "\n", "for", "char", ",", "count", "in", "char_counter", ".", "most_common", "(", ")", ":", "\n", "            ", "if", "char", "not", "in", "self", ".", "char2id", ":", "\n", "                ", "self", ".", "char2id", "[", "char", "]", "=", "len", "(", "self", ".", "char2id", ")", "\n", "\n", "", "", "self", ".", "word2id", "=", "collections", ".", "OrderedDict", "(", "[", "(", "self", ".", "UNK", ",", "0", ")", "]", ")", "\n", "for", "word", ",", "count", "in", "word_counter", ".", "most_common", "(", ")", ":", "\n", "            ", "if", "self", ".", "config", "[", "\"min_word_freq\"", "]", "<=", "0", "or", "count", ">=", "self", ".", "config", "[", "\"min_word_freq\"", "]", ":", "\n", "                ", "if", "word", "not", "in", "self", ".", "word2id", ":", "\n", "                    ", "self", ".", "word2id", "[", "word", "]", "=", "len", "(", "self", ".", "word2id", ")", "\n", "\n", "", "", "", "self", ".", "singletons", "=", "set", "(", "[", "word", "for", "word", "in", "word_counter", "if", "word_counter", "[", "word", "]", "==", "1", "]", ")", "\n", "\n", "if", "embedding_path", "and", "self", ".", "config", "[", "\"vocab_only_embedded\"", "]", ":", "\n", "            ", "embedding_vocab", "=", "{", "self", ".", "UNK", "}", "\n", "with", "open", "(", "embedding_path", ")", "as", "f", ":", "\n", "                ", "for", "line", "in", "f", ":", "\n", "                    ", "line_parts", "=", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "if", "len", "(", "line_parts", ")", "<=", "2", ":", "\n", "                        ", "continue", "\n", "", "w", "=", "line_parts", "[", "0", "]", "\n", "if", "self", ".", "config", "[", "\"lowercase\"", "]", ":", "\n", "                        ", "w", "=", "w", ".", "lower", "(", ")", "\n", "", "if", "self", ".", "config", "[", "\"replace_digits\"", "]", ":", "\n", "                        ", "w", "=", "re", ".", "sub", "(", "r'\\d'", ",", "'0'", ",", "w", ")", "\n", "", "embedding_vocab", ".", "add", "(", "w", ")", "\n", "", "", "word2id_revised", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "for", "word", "in", "self", ".", "word2id", ":", "\n", "                ", "if", "word", "in", "embedding_vocab", "and", "word", "not", "in", "word2id_revised", ":", "\n", "                    ", "word2id_revised", "[", "word", "]", "=", "len", "(", "word2id_revised", ")", "\n", "", "", "self", ".", "word2id", "=", "word2id_revised", "\n", "\n", "", "print", "(", "\"Total number of words: %d.\"", "%", "len", "(", "self", ".", "word2id", ")", ")", "\n", "print", "(", "\"Total number of chars: %d.\"", "%", "len", "(", "self", ".", "char2id", ")", ")", "\n", "print", "(", "\"Total number of singletons: %d.\"", "%", "len", "(", "self", ".", "singletons", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.construct_network": [[113, 589], ["tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.zeros_initializer", "tensorflow.get_variable", "tensorflow.nn.embedding_lookup", "tensorflow.nn.rnn_cell.LSTMCell", "tensorflow.nn.rnn_cell.LSTMCell", "tensorflow.concat", "tensorflow.concat", "tensorflow.tile", "tensorflow.nn.softmax", "tensorflow.where", "tensorflow.argmax", "tensorflow.nn.softmax", "tensorflow.argmax", "tensorflow.reduce_max", "tensorflow.one_hot", "model.Model.construct_optimizer", "print", "tensorflow.random_normal_initializer", "tensorflow.nn.dropout", "tensorflow.control_dependencies", "tensorflow.nn.bidirectional_dynamic_rnn", "tensorflow.nn.dropout", "tensorflow.nn.dropout", "tensorflow.nn.dropout", "tensorflow.layers.dense", "tensorflow.layers.dense", "tensorflow.layers.dense", "tensorflow.zeros_like", "tensorflow.nn.sparse_softmax_cross_entropy_with_logits", "tensorflow.where", "tensorflow.cast", "modules.label_smoothing", "tensorflow.nn.softmax", "tensorflow.gather", "tensorflow.squeeze", "tensorflow.gather", "tensorflow.reduce_max", "tensorflow.reduce_mean", "tensorflow.nn.softmax", "tensorflow.nn.softmax", "tensorflow.glorot_uniform_initializer", "tensorflow.variable_scope", "tensorflow.control_dependencies", "tensorflow.get_variable", "tensorflow.nn.embedding_lookup", "tensorflow.shape", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.nn.rnn_cell.LSTMCell", "tensorflow.nn.rnn_cell.LSTMCell", "tensorflow.nn.bidirectional_dynamic_rnn", "tensorflow.concat", "tensorflow.reshape", "tensorflow.layers.dense", "ValueError", "tensorflow.expand_dims", "tensorflow.sequence_mask", "tensorflow.zeros_like", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "len", "len", "len", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "len", "len", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.squeeze", "tensorflow.reduce_sum", "len", "len", "tensorflow.losses.mean_pairwise_squared_error", "tensorflow.reduce_sum", "model.Model.construct_lm_cost", "model.Model.construct_lm_cost", "tensorflow.glorot_normal_initializer", "len", "tensorflow.layers.dense", "tensorflow.concat", "tensorflow.cast", "tensorflow.cast", "tensorflow.assert_equal", "tensorflow.cast", "tensorflow.cast", "tensorflow.convert_to_tensor", "tensorflow.convert_to_tensor", "len", "len", "tensorflow.variable_scope", "len", "len", "tensorflow.layers.dense", "tensorflow.math.reduce_mean", "tensorflow.expand_dims", "tensorflow.layers.dense", "tensorflow.layers.dense", "tensorflow.concat", "tensorflow.concat", "tensorflow.concat", "tensorflow.matmul", "tensorflow.squeeze", "tensorflow.concat", "tensorflow.tile", "tensorflow.where", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.layers.dense", "tensorflow.concat", "tensorflow.sequence_mask", "len", "tensorflow.cast", "len", "tensorflow.gather", "tensorflow.reduce_max", "tensorflow.concat", "ValueError", "len", "tensorflow.gather", "tensorflow.reduce_max", "tensorflow.concat", "ValueError", "len", "tensorflow.gather", "tensorflow.reduce_max", "tensorflow.concat", "ValueError", "tensorflow.assert_equal", "model.Model.construct_lm_cost", "model.Model.construct_lm_cost", "ValueError", "tensorflow.reduce_max", "tensorflow.layers.dense.get_shape().as_list", "tensorflow.layers.dense", "tensorflow.split", "tensorflow.split", "tensorflow.split", "tensorflow.split", "tensorflow.exp", "tensorflow.zeros_like", "tensorflow.expand_dims", "tensorflow.layers.dense", "tensorflow.split", "tensorflow.nn.sparse_softmax_cross_entropy_with_logits", "tensorflow.gather", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.gather", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.square", "tensorflow.gather", "tensorflow.distributions.kl_divergence", "tensorflow.reduce_max", "len", "tensorflow.shape", "math.ceil", "modules.cosine_distance_loss", "modules.cosine_distance_loss", "modules.cosine_distance_loss", "tensorflow.expand_dims", "tensorflow.sigmoid", "tensorflow.sequence_mask", "modules.cosine_distance_loss", "tensorflow.gather", "tensorflow.gather", "tensorflow.reduce_max", "tensorflow.concat", "tensorflow.layers.dense", "tensorflow.square", "tensorflow.square", "tensorflow.square", "tensorflow.cast", "tensorflow.square", "tensorflow.shape", "tensorflow.shape", "tensorflow.shape", "tensorflow.layers.dense.get_shape", "tensorflow.concat", "tensorflow.concat", "tensorflow.concat", "ValueError", "tensorflow.concat", "tensorflow.cast", "list", "list", "tensorflow.equal", "tensorflow.ones_like", "list", "tensorflow.distributions.Categorical", "tensorflow.distributions.Categorical", "tensorflow.split", "tensorflow.split", "tensorflow.split", "tensorflow.split", "list", "range", "range", "tensorflow.ones_like", "range", "tensorflow.transpose", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.expand_dims", "range", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.construct_optimizer", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.label_smoothing", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.construct_lm_cost", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.construct_lm_cost", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.construct_lm_cost", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.construct_lm_cost", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.cosine_distance_loss", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.cosine_distance_loss", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.cosine_distance_loss", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.modules.cosine_distance_loss"], ["", "def", "construct_network", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Constructs the multi-head attention labeller (MHAL) as described\n        in our paper/MPhil study. It uses keys, queries and values, to\n        apply a dot-product attention, allowing for query regularisation.\n        \"\"\"", "\n", "self", ".", "word_ids", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "None", ",", "None", "]", ",", "name", "=", "\"word_ids\"", ")", "\n", "self", ".", "char_ids", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "None", ",", "None", ",", "None", "]", ",", "name", "=", "\"char_ids\"", ")", "\n", "self", ".", "sentence_lengths", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "None", "]", ",", "name", "=", "\"sentence_lengths\"", ")", "\n", "self", ".", "word_lengths", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "None", ",", "None", "]", ",", "name", "=", "\"word_lengths\"", ")", "\n", "self", ".", "sentence_labels", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "None", "]", ",", "name", "=", "\"sentence_labels\"", ")", "\n", "self", ".", "word_labels", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "None", ",", "None", "]", ",", "name", "=", "\"word_labels\"", ")", "\n", "\n", "self", ".", "word_objective_weights", "=", "tf", ".", "placeholder", "(", "\n", "tf", ".", "float32", ",", "[", "None", ",", "None", "]", ",", "name", "=", "\"word_objective_weights\"", ")", "\n", "self", ".", "sentence_objective_weights", "=", "tf", ".", "placeholder", "(", "\n", "tf", ".", "float32", ",", "[", "None", "]", ",", "name", "=", "\"sentence_objective_weights\"", ")", "\n", "\n", "self", ".", "learning_rate", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "name", "=", "\"learning_rate\"", ")", "\n", "self", ".", "is_training", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "name", "=", "\"is_training\"", ")", "\n", "self", ".", "loss", "=", "0.0", "\n", "\n", "if", "self", ".", "config", "[", "\"initializer\"", "]", "==", "\"normal\"", ":", "\n", "            ", "self", ".", "initializer", "=", "tf", ".", "random_normal_initializer", "(", "stddev", "=", "0.1", ")", "\n", "", "elif", "self", ".", "config", "[", "\"initializer\"", "]", "==", "\"glorot\"", ":", "\n", "            ", "self", ".", "initializer", "=", "tf", ".", "glorot_uniform_initializer", "(", ")", "\n", "", "elif", "self", ".", "config", "[", "\"initializer\"", "]", "==", "\"xavier\"", ":", "\n", "            ", "self", ".", "initializer", "=", "tf", ".", "glorot_normal_initializer", "(", ")", "\n", "\n", "", "zeros_initializer", "=", "tf", ".", "zeros_initializer", "(", ")", "\n", "\n", "self", ".", "word_embeddings", "=", "tf", ".", "get_variable", "(", "\n", "name", "=", "\"word_embeddings\"", ",", "\n", "shape", "=", "[", "len", "(", "self", ".", "word2id", ")", ",", "self", ".", "config", "[", "\"word_embedding_size\"", "]", "]", ",", "\n", "initializer", "=", "(", "zeros_initializer", "if", "self", ".", "config", "[", "\"emb_initial_zero\"", "]", "else", "self", ".", "initializer", ")", ",", "\n", "trainable", "=", "(", "True", "if", "self", ".", "config", "[", "\"train_embeddings\"", "]", "else", "False", ")", ")", "\n", "word_input_tensor", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "self", ".", "word_embeddings", ",", "self", ".", "word_ids", ")", "\n", "\n", "if", "self", ".", "config", "[", "\"char_embedding_size\"", "]", ">", "0", "and", "self", ".", "config", "[", "\"char_recurrent_size\"", "]", ">", "0", ":", "\n", "            ", "with", "tf", ".", "variable_scope", "(", "\"chars\"", ")", ",", "tf", ".", "control_dependencies", "(", "\n", "[", "tf", ".", "assert_equal", "(", "tf", ".", "shape", "(", "self", ".", "char_ids", ")", "[", "2", "]", ",", "\n", "tf", ".", "reduce_max", "(", "self", ".", "word_lengths", ")", ",", "\n", "message", "=", "\"Char dimensions don't match\"", ")", "]", ")", ":", "\n", "                ", "self", ".", "char_embeddings", "=", "tf", ".", "get_variable", "(", "\n", "name", "=", "\"char_embeddings\"", ",", "\n", "shape", "=", "[", "len", "(", "self", ".", "char2id", ")", ",", "self", ".", "config", "[", "\"char_embedding_size\"", "]", "]", ",", "\n", "initializer", "=", "self", ".", "initializer", ",", "\n", "trainable", "=", "True", ")", "\n", "char_input_tensor", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "self", ".", "char_embeddings", ",", "self", ".", "char_ids", ")", "\n", "\n", "char_input_tensor_shape", "=", "tf", ".", "shape", "(", "char_input_tensor", ")", "\n", "char_input_tensor", "=", "tf", ".", "reshape", "(", "\n", "char_input_tensor", ",", "\n", "shape", "=", "[", "char_input_tensor_shape", "[", "0", "]", "\n", "*", "char_input_tensor_shape", "[", "1", "]", ",", "\n", "char_input_tensor_shape", "[", "2", "]", ",", "\n", "self", ".", "config", "[", "\"char_embedding_size\"", "]", "]", ")", "\n", "_word_lengths", "=", "tf", ".", "reshape", "(", "\n", "self", ".", "word_lengths", ",", "shape", "=", "[", "char_input_tensor_shape", "[", "0", "]", "\n", "*", "char_input_tensor_shape", "[", "1", "]", "]", ")", "\n", "\n", "char_lstm_cell_fw", "=", "tf", ".", "nn", ".", "rnn_cell", ".", "LSTMCell", "(", "\n", "self", ".", "config", "[", "\"char_recurrent_size\"", "]", ",", "\n", "use_peepholes", "=", "self", ".", "config", "[", "\"lstm_use_peepholes\"", "]", ",", "\n", "state_is_tuple", "=", "True", ",", "\n", "initializer", "=", "self", ".", "initializer", ",", "\n", "reuse", "=", "False", ")", "\n", "char_lstm_cell_bw", "=", "tf", ".", "nn", ".", "rnn_cell", ".", "LSTMCell", "(", "\n", "self", ".", "config", "[", "\"char_recurrent_size\"", "]", ",", "\n", "use_peepholes", "=", "self", ".", "config", "[", "\"lstm_use_peepholes\"", "]", ",", "\n", "state_is_tuple", "=", "True", ",", "\n", "initializer", "=", "self", ".", "initializer", ",", "\n", "reuse", "=", "False", ")", "\n", "\n", "# Concatenate the final forward and the backward character contexts", "\n", "# to obtain a compact character representation for each word.", "\n", "_", ",", "(", "(", "_", ",", "char_output_fw", ")", ",", "(", "_", ",", "char_output_bw", ")", ")", "=", "tf", ".", "nn", ".", "bidirectional_dynamic_rnn", "(", "\n", "cell_fw", "=", "char_lstm_cell_fw", ",", "cell_bw", "=", "char_lstm_cell_bw", ",", "inputs", "=", "char_input_tensor", ",", "\n", "sequence_length", "=", "_word_lengths", ",", "dtype", "=", "tf", ".", "float32", ",", "time_major", "=", "False", ")", "\n", "\n", "char_output_tensor", "=", "tf", ".", "concat", "(", "[", "char_output_fw", ",", "char_output_bw", "]", ",", "axis", "=", "-", "1", ")", "\n", "char_output_tensor", "=", "tf", ".", "reshape", "(", "\n", "char_output_tensor", ",", "\n", "shape", "=", "[", "char_input_tensor_shape", "[", "0", "]", ",", "char_input_tensor_shape", "[", "1", "]", ",", "\n", "2", "*", "self", ".", "config", "[", "\"char_recurrent_size\"", "]", "]", ")", "\n", "\n", "# Include a char-based language modelling loss, LM-c.", "\n", "if", "self", ".", "config", "[", "\"lm_cost_char_gamma\"", "]", ">", "0.0", ":", "\n", "                    ", "self", ".", "loss", "+=", "self", ".", "config", "[", "\"lm_cost_char_gamma\"", "]", "*", "self", ".", "construct_lm_cost", "(", "\n", "input_tensor_fw", "=", "char_output_tensor", ",", "\n", "input_tensor_bw", "=", "char_output_tensor", ",", "\n", "sentence_lengths", "=", "self", ".", "sentence_lengths", ",", "\n", "target_ids", "=", "self", ".", "word_ids", ",", "\n", "lm_cost_type", "=", "\"separate\"", ",", "\n", "name", "=", "\"lm_cost_char_separate\"", ")", "\n", "\n", "", "if", "self", ".", "config", "[", "\"lm_cost_joint_char_gamma\"", "]", ">", "0.0", ":", "\n", "                    ", "self", ".", "loss", "+=", "self", ".", "config", "[", "\"lm_cost_joint_char_gamma\"", "]", "*", "self", ".", "construct_lm_cost", "(", "\n", "input_tensor_fw", "=", "char_output_tensor", ",", "\n", "input_tensor_bw", "=", "char_output_tensor", ",", "\n", "sentence_lengths", "=", "self", ".", "sentence_lengths", ",", "\n", "target_ids", "=", "self", ".", "word_ids", ",", "\n", "lm_cost_type", "=", "\"joint\"", ",", "\n", "name", "=", "\"lm_cost_char_joint\"", ")", "\n", "\n", "", "if", "self", ".", "config", "[", "\"char_hidden_layer_size\"", "]", ">", "0", ":", "\n", "                    ", "char_output_tensor", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", "=", "char_output_tensor", ",", "units", "=", "self", ".", "config", "[", "\"char_hidden_layer_size\"", "]", ",", "\n", "activation", "=", "tf", ".", "tanh", ",", "kernel_initializer", "=", "self", ".", "initializer", ")", "\n", "\n", "", "if", "self", ".", "config", "[", "\"char_integration_method\"", "]", "==", "\"concat\"", ":", "\n", "                    ", "word_input_tensor", "=", "tf", ".", "concat", "(", "[", "word_input_tensor", ",", "char_output_tensor", "]", ",", "axis", "=", "-", "1", ")", "\n", "", "elif", "self", ".", "config", "[", "\"char_integration_method\"", "]", "==", "\"none\"", ":", "\n", "                    ", "word_input_tensor", "=", "word_input_tensor", "\n", "", "else", ":", "\n", "                    ", "raise", "ValueError", "(", "\"Unknown char integration method\"", ")", "\n", "\n", "", "", "", "if", "self", ".", "config", "[", "\"dropout_input\"", "]", ">", "0.0", ":", "\n", "            ", "dropout_input", "=", "(", "self", ".", "config", "[", "\"dropout_input\"", "]", "*", "tf", ".", "cast", "(", "self", ".", "is_training", ",", "tf", ".", "float32", ")", "\n", "+", "(", "1.0", "-", "tf", ".", "cast", "(", "self", ".", "is_training", ",", "tf", ".", "float32", ")", ")", ")", "\n", "word_input_tensor", "=", "tf", ".", "nn", ".", "dropout", "(", "\n", "word_input_tensor", ",", "dropout_input", ",", "name", "=", "\"dropout_word\"", ")", "\n", "\n", "", "word_lstm_cell_fw", "=", "tf", ".", "nn", ".", "rnn_cell", ".", "LSTMCell", "(", "\n", "self", ".", "config", "[", "\"word_recurrent_size\"", "]", ",", "\n", "use_peepholes", "=", "self", ".", "config", "[", "\"lstm_use_peepholes\"", "]", ",", "\n", "state_is_tuple", "=", "True", ",", "\n", "initializer", "=", "self", ".", "initializer", ",", "\n", "reuse", "=", "False", ")", "\n", "word_lstm_cell_bw", "=", "tf", ".", "nn", ".", "rnn_cell", ".", "LSTMCell", "(", "\n", "self", ".", "config", "[", "\"word_recurrent_size\"", "]", ",", "\n", "use_peepholes", "=", "self", ".", "config", "[", "\"lstm_use_peepholes\"", "]", ",", "\n", "state_is_tuple", "=", "True", ",", "\n", "initializer", "=", "self", ".", "initializer", ",", "\n", "reuse", "=", "False", ")", "\n", "\n", "with", "tf", ".", "control_dependencies", "(", "\n", "[", "tf", ".", "assert_equal", "(", "\n", "tf", ".", "shape", "(", "self", ".", "word_ids", ")", "[", "1", "]", ",", "\n", "tf", ".", "reduce_max", "(", "self", ".", "sentence_lengths", ")", ",", "\n", "message", "=", "\"Sentence dimensions don't match\"", ")", "]", ")", ":", "\n", "            ", "(", "lstm_outputs_fw", ",", "lstm_outputs_bw", ")", ",", "(", "(", "_", ",", "lstm_output_fw", ")", ",", "(", "_", ",", "lstm_output_bw", ")", ")", "=", "tf", ".", "nn", ".", "bidirectional_dynamic_rnn", "(", "\n", "cell_fw", "=", "word_lstm_cell_fw", ",", "cell_bw", "=", "word_lstm_cell_bw", ",", "inputs", "=", "word_input_tensor", ",", "\n", "sequence_length", "=", "self", ".", "sentence_lengths", ",", "dtype", "=", "tf", ".", "float32", ",", "time_major", "=", "False", ")", "\n", "\n", "", "lstm_output_states", "=", "tf", ".", "concat", "(", "[", "lstm_output_fw", ",", "lstm_output_bw", "]", ",", "axis", "=", "-", "1", ")", "\n", "\n", "if", "self", ".", "config", "[", "\"dropout_word_lstm\"", "]", ">", "0.0", ":", "\n", "            ", "dropout_word_lstm", "=", "(", "self", ".", "config", "[", "\"dropout_word_lstm\"", "]", "*", "tf", ".", "cast", "(", "self", ".", "is_training", ",", "tf", ".", "float32", ")", "\n", "+", "(", "1.0", "-", "tf", ".", "cast", "(", "self", ".", "is_training", ",", "tf", ".", "float32", ")", ")", ")", "\n", "lstm_outputs_fw", "=", "tf", ".", "nn", ".", "dropout", "(", "\n", "lstm_outputs_fw", ",", "dropout_word_lstm", ",", "\n", "noise_shape", "=", "tf", ".", "convert_to_tensor", "(", "\n", "[", "tf", ".", "shape", "(", "self", ".", "word_ids", ")", "[", "0", "]", ",", "1", ",", "self", ".", "config", "[", "\"word_recurrent_size\"", "]", "]", ",", "dtype", "=", "tf", ".", "int32", ")", ")", "\n", "lstm_outputs_bw", "=", "tf", ".", "nn", ".", "dropout", "(", "\n", "lstm_outputs_bw", ",", "dropout_word_lstm", ",", "\n", "noise_shape", "=", "tf", ".", "convert_to_tensor", "(", "\n", "[", "tf", ".", "shape", "(", "self", ".", "word_ids", ")", "[", "0", "]", ",", "1", ",", "self", ".", "config", "[", "\"word_recurrent_size\"", "]", "]", ",", "dtype", "=", "tf", ".", "int32", ")", ")", "\n", "lstm_output_states", "=", "tf", ".", "nn", ".", "dropout", "(", "lstm_output_states", ",", "dropout_word_lstm", ")", "\n", "\n", "# The forward and backward states are concatenated at every token position.", "\n", "", "lstm_outputs_states", "=", "tf", ".", "concat", "(", "[", "lstm_outputs_fw", ",", "lstm_outputs_bw", "]", ",", "axis", "=", "-", "1", ")", "\n", "\n", "if", "self", ".", "config", "[", "\"whidden_layer_size\"", "]", ">", "0", ":", "\n", "            ", "lstm_outputs_states", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "lstm_outputs_states", ",", "self", ".", "config", "[", "\"whidden_layer_size\"", "]", ",", "\n", "activation", "=", "tf", ".", "tanh", ",", "kernel_initializer", "=", "self", ".", "initializer", ")", "\n", "\n", "", "if", "\"last\"", "in", "self", ".", "config", "[", "\"model_type\"", "]", ":", "\n", "            ", "processed_tensor", "=", "lstm_output_states", "\n", "token_scores", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "lstm_outputs_states", ",", "units", "=", "len", "(", "self", ".", "label2id_tok", ")", ",", "\n", "kernel_initializer", "=", "self", ".", "initializer", ",", "\n", "name", "=", "\"token_scores_last_lstm_outputs_ff\"", ")", "\n", "if", "self", ".", "config", "[", "\"hidden_layer_size\"", "]", ">", "0", ":", "\n", "                ", "processed_tensor", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "processed_tensor", ",", "units", "=", "self", ".", "config", "[", "\"hidden_layer_size\"", "]", ",", "\n", "activation", "=", "tf", ".", "tanh", ",", "kernel_initializer", "=", "self", ".", "initializer", ")", "\n", "", "sentence_scores", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "processed_tensor", ",", "units", "=", "len", "(", "self", ".", "label2id_sent", ")", ",", "\n", "kernel_initializer", "=", "self", ".", "initializer", ",", "\n", "name", "=", "\"sentence_scores_last_lstm_outputs_ff\"", ")", "\n", "", "elif", "\"attention\"", "in", "self", ".", "config", "[", "\"model_type\"", "]", ":", "\n", "            ", "with", "tf", ".", "variable_scope", "(", "\"attention\"", ")", ":", "\n", "                ", "num_heads", "=", "len", "(", "self", ".", "label2id_tok", ")", "\n", "num_sentence_labels", "=", "len", "(", "self", ".", "label2id_sent", ")", "\n", "num_units", "=", "lstm_outputs_states", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "-", "1", "]", "\n", "if", "num_units", "%", "num_heads", "!=", "0", ":", "\n", "                    ", "num_units", "=", "ceil", "(", "num_units", "/", "num_heads", ")", "*", "num_heads", "\n", "inputs", "=", "tf", ".", "layers", ".", "dense", "(", "lstm_outputs_states", ",", "num_units", ")", "# [B, M, num_units]", "\n", "", "else", ":", "\n", "                    ", "inputs", "=", "lstm_outputs_states", "\n", "\n", "# Project the inputs to get the keys, queries and values.", "\n", "", "queries", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", ",", "num_units", ",", "activation", "=", "tf", ".", "tanh", ",", "\n", "kernel_initializer", "=", "self", ".", "initializer", ")", "# [B, M, num_units]", "\n", "queries", "=", "tf", ".", "math", ".", "reduce_mean", "(", "queries", ",", "axis", "=", "1", ")", "# [B, num_units]", "\n", "queries", "=", "tf", ".", "expand_dims", "(", "queries", ",", "axis", "=", "-", "1", ")", "# [B, num_units, 1]", "\n", "keys", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", ",", "num_units", ",", "activation", "=", "tf", ".", "nn", ".", "tanh", ",", "\n", "kernel_initializer", "=", "self", ".", "initializer", ")", "# [B, M, num_units]", "\n", "values", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", ",", "num_units", ",", "activation", "=", "tf", ".", "tanh", ",", "\n", "kernel_initializer", "=", "self", ".", "initializer", ")", "# [B, M, num_units]", "\n", "\n", "# Split and concat to get as many projections as num_heads.", "\n", "queries", "=", "tf", ".", "concat", "(", "\n", "tf", ".", "split", "(", "queries", ",", "num_heads", ",", "axis", "=", "1", ")", ",", "\n", "axis", "=", "0", ")", "# [B*num_heads, num_units/num_heads, 1]", "\n", "keys", "=", "tf", ".", "concat", "(", "\n", "tf", ".", "split", "(", "keys", ",", "num_heads", ",", "axis", "=", "2", ")", ",", "\n", "axis", "=", "0", ")", "# [B*num_heads, M, num_units/num_heads]", "\n", "values", "=", "tf", ".", "concat", "(", "\n", "tf", ".", "split", "(", "values", ",", "num_heads", ",", "axis", "=", "2", ")", ",", "\n", "axis", "=", "0", ")", "# [B*num_heads, M, num_units/num_heads]", "\n", "\n", "if", "self", ".", "config", "[", "\"regularize_queries\"", "]", ">", "0", ":", "\n", "                    ", "self", ".", "loss", "+=", "self", ".", "config", "[", "\"regularize_queries\"", "]", "*", "cosine_distance_loss", "(", "\n", "tf", ".", "concat", "(", "tf", ".", "split", "(", "tf", ".", "transpose", "(", "queries", ",", "[", "0", ",", "2", ",", "1", "]", ")", ",", "num_heads", ")", ",", "axis", "=", "1", ")", ",", "\n", "take_abs", "=", "self", ".", "config", "[", "\"take_abs\"", "]", "if", "\"take_abs\"", "in", "self", ".", "config", "else", "False", ")", "\n", "\n", "", "if", "self", ".", "config", "[", "\"regularize_keys\"", "]", ">", "0", ":", "\n", "                    ", "self", ".", "loss", "+=", "self", ".", "config", "[", "\"regularize_keys\"", "]", "*", "cosine_distance_loss", "(", "\n", "tf", ".", "concat", "(", "tf", ".", "split", "(", "tf", ".", "expand_dims", "(", "keys", ",", "axis", "=", "2", ")", ",", "num_heads", ")", ",", "axis", "=", "2", ")", ",", "\n", "take_abs", "=", "self", ".", "config", "[", "\"take_abs\"", "]", "if", "\"take_abs\"", "in", "self", ".", "config", "else", "False", ")", "\n", "\n", "", "if", "self", ".", "config", "[", "\"regularize_values\"", "]", ">", "0", ":", "\n", "                    ", "self", ".", "loss", "+=", "self", ".", "config", "[", "\"regularize_values\"", "]", "*", "cosine_distance_loss", "(", "\n", "tf", ".", "concat", "(", "tf", ".", "split", "(", "tf", ".", "expand_dims", "(", "values", ",", "axis", "=", "2", ")", ",", "num_heads", ")", ",", "axis", "=", "2", ")", ",", "\n", "take_abs", "=", "self", ".", "config", "[", "\"take_abs\"", "]", "if", "\"take_abs\"", "in", "self", ".", "config", "else", "False", ")", "\n", "\n", "# Multiply each key by its query to get the attention evidence scores.", "\n", "", "attention_evidence", "=", "tf", ".", "matmul", "(", "keys", ",", "queries", ")", "# [B*num_heads, M, 1]", "\n", "attention_evidence", "=", "tf", ".", "squeeze", "(", "attention_evidence", ",", "axis", "=", "-", "1", ")", "# [B*num_heads, M]", "\n", "\n", "# Obtain token scores from the attention evidence scores.", "\n", "token_scores", "=", "tf", ".", "concat", "(", "tf", ".", "split", "(", "\n", "tf", ".", "expand_dims", "(", "attention_evidence", ",", "axis", "=", "-", "1", ")", ",", "\n", "num_heads", ")", ",", "axis", "=", "2", ")", "# [B, M, num_heads]", "\n", "\n", "# Apply a non-linear layer to obtain (un-normalized) attention weights.", "\n", "if", "self", ".", "config", "[", "\"attention_activation\"", "]", "==", "\"sharp\"", ":", "\n", "                    ", "attention_weights", "=", "tf", ".", "exp", "(", "attention_evidence", ")", "\n", "", "elif", "self", ".", "config", "[", "\"attention_activation\"", "]", "==", "\"soft\"", ":", "\n", "                    ", "attention_weights", "=", "tf", ".", "sigmoid", "(", "attention_evidence", ")", "\n", "", "elif", "self", ".", "config", "[", "\"attention_activation\"", "]", "==", "\"linear\"", ":", "\n", "                    ", "attention_weights", "=", "attention_evidence", "\n", "", "else", ":", "\n", "                    ", "raise", "ValueError", "(", "\"Unknown/unsupported token scoring method: %s\"", "\n", "%", "self", ".", "config", "[", "\"attention_activation\"", "]", ")", "\n", "\n", "# Mask positions that are not valid.", "\n", "", "tiled_sentence_lengths", "=", "tf", ".", "tile", "(", "\n", "input", "=", "tf", ".", "sequence_mask", "(", "self", ".", "sentence_lengths", ")", ",", "\n", "multiples", "=", "[", "num_heads", ",", "1", "]", ")", "# [B*num_heads, M]", "\n", "attention_weights", "=", "tf", ".", "where", "(", "\n", "tiled_sentence_lengths", ",", "\n", "attention_weights", ",", "\n", "tf", ".", "zeros_like", "(", "attention_weights", ")", ")", "\n", "\n", "# Normalize attention weights.", "\n", "attention_weights", "/=", "tf", ".", "reduce_sum", "(", "\n", "attention_weights", ",", "axis", "=", "-", "1", ",", "keep_dims", "=", "True", ")", "# [B*num_heads, M]", "\n", "\n", "product", "=", "values", "*", "tf", ".", "expand_dims", "(", "attention_weights", ",", "axis", "=", "-", "1", ")", "# [B*num_heads, M, num_units/num_heads]", "\n", "product", "=", "tf", ".", "reduce_sum", "(", "product", ",", "axis", "=", "1", ")", "# [B*num_heads, num_units/num_heads]", "\n", "\n", "if", "self", ".", "config", "[", "\"regularize_sentence_repr\"", "]", ">", "0", ":", "\n", "                    ", "self", ".", "loss", "+=", "self", ".", "config", "[", "\"regularize_sentence_repr\"", "]", "*", "cosine_distance_loss", "(", "\n", "tf", ".", "concat", "(", "tf", ".", "split", "(", "tf", ".", "expand_dims", "(", "product", ",", "axis", "=", "1", ")", ",", "num_heads", ")", ",", "axis", "=", "1", ")", ",", "\n", "take_abs", "=", "self", ".", "config", "[", "\"take_abs\"", "]", "if", "\"take_abs\"", "in", "self", ".", "config", "else", "False", ")", "\n", "\n", "", "if", "self", ".", "config", "[", "\"hidden_layer_size\"", "]", ">", "0", ":", "\n", "                    ", "product", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", "=", "product", ",", "units", "=", "self", ".", "config", "[", "\"hidden_layer_size\"", "]", ",", "\n", "activation", "=", "tf", ".", "tanh", ",", "kernel_initializer", "=", "self", ".", "initializer", ")", "\n", "\n", "", "processed_tensor", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", "=", "product", ",", "units", "=", "1", ",", "\n", "kernel_initializer", "=", "self", ".", "initializer", ")", "# [B*num_heads, 1]", "\n", "\n", "processed_tensor", "=", "tf", ".", "concat", "(", "\n", "tf", ".", "split", "(", "processed_tensor", ",", "num_heads", ")", ",", "axis", "=", "1", ")", "# [B, num_heads]", "\n", "\n", "sentence_scores", "=", "processed_tensor", "\n", "if", "num_heads", "!=", "num_sentence_labels", ":", "\n", "                    ", "if", "num_sentence_labels", "==", "2", ":", "\n", "                        ", "default_sentence_score", "=", "tf", ".", "gather", "(", "\n", "processed_tensor", ",", "indices", "=", "[", "0", "]", ",", "axis", "=", "1", ")", "# [B, 1]", "\n", "maximum_non_default_sentence_score", "=", "tf", ".", "gather", "(", "\n", "processed_tensor", ",", "indices", "=", "list", "(", "\n", "range", "(", "1", ",", "num_heads", ")", ")", ",", "axis", "=", "1", ")", "# [B, num_heads-1]", "\n", "maximum_non_default_sentence_score", "=", "tf", ".", "reduce_max", "(", "\n", "maximum_non_default_sentence_score", ",", "axis", "=", "1", ",", "keep_dims", "=", "True", ")", "# [B, 1]", "\n", "sentence_scores", "=", "tf", ".", "concat", "(", "\n", "[", "default_sentence_score", ",", "maximum_non_default_sentence_score", "]", ",", "\n", "axis", "=", "-", "1", ",", "name", "=", "\"sentence_scores_concatenation\"", ")", "# [B, 2]", "\n", "", "else", ":", "\n", "                        ", "sentence_scores", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "processed_tensor", ",", "units", "=", "num_sentence_labels", ",", "\n", "kernel_initializer", "=", "self", ".", "initializer", ")", "# [B, num_sent_labels]", "\n", "", "", "", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unknown/unsupported model_type: %s.\"", "%", "self", ".", "config", "[", "\"model_type\"", "]", ")", "\n", "\n", "# Mask the token scores that do not fall in the range of the true sentence length.", "\n", "# Do this for each head (change shape from [B, M] to [B, M, num_heads]).", "\n", "", "tiled_sentence_lengths", "=", "tf", ".", "tile", "(", "\n", "input", "=", "tf", ".", "expand_dims", "(", "\n", "tf", ".", "sequence_mask", "(", "self", ".", "sentence_lengths", ")", ",", "axis", "=", "-", "1", ")", ",", "\n", "multiples", "=", "[", "1", ",", "1", ",", "len", "(", "self", ".", "label2id_tok", ")", "]", ")", "\n", "self", ".", "token_probabilities", "=", "tf", ".", "nn", ".", "softmax", "(", "token_scores", ",", "axis", "=", "-", "1", ")", "\n", "self", ".", "token_probabilities", "=", "tf", ".", "where", "(", "\n", "tiled_sentence_lengths", ",", "\n", "self", ".", "token_probabilities", ",", "\n", "tf", ".", "zeros_like", "(", "self", ".", "token_probabilities", ")", ")", "\n", "self", ".", "token_predictions", "=", "tf", ".", "argmax", "(", "self", ".", "token_probabilities", ",", "axis", "=", "2", ")", "\n", "\n", "self", ".", "sentence_probabilities", "=", "tf", ".", "nn", ".", "softmax", "(", "sentence_scores", ")", "\n", "self", ".", "sentence_predictions", "=", "tf", ".", "argmax", "(", "self", ".", "sentence_probabilities", ",", "axis", "=", "1", ")", "\n", "\n", "if", "self", ".", "config", "[", "\"word_objective_weight\"", "]", ">", "0", ":", "\n", "            ", "word_objective_loss", "=", "tf", ".", "nn", ".", "sparse_softmax_cross_entropy_with_logits", "(", "\n", "logits", "=", "token_scores", ",", "labels", "=", "tf", ".", "cast", "(", "self", ".", "word_labels", ",", "tf", ".", "int32", ")", ")", "\n", "word_objective_loss", "=", "tf", ".", "where", "(", "\n", "tf", ".", "sequence_mask", "(", "self", ".", "sentence_lengths", ")", ",", "\n", "word_objective_loss", ",", "\n", "tf", ".", "zeros_like", "(", "word_objective_loss", ")", ")", "\n", "self", ".", "loss", "+=", "self", ".", "config", "[", "\"word_objective_weight\"", "]", "*", "tf", ".", "reduce_sum", "(", "\n", "self", ".", "word_objective_weights", "*", "word_objective_loss", ")", "\n", "\n", "", "if", "self", ".", "config", "[", "\"sentence_objective_weight\"", "]", ">", "0", ":", "\n", "            ", "self", ".", "loss", "+=", "self", ".", "config", "[", "\"sentence_objective_weight\"", "]", "*", "tf", ".", "reduce_sum", "(", "\n", "self", ".", "sentence_objective_weights", "*", "\n", "tf", ".", "nn", ".", "sparse_softmax_cross_entropy_with_logits", "(", "\n", "logits", "=", "sentence_scores", ",", "labels", "=", "tf", ".", "cast", "(", "self", ".", "sentence_labels", ",", "tf", ".", "int32", ")", ")", ")", "\n", "\n", "", "max_over_token_heads", "=", "tf", ".", "reduce_max", "(", "self", ".", "token_probabilities", ",", "axis", "=", "1", ")", "# [B, H]", "\n", "one_hot_sentence_labels", "=", "tf", ".", "one_hot", "(", "\n", "tf", ".", "cast", "(", "self", ".", "sentence_labels", ",", "tf", ".", "int32", ")", ",", "\n", "depth", "=", "len", "(", "self", ".", "label2id_sent", ")", ")", "\n", "if", "self", ".", "config", "[", "\"enable_label_smoothing\"", "]", ":", "\n", "            ", "one_hot_sentence_labels_smoothed", "=", "label_smoothing", "(", "\n", "one_hot_sentence_labels", ",", "epsilon", "=", "self", ".", "config", "[", "\"smoothing_epsilon\"", "]", ")", "\n", "", "else", ":", "\n", "            ", "one_hot_sentence_labels_smoothed", "=", "one_hot_sentence_labels", "\n", "\n", "# At least one token has a label corresponding to the true sentence label.", "\n", "# This also pushes the other maximum heads towards (a smoothed) 0.", "\n", "", "if", "self", ".", "config", "[", "\"type1_attention_objective_weight\"", "]", ">", "0", ":", "\n", "            ", "this_max_over_token_heads", "=", "max_over_token_heads", "\n", "if", "len", "(", "self", ".", "label2id_tok", ")", "!=", "len", "(", "self", ".", "label2id_sent", ")", ":", "\n", "                ", "if", "len", "(", "self", ".", "label2id_sent", ")", "==", "2", ":", "\n", "                    ", "max_default_head", "=", "tf", ".", "gather", "(", "\n", "max_over_token_heads", ",", "indices", "=", "[", "0", "]", ",", "axis", "=", "-", "1", ")", "# [B, 1]", "\n", "max_non_default_head", "=", "tf", ".", "reduce_max", "(", "tf", ".", "gather", "(", "\n", "max_over_token_heads", ",", "indices", "=", "list", "(", "\n", "range", "(", "1", ",", "len", "(", "self", ".", "label2id_tok", ")", ")", ")", ",", "axis", "=", "-", "1", ")", ",", "\n", "axis", "=", "1", ",", "keep_dims", "=", "True", ")", "# [B, 1]", "\n", "this_max_over_token_heads", "=", "tf", ".", "concat", "(", "\n", "[", "max_default_head", ",", "max_non_default_head", "]", ",", "axis", "=", "-", "1", ")", "# [B, 2]", "\n", "", "else", ":", "\n", "                    ", "raise", "ValueError", "(", "\n", "\"Unsupported attention loss for num_heads != num_sent_lables \"", "\n", "\"and num_sentence_labels != 2.\"", ")", "\n", "", "", "self", ".", "loss", "+=", "self", ".", "config", "[", "\"type1_attention_objective_weight\"", "]", "*", "(", "\n", "tf", ".", "reduce_sum", "(", "self", ".", "sentence_objective_weights", "*", "tf", ".", "reduce_sum", "(", "tf", ".", "square", "(", "\n", "this_max_over_token_heads", "-", "one_hot_sentence_labels_smoothed", ")", ",", "axis", "=", "-", "1", ")", ")", ")", "\n", "\n", "# The predicted distribution over the token labels (heads) should be similar", "\n", "# to the predicted distribution over the sentence representations.", "\n", "", "if", "self", ".", "config", "[", "\"type2_attention_objective_weight\"", "]", ">", "0", ":", "\n", "            ", "all_sentence_scores_probabilities", "=", "tf", ".", "nn", ".", "softmax", "(", "processed_tensor", ")", "# [B, H]", "\n", "self", ".", "loss", "+=", "self", ".", "config", "[", "\"type2_attention_objective_weight\"", "]", "*", "(", "\n", "tf", ".", "reduce_sum", "(", "self", ".", "sentence_objective_weights", "*", "tf", ".", "reduce_sum", "(", "tf", ".", "square", "(", "\n", "max_over_token_heads", "-", "all_sentence_scores_probabilities", ")", ",", "axis", "=", "-", "1", ")", ")", ")", "\n", "\n", "# At least one token has a label corresponding to the true sentence label.", "\n", "", "if", "self", ".", "config", "[", "\"type3_attention_objective_weight\"", "]", ">", "0", ":", "\n", "            ", "this_max_over_token_heads", "=", "max_over_token_heads", "\n", "if", "len", "(", "self", ".", "label2id_tok", ")", "!=", "len", "(", "self", ".", "label2id_sent", ")", ":", "\n", "                ", "if", "len", "(", "self", ".", "label2id_sent", ")", "==", "2", ":", "\n", "                    ", "max_default_head", "=", "tf", ".", "gather", "(", "\n", "max_over_token_heads", ",", "indices", "=", "[", "0", "]", ",", "axis", "=", "-", "1", ")", "# [B, 1]", "\n", "max_non_default_head", "=", "tf", ".", "reduce_max", "(", "tf", ".", "gather", "(", "\n", "max_over_token_heads", ",", "indices", "=", "list", "(", "\n", "range", "(", "1", ",", "len", "(", "self", ".", "label2id_tok", ")", ")", ")", ",", "axis", "=", "-", "1", ")", ",", "\n", "axis", "=", "1", ",", "keep_dims", "=", "True", ")", "# [B, 1]", "\n", "this_max_over_token_heads", "=", "tf", ".", "concat", "(", "\n", "[", "max_default_head", ",", "max_non_default_head", "]", ",", "axis", "=", "-", "1", ")", "# [B, 2]", "\n", "", "else", ":", "\n", "                    ", "raise", "ValueError", "(", "\n", "\"Unsupported attention loss for num_heads != num_sent_lables \"", "\n", "\"and num_sentence_labels != 2.\"", ")", "\n", "", "", "self", ".", "loss", "+=", "self", ".", "config", "[", "\"type3_attention_objective_weight\"", "]", "*", "(", "\n", "tf", ".", "reduce_sum", "(", "self", ".", "sentence_objective_weights", "*", "tf", ".", "reduce_sum", "(", "tf", ".", "square", "(", "\n", "(", "this_max_over_token_heads", "*", "one_hot_sentence_labels", ")", "\n", "-", "one_hot_sentence_labels_smoothed", ")", ",", "axis", "=", "-", "1", ")", ")", ")", "\n", "\n", "# A sentence that has a default label, should only contain tokens labeled as default.", "\n", "", "if", "self", ".", "config", "[", "\"type4_attention_objective_weight\"", "]", ">", "0", ":", "\n", "            ", "default_head", "=", "tf", ".", "gather", "(", "self", ".", "token_probabilities", ",", "indices", "=", "[", "0", "]", ",", "axis", "=", "-", "1", ")", "# [B, M, 1]", "\n", "default_head", "=", "tf", ".", "squeeze", "(", "default_head", ",", "axis", "=", "-", "1", ")", "# [B, M]", "\n", "self", ".", "loss", "+=", "self", ".", "config", "[", "\"type4_attention_objective_weight\"", "]", "*", "(", "\n", "tf", ".", "reduce_sum", "(", "self", ".", "sentence_objective_weights", "*", "tf", ".", "cast", "(", "\n", "tf", ".", "equal", "(", "self", ".", "sentence_labels", ",", "0.0", ")", ",", "tf", ".", "float32", ")", "*", "tf", ".", "reduce_sum", "(", "\n", "tf", ".", "square", "(", "default_head", "-", "tf", ".", "ones_like", "(", "default_head", ")", ")", ",", "axis", "=", "-", "1", ")", ")", ")", "\n", "\n", "# Every sentence has at least one default label.", "\n", "", "if", "self", ".", "config", "[", "\"type5_attention_objective_weight\"", "]", ">", "0", ":", "\n", "            ", "default_head", "=", "tf", ".", "gather", "(", "self", ".", "token_probabilities", ",", "indices", "=", "[", "0", "]", ",", "axis", "=", "-", "1", ")", "# [B, M, 1]", "\n", "max_default_head", "=", "tf", ".", "reduce_max", "(", "tf", ".", "squeeze", "(", "default_head", ",", "axis", "=", "-", "1", ")", ",", "axis", "=", "-", "1", ")", "# [B]", "\n", "self", ".", "loss", "+=", "self", ".", "config", "[", "\"type5_attention_objective_weight\"", "]", "*", "(", "\n", "tf", ".", "reduce_sum", "(", "self", ".", "sentence_objective_weights", "*", "tf", ".", "square", "(", "\n", "max_default_head", "-", "tf", ".", "ones_like", "(", "max_default_head", ")", ")", ")", ")", "\n", "\n", "# Pairwise attention objective function.", "\n", "", "if", "self", ".", "config", "[", "\"type6_attention_objective_weight\"", "]", ">", "0", ":", "\n", "            ", "this_max_over_token_heads", "=", "max_over_token_heads", "\n", "if", "len", "(", "self", ".", "label2id_tok", ")", "!=", "len", "(", "self", ".", "label2id_sent", ")", ":", "\n", "                ", "if", "len", "(", "self", ".", "label2id_sent", ")", "==", "2", ":", "\n", "                    ", "max_default_head", "=", "tf", ".", "gather", "(", "\n", "max_over_token_heads", ",", "indices", "=", "[", "0", "]", ",", "axis", "=", "-", "1", ")", "# [B, 1]", "\n", "max_non_default_head", "=", "tf", ".", "reduce_max", "(", "tf", ".", "gather", "(", "\n", "max_over_token_heads", ",", "indices", "=", "list", "(", "\n", "range", "(", "1", ",", "len", "(", "self", ".", "label2id_tok", ")", ")", ")", ",", "axis", "=", "-", "1", ")", ",", "\n", "axis", "=", "1", ",", "keep_dims", "=", "True", ")", "# [B, 1]", "\n", "this_max_over_token_heads", "=", "tf", ".", "concat", "(", "\n", "[", "max_default_head", ",", "max_non_default_head", "]", ",", "axis", "=", "-", "1", ")", "# [B, 2]", "\n", "", "else", ":", "\n", "                    ", "raise", "ValueError", "(", "\n", "\"Unsupported attention loss for num_heads != num_sent_lables \"", "\n", "\"and num_sentence_labels != 2.\"", ")", "\n", "", "", "self", ".", "loss", "+=", "self", ".", "config", "[", "\"type6_attention_objective_weight\"", "]", "*", "(", "\n", "tf", ".", "losses", ".", "mean_pairwise_squared_error", "(", "\n", "labels", "=", "one_hot_sentence_labels_smoothed", ",", "\n", "predictions", "=", "this_max_over_token_heads", ")", ")", "\n", "\n", "# The distribution over tokens should be similar to the distribution over sentences.", "\n", "", "if", "self", ".", "config", "[", "\"type7_attention_objective_weight\"", "]", ">", "0", ":", "\n", "            ", "op_over_token_heads", "=", "tf", ".", "reduce_mean", "(", "self", ".", "token_probabilities", ",", "axis", "=", "1", ")", "# [B, H]", "\n", "distribution_over_tokens", "=", "tf", ".", "nn", ".", "softmax", "(", "op_over_token_heads", ")", "\n", "distribution_over_sentences", "=", "tf", ".", "nn", ".", "softmax", "(", "processed_tensor", ")", "# [B, H]", "\n", "self", ".", "loss", "+=", "self", ".", "config", "[", "\"type7_attention_objective_weight\"", "]", "*", "(", "\n", "tf", ".", "reduce_sum", "(", "self", ".", "sentence_objective_weights", "*", "tf", ".", "distributions", ".", "kl_divergence", "(", "\n", "distribution_a", "=", "tf", ".", "distributions", ".", "Categorical", "(", "distribution_over_sentences", ")", ",", "\n", "distribution_b", "=", "tf", ".", "distributions", ".", "Categorical", "(", "distribution_over_tokens", ")", ")", ")", ")", "\n", "\n", "# Include a word-based language modelling loss, LMw.", "\n", "", "if", "self", ".", "config", "[", "\"lm_cost_lstm_gamma\"", "]", ">", "0.0", ":", "\n", "            ", "self", ".", "loss", "+=", "self", ".", "config", "[", "\"lm_cost_lstm_gamma\"", "]", "*", "self", ".", "construct_lm_cost", "(", "\n", "input_tensor_fw", "=", "lstm_outputs_fw", ",", "\n", "input_tensor_bw", "=", "lstm_outputs_bw", ",", "\n", "sentence_lengths", "=", "self", ".", "sentence_lengths", ",", "\n", "target_ids", "=", "self", ".", "word_ids", ",", "\n", "lm_cost_type", "=", "\"separate\"", ",", "\n", "name", "=", "\"lm_cost_lstm_separate\"", ")", "\n", "\n", "", "if", "self", ".", "config", "[", "\"lm_cost_joint_lstm_gamma\"", "]", ">", "0.0", ":", "\n", "            ", "self", ".", "loss", "+=", "self", ".", "config", "[", "\"lm_cost_joint_lstm_gamma\"", "]", "*", "self", ".", "construct_lm_cost", "(", "\n", "input_tensor_fw", "=", "lstm_outputs_fw", ",", "\n", "input_tensor_bw", "=", "lstm_outputs_bw", ",", "\n", "sentence_lengths", "=", "self", ".", "sentence_lengths", ",", "\n", "target_ids", "=", "self", ".", "word_ids", ",", "\n", "lm_cost_type", "=", "\"joint\"", ",", "\n", "name", "=", "\"lm_cost_lstm_joint\"", ")", "\n", "\n", "", "self", ".", "train_op", "=", "self", ".", "construct_optimizer", "(", "\n", "opt_strategy", "=", "self", ".", "config", "[", "\"opt_strategy\"", "]", ",", "\n", "loss", "=", "self", ".", "loss", ",", "\n", "learning_rate", "=", "self", ".", "learning_rate", ",", "\n", "clip", "=", "self", ".", "config", "[", "\"clip\"", "]", ")", "\n", "print", "(", "\"Notwork built.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.construct_lm_cost": [[590, 636], ["tensorflow.variable_scope", "min", "tensorflow.where", "len", "tensorflow.greater_equal", "model.Model._construct_lm_cost", "model.Model._construct_lm_cost", "tensorflow.sequence_mask", "tensorflow.sequence_mask", "tensorflow.concat", "model.Model._construct_lm_cost", "ValueError", "tensorflow.zeros_like", "tensorflow.sequence_mask", "tensorflow.shape", "tensorflow.shape", "tensorflow.shape"], "methods", ["home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model._construct_lm_cost", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model._construct_lm_cost", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model._construct_lm_cost"], ["", "def", "construct_lm_cost", "(", "\n", "self", ",", "input_tensor_fw", ",", "input_tensor_bw", ",", "\n", "sentence_lengths", ",", "target_ids", ",", "lm_cost_type", ",", "name", ")", ":", "\n", "        ", "\"\"\"\n        Constructs the char/word-based language modelling objective.\n        \"\"\"", "\n", "with", "tf", ".", "variable_scope", "(", "name", ")", ":", "\n", "            ", "lm_cost_max_vocab_size", "=", "min", "(", "\n", "len", "(", "self", ".", "word2id", ")", ",", "self", ".", "config", "[", "\"lm_cost_max_vocab_size\"", "]", ")", "\n", "target_ids", "=", "tf", ".", "where", "(", "\n", "tf", ".", "greater_equal", "(", "target_ids", ",", "lm_cost_max_vocab_size", "-", "1", ")", ",", "\n", "x", "=", "(", "lm_cost_max_vocab_size", "-", "1", ")", "+", "tf", ".", "zeros_like", "(", "target_ids", ")", ",", "\n", "y", "=", "target_ids", ")", "\n", "cost", "=", "0.0", "\n", "if", "lm_cost_type", "==", "\"separate\"", ":", "\n", "                ", "lm_cost_fw_mask", "=", "tf", ".", "sequence_mask", "(", "\n", "sentence_lengths", ",", "maxlen", "=", "tf", ".", "shape", "(", "target_ids", ")", "[", "1", "]", ")", "[", ":", ",", "1", ":", "]", "\n", "lm_cost_bw_mask", "=", "tf", ".", "sequence_mask", "(", "\n", "sentence_lengths", ",", "maxlen", "=", "tf", ".", "shape", "(", "target_ids", ")", "[", "1", "]", ")", "[", ":", ",", ":", "-", "1", "]", "\n", "lm_cost_fw", "=", "self", ".", "_construct_lm_cost", "(", "\n", "input_tensor_fw", "[", ":", ",", ":", "-", "1", ",", ":", "]", ",", "\n", "lm_cost_max_vocab_size", ",", "\n", "lm_cost_fw_mask", ",", "\n", "target_ids", "[", ":", ",", "1", ":", "]", ",", "\n", "name", "=", "name", "+", "\"_fw\"", ")", "\n", "lm_cost_bw", "=", "self", ".", "_construct_lm_cost", "(", "\n", "input_tensor_bw", "[", ":", ",", "1", ":", ",", ":", "]", ",", "\n", "lm_cost_max_vocab_size", ",", "\n", "lm_cost_bw_mask", ",", "\n", "target_ids", "[", ":", ",", ":", "-", "1", "]", ",", "\n", "name", "=", "name", "+", "\"_bw\"", ")", "\n", "cost", "+=", "lm_cost_fw", "+", "lm_cost_bw", "\n", "", "elif", "lm_cost_type", "==", "\"joint\"", ":", "\n", "                ", "joint_input_tensor", "=", "tf", ".", "concat", "(", "\n", "[", "input_tensor_fw", "[", ":", ",", ":", "-", "2", ",", ":", "]", ",", "input_tensor_bw", "[", ":", ",", "2", ":", ",", ":", "]", "]", ",", "axis", "=", "-", "1", ")", "\n", "lm_cost_mask", "=", "tf", ".", "sequence_mask", "(", "\n", "sentence_lengths", ",", "maxlen", "=", "tf", ".", "shape", "(", "target_ids", ")", "[", "1", "]", ")", "[", ":", ",", "1", ":", "-", "1", "]", "\n", "cost", "+=", "self", ".", "_construct_lm_cost", "(", "\n", "joint_input_tensor", ",", "\n", "lm_cost_max_vocab_size", ",", "\n", "lm_cost_mask", ",", "\n", "target_ids", "[", ":", ",", "1", ":", "-", "1", "]", ",", "\n", "name", "=", "name", "+", "\"_joint\"", ")", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"Unknown lm_cost_type: %s.\"", "%", "lm_cost_type", ")", "\n", "", "return", "cost", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model._construct_lm_cost": [[637, 651], ["tensorflow.variable_scope", "tensorflow.layers.dense", "tensorflow.layers.dense", "tensorflow.nn.sparse_softmax_cross_entropy_with_logits", "tensorflow.where", "tensorflow.reduce_sum", "tensorflow.zeros_like"], "methods", ["None"], ["", "", "def", "_construct_lm_cost", "(", "\n", "self", ",", "input_tensor", ",", "lm_cost_max_vocab_size", ",", "\n", "lm_cost_mask", ",", "target_ids", ",", "name", ")", ":", "\n", "        ", "with", "tf", ".", "variable_scope", "(", "name", ")", ":", "\n", "            ", "lm_cost_hidden_layer", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", "=", "input_tensor", ",", "units", "=", "self", ".", "config", "[", "\"lm_cost_hidden_layer_size\"", "]", ",", "\n", "activation", "=", "tf", ".", "tanh", ",", "kernel_initializer", "=", "self", ".", "initializer", ")", "\n", "lm_cost_output", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "inputs", "=", "lm_cost_hidden_layer", ",", "units", "=", "lm_cost_max_vocab_size", ",", "\n", "kernel_initializer", "=", "self", ".", "initializer", ")", "\n", "lm_cost_loss", "=", "tf", ".", "nn", ".", "sparse_softmax_cross_entropy_with_logits", "(", "\n", "logits", "=", "lm_cost_output", ",", "labels", "=", "target_ids", ")", "\n", "lm_cost_loss", "=", "tf", ".", "where", "(", "lm_cost_mask", ",", "lm_cost_loss", ",", "tf", ".", "zeros_like", "(", "lm_cost_loss", ")", ")", "\n", "return", "tf", ".", "reduce_sum", "(", "lm_cost_loss", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.construct_optimizer": [[652, 673], ["tensorflow.train.AdadeltaOptimizer", "zip", "tensorflow.clip_by_global_norm", "tensorflow.train.GradientDescentOptimizer.apply_gradients", "tensorflow.train.GradientDescentOptimizer.minimize", "tensorflow.train.AdamOptimizer", "zip", "tensorflow.train.GradientDescentOptimizer", "ValueError", "tensorflow.train.GradientDescentOptimizer.compute_gradients"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "construct_optimizer", "(", "opt_strategy", ",", "loss", ",", "learning_rate", ",", "clip", ")", ":", "\n", "        ", "\"\"\"\n        Applies an optimization strategy to minimize the loss.\n        \"\"\"", "\n", "if", "opt_strategy", "==", "\"adadelta\"", ":", "\n", "            ", "optimizer", "=", "tf", ".", "train", ".", "AdadeltaOptimizer", "(", "learning_rate", "=", "learning_rate", ")", "\n", "", "elif", "opt_strategy", "==", "\"adam\"", ":", "\n", "            ", "optimizer", "=", "tf", ".", "train", ".", "AdamOptimizer", "(", "learning_rate", "=", "learning_rate", ")", "\n", "", "elif", "opt_strategy", "==", "\"sgd\"", ":", "\n", "            ", "optimizer", "=", "tf", ".", "train", ".", "GradientDescentOptimizer", "(", "learning_rate", "=", "learning_rate", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unknown optimisation strategy: %s.\"", "%", "opt_strategy", ")", "\n", "\n", "", "if", "clip", ">", "0.0", ":", "\n", "            ", "grads", ",", "vs", "=", "zip", "(", "*", "optimizer", ".", "compute_gradients", "(", "loss", ")", ")", "\n", "grads", ",", "gnorm", "=", "tf", ".", "clip_by_global_norm", "(", "grads", ",", "clip", ")", "\n", "train_op", "=", "optimizer", ".", "apply_gradients", "(", "zip", "(", "grads", ",", "vs", ")", ")", "\n", "", "else", ":", "\n", "            ", "train_op", "=", "optimizer", ".", "minimize", "(", "loss", ")", "\n", "", "return", "train_op", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.preload_word_embeddings": [[674, 698], ["set", "model.Model.session.run", "model.Model.session.run", "print", "open", "model.Model.word_embeddings.assign", "line.strip().split", "len", "len", "re.sub.lower", "re.sub", "numpy.array", "set.add", "line.strip"], "methods", ["None"], ["", "def", "preload_word_embeddings", "(", "self", ",", "embedding_path", ")", ":", "\n", "        ", "\"\"\"\n        Load the word embeddings in advance to get a feel\n        of the proportion of singletons in the dataset.\n        \"\"\"", "\n", "loaded_embeddings", "=", "set", "(", ")", "\n", "embedding_matrix", "=", "self", ".", "session", ".", "run", "(", "self", ".", "word_embeddings", ")", "\n", "with", "open", "(", "embedding_path", ")", "as", "f", ":", "\n", "            ", "for", "line", "in", "f", ":", "\n", "                ", "line_parts", "=", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "if", "len", "(", "line_parts", ")", "<=", "2", ":", "\n", "                    ", "continue", "\n", "", "w", "=", "line_parts", "[", "0", "]", "\n", "if", "self", ".", "config", "[", "\"lowercase\"", "]", ":", "\n", "                    ", "w", "=", "w", ".", "lower", "(", ")", "\n", "", "if", "self", ".", "config", "[", "\"replace_digits\"", "]", ":", "\n", "                    ", "w", "=", "re", ".", "sub", "(", "r'\\d'", ",", "'0'", ",", "w", ")", "\n", "", "if", "w", "in", "self", ".", "word2id", "and", "w", "not", "in", "loaded_embeddings", ":", "\n", "                    ", "word_id", "=", "self", ".", "word2id", "[", "w", "]", "\n", "embedding", "=", "numpy", ".", "array", "(", "line_parts", "[", "1", ":", "]", ")", "\n", "embedding_matrix", "[", "word_id", "]", "=", "embedding", "\n", "loaded_embeddings", ".", "add", "(", "w", ")", "\n", "", "", "", "self", ".", "session", ".", "run", "(", "self", ".", "word_embeddings", ".", "assign", "(", "embedding_matrix", ")", ")", "\n", "print", "(", "\"No. of pre-loaded embeddings: %d.\"", "%", "len", "(", "loaded_embeddings", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.translate2id": [[699, 722], ["re.sub.lower", "re.sub", "numpy.random.uniform", "ValueError"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "translate2id", "(", "\n", "token", ",", "token2id", ",", "unk_token", "=", "None", ",", "lowercase", "=", "False", ",", "\n", "replace_digits", "=", "False", ",", "singletons", "=", "None", ",", "singletons_prob", "=", "0.0", ")", ":", "\n", "        ", "\"\"\"\n        Maps each token/character to its index.\n        \"\"\"", "\n", "if", "lowercase", ":", "\n", "            ", "token", "=", "token", ".", "lower", "(", ")", "\n", "", "if", "replace_digits", ":", "\n", "            ", "token", "=", "re", ".", "sub", "(", "r'\\d'", ",", "'0'", ",", "token", ")", "\n", "\n", "", "if", "singletons", "and", "token", "in", "singletons", "and", "token", "in", "token2id", "and", "unk_token", "and", "numpy", ".", "random", ".", "uniform", "(", ")", "<", "singletons_prob", ":", "\n", "            ", "token_id", "=", "token2id", "[", "unk_token", "]", "\n", "", "elif", "token", "in", "token2id", ":", "\n", "            ", "token_id", "=", "token2id", "[", "token", "]", "\n", "", "elif", "unk_token", ":", "\n", "            ", "token_id", "=", "token2id", "[", "unk_token", "]", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unable to handle value, no UNK token: %s.\"", "%", "token", ")", "\n", "", "return", "token_id", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.create_input_dictionary_for_batch": [[723, 797], ["numpy.array", "numpy.array.max", "numpy.array().max", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "enumerate", "min", "len", "len", "enumerate", "len", "numpy.array", "len", "len", "len", "len", "len", "model.Model.translate2id", "len", "range", "min", "model.Model.translate2id", "numpy.array().max", "len", "numpy.array", "len"], "methods", ["home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.translate2id", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.translate2id"], ["", "def", "create_input_dictionary_for_batch", "(", "self", ",", "batch", ",", "is_training", ",", "learning_rate", ")", ":", "\n", "        ", "\"\"\"\n        Creates the dictionary fed to the the TF model.\n        \"\"\"", "\n", "sentence_lengths", "=", "numpy", ".", "array", "(", "[", "len", "(", "sentence", ".", "tokens", ")", "for", "sentence", "in", "batch", "]", ")", "\n", "max_sentence_length", "=", "sentence_lengths", ".", "max", "(", ")", "\n", "max_word_length", "=", "numpy", ".", "array", "(", "\n", "[", "numpy", ".", "array", "(", "[", "len", "(", "token", ".", "value", ")", "for", "token", "in", "sentence", ".", "tokens", "]", ")", ".", "max", "(", ")", "\n", "for", "sentence", "in", "batch", "]", ")", ".", "max", "(", ")", "\n", "\n", "if", "0", "<", "self", ".", "config", "[", "\"allowed_word_length\"", "]", "<", "max_word_length", ":", "\n", "            ", "max_word_length", "=", "min", "(", "max_word_length", ",", "self", ".", "config", "[", "\"allowed_word_length\"", "]", ")", "\n", "\n", "", "word_ids", "=", "numpy", ".", "zeros", "(", "\n", "(", "len", "(", "batch", ")", ",", "max_sentence_length", ")", ",", "dtype", "=", "numpy", ".", "int32", ")", "\n", "char_ids", "=", "numpy", ".", "zeros", "(", "\n", "(", "len", "(", "batch", ")", ",", "max_sentence_length", ",", "max_word_length", ")", ",", "dtype", "=", "numpy", ".", "int32", ")", "\n", "word_lengths", "=", "numpy", ".", "zeros", "(", "\n", "(", "len", "(", "batch", ")", ",", "max_sentence_length", ")", ",", "dtype", "=", "numpy", ".", "int32", ")", "\n", "word_labels", "=", "numpy", ".", "zeros", "(", "\n", "(", "len", "(", "batch", ")", ",", "max_sentence_length", ")", ",", "dtype", "=", "numpy", ".", "float32", ")", "\n", "sentence_labels", "=", "numpy", ".", "zeros", "(", "\n", "(", "len", "(", "batch", ")", ")", ",", "dtype", "=", "numpy", ".", "float32", ")", "\n", "word_objective_weights", "=", "numpy", ".", "zeros", "(", "\n", "(", "len", "(", "batch", ")", ",", "max_sentence_length", ")", ",", "dtype", "=", "numpy", ".", "float32", ")", "\n", "sentence_objective_weights", "=", "numpy", ".", "zeros", "(", "(", "len", "(", "batch", ")", ")", ",", "dtype", "=", "numpy", ".", "float32", ")", "\n", "\n", "# A proportion of the singletons are assigned to UNK (do this just for training).", "\n", "singletons", "=", "self", ".", "singletons", "if", "is_training", "else", "None", "\n", "singletons_prob", "=", "self", ".", "config", "[", "\"singletons_prob\"", "]", "if", "is_training", "else", "0.0", "\n", "\n", "for", "i", ",", "sentence", "in", "enumerate", "(", "batch", ")", ":", "\n", "            ", "sentence_labels", "[", "i", "]", "=", "sentence", ".", "label_sent", "\n", "\n", "if", "sentence_labels", "[", "i", "]", "!=", "0", ":", "\n", "                ", "if", "self", ".", "config", "[", "\"sentence_objective_weights_non_default\"", "]", ">", "0.0", ":", "\n", "                    ", "sentence_objective_weights", "[", "i", "]", "=", "self", ".", "config", "[", "\n", "\"sentence_objective_weights_non_default\"", "]", "\n", "", "else", ":", "\n", "                    ", "sentence_objective_weights", "[", "i", "]", "=", "1.0", "\n", "", "", "else", ":", "\n", "                ", "sentence_objective_weights", "[", "i", "]", "=", "1.0", "\n", "\n", "", "for", "j", ",", "token", "in", "enumerate", "(", "sentence", ".", "tokens", ")", ":", "\n", "                ", "word_ids", "[", "i", "]", "[", "j", "]", "=", "self", ".", "translate2id", "(", "\n", "token", "=", "token", ".", "value", ",", "\n", "token2id", "=", "self", ".", "word2id", ",", "\n", "unk_token", "=", "self", ".", "UNK", ",", "\n", "lowercase", "=", "self", ".", "config", "[", "\"lowercase\"", "]", ",", "\n", "replace_digits", "=", "self", ".", "config", "[", "\"replace_digits\"", "]", ",", "\n", "singletons", "=", "singletons", ",", "\n", "singletons_prob", "=", "singletons_prob", ")", "\n", "word_labels", "[", "i", "]", "[", "j", "]", "=", "token", ".", "label_tok", "\n", "word_lengths", "[", "i", "]", "[", "j", "]", "=", "len", "(", "token", ".", "value", ")", "\n", "for", "k", "in", "range", "(", "min", "(", "len", "(", "token", ".", "value", ")", ",", "max_word_length", ")", ")", ":", "\n", "                    ", "char_ids", "[", "i", "]", "[", "j", "]", "[", "k", "]", "=", "self", ".", "translate2id", "(", "\n", "token", "=", "token", ".", "value", "[", "k", "]", ",", "\n", "token2id", "=", "self", ".", "char2id", ",", "\n", "unk_token", "=", "self", ".", "CUNK", ")", "\n", "", "if", "token", ".", "enable_supervision", "is", "True", ":", "\n", "                    ", "word_objective_weights", "[", "i", "]", "[", "j", "]", "=", "1.0", "\n", "\n", "", "", "", "input_dictionary", "=", "{", "\n", "self", ".", "word_ids", ":", "word_ids", ",", "\n", "self", ".", "char_ids", ":", "char_ids", ",", "\n", "self", ".", "sentence_lengths", ":", "sentence_lengths", ",", "\n", "self", ".", "word_lengths", ":", "word_lengths", ",", "\n", "self", ".", "sentence_labels", ":", "sentence_labels", ",", "\n", "self", ".", "word_labels", ":", "word_labels", ",", "\n", "self", ".", "word_objective_weights", ":", "word_objective_weights", ",", "\n", "self", ".", "sentence_objective_weights", ":", "sentence_objective_weights", ",", "\n", "self", ".", "learning_rate", ":", "learning_rate", ",", "\n", "self", ".", "is_training", ":", "is_training", "}", "\n", "return", "input_dictionary", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.process_batch": [[798, 813], ["model.Model.create_input_dictionary_for_batch", "model.Model.session.run"], "methods", ["home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.create_input_dictionary_for_batch"], ["", "def", "process_batch", "(", "self", ",", "batch", ",", "is_training", ",", "learning_rate", ")", ":", "\n", "        ", "\"\"\"\n        Processes a batch of sentences.\n        :param batch: a set of sentences of size \"max_batch_size\".\n        :param is_training: whether the current batch is a training instance or not.\n        :param learning_rate: the pace at which learning should be performed.\n        :return: the cost, the sentence predictions, the sentence label distribution,\n        the token predictions and the token label distribution.\n        \"\"\"", "\n", "feed_dict", "=", "self", ".", "create_input_dictionary_for_batch", "(", "batch", ",", "is_training", ",", "learning_rate", ")", "\n", "cost", ",", "sentence_pred", ",", "sentence_prob", ",", "token_pred", ",", "token_prob", "=", "self", ".", "session", ".", "run", "(", "\n", "[", "self", ".", "loss", ",", "self", ".", "sentence_predictions", ",", "self", ".", "sentence_probabilities", ",", "\n", "self", ".", "token_predictions", ",", "self", ".", "token_probabilities", "]", "+", "\n", "(", "[", "self", ".", "train_op", "]", "if", "is_training", "else", "[", "]", ")", ",", "feed_dict", "=", "feed_dict", ")", "[", ":", "5", "]", "\n", "return", "cost", ",", "sentence_pred", ",", "sentence_prob", ",", "token_pred", ",", "token_prob", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.initialize_session": [[814, 826], ["tensorflow.set_random_seed", "tensorflow.ConfigProto", "tensorflow.Session", "model.Model.session.run", "tensorflow.train.Saver", "tensorflow.global_variables_initializer"], "methods", ["None"], ["", "def", "initialize_session", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Initializes a tensorflow session and sets the random seed.\n        \"\"\"", "\n", "tf", ".", "set_random_seed", "(", "self", ".", "config", "[", "\"random_seed\"", "]", ")", "\n", "session_config", "=", "tf", ".", "ConfigProto", "(", ")", "\n", "session_config", ".", "gpu_options", ".", "allow_growth", "=", "self", ".", "config", "[", "\"tf_allow_growth\"", "]", "\n", "session_config", ".", "gpu_options", ".", "per_process_gpu_memory_fraction", "=", "self", ".", "config", "[", "\n", "\"tf_per_process_gpu_memory_fraction\"", "]", "\n", "self", ".", "session", "=", "tf", ".", "Session", "(", "config", "=", "session_config", ")", "\n", "self", ".", "session", ".", "run", "(", "tf", ".", "global_variables_initializer", "(", ")", ")", "\n", "self", ".", "saver", "=", "tf", ".", "train", ".", "Saver", "(", "max_to_keep", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.get_parameter_count": [[827, 840], ["tensorflow.trainable_variables", "variable.get_shape"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_parameter_count", "(", ")", ":", "\n", "        ", "\"\"\"\n        Counts the total number of parameters.\n        \"\"\"", "\n", "total_parameters", "=", "0", "\n", "for", "variable", "in", "tf", ".", "trainable_variables", "(", ")", ":", "\n", "            ", "shape", "=", "variable", ".", "get_shape", "(", ")", "\n", "variable_parameters", "=", "1", "\n", "for", "dim", "in", "shape", ":", "\n", "                ", "variable_parameters", "*=", "dim", ".", "value", "\n", "", "total_parameters", "+=", "variable_parameters", "\n", "", "return", "total_parameters", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.get_parameter_count_without_word_embeddings": [[841, 850], ["model.Model.word_embeddings.get_shape", "model.Model.get_parameter_count"], "methods", ["home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.get_parameter_count"], ["", "def", "get_parameter_count_without_word_embeddings", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Counts the number of parameters without those introduced by word embeddings.\n        \"\"\"", "\n", "shape", "=", "self", ".", "word_embeddings", ".", "get_shape", "(", ")", "\n", "variable_parameters", "=", "1", "\n", "for", "dim", "in", "shape", ":", "\n", "            ", "variable_parameters", "*=", "dim", ".", "value", "\n", "", "return", "self", ".", "get_parameter_count", "(", ")", "-", "variable_parameters", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.save": [[851, 873], ["dict", "tensorflow.global_variables", "model.Model.session.run", "open", "pickle.dump"], "methods", ["None"], ["", "def", "save", "(", "self", ",", "filename", ")", ":", "\n", "        ", "\"\"\"\n        Saves a trained model to the path in filename.\n        \"\"\"", "\n", "dump", "=", "dict", "(", ")", "\n", "dump", "[", "\"config\"", "]", "=", "self", ".", "config", "\n", "dump", "[", "\"label2id_sent\"", "]", "=", "self", ".", "label2id_sent", "\n", "dump", "[", "\"label2id_tok\"", "]", "=", "self", ".", "label2id_tok", "\n", "dump", "[", "\"UNK\"", "]", "=", "self", ".", "UNK", "\n", "dump", "[", "\"CUNK\"", "]", "=", "self", ".", "CUNK", "\n", "dump", "[", "\"word2id\"", "]", "=", "self", ".", "word2id", "\n", "dump", "[", "\"char2id\"", "]", "=", "self", ".", "char2id", "\n", "dump", "[", "\"singletons\"", "]", "=", "self", ".", "singletons", "\n", "\n", "dump", "[", "\"params\"", "]", "=", "{", "}", "\n", "for", "variable", "in", "tf", ".", "global_variables", "(", ")", ":", "\n", "            ", "assert", "(", "\n", "variable", ".", "name", "not", "in", "dump", "[", "\"params\"", "]", ")", ",", "\"Error: variable with this name already exists: %s.\"", "%", "variable", ".", "name", "\n", "dump", "[", "\"params\"", "]", "[", "variable", ".", "name", "]", "=", "self", ".", "session", ".", "run", "(", "variable", ")", "\n", "", "with", "open", "(", "filename", ",", "'wb'", ")", "as", "f", ":", "\n", "            ", "pickle", ".", "dump", "(", "dump", ",", "f", ",", "protocol", "=", "pickle", ".", "HIGHEST_PROTOCOL", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.load": [[874, 901], ["open", "pickle.load", "model.Model", "model.Model.construct_network", "model.Model.initialize_session", "model.Model.load_params"], "methods", ["home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.load", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.construct_network", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.initialize_session", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.load_params"], ["", "", "@", "staticmethod", "\n", "def", "load", "(", "filename", ",", "new_config", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Loads a pre-trained MHAL model.\n        \"\"\"", "\n", "with", "open", "(", "filename", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "dump", "=", "pickle", ".", "load", "(", "f", ")", "\n", "dump", "[", "\"config\"", "]", "[", "\"save\"", "]", "=", "None", "\n", "\n", "# Use the saved config, except for values that are present in the new config.", "\n", "if", "new_config", ":", "\n", "                ", "for", "key", "in", "new_config", ":", "\n", "                    ", "dump", "[", "\"config\"", "]", "[", "key", "]", "=", "new_config", "[", "key", "]", "\n", "\n", "", "", "labeler", "=", "Model", "(", "dump", "[", "\"config\"", "]", ",", "dump", "[", "\"label2id_sent\"", "]", ",", "dump", "[", "\"label2id_tok\"", "]", ")", "\n", "labeler", ".", "UNK", "=", "dump", "[", "\"UNK\"", "]", "\n", "labeler", ".", "CUNK", "=", "dump", "[", "\"CUNK\"", "]", "\n", "labeler", ".", "word2id", "=", "dump", "[", "\"word2id\"", "]", "\n", "labeler", ".", "char2id", "=", "dump", "[", "\"char2id\"", "]", "\n", "labeler", ".", "singletons", "=", "dump", "[", "\"singletons\"", "]", "\n", "\n", "labeler", ".", "construct_network", "(", ")", "\n", "labeler", ".", "initialize_session", "(", ")", "\n", "\n", "labeler", ".", "load_params", "(", "filename", ")", "\n", "\n", "return", "labeler", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.load_params": [[902, 918], ["open", "pickle.load", "tensorflow.global_variables", "numpy.asarray", "model.Model.session.run", "variable.assign", "str", "str"], "methods", ["home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.model.Model.load"], ["", "", "def", "load_params", "(", "self", ",", "filename", ")", ":", "\n", "        ", "\"\"\"\n        Loads the parameters of a trained model.\n        \"\"\"", "\n", "with", "open", "(", "filename", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "dump", "=", "pickle", ".", "load", "(", "f", ")", "\n", "\n", "for", "variable", "in", "tf", ".", "global_variables", "(", ")", ":", "\n", "                ", "assert", "(", "variable", ".", "name", "in", "dump", "[", "\"params\"", "]", ")", ",", "\"Variable not in dump: %s.\"", "%", "variable", ".", "name", "\n", "assert", "(", "variable", ".", "shape", "==", "dump", "[", "\"params\"", "]", "[", "variable", ".", "name", "]", ".", "shape", ")", ",", "\"Variable shape not as expected: %s, of shape %s. %s\"", "%", "(", "\n", "variable", ".", "name", ",", "str", "(", "variable", ".", "shape", ")", ",", "\n", "str", "(", "dump", "[", "\"params\"", "]", "[", "variable", ".", "name", "]", ".", "shape", ")", ")", "\n", "value", "=", "numpy", ".", "asarray", "(", "dump", "[", "\"params\"", "]", "[", "variable", ".", "name", "]", ")", "\n", "self", ".", "session", ".", "run", "(", "variable", ".", "assign", "(", "value", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.visualize.plot_token_scores": [[29, 64], ["len", "enumerate", "matplotlib.figure", "matplotlib.imshow", "matplotlib.xticks", "matplotlib.yticks", "matplotlib.tight_layout", "range", "color_data.append", "range", "range", "matplotlib.savefig", "matplotlib.show", "row.append", "str", "range", "int", "time.time"], "function", ["None"], ["def", "plot_token_scores", "(", "\n", "token_probs", ",", "sentence", ",", "id2label_tok", ",", "\n", "plot_name", "=", "None", ",", "show", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Plot the (normalized) token scores onto a grid of heads.\n    :param token_probs: normalized token scores of shape [batch_size, num_heads].\n    :param sentence: contains all the tokens corresponding to the token probs.\n    :param id2label_tok: dictionary mapping ids to token labels.\n    :param plot_name: name of file where to save the plot. Doesn't save it if None.\n    :param show: whether to show or not the plot to the screen.\n    :return: Nothing, just plot the token scores.\n    \"\"\"", "\n", "sentence_length", "=", "len", "(", "sentence", ".", "tokens", ")", "\n", "token_probs", "=", "token_probs", "[", ":", "]", "[", ":", "sentence_length", "]", ".", "T", "\n", "(", "nrows", ",", "ncols", ")", "=", "token_probs", ".", "shape", "\n", "color_data", "=", "[", "]", "\n", "\n", "for", "i", ",", "[", "r", ",", "g", ",", "b", "]", "in", "enumerate", "(", "head_colours", "[", ":", "nrows", "]", ")", ":", "\n", "        ", "row", "=", "[", "]", "\n", "for", "j", "in", "range", "(", "ncols", ")", ":", "\n", "            ", "row", ".", "append", "(", "[", "r", ",", "g", ",", "b", ",", "token_probs", "[", "i", "]", "[", "j", "]", "]", ")", "\n", "", "color_data", ".", "append", "(", "row", ")", "\n", "\n", "", "plt", ".", "figure", "(", "figsize", "=", "(", "16", ",", "12", ")", ",", "dpi", "=", "100", ")", "\n", "row_labels", "=", "[", "\"O\"", "]", "+", "[", "str", "(", "id2label_tok", "[", "i", "+", "1", "]", ")", "for", "i", "in", "range", "(", "nrows", "-", "1", ")", "]", "\n", "col_labels", "=", "[", "token", ".", "value", "for", "token", "in", "sentence", ".", "tokens", "]", "\n", "plt", ".", "imshow", "(", "color_data", ",", "vmin", "=", "0", ",", "vmax", "=", "sentence_length", ")", "\n", "plt", ".", "xticks", "(", "range", "(", "ncols", ")", ",", "col_labels", ",", "rotation", "=", "45", ")", "\n", "plt", ".", "yticks", "(", "range", "(", "nrows", ")", ",", "row_labels", ")", "\n", "plt", ".", "tight_layout", "(", ")", "\n", "if", "plot_name", "is", "not", "None", ":", "\n", "        ", "plt", ".", "savefig", "(", "\"%s_%d.png\"", "%", "(", "plot_name", ",", "int", "(", "time", ".", "time", "(", ")", ")", ")", ",", "\n", "format", "=", "\"png\"", ",", "dpi", "=", "100", ",", "bbox_inches", "=", "'tight'", ",", "pad_inches", "=", "0", ")", "\n", "", "if", "show", ":", "\n", "        ", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.visualize.plot_predictions": [[66, 182], ["print", "print", "open", "html_file.write", "html_file.write", "html_file.write", "html_file.write", "html_file.write", "html_file.write", "html_file.write", "range", "html_file.write", "html_file.write", "html_file.write", "tqdm.tqdm", "html_file.write", "int", "html_file.write", "html_file.write", "html_file.write", "html_file.write", "html_file.write", "html_file.write", "len", "html_file.write", "zip", "zip", "time.time", "len", "all", "int", "html_file.write", "html_file.write", "zip", "html_file.write", "numpy.argmax", "all", "int", "html_file.write", "int", "int", "int", "int", "int", "int", "int", "int", "int", "str", "numpy.argmax", "int", "int", "int", "int", "int", "int", "int", "int", "int"], "function", ["home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Writer.write", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Writer.write", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Writer.write", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Writer.write", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Writer.write", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Writer.write", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Writer.write", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Writer.write", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Writer.write", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Writer.write", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Writer.write", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Writer.write", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Writer.write", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Writer.write", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Writer.write", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Writer.write", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Writer.write", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Writer.write", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Writer.write", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Writer.write", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Writer.write", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.experiment.Writer.write"], ["", "", "def", "plot_predictions", "(", "\n", "all_sentences", ",", "all_sentence_probs", ",", "all_token_probs", ",", "\n", "id2label_tok", ",", "html_name", ",", "sent_binary", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Writes a HTML file with the predictions at the sentence and token level.\n    :param all_sentences: list of all the sentences in all batches.\n    :param all_sentence_probs: a list of all the sentence probabilities in all batches;\n    each batch of sentence_prob has shape [B, num_sent_labels] and must contain normalized data.\n    :param all_token_probs: a list of all the token probabilities in all batches;\n    each batch of token_probs has shape [B, M, num_tok_labels] and must contain normalized data.\n    :param id2label_tok: dictionary mapping ids to token labels.\n    :param html_name: name of the html file that will be produced.\n    :param sent_binary: whether the sentence labels are binary or not. This is needed\n    to use different colours than the token labels if the sentence labels don't match\n    the token labels (for our purposes, this happens when the sentence labels are binary).\n    :return: Nothing, just saves a html file with the coloured predictions,\n     which you can see in your browser.\n    \"\"\"", "\n", "html_filename", "=", "\"%s_%d.html\"", "%", "(", "html_name", ",", "int", "(", "time", ".", "time", "(", ")", ")", ")", "\n", "print", "(", "\"Plotting predictions across all batches...\"", "\n", "\"Saving to html file %s\"", "%", "html_filename", ")", "\n", "with", "open", "(", "html_filename", ",", "\"w\"", ")", "as", "html_file", ":", "\n", "\n", "# Write the usual html file header.", "\n", "        ", "html_file", ".", "write", "(", "html_header", ")", "\n", "\n", "# Print a legend of the colours assigned to the sentence and token labels.", "\n", "html_file", ".", "write", "(", "' ============================== '", ")", "\n", "html_file", ".", "write", "(", "'<br>'", ")", "\n", "html_file", ".", "write", "(", "'LEGEND'", ")", "\n", "html_file", ".", "write", "(", "'<br>'", ")", "\n", "html_file", ".", "write", "(", "' ============================== '", ")", "\n", "html_file", ".", "write", "(", "'<br>'", ")", "\n", "if", "sent_binary", ":", "\n", "            ", "html_file", ".", "write", "(", "'Sentence labels to colours: '", ")", "\n", "[", "r", ",", "g", ",", "b", "]", "=", "head_colours_sent", "[", "0", "]", "\n", "html_file", ".", "write", "(", "\n", "'<font style=\"background: rgba(%d, %d, %d, %f)\"><b>%s</b></font>\\n'", "\n", "%", "(", "int", "(", "r", "*", "255", ")", ",", "int", "(", "g", "*", "255", ")", ",", "int", "(", "b", "*", "255", ")", ",", "\n", "1.0", ",", "\"DEFAULT\"", ")", ")", "\n", "[", "r", ",", "g", ",", "b", "]", "=", "head_colours_sent", "[", "1", "]", "\n", "html_file", ".", "write", "(", "\n", "'<font style=\"background: rgba(%d, %d, %d, %f)\"><b>%s</b></font>\\n'", "\n", "%", "(", "int", "(", "r", "*", "255", ")", ",", "int", "(", "g", "*", "255", ")", ",", "int", "(", "b", "*", "255", ")", ",", "\n", "1.0", ",", "\"NON-DEFAULT\"", ")", ")", "\n", "html_file", ".", "write", "(", "'<br>'", ")", "\n", "html_file", ".", "write", "(", "'Token labels to colours: '", ")", "\n", "", "else", ":", "\n", "            ", "html_file", ".", "write", "(", "'Sentence/Token labels to colours: '", ")", "\n", "", "for", "i", "in", "range", "(", "len", "(", "id2label_tok", ")", ")", ":", "\n", "            ", "[", "r", ",", "g", ",", "b", "]", "=", "head_colours", "[", "i", "]", "\n", "html_file", ".", "write", "(", "\n", "'<font style=\"background: rgba(%d, %d, %d, %f)\"><b>%s</b></font>\\n'", "\n", "%", "(", "int", "(", "r", "*", "255", ")", ",", "int", "(", "g", "*", "255", ")", ",", "int", "(", "b", "*", "255", ")", ",", "\n", "1.0", ",", "str", "(", "id2label_tok", "[", "i", "]", ")", ")", ")", "\n", "", "html_file", ".", "write", "(", "'<br>'", ")", "\n", "html_file", ".", "write", "(", "' ============================== '", ")", "\n", "html_file", ".", "write", "(", "'<br><br>'", ")", "\n", "\n", "# Go through each batch.", "\n", "for", "sentences", ",", "sentence_probs", ",", "token_probs", "in", "tqdm", "(", "zip", "(", "\n", "all_sentences", ",", "all_sentence_probs", ",", "all_token_probs", ")", ",", "\n", "total", "=", "len", "(", "all_sentences", ")", ")", ":", "\n", "\n", "# Go through each sentence in the batch.", "\n", "            ", "for", "sent", ",", "sent_prob", ",", "tok_probs_this_sent", "in", "zip", "(", "\n", "sentences", ",", "sentence_probs", ",", "token_probs", ")", ":", "\n", "\n", "                ", "assert", "all", "(", "0", "<=", "prob", "<=", "1", "for", "prob", "in", "sent_prob", ")", ",", "\"Passed sent_prob = %f which is not a valid probability!\"", "%", "sent_prob", "\n", "\n", "# Represent by colour the gold and the predicted sentence labels.", "\n", "predicted_sent_label", "=", "int", "(", "np", ".", "argmax", "(", "sent_prob", ")", ")", "\n", "gold_sent_label", "=", "sent", ".", "label_sent", "\n", "alpha_sent", "=", "sent_prob", "[", "predicted_sent_label", "]", "\n", "\n", "if", "sent_binary", ":", "\n", "                    ", "[", "r_pred", ",", "g_pred", ",", "b_pred", "]", "=", "head_colours_sent", "[", "predicted_sent_label", "]", "\n", "[", "r_gold", ",", "g_gold", ",", "b_gold", "]", "=", "head_colours_sent", "[", "gold_sent_label", "]", "\n", "", "else", ":", "\n", "                    ", "[", "r_pred", ",", "g_pred", ",", "b_pred", "]", "=", "head_colours", "[", "predicted_sent_label", "]", "\n", "[", "r_gold", ",", "g_gold", ",", "b_gold", "]", "=", "head_colours", "[", "gold_sent_label", "]", "\n", "\n", "", "html_file", ".", "write", "(", "\n", "'<font style=\"background: rgba(%d, %d, %d, %f)\">%s</font>\\n'", "\n", "%", "(", "int", "(", "r_pred", "*", "255", ")", ",", "int", "(", "g_pred", "*", "255", ")", ",", "int", "(", "b_pred", "*", "255", ")", ",", "\n", "alpha_sent", ",", "\"<b>PRED</b>\"", ")", ")", "\n", "html_file", ".", "write", "(", "\n", "'<font style=\"background: rgba(%d, %d, %d, %f)\">%s</font>\\n'", "\n", "%", "(", "int", "(", "r_gold", "*", "255", ")", ",", "int", "(", "g_gold", "*", "255", ")", ",", "int", "(", "b_gold", "*", "255", ")", ",", "\n", "0.9", ",", "\"<b>GOLD</b>\"", ")", ")", "\n", "\n", "# Write each token in the colour background of its most probable", "\n", "# head prediction. Incorrect predictions will be underlined.", "\n", "for", "token", ",", "tok_prob", "in", "zip", "(", "sent", ".", "tokens", ",", "tok_probs_this_sent", ")", ":", "\n", "\n", "                    ", "assert", "all", "(", "0", "<=", "prob", "<=", "1", "for", "prob", "in", "tok_prob", ")", ",", "\"Passed tok_prob = %f which is not a valid probability!\"", "%", "tok_prob", "\n", "\n", "predicted_head", "=", "int", "(", "np", ".", "argmax", "(", "tok_prob", ")", ")", "\n", "alpha_tok", "=", "tok_prob", "[", "predicted_head", "]", "\n", "[", "r", ",", "g", ",", "b", "]", "=", "head_colours", "[", "predicted_head", "]", "\n", "if", "predicted_head", "==", "token", ".", "label_tok", ":", "\n", "                        ", "token_html", "=", "\"%s\"", "%", "token", ".", "value", "\n", "", "else", ":", "\n", "                        ", "token_html", "=", "\"<u>%s</u>\"", "%", "token", ".", "value", "\n", "\n", "", "html_file", ".", "write", "(", "\n", "'<font style=\"background: rgba(%d, %d, %d, %f)\">%s</font>\\n'", "\n", "%", "(", "int", "(", "r", "*", "255", ")", ",", "int", "(", "g", "*", "255", ")", ",", "int", "(", "b", "*", "255", ")", ",", "\n", "alpha_tok", ",", "token_html", ")", ")", "\n", "", "html_file", ".", "write", "(", "'<br><br>'", ")", "\n", "", "", "html_file", ".", "write", "(", "html_footer", ")", "\n", "", "print", "(", "\"HTML visualizations: Done!\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.evaluator.Evaluator.__init__": [[13, 40], ["time.time", "label2id_sent.items", "label2id_tok.items", "evaluator.Evaluator.id2label_sent.keys", "evaluator.Evaluator.id2label_sent.keys", "evaluator.Evaluator.id2label_sent.keys", "evaluator.Evaluator.id2label_tok.keys", "evaluator.Evaluator.id2label_tok.keys", "evaluator.Evaluator.id2label_tok.keys"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "label2id_sent", ",", "label2id_tok", ",", "conll03_eval", ")", ":", "\n", "        ", "self", ".", "id2label_sent", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "label2id_sent", ".", "items", "(", ")", "}", "\n", "self", ".", "id2label_tok", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "label2id_tok", ".", "items", "(", ")", "}", "\n", "\n", "self", ".", "conll03_eval", "=", "conll03_eval", "\n", "self", ".", "conll_format", "=", "[", "]", "\n", "\n", "self", ".", "true_sent", "=", "[", "]", "\n", "self", ".", "pred_sent", "=", "[", "]", "\n", "self", ".", "true_tok", "=", "[", "]", "\n", "self", ".", "pred_tok", "=", "[", "]", "\n", "\n", "self", ".", "cost_sum", "=", "0.0", "\n", "self", ".", "count_sent", "=", "0.0", "\n", "self", ".", "correct_binary_sent", "=", "0.0", "\n", "self", ".", "count_tok", "=", "0.0", "\n", "self", ".", "correct_binary_tok", "=", "0.0", "\n", "\n", "self", ".", "sentence_predicted", "=", "{", "k", ":", "0.0", "for", "k", "in", "self", ".", "id2label_sent", ".", "keys", "(", ")", "}", "\n", "self", ".", "sentence_correct", "=", "{", "k", ":", "0.0", "for", "k", "in", "self", ".", "id2label_sent", ".", "keys", "(", ")", "}", "\n", "self", ".", "sentence_total", "=", "{", "k", ":", "0.0", "for", "k", "in", "self", ".", "id2label_sent", ".", "keys", "(", ")", "}", "\n", "\n", "self", ".", "token_predicted", "=", "{", "k", ":", "0.0", "for", "k", "in", "self", ".", "id2label_tok", ".", "keys", "(", ")", "}", "\n", "self", ".", "token_correct", "=", "{", "k", ":", "0.0", "for", "k", "in", "self", ".", "id2label_tok", ".", "keys", "(", ")", "}", "\n", "self", ".", "token_total", "=", "{", "k", ":", "0.0", "for", "k", "in", "self", ".", "id2label_tok", ".", "keys", "(", ")", "}", "\n", "\n", "self", ".", "start_time", "=", "time", ".", "time", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.evaluator.Evaluator.append_token_data_for_sentence": [[41, 71], ["len", "zip", "evaluator.Evaluator.true_tok.append", "evaluator.Evaluator.pred_tok.append", "evaluator.Evaluator.conll_format.append", "evaluator.Evaluator.conll_format.append"], "methods", ["None"], ["", "def", "append_token_data_for_sentence", "(", "self", ",", "tokens", ",", "true_labels_tok", ",", "pred_labels_tok", ")", ":", "\n", "        ", "\"\"\"\n        Gets statistical results for the tokens in a sentence.\n        \"\"\"", "\n", "self", ".", "count_tok", "+=", "len", "(", "true_labels_tok", ")", "\n", "\n", "# For each token, calculate the same metrics as for the sentence scores.", "\n", "for", "token", ",", "true_label", ",", "pred_label", "in", "zip", "(", "tokens", ",", "true_labels_tok", ",", "pred_labels_tok", ")", ":", "\n", "            ", "self", ".", "true_tok", ".", "append", "(", "true_label", ")", "\n", "self", ".", "pred_tok", ".", "append", "(", "pred_label", ")", "\n", "\n", "if", "true_label", "==", "pred_label", ":", "\n", "                ", "self", ".", "correct_binary_tok", "+=", "1.0", "# accuracy", "\n", "\n", "", "self", ".", "token_predicted", "[", "pred_label", "]", "+=", "1.0", "# TP + FP", "\n", "\n", "self", ".", "token_total", "[", "true_label", "]", "+=", "1.0", "# TP + FN", "\n", "\n", "if", "true_label", "==", "pred_label", ":", "\n", "                ", "self", ".", "token_correct", "[", "true_label", "]", "+=", "1.0", "# TP", "\n", "\n", "", "if", "self", ".", "conll03_eval", "is", "True", ":", "\n", "                ", "gold_token_label", "=", "self", ".", "id2label_tok", "[", "true_label", "]", "\n", "gold_token_label", "=", "\"B-\"", "+", "gold_token_label", "if", "true_label", "!=", "0", "else", "gold_token_label", "\n", "pred_token_label", "=", "self", ".", "id2label_tok", "[", "pred_label", "]", "\n", "pred_token_label", "=", "\"B-\"", "+", "pred_token_label", "if", "true_label", "!=", "0", "else", "pred_token_label", "\n", "self", ".", "conll_format", ".", "append", "(", "\n", "token", "+", "\"\\t\"", "+", "gold_token_label", "+", "\"\\t\"", "+", "pred_token_label", ")", "\n", "", "", "if", "self", ".", "conll03_eval", "is", "True", ":", "\n", "            ", "self", ".", "conll_format", ".", "append", "(", "\"\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.evaluator.Evaluator.append_data": [[72, 103], ["len", "enumerate", "evaluator.Evaluator.true_sent.append", "evaluator.Evaluator.pred_sent.append", "evaluator.Evaluator.append_token_data_for_sentence", "list", "len"], "methods", ["home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.evaluator.Evaluator.append_token_data_for_sentence"], ["", "", "def", "append_data", "(", "self", ",", "cost", ",", "batch", ",", "sentence_predictions", ",", "token_predictions", ")", ":", "\n", "        ", "\"\"\"\n        Gets statistical results for the sentence and token scores in a batch.\n        \"\"\"", "\n", "self", ".", "cost_sum", "+=", "cost", "\n", "self", ".", "count_sent", "+=", "len", "(", "batch", ")", "\n", "\n", "for", "i", ",", "sentence", "in", "enumerate", "(", "batch", ")", ":", "\n", "            ", "true_labels_tok", "=", "[", "token", ".", "label_tok", "for", "token", "in", "sentence", ".", "tokens", "]", "\n", "true_labels_sent", "=", "sentence", ".", "label_sent", "\n", "self", ".", "true_sent", ".", "append", "(", "true_labels_sent", ")", "\n", "self", ".", "pred_sent", ".", "append", "(", "sentence_predictions", "[", "i", "]", ")", "\n", "\n", "# Calculate accuracy.", "\n", "if", "true_labels_sent", "==", "sentence_predictions", "[", "i", "]", ":", "\n", "                ", "self", ".", "correct_binary_sent", "+=", "1.0", "\n", "\n", "# Calculate TP + FP.", "\n", "", "self", ".", "sentence_predicted", "[", "sentence_predictions", "[", "i", "]", "]", "+=", "1.0", "\n", "\n", "# Calculate TP + FN.", "\n", "self", ".", "sentence_total", "[", "true_labels_sent", "]", "+=", "1.0", "\n", "\n", "# Calculate TP.", "\n", "if", "true_labels_sent", "==", "sentence_predictions", "[", "i", "]", ":", "\n", "                ", "self", ".", "sentence_correct", "[", "true_labels_sent", "]", "+=", "1.0", "\n", "\n", "# Get the scores for the tokens in this sentence", "\n", "", "self", ".", "append_token_data_for_sentence", "(", "\n", "[", "token", ".", "value", "for", "token", "in", "sentence", ".", "tokens", "]", ",", "\n", "true_labels_tok", ",", "list", "(", "token_predictions", "[", "i", "]", ")", "[", ":", "len", "(", "true_labels_tok", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.evaluator.Evaluator.calculate_metrics": [[104, 118], ["None"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "calculate_metrics", "(", "correct", ",", "predicted", ",", "total", ")", ":", "\n", "        ", "\"\"\"\n        Calculates the basic metrics.\n        :param correct: the number of examples predicted as correct that are actually correct.\n        :param predicted: the number of examples predicted as correct.\n        :param total: the number of examples that are correct by the gold standard.\n        :return: the precision, recall, F1 and F05 scores\n        \"\"\"", "\n", "p", "=", "correct", "/", "predicted", "if", "predicted", "else", "0.0", "\n", "r", "=", "correct", "/", "total", "if", "total", "else", "0.0", "\n", "f", "=", "2.0", "*", "p", "*", "r", "/", "(", "p", "+", "r", ")", "if", "p", "+", "r", "else", "0.0", "\n", "f05", "=", "(", "1", "+", "0.5", "*", "0.5", ")", "*", "p", "*", "r", "/", "(", "0.5", "*", "0.5", "*", "p", "+", "r", ")", "if", "0.5", "*", "0.5", "*", "p", "+", "r", "else", "0.0", "\n", "return", "p", ",", "r", ",", "f", ",", "f05", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.evaluator.Evaluator.get_results": [[119, 291], ["collections.OrderedDict", "evaluator.Evaluator.id2label_sent.keys", "len", "len", "len", "len", "evaluator.Evaluator.calculate_metrics", "evaluator.Evaluator.calculate_metrics", "evaluator.Evaluator.calculate_metrics", "evaluator.Evaluator.id2label_sent.keys", "evaluator.Evaluator.id2label_sent.keys", "evaluator.Evaluator.id2label_sent.keys", "evaluator.Evaluator.id2label_sent.keys", "len", "len", "len", "len", "sum", "sum", "sum", "sum", "sum", "sum", "evaluator.Evaluator.id2label_tok.keys", "len", "len", "len", "len", "evaluator.Evaluator.calculate_metrics", "evaluator.Evaluator.calculate_metrics", "float", "float", "float", "float", "evaluator.Evaluator.id2label_sent.keys", "evaluator.Evaluator.id2label_sent.keys", "evaluator.Evaluator.id2label_sent.keys", "evaluator.Evaluator.id2label_sent.keys", "evaluator.Evaluator.sentence_correct.values", "evaluator.Evaluator.sentence_predicted.values", "evaluator.Evaluator.sentence_total.values", "evaluator.Evaluator.calculate_metrics", "evaluator.Evaluator.id2label_tok.keys", "evaluator.Evaluator.id2label_tok.keys", "evaluator.Evaluator.id2label_tok.keys", "evaluator.Evaluator.id2label_tok.keys", "len", "len", "len", "len", "sum", "sum", "sum", "sum", "sum", "sum", "conlleval.evaluate", "conlleval.metrics", "time.time", "float", "evaluator.Evaluator.id2label_tok.keys", "evaluator.Evaluator.id2label_tok.keys", "evaluator.Evaluator.id2label_tok.keys", "evaluator.Evaluator.id2label_tok.keys", "evaluator.Evaluator.token_correct.values", "evaluator.Evaluator.token_predicted.values", "evaluator.Evaluator.token_total.values", "float", "float", "evaluator.Evaluator.sentence_correct.items", "evaluator.Evaluator.sentence_predicted.items", "evaluator.Evaluator.sentence_total.items", "evaluator.Evaluator.token_correct.items", "evaluator.Evaluator.token_predicted.items", "evaluator.Evaluator.token_total.items"], "methods", ["home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.evaluator.Evaluator.calculate_metrics", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.evaluator.Evaluator.calculate_metrics", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.evaluator.Evaluator.calculate_metrics", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.evaluator.Evaluator.calculate_metrics", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.evaluator.Evaluator.calculate_metrics", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.evaluator.Evaluator.calculate_metrics", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.conlleval.evaluate", "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.conlleval.metrics"], ["", "def", "get_results", "(", "self", ",", "name", ",", "token_labels_available", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Gets the statistical results both at the sentence and at the token level.\n        :param name: train, dev or test (+ epoch number).\n        :param token_labels_available: whether there are token annotations.\n        :return: an ordered dictionary containing the collection of results.\n        \"\"\"", "\n", "results", "=", "OrderedDict", "(", ")", "\n", "\n", "results", "[", "\"name\"", "]", "=", "name", "\n", "results", "[", "\"cost_sum\"", "]", "=", "self", ".", "cost_sum", "\n", "results", "[", "\"cost_avg\"", "]", "=", "(", "self", ".", "cost_sum", "/", "float", "(", "self", ".", "count_sent", ")", "\n", "if", "self", ".", "count_sent", "else", "0.0", ")", "\n", "\n", "results", "[", "\"count_sent\"", "]", "=", "self", ".", "count_sent", "\n", "results", "[", "\"total_correct_sent\"", "]", "=", "self", ".", "correct_binary_sent", "\n", "results", "[", "\"accuracy_sent\"", "]", "=", "(", "self", ".", "correct_binary_sent", "/", "float", "(", "self", ".", "count_sent", ")", "\n", "if", "self", ".", "count_sent", "else", "0.0", ")", "\n", "\n", "# Calculate the micro and macro averages for the sentence predictions", "\n", "f_macro_sent", ",", "p_macro_sent", ",", "r_macro_sent", ",", "f05_macro_sent", "=", "0.0", ",", "0.0", ",", "0.0", ",", "0.0", "\n", "f_non_default_macro_sent", ",", "p_non_default_macro_sent", ",", "r_non_default_macro_sent", ",", "f05_non_default_macro_sent", "=", "0.0", ",", "0.0", ",", "0.0", ",", "0.0", "\n", "\n", "for", "key", "in", "self", ".", "id2label_sent", ".", "keys", "(", ")", ":", "\n", "            ", "p", ",", "r", ",", "f", ",", "f05", "=", "self", ".", "calculate_metrics", "(", "\n", "self", ".", "sentence_correct", "[", "key", "]", ",", "\n", "self", ".", "sentence_predicted", "[", "key", "]", ",", "\n", "self", ".", "sentence_total", "[", "key", "]", ")", "\n", "label", "=", "\"label=%s\"", "%", "self", ".", "id2label_sent", "[", "key", "]", "\n", "results", "[", "label", "+", "\"_predicted_sent\"", "]", "=", "self", ".", "sentence_predicted", "[", "key", "]", "\n", "results", "[", "label", "+", "\"_correct_sent\"", "]", "=", "self", ".", "sentence_correct", "[", "key", "]", "\n", "results", "[", "label", "+", "\"_total_sent\"", "]", "=", "self", ".", "sentence_total", "[", "key", "]", "\n", "results", "[", "label", "+", "\"_precision_sent\"", "]", "=", "p", "\n", "results", "[", "label", "+", "\"_recall_sent\"", "]", "=", "r", "\n", "results", "[", "label", "+", "\"_f-score_sent\"", "]", "=", "f", "\n", "results", "[", "label", "+", "\"_f05-score_sent\"", "]", "=", "f05", "\n", "p_macro_sent", "+=", "p", "\n", "r_macro_sent", "+=", "r", "\n", "f_macro_sent", "+=", "f", "\n", "f05_macro_sent", "+=", "f05", "\n", "if", "key", "!=", "0", ":", "\n", "                ", "p_non_default_macro_sent", "+=", "p", "\n", "r_non_default_macro_sent", "+=", "r", "\n", "f_non_default_macro_sent", "+=", "f", "\n", "f05_non_default_macro_sent", "+=", "f05", "\n", "\n", "", "", "p_macro_sent", "/=", "len", "(", "self", ".", "id2label_sent", ".", "keys", "(", ")", ")", "\n", "r_macro_sent", "/=", "len", "(", "self", ".", "id2label_sent", ".", "keys", "(", ")", ")", "\n", "f_macro_sent", "/=", "len", "(", "self", ".", "id2label_sent", ".", "keys", "(", ")", ")", "\n", "f05_macro_sent", "/=", "len", "(", "self", ".", "id2label_sent", ".", "keys", "(", ")", ")", "\n", "\n", "p_non_default_macro_sent", "/=", "(", "len", "(", "self", ".", "id2label_sent", ".", "keys", "(", ")", ")", "-", "1", ")", "\n", "r_non_default_macro_sent", "/=", "(", "len", "(", "self", ".", "id2label_sent", ".", "keys", "(", ")", ")", "-", "1", ")", "\n", "f_non_default_macro_sent", "/=", "(", "len", "(", "self", ".", "id2label_sent", ".", "keys", "(", ")", ")", "-", "1", ")", "\n", "f05_non_default_macro_sent", "/=", "(", "len", "(", "self", ".", "id2label_sent", ".", "keys", "(", ")", ")", "-", "1", ")", "\n", "\n", "p_micro_sent", ",", "r_micro_sent", ",", "f_micro_sent", ",", "f05_micro_sent", "=", "self", ".", "calculate_metrics", "(", "\n", "sum", "(", "self", ".", "sentence_correct", ".", "values", "(", ")", ")", ",", "\n", "sum", "(", "self", ".", "sentence_predicted", ".", "values", "(", ")", ")", ",", "\n", "sum", "(", "self", ".", "sentence_total", ".", "values", "(", ")", ")", ")", "\n", "\n", "p_non_default_micro_sent", ",", "r_non_default_micro_sent", ",", "f_non_default_micro_sent", ",", "f05_non_default_micro_sent", "=", "self", ".", "calculate_metrics", "(", "\n", "sum", "(", "[", "value", "for", "key", ",", "value", "in", "self", ".", "sentence_correct", ".", "items", "(", ")", "if", "key", "!=", "0", "]", ")", ",", "\n", "sum", "(", "[", "value", "for", "key", ",", "value", "in", "self", ".", "sentence_predicted", ".", "items", "(", ")", "if", "key", "!=", "0", "]", ")", ",", "\n", "sum", "(", "[", "value", "for", "key", ",", "value", "in", "self", ".", "sentence_total", ".", "items", "(", ")", "if", "key", "!=", "0", "]", ")", ")", "\n", "\n", "results", "[", "\"precision_macro_sent\"", "]", "=", "p_macro_sent", "\n", "results", "[", "\"recall_macro_sent\"", "]", "=", "r_macro_sent", "\n", "results", "[", "\"f-score_macro_sent\"", "]", "=", "f_macro_sent", "\n", "results", "[", "\"f05-score_macro_sent\"", "]", "=", "f05_macro_sent", "\n", "\n", "results", "[", "\"precision_micro_sent\"", "]", "=", "p_micro_sent", "\n", "results", "[", "\"recall_micro_sent\"", "]", "=", "r_micro_sent", "\n", "results", "[", "\"f-score_micro_sent\"", "]", "=", "f_micro_sent", "\n", "results", "[", "\"f05-score_micro_sent\"", "]", "=", "f05_micro_sent", "\n", "\n", "results", "[", "\"precision_non_default_macro_sent\"", "]", "=", "p_non_default_macro_sent", "\n", "results", "[", "\"recall_non_default_macro_sent\"", "]", "=", "r_non_default_macro_sent", "\n", "results", "[", "\"f-score_non_default_macro_sent\"", "]", "=", "f_non_default_macro_sent", "\n", "results", "[", "\"f05-score_non_default_macro_sent\"", "]", "=", "f05_non_default_macro_sent", "\n", "\n", "results", "[", "\"precision_non_default_micro_sent\"", "]", "=", "p_non_default_micro_sent", "\n", "results", "[", "\"recall_non_default_micro_sent\"", "]", "=", "r_non_default_micro_sent", "\n", "results", "[", "\"f-score_non_default_micro_sent\"", "]", "=", "f_non_default_micro_sent", "\n", "results", "[", "\"f05-score_non_default_micro_sent\"", "]", "=", "f05_non_default_micro_sent", "\n", "\n", "if", "token_labels_available", "or", "\"test\"", "in", "name", ":", "\n", "            ", "results", "[", "\"count_tok\"", "]", "=", "self", ".", "count_tok", "\n", "results", "[", "\"total_correct_tok\"", "]", "=", "self", ".", "correct_binary_tok", "\n", "results", "[", "\"accuracy_tok\"", "]", "=", "(", "self", ".", "correct_binary_tok", "/", "float", "(", "self", ".", "count_tok", ")", "\n", "if", "self", ".", "count_tok", "else", "0.0", ")", "\n", "\n", "# Calculate the micro and macro averages for the token predictions.", "\n", "f_tok_macro", ",", "p_tok_macro", ",", "r_tok_macro", ",", "f05_tok_macro", "=", "0.0", ",", "0.0", ",", "0.0", ",", "0.0", "\n", "f_non_default_macro_tok", ",", "p_non_default_macro_tok", ",", "r_non_default_macro_tok", ",", "f05_non_default_macro_tok", "=", "0.0", ",", "0.0", ",", "0.0", ",", "0.0", "\n", "\n", "for", "key", "in", "self", ".", "id2label_tok", ".", "keys", "(", ")", ":", "\n", "                ", "p", ",", "r", ",", "f", ",", "f05", "=", "self", ".", "calculate_metrics", "(", "\n", "self", ".", "token_correct", "[", "key", "]", ",", "self", ".", "token_predicted", "[", "key", "]", ",", "self", ".", "token_total", "[", "key", "]", ")", "\n", "label", "=", "\"label=%s\"", "%", "self", ".", "id2label_tok", "[", "key", "]", "\n", "results", "[", "label", "+", "\"_predicted_tok\"", "]", "=", "self", ".", "token_predicted", "[", "key", "]", "\n", "results", "[", "label", "+", "\"_correct_tok\"", "]", "=", "self", ".", "token_correct", "[", "key", "]", "\n", "results", "[", "label", "+", "\"_total_tok\"", "]", "=", "self", ".", "token_total", "[", "key", "]", "\n", "results", "[", "label", "+", "\"_precision_tok\"", "]", "=", "p", "\n", "results", "[", "label", "+", "\"_recall_tok\"", "]", "=", "r", "\n", "results", "[", "label", "+", "\"_f-score_tok\"", "]", "=", "f", "\n", "results", "[", "label", "+", "\"_tok_f05\"", "]", "=", "f05", "\n", "p_tok_macro", "+=", "p", "\n", "r_tok_macro", "+=", "r", "\n", "f_tok_macro", "+=", "f", "\n", "f05_tok_macro", "+=", "f05", "\n", "if", "key", "!=", "0", ":", "\n", "                    ", "p_non_default_macro_tok", "+=", "p", "\n", "r_non_default_macro_tok", "+=", "r", "\n", "f_non_default_macro_tok", "+=", "f", "\n", "f05_non_default_macro_tok", "+=", "f05", "\n", "\n", "", "", "p_tok_macro", "/=", "len", "(", "self", ".", "id2label_tok", ".", "keys", "(", ")", ")", "\n", "r_tok_macro", "/=", "len", "(", "self", ".", "id2label_tok", ".", "keys", "(", ")", ")", "\n", "f_tok_macro", "/=", "len", "(", "self", ".", "id2label_tok", ".", "keys", "(", ")", ")", "\n", "f05_tok_macro", "/=", "len", "(", "self", ".", "id2label_tok", ".", "keys", "(", ")", ")", "\n", "\n", "p_non_default_macro_tok", "/=", "(", "len", "(", "self", ".", "id2label_tok", ".", "keys", "(", ")", ")", "-", "1", ")", "\n", "r_non_default_macro_tok", "/=", "(", "len", "(", "self", ".", "id2label_tok", ".", "keys", "(", ")", ")", "-", "1", ")", "\n", "f_non_default_macro_tok", "/=", "(", "len", "(", "self", ".", "id2label_tok", ".", "keys", "(", ")", ")", "-", "1", ")", "\n", "f05_non_default_macro_tok", "/=", "(", "len", "(", "self", ".", "id2label_tok", ".", "keys", "(", ")", ")", "-", "1", ")", "\n", "\n", "p_tok_micro", ",", "r_tok_micro", ",", "f_tok_micro", ",", "f05_tok_micro", "=", "self", ".", "calculate_metrics", "(", "\n", "sum", "(", "self", ".", "token_correct", ".", "values", "(", ")", ")", ",", "\n", "sum", "(", "self", ".", "token_predicted", ".", "values", "(", ")", ")", ",", "\n", "sum", "(", "self", ".", "token_total", ".", "values", "(", ")", ")", ")", "\n", "\n", "p_non_default_micro_tok", ",", "r_non_default_micro_tok", ",", "f_non_default_micro_tok", ",", "f05_non_default_micro_tok", "=", "self", ".", "calculate_metrics", "(", "\n", "sum", "(", "[", "value", "for", "key", ",", "value", "in", "self", ".", "token_correct", ".", "items", "(", ")", "if", "key", "!=", "0", "]", ")", ",", "\n", "sum", "(", "[", "value", "for", "key", ",", "value", "in", "self", ".", "token_predicted", ".", "items", "(", ")", "if", "key", "!=", "0", "]", ")", ",", "\n", "sum", "(", "[", "value", "for", "key", ",", "value", "in", "self", ".", "token_total", ".", "items", "(", ")", "if", "key", "!=", "0", "]", ")", ")", "\n", "\n", "results", "[", "\"precision_macro_tok\"", "]", "=", "p_tok_macro", "\n", "results", "[", "\"recall_macro_tok\"", "]", "=", "r_tok_macro", "\n", "results", "[", "\"f-score_macro_tok\"", "]", "=", "f_tok_macro", "\n", "results", "[", "\"f05-score_macro_tok\"", "]", "=", "f05_tok_macro", "\n", "\n", "results", "[", "\"precision_micro_tok\"", "]", "=", "p_tok_micro", "\n", "results", "[", "\"recall_micro_tok\"", "]", "=", "r_tok_micro", "\n", "results", "[", "\"f-score_micro_tok\"", "]", "=", "f_tok_micro", "\n", "results", "[", "\"f05-score_micro_tok\"", "]", "=", "f05_tok_micro", "\n", "\n", "results", "[", "\"precision_non_default_macro_tok\"", "]", "=", "p_non_default_macro_tok", "\n", "results", "[", "\"recall_non_default_macro_tok\"", "]", "=", "r_non_default_macro_tok", "\n", "results", "[", "\"f-score_non_default_macro_tok\"", "]", "=", "f_non_default_macro_tok", "\n", "results", "[", "\"f05-score_non_default_macro_tok\"", "]", "=", "f05_non_default_macro_tok", "\n", "\n", "results", "[", "\"precision_non_default_micro_tok\"", "]", "=", "p_non_default_micro_tok", "\n", "results", "[", "\"recall_non_default_micro_tok\"", "]", "=", "r_non_default_micro_tok", "\n", "results", "[", "\"f-score_non_default_micro_tok\"", "]", "=", "f_non_default_micro_tok", "\n", "results", "[", "\"f05-score_non_default_micro_tok\"", "]", "=", "f05_non_default_micro_tok", "\n", "\n", "if", "self", ".", "id2label_tok", "is", "not", "None", "and", "self", ".", "conll03_eval", "is", "True", ":", "\n", "                ", "conll_counts", "=", "conlleval", ".", "evaluate", "(", "self", ".", "conll_format", ")", "\n", "conll_metrics_overall", ",", "conll_metrics_by_type", "=", "conlleval", ".", "metrics", "(", "conll_counts", ")", "\n", "results", "[", "\"conll_accuracy\"", "]", "=", "(", "float", "(", "conll_counts", ".", "correct_tags", ")", "\n", "/", "float", "(", "conll_counts", ".", "token_counter", ")", ")", "\n", "results", "[", "\"conll_p\"", "]", "=", "conll_metrics_overall", ".", "prec", "\n", "results", "[", "\"conll_r\"", "]", "=", "conll_metrics_overall", ".", "rec", "\n", "results", "[", "\"conll_f\"", "]", "=", "conll_metrics_overall", ".", "fscore", "\n", "\n", "", "", "results", "[", "\"time\"", "]", "=", "float", "(", "time", ".", "time", "(", ")", ")", "-", "float", "(", "self", ".", "start_time", ")", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.MirunaPislar_multi-head-attention-labeller.None.evaluator.Evaluator.get_results_nice_print": [[292, 313], ["print", "print", "print", "sklearn.metrics.classification_report", "print", "print", "print", "sklearn.metrics.classification_report", "numpy.array", "range", "numpy.array", "len", "range", "range", "len", "len", "range", "len"], "methods", ["None"], ["", "def", "get_results_nice_print", "(", "self", ",", "name", ",", "token_labels_available", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        This method is a wrapper around the statistical results already computed,\n        just to print them in a nicer format. Can also use it to check the basic metrics.\n        \"\"\"", "\n", "if", "self", ".", "true_sent", "and", "self", ".", "pred_sent", ":", "\n", "            ", "print", "(", "\"*\"", "*", "50", ")", "\n", "print", "(", "\"Sentence predictions: \"", ")", "\n", "print", "(", "classification_report", "(", "\n", "self", ".", "true_sent", ",", "self", ".", "pred_sent", ",", "digits", "=", "4", ",", "\n", "labels", "=", "np", ".", "array", "(", "range", "(", "len", "(", "self", ".", "id2label_sent", ")", ")", ")", ",", "\n", "target_names", "=", "[", "self", ".", "id2label_sent", "[", "i", "]", "for", "i", "in", "range", "(", "len", "(", "self", ".", "id2label_sent", ")", ")", "]", ")", ")", "\n", "\n", "", "if", "token_labels_available", "or", "\"test\"", "in", "name", ":", "\n", "            ", "if", "self", ".", "true_tok", "and", "self", ".", "pred_tok", ":", "\n", "                ", "print", "(", "\"*\"", "*", "50", ")", "\n", "print", "(", "\"Token predictions: \"", ")", "\n", "print", "(", "classification_report", "(", "\n", "self", ".", "true_tok", ",", "self", ".", "pred_tok", ",", "digits", "=", "4", ",", "\n", "labels", "=", "np", ".", "array", "(", "range", "(", "len", "(", "self", ".", "id2label_tok", ")", ")", ")", ",", "\n", "target_names", "=", "[", "self", ".", "id2label_tok", "[", "i", "]", "for", "i", "in", "range", "(", "len", "(", "self", ".", "id2label_tok", ")", ")", "]", ")", ")", "\n", "\n"]]}