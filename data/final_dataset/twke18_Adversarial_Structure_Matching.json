{"home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.utils.metrics.iou_stats": [[6, 35], ["numpy.logical_and", "numpy.histogram", "numpy.histogram", "numpy.logical_and", "numpy.histogram", "numpy.arange", "numpy.arange", "numpy.arange"], "function", ["None"], ["def", "iou_stats", "(", "pred", ",", "target", ",", "num_classes", "=", "21", ",", "background", "=", "0", ")", ":", "\n", "  ", "\"\"\"Computes statistics of true positive (TP), false negative (FN) and\n  false positive (FP).\n\n  Args:\n    pred: A numpy array.\n    target: A numpy array which should be in the same size as pred.\n    num_classes: A number indicating the number of valid classes.\n    background: A number indicating the class index of the back ground.\n\n  Returns:\n    Three num_classes-D vector indicating the statistics of (TP+FN), (TP+FP)\n    and TP across each class.\n  \"\"\"", "\n", "# Set redundant classes to background.", "\n", "locs", "=", "np", ".", "logical_and", "(", "target", ">", "-", "1", ",", "target", "<", "num_classes", ")", "\n", "\n", "# true positive + false negative", "\n", "tp_fn", ",", "_", "=", "np", ".", "histogram", "(", "target", "[", "locs", "]", ",", "\n", "bins", "=", "np", ".", "arange", "(", "num_classes", "+", "1", ")", ")", "\n", "# true positive + false positive", "\n", "tp_fp", ",", "_", "=", "np", ".", "histogram", "(", "pred", "[", "locs", "]", ",", "\n", "bins", "=", "np", ".", "arange", "(", "num_classes", "+", "1", ")", ")", "\n", "# true positive", "\n", "tp_locs", "=", "np", ".", "logical_and", "(", "locs", ",", "pred", "==", "target", ")", "\n", "tp", ",", "_", "=", "np", ".", "histogram", "(", "target", "[", "tp_locs", "]", ",", "\n", "bins", "=", "np", ".", "arange", "(", "num_classes", "+", "1", ")", ")", "\n", "\n", "return", "tp_fn", ",", "tp_fp", ",", "tp", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.utils.metrics.confusion_matrix": [[37, 58], ["numpy.zeros", "range", "mask.any", "numpy.histogram", "numpy.arange"], "function", ["None"], ["", "def", "confusion_matrix", "(", "pred", ",", "target", ",", "num_classes", "=", "21", ")", ":", "\n", "  ", "\"\"\"Computes the confusion matrix between prediction and ground-truth.\n\n  Args:\n    pred: A numpy array.\n    target: A numpy array which should be in the same size as pred.\n    num_classes: A number indicating the number of valid classes.\n\n  Returns:\n    A (num_classes)x(num_classes) 2-D array, in which each row denotes\n    ground-truth class, and each column represents predicted class.\n  \"\"\"", "\n", "mat", "=", "np", ".", "zeros", "(", "(", "num_classes", ",", "num_classes", ")", ")", "\n", "for", "c", "in", "range", "(", "num_classes", ")", ":", "\n", "    ", "mask", "=", "target", "==", "c", "\n", "if", "mask", ".", "any", "(", ")", ":", "\n", "      ", "vec", ",", "_", "=", "np", ".", "histogram", "(", "pred", "[", "mask", "]", ",", "\n", "bins", "=", "np", ".", "arange", "(", "num_classes", "+", "1", ")", ")", "\n", "mat", "[", "c", ",", ":", "]", "+=", "vec", "\n", "\n", "", "", "return", "mat", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.utils.metrics.accuracy": [[60, 74], ["None"], "function", ["None"], ["", "def", "accuracy", "(", "pred", ",", "target", ")", ":", "\n", "  ", "\"\"\"Computes pixel accuracy.\n\n  acc = true_positive / (true_positive + false_positive)\n\n  Args:\n    pred: A numpy array.\n    target: A numpy array which should be in the same size as pred.\n\n  Returns:\n    A number indicating the average accuracy.\n  \"\"\"", "\n", "N", "=", "pred", ".", "shape", "[", "0", "]", "\n", "return", "(", "pred", "==", "target", ")", ".", "sum", "(", ")", "*", "1.0", "/", "N", "\n", "", ""]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.utils.general.decode_labels": [[15, 31], ["numpy.zeros", "range"], "function", ["None"], ["def", "decode_labels", "(", "labels", ",", "num_classes", "=", "21", ")", ":", "\n", "  ", "\"\"\"Encodes label indices to color maps.\n    \n  Args:\n    labels: A tensor of size [batch_size, height_in, width_in, 1]\n    num_classes: A number indicating number of valid classes.\n    \n  Returns:\n    A tensor of size [batch_size, height_in, width_in, 3]\n  \"\"\"", "\n", "n", ",", "h", ",", "w", ",", "c", "=", "labels", ".", "shape", "\n", "outputs", "=", "np", ".", "zeros", "(", "(", "n", ",", "h", ",", "w", ",", "3", ")", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "for", "i", "in", "range", "(", "n", ")", ":", "\n", "    ", "outputs", "[", "i", "]", "=", "LABEL_COLORS", "[", "labels", "[", "i", ",", ":", ",", ":", ",", "0", "]", "]", "\n", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.utils.general.inv_preprocess": [[33, 51], ["numpy.zeros", "range"], "function", ["None"], ["", "def", "inv_preprocess", "(", "imgs", ",", "img_mean", ")", ":", "\n", "  ", "\"\"\"Inverses image preprocessing of the input images. \n  \n  This function adds back the mean vector and convert BGR to RGB.\n       \n  Args:\n    imgs: A tensor of size [batch_size, height_in, width_in, 3]\n    img_mean: A 1-D tensor indicating the vector of mean colour values.\n  \n  Returns:\n    A tensor of size [batch_size, height_in, width_in, 3]\n  \"\"\"", "\n", "n", ",", "h", ",", "w", ",", "c", "=", "imgs", ".", "shape", "\n", "outputs", "=", "np", ".", "zeros", "(", "(", "n", ",", "h", ",", "w", ",", "c", ")", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "for", "i", "in", "range", "(", "n", ")", ":", "\n", "    ", "outputs", "[", "i", "]", "=", "(", "imgs", "[", "i", "]", "+", "img_mean", ")", ".", "astype", "(", "np", ".", "uint8", ")", "\n", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.utils.general.snapshot_arg": [[53, 69], ["vars", "print", "print", "print", "print", "os.path.isdir", "os.makedirs", "open", "vars.items", "os.path.join", "print", "argsfile.write"], "function", ["None"], ["", "def", "snapshot_arg", "(", "args", ")", ":", "\n", "  ", "\"\"\"Print and snapshots Command-Line arguments to a text file.\n  \"\"\"", "\n", "snap_dir", "=", "args", ".", "snapshot_dir", "\n", "dictargs", "=", "vars", "(", "args", ")", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "snap_dir", ")", ":", "\n", "    ", "os", ".", "makedirs", "(", "snap_dir", ")", "\n", "", "print", "(", "'-----------------------------------------------'", ")", "\n", "print", "(", "'-----------------------------------------------'", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "snap_dir", ",", "'config'", ")", ",", "'w'", ")", "as", "argsfile", ":", "\n", "    ", "for", "key", ",", "val", "in", "dictargs", ".", "items", "(", ")", ":", "\n", "      ", "line", "=", "'| {0} = {1}'", ".", "format", "(", "key", ",", "val", ")", "\n", "print", "(", "line", ")", "\n", "argsfile", ".", "write", "(", "line", "+", "'\\n'", ")", "\n", "", "", "print", "(", "'-----------------------------------------------'", ")", "\n", "print", "(", "'-----------------------------------------------'", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.common.resnet_v1.bottleneck": [[6, 131], ["ValueError", "tensorflow.variable_scope", "network.conv", "network.conv", "tensorflow.add_n", "tensorflow.nn.relu", "ValueError", "nn.conv.get_shape().as_list", "network.conv", "network.atrous_conv", "network.conv", "network.max_pool", "nn.conv.get_shape"], "function", ["home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.fcrn.conv", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.fcrn.conv", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.fcrn.conv", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.common.layers.atrous_conv", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.fcrn.conv", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.common.layers.max_pool"], ["def", "bottleneck", "(", "x", ",", "\n", "name", ",", "\n", "filters", ",", "\n", "strides", "=", "None", ",", "\n", "dilation", "=", "None", ",", "\n", "is_training", "=", "True", ",", "\n", "use_global_status", "=", "True", ")", ":", "\n", "  ", "\"\"\"Builds the bottleneck module in ResNet.\n\n  This function stack 3 convolutional layers and fuse the output with\n  the residual connection.\n\n  Args:\n    x: A tensor of size [batch_size, height_in, width_in, channels].\n    name: The prefix of tensorflow variables defined in this layer.\n    filters: A number indicating the number of output channels.\n    strides: A number indicating the stride of the sliding window for\n      height and width.\n    dilation: A number indicating the dilation factor for height and width.\n    is_training: If the tensorflow variables defined in this layer \n      would be used for training.\n    use_global_status: enable/disable use_global_status for batch\n      normalization. If True, moving mean and moving variance are updated\n      by exponential decay.\n\n  Returns:\n    A tensor of size [batch_size, height_out, width_out, channels_out].\n  \"\"\"", "\n", "if", "strides", "is", "None", "and", "dilation", "is", "None", ":", "\n", "    ", "raise", "ValueError", "(", "'None of strides or dilation is specified, '", "\n", "+", "'set one of them to 1 or bigger number.'", ")", "\n", "", "elif", "strides", ">", "1", "and", "dilation", "is", "not", "None", "and", "dilation", ">", "1", ":", "\n", "    ", "raise", "ValueError", "(", "'strides and dilation are both specified, '", "\n", "+", "'set one of them to 1 or None.'", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "name", ")", "as", "scope", ":", "\n", "    ", "c_i", "=", "x", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "-", "1", "]", "\n", "\n", "if", "c_i", "!=", "filters", "*", "4", ":", "\n", "# Use a convolutional layer as residual connection when the", "\n", "# number of input channels is different from output channels.", "\n", "      ", "shortcut", "=", "nn", ".", "conv", "(", "x", ",", "\n", "name", "=", "'shortcut'", ",", "\n", "filters", "=", "filters", "*", "4", ",", "\n", "kernel_size", "=", "1", ",", "\n", "strides", "=", "strides", ",", "\n", "padding", "=", "'VALID'", ",", "\n", "biased", "=", "False", ",", "\n", "bn", "=", "True", ",", "\n", "relu", "=", "False", ",", "\n", "is_training", "=", "is_training", ",", "\n", "use_global_status", "=", "use_global_status", ")", "\n", "", "elif", "strides", ">", "1", ":", "\n", "# Use max-pooling as residual connection when the number of", "\n", "# input channel is same as output channels, but stride is ", "\n", "# larger than 1.", "\n", "      ", "shortcut", "=", "nn", ".", "max_pool", "(", "x", ",", "\n", "name", "=", "'shortcut'", ",", "\n", "kernel_size", "=", "1", ",", "\n", "strides", "=", "strides", ",", "\n", "padding", "=", "'VALID'", ")", "\n", "", "else", ":", "\n", "# Otherwise, keep the original input as residual connection.", "\n", "      ", "shortcut", "=", "x", "\n", "\n", "# Build the 1st convolutional layer.", "\n", "", "x", "=", "nn", ".", "conv", "(", "x", ",", "\n", "name", "=", "'conv1'", ",", "\n", "filters", "=", "filters", ",", "\n", "kernel_size", "=", "1", ",", "\n", "strides", "=", "1", ",", "\n", "padding", "=", "'SAME'", ",", "\n", "biased", "=", "False", ",", "\n", "bn", "=", "True", ",", "\n", "relu", "=", "True", ",", "\n", "is_training", "=", "is_training", ",", "\n", "use_global_status", "=", "use_global_status", ")", "\n", "\n", "if", "dilation", "is", "not", "None", "and", "dilation", ">", "1", ":", "\n", "# If dilation > 1, apply atrous conv to the 2nd convolutional layer.", "\n", "      ", "x", "=", "nn", ".", "atrous_conv", "(", "\n", "x", ",", "\n", "name", "=", "'conv2'", ",", "\n", "filters", "=", "filters", ",", "\n", "kernel_size", "=", "3", ",", "\n", "dilation", "=", "dilation", ",", "\n", "padding", "=", "'SAME'", ",", "\n", "biased", "=", "False", ",", "\n", "bn", "=", "True", ",", "\n", "relu", "=", "True", ",", "\n", "is_training", "=", "is_training", ",", "\n", "use_global_status", "=", "use_global_status", ")", "\n", "", "else", ":", "\n", "      ", "padding", "=", "'VALID'", "if", "strides", ">", "1", "else", "'SAME'", "\n", "x", "=", "nn", ".", "conv", "(", "\n", "x", ",", "\n", "name", "=", "'conv2'", ",", "\n", "filters", "=", "filters", ",", "\n", "kernel_size", "=", "3", ",", "\n", "strides", "=", "strides", ",", "\n", "padding", "=", "padding", ",", "\n", "biased", "=", "False", ",", "\n", "bn", "=", "True", ",", "\n", "relu", "=", "True", ",", "\n", "is_training", "=", "is_training", ",", "\n", "use_global_status", "=", "use_global_status", ")", "\n", "\n", "# Build the 3rd convolutional layer (increase the channels).", "\n", "", "x", "=", "nn", ".", "conv", "(", "x", ",", "\n", "name", "=", "'conv3'", ",", "\n", "filters", "=", "filters", "*", "4", ",", "\n", "kernel_size", "=", "1", ",", "\n", "strides", "=", "1", ",", "\n", "padding", "=", "'SAME'", ",", "\n", "biased", "=", "False", ",", "\n", "bn", "=", "True", ",", "\n", "relu", "=", "False", ",", "\n", "is_training", "=", "is_training", ",", "\n", "use_global_status", "=", "use_global_status", ")", "\n", "\n", "# Fuse the convolutional outputs with residual connection.", "\n", "x", "=", "tf", ".", "add_n", "(", "[", "x", ",", "shortcut", "]", ",", "name", "=", "'add'", ")", "\n", "x", "=", "tf", ".", "nn", ".", "relu", "(", "x", ",", "name", "=", "'relu'", ")", "\n", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.common.resnet_v1.resnet_v1": [[133, 206], ["ValueError", "tensorflow.variable_scope", "network.conv", "network.max_pool", "range", "len", "len", "len", "len", "len", "range", "name_format.format", "resnet_v1.bottleneck"], "function", ["home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.fcrn.conv", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.common.layers.max_pool", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.common.resnet_v1.bottleneck"], ["", "def", "resnet_v1", "(", "x", ",", "\n", "name", ",", "\n", "filters", "=", "[", "64", ",", "128", ",", "256", ",", "512", "]", ",", "\n", "num_blocks", "=", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "\n", "strides", "=", "[", "2", ",", "1", ",", "1", ",", "1", "]", ",", "\n", "dilations", "=", "[", "None", ",", "None", ",", "2", ",", "2", "]", ",", "\n", "is_training", "=", "True", ",", "\n", "use_global_status", "=", "True", ",", "\n", "reuse", "=", "False", ")", ":", "\n", "  ", "\"\"\"Helper function to build ResNet.\n\n  Args:\n    x: A tensor of size [batch_size, height_in, width_in, channels].\n    name: The prefix of tensorflow variables defined in this network.\n    filters: A list of numbers indicating the number of output channels\n      (The output channels would be 4 times to the numbers).\n    strides: A list of numbers indicating the stride of the sliding window for\n      height and width.\n    dilation: A number indicating the dilation factor for height and width.\n    is_training: If the tensorflow variables defined in this layer \n      would be used for training.\n    use_global_status: enable/disable use_global_status for batch\n      normalization. If True, moving mean and moving variance are updated\n      by exponential decay.\n    reuse: enable/disable reuse for reusing tensorflow variables. It is \n      useful for sharing weight parameters across two identical networks.\n\n  Returns:\n    A tensor of size [batch_size, height_out, width_out, channels_out].\n  \"\"\"", "\n", "if", "len", "(", "filters", ")", "!=", "len", "(", "num_blocks", ")", "or", "len", "(", "filters", ")", "!=", "len", "(", "strides", ")", ":", "\n", "    ", "raise", "ValueError", "(", "'length of lists are not consistent'", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "name", ",", "reuse", "=", "reuse", ")", "as", "scope", ":", "\n", "# Build conv1.", "\n", "    ", "x", "=", "nn", ".", "conv", "(", "x", ",", "\n", "name", "=", "'conv1'", ",", "\n", "filters", "=", "64", ",", "\n", "kernel_size", "=", "7", ",", "\n", "strides", "=", "2", ",", "\n", "padding", "=", "'VALID'", ",", "\n", "biased", "=", "False", ",", "\n", "bn", "=", "True", ",", "\n", "relu", "=", "True", ",", "\n", "is_training", "=", "is_training", ",", "\n", "use_global_status", "=", "use_global_status", ")", "\n", "\n", "# Build pool1.", "\n", "x", "=", "nn", ".", "max_pool", "(", "x", ",", "\n", "name", "=", "'pool1'", ",", "\n", "kernel_size", "=", "3", ",", "\n", "strides", "=", "2", ",", "\n", "padding", "=", "'VALID'", ")", "\n", "\n", "# Build residual bottleneck blocks.", "\n", "for", "ib", "in", "range", "(", "len", "(", "filters", ")", ")", ":", "\n", "      ", "for", "iu", "in", "range", "(", "num_blocks", "[", "ib", "]", ")", ":", "\n", "        ", "name_format", "=", "'block{:d}/unit_{:d}/bottleneck_v1'", "\n", "block_name", "=", "name_format", ".", "format", "(", "ib", "+", "1", ",", "iu", "+", "1", ")", "\n", "\n", "c_o", "=", "filters", "[", "ib", "]", "# output channel", "\n", "# Apply strides to the last block.", "\n", "s", "=", "strides", "[", "ib", "]", "if", "iu", "==", "num_blocks", "[", "ib", "]", "-", "1", "else", "1", "\n", "d", "=", "dilations", "[", "ib", "]", "\n", "x", "=", "bottleneck", "(", "x", ",", "\n", "name", "=", "block_name", ",", "\n", "filters", "=", "c_o", ",", "\n", "strides", "=", "s", ",", "\n", "dilation", "=", "d", ",", "\n", "is_training", "=", "is_training", ",", "\n", "use_global_status", "=", "use_global_status", ")", "\n", "\n", "", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.common.resnet_v1.resnet_v1_101": [[208, 238], ["resnet_v1.resnet_v1"], "function", ["home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.common.resnet_v1.resnet_v1"], ["", "", "def", "resnet_v1_101", "(", "x", ",", "\n", "name", ",", "\n", "is_training", ",", "\n", "use_global_status", ",", "\n", "reuse", "=", "False", ")", ":", "\n", "  ", "\"\"\"Builds ResNet101 v1.\n\n  Args:\n    x: A tensor of size [batch_size, height_in, width_in, channels].\n    name: The prefix of tensorflow variables defined in this network.\n    is_training: If the tensorflow variables defined in this layer \n      would be used for training.\n    use_global_status: enable/disable use_global_status for batch\n      normalization. If True, moving mean and moving variance are updated\n      by exponential decay.\n    reuse: enable/disable reuse for reusing tensorflow variables. It is \n      useful for sharing weight parameters across two identical networks.\n\n  Returns:\n    A tensor of size [batch_size, height_out, width_out, channels_out].\n  \"\"\"", "\n", "return", "resnet_v1", "(", "x", ",", "\n", "name", "=", "name", ",", "\n", "filters", "=", "[", "64", ",", "128", ",", "256", ",", "512", "]", ",", "\n", "num_blocks", "=", "[", "3", ",", "4", ",", "23", ",", "3", "]", ",", "\n", "strides", "=", "[", "2", ",", "1", ",", "1", ",", "1", "]", ",", "\n", "dilations", "=", "[", "None", ",", "None", ",", "2", ",", "4", "]", ",", "\n", "is_training", "=", "is_training", ",", "\n", "use_global_status", "=", "use_global_status", ",", "\n", "reuse", "=", "reuse", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.common.layers.batch_norm": [[7, 105], ["print", "tensorflow.variable_scope", "x.get_shape().as_list", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.nn.moments", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.assign", "tensorflow.assign", "tensorflow.nn.batch_normalization", "activation_fn", "x.get_shape", "tensorflow.constant_initializer", "tensorflow.constant_initializer", "tensorflow.constant_initializer", "tensorflow.constant_initializer", "numpy.arange", "tensorflow.control_dependencies", "tensorflow.nn.batch_normalization", "len", "tf.reshape.shape.as_list", "tf.reshape.shape.as_list"], "function", ["None"], ["def", "batch_norm", "(", "x", ",", "\n", "name", ",", "\n", "activation_fn", "=", "None", ",", "\n", "decay", "=", "0.99", ",", "\n", "epsilon", "=", "0.001", ",", "\n", "is_training", "=", "True", ")", ":", "\n", "  ", "\"\"\"Batch normalization.\n\n  This function perform batch normalization. If it is set for training,\n  it will update moving mean and moving variance to keep track of global\n  statistics by exponential decay.\n\n  output =  [(x - mean) / sqrt(var)] * gamma + beta.\n\n  Args:\n    x: A tensor of size [batch_size, height_in, width_in, channels].\n    name: The prefix of tensorflow variables defined in this layer.\n    activation_fn: The non-linear function, such as tf.nn.relu. If \n      activation_fn is None, skip it and maintain a linear activation.\n    decay: The exponential decay rate.\n    epsilon: Small float added to variance to avoid dividing by zero.\n    is_training: enable/disable is_training for updating moving mean and\n      moving variance by exponential decay. If True, compute batch mean\n      and batch variance per batch; otherwise, use moving mean and moving\n      variance as batch mean and batch variance.\n\n  Returns:\n    A tensor of size [batch_size, height_in, width_in, channels]\n  \"\"\"", "\n", "print", "(", "name", ",", "decay", ",", "is_training", ")", "\n", "with", "tf", ".", "variable_scope", "(", "name", ")", "as", "scope", ":", "\n", "    ", "shape_x", "=", "x", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "\n", "\n", "beta", "=", "tf", ".", "get_variable", "(", "\n", "'beta'", ",", "\n", "shape_x", "[", "-", "1", "]", ",", "\n", "initializer", "=", "tf", ".", "constant_initializer", "(", "0.0", ")", ",", "\n", "trainable", "=", "is_training", ")", "\n", "gamma", "=", "tf", ".", "get_variable", "(", "\n", "'gamma'", ",", "\n", "shape_x", "[", "-", "1", "]", ",", "\n", "initializer", "=", "tf", ".", "constant_initializer", "(", "1.0", ")", ",", "\n", "trainable", "=", "is_training", ")", "\n", "moving_mean", "=", "tf", ".", "get_variable", "(", "\n", "'moving_mean'", ",", "\n", "shape_x", "[", "-", "1", "]", ",", "\n", "initializer", "=", "tf", ".", "constant_initializer", "(", "0.0", ")", ",", "\n", "trainable", "=", "False", ")", "\n", "moving_var", "=", "tf", ".", "get_variable", "(", "\n", "'moving_variance'", ",", "\n", "shape_x", "[", "-", "1", "]", ",", "\n", "initializer", "=", "tf", ".", "constant_initializer", "(", "1.0", ")", ",", "\n", "trainable", "=", "False", ")", "\n", "\n", "if", "is_training", ":", "\n", "# Update moving mean and variance before", "\n", "# applying batch normalization", "\n", "      ", "mean", ",", "var", "=", "tf", ".", "nn", ".", "moments", "(", "x", ",", "\n", "np", ".", "arange", "(", "len", "(", "shape_x", ")", "-", "1", ")", ",", "\n", "keep_dims", "=", "True", ")", "\n", "mean", "=", "tf", ".", "reshape", "(", "mean", ",", "\n", "[", "mean", ".", "shape", ".", "as_list", "(", ")", "[", "-", "1", "]", "]", ")", "\n", "var", "=", "tf", ".", "reshape", "(", "var", ",", "\n", "[", "var", ".", "shape", ".", "as_list", "(", ")", "[", "-", "1", "]", "]", ")", "\n", "\n", "# Update moving mean and moving variance by exponential decay.", "\n", "update_moving_mean", "=", "tf", ".", "assign", "(", "\n", "moving_mean", ",", "\n", "moving_mean", "*", "decay", "+", "mean", "*", "(", "1", "-", "decay", ")", ")", "\n", "update_moving_var", "=", "tf", ".", "assign", "(", "\n", "moving_var", ",", "\n", "moving_var", "*", "decay", "+", "var", "*", "(", "1", "-", "decay", ")", ")", "\n", "update_ops", "=", "[", "update_moving_mean", ",", "update_moving_var", "]", "\n", "\n", "with", "tf", ".", "control_dependencies", "(", "update_ops", ")", ":", "\n", "        ", "output", "=", "tf", ".", "nn", ".", "batch_normalization", "(", "x", ",", "\n", "mean", ",", "\n", "var", ",", "\n", "beta", ",", "\n", "gamma", ",", "\n", "epsilon", ")", "\n", "", "", "else", ":", "\n", "# Use collected moving mean and moving variance for normalization.", "\n", "      ", "mean", "=", "moving_mean", "\n", "var", "=", "moving_var", "\n", "\n", "output", "=", "tf", ".", "nn", ".", "batch_normalization", "(", "x", ",", "\n", "mean", ",", "\n", "var", ",", "\n", "beta", ",", "\n", "gamma", ",", "\n", "epsilon", ")", "\n", "\n", "# Apply activation_fn, if it is not None.", "\n", "", "if", "activation_fn", ":", "\n", "      ", "output", "=", "activation_fn", "(", "output", ")", "\n", "\n", "", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.common.layers.conv": [[107, 192], ["tf.pad.get_shape().as_list", "tensorflow.nn.conv2d", "tensorflow.variable_scope", "tensorflow.get_variable", "convolve", "tensorflow.pad", "tensorflow.get_variable", "tensorflow.nn.bias_add", "layers.batch_norm", "tensorflow.nn.relu", "tf.pad.get_shape"], "function", ["home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.common.layers.batch_norm"], ["", "def", "conv", "(", "x", ",", "\n", "name", ",", "\n", "filters", ",", "\n", "kernel_size", ",", "\n", "strides", ",", "\n", "padding", ",", "\n", "relu", "=", "True", ",", "\n", "biased", "=", "True", ",", "\n", "bn", "=", "True", ",", "\n", "decay", "=", "0.9997", ",", "\n", "is_training", "=", "True", ",", "\n", "use_global_status", "=", "True", ")", ":", "\n", "  ", "\"\"\"Convolutional layers with batch normalization and ReLU.\n\n  This function perform convolution, batch_norm (if bn=True),\n  and ReLU (if relu=True).\n\n  Args:\n    x: A tensor of size [batch_size, height_in, width_in, channels].\n    name: The prefix of tensorflow variables defined in this layer.\n    filters: A number indicating the number of output channels.\n    kernel_size: A number indicating the size of convolutional kernels.\n    strides: A number indicating the stride of the sliding window for\n      height and width.\n    padding: 'VALID' or 'SAME'.\n    relu: enable/disable relu for ReLU as activation function. If relu \n      is False, maintain linear activation.\n    biased: enable/disable biased for adding biases after convolution.\n    bn: enable/disable bn for batch normalization.\n    decay: A number indication decay rate for updating moving mean and \n      moving variance in batch normalization.\n    is_training: If the tensorflow variables defined in this layer \n      would be used for training.\n    use_global_status: enable/disable use_global_status for batch\n      normalization. If True, moving mean and moving variance are updated\n      by exponential decay.\n\n  Returns:\n    A tensor of size [batch_size, height_out, width_out, channels_out].\n  \"\"\"", "\n", "c_i", "=", "x", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "-", "1", "]", "# input channels", "\n", "c_o", "=", "filters", "# output channels", "\n", "\n", "# Define helper function.", "\n", "convolve", "=", "lambda", "i", ",", "k", ":", "tf", ".", "nn", ".", "conv2d", "(", "\n", "i", ",", "\n", "k", ",", "\n", "[", "1", ",", "strides", ",", "strides", ",", "1", "]", ",", "\n", "padding", "=", "padding", ")", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "name", ")", "as", "scope", ":", "\n", "    ", "kernel", "=", "tf", ".", "get_variable", "(", "\n", "name", "=", "'weights'", ",", "\n", "shape", "=", "[", "kernel_size", ",", "kernel_size", ",", "c_i", ",", "c_o", "]", ",", "\n", "trainable", "=", "is_training", ")", "\n", "\n", "if", "strides", ">", "1", ":", "\n", "      ", "pad", "=", "kernel_size", "-", "1", "\n", "pad_beg", "=", "pad", "//", "2", "\n", "pad_end", "=", "pad", "-", "pad_beg", "\n", "pad_h", "=", "[", "pad_beg", ",", "pad_end", "]", "\n", "pad_w", "=", "[", "pad_beg", ",", "pad_end", "]", "\n", "x", "=", "tf", ".", "pad", "(", "x", ",", "[", "[", "0", ",", "0", "]", ",", "pad_h", ",", "pad_w", ",", "[", "0", ",", "0", "]", "]", ")", "\n", "\n", "", "output", "=", "convolve", "(", "x", ",", "kernel", ")", "\n", "\n", "# Add the biases.", "\n", "if", "biased", ":", "\n", "      ", "biases", "=", "tf", ".", "get_variable", "(", "'biases'", ",", "[", "c_o", "]", ",", "trainable", "=", "is_training", ")", "\n", "output", "=", "tf", ".", "nn", ".", "bias_add", "(", "output", ",", "biases", ")", "\n", "\n", "# Apply batch normalization.", "\n", "", "if", "bn", ":", "\n", "      ", "is_bn_training", "=", "not", "use_global_status", "\n", "output", "=", "batch_norm", "(", "output", ",", "\n", "'BatchNorm'", ",", "\n", "is_training", "=", "is_bn_training", ",", "\n", "decay", "=", "decay", ",", "\n", "activation_fn", "=", "None", ")", "\n", "\n", "# Apply ReLU as activation function.", "\n", "", "if", "relu", ":", "\n", "      ", "output", "=", "tf", ".", "nn", ".", "relu", "(", "output", ")", "\n", "\n", "", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.common.layers.atrous_conv": [[194, 268], ["x.get_shape().as_list", "tensorflow.nn.atrous_conv2d", "tensorflow.variable_scope", "tensorflow.get_variable", "convolve", "tensorflow.get_variable", "tensorflow.nn.bias_add", "layers.batch_norm", "tensorflow.nn.relu", "x.get_shape"], "function", ["home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.common.layers.batch_norm"], ["", "def", "atrous_conv", "(", "x", ",", "\n", "name", ",", "\n", "filters", ",", "\n", "kernel_size", ",", "\n", "dilation", ",", "\n", "padding", ",", "\n", "relu", "=", "True", ",", "\n", "biased", "=", "True", ",", "\n", "bn", "=", "True", ",", "\n", "decay", "=", "0.9997", ",", "\n", "is_training", "=", "True", ",", "\n", "use_global_status", "=", "True", ")", ":", "\n", "  ", "\"\"\"Atrous convolutional layers with batch normalization and ReLU.\n\n  This function perform atrous convolution, batch_norm (if bn=True),\n  and ReLU (if relu=True).\n\n  Args:\n    x: A tensor of size [batch_size, height_in, width_in, channels].\n    name: The prefix of tensorflow variables defined in this layer.\n    filters: A number indicating the number of output channels.\n    kernel_size: A number indicating the size of convolutional kernels.\n    dilation: A number indicating the dilation factor for height and width.\n    padding: 'VALID' or 'SAME'.\n    relu: enable/disable relu for ReLU as activation function. If relu \n      is False, maintain linear activation.\n    biased: enable/disable biased for adding biases after convolution.\n    bn: enable/disable bn for batch normalization.\n    decay: A number indication decay rate for updating moving mean and \n      moving variance in batch normalization.\n    is_training: If the tensorflow variables defined in this layer \n      would be used for training.\n    use_global_status: enable/disable use_global_status for batch\n      normalization. If True, moving mean and moving variance are updated\n      by exponential decay.\n\n  Returns:\n    A tensor of size [batch_size, height_out, width_out, channels_out].\n  \"\"\"", "\n", "c_i", "=", "x", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "-", "1", "]", "# input channels", "\n", "c_o", "=", "filters", "# output channels", "\n", "\n", "# Define helper function.", "\n", "convolve", "=", "lambda", "i", ",", "k", ":", "tf", ".", "nn", ".", "atrous_conv2d", "(", "\n", "i", ",", "\n", "k", ",", "\n", "dilation", ",", "\n", "padding", ")", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "name", ")", "as", "scope", ":", "\n", "    ", "kernel", "=", "tf", ".", "get_variable", "(", "\n", "name", "=", "'weights'", ",", "\n", "shape", "=", "[", "kernel_size", ",", "kernel_size", ",", "c_i", ",", "c_o", "]", ",", "\n", "trainable", "=", "is_training", ",", ")", "\n", "output", "=", "convolve", "(", "x", ",", "kernel", ")", "\n", "\n", "# Add the biases.", "\n", "if", "biased", ":", "\n", "      ", "biases", "=", "tf", ".", "get_variable", "(", "'biases'", ",", "[", "c_o", "]", ",", "trainable", "=", "is_training", ")", "\n", "output", "=", "tf", ".", "nn", ".", "bias_add", "(", "output", ",", "biases", ")", "\n", "\n", "# Apply batch normalization.", "\n", "", "if", "bn", ":", "\n", "      ", "is_bn_training", "=", "not", "use_global_status", "\n", "output", "=", "batch_norm", "(", "output", ",", "'BatchNorm'", ",", "\n", "is_training", "=", "is_bn_training", ",", "\n", "decay", "=", "decay", ",", "\n", "activation_fn", "=", "None", ")", "\n", "\n", "# Apply ReLU as activation function.", "\n", "", "if", "relu", ":", "\n", "      ", "output", "=", "tf", ".", "nn", ".", "relu", "(", "output", ")", "\n", "\n", "", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.common.layers._pool": [[269, 307], ["tensorflow.pad", "tensorflow.tf.nn.max_pool", "tensorflow.tf.nn.avg_pool"], "function", ["home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.common.layers.max_pool", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.common.layers.avg_pool"], ["", "def", "_pool", "(", "x", ",", "\n", "name", ",", "\n", "kernel_size", ",", "\n", "strides", ",", "\n", "padding", ",", "\n", "pool_fn", ")", ":", "\n", "  ", "\"\"\"Helper function for spatial pooling layer.\n\n  Args:\n    x: A tensor of size [batch_size, height_in, width_in, channels].\n    name: The prefix of tensorflow variables defined in this layer.\n    kernel_size: A number indicating the size of pooling kernels.\n    strides: A number indicating the stride of the sliding window for\n      height and width.\n    padding: 'VALID' or 'SAME'.\n    pool_fn: A tensorflow operation for pooling, such as tf.nn.max_pool.\n\n  Returns:\n    A tensor of size [batch_size, height_out, width_out, channels].\n  \"\"\"", "\n", "k", "=", "kernel_size", "\n", "s", "=", "strides", "\n", "if", "s", ">", "1", "and", "padding", "!=", "'SAME'", ":", "\n", "    ", "pad", "=", "k", "-", "1", "\n", "pad_beg", "=", "pad", "//", "2", "\n", "pad_end", "=", "pad", "-", "pad_beg", "\n", "pad_h", "=", "[", "pad_beg", ",", "pad_end", "]", "\n", "pad_w", "=", "[", "pad_beg", ",", "pad_end", "]", "\n", "x", "=", "tf", ".", "pad", "(", "x", ",", "[", "[", "0", ",", "0", "]", ",", "pad_h", ",", "pad_w", ",", "[", "0", ",", "0", "]", "]", ")", "\n", "\n", "\n", "", "output", "=", "pool_fn", "(", "x", ",", "\n", "ksize", "=", "[", "1", ",", "k", ",", "k", ",", "1", "]", ",", "\n", "strides", "=", "[", "1", ",", "s", ",", "s", ",", "1", "]", ",", "\n", "padding", "=", "padding", ",", "\n", "name", "=", "name", ")", "\n", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.common.layers.max_pool": [[308, 327], ["layers._pool"], "function", ["home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.common.layers._pool"], ["", "def", "max_pool", "(", "x", ",", "\n", "name", ",", "\n", "kernel_size", ",", "\n", "strides", ",", "\n", "padding", ")", ":", "\n", "  ", "\"\"\"Max pooling layer.\n\n  Args:\n    x: A tensor of size [batch_size, height_in, width_in, channels].\n    name: The prefix of tensorflow variables defined in this layer.\n    kernel_size: A number indicating the size of pooling kernels.\n    strides: A number indicating the stride of the sliding window for\n      height and width.\n    padding: 'VALID' or 'SAME'.\n\n  Returns:\n    A tensor of size [batch_size, height_out, width_out, channels].\n  \"\"\"", "\n", "return", "_pool", "(", "x", ",", "name", ",", "kernel_size", ",", "strides", ",", "padding", ",", "tf", ".", "nn", ".", "max_pool", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.common.layers.avg_pool": [[328, 343], ["layers._pool"], "function", ["home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.common.layers._pool"], ["", "def", "avg_pool", "(", "x", ",", "name", ",", "kernel_size", ",", "strides", ",", "padding", ")", ":", "\n", "  ", "\"\"\"Average pooling layer.\n\n  Args:\n    x: A tensor of size [batch_size, height_in, width_in, channels].\n    name: The prefix of tensorflow variables defined in this layer.\n    kernel_size: A number indicating the size of pooling kernels.\n    strides: A number indicating the stride of the sliding window for\n      height and width.\n    padding: 'VALID' or 'SAME'.\n\n  Returns:\n    A tensor of size [batch_size, height_out, width_out, channels].\n  \"\"\"", "\n", "return", "_pool", "(", "x", ",", "name", ",", "kernel_size", ",", "strides", ",", "padding", ",", "tf", ".", "nn", ".", "avg_pool", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.train.train_stanford_scenes_asm.get_arguments": [[22, 97], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.preprocess.stanford_scenes_down_sample.parse_args"], ["def", "get_arguments", "(", ")", ":", "\n", "  ", "\"\"\"Parse all the arguments provided from the CLI.\n    \n  Returns:\n    A list of parsed arguments.\n  \"\"\"", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Semantic Segmentation'", ")", "\n", "# Data parameters", "\n", "parser", ".", "add_argument", "(", "'--batch_size'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "'Number of images in one step.'", ")", "\n", "parser", ".", "add_argument", "(", "'--data_dir'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "\n", "help", "=", "'/path/to/dataset/.'", ")", "\n", "parser", ".", "add_argument", "(", "'--data_list'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "\n", "help", "=", "'/path/to/datalist/file.'", ")", "\n", "parser", ".", "add_argument", "(", "'--ignore_label'", ",", "type", "=", "int", ",", "default", "=", "255", ",", "\n", "help", "=", "'The index of the label to ignore.'", ")", "\n", "parser", ".", "add_argument", "(", "'--input_size'", ",", "type", "=", "str", ",", "default", "=", "'336,336'", ",", "\n", "help", "=", "'Comma-separated string with H and W of image.'", ")", "\n", "parser", ".", "add_argument", "(", "'--depth_unit'", ",", "type", "=", "float", ",", "default", "=", "512.0", ",", "\n", "help", "=", "'Each pixel value difference means 1/depth_unit '", "\n", "'meters.'", ")", "\n", "parser", ".", "add_argument", "(", "'--train_segmentation'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Whether to train for semantic segmentation.'", ")", "\n", "parser", ".", "add_argument", "(", "'--train_depth'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Whether to train for depth estimation.'", ")", "\n", "parser", ".", "add_argument", "(", "'--train_normal'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Whether to train for surface normals prediction.'", ")", "\n", "parser", ".", "add_argument", "(", "'--adv_lambda'", ",", "type", "=", "float", ",", "default", "=", "0.01", ",", "\n", "help", "=", "'Weighting for adversarial loss of Analyzer.'", ")", "\n", "parser", ".", "add_argument", "(", "'--rec_lambda'", ",", "type", "=", "float", ",", "default", "=", "10.0", ",", "\n", "help", "=", "'Weighting for reconstruction loss of Analyzer.'", ")", "\n", "parser", ".", "add_argument", "(", "'--sup_lambda'", ",", "type", "=", "float", ",", "default", "=", "1.0", ",", "\n", "help", "=", "'Weighting paramters for supervising loss.'", ")", "\n", "parser", ".", "add_argument", "(", "'--loss_on_structured_predictor'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Whether impose loss on structured predictor.'", ")", "\n", "# Training paramters", "\n", "parser", ".", "add_argument", "(", "'--is_training'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Whether to updates weights.'", ")", "\n", "parser", ".", "add_argument", "(", "'--use_global_status'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Whether to updates moving mean and variance.'", ")", "\n", "parser", ".", "add_argument", "(", "'--learning_rate'", ",", "type", "=", "float", ",", "default", "=", "2.5e-4", ",", "\n", "help", "=", "'Base learning rate.'", ")", "\n", "parser", ".", "add_argument", "(", "'--power'", ",", "type", "=", "float", ",", "default", "=", "0.9", ",", "\n", "help", "=", "'Decay for poly learing rate policy.'", ")", "\n", "parser", ".", "add_argument", "(", "'--momentum'", ",", "type", "=", "float", ",", "default", "=", "0.9", ",", "\n", "help", "=", "'Momentum component of the optimiser.'", ")", "\n", "parser", ".", "add_argument", "(", "'--weight_decay'", ",", "type", "=", "float", ",", "default", "=", "5e-4", ",", "\n", "help", "=", "'Regularisation parameter for L2-loss.'", ")", "\n", "parser", ".", "add_argument", "(", "'--num_classes'", ",", "type", "=", "int", ",", "default", "=", "50", ",", "\n", "help", "=", "'Number of classes to predict.'", ")", "\n", "parser", ".", "add_argument", "(", "'--num_steps'", ",", "type", "=", "int", ",", "default", "=", "20000", ",", "\n", "help", "=", "'Number of training steps.'", ")", "\n", "parser", ".", "add_argument", "(", "'--iter_size'", ",", "type", "=", "int", ",", "default", "=", "10", ",", "\n", "help", "=", "'Number of iteration to update weights'", ")", "\n", "parser", ".", "add_argument", "(", "'--random_mirror'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Whether to randomly mirror the inputs.'", ")", "\n", "parser", ".", "add_argument", "(", "'--random_crop'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Whether to randomly crop the inputs.'", ")", "\n", "parser", ".", "add_argument", "(", "'--random_scale'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Whether to randomly scale the inputs.'", ")", "\n", "parser", ".", "add_argument", "(", "'--upscale_predictions'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Whether to upscale resolution of predictions.'", ")", "\n", "# Misc paramters", "\n", "parser", ".", "add_argument", "(", "'--restore_from'", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "'Where restore model parameters from.'", ")", "\n", "parser", ".", "add_argument", "(", "'--save_pred_every'", ",", "type", "=", "int", ",", "default", "=", "10000", ",", "\n", "help", "=", "'Save summaries and checkpoint every often.'", ")", "\n", "parser", ".", "add_argument", "(", "'--update_tb_every'", ",", "type", "=", "int", ",", "default", "=", "200", ",", "\n", "help", "=", "'Update Tensorboard summaries every often.'", ")", "\n", "parser", ".", "add_argument", "(", "'--snapshot_dir'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "\n", "help", "=", "'Where to save snapshots of the model.'", ")", "\n", "parser", ".", "add_argument", "(", "'--not_restore_classifier'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Whether to not restore classifier layers.'", ")", "\n", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.train.train_stanford_scenes_asm.save": [[99, 115], ["os.path.join", "saver.save", "print", "os.path.exists", "os.makedirs"], "function", ["home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.train.train_stanford_scenes_cgan.save"], ["", "def", "save", "(", "saver", ",", "sess", ",", "logdir", ",", "step", ")", ":", "\n", "  ", "\"\"\"Saves the trained weights.\n   \n  Args:\n    saver: TensorFlow Saver object.\n    sess: TensorFlow session.\n    logdir: path to the snapshots directory.\n    step: current training step.\n  \"\"\"", "\n", "model_name", "=", "'model.ckpt'", "\n", "checkpoint_path", "=", "os", ".", "path", ".", "join", "(", "logdir", ",", "model_name", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "logdir", ")", ":", "\n", "    ", "os", ".", "makedirs", "(", "logdir", ")", "\n", "", "saver", ".", "save", "(", "sess", ",", "checkpoint_path", ",", "global_step", "=", "step", ")", "\n", "print", "(", "'The checkpoint has been created.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.train.train_stanford_scenes_asm.load": [[117, 127], ["saver.restore", "print"], "function", ["None"], ["", "def", "load", "(", "saver", ",", "sess", ",", "ckpt_path", ")", ":", "\n", "  ", "\"\"\"Loads the trained weights.\n    \n  Args:\n    saver: TensorFlow Saver object.\n    sess: TensorFlow session.\n    ckpt_path: path to checkpoint file with parameters.\n  \"\"\"", "\n", "saver", ".", "restore", "(", "sess", ",", "ckpt_path", ")", "\n", "print", "(", "'Restored model parameters from {}'", ".", "format", "(", "ckpt_path", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.train.train_stanford_scenes_asm.main": [[129, 586], ["train_stanford_scenes_asm.get_arguments", "utils.general.snapshot_arg", "map", "tensorflow.train.Coordinator", "tensorflow.placeholder", "stanford_scenes.models.structured_predictor.structured_predictor", "tensorflow.reshape", "tensorflow.less_equal", "tensorflow.reshape", "tensorflow.greater", "tensorflow.reduce_all", "tensorflow.equal", "tensorflow.logical_and", "tensorflow.reshape", "tensorflow.logical_not", "tensorflow.nn.l2_normalize", "tensorflow.reshape", "tensorflow.squeeze", "tensorflow.squeeze", "tensorflow.squeeze", "tensorflow.logical_and", "tensorflow.reshape", "tensorflow.expand_dims", "tensorflow.cast", "stanford_scenes.models.analyzer.analyzer", "stanford_scenes.models.analyzer.analyzer", "stanford_scenes.models.analyzer.analyzer", "tensorflow.split", "tensorflow.split", "tensorflow.get_collection", "print", "reduced_losses.append", "reduced_losses.append", "tensorflow.add_n", "tensorflow.pow", "tensorflow.add_n", "tensorflow.trainable_variables", "tensorflow.gradients", "tensorflow.gradients", "tensorflow.constant", "tensorflow.scalar_mul", "tensorflow.train.MomentumOptimizer", "tensorflow.train.MomentumOptimizer", "tensorflow.train.MomentumOptimizer", "tensorflow.train.MomentumOptimizer", "tf.train.MomentumOptimizer.apply_gradients", "tf.train.MomentumOptimizer.apply_gradients", "tf.train.MomentumOptimizer.apply_gradients", "tf.train.MomentumOptimizer.apply_gradients", "tensorflow.group", "tensorflow.group", "tensorflow.ConfigProto", "tensorflow.Session", "tensorflow.global_variables_initializer", "tf.Session.run", "tensorflow.train.Saver", "tensorflow.train.start_queue_runners", "tqdm.tqdm", "tf.train.Coordinator.request_stop", "tf.train.Coordinator.join", "get_arguments.input_size.split", "tensorflow.device", "enumerate", "tensorflow.image.resize_bilinear", "tensorflow.image.resize_nearest_neighbor", "tensorflow.image.resize_nearest_neighbor", "tensorflow.image.resize_nearest_neighbor", "tensorflow.equal", "tensorflow.abs", "tensorflow.where", "tensorflow.where", "tensorflow.where", "gts.append", "negs.append", "preds.append", "gts.append", "tensorflow.log", "negs.append", "preds.append", "gts.append", "negs.append", "preds.append", "tensorflow.to_int32", "tensorflow.reshape", "tensorflow.gather", "tensorflow.nn.sparse_softmax_cross_entropy_with_logits", "tensorflow.reduce_mean", "tensorflow.reshape", "tensorflow.gather", "tensorflow.nn.sparse_softmax_cross_entropy_with_logits", "tensorflow.reduce_mean", "analyzer_rec_losses.append", "tensorflow.gather", "tensorflow.gather", "tensorflow.reduce_mean", "tensorflow.reduce_sum", "tensorflow.nn.relu", "tensorflow.gather", "tensorflow.reduce_mean", "analyzer_rec_losses.append", "tensorflow.gather", "tensorflow.gather", "tensorflow.gather", "tensorflow.nn.l2_normalize", "analyzer_rec_losses.append", "tensorflow.nn.l2_normalize", "tensorflow.nn.l2_normalize", "tensorflow.nn.l2_normalize", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.maximum", "tensorflow.reduce_mean", "tensorflow.reduce_mean", "match_losses.append", "analyzer_adv_losses.append", "tensorflow.add_n", "tensorflow.add_n", "tensorflow.add_n", "tensorflow.pow", "zip", "zip", "zip", "zip", "tensorflow.device", "tensorflow.py_func", "output_vis.append", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.image", "tensorflow.summary.merge_all", "tensorflow.summary.FileWriter", "tensorflow.train.Saver", "train_stanford_scenes_asm.load", "range", "time.time", "tf.Session.run", "tf.Session.run", "range", "tf.Session.run", "tqdm.tqdm.set_description", "tensorflow.name_scope", "stanford_scenes.image_reader.ImageReader", "stanford_scenes.image_reader.ImageReader.dequeue", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.global_variables", "tensorflow.one_hot", "tensorflow.one_hot", "tensorflow.nn.softmax", "tensorflow.log", "tensorflow.maximum", "tensorflow.shape", "tensorflow.gather", "reduced_losses.append", "tensorflow.reshape", "reduced_losses.append", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.reduce_mean", "reduced_losses.append", "tensorflow.reshape", "tensorflow.reduce_mean", "tensorflow.squared_difference", "tensorflow.squared_difference", "tensorflow.nn.l2_loss", "tensorflow.trainable_variables", "tensorflow.nn.l2_loss", "tensorflow.trainable_variables", "tensorflow.add_n", "tensorflow.trainable_variables", "len", "len", "tensorflow.gradients", "tensorflow.cast", "tensorflow.py_func", "output_vis.append", "tensorflow.image.resize_images", "tensorflow.argmax", "tensorflow.expand_dims", "tensorflow.cast", "tensorflow.py_func", "output_vis.append", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.image.resize_bilinear", "tensorflow.reduce_max", "tensorflow.clip_by_value", "tensorflow.cast", "tensorflow.tile", "output_vis.append", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.image.resize_bilinear", "tensorflow.cast", "output_vis.append", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.concat", "tensorflow.global_variables", "train_stanford_scenes_asm.save", "time.time", "output.get_shape().as_list", "tensorflow.image.resize_bilinear", "outputs[].get_shape().as_list", "outputs[].get_shape().as_list", "outputs[].get_shape().as_list", "outputs[].get_shape().as_list", "tensorflow.squeeze", "tensorflow.argmax", "tensorflow.maximum", "tensorflow.get_collection", "tensorflow.get_collection", "tensorflow.abs", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.abs", "tensorflow.abs", "tensorflow.abs", "tensorflow.get_default_graph", "tf.Session.run", "tf.summary.FileWriter.add_summary", "tf.Session.run", "tensorflow.shape", "tensorflow.shape", "tensorflow.shape", "output.get_shape", "outputs[].get_shape", "outputs[].get_shape", "outputs[].get_shape", "outputs[].get_shape", "tensorflow.reduce_max"], "function", ["home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.inference.inference_stanford_scenes.get_arguments", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.utils.general.snapshot_arg", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.fcrn.structured_predictor", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.analyzer.analyzer", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.analyzer.analyzer", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.analyzer.analyzer", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.inference.inference_stanford_scenes.load", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.stanford_scenes.image_reader.ImageReader.dequeue", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.train.train_stanford_scenes_cgan.save"], ["", "def", "main", "(", ")", ":", "\n", "  ", "\"\"\"Create the model and start training.\n  \"\"\"", "\n", "# Read CL arguments and snapshot the arguments into text file.", "\n", "args", "=", "get_arguments", "(", ")", "\n", "utils", ".", "general", ".", "snapshot_arg", "(", "args", ")", "\n", "\n", "# The input size.", "\n", "h", ",", "w", "=", "map", "(", "int", ",", "args", ".", "input_size", ".", "split", "(", "','", ")", ")", "\n", "input_size", "=", "(", "h", ",", "w", ")", "\n", "\n", "# Create queue coordinator.", "\n", "coord", "=", "tf", ".", "train", ".", "Coordinator", "(", ")", "\n", "\n", "# current step", "\n", "step_ph", "=", "tf", ".", "placeholder", "(", "dtype", "=", "tf", ".", "float32", ",", "shape", "=", "(", ")", ")", "\n", "\n", "# Load the data reader.", "\n", "with", "tf", ".", "device", "(", "'/cpu:0'", ")", ":", "\n", "    ", "with", "tf", ".", "name_scope", "(", "'create_inputs'", ")", ":", "\n", "      ", "reader", "=", "ImageReader", "(", "\n", "args", ".", "data_dir", ",", "\n", "args", ".", "data_list", ",", "\n", "input_size", ",", "\n", "args", ".", "random_scale", ",", "\n", "args", ".", "random_mirror", ",", "\n", "args", ".", "random_crop", ",", "\n", "args", ".", "ignore_label", ",", "\n", "IMG_MEAN", ")", "\n", "\n", "datas_batch", "=", "reader", ".", "dequeue", "(", "args", ".", "batch_size", ")", "\n", "datas_batch", "[", "2", "]", "=", "datas_batch", "[", "2", "]", "/", "args", ".", "depth_unit", "\n", "image_batch", "=", "tf", ".", "placeholder", "(", "datas_batch", "[", "0", "]", ".", "dtype", ",", "\n", "datas_batch", "[", "0", "]", ".", "shape", ")", "\n", "label_batch", "=", "tf", ".", "placeholder", "(", "datas_batch", "[", "1", "]", ".", "dtype", ",", "\n", "datas_batch", "[", "1", "]", ".", "shape", ")", "\n", "depth_batch", "=", "tf", ".", "placeholder", "(", "datas_batch", "[", "2", "]", ".", "dtype", ",", "\n", "datas_batch", "[", "2", "]", ".", "shape", ")", "\n", "normal_batch", "=", "tf", ".", "placeholder", "(", "datas_batch", "[", "3", "]", ".", "dtype", ",", "\n", "datas_batch", "[", "3", "]", ".", "shape", ")", "\n", "\n", "# Create network and predictions.", "\n", "", "", "outputs", "=", "structured_predictor", "(", "image_batch", ",", "\n", "args", ".", "num_classes", ",", "\n", "args", ".", "is_training", ",", "\n", "args", ".", "use_global_status", ")", "\n", "\n", "# Either up-sample predictions or down-sample ground-truths.", "\n", "if", "args", ".", "upscale_predictions", ":", "\n", "    ", "for", "output_index", ",", "output", "in", "enumerate", "(", "outputs", ")", ":", "\n", "      ", "out_h", ",", "out_w", "=", "output", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "1", ":", "3", "]", "\n", "if", "out_h", "!=", "h", "or", "out_w", "!=", "w", ":", "\n", "        ", "outputs", "[", "output_index", "]", "=", "tf", ".", "image", ".", "resize_bilinear", "(", "\n", "output", ",", "[", "h", ",", "w", "]", ")", "\n", "", "", "images", "=", "image_batch", "\n", "labels", "=", "label_batch", "\n", "depths", "=", "depth_batch", "\n", "normals", "=", "normal_batch", "\n", "", "else", ":", "\n", "    ", "images", "=", "tf", ".", "image", ".", "resize_bilinear", "(", "\n", "image_batch", ",", "outputs", "[", "0", "]", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "1", ":", "3", "]", ")", "\n", "labels", "=", "tf", ".", "image", ".", "resize_nearest_neighbor", "(", "\n", "label_batch", ",", "outputs", "[", "0", "]", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "1", ":", "3", "]", ")", "\n", "depths", "=", "tf", ".", "image", ".", "resize_nearest_neighbor", "(", "\n", "depth_batch", ",", "outputs", "[", "1", "]", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "1", ":", "3", "]", ")", "\n", "normals", "=", "tf", ".", "image", ".", "resize_nearest_neighbor", "(", "\n", "normal_batch", ",", "outputs", "[", "2", "]", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "1", ":", "3", "]", ")", "\n", "\n", "# Ignore the location where the label value is larger than args.num_classes.", "\n", "", "labels_flat", "=", "tf", ".", "reshape", "(", "labels", ",", "(", "-", "1", ",", ")", ")", "\n", "not_ignore_labels", "=", "tf", ".", "less_equal", "(", "labels_flat", ",", "args", ".", "num_classes", "-", "1", ")", "\n", "\n", "# Ignore the location where the depth value <= 0", "\n", "depths_flat", "=", "tf", ".", "reshape", "(", "depths", ",", "(", "-", "1", ",", ")", ")", "\n", "not_ignore_depths", "=", "tf", ".", "greater", "(", "depths_flat", ",", "0.0", ")", "\n", "\n", "# Ignore the location where the normal value != [128,128,128].", "\n", "# The normal is centered at 127.5 in ImageReader.", "\n", "ignore_normal_yz", "=", "tf", ".", "reduce_all", "(", "\n", "tf", ".", "equal", "(", "normals", "[", ":", ",", ":", ",", ":", ",", "1", ":", "]", ",", "0.5", ")", ",", "\n", "axis", "=", "-", "1", ")", "\n", "ignore_normal_x", "=", "tf", ".", "equal", "(", "tf", ".", "abs", "(", "normals", "[", ":", ",", ":", ",", ":", ",", "0", "]", ")", ",", "0.5", ")", "\n", "ignore_normals", "=", "tf", ".", "logical_and", "(", "ignore_normal_yz", ",", "\n", "ignore_normal_x", ")", "\n", "ignore_normals", "=", "tf", ".", "reshape", "(", "ignore_normals", ",", "(", "-", "1", ",", ")", ")", "\n", "not_ignore_normals", "=", "tf", ".", "logical_not", "(", "ignore_normals", ")", "\n", "normals", "=", "tf", ".", "nn", ".", "l2_normalize", "(", "normals", ",", "dim", "=", "-", "1", ")", "\n", "normals_flat", "=", "tf", ".", "reshape", "(", "normals", ",", "(", "-", "1", ",", "3", ")", ")", "\n", "\n", "# Extract the indices of labels where the gradients are propogated.", "\n", "valid_label_inds", "=", "tf", ".", "squeeze", "(", "tf", ".", "where", "(", "not_ignore_labels", ")", ",", "1", ")", "\n", "valid_depth_inds", "=", "tf", ".", "squeeze", "(", "tf", ".", "where", "(", "not_ignore_depths", ")", ",", "1", ")", "\n", "valid_normal_inds", "=", "tf", ".", "squeeze", "(", "tf", ".", "where", "(", "not_ignore_normals", ")", ",", "1", ")", "\n", "\n", "# Grab variable names which should be restored from checkpoints.", "\n", "restore_var", "=", "[", "\n", "v", "for", "v", "in", "tf", ".", "global_variables", "(", ")", "\n", "if", "'block5'", "not", "in", "v", ".", "name", "or", "not", "args", ".", "not_restore_classifier", "]", "\n", "\n", "# Prepare inputs for Analyzer.", "\n", "gts", ",", "negs", ",", "preds", ",", "split_inds", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "0", ",", "0", ",", "0", "]", "\n", "if", "args", ".", "train_segmentation", ":", "\n", "    ", "gts", ".", "append", "(", "tf", ".", "one_hot", "(", "tf", ".", "squeeze", "(", "labels", ",", "axis", "=", "-", "1", ")", ",", "\n", "depth", "=", "args", ".", "num_classes", ")", ")", "\n", "negs", ".", "append", "(", "tf", ".", "one_hot", "(", "tf", ".", "argmax", "(", "outputs", "[", "0", "]", ",", "axis", "=", "-", "1", ")", ",", "\n", "depth", "=", "args", ".", "num_classes", ")", ")", "\n", "preds", ".", "append", "(", "tf", ".", "nn", ".", "softmax", "(", "outputs", "[", "0", "]", ",", "dim", "=", "-", "1", ")", ")", "\n", "split_inds", "[", "0", "]", "=", "args", ".", "num_classes", "\n", "\n", "", "if", "args", ".", "train_depth", ":", "\n", "    ", "gts", ".", "append", "(", "tf", ".", "log", "(", "tf", ".", "maximum", "(", "depths", ",", "1e-2", ")", ")", ")", "\n", "log_out", "=", "tf", ".", "log", "(", "tf", ".", "maximum", "(", "outputs", "[", "1", "]", ",", "1e-2", ")", ")", "\n", "negs", ".", "append", "(", "log_out", ")", "\n", "preds", ".", "append", "(", "log_out", ")", "\n", "split_inds", "[", "1", "]", "=", "1", "\n", "\n", "", "if", "args", ".", "train_normal", ":", "\n", "    ", "gts", ".", "append", "(", "normals", ")", "\n", "negs", ".", "append", "(", "outputs", "[", "2", "]", ")", "\n", "preds", ".", "append", "(", "outputs", "[", "2", "]", ")", "\n", "split_inds", "[", "2", "]", "=", "3", "\n", "\n", "", "not_ignore_mask", "=", "tf", ".", "logical_and", "(", "not_ignore_depths", ",", "\n", "not_ignore_normals", ")", "\n", "not_ignore_mask", "=", "tf", ".", "reshape", "(", "not_ignore_mask", ",", "\n", "tf", ".", "shape", "(", "depths", ")", "[", ":", "3", "]", ")", "\n", "not_ignore_mask", "=", "tf", ".", "expand_dims", "(", "not_ignore_mask", ",", "axis", "=", "-", "1", ")", "\n", "not_ignore_mask", "=", "tf", ".", "cast", "(", "not_ignore_mask", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "ignore_mask", "=", "1", "-", "not_ignore_mask", "\n", "analyzer", "(", "gts", ",", "ignore_mask", ",", "'gts'", ",", "True", ",", "False", ",", "False", ")", "\n", "analyzer", "(", "negs", ",", "ignore_mask", ",", "'negs'", ",", "True", ",", "False", ",", "True", ")", "\n", "analyzer", "(", "preds", ",", "ignore_mask", ",", "'preds'", ",", "True", ",", "False", ",", "True", ")", "\n", "\n", "gt_recs", "=", "[", "v", "for", "v", "in", "tf", ".", "get_collection", "(", "'Analyzer/outputs'", ")", "\n", "if", "'gts'", "in", "v", ".", "name", "]", "[", "0", "]", "\n", "gt_recs", "=", "tf", ".", "split", "(", "gt_recs", ",", "split_inds", ",", "axis", "=", "-", "1", ")", "\n", "pred_recs", "=", "[", "v", "for", "v", "in", "tf", ".", "get_collection", "(", "'Analyzer/outputs'", ")", "\n", "if", "'preds'", "in", "v", ".", "name", "]", "[", "0", "]", "\n", "pred_recs", "=", "tf", ".", "split", "(", "pred_recs", ",", "split_inds", ",", "axis", "=", "-", "1", ")", "\n", "\n", "# Define losses.", "\n", "analyzer_adv_losses", ",", "analyzer_rec_losses", ",", "match_losses", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "reduced_losses", "=", "[", "]", "\n", "# Define segmentation loss and prediction errors.", "\n", "if", "args", ".", "train_segmentation", ":", "\n", "# Segmentation loss of prediction.", "\n", "    ", "labels_gather", "=", "tf", ".", "to_int32", "(", "tf", ".", "gather", "(", "labels_flat", ",", "valid_label_inds", ")", ")", "\n", "segmentation_output", "=", "tf", ".", "reshape", "(", "outputs", "[", "0", "]", ",", "\n", "[", "-", "1", ",", "args", ".", "num_classes", "]", ")", "\n", "segmentation_output", "=", "tf", ".", "gather", "(", "segmentation_output", ",", "\n", "valid_label_inds", ")", "\n", "segmentation_loss", "=", "tf", ".", "nn", ".", "sparse_softmax_cross_entropy_with_logits", "(", "\n", "logits", "=", "segmentation_output", ",", "\n", "labels", "=", "labels_gather", ")", "\n", "segmentation_loss", "=", "tf", ".", "reduce_mean", "(", "segmentation_loss", ")", "\n", "\n", "if", "args", ".", "loss_on_structured_predictor", ":", "\n", "      ", "reduced_losses", ".", "append", "(", "segmentation_loss", ")", "\n", "\n", "# ASML Loss (Reconstruction segmentation).", "\n", "", "rec_segmentation_output", "=", "tf", ".", "reshape", "(", "gt_recs", "[", "0", "]", ",", "\n", "[", "-", "1", ",", "args", ".", "num_classes", "]", ")", "\n", "rec_segmentation_output", "=", "tf", ".", "gather", "(", "rec_segmentation_output", ",", "\n", "valid_label_inds", ")", "\n", "rec_segmentation_loss", "=", "tf", ".", "nn", ".", "sparse_softmax_cross_entropy_with_logits", "(", "\n", "logits", "=", "rec_segmentation_output", ",", "\n", "labels", "=", "labels_gather", ")", "\n", "rec_segmentation_loss", "=", "tf", ".", "reduce_mean", "(", "rec_segmentation_loss", ")", "\n", "analyzer_rec_losses", ".", "append", "(", "rec_segmentation_loss", ")", "\n", "\n", "# Define depth loss and prediction errors.", "\n", "", "if", "args", ".", "train_depth", ":", "\n", "# Depth loss of prediction.", "\n", "    ", "depths_gather", "=", "tf", ".", "gather", "(", "depths_flat", ",", "valid_depth_inds", ")", "\n", "depth_output", "=", "tf", ".", "gather", "(", "tf", ".", "reshape", "(", "outputs", "[", "1", "]", ",", "[", "-", "1", ",", "]", ")", ",", "\n", "valid_depth_inds", ")", "\n", "depth_diff", "=", "depth_output", "-", "depths_gather", "\n", "depth_loss", "=", "tf", ".", "reduce_mean", "(", "depth_diff", "**", "2", ")", "\n", "depth_absrel", "=", "tf", ".", "reduce_sum", "(", "\n", "tf", ".", "abs", "(", "depth_diff", ")", "/", "depths_gather", ")", "\n", "depth_absrel", "/=", "args", ".", "batch_size", "\n", "\n", "if", "args", ".", "loss_on_structured_predictor", ":", "\n", "      ", "reduced_losses", ".", "append", "(", "depth_loss", ")", "\n", "\n", "# Reconstruction loss.", "\n", "", "rec_depth_output", "=", "tf", ".", "nn", ".", "relu", "(", "gt_recs", "[", "1", "]", ")", "\n", "rec_depth_output", "=", "tf", ".", "gather", "(", "\n", "tf", ".", "reshape", "(", "rec_depth_output", ",", "[", "-", "1", ",", "]", ")", ",", "\n", "valid_depth_inds", ")", "\n", "rec_depth_loss", "=", "tf", ".", "reduce_mean", "(", "\n", "(", "rec_depth_output", "-", "depths_gather", ")", "**", "2", ")", "\n", "analyzer_rec_losses", ".", "append", "(", "rec_depth_loss", ")", "\n", "\n", "# Define surface normal loss and prediction errors.", "\n", "", "if", "args", ".", "train_normal", ":", "\n", "# Surface normal loss of prediction.", "\n", "    ", "normals_gather", "=", "tf", ".", "gather", "(", "normals_flat", ",", "valid_normal_inds", ")", "\n", "normal_output", "=", "tf", ".", "gather", "(", "tf", ".", "reshape", "(", "outputs", "[", "2", "]", ",", "[", "-", "1", ",", "3", "]", ")", ",", "\n", "valid_normal_inds", ")", "\n", "normal_loss", "=", "-", "tf", ".", "reduce_mean", "(", "tf", ".", "reduce_sum", "(", "\n", "normal_output", "*", "normals_gather", ",", "axis", "=", "-", "1", ")", ")", "\n", "normal_loss", "*=", "10.0", "\n", "\n", "if", "args", ".", "loss_on_structured_predictor", ":", "\n", "      ", "reduced_losses", ".", "append", "(", "normal_loss", ")", "\n", "\n", "# Reconstruction loss.", "\n", "", "rec_normal_output", "=", "tf", ".", "gather", "(", "tf", ".", "reshape", "(", "gt_recs", "[", "2", "]", ",", "[", "-", "1", ",", "3", "]", ")", ",", "\n", "valid_normal_inds", ")", "\n", "rec_normal_output", "=", "tf", ".", "nn", ".", "l2_normalize", "(", "rec_normal_output", ",", "\n", "dim", "=", "-", "1", ")", "\n", "rec_normal_loss", "=", "-", "tf", ".", "reduce_mean", "(", "tf", ".", "reduce_sum", "(", "\n", "rec_normal_output", "*", "normals_gather", ",", "axis", "=", "-", "1", ")", ")", "\n", "analyzer_rec_losses", ".", "append", "(", "rec_normal_loss", ")", "\n", "\n", "# Compute adversarial loss for analyzer and matching loss for predictor.", "\n", "", "feat_collections", "=", "tf", ".", "get_collection", "(", "'Analyzer/features'", ")", "\n", "print", "(", "feat_collections", ")", "\n", "feat_names", "=", "[", "'layer1'", ",", "'layer2'", "]", "\n", "for", "feat_name", "in", "feat_names", ":", "\n", "    ", "pred", "=", "[", "v", "for", "v", "in", "feat_collections", "\n", "if", "feat_name", "in", "v", ".", "name", "and", "'preds'", "in", "v", ".", "name", "]", "[", "0", "]", "\n", "gt", "=", "[", "v", "for", "v", "in", "feat_collections", "\n", "if", "feat_name", "in", "v", ".", "name", "and", "'gts'", "in", "v", ".", "name", "]", "[", "0", "]", "\n", "neg", "=", "[", "v", "for", "v", "in", "feat_collections", "\n", "if", "feat_name", "in", "v", ".", "name", "and", "'negs'", "in", "v", ".", "name", "]", "[", "0", "]", "\n", "pred", "=", "tf", ".", "nn", ".", "l2_normalize", "(", "pred", ",", "dim", "=", "-", "1", ")", "\n", "gt", "=", "tf", ".", "nn", ".", "l2_normalize", "(", "gt", ",", "dim", "=", "-", "1", ")", "\n", "neg", "=", "tf", ".", "nn", ".", "l2_normalize", "(", "neg", ",", "dim", "=", "-", "1", ")", "\n", "match_loss", "=", "tf", ".", "reduce_sum", "(", "tf", ".", "squared_difference", "(", "gt", ",", "pred", ")", ",", "\n", "axis", "=", "-", "1", ")", "\n", "adv_loss", "=", "tf", ".", "reduce_sum", "(", "tf", ".", "squared_difference", "(", "gt", ",", "neg", ")", ",", "\n", "axis", "=", "-", "1", ")", "\n", "adv_loss", "=", "tf", ".", "maximum", "(", "0.0", ",", "0.80", "-", "adv_loss", ")", "\n", "match_loss", "=", "tf", ".", "reduce_mean", "(", "match_loss", ")", "\n", "adv_loss", "=", "tf", ".", "reduce_mean", "(", "adv_loss", ")", "\n", "\n", "match_losses", ".", "append", "(", "match_loss", ")", "\n", "analyzer_adv_losses", ".", "append", "(", "adv_loss", ")", "\n", "\n", "# Define weight regularization loss.", "\n", "", "w", "=", "args", ".", "weight_decay", "\n", "l2_losses", "=", "[", "w", "*", "tf", ".", "nn", ".", "l2_loss", "(", "v", ")", "for", "v", "in", "tf", ".", "trainable_variables", "(", ")", "\n", "if", "'weights'", "in", "v", ".", "name", "and", "'Analyzer'", "not", "in", "v", ".", "name", "]", "\n", "analyzer_l2_losses", "=", "[", "\n", "w", "*", "tf", ".", "nn", ".", "l2_loss", "(", "v", ")", "for", "v", "in", "tf", ".", "trainable_variables", "(", ")", "\n", "if", "'weights'", "in", "v", ".", "name", "and", "'Analyzer'", "in", "v", ".", "name", "]", "\n", "\n", "# Sum losses on Structured Predictor.", "\n", "if", "args", ".", "loss_on_structured_predictor", ":", "\n", "    ", "reduced_losses", "=", "[", "l", "*", "args", ".", "sup_lambda", "for", "l", "in", "reduced_losses", "]", "\n", "", "reduced_losses", ".", "append", "(", "tf", ".", "add_n", "(", "match_losses", ")", ")", "\n", "reduced_losses", ".", "append", "(", "tf", ".", "add_n", "(", "l2_losses", ")", ")", "\n", "reduced_loss", "=", "tf", ".", "add_n", "(", "reduced_losses", ")", "\n", "\n", "# Sum losses on Analyzer.", "\n", "dec", "=", "tf", ".", "pow", "(", "10.0", ",", "-", "step_ph", "/", "args", ".", "num_steps", ")", "# 1~0.1", "\n", "analyzer_max_loss", "=", "tf", ".", "add_n", "(", "analyzer_adv_losses", ")", "*", "args", ".", "adv_lambda", "*", "dec", "\n", "analyzer_min_loss", "=", "tf", ".", "add_n", "(", "analyzer_rec_losses", ")", "*", "args", ".", "rec_lambda", "\n", "analyzer_min_loss", "+=", "tf", ".", "add_n", "(", "analyzer_l2_losses", ")", "\n", "\n", "# Grab variable names which are used for training.", "\n", "all_trainable", "=", "tf", ".", "trainable_variables", "(", ")", "\n", "pred_trainable", "=", "[", "\n", "v", "for", "v", "in", "all_trainable", "\n", "if", "'block5'", "in", "v", ".", "name", "and", "'Analyzer'", "not", "in", "v", ".", "name", "]", "\n", "base_trainable", "=", "[", "\n", "v", "for", "v", "in", "all_trainable", "\n", "if", "'block5'", "not", "in", "v", ".", "name", "and", "'Analyzer'", "not", "in", "v", ".", "name", "]", "\n", "analyzer_trainable", "=", "[", "\n", "v", "for", "v", "in", "tf", ".", "trainable_variables", "(", ")", "if", "'Analyzer'", "in", "v", ".", "name", "]", "\n", "\n", "# Computes gradients per iteration.", "\n", "grads", "=", "tf", ".", "gradients", "(", "reduced_loss", ",", "\n", "base_trainable", "+", "pred_trainable", ")", "\n", "grads_base", "=", "grads", "[", ":", "len", "(", "base_trainable", ")", "]", "\n", "grads_pred", "=", "grads", "[", "len", "(", "base_trainable", ")", ":", "]", "\n", "grads_analyzer_max", "=", "[", "\n", "g", "if", "g", "is", "not", "None", "else", "None", "\n", "for", "g", "in", "tf", ".", "gradients", "(", "analyzer_max_loss", ",", "analyzer_trainable", ")", "]", "\n", "grads_analyzer_min", "=", "tf", ".", "gradients", "(", "analyzer_min_loss", ",", "\n", "analyzer_trainable", ")", "\n", "\n", "# Define optimisation parameters.", "\n", "base_lr", "=", "tf", ".", "constant", "(", "args", ".", "learning_rate", ")", "\n", "learning_rate", "=", "tf", ".", "scalar_mul", "(", "\n", "base_lr", ",", "\n", "tf", ".", "pow", "(", "(", "1", "-", "step_ph", "/", "args", ".", "num_steps", ")", ",", "args", ".", "power", ")", ")", "\n", "\n", "opt_base", "=", "tf", ".", "train", ".", "MomentumOptimizer", "(", "learning_rate", "*", "1.0", ",", "\n", "args", ".", "momentum", ")", "\n", "opt_pred", "=", "tf", ".", "train", ".", "MomentumOptimizer", "(", "learning_rate", "*", "1.0", ",", "\n", "args", ".", "momentum", ")", "\n", "opt_analyzer_max", "=", "tf", ".", "train", ".", "MomentumOptimizer", "(", "learning_rate", ",", "\n", "args", ".", "momentum", ")", "\n", "opt_analyzer_min", "=", "tf", ".", "train", ".", "MomentumOptimizer", "(", "learning_rate", ",", "\n", "args", ".", "momentum", ")", "\n", "\n", "# Define tensorflow operations which apply gradients to update variables.", "\n", "train_op_base", "=", "opt_base", ".", "apply_gradients", "(", "\n", "zip", "(", "grads_base", ",", "base_trainable", ")", ")", "\n", "train_op_pred", "=", "opt_pred", ".", "apply_gradients", "(", "\n", "zip", "(", "grads_pred", ",", "pred_trainable", ")", ")", "\n", "train_op_analyzer_max", "=", "opt_analyzer_max", ".", "apply_gradients", "(", "\n", "zip", "(", "grads_analyzer_max", ",", "analyzer_trainable", ")", ")", "\n", "train_op_analyzer_min", "=", "opt_analyzer_min", ".", "apply_gradients", "(", "\n", "zip", "(", "grads_analyzer_min", ",", "analyzer_trainable", ")", ")", "\n", "\n", "train_op_s", "=", "tf", ".", "group", "(", "train_op_base", ",", "train_op_pred", ")", "\n", "train_op_a", "=", "tf", ".", "group", "(", "train_op_analyzer_max", ",", "\n", "train_op_analyzer_min", ")", "\n", "\n", "# Process for visualisation.", "\n", "with", "tf", ".", "device", "(", "'/cpu:0'", ")", ":", "\n", "# Image summary for input image, ground-truth label and prediction.", "\n", "    ", "output_vis", "=", "[", "]", "\n", "in_summary", "=", "tf", ".", "py_func", "(", "\n", "utils", ".", "general", ".", "inv_preprocess", ",", "\n", "[", "image_batch", ",", "IMG_MEAN", "]", ",", "\n", "tf", ".", "uint8", ")", "\n", "output_vis", ".", "append", "(", "in_summary", ")", "\n", "\n", "if", "args", ".", "train_segmentation", ":", "\n", "# Visualize segmentation ground-truths.", "\n", "      ", "labels_vis", "=", "tf", ".", "cast", "(", "label_batch", ",", "dtype", "=", "tf", ".", "uint8", ")", "\n", "lab_summary", "=", "tf", ".", "py_func", "(", "\n", "utils", ".", "general", ".", "decode_labels", ",", "\n", "[", "labels_vis", ",", "args", ".", "num_classes", "]", ",", "\n", "tf", ".", "uint8", ")", "\n", "output_vis", ".", "append", "(", "lab_summary", ")", "\n", "# Visualize segmentation predictions.", "\n", "segmentation_vis", "=", "tf", ".", "image", ".", "resize_images", "(", "\n", "outputs", "[", "0", "]", ",", "\n", "tf", ".", "shape", "(", "image_batch", ")", "[", "1", ":", "3", ",", "]", ",", "\n", "tf", ".", "image", ".", "ResizeMethod", ".", "NEAREST_NEIGHBOR", ")", "\n", "segmentation_vis", "=", "tf", ".", "argmax", "(", "segmentation_vis", ",", "axis", "=", "3", ")", "\n", "segmentation_vis", "=", "tf", ".", "expand_dims", "(", "segmentation_vis", ",", "dim", "=", "3", ")", "\n", "segmentation_vis", "=", "tf", ".", "cast", "(", "segmentation_vis", ",", "\n", "dtype", "=", "tf", ".", "uint8", ")", "\n", "segmentation_summary", "=", "tf", ".", "py_func", "(", "\n", "utils", ".", "general", ".", "decode_labels", ",", "\n", "[", "segmentation_vis", ",", "args", ".", "num_classes", "]", ",", "\n", "tf", ".", "uint8", ")", "\n", "output_vis", ".", "append", "(", "segmentation_summary", ")", "\n", "# Scalar summary of segmentaiton loss.", "\n", "segmentation_loss_summary", "=", "tf", ".", "summary", ".", "scalar", "(", "\n", "'segmentation_loss'", ",", "segmentation_loss", ")", "\n", "rec_segmentation_loss_summary", "=", "tf", ".", "summary", ".", "scalar", "(", "\n", "'rec_segmentation_loss'", ",", "rec_segmentation_loss", ")", "\n", "\n", "", "if", "args", ".", "train_depth", ":", "\n", "# Visualize difference.", "\n", "      ", "depth_vis", "=", "tf", ".", "image", ".", "resize_bilinear", "(", "\n", "tf", ".", "abs", "(", "outputs", "[", "1", "]", "-", "depths", ")", ",", "\n", "tf", ".", "shape", "(", "image_batch", ")", "[", "1", ":", "3", ",", "]", ")", "\n", "depth_vis", "/=", "tf", ".", "reduce_max", "(", "tf", ".", "abs", "(", "depth_diff", ")", ")", "\n", "depth_vis", "=", "tf", ".", "clip_by_value", "(", "depth_vis", ",", "0.0", ",", "1.0", ")", "\n", "depth_vis", "=", "tf", ".", "cast", "(", "depth_vis", "*", "255", ",", "dtype", "=", "tf", ".", "uint8", ")", "\n", "depth_summary", "=", "tf", ".", "tile", "(", "depth_vis", ",", "[", "1", ",", "1", ",", "1", ",", "3", "]", ")", "\n", "output_vis", ".", "append", "(", "depth_summary", ")", "\n", "# Scalar summary of depth loss.", "\n", "depth_loss_summary", "=", "tf", ".", "summary", ".", "scalar", "(", "\n", "'depth_loss'", ",", "depth_loss", ")", "\n", "rec_depth_loss_summary", "=", "tf", ".", "summary", ".", "scalar", "(", "\n", "'rec_depth_loss'", ",", "rec_depth_loss", ")", "\n", "\n", "", "if", "args", ".", "train_normal", ":", "\n", "# Visualize difference.", "\n", "      ", "normal_vis", "=", "tf", ".", "image", ".", "resize_bilinear", "(", "\n", "tf", ".", "abs", "(", "outputs", "[", "2", "]", "-", "normals", ")", ",", "\n", "tf", ".", "shape", "(", "image_batch", ")", "[", "1", ":", "3", ",", "]", ")", "\n", "normal_summary", "=", "tf", ".", "cast", "(", "\n", "normal_vis", "/", "tf", ".", "reduce_max", "(", "normal_vis", ")", "*", "255", ",", "\n", "dtype", "=", "tf", ".", "uint8", ")", "\n", "output_vis", ".", "append", "(", "normal_summary", ")", "\n", "# Scalar summary of surface normal loss.", "\n", "normal_loss_summary", "=", "tf", ".", "summary", ".", "scalar", "(", "\n", "'normal_loss'", ",", "normal_loss", ")", "\n", "rec_normal_loss_summary", "=", "tf", ".", "summary", ".", "scalar", "(", "\n", "'rec_normal_loss'", ",", "rec_normal_loss", ")", "\n", "\n", "", "analyzer_max_loss_summary", "=", "tf", ".", "summary", ".", "scalar", "(", "\n", "'analyzer_max_loss'", ",", "analyzer_max_loss", ")", "\n", "analyzer_min_loss_summary", "=", "tf", ".", "summary", ".", "scalar", "(", "\n", "'analyzer_min_loss'", ",", "analyzer_min_loss", ")", "\n", "\n", "image_summary", "=", "tf", ".", "summary", ".", "image", "(", "\n", "'images'", ",", "\n", "tf", ".", "concat", "(", "axis", "=", "2", ",", "values", "=", "output_vis", ")", ",", "\n", "max_outputs", "=", "args", ".", "batch_size", ")", "\n", "\n", "total_summary", "=", "tf", ".", "summary", ".", "merge_all", "(", ")", "\n", "summary_writer", "=", "tf", ".", "summary", ".", "FileWriter", "(", "\n", "args", ".", "snapshot_dir", ",", "\n", "graph", "=", "tf", ".", "get_default_graph", "(", ")", ")", "\n", "\n", "# Set up tf session and initialize variables. ", "\n", "", "config", "=", "tf", ".", "ConfigProto", "(", ")", "\n", "config", ".", "gpu_options", ".", "allow_growth", "=", "True", "\n", "sess", "=", "tf", ".", "Session", "(", "config", "=", "config", ")", "\n", "init", "=", "tf", ".", "global_variables_initializer", "(", ")", "\n", "\n", "sess", ".", "run", "(", "init", ")", "\n", "\n", "# Saver for storing checkpoints of the model.", "\n", "saver", "=", "tf", ".", "train", ".", "Saver", "(", "var_list", "=", "tf", ".", "global_variables", "(", ")", ",", "\n", "max_to_keep", "=", "10", ")", "\n", "\n", "# Load variables if the checkpoint is provided.", "\n", "if", "args", ".", "restore_from", "is", "not", "None", ":", "\n", "    ", "loader", "=", "tf", ".", "train", ".", "Saver", "(", "var_list", "=", "restore_var", ")", "\n", "load", "(", "loader", ",", "sess", ",", "args", ".", "restore_from", ")", "\n", "\n", "# Start queue threads.", "\n", "", "threads", "=", "tf", ".", "train", ".", "start_queue_runners", "(", "coord", "=", "coord", ",", "sess", "=", "sess", ")", "\n", "\n", "# Iterate over training steps.", "\n", "pbar", "=", "tqdm", "(", "range", "(", "args", ".", "num_steps", ")", ")", "\n", "for", "step", "in", "pbar", ":", "\n", "    ", "start_time", "=", "time", ".", "time", "(", ")", "\n", "img_data", ",", "lab_data", ",", "dph_data", ",", "nrm_data", "=", "sess", ".", "run", "(", "datas_batch", ")", "\n", "feed_dict", "=", "{", "step_ph", ":", "step", ",", "\n", "image_batch", ":", "img_data", ",", "\n", "label_batch", ":", "lab_data", ",", "\n", "depth_batch", ":", "dph_data", ",", "\n", "normal_batch", ":", "nrm_data", "}", "\n", "\n", "sess", ".", "run", "(", "train_op_a", ",", "feed_dict", "=", "feed_dict", ")", "\n", "\n", "step_loss", "=", "0", "\n", "for", "it", "in", "range", "(", "args", ".", "iter_size", ")", ":", "\n", "# Update summary periodically.", "\n", "      ", "if", "it", "==", "args", ".", "iter_size", "-", "1", "and", "step", "%", "args", ".", "update_tb_every", "==", "0", ":", "\n", "        ", "sess_outs", "=", "[", "reduced_loss", ",", "total_summary", ",", "train_op_s", "]", "\n", "loss_value", ",", "summary", ",", "_", "=", "sess", ".", "run", "(", "sess_outs", ",", "feed_dict", "=", "feed_dict", ")", "\n", "summary_writer", ".", "add_summary", "(", "summary", ",", "step", ")", "\n", "", "else", ":", "\n", "        ", "sess_outs", "=", "[", "reduced_loss", ",", "train_op_s", "]", "\n", "loss_value", ",", "_", "=", "sess", ".", "run", "(", "sess_outs", ",", "feed_dict", "=", "feed_dict", ")", "\n", "\n", "", "step_loss", "+=", "loss_value", "\n", "\n", "", "step_loss", "/=", "args", ".", "iter_size", "\n", "\n", "lr", "=", "sess", ".", "run", "(", "learning_rate", ",", "feed_dict", "=", "{", "step_ph", ":", "step", "}", ")", "\n", "\n", "# Save trained model periodically.", "\n", "if", "step", "%", "args", ".", "save_pred_every", "==", "0", "and", "step", ">", "0", ":", "\n", "      ", "save", "(", "saver", ",", "sess", ",", "args", ".", "snapshot_dir", ",", "step", ")", "\n", "\n", "", "duration", "=", "time", ".", "time", "(", ")", "-", "start_time", "\n", "desc", "=", "'loss = {:.3f}, lr = {:.6f}'", ".", "format", "(", "step_loss", ",", "lr", ")", "\n", "pbar", ".", "set_description", "(", "desc", ")", "\n", "\n", "", "coord", ".", "request_stop", "(", ")", "\n", "coord", ".", "join", "(", "threads", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.train.train_stanford_scenes.get_arguments": [[21, 89], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.preprocess.stanford_scenes_down_sample.parse_args"], ["def", "get_arguments", "(", ")", ":", "\n", "  ", "\"\"\"Parse all the arguments provided from the CLI.\n    \n  Returns:\n    A list of parsed arguments.\n  \"\"\"", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Semantic Segmentation'", ")", "\n", "# Data parameters", "\n", "parser", ".", "add_argument", "(", "'--batch_size'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "'Number of images in one step.'", ")", "\n", "parser", ".", "add_argument", "(", "'--data_dir'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "\n", "help", "=", "'/path/to/dataset/.'", ")", "\n", "parser", ".", "add_argument", "(", "'--data_list'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "\n", "help", "=", "'/path/to/datalist/file.'", ")", "\n", "parser", ".", "add_argument", "(", "'--ignore_label'", ",", "type", "=", "int", ",", "default", "=", "255", ",", "\n", "help", "=", "'The index of the label to ignore.'", ")", "\n", "parser", ".", "add_argument", "(", "'--input_size'", ",", "type", "=", "str", ",", "default", "=", "'336,336'", ",", "\n", "help", "=", "'Comma-separated string with H and W of image.'", ")", "\n", "parser", ".", "add_argument", "(", "'--depth_unit'", ",", "type", "=", "float", ",", "default", "=", "512.0", ",", "\n", "help", "=", "'Each pixel value difference means 1/depth_unit '", "\n", "'meters.'", ")", "\n", "parser", ".", "add_argument", "(", "'--train_segmentation'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Whether to train for semantic segmentation.'", ")", "\n", "parser", ".", "add_argument", "(", "'--train_depth'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Whether to train for depth estimation.'", ")", "\n", "parser", ".", "add_argument", "(", "'--train_normal'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Whether to train for surface normals prediction.'", ")", "\n", "\n", "# Training paramters", "\n", "parser", ".", "add_argument", "(", "'--is_training'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Whether to updates weights.'", ")", "\n", "parser", ".", "add_argument", "(", "'--use_global_status'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Whether to updates moving mean and variance.'", ")", "\n", "parser", ".", "add_argument", "(", "'--learning_rate'", ",", "type", "=", "float", ",", "default", "=", "2.5e-4", ",", "\n", "help", "=", "'Base learning rate.'", ")", "\n", "parser", ".", "add_argument", "(", "'--power'", ",", "type", "=", "float", ",", "default", "=", "0.9", ",", "\n", "help", "=", "'Decay for poly learing rate policy.'", ")", "\n", "parser", ".", "add_argument", "(", "'--momentum'", ",", "type", "=", "float", ",", "default", "=", "0.9", ",", "\n", "help", "=", "'Momentum component of the optimiser.'", ")", "\n", "parser", ".", "add_argument", "(", "'--weight_decay'", ",", "type", "=", "float", ",", "default", "=", "5e-4", ",", "\n", "help", "=", "'Regularisation parameter for L2-loss.'", ")", "\n", "parser", ".", "add_argument", "(", "'--num_classes'", ",", "type", "=", "int", ",", "default", "=", "50", ",", "\n", "help", "=", "'Number of classes to predict.'", ")", "\n", "parser", ".", "add_argument", "(", "'--num_steps'", ",", "type", "=", "int", ",", "default", "=", "20000", ",", "\n", "help", "=", "'Number of training steps.'", ")", "\n", "parser", ".", "add_argument", "(", "'--iter_size'", ",", "type", "=", "int", ",", "default", "=", "10", ",", "\n", "help", "=", "'Number of iteration to update weights'", ")", "\n", "parser", ".", "add_argument", "(", "'--random_mirror'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Whether to randomly mirror the inputs.'", ")", "\n", "parser", ".", "add_argument", "(", "'--random_crop'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Whether to randomly crop the inputs.'", ")", "\n", "parser", ".", "add_argument", "(", "'--random_scale'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Whether to randomly scale the inputs.'", ")", "\n", "parser", ".", "add_argument", "(", "'--upscale_predictions'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Whether to upscale resolution of predictions.'", ")", "\n", "# Misc paramters", "\n", "parser", ".", "add_argument", "(", "'--restore_from'", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "'Where restore model parameters from.'", ")", "\n", "parser", ".", "add_argument", "(", "'--save_pred_every'", ",", "type", "=", "int", ",", "default", "=", "10000", ",", "\n", "help", "=", "'Save summaries and checkpoint every often.'", ")", "\n", "parser", ".", "add_argument", "(", "'--update_tb_every'", ",", "type", "=", "int", ",", "default", "=", "200", ",", "\n", "help", "=", "'Update Tensorboard summaries every often.'", ")", "\n", "parser", ".", "add_argument", "(", "'--snapshot_dir'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "\n", "help", "=", "'Where to save snapshots of the model.'", ")", "\n", "parser", ".", "add_argument", "(", "'--not_restore_classifier'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Whether to not restore classifier layers.'", ")", "\n", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.train.train_stanford_scenes.save": [[91, 107], ["os.path.join", "saver.save", "print", "os.path.exists", "os.makedirs"], "function", ["home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.train.train_stanford_scenes_cgan.save"], ["", "def", "save", "(", "saver", ",", "sess", ",", "logdir", ",", "step", ")", ":", "\n", "  ", "\"\"\"Saves the trained weights.\n   \n  Args:\n    saver: TensorFlow Saver object.\n    sess: TensorFlow session.\n    logdir: path to the snapshots directory.\n    step: current training step.\n  \"\"\"", "\n", "model_name", "=", "'model.ckpt'", "\n", "checkpoint_path", "=", "os", ".", "path", ".", "join", "(", "logdir", ",", "model_name", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "logdir", ")", ":", "\n", "    ", "os", ".", "makedirs", "(", "logdir", ")", "\n", "", "saver", ".", "save", "(", "sess", ",", "checkpoint_path", ",", "global_step", "=", "step", ")", "\n", "print", "(", "'The checkpoint has been created.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.train.train_stanford_scenes.load": [[109, 119], ["saver.restore", "print"], "function", ["None"], ["", "def", "load", "(", "saver", ",", "sess", ",", "ckpt_path", ")", ":", "\n", "  ", "\"\"\"Loads the trained weights.\n    \n  Args:\n    saver: TensorFlow Saver object.\n    sess: TensorFlow session.\n    ckpt_path: path to checkpoint file with parameters.\n  \"\"\"", "\n", "saver", ".", "restore", "(", "sess", ",", "ckpt_path", ")", "\n", "print", "(", "'Restored model parameters from {}'", ".", "format", "(", "ckpt_path", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.train.train_stanford_scenes.main": [[121, 416], ["train_stanford_scenes.get_arguments", "utils.general.snapshot_arg", "map", "tensorflow.train.Coordinator", "tensorflow.placeholder", "stanford_scenes.models.structured_predictor.structured_predictor", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.less_equal", "tensorflow.greater", "tensorflow.reduce_all", "tensorflow.equal", "tensorflow.logical_and", "tensorflow.logical_not", "tensorflow.squeeze", "tensorflow.squeeze", "tensorflow.squeeze", "tensorflow.to_int32", "tensorflow.reshape", "tensorflow.gather", "tensorflow.nn.sparse_softmax_cross_entropy_with_logits", "tensorflow.reduce_mean", "tensorflow.gather", "tensorflow.gather", "tensorflow.reduce_mean", "tensorflow.reduce_sum", "tensorflow.gather", "tensorflow.nn.l2_normalize", "tensorflow.gather", "tf.add_n.append", "tensorflow.add_n", "tensorflow.trainable_variables", "tensorflow.gradients", "tensorflow.constant", "tensorflow.scalar_mul", "tensorflow.train.MomentumOptimizer", "tensorflow.train.MomentumOptimizer", "tf.train.MomentumOptimizer.apply_gradients", "tf.train.MomentumOptimizer.apply_gradients", "tensorflow.group", "tensorflow.ConfigProto", "tensorflow.Session", "tensorflow.global_variables_initializer", "tf.Session.run", "tensorflow.train.Saver", "tensorflow.train.start_queue_runners", "tqdm.tqdm", "tf.train.Coordinator.request_stop", "tf.train.Coordinator.join", "get_arguments.input_size.split", "tensorflow.device", "enumerate", "tensorflow.image.resize_nearest_neighbor", "tensorflow.image.resize_nearest_neighbor", "tensorflow.image.resize_nearest_neighbor", "tensorflow.equal", "tensorflow.abs", "tensorflow.where", "tensorflow.where", "tensorflow.where", "tensorflow.gather", "tf.add_n.append", "tensorflow.reshape", "tf.add_n.append", "tensorflow.reshape", "tensorflow.reduce_mean", "tf.add_n.append", "tensorflow.add_n", "tensorflow.pow", "zip", "zip", "tensorflow.device", "tensorflow.py_func", "output_vis.append", "tensorflow.summary.image", "tensorflow.summary.merge_all", "tensorflow.summary.FileWriter", "tensorflow.train.Saver", "train_stanford_scenes.load", "range", "time.time", "range", "tf.Session.run", "tqdm.tqdm.set_description", "tensorflow.name_scope", "stanford_scenes.image_reader.ImageReader", "stanford_scenes.image_reader.ImageReader.dequeue", "tensorflow.global_variables", "tensorflow.abs", "tensorflow.reduce_sum", "tensorflow.nn.l2_loss", "tensorflow.trainable_variables", "len", "len", "tensorflow.cast", "tensorflow.py_func", "output_vis.append", "tensorflow.image.resize_images", "tensorflow.argmax", "tensorflow.expand_dims", "tensorflow.cast", "tensorflow.py_func", "output_vis.append", "tensorflow.summary.scalar", "tensorflow.image.resize_bilinear", "tensorflow.reduce_max", "tensorflow.clip_by_value", "tensorflow.cast", "tensorflow.tile", "output_vis.append", "tensorflow.summary.scalar", "tensorflow.image.resize_bilinear", "tensorflow.cast", "output_vis.append", "tensorflow.summary.scalar", "tensorflow.concat", "tensorflow.global_variables", "train_stanford_scenes.save", "time.time", "output.get_shape().as_list", "tensorflow.image.resize_bilinear", "outputs[].get_shape().as_list", "outputs[].get_shape().as_list", "outputs[].get_shape().as_list", "tensorflow.abs", "tensorflow.abs", "tensorflow.abs", "tensorflow.get_default_graph", "tf.Session.run", "tf.summary.FileWriter.add_summary", "tf.Session.run", "tensorflow.shape", "tensorflow.shape", "tensorflow.shape", "output.get_shape", "outputs[].get_shape", "outputs[].get_shape", "outputs[].get_shape", "tensorflow.nn.l2_normalize", "tensorflow.reduce_max"], "function", ["home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.inference.inference_stanford_scenes.get_arguments", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.utils.general.snapshot_arg", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.fcrn.structured_predictor", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.inference.inference_stanford_scenes.load", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.stanford_scenes.image_reader.ImageReader.dequeue", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.train.train_stanford_scenes_cgan.save"], ["", "def", "main", "(", ")", ":", "\n", "  ", "\"\"\"Create the model and start training.\n  \"\"\"", "\n", "# Read CL arguments and snapshot the arguments into text file.", "\n", "args", "=", "get_arguments", "(", ")", "\n", "utils", ".", "general", ".", "snapshot_arg", "(", "args", ")", "\n", "\n", "# The input size.", "\n", "h", ",", "w", "=", "map", "(", "int", ",", "args", ".", "input_size", ".", "split", "(", "','", ")", ")", "\n", "input_size", "=", "(", "h", ",", "w", ")", "\n", "\n", "# Create queue coordinator.", "\n", "coord", "=", "tf", ".", "train", ".", "Coordinator", "(", ")", "\n", "\n", "# current step", "\n", "step_ph", "=", "tf", ".", "placeholder", "(", "dtype", "=", "tf", ".", "float32", ",", "shape", "=", "(", ")", ")", "\n", "\n", "# Load the data reader.", "\n", "with", "tf", ".", "device", "(", "'/cpu:0'", ")", ":", "\n", "    ", "with", "tf", ".", "name_scope", "(", "'create_inputs'", ")", ":", "\n", "      ", "reader", "=", "ImageReader", "(", "\n", "args", ".", "data_dir", ",", "\n", "args", ".", "data_list", ",", "\n", "input_size", ",", "\n", "args", ".", "random_scale", ",", "\n", "args", ".", "random_mirror", ",", "\n", "args", ".", "random_crop", ",", "\n", "args", ".", "ignore_label", ",", "\n", "IMG_MEAN", ")", "\n", "\n", "datas_batch", "=", "reader", ".", "dequeue", "(", "args", ".", "batch_size", ")", "\n", "image_batch", "=", "datas_batch", "[", "0", "]", "\n", "label_batch", "=", "datas_batch", "[", "1", "]", "\n", "depth_batch", "=", "datas_batch", "[", "2", "]", "/", "args", ".", "depth_unit", "\n", "normal_batch", "=", "datas_batch", "[", "3", "]", "\n", "\n", "# Create network and predictions.", "\n", "", "", "outputs", "=", "structured_predictor", "(", "image_batch", ",", "\n", "args", ".", "num_classes", ",", "\n", "args", ".", "is_training", ",", "\n", "args", ".", "use_global_status", ")", "\n", "\n", "# Either up-sample predictions or down-sample ground-truths.", "\n", "if", "args", ".", "upscale_predictions", ":", "\n", "    ", "for", "output_index", ",", "output", "in", "enumerate", "(", "outputs", ")", ":", "\n", "      ", "out_h", ",", "out_w", "=", "output", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "1", ":", "3", "]", "\n", "if", "out_h", "!=", "h", "or", "out_w", "!=", "w", ":", "\n", "        ", "outputs", "[", "output_index", "]", "=", "tf", ".", "image", ".", "resize_bilinear", "(", "\n", "output", ",", "[", "h", ",", "w", "]", ")", "\n", "", "", "labels", "=", "label_batch", "\n", "depths", "=", "depth_batch", "\n", "normals", "=", "normal_batch", "\n", "", "else", ":", "\n", "    ", "labels", "=", "tf", ".", "image", ".", "resize_nearest_neighbor", "(", "\n", "label_batch", ",", "outputs", "[", "0", "]", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "1", ":", "3", "]", ")", "\n", "depths", "=", "tf", ".", "image", ".", "resize_nearest_neighbor", "(", "\n", "depth_batch", ",", "outputs", "[", "1", "]", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "1", ":", "3", "]", ")", "\n", "normals", "=", "tf", ".", "image", ".", "resize_nearest_neighbor", "(", "\n", "normal_batch", ",", "outputs", "[", "2", "]", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "1", ":", "3", "]", ")", "\n", "\n", "", "labels_flat", "=", "tf", ".", "reshape", "(", "labels", ",", "(", "-", "1", ",", ")", ")", "\n", "depths_flat", "=", "tf", ".", "reshape", "(", "depths", ",", "(", "-", "1", ",", ")", ")", "\n", "normals_flat", "=", "tf", ".", "reshape", "(", "normals", ",", "(", "-", "1", ",", "3", ")", ")", "\n", "\n", "# Ignore the location where the label value is larger than args.num_classes.", "\n", "not_ignore_labels", "=", "tf", ".", "less_equal", "(", "labels_flat", ",", "args", ".", "num_classes", "-", "1", ")", "\n", "\n", "# Ignore the location where the depth value <= 0", "\n", "not_ignore_depths", "=", "tf", ".", "greater", "(", "depths_flat", ",", "0.0", ")", "\n", "\n", "# Ignore the location where the normal value != [128,128,128].", "\n", "# The normal is centered at 127.5 in ImageReader.", "\n", "ignore_normal_yz", "=", "tf", ".", "reduce_all", "(", "\n", "tf", ".", "equal", "(", "normals_flat", "[", ":", ",", "1", ":", "]", ",", "0.5", ")", ",", "\n", "axis", "=", "-", "1", ")", "\n", "ignore_normal_x", "=", "tf", ".", "equal", "(", "tf", ".", "abs", "(", "normals_flat", "[", ":", ",", "0", "]", ")", ",", "0.5", ")", "\n", "ignore_normals", "=", "tf", ".", "logical_and", "(", "ignore_normal_yz", ",", "ignore_normal_x", ")", "\n", "not_ignore_normals", "=", "tf", ".", "logical_not", "(", "ignore_normals", ")", "\n", "\n", "# Extract the indices of labels where the gradients are propogated.", "\n", "valid_label_inds", "=", "tf", ".", "squeeze", "(", "tf", ".", "where", "(", "not_ignore_labels", ")", ",", "1", ")", "\n", "valid_depth_inds", "=", "tf", ".", "squeeze", "(", "tf", ".", "where", "(", "not_ignore_depths", ")", ",", "1", ")", "\n", "valid_normal_inds", "=", "tf", ".", "squeeze", "(", "tf", ".", "where", "(", "not_ignore_normals", ")", ",", "1", ")", "\n", "\n", "# Grab variable names which should be restored from checkpoints.", "\n", "restore_var", "=", "[", "\n", "v", "for", "v", "in", "tf", ".", "global_variables", "(", ")", "\n", "if", "'block5'", "not", "in", "v", ".", "name", "or", "not", "args", ".", "not_restore_classifier", "]", "\n", "\n", "reduced_loss", "=", "[", "]", "\n", "\n", "# Define softmax loss.", "\n", "labels_gather", "=", "tf", ".", "to_int32", "(", "tf", ".", "gather", "(", "labels_flat", ",", "valid_label_inds", ")", ")", "\n", "segmentation_output", "=", "tf", ".", "reshape", "(", "outputs", "[", "0", "]", ",", "\n", "[", "-", "1", ",", "args", ".", "num_classes", "]", ")", "\n", "segmentation_output", "=", "tf", ".", "gather", "(", "segmentation_output", ",", "\n", "valid_label_inds", ")", "\n", "segmentation_loss", "=", "tf", ".", "nn", ".", "sparse_softmax_cross_entropy_with_logits", "(", "\n", "logits", "=", "segmentation_output", ",", "\n", "labels", "=", "labels_gather", ")", "\n", "segmentation_loss", "=", "tf", ".", "reduce_mean", "(", "segmentation_loss", ")", "\n", "if", "args", ".", "train_segmentation", ":", "\n", "    ", "reduced_loss", ".", "append", "(", "segmentation_loss", ")", "\n", "\n", "# Define depth loss.", "\n", "", "depths_gather", "=", "tf", ".", "gather", "(", "depths_flat", ",", "valid_depth_inds", ")", "\n", "depth_output", "=", "tf", ".", "gather", "(", "tf", ".", "reshape", "(", "outputs", "[", "1", "]", ",", "[", "-", "1", ",", "]", ")", ",", "\n", "valid_depth_inds", ")", "\n", "depth_diff", "=", "depth_output", "-", "depths_gather", "\n", "depth_loss", "=", "tf", ".", "reduce_mean", "(", "depth_diff", "**", "2", ")", "\n", "depth_absrel", "=", "tf", ".", "reduce_sum", "(", "\n", "tf", ".", "abs", "(", "depth_diff", ")", "/", "depths_gather", ")", "\n", "depth_absrel", "/=", "args", ".", "batch_size", "\n", "if", "args", ".", "train_depth", ":", "\n", "    ", "reduced_loss", ".", "append", "(", "depth_loss", ")", "\n", "\n", "# Define surface normal loss.", "\n", "", "normals_gather", "=", "tf", ".", "gather", "(", "normals_flat", ",", "valid_normal_inds", ")", "\n", "normals_gather", "=", "tf", ".", "nn", ".", "l2_normalize", "(", "normals_gather", ",", "dim", "=", "-", "1", ")", "\n", "normal_output", "=", "tf", ".", "gather", "(", "tf", ".", "reshape", "(", "outputs", "[", "2", "]", ",", "[", "-", "1", ",", "3", "]", ")", ",", "\n", "valid_normal_inds", ")", "\n", "normal_loss", "=", "-", "tf", ".", "reduce_mean", "(", "tf", ".", "reduce_sum", "(", "\n", "normal_output", "*", "normals_gather", ",", "axis", "=", "-", "1", ")", ")", "\n", "normal_loss", "*=", "10.0", "\n", "if", "args", ".", "train_normal", ":", "\n", "    ", "reduced_loss", ".", "append", "(", "normal_loss", ")", "\n", "\n", "# Define weight regularization loss.", "\n", "", "w", "=", "args", ".", "weight_decay", "\n", "l2_losses", "=", "[", "w", "*", "tf", ".", "nn", ".", "l2_loss", "(", "v", ")", "for", "v", "in", "tf", ".", "trainable_variables", "(", ")", "\n", "if", "'weights'", "in", "v", ".", "name", "]", "\n", "reduced_loss", ".", "append", "(", "tf", ".", "add_n", "(", "l2_losses", ")", ")", "\n", "\n", "# Sum all losses.", "\n", "reduced_loss", "=", "tf", ".", "add_n", "(", "reduced_loss", ")", "\n", "\n", "# Grab variable names which are used for training.", "\n", "all_trainable", "=", "tf", ".", "trainable_variables", "(", ")", "\n", "pred_trainable", "=", "[", "\n", "v", "for", "v", "in", "all_trainable", "if", "'block5'", "in", "v", ".", "name", "]", "# lr*10", "\n", "base_trainable", "=", "[", "\n", "v", "for", "v", "in", "all_trainable", "if", "'block5'", "not", "in", "v", ".", "name", "]", "# lr*1", "\n", "\n", "# Computes gradients per iteration.", "\n", "grads", "=", "tf", ".", "gradients", "(", "reduced_loss", ",", "\n", "base_trainable", "+", "pred_trainable", ")", "\n", "grads_base", "=", "grads", "[", ":", "len", "(", "base_trainable", ")", "]", "\n", "grads_pred", "=", "grads", "[", "len", "(", "base_trainable", ")", ":", "]", "\n", "\n", "# Define optimisation parameters.", "\n", "base_lr", "=", "tf", ".", "constant", "(", "args", ".", "learning_rate", ")", "\n", "learning_rate", "=", "tf", ".", "scalar_mul", "(", "\n", "base_lr", ",", "\n", "tf", ".", "pow", "(", "(", "1", "-", "step_ph", "/", "args", ".", "num_steps", ")", ",", "args", ".", "power", ")", ")", "\n", "\n", "opt_base", "=", "tf", ".", "train", ".", "MomentumOptimizer", "(", "learning_rate", "*", "1.0", ",", "\n", "args", ".", "momentum", ")", "\n", "opt_pred", "=", "tf", ".", "train", ".", "MomentumOptimizer", "(", "learning_rate", "*", "1.0", ",", "\n", "args", ".", "momentum", ")", "\n", "\n", "# Define tensorflow operations which apply gradients to update variables.", "\n", "train_op_base", "=", "opt_base", ".", "apply_gradients", "(", "\n", "zip", "(", "grads_base", ",", "base_trainable", ")", ")", "\n", "train_op_pred", "=", "opt_pred", ".", "apply_gradients", "(", "\n", "zip", "(", "grads_pred", ",", "pred_trainable", ")", ")", "\n", "train_op", "=", "tf", ".", "group", "(", "train_op_base", ",", "train_op_pred", ")", "\n", "\n", "# Process for visualisation.", "\n", "with", "tf", ".", "device", "(", "'/cpu:0'", ")", ":", "\n", "# Image summary for input image, ground-truth label and prediction.", "\n", "    ", "output_vis", "=", "[", "]", "\n", "in_summary", "=", "tf", ".", "py_func", "(", "\n", "utils", ".", "general", ".", "inv_preprocess", ",", "\n", "[", "image_batch", ",", "IMG_MEAN", "]", ",", "\n", "tf", ".", "uint8", ")", "\n", "output_vis", ".", "append", "(", "in_summary", ")", "\n", "\n", "if", "args", ".", "train_segmentation", ":", "\n", "# Visualize segmentation ground-truths.", "\n", "      ", "labels_vis", "=", "tf", ".", "cast", "(", "label_batch", ",", "dtype", "=", "tf", ".", "uint8", ")", "\n", "lab_summary", "=", "tf", ".", "py_func", "(", "\n", "utils", ".", "general", ".", "decode_labels", ",", "\n", "[", "labels_vis", ",", "args", ".", "num_classes", "]", ",", "\n", "tf", ".", "uint8", ")", "\n", "output_vis", ".", "append", "(", "lab_summary", ")", "\n", "# Visualize segmentation predictions.", "\n", "segmentation_vis", "=", "tf", ".", "image", ".", "resize_images", "(", "\n", "outputs", "[", "0", "]", ",", "\n", "tf", ".", "shape", "(", "image_batch", ")", "[", "1", ":", "3", ",", "]", ",", "\n", "tf", ".", "image", ".", "ResizeMethod", ".", "NEAREST_NEIGHBOR", ")", "\n", "segmentation_vis", "=", "tf", ".", "argmax", "(", "segmentation_vis", ",", "\n", "axis", "=", "3", ")", "\n", "segmentation_vis", "=", "tf", ".", "expand_dims", "(", "segmentation_vis", ",", "\n", "dim", "=", "3", ")", "\n", "segmentation_vis", "=", "tf", ".", "cast", "(", "segmentation_vis", ",", "\n", "dtype", "=", "tf", ".", "uint8", ")", "\n", "segmentation_summary", "=", "tf", ".", "py_func", "(", "\n", "utils", ".", "general", ".", "decode_labels", ",", "\n", "[", "segmentation_vis", ",", "args", ".", "num_classes", "]", ",", "\n", "tf", ".", "uint8", ")", "\n", "output_vis", ".", "append", "(", "segmentation_summary", ")", "\n", "# Scalar summary of segmentaiton loss.", "\n", "segmentation_loss_summary", "=", "tf", ".", "summary", ".", "scalar", "(", "\n", "'segmentation_loss'", ",", "segmentation_loss", ")", "\n", "\n", "", "if", "args", ".", "train_depth", ":", "\n", "# Visualize difference.", "\n", "      ", "depth_vis", "=", "tf", ".", "image", ".", "resize_bilinear", "(", "\n", "tf", ".", "abs", "(", "outputs", "[", "1", "]", "-", "depths", ")", ",", "\n", "tf", ".", "shape", "(", "image_batch", ")", "[", "1", ":", "3", ",", "]", ")", "\n", "depth_vis", "/=", "tf", ".", "reduce_max", "(", "tf", ".", "abs", "(", "depth_diff", ")", ")", "\n", "depth_vis", "=", "tf", ".", "clip_by_value", "(", "depth_vis", ",", "0.0", ",", "1.0", ")", "\n", "depth_vis", "=", "tf", ".", "cast", "(", "depth_vis", "*", "255", ",", "dtype", "=", "tf", ".", "uint8", ")", "\n", "depth_summary", "=", "tf", ".", "tile", "(", "depth_vis", ",", "[", "1", ",", "1", ",", "1", ",", "3", "]", ")", "\n", "output_vis", ".", "append", "(", "depth_summary", ")", "\n", "# Scalar summary of depth loss.", "\n", "depth_loss_summary", "=", "tf", ".", "summary", ".", "scalar", "(", "\n", "'depth_loss'", ",", "depth_loss", ")", "\n", "\n", "", "if", "args", ".", "train_normal", ":", "\n", "# Visualize difference.", "\n", "      ", "normal_vis", "=", "tf", ".", "image", ".", "resize_bilinear", "(", "\n", "tf", ".", "abs", "(", "outputs", "[", "2", "]", "-", "tf", ".", "nn", ".", "l2_normalize", "(", "normals", ",", "dim", "=", "-", "1", ")", ")", ",", "\n", "tf", ".", "shape", "(", "image_batch", ")", "[", "1", ":", "3", ",", "]", ")", "\n", "normal_summary", "=", "tf", ".", "cast", "(", "\n", "normal_vis", "/", "tf", ".", "reduce_max", "(", "normal_vis", ")", "*", "255", ",", "\n", "dtype", "=", "tf", ".", "uint8", ")", "\n", "output_vis", ".", "append", "(", "normal_summary", ")", "\n", "# Scalar summary of surface normal loss.", "\n", "normal_loss_summary", "=", "tf", ".", "summary", ".", "scalar", "(", "\n", "'normal_loss'", ",", "normal_loss", ")", "\n", "\n", "", "image_summary", "=", "tf", ".", "summary", ".", "image", "(", "\n", "'images'", ",", "\n", "tf", ".", "concat", "(", "axis", "=", "2", ",", "values", "=", "output_vis", ")", ",", "\n", "max_outputs", "=", "args", ".", "batch_size", ")", "\n", "\n", "total_summary", "=", "tf", ".", "summary", ".", "merge_all", "(", ")", "\n", "summary_writer", "=", "tf", ".", "summary", ".", "FileWriter", "(", "\n", "args", ".", "snapshot_dir", ",", "\n", "graph", "=", "tf", ".", "get_default_graph", "(", ")", ")", "\n", "\n", "# Set up tf session and initialize variables. ", "\n", "", "config", "=", "tf", ".", "ConfigProto", "(", ")", "\n", "config", ".", "gpu_options", ".", "allow_growth", "=", "True", "\n", "sess", "=", "tf", ".", "Session", "(", "config", "=", "config", ")", "\n", "init", "=", "tf", ".", "global_variables_initializer", "(", ")", "\n", "\n", "sess", ".", "run", "(", "init", ")", "\n", "\n", "# Saver for storing checkpoints of the model.", "\n", "saver", "=", "tf", ".", "train", ".", "Saver", "(", "var_list", "=", "tf", ".", "global_variables", "(", ")", ",", "\n", "max_to_keep", "=", "10", ")", "\n", "\n", "# Load variables if the checkpoint is provided.", "\n", "if", "args", ".", "restore_from", "is", "not", "None", ":", "\n", "    ", "loader", "=", "tf", ".", "train", ".", "Saver", "(", "var_list", "=", "restore_var", ")", "\n", "load", "(", "loader", ",", "sess", ",", "args", ".", "restore_from", ")", "\n", "\n", "# Start queue threads.", "\n", "", "threads", "=", "tf", ".", "train", ".", "start_queue_runners", "(", "coord", "=", "coord", ",", "sess", "=", "sess", ")", "\n", "\n", "# Iterate over training steps.", "\n", "pbar", "=", "tqdm", "(", "range", "(", "args", ".", "num_steps", ")", ")", "\n", "for", "step", "in", "pbar", ":", "\n", "    ", "start_time", "=", "time", ".", "time", "(", ")", "\n", "feed_dict", "=", "{", "step_ph", ":", "step", "}", "\n", "\n", "step_loss", "=", "0", "\n", "for", "it", "in", "range", "(", "args", ".", "iter_size", ")", ":", "\n", "# Update summary periodically.", "\n", "      ", "if", "it", "==", "args", ".", "iter_size", "-", "1", "and", "step", "%", "args", ".", "update_tb_every", "==", "0", ":", "\n", "        ", "sess_outs", "=", "[", "reduced_loss", ",", "total_summary", ",", "train_op", "]", "\n", "loss_value", ",", "summary", ",", "_", "=", "sess", ".", "run", "(", "sess_outs", ",", "feed_dict", "=", "feed_dict", ")", "\n", "summary_writer", ".", "add_summary", "(", "summary", ",", "step", ")", "\n", "", "else", ":", "\n", "        ", "sess_outs", "=", "[", "reduced_loss", ",", "train_op", "]", "\n", "loss_value", ",", "_", "=", "sess", ".", "run", "(", "sess_outs", ",", "feed_dict", "=", "feed_dict", ")", "\n", "\n", "", "step_loss", "+=", "loss_value", "\n", "\n", "", "step_loss", "/=", "args", ".", "iter_size", "\n", "\n", "lr", "=", "sess", ".", "run", "(", "learning_rate", ",", "feed_dict", "=", "feed_dict", ")", "\n", "\n", "# Save trained model periodically.", "\n", "if", "step", "%", "args", ".", "save_pred_every", "==", "0", "and", "step", ">", "0", ":", "\n", "      ", "save", "(", "saver", ",", "sess", ",", "args", ".", "snapshot_dir", ",", "step", ")", "\n", "\n", "", "duration", "=", "time", ".", "time", "(", ")", "-", "start_time", "\n", "desc", "=", "'loss = {:.3f}, lr = {:.6f}'", ".", "format", "(", "step_loss", ",", "lr", ")", "\n", "pbar", ".", "set_description", "(", "desc", ")", "\n", "\n", "", "coord", ".", "request_stop", "(", ")", "\n", "coord", ".", "join", "(", "threads", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.train.train_stanford_scenes_cgan.get_arguments": [[22, 94], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.preprocess.stanford_scenes_down_sample.parse_args"], ["def", "get_arguments", "(", ")", ":", "\n", "  ", "\"\"\"Parse all the arguments provided from the CLI.\n    \n  Returns:\n    A list of parsed arguments.\n  \"\"\"", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Semantic Segmentation'", ")", "\n", "# Data parameters", "\n", "parser", ".", "add_argument", "(", "'--batch_size'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "'Number of images in one step.'", ")", "\n", "parser", ".", "add_argument", "(", "'--data_dir'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "\n", "help", "=", "'/path/to/dataset/.'", ")", "\n", "parser", ".", "add_argument", "(", "'--data_list'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "\n", "help", "=", "'/path/to/datalist/file.'", ")", "\n", "parser", ".", "add_argument", "(", "'--ignore_label'", ",", "type", "=", "int", ",", "default", "=", "255", ",", "\n", "help", "=", "'The index of the label to ignore.'", ")", "\n", "parser", ".", "add_argument", "(", "'--input_size'", ",", "type", "=", "str", ",", "default", "=", "'336,336'", ",", "\n", "help", "=", "'Comma-separated string with H and W of image.'", ")", "\n", "parser", ".", "add_argument", "(", "'--depth_unit'", ",", "type", "=", "float", ",", "default", "=", "512.0", ",", "\n", "help", "=", "'Each pixel value difference means 1/depth_unit '", "\n", "'meters.'", ")", "\n", "parser", ".", "add_argument", "(", "'--train_segmentation'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Whether to train for semantic segmentation.'", ")", "\n", "parser", ".", "add_argument", "(", "'--train_depth'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Whether to train for depth estimation.'", ")", "\n", "parser", ".", "add_argument", "(", "'--train_normal'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Whether to train for surface normals prediction.'", ")", "\n", "parser", ".", "add_argument", "(", "'--gan_lambda'", ",", "type", "=", "float", ",", "default", "=", "0.01", ",", "\n", "help", "=", "'Weighting paramters for GAN loss.'", ")", "\n", "parser", ".", "add_argument", "(", "'--sup_lambda'", ",", "type", "=", "float", ",", "default", "=", "1.0", ",", "\n", "help", "=", "'Weighting paramters for supervising loss.'", ")", "\n", "\n", "# Training paramters", "\n", "parser", ".", "add_argument", "(", "'--is_training'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Whether to updates weights.'", ")", "\n", "parser", ".", "add_argument", "(", "'--use_global_status'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Whether to updates moving mean and variance.'", ")", "\n", "parser", ".", "add_argument", "(", "'--learning_rate'", ",", "type", "=", "float", ",", "default", "=", "2.5e-4", ",", "\n", "help", "=", "'Base learning rate.'", ")", "\n", "parser", ".", "add_argument", "(", "'--power'", ",", "type", "=", "float", ",", "default", "=", "0.9", ",", "\n", "help", "=", "'Decay for poly learing rate policy.'", ")", "\n", "parser", ".", "add_argument", "(", "'--momentum'", ",", "type", "=", "float", ",", "default", "=", "0.9", ",", "\n", "help", "=", "'Momentum component of the optimiser.'", ")", "\n", "parser", ".", "add_argument", "(", "'--weight_decay'", ",", "type", "=", "float", ",", "default", "=", "5e-4", ",", "\n", "help", "=", "'Regularisation parameter for L2-loss.'", ")", "\n", "parser", ".", "add_argument", "(", "'--num_classes'", ",", "type", "=", "int", ",", "default", "=", "50", ",", "\n", "help", "=", "'Number of classes to predict.'", ")", "\n", "parser", ".", "add_argument", "(", "'--num_steps'", ",", "type", "=", "int", ",", "default", "=", "20000", ",", "\n", "help", "=", "'Number of training steps.'", ")", "\n", "parser", ".", "add_argument", "(", "'--iter_size'", ",", "type", "=", "int", ",", "default", "=", "10", ",", "\n", "help", "=", "'Number of iteration to update weights'", ")", "\n", "parser", ".", "add_argument", "(", "'--random_mirror'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Whether to randomly mirror the inputs.'", ")", "\n", "parser", ".", "add_argument", "(", "'--random_crop'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Whether to randomly crop the inputs.'", ")", "\n", "parser", ".", "add_argument", "(", "'--random_scale'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Whether to randomly scale the inputs.'", ")", "\n", "parser", ".", "add_argument", "(", "'--upscale_predictions'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Whether to upscale resolution of predictions.'", ")", "\n", "# Misc paramters", "\n", "parser", ".", "add_argument", "(", "'--restore_from'", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "'Where restore model parameters from.'", ")", "\n", "parser", ".", "add_argument", "(", "'--save_pred_every'", ",", "type", "=", "int", ",", "default", "=", "10000", ",", "\n", "help", "=", "'Save summaries and checkpoint every often.'", ")", "\n", "parser", ".", "add_argument", "(", "'--update_tb_every'", ",", "type", "=", "int", ",", "default", "=", "200", ",", "\n", "help", "=", "'Update Tensorboard summaries every often.'", ")", "\n", "parser", ".", "add_argument", "(", "'--snapshot_dir'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "\n", "help", "=", "'Where to save snapshots of the model.'", ")", "\n", "parser", ".", "add_argument", "(", "'--not_restore_classifier'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Whether to not restore classifier layers.'", ")", "\n", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.train.train_stanford_scenes_cgan.save": [[96, 112], ["os.path.join", "saver.save", "print", "os.path.exists", "os.makedirs"], "function", ["home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.train.train_stanford_scenes_cgan.save"], ["", "def", "save", "(", "saver", ",", "sess", ",", "logdir", ",", "step", ")", ":", "\n", "  ", "\"\"\"Saves the trained weights.\n   \n  Args:\n    saver: TensorFlow Saver object.\n    sess: TensorFlow session.\n    logdir: path to the snapshots directory.\n    step: current training step.\n  \"\"\"", "\n", "model_name", "=", "'model.ckpt'", "\n", "checkpoint_path", "=", "os", ".", "path", ".", "join", "(", "logdir", ",", "model_name", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "logdir", ")", ":", "\n", "    ", "os", ".", "makedirs", "(", "logdir", ")", "\n", "", "saver", ".", "save", "(", "sess", ",", "checkpoint_path", ",", "global_step", "=", "step", ")", "\n", "print", "(", "'The checkpoint has been created.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.train.train_stanford_scenes_cgan.load": [[114, 124], ["saver.restore", "print"], "function", ["None"], ["", "def", "load", "(", "saver", ",", "sess", ",", "ckpt_path", ")", ":", "\n", "  ", "\"\"\"Loads the trained weights.\n    \n  Args:\n    saver: TensorFlow Saver object.\n    sess: TensorFlow session.\n    ckpt_path: path to checkpoint file with parameters.\n  \"\"\"", "\n", "saver", ".", "restore", "(", "sess", ",", "ckpt_path", ")", "\n", "print", "(", "'Restored model parameters from {}'", ".", "format", "(", "ckpt_path", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.train.train_stanford_scenes_cgan.gan_loss": [[126, 159], ["tensorflow.concat", "stanford_scenes.models.discriminator.discriminator", "stanford_scenes.models.discriminator.discriminator", "tensorflow.ones_like", "tensorflow.zeros_like", "tensorflow.nn.sigmoid_cross_entropy_with_logits", "tensorflow.nn.sigmoid_cross_entropy_with_logits", "tensorflow.nn.sigmoid_cross_entropy_with_logits", "tensorflow.reduce_mean", "tensorflow.reduce_mean", "real_data.get_shape().as_list", "fake_data.get_shape().as_list", "real_data.get_shape", "fake_data.get_shape"], "function", ["home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.discriminator.discriminator", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.discriminator.discriminator"], ["", "def", "gan_loss", "(", "real_data", ",", "fake_data", ")", ":", "\n", "  ", "\"\"\"Create GAN and compute adversarial loss.\n\n  Args:\n    real_data: A tensor of size [batch_size, height_in, width_in, channels].\n    fake_data: A tensor of size [batch_size, height_in, width_in, channels].\n    num_classes: A number indicating the quantity of classes.\n\n  Returns:\n    Two scalar indicating loss of discriminator and generator.\n  \"\"\"", "\n", "n_real", "=", "real_data", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "0", "]", "\n", "n_fake", "=", "fake_data", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "0", "]", "\n", "assert", "(", "n_real", "==", "n_fake", ")", "\n", "x", "=", "tf", ".", "concat", "(", "[", "real_data", ",", "fake_data", "]", ",", "axis", "=", "0", ")", "\n", "# Predict if the inputs is real/fake.", "\n", "pred_real", "=", "discriminator", "(", "real_data", ",", "1", ",", "True", ",", "False", ",", "False", ")", "\n", "pred_fake", "=", "discriminator", "(", "fake_data", ",", "1", ",", "True", ",", "False", ",", "True", ")", "\n", "\n", "# Compute loss for Discriminator/Generator.", "\n", "ones", "=", "tf", ".", "ones_like", "(", "pred_real", ")", "\n", "zeros", "=", "tf", ".", "zeros_like", "(", "pred_real", ")", "\n", "d_loss", "=", "tf", ".", "nn", ".", "sigmoid_cross_entropy_with_logits", "(", "\n", "logits", "=", "pred_real", ",", "labels", "=", "ones", ")", "\n", "d_loss", "+=", "tf", ".", "nn", ".", "sigmoid_cross_entropy_with_logits", "(", "\n", "logits", "=", "pred_fake", ",", "labels", "=", "zeros", ")", "\n", "g_loss", "=", "tf", ".", "nn", ".", "sigmoid_cross_entropy_with_logits", "(", "\n", "logits", "=", "pred_fake", ",", "labels", "=", "ones", ")", "\n", "\n", "d_loss", "=", "tf", ".", "reduce_mean", "(", "d_loss", ")", "\n", "g_loss", "=", "tf", ".", "reduce_mean", "(", "g_loss", ")", "\n", "\n", "return", "d_loss", ",", "g_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.train.train_stanford_scenes_cgan.main": [[161, 511], ["train_stanford_scenes_cgan.get_arguments", "utils.general.snapshot_arg", "map", "tensorflow.train.Coordinator", "tensorflow.placeholder", "stanford_scenes.models.structured_predictor.structured_predictor", "tensorflow.reshape", "tensorflow.less_equal", "tensorflow.reshape", "tensorflow.greater", "tensorflow.reduce_all", "tensorflow.equal", "tensorflow.logical_and", "tensorflow.reshape", "tensorflow.logical_not", "tensorflow.nn.l2_normalize", "tensorflow.reshape", "tensorflow.squeeze", "tensorflow.squeeze", "tensorflow.squeeze", "tensorflow.to_int32", "tensorflow.reshape", "tensorflow.gather", "tensorflow.nn.sparse_softmax_cross_entropy_with_logits", "tensorflow.reduce_mean", "tensorflow.gather", "tensorflow.gather", "tensorflow.reduce_mean", "tensorflow.reduce_sum", "tensorflow.gather", "tensorflow.gather", "tensorflow.concat", "tensorflow.concat", "train_stanford_scenes_cgan.gan_loss", "tf.add_n.append", "tf.add_n.append", "tensorflow.add_n", "tensorflow.add_n", "tensorflow.trainable_variables", "tensorflow.gradients", "tensorflow.gradients", "tensorflow.constant", "tensorflow.scalar_mul", "tensorflow.train.MomentumOptimizer", "tensorflow.train.MomentumOptimizer", "tensorflow.train.MomentumOptimizer", "tf.train.MomentumOptimizer.apply_gradients", "tf.train.MomentumOptimizer.apply_gradients", "tf.train.MomentumOptimizer.apply_gradients", "tensorflow.group", "tensorflow.ConfigProto", "tensorflow.Session", "tensorflow.global_variables_initializer", "tf.Session.run", "tensorflow.train.Saver", "tensorflow.train.start_queue_runners", "tqdm.tqdm", "tf.train.Coordinator.request_stop", "tf.train.Coordinator.join", "get_arguments.input_size.split", "tensorflow.device", "enumerate", "tensorflow.image.resize_bilinear", "tensorflow.image.resize_nearest_neighbor", "tensorflow.image.resize_nearest_neighbor", "tensorflow.image.resize_nearest_neighbor", "tensorflow.equal", "tensorflow.abs", "tensorflow.where", "tensorflow.where", "tensorflow.where", "tensorflow.gather", "tf.add_n.append", "tensorflow.reshape", "tf.add_n.append", "tensorflow.reshape", "tensorflow.reduce_mean", "tf.add_n.append", "tf.concat.append", "tf.concat.append", "tf.concat.append", "tf.concat.append", "tf.concat.append", "tf.concat.append", "tensorflow.add_n", "tensorflow.pow", "zip", "zip", "zip", "tensorflow.device", "tensorflow.py_func", "output_vis.append", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.image", "tensorflow.summary.merge_all", "tensorflow.summary.FileWriter", "tensorflow.train.Saver", "train_stanford_scenes_cgan.load", "range", "time.time", "tf.Session.run", "tf.Session.run", "range", "tf.Session.run", "tqdm.tqdm.set_description", "tensorflow.name_scope", "stanford_scenes.image_reader.ImageReader", "stanford_scenes.image_reader.ImageReader.dequeue", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.global_variables", "tensorflow.abs", "tensorflow.reduce_sum", "tensorflow.one_hot", "tensorflow.nn.softmax", "tensorflow.log", "tensorflow.log", "tensorflow.nn.l2_loss", "tensorflow.trainable_variables", "tensorflow.nn.l2_loss", "tensorflow.trainable_variables", "tensorflow.global_variables", "len", "len", "tensorflow.cast", "tensorflow.py_func", "output_vis.append", "tensorflow.image.resize_images", "tensorflow.argmax", "tensorflow.expand_dims", "tensorflow.cast", "tensorflow.py_func", "output_vis.append", "tensorflow.summary.scalar", "tensorflow.image.resize_bilinear", "tensorflow.reduce_max", "tensorflow.clip_by_value", "tensorflow.cast", "tensorflow.tile", "output_vis.append", "tensorflow.summary.scalar", "tensorflow.image.resize_bilinear", "tensorflow.cast", "output_vis.append", "tensorflow.summary.scalar", "tensorflow.concat", "tensorflow.global_variables", "train_stanford_scenes_cgan.save", "time.time", "output.get_shape().as_list", "tensorflow.image.resize_bilinear", "outputs[].get_shape().as_list", "outputs[].get_shape().as_list", "outputs[].get_shape().as_list", "outputs[].get_shape().as_list", "tensorflow.maximum", "tensorflow.maximum", "tensorflow.abs", "tensorflow.abs", "tensorflow.abs", "tensorflow.get_default_graph", "tf.Session.run", "tf.summary.FileWriter.add_summary", "tf.Session.run", "tensorflow.shape", "tensorflow.shape", "tensorflow.shape", "output.get_shape", "outputs[].get_shape", "outputs[].get_shape", "outputs[].get_shape", "outputs[].get_shape", "tensorflow.nn.l2_normalize", "tensorflow.reduce_max"], "function", ["home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.inference.inference_stanford_scenes.get_arguments", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.utils.general.snapshot_arg", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.fcrn.structured_predictor", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.train.train_stanford_scenes_cgan.gan_loss", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.inference.inference_stanford_scenes.load", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.stanford_scenes.image_reader.ImageReader.dequeue", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.train.train_stanford_scenes_cgan.save"], ["", "def", "main", "(", ")", ":", "\n", "  ", "\"\"\"Create the model and start training.\n  \"\"\"", "\n", "# Read CL arguments and snapshot the arguments into text file.", "\n", "args", "=", "get_arguments", "(", ")", "\n", "utils", ".", "general", ".", "snapshot_arg", "(", "args", ")", "\n", "\n", "# The input size.", "\n", "h", ",", "w", "=", "map", "(", "int", ",", "args", ".", "input_size", ".", "split", "(", "','", ")", ")", "\n", "input_size", "=", "(", "h", ",", "w", ")", "\n", "\n", "# Create queue coordinator.", "\n", "coord", "=", "tf", ".", "train", ".", "Coordinator", "(", ")", "\n", "\n", "# current step", "\n", "step_ph", "=", "tf", ".", "placeholder", "(", "dtype", "=", "tf", ".", "float32", ",", "shape", "=", "(", ")", ")", "\n", "\n", "# Load the data reader.", "\n", "with", "tf", ".", "device", "(", "'/cpu:0'", ")", ":", "\n", "    ", "with", "tf", ".", "name_scope", "(", "'create_inputs'", ")", ":", "\n", "      ", "reader", "=", "ImageReader", "(", "\n", "args", ".", "data_dir", ",", "\n", "args", ".", "data_list", ",", "\n", "input_size", ",", "\n", "args", ".", "random_scale", ",", "\n", "args", ".", "random_mirror", ",", "\n", "args", ".", "random_crop", ",", "\n", "args", ".", "ignore_label", ",", "\n", "IMG_MEAN", ")", "\n", "\n", "datas_batch", "=", "reader", ".", "dequeue", "(", "args", ".", "batch_size", ")", "\n", "datas_batch", "[", "2", "]", "=", "datas_batch", "[", "2", "]", "/", "args", ".", "depth_unit", "\n", "image_batch", "=", "tf", ".", "placeholder", "(", "datas_batch", "[", "0", "]", ".", "dtype", ",", "\n", "datas_batch", "[", "0", "]", ".", "shape", ")", "\n", "label_batch", "=", "tf", ".", "placeholder", "(", "datas_batch", "[", "1", "]", ".", "dtype", ",", "\n", "datas_batch", "[", "1", "]", ".", "shape", ")", "\n", "depth_batch", "=", "tf", ".", "placeholder", "(", "datas_batch", "[", "2", "]", ".", "dtype", ",", "\n", "datas_batch", "[", "2", "]", ".", "shape", ")", "\n", "normal_batch", "=", "tf", ".", "placeholder", "(", "datas_batch", "[", "3", "]", ".", "dtype", ",", "\n", "datas_batch", "[", "3", "]", ".", "shape", ")", "\n", "\n", "# Create network and predictions.", "\n", "", "", "outputs", "=", "structured_predictor", "(", "image_batch", ",", "\n", "args", ".", "num_classes", ",", "\n", "args", ".", "is_training", ",", "\n", "args", ".", "use_global_status", ")", "\n", "\n", "# Either up-sample predictions or down-sample ground-truths.", "\n", "if", "args", ".", "upscale_predictions", ":", "\n", "    ", "for", "output_index", ",", "output", "in", "enumerate", "(", "outputs", ")", ":", "\n", "      ", "out_h", ",", "out_w", "=", "output", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "1", ":", "3", "]", "\n", "if", "out_h", "!=", "h", "or", "out_w", "!=", "w", ":", "\n", "        ", "outputs", "[", "output_index", "]", "=", "tf", ".", "image", ".", "resize_bilinear", "(", "\n", "output", ",", "[", "h", ",", "w", "]", ")", "\n", "", "", "images", "=", "image_batch", "\n", "labels", "=", "label_batch", "\n", "depths", "=", "depth_batch", "\n", "normals", "=", "normal_batch", "\n", "", "else", ":", "\n", "    ", "images", "=", "tf", ".", "image", ".", "resize_bilinear", "(", "\n", "image_batch", ",", "outputs", "[", "0", "]", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "1", ":", "3", "]", ")", "\n", "labels", "=", "tf", ".", "image", ".", "resize_nearest_neighbor", "(", "\n", "label_batch", ",", "outputs", "[", "0", "]", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "1", ":", "3", "]", ")", "\n", "depths", "=", "tf", ".", "image", ".", "resize_nearest_neighbor", "(", "\n", "depth_batch", ",", "outputs", "[", "1", "]", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "1", ":", "3", "]", ")", "\n", "normals", "=", "tf", ".", "image", ".", "resize_nearest_neighbor", "(", "\n", "normal_batch", ",", "outputs", "[", "2", "]", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "1", ":", "3", "]", ")", "\n", "\n", "# Ignore the location where the label value is larger than args.num_classes.", "\n", "", "labels_flat", "=", "tf", ".", "reshape", "(", "labels", ",", "(", "-", "1", ",", ")", ")", "\n", "not_ignore_labels", "=", "tf", ".", "less_equal", "(", "labels_flat", ",", "args", ".", "num_classes", "-", "1", ")", "\n", "\n", "# Ignore the location where the depth value <= 0", "\n", "depths_flat", "=", "tf", ".", "reshape", "(", "depths", ",", "(", "-", "1", ",", ")", ")", "\n", "not_ignore_depths", "=", "tf", ".", "greater", "(", "depths_flat", ",", "0.0", ")", "\n", "\n", "# Ignore the location where the normal value != [128,128,128].", "\n", "# The normal is centered at 127.5 in ImageReader.", "\n", "ignore_normal_yz", "=", "tf", ".", "reduce_all", "(", "\n", "tf", ".", "equal", "(", "normals", "[", ":", ",", ":", ",", ":", ",", "1", ":", "]", ",", "0.5", ")", ",", "\n", "axis", "=", "-", "1", ")", "\n", "ignore_normal_x", "=", "tf", ".", "equal", "(", "tf", ".", "abs", "(", "normals", "[", ":", ",", ":", ",", ":", ",", "0", "]", ")", ",", "0.5", ")", "\n", "ignore_normals", "=", "tf", ".", "logical_and", "(", "ignore_normal_yz", ",", "\n", "ignore_normal_x", ")", "\n", "ignore_normals", "=", "tf", ".", "reshape", "(", "ignore_normals", ",", "(", "-", "1", ",", ")", ")", "\n", "not_ignore_normals", "=", "tf", ".", "logical_not", "(", "ignore_normals", ")", "\n", "normals", "=", "tf", ".", "nn", ".", "l2_normalize", "(", "normals", ",", "dim", "=", "-", "1", ")", "\n", "normals_flat", "=", "tf", ".", "reshape", "(", "normals", ",", "(", "-", "1", ",", "3", ")", ")", "\n", "\n", "# Extract the indices of labels where the gradients are propogated.", "\n", "valid_label_inds", "=", "tf", ".", "squeeze", "(", "tf", ".", "where", "(", "not_ignore_labels", ")", ",", "1", ")", "\n", "valid_depth_inds", "=", "tf", ".", "squeeze", "(", "tf", ".", "where", "(", "not_ignore_depths", ")", ",", "1", ")", "\n", "valid_normal_inds", "=", "tf", ".", "squeeze", "(", "tf", ".", "where", "(", "not_ignore_normals", ")", ",", "1", ")", "\n", "\n", "# Grab variable names which should be restored from checkpoints.", "\n", "restore_var", "=", "[", "\n", "v", "for", "v", "in", "tf", ".", "global_variables", "(", ")", "\n", "if", "'block5'", "not", "in", "v", ".", "name", "or", "not", "args", ".", "not_restore_classifier", "]", "\n", "\n", "reduced_loss", "=", "[", "]", "\n", "\n", "# Define softmax loss.", "\n", "labels_gather", "=", "tf", ".", "to_int32", "(", "tf", ".", "gather", "(", "labels_flat", ",", "valid_label_inds", ")", ")", "\n", "segmentation_output", "=", "tf", ".", "reshape", "(", "outputs", "[", "0", "]", ",", "\n", "[", "-", "1", ",", "args", ".", "num_classes", "]", ")", "\n", "segmentation_output", "=", "tf", ".", "gather", "(", "segmentation_output", ",", "\n", "valid_label_inds", ")", "\n", "segmentation_loss", "=", "tf", ".", "nn", ".", "sparse_softmax_cross_entropy_with_logits", "(", "\n", "logits", "=", "segmentation_output", ",", "\n", "labels", "=", "labels_gather", ")", "\n", "segmentation_loss", "=", "tf", ".", "reduce_mean", "(", "segmentation_loss", ")", "\n", "if", "args", ".", "train_segmentation", ":", "\n", "    ", "reduced_loss", ".", "append", "(", "segmentation_loss", "*", "args", ".", "sup_lambda", ")", "\n", "\n", "# Define depth loss.", "\n", "", "depths_gather", "=", "tf", ".", "gather", "(", "depths_flat", ",", "valid_depth_inds", ")", "\n", "depth_output", "=", "tf", ".", "gather", "(", "tf", ".", "reshape", "(", "outputs", "[", "1", "]", ",", "[", "-", "1", ",", "]", ")", ",", "\n", "valid_depth_inds", ")", "\n", "depth_diff", "=", "depth_output", "-", "depths_gather", "\n", "depth_loss", "=", "tf", ".", "reduce_mean", "(", "depth_diff", "**", "2", ")", "\n", "depth_absrel", "=", "tf", ".", "reduce_sum", "(", "\n", "tf", ".", "abs", "(", "depth_diff", ")", "/", "depths_gather", ")", "\n", "depth_absrel", "/=", "args", ".", "batch_size", "\n", "if", "args", ".", "train_depth", ":", "\n", "    ", "reduced_loss", ".", "append", "(", "depth_loss", "*", "args", ".", "sup_lambda", ")", "\n", "\n", "# Define surface normal loss.", "\n", "", "normals_gather", "=", "tf", ".", "gather", "(", "normals_flat", ",", "valid_normal_inds", ")", "\n", "normal_output", "=", "tf", ".", "gather", "(", "tf", ".", "reshape", "(", "outputs", "[", "2", "]", ",", "[", "-", "1", ",", "3", "]", ")", ",", "\n", "valid_normal_inds", ")", "\n", "normal_loss", "=", "-", "tf", ".", "reduce_mean", "(", "tf", ".", "reduce_sum", "(", "\n", "normal_output", "*", "normals_gather", ",", "axis", "=", "-", "1", ")", ")", "\n", "normal_loss", "*=", "10.0", "\n", "if", "args", ".", "train_normal", ":", "\n", "    ", "reduced_loss", ".", "append", "(", "normal_loss", "*", "args", ".", "sup_lambda", ")", "\n", "\n", "# Define GAN loss.", "\n", "", "real_data", ",", "fake_data", "=", "[", "images", "]", ",", "[", "images", "]", "\n", "if", "args", ".", "train_segmentation", ":", "\n", "    ", "real_data", ".", "append", "(", "tf", ".", "one_hot", "(", "labels", ",", "depth", "=", "args", ".", "num_classes", ")", ")", "\n", "fake_data", ".", "append", "(", "tf", ".", "nn", ".", "softmax", "(", "outputs", "[", "0", "]", ",", "axis", "=", "-", "1", ")", ")", "\n", "", "if", "args", ".", "train_depth", ":", "\n", "    ", "real_data", ".", "append", "(", "tf", ".", "log", "(", "tf", ".", "maximum", "(", "depths", ",", "1e-2", ")", ")", ")", "\n", "fake_data", ".", "append", "(", "tf", ".", "log", "(", "tf", ".", "maximum", "(", "outputs", "[", "1", "]", ",", "1e-2", ")", ")", ")", "\n", "", "if", "args", ".", "train_normal", ":", "\n", "    ", "real_data", ".", "append", "(", "normals", ")", "\n", "fake_data", ".", "append", "(", "outputs", "[", "2", "]", ")", "\n", "\n", "", "real_data", "=", "tf", ".", "concat", "(", "real_data", ",", "axis", "=", "-", "1", ")", "\n", "fake_data", "=", "tf", ".", "concat", "(", "fake_data", ",", "axis", "=", "-", "1", ")", "\n", "d_loss", ",", "g_loss", "=", "gan_loss", "(", "real_data", ",", "fake_data", ")", "\n", "d_loss", "*=", "args", ".", "gan_lambda", "\n", "g_loss", "*=", "args", ".", "gan_lambda", "\n", "reduced_loss", ".", "append", "(", "g_loss", ")", "\n", "\n", "# Define weight regularization loss.", "\n", "w", "=", "args", ".", "weight_decay", "\n", "l2_losses", "=", "[", "w", "*", "tf", ".", "nn", ".", "l2_loss", "(", "v", ")", "for", "v", "in", "tf", ".", "trainable_variables", "(", ")", "\n", "if", "'weights'", "in", "v", ".", "name", "and", "'Discriminator'", "not", "in", "v", ".", "name", "]", "\n", "d_l2_losses", "=", "[", "w", "*", "tf", ".", "nn", ".", "l2_loss", "(", "v", ")", "for", "v", "in", "tf", ".", "trainable_variables", "(", ")", "\n", "if", "'weights'", "in", "v", ".", "name", "and", "'Discriminator'", "in", "v", ".", "name", "]", "\n", "reduced_loss", ".", "append", "(", "tf", ".", "add_n", "(", "l2_losses", ")", ")", "\n", "d_loss", "+=", "tf", ".", "add_n", "(", "d_l2_losses", ")", "\n", "\n", "# Sum all losses.", "\n", "reduced_loss", "=", "tf", ".", "add_n", "(", "reduced_loss", ")", "\n", "\n", "\n", "restore_var_d", "=", "[", "\n", "v", "for", "v", "in", "tf", ".", "global_variables", "(", ")", "\n", "if", "(", "'block5'", "not", "in", "v", ".", "name", "or", "not", "args", ".", "not_restore_classifier", ")", "and", "'Discriminator'", "in", "v", ".", "name", "]", "\n", "\n", "# Grab variable names which are used for training.", "\n", "all_trainable", "=", "tf", ".", "trainable_variables", "(", ")", "\n", "pred_trainable", "=", "[", "\n", "v", "for", "v", "in", "all_trainable", "if", "'block5'", "in", "v", ".", "name", "and", "'Discriminator'", "not", "in", "v", ".", "name", "]", "# lr*10", "\n", "base_trainable", "=", "[", "\n", "v", "for", "v", "in", "all_trainable", "if", "'block5'", "not", "in", "v", ".", "name", "and", "'Discriminator'", "not", "in", "v", ".", "name", "]", "# lr*1", "\n", "d_trainable", "=", "[", "\n", "v", "for", "v", "in", "all_trainable", "if", "'Discriminator'", "in", "v", ".", "name", "]", "\n", "\n", "# Computes gradients per iteration.", "\n", "grads", "=", "tf", ".", "gradients", "(", "reduced_loss", ",", "\n", "base_trainable", "+", "pred_trainable", ")", "\n", "grads_base", "=", "grads", "[", ":", "len", "(", "base_trainable", ")", "]", "\n", "grads_pred", "=", "grads", "[", "len", "(", "base_trainable", ")", ":", "]", "\n", "grads_d", "=", "tf", ".", "gradients", "(", "d_loss", ",", "d_trainable", ")", "\n", "\n", "# Define optimisation parameters.", "\n", "base_lr", "=", "tf", ".", "constant", "(", "args", ".", "learning_rate", ")", "\n", "learning_rate", "=", "tf", ".", "scalar_mul", "(", "\n", "base_lr", ",", "\n", "tf", ".", "pow", "(", "(", "1", "-", "step_ph", "/", "args", ".", "num_steps", ")", ",", "args", ".", "power", ")", ")", "\n", "\n", "opt_base", "=", "tf", ".", "train", ".", "MomentumOptimizer", "(", "learning_rate", "*", "1.0", ",", "\n", "args", ".", "momentum", ")", "\n", "opt_pred", "=", "tf", ".", "train", ".", "MomentumOptimizer", "(", "learning_rate", "*", "1.0", ",", "\n", "args", ".", "momentum", ")", "\n", "opt_d", "=", "tf", ".", "train", ".", "MomentumOptimizer", "(", "learning_rate", "*", "1.0", ",", "\n", "args", ".", "momentum", ")", "\n", "\n", "# Define tensorflow operations which apply gradients to update variables.", "\n", "train_op_base", "=", "opt_base", ".", "apply_gradients", "(", "\n", "zip", "(", "grads_base", ",", "base_trainable", ")", ")", "\n", "train_op_pred", "=", "opt_pred", ".", "apply_gradients", "(", "\n", "zip", "(", "grads_pred", ",", "pred_trainable", ")", ")", "\n", "train_op_d", "=", "opt_d", ".", "apply_gradients", "(", "\n", "zip", "(", "grads_d", ",", "d_trainable", ")", ")", "\n", "train_op_g", "=", "tf", ".", "group", "(", "train_op_base", ",", "train_op_pred", ")", "\n", "\n", "# Process for visualisation.", "\n", "with", "tf", ".", "device", "(", "'/cpu:0'", ")", ":", "\n", "# Image summary for input image, ground-truth label and prediction.", "\n", "    ", "output_vis", "=", "[", "]", "\n", "in_summary", "=", "tf", ".", "py_func", "(", "\n", "utils", ".", "general", ".", "inv_preprocess", ",", "\n", "[", "image_batch", ",", "IMG_MEAN", "]", ",", "\n", "tf", ".", "uint8", ")", "\n", "output_vis", ".", "append", "(", "in_summary", ")", "\n", "\n", "if", "args", ".", "train_segmentation", ":", "\n", "# Visualize segmentation ground-truths.", "\n", "      ", "labels_vis", "=", "tf", ".", "cast", "(", "label_batch", ",", "dtype", "=", "tf", ".", "uint8", ")", "\n", "lab_summary", "=", "tf", ".", "py_func", "(", "\n", "utils", ".", "general", ".", "decode_labels", ",", "\n", "[", "labels_vis", ",", "args", ".", "num_classes", "]", ",", "\n", "tf", ".", "uint8", ")", "\n", "output_vis", ".", "append", "(", "lab_summary", ")", "\n", "# Visualize segmentation predictions.", "\n", "segmentation_vis", "=", "tf", ".", "image", ".", "resize_images", "(", "\n", "outputs", "[", "0", "]", ",", "\n", "tf", ".", "shape", "(", "image_batch", ")", "[", "1", ":", "3", ",", "]", ",", "\n", "tf", ".", "image", ".", "ResizeMethod", ".", "NEAREST_NEIGHBOR", ")", "\n", "segmentation_vis", "=", "tf", ".", "argmax", "(", "segmentation_vis", ",", "\n", "axis", "=", "3", ")", "\n", "segmentation_vis", "=", "tf", ".", "expand_dims", "(", "segmentation_vis", ",", "\n", "dim", "=", "3", ")", "\n", "segmentation_vis", "=", "tf", ".", "cast", "(", "segmentation_vis", ",", "\n", "dtype", "=", "tf", ".", "uint8", ")", "\n", "segmentation_summary", "=", "tf", ".", "py_func", "(", "\n", "utils", ".", "general", ".", "decode_labels", ",", "\n", "[", "segmentation_vis", ",", "args", ".", "num_classes", "]", ",", "\n", "tf", ".", "uint8", ")", "\n", "output_vis", ".", "append", "(", "segmentation_summary", ")", "\n", "# Scalar summary of segmentaiton loss.", "\n", "segmentation_loss_summary", "=", "tf", ".", "summary", ".", "scalar", "(", "\n", "'segmentation_loss'", ",", "segmentation_loss", ")", "\n", "\n", "", "if", "args", ".", "train_depth", ":", "\n", "# Visualize difference.", "\n", "      ", "depth_vis", "=", "tf", ".", "image", ".", "resize_bilinear", "(", "\n", "tf", ".", "abs", "(", "outputs", "[", "1", "]", "-", "depths", ")", ",", "\n", "tf", ".", "shape", "(", "image_batch", ")", "[", "1", ":", "3", ",", "]", ")", "\n", "depth_vis", "/=", "tf", ".", "reduce_max", "(", "tf", ".", "abs", "(", "depth_diff", ")", ")", "\n", "depth_vis", "=", "tf", ".", "clip_by_value", "(", "depth_vis", ",", "0.0", ",", "1.0", ")", "\n", "depth_vis", "=", "tf", ".", "cast", "(", "depth_vis", "*", "255", ",", "dtype", "=", "tf", ".", "uint8", ")", "\n", "depth_summary", "=", "tf", ".", "tile", "(", "depth_vis", ",", "[", "1", ",", "1", ",", "1", ",", "3", "]", ")", "\n", "output_vis", ".", "append", "(", "depth_summary", ")", "\n", "# Scalar summary of depth loss.", "\n", "depth_loss_summary", "=", "tf", ".", "summary", ".", "scalar", "(", "\n", "'depth_loss'", ",", "depth_loss", ")", "\n", "\n", "", "if", "args", ".", "train_normal", ":", "\n", "# Visualize difference.", "\n", "      ", "normal_vis", "=", "tf", ".", "image", ".", "resize_bilinear", "(", "\n", "tf", ".", "abs", "(", "outputs", "[", "2", "]", "-", "tf", ".", "nn", ".", "l2_normalize", "(", "normals", ",", "dim", "=", "-", "1", ")", ")", ",", "\n", "tf", ".", "shape", "(", "image_batch", ")", "[", "1", ":", "3", ",", "]", ")", "\n", "normal_summary", "=", "tf", ".", "cast", "(", "\n", "normal_vis", "/", "tf", ".", "reduce_max", "(", "normal_vis", ")", "*", "255", ",", "\n", "dtype", "=", "tf", ".", "uint8", ")", "\n", "output_vis", ".", "append", "(", "normal_summary", ")", "\n", "# Scalar summary of surface normal loss.", "\n", "normal_loss_summary", "=", "tf", ".", "summary", ".", "scalar", "(", "\n", "'normal_loss'", ",", "normal_loss", ")", "\n", "\n", "", "g_loss_summary", "=", "tf", ".", "summary", ".", "scalar", "(", "\n", "'g_loss'", ",", "g_loss", ")", "\n", "d_loss_summary", "=", "tf", ".", "summary", ".", "scalar", "(", "\n", "'d_loss'", ",", "d_loss", ")", "\n", "\n", "image_summary", "=", "tf", ".", "summary", ".", "image", "(", "\n", "'images'", ",", "\n", "tf", ".", "concat", "(", "axis", "=", "2", ",", "values", "=", "output_vis", ")", ",", "\n", "max_outputs", "=", "args", ".", "batch_size", ")", "\n", "\n", "total_summary", "=", "tf", ".", "summary", ".", "merge_all", "(", ")", "\n", "summary_writer", "=", "tf", ".", "summary", ".", "FileWriter", "(", "\n", "args", ".", "snapshot_dir", ",", "\n", "graph", "=", "tf", ".", "get_default_graph", "(", ")", ")", "\n", "\n", "# Set up tf session and initialize variables. ", "\n", "", "config", "=", "tf", ".", "ConfigProto", "(", ")", "\n", "config", ".", "gpu_options", ".", "allow_growth", "=", "True", "\n", "sess", "=", "tf", ".", "Session", "(", "config", "=", "config", ")", "\n", "init", "=", "tf", ".", "global_variables_initializer", "(", ")", "\n", "\n", "sess", ".", "run", "(", "init", ")", "\n", "\n", "# Saver for storing checkpoints of the model.", "\n", "saver", "=", "tf", ".", "train", ".", "Saver", "(", "var_list", "=", "tf", ".", "global_variables", "(", ")", ",", "\n", "max_to_keep", "=", "10", ")", "\n", "\n", "# Load variables if the checkpoint is provided.", "\n", "if", "args", ".", "restore_from", "is", "not", "None", ":", "\n", "    ", "loader", "=", "tf", ".", "train", ".", "Saver", "(", "var_list", "=", "restore_var", ")", "\n", "load", "(", "loader", ",", "sess", ",", "args", ".", "restore_from", ")", "\n", "\n", "# Start queue threads.", "\n", "", "threads", "=", "tf", ".", "train", ".", "start_queue_runners", "(", "coord", "=", "coord", ",", "sess", "=", "sess", ")", "\n", "\n", "# Iterate over training steps.", "\n", "pbar", "=", "tqdm", "(", "range", "(", "args", ".", "num_steps", ")", ")", "\n", "for", "step", "in", "pbar", ":", "\n", "    ", "start_time", "=", "time", ".", "time", "(", ")", "\n", "img_data", ",", "lab_data", ",", "dph_data", ",", "nrm_data", "=", "sess", ".", "run", "(", "datas_batch", ")", "\n", "feed_dict", "=", "{", "step_ph", ":", "step", ",", "\n", "image_batch", ":", "img_data", ",", "\n", "label_batch", ":", "lab_data", ",", "\n", "depth_batch", ":", "dph_data", ",", "\n", "normal_batch", ":", "nrm_data", "}", "\n", "\n", "sess", ".", "run", "(", "train_op_d", ",", "feed_dict", "=", "feed_dict", ")", "\n", "\n", "step_loss", "=", "0", "\n", "for", "it", "in", "range", "(", "args", ".", "iter_size", ")", ":", "\n", "# Update summary periodically.", "\n", "      ", "if", "it", "==", "args", ".", "iter_size", "-", "1", "and", "step", "%", "args", ".", "update_tb_every", "==", "0", ":", "\n", "        ", "sess_outs", "=", "[", "reduced_loss", ",", "total_summary", ",", "train_op_g", "]", "\n", "loss_value", ",", "summary", ",", "_", "=", "sess", ".", "run", "(", "sess_outs", ",", "feed_dict", "=", "feed_dict", ")", "\n", "summary_writer", ".", "add_summary", "(", "summary", ",", "step", ")", "\n", "", "else", ":", "\n", "        ", "sess_outs", "=", "[", "reduced_loss", ",", "train_op_g", "]", "\n", "loss_value", ",", "_", "=", "sess", ".", "run", "(", "sess_outs", ",", "feed_dict", "=", "feed_dict", ")", "\n", "\n", "", "step_loss", "+=", "loss_value", "\n", "\n", "", "step_loss", "/=", "args", ".", "iter_size", "\n", "\n", "lr", "=", "sess", ".", "run", "(", "learning_rate", ",", "feed_dict", "=", "{", "step_ph", ":", "step", "}", ")", "\n", "\n", "# Save trained model periodically.", "\n", "if", "step", "%", "args", ".", "save_pred_every", "==", "0", "and", "step", ">", "0", ":", "\n", "      ", "save", "(", "saver", ",", "sess", ",", "args", ".", "snapshot_dir", ",", "step", ")", "\n", "\n", "", "duration", "=", "time", ".", "time", "(", ")", "-", "start_time", "\n", "desc", "=", "'loss = {:.3f}, lr = {:.6f}'", ".", "format", "(", "step_loss", ",", "lr", ")", "\n", "pbar", ".", "set_description", "(", "desc", ")", "\n", "\n", "", "coord", ".", "request_stop", "(", ")", "\n", "coord", ".", "join", "(", "threads", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.preprocess.stanford_scenes_down_sample.parse_args": [[11, 22], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.preprocess.stanford_scenes_down_sample.parse_args"], ["def", "parse_args", "(", ")", ":", "\n", "  ", "\"\"\"Parsse Command Line Arguments.\n  \"\"\"", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "description", "=", "'Helper scripts to down-sample Stanford 2D3DS'", ")", "\n", "parser", ".", "add_argument", "(", "'--data_dir'", ",", "type", "=", "str", ",", "\n", "help", "=", "'/path/to/Stanford/2D3DS/dir.'", ")", "\n", "parser", ".", "add_argument", "(", "'--new_dir'", ",", "type", "=", "str", ",", "\n", "help", "=", "'/path/to/down-sampled/Stanford/2D3DS/dir.'", ")", "\n", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.preprocess.stanford_scenes_down_sample.main": [[24, 60], ["stanford_scenes_down_sample.parse_args", "os.path.join", "os.walk", "numpy.array", "dirpath.replace", "os.path.join", "PIL.fromarray().save", "PIL.open", "int", "int", "cv2.resize", "cv2.resize", "os.path.isdir", "os.makedirs", "os.path.join", "PIL.fromarray"], "function", ["home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.preprocess.stanford_scenes_down_sample.parse_args", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.train.train_stanford_scenes_cgan.save"], ["", "def", "main", "(", ")", ":", "\n", "  ", "\"\"\"Down-sample RGB and Surface Normal.\n  \"\"\"", "\n", "args", "=", "parse_args", "(", ")", "\n", "\n", "dir_names", "=", "[", "'area_1'", ",", "'area_2'", ",", "'area_3'", ",", "'area_4'", ",", "\n", "'area_5a'", ",", "'area_5b'", ",", "'area_6'", "]", "\n", "\n", "for", "root_dir_name", "in", "dir_names", ":", "\n", "    ", "for", "sub_dir_name", "in", "[", "'rgb'", ",", "'normal'", "]", ":", "\n", "      ", "dir_name", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "\n", "root_dir_name", ",", "\n", "'data'", ",", "\n", "sub_dir_name", ")", "\n", "for", "dirpath", ",", "dirnames", ",", "filenames", "in", "os", ".", "walk", "(", "dir_name", ")", ":", "\n", "        ", "for", "file_name", "in", "filenames", ":", "\n", "          ", "if", "'.png'", "not", "in", "file_name", "and", "'.jpg'", "not", "in", "file_name", ":", "\n", "            ", "continue", "\n", "", "arr", "=", "np", ".", "array", "(", "Image", ".", "open", "(", "os", ".", "path", ".", "join", "(", "dirpath", ",", "file_name", ")", ")", ")", "\n", "h", ",", "w", "=", "arr", ".", "shape", "[", ":", "2", "]", "\n", "new_h", ",", "new_w", "=", "int", "(", "h", "/", "2", ")", ",", "int", "(", "w", "/", "2", ")", "\n", "\n", "if", "'rgb'", "==", "sub_dir_name", ":", "\n", "            ", "arr", "=", "cv2", ".", "resize", "(", "arr", ",", "\n", "(", "new_w", ",", "new_h", ")", ",", "\n", "interpolation", "=", "cv2", ".", "INTER_LINEAR", ")", "\n", "", "else", ":", "\n", "            ", "arr", "=", "cv2", ".", "resize", "(", "arr", ",", "\n", "(", "new_w", ",", "new_h", ")", ",", "\n", "interpolation", "=", "cv2", ".", "INTER_NEAREST", ")", "\n", "\n", "", "new_dir", "=", "dirpath", ".", "replace", "(", "args", ".", "data_dir", ",", "args", ".", "new_dir", ")", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "new_dir", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "new_dir", ")", "\n", "", "new_name", "=", "os", ".", "path", ".", "join", "(", "new_dir", ",", "file_name", ")", "\n", "Image", ".", "fromarray", "(", "arr", ",", "mode", "=", "'RGB'", ")", ".", "save", "(", "new_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.inference.inference_stanford_scenes.get_arguments": [[23, 60], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.preprocess.stanford_scenes_down_sample.parse_args"], ["def", "get_arguments", "(", ")", ":", "\n", "  ", "\"\"\"Parse all the arguments provided from the CLI.\n    \n  Returns:\n    A list of parsed arguments.\n  \"\"\"", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "description", "=", "'Inference Segmentation, Depth, Surface Normals on NYUv2'", ")", "\n", "parser", ".", "add_argument", "(", "'--data_dir'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "\n", "help", "=", "'/path/to/dataset/.'", ")", "\n", "parser", ".", "add_argument", "(", "'--data_list'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "\n", "help", "=", "'/path/to/datalist/file.'", ")", "\n", "parser", ".", "add_argument", "(", "'--input_size'", ",", "type", "=", "str", ",", "default", "=", "'512,512'", ",", "\n", "help", "=", "'Comma-separated string with H and W of image.'", ")", "\n", "parser", ".", "add_argument", "(", "'--depth_unit'", ",", "type", "=", "float", ",", "default", "=", "512.0", ",", "\n", "help", "=", "'Each pixel value difference means 1/depth_unit '", "\n", "'meters.'", ")", "\n", "parser", ".", "add_argument", "(", "'--train_segmentation'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Whether to predict segmentations.'", ")", "\n", "parser", ".", "add_argument", "(", "'--train_depth'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Whether to predict depths.'", ")", "\n", "parser", ".", "add_argument", "(", "'--train_normal'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Whether to predict surface normals.'", ")", "\n", "parser", ".", "add_argument", "(", "'--strides'", ",", "type", "=", "str", ",", "default", "=", "'512,512'", ",", "\n", "help", "=", "'Comma-separated string with strides of H and W.'", ")", "\n", "parser", ".", "add_argument", "(", "'--num_classes'", ",", "type", "=", "int", ",", "default", "=", "21", ",", "\n", "help", "=", "'Number of classes to predict.'", ")", "\n", "parser", ".", "add_argument", "(", "'--ignore_label'", ",", "type", "=", "int", ",", "default", "=", "255", ",", "\n", "help", "=", "'Index of label to ignore.'", ")", "\n", "parser", ".", "add_argument", "(", "'--restore_from'", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "'Where restore model parameters from.'", ")", "\n", "parser", ".", "add_argument", "(", "'--save_dir'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "\n", "help", "=", "'/path/to/save/predictions.'", ")", "\n", "parser", ".", "add_argument", "(", "'--colormap'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "\n", "help", "=", "'/path/to/colormap/file.'", ")", "\n", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.inference.inference_stanford_scenes.load": [[62, 72], ["saver.restore", "print"], "function", ["None"], ["", "def", "load", "(", "saver", ",", "sess", ",", "ckpt_path", ")", ":", "\n", "  ", "\"\"\"Loads the trained weights.\n    \n  Args:\n    saver: TensorFlow Saver object.\n    sess: TensorFlow session.\n    ckpt_path: path to checkpoint file with parameters.\n  \"\"\"", "\n", "saver", ".", "restore", "(", "sess", ",", "ckpt_path", ")", "\n", "print", "(", "'Restored model parameters from {}'", ".", "format", "(", "ckpt_path", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.inference.inference_stanford_scenes.parse_commastr": [[74, 83], ["map", "str_comma.split"], "function", ["None"], ["", "def", "parse_commastr", "(", "str_comma", ")", ":", "\n", "  ", "\"\"\"Read comma-sperated string.\n  \"\"\"", "\n", "if", "''", "==", "str_comma", ":", "\n", "    ", "return", "None", "\n", "", "else", ":", "\n", "    ", "a", ",", "b", "=", "map", "(", "int", ",", "str_comma", ".", "split", "(", "','", ")", ")", "\n", "\n", "", "return", "[", "a", ",", "b", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.inference.inference_stanford_scenes.main": [[85, 300], ["inference_stanford_scenes.get_arguments", "inference_stanford_scenes.parse_commastr", "inference_stanford_scenes.parse_commastr", "tensorflow.train.Coordinator", "tensorflow.expand_dims", "tensorflow.placeholder", "stanford_scenes.models.structured_predictor.structured_predictor", "tensorflow.image.resize_bilinear", "tensorflow.nn.softmax", "tensorflow.image.resize_bilinear", "tensorflow.image.resize_bilinear", "tensorflow.ConfigProto", "tensorflow.Session", "tensorflow.global_variables_initializer", "tf.Session.run", "tf.Session.run", "tensorflow.train.start_queue_runners", "scipy.io.loadmat", "os.path.basename().replace", "colormap.astype.astype", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "range", "tf.train.Coordinator.request_stop", "tf.train.Coordinator.join", "tensorflow.name_scope", "stanford_scenes.image_reader.ImageReader", "tensorflow.local_variables_initializer", "tensorflow.train.Saver", "inference_stanford_scenes.load", "os.makedirs", "os.makedirs", "os.makedirs", "os.makedirs", "open", "tf.Session.run", "list", "numpy.zeros", "list", "numpy.zeros", "segmentation_batch.astype.fill", "list", "numpy.zeros", "list", "numpy.zeros", "numpy.zeros_like", "numpy.linspace", "numpy.linspace", "numpy.argmax", "segmentation_batch.astype.astype", "os.path.basename", "basename.replace.replace", "tensorflow.global_variables", "tensorflow.shape", "tensorflow.shape", "tensorflow.shape", "os.path.basename", "os.path.isdir", "os.path.isdir", "os.path.isdir", "os.path.isdir", "len", "math.ceil", "math.ceil", "os.path.join", "PIL.Image.fromarray().save", "os.path.join", "PIL.Image.fromarray().save", "os.path.join", "PIL.Image.fromarray().save", "PIL.Image.fromarray().save", "listf.read().split", "tf.Session.run", "basename.replace.replace", "basename.replace.replace", "basename.replace.replace", "os.path.join", "os.path.join", "PIL.Image.fromarray", "PIL.Image.fromarray", "PIL.Image.fromarray", "basename.replace.replace", "basename.replace.replace", "PIL.Image.fromarray", "listf.read"], "function", ["home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.inference.inference_stanford_scenes.get_arguments", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.inference.inference_stanford_scenes.parse_commastr", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.inference.inference_stanford_scenes.parse_commastr", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.fcrn.structured_predictor", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.inference.inference_stanford_scenes.load", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.train.train_stanford_scenes_cgan.save", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.train.train_stanford_scenes_cgan.save", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.train.train_stanford_scenes_cgan.save", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.train.train_stanford_scenes_cgan.save"], ["", "def", "main", "(", ")", ":", "\n", "  ", "\"\"\"Create the model and start the Inference process.\n  \"\"\"", "\n", "# Read CL arguments and snapshot the arguments into text file.", "\n", "args", "=", "get_arguments", "(", ")", "\n", "\n", "# Parse image processing arguments.", "\n", "input_size", "=", "parse_commastr", "(", "args", ".", "input_size", ")", "\n", "strides", "=", "parse_commastr", "(", "args", ".", "strides", ")", "\n", "assert", "(", "input_size", "is", "not", "None", "and", "strides", "is", "not", "None", ")", "\n", "h", ",", "w", "=", "input_size", "\n", "\n", "# Create queue coordinator.", "\n", "coord", "=", "tf", ".", "train", ".", "Coordinator", "(", ")", "\n", "\n", "# Load the data reader.", "\n", "with", "tf", ".", "name_scope", "(", "'create_inputs'", ")", ":", "\n", "    ", "reader", "=", "ImageReader", "(", "\n", "args", ".", "data_dir", ",", "\n", "args", ".", "data_list", ",", "\n", "None", ",", "\n", "False", ",", "# No random scale.", "\n", "False", ",", "# No random mirror.", "\n", "False", ",", "# No random crop, center crop instead", "\n", "args", ".", "ignore_label", ",", "\n", "IMG_MEAN", ")", "\n", "image", "=", "reader", ".", "datas", "[", "0", "]", "\n", "image_list", "=", "reader", ".", "file_lists", "[", "0", "]", "\n", "", "image_batch", "=", "tf", ".", "expand_dims", "(", "image", ",", "dim", "=", "0", ")", "\n", "\n", "# Create input tensor to the Network", "\n", "crop_image_batch", "=", "tf", ".", "placeholder", "(", "\n", "name", "=", "'crop_image_batch'", ",", "\n", "shape", "=", "[", "1", ",", "input_size", "[", "0", "]", ",", "input_size", "[", "1", "]", ",", "3", "]", ",", "\n", "dtype", "=", "tf", ".", "float32", ")", "\n", "\n", "# Create network and output predictions.", "\n", "outputs", "=", "structured_predictor", "(", "crop_image_batch", ",", "\n", "args", ".", "num_classes", ",", "\n", "False", ",", "\n", "True", ")", "\n", "\n", "# Grab variable names which should be restored from checkpoints.", "\n", "restore_var", "=", "[", "\n", "v", "for", "v", "in", "tf", ".", "global_variables", "(", ")", "\n", "if", "'crop_image_batch'", "not", "in", "v", ".", "name", "]", "\n", "\n", "# Output Segmentation Predictions.", "\n", "segmentation_output", "=", "outputs", "[", "0", "]", "\n", "segmentation_output", "=", "tf", ".", "image", ".", "resize_bilinear", "(", "\n", "segmentation_output", ",", "\n", "tf", ".", "shape", "(", "crop_image_batch", ")", "[", "1", ":", "3", ",", "]", ")", "\n", "segmentation_output", "=", "tf", ".", "nn", ".", "softmax", "(", "segmentation_output", ",", "dim", "=", "3", ")", "\n", "\n", "# Output Depth Estimations.", "\n", "depth_output", "=", "outputs", "[", "1", "]", "\n", "depth_output", "=", "tf", ".", "image", ".", "resize_bilinear", "(", "\n", "depth_output", ",", "\n", "tf", ".", "shape", "(", "crop_image_batch", ")", "[", "1", ":", "3", ",", "]", ")", "\n", "\n", "# Output Surface Normal Estimations.", "\n", "normal_output", "=", "outputs", "[", "2", "]", "\n", "normal_output", "=", "tf", ".", "image", ".", "resize_bilinear", "(", "\n", "normal_output", ",", "\n", "tf", ".", "shape", "(", "crop_image_batch", ")", "[", "1", ":", "3", ",", "]", ")", "\n", "\n", "# Set up tf session and initialize variables. ", "\n", "config", "=", "tf", ".", "ConfigProto", "(", ")", "\n", "config", ".", "gpu_options", ".", "allow_growth", "=", "True", "\n", "sess", "=", "tf", ".", "Session", "(", "config", "=", "config", ")", "\n", "init", "=", "tf", ".", "global_variables_initializer", "(", ")", "\n", "\n", "sess", ".", "run", "(", "init", ")", "\n", "sess", ".", "run", "(", "tf", ".", "local_variables_initializer", "(", ")", ")", "\n", "\n", "# Load weights.", "\n", "if", "args", ".", "restore_from", "is", "not", "None", ":", "\n", "    ", "loader", "=", "tf", ".", "train", ".", "Saver", "(", "var_list", "=", "restore_var", ")", "\n", "load", "(", "loader", ",", "sess", ",", "args", ".", "restore_from", ")", "\n", "\n", "# Start queue threads.", "\n", "", "threads", "=", "tf", ".", "train", ".", "start_queue_runners", "(", "coord", "=", "coord", ",", "sess", "=", "sess", ")", "\n", "\n", "# Get colormap.", "\n", "map_data", "=", "scipy", ".", "io", ".", "loadmat", "(", "args", ".", "colormap", ")", "\n", "key", "=", "os", ".", "path", ".", "basename", "(", "args", ".", "colormap", ")", ".", "replace", "(", "'.mat'", ",", "''", ")", "\n", "colormap", "=", "map_data", "[", "key", "]", "\n", "colormap", "*=", "255", "\n", "colormap", "=", "colormap", ".", "astype", "(", "np", ".", "uint8", ")", "\n", "\n", "# Create directory for saving predictions.", "\n", "segmentation_dir", "=", "os", ".", "path", ".", "join", "(", "args", ".", "save_dir", ",", "'segmentation_gray'", ")", "\n", "segmentation_rgb_dir", "=", "os", ".", "path", ".", "join", "(", "args", ".", "save_dir", ",", "'segmentation_color'", ")", "\n", "depth_dir", "=", "os", ".", "path", ".", "join", "(", "args", ".", "save_dir", ",", "'depth'", ")", "\n", "normal_dir", "=", "os", ".", "path", ".", "join", "(", "args", ".", "save_dir", ",", "'normal'", ")", "\n", "if", "args", ".", "train_segmentation", "and", "not", "os", ".", "path", ".", "isdir", "(", "segmentation_dir", ")", ":", "\n", "    ", "os", ".", "makedirs", "(", "segmentation_dir", ")", "\n", "", "if", "args", ".", "train_segmentation", "and", "not", "os", ".", "path", ".", "isdir", "(", "segmentation_rgb_dir", ")", ":", "\n", "    ", "os", ".", "makedirs", "(", "segmentation_rgb_dir", ")", "\n", "", "if", "args", ".", "train_depth", "and", "not", "os", ".", "path", ".", "isdir", "(", "depth_dir", ")", ":", "\n", "    ", "os", ".", "makedirs", "(", "depth_dir", ")", "\n", "", "if", "args", ".", "train_normal", "and", "not", "os", ".", "path", ".", "isdir", "(", "normal_dir", ")", ":", "\n", "    ", "os", ".", "makedirs", "(", "normal_dir", ")", "\n", "\n", "# Iterate over inference steps.", "\n", "", "with", "open", "(", "args", ".", "data_list", ",", "'r'", ")", "as", "listf", ":", "\n", "    ", "num_steps", "=", "len", "(", "listf", ".", "read", "(", ")", ".", "split", "(", "'\\n'", ")", ")", "-", "1", "\n", "\n", "", "for", "step", "in", "range", "(", "num_steps", ")", ":", "\n", "    ", "img_batch", "=", "sess", ".", "run", "(", "image_batch", ")", "\n", "img_size", "=", "img_batch", ".", "shape", "\n", "padimg_size", "=", "list", "(", "img_size", ")", "# deep copy of img_size", "\n", "\n", "padimg_h", ",", "padimg_w", "=", "padimg_size", "[", "1", ":", "3", "]", "\n", "input_h", ",", "input_w", "=", "input_size", "\n", "\n", "if", "input_h", ">", "padimg_h", ":", "\n", "      ", "padimg_h", "=", "input_h", "\n", "", "if", "input_w", ">", "padimg_w", ":", "\n", "      ", "padimg_w", "=", "input_w", "\n", "# Update padded image size.", "\n", "", "padimg_size", "[", "1", "]", "=", "padimg_h", "\n", "padimg_size", "[", "2", "]", "=", "padimg_w", "\n", "padimg_batch", "=", "np", ".", "zeros", "(", "padimg_size", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "img_h", ",", "img_w", "=", "img_size", "[", "1", ":", "3", "]", "\n", "padimg_batch", "[", ":", ",", ":", "img_h", ",", ":", "img_w", ",", ":", "]", "=", "img_batch", "\n", "\n", "# Create padded segmentation prediction array.", "\n", "segmentation_size", "=", "list", "(", "padimg_size", ")", "\n", "segmentation_size", "[", "-", "1", "]", "=", "args", ".", "num_classes", "\n", "segmentation_batch", "=", "np", ".", "zeros", "(", "segmentation_size", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "segmentation_batch", ".", "fill", "(", "args", ".", "ignore_label", ")", "\n", "\n", "# Create padded depth estimation array.", "\n", "depth_size", "=", "list", "(", "padimg_size", ")", "\n", "depth_size", "[", "-", "1", "]", "=", "1", "\n", "depth_batch", "=", "np", ".", "zeros", "(", "depth_size", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "# Create padded surface normal estimation array.", "\n", "normal_size", "=", "list", "(", "padimg_size", ")", "\n", "normal_size", "[", "-", "1", "]", "=", "3", "\n", "normal_batch", "=", "np", ".", "zeros", "(", "normal_size", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "# Create padding array for recording number of patches.", "\n", "num_batch", "=", "np", ".", "zeros_like", "(", "depth_batch", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "stride_h", ",", "stride_w", "=", "strides", "\n", "npatches_h", "=", "math", ".", "ceil", "(", "1.0", "*", "(", "padimg_h", "-", "input_h", ")", "/", "stride_h", ")", "+", "1", "\n", "npatches_w", "=", "math", ".", "ceil", "(", "1.0", "*", "(", "padimg_w", "-", "input_w", ")", "/", "stride_w", ")", "+", "1", "\n", "# Create the ending index of each patch", "\n", "patch_indh", "=", "np", ".", "linspace", "(", "input_h", ",", "padimg_h", ",", "\n", "npatches_h", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "patch_indw", "=", "np", ".", "linspace", "(", "input_w", ",", "padimg_w", ",", "\n", "npatches_w", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "\n", "for", "indh", "in", "patch_indh", ":", "\n", "      ", "for", "indw", "in", "patch_indw", ":", "\n", "        ", "sh", ",", "eh", "=", "indh", "-", "input_h", ",", "indh", "# start&end ind of H", "\n", "sw", ",", "ew", "=", "indw", "-", "input_w", ",", "indw", "# start&end ind of W", "\n", "cropimg_batch", "=", "padimg_batch", "[", ":", ",", "sh", ":", "eh", ",", "sw", ":", "ew", ",", ":", "]", "\n", "feed_dict", "=", "{", "crop_image_batch", ":", "cropimg_batch", "}", "\n", "\n", "outs", "=", "sess", ".", "run", "(", "\n", "[", "segmentation_output", ",", "depth_output", ",", "normal_output", "]", ",", "\n", "feed_dict", "=", "feed_dict", ")", "\n", "segmentation_batch", "[", ":", ",", "sh", ":", "eh", ",", "sw", ":", "ew", ",", ":", "]", "+=", "outs", "[", "0", "]", "\n", "depth_batch", "[", ":", ",", "sh", ":", "eh", ",", "sw", ":", "ew", ",", ":", "]", "+=", "outs", "[", "1", "]", "\n", "normal_batch", "[", ":", ",", "sh", ":", "eh", ",", "sw", ":", "ew", ",", ":", "]", "+=", "outs", "[", "2", "]", "\n", "num_batch", "[", ":", ",", "sh", ":", "eh", ",", "sw", ":", "ew", ",", ":", "]", "+=", "1", "\n", "\n", "", "", "num_batch", "=", "num_batch", "[", "0", ",", ":", ",", ":", ",", ":", "]", "\n", "\n", "# Discretize probability prediction to class index.", "\n", "segmentation_batch", "=", "segmentation_batch", "[", "0", ",", ":", "img_h", ",", ":", "img_w", ",", ":", "]", "\n", "segmentation_batch", "=", "np", ".", "argmax", "(", "segmentation_batch", ",", "axis", "=", "-", "1", ")", "\n", "segmentation_batch", "=", "segmentation_batch", ".", "astype", "(", "np", ".", "uint8", ")", "\n", "\n", "# Average the depth estimations.", "\n", "depth_batch", "=", "(", "depth_batch", "/", "num_batch", "*", "args", ".", "depth_unit", ")", ".", "astype", "(", "np", ".", "int32", ")", "\n", "depth_batch", "=", "depth_batch", "[", "0", ",", ":", "img_h", ",", ":", "img_w", ",", "0", "]", "\n", "\n", "# Average the surface normal estimations.", "\n", "normal_batch", "=", "(", "normal_batch", "/", "num_batch", "*", "127.5", "+", "127.5", ")", ".", "astype", "(", "np", ".", "uint8", ")", "\n", "normal_batch", "=", "normal_batch", "[", "0", ",", ":", "img_h", ",", ":", "img_w", ",", ":", "]", "\n", "\n", "basename", "=", "os", ".", "path", ".", "basename", "(", "image_list", "[", "step", "]", ")", "\n", "basename", "=", "basename", ".", "replace", "(", "'jpg'", ",", "'png'", ")", "\n", "\n", "# Save segmentation and colorized results to files.", "\n", "if", "args", ".", "train_segmentation", ":", "\n", "      ", "segmentation_name", "=", "os", ".", "path", ".", "join", "(", "\n", "segmentation_dir", ",", "\n", "basename", ".", "replace", "(", "'rgb'", ",", "'semantic'", ")", ")", "\n", "Image", ".", "fromarray", "(", "segmentation_batch", ",", "mode", "=", "'L'", ")", ".", "save", "(", "segmentation_name", ")", "\n", "\n", "segmentation_rgb_name", "=", "os", ".", "path", ".", "join", "(", "segmentation_rgb_dir", ",", "\n", "basename", ".", "replace", "(", "'rgb'", ",", "'semantic'", ")", ")", "\n", "color", "=", "colormap", "[", "segmentation_batch", "]", "\n", "Image", ".", "fromarray", "(", "color", ",", "mode", "=", "'RGB'", ")", ".", "save", "(", "segmentation_rgb_name", ")", "\n", "\n", "# Save depth estimation results to files.", "\n", "", "if", "args", ".", "train_depth", ":", "\n", "      ", "depth_name", "=", "os", ".", "path", ".", "join", "(", "depth_dir", ",", "basename", ".", "replace", "(", "'rgb'", ",", "'depth'", ")", ")", "\n", "Image", ".", "fromarray", "(", "depth_batch", ",", "mode", "=", "'I'", ")", ".", "save", "(", "depth_name", ")", "\n", "\n", "# Save surface normal estimation results to files.", "\n", "", "if", "args", ".", "train_normal", ":", "\n", "      ", "if", "'area_6'", "in", "image_list", "[", "step", "]", ":", "\n", "        ", "normal_name", "=", "os", ".", "path", ".", "join", "(", "normal_dir", ",", "basename", ".", "replace", "(", "'rgb'", ",", "'normal'", ")", ")", "\n", "", "else", ":", "\n", "        ", "normal_name", "=", "os", ".", "path", ".", "join", "(", "normal_dir", ",", "basename", ".", "replace", "(", "'rgb'", ",", "'normals'", ")", ")", "\n", "", "Image", ".", "fromarray", "(", "normal_batch", ",", "mode", "=", "'RGB'", ")", ".", "save", "(", "normal_name", ")", "\n", "\n", "", "", "coord", ".", "request_stop", "(", ")", "\n", "coord", ".", "join", "(", "threads", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.benchmark.benchmark_stanford_scenes.parse_argument": [[10, 33], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.preprocess.stanford_scenes_down_sample.parse_args"], ["def", "parse_argument", "(", ")", ":", "\n", "  ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "description", "=", "'Benchmark over 2D-3D-Semantics on segmentation, '", "+", "'depth and surface normals estimation'", ")", "\n", "parser", ".", "add_argument", "(", "'--pred_dir'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "\n", "help", "=", "'/path/to/prediction.'", ")", "\n", "parser", ".", "add_argument", "(", "'--gt_dir'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "\n", "help", "=", "'/path/to/ground-truths.'", ")", "\n", "parser", ".", "add_argument", "(", "'--depth_unit'", ",", "type", "=", "float", ",", "default", "=", "512.0", ",", "\n", "help", "=", "'Each pixel value difference means 1/depth_unit meters.'", ")", "\n", "parser", ".", "add_argument", "(", "'--num_classes'", ",", "type", "=", "int", ",", "default", "=", "21", ",", "\n", "help", "=", "'number of segmentation classes.'", ")", "\n", "parser", ".", "add_argument", "(", "'--string_replace'", ",", "type", "=", "str", ",", "default", "=", "','", ",", "\n", "help", "=", "'replace the first string with the second one.'", ")", "\n", "parser", ".", "add_argument", "(", "'--train_segmentation'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'enable/disable to benchmark segmentation on mIoU.'", ")", "\n", "parser", ".", "add_argument", "(", "'--train_depth'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'enable/disable to benchmark depth.'", ")", "\n", "parser", ".", "add_argument", "(", "'--train_normal'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'enable/disable to benchmark surface normal.'", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.benchmark.benchmark_stanford_scenes.benchmark_segmentation": [[35, 84], ["print", "os.path.isdir", "os.path.isdir", "numpy.zeros", "numpy.zeros", "numpy.zeros", "os.walk", "range", "print", "print", "print", "iou.sum", "np.zeros.sum", "os.path.join", "os.path.join.replace", "numpy.asarray", "numpy.asarray", "utils.metrics.iou_stats", "np.zeros.sum", "string_replace.split", "gtname.replace.replace", "PIL.Image.open().convert", "PIL.Image.open().convert", "PIL.Image.open", "PIL.Image.open"], "function", ["home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.utils.metrics.iou_stats"], ["", "def", "benchmark_segmentation", "(", "pred_dir", ",", "gt_dir", ",", "num_classes", ",", "string_replace", ")", ":", "\n", "  ", "\"\"\"Benchmark segmentaion on mean Intersection over Union (mIoU).\n  \"\"\"", "\n", "print", "(", "'Benchmarking semantic segmentation.'", ")", "\n", "assert", "(", "os", ".", "path", ".", "isdir", "(", "pred_dir", ")", ")", "\n", "assert", "(", "os", ".", "path", ".", "isdir", "(", "gt_dir", ")", ")", "\n", "tp_fn", "=", "np", ".", "zeros", "(", "num_classes", ",", "dtype", "=", "np", ".", "float64", ")", "\n", "tp_fp", "=", "np", ".", "zeros", "(", "num_classes", ",", "dtype", "=", "np", ".", "float64", ")", "\n", "tp", "=", "np", ".", "zeros", "(", "num_classes", ",", "dtype", "=", "np", ".", "float64", ")", "\n", "for", "dirpath", ",", "dirnames", ",", "filenames", "in", "os", ".", "walk", "(", "pred_dir", ")", ":", "\n", "    ", "for", "filename", "in", "filenames", ":", "\n", "      ", "predname", "=", "os", ".", "path", ".", "join", "(", "dirpath", ",", "filename", ")", "\n", "gtname", "=", "predname", ".", "replace", "(", "pred_dir", ",", "gt_dir", ")", "\n", "if", "string_replace", "!=", "''", ":", "\n", "        ", "stra", ",", "strb", "=", "string_replace", ".", "split", "(", "','", ")", "\n", "gtname", "=", "gtname", ".", "replace", "(", "stra", ",", "strb", ")", "\n", "\n", "", "pred", "=", "np", ".", "asarray", "(", "\n", "Image", ".", "open", "(", "predname", ")", ".", "convert", "(", "mode", "=", "'L'", ")", ",", "\n", "dtype", "=", "np", ".", "uint8", "\n", ")", "\n", "gt", "=", "np", ".", "asarray", "(", "\n", "Image", ".", "open", "(", "gtname", ")", ".", "convert", "(", "mode", "=", "'L'", ")", ",", "\n", "dtype", "=", "np", ".", "uint8", "\n", ")", "\n", "_tp_fn", ",", "_tp_fp", ",", "_tp", "=", "iou_stats", "(", "\n", "pred", ",", "\n", "gt", ",", "\n", "num_classes", "=", "num_classes", ",", "\n", "background", "=", "0", "\n", ")", "\n", "tp_fn", "+=", "_tp_fn", "\n", "tp_fp", "+=", "_tp_fp", "\n", "tp", "+=", "_tp", "\n", "\n", "", "", "iou", "=", "tp", "/", "(", "tp_fn", "+", "tp_fp", "-", "tp", "+", "1e-12", ")", "*", "100.0", "\n", "\n", "class_names", "=", "[", "'beam'", ",", "'board'", ",", "'bookcase'", ",", "'ceiling'", ",", "'chair'", ",", "'clutter'", ",", "\n", "'column'", ",", "'door'", ",", "'floor'", ",", "'sofa'", ",", "'table'", ",", "'wall'", ",", "'window'", "]", "\n", "\n", "for", "i", "in", "range", "(", "num_classes", ")", ":", "\n", "    ", "print", "(", "'class {:10s}: {:02d}, acc: {:4.4f}%'", ".", "format", "(", "\n", "class_names", "[", "i", "]", ",", "i", ",", "iou", "[", "i", "]", ")", "\n", ")", "\n", "", "mean_iou", "=", "iou", ".", "sum", "(", ")", "/", "num_classes", "\n", "print", "(", "'mean IOU: {:4.4f}%'", ".", "format", "(", "mean_iou", ")", ")", "\n", "\n", "mean_pixel_acc", "=", "tp", ".", "sum", "(", ")", "/", "(", "tp_fp", ".", "sum", "(", ")", "+", "1e-12", ")", "\n", "print", "(", "'mean Pixel Acc: {:4.4f}%'", ".", "format", "(", "mean_pixel_acc", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.benchmark.benchmark_stanford_scenes.benchmark_depth": [[86, 150], ["print", "os.path.isdir", "os.path.isdir", "os.walk", "numpy.sqrt", "numpy.sqrt", "range", "print", "print", "print", "print", "range", "print", "print", "print", "len", "len", "print", "os.path.join", "os.path.join.replace", "numpy.asarray", "numpy.asarray", "numpy.reshape", "numpy.reshape", "numpy.clip", "numpy.sum", "numpy.sum", "numpy.sum", "numpy.sum", "numpy.maximum", "range", "string_replace.split", "gtname.replace.replace", "PIL.Image.open().convert", "PIL.Image.open().convert", "pred[].astype", "gt[].astype", "len", "numpy.sum", "numpy.abs", "PIL.Image.open", "PIL.Image.open", "numpy.log", "numpy.log", "numpy.maximum", "numpy.maximum"], "function", ["None"], ["", "def", "benchmark_depth", "(", "pred_dir", ",", "gt_dir", ",", "string_replace", ")", ":", "\n", "  ", "\"\"\"Benchmark depth estimation.\n  \"\"\"", "\n", "print", "(", "'Benchmarking depth estimations.'", ")", "\n", "assert", "(", "os", ".", "path", ".", "isdir", "(", "pred_dir", ")", ")", "\n", "assert", "(", "os", ".", "path", ".", "isdir", "(", "gt_dir", ")", ")", "\n", "N", "=", "0.0", "\n", "rmse_linear", "=", "0.0", "\n", "rmse_log", "=", "0.0", "\n", "absrel", "=", "0.0", "\n", "sqrrel", "=", "0.0", "\n", "thresholds", "=", "[", "0.0", ",", "0.0", ",", "0.0", ",", "0.0", ",", "0.0", ",", "0.0", "]", "\n", "powers", "=", "[", "1", "/", "8.0", ",", "1", "/", "4.0", ",", "1", "/", "2.0", ",", "1.0", ",", "2.0", ",", "3.0", "]", "\n", "\n", "for", "dirpath", ",", "dirnames", ",", "filenames", "in", "os", ".", "walk", "(", "pred_dir", ")", ":", "\n", "    ", "for", "filename", "in", "filenames", ":", "\n", "      ", "predname", "=", "os", ".", "path", ".", "join", "(", "dirpath", ",", "filename", ")", "\n", "gtname", "=", "predname", ".", "replace", "(", "pred_dir", ",", "gt_dir", ")", "\n", "if", "string_replace", "!=", "''", ":", "\n", "        ", "stra", ",", "strb", "=", "string_replace", ".", "split", "(", "','", ")", "\n", "gtname", "=", "gtname", ".", "replace", "(", "stra", ",", "strb", ")", "\n", "\n", "", "pred", "=", "np", ".", "asarray", "(", "\n", "Image", ".", "open", "(", "predname", ")", ".", "convert", "(", "mode", "=", "'I'", ")", ",", "\n", "dtype", "=", "np", ".", "int32", ")", "\n", "gt", "=", "np", ".", "asarray", "(", "\n", "Image", ".", "open", "(", "gtname", ")", ".", "convert", "(", "mode", "=", "'I'", ")", ",", "\n", "dtype", "=", "np", ".", "int32", ")", "\n", "\n", "pred", "=", "np", ".", "reshape", "(", "pred", ",", "(", "-", "1", ",", ")", ")", "\n", "gt", "=", "np", ".", "reshape", "(", "gt", ",", "(", "-", "1", ",", ")", ")", "\n", "#mask = np.logical_and(gt >= 51, gt <= 26560)", "\n", "mask", "=", "gt", "<", "2", "**", "16", "-", "1", "\n", "\n", "pred", "=", "np", ".", "clip", "(", "pred", ",", "51", ",", "26560", ")", "\n", "pred", "=", "pred", "[", "mask", "]", ".", "astype", "(", "np", ".", "float32", ")", "/", "args", ".", "depth_unit", "\n", "gt", "=", "gt", "[", "mask", "]", ".", "astype", "(", "np", ".", "float32", ")", "/", "args", ".", "depth_unit", "\n", "\n", "rmse_linear", "+=", "np", ".", "sum", "(", "(", "pred", "-", "gt", ")", "**", "2", ")", "\n", "rmse_log", "+=", "np", ".", "sum", "(", "\n", "(", "np", ".", "log", "(", "np", ".", "maximum", "(", "pred", ",", "1e-12", ")", ")", "-", "np", ".", "log", "(", "np", ".", "maximum", "(", "gt", ",", "1e-12", ")", ")", ")", "**", "2", ")", "\n", "absrel", "+=", "np", ".", "sum", "(", "np", ".", "abs", "(", "pred", "-", "gt", ")", "/", "gt", ")", "\n", "sqrrel", "+=", "np", ".", "sum", "(", "(", "pred", "-", "gt", ")", "**", "2", "/", "gt", ")", "\n", "th", "=", "np", ".", "maximum", "(", "pred", "/", "gt", ",", "gt", "/", "pred", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "thresholds", ")", ")", ":", "\n", "#thresholds[i] += np.sum(th < 1.25**(i+1))", "\n", "        ", "thresholds", "[", "i", "]", "+=", "np", ".", "sum", "(", "th", "<", "1.25", "**", "powers", "[", "i", "]", ")", "\n", "", "N", "+=", "pred", ".", "shape", "[", "0", "]", "\n", "\n", "", "", "rmse_linear", "=", "np", ".", "sqrt", "(", "rmse_linear", "/", "N", ")", "\n", "rmse_log", "=", "np", ".", "sqrt", "(", "rmse_log", "/", "N", ")", "\n", "absrel", "=", "absrel", "/", "N", "\n", "sqrrel", "=", "sqrrel", "/", "N", "\n", "for", "i", "in", "range", "(", "len", "(", "thresholds", ")", ")", ":", "\n", "    ", "thresholds", "[", "i", "]", "=", "thresholds", "[", "i", "]", "/", "N", "\n", "", "print", "(", "'RMSE(lin): {:.4f}'", ".", "format", "(", "rmse_linear", ")", ")", "\n", "print", "(", "'RMSE(log): {:.4f}'", ".", "format", "(", "rmse_log", ")", ")", "\n", "print", "(", "'abs rel: {:.4f}'", ".", "format", "(", "absrel", ")", ")", "\n", "print", "(", "'sqr rel: {:.4f}'", ".", "format", "(", "sqrrel", ")", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "thresholds", ")", ")", ":", "\n", "    ", "print", "(", "'\\sigma < 1.25^{:.4f}: {:.4f}'", ".", "format", "(", "powers", "[", "i", "]", ",", "thresholds", "[", "i", "]", ")", ")", "\n", "", "print", "(", "'\\sigma < 1.25: {:.4f}'", ".", "format", "(", "thresholds", "[", "0", "]", ")", ")", "\n", "print", "(", "'\\sigma < 1.25^2: {:.4f}'", ".", "format", "(", "thresholds", "[", "1", "]", ")", ")", "\n", "print", "(", "'\\sigma < 1.25^3: {:.4f}'", ".", "format", "(", "thresholds", "[", "2", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.benchmark.benchmark_stanford_scenes.benchmark_normal": [[152, 201], ["print", "os.path.isdir", "os.path.isdir", "os.walk", "numpy.concatenate", "print", "print", "print", "print", "print", "print", "print", "numpy.arccos", "os.path.join", "os.path.join.replace", "numpy.asarray", "numpy.asarray", "numpy.reshape", "numpy.reshape", "numpy.any", "numpy.sum", "numpy.abs", "numpy.clip", "np.concatenate.append", "numpy.mean", "numpy.median", "string_replace.split", "gtname.replace.replace", "PIL.Image.open().convert", "PIL.Image.open().convert", "pred[].astype", "gt[].astype", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.linalg.norm", "numpy.linalg.norm", "PIL.Image.open", "PIL.Image.open"], "function", ["None"], ["", "def", "benchmark_normal", "(", "pred_dir", ",", "gt_dir", ",", "string_replace", ")", ":", "\n", "  ", "\"\"\"Benchmark surface normal estimations.\n  \"\"\"", "\n", "print", "(", "'Benchmarking surface normal estimations.'", ")", "\n", "assert", "(", "os", ".", "path", ".", "isdir", "(", "pred_dir", ")", ")", "\n", "assert", "(", "os", ".", "path", ".", "isdir", "(", "gt_dir", ")", ")", "\n", "N", "=", "0.0", "\n", "angles", "=", "[", "]", "\n", "\n", "for", "dirpath", ",", "dirnames", ",", "filenames", "in", "os", ".", "walk", "(", "pred_dir", ")", ":", "\n", "    ", "for", "filename", "in", "filenames", ":", "\n", "      ", "predname", "=", "os", ".", "path", ".", "join", "(", "dirpath", ",", "filename", ")", "\n", "gtname", "=", "predname", ".", "replace", "(", "pred_dir", ",", "gt_dir", ")", "\n", "if", "string_replace", "!=", "''", ":", "\n", "        ", "stra", ",", "strb", "=", "string_replace", ".", "split", "(", "','", ")", "\n", "gtname", "=", "gtname", ".", "replace", "(", "stra", ",", "strb", ")", "\n", "\n", "", "pred", "=", "np", ".", "asarray", "(", "\n", "Image", ".", "open", "(", "predname", ")", ".", "convert", "(", "mode", "=", "'RGB'", ")", ",", "\n", "dtype", "=", "np", ".", "uint8", ")", "\n", "gt", "=", "np", ".", "asarray", "(", "\n", "Image", ".", "open", "(", "gtname", ")", ".", "convert", "(", "mode", "=", "'RGB'", ")", ",", "\n", "dtype", "=", "np", ".", "uint8", ")", "\n", "\n", "pred", "=", "np", ".", "reshape", "(", "pred", ",", "(", "-", "1", ",", "3", ")", ")", "\n", "gt", "=", "np", ".", "reshape", "(", "gt", ",", "(", "-", "1", ",", "3", ")", ")", "\n", "mask", "=", "np", ".", "any", "(", "gt", "!=", "128", ",", "axis", "=", "-", "1", ")", "\n", "\n", "pred", "=", "pred", "[", "mask", ",", ":", "]", ".", "astype", "(", "np", ".", "float32", ")", "-", "127.5", "\n", "gt", "=", "gt", "[", "mask", ",", ":", "]", ".", "astype", "(", "np", ".", "float32", ")", "-", "127.5", "\n", "\n", "pred", "=", "pred", "/", "(", "np", ".", "linalg", ".", "norm", "(", "pred", ",", "axis", "=", "-", "1", ",", "keepdims", "=", "True", ")", "+", "1e-12", ")", "\n", "gt", "=", "gt", "/", "(", "np", ".", "linalg", ".", "norm", "(", "gt", ",", "axis", "=", "-", "1", ",", "keepdims", "=", "True", ")", "+", "1e-12", ")", "\n", "\n", "cos", "=", "np", ".", "sum", "(", "pred", "*", "gt", ",", "axis", "=", "-", "1", ")", "\n", "abs_cos", "=", "np", ".", "abs", "(", "cos", ")", "\n", "assert", "(", "not", "(", "abs_cos", "-", "1", ">", "1e-5", ")", ".", "any", "(", ")", ")", "\n", "cos", "=", "np", ".", "clip", "(", "cos", ",", "-", "1", ",", "1", ")", "\n", "angles", ".", "append", "(", "cos", ")", "\n", "\n", "", "", "angles", "=", "np", ".", "concatenate", "(", "angles", ",", "axis", "=", "0", ")", "\n", "angles", "=", "np", ".", "arccos", "(", "angles", ")", "*", "(", "180.0", "/", "np", ".", "pi", ")", "\n", "print", "(", "'Angle Mean: {:.4f}'", ".", "format", "(", "np", ".", "mean", "(", "angles", ")", ")", ")", "\n", "print", "(", "'Angle Median: {:.4f}'", ".", "format", "(", "np", ".", "median", "(", "angles", ")", ")", ")", "\n", "print", "(", "'Angles within 2.8125: {:.4f}%'", ".", "format", "(", "np", ".", "mean", "(", "angles", "<=", "2.8125", ")", "*", "100.0", ")", ")", "\n", "print", "(", "'Angles within 5.625: {:.4f}%'", ".", "format", "(", "np", ".", "mean", "(", "angles", "<=", "5.625", ")", "*", "100.0", ")", ")", "\n", "print", "(", "'Angles within 11.25: {:.4f}%'", ".", "format", "(", "np", ".", "mean", "(", "angles", "<=", "11.25", ")", "*", "100.0", ")", ")", "\n", "print", "(", "'Angles within 22.5: {:.4f}%'", ".", "format", "(", "np", ".", "mean", "(", "angles", "<=", "22.5", ")", "*", "100.0", ")", ")", "\n", "print", "(", "'Angles within 30: {:.4f}%'", ".", "format", "(", "np", ".", "mean", "(", "angles", "<=", "30", ")", "*", "100.0", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.benchmark.benchmark_stanford_scenes_by_instance.parse_argument": [[11, 35], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.preprocess.stanford_scenes_down_sample.parse_args"], ["def", "parse_argument", "(", ")", ":", "\n", "  ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "description", "=", "'Benchmark over 2D-3D-Semantics on segmentation, '", "+", "'depth and surface normals estimation'", ")", "\n", "parser", ".", "add_argument", "(", "'--pred_dir'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "\n", "help", "=", "'/path/to/prediction.'", ")", "\n", "parser", ".", "add_argument", "(", "'--gt_dir'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "\n", "help", "=", "'/path/to/ground-truths.'", ")", "\n", "parser", ".", "add_argument", "(", "'--inst_class_json'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "\n", "help", "=", "'/path/to/json/of/inst/classes.'", ")", "\n", "parser", ".", "add_argument", "(", "'--depth_unit'", ",", "type", "=", "float", ",", "default", "=", "512.0", ",", "\n", "help", "=", "'Each pixel value difference means 1/depth_unit '", "\n", "'meters.'", ")", "\n", "parser", ".", "add_argument", "(", "'--num_classes'", ",", "type", "=", "int", ",", "default", "=", "21", ",", "\n", "help", "=", "'number of segmentation classes.'", ")", "\n", "parser", ".", "add_argument", "(", "'--string_replace'", ",", "type", "=", "str", ",", "default", "=", "','", ",", "\n", "help", "=", "'replace the first string with the second one.'", ")", "\n", "parser", ".", "add_argument", "(", "'--train_depth'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'enable/disable to benchmark depth.'", ")", "\n", "parser", ".", "add_argument", "(", "'--train_normal'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'enable/disable to benchmark surface normal.'", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.benchmark.benchmark_stanford_scenes_by_instance.benchmark_depth": [[37, 167], ["print", "os.path.isdir", "os.path.isdir", "os.path.isdir", "len", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "os.walk", "numpy.maximum", "numpy.sqrt", "numpy.sqrt", "range", "enumerate", "numpy.maximum", "range", "cls_names.items", "numpy.zeros", "len", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "len", "print", "print", "print", "print", "range", "print", "print", "print", "range", "os.path.join", "os.path.join.replace", "gtname.replace.replace", "numpy.asarray", "numpy.asarray", "numpy.asarray", "numpy.reshape", "numpy.reshape", "numpy.reshape", "numpy.clip", "numpy.unique", "inst_name.split", "cls_names.get", "len", "cls_rmse_linear.append", "cls_rmse_log.append", "cls_absrel.append", "cls_sqrrel.append", "np.maximum.append", "zip", "numpy.array", "len", "print", "string_replace.split", "gtname.replace.replace", "PIL.Image.open().convert", "PIL.Image.open().convert", "PIL.Image.open().convert", "pred[].astype", "gt[].astype", "numpy.sum", "numpy.sum", "numpy.sum", "numpy.sum", "numpy.maximum", "range", "m.sum", "t.append", "m.any", "len", "numpy.sum", "numpy.power", "PIL.Image.open", "PIL.Image.open", "PIL.Image.open", "numpy.abs", "numpy.log", "numpy.log", "numpy.power"], "function", ["None"], ["", "def", "benchmark_depth", "(", "pred_dir", ",", "gt_dir", ",", "inst_dir", ",", "\n", "instance_labels", ",", "string_replace", ")", ":", "\n", "  ", "\"\"\"Benchmark depth estimation.\n  \"\"\"", "\n", "print", "(", "'Benchmarking instance-level depth estimations.'", ")", "\n", "assert", "(", "os", ".", "path", ".", "isdir", "(", "pred_dir", ")", ")", "\n", "assert", "(", "os", ".", "path", ".", "isdir", "(", "gt_dir", ")", ")", "\n", "assert", "(", "os", ".", "path", ".", "isdir", "(", "inst_dir", ")", ")", "\n", "num_instance", "=", "len", "(", "instance_labels", ")", "\n", "num_pixels", "=", "np", ".", "zeros", "(", "num_instance", ",", "dtype", "=", "np", ".", "float64", ")", "\n", "rmse_linear", "=", "np", ".", "zeros", "(", "num_instance", ",", "dtype", "=", "np", ".", "float64", ")", "\n", "rmse_log", "=", "np", ".", "zeros", "(", "num_instance", ",", "dtype", "=", "np", ".", "float64", ")", "\n", "absrel", "=", "np", ".", "zeros", "(", "num_instance", ",", "dtype", "=", "np", ".", "float64", ")", "\n", "sqrrel", "=", "np", ".", "zeros", "(", "num_instance", ",", "dtype", "=", "np", ".", "float64", ")", "\n", "thresholds", "=", "[", "np", ".", "zeros", "(", "num_instance", ",", "dtype", "=", "np", ".", "float64", ")", "for", "_", "in", "range", "(", "5", ")", "]", "\n", "\n", "for", "dirpath", ",", "dirnames", ",", "filenames", "in", "os", ".", "walk", "(", "pred_dir", ")", ":", "\n", "    ", "for", "filename", "in", "filenames", ":", "\n", "      ", "predname", "=", "os", ".", "path", ".", "join", "(", "dirpath", ",", "filename", ")", "\n", "gtname", "=", "predname", ".", "replace", "(", "pred_dir", ",", "gt_dir", ")", "\n", "instname", "=", "gtname", ".", "replace", "(", "'depth'", ",", "'semantic'", ")", "\n", "if", "string_replace", "!=", "''", ":", "\n", "        ", "stra", ",", "strb", "=", "string_replace", ".", "split", "(", "','", ")", "\n", "gtname", "=", "gtname", ".", "replace", "(", "stra", ",", "strb", ")", "\n", "\n", "", "pred", "=", "np", ".", "asarray", "(", "\n", "Image", ".", "open", "(", "predname", ")", ".", "convert", "(", "mode", "=", "'I'", ")", ",", "\n", "dtype", "=", "np", ".", "int32", ")", "\n", "gt", "=", "np", ".", "asarray", "(", "\n", "Image", ".", "open", "(", "gtname", ")", ".", "convert", "(", "mode", "=", "'I'", ")", ",", "\n", "dtype", "=", "np", ".", "int32", ")", "\n", "inst", "=", "np", ".", "asarray", "(", "\n", "Image", ".", "open", "(", "instname", ")", ".", "convert", "(", "mode", "=", "'RGB'", ")", ",", "\n", "dtype", "=", "np", ".", "uint8", ")", "\n", "inst", "=", "inst", "[", ":", ",", ":", ",", "0", "]", "*", "256", "**", "2", "+", "inst", "[", ":", ",", ":", ",", "1", "]", "*", "256", "+", "inst", "[", ":", ",", ":", ",", "2", "]", "\n", "\n", "pred", "=", "np", ".", "reshape", "(", "pred", ",", "(", "-", "1", ",", ")", ")", "\n", "gt", "=", "np", ".", "reshape", "(", "gt", ",", "(", "-", "1", ",", ")", ")", "\n", "inst", "=", "np", ".", "reshape", "(", "inst", ",", "(", "-", "1", ",", ")", ")", "\n", "mask", "=", "gt", "<", "2", "**", "16", "-", "1", "\n", "\n", "pred", "=", "np", ".", "clip", "(", "pred", ",", "51", ",", "26560", ")", "\n", "pred", "=", "pred", "[", "mask", "]", ".", "astype", "(", "np", ".", "float32", ")", "/", "args", ".", "depth_unit", "\n", "gt", "=", "gt", "[", "mask", "]", ".", "astype", "(", "np", ".", "float32", ")", "/", "args", ".", "depth_unit", "\n", "inst", "=", "inst", "[", "mask", "]", "\n", "\n", "for", "inst_ind", "in", "np", ".", "unique", "(", "inst", ")", ":", "\n", "        ", "if", "inst_ind", "==", "855309", ":", "\n", "          ", "continue", "\n", "", "m", "=", "inst", "==", "inst_ind", "\n", "if", "not", "m", ".", "any", "(", ")", ":", "\n", "          ", "continue", "\n", "\n", "", "pred_m", "=", "pred", "[", "m", "]", "\n", "gt_m", "=", "gt", "[", "m", "]", "\n", "rmse_linear", "[", "inst_ind", "]", "+=", "np", ".", "sum", "(", "(", "pred_m", "-", "gt_m", ")", "**", "2", ")", "\n", "rmse_log", "[", "inst_ind", "]", "+=", "np", ".", "sum", "(", "\n", "(", "np", ".", "log", "(", "pred_m", ")", "-", "np", ".", "log", "(", "gt_m", ")", ")", "**", "2", ")", "\n", "absrel", "[", "inst_ind", "]", "+=", "np", ".", "sum", "(", "np", ".", "abs", "(", "pred_m", "-", "gt_m", ")", "/", "gt_m", ")", "\n", "sqrrel", "[", "inst_ind", "]", "+=", "np", ".", "sum", "(", "(", "pred_m", "-", "gt_m", ")", "**", "2", "/", "gt_m", ")", "\n", "th", "=", "np", ".", "maximum", "(", "pred_m", "/", "gt_m", ",", "gt_m", "/", "pred_m", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "thresholds", ")", ")", ":", "\n", "          ", "thresholds", "[", "i", "]", "[", "inst_ind", "]", "+=", "np", ".", "sum", "(", "th", "<", "1.25", "**", "(", "np", ".", "power", "(", "2.0", ",", "i", "-", "2", ")", ")", ")", "\n", "", "num_pixels", "[", "inst_ind", "]", "+=", "m", ".", "sum", "(", ")", "\n", "\n", "# instance level metrics.", "\n", "", "", "", "num_pixels", "=", "np", ".", "maximum", "(", "num_pixels", ",", "1e-12", ")", "\n", "rmse_linear", "=", "np", ".", "sqrt", "(", "rmse_linear", "/", "num_pixels", ")", "\n", "rmse_log", "=", "np", ".", "sqrt", "(", "rmse_log", "/", "num_pixels", ")", "\n", "absrel", "=", "absrel", "/", "num_pixels", "\n", "sqrrel", "=", "sqrrel", "/", "num_pixels", "\n", "for", "i", "in", "range", "(", "len", "(", "thresholds", ")", ")", ":", "\n", "    ", "thresholds", "[", "i", "]", "=", "thresholds", "[", "i", "]", "/", "num_pixels", "\n", "\n", "# semantic level metrics.", "\n", "", "cls_names", "=", "{", "}", "\n", "cls_num_insts", "=", "[", "]", "\n", "cls_rmse_linear", "=", "[", "]", "\n", "cls_rmse_log", "=", "[", "]", "\n", "cls_absrel", "=", "[", "]", "\n", "cls_sqrrel", "=", "[", "]", "\n", "cls_thresholds", "=", "[", "[", "]", "for", "t", "in", "thresholds", "]", "\n", "for", "inst_ind", ",", "inst_name", "in", "enumerate", "(", "instance_labels", ")", ":", "\n", "    ", "cls_name", "=", "inst_name", ".", "split", "(", "'_'", ")", "[", "0", "]", "\n", "if", "cls_names", ".", "get", "(", "cls_name", ",", "None", ")", "is", "None", ":", "\n", "      ", "cls_names", "[", "cls_name", "]", "=", "len", "(", "cls_names", ")", "\n", "cls_rmse_linear", ".", "append", "(", "0.0", ")", "\n", "cls_rmse_log", ".", "append", "(", "0.0", ")", "\n", "cls_absrel", ".", "append", "(", "0.0", ")", "\n", "cls_sqrrel", ".", "append", "(", "0.0", ")", "\n", "for", "t", "in", "cls_thresholds", ":", "\n", "        ", "t", ".", "append", "(", "0.0", ")", "\n", "", "cls_num_insts", ".", "append", "(", "0", ")", "\n", "\n", "", "if", "num_pixels", "[", "inst_ind", "]", ">=", "1", ":", "\n", "      ", "cls_ind", "=", "cls_names", "[", "cls_name", "]", "\n", "cls_num_insts", "[", "cls_ind", "]", "+=", "1", "\n", "cls_rmse_linear", "[", "cls_ind", "]", "+=", "rmse_linear", "[", "inst_ind", "]", "\n", "cls_rmse_log", "[", "cls_ind", "]", "+=", "rmse_log", "[", "inst_ind", "]", "\n", "cls_absrel", "[", "cls_ind", "]", "+=", "absrel", "[", "inst_ind", "]", "\n", "cls_sqrrel", "[", "cls_ind", "]", "+=", "sqrrel", "[", "inst_ind", "]", "\n", "for", "ct", ",", "it", "in", "zip", "(", "cls_thresholds", ",", "thresholds", ")", ":", "\n", "        ", "ct", "[", "cls_ind", "]", "+=", "it", "[", "inst_ind", "]", "\n", "\n", "", "", "", "cls_num_insts", "=", "np", ".", "maximum", "(", "np", ".", "array", "(", "cls_num_insts", ")", ",", "1e-12", ")", "\n", "cls_rmse_linear", "=", "np", ".", "array", "(", "cls_rmse_linear", ")", "/", "cls_num_insts", "\n", "cls_rmse_log", "=", "np", ".", "array", "(", "cls_rmse_log", ")", "/", "cls_num_insts", "\n", "cls_absrel", "=", "np", ".", "array", "(", "cls_absrel", ")", "/", "cls_num_insts", "\n", "cls_sqrrel", "=", "np", ".", "array", "(", "cls_sqrrel", ")", "/", "cls_num_insts", "\n", "for", "i", "in", "range", "(", "len", "(", "cls_thresholds", ")", ")", ":", "\n", "    ", "cls_thresholds", "[", "i", "]", "=", "np", ".", "array", "(", "cls_thresholds", "[", "i", "]", ")", "/", "cls_num_insts", "\n", "\n", "", "for", "cls_name", ",", "cls_ind", "in", "cls_names", ".", "items", "(", ")", ":", "\n", "    ", "print", "(", "'class {:s}, RMSE(lin): {:.4f}'", ".", "format", "(", "\n", "cls_name", ",", "cls_rmse_linear", "[", "cls_ind", "]", ")", ")", "\n", "print", "(", "'class {:s}, RMSE(log): {:.4f}'", ".", "format", "(", "\n", "cls_name", ",", "cls_rmse_log", "[", "cls_ind", "]", ")", ")", "\n", "print", "(", "'class {:s}, abs rel: {:.4f}'", ".", "format", "(", "\n", "cls_name", ",", "cls_absrel", "[", "cls_ind", "]", ")", ")", "\n", "print", "(", "'class {:s}, sqr rel: {:.4f}'", ".", "format", "(", "\n", "cls_name", ",", "cls_sqrrel", "[", "cls_ind", "]", ")", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "cls_thresholds", ")", ")", ":", "\n", "      ", "print", "(", "'class {:s}, \\sigma < 1.25^{:.4f}: {:.4f}'", ".", "format", "(", "\n", "cls_name", ",", "np", ".", "power", "(", "2.0", ",", "i", "-", "2", ")", ",", "cls_thresholds", "[", "i", "]", "[", "cls_ind", "]", ")", ")", "\n", "", "print", "(", "'class {:s}, \\sigma < 1.25: {:.4f}'", ".", "format", "(", "\n", "cls_name", ",", "cls_thresholds", "[", "0", "]", "[", "cls_ind", "]", ")", ")", "\n", "print", "(", "'class {:s}, \\sigma < 1.25^2: {:.4f}'", ".", "format", "(", "\n", "cls_name", ",", "cls_thresholds", "[", "1", "]", "[", "cls_ind", "]", ")", ")", "\n", "print", "(", "'class {:s}, \\sigma < 1.25^3: {:.4f}'", ".", "format", "(", "\n", "cls_name", ",", "cls_thresholds", "[", "2", "]", "[", "cls_ind", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.benchmark.benchmark_stanford_scenes_by_instance.benchmark_normal": [[169, 287], ["print", "os.path.isdir", "os.path.isdir", "os.path.isdir", "len", "os.walk", "enumerate", "numpy.maximum", "cls_names.items", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "print", "print", "print", "print", "print", "print", "print", "range", "os.path.join", "os.path.join.replace", "gtname.replace.replace().replace", "numpy.asarray", "numpy.asarray", "numpy.asarray", "numpy.reshape", "numpy.reshape", "numpy.reshape", "numpy.any", "numpy.sum", "numpy.abs", "numpy.clip", "numpy.unique", "inst_name.split", "cls_names.get", "len", "cls_mean_angles.append", "cls_med_angles.append", "cls_angles_3.append", "cls_angles_6.append", "cls_angles_11.append", "cls_angles_22.append", "cls_angles_30.append", "np.maximum.append", "len", "numpy.hstack", "numpy.mean", "numpy.median", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "string_replace.split", "gtname.replace.replace", "PIL.Image.open().convert", "PIL.Image.open().convert", "PIL.Image.open().convert", "pred[].astype", "gt[].astype", "m.any", "numpy.arccos", "gtname.replace.replace", "numpy.linalg.norm", "numpy.linalg.norm", "angles[].append", "PIL.Image.open", "PIL.Image.open", "PIL.Image.open"], "function", ["None"], ["", "", "def", "benchmark_normal", "(", "pred_dir", ",", "gt_dir", ",", "inst_dir", ",", "\n", "instance_labels", ",", "string_replace", ")", ":", "\n", "  ", "\"\"\"Benchmark surface normal estimations.\n  \"\"\"", "\n", "print", "(", "'Benchmarking instance-level surface normal estimations.'", ")", "\n", "assert", "(", "os", ".", "path", ".", "isdir", "(", "pred_dir", ")", ")", "\n", "assert", "(", "os", ".", "path", ".", "isdir", "(", "gt_dir", ")", ")", "\n", "assert", "(", "os", ".", "path", ".", "isdir", "(", "inst_dir", ")", ")", "\n", "num_instance", "=", "len", "(", "instance_labels", ")", "\n", "angles", "=", "[", "[", "]", "for", "_", "in", "range", "(", "num_instance", ")", "]", "\n", "\n", "for", "dirpath", ",", "dirnames", ",", "filenames", "in", "os", ".", "walk", "(", "pred_dir", ")", ":", "\n", "    ", "for", "filename", "in", "filenames", ":", "\n", "      ", "predname", "=", "os", ".", "path", ".", "join", "(", "dirpath", ",", "filename", ")", "\n", "gtname", "=", "predname", ".", "replace", "(", "pred_dir", ",", "gt_dir", ")", "\n", "instname", "=", "(", "gtname", ".", "replace", "(", "'normal'", ",", "'semantic'", ")", "\n", ".", "replace", "(", "'semantics'", ",", "'semantic'", ")", ")", "\n", "if", "string_replace", "!=", "''", ":", "\n", "        ", "stra", ",", "strb", "=", "string_replace", ".", "split", "(", "','", ")", "\n", "gtname", "=", "gtname", ".", "replace", "(", "stra", ",", "strb", ")", "\n", "\n", "", "pred", "=", "np", ".", "asarray", "(", "\n", "Image", ".", "open", "(", "predname", ")", ".", "convert", "(", "mode", "=", "'RGB'", ")", ",", "\n", "dtype", "=", "np", ".", "uint8", ")", "\n", "gt", "=", "np", ".", "asarray", "(", "\n", "Image", ".", "open", "(", "gtname", ")", ".", "convert", "(", "mode", "=", "'RGB'", ")", ",", "\n", "dtype", "=", "np", ".", "uint8", ")", "\n", "inst", "=", "np", ".", "asarray", "(", "\n", "Image", ".", "open", "(", "instname", ")", ".", "convert", "(", "mode", "=", "'RGB'", ")", ",", "\n", "dtype", "=", "np", ".", "uint8", ")", "\n", "inst", "=", "inst", "[", ":", ",", ":", ",", "0", "]", "*", "256", "**", "2", "+", "inst", "[", ":", ",", ":", ",", "1", "]", "*", "256", "+", "inst", "[", ":", ",", ":", ",", "2", "]", "\n", "\n", "pred", "=", "np", ".", "reshape", "(", "pred", ",", "(", "-", "1", ",", "3", ")", ")", "\n", "gt", "=", "np", ".", "reshape", "(", "gt", ",", "(", "-", "1", ",", "3", ")", ")", "\n", "inst", "=", "np", ".", "reshape", "(", "inst", ",", "(", "-", "1", ",", ")", ")", "\n", "mask", "=", "np", ".", "any", "(", "gt", "!=", "128", ",", "axis", "=", "-", "1", ")", "\n", "\n", "pred", "=", "pred", "[", "mask", ",", ":", "]", ".", "astype", "(", "np", ".", "float32", ")", "-", "127.5", "\n", "gt", "=", "gt", "[", "mask", ",", ":", "]", ".", "astype", "(", "np", ".", "float32", ")", "-", "127.5", "\n", "inst", "=", "inst", "[", "mask", "]", "\n", "\n", "pred", "=", "pred", "/", "(", "np", ".", "linalg", ".", "norm", "(", "pred", ",", "axis", "=", "-", "1", ",", "keepdims", "=", "True", ")", "+", "1e-12", ")", "\n", "gt", "=", "gt", "/", "(", "np", ".", "linalg", ".", "norm", "(", "gt", ",", "axis", "=", "-", "1", ",", "keepdims", "=", "True", ")", "+", "1e-12", ")", "\n", "\n", "cos", "=", "np", ".", "sum", "(", "pred", "*", "gt", ",", "axis", "=", "-", "1", ")", "\n", "abs_cos", "=", "np", ".", "abs", "(", "cos", ")", "\n", "assert", "(", "not", "(", "abs_cos", "-", "1", ">", "1e-5", ")", ".", "any", "(", ")", ")", "\n", "cos", "=", "np", ".", "clip", "(", "cos", ",", "-", "1", ",", "1", ")", "\n", "\n", "for", "inst_ind", "in", "np", ".", "unique", "(", "inst", ")", ":", "\n", "        ", "if", "inst_ind", "==", "855309", ":", "\n", "          ", "continue", "\n", "", "m", "=", "inst", "==", "inst_ind", "\n", "if", "m", ".", "any", "(", ")", ":", "\n", "          ", "angles", "[", "inst_ind", "]", ".", "append", "(", "cos", "[", "m", "]", ")", "\n", "\n", "# semantic level metrics.", "\n", "", "", "", "", "cls_names", "=", "{", "}", "\n", "cls_mean_angles", "=", "[", "]", "\n", "cls_med_angles", "=", "[", "]", "\n", "cls_angles_3", "=", "[", "]", "\n", "cls_angles_6", "=", "[", "]", "\n", "cls_angles_11", "=", "[", "]", "\n", "cls_angles_22", "=", "[", "]", "\n", "cls_angles_30", "=", "[", "]", "\n", "cls_num_insts", "=", "[", "]", "\n", "for", "inst_ind", ",", "inst_name", "in", "enumerate", "(", "instance_labels", ")", ":", "\n", "    ", "cls_name", "=", "inst_name", ".", "split", "(", "'_'", ")", "[", "0", "]", "\n", "if", "cls_names", ".", "get", "(", "cls_name", ",", "None", ")", "is", "None", ":", "\n", "      ", "cls_names", "[", "cls_name", "]", "=", "len", "(", "cls_names", ")", "\n", "cls_mean_angles", ".", "append", "(", "0.0", ")", "\n", "cls_med_angles", ".", "append", "(", "0.0", ")", "\n", "cls_angles_3", ".", "append", "(", "0.0", ")", "\n", "cls_angles_6", ".", "append", "(", "0.0", ")", "\n", "cls_angles_11", ".", "append", "(", "0.0", ")", "\n", "cls_angles_22", ".", "append", "(", "0.0", ")", "\n", "cls_angles_30", ".", "append", "(", "0.0", ")", "\n", "cls_num_insts", ".", "append", "(", "0", ")", "\n", "\n", "", "inst_angs", "=", "angles", "[", "inst_ind", "]", "\n", "if", "len", "(", "inst_angs", ")", ">", "0", ":", "\n", "      ", "inst_angs", "=", "np", ".", "hstack", "(", "inst_angs", ")", "\n", "inst_angs", "=", "np", ".", "arccos", "(", "inst_angs", ")", "*", "(", "180.0", "/", "np", ".", "pi", ")", "\n", "cls_ind", "=", "cls_names", "[", "cls_name", "]", "\n", "\n", "cls_mean_angles", "[", "cls_ind", "]", "+=", "np", ".", "mean", "(", "inst_angs", ")", "\n", "cls_med_angles", "[", "cls_ind", "]", "+=", "np", ".", "median", "(", "inst_angs", ")", "\n", "cls_angles_3", "[", "cls_ind", "]", "+=", "np", ".", "mean", "(", "inst_angs", "<=", "2.8125", ")", "\n", "cls_angles_6", "[", "cls_ind", "]", "+=", "np", ".", "mean", "(", "inst_angs", "<=", "5.625", ")", "\n", "cls_angles_11", "[", "cls_ind", "]", "+=", "np", ".", "mean", "(", "inst_angs", "<=", "11.25", ")", "\n", "cls_angles_22", "[", "cls_ind", "]", "+=", "np", ".", "mean", "(", "inst_angs", "<=", "22.5", ")", "\n", "cls_angles_30", "[", "cls_ind", "]", "+=", "np", ".", "mean", "(", "inst_angs", "<=", "30", ")", "\n", "cls_num_insts", "[", "cls_ind", "]", "+=", "1", "\n", "\n", "", "", "cls_num_insts", "=", "np", ".", "maximum", "(", "np", ".", "array", "(", "cls_num_insts", ")", ",", "1e-12", ")", "\n", "cls_mean_angles", "=", "np", ".", "array", "(", "cls_mean_angles", ")", "/", "cls_num_insts", "\n", "cls_med_angles", "=", "np", ".", "array", "(", "cls_med_angles", ")", "/", "cls_num_insts", "\n", "cls_angles_3", "=", "np", ".", "array", "(", "cls_angles_3", ")", "/", "cls_num_insts", "\n", "cls_angles_6", "=", "np", ".", "array", "(", "cls_angles_6", ")", "/", "cls_num_insts", "\n", "cls_angles_11", "=", "np", ".", "array", "(", "cls_angles_11", ")", "/", "cls_num_insts", "\n", "cls_angles_22", "=", "np", ".", "array", "(", "cls_angles_22", ")", "/", "cls_num_insts", "\n", "cls_angles_30", "=", "np", ".", "array", "(", "cls_angles_30", ")", "/", "cls_num_insts", "\n", "\n", "for", "cls_name", ",", "cls_ind", "in", "cls_names", ".", "items", "(", ")", ":", "\n", "    ", "print", "(", "'class {:s}, Angle Mean: {:.4f}'", ".", "format", "(", "\n", "cls_name", ",", "cls_mean_angles", "[", "cls_ind", "]", ")", ")", "\n", "print", "(", "'class {:s}, Angle Median: {:.4f}'", ".", "format", "(", "\n", "cls_name", ",", "cls_med_angles", "[", "cls_ind", "]", ")", ")", "\n", "print", "(", "'class {:s}, Angle within 2.8125: {:.4f}'", ".", "format", "(", "\n", "cls_name", ",", "cls_angles_3", "[", "cls_ind", "]", "*", "100.0", ")", ")", "\n", "print", "(", "'class {:s}, Angle within 5.625: {:.4f}'", ".", "format", "(", "\n", "cls_name", ",", "cls_angles_6", "[", "cls_ind", "]", "*", "100.0", ")", ")", "\n", "print", "(", "'class {:s}, Angle within 11.25: {:.4f}'", ".", "format", "(", "\n", "cls_name", ",", "cls_angles_11", "[", "cls_ind", "]", "*", "100.0", ")", ")", "\n", "print", "(", "'class {:s}, Angle within 22.5: {:.4f}'", ".", "format", "(", "\n", "cls_name", ",", "cls_angles_22", "[", "cls_ind", "]", "*", "100.0", ")", ")", "\n", "print", "(", "'class {:s}, Angle within 30: {:.4f}'", ".", "format", "(", "\n", "cls_name", ",", "cls_angles_30", "[", "cls_ind", "]", "*", "100.0", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.stanford_scenes.image_reader.ImageReader.__init__": [[242, 289], ["image_reader.read_labeled_image_list", "tensorflow.train.slice_input_producer", "image_reader.read_images_from_disk", "tensorflow.convert_to_tensor"], "methods", ["home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.stanford_scenes.image_reader.read_labeled_image_list", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.stanford_scenes.image_reader.read_images_from_disk"], ["def", "__init__", "(", "self", ",", "\n", "data_dir", ",", "\n", "data_list", ",", "\n", "input_size", ",", "\n", "random_scale", ",", "\n", "random_mirror", ",", "\n", "random_crop", ",", "\n", "ignore_label", ",", "\n", "img_mean", ")", ":", "\n", "    ", "\"\"\"\n    Initialise an ImageReader.\n          \n    Args:\n      data_dir: path to the directory with images, segmentations, depths,\n        surface normals and validity maps.\n      data_list: path to the file with lines of the form\n                 '/path/to/image /path/to/mask'.\n      input_size: a tuple with (height, width) values, to which all the\n                  images will be resized.\n      random_scale: whether to randomly scale the images.\n      random_mirror: whether to randomly mirror the images.\n      ignore_label: index of label to ignore during the training.\n      img_mean: vector of mean colour values.\n\n    Returns:\n      A tensor of size [batch_size, height_out, width_out, channels], and\n      another tensor of size [batch_size, height_out, width_out]\n    \"\"\"", "\n", "self", ".", "data_dir", "=", "data_dir", "\n", "self", ".", "data_list", "=", "data_list", "\n", "self", ".", "input_size", "=", "input_size", "\n", "\n", "self", ".", "file_lists", "=", "read_labeled_image_list", "(", "\n", "self", ".", "data_dir", ",", "self", ".", "data_list", ")", "\n", "file_lists", "=", "[", "tf", ".", "convert_to_tensor", "(", "f", ",", "dtype", "=", "tf", ".", "string", ")", "\n", "for", "f", "in", "self", ".", "file_lists", "]", "\n", "self", ".", "queue", "=", "tf", ".", "train", ".", "slice_input_producer", "(", "\n", "file_lists", ",", "\n", "shuffle", "=", "input_size", "is", "not", "None", ")", "# not shuffling if it is val", "\n", "self", ".", "datas", "=", "read_images_from_disk", "(", "\n", "self", ".", "queue", ",", "\n", "self", ".", "input_size", ",", "\n", "random_scale", ",", "\n", "random_mirror", ",", "\n", "random_crop", ",", "\n", "ignore_label", ",", "\n", "img_mean", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.stanford_scenes.image_reader.ImageReader.dequeue": [[291, 307], ["tensorflow.train.batch"], "methods", ["None"], ["", "def", "dequeue", "(", "self", ",", "num_elements", ")", ":", "\n", "    ", "\"\"\"Packs images and labels into a batch.\n        \n    Args:\n      num_elements: A number indicating the batch size.\n          \n    Returns:\n      Two tensors of size [batch_size, height_out, width_out, 3], and\n      three tensors of size [batch_size, height_out, width_out, 1]\n    \"\"\"", "\n", "# images, segmentations, depths, normals.", "\n", "datas_batch", "=", "tf", ".", "train", ".", "batch", "(", "\n", "self", ".", "datas", ",", "\n", "num_elements", ",", "\n", "num_threads", "=", "2", ")", "\n", "return", "datas_batch", "\n", "", "", ""]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.stanford_scenes.image_reader.image_scaling": [[9, 46], ["tensorflow.random_uniform", "tensorflow.to_int32", "tensorflow.to_int32", "tensorflow.squeeze", "tensorflow.image.resize_images", "tensorflow.image.resize_images", "tensorflow.image.resize_images", "tensorflow.image.resize_images", "tensorflow.stack", "tensorflow.to_float", "tensorflow.to_float", "tensorflow.shape", "tensorflow.shape"], "function", ["None"], ["def", "image_scaling", "(", "img", ",", "label", ",", "depth", ",", "normal", ")", ":", "\n", "  ", "\"\"\"Randomly scales the images between 0.5 to 1.5 times the original size.\n\n  Args:\n    img: A tensor of size [height_in, width_in, 3]\n    label: A tensor of size [height_in, width_in, 1]\n    depth: A tensor of size [height_in, width_in, 1]\n    normal: A tensor of size [height_in, width_in, 3]\n\n  Returns:\n    Two tensors of size [height_out, width_out, 3], and another\n    three tensors of size [height_out, width_out, 1]\n  \"\"\"", "\n", "scale", "=", "tf", ".", "random_uniform", "(", "\n", "[", "1", "]", ",", "minval", "=", "0.5", ",", "maxval", "=", "2.0", ",", "dtype", "=", "tf", ".", "float32", ",", "seed", "=", "None", ")", "\n", "h_new", "=", "tf", ".", "to_int32", "(", "tf", ".", "to_float", "(", "tf", ".", "shape", "(", "img", ")", "[", "0", "]", ")", "*", "scale", ")", "\n", "w_new", "=", "tf", ".", "to_int32", "(", "tf", ".", "to_float", "(", "tf", ".", "shape", "(", "img", ")", "[", "1", "]", ")", "*", "scale", ")", "\n", "new_shape", "=", "tf", ".", "squeeze", "(", "tf", ".", "stack", "(", "[", "h_new", ",", "w_new", "]", ")", ",", "squeeze_dims", "=", "[", "1", "]", ")", "\n", "# Rescale images by bilinear sampling.", "\n", "img", "=", "tf", ".", "image", ".", "resize_images", "(", "img", ",", "\n", "new_shape", ",", "\n", "tf", ".", "image", ".", "ResizeMethod", ".", "BILINEAR", ")", "\n", "# Rescale segmentations by nearest neighbor sampling.", "\n", "label", "=", "tf", ".", "image", ".", "resize_images", "(", "label", ",", "\n", "new_shape", ",", "\n", "tf", ".", "image", ".", "ResizeMethod", ".", "NEAREST_NEIGHBOR", ")", "\n", "# Rescale depths by bilinear sampling.", "\n", "depth", "=", "tf", ".", "image", ".", "resize_images", "(", "depth", ",", "\n", "new_shape", ",", "\n", "tf", ".", "image", ".", "ResizeMethod", ".", "NEAREST_NEIGHBOR", ")", "\n", "depth", "/=", "scale", "\n", "# Rescale normals by nearest neighbor.", "\n", "normal", "=", "tf", ".", "image", ".", "resize_images", "(", "normal", ",", "\n", "new_shape", ",", "\n", "tf", ".", "image", ".", "ResizeMethod", ".", "NEAREST_NEIGHBOR", ")", "\n", "\n", "return", "img", ",", "label", ",", "depth", ",", "normal", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.stanford_scenes.image_reader.image_mirroring": [[48, 80], ["tensorflow.random_uniform", "tensorflow.less", "tensorflow.boolean_mask", "tensorflow.reverse", "tensorflow.reverse", "tensorflow.reverse", "tensorflow.reverse", "tensorflow.less", "tensorflow.to_int32", "tensorflow.gather", "tensorflow.cast", "tensorflow.stack", "tensorflow.stack", "tensorflow.stack"], "function", ["None"], ["", "def", "image_mirroring", "(", "img", ",", "label", ",", "depth", ",", "normal", ")", ":", "\n", "  ", "\"\"\"Randomly horizontally mirrors the images and their labels.\n\n  Args:\n    img: A tensor of size [height_in, width_in, 3]\n    label: A tensor of size [height_in, width_in, 1]\n    depth: A tensor of size [height_in, width_in, 1]\n    normal: A tensor of size [height_in, width_in, 3]\n\n  Returns:\n    Two tensor of size [height_in, width_in, 3], and another\n    three tensors of size [height_in, width_in, 1]\n  \"\"\"", "\n", "distort_left_right_random", "=", "tf", ".", "random_uniform", "(", "\n", "[", "1", "]", ",", "0", ",", "1.0", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "distort_left_right_random", "=", "distort_left_right_random", "[", "0", "]", "\n", "\n", "mirror", "=", "tf", ".", "less", "(", "tf", ".", "stack", "(", "[", "1.0", ",", "distort_left_right_random", ",", "1.0", "]", ")", ",", "0.5", ")", "\n", "mirror", "=", "tf", ".", "boolean_mask", "(", "[", "0", ",", "1", ",", "2", "]", ",", "mirror", ")", "\n", "img", "=", "tf", ".", "reverse", "(", "img", ",", "mirror", ")", "\n", "label", "=", "tf", ".", "reverse", "(", "label", ",", "mirror", ")", "\n", "depth", "=", "tf", ".", "reverse", "(", "depth", ",", "mirror", ")", "\n", "normal", "=", "tf", ".", "reverse", "(", "normal", ",", "mirror", ")", "\n", "# Horizontal flipping the x-direction vector value.", "\n", "normal_mirror", "=", "tf", ".", "less", "(", "\n", "tf", ".", "stack", "(", "[", "distort_left_right_random", ",", "1.0", ",", "1.0", "]", ")", ",", "\n", "0.5", ")", "\n", "normal_mirror", "=", "tf", ".", "to_int32", "(", "normal_mirror", ")", "\n", "normal_mirror", "=", "tf", ".", "gather", "(", "tf", ".", "stack", "(", "[", "1.0", ",", "-", "1.0", "]", ")", ",", "normal_mirror", ")", "\n", "normal", "*=", "tf", ".", "cast", "(", "normal_mirror", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "\n", "return", "img", ",", "label", ",", "depth", ",", "normal", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.stanford_scenes.image_reader.crop_and_pad_image_and_labels": [[82, 138], ["tensorflow.cast", "tensorflow.concat", "tensorflow.shape", "tensorflow.image.pad_to_bounding_box", "tensorflow.cast", "img.set_shape", "tf.cast.set_shape", "depth.set_shape", "normal.set_shape", "tensorflow.maximum", "tensorflow.maximum", "tensorflow.random_crop", "tensorflow.image.resize_image_with_crop_or_pad", "tf.image.resize_image_with_crop_or_pad.get_shape().as_list", "tf.image.resize_image_with_crop_or_pad.get_shape"], "function", ["None"], ["", "def", "crop_and_pad_image_and_labels", "(", "img", ",", "\n", "label", ",", "\n", "depth", ",", "\n", "normal", ",", "\n", "crop_h", ",", "\n", "crop_w", ",", "\n", "ignore_label", "=", "255", ",", "\n", "random_crop", "=", "True", ")", ":", "\n", "  ", "\"\"\"Randomly crops and pads the images and their labels.\n\n  Args:\n    img: A tensor of size [batch_size, height_in, width_in, channels]\n    label: A tensor of size [batch_size, height_in, width_in]\n    crop_h: A number indicating the height of output data.\n    crop_w: A number indicating the width of output data.\n    ignore_label: A number indicating the indices of ignored label.\n    random_crop: enable/disable random_crop for random cropping.\n\n  Returns:\n    A tensor of size [batch_size, height_out, width_out, channels], and another\n    tensor of size [batch_size, height_out, width_out, 1]\n  \"\"\"", "\n", "# Needs to be subtracted and later added due to 0 padding.", "\n", "label", "=", "tf", ".", "cast", "(", "label", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "label", "=", "label", "-", "ignore_label", "\n", "\n", "# Concatenate images with labels, which makes random cropping easier.", "\n", "combined", "=", "tf", ".", "concat", "(", "values", "=", "[", "img", ",", "label", ",", "depth", ",", "normal", "]", ",", "axis", "=", "-", "1", ")", "\n", "combined_shape", "=", "tf", ".", "shape", "(", "combined", ")", "\n", "combined", "=", "tf", ".", "image", ".", "pad_to_bounding_box", "(", "\n", "combined", ",", "\n", "0", ",", "\n", "0", ",", "\n", "tf", ".", "maximum", "(", "crop_h", ",", "combined_shape", "[", "0", "]", ")", ",", "\n", "tf", ".", "maximum", "(", "crop_w", ",", "combined_shape", "[", "1", "]", ")", ")", "\n", "\n", "if", "random_crop", ":", "\n", "    ", "channels", "=", "combined", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "-", "1", "]", "\n", "combined", "=", "tf", ".", "random_crop", "(", "combined", ",", "[", "crop_h", ",", "crop_w", ",", "channels", "]", ")", "\n", "", "else", ":", "\n", "    ", "combined", "=", "tf", ".", "image", ".", "resize_image_with_crop_or_pad", "(", "combined", ",", "crop_h", ",", "crop_w", ")", "\n", "\n", "", "img", "=", "combined", "[", ":", ",", ":", ",", ":", "3", "]", "\n", "label", "=", "combined", "[", ":", ",", ":", ",", "3", ":", "4", "]", "\n", "label", "=", "label", "+", "ignore_label", "\n", "label", "=", "tf", ".", "cast", "(", "label", ",", "dtype", "=", "tf", ".", "uint8", ")", "\n", "depth", "=", "combined", "[", ":", ",", ":", ",", "4", ":", "5", "]", "\n", "normal", "=", "combined", "[", ":", ",", ":", ",", "5", ":", "8", "]", "\n", "\n", "# Set static shape so that tensorflow knows shape at running. ", "\n", "img", ".", "set_shape", "(", "(", "crop_h", ",", "crop_w", ",", "3", ")", ")", "\n", "label", ".", "set_shape", "(", "(", "crop_h", ",", "crop_w", ",", "1", ")", ")", "\n", "depth", ".", "set_shape", "(", "(", "crop_h", ",", "crop_w", ",", "1", ")", ")", "\n", "normal", ".", "set_shape", "(", "(", "crop_h", ",", "crop_w", ",", "3", ")", ")", "\n", "\n", "return", "img", ",", "label", ",", "depth", ",", "normal", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.stanford_scenes.image_reader.read_labeled_image_list": [[140, 174], ["open", "line.strip.strip", "images.append", "segmentations.append", "depths.append", "os.path.join", "os.path.join", "os.path.join", "normals.append", "normals.append", "line.strip.format", "line.strip.format", "line.strip.format", "os.path.join", "os.path.join", "line.strip.format", "line.strip.format"], "function", ["None"], ["", "def", "read_labeled_image_list", "(", "data_dir", ",", "data_list", ")", ":", "\n", "  ", "\"\"\"Reads txt file containing paths to images and ground truth masks.\n    \n  Args:\n    data_dir: A string indicating the path to the root directory of images\n      and masks.\n    data_list: A string indicating the path to the file with lines of the form\n      '/path/to/image /path/to/label'.\n       \n  Returns:\n    Five lists with all file names for images, segmentations, depths, normals\n    and validity masks, respectively.\n  \"\"\"", "\n", "f", "=", "open", "(", "data_list", ",", "'r'", ")", "\n", "images", "=", "[", "]", "\n", "segmentations", "=", "[", "]", "\n", "depths", "=", "[", "]", "\n", "normals", "=", "[", "]", "\n", "for", "line", "in", "f", ":", "\n", "    ", "line", "=", "line", ".", "strip", "(", "\"\\n\"", ")", "\n", "images", ".", "append", "(", "\n", "os", ".", "path", ".", "join", "(", "data_dir", ",", "line", ".", "format", "(", "'rgb'", ",", "'rgb'", ")", ")", ")", "\n", "segmentations", ".", "append", "(", "\n", "#os.path.join(data_dir, line.format('segcls', 'semantic')))", "\n", "os", ".", "path", ".", "join", "(", "data_dir", ",", "line", ".", "format", "(", "'depth'", ",", "'depth'", ")", ")", ")", "\n", "depths", ".", "append", "(", "\n", "os", ".", "path", ".", "join", "(", "data_dir", ",", "line", ".", "format", "(", "'depth'", ",", "'depth'", ")", ")", ")", "\n", "if", "'area_6'", "in", "line", ":", "\n", "      ", "normals", ".", "append", "(", "\n", "os", ".", "path", ".", "join", "(", "data_dir", ",", "line", ".", "format", "(", "'normal'", ",", "'normal'", ")", ")", ")", "\n", "", "else", ":", "\n", "      ", "normals", ".", "append", "(", "\n", "os", ".", "path", ".", "join", "(", "data_dir", ",", "line", ".", "format", "(", "'normal'", ",", "'normals'", ")", ")", ")", "\n", "", "", "return", "images", ",", "segmentations", ",", "depths", ",", "normals", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.stanford_scenes.image_reader.read_images_from_disk": [[176, 233], ["tensorflow.image.decode_jpeg", "tensorflow.cast", "tensorflow.image.decode_png", "tensorflow.image.decode_png", "tensorflow.image.decode_png", "tensorflow.cast", "tensorflow.read_file", "tensorflow.cast", "tensorflow.cast", "image_reader.crop_and_pad_image_and_labels", "image_reader.image_scaling", "image_reader.image_mirroring"], "function", ["home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.stanford_scenes.image_reader.crop_and_pad_image_and_labels", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.stanford_scenes.image_reader.image_scaling", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.stanford_scenes.image_reader.image_mirroring"], ["", "def", "read_images_from_disk", "(", "input_queue", ",", "\n", "input_size", ",", "\n", "random_scale", ",", "\n", "random_mirror", ",", "\n", "random_crop", ",", "\n", "ignore_label", ",", "\n", "img_mean", ")", ":", "\n", "  ", "\"\"\"Reads one image and its corresponding label and perform pre-processing.\n    \n  Args:\n    input_queue: A tensorflow queue with paths to the image and its\n      segmentation, depth, surface normal.\n    input_size: A tuple with entries of height and width. If None, return\n      images of original size.\n    random_scale: enable/disable random_scale for randomly scaling images\n      and their labels.\n    random_mirror: enable/disable random_mirror for randomly and horizontally\n      flipping images and their labels.\n    ignore_label: A number indicating the index of label to ignore.\n    img_mean: A vector indicating the mean colour values of RGB channels.\n      \n  Returns:\n    Five tensors: the decoded image and its segmentation, depth, surface normal.\n  \"\"\"", "\n", "\n", "contents", "=", "[", "tf", ".", "read_file", "(", "c", ")", "for", "c", "in", "input_queue", "]", "\n", "\n", "img", "=", "tf", ".", "image", ".", "decode_jpeg", "(", "contents", "[", "0", "]", ",", "channels", "=", "3", ")", "\n", "img", "=", "tf", ".", "cast", "(", "img", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "# Extract mean.", "\n", "img", "-=", "img_mean", "\n", "\n", "label", "=", "tf", ".", "image", ".", "decode_png", "(", "contents", "[", "1", "]", ",", "channels", "=", "1", ")", "\n", "depth", "=", "tf", ".", "image", ".", "decode_png", "(", "contents", "[", "2", "]", ",", "channels", "=", "1", ",", "dtype", "=", "tf", ".", "uint16", ")", "\n", "depth", "=", "tf", ".", "cast", "(", "depth", ",", "dtype", "=", "tf", ".", "int16", ")", "+", "1", "# make invalid depth value to 0", "\n", "depth", "=", "tf", ".", "cast", "(", "depth", ",", "dtype", "=", "tf", ".", "float32", ")", "-", "1", "# make invalid depth value to -1", "\n", "normal", "=", "tf", ".", "image", ".", "decode_png", "(", "contents", "[", "3", "]", ",", "channels", "=", "3", ")", "\n", "normal", "=", "tf", ".", "cast", "(", "normal", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "normal", "-=", "127.5", "\n", "\n", "if", "input_size", "is", "not", "None", ":", "\n", "    ", "h", ",", "w", "=", "input_size", "\n", "\n", "# Randomly scale the images and labels.", "\n", "if", "random_scale", ":", "\n", "      ", "img", ",", "label", ",", "depth", ",", "normal", "=", "image_scaling", "(", "img", ",", "label", ",", "depth", ",", "normal", ")", "\n", "\n", "# Randomly mirror the images and labels.", "\n", "", "if", "random_mirror", ":", "\n", "      ", "img", ",", "label", ",", "depth", ",", "normal", "=", "image_mirroring", "(", "img", ",", "label", ",", "depth", ",", "normal", ")", "\n", "\n", "# Randomly crops the images and labels.", "\n", "", "img", ",", "label", ",", "depth", ",", "normal", "=", "crop_and_pad_image_and_labels", "(", "\n", "img", ",", "label", ",", "depth", ",", "normal", ",", "h", ",", "w", ",", "ignore_label", ",", "random_crop", ")", "\n", "\n", "\n", "", "return", "img", ",", "label", ",", "depth", ",", "normal", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.discriminator._discriminator_builder": [[8, 60], ["ValueError", "tensorflow.variable_scope", "range", "network.conv", "print", "len", "len", "len", "len", "len", "range", "name_format.format", "network.conv"], "function", ["home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.fcrn.conv", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.fcrn.conv"], ["def", "_discriminator_builder", "(", "x", ",", "\n", "name", ",", "\n", "num_classes", "=", "40", ",", "\n", "filters", "=", "[", "64", ",", "128", ",", "256", ",", "512", ",", "512", "]", ",", "\n", "num_blocks", "=", "[", "2", ",", "2", ",", "3", ",", "3", ",", "3", "]", ",", "\n", "strides", "=", "[", "2", ",", "2", ",", "2", ",", "2", ",", "1", "]", ",", "\n", "is_training", "=", "True", ",", "\n", "use_global_status", "=", "True", ",", "\n", "reuse", "=", "False", ")", ":", "\n", "  ", "\"\"\"Helper function to construct Discriminator.\n  \"\"\"", "\n", "if", "len", "(", "filters", ")", "!=", "len", "(", "num_blocks", ")", "or", "len", "(", "filters", ")", "!=", "len", "(", "strides", ")", ":", "\n", "    ", "raise", "ValueError", "(", "'length of lists are not consistent'", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "name", ",", "reuse", "=", "reuse", ")", "as", "scope", ":", "\n", "# blocks", "\n", "    ", "for", "ib", "in", "range", "(", "len", "(", "filters", ")", ")", ":", "\n", "      ", "for", "iu", "in", "range", "(", "num_blocks", "[", "ib", "]", ")", ":", "\n", "        ", "name_format", "=", "'layer{:d}/conv{:d}_{:d}'", "\n", "block_name", "=", "name_format", ".", "format", "(", "ib", "+", "1", ",", "ib", "+", "1", ",", "iu", "+", "1", ")", "\n", "\n", "c_o", "=", "filters", "[", "ib", "]", "# output channel", "\n", "# strides at the end", "\n", "s", "=", "strides", "[", "ib", "]", "if", "strides", "[", "ib", "]", "else", "1", "\n", "pad", "=", "'VALID'", "if", "s", ">", "1", "else", "'SAME'", "\n", "x", "=", "nn", ".", "conv", "(", "x", ",", "\n", "name", "=", "block_name", ",", "\n", "filters", "=", "c_o", ",", "\n", "kernel_size", "=", "4", ",", "\n", "strides", "=", "s", ",", "\n", "padding", "=", "pad", ",", "\n", "biased", "=", "False", ",", "\n", "bn", "=", "True", ",", "\n", "relu", "=", "True", ",", "\n", "decay", "=", "0.99", ",", "\n", "is_training", "=", "is_training", ",", "\n", "use_global_status", "=", "use_global_status", ")", "\n", "\n", "", "", "x", "=", "nn", ".", "conv", "(", "x", ",", "\n", "name", "=", "'block5/fc1_out'", ",", "\n", "filters", "=", "num_classes", ",", "\n", "kernel_size", "=", "4", ",", "\n", "strides", "=", "1", ",", "\n", "padding", "=", "'SAME'", ",", "\n", "biased", "=", "True", ",", "\n", "bn", "=", "False", ",", "\n", "relu", "=", "False", ",", "\n", "is_training", "=", "is_training", ")", "\n", "print", "(", "x", ")", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.discriminator.discriminator": [[62, 76], ["discriminator._discriminator_builder"], "function", ["home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.discriminator._discriminator_builder"], ["", "", "def", "discriminator", "(", "x", ",", "\n", "num_classes", ",", "\n", "is_training", ",", "\n", "use_global_status", ",", "\n", "reuse", "=", "False", ")", ":", "\n", "  ", "return", "_discriminator_builder", "(", "x", ",", "\n", "name", "=", "'Discriminator'", ",", "\n", "num_classes", "=", "num_classes", ",", "\n", "filters", "=", "[", "64", ",", "128", ",", "256", ",", "512", ",", "512", "]", ",", "\n", "num_blocks", "=", "[", "1", ",", "1", ",", "1", ",", "1", ",", "1", "]", ",", "\n", "strides", "=", "[", "2", ",", "2", ",", "2", ",", "2", ",", "1", "]", ",", "\n", "is_training", "=", "is_training", ",", "\n", "use_global_status", "=", "use_global_status", ",", "\n", "reuse", "=", "reuse", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.structured_predictor._unet_builder": [[8, 104], ["ValueError", "tensorflow.variable_scope", "range", "range", "network.conv", "network.conv", "network.conv", "tensorflow.nn.l2_normalize", "len", "len", "len", "len", "len", "range", "print", "shortcuts.append", "range", "print", "name_format.format", "len", "shortcuts[].get_shape().as_list", "name_format.format", "network.conv", "network.atrous_conv", "network.conv", "tensorflow.image.resize_bilinear", "tensorflow.concat", "shortcuts[].get_shape", "len"], "function", ["home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.fcrn.conv", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.fcrn.conv", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.fcrn.conv", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.fcrn.conv", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.common.layers.atrous_conv", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.fcrn.conv"], ["def", "_unet_builder", "(", "x", ",", "\n", "name", ",", "\n", "filters", "=", "[", "64", ",", "128", ",", "256", ",", "512", ",", "1024", "]", ",", "\n", "num_blocks", "=", "[", "2", ",", "3", ",", "3", ",", "3", ",", "3", "]", ",", "\n", "strides", "=", "[", "2", ",", "2", ",", "2", ",", "2", ",", "2", "]", ",", "\n", "dilations", "=", "[", "None", ",", "None", ",", "None", ",", "None", ",", "None", "]", ",", "\n", "num_classes", "=", "40", ",", "\n", "is_training", "=", "True", ",", "\n", "use_global_status", "=", "False", ",", "\n", "reuse", "=", "False", ")", ":", "\n", "  ", "\"\"\"Helper function to construct UNet.\n  \"\"\"", "\n", "if", "len", "(", "filters", ")", "!=", "len", "(", "num_blocks", ")", "or", "len", "(", "filters", ")", "!=", "len", "(", "strides", ")", ":", "\n", "    ", "raise", "ValueError", "(", "'length of lists are not consistent'", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "name", ",", "reuse", "=", "reuse", ")", "as", "scope", ":", "\n", "# Encoder.", "\n", "    ", "shortcuts", "=", "[", "]", "\n", "for", "ib", "in", "range", "(", "len", "(", "filters", ")", ")", ":", "\n", "      ", "for", "iu", "in", "range", "(", "num_blocks", "[", "ib", "]", ")", ":", "\n", "        ", "name_format", "=", "'layer{:d}/unit_{:d}/encoder/'", "\n", "block_name", "=", "name_format", ".", "format", "(", "ib", "+", "1", ",", "iu", "+", "1", ")", "\n", "c_o", "=", "filters", "[", "ib", "]", "# output channel", "\n", "\n", "# strides at the begginning", "\n", "s", "=", "strides", "[", "ib", "]", "if", "iu", "==", "0", "else", "1", "\n", "d", "=", "dilations", "[", "ib", "]", "\n", "if", "d", "is", "not", "None", "and", "d", ">", "1", "and", "s", "==", "1", ":", "\n", "          ", "x", "=", "nn", ".", "atrous_conv", "(", "x", ",", "\n", "name", "=", "block_name", "+", "'/conv'", ",", "\n", "filters", "=", "c_o", ",", "\n", "kernel_size", "=", "3", ",", "\n", "dilation", "=", "d", ",", "\n", "padding", "=", "'SAME'", ",", "\n", "biased", "=", "False", ",", "\n", "bn", "=", "True", ",", "\n", "relu", "=", "True", ",", "\n", "decay", "=", "0.99", ",", "\n", "is_training", "=", "is_training", ",", "\n", "use_global_status", "=", "use_global_status", ")", "\n", "", "else", ":", "\n", "          ", "padding", "=", "'VALID'", "if", "s", ">", "1", "else", "'SAME'", "\n", "ksize", "=", "s", "*", "2", "if", "s", ">", "1", "else", "3", "\n", "x", "=", "nn", ".", "conv", "(", "x", ",", "\n", "name", "=", "block_name", "+", "'/conv'", ",", "\n", "filters", "=", "c_o", ",", "\n", "kernel_size", "=", "ksize", ",", "\n", "strides", "=", "s", ",", "\n", "padding", "=", "padding", ",", "\n", "biased", "=", "False", ",", "\n", "bn", "=", "True", ",", "\n", "relu", "=", "True", ",", "\n", "decay", "=", "0.99", ",", "\n", "is_training", "=", "is_training", ",", "\n", "use_global_status", "=", "use_global_status", ")", "\n", "", "", "print", "(", "x", ")", "\n", "shortcuts", ".", "append", "(", "x", ")", "\n", "\n", "# Decoder.", "\n", "", "for", "ib", "in", "range", "(", "len", "(", "shortcuts", ")", "-", "1", ",", "0", ",", "-", "1", ")", ":", "\n", "#for iu in range(num_blocks[ib-1]):", "\n", "      ", "for", "iu", "in", "range", "(", "3", ")", ":", "\n", "        ", "n", ",", "h", ",", "w", ",", "c_o", "=", "shortcuts", "[", "ib", "-", "1", "]", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "\n", "name_format", "=", "'layer{:d}/unit_{:d}/decoder/'", "\n", "block_name", "=", "name_format", ".", "format", "(", "2", "*", "len", "(", "filters", ")", "-", "ib", ",", "iu", "+", "1", ")", "\n", "x", "=", "nn", ".", "conv", "(", "x", ",", "\n", "name", "=", "block_name", "+", "'conv'", ",", "\n", "filters", "=", "c_o", ",", "\n", "kernel_size", "=", "3", ",", "\n", "strides", "=", "1", ",", "\n", "padding", "=", "'SAME'", ",", "\n", "biased", "=", "False", ",", "\n", "bn", "=", "True", ",", "\n", "relu", "=", "True", ",", "\n", "decay", "=", "0.99", ",", "\n", "is_training", "=", "is_training", ",", "\n", "use_global_status", "=", "use_global_status", ")", "\n", "if", "iu", "==", "0", ":", "\n", "          ", "x", "=", "tf", ".", "image", ".", "resize_bilinear", "(", "x", ",", "[", "h", ",", "w", "]", ")", "\n", "x", "=", "tf", ".", "concat", "(", "[", "x", ",", "shortcuts", "[", "ib", "-", "1", "]", "]", ",", "axis", "=", "-", "1", ")", "\n", "", "", "print", "(", "x", ")", "\n", "\n", "# output segmentation, depth and surface normal estimation.", "\n", "", "block_name", "=", "'block5'", "\n", "seg", "=", "nn", ".", "conv", "(", "x", ",", "block_name", "+", "'/fc1_seg'", ",", "num_classes", ",", "3", ",", "1", ",", "padding", "=", "'SAME'", ",", "\n", "biased", "=", "True", ",", "bn", "=", "False", ",", "relu", "=", "False", ",", "is_training", "=", "is_training", ")", "\n", "\n", "dph", "=", "nn", ".", "conv", "(", "x", ",", "block_name", "+", "'/fc1_depth'", ",", "1", ",", "3", ",", "1", ",", "padding", "=", "'SAME'", ",", "\n", "biased", "=", "True", ",", "bn", "=", "False", ",", "relu", "=", "True", ",", "is_training", "=", "is_training", ")", "\n", "\n", "nrm", "=", "nn", ".", "conv", "(", "x", ",", "block_name", "+", "'/fc1_normal'", ",", "3", ",", "3", ",", "1", ",", "padding", "=", "'SAME'", ",", "\n", "biased", "=", "True", ",", "bn", "=", "False", ",", "relu", "=", "False", ",", "is_training", "=", "is_training", ")", "\n", "nrm", "=", "tf", ".", "nn", ".", "l2_normalize", "(", "nrm", ",", "dim", "=", "-", "1", ")", "\n", "\n", "return", "[", "seg", ",", "dph", ",", "nrm", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.structured_predictor.structured_predictor": [[106, 126], ["structured_predictor._unet_builder"], "function", ["home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.analyzer._unet_builder"], ["", "", "def", "structured_predictor", "(", "x", ",", "\n", "num_classes", "=", "40", ",", "\n", "is_training", "=", "True", ",", "\n", "use_global_status", "=", "False", ",", "\n", "reuse", "=", "False", ")", ":", "\n", "  ", "\"\"\"Build UNet as structured predictor.\n  \"\"\"", "\n", "scores", "=", "_unet_builder", "(", "x", ",", "\n", "name", "=", "'unet_512'", ",", "\n", "filters", "=", "[", "64", ",", "128", ",", "256", ",", "512", ",", "512", ",", "512", ",", "512", ",", "512", ",", "512", "]", ",", "\n", "#num_blocks=[3]*9,", "\n", "num_blocks", "=", "[", "1", "]", "*", "9", ",", "\n", "strides", "=", "[", "2", "]", "*", "9", ",", "\n", "dilations", "=", "[", "None", "]", "*", "9", ",", "\n", "num_classes", "=", "num_classes", ",", "\n", "is_training", "=", "is_training", ",", "\n", "use_global_status", "=", "use_global_status", ",", "\n", "reuse", "=", "reuse", ")", "\n", "\n", "return", "scores", "\n", "", ""]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.fcrn.conv": [[9, 70], ["tf.pad.get_shape().as_list", "tensorflow.nn.conv2d", "tensorflow.variable_scope", "tensorflow.get_variable", "convolve", "tensorflow.pad", "tensorflow.get_variable", "tensorflow.nn.bias_add", "network.batch_norm", "tensorflow.nn.relu", "tf.pad.get_shape"], "function", ["home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.common.layers.batch_norm"], ["def", "conv", "(", "x", ",", "\n", "name", ",", "\n", "filters", ",", "\n", "kernel_sizes", ",", "\n", "strides", ",", "\n", "padding", ",", "\n", "relu", "=", "True", ",", "\n", "biased", "=", "True", ",", "\n", "bn", "=", "True", ",", "\n", "decay", "=", "0.99", ",", "\n", "is_training", "=", "True", ",", "\n", "use_global_status", "=", "True", ")", ":", "\n", "  ", "\"\"\"Convolutional layers with batch normalization and ReLU.\n  \"\"\"", "\n", "c_i", "=", "x", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "-", "1", "]", "# input channels", "\n", "c_o", "=", "filters", "# output channels", "\n", "\n", "# Define helper function.", "\n", "convolve", "=", "lambda", "i", ",", "k", ":", "tf", ".", "nn", ".", "conv2d", "(", "\n", "i", ",", "\n", "k", ",", "\n", "[", "1", ",", "strides", ",", "strides", ",", "1", "]", ",", "\n", "padding", "=", "padding", ")", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "name", ")", "as", "scope", ":", "\n", "    ", "kernel", "=", "tf", ".", "get_variable", "(", "\n", "name", "=", "'weights'", ",", "\n", "shape", "=", "[", "kernel_sizes", "[", "0", "]", ",", "kernel_sizes", "[", "1", "]", ",", "c_i", ",", "c_o", "]", ",", "\n", "trainable", "=", "is_training", ")", "\n", "\n", "if", "strides", ">", "1", ":", "\n", "      ", "pad", "=", "kernel_size", "-", "1", "\n", "pad_beg", "=", "pad", "//", "2", "\n", "pad_end", "=", "pad", "-", "pad_beg", "\n", "pad_h", "=", "[", "pad_beg", ",", "pad_end", "]", "\n", "pad_w", "=", "[", "pad_beg", ",", "pad_end", "]", "\n", "x", "=", "tf", ".", "pad", "(", "x", ",", "[", "[", "0", ",", "0", "]", ",", "pad_h", ",", "pad_w", ",", "[", "0", ",", "0", "]", "]", ")", "\n", "\n", "", "output", "=", "convolve", "(", "x", ",", "kernel", ")", "\n", "\n", "# Add the biases.", "\n", "if", "biased", ":", "\n", "      ", "biases", "=", "tf", ".", "get_variable", "(", "'biases'", ",", "\n", "[", "c_o", "]", ",", "\n", "trainable", "=", "is_training", ")", "\n", "output", "=", "tf", ".", "nn", ".", "bias_add", "(", "output", ",", "biases", ")", "\n", "\n", "# Apply batch normalization.", "\n", "", "if", "bn", ":", "\n", "      ", "is_bn_training", "=", "not", "use_global_status", "\n", "output", "=", "nn", ".", "batch_norm", "(", "output", ",", "\n", "'BatchNorm'", ",", "\n", "is_training", "=", "is_bn_training", ",", "\n", "decay", "=", "decay", ",", "\n", "activation_fn", "=", "None", ")", "\n", "\n", "# Apply ReLU as activation function.", "\n", "", "if", "relu", ":", "\n", "      ", "output", "=", "tf", ".", "nn", ".", "relu", "(", "output", ")", "\n", "\n", "", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.fcrn.interleave": [[72, 77], ["len", "tensorflow.reshape", "tensors[].get_shape().as_list", "tensorflow.stack", "tensors[].get_shape"], "function", ["None"], ["", "def", "interleave", "(", "tensors", ",", "axis", ")", ":", "\n", "  ", "old_shape", "=", "tensors", "[", "0", "]", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "1", ":", "]", "\n", "new_shape", "=", "[", "-", "1", "]", "+", "old_shape", "\n", "new_shape", "[", "axis", "]", "*=", "len", "(", "tensors", ")", "\n", "return", "tf", ".", "reshape", "(", "tf", ".", "stack", "(", "tensors", ",", "axis", "+", "1", ")", ",", "new_shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.fcrn.unpool_as_conv": [[79, 120], ["tensorflow.variable_scope", "tensorflow.pad", "fcrn.conv", "tensorflow.pad", "fcrn.conv", "tensorflow.pad", "fcrn.conv", "tensorflow.pad", "fcrn.conv", "fcrn.interleave", "fcrn.interleave", "fcrn.interleave", "network.batch_norm"], "function", ["home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.fcrn.conv", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.fcrn.conv", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.fcrn.conv", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.fcrn.conv", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.fcrn.interleave", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.fcrn.interleave", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.fcrn.interleave", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.common.layers.batch_norm"], ["", "def", "unpool_as_conv", "(", "x", ",", "name", ",", "c_o", ",", "is_training", "=", "True", ",", "\n", "use_global_status", "=", "True", ")", ":", "\n", "  ", "\"\"\"Model upconvolutions (unpooling + convolution) as\n  interleaving feature maps of four convolutions (A,B,C,D).\n  \n  This function is the Building block for up-projections. \n  \"\"\"", "\n", "with", "tf", ".", "variable_scope", "(", "name", ")", "as", "scope", ":", "\n", "# Convolution A (3x3)", "\n", "    ", "x_a", "=", "tf", ".", "pad", "(", "x", ",", "[", "[", "0", ",", "0", "]", ",", "[", "1", ",", "1", "]", ",", "[", "1", ",", "1", "]", ",", "[", "0", ",", "0", "]", "]", ",", "mode", "=", "'CONSTANT'", ")", "\n", "x_a", "=", "conv", "(", "x_a", ",", "'conv_a'", ",", "c_o", ",", "[", "3", ",", "3", "]", ",", "1", ",", "padding", "=", "'VALID'", ",", "\n", "relu", "=", "False", ",", "biased", "=", "True", ",", "bn", "=", "False", ",", "is_training", "=", "is_training", ")", "\n", "\n", "# Convolution B (2x3)", "\n", "x_b", "=", "tf", ".", "pad", "(", "x", ",", "[", "[", "0", ",", "0", "]", ",", "[", "1", ",", "0", "]", ",", "[", "1", ",", "1", "]", ",", "[", "0", ",", "0", "]", "]", ",", "mode", "=", "'CONSTANT'", ")", "\n", "x_b", "=", "conv", "(", "x_b", ",", "'conv_b'", ",", "c_o", ",", "[", "2", ",", "3", "]", ",", "1", ",", "padding", "=", "'VALID'", ",", "\n", "relu", "=", "False", ",", "biased", "=", "True", ",", "bn", "=", "False", ",", "is_training", "=", "is_training", ")", "\n", "\n", "# Convolution C (3x2)", "\n", "x_c", "=", "tf", ".", "pad", "(", "x", ",", "[", "[", "0", ",", "0", "]", ",", "[", "1", ",", "1", "]", ",", "[", "1", ",", "0", "]", ",", "[", "0", ",", "0", "]", "]", ",", "mode", "=", "'CONSTANT'", ")", "\n", "x_c", "=", "conv", "(", "x_c", ",", "'conv_c'", ",", "c_o", ",", "[", "3", ",", "2", "]", ",", "1", ",", "padding", "=", "'VALID'", ",", "\n", "relu", "=", "False", ",", "biased", "=", "True", ",", "bn", "=", "False", ",", "is_training", "=", "is_training", ")", "\n", "\n", "# Convolution D (2x2)", "\n", "x_d", "=", "tf", ".", "pad", "(", "x", ",", "[", "[", "0", ",", "0", "]", ",", "[", "1", ",", "0", "]", ",", "[", "1", ",", "0", "]", ",", "[", "0", ",", "0", "]", "]", ",", "mode", "=", "'CONSTANT'", ")", "\n", "x_d", "=", "conv", "(", "x_d", ",", "'conv_d'", ",", "c_o", ",", "[", "2", ",", "2", "]", ",", "1", ",", "padding", "=", "'VALID'", ",", "\n", "relu", "=", "False", ",", "biased", "=", "True", ",", "bn", "=", "False", ",", "is_training", "=", "is_training", ")", "\n", "\n", "\n", "# Interleaving elements of the four feature maps.", "\n", "x_l", "=", "interleave", "(", "[", "x_a", ",", "x_b", "]", ",", "axis", "=", "1", ")", "# columns", "\n", "x_r", "=", "interleave", "(", "[", "x_c", ",", "x_d", "]", ",", "axis", "=", "1", ")", "# columns", "\n", "output", "=", "interleave", "(", "[", "x_l", ",", "x_r", "]", ",", "axis", "=", "2", ")", "# rows", "\n", "\n", "is_bn_training", "=", "not", "use_global_status", "\n", "output", "=", "nn", ".", "batch_norm", "(", "output", ",", "'BatchNorm'", ",", "\n", "is_training", "=", "is_bn_training", ",", "\n", "decay", "=", "0.99", ",", "\n", "activation_fn", "=", "None", ")", "\n", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.fcrn.up_project": [[122, 150], ["tensorflow.variable_scope", "fcrn.unpool_as_conv", "tensorflow.nn.relu", "fcrn.conv", "fcrn.unpool_as_conv", "tensorflow.nn.relu"], "function", ["home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.fcrn.unpool_as_conv", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.fcrn.conv", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.fcrn.unpool_as_conv"], ["", "def", "up_project", "(", "x", ",", "name", ",", "c_o", ",", "is_training", "=", "True", ",", "\n", "use_global_status", "=", "False", ")", ":", "\n", "  ", "\"\"\"Up-Porjection depicted in: Deeper Depth Prediction with Fully\n  Convolutional Residual Networks.\n  \"\"\"", "\n", "with", "tf", ".", "variable_scope", "(", "name", ")", ":", "\n", "# Branch1.", "\n", "    ", "x1", "=", "unpool_as_conv", "(", "\n", "x", ",", "'br1/unpool'", ",", "c_o", ",", "\n", "is_training", "=", "is_training", ",", "\n", "use_global_status", "=", "use_global_status", ")", "\n", "x1", "=", "tf", ".", "nn", ".", "relu", "(", "x1", ")", "\n", "\n", "x1", "=", "conv", "(", "x1", ",", "'br1/conv1'", ",", "c_o", ",", "[", "3", ",", "3", "]", ",", "1", ",", "padding", "=", "'SAME'", ",", "\n", "biased", "=", "False", ",", "relu", "=", "False", ",", "bn", "=", "True", ",", "decay", "=", "0.99", ",", "\n", "is_training", "=", "is_training", ",", "use_global_status", "=", "use_global_status", ")", "\n", "\n", "\n", "# Branch 2", "\n", "x2", "=", "unpool_as_conv", "(", "\n", "x", ",", "'br2/unpool'", ",", "c_o", ",", "\n", "is_training", "=", "is_training", ",", "\n", "use_global_status", "=", "use_global_status", ")", "\n", "\n", "# sum branches", "\n", "x", "=", "tf", ".", "nn", ".", "relu", "(", "x1", "+", "x2", ")", "\n", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.fcrn._fcrn_builder": [[152, 198], ["up_project.get_shape().as_list", "tensorflow.variable_scope", "tensorflow.variable_scope", "fcrn.conv", "fcrn.up_project", "fcrn.up_project", "fcrn.up_project", "fcrn.up_project", "fcrn.conv", "fcrn.conv", "fcrn.conv", "tensorflow.nn.l2_normalize", "up_project.get_shape", "fcrn.resnet_v1_50"], "function", ["home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.fcrn.conv", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.fcrn.up_project", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.fcrn.up_project", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.fcrn.up_project", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.fcrn.up_project", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.fcrn.conv", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.fcrn.conv", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.fcrn.conv", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.fcrn.resnet_v1_50"], ["", "def", "_fcrn_builder", "(", "x", ",", "\n", "name", ",", "\n", "cnn_fn", ",", "\n", "num_classes", ",", "\n", "is_training", ",", "\n", "use_global_status", ",", "\n", "reuse", "=", "False", ")", ":", "\n", "  ", "\"\"\"Helper function to build FCRN model for depth estimation.\n  \"\"\"", "\n", "# Input size.", "\n", "h", ",", "w", "=", "x", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "1", ":", "3", "]", "# NxHxWxC", "\n", "\n", "# Build the base network.", "\n", "x", "=", "cnn_fn", "(", "x", ",", "name", ",", "is_training", ",", "use_global_status", ",", "reuse", ")", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "name", ",", "reuse", "=", "reuse", ")", "as", "scope", ":", "\n", "    ", "with", "tf", ".", "variable_scope", "(", "'block5'", ")", "as", "scope", ":", "\n", "# Build Upsampling Porjection.", "\n", "      ", "x", "=", "conv", "(", "x", ",", "'conv1'", ",", "1024", ",", "[", "1", ",", "1", "]", ",", "1", ",", "padding", "=", "'SAME'", ",", "\n", "biased", "=", "False", ",", "bn", "=", "True", ",", "relu", "=", "False", ",", "decay", "=", "0.99", ",", "\n", "is_training", "=", "is_training", ",", "\n", "use_global_status", "=", "use_global_status", ")", "\n", "\n", "x", "=", "up_project", "(", "x", ",", "'upproj1'", ",", "512", ",", "\n", "is_training", ",", "use_global_status", ")", "\n", "x", "=", "up_project", "(", "x", ",", "'upproj2'", ",", "256", ",", "\n", "is_training", ",", "use_global_status", ")", "\n", "x", "=", "up_project", "(", "x", ",", "'upproj3'", ",", "128", ",", "\n", "is_training", ",", "use_global_status", ")", "\n", "x", "=", "up_project", "(", "x", ",", "'upproj4'", ",", "64", ",", "\n", "is_training", ",", "use_global_status", ")", "\n", "\n", "seg", "=", "conv", "(", "x", ",", "'fc1_seg'", ",", "num_classes", ",", "[", "3", ",", "3", "]", ",", "1", ",", "padding", "=", "'SAME'", ",", "\n", "biased", "=", "True", ",", "bn", "=", "False", ",", "relu", "=", "False", ",", "\n", "is_training", "=", "is_training", ")", "\n", "\n", "dph", "=", "conv", "(", "x", ",", "'fc1_depth'", ",", "1", ",", "[", "3", ",", "3", "]", ",", "1", ",", "padding", "=", "'SAME'", ",", "\n", "biased", "=", "True", ",", "bn", "=", "False", ",", "relu", "=", "True", ",", "\n", "is_training", "=", "is_training", ")", "\n", "\n", "nrm", "=", "conv", "(", "x", ",", "'fc1_normal'", ",", "3", ",", "[", "3", ",", "3", "]", ",", "1", ",", "padding", "=", "'SAME'", ",", "\n", "biased", "=", "True", ",", "bn", "=", "False", ",", "relu", "=", "False", ",", "\n", "is_training", "=", "is_training", ")", "\n", "nrm", "=", "tf", ".", "nn", ".", "l2_normalize", "(", "nrm", ",", "dim", "=", "-", "1", ")", "\n", "\n", "", "return", "[", "seg", ",", "dph", ",", "nrm", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.fcrn.resnet_v1_50": [[200, 216], ["network.common.resnet_v1.resnet_v1"], "function", ["home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.common.resnet_v1.resnet_v1"], ["", "", "def", "resnet_v1_50", "(", "x", ",", "\n", "name", ",", "\n", "is_training", ",", "\n", "use_global_status", ",", "\n", "reuse", "=", "False", ")", ":", "\n", "  ", "\"\"\"Builds ResNet50 v1.\n  \"\"\"", "\n", "return", "resnet_v1", "(", "x", ",", "\n", "name", "=", "name", ",", "\n", "filters", "=", "[", "64", ",", "128", ",", "256", ",", "512", "]", ",", "\n", "num_blocks", "=", "[", "3", ",", "4", ",", "6", ",", "3", "]", ",", "\n", "strides", "=", "[", "2", ",", "2", ",", "2", ",", "1", "]", ",", "\n", "dilations", "=", "[", "None", ",", "None", ",", "None", ",", "None", "]", ",", "\n", "is_training", "=", "is_training", ",", "\n", "use_global_status", "=", "use_global_status", ",", "\n", "reuse", "=", "reuse", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.fcrn.structured_predictor": [[218, 234], ["fcrn._fcrn_builder"], "function", ["home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.fcrn._fcrn_builder"], ["", "def", "structured_predictor", "(", "x", ",", "\n", "num_classes", ",", "\n", "is_training", ",", "\n", "use_global_status", ",", "\n", "reuse", "=", "False", ")", ":", "\n", "  ", "\"\"\"Helper function to build FCRN as structured predictor.\n  \"\"\"", "\n", "scores", "=", "_fcrn_builder", "(", "x", ",", "\n", "'resnet_v1_50'", ",", "\n", "resnet_v1_50", ",", "\n", "num_classes", ",", "\n", "is_training", ",", "\n", "use_global_status", ",", "\n", "reuse", "=", "reuse", ")", "\n", "\n", "return", "scores", "\n", "", ""]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.analyzer._unet_builder": [[8, 136], ["ValueError", "tensorflow.variable_scope", "len", "len", "len", "len", "tensorflow.name_scope", "range", "range", "network.conv", "tensorflow.image.resize_bilinear", "tensorflow.add_to_collection", "len", "range", "print", "shortcuts.append", "range", "print", "name_format.format", "tensorflow.multiply", "tensorflow.add_to_collection", "tensorflow.nn.relu", "len", "shortcuts[].get_shape().as_list", "name_format.format", "network.conv", "tensorflow.multiply", "tensorflow.add_to_collection", "tensorflow.nn.relu", "in_x.get_shape().as_list", "tensorflow.shape", "enumerate", "tensorflow.concat", "network.conv", "network.max_pool", "not_ignore_masks.append", "tensorflow.image.resize_bilinear", "tensorflow.concat", "tf.concat.append", "shortcuts[].get_shape", "in_x.get_shape", "network.conv", "len", "int"], "function", ["home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.fcrn.conv", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.fcrn.conv", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.fcrn.conv", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.common.layers.max_pool", "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.fcrn.conv"], ["def", "_unet_builder", "(", "x", ",", "\n", "mask", ",", "\n", "name", ",", "\n", "filters", "=", "[", "64", ",", "128", ",", "256", ",", "512", ",", "1024", "]", ",", "\n", "num_blocks", "=", "[", "2", ",", "3", ",", "3", ",", "3", ",", "3", "]", ",", "\n", "strides", "=", "[", "2", ",", "2", ",", "2", ",", "2", ",", "2", "]", ",", "\n", "is_training", "=", "True", ",", "\n", "use_global_status", "=", "False", ",", "\n", "reuse", "=", "False", ")", ":", "\n", "  ", "\"\"\"Helper function to construct UNet.\n  \"\"\"", "\n", "if", "len", "(", "filters", ")", "!=", "len", "(", "num_blocks", ")", "or", "len", "(", "filters", ")", "!=", "len", "(", "strides", ")", ":", "\n", "    ", "raise", "ValueError", "(", "'length of lists are not consistent'", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "'Analyzer'", ",", "reuse", "=", "reuse", ")", "as", "scope", ":", "\n", "    ", "with", "tf", ".", "name_scope", "(", "name", ")", ":", "\n", "      ", "input_x", "=", "x", "\n", "\n", "# Encoder.", "\n", "shortcuts", "=", "[", "]", "\n", "not_ignore_masks", "=", "[", "]", "\n", "for", "ib", "in", "range", "(", "len", "(", "filters", ")", ")", ":", "\n", "        ", "for", "iu", "in", "range", "(", "num_blocks", "[", "ib", "]", ")", ":", "\n", "          ", "name_format", "=", "'layer{:d}/unit_{:d}/encoder/'", "\n", "block_name", "=", "name_format", ".", "format", "(", "ib", "+", "1", ",", "iu", "+", "1", ")", "\n", "c_o", "=", "filters", "[", "ib", "]", "# output channel", "\n", "\n", "# strides at the begginning", "\n", "s", "=", "strides", "[", "ib", "]", "if", "iu", "==", "0", "else", "1", "\n", "padding", "=", "'VALID'", "if", "s", ">", "1", "else", "'SAME'", "\n", "if", "ib", "==", "0", "and", "iu", "==", "0", ":", "\n", "            ", "x", "=", "[", "]", "\n", "for", "ix", ",", "in_x", "in", "enumerate", "(", "input_x", ")", ":", "\n", "              ", "x", ".", "append", "(", "nn", ".", "conv", "(", "in_x", ",", "\n", "name", "=", "block_name", "+", "'conv{:d}'", ".", "format", "(", "ix", ")", ",", "\n", "filters", "=", "int", "(", "c_o", "/", "2", ")", ",", "\n", "#filters=c_o,", "\n", "kernel_size", "=", "3", ",", "\n", "strides", "=", "s", ",", "\n", "padding", "=", "padding", ",", "\n", "#biased=False,", "\n", "#bn=True,", "\n", "biased", "=", "True", ",", "\n", "bn", "=", "False", ",", "\n", "relu", "=", "False", ",", "\n", "decay", "=", "0.99", ",", "\n", "is_training", "=", "is_training", ",", "\n", "use_global_status", "=", "use_global_status", ")", ")", "\n", "", "x", "=", "tf", ".", "concat", "(", "x", ",", "axis", "=", "-", "1", ",", "name", "=", "block_name", "+", "'concat'", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "nn", ".", "conv", "(", "x", ",", "\n", "name", "=", "block_name", "+", "'conv'", ",", "\n", "filters", "=", "c_o", ",", "\n", "kernel_size", "=", "3", ",", "\n", "strides", "=", "s", ",", "\n", "padding", "=", "padding", ",", "\n", "#biased=False,", "\n", "#bn=True,", "\n", "biased", "=", "True", ",", "\n", "bn", "=", "False", ",", "\n", "relu", "=", "False", ",", "\n", "decay", "=", "0.99", ",", "\n", "is_training", "=", "is_training", ",", "\n", "use_global_status", "=", "use_global_status", ")", "\n", "\n", "", "if", "iu", "==", "0", ":", "\n", "            ", "mask", "=", "nn", ".", "max_pool", "(", "mask", ",", "\n", "block_name", "+", "'mask_pool'", ",", "\n", "3", ",", "\n", "s", ",", "\n", "padding", "=", "padding", ")", "\n", "not_ignore_masks", ".", "append", "(", "1", "-", "mask", ")", "\n", "", "f", "=", "tf", ".", "multiply", "(", "x", ",", "\n", "not_ignore_masks", "[", "-", "1", "]", ",", "\n", "name", "=", "block_name", "+", "'masked_conv'", ")", "\n", "tf", ".", "add_to_collection", "(", "'Analyzer/features'", ",", "f", ")", "\n", "x", "=", "tf", ".", "nn", ".", "relu", "(", "x", ")", "\n", "", "print", "(", "x", ")", "\n", "shortcuts", ".", "append", "(", "x", ")", "\n", "\n", "# Decoder.", "\n", "", "for", "ib", "in", "range", "(", "len", "(", "shortcuts", ")", "-", "1", ",", "0", ",", "-", "1", ")", ":", "\n", "        ", "for", "iu", "in", "range", "(", "num_blocks", "[", "ib", "-", "1", "]", ")", ":", "\n", "          ", "n", ",", "h", ",", "w", ",", "c_o", "=", "shortcuts", "[", "ib", "-", "1", "]", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "\n", "name_format", "=", "'layer{:d}/unit_{:d}/decoder/'", "\n", "block_name", "=", "name_format", ".", "format", "(", "2", "*", "len", "(", "filters", ")", "-", "ib", ",", "iu", "+", "1", ")", "\n", "x", "=", "nn", ".", "conv", "(", "x", ",", "\n", "name", "=", "block_name", "+", "'conv'", ",", "\n", "filters", "=", "c_o", ",", "\n", "kernel_size", "=", "3", ",", "\n", "strides", "=", "1", ",", "\n", "padding", "=", "'SAME'", ",", "\n", "#biased=False,", "\n", "#bn=True,", "\n", "biased", "=", "True", ",", "\n", "bn", "=", "False", ",", "\n", "relu", "=", "False", ",", "\n", "decay", "=", "0.99", ",", "\n", "is_training", "=", "is_training", ",", "\n", "use_global_status", "=", "use_global_status", ")", "\n", "\n", "f", "=", "tf", ".", "multiply", "(", "x", ",", "\n", "not_ignore_masks", "[", "ib", "]", ",", "\n", "name", "=", "block_name", "+", "'masked_conv'", ")", "\n", "tf", ".", "add_to_collection", "(", "'Analyzer/features'", ",", "f", ")", "\n", "x", "=", "tf", ".", "nn", ".", "relu", "(", "x", ")", "\n", "if", "iu", "==", "0", ":", "\n", "            ", "x", "=", "tf", ".", "image", ".", "resize_bilinear", "(", "x", ",", "[", "h", ",", "w", "]", ")", "\n", "x", "=", "tf", ".", "concat", "(", "[", "x", ",", "shortcuts", "[", "ib", "-", "1", "]", "]", ",", "axis", "=", "-", "1", ")", "\n", "", "", "print", "(", "x", ")", "\n", "\n", "", "c_i", "=", "0", "\n", "for", "in_x", "in", "input_x", ":", "\n", "        ", "c_i", "+=", "in_x", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "-", "1", "]", "\n", "", "x", "=", "nn", ".", "conv", "(", "x", ",", "\n", "name", "=", "'block5/fc'", ",", "\n", "filters", "=", "c_i", ",", "\n", "kernel_size", "=", "1", ",", "\n", "strides", "=", "1", ",", "\n", "padding", "=", "'SAME'", ",", "\n", "biased", "=", "True", ",", "\n", "bn", "=", "False", ",", "\n", "relu", "=", "False", ",", "\n", "is_training", "=", "is_training", ")", "\n", "x", "=", "tf", ".", "image", ".", "resize_bilinear", "(", "x", ",", "tf", ".", "shape", "(", "input_x", "[", "0", "]", ")", "[", "1", ":", "3", "]", ")", "\n", "tf", ".", "add_to_collection", "(", "'Analyzer/outputs'", ",", "x", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.analyzer.analyzer": [[138, 153], ["analyzer._unet_builder"], "function", ["home.repos.pwc.inspect_result.twke18_Adversarial_Structure_Matching.models.analyzer._unet_builder"], ["", "", "def", "analyzer", "(", "x", ",", "mask", ",", "name", ",", "is_training", "=", "True", ",", "\n", "use_global_status", "=", "False", ",", "reuse", "=", "False", ")", ":", "\n", "  ", "\"\"\"Build UNet.\n  \"\"\"", "\n", "score", "=", "_unet_builder", "(", "x", ",", "\n", "mask", ",", "\n", "name", "=", "name", ",", "\n", "filters", "=", "[", "32", ",", "64", ",", "128", ",", "128", ",", "128", "]", ",", "\n", "num_blocks", "=", "[", "1", "]", "*", "5", ",", "\n", "strides", "=", "[", "2", "]", "*", "5", ",", "\n", "is_training", "=", "is_training", ",", "\n", "use_global_status", "=", "use_global_status", ",", "\n", "reuse", "=", "reuse", ")", "\n", "\n", "return", "score", "\n", "", ""]]}