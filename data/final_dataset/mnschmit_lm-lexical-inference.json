{"home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.preprocessing.split_data.process_levy": [[6, 22], ["sklearn.model_selection.train_test_split", "open", "open", "open", "fout1.writelines", "fout2.writelines", "line.strip().split", "lines.append", "targets.append", "line.strip"], "function", ["None"], ["def", "process_levy", "(", "args", ":", "argparse", ".", "Namespace", ")", ":", "\n", "    ", "lines", ",", "targets", "=", "[", "]", ",", "[", "]", "\n", "with", "open", "(", "args", ".", "data_file", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "hypo", ",", "prem", ",", "target", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "lines", ".", "append", "(", "line", ")", "\n", "targets", ".", "append", "(", "target", "==", "'True'", ")", "\n", "\n", "", "", "lines1", ",", "lines2", "=", "train_test_split", "(", "\n", "lines", ",", "stratify", "=", "targets", ",", "\n", "random_state", "=", "args", ".", "seed", ",", "test_size", "=", "args", ".", "test_size", "\n", ")", "\n", "\n", "with", "open", "(", "args", ".", "split1_file", ",", "'w'", ")", "as", "fout1", ",", "open", "(", "args", ".", "split2_file", ",", "'w'", ")", "as", "fout2", ":", "\n", "        ", "fout1", ".", "writelines", "(", "lines1", ")", "\n", "fout2", ".", "writelines", "(", "lines2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.preprocessing.split_data.process_sherliic": [[24, 43], ["sklearn.model_selection.train_test_split", "open", "csv.reader", "next", "f.seek", "next", "f.readlines", "len", "len", "open", "open", "fout1.write", "fout1.writelines", "fout2.write", "fout2.writelines"], "function", ["None"], ["", "", "def", "process_sherliic", "(", "args", ":", "argparse", ".", "Namespace", ")", ":", "\n", "    ", "with", "open", "(", "args", ".", "data_file", ")", "as", "f", ":", "\n", "        ", "cr", "=", "csv", ".", "reader", "(", "f", ")", "\n", "next", "(", "cr", ")", "# headers", "\n", "targets", "=", "[", "row", "[", "18", "]", "==", "'yes'", "for", "row", "in", "cr", "]", "\n", "f", ".", "seek", "(", "0", ")", "\n", "header_line", "=", "next", "(", "f", ")", "\n", "lines", "=", "f", ".", "readlines", "(", ")", "\n", "", "assert", "len", "(", "lines", ")", "==", "len", "(", "targets", ")", "\n", "\n", "lines1", ",", "lines2", "=", "train_test_split", "(", "\n", "lines", ",", "stratify", "=", "targets", ",", "\n", "random_state", "=", "args", ".", "seed", ",", "test_size", "=", "args", ".", "test_size", "\n", ")", "\n", "with", "open", "(", "args", ".", "split1_file", ",", "'w'", ")", "as", "fout1", ",", "open", "(", "args", ".", "split2_file", ",", "'w'", ")", "as", "fout2", ":", "\n", "        ", "fout1", ".", "write", "(", "header_line", ")", "\n", "fout1", ".", "writelines", "(", "lines1", ")", "\n", "fout2", ".", "write", "(", "header_line", ")", "\n", "fout2", ".", "writelines", "(", "lines2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.preprocessing.split_data.main": [[45, 50], ["split_data.process_levy", "split_data.process_sherliic"], "function", ["home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.preprocessing.split_data.process_levy", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.preprocessing.split_data.process_sherliic"], ["", "", "def", "main", "(", "args", ":", "argparse", ".", "Namespace", ")", ":", "\n", "    ", "if", "args", ".", "levy", ":", "\n", "        ", "process_levy", "(", "args", ")", "\n", "", "else", ":", "\n", "        ", "process_sherliic", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.preprocessing.split_overlap.extract_relations": [[5, 13], ["open", "line.strip().split", "rels.append", "line.strip", "hypo.split"], "function", ["None"], ["def", "extract_relations", "(", "fname", ":", "str", ")", "->", "List", "[", "str", "]", ":", "\n", "    ", "rels", "=", "[", "]", "\n", "with", "open", "(", "fname", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "hypo", ",", "prem", ",", "cls", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "rels", ".", "append", "(", "hypo", ".", "split", "(", "','", ")", "[", "1", "]", ")", "\n", "# rels.append(prem.split(',')[1])", "\n", "", "", "return", "rels", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.preprocessing.split_overlap.main": [[15, 22], ["set", "set", "print", "print", "split_overlap.extract_relations", "split_overlap.extract_relations", "len"], "function", ["home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.preprocessing.split_overlap.extract_relations", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.preprocessing.split_overlap.extract_relations"], ["", "def", "main", "(", "args", ")", ":", "\n", "    ", "train_relations", "=", "set", "(", "extract_relations", "(", "args", ".", "train_file", ")", ")", "\n", "dev_relations", "=", "set", "(", "extract_relations", "(", "args", ".", "dev_file", ")", ")", "\n", "common", "=", "train_relations", "&", "dev_relations", "\n", "\n", "print", "(", "common", ")", "\n", "print", "(", "len", "(", "common", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.find_patterns.process_sentence_file": [[13, 55], ["open", "len", "len", "zip", "open", "open", "regex.match", "print", "print", "regex.match.groups", "modified_template.format", "match_storage.append", "modsent_buffer.append"], "function", ["None"], ["def", "process_sentence_file", "(", "\n", "pos_regex", ":", "re", ".", "_pattern_type", ",", "\n", "neg_regex", ":", "re", ".", "_pattern_type", ",", "\n", "files", ":", "Tuple", "[", "str", ",", "str", ",", "str", "]", ",", "\n", "prem_markers", "=", "(", "'*_'", ",", "'_*'", ")", ",", "hypo_markers", "=", "(", "'>>'", ",", "'<<'", ")", ",", "\n", "prem_first", "=", "True", "\n", ")", "->", "Tuple", "[", "int", ",", "int", "]", ":", "\n", "\n", "    ", "if", "prem_first", ":", "\n", "        ", "modified_template", "=", "'{}'", "+", "prem_markers", "[", "0", "]", "+", "'{}'", "+", "prem_markers", "[", "1", "]", "+", "'{}'", "+", "hypo_markers", "[", "0", "]", "+", "'{}'", "+", "hypo_markers", "[", "1", "]", "+", "'{}'", "\n", "", "else", ":", "\n", "        ", "modified_template", "=", "'{}'", "+", "hypo_markers", "[", "0", "]", "+", "'{}'", "+", "hypo_markers", "[", "1", "]", "+", "'{}'", "+", "prem_markers", "[", "0", "]", "+", "'{}'", "+", "prem_markers", "[", "1", "]", "+", "'{}'", "\n", "\n", "", "file_in", ",", "file_out_pos", ",", "file_out_neg", "=", "files", "\n", "pos_matches", ",", "neg_matches", "=", "[", "]", ",", "[", "]", "\n", "with", "open", "(", "file_in", ")", "as", "f", ":", "\n", "        ", "for", "sent", "in", "f", ":", "\n", "            ", "for", "regex", ",", "match_storage", "in", "zip", "(", "(", "pos_regex", ",", "neg_regex", ")", ",", "(", "pos_matches", ",", "neg_matches", ")", ")", ":", "\n", "                ", "match", "=", "regex", ".", "match", "(", "sent", ")", "\n", "if", "match", ":", "\n", "                    ", "modsent_buffer", "=", "[", "]", "\n", "for", "g", "in", "match", ".", "groups", "(", ")", ":", "\n", "                        ", "if", "g", "is", "not", "None", ":", "\n", "                            ", "modsent_buffer", ".", "append", "(", "g", ")", "\n", "", "", "modified_sent", "=", "modified_template", ".", "format", "(", "\n", "*", "modsent_buffer", ")", "\n", "match_storage", ".", "append", "(", "modified_sent", ")", "\n", "\n", "", "", "", "", "if", "pos_matches", ":", "\n", "        ", "with", "open", "(", "file_out_pos", ",", "'w'", ")", "as", "fout", ":", "\n", "            ", "for", "sent", "in", "pos_matches", ":", "\n", "                ", "print", "(", "sent", ",", "file", "=", "fout", ")", "\n", "", "", "", "if", "neg_matches", ":", "\n", "        ", "with", "open", "(", "file_out_neg", ",", "'w'", ")", "as", "fout", ":", "\n", "            ", "for", "sent", "in", "neg_matches", ":", "\n", "                ", "print", "(", "sent", ",", "file", "=", "fout", ")", "\n", "\n", "", "", "", "return", "len", "(", "pos_matches", ")", ",", "len", "(", "neg_matches", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.find_patterns.create_pattern_from_word_pair": [[57, 60], ["None"], "function", ["None"], ["", "def", "create_pattern_from_word_pair", "(", "prem", ":", "str", ",", "hypo", ":", "str", ")", "->", "str", ":", "\n", "# NB: word boundaries at beginning and end of relations", "\n", "    ", "return", "r'(.*)\\b({})\\b(.*\\s+\\S+\\s+.*)\\b({})\\b(.*)'", ".", "format", "(", "prem", ",", "hypo", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.find_patterns.generate_forms": [[62, 77], ["verb_forms.verb_present", "find_patterns.generate_forms_of_be", "verb_forms.verb_present_participle", "verb_forms.verb_past", "forms.append", "forms.append", "forms.append"], "function", ["home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.find_patterns.generate_forms_of_be"], ["", "def", "generate_forms", "(", "verb_inf", ":", "str", ")", "->", "List", "[", "str", "]", ":", "\n", "    ", "forms", "=", "[", "verb_inf", "]", "\n", "pres3sg", "=", "verb_present", "(", "verb_inf", ",", "person", "=", "'3'", ")", "\n", "if", "pres3sg", ":", "\n", "        ", "forms", ".", "append", "(", "pres3sg", ")", "\n", "\n", "", "be", "=", "generate_forms_of_be", "(", ")", "\n", "gerund", "=", "verb_present_participle", "(", "verb_inf", ")", "\n", "if", "gerund", ":", "\n", "        ", "forms", ".", "append", "(", "be", "+", "' '", "+", "gerund", ")", "\n", "\n", "", "past", "=", "verb_past", "(", "verb_inf", ",", "person", "=", "'3'", ")", "\n", "if", "past", ":", "\n", "        ", "forms", ".", "append", "(", "past", ")", "\n", "", "return", "forms", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.find_patterns.generate_forms_of_be": [[79, 84], ["sorted"], "function", ["None"], ["", "def", "generate_forms_of_be", "(", ")", "->", "List", "[", "str", "]", ":", "\n", "    ", "return", "'(?:'", "+", "\"|\"", ".", "join", "(", "\n", "sorted", "(", "[", "form", "for", "form", "in", "verb_tenses", "[", "'be'", "]", "\n", "if", "form", "]", ",", "key", "=", "len", ",", "reverse", "=", "True", ")", "\n", ")", "+", "')'", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.find_patterns.create_pattern_from_relation_path": [[86, 127], ["lemmas.reverse", "verb_forms.is_verb_inf", "find_patterns.generate_forms_of_be", "verb_forms.verb_past_participle", "relpath.startswith", "relpath.endswith", "relpath.endswith", "len", "enumerate", "verb_forms.is_non_verb", "relpath.startswith", "relpath.endswith", "relpath.endswith", "find_patterns.generate_forms_of_be", "lemmas.insert", "len", "relpath.split", "sorted", "find_patterns.generate_forms"], "function", ["home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.find_patterns.generate_forms_of_be", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.find_patterns.generate_forms_of_be", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.find_patterns.generate_forms"], ["", "def", "create_pattern_from_relation_path", "(", "relpath", ":", "str", ",", "is_reversed", ":", "bool", ")", "->", "str", ":", "\n", "    ", "lemmas", "=", "[", "\n", "word", "\n", "for", "i", ",", "word", "in", "enumerate", "(", "relpath", ".", "split", "(", "'___'", ")", ")", "\n", "if", "i", "%", "2", "==", "1", "\n", "]", "\n", "if", "is_reversed", ":", "\n", "        ", "lemmas", ".", "reverse", "(", ")", "\n", "\n", "", "pred_is_verb", "=", "is_verb_inf", "(", "lemmas", "[", "0", "]", ")", "and", "not", "is_non_verb", "(", "lemmas", "[", "0", "]", ")", "\n", "\n", "if", "(", "relpath", ".", "startswith", "(", "'nsubjpass'", ")", "or", "relpath", ".", "endswith", "(", "'nsubjpass'", ")", "\n", "or", "relpath", ".", "endswith", "(", "'nsubjpass^-'", ")", ")", "and", "pred_is_verb", ":", "\n", "        ", "be", "=", "generate_forms_of_be", "(", ")", "\n", "pred", "=", "verb_past_participle", "(", "lemmas", "[", "0", "]", ")", "\n", "pred", "=", "be", "+", "' '", "+", "pred", "\n", "", "else", ":", "\n", "        ", "if", "pred_is_verb", ":", "\n", "            ", "pred", "=", "'(?:'", "+", "\"|\"", ".", "join", "(", "\n", "sorted", "(", "generate_forms", "(", "lemmas", "[", "0", "]", ")", ",", "key", "=", "len", ",", "reverse", "=", "True", ")", "\n", ")", "+", "')'", "\n", "", "else", ":", "\n", "            ", "pred", "=", "generate_forms_of_be", "(", ")", "\n", "lemmas", ".", "insert", "(", "0", ",", "'be'", ")", "\n", "\n", "", "", "poss_involved", "=", "relpath", ".", "startswith", "(", "'poss'", ")", "or", "relpath", ".", "endswith", "(", "\n", "'poss'", ")", "or", "relpath", ".", "endswith", "(", "'poss^-'", ")", "\n", "\n", "if", "len", "(", "lemmas", ")", "==", "1", ":", "\n", "        ", "return", "pred", "\n", "", "elif", "len", "(", "lemmas", ")", "==", "2", ":", "\n", "        ", "if", "poss_involved", ":", "\n", "            ", "return", "pred", "+", "r'(?:\\s*\\S+\\s*){0,5}'", "+", "r\"'s\\s*\"", "+", "lemmas", "[", "1", "]", "\n", "", "else", ":", "\n", "            ", "return", "pred", "+", "' '", "+", "lemmas", "[", "1", "]", "\n", "", "", "else", ":", "\n", "        ", "if", "poss_involved", ":", "\n", "            ", "return", "(", "pred", "+", "' '", "+", "' '", ".", "join", "(", "lemmas", "[", "1", ":", "-", "1", "]", ")", "\n", "+", "r'(?:\\s*\\S+\\s*){0,5}'", "+", "r\"'s\\s*\"", "+", "lemmas", "[", "-", "1", "]", ")", "\n", "", "else", ":", "\n", "            ", "return", "pred", "+", "' '", "+", "' '", ".", "join", "(", "lemmas", "[", "1", ":", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.find_patterns.create_pattern_from_relation": [[129, 159], ["rel_end.strip.strip", "rel_middle.strip.strip", "rel_middle.strip.startswith", "rel_middle.strip.split", "verb_forms.is_verb_form", "verb_forms.verb_infinitive", "verb_forms.is_non_verb", "sorted"], "function", ["None"], ["", "", "", "def", "create_pattern_from_relation", "(", "rel_middle", ":", "str", ",", "rel_end", ":", "str", ")", "->", "str", ":", "\n", "    ", "rel_end", "=", "rel_end", ".", "strip", "(", ")", "\n", "rel_middle", "=", "rel_middle", ".", "strip", "(", ")", "\n", "\n", "if", "rel_middle", "==", "'is'", ":", "\n", "        ", "return", "rel_end", "\n", "\n", "# is preming -> preming", "\n", "# is supporter -> supporter", "\n", "", "if", "rel_middle", ".", "startswith", "(", "'is '", ")", ":", "\n", "        ", "rel_middle", "=", "rel_middle", "[", "3", ":", "]", "\n", "# if rel_middle.endswith('ing') and len(rel_middle) > 5:", "\n", "#     rel_middle = rel_middle[:-3]", "\n", "\n", "", "pred", ",", "*", "rest", "=", "rel_middle", ".", "split", "(", ")", "\n", "\n", "# prem -> prem|preming|prems|premed", "\n", "if", "is_verb_form", "(", "pred", ")", "and", "not", "is_non_verb", "(", "pred", ")", ":", "\n", "        ", "inf", "=", "verb_infinitive", "(", "pred", ")", "\n", "rel_middle", "=", "'(?:'", "+", "\"|\"", ".", "join", "(", "\n", "sorted", "(", "[", "form", "for", "form", "in", "verb_tenses", "[", "inf", "]", "\n", "if", "form", "]", ",", "key", "=", "len", ",", "reverse", "=", "True", ")", "\n", ")", "+", "')'", "\n", "if", "rest", ":", "\n", "            ", "rel_middle", "+=", "' '", "+", "' '", ".", "join", "(", "rest", ")", "\n", "\n", "", "", "if", "not", "rel_end", ":", "\n", "        ", "return", "rel_middle", "\n", "", "else", ":", "\n", "        ", "return", "rel_middle", "+", "r'(?:\\s*\\S+\\s*){0,5}'", "+", "rel_end", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.find_patterns.create_pattern_from_dataset": [[161, 182], ["find_patterns.create_pattern_from_relation_path", "find_patterns.create_pattern_from_relation_path", "sentence_patterns.append", "find_patterns.create_pattern_from_word_pair", "find_patterns.create_pattern_from_word_pair"], "function", ["home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.find_patterns.create_pattern_from_relation_path", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.find_patterns.create_pattern_from_relation_path", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.find_patterns_levy.create_pattern_from_word_pair", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.find_patterns_levy.create_pattern_from_word_pair"], ["", "", "def", "create_pattern_from_dataset", "(", "\n", "rows", ":", "Iterable", "[", "Tuple", "[", "str", ",", "bool", ",", "str", ",", "bool", "]", "]", ",", "prem_first", ":", "bool", "\n", ")", "->", "str", ":", "\n", "    ", "sentence_patterns", "=", "[", "]", "\n", "for", "prem_path", ",", "is_prem_reversed", ",", "hypo_path", ",", "is_hypo_reversed", "in", "rows", ":", "\n", "        ", "prem_pattern", "=", "create_pattern_from_relation_path", "(", "\n", "prem_path", ",", "is_prem_reversed", ")", "\n", "hypo_pattern", "=", "create_pattern_from_relation_path", "(", "\n", "hypo_path", ",", "is_hypo_reversed", ")", "\n", "if", "prem_first", ":", "\n", "            ", "sentence_pattern", "=", "create_pattern_from_word_pair", "(", "\n", "prem_pattern", ",", "hypo_pattern", "\n", ")", "\n", "", "else", ":", "\n", "            ", "sentence_pattern", "=", "create_pattern_from_word_pair", "(", "\n", "hypo_pattern", ",", "prem_pattern", "\n", ")", "\n", "\n", "", "sentence_patterns", ".", "append", "(", "sentence_pattern", ")", "\n", "\n", "", "return", "'|'", ".", "join", "(", "sentence_patterns", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.find_patterns.extract_instances_from_dataset": [[184, 208], ["open", "csv.reader", "next", "pos_inst.append", "neg_inst.append"], "function", ["None"], ["", "def", "extract_instances_from_dataset", "(", "\n", "file_name", ":", "str", ",", "relation_index", ":", "Dict", "[", "int", ",", "str", "]", ")", "->", "Tuple", "[", "\n", "List", "[", "Tuple", "[", "str", ",", "bool", ",", "str", ",", "bool", "]", "]", ",", "\n", "List", "[", "Tuple", "[", "str", ",", "bool", ",", "str", ",", "bool", "]", "]", "\n", "]", ":", "\n", "    ", "pos_inst", ",", "neg_inst", "=", "[", "]", ",", "[", "]", "\n", "with", "open", "(", "file_name", ")", "as", "f", ":", "\n", "        ", "reader", "=", "csv", ".", "reader", "(", "f", ")", "\n", "next", "(", "reader", ")", "# headers", "\n", "for", "row", "in", "reader", ":", "\n", "# prem = tuple(row[5:9])", "\n", "# hypo = tuple(row[9:13])", "\n", "            ", "prem_path", "=", "relation_index", "[", "row", "[", "2", "]", "]", "\n", "hypo_path", "=", "relation_index", "[", "row", "[", "4", "]", "]", "\n", "is_prem_reversed", "=", "row", "[", "13", "]", "==", "'True'", "\n", "is_hypo_reversed", "=", "row", "[", "14", "]", "==", "'True'", "\n", "cls", "=", "row", "[", "17", "]", "==", "'yes'", "\n", "if", "cls", ":", "\n", "                ", "pos_inst", ".", "append", "(", "(", "prem_path", ",", "is_prem_reversed", ",", "\n", "hypo_path", ",", "is_hypo_reversed", ")", ")", "\n", "", "else", ":", "\n", "                ", "neg_inst", ".", "append", "(", "(", "prem_path", ",", "is_prem_reversed", ",", "\n", "hypo_path", ",", "is_hypo_reversed", ")", ")", "\n", "", "", "", "return", "pos_inst", ",", "neg_inst", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.find_patterns.find_patterns": [[210, 249], ["find_patterns.extract_instances_from_dataset", "find_patterns.create_pattern_from_dataset", "find_patterns.create_pattern_from_dataset", "re.compile", "re.compile", "tqdm.tqdm", "in_fn.split", "os.path.join", "os.path.join", "tqdm.tqdm.append", "multiprocessing.Pool", "pool.imap_unordered", "find_patterns.process_sentence_file", "functools.partial"], "function", ["home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.find_patterns_levy.extract_instances_from_dataset", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.find_patterns_levy.create_pattern_from_dataset", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.find_patterns_levy.create_pattern_from_dataset", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.find_patterns_levy.process_sentence_file"], ["", "def", "find_patterns", "(", "args", ",", "rel_idx", ",", "prem_first", ")", ":", "\n", "    ", "pos_inst", ",", "neg_inst", "=", "extract_instances_from_dataset", "(", "\n", "args", ".", "dataset_file", ",", "rel_idx", "\n", ")", "\n", "pos_pattern", "=", "create_pattern_from_dataset", "(", "pos_inst", ",", "prem_first", ")", "\n", "neg_pattern", "=", "create_pattern_from_dataset", "(", "neg_inst", ",", "prem_first", ")", "\n", "pos_regex", "=", "re", ".", "compile", "(", "pos_pattern", ")", "\n", "neg_regex", "=", "re", ".", "compile", "(", "neg_pattern", ")", "\n", "\n", "sentence_file_tuples", "=", "[", "]", "\n", "for", "in_fn", "in", "args", ".", "sentences_in", ":", "\n", "        ", "path_elements", "=", "in_fn", ".", "split", "(", "os", ".", "path", ".", "sep", ")", "\n", "base_fn", "=", "path_elements", "[", "-", "2", "]", "+", "'_'", "+", "path_elements", "[", "-", "1", "]", "\n", "out_fn1", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "out_dir", ",", "base_fn", "+", "'-pos-'", "+", "(", "'prem_first'", "if", "prem_first", "else", "'hypo_first'", ")", ")", "\n", "out_fn2", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "out_dir", ",", "base_fn", "+", "'-neg-'", "+", "(", "'prem_first'", "if", "prem_first", "else", "'hypo_first'", ")", ")", "\n", "sentence_file_tuples", ".", "append", "(", "(", "in_fn", ",", "out_fn1", ",", "out_fn2", ")", ")", "\n", "", "sentence_file_tuples", "=", "tqdm", "(", "sentence_file_tuples", ")", "\n", "\n", "global_pcount", "=", "0", "\n", "global_ncount", "=", "0", "\n", "if", "args", ".", "num_threads", ">", "1", ":", "\n", "        ", "with", "Pool", "(", "processes", "=", "args", ".", "num_threads", ")", "as", "pool", ":", "\n", "            ", "for", "pcount", ",", "ncount", "in", "pool", ".", "imap_unordered", "(", "\n", "partial", "(", "process_sentence_file", ",", "pos_regex", ",", "\n", "neg_regex", ",", "prem_first", "=", "prem_first", ")", ",", "\n", "sentence_file_tuples", "\n", ")", ":", "\n", "                ", "global_pcount", "+=", "pcount", "\n", "global_ncount", "+=", "ncount", "\n", "", "", "", "else", ":", "\n", "        ", "for", "sftup", "in", "sentence_file_tuples", ":", "\n", "            ", "pcount", ",", "ncount", "=", "process_sentence_file", "(", "\n", "pos_regex", ",", "neg_regex", ",", "sftup", ",", "prem_first", "=", "prem_first", ")", "\n", "global_pcount", "+=", "pcount", "\n", "global_ncount", "+=", "ncount", "\n", "\n", "", "", "return", "global_pcount", ",", "global_ncount", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.find_patterns.main": [[251, 264], ["find_patterns.find_patterns", "find_patterns.find_patterns", "print", "open", "line.strip().split", "line.strip"], "function", ["home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.find_patterns_levy.find_patterns", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.find_patterns_levy.find_patterns"], ["", "def", "main", "(", "args", ":", "argparse", ".", "Namespace", ")", ":", "\n", "    ", "rel_idx", "=", "{", "}", "\n", "with", "open", "(", "args", ".", "relation_index", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "idx", ",", "relpath", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "rel_idx", "[", "idx", "]", "=", "relpath", "\n", "\n", "", "", "pcount1", ",", "ncount1", "=", "find_patterns", "(", "args", ",", "rel_idx", ",", "True", ")", "\n", "pcount2", ",", "ncount2", "=", "find_patterns", "(", "args", ",", "rel_idx", ",", "False", ")", "\n", "\n", "print", "(", "\n", "\"Found {} positive and {} negative matches.\"", ".", "format", "(", "\n", "pcount1", "+", "pcount2", ",", "ncount1", "+", "ncount2", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.score_patterns.put_on_gpu": [[10, 15], ["encoding.items", "v.cuda"], "function", ["None"], ["def", "put_on_gpu", "(", "encoding", ":", "BatchEncoding", ",", "device", ":", "int", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "    ", "res", "=", "{", "}", "\n", "for", "k", ",", "v", "in", "encoding", ".", "items", "(", ")", ":", "\n", "        ", "res", "[", "k", "]", "=", "v", ".", "cuda", "(", "device", ")", "\n", "", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.score_patterns.batch_generator": [[17, 43], ["template.format", "sents.append", "exp_words.append", "torch.LongTensor().cuda", "tokenizer", "score_patterns.put_on_gpu", "torch.argmax", "tokenizer.encode", "len", "torch.LongTensor().cuda", "tokenizer", "score_patterns.put_on_gpu", "torch.argmax", "sents.clear", "exp_words.clear", "mask_token_mask.long", "mask_token_mask.long", "torch.LongTensor", "torch.LongTensor"], "function", ["home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.score_patterns_levy.put_on_gpu", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.score_patterns_levy.put_on_gpu"], ["", "def", "batch_generator", "(", "template", ",", "pos_pairs", ",", "insert_idx", ",", "tokenizer", ",", "device", ",", "batch_size", ")", ":", "\n", "    ", "sents", ",", "exp_words", "=", "[", "]", ",", "[", "]", "\n", "for", "pair", "in", "pos_pairs", ":", "\n", "        ", "sent", "=", "template", ".", "format", "(", "pair", "[", "insert_idx", "]", ")", "\n", "sents", ".", "append", "(", "sent", ")", "\n", "expected_word", ":", "int", "=", "tokenizer", ".", "encode", "(", "\n", "pair", "[", "1", "-", "insert_idx", "]", ",", "add_special_tokens", "=", "False", ")", "[", "0", "]", "\n", "exp_words", ".", "append", "(", "expected_word", ")", "\n", "if", "len", "(", "exp_words", ")", "==", "batch_size", ":", "\n", "            ", "expw_tensor", "=", "torch", ".", "LongTensor", "(", "exp_words", ")", ".", "cuda", "(", "device", ")", "\n", "sents_enc", "=", "tokenizer", "(", "sents", ",", "padding", "=", "True", ",", "truncation", "=", "True", ",", "\n", "return_tensors", "=", "'pt'", ")", "\n", "sents_enc", "=", "put_on_gpu", "(", "sents_enc", ",", "device", ")", "\n", "mask_token_mask", "=", "sents_enc", "[", "'input_ids'", "]", "==", "tokenizer", ".", "mask_token_id", "\n", "mask_idx", "=", "torch", ".", "argmax", "(", "mask_token_mask", ".", "long", "(", ")", ",", "dim", "=", "1", ")", "\n", "yield", "sents_enc", ",", "mask_idx", ",", "expw_tensor", "\n", "sents", ".", "clear", "(", ")", "\n", "exp_words", ".", "clear", "(", ")", "\n", "", "", "if", "sents", ":", "\n", "        ", "expw_tensor", "=", "torch", ".", "LongTensor", "(", "exp_words", ")", ".", "cuda", "(", "device", ")", "\n", "sents_enc", "=", "tokenizer", "(", "sents", ",", "padding", "=", "True", ",", "truncation", "=", "True", ",", "\n", "return_tensors", "=", "'pt'", ")", "\n", "sents_enc", "=", "put_on_gpu", "(", "sents_enc", ",", "device", ")", "\n", "mask_token_mask", "=", "sents_enc", "[", "'input_ids'", "]", "==", "tokenizer", ".", "mask_token_id", "\n", "mask_idx", "=", "torch", ".", "argmax", "(", "mask_token_mask", ".", "long", "(", ")", ",", "dim", "=", "1", ")", "\n", "yield", "sents_enc", ",", "mask_idx", ",", "expw_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.score_patterns.count_hits": [[45, 68], ["score_patterns.batch_generator", "lm_model", "mask_logits.topk", "torch.gather", "mask_idx[].expand_as", "expected_word.unsqueeze().expand_as", "expected_word.unsqueeze"], "function", ["home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.score_patterns_levy.batch_generator"], ["", "", "def", "count_hits", "(", "masked_sentence_template", ":", "str", ",", "lm_model", ":", "AutoModelForMaskedLM", ",", "k", ":", "int", ",", "\n", "tokenizer", ":", "PreTrainedTokenizer", ",", "\n", "pos_pairs", ":", "Iterable", "[", "Tuple", "[", "str", ",", "str", "]", "]", ",", "insert_idx", ":", "int", ",", "\n", "device", ":", "int", ",", "batch_size", ":", "int", ")", "->", "int", ":", "\n", "    ", "batches", "=", "batch_generator", "(", "\n", "masked_sentence_template", ",", "\n", "pos_pairs", ",", "insert_idx", ",", "tokenizer", ",", "device", ",", "\n", "batch_size", "\n", ")", "\n", "hits", "=", "0", "\n", "for", "batch", "in", "batches", ":", "\n", "        ", "masked_sent", ",", "mask_idx", ",", "expected_word", "=", "batch", "\n", "out", "=", "lm_model", "(", "**", "masked_sent", ")", "\n", "# (batch_size, seq_len, vocab_size)", "\n", "logits", "=", "out", "[", "0", "]", "\n", "# (batch_size, vocab_size)", "\n", "mask_logits", "=", "torch", ".", "gather", "(", "\n", "logits", ",", "1", ",", "mask_idx", "[", ":", ",", "None", ",", "None", "]", ".", "expand_as", "(", "logits", ")", "\n", ")", "[", ":", ",", "0", ",", ":", "]", "\n", "# (batch_size, k)", "\n", "scores", ",", "indices", "=", "mask_logits", ".", "topk", "(", "k", ")", "\n", "hits", "+=", "(", "indices", "==", "expected_word", ".", "unsqueeze", "(", "1", ")", ".", "expand_as", "(", "indices", ")", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "", "return", "hits", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.score_patterns.score_pattern": [[70, 80], ["pattern.format", "score_patterns.count_hits", "pattern.format", "score_patterns.count_hits"], "function", ["home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.score_patterns_levy.count_hits", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.score_patterns_levy.count_hits"], ["", "def", "score_pattern", "(", "pattern", ":", "str", ",", "pos_pairs", ":", "Iterable", "[", "Tuple", "[", "str", ",", "str", "]", "]", ",", "\n", "lm_model", ":", "AutoModelForMaskedLM", ",", "tokenizer", ":", "PreTrainedTokenizer", ",", "\n", "device", ":", "int", ",", "k", ":", "int", "=", "100", ",", "batch_size", ":", "int", "=", "2", ")", "->", "int", ":", "\n", "    ", "prem_masked_pattern", "=", "pattern", ".", "format", "(", "prem", "=", "tokenizer", ".", "mask_token", ",", "hypo", "=", "'{}'", ")", "\n", "prem_hits", "=", "count_hits", "(", "prem_masked_pattern", ",", "lm_model", ",", "\n", "k", ",", "tokenizer", ",", "pos_pairs", ",", "1", ",", "device", ",", "batch_size", ")", "\n", "hypo_masked_pattern", "=", "pattern", ".", "format", "(", "hypo", "=", "tokenizer", ".", "mask_token", ",", "prem", "=", "'{}'", ")", "\n", "hypo_hits", "=", "count_hits", "(", "hypo_masked_pattern", ",", "lm_model", ",", "\n", "k", ",", "tokenizer", ",", "pos_pairs", ",", "0", ",", "device", ",", "batch_size", ")", "\n", "return", "prem_hits", "+", "hypo_hits", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.score_patterns.main": [[82, 128], ["transformers.AutoModelForMaskedLM.from_pretrained", "transformers.AutoTokenizer.from_pretrained", "AutoModelForMaskedLM.from_pretrained.cuda", "tqdm.tqdm", "open", "open", "csv.reader", "next", "open", "patterns.sort", "score_patterns.score_pattern", "open", "sorted", "line.strip().split", "pos_pairs.append", "patterns.append", "pattern_score.keys", "print", "prem_path.split", "hypo_path.split", "pat.strip", "line.strip"], "function", ["home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.score_patterns_levy.score_pattern"], ["", "def", "main", "(", "args", ":", "argparse", ".", "Namespace", ")", ":", "\n", "    ", "rel_idx", "=", "{", "}", "\n", "with", "open", "(", "args", ".", "relation_index", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "idx", ",", "rel", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "rel_idx", "[", "idx", "]", "=", "rel", "\n", "\n", "", "", "pos_pairs", "=", "[", "]", "\n", "with", "open", "(", "args", ".", "sherliic_file", ")", "as", "f", ":", "\n", "        ", "r", "=", "csv", ".", "reader", "(", "f", ")", "\n", "next", "(", "r", ")", "# headers", "\n", "for", "row", "in", "r", ":", "\n", "            ", "cls", "=", "row", "[", "17", "]", "==", "'yes'", "\n", "if", "args", ".", "ent_cls", "!=", "cls", ":", "\n", "                ", "continue", "\n", "", "prem_path", "=", "rel_idx", "[", "row", "[", "2", "]", "]", "\n", "hypo_path", "=", "rel_idx", "[", "row", "[", "4", "]", "]", "\n", "prem_idx", "=", "-", "2", "if", "row", "[", "13", "]", "==", "'True'", "else", "1", "\n", "hypo_idx", "=", "-", "2", "if", "row", "[", "14", "]", "==", "'True'", "else", "1", "\n", "prem", "=", "prem_path", ".", "split", "(", "'___'", ")", "[", "prem_idx", "]", "\n", "hypo", "=", "hypo_path", ".", "split", "(", "'___'", ")", "[", "hypo_idx", "]", "\n", "pos_pairs", ".", "append", "(", "(", "prem", ",", "hypo", ")", ")", "\n", "\n", "", "", "lm_model", "=", "AutoModelForMaskedLM", ".", "from_pretrained", "(", "args", ".", "model_string", ")", "\n", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "args", ".", "model_string", ")", "\n", "\n", "lm_model", ".", "cuda", "(", "args", ".", "gpu", ")", "\n", "\n", "patterns", "=", "[", "]", "\n", "with", "open", "(", "args", ".", "pattern_file", ")", "as", "f", ":", "\n", "        ", "for", "pat", "in", "f", ":", "\n", "            ", "patterns", ".", "append", "(", "pat", ".", "strip", "(", ")", ")", "\n", "", "", "if", "args", ".", "longest_first", ":", "\n", "        ", "patterns", ".", "sort", "(", "key", "=", "len", ",", "reverse", "=", "True", ")", "\n", "\n", "", "pattern_score", "=", "{", "}", "\n", "for", "pat", "in", "tqdm", "(", "patterns", ")", ":", "\n", "        ", "score", "=", "score_pattern", "(", "\n", "pat", ",", "pos_pairs", ",", "lm_model", ",", "tokenizer", ",", "args", ".", "gpu", ",", "\n", "k", "=", "args", ".", "k", ",", "batch_size", "=", "args", ".", "batch_size", "\n", ")", "\n", "pattern_score", "[", "pat", "]", "=", "score", "\n", "\n", "", "with", "open", "(", "args", ".", "scored_pattern_file", ",", "'w'", ")", "as", "fout", ":", "\n", "        ", "for", "pat", "in", "sorted", "(", "pattern_score", ".", "keys", "(", ")", ",", "key", "=", "pattern_score", ".", "__getitem__", ",", "reverse", "=", "True", ")", ":", "\n", "            ", "print", "(", "pattern_score", "[", "pat", "]", ",", "pat", ",", "sep", "=", "'\\t'", ",", "file", "=", "fout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.score_patterns_levy.put_on_gpu": [[10, 15], ["encoding.items", "v.cuda"], "function", ["None"], ["def", "put_on_gpu", "(", "encoding", ":", "BatchEncoding", ",", "device", ":", "int", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "    ", "res", "=", "{", "}", "\n", "for", "k", ",", "v", "in", "encoding", ".", "items", "(", ")", ":", "\n", "        ", "res", "[", "k", "]", "=", "v", ".", "cuda", "(", "device", ")", "\n", "", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.score_patterns_levy.batch_generator": [[17, 55], ["template.format", "sents.append", "tokenizer.encode", "range", "exp_words.extend", "torch.LongTensor().cuda", "tokenizer", "score_patterns_levy.put_on_gpu", "nltk.word_tokenize", "tokenizer.encode.append", "len", "torch.LongTensor().cuda", "tokenizer", "score_patterns_levy.put_on_gpu", "sents.clear", "exp_words.clear", "len", "torch.LongTensor", "torch.LongTensor"], "function", ["home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.score_patterns_levy.put_on_gpu", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.score_patterns_levy.put_on_gpu"], ["", "def", "batch_generator", "(", "template", ",", "pos_pairs", ",", "insert_idx", ",", "tokenizer", ",", "\n", "device", ",", "batch_size", ",", "num_mask_tokens_in_template", ")", ":", "\n", "    ", "sents", ",", "exp_words", "=", "[", "]", ",", "[", "]", "\n", "for", "pair", "in", "pos_pairs", ":", "\n", "        ", "last_words", "=", "nltk", ".", "word_tokenize", "(", "\n", "pair", "[", "insert_idx", "]", ")", "[", "-", "num_mask_tokens_in_template", ":", "]", "\n", "sent", "=", "template", ".", "format", "(", "' '", ".", "join", "(", "last_words", ")", ")", "\n", "sents", ".", "append", "(", "sent", ")", "\n", "\n", "# NB: all expected words are saved and counted", "\n", "# but not more than num_mask_token_in_batch", "\n", "new_expected_words", "=", "tokenizer", ".", "encode", "(", "\n", "pair", "[", "1", "-", "insert_idx", "]", ",", "add_special_tokens", "=", "False", ")", "\n", "# (1) fill with mask_tokens if relation too short", "\n", "for", "_", "in", "range", "(", "num_mask_tokens_in_template", "-", "len", "(", "new_expected_words", ")", ")", ":", "\n", "            ", "new_expected_words", ".", "append", "(", "tokenizer", ".", "mask_token_id", ")", "\n", "# (2) cut tokens if relation too long", "\n", "", "new_expected_words", "=", "new_expected_words", "[", "-", "num_mask_tokens_in_template", ":", "]", "\n", "\n", "exp_words", ".", "extend", "(", "new_expected_words", ")", "\n", "if", "len", "(", "sents", ")", "==", "batch_size", ":", "\n", "            ", "expw_tensor", "=", "torch", ".", "LongTensor", "(", "exp_words", ")", ".", "cuda", "(", "device", ")", "\n", "sents_enc", "=", "tokenizer", "(", "sents", ",", "padding", "=", "True", ",", "truncation", "=", "True", ",", "\n", "return_tensors", "=", "'pt'", ")", "\n", "sents_enc", "=", "put_on_gpu", "(", "sents_enc", ",", "device", ")", "\n", "mask_token_mask", "=", "sents_enc", "[", "'input_ids'", "]", "==", "tokenizer", ".", "mask_token_id", "\n", "# mask_idx = torch.argmax(mask_token_mask.long(), dim=1)", "\n", "yield", "sents_enc", ",", "mask_token_mask", ",", "expw_tensor", "\n", "sents", ".", "clear", "(", ")", "\n", "exp_words", ".", "clear", "(", ")", "\n", "", "", "if", "sents", ":", "\n", "        ", "expw_tensor", "=", "torch", ".", "LongTensor", "(", "exp_words", ")", ".", "cuda", "(", "device", ")", "\n", "sents_enc", "=", "tokenizer", "(", "sents", ",", "padding", "=", "True", ",", "truncation", "=", "True", ",", "\n", "return_tensors", "=", "'pt'", ")", "\n", "sents_enc", "=", "put_on_gpu", "(", "sents_enc", ",", "device", ")", "\n", "mask_token_mask", "=", "sents_enc", "[", "'input_ids'", "]", "==", "tokenizer", ".", "mask_token_id", "\n", "# mask_idx = torch.argmax(mask_token_mask.long(), dim=1)", "\n", "yield", "sents_enc", ",", "mask_token_mask", ",", "expw_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.score_patterns_levy.count_hits": [[57, 88], ["score_patterns_levy.batch_generator", "lm_model", "mask_logits.topk", "mask_logits.size", "expected_word.unsqueeze().expand_as", "expected_word.unsqueeze"], "function", ["home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.score_patterns_levy.batch_generator"], ["", "", "def", "count_hits", "(", "masked_sentence_template", ":", "str", ",", "lm_model", ":", "AutoModelForMaskedLM", ",", "k", ":", "int", ",", "\n", "tokenizer", ":", "PreTrainedTokenizer", ",", "\n", "pos_pairs", ":", "Iterable", "[", "Tuple", "[", "str", ",", "str", "]", "]", ",", "insert_idx", ":", "int", ",", "\n", "device", ":", "int", ",", "batch_size", ":", "int", ",", "num_mask_tokens_in_template", ":", "int", ")", "->", "int", ":", "\n", "    ", "batches", "=", "batch_generator", "(", "\n", "masked_sentence_template", ",", "\n", "pos_pairs", ",", "insert_idx", ",", "tokenizer", ",", "device", ",", "\n", "batch_size", ",", "num_mask_tokens_in_template", "\n", ")", "\n", "hits", "=", "0", "\n", "for", "batch", "in", "batches", ":", "\n", "        ", "masked_sent", ",", "mask_token_mask", ",", "expected_word", "=", "batch", "\n", "out", "=", "lm_model", "(", "**", "masked_sent", ")", "\n", "# (batch_size, seq_len, vocab_size)", "\n", "logits", "=", "out", "[", "0", "]", "\n", "# (num_mask_token_in_batch, vocab_size)", "\n", "mask_logits", "=", "logits", "[", "mask_token_mask", "]", "\n", "# (num_mask_token_in_batch, k)", "\n", "scores", ",", "indices", "=", "mask_logits", ".", "topk", "(", "k", ")", "\n", "hits_per_mask_token", "=", "(", "indices", "==", "expected_word", ".", "unsqueeze", "(", "\n", "1", ")", ".", "expand_as", "(", "indices", ")", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "/", "mask_logits", ".", "size", "(", "0", ")", "\n", "\n", "# # (batch_size, vocab_size)", "\n", "# mask_logits = torch.gather(", "\n", "#     logits, 1, mask_idx[:, None, None].expand_as(logits)", "\n", "# )[:, 0, :]", "\n", "# # (batch_size, k)", "\n", "# scores, indices = mask_logits.topk(k)", "\n", "\n", "hits", "+=", "hits_per_mask_token", "\n", "", "return", "hits", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.score_patterns_levy.score_pattern": [[90, 112], ["range", "pattern.format", "score_patterns_levy.count_hits", "pattern.format", "score_patterns_levy.count_hits"], "function", ["home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.score_patterns_levy.count_hits", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.score_patterns_levy.count_hits"], ["", "def", "score_pattern", "(", "pattern", ":", "str", ",", "pos_pairs", ":", "Iterable", "[", "Tuple", "[", "str", ",", "str", "]", "]", ",", "\n", "lm_model", ":", "AutoModelForMaskedLM", ",", "tokenizer", ":", "PreTrainedTokenizer", ",", "\n", "device", ":", "int", ",", "k", ":", "int", "=", "100", ",", "\n", "batch_size", ":", "int", "=", "2", ",", "longest_mask_span", ":", "int", "=", "1", ")", "->", "int", ":", "\n", "    ", "hits", "=", "0", "\n", "for", "num_masks", "in", "range", "(", "1", ",", "longest_mask_span", "+", "1", ")", ":", "\n", "        ", "masks", "=", "' '", ".", "join", "(", "[", "tokenizer", ".", "mask_token", "]", "*", "num_masks", ")", "\n", "prem_masked_pattern", "=", "pattern", ".", "format", "(", "\n", "prem", "=", "masks", ",", "hypo", "=", "'{}'", "\n", ")", "\n", "prem_hits", "=", "count_hits", "(", "prem_masked_pattern", ",", "lm_model", ",", "\n", "k", ",", "tokenizer", ",", "pos_pairs", ",", "1", ",", "\n", "device", ",", "batch_size", ",", "num_masks", ")", "\n", "hypo_masked_pattern", "=", "pattern", ".", "format", "(", "\n", "hypo", "=", "masks", ",", "prem", "=", "'{}'", ")", "\n", "hypo_hits", "=", "count_hits", "(", "hypo_masked_pattern", ",", "lm_model", ",", "\n", "k", ",", "tokenizer", ",", "pos_pairs", ",", "0", ",", "\n", "device", ",", "batch_size", ",", "num_masks", ")", "\n", "hits", "+=", "prem_hits", "\n", "hits", "+=", "hypo_hits", "\n", "\n", "", "return", "hits", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.score_patterns_levy.main": [[114, 149], ["transformers.AutoModelForMaskedLM.from_pretrained", "transformers.AutoTokenizer.from_pretrained", "AutoModelForMaskedLM.from_pretrained.cuda", "tqdm.tqdm", "open", "open", "patterns.sort", "score_patterns_levy.score_pattern", "open", "sorted", "line.strip().split", "prem.split.split", "hypo.split.split", "pos_pairs.append", "patterns.append", "pattern_score.keys", "print", "pat.strip", "line.strip"], "function", ["home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.score_patterns_levy.score_pattern"], ["", "def", "main", "(", "args", ":", "argparse", ".", "Namespace", ")", ":", "\n", "    ", "pos_pairs", "=", "[", "]", "\n", "with", "open", "(", "args", ".", "data_file", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "hypo", ",", "prem", ",", "cls", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "cls", "=", "cls", "==", "'True'", "\n", "prem", "=", "prem", ".", "split", "(", "','", ")", "\n", "hypo", "=", "hypo", ".", "split", "(", "','", ")", "\n", "if", "args", ".", "ent_cls", "!=", "cls", ":", "\n", "                ", "continue", "\n", "", "pos_pairs", ".", "append", "(", "(", "prem", "[", "1", "]", ",", "hypo", "[", "1", "]", ")", ")", "\n", "\n", "", "", "lm_model", "=", "AutoModelForMaskedLM", ".", "from_pretrained", "(", "args", ".", "model_string", ")", "\n", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "args", ".", "model_string", ")", "\n", "\n", "lm_model", ".", "cuda", "(", "args", ".", "gpu", ")", "\n", "\n", "patterns", "=", "[", "]", "\n", "with", "open", "(", "args", ".", "pattern_file", ")", "as", "f", ":", "\n", "        ", "for", "pat", "in", "f", ":", "\n", "            ", "patterns", ".", "append", "(", "pat", ".", "strip", "(", ")", ")", "\n", "", "", "if", "args", ".", "longest_first", ":", "\n", "        ", "patterns", ".", "sort", "(", "key", "=", "len", ",", "reverse", "=", "True", ")", "\n", "\n", "", "pattern_score", "=", "{", "}", "\n", "for", "pat", "in", "tqdm", "(", "patterns", ")", ":", "\n", "        ", "score", "=", "score_pattern", "(", "\n", "pat", ",", "pos_pairs", ",", "lm_model", ",", "tokenizer", ",", "args", ".", "gpu", ",", "\n", "k", "=", "args", ".", "k", ",", "batch_size", "=", "args", ".", "batch_size", ",", "longest_mask_span", "=", "args", ".", "longest_mask_span", "\n", ")", "\n", "pattern_score", "[", "pat", "]", "=", "score", "\n", "\n", "", "with", "open", "(", "args", ".", "scored_pattern_file", ",", "'w'", ")", "as", "fout", ":", "\n", "        ", "for", "pat", "in", "sorted", "(", "pattern_score", ".", "keys", "(", ")", ",", "key", "=", "pattern_score", ".", "__getitem__", ",", "reverse", "=", "True", ")", ":", "\n", "            ", "print", "(", "pattern_score", "[", "pat", "]", ",", "pat", ",", "sep", "=", "'\\t'", ",", "file", "=", "fout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.find_patterns_levy.process_sentence_file": [[10, 52], ["open", "len", "len", "zip", "open", "open", "regex.match", "print", "print", "regex.match.groups", "modified_template.format", "match_storage.append", "modsent_buffer.append"], "function", ["None"], ["def", "process_sentence_file", "(", "\n", "pos_regex", ":", "re", ".", "_pattern_type", ",", "\n", "neg_regex", ":", "re", ".", "_pattern_type", ",", "\n", "files", ":", "Tuple", "[", "str", ",", "str", ",", "str", "]", ",", "\n", "prem_markers", "=", "(", "'*_'", ",", "'_*'", ")", ",", "hypo_markers", "=", "(", "'>>'", ",", "'<<'", ")", ",", "\n", "prem_first", "=", "True", "\n", ")", "->", "Tuple", "[", "int", ",", "int", "]", ":", "\n", "\n", "    ", "if", "prem_first", ":", "\n", "        ", "modified_template", "=", "'{}'", "+", "prem_markers", "[", "0", "]", "+", "'{}'", "+", "prem_markers", "[", "1", "]", "+", "'{}'", "+", "hypo_markers", "[", "0", "]", "+", "'{}'", "+", "hypo_markers", "[", "1", "]", "+", "'{}'", "\n", "", "else", ":", "\n", "        ", "modified_template", "=", "'{}'", "+", "hypo_markers", "[", "0", "]", "+", "'{}'", "+", "hypo_markers", "[", "1", "]", "+", "'{}'", "+", "prem_markers", "[", "0", "]", "+", "'{}'", "+", "prem_markers", "[", "1", "]", "+", "'{}'", "\n", "\n", "", "file_in", ",", "file_out_pos", ",", "file_out_neg", "=", "files", "\n", "pos_matches", ",", "neg_matches", "=", "[", "]", ",", "[", "]", "\n", "with", "open", "(", "file_in", ")", "as", "f", ":", "\n", "        ", "for", "sent", "in", "f", ":", "\n", "            ", "for", "regex", ",", "match_storage", "in", "zip", "(", "(", "pos_regex", ",", "neg_regex", ")", ",", "(", "pos_matches", ",", "neg_matches", ")", ")", ":", "\n", "                ", "match", "=", "regex", ".", "match", "(", "sent", ")", "\n", "if", "match", ":", "\n", "                    ", "modsent_buffer", "=", "[", "]", "\n", "for", "g", "in", "match", ".", "groups", "(", ")", ":", "\n", "                        ", "if", "g", "is", "not", "None", ":", "\n", "                            ", "modsent_buffer", ".", "append", "(", "g", ")", "\n", "", "", "modified_sent", "=", "modified_template", ".", "format", "(", "\n", "*", "modsent_buffer", ")", "\n", "match_storage", ".", "append", "(", "modified_sent", ")", "\n", "\n", "", "", "", "", "if", "pos_matches", ":", "\n", "        ", "with", "open", "(", "file_out_pos", ",", "'w'", ")", "as", "fout", ":", "\n", "            ", "for", "sent", "in", "pos_matches", ":", "\n", "                ", "print", "(", "sent", ",", "file", "=", "fout", ")", "\n", "", "", "", "if", "neg_matches", ":", "\n", "        ", "with", "open", "(", "file_out_neg", ",", "'w'", ")", "as", "fout", ":", "\n", "            ", "for", "sent", "in", "neg_matches", ":", "\n", "                ", "print", "(", "sent", ",", "file", "=", "fout", ")", "\n", "\n", "", "", "", "return", "len", "(", "pos_matches", ")", ",", "len", "(", "neg_matches", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.find_patterns_levy.create_pattern_from_word_pair": [[54, 57], ["None"], "function", ["None"], ["", "def", "create_pattern_from_word_pair", "(", "prem", ":", "str", ",", "hypo", ":", "str", ")", "->", "str", ":", "\n", "# NB: word boundaries at beginning and end of relations", "\n", "    ", "return", "r'(.*)\\b({})\\b(.*\\s+\\S+\\s+.*)\\b({})\\b(.*)'", ".", "format", "(", "prem", ",", "hypo", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.find_patterns_levy.create_pattern_from_dataset": [[59, 76], ["sentence_patterns.append", "find_patterns_levy.create_pattern_from_word_pair", "find_patterns_levy.create_pattern_from_word_pair"], "function", ["home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.find_patterns_levy.create_pattern_from_word_pair", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.find_patterns_levy.create_pattern_from_word_pair"], ["", "def", "create_pattern_from_dataset", "(", "\n", "rows", ":", "Iterable", "[", "Tuple", "[", "str", ",", "str", "]", "]", ",", "prem_first", ":", "bool", "\n", ")", "->", "str", ":", "\n", "    ", "sentence_patterns", "=", "[", "]", "\n", "for", "prem", ",", "hypo", "in", "rows", ":", "\n", "        ", "if", "prem_first", ":", "\n", "            ", "sentence_pattern", "=", "create_pattern_from_word_pair", "(", "\n", "prem", ",", "hypo", "\n", ")", "\n", "", "else", ":", "\n", "            ", "sentence_pattern", "=", "create_pattern_from_word_pair", "(", "\n", "hypo", ",", "prem", "\n", ")", "\n", "\n", "", "sentence_patterns", ".", "append", "(", "sentence_pattern", ")", "\n", "\n", "", "return", "'|'", ".", "join", "(", "sentence_patterns", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.find_patterns_levy.extract_instances_from_dataset": [[78, 92], ["open", "line.strip().split", "prem.split.split", "hypo.split.split", "pos_inst.append", "neg_inst.append", "line.strip"], "function", ["None"], ["", "def", "extract_instances_from_dataset", "(", "\n", "file_name", ":", "str", "\n", ")", "->", "Tuple", "[", "List", "[", "Tuple", "[", "str", ",", "str", "]", "]", ",", "List", "[", "Tuple", "[", "str", ",", "str", "]", "]", "]", ":", "\n", "    ", "pos_inst", ",", "neg_inst", "=", "[", "]", ",", "[", "]", "\n", "with", "open", "(", "file_name", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "hypo", ",", "prem", ",", "cls", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "prem", "=", "prem", ".", "split", "(", "','", ")", "\n", "hypo", "=", "hypo", ".", "split", "(", "','", ")", "\n", "if", "cls", "==", "'True'", ":", "\n", "                ", "pos_inst", ".", "append", "(", "(", "prem", "[", "1", "]", ",", "hypo", "[", "1", "]", ")", ")", "\n", "", "else", ":", "\n", "                ", "neg_inst", ".", "append", "(", "(", "prem", "[", "1", "]", ",", "hypo", "[", "1", "]", ")", ")", "\n", "", "", "", "return", "pos_inst", ",", "neg_inst", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.find_patterns_levy.find_patterns": [[94, 131], ["find_patterns_levy.extract_instances_from_dataset", "find_patterns_levy.create_pattern_from_dataset", "find_patterns_levy.create_pattern_from_dataset", "re.compile", "re.compile", "tqdm.tqdm", "in_fn.split", "os.path.join", "os.path.join", "tqdm.tqdm.append", "multiprocessing.Pool", "pool.imap_unordered", "find_patterns_levy.process_sentence_file", "functools.partial"], "function", ["home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.find_patterns_levy.extract_instances_from_dataset", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.find_patterns_levy.create_pattern_from_dataset", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.find_patterns_levy.create_pattern_from_dataset", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.find_patterns_levy.process_sentence_file"], ["", "def", "find_patterns", "(", "args", ",", "prem_first", ")", ":", "\n", "    ", "pos_inst", ",", "neg_inst", "=", "extract_instances_from_dataset", "(", "args", ".", "dataset_file", ")", "\n", "pos_pattern", "=", "create_pattern_from_dataset", "(", "pos_inst", ",", "prem_first", ")", "\n", "neg_pattern", "=", "create_pattern_from_dataset", "(", "neg_inst", ",", "prem_first", ")", "\n", "pos_regex", "=", "re", ".", "compile", "(", "pos_pattern", ")", "\n", "neg_regex", "=", "re", ".", "compile", "(", "neg_pattern", ")", "\n", "\n", "sentence_file_tuples", "=", "[", "]", "\n", "for", "in_fn", "in", "args", ".", "sentences_in", ":", "\n", "        ", "path_elements", "=", "in_fn", ".", "split", "(", "os", ".", "path", ".", "sep", ")", "\n", "base_fn", "=", "path_elements", "[", "-", "2", "]", "+", "'_'", "+", "path_elements", "[", "-", "1", "]", "\n", "out_fn1", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "out_dir", ",", "base_fn", "+", "'-pos-'", "+", "(", "'prem_first'", "if", "prem_first", "else", "'hypo_first'", ")", ")", "\n", "out_fn2", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "out_dir", ",", "base_fn", "+", "'-neg-'", "+", "(", "'prem_first'", "if", "prem_first", "else", "'hypo_first'", ")", ")", "\n", "sentence_file_tuples", ".", "append", "(", "(", "in_fn", ",", "out_fn1", ",", "out_fn2", ")", ")", "\n", "", "sentence_file_tuples", "=", "tqdm", "(", "sentence_file_tuples", ")", "\n", "\n", "global_pcount", "=", "0", "\n", "global_ncount", "=", "0", "\n", "if", "args", ".", "num_threads", ">", "1", ":", "\n", "        ", "with", "Pool", "(", "processes", "=", "args", ".", "num_threads", ")", "as", "pool", ":", "\n", "            ", "for", "pcount", ",", "ncount", "in", "pool", ".", "imap_unordered", "(", "\n", "partial", "(", "process_sentence_file", ",", "pos_regex", ",", "\n", "neg_regex", ",", "prem_first", "=", "prem_first", ")", ",", "\n", "sentence_file_tuples", "\n", ")", ":", "\n", "                ", "global_pcount", "+=", "pcount", "\n", "global_ncount", "+=", "ncount", "\n", "", "", "", "else", ":", "\n", "        ", "for", "sftup", "in", "sentence_file_tuples", ":", "\n", "            ", "pcount", ",", "ncount", "=", "process_sentence_file", "(", "\n", "pos_regex", ",", "neg_regex", ",", "sftup", ",", "prem_first", "=", "prem_first", ")", "\n", "global_pcount", "+=", "pcount", "\n", "global_ncount", "+=", "ncount", "\n", "\n", "", "", "return", "global_pcount", ",", "global_ncount", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.find_patterns_levy.main": [[133, 140], ["find_patterns_levy.find_patterns", "find_patterns_levy.find_patterns", "print"], "function", ["home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.find_patterns_levy.find_patterns", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.find_patterns_levy.find_patterns"], ["", "def", "main", "(", "args", ":", "argparse", ".", "Namespace", ")", ":", "\n", "    ", "pcount1", ",", "ncount1", "=", "find_patterns", "(", "args", ",", "True", ")", "\n", "pcount2", ",", "ncount2", "=", "find_patterns", "(", "args", ",", "False", ")", "\n", "\n", "print", "(", "\n", "\"Found {} positive and {} negative matches.\"", ".", "format", "(", "\n", "pcount1", "+", "pcount2", ",", "ncount1", "+", "ncount2", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.create_pattern_file.clean_instantiated_pattern": [[9, 41], ["regex.match", "first.startswith", "regex.sub", "len", "print", "print", "print", "exit", "first.startswith", "regex.match.groups", "regex.match.groups", "regex.match.groups", "print", "print", "print", "print", "exit"], "function", ["None"], ["def", "clean_instantiated_pattern", "(", "\n", "pattern", ":", "str", ",", "\n", "regex", ":", "re", ".", "_pattern_type", "\n", ")", "->", "str", ":", "\n", "    ", "match", "=", "regex", ".", "match", "(", "pattern", ")", "\n", "assert", "match", "is", "not", "None", "\n", "\n", "if", "len", "(", "match", ".", "groups", "(", ")", ")", "!=", "10", ":", "\n", "        ", "print", "(", "\"Expected 10 captured groups but did not find them!\"", ")", "\n", "print", "(", "pattern", ")", "\n", "print", "(", "match", ".", "groups", "(", ")", ")", "\n", "exit", "(", "1", ")", "\n", "\n", "", "groups", "=", "[", "g", "for", "g", "in", "match", ".", "groups", "(", ")", "if", "g", "is", "not", "None", "]", "\n", "_", ",", "first", ",", "_", ",", "second", ",", "_", "=", "groups", "\n", "\n", "if", "first", ".", "startswith", "(", "PREM_MARKERS", "[", "0", "]", ")", ":", "\n", "        ", "first", "=", "'{prem}'", "\n", "second", "=", "'{hypo}'", "\n", "subst", "=", "r'\\1'", "+", "first", "+", "r'\\3'", "+", "second", "+", "r'\\5'", "\n", "", "elif", "first", ".", "startswith", "(", "HYPO_MARKERS", "[", "0", "]", ")", ":", "\n", "        ", "second", "=", "'{prem}'", "\n", "first", "=", "'{hypo}'", "\n", "subst", "=", "r'\\6'", "+", "first", "+", "r'\\8'", "+", "second", "+", "r'\\10'", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"Extraction does not start with prem nor hypo markers!\"", ")", "\n", "print", "(", "first", ")", "\n", "print", "(", "pattern", ")", "\n", "print", "(", "groups", ")", "\n", "exit", "(", "1", ")", "\n", "\n", "", "return", "regex", ".", "sub", "(", "subst", ",", "pattern", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.create_pattern_file.precompile_extraction_regex": [[43, 67], ["template.format", "template.format", "re.compile", "re.escape", "re.escape", "re.escape", "re.escape", "re.escape", "re.escape", "re.escape", "re.escape", "re.escape", "re.escape", "re.escape", "re.escape"], "function", ["None"], ["", "def", "precompile_extraction_regex", "(", ")", ":", "\n", "    ", "global", "PREM_MARKERS", "\n", "global", "HYPO_MARKERS", "\n", "\n", "template", "=", "'(.*)({}[^{}]+{})(.*)({}[^{}]+{})(.*)'", "\n", "\n", "prem_first", "=", "template", ".", "format", "(", "\n", "re", ".", "escape", "(", "PREM_MARKERS", "[", "0", "]", ")", ",", "\n", "re", ".", "escape", "(", "PREM_MARKERS", "[", "1", "]", "[", "0", "]", ")", ",", "\n", "re", ".", "escape", "(", "PREM_MARKERS", "[", "1", "]", ")", ",", "\n", "re", ".", "escape", "(", "HYPO_MARKERS", "[", "0", "]", ")", ",", "\n", "re", ".", "escape", "(", "HYPO_MARKERS", "[", "1", "]", "[", "0", "]", ")", ",", "\n", "re", ".", "escape", "(", "HYPO_MARKERS", "[", "1", "]", ")", "\n", ")", "\n", "hypo_first", "=", "template", ".", "format", "(", "\n", "re", ".", "escape", "(", "HYPO_MARKERS", "[", "0", "]", ")", ",", "\n", "re", ".", "escape", "(", "HYPO_MARKERS", "[", "1", "]", "[", "0", "]", ")", ",", "\n", "re", ".", "escape", "(", "HYPO_MARKERS", "[", "1", "]", ")", ",", "\n", "re", ".", "escape", "(", "PREM_MARKERS", "[", "0", "]", ")", ",", "\n", "re", ".", "escape", "(", "PREM_MARKERS", "[", "1", "]", "[", "0", "]", ")", ",", "\n", "re", ".", "escape", "(", "PREM_MARKERS", "[", "1", "]", ")", "\n", ")", "\n", "\n", "return", "re", ".", "compile", "(", "'{}|{}'", ".", "format", "(", "prem_first", ",", "hypo_first", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.create_pattern_file.main": [[69, 89], ["create_pattern_file.precompile_extraction_regex", "tqdm.tqdm", "open", "open", "print", "create_pattern_file.clean_instantiated_pattern", "patterns.append", "line.strip"], "function", ["home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.create_pattern_file.precompile_extraction_regex", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.pattern_creation.create_pattern_file.clean_instantiated_pattern"], ["", "def", "main", "(", "args", ":", "argparse", ".", "Namespace", ")", ":", "\n", "    ", "'''\n    - unify multiple text files with instantiated patterns\n    - identify which is premise and which is hypothesis; and mark them with {prem}/{hypo}\n    - write everything out in a single file\n    '''", "\n", "\n", "regex", "=", "precompile_extraction_regex", "(", ")", "\n", "patterns", "=", "[", "]", "\n", "for", "pat_file", "in", "tqdm", "(", "args", ".", "inst_pat_file", ")", ":", "\n", "        ", "with", "open", "(", "pat_file", ")", "as", "f", ":", "\n", "            ", "for", "line", "in", "f", ":", "\n", "                ", "pattern", "=", "clean_instantiated_pattern", "(", "\n", "line", ".", "strip", "(", ")", ",", "regex", "\n", ")", "\n", "patterns", ".", "append", "(", "pattern", ")", "\n", "\n", "", "", "", "with", "open", "(", "args", ".", "unified_pattern_file", ",", "'w'", ")", "as", "fout", ":", "\n", "        ", "for", "p", "in", "patterns", ":", "\n", "            ", "print", "(", "p", ",", "file", "=", "fout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.analysis.classifications_from_scores.str_float_pair": [[4, 7], ["pair.split", "float"], "function", ["None"], ["def", "str_float_pair", "(", "pair", ")", ":", "\n", "    ", "s", ",", "f", "=", "pair", ".", "split", "(", "','", ")", "\n", "return", "s", ",", "float", "(", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.analysis.classifications_from_scores.main": [[9, 25], ["all", "print", "pred.items", "lines[].strip.split", "truth.append", "print", "open", "f.readlines", "lines[].strip", "int", "float", "range", "len"], "function", ["None"], ["", "def", "main", "(", "args", ")", ":", "\n", "    ", "pred", "=", "{", "}", "\n", "truth", "=", "[", "]", "\n", "for", "fname", ",", "thr", "in", "args", ".", "score_file_with_threshold", ":", "\n", "        ", "with", "open", "(", "fname", ")", "as", "f", ":", "\n", "            ", "lines", "=", "f", ".", "readlines", "(", ")", "\n", "sample", "=", "lines", "[", "args", ".", "sample_no", "+", "1", "]", ".", "strip", "(", ")", "\n", "\n", "", "score", ",", "label", "=", "sample", ".", "split", "(", "'\\t'", ")", "\n", "truth", ".", "append", "(", "int", "(", "label", ")", ")", "\n", "pred", "[", "fname", "]", "=", "1", "if", "float", "(", "score", ")", ">", "thr", "else", "0", "\n", "", "assert", "all", "(", "[", "truth", "[", "i", "]", "==", "truth", "[", "i", "-", "1", "]", "for", "i", "in", "range", "(", "len", "(", "truth", ")", ")", "]", ")", "\n", "\n", "print", "(", "'Truth :'", ",", "truth", "[", "0", "]", ")", "\n", "for", "fname", ",", "pred_value", "in", "pred", ".", "items", "(", ")", ":", "\n", "        ", "print", "(", "fname", ",", "':'", ",", "pred_value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.analysis.errors.with_examples": [[5, 14], ["left.strip().endswith", "left.strip"], "function", ["None"], ["def", "with_examples", "(", "sent_tuple", ",", "A_ex", ",", "B_ex", ")", ":", "\n", "    ", "left", ",", "middle", ",", "right", ",", "end", "=", "sent_tuple", "\n", "if", "left", ".", "strip", "(", ")", ".", "endswith", "(", "'[A]'", ")", ":", "\n", "        ", "left", "=", "A_ex", "\n", "right", "=", "B_ex", "\n", "", "else", ":", "\n", "        ", "right", "=", "A_ex", "\n", "left", "=", "B_ex", "\n", "", "return", "left", ",", "middle", ",", "right", ",", "end", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.analysis.errors.file_thr_pair": [[16, 19], ["arg.split", "float"], "function", ["None"], ["", "def", "file_thr_pair", "(", "arg", ":", "str", ")", ":", "\n", "    ", "fn", ",", "thr", "=", "arg", ".", "split", "(", "','", ")", "\n", "return", "fn", ",", "float", "(", "thr", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.analysis.errors.load_data_levy_holt": [[21, 30], ["open", "line.strip().split", "data.append", "line.strip"], "function", ["None"], ["", "def", "load_data_levy_holt", "(", "data_file", ")", ":", "\n", "    ", "data", "=", "[", "]", "\n", "with", "open", "(", "data_file", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "hypo", ",", "prem", ",", "cls", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "# hypo = hypo.split(',')", "\n", "# prem = prem.split(',')", "\n", "data", ".", "append", "(", "(", "prem", ",", "hypo", ",", "cls", "==", "'True'", ")", ")", "\n", "", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.analysis.errors.load_data_sherliic": [[32, 44], ["open", "csv.reader", "next", "errors.with_examples", "errors.with_examples", "data.append", "row[].split", "row[].split"], "function", ["home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.analysis.errors.with_examples", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.analysis.errors.with_examples"], ["", "def", "load_data_sherliic", "(", "data_file", ")", ":", "\n", "    ", "data", "=", "[", "]", "\n", "with", "open", "(", "data_file", ")", "as", "f", ":", "\n", "        ", "r", "=", "csv", ".", "reader", "(", "f", ")", "\n", "next", "(", "r", ")", "# headers", "\n", "for", "row", "in", "r", ":", "\n", "            ", "A_example", "=", "row", "[", "15", "]", ".", "split", "(", "' / '", ")", "[", "0", "]", "\n", "B_example", "=", "row", "[", "16", "]", ".", "split", "(", "' / '", ")", "[", "0", "]", "\n", "prem", "=", "with_examples", "(", "row", "[", "5", ":", "9", "]", ",", "A_example", ",", "B_example", ")", "\n", "hypo", "=", "with_examples", "(", "row", "[", "9", ":", "13", "]", ",", "A_example", ",", "B_example", ")", "\n", "data", ".", "append", "(", "(", "prem", ",", "hypo", ",", "row", "[", "17", "]", "==", "'yes'", ")", ")", "\n", "", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.analysis.errors.load_scores": [[46, 54], ["open", "next", "line.strip().split", "scores.append", "float", "line.strip"], "function", ["None"], ["", "def", "load_scores", "(", "score_filename", ")", ":", "\n", "    ", "scores", "=", "[", "]", "\n", "with", "open", "(", "score_filename", ")", "as", "f", ":", "\n", "        ", "next", "(", "f", ")", "# headers", "\n", "for", "line", "in", "f", ":", "\n", "            ", "score", ",", "label", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "scores", ".", "append", "(", "float", "(", "score", ")", ")", "\n", "", "", "return", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.analysis.errors.search_errors": [[56, 70], ["enumerate", "fp.sort", "fn.sort", "zip", "fp.append", "fn.append"], "function", ["None"], ["", "def", "search_errors", "(", "data", ",", "scores", ",", "thr", ")", ":", "\n", "    ", "fp", ",", "fn", "=", "[", "]", ",", "[", "]", "\n", "for", "line_no", ",", "(", "inst", ",", "score", ")", "in", "enumerate", "(", "zip", "(", "data", ",", "scores", ")", ")", ":", "\n", "        ", "if", "score", ">", "thr", ":", "\n", "            ", "if", "not", "inst", "[", "-", "1", "]", ":", "\n", "                ", "fp", ".", "append", "(", "(", "inst", ",", "score", ",", "line_no", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "if", "inst", "[", "-", "1", "]", ":", "\n", "                ", "fn", ".", "append", "(", "(", "inst", ",", "score", ",", "line_no", ")", ")", "\n", "\n", "", "", "", "fp", ".", "sort", "(", "key", "=", "lambda", "p", ":", "p", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "fn", ".", "sort", "(", "key", "=", "lambda", "p", ":", "p", "[", "1", "]", ",", "reverse", "=", "False", ")", "\n", "\n", "return", "fp", ",", "fn", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.analysis.errors.get_errors_from_system": [[72, 75], ["errors.load_scores", "errors.search_errors"], "function", ["home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.analysis.errors.load_scores", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.analysis.errors.search_errors"], ["", "def", "get_errors_from_system", "(", "data", ",", "score_fn", ",", "thr", ")", ":", "\n", "    ", "scores", "=", "load_scores", "(", "score_fn", ")", "\n", "return", "search_errors", "(", "data", ",", "scores", ",", "thr", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.analysis.errors.filter_errors": [[77, 88], ["filtered_errors.append"], "function", ["None"], ["", "def", "filter_errors", "(", "errors", ",", "to_be_excluded", ")", ":", "\n", "    ", "filtered_errors", "=", "[", "]", "\n", "for", "dp1", ",", "model_score1", ",", "line_no1", "in", "errors", ":", "\n", "        ", "add_it", "=", "True", "\n", "for", "dp2", ",", "model_score2", ",", "line_no2", "in", "to_be_excluded", ":", "\n", "            ", "if", "dp2", "==", "dp1", ":", "\n", "                ", "add_it", "=", "False", "\n", "break", "\n", "", "", "if", "add_it", ":", "\n", "            ", "filtered_errors", ".", "append", "(", "(", "dp1", ",", "model_score1", ",", "line_no1", ")", ")", "\n", "", "", "return", "filtered_errors", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.analysis.errors.main": [[90, 109], ["errors.get_errors_from_system", "print", "print", "errors.load_data_levy_holt", "errors.load_data_sherliic", "errors.get_errors_from_system", "errors.filter_errors", "errors.filter_errors", "print", "print"], "function", ["home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.analysis.errors.get_errors_from_system", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.analysis.errors.load_data_levy_holt", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.analysis.errors.load_data_sherliic", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.analysis.errors.get_errors_from_system", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.analysis.errors.filter_errors", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.analysis.errors.filter_errors"], ["", "def", "main", "(", "args", ")", ":", "\n", "    ", "if", "args", ".", "levy_holt", ":", "\n", "        ", "data", "=", "load_data_levy_holt", "(", "args", ".", "data_file", ")", "\n", "", "else", ":", "\n", "        ", "data", "=", "load_data_sherliic", "(", "args", ".", "data_file", ")", "\n", "\n", "", "fp", ",", "fn", "=", "get_errors_from_system", "(", "data", ",", "*", "args", ".", "score_file_with_threshold", ")", "\n", "\n", "if", "args", ".", "exclude_errors_from", "is", "not", "None", ":", "\n", "        ", "ex_fp", ",", "ex_fn", "=", "get_errors_from_system", "(", "data", ",", "*", "args", ".", "exclude_errors_from", ")", "\n", "fp", "=", "filter_errors", "(", "fp", ",", "ex_fp", ")", "\n", "fn", "=", "filter_errors", "(", "fn", ",", "ex_fn", ")", "\n", "\n", "", "print", "(", "'=== False Positives ==='", ")", "\n", "for", "e", "in", "fp", ":", "\n", "        ", "print", "(", "e", ")", "\n", "", "print", "(", "'\\n=== False Negative ==='", ")", "\n", "for", "e", "in", "fn", ":", "\n", "        ", "print", "(", "e", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.models.multnat_model.MultNatModel.__init__": [[30, 72], ["pytorch_lightning.LightningModule.__init__", "multnat_model.MultNatModel.save_hyperparameters", "transformers.AutoConfig.from_pretrained", "transformers.AutoTokenizer.from_pretrained", "transformers.AutoModelForSequenceClassification.from_pretrained", "bool"], "methods", ["home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.levy_holt.LevyHoltPattern.__init__"], ["    ", "def", "__init__", "(", "self", ",", "hparams", ":", "Union", "[", "argparse", ".", "Namespace", ",", "\n", "Dict", "[", "str", ",", "Union", "[", "int", ",", "bool", ",", "float", ",", "str", "]", "]", "]", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "save_hyperparameters", "(", "hparams", ")", "\n", "\n", "try", ":", "\n", "            ", "self", ".", "classification_threshold", "=", "self", ".", "hparams", ".", "classification_threshold", "\n", "", "except", "AttributeError", ":", "\n", "            ", "self", ".", "classification_threshold", "=", "0.0", "\n", "\n", "", "try", ":", "\n", "            ", "self", ".", "minimum_precision", "=", "self", ".", "hparams", ".", "minimum_precision", "\n", "", "except", "AttributeError", ":", "\n", "            ", "self", ".", "minimum_precision", "=", "0.5", "\n", "\n", "", "try", ":", "\n", "            ", "self", ".", "curated_auto", "=", "self", ".", "hparams", ".", "curated_auto", "\n", "", "except", "AttributeError", ":", "\n", "            ", "self", ".", "curated_auto", "=", "False", "\n", "\n", "", "cache_dir", "=", "self", ".", "hparams", ".", "cache_dir", "if", "self", ".", "hparams", ".", "cache_dir", "else", "None", "\n", "\n", "self", ".", "config", "=", "AutoConfig", ".", "from_pretrained", "(", "\n", "self", ".", "hparams", ".", "config_name", "\n", "if", "self", ".", "hparams", ".", "config_name", "else", "self", ".", "hparams", ".", "model_name_or_path", ",", "\n", "num_labels", "=", "2", ",", "\n", "cache_dir", "=", "cache_dir", "\n", ")", "\n", "self", ".", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "\n", "self", ".", "hparams", ".", "tokenizer_name", "\n", "if", "self", ".", "hparams", ".", "tokenizer_name", "else", "self", ".", "hparams", ".", "model_name_or_path", ",", "\n", "cache_dir", "=", "cache_dir", ",", "\n", ")", "\n", "self", ".", "model", "=", "AutoModelForSequenceClassification", ".", "from_pretrained", "(", "\n", "self", ".", "hparams", ".", "model_name_or_path", ",", "\n", "from_tf", "=", "bool", "(", "\".ckpt\"", "in", "self", ".", "hparams", ".", "model_name_or_path", ")", ",", "\n", "config", "=", "self", ".", "config", ",", "\n", "cache_dir", "=", "cache_dir", ",", "\n", ")", "\n", "\n", "self", ".", "score_outfile", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.models.multnat_model.MultNatModel.set_classification_threshold": [[73, 75], ["None"], "methods", ["None"], ["", "def", "set_classification_threshold", "(", "self", ",", "thr", ")", ":", "\n", "        ", "self", ".", "classification_threshold", "=", "thr", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.models.multnat_model.MultNatModel.set_minimum_precision": [[76, 78], ["None"], "methods", ["None"], ["", "def", "set_minimum_precision", "(", "self", ",", "min_prec", ")", ":", "\n", "        ", "self", ".", "minimum_precision", "=", "min_prec", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.models.multnat_model.MultNatModel.configure_optimizers": [[79, 113], ["transformers.AdamW", "transformers.get_linear_schedule_with_warmup", "model.named_parameters", "model.named_parameters", "any", "any"], "methods", ["None"], ["", "def", "configure_optimizers", "(", "self", ")", ":", "\n", "        ", "\"Prepare optimizer and schedule (linear warmup and decay)\"", "\n", "model", "=", "self", ".", "model", "\n", "no_decay", "=", "[", "\"bias\"", ",", "\"LayerNorm.weight\"", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "\n", "\"params\"", ":", "[", "\n", "p", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "\n", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "\n", "]", ",", "\n", "\"weight_decay\"", ":", "self", ".", "hparams", ".", "weight_decay", ",", "\n", "}", ",", "\n", "{", "\n", "\"params\"", ":", "[", "\n", "p", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "\n", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "\n", "]", ",", "\n", "\"weight_decay\"", ":", "0.0", ",", "\n", "}", ",", "\n", "]", "\n", "optimizer", "=", "AdamW", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "self", ".", "hparams", ".", "learning_rate", ",", "eps", "=", "self", ".", "hparams", ".", "adam_epsilon", ")", "\n", "self", ".", "opt", "=", "optimizer", "\n", "\n", "scheduler", "=", "get_linear_schedule_with_warmup", "(", "\n", "self", ".", "opt", ",", "num_warmup_steps", "=", "self", ".", "hparams", ".", "warmup_steps", ",", "\n", "num_training_steps", "=", "self", ".", "total_steps", "\n", ")", "\n", "scheduler", "=", "{", "\n", "\"scheduler\"", ":", "scheduler", ",", "\n", "\"interval\"", ":", "\"step\"", ",", "\n", "\"frequency\"", ":", "1", "\n", "}", "\n", "return", "[", "optimizer", "]", ",", "[", "scheduler", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.models.multnat_model.MultNatModel.forward": [[114, 151], ["multnat_model.MultNatModel.model", "losses.append", "list_of_logits.append", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat.detach().unsqueeze", "torch.cat.detach().unsqueeze", "sum", "len", "multnat_model.MultNatModel.model", "losses.append", "list_of_logits.append", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat.detach().unsqueeze", "torch.cat.detach().unsqueeze", "sum", "len", "torch.cat.detach", "torch.cat.detach", "torch.cat.detach", "torch.cat.detach"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "encs", ",", "anti_encs", ",", "labels", ")", ":", "\n", "# length of these lists will be num_sents", "\n", "        ", "losses", ",", "list_of_logits", "=", "[", "]", ",", "[", "]", "\n", "for", "enc", "in", "encs", ":", "\n", "            ", "out", "=", "self", ".", "model", "(", "**", "enc", ",", "labels", "=", "labels", ")", "\n", "loss", ",", "logits", "=", "out", "[", ":", "2", "]", "\n", "losses", ".", "append", "(", "loss", ")", "\n", "list_of_logits", ".", "append", "(", "logits", ".", "detach", "(", ")", ".", "unsqueeze", "(", "1", ")", ")", "\n", "", "if", "losses", ":", "\n", "            ", "loss1", "=", "sum", "(", "losses", ")", "/", "len", "(", "losses", ")", "\n", "# (batch_size, num_sents, num_classes)", "\n", "logits", "=", "torch", ".", "cat", "(", "list_of_logits", ",", "1", ")", "\n", "", "else", ":", "\n", "            ", "loss1", "=", "0.0", "\n", "logits", "=", "None", "\n", "\n", "", "if", "self", ".", "hparams", ".", "use_antipatterns", ":", "\n", "            ", "losses", ",", "list_of_logits", "=", "[", "]", ",", "[", "]", "\n", "for", "anti_enc", "in", "anti_encs", ":", "\n", "                ", "out", "=", "self", ".", "model", "(", "**", "anti_enc", ",", "labels", "=", "1", "-", "labels", ")", "\n", "loss", ",", "anti_logits", "=", "out", "[", ":", "2", "]", "\n", "losses", ".", "append", "(", "loss", ")", "\n", "list_of_logits", ".", "append", "(", "anti_logits", ".", "detach", "(", ")", ".", "unsqueeze", "(", "1", ")", ")", "\n", "", "if", "losses", ":", "\n", "                ", "loss2", "=", "sum", "(", "losses", ")", "/", "len", "(", "losses", ")", "\n", "anti_logits", "=", "torch", ".", "cat", "(", "list_of_logits", ",", "1", ")", "\n", "", "else", ":", "\n", "                ", "loss2", "=", "0.0", "\n", "anti_logits", "=", "None", "\n", "", "", "else", ":", "\n", "            ", "loss2", "=", "0.0", "\n", "anti_logits", "=", "None", "\n", "\n", "", "loss", "=", "loss1", "+", "loss2", "\n", "\n", "# logits.view(bs, num_sents, -1)", "\n", "return", "loss", ",", "logits", ",", "anti_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.models.multnat_model.MultNatModel.training_step": [[152, 158], ["multnat_model.MultNatModel.forward"], "methods", ["home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.models.nli_model.NLIModel.forward"], ["", "def", "training_step", "(", "self", ",", "batch", ",", "batch_idx", ")", ":", "\n", "        ", "enc", ",", "anti_enc", ",", "labels", "=", "batch", "\n", "loss", ",", "logits", ",", "anti_logits", "=", "self", ".", "forward", "(", "enc", ",", "anti_enc", ",", "labels", ")", "\n", "\n", "# return loss", "\n", "return", "{", "'loss'", ":", "loss", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.models.multnat_model.MultNatModel.validation_step": [[159, 177], ["multnat_model.MultNatModel.forward", "torch.softmax", "torch.softmax", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.softmax", "torch.softmax", "torch.max", "torch.max", "torch.max", "torch.max", "loss.detach"], "methods", ["home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.models.nli_model.NLIModel.forward"], ["", "def", "validation_step", "(", "self", ",", "batch", ",", "batch_idx", ")", ":", "\n", "        ", "enc", ",", "anti_enc", ",", "labels", "=", "batch", "\n", "loss", ",", "logits", ",", "anti_logits", "=", "self", ".", "forward", "(", "enc", ",", "anti_enc", ",", "labels", ")", "\n", "\n", "# (batch_size, num_sents, num_classes)", "\n", "probs", "=", "F", ".", "softmax", "(", "logits", ",", "dim", "=", "2", ")", "\n", "# (batch_size,)", "\n", "max_prob", ",", "_", "=", "torch", ".", "max", "(", "probs", "[", ":", ",", ":", ",", "1", "]", ",", "dim", "=", "1", ")", "\n", "\n", "if", "anti_logits", "is", "None", ":", "\n", "            ", "min_prob", ",", "_", "=", "torch", ".", "max", "(", "probs", "[", ":", ",", ":", ",", "0", "]", ",", "dim", "=", "1", ")", "\n", "scores", "=", "max_prob", "-", "min_prob", "# (max_prob > min_prob).long()", "\n", "", "else", ":", "\n", "            ", "anti_probs", "=", "F", ".", "softmax", "(", "anti_logits", ",", "dim", "=", "2", ")", "\n", "anti_max_prob", ",", "_", "=", "torch", ".", "max", "(", "anti_probs", "[", ":", ",", ":", ",", "1", "]", ",", "dim", "=", "1", ")", "\n", "scores", "=", "max_prob", "-", "anti_max_prob", "\n", "\n", "", "return", "{", "'val_loss'", ":", "loss", ".", "detach", "(", ")", ",", "'scores'", ":", "scores", ",", "'truth'", ":", "labels", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.models.multnat_model.MultNatModel.validation_epoch_end": [[178, 206], ["torch.stack().mean", "torch.stack().mean", "torch.stack().mean", "torch.stack().mean", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "pytorch_lightning.metrics.functional.f1_score", "pytorch_lightning.metrics.functional.classification.precision", "pytorch_lightning.metrics.functional.classification.recall", "pytorch_lightning.metrics.functional.classification.precision_recall_curve", "utils.compute_auc", "torch.stack", "torch.stack", "torch.stack", "torch.stack"], "methods", ["home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.models.utils.compute_auc"], ["", "def", "validation_epoch_end", "(", "self", ",", "outputs", ")", ":", "\n", "        ", "val_loss_mean", "=", "torch", ".", "stack", "(", "[", "x", "[", "'val_loss'", "]", "for", "x", "in", "outputs", "]", ")", ".", "mean", "(", ")", "\n", "\n", "scores", "=", "torch", ".", "cat", "(", "[", "x", "[", "'scores'", "]", "for", "x", "in", "outputs", "]", ",", "0", ")", "\n", "truth", "=", "torch", ".", "cat", "(", "[", "x", "[", "'truth'", "]", "for", "x", "in", "outputs", "]", ",", "0", ")", "\n", "\n", "pred", "=", "(", "scores", ">", "self", ".", "classification_threshold", ")", ".", "long", "(", ")", "\n", "\n", "f1_neg", ",", "f1_pos", "=", "f1_score", "(", "pred", ",", "truth", ",", "num_classes", "=", "2", ",", "reduction", "=", "'none'", ")", "\n", "prec_neg", ",", "prec_pos", "=", "precision", "(", "\n", "pred", ",", "truth", ",", "num_classes", "=", "2", ",", "reduction", "=", "'none'", ")", "\n", "rec_neg", ",", "rec_pos", "=", "recall", "(", "\n", "pred", ",", "truth", ",", "num_classes", "=", "2", ",", "reduction", "=", "'none'", ")", "\n", "\n", "prec", ",", "rec", ",", "_", "=", "precision_recall_curve", "(", "scores", ",", "truth", ")", "\n", "area_under_pr_rec_curve", "=", "compute_auc", "(", "\n", "prec", ",", "rec", ",", "\n", "filter_threshold", "=", "self", ".", "minimum_precision", "\n", ")", "\n", "\n", "metrics", "=", "{", "\n", "'F1'", ":", "f1_pos", ",", "'val_loss'", ":", "val_loss_mean", ",", "\n", "'Precision'", ":", "prec_pos", ",", "'Recall'", ":", "rec_pos", ",", "\n", "'AUC'", ":", "area_under_pr_rec_curve", "\n", "}", "\n", "\n", "return", "{", "'val_loss'", ":", "val_loss_mean", ",", "'AUC'", ":", "area_under_pr_rec_curve", ",", "\n", "'F1'", ":", "f1_pos", ",", "'log'", ":", "metrics", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.models.multnat_model.MultNatModel.set_score_outfile": [[207, 211], ["open", "print"], "methods", ["None"], ["", "def", "set_score_outfile", "(", "self", ",", "fname", ")", ":", "\n", "        ", "self", ".", "score_outfile", "=", "fname", "\n", "with", "open", "(", "fname", ",", "'w'", ")", "as", "fout", ":", "\n", "            ", "print", "(", "'score'", ",", "'label'", ",", "sep", "=", "'\\t'", ",", "file", "=", "fout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.models.multnat_model.MultNatModel.test_step": [[212, 219], ["multnat_model.MultNatModel.validation_step", "open", "zip", "print", "s.item", "t.item"], "methods", ["home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.models.nli_model.NLIModel.validation_step"], ["", "", "def", "test_step", "(", "self", ",", "batch", ",", "batch_idx", ")", ":", "\n", "        ", "res", "=", "self", ".", "validation_step", "(", "batch", ",", "batch_idx", ")", "\n", "if", "self", ".", "score_outfile", "is", "not", "None", ":", "\n", "            ", "with", "open", "(", "self", ".", "score_outfile", ",", "'a'", ")", "as", "fout", ":", "\n", "                ", "for", "s", ",", "t", "in", "zip", "(", "res", "[", "'scores'", "]", ",", "res", "[", "'truth'", "]", ")", ":", "\n", "                    ", "print", "(", "s", ".", "item", "(", ")", ",", "t", ".", "item", "(", ")", ",", "sep", "=", "'\\t'", ",", "file", "=", "fout", ")", "\n", "", "", "", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.models.multnat_model.MultNatModel.test_epoch_end": [[220, 224], ["multnat_model.MultNatModel.validation_epoch_end", "print"], "methods", ["home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.models.nli_model.NLIModel.validation_epoch_end"], ["", "def", "test_epoch_end", "(", "self", ",", "outputs", ")", ":", "\n", "        ", "eval_results", "=", "self", ".", "validation_epoch_end", "(", "outputs", ")", "\n", "print", "(", "eval_results", "[", "'log'", "]", ")", "\n", "return", "eval_results", "[", "'log'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.models.multnat_model.MultNatModel.setup": [[225, 299], ["data.levy_holt.LevyHoltPattern", "data.levy_holt.LevyHoltPattern", "data.levy_holt.LevyHoltPattern", "data.sherliic.SherliicPattern", "data.sherliic.SherliicPattern", "data.sherliic.SherliicPattern", "float", "multnat_model.MultNatModel.print", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "len", "max", "len"], "methods", ["None"], ["", "def", "setup", "(", "self", ",", "stage", ")", ":", "\n", "        ", "if", "self", ".", "hparams", ".", "levy_holt", ":", "\n", "            ", "if", "self", ".", "hparams", ".", "augment", ":", "\n", "                ", "self", ".", "print", "(", "\n", "\"WARNING: The Levy/Holt dataset does not support data augmentation.\"", ",", "\n", "file", "=", "sys", ".", "stderr", "\n", ")", "\n", "\n", "", "self", ".", "train_dataset", "=", "LevyHoltPattern", "(", "\n", "os", ".", "path", ".", "join", "(", "self", ".", "hparams", ".", "data_dir", ",", "'levy_holt'", ",", "'train.txt'", ")", ",", "\n", "pattern_file", "=", "self", ".", "hparams", ".", "pattern_file", ",", "\n", "antipattern_file", "=", "self", ".", "hparams", ".", "antipattern_file", ",", "\n", "best_k_patterns", "=", "self", ".", "hparams", ".", "best_k_patterns", ",", "\n", "training", "=", "True", ",", "\n", "curated_auto", "=", "self", ".", "curated_auto", "\n", ")", "\n", "self", ".", "val_dataset", "=", "LevyHoltPattern", "(", "\n", "os", ".", "path", ".", "join", "(", "self", ".", "hparams", ".", "data_dir", ",", "'levy_holt'", ",", "'dev.txt'", ")", ",", "\n", "pattern_file", "=", "self", ".", "hparams", ".", "pattern_file", ",", "\n", "antipattern_file", "=", "self", ".", "hparams", ".", "antipattern_file", ",", "\n", "best_k_patterns", "=", "self", ".", "hparams", ".", "best_k_patterns", ",", "\n", "training", "=", "False", ",", "\n", "curated_auto", "=", "self", ".", "curated_auto", "\n", ")", "\n", "self", ".", "test_dataset", "=", "LevyHoltPattern", "(", "\n", "os", ".", "path", ".", "join", "(", "self", ".", "hparams", ".", "data_dir", ",", "'levy_holt'", ",", "'test.txt'", ")", ",", "\n", "pattern_file", "=", "self", ".", "hparams", ".", "pattern_file", ",", "\n", "antipattern_file", "=", "self", ".", "hparams", ".", "antipattern_file", ",", "\n", "best_k_patterns", "=", "self", ".", "hparams", ".", "best_k_patterns", ",", "\n", "training", "=", "False", ",", "\n", "curated_auto", "=", "self", ".", "curated_auto", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "train_dataset", "=", "SherliicPattern", "(", "\n", "os", ".", "path", ".", "join", "(", "self", ".", "hparams", ".", "data_dir", ",", "'sherliic'", ",", "'train.csv'", ")", ",", "\n", "pattern_idx", "=", "-", "1", ",", "\n", "antipattern_idx", "=", "-", "1", ",", "\n", "with_examples", "=", "True", ",", "\n", "augment", "=", "self", ".", "hparams", ".", "augment", ",", "\n", "pattern_file", "=", "self", ".", "hparams", ".", "pattern_file", ",", "\n", "antipattern_file", "=", "self", ".", "hparams", ".", "antipattern_file", ",", "\n", "best_k_patterns", "=", "self", ".", "hparams", ".", "best_k_patterns", ",", "\n", "training", "=", "True", ",", "\n", "curated_auto", "=", "self", ".", "curated_auto", "\n", ")", "\n", "self", ".", "val_dataset", "=", "SherliicPattern", "(", "\n", "os", ".", "path", ".", "join", "(", "self", ".", "hparams", ".", "data_dir", ",", "'sherliic'", ",", "'dev.csv'", ")", ",", "\n", "pattern_idx", "=", "-", "1", ",", "\n", "antipattern_idx", "=", "-", "1", ",", "\n", "with_examples", "=", "True", ",", "\n", "pattern_file", "=", "self", ".", "hparams", ".", "pattern_file", ",", "\n", "antipattern_file", "=", "self", ".", "hparams", ".", "antipattern_file", ",", "\n", "best_k_patterns", "=", "self", ".", "hparams", ".", "best_k_patterns", ",", "\n", "training", "=", "False", ",", "\n", "curated_auto", "=", "self", ".", "curated_auto", "\n", ")", "\n", "self", ".", "test_dataset", "=", "SherliicPattern", "(", "\n", "os", ".", "path", ".", "join", "(", "self", ".", "hparams", ".", "data_dir", ",", "'sherliic'", ",", "'test.csv'", ")", ",", "\n", "pattern_idx", "=", "-", "1", ",", "\n", "antipattern_idx", "=", "-", "1", ",", "\n", "with_examples", "=", "True", ",", "\n", "pattern_file", "=", "self", ".", "hparams", ".", "pattern_file", ",", "\n", "antipattern_file", "=", "self", ".", "hparams", ".", "antipattern_file", ",", "\n", "best_k_patterns", "=", "self", ".", "hparams", ".", "best_k_patterns", ",", "\n", "training", "=", "False", ",", "\n", "curated_auto", "=", "self", ".", "curated_auto", "\n", ")", "\n", "\n", "", "train_batch_size", "=", "self", ".", "hparams", ".", "train_batch_size", "\n", "self", ".", "total_steps", "=", "(", "\n", "(", "len", "(", "self", ".", "train_dataset", ")", "//", "\n", "(", "train_batch_size", "*", "max", "(", "1", ",", "len", "(", "self", ".", "hparams", ".", "gpus", ")", ")", ")", ")", "\n", "//", "self", ".", "hparams", ".", "accumulate_grad_batches", "\n", "*", "float", "(", "self", ".", "hparams", ".", "max_epochs", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.models.multnat_model.MultNatModel.encode_batch_of_sentences": [[301, 318], ["range", "len", "restructured_sents.append", "sents_enc.append", "new_list.append", "multnat_model.MultNatModel.tokenizer"], "methods", ["None"], ["", "def", "encode_batch_of_sentences", "(", "self", ",", "sents", ":", "List", "[", "List", "[", "str", "]", "]", ")", "->", "List", "[", "BatchEncoding", "]", ":", "\n", "        ", "restructured_sents", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "sents", "[", "0", "]", ")", ")", ":", "\n", "            ", "new_list", "=", "[", "]", "\n", "for", "slist", "in", "sents", ":", "\n", "                ", "new_list", ".", "append", "(", "slist", "[", "i", "]", ")", "\n", "", "restructured_sents", ".", "append", "(", "new_list", ")", "\n", "\n", "", "sents_enc", "=", "[", "]", "\n", "for", "batch_sized_slist", "in", "restructured_sents", ":", "\n", "            ", "sents_enc", ".", "append", "(", "\n", "self", ".", "tokenizer", "(", "\n", "batch_sized_slist", ",", "truncation", "=", "True", ",", "\n", "padding", "=", "True", ",", "return_tensors", "=", "'pt'", "\n", ")", "\n", ")", "\n", "", "return", "sents_enc", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.models.multnat_model.MultNatModel.collate": [[319, 327], ["map", "multnat_model.MultNatModel.encode_batch_of_sentences", "multnat_model.MultNatModel.encode_batch_of_sentences", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "zip"], "methods", ["home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.models.multnat_model.MultNatModel.encode_batch_of_sentences", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.models.multnat_model.MultNatModel.encode_batch_of_sentences"], ["", "def", "collate", "(", "self", ",", "samples", ")", ":", "\n", "        ", "sents", ",", "antisents", ",", "labels", "=", "map", "(", "list", ",", "zip", "(", "*", "samples", ")", ")", "\n", "\n", "sents_enc", "=", "self", ".", "encode_batch_of_sentences", "(", "sents", ")", "\n", "antisents_enc", "=", "self", ".", "encode_batch_of_sentences", "(", "antisents", ")", "\n", "\n", "label_tensor", "=", "torch", ".", "LongTensor", "(", "labels", ")", "\n", "return", "sents_enc", ",", "antisents_enc", ",", "label_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.models.multnat_model.MultNatModel.train_dataloader": [[328, 336], ["torch.utils.data.DataLoader", "torch.utils.data.DataLoader"], "methods", ["None"], ["", "def", "train_dataloader", "(", "self", ")", ":", "\n", "        ", "return", "DataLoader", "(", "\n", "self", ".", "train_dataset", ",", "\n", "batch_size", "=", "self", ".", "hparams", ".", "train_batch_size", ",", "\n", "num_workers", "=", "self", ".", "hparams", ".", "num_workers", ",", "\n", "collate_fn", "=", "self", ".", "collate", ",", "\n", "pin_memory", "=", "True", ",", "\n", "shuffle", "=", "True", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.models.multnat_model.MultNatModel.val_dataloader": [[338, 345], ["torch.utils.data.DataLoader", "torch.utils.data.DataLoader"], "methods", ["None"], ["", "def", "val_dataloader", "(", "self", ")", ":", "\n", "        ", "return", "DataLoader", "(", "\n", "self", ".", "val_dataset", ",", "\n", "batch_size", "=", "self", ".", "hparams", ".", "eval_batch_size", ",", "\n", "num_workers", "=", "self", ".", "hparams", ".", "num_workers", ",", "\n", "collate_fn", "=", "self", ".", "collate", ",", "\n", "pin_memory", "=", "True", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.models.multnat_model.MultNatModel.test_dataloader": [[347, 354], ["torch.utils.data.DataLoader", "torch.utils.data.DataLoader"], "methods", ["None"], ["", "def", "test_dataloader", "(", "self", ")", ":", "\n", "        ", "return", "DataLoader", "(", "\n", "self", ".", "test_dataset", ",", "\n", "batch_size", "=", "self", ".", "hparams", ".", "eval_batch_size", ",", "\n", "num_workers", "=", "self", ".", "hparams", ".", "num_workers", ",", "\n", "collate_fn", "=", "self", ".", "collate", ",", "\n", "pin_memory", "=", "True", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.models.multnat_model.MultNatModel.add_model_specific_args": [[356, 413], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "os.path.join", "os.getcwd"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_model_specific_args", "(", "parser", ")", ":", "\n", "        ", "parser", ".", "add_argument", "(", "\n", "\"--model_name_or_path\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"Path to pretrained model or model identifier from huggingface.co/models\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--config_name\"", ",", "default", "=", "\"\"", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Pretrained config name or path if not the same as model_name\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--tokenizer_name\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"Pretrained tokenizer name or path if not the same as model_name\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--cache_dir\"", ",", "\n", "default", "=", "\"\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"Where do you want to store the pre-trained models downloaded from s3\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--data_dir\"", ",", "default", "=", "os", ".", "path", ".", "join", "(", "os", ".", "getcwd", "(", ")", ",", "'data'", ")", ",", "type", "=", "str", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\"--learning_rate\"", ",", "default", "=", "5e-5", ",", "\n", "type", "=", "float", ",", "help", "=", "\"The initial learning rate for Adam.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--weight_decay\"", ",", "default", "=", "0.0", ",", "\n", "type", "=", "float", ",", "help", "=", "\"Weight decay if we apply some.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--adam_epsilon\"", ",", "default", "=", "1e-8", ",", "\n", "type", "=", "float", ",", "help", "=", "\"Epsilon for Adam optimizer.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--warmup_steps\"", ",", "default", "=", "0", ",", "\n", "type", "=", "int", ",", "help", "=", "\"Linear warmup over warmup_steps.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_workers\"", ",", "default", "=", "2", ",", "\n", "type", "=", "int", ",", "help", "=", "\"kwarg passed to DataLoader\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_train_epochs\"", ",", "\n", "dest", "=", "\"max_epochs\"", ",", "default", "=", "3", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "\"--train_batch_size\"", ",", "default", "=", "20", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "\"--eval_batch_size\"", ",", "default", "=", "10", ",", "type", "=", "int", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--classification_threshold\"", ",", "\n", "default", "=", "0.0", ",", "type", "=", "float", ")", "\n", "parser", ".", "add_argument", "(", "\"--minimum_precision\"", ",", "default", "=", "0.5", ",", "type", "=", "float", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--use_antipatterns\"", ",", "action", "=", "'store_true'", ")", "\n", "parser", ".", "add_argument", "(", "\"--augment\"", ",", "action", "=", "'store_true'", ")", "\n", "parser", ".", "add_argument", "(", "\"--pattern_file\"", ",", "default", "=", "None", ")", "\n", "parser", ".", "add_argument", "(", "\"--antipattern_file\"", ",", "default", "=", "None", ")", "\n", "parser", ".", "add_argument", "(", "\"--best_k_patterns\"", ",", "type", "=", "int", ",", "default", "=", "100", ")", "\n", "parser", ".", "add_argument", "(", "\"--curated_auto\"", ",", "action", "=", "'store_true'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--levy_holt\"", ",", "action", "=", "'store_true'", ")", "\n", "\n", "return", "parser", "\n", "", "", ""]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.models.nli_model.NLIModel.__init__": [[28, 66], ["pytorch_lightning.LightningModule.__init__", "nli_model.NLIModel.save_hyperparameters", "transformers.AutoConfig.from_pretrained", "transformers.AutoTokenizer.from_pretrained", "transformers.AutoModelForSequenceClassification.from_pretrained", "bool"], "methods", ["home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.levy_holt.LevyHoltPattern.__init__"], ["    ", "def", "__init__", "(", "self", ",", "hparams", ":", "argparse", ".", "Namespace", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "save_hyperparameters", "(", "hparams", ")", "\n", "\n", "try", ":", "\n", "            ", "self", ".", "classification_threshold", "=", "self", ".", "hparams", ".", "classification_threshold", "\n", "", "except", "AttributeError", ":", "\n", "            ", "self", ".", "classification_threshold", "=", "0.5", "\n", "\n", "", "try", ":", "\n", "            ", "self", ".", "minimum_precision", "=", "self", ".", "hparams", ".", "minimum_precision", "\n", "", "except", "AttributeError", ":", "\n", "            ", "self", ".", "minimum_precision", "=", "0.5", "\n", "\n", "", "self", ".", "step_count", "=", "0", "\n", "self", ".", "tfmr_ckpts", "=", "{", "}", "\n", "cache_dir", "=", "self", ".", "hparams", ".", "cache_dir", "if", "self", ".", "hparams", ".", "cache_dir", "else", "None", "\n", "\n", "self", ".", "config", "=", "AutoConfig", ".", "from_pretrained", "(", "\n", "self", ".", "hparams", ".", "config_name", "\n", "if", "self", ".", "hparams", ".", "config_name", "else", "self", ".", "hparams", ".", "model_name_or_path", ",", "\n", "num_labels", "=", "2", ",", "\n", "cache_dir", "=", "cache_dir", "\n", ")", "\n", "self", ".", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "\n", "self", ".", "hparams", ".", "tokenizer_name", "\n", "if", "self", ".", "hparams", ".", "tokenizer_name", "else", "self", ".", "hparams", ".", "model_name_or_path", ",", "\n", "cache_dir", "=", "cache_dir", ",", "\n", ")", "\n", "self", ".", "model", "=", "AutoModelForSequenceClassification", ".", "from_pretrained", "(", "\n", "self", ".", "hparams", ".", "model_name_or_path", ",", "\n", "from_tf", "=", "bool", "(", "\".ckpt\"", "in", "self", ".", "hparams", ".", "model_name_or_path", ")", ",", "\n", "config", "=", "self", ".", "config", ",", "\n", "cache_dir", "=", "cache_dir", ",", "\n", ")", "\n", "\n", "self", ".", "score_outfile", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.models.nli_model.NLIModel.set_classification_threshold": [[67, 69], ["None"], "methods", ["None"], ["", "def", "set_classification_threshold", "(", "self", ",", "thr", ")", ":", "\n", "        ", "self", ".", "classification_threshold", "=", "thr", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.models.nli_model.NLIModel.set_minimum_precision": [[70, 72], ["None"], "methods", ["None"], ["", "def", "set_minimum_precision", "(", "self", ",", "min_prec", ")", ":", "\n", "        ", "self", ".", "minimum_precision", "=", "min_prec", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.models.nli_model.NLIModel.configure_optimizers": [[73, 107], ["transformers.AdamW", "transformers.get_linear_schedule_with_warmup", "model.named_parameters", "model.named_parameters", "any", "any"], "methods", ["None"], ["", "def", "configure_optimizers", "(", "self", ")", ":", "\n", "        ", "\"Prepare optimizer and schedule (linear warmup and decay)\"", "\n", "model", "=", "self", ".", "model", "\n", "no_decay", "=", "[", "\"bias\"", ",", "\"LayerNorm.weight\"", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "\n", "\"params\"", ":", "[", "\n", "p", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "\n", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "\n", "]", ",", "\n", "\"weight_decay\"", ":", "self", ".", "hparams", ".", "weight_decay", ",", "\n", "}", ",", "\n", "{", "\n", "\"params\"", ":", "[", "\n", "p", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "\n", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "\n", "]", ",", "\n", "\"weight_decay\"", ":", "0.0", ",", "\n", "}", ",", "\n", "]", "\n", "optimizer", "=", "AdamW", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "self", ".", "hparams", ".", "learning_rate", ",", "eps", "=", "self", ".", "hparams", ".", "adam_epsilon", ")", "\n", "self", ".", "opt", "=", "optimizer", "\n", "\n", "scheduler", "=", "get_linear_schedule_with_warmup", "(", "\n", "self", ".", "opt", ",", "num_warmup_steps", "=", "self", ".", "hparams", ".", "warmup_steps", ",", "\n", "num_training_steps", "=", "self", ".", "total_steps", "\n", ")", "\n", "scheduler", "=", "{", "\n", "\"scheduler\"", ":", "scheduler", ",", "\n", "\"interval\"", ":", "\"step\"", ",", "\n", "\"frequency\"", ":", "1", "\n", "}", "\n", "return", "[", "optimizer", "]", ",", "[", "scheduler", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.models.nli_model.NLIModel.forward": [[108, 112], ["nli_model.NLIModel.model"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "encoded", ",", "labels", ")", ":", "\n", "        ", "outputs", "=", "self", ".", "model", "(", "**", "encoded", ",", "labels", "=", "labels", ")", "\n", "loss", ",", "logits", "=", "outputs", "[", ":", "2", "]", "\n", "return", "loss", ",", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.models.nli_model.NLIModel.training_step": [[113, 117], ["nli_model.NLIModel.forward"], "methods", ["home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.models.nli_model.NLIModel.forward"], ["", "def", "training_step", "(", "self", ",", "batch", ",", "batch_idx", ")", ":", "\n", "        ", "encoded", ",", "labels", "=", "batch", "\n", "loss", ",", "logits", "=", "self", ".", "forward", "(", "encoded", ",", "labels", ")", "\n", "return", "{", "'loss'", ":", "loss", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.models.nli_model.NLIModel.validation_step": [[118, 125], ["nli_model.NLIModel.forward", "torch.softmax", "torch.softmax", "logits.detach", "loss.detach"], "methods", ["home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.models.nli_model.NLIModel.forward"], ["", "def", "validation_step", "(", "self", ",", "batch", ",", "batch_idx", ")", ":", "\n", "        ", "encoded", ",", "labels", "=", "batch", "\n", "loss", ",", "logits", "=", "self", ".", "forward", "(", "encoded", ",", "labels", ")", "\n", "\n", "scores", "=", "F", ".", "softmax", "(", "logits", ".", "detach", "(", ")", ",", "dim", "=", "1", ")", "\n", "\n", "return", "{", "'val_loss'", ":", "loss", ".", "detach", "(", ")", ",", "'scores'", ":", "scores", ",", "'truth'", ":", "labels", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.models.nli_model.NLIModel.validation_epoch_end": [[126, 154], ["torch.stack().mean", "torch.stack().mean", "torch.stack().mean", "torch.stack().mean", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "pytorch_lightning.metrics.functional.f1_score", "pytorch_lightning.metrics.functional.classification.precision", "pytorch_lightning.metrics.functional.classification.recall", "pytorch_lightning.metrics.functional.classification.precision_recall_curve", "utils.compute_auc", "torch.stack", "torch.stack", "torch.stack", "torch.stack"], "methods", ["home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.models.utils.compute_auc"], ["", "def", "validation_epoch_end", "(", "self", ",", "outputs", ")", ":", "\n", "        ", "val_loss_mean", "=", "torch", ".", "stack", "(", "[", "x", "[", "'val_loss'", "]", "for", "x", "in", "outputs", "]", ")", ".", "mean", "(", ")", "\n", "\n", "scores", "=", "torch", ".", "cat", "(", "[", "x", "[", "'scores'", "]", "for", "x", "in", "outputs", "]", ",", "0", ")", "\n", "truth", "=", "torch", ".", "cat", "(", "[", "x", "[", "'truth'", "]", "for", "x", "in", "outputs", "]", ")", "\n", "\n", "pred", "=", "(", "scores", "[", ":", ",", "1", "]", ">", "self", ".", "classification_threshold", ")", ".", "long", "(", ")", "\n", "\n", "f1_neg", ",", "f1_pos", "=", "f1_score", "(", "pred", ",", "truth", ",", "num_classes", "=", "2", ",", "reduction", "=", "'none'", ")", "\n", "prec_neg", ",", "prec_pos", "=", "precision", "(", "\n", "pred", ",", "truth", ",", "num_classes", "=", "2", ",", "reduction", "=", "'none'", ")", "\n", "rec_neg", ",", "rec_pos", "=", "recall", "(", "\n", "pred", ",", "truth", ",", "num_classes", "=", "2", ",", "reduction", "=", "'none'", ")", "\n", "\n", "prec", ",", "rec", ",", "_", "=", "precision_recall_curve", "(", "scores", "[", ":", ",", "1", "]", ",", "truth", ")", "\n", "area_under_pr_rec_curve", "=", "compute_auc", "(", "\n", "prec", ",", "rec", ",", "\n", "filter_threshold", "=", "self", ".", "minimum_precision", "\n", ")", "\n", "\n", "metrics", "=", "{", "\n", "'F1'", ":", "f1_pos", ",", "'val_loss'", ":", "val_loss_mean", ",", "\n", "'Precision'", ":", "prec_pos", ",", "'Recall'", ":", "rec_pos", ",", "\n", "'AUC'", ":", "area_under_pr_rec_curve", "\n", "}", "\n", "\n", "return", "{", "'val_loss'", ":", "val_loss_mean", ",", "'AUC'", ":", "area_under_pr_rec_curve", ",", "\n", "'F1'", ":", "f1_pos", ",", "'log'", ":", "metrics", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.models.nli_model.NLIModel.set_score_outfile": [[155, 159], ["open", "print"], "methods", ["None"], ["", "def", "set_score_outfile", "(", "self", ",", "fname", ")", ":", "\n", "        ", "self", ".", "score_outfile", "=", "fname", "\n", "with", "open", "(", "fname", ",", "'w'", ")", "as", "fout", ":", "\n", "            ", "print", "(", "'score'", ",", "'label'", ",", "sep", "=", "'\\t'", ",", "file", "=", "fout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.models.nli_model.NLIModel.test_step": [[160, 167], ["nli_model.NLIModel.validation_step", "open", "zip", "print", "s.item", "t.item"], "methods", ["home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.models.nli_model.NLIModel.validation_step"], ["", "", "def", "test_step", "(", "self", ",", "batch", ",", "batch_idx", ")", ":", "\n", "        ", "res", "=", "self", ".", "validation_step", "(", "batch", ",", "batch_idx", ")", "\n", "if", "self", ".", "score_outfile", "is", "not", "None", ":", "\n", "            ", "with", "open", "(", "self", ".", "score_outfile", ",", "'a'", ")", "as", "fout", ":", "\n", "                ", "for", "s", ",", "t", "in", "zip", "(", "res", "[", "'scores'", "]", "[", ":", ",", "1", "]", ",", "res", "[", "'truth'", "]", ")", ":", "\n", "                    ", "print", "(", "s", ".", "item", "(", ")", ",", "t", ".", "item", "(", ")", ",", "sep", "=", "'\\t'", ",", "file", "=", "fout", ")", "\n", "", "", "", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.models.nli_model.NLIModel.test_epoch_end": [[168, 172], ["nli_model.NLIModel.validation_epoch_end", "print"], "methods", ["home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.models.nli_model.NLIModel.validation_epoch_end"], ["", "def", "test_epoch_end", "(", "self", ",", "outputs", ")", ":", "\n", "        ", "eval_results", "=", "self", ".", "validation_epoch_end", "(", "outputs", ")", "\n", "print", "(", "eval_results", "[", "'log'", "]", ")", "\n", "return", "eval_results", "[", "'log'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.models.nli_model.NLIModel.setup": [[173, 213], ["data.levy_holt.LevyHoltSentences", "data.levy_holt.LevyHoltSentences", "data.levy_holt.LevyHoltSentences", "data.sherliic.SherliicSentences", "data.sherliic.SherliicSentences", "data.sherliic.SherliicSentences", "float", "nli_model.NLIModel.print", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "len", "max", "len"], "methods", ["None"], ["", "def", "setup", "(", "self", ",", "stage", ")", ":", "\n", "        ", "if", "self", ".", "hparams", ".", "levy_holt", ":", "\n", "            ", "if", "self", ".", "hparams", ".", "augment", ":", "\n", "                ", "self", ".", "print", "(", "\n", "\"WARNING: The Levy/Holt dataset does not support data augmentation.\"", ",", "\n", "file", "=", "sys", ".", "stderr", "\n", ")", "\n", "\n", "", "self", ".", "train_dataset", "=", "LevyHoltSentences", "(", "\n", "os", ".", "path", ".", "join", "(", "self", ".", "hparams", ".", "data_dir", ",", "'levy_holt'", ",", "'train.txt'", ")", "\n", ")", "\n", "self", ".", "val_dataset", "=", "LevyHoltSentences", "(", "\n", "os", ".", "path", ".", "join", "(", "self", ".", "hparams", ".", "data_dir", ",", "'levy_holt'", ",", "'dev.txt'", ")", "\n", ")", "\n", "self", ".", "test_dataset", "=", "LevyHoltSentences", "(", "\n", "os", ".", "path", ".", "join", "(", "self", ".", "hparams", ".", "data_dir", ",", "'levy_holt'", ",", "'test.txt'", ")", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "train_dataset", "=", "SherliicSentences", "(", "\n", "os", ".", "path", ".", "join", "(", "self", ".", "hparams", ".", "data_dir", ",", "'sherliic'", ",", "'train.csv'", ")", ",", "\n", "with_examples", "=", "True", ",", "\n", "augment", "=", "self", ".", "hparams", ".", "augment", "\n", ")", "\n", "self", ".", "val_dataset", "=", "SherliicSentences", "(", "\n", "os", ".", "path", ".", "join", "(", "self", ".", "hparams", ".", "data_dir", ",", "'sherliic'", ",", "'dev.csv'", ")", ",", "\n", "with_examples", "=", "True", ",", "\n", "augment", "=", "False", "\n", ")", "\n", "self", ".", "test_dataset", "=", "SherliicSentences", "(", "\n", "os", ".", "path", ".", "join", "(", "self", ".", "hparams", ".", "data_dir", ",", "'sherliic'", ",", "'test.csv'", ")", ",", "\n", "with_examples", "=", "True", ",", "\n", "augment", "=", "False", "\n", ")", "\n", "\n", "", "train_batch_size", "=", "self", ".", "hparams", ".", "train_batch_size", "\n", "self", ".", "total_steps", "=", "(", "\n", "(", "len", "(", "self", ".", "train_dataset", ")", "//", "\n", "(", "train_batch_size", "*", "max", "(", "1", ",", "len", "(", "self", ".", "hparams", ".", "gpus", ")", ")", ")", ")", "\n", "//", "self", ".", "hparams", ".", "accumulate_grad_batches", "\n", "*", "float", "(", "self", ".", "hparams", ".", "max_epochs", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.models.nli_model.NLIModel.collate": [[215, 223], ["map", "nli_model.NLIModel.tokenizer", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "zip"], "methods", ["None"], ["", "def", "collate", "(", "self", ",", "samples", ")", ":", "\n", "        ", "prems", ",", "hypos", ",", "labels", "=", "map", "(", "list", ",", "zip", "(", "*", "samples", ")", ")", "\n", "encoded", "=", "self", ".", "tokenizer", "(", "\n", "prems", ",", "hypos", ",", "\n", "padding", "=", "True", ",", "return_tensors", "=", "'pt'", "\n", ")", "\n", "label_tensor", "=", "torch", ".", "LongTensor", "(", "labels", ")", "\n", "return", "encoded", ",", "label_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.models.nli_model.NLIModel.train_dataloader": [[224, 232], ["torch.utils.data.DataLoader", "torch.utils.data.DataLoader"], "methods", ["None"], ["", "def", "train_dataloader", "(", "self", ")", ":", "\n", "        ", "return", "DataLoader", "(", "\n", "self", ".", "train_dataset", ",", "\n", "batch_size", "=", "self", ".", "hparams", ".", "train_batch_size", ",", "\n", "num_workers", "=", "self", ".", "hparams", ".", "num_workers", ",", "\n", "collate_fn", "=", "self", ".", "collate", ",", "\n", "pin_memory", "=", "True", ",", "\n", "shuffle", "=", "True", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.models.nli_model.NLIModel.val_dataloader": [[234, 241], ["torch.utils.data.DataLoader", "torch.utils.data.DataLoader"], "methods", ["None"], ["", "def", "val_dataloader", "(", "self", ")", ":", "\n", "        ", "return", "DataLoader", "(", "\n", "self", ".", "val_dataset", ",", "\n", "batch_size", "=", "self", ".", "hparams", ".", "eval_batch_size", ",", "\n", "num_workers", "=", "self", ".", "hparams", ".", "num_workers", ",", "\n", "collate_fn", "=", "self", ".", "collate", ",", "\n", "pin_memory", "=", "True", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.models.nli_model.NLIModel.test_dataloader": [[243, 250], ["torch.utils.data.DataLoader", "torch.utils.data.DataLoader"], "methods", ["None"], ["", "def", "test_dataloader", "(", "self", ")", ":", "\n", "        ", "return", "DataLoader", "(", "\n", "self", ".", "test_dataset", ",", "\n", "batch_size", "=", "self", ".", "hparams", ".", "eval_batch_size", ",", "\n", "num_workers", "=", "self", ".", "hparams", ".", "num_workers", ",", "\n", "collate_fn", "=", "self", ".", "collate", ",", "\n", "pin_memory", "=", "True", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.models.nli_model.NLIModel.add_model_specific_args": [[252, 304], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "os.path.join", "os.getcwd"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_model_specific_args", "(", "parser", ")", ":", "\n", "        ", "parser", ".", "add_argument", "(", "\n", "\"--model_name_or_path\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"Path to pretrained model or model identifier from huggingface.co/models\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--config_name\"", ",", "default", "=", "\"\"", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Pretrained config name or path if not the same as model_name\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--tokenizer_name\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"Pretrained tokenizer name or path if not the same as model_name\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--cache_dir\"", ",", "\n", "default", "=", "\"\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"Where do you want to store the pre-trained models downloaded from s3\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--data_dir\"", ",", "default", "=", "os", ".", "path", ".", "join", "(", "os", ".", "getcwd", "(", ")", ",", "'data'", ")", ",", "type", "=", "str", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\"--learning_rate\"", ",", "default", "=", "5e-5", ",", "\n", "type", "=", "float", ",", "help", "=", "\"The initial learning rate for Adam.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--weight_decay\"", ",", "default", "=", "0.0", ",", "\n", "type", "=", "float", ",", "help", "=", "\"Weight decay if we apply some.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--adam_epsilon\"", ",", "default", "=", "1e-8", ",", "\n", "type", "=", "float", ",", "help", "=", "\"Epsilon for Adam optimizer.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--warmup_steps\"", ",", "default", "=", "0", ",", "\n", "type", "=", "int", ",", "help", "=", "\"Linear warmup over warmup_steps.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_workers\"", ",", "default", "=", "2", ",", "\n", "type", "=", "int", ",", "help", "=", "\"kwarg passed to DataLoader\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_train_epochs\"", ",", "\n", "dest", "=", "\"max_epochs\"", ",", "default", "=", "3", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "\"--train_batch_size\"", ",", "default", "=", "32", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "\"--eval_batch_size\"", ",", "default", "=", "32", ",", "type", "=", "int", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--augment\"", ",", "action", "=", "'store_true'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--minimum_precision\"", ",", "type", "=", "float", ",", "default", "=", "0.5", ")", "\n", "parser", ".", "add_argument", "(", "\"--classification_threshold\"", ",", "\n", "type", "=", "float", ",", "default", "=", "0.5", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--levy_holt\"", ",", "action", "=", "'store_true'", ")", "\n", "\n", "return", "parser", "\n", "", "", ""]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.models.utils.compute_auc": [[5, 16], ["zip", "pytorch_lightning.metrics.functional.auc", "torch.cat", "torch.cat", "xs.append", "ys.append", "x.unsqueeze", "y.unsqueeze"], "function", ["None"], ["def", "compute_auc", "(", "precisions", ":", "torch", ".", "FloatTensor", ",", "recalls", ":", "torch", ".", "FloatTensor", ",", "\n", "filter_threshold", ":", "float", "=", "0.5", ")", "->", "torch", ".", "FloatTensor", ":", "\n", "    ", "xs", ",", "ys", "=", "[", "]", ",", "[", "]", "\n", "for", "p", ",", "r", "in", "zip", "(", "precisions", ",", "recalls", ")", ":", "\n", "        ", "if", "p", ">=", "filter_threshold", ":", "\n", "            ", "xs", ".", "append", "(", "r", ")", "\n", "ys", ".", "append", "(", "p", ")", "\n", "\n", "", "", "return", "auc", "(", "\n", "torch", ".", "cat", "(", "[", "x", ".", "unsqueeze", "(", "0", ")", "for", "x", "in", "xs", "]", ",", "0", ")", ",", "\n", "torch", ".", "cat", "(", "[", "y", ".", "unsqueeze", "(", "0", ")", "for", "y", "in", "ys", "]", ",", "0", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.train.tune_threshold.main": [[5, 24], ["sklearn.metrics.precision_recall_curve", "zip", "max", "print", "open", "next", "thr2scores.keys", "line.strip().split", "scores.append", "labels.append", "float", "int", "line.strip"], "function", ["None"], ["def", "main", "(", "args", ")", ":", "\n", "    ", "scores", ",", "labels", "=", "[", "]", ",", "[", "]", "\n", "with", "open", "(", "args", ".", "score_file", ")", "as", "f", ":", "\n", "        ", "next", "(", "f", ")", "# headers", "\n", "for", "line", "in", "f", ":", "\n", "            ", "score", ",", "label", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "scores", ".", "append", "(", "float", "(", "score", ")", ")", "\n", "labels", ".", "append", "(", "int", "(", "label", ")", ")", "\n", "\n", "", "", "prec_rec_thr", "=", "precision_recall_curve", "(", "labels", ",", "scores", ",", "pos_label", "=", "1", ")", "\n", "thr2scores", "=", "{", "}", "\n", "for", "p", ",", "r", ",", "t", "in", "zip", "(", "*", "prec_rec_thr", ")", ":", "\n", "        ", "f1", "=", "2", "*", "p", "*", "r", "/", "(", "p", "+", "r", ")", "\n", "thr2scores", "[", "t", "]", "=", "(", "p", ",", "r", ",", "f1", ")", "\n", "", "best_thr", "=", "max", "(", "thr2scores", ".", "keys", "(", ")", ",", "key", "=", "lambda", "t", ":", "thr2scores", "[", "t", "]", "[", "2", "]", ")", "\n", "\n", "print", "(", "\n", "\"Best threshold is {} with P/R/F1 scores of {}/{}/{}.\"", ".", "format", "(", "\n", "best_thr", ",", "*", "thr2scores", "[", "best_thr", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.train.utils.add_generic_args": [[13, 53], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "function", ["None"], ["", "", "return", "auc", "(", "\n", "torch", ".", "cat", "(", "[", "x", ".", "unsqueeze", "(", "0", ")", "for", "x", "in", "xs", "]", ",", "0", ")", ",", "\n", "torch", ".", "cat", "(", "[", "y", ".", "unsqueeze", "(", "0", ")", "for", "y", "in", "ys", "]", ",", "0", ")", "\n", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.train.utils.generic_train": [[55, 96], ["pytorch_lightning.seed_everything", "model_cls", "pathlib.Path", "pathlib.Path.mkdir", "pytorch_lightning.callbacks.ModelCheckpoint", "pytorch_lightning.loggers.test_tube.TestTubeLogger", "pytorch_lightning.Trainer.from_argparse_args", "pl.Trainer.from_argparse_args.fit", "os.path.join", "len"], "function", ["None"], []], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.train.utils.add_dataset_specific_args": [[104, 111], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "function", ["None"], []], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.train.utils.load_custom_data": [[113, 151], ["data_cls", "torch.utils.data.DataLoader"], "function", ["None"], []], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.sherliic.SherliicBase.__init__": [[45, 49], ["torch.utils.data.Dataset.__init__", "sherliic.SherliicBase.load_dataset"], "methods", ["home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.levy_holt.LevyHoltPattern.__init__", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.levy_holt.LevyHoltBase.load_dataset"], ["    ", "def", "__init__", "(", "self", ",", "path_to_csv", ":", "str", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "fn", "=", "path_to_csv", "\n", "self", ".", "data", "=", "self", ".", "load_dataset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.sherliic.SherliicBase.load_dataset": [[50, 59], ["logging.getLogger", "logging.getLogger.info", "open", "csv.reader", "next", "tqdm.tqdm.tqdm", "sherliic.SherliicBase.create_instances", "sherliic.unpack_row"], "methods", ["home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.levy_holt.LevyHoltPattern.create_instances", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.sherliic.unpack_row"], ["", "def", "load_dataset", "(", "self", ")", ":", "\n", "        ", "logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "\n", "logger", ".", "info", "(", "'Loading dataset from {}'", ".", "format", "(", "self", ".", "fn", ")", ")", "\n", "with", "open", "(", "self", ".", "fn", ")", "as", "f", ":", "\n", "            ", "cr", "=", "csv", ".", "reader", "(", "f", ")", "\n", "next", "(", "cr", ")", "# headers", "\n", "data", "=", "[", "inst", "for", "row", "in", "tqdm", "(", "cr", ")", "\n", "for", "inst", "in", "self", ".", "create_instances", "(", "*", "unpack_row", "(", "row", ")", ")", "]", "\n", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.sherliic.SherliicBase.create_instances": [[60, 70], ["NotImplementedError"], "methods", ["None"], ["", "def", "create_instances", "(", "\n", "self", ",", "sample_id", ":", "int", ",", "prem_type", ":", "int", ",", "prem_rel", ":", "int", ",", "hypo_type", ":", "int", ",", "hypo_rel", ":", "int", ",", "\n", "prem_argleft", ":", "str", ",", "prem_middle", ":", "str", ",", "prem_argright", ":", "str", ",", "prem_end", ":", "str", ",", "\n", "hypo_argleft", ":", "str", ",", "hypo_middle", ":", "str", ",", "hypo_argright", ":", "str", ",", "hypo_end", ":", "str", ",", "\n", "is_prem_reversed", ":", "bool", ",", "is_hypo_reversed", ":", "bool", ",", "\n", "examples_A", ":", "Tuple", "[", "str", ",", "str", ",", "str", "]", ",", "examples_B", ":", "Tuple", "[", "str", ",", "str", ",", "str", "]", ",", "gold_label", ":", "bool", ",", "\n", "rel_score", ":", "float", ",", "sign_score", ":", "float", ",", "esr_score", ":", "float", ",", "num_disagr", ":", "int", "\n", ")", "->", "List", "[", "Dict", "[", "str", ",", "Union", "[", "bool", ",", "str", "]", "]", "]", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\n", "\"You need to implement `create_instances` in a child class derived from `SherliicBase`\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.sherliic.SherliicBase.__getitem__": [[72, 75], ["NotImplementedError"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\n", "\"You need to implement `__getitem__` in a child class derived from `SherliicBase`\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.sherliic.SherliicBase.__len__": [[77, 79], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.sherliic.SherliicPattern.__init__": [[82, 150], ["sherliic.SherliicBase.__init__", "common.load_patterns", "common.load_patterns", "len", "len", "len", "len", "print", "print"], "methods", ["home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.levy_holt.LevyHoltPattern.__init__", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.common.load_patterns", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.common.load_patterns"], ["    ", "def", "__init__", "(", "self", ",", "path_to_csv", ":", "str", ",", "tokenizer", ":", "Optional", "[", "PreTrainedTokenizer", "]", "=", "None", ",", "\n", "pattern_file", ":", "Optional", "[", "str", "]", "=", "None", ",", "antipattern_file", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "best_k_patterns", ":", "int", "=", "100", ",", "\n", "pattern_idx", ":", "int", "=", "0", ",", "antipattern_idx", ":", "int", "=", "0", ",", "\n", "with_examples", ":", "bool", "=", "False", ",", "\n", "mask_prem_not_hypo", ":", "bool", "=", "False", ",", "augment", ":", "bool", "=", "False", ",", "\n", "training", ":", "bool", "=", "False", ",", "pattern_chunk_size", ":", "int", "=", "5", ",", "\n", "curated_auto", ":", "bool", "=", "False", ")", ":", "\n", "        ", "self", ".", "with_examples", "=", "with_examples", "\n", "self", ".", "mask_prem", "=", "mask_prem_not_hypo", "\n", "self", ".", "training", "=", "training", "\n", "self", ".", "pattern_chunk_size", "=", "pattern_chunk_size", "\n", "\n", "if", "pattern_file", "is", "None", ":", "\n", "            ", "self", ".", "patterns", "=", "PATTERNS", "\n", "self", ".", "handcrafted", "=", "True", "\n", "self", ".", "antipatterns", "=", "ANTIPATTERNS", "\n", "self", ".", "negation_necessary", "=", "NEGATION_NECESSARY", "\n", "self", ".", "anti_negation_necessary", "=", "ANTI_NEGATION_NECESSARY", "\n", "if", "antipattern_file", "is", "not", "None", ":", "\n", "                ", "print", "(", "\n", "\"WARNING: antipattern_file is ignored because pattern_file was not specified\"", ",", "\n", "file", "=", "sys", ".", "stderr", "\n", ")", "\n", "", "", "else", ":", "\n", "            ", "if", "best_k_patterns", "is", "not", "None", "and", "best_k_patterns", "%", "pattern_chunk_size", "!=", "0", ":", "\n", "                ", "print", "(", "\n", "\"WARNING: best_k_patterns should be a\"", "\n", "+", "\" multiple of pattern_chunk_size (default: 5)\"", ",", "\n", "file", "=", "sys", ".", "stderr", "\n", ")", "\n", "", "self", ".", "patterns", "=", "load_patterns", "(", "pattern_file", ",", "best_k_patterns", ")", "\n", "self", ".", "handcrafted", "=", "curated_auto", "\n", "assert", "curated_auto", "or", "(", "not", "(", "with_examples", "or", "augment", ")", ")", ",", "\"automatic patterns do not make use of relation arguments; \"", "+", "\"thus neither examples nor augmenting are supported.\"", "\n", "assert", "antipattern_file", "is", "not", "None", ",", "\"antipattern_file has to be specified when pattern_file is used.\"", "\n", "self", ".", "antipatterns", "=", "load_patterns", "(", "\n", "antipattern_file", ",", "best_k_patterns", "\n", ")", "\n", "self", ".", "negation_necessary", "=", "[", "\n", "(", "False", ",", "False", ")", "for", "_", "in", "self", ".", "patterns", "\n", "]", "\n", "self", ".", "anti_negation_necessary", "=", "[", "\n", "(", "False", ",", "False", ")", "for", "_", "in", "self", ".", "antipatterns", "\n", "]", "\n", "\n", "", "self", ".", "pattern_idx", "=", "pattern_idx", "\n", "self", ".", "antipattern_idx", "=", "antipattern_idx", "\n", "assert", "pattern_idx", "<", "len", "(", "self", ".", "patterns", ")", ",", "\"You cannot choose among more than {} patterns.\"", ".", "format", "(", "\n", "len", "(", "self", ".", "patterns", ")", "\n", ")", "\n", "assert", "antipattern_idx", "<", "len", "(", "self", ".", "antipatterns", ")", ",", "\"You cannot choose among more than {} antipatterns.\"", ".", "format", "(", "\n", "len", "(", "self", ".", "antipatterns", ")", "\n", ")", "\n", "\n", "# TODO: implement data augmentation with different variables", "\n", "assert", "not", "augment", "or", "with_examples", ",", "\"data augmentation without examples is not implemented\"", "\n", "self", ".", "augment", "=", "augment", "\n", "\n", "self", ".", "tokenizer", "=", "tokenizer", "\n", "if", "self", ".", "tokenizer", ":", "\n", "            ", "self", ".", "mask_token", "=", "self", ".", "tokenizer", ".", "special_tokens_map", "[", "'mask_token'", "]", "\n", "", "super", "(", ")", ".", "__init__", "(", "path_to_csv", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.sherliic.SherliicPattern.create_sent_from_pattern": [[151, 199], ["re.sub", "common.choose_examples", "common.choose_examples", "common.negate", "common.negate", "pattern.format", "common.mask_equivalent", "common.mask_equivalent.strip", "common.mask_equivalent", "common.mask_equivalent.strip", "pattern.format", "common.mask_equivalent", "common.mask_equivalent", "print"], "methods", ["home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.common.choose_examples", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.common.choose_examples", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.common.negate", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.common.negate", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.common.mask_equivalent", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.common.mask_equivalent", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.common.mask_equivalent", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.common.mask_equivalent"], ["", "def", "create_sent_from_pattern", "(", "self", ",", "pattern", ":", "str", ",", "premise", ",", "hypothesis", ",", "\n", "is_prem_reversed", ",", "is_hypo_reversed", ",", "\n", "examples_A", ",", "examples_B", ",", "\n", "do_negation", ":", "Tuple", "[", "bool", ",", "bool", "]", ",", "no_mask", ":", "bool", "=", "False", ")", "->", "str", ":", "\n", "        ", "prem_argleft", ",", "prem_middle", ",", "prem_argright", ",", "prem_end", "=", "premise", "\n", "hypo_argleft", ",", "hypo_middle", ",", "hypo_argright", ",", "hypo_end", "=", "hypothesis", "\n", "\n", "if", "not", "no_mask", ":", "\n", "            ", "if", "self", ".", "mask_prem", ":", "\n", "                ", "prem_middle", "=", "mask_equivalent", "(", "\n", "prem_middle", ",", "self", ".", "mask_token", ",", "self", ".", "tokenizer", ")", "\n", "if", "prem_end", ".", "strip", "(", ")", ":", "\n", "                    ", "prem_end", "=", "mask_equivalent", "(", "\n", "prem_end", ",", "self", ".", "mask_token", ",", "self", ".", "tokenizer", ",", "add_space", "=", "False", ")", "\n", "", "", "else", ":", "\n", "                ", "hypo_middle", "=", "mask_equivalent", "(", "\n", "hypo_middle", ",", "self", ".", "mask_token", ",", "self", ".", "tokenizer", ")", "\n", "if", "hypo_end", ".", "strip", "(", ")", ":", "\n", "                    ", "hypo_end", "=", "mask_equivalent", "(", "\n", "hypo_end", ",", "self", ".", "mask_token", ",", "self", ".", "tokenizer", ",", "add_space", "=", "False", ")", "\n", "\n", "", "", "", "if", "self", ".", "with_examples", ":", "\n", "            ", "prem_argleft", ",", "prem_argright", "=", "choose_examples", "(", "\n", "examples_A", ",", "examples_B", ",", "is_prem_reversed", ")", "\n", "hypo_argleft", ",", "hypo_argright", "=", "choose_examples", "(", "\n", "examples_A", ",", "examples_B", ",", "is_hypo_reversed", ")", "\n", "\n", "", "if", "do_negation", "[", "0", "]", ":", "\n", "            ", "prem_middle", "=", "negate", "(", "prem_middle", ")", "\n", "", "if", "do_negation", "[", "1", "]", ":", "\n", "            ", "hypo_middle", "=", "negate", "(", "hypo_middle", ")", "\n", "\n", "", "if", "self", ".", "handcrafted", ":", "\n", "            ", "inserted", "=", "pattern", ".", "format", "(", "\n", "pal", "=", "prem_argleft", ",", "prem", "=", "prem_middle", ",", "par", "=", "prem_argright", "+", "prem_end", ",", "\n", "hal", "=", "hypo_argleft", ",", "hypo", "=", "hypo_middle", ",", "har", "=", "hypo_argright", "+", "hypo_end", "\n", ")", "\n", "", "else", ":", "\n", "            ", "try", ":", "\n", "                ", "inserted", "=", "pattern", ".", "format", "(", "\n", "prem", "=", "prem_middle", "+", "' '", "+", "prem_end", ",", "\n", "hypo", "=", "hypo_middle", "+", "' '", "+", "hypo_end", "\n", ")", "\n", "", "except", "KeyError", "as", "e", ":", "\n", "                ", "print", "(", "pattern", ")", "\n", "raise", "e", "\n", "\n", "", "", "return", "re", ".", "sub", "(", "r'\\s+'", ",", "' '", ",", "inserted", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.sherliic.SherliicPattern.create_single_instance": [[200, 295], ["zip", "sherliic.SherliicPattern.create_sent_from_pattern", "zip", "sherliic.SherliicPattern.create_sent_from_pattern", "inst[].append", "sherliic.SherliicPattern.create_sent_from_pattern", "inst[].append", "sherliic.SherliicPattern.create_sent_from_pattern", "sherliic.SherliicPattern.create_sent_from_pattern", "inst[].append", "sherliic.SherliicPattern.create_sent_from_pattern", "inst[].append", "sherliic.SherliicPattern.create_sent_from_pattern", "sherliic.SherliicPattern.create_sent_from_pattern"], "methods", ["home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.levy_holt.LevyHoltPattern.create_sent_from_pattern", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.levy_holt.LevyHoltPattern.create_sent_from_pattern", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.levy_holt.LevyHoltPattern.create_sent_from_pattern", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.levy_holt.LevyHoltPattern.create_sent_from_pattern", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.levy_holt.LevyHoltPattern.create_sent_from_pattern", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.levy_holt.LevyHoltPattern.create_sent_from_pattern", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.levy_holt.LevyHoltPattern.create_sent_from_pattern", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.levy_holt.LevyHoltPattern.create_sent_from_pattern"], ["", "def", "create_single_instance", "(", "self", ",", "premise", ":", "Tuple", "[", "str", ",", "str", ",", "str", ",", "str", "]", ",", "\n", "hypothesis", ":", "Tuple", "[", "str", ",", "str", ",", "str", ",", "str", "]", ",", "\n", "is_prem_reversed", ":", "bool", ",", "is_hypo_reversed", ":", "bool", ",", "\n", "examples_A", ":", "Tuple", "[", "str", ",", "str", ",", "str", "]", ",", "\n", "examples_B", ":", "Tuple", "[", "str", ",", "str", ",", "str", "]", ",", "gold_label", ":", "bool", "\n", ")", "->", "Dict", "[", "str", ",", "Union", "[", "bool", ",", "str", ",", "List", "[", "str", "]", "]", "]", ":", "\n", "        ", "inst", "=", "{", "\n", "LABEL_KEY", ":", "gold_label", "\n", "}", "\n", "\n", "if", "self", ".", "pattern_idx", "<", "0", ":", "\n", "            ", "inst", "[", "SENT_KEY", "]", "=", "[", "]", "\n", "if", "self", ".", "tokenizer", ":", "\n", "                ", "inst", "[", "MASKED_SENT_KEY", "]", "=", "[", "]", "\n", "\n", "", "for", "pat", ",", "do_neg", "in", "zip", "(", "self", ".", "patterns", ",", "self", ".", "negation_necessary", ")", ":", "\n", "                ", "inst", "[", "SENT_KEY", "]", ".", "append", "(", "\n", "self", ".", "create_sent_from_pattern", "(", "\n", "pat", ",", "\n", "premise", ",", "hypothesis", ",", "\n", "is_prem_reversed", ",", "is_hypo_reversed", ",", "\n", "examples_A", ",", "examples_B", ",", "\n", "do_neg", ",", "\n", "no_mask", "=", "True", "\n", ")", "\n", ")", "\n", "if", "self", ".", "tokenizer", ":", "\n", "                    ", "inst", "[", "MASKED_SENT_KEY", "]", ".", "append", "(", "\n", "self", ".", "create_sent_from_pattern", "(", "\n", "pat", ",", "premise", ",", "hypothesis", ",", "\n", "is_prem_reversed", ",", "is_hypo_reversed", ",", "\n", "examples_A", ",", "examples_B", ",", "\n", "do_neg", ",", "no_mask", "=", "False", "\n", ")", "\n", ")", "\n", "", "", "", "else", ":", "\n", "            ", "inst", "[", "SENT_KEY", "]", "=", "self", ".", "create_sent_from_pattern", "(", "\n", "self", ".", "patterns", "[", "self", ".", "pattern_idx", "]", ",", "\n", "premise", ",", "hypothesis", ",", "\n", "is_prem_reversed", ",", "is_hypo_reversed", ",", "examples_A", ",", "examples_B", ",", "\n", "self", ".", "negation_necessary", "[", "self", ".", "pattern_idx", "]", ",", "\n", "no_mask", "=", "True", "\n", ")", "\n", "\n", "if", "self", ".", "tokenizer", ":", "\n", "                ", "inst", "[", "MASKED_SENT_KEY", "]", "=", "self", ".", "create_sent_from_pattern", "(", "\n", "self", ".", "patterns", "[", "self", ".", "pattern_idx", "]", ",", "\n", "premise", ",", "hypothesis", ",", "\n", "is_prem_reversed", ",", "is_hypo_reversed", ",", "examples_A", ",", "examples_B", ",", "\n", "self", ".", "negation_necessary", "[", "self", ".", "pattern_idx", "]", ",", "\n", "no_mask", "=", "False", "\n", ")", "\n", "\n", "", "", "if", "self", ".", "antipattern_idx", "<", "0", ":", "\n", "            ", "inst", "[", "ANTI_KEY", "]", "=", "[", "]", "\n", "if", "self", ".", "tokenizer", ":", "\n", "                ", "inst", "[", "MASKED_ANTI_KEY", "]", "=", "[", "]", "\n", "\n", "", "for", "pat", ",", "do_neg", "in", "zip", "(", "self", ".", "antipatterns", ",", "self", ".", "anti_negation_necessary", ")", ":", "\n", "                ", "inst", "[", "ANTI_KEY", "]", ".", "append", "(", "\n", "self", ".", "create_sent_from_pattern", "(", "\n", "pat", ",", "premise", ",", "hypothesis", ",", "\n", "is_prem_reversed", ",", "is_hypo_reversed", ",", "\n", "examples_A", ",", "examples_B", ",", "\n", "do_neg", ",", "no_mask", "=", "True", "\n", ")", "\n", ")", "\n", "if", "self", ".", "tokenizer", ":", "\n", "                    ", "inst", "[", "MASKED_ANTI_KEY", "]", ".", "append", "(", "\n", "self", ".", "create_sent_from_pattern", "(", "\n", "pat", ",", "premise", ",", "hypothesis", ",", "\n", "is_prem_reversed", ",", "is_hypo_reversed", ",", "\n", "examples_A", ",", "examples_B", ",", "\n", "do_neg", ",", "no_mask", "=", "False", "\n", ")", "\n", ")", "\n", "", "", "", "else", ":", "\n", "            ", "inst", "[", "ANTI_KEY", "]", "=", "self", ".", "create_sent_from_pattern", "(", "\n", "self", ".", "antipatterns", "[", "self", ".", "antipattern_idx", "]", ",", "\n", "premise", ",", "hypothesis", ",", "\n", "is_prem_reversed", ",", "is_hypo_reversed", ",", "examples_A", ",", "examples_B", ",", "\n", "self", ".", "anti_negation_necessary", "[", "self", ".", "antipattern_idx", "]", ",", "\n", "no_mask", "=", "True", "\n", ")", "\n", "\n", "if", "self", ".", "tokenizer", ":", "\n", "                ", "inst", "[", "MASKED_ANTI_KEY", "]", "=", "self", ".", "create_sent_from_pattern", "(", "\n", "self", ".", "antipatterns", "[", "self", ".", "antipattern_idx", "]", ",", "\n", "premise", ",", "hypothesis", ",", "\n", "is_prem_reversed", ",", "is_hypo_reversed", ",", "examples_A", ",", "examples_B", ",", "\n", "self", ".", "anti_negation_necessary", "[", "self", ".", "antipattern_idx", "]", ",", "\n", "no_mask", "=", "False", "\n", ")", "\n", "\n", "", "", "return", "inst", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.sherliic.SherliicPattern.create_instances": [[296, 350], ["itertools.product", "sherliic.SherliicPattern.create_single_instance", "instances.append", "sherliic.SherliicPattern.unpack_instance", "zip", "instances.append", "sherliic.SherliicPattern.create_single_instance", "common.chunks", "instances.append"], "methods", ["home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.levy_holt.LevyHoltPattern.create_single_instance", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.sherliic.SherliicPattern.unpack_instance", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.levy_holt.LevyHoltPattern.create_single_instance", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.common.chunks"], ["", "@", "overrides", "\n", "def", "create_instances", "(", "\n", "self", ",", "sample_id", ":", "int", ",", "prem_type", ":", "int", ",", "prem_rel", ":", "int", ",", "hypo_type", ":", "int", ",", "hypo_rel", ":", "int", ",", "\n", "prem_argleft", ":", "str", ",", "prem_middle", ":", "str", ",", "prem_argright", ":", "str", ",", "prem_end", ":", "str", ",", "\n", "hypo_argleft", ":", "str", ",", "hypo_middle", ":", "str", ",", "hypo_argright", ":", "str", ",", "hypo_end", ":", "str", ",", "\n", "is_prem_reversed", ":", "bool", ",", "is_hypo_reversed", ":", "bool", ",", "\n", "examples_A", ":", "Tuple", "[", "str", ",", "str", ",", "str", "]", ",", "examples_B", ":", "Tuple", "[", "str", ",", "str", ",", "str", "]", ",", "gold_label", ":", "bool", ",", "\n", "rel_score", ":", "float", ",", "sign_score", ":", "float", ",", "esr_score", ":", "float", ",", "num_disagr", ":", "int", "\n", ")", "->", "List", "[", "Dict", "[", "str", ",", "Union", "[", "bool", ",", "str", ",", "List", "[", "str", "]", "]", "]", "]", ":", "\n", "\n", "        ", "instances", "=", "[", "]", "\n", "\n", "if", "self", ".", "augment", ":", "\n", "            ", "for", "chosen_A", ",", "chosen_B", "in", "product", "(", "examples_A", ",", "examples_B", ")", ":", "\n", "                ", "if", "chosen_A", "==", "chosen_B", ":", "\n", "                    ", "continue", "\n", "", "instances", ".", "append", "(", "\n", "self", ".", "create_single_instance", "(", "\n", "(", "prem_argleft", ",", "prem_middle", ",", "prem_argright", ",", "prem_end", ")", ",", "\n", "(", "hypo_argleft", ",", "hypo_middle", ",", "hypo_argright", ",", "hypo_end", ")", ",", "\n", "is_prem_reversed", ",", "is_hypo_reversed", ",", "\n", "(", "chosen_A", ",", ")", ",", "(", "chosen_B", ",", ")", ",", "\n", "gold_label", "\n", ")", "\n", ")", "\n", "", "", "else", ":", "\n", "            ", "inst", "=", "self", ".", "create_single_instance", "(", "\n", "(", "prem_argleft", ",", "prem_middle", ",", "prem_argright", ",", "prem_end", ")", ",", "\n", "(", "hypo_argleft", ",", "hypo_middle", ",", "hypo_argright", ",", "hypo_end", ")", ",", "\n", "is_prem_reversed", ",", "is_hypo_reversed", ",", "\n", "examples_A", ",", "examples_B", ",", "\n", "gold_label", "\n", ")", "\n", "if", "self", ".", "training", "and", "self", ".", "pattern_idx", "<", "0", ":", "\n", "                ", "lists", "=", "self", ".", "unpack_instance", "(", "inst", ")", "\n", "label", "=", "lists", "[", "-", "1", "]", "\n", "chunked", "=", "[", "chunks", "(", "x", ",", "self", ".", "pattern_chunk_size", ")", "\n", "for", "x", "in", "lists", "[", ":", "-", "1", "]", "]", "\n", "for", "chunk", "in", "zip", "(", "*", "chunked", ")", ":", "\n", "                    ", "smaller_inst", "=", "{", "\n", "SENT_KEY", ":", "chunk", "[", "0", "]", ",", "\n", "ANTI_KEY", ":", "chunk", "[", "1", "]", ",", "\n", "LABEL_KEY", ":", "label", "\n", "}", "\n", "\n", "if", "self", ".", "tokenizer", ":", "\n", "                        ", "smaller_inst", "[", "MASKED_SENT_KEY", "]", "=", "chunk", "[", "2", "]", "\n", "smaller_inst", "[", "MASKED_ANTI_KEY", "]", "=", "chunk", "[", "3", "]", "\n", "\n", "", "instances", ".", "append", "(", "smaller_inst", ")", "\n", "", "", "else", ":", "\n", "                ", "instances", ".", "append", "(", "inst", ")", "\n", "\n", "", "", "return", "instances", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.sherliic.SherliicPattern.unpack_instance": [[351, 362], ["None"], "methods", ["None"], ["", "def", "unpack_instance", "(", "\n", "self", ",", "inst", ":", "Dict", "[", "str", ",", "Union", "[", "str", ",", "bool", ",", "List", "[", "str", "]", "]", "]", "\n", ")", "->", "List", "[", "Union", "[", "str", ",", "bool", ",", "List", "[", "str", "]", "]", "]", ":", "\n", "        ", "if", "self", ".", "tokenizer", ":", "\n", "            ", "return", "[", "\n", "inst", "[", "SENT_KEY", "]", ",", "inst", "[", "ANTI_KEY", "]", ",", "\n", "inst", "[", "MASKED_SENT_KEY", "]", ",", "inst", "[", "MASKED_ANTI_KEY", "]", ",", "\n", "inst", "[", "LABEL_KEY", "]", "\n", "]", "\n", "", "else", ":", "\n", "            ", "return", "[", "inst", "[", "SENT_KEY", "]", ",", "inst", "[", "ANTI_KEY", "]", ",", "inst", "[", "LABEL_KEY", "]", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.sherliic.SherliicPattern.__getitem__": [[363, 367], ["sherliic.SherliicPattern.unpack_instance"], "methods", ["home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.sherliic.SherliicPattern.unpack_instance"], ["", "", "@", "overrides", "\n", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "inst", "=", "self", ".", "data", "[", "index", "]", "\n", "return", "self", ".", "unpack_instance", "(", "inst", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.sherliic.SherliicSentences.__init__": [[371, 375], ["sherliic.SherliicBase.__init__"], "methods", ["home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.levy_holt.LevyHoltPattern.__init__"], ["    ", "def", "__init__", "(", "self", ",", "path_to_csv", ",", "with_examples", "=", "False", ",", "augment", "=", "False", ")", ":", "\n", "        ", "self", ".", "with_examples", "=", "with_examples", "\n", "self", ".", "augment", "=", "augment", "\n", "super", "(", ")", ".", "__init__", "(", "path_to_csv", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.sherliic.SherliicSentences.construct_from_parts": [[376, 380], ["joined.replace.replace.replace", "part.strip"], "methods", ["None"], ["", "def", "construct_from_parts", "(", "self", ",", "*", "parts", ")", ":", "\n", "        ", "joined", "=", "\" \"", ".", "join", "(", "[", "part", ".", "strip", "(", ")", "for", "part", "in", "parts", "]", ")", ".", "strip", "(", ")", "\n", "joined", "=", "joined", ".", "replace", "(", "\" 's\"", ",", "\"'s\"", ")", "\n", "return", "joined", "+", "'.'", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.sherliic.SherliicSentences.insert_examples": [[381, 406], ["sherliic.SherliicSentences.construct_from_parts", "sherliic.SherliicSentences.construct_from_parts"], "methods", ["home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.sherliic.SherliicSentences.construct_from_parts", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.sherliic.SherliicSentences.construct_from_parts"], ["", "def", "insert_examples", "(", "\n", "self", ",", "chosen_A", ",", "chosen_B", ",", "is_prem_reversed", ",", "is_hypo_reversed", ",", "\n", "prem_argleft", ",", "prem_middle", ",", "prem_argright", ",", "prem_end", ",", "\n", "hypo_argleft", ",", "hypo_middle", ",", "hypo_argright", ",", "hypo_end", "\n", ")", ":", "\n", "        ", "if", "is_prem_reversed", ":", "\n", "            ", "prem_argleft", "=", "chosen_B", "\n", "prem_argright", "=", "chosen_A", "\n", "", "else", ":", "\n", "            ", "prem_argleft", "=", "chosen_A", "\n", "prem_argright", "=", "chosen_B", "\n", "", "if", "is_hypo_reversed", ":", "\n", "            ", "hypo_argleft", "=", "chosen_B", "\n", "hypo_argright", "=", "chosen_A", "\n", "", "else", ":", "\n", "            ", "hypo_argleft", "=", "chosen_A", "\n", "hypo_argright", "=", "chosen_B", "\n", "\n", "", "prem", "=", "self", ".", "construct_from_parts", "(", "\n", "prem_argleft", ",", "prem_middle", ",", "prem_argright", ",", "prem_end", "\n", ")", "\n", "hypo", "=", "self", ".", "construct_from_parts", "(", "\n", "hypo_argleft", ",", "hypo_middle", ",", "hypo_argright", ",", "hypo_end", "\n", ")", "\n", "return", "prem", ",", "hypo", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.sherliic.SherliicSentences.change_var": [[407, 409], ["None"], "methods", ["None"], ["", "def", "change_var", "(", "self", ",", "var", ",", "placeholder", ")", ":", "\n", "        ", "return", "placeholder", "[", ":", "-", "2", "]", "+", "var", "+", "']'", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.sherliic.SherliicSentences.create_instances": [[410, 476], ["zip", "instances.append", "itertools.product", "sherliic.SherliicSentences.insert_examples", "prems.append", "hypos.append", "prems.append", "hypos.append", "sherliic.SherliicSentences.insert_examples", "prems.append", "hypos.append", "prems.append", "hypos.append", "sherliic.SherliicSentences.construct_from_parts", "sherliic.SherliicSentences.construct_from_parts", "sherliic.SherliicSentences.change_var", "sherliic.SherliicSentences.change_var", "sherliic.SherliicSentences.change_var", "sherliic.SherliicSentences.change_var", "sherliic.SherliicSentences.change_var", "sherliic.SherliicSentences.change_var", "sherliic.SherliicSentences.change_var", "sherliic.SherliicSentences.change_var", "sherliic.SherliicSentences.construct_from_parts", "sherliic.SherliicSentences.construct_from_parts"], "methods", ["home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.sherliic.SherliicSentences.insert_examples", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.sherliic.SherliicSentences.insert_examples", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.sherliic.SherliicSentences.construct_from_parts", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.sherliic.SherliicSentences.construct_from_parts", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.sherliic.SherliicSentences.change_var", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.sherliic.SherliicSentences.change_var", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.sherliic.SherliicSentences.change_var", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.sherliic.SherliicSentences.change_var", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.sherliic.SherliicSentences.change_var", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.sherliic.SherliicSentences.change_var", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.sherliic.SherliicSentences.change_var", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.sherliic.SherliicSentences.change_var", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.sherliic.SherliicSentences.construct_from_parts", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.sherliic.SherliicSentences.construct_from_parts"], ["", "@", "overrides", "\n", "def", "create_instances", "(", "\n", "self", ",", "sample_id", ":", "int", ",", "prem_type", ":", "int", ",", "prem_rel", ":", "int", ",", "hypo_type", ":", "int", ",", "hypo_rel", ":", "int", ",", "\n", "prem_argleft", ":", "str", ",", "prem_middle", ":", "str", ",", "prem_argright", ":", "str", ",", "prem_end", ":", "str", ",", "\n", "hypo_argleft", ":", "str", ",", "hypo_middle", ":", "str", ",", "hypo_argright", ":", "str", ",", "hypo_end", ":", "str", ",", "\n", "is_prem_reversed", ":", "bool", ",", "is_hypo_reversed", ":", "bool", ",", "\n", "examples_A", ":", "Tuple", "[", "str", ",", "str", ",", "str", "]", ",", "examples_B", ":", "Tuple", "[", "str", ",", "str", ",", "str", "]", ",", "gold_label", ":", "bool", ",", "\n", "rel_score", ":", "float", ",", "sign_score", ":", "float", ",", "esr_score", ":", "float", ",", "num_disagr", ":", "int", "\n", ")", "->", "List", "[", "Dict", "[", "str", ",", "Union", "[", "bool", ",", "str", "]", "]", "]", ":", "\n", "\n", "        ", "prems", "=", "[", "]", "\n", "hypos", "=", "[", "]", "\n", "if", "self", ".", "with_examples", ":", "\n", "            ", "if", "self", ".", "augment", ":", "\n", "                ", "for", "chosen_A", ",", "chosen_B", "in", "product", "(", "examples_A", ",", "examples_B", ")", ":", "\n", "                    ", "if", "chosen_A", "==", "chosen_B", ":", "\n", "                        ", "continue", "\n", "", "prem", ",", "hypo", "=", "self", ".", "insert_examples", "(", "\n", "chosen_A", ",", "chosen_B", ",", "is_prem_reversed", ",", "is_hypo_reversed", ",", "\n", "prem_argleft", ",", "prem_middle", ",", "prem_argright", ",", "prem_end", ",", "\n", "hypo_argleft", ",", "hypo_middle", ",", "hypo_argright", ",", "hypo_end", "\n", ")", "\n", "prems", ".", "append", "(", "prem", ")", "\n", "hypos", ".", "append", "(", "hypo", ")", "\n", "", "", "else", ":", "\n", "                ", "prem", ",", "hypo", "=", "self", ".", "insert_examples", "(", "\n", "examples_A", "[", "0", "]", ",", "examples_B", "[", "0", "]", ",", "is_prem_reversed", ",", "is_hypo_reversed", ",", "\n", "prem_argleft", ",", "prem_middle", ",", "prem_argright", ",", "prem_end", ",", "\n", "hypo_argleft", ",", "hypo_middle", ",", "hypo_argright", ",", "hypo_end", "\n", ")", "\n", "prems", ".", "append", "(", "prem", ")", "\n", "hypos", ".", "append", "(", "hypo", ")", "\n", "", "", "else", ":", "\n", "            ", "if", "self", ".", "augment", ":", "\n", "                ", "for", "var1", ",", "var2", "in", "[", "(", "'A'", ",", "'B'", ")", ",", "(", "'B'", ",", "'A'", ")", ",", "(", "'X'", ",", "'Y'", ")", ",", "(", "'Y'", ",", "'X'", ")", "]", ":", "\n", "                    ", "if", "is_prem_reversed", ":", "\n", "                        ", "prem_argleft", "=", "self", ".", "change_var", "(", "var2", ",", "prem_argleft", ")", "\n", "prem_argright", "=", "self", ".", "change_var", "(", "var1", ",", "prem_argright", ")", "\n", "", "else", ":", "\n", "                        ", "prem_argleft", "=", "self", ".", "change_var", "(", "var1", ",", "prem_argleft", ")", "\n", "prem_argright", "=", "self", ".", "change_var", "(", "var2", ",", "prem_argright", ")", "\n", "", "if", "is_hypo_reversed", ":", "\n", "                        ", "hypo_argleft", "=", "self", ".", "change_var", "(", "var2", ",", "hypo_argleft", ")", "\n", "hypo_argright", "=", "self", ".", "change_var", "(", "var1", ",", "hypo_argright", ")", "\n", "", "else", ":", "\n", "                        ", "hypo_argleft", "=", "self", ".", "change_var", "(", "var1", ",", "hypo_argleft", ")", "\n", "hypo_argright", "=", "self", ".", "change_var", "(", "var2", ",", "hypo_argright", ")", "\n", "\n", "", "prems", ".", "append", "(", "self", ".", "construct_from_parts", "(", "prem_argleft", ",", "prem_middle", ",", "\n", "prem_argright", ",", "prem_end", ")", ")", "\n", "hypos", ".", "append", "(", "self", ".", "construct_from_parts", "(", "hypo_argleft", ",", "hypo_middle", ",", "\n", "hypo_argright", ",", "hypo_end", ")", ")", "\n", "", "", "else", ":", "\n", "                ", "prems", ".", "append", "(", "self", ".", "construct_from_parts", "(", "prem_argleft", ",", "prem_middle", ",", "\n", "prem_argright", ",", "prem_end", ")", ")", "\n", "hypos", ".", "append", "(", "self", ".", "construct_from_parts", "(", "hypo_argleft", ",", "hypo_middle", ",", "\n", "hypo_argright", ",", "hypo_end", ")", ")", "\n", "\n", "", "", "instances", "=", "[", "]", "\n", "for", "p", ",", "h", "in", "zip", "(", "prems", ",", "hypos", ")", ":", "\n", "            ", "instances", ".", "append", "(", "{", "\n", "PREM_KEY", ":", "p", ",", "\n", "HYPO_KEY", ":", "h", ",", "\n", "LABEL_KEY", ":", "gold_label", "\n", "}", ")", "\n", "", "return", "instances", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.sherliic.SherliicSentences.__getitem__": [[477, 481], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "inst", "=", "self", ".", "data", "[", "index", "]", "\n", "return", "inst", "[", "PREM_KEY", "]", ",", "inst", "[", "HYPO_KEY", "]", ",", "inst", "[", "LABEL_KEY", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.sherliic.unpack_row": [[17, 41], ["int", "int", "int", "int", "int", "tuple", "tuple", "float", "float", "float", "int", "examples_A.split", "examples_B.split"], "function", ["None"], ["def", "unpack_row", "(", "row", ":", "Sequence", "[", "str", "]", ")", "->", "Tuple", "[", "\n", "int", ",", "int", ",", "int", ",", "int", ",", "int", ",", "\n", "str", ",", "str", ",", "str", ",", "str", ",", "\n", "str", ",", "str", ",", "str", ",", "str", ",", "\n", "bool", ",", "bool", ",", "\n", "Tuple", "[", "str", ",", "str", ",", "str", "]", ",", "Tuple", "[", "str", ",", "str", ",", "str", "]", ",", "\n", "bool", ",", "float", ",", "float", ",", "\n", "float", ",", "int", "]", ":", "\n", "    ", "sample_id", ",", "prem_type", ",", "prem_rel", ",", "hypo_type", ",", "hypo_rel", ",", "prem_argleft", ",", "prem_middle", ",", "prem_argright", ",", "prem_end", ",", "hypo_argleft", ",", "hypo_middle", ",", "hypo_argright", ",", "hypo_end", ",", "is_prem_reversed", ",", "is_hypo_reversed", ",", "examples_A", ",", "examples_B", ",", "gold_label", ",", "rel_score", ",", "sign_score", ",", "esr_score", ",", "num_disagr", "=", "row", "\n", "\n", "return", "int", "(", "sample_id", ")", ",", "int", "(", "prem_type", ")", ",", "int", "(", "prem_rel", ")", ",", "int", "(", "hypo_type", ")", ",", "int", "(", "hypo_rel", ")", ",", "prem_argleft", ",", "prem_middle", ",", "prem_argright", ",", "prem_end", ",", "hypo_argleft", ",", "hypo_middle", ",", "hypo_argright", ",", "hypo_end", ",", "is_prem_reversed", "==", "'True'", ",", "is_hypo_reversed", "==", "'True'", ",", "tuple", "(", "examples_A", ".", "split", "(", "' / '", ")", ")", ",", "tuple", "(", "examples_B", ".", "split", "(", "' / '", ")", ")", ",", "gold_label", "==", "'yes'", ",", "float", "(", "rel_score", ")", ",", "float", "(", "sign_score", ")", ",", "float", "(", "esr_score", ")", ",", "int", "(", "num_disagr", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.common.choose_examples": [[43, 48], ["None"], "function", ["None"], ["def", "choose_examples", "(", "examples_A", ",", "examples_B", ",", "is_reversed", ":", "bool", ")", ":", "\n", "    ", "if", "is_reversed", ":", "\n", "        ", "return", "examples_B", "[", "0", "]", ",", "examples_A", "[", "0", "]", "\n", "", "else", ":", "\n", "        ", "return", "examples_A", "[", "0", "]", ",", "examples_B", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.common.negate": [[50, 60], ["re.split", "tokens[].endswith"], "function", ["None"], ["", "", "def", "negate", "(", "verb_phrase", ":", "str", ")", "->", "str", ":", "\n", "    ", "tokens", "=", "re", ".", "split", "(", "r'\\s+'", ",", "verb_phrase", ")", "\n", "if", "tokens", "[", "0", "]", "in", "[", "'is'", ",", "'are'", ",", "'were'", ",", "'was'", "]", ":", "\n", "        ", "new_tokens", "=", "tokens", "[", ":", "1", "]", "+", "[", "'not'", "]", "+", "tokens", "[", "1", ":", "]", "\n", "", "else", ":", "\n", "        ", "if", "tokens", "[", "0", "]", ".", "endswith", "(", "'s'", ")", ":", "\n", "            ", "new_tokens", "=", "[", "'does'", ",", "'not'", ",", "tokens", "[", "0", "]", "[", ":", "-", "1", "]", "]", "+", "tokens", "[", "1", ":", "]", "\n", "", "else", ":", "\n", "            ", "new_tokens", "=", "[", "'do'", ",", "'not'", ",", "tokens", "[", "0", "]", "[", ":", "-", "1", "]", "]", "+", "tokens", "[", "1", ":", "]", "\n", "", "", "return", "' '", ".", "join", "(", "new_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.common.mask_equivalent": [[62, 71], ["string.strip", "len", "tokenizer.encode"], "function", ["None"], ["", "def", "mask_equivalent", "(", "self", ",", "string", ":", "str", ",", "mask_token", ",", "tokenizer", ",", "add_space", "=", "True", ")", "->", "str", ":", "\n", "    ", "longer_string", "=", "mask_token", "\n", "if", "add_space", ":", "\n", "        ", "longer_string", "=", "longer_string", "+", "' '", "\n", "", "longer_string", "=", "longer_string", "+", "string", ".", "strip", "(", ")", "\n", "num_tokens", "=", "len", "(", "\n", "tokenizer", ".", "encode", "(", "longer_string", ",", "add_special_tokens", "=", "False", ")", "\n", ")", "-", "1", "\n", "return", "\" \"", ".", "join", "(", "[", "mask_token", "]", "*", "num_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.common.load_patterns": [[73, 82], ["open", "line.strip().split", "patterns.append", "line.strip", "len"], "function", ["None"], ["", "def", "load_patterns", "(", "pattern_file", ":", "str", ",", "best_k_patterns", ":", "Optional", "[", "int", "]", ")", "->", "List", "[", "str", "]", ":", "\n", "    ", "patterns", "=", "[", "]", "\n", "with", "open", "(", "pattern_file", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "score", ",", "pattern", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "patterns", ".", "append", "(", "pattern", ")", "\n", "if", "best_k_patterns", "is", "not", "None", "and", "len", "(", "patterns", ")", "==", "best_k_patterns", ":", "\n", "                ", "break", "\n", "", "", "", "return", "patterns", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.common.chunks": [[87, 91], ["range", "len"], "function", ["None"], ["def", "chunks", "(", "lst", ":", "List", "[", "T", "]", ",", "n", ":", "int", ")", "->", "Iterable", "[", "List", "[", "T", "]", "]", ":", "\n", "    ", "\"\"\"Yield successive n-sized chunks from lst.\"\"\"", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "lst", ")", ",", "n", ")", ":", "\n", "        ", "yield", "lst", "[", "i", ":", "i", "+", "n", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.levy_holt.LevyHoltBase.__init__": [[9, 11], ["levy_holt.LevyHoltBase.load_dataset"], "methods", ["home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.levy_holt.LevyHoltBase.load_dataset"], ["    ", "def", "__init__", "(", "self", ",", "txt_file", ")", ":", "\n", "        ", "self", ".", "data", "=", "self", ".", "load_dataset", "(", "txt_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.levy_holt.LevyHoltBase.load_dataset": [[12, 22], ["open", "line.strip().split", "tuple", "tuple", "data.extend", "levy_holt.LevyHoltBase.create_instances", "line.strip", "h.strip", "p.strip", "tuple.split", "tuple.split"], "methods", ["home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.levy_holt.LevyHoltPattern.create_instances"], ["", "def", "load_dataset", "(", "self", ",", "txt_file", ")", ":", "\n", "        ", "data", "=", "[", "]", "\n", "with", "open", "(", "txt_file", ")", "as", "f", ":", "\n", "            ", "for", "line", "in", "f", ":", "\n", "                ", "hypo", ",", "prem", ",", "label", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "hypo", "=", "tuple", "(", "h", ".", "strip", "(", ")", "for", "h", "in", "hypo", ".", "split", "(", "','", ")", ")", "\n", "prem", "=", "tuple", "(", "p", ".", "strip", "(", ")", "for", "p", "in", "prem", ".", "split", "(", "','", ")", ")", "\n", "label", "=", "label", "==", "'True'", "\n", "data", ".", "extend", "(", "self", ".", "create_instances", "(", "prem", ",", "hypo", ",", "label", ")", ")", "\n", "", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.levy_holt.LevyHoltBase.create_instances": [[23, 31], ["NotImplementedError"], "methods", ["None"], ["", "def", "create_instances", "(", "\n", "self", ",", "\n", "prem", ":", "Tuple", "[", "str", ",", "str", ",", "str", "]", ",", "\n", "hypo", ":", "Tuple", "[", "str", ",", "str", ",", "str", "]", ",", "\n", "label", ":", "bool", "\n", ")", "->", "List", "[", "Dict", "[", "str", ",", "Union", "[", "bool", ",", "str", "]", "]", "]", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\n", "\"You have to implement `create_instances` in a subclass inheriting from `LevyHoltBase`\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.levy_holt.LevyHoltBase.__len__": [[33, 35], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.levy_holt.LevyHoltBase.__getitem__": [[36, 39], ["NotImplementedError"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\n", "\"You have to implement `__getitem__` in a subclass inheriting from `LevyHoltBase`\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.levy_holt.LevyHoltSentences.create_instances": [[44, 57], ["None"], "methods", ["None"], ["    ", "@", "overrides", "\n", "def", "create_instances", "(", "\n", "self", ",", "\n", "prem", ":", "Tuple", "[", "str", ",", "str", ",", "str", "]", ",", "\n", "hypo", ":", "Tuple", "[", "str", ",", "str", ",", "str", "]", ",", "\n", "label", ":", "bool", "\n", ")", "->", "List", "[", "Dict", "[", "str", ",", "Union", "[", "bool", ",", "str", "]", "]", "]", ":", "\n", "        ", "inst", "=", "{", "\n", "PREM_KEY", ":", "' '", ".", "join", "(", "prem", ")", ",", "\n", "HYPO_KEY", ":", "' '", ".", "join", "(", "hypo", ")", ",", "\n", "LABEL_KEY", ":", "label", "\n", "}", "\n", "return", "[", "inst", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.levy_holt.LevyHoltSentences.__getitem__": [[58, 62], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "inst", "=", "self", ".", "data", "[", "index", "]", "\n", "return", "inst", "[", "PREM_KEY", "]", ",", "inst", "[", "HYPO_KEY", "]", ",", "inst", "[", "LABEL_KEY", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.levy_holt.LevyHoltPattern.__init__": [[66, 91], ["levy_holt.LevyHoltBase.__init__", "common.load_patterns", "common.load_patterns", "print"], "methods", ["home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.levy_holt.LevyHoltPattern.__init__", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.common.load_patterns", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.common.load_patterns"], ["    ", "def", "__init__", "(", "self", ",", "txt_file", ":", "str", ",", "pattern_file", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "antipattern_file", ":", "Optional", "[", "str", "]", "=", "None", ",", "best_k_patterns", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "pattern_chunk_size", ":", "int", "=", "5", ",", "training", ":", "bool", "=", "False", ",", "\n", "curated_auto", ":", "bool", "=", "False", ")", ":", "\n", "        ", "self", ".", "training", "=", "training", "\n", "self", ".", "pattern_chunk_size", "=", "pattern_chunk_size", "\n", "\n", "if", "pattern_file", "is", "None", ":", "\n", "            ", "self", ".", "patterns", "=", "PATTERNS", "\n", "self", ".", "antipatterns", "=", "ANTIPATTERNS", "\n", "self", ".", "handcrafted", "=", "True", "\n", "", "else", ":", "\n", "            ", "if", "best_k_patterns", "is", "not", "None", "and", "best_k_patterns", "%", "pattern_chunk_size", "!=", "0", ":", "\n", "                ", "print", "(", "\n", "\"WARNING: best_k_patterns should be a\"", "\n", "+", "\" multiple of pattern_chunk_size ({})\"", ".", "format", "(", "pattern_chunk_size", ")", ")", "\n", "", "self", ".", "patterns", "=", "load_patterns", "(", "pattern_file", ",", "best_k_patterns", ")", "\n", "assert", "antipattern_file", "is", "not", "None", ",", "\"pattern_file and antipattern_file must either\"", "+", "\" both be None or both set to file paths.\"", "\n", "self", ".", "antipatterns", "=", "load_patterns", "(", "\n", "antipattern_file", ",", "best_k_patterns", ")", "\n", "self", ".", "handcrafted", "=", "curated_auto", "\n", "\n", "", "super", "(", ")", ".", "__init__", "(", "txt_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.levy_holt.LevyHoltPattern.create_sent_from_pattern": [[92, 100], ["pattern.format", "pattern.format"], "methods", ["None"], ["", "def", "create_sent_from_pattern", "(", "self", ",", "pattern", ":", "str", ",", "prem", ":", "Tuple", "[", "str", ",", "str", ",", "str", "]", ",", "\n", "hypo", ":", "Tuple", "[", "str", ",", "str", ",", "str", "]", ")", "->", "str", ":", "\n", "        ", "if", "self", ".", "handcrafted", ":", "\n", "            ", "sent", "=", "pattern", ".", "format", "(", "pal", "=", "prem", "[", "0", "]", ",", "prem", "=", "prem", "[", "1", "]", ",", "par", "=", "prem", "[", "2", "]", ",", "\n", "hal", "=", "hypo", "[", "0", "]", ",", "hypo", "=", "hypo", "[", "1", "]", ",", "har", "=", "hypo", "[", "2", "]", ")", "\n", "", "else", ":", "\n", "            ", "sent", "=", "pattern", ".", "format", "(", "prem", "=", "prem", "[", "1", "]", ",", "hypo", "=", "hypo", "[", "1", "]", ")", "\n", "", "return", "sent", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.levy_holt.LevyHoltPattern.create_single_instance": [[101, 121], ["levy_holt.LevyHoltPattern.create_sent_from_pattern", "levy_holt.LevyHoltPattern.create_sent_from_pattern"], "methods", ["home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.levy_holt.LevyHoltPattern.create_sent_from_pattern", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.levy_holt.LevyHoltPattern.create_sent_from_pattern"], ["", "def", "create_single_instance", "(", "\n", "self", ",", "\n", "prem", ":", "Tuple", "[", "str", ",", "str", ",", "str", "]", ",", "\n", "hypo", ":", "Tuple", "[", "str", ",", "str", ",", "str", "]", ",", "\n", "label", ":", "bool", ",", "\n", "patterns", ":", "Iterable", "[", "str", "]", ",", "\n", "antipatterns", ":", "Iterable", "[", "str", "]", ")", "->", "Dict", "[", "str", ",", "Union", "[", "bool", ",", "str", "]", "]", ":", "\n", "        ", "inst", "=", "{", "}", "\n", "\n", "inst", "[", "SENT_KEY", "]", "=", "[", "\n", "self", ".", "create_sent_from_pattern", "(", "pat", ",", "prem", ",", "hypo", ")", "\n", "for", "pat", "in", "patterns", "\n", "]", "\n", "inst", "[", "ANTI_KEY", "]", "=", "[", "\n", "self", ".", "create_sent_from_pattern", "(", "pat", ",", "prem", ",", "hypo", ")", "\n", "for", "pat", "in", "antipatterns", "\n", "]", "\n", "inst", "[", "LABEL_KEY", "]", "=", "label", "\n", "\n", "return", "inst", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.levy_holt.LevyHoltPattern.create_instances": [[122, 146], ["zip", "levy_holt.LevyHoltPattern.create_single_instance", "instances.append", "common.chunks", "common.chunks", "levy_holt.LevyHoltPattern.create_single_instance", "instances.append"], "methods", ["home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.levy_holt.LevyHoltPattern.create_single_instance", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.common.chunks", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.common.chunks", "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.levy_holt.LevyHoltPattern.create_single_instance"], ["", "@", "overrides", "\n", "def", "create_instances", "(", "\n", "self", ",", "\n", "prem", ":", "Tuple", "[", "str", ",", "str", ",", "str", "]", ",", "\n", "hypo", ":", "Tuple", "[", "str", ",", "str", ",", "str", "]", ",", "\n", "label", ":", "bool", "\n", ")", "->", "List", "[", "Dict", "[", "str", ",", "Union", "[", "bool", ",", "str", "]", "]", "]", ":", "\n", "        ", "instances", "=", "[", "]", "\n", "\n", "if", "self", ".", "training", ":", "\n", "            ", "chunked", "=", "[", "\n", "chunks", "(", "self", ".", "patterns", ",", "self", ".", "pattern_chunk_size", ")", ",", "\n", "chunks", "(", "self", ".", "antipatterns", ",", "self", ".", "pattern_chunk_size", ")", "\n", "]", "\n", "for", "pattern_chunk", ",", "antipattern_chunk", "in", "zip", "(", "*", "chunked", ")", ":", "\n", "                ", "inst", "=", "self", ".", "create_single_instance", "(", "\n", "prem", ",", "hypo", ",", "label", ",", "pattern_chunk", ",", "antipattern_chunk", ")", "\n", "instances", ".", "append", "(", "inst", ")", "\n", "", "", "else", ":", "\n", "            ", "inst", "=", "self", ".", "create_single_instance", "(", "\n", "prem", ",", "hypo", ",", "label", ",", "self", ".", "patterns", ",", "self", ".", "antipatterns", ")", "\n", "instances", ".", "append", "(", "inst", ")", "\n", "\n", "", "return", "instances", "\n", "\n"]], "home.repos.pwc.inspect_result.mnschmit_lm-lexical-inference.data.levy_holt.LevyHoltPattern.__getitem__": [[147, 151], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "inst", "=", "self", ".", "data", "[", "index", "]", "\n", "return", "inst", "[", "SENT_KEY", "]", ",", "inst", "[", "ANTI_KEY", "]", ",", "inst", "[", "LABEL_KEY", "]", "\n", "", "", ""]]}