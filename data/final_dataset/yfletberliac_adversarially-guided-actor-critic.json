{"home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.agac.AGAC.__init__": [[17, 70], ["core.tf_util.linear_schedule", "core.base_class.ActorCriticRLModel.__init__", "agac.AGAC.setup_model"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util.linear_schedule", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__init__", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.OffPolicyRLModel.setup_model"], ["    ", "def", "__init__", "(", "self", ",", "policy", ",", "env", ",", "gamma", "=", "0.99", ",", "n_steps", "=", "128", ",", "ent_coef", "=", "0.01", ",", "learning_rate", "=", "linear_schedule", "(", "0.001", ")", ",", "\n", "vf_coef", "=", "0.5", ",", "max_grad_norm", "=", "0.5", ",", "lam", "=", "0.95", ",", "nminibatches", "=", "4", ",", "noptepochs", "=", "4", ",", "cliprange", "=", "0.2", ",", "\n", "agac_c", "=", "0.01", ",", "beta_adv", "=", "0.1", ",", "episodic_count", "=", "True", ",", "\n", "verbose", "=", "0", ",", "tensorboard_log", "=", "None", ",", "full_tensorboard_log", "=", "False", ",", "_init_setup_model", "=", "True", ",", "\n", "policy_kwargs", "=", "None", ",", "seed", "=", "None", ",", "n_cpu_tf_sess", "=", "None", ")", ":", "\n", "\n", "        ", "self", ".", "learning_rate", "=", "learning_rate", "\n", "self", ".", "cliprange", "=", "cliprange", "\n", "self", ".", "n_steps", "=", "n_steps", "\n", "self", ".", "ent_coef", "=", "ent_coef", "\n", "self", ".", "vf_coef", "=", "vf_coef", "\n", "self", ".", "agac_c", "=", "agac_c", "\n", "self", ".", "beta_adv", "=", "beta_adv", "\n", "self", ".", "max_grad_norm", "=", "max_grad_norm", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "lam", "=", "lam", "\n", "self", ".", "nminibatches", "=", "nminibatches", "\n", "self", ".", "noptepochs", "=", "noptepochs", "\n", "self", ".", "tensorboard_log", "=", "tensorboard_log", "\n", "self", ".", "full_tensorboard_log", "=", "full_tensorboard_log", "\n", "self", ".", "episodic_count", "=", "episodic_count", "\n", "\n", "self", ".", "action_ph", "=", "None", "\n", "self", ".", "advs_ph", "=", "None", "\n", "self", ".", "rewards_ph", "=", "None", "\n", "self", ".", "true_rewards_ph", "=", "None", "\n", "self", ".", "old_pi_neglogpac_ph", "=", "None", "\n", "self", ".", "old_pi_probas_ph", "=", "None", "\n", "self", ".", "old_pi_adv_logits_ph", "=", "None", "\n", "self", ".", "old_vpred_ph", "=", "None", "\n", "self", ".", "learning_rate_ph", "=", "None", "\n", "self", ".", "agac_c_ph", "=", "None", "\n", "self", ".", "clip_range_ph", "=", "None", "\n", "self", ".", "entropy", "=", "None", "\n", "self", ".", "vf_loss", "=", "None", "\n", "self", ".", "pg_loss", "=", "None", "\n", "self", ".", "approxkl", "=", "None", "\n", "self", ".", "clipfrac", "=", "None", "\n", "self", ".", "_train", "=", "None", "\n", "self", ".", "training_op", "=", "None", "\n", "self", ".", "loss_names", "=", "None", "\n", "self", ".", "train_model", "=", "None", "\n", "self", ".", "act_model", "=", "None", "\n", "self", ".", "value", "=", "None", "\n", "self", ".", "n_batch", "=", "None", "\n", "self", ".", "summary", "=", "None", "\n", "\n", "super", "(", ")", ".", "__init__", "(", "policy", "=", "policy", ",", "env", "=", "env", ",", "verbose", "=", "verbose", ",", "requires_vec_env", "=", "True", ",", "\n", "_init_setup_model", "=", "_init_setup_model", ",", "policy_kwargs", "=", "policy_kwargs", ",", "\n", "seed", "=", "seed", ",", "n_cpu_tf_sess", "=", "n_cpu_tf_sess", ")", "\n", "\n", "if", "_init_setup_model", ":", "\n", "            ", "self", ".", "setup_model", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.agac.AGAC._make_runner": [[71, 74], ["agac.Runner"], "methods", ["None"], ["", "", "def", "_make_runner", "(", "self", ")", ":", "\n", "        ", "return", "Runner", "(", "env", "=", "self", ".", "env", ",", "model", "=", "self", ",", "n_steps", "=", "self", ".", "n_steps", ",", "\n", "episodic_count", "=", "self", ".", "episodic_count", ",", "gamma", "=", "self", ".", "gamma", ",", "lam", "=", "self", ".", "lam", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.agac.AGAC._get_pretrain_placeholders": [[75, 80], ["isinstance"], "methods", ["None"], ["", "def", "_get_pretrain_placeholders", "(", "self", ")", ":", "\n", "        ", "policy", "=", "self", ".", "act_model", "\n", "if", "isinstance", "(", "self", ".", "action_space", ",", "gym", ".", "spaces", ".", "Discrete", ")", ":", "\n", "            ", "return", "policy", ".", "obs_ph", ",", "self", ".", "action_ph", ",", "policy", ".", "policy", "\n", "", "return", "policy", ".", "obs_ph", ",", "self", ".", "action_ph", ",", "policy", ".", "deterministic_action", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.agac.AGAC.setup_model": [[81, 255], ["core.base_class.SetVerbosity", "issubclass", "tensorflow.Graph", "agac.AGAC.graph.as_default", "agac.AGAC.set_random_seed", "core.tf_util.make_session", "agac.AGAC.policy", "tensorflow.compat.v1.train.AdamOptimizer().apply_gradients", "tensorflow.compat.v1.train.AdamOptimizer().apply_gradients", "tensorflow.group", "tensorflow.compat.v1.global_variables_initializer().run", "tensorflow.compat.v1.summary.merge_all", "tensorflow.compat.v1.variable_scope", "agac.AGAC.policy", "tensorflow.compat.v1.variable_scope", "agac.AGAC.pdtype.sample_placeholder", "tensorflow.compat.v1.placeholder", "tensorflow.compat.v1.placeholder", "tensorflow.compat.v1.placeholder", "tensorflow.compat.v1.placeholder", "tensorflow.compat.v1.placeholder", "tensorflow.compat.v1.placeholder", "tensorflow.compat.v1.placeholder", "tensorflow.compat.v1.placeholder", "tensorflow.compat.v1.placeholder", "tensorflow.compat.v1.placeholder", "agac.AGAC.proba_distribution.neglogp", "tensorflow.reduce_mean", "tensorflow.nn.softmax_cross_entropy_with_logits_v2", "tensorflow.nn.softmax", "tensorflow.nn.softmax", "tensorflow.stop_gradient", "tensorflow.reduce_sum", "tensorflow.square", "tensorflow.square", "tensorflow.exp", "tensorflow.nn.moments", "tensorflow.reduce_mean", "tensorflow.reduce_mean", "tensorflow.compat.v1.summary.scalar", "tensorflow.compat.v1.summary.scalar", "tensorflow.compat.v1.summary.scalar", "tensorflow.compat.v1.summary.scalar", "tensorflow.compat.v1.summary.scalar", "tensorflow.compat.v1.summary.scalar", "tensorflow.compat.v1.summary.scalar", "tensorflow.compat.v1.summary.scalar", "tensorflow.gradients", "list", "tensorflow.gradients", "list", "tensorflow.gradients", "list", "tensorflow.compat.v1.variable_scope", "tensorflow.compat.v1.summary.scalar", "tensorflow.compat.v1.summary.scalar", "tensorflow.compat.v1.summary.scalar", "tensorflow.compat.v1.summary.scalar", "tensorflow.compat.v1.summary.scalar", "tensorflow.compat.v1.summary.scalar", "tensorflow.compat.v1.summary.scalar", "tensorflow.compat.v1.summary.scalar", "agac.AGAC.proba_distribution.entropy", "tensorflow.clip_by_value", "tensorflow.reduce_sum", "tensorflow.reduce_mean", "tensorflow.clip_by_value", "tensorflow.maximum", "tensorflow.reduce_mean", "tensorflow.cast", "tensorflow.reduce_mean", "tensorflow.reduce_mean", "tensorflow.compat.v1.variable_scope", "tensorflow.compat.v1.trainable_variables", "tensorflow.clip_by_global_norm", "zip", "tensorflow.clip_by_global_norm", "zip", "tensorflow.clip_by_global_norm", "zip", "tensorflow.compat.v1.train.AdamOptimizer", "tensorflow.compat.v1.train.AdamOptimizer", "tensorflow.reduce_mean", "tensorflow.reduce_mean", "tensorflow.reduce_mean", "tensorflow.reduce_mean", "tensorflow.reduce_mean", "tensorflow.reduce_mean", "tensorflow.compat.v1.summary.scalar", "tensorflow.reduce_mean", "tensorflow.reduce_mean", "tensorflow.compat.v1.summary.histogram", "tensorflow.compat.v1.summary.histogram", "tensorflow.compat.v1.summary.histogram", "tensorflow.compat.v1.summary.histogram", "tensorflow.compat.v1.summary.histogram", "tensorflow.compat.v1.summary.histogram", "core.tf_util.is_image", "tensorflow.compat.v1.global_variables_initializer", "core.tf_util.outer_scope_getter", "tensorflow.one_hot", "tensorflow.stop_gradient", "tensorflow.math.log", "tensorflow.maximum", "tensorflow.math.sqrt", "tensorflow.square", "tensorflow.greater", "tensorflow.reduce_mean", "tensorflow.reduce_mean", "tensorflow.compat.v1.summary.image", "tensorflow.compat.v1.summary.histogram", "tensorflow.stop_gradient", "tensorflow.math.log", "tensorflow.stop_gradient", "tensorflow.abs", "tensorflow.summary.histogram", "agac.AGAC.pi_adv_logits.get_shape().as_list", "tensorflow.stop_gradient", "agac.AGAC.pi_adv_logits.get_shape"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.BaseRLModel.set_random_seed", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util.make_session", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.policies.ActorCriticPolicy.policy", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.runners.AbstractEnvRunner.run", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.policies.ActorCriticPolicy.policy", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.ProbabilityDistributionType.sample_placeholder", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.BernoulliProbabilityDistribution.neglogp", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.BernoulliProbabilityDistribution.entropy", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util.is_image", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util.outer_scope_getter", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.log", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.log"], ["", "def", "setup_model", "(", "self", ")", ":", "\n", "        ", "with", "SetVerbosity", "(", "self", ".", "verbose", ")", ":", "\n", "\n", "            ", "assert", "issubclass", "(", "self", ".", "policy", ",", "ActorCriticPolicy", ")", ",", "\"Error: the input policy for the AGAC model must be \"", "\"an instance of core.policies.ActorCriticPolicy.\"", "\n", "\n", "self", ".", "n_batch", "=", "self", ".", "n_envs", "*", "self", ".", "n_steps", "\n", "\n", "self", ".", "graph", "=", "tf", ".", "Graph", "(", ")", "\n", "with", "self", ".", "graph", ".", "as_default", "(", ")", ":", "\n", "                ", "self", ".", "set_random_seed", "(", "self", ".", "seed", ")", "\n", "self", ".", "sess", "=", "tf_util", ".", "make_session", "(", "num_cpu", "=", "self", ".", "n_cpu_tf_sess", ",", "graph", "=", "self", ".", "graph", ")", "\n", "\n", "n_batch_step", "=", "None", "\n", "n_batch_train", "=", "None", "\n", "\n", "act_model", "=", "self", ".", "policy", "(", "self", ".", "sess", ",", "self", ".", "observation_space", ",", "self", ".", "action_space", ",", "self", ".", "n_envs", ",", "1", ",", "\n", "n_batch_step", ",", "reuse", "=", "False", ",", "**", "self", ".", "policy_kwargs", ")", "\n", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "\"train_model\"", ",", "reuse", "=", "True", ",", "\n", "custom_getter", "=", "tf_util", ".", "outer_scope_getter", "(", "\"train_model\"", ")", ")", ":", "\n", "                    ", "train_model", "=", "self", ".", "policy", "(", "self", ".", "sess", ",", "self", ".", "observation_space", ",", "self", ".", "action_space", ",", "\n", "self", ".", "n_envs", "//", "self", ".", "nminibatches", ",", "self", ".", "n_steps", ",", "n_batch_train", ",", "\n", "reuse", "=", "True", ",", "**", "self", ".", "policy_kwargs", ")", "\n", "\n", "", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "\"loss\"", ",", "reuse", "=", "False", ")", ":", "\n", "                    ", "self", ".", "action_ph", "=", "train_model", ".", "pdtype", ".", "sample_placeholder", "(", "[", "None", "]", ",", "name", "=", "\"action_ph\"", ")", "\n", "self", ".", "advs_ph", "=", "tf", ".", "compat", ".", "v1", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "None", "]", ",", "name", "=", "\"advs_ph\"", ")", "\n", "self", ".", "rewards_ph", "=", "tf", ".", "compat", ".", "v1", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "None", "]", ",", "name", "=", "\"rewards_ph\"", ")", "\n", "self", ".", "true_rewards_ph", "=", "tf", ".", "compat", ".", "v1", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "None", "]", ",", "name", "=", "\"true_rewards_ph\"", ")", "\n", "self", ".", "old_pi_neglogpac_ph", "=", "tf", ".", "compat", ".", "v1", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "None", "]", ",", "name", "=", "\"old_pi_neglogpac_ph\"", ")", "\n", "self", ".", "old_pi_probas_ph", "=", "tf", ".", "compat", ".", "v1", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "None", ",", "None", "]", ",", "name", "=", "\"old_pi_probas_ph\"", ")", "\n", "self", ".", "old_pi_adv_logits_ph", "=", "tf", ".", "compat", ".", "v1", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "None", ",", "None", "]", ",", "\n", "name", "=", "\"old_pi_adv_logits_ph\"", ")", "\n", "self", ".", "old_vpred_ph", "=", "tf", ".", "compat", ".", "v1", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "None", "]", ",", "name", "=", "\"old_vpred_ph\"", ")", "\n", "self", ".", "learning_rate_ph", "=", "tf", ".", "compat", ".", "v1", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "]", ",", "name", "=", "\"learning_rate_ph\"", ")", "\n", "self", ".", "agac_c_ph", "=", "tf", ".", "compat", ".", "v1", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "]", ",", "name", "=", "\"agac_c_ph\"", ")", "\n", "self", ".", "clip_range_ph", "=", "tf", ".", "compat", ".", "v1", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "]", ",", "name", "=", "\"clip_range_ph\"", ")", "\n", "\n", "neglogpac", "=", "train_model", ".", "proba_distribution", ".", "neglogp", "(", "self", ".", "action_ph", ")", "\n", "self", ".", "entropy", "=", "tf", ".", "reduce_mean", "(", "train_model", ".", "proba_distribution", ".", "entropy", "(", ")", ")", "\n", "\n", "vpred", "=", "train_model", ".", "value_flat", "\n", "\n", "# Value function clipping: not present in the original PPO", "\n", "# Default behavior (legacy from OpenAI baselines): use the same clipping as for the policy", "\n", "self", ".", "clip_range_vf_ph", "=", "self", ".", "clip_range_ph", "\n", "self", ".", "cliprange_vf", "=", "self", ".", "cliprange", "\n", "vpred_clipped", "=", "self", ".", "old_vpred_ph", "+", "tf", ".", "clip_by_value", "(", "train_model", ".", "value_flat", "-", "self", ".", "old_vpred_ph", ",", "\n", "-", "self", ".", "clip_range_vf_ph", ",", "self", ".", "clip_range_vf_ph", ")", "\n", "\n", "self", ".", "old_pi_adv_neglogpac", "=", "tf", ".", "nn", ".", "softmax_cross_entropy_with_logits_v2", "(", "\n", "logits", "=", "self", ".", "old_pi_adv_logits_ph", ",", "\n", "labels", "=", "tf", ".", "one_hot", "(", "tf", ".", "stop_gradient", "(", "self", ".", "action_ph", ")", ",", "\n", "train_model", ".", "pi_adv_logits", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "-", "1", "]", ")", ")", "\n", "\n", "old_pi_softmax", "=", "self", ".", "old_pi_probas_ph", "\n", "pi_adv_softmax", "=", "tf", ".", "nn", ".", "softmax", "(", "train_model", ".", "pi_adv_logits", ")", "\n", "old_pi_adv_softmax", "=", "tf", ".", "nn", ".", "softmax", "(", "self", ".", "old_pi_adv_logits_ph", ")", "\n", "\n", "# KL", "\n", "pi_piadv_kl", "=", "tf", ".", "stop_gradient", "(", "\n", "tf", ".", "reduce_sum", "(", "old_pi_softmax", "*", "tf", ".", "math", ".", "log", "(", "old_pi_softmax", "/", "(", "old_pi_adv_softmax", "+", "1e-8", ")", "+", "1e-8", ")", ",", "\n", "axis", "=", "-", "1", ")", ")", "\n", "\n", "# Adversary loss", "\n", "l_adv", "=", "tf", ".", "reduce_sum", "(", "tf", ".", "stop_gradient", "(", "old_pi_softmax", ")", "*", "tf", ".", "math", ".", "log", "(", "\n", "tf", ".", "stop_gradient", "(", "old_pi_softmax", ")", "/", "(", "pi_adv_softmax", "+", "1e-8", ")", "+", "1e-8", ")", ",", "axis", "=", "-", "1", ")", "\n", "\n", "# Value function loss", "\n", "vf_losses1", "=", "tf", ".", "square", "(", "\n", "vpred", "-", "self", ".", "true_rewards_ph", "-", "self", ".", "agac_c_ph", "*", "pi_piadv_kl", ")", "\n", "vf_losses2", "=", "tf", ".", "square", "(", "\n", "vpred_clipped", "-", "self", ".", "true_rewards_ph", "-", "self", ".", "agac_c_ph", "*", "pi_piadv_kl", ")", "\n", "self", ".", "vf_loss", "=", ".5", "*", "tf", ".", "reduce_mean", "(", "tf", ".", "maximum", "(", "vf_losses1", ",", "vf_losses2", ")", ")", "\n", "\n", "# Policy loss", "\n", "ratio", "=", "tf", ".", "exp", "(", "self", ".", "old_pi_neglogpac_ph", "-", "neglogpac", ")", "\n", "\n", "agac_advs_ph", "=", "self", ".", "rewards_ph", "+", "self", ".", "agac_c_ph", "*", "tf", ".", "stop_gradient", "(", "\n", "(", "self", ".", "old_pi_adv_neglogpac", "-", "self", ".", "old_pi_neglogpac_ph", ")", ")", "-", "self", ".", "old_vpred_ph", "\n", "mean_adv", ",", "var_adv", "=", "tf", ".", "nn", ".", "moments", "(", "agac_advs_ph", ",", "axes", "=", "0", ")", "\n", "agac_advs_ph", "=", "(", "agac_advs_ph", "-", "mean_adv", ")", "/", "(", "tf", ".", "math", ".", "sqrt", "(", "var_adv", ")", "+", "1e-8", ")", "\n", "\n", "pg_losses", "=", "-", "agac_advs_ph", "*", "ratio", "\n", "pg_losses2", "=", "-", "agac_advs_ph", "*", "tf", ".", "clip_by_value", "(", "ratio", ",", "1.0", "-", "self", ".", "clip_range_ph", ",", "1.0", "+", "\n", "self", ".", "clip_range_ph", ")", "\n", "self", ".", "pg_loss", "=", "tf", ".", "reduce_mean", "(", "tf", ".", "maximum", "(", "pg_losses", ",", "pg_losses2", ")", ")", "\n", "\n", "self", ".", "approxkl", "=", ".5", "*", "tf", ".", "reduce_mean", "(", "tf", ".", "square", "(", "neglogpac", "-", "self", ".", "old_pi_neglogpac_ph", ")", ")", "\n", "self", ".", "clipfrac", "=", "tf", ".", "reduce_mean", "(", "tf", ".", "cast", "(", "tf", ".", "greater", "(", "tf", ".", "abs", "(", "ratio", "-", "1.0", ")", ",", "\n", "self", ".", "clip_range_ph", ")", ",", "tf", ".", "float32", ")", ")", "\n", "\n", "loss", "=", "self", ".", "pg_loss", "-", "self", ".", "entropy", "*", "self", ".", "ent_coef", "+", "self", ".", "vf_coef", "*", "self", ".", "vf_loss", "+", "self", ".", "beta_adv", "*", "tf", ".", "reduce_mean", "(", "\n", "l_adv", ")", "\n", "\n", "tf", ".", "compat", ".", "v1", ".", "summary", ".", "scalar", "(", "'entropy_loss'", ",", "self", ".", "entropy", ")", "\n", "tf", ".", "compat", ".", "v1", ".", "summary", ".", "scalar", "(", "'policy_gradient_loss'", ",", "self", ".", "pg_loss", ")", "\n", "tf", ".", "compat", ".", "v1", ".", "summary", ".", "scalar", "(", "'value_function_loss'", ",", "self", ".", "vf_loss", ")", "\n", "tf", ".", "compat", ".", "v1", ".", "summary", ".", "scalar", "(", "'pi_adv_logits'", ",", "tf", ".", "reduce_mean", "(", "train_model", ".", "pi_adv_logits", ")", ")", "\n", "tf", ".", "compat", ".", "v1", ".", "summary", ".", "scalar", "(", "'pi_piold_kl'", ",", "self", ".", "approxkl", ")", "\n", "tf", ".", "compat", ".", "v1", ".", "summary", ".", "scalar", "(", "'pi_piadv_kl'", ",", "tf", ".", "reduce_mean", "(", "l_adv", ")", ")", "\n", "tf", ".", "compat", ".", "v1", ".", "summary", ".", "scalar", "(", "'clip_factor'", ",", "self", ".", "clipfrac", ")", "\n", "tf", ".", "compat", ".", "v1", ".", "summary", ".", "scalar", "(", "'loss'", ",", "loss", ")", "\n", "\n", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "'model'", ")", ":", "\n", "                        ", "self", ".", "params", "=", "tf", ".", "compat", ".", "v1", ".", "trainable_variables", "(", ")", "\n", "if", "self", ".", "full_tensorboard_log", ":", "\n", "                            ", "for", "var", "in", "self", ".", "params", ":", "\n", "                                ", "tf", ".", "summary", ".", "histogram", "(", "var", ".", "name", ",", "var", ")", "\n", "", "", "", "grads", "=", "tf", ".", "gradients", "(", "loss", ",", "self", ".", "params", ")", "\n", "\n", "if", "self", ".", "max_grad_norm", "is", "not", "None", ":", "\n", "                        ", "grads", ",", "_grad_norm", "=", "tf", ".", "clip_by_global_norm", "(", "grads", ",", "self", ".", "max_grad_norm", ")", "\n", "", "grads", "=", "list", "(", "zip", "(", "grads", ",", "self", ".", "params", ")", ")", "\n", "\n", "# Different LR for the adversary", "\n", "var_list1", "=", "[", "v", "for", "(", "t", ",", "v", ")", "in", "grads", "if", "\"adv\"", "in", "v", ".", "name", "]", "\n", "grads1", "=", "tf", ".", "gradients", "(", "loss", ",", "var_list1", ")", "\n", "if", "self", ".", "max_grad_norm", "is", "not", "None", ":", "\n", "                        ", "grads1", ",", "_grad_norm1", "=", "tf", ".", "clip_by_global_norm", "(", "grads1", ",", "self", ".", "max_grad_norm", ")", "\n", "", "grads1", "=", "list", "(", "zip", "(", "grads1", ",", "var_list1", ")", ")", "\n", "var_list2", "=", "[", "v", "for", "(", "t", ",", "v", ")", "in", "grads", "if", "\"adv\"", "not", "in", "v", ".", "name", "]", "\n", "grads2", "=", "tf", ".", "gradients", "(", "loss", ",", "var_list2", ")", "\n", "if", "self", ".", "max_grad_norm", "is", "not", "None", ":", "\n", "                        ", "grads2", ",", "_grad_norm2", "=", "tf", ".", "clip_by_global_norm", "(", "grads2", ",", "self", ".", "max_grad_norm", ")", "\n", "", "grads2", "=", "list", "(", "zip", "(", "grads2", ",", "var_list2", ")", ")", "\n", "\n", "", "train_op1", "=", "tf", ".", "compat", ".", "v1", ".", "train", ".", "AdamOptimizer", "(", "learning_rate", "=", "self", ".", "learning_rate_ph", "*", "0.3", ",", "\n", "epsilon", "=", "1e-5", ")", ".", "apply_gradients", "(", "grads1", ")", "\n", "train_op2", "=", "tf", ".", "compat", ".", "v1", ".", "train", ".", "AdamOptimizer", "(", "learning_rate", "=", "self", ".", "learning_rate_ph", ",", "epsilon", "=", "1e-5", ")", ".", "apply_gradients", "(", "\n", "grads2", ")", "\n", "self", ".", "_train", "=", "tf", ".", "group", "(", "train_op1", ",", "train_op2", ")", "\n", "\n", "self", ".", "loss_names", "=", "[", "'policy_loss'", ",", "'value_loss'", ",", "'policy_entropy'", ",", "'approxkl'", ",", "'clipfrac'", "]", "\n", "\n", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "\"input_info\"", ",", "reuse", "=", "False", ")", ":", "\n", "                    ", "tf", ".", "compat", ".", "v1", ".", "summary", ".", "scalar", "(", "'discounted_rewards'", ",", "tf", ".", "reduce_mean", "(", "self", ".", "rewards_ph", ")", ")", "\n", "tf", ".", "compat", ".", "v1", ".", "summary", ".", "scalar", "(", "'learning_rate'", ",", "tf", ".", "reduce_mean", "(", "self", ".", "learning_rate_ph", ")", ")", "\n", "tf", ".", "compat", ".", "v1", ".", "summary", ".", "scalar", "(", "'agac_c'", ",", "tf", ".", "reduce_mean", "(", "self", ".", "agac_c_ph", ")", ")", "\n", "tf", ".", "compat", ".", "v1", ".", "summary", ".", "scalar", "(", "'advantage'", ",", "tf", ".", "reduce_mean", "(", "self", ".", "advs_ph", ")", ")", "\n", "tf", ".", "compat", ".", "v1", ".", "summary", ".", "scalar", "(", "'agac_advantage'", ",", "tf", ".", "reduce_mean", "(", "agac_advs_ph", ")", ")", "\n", "\n", "tf", ".", "compat", ".", "v1", ".", "summary", ".", "scalar", "(", "'clip_range'", ",", "tf", ".", "reduce_mean", "(", "self", ".", "clip_range_ph", ")", ")", "\n", "if", "self", ".", "clip_range_vf_ph", "is", "not", "None", ":", "\n", "                        ", "tf", ".", "compat", ".", "v1", ".", "summary", ".", "scalar", "(", "'clip_range_vf'", ",", "tf", ".", "reduce_mean", "(", "self", ".", "clip_range_vf_ph", ")", ")", "\n", "\n", "", "tf", ".", "compat", ".", "v1", ".", "summary", ".", "scalar", "(", "'old_neglog_action_probability'", ",", "tf", ".", "reduce_mean", "(", "self", ".", "old_pi_neglogpac_ph", ")", ")", "\n", "tf", ".", "compat", ".", "v1", ".", "summary", ".", "scalar", "(", "'old_value_pred'", ",", "tf", ".", "reduce_mean", "(", "self", ".", "old_vpred_ph", ")", ")", "\n", "\n", "if", "self", ".", "full_tensorboard_log", ":", "\n", "                        ", "tf", ".", "compat", ".", "v1", ".", "summary", ".", "histogram", "(", "'discounted_rewards'", ",", "self", ".", "rewards_ph", ")", "\n", "tf", ".", "compat", ".", "v1", ".", "summary", ".", "histogram", "(", "'learning_rate'", ",", "self", ".", "learning_rate_ph", ")", "\n", "tf", ".", "compat", ".", "v1", ".", "summary", ".", "histogram", "(", "'advantage'", ",", "self", ".", "advs_ph", ")", "\n", "tf", ".", "compat", ".", "v1", ".", "summary", ".", "histogram", "(", "'clip_range'", ",", "self", ".", "clip_range_ph", ")", "\n", "tf", ".", "compat", ".", "v1", ".", "summary", ".", "histogram", "(", "'old_neglog_action_probability'", ",", "self", ".", "old_pi_neglogpac_ph", ")", "\n", "tf", ".", "compat", ".", "v1", ".", "summary", ".", "histogram", "(", "'old_value_pred'", ",", "self", ".", "old_vpred_ph", ")", "\n", "if", "tf_util", ".", "is_image", "(", "self", ".", "observation_space", ")", ":", "\n", "                            ", "tf", ".", "compat", ".", "v1", ".", "summary", ".", "image", "(", "'observation'", ",", "train_model", ".", "obs_ph", ")", "\n", "", "else", ":", "\n", "                            ", "tf", ".", "compat", ".", "v1", ".", "summary", ".", "histogram", "(", "'observation'", ",", "train_model", ".", "obs_ph", ")", "\n", "\n", "", "", "", "self", ".", "train_model", "=", "train_model", "\n", "self", ".", "act_model", "=", "act_model", "\n", "self", ".", "step", "=", "act_model", ".", "step", "\n", "self", ".", "proba_step", "=", "act_model", ".", "proba_step", "\n", "self", ".", "value", "=", "act_model", ".", "value", "\n", "self", ".", "initial_state", "=", "act_model", ".", "initial_state", "\n", "tf", ".", "compat", ".", "v1", ".", "global_variables_initializer", "(", ")", ".", "run", "(", "session", "=", "self", ".", "sess", ")", "# pylint: disable=E1101", "\n", "\n", "self", ".", "summary", "=", "tf", ".", "compat", ".", "v1", ".", "summary", ".", "merge_all", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.agac.AGAC._train_step": [[256, 323], ["writer.add_summary", "agac.AGAC.sess.run", "advs.mean", "advs.std", "tensorflow.RunOptions", "tensorflow.RunMetadata", "agac.AGAC.sess.run", "writer.add_run_metadata", "agac.AGAC.sess.run"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.runners.AbstractEnvRunner.run", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.runners.AbstractEnvRunner.run", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.runners.AbstractEnvRunner.run"], ["", "", "", "def", "_train_step", "(", "self", ",", "learning_rate", ",", "agac_c_now", ",", "cliprange", ",", "obs", ",", "returns", ",", "true_returns", ",", "masks", ",", "actions", ",", "values", ",", "\n", "neglogpacs", ",", "pi_probas", ",", "pi_adv_logits", ",", "update", ",", "writer", ",", "states", "=", "None", ",", "cliprange_vf", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Training of Algorithm\n\n        :param learning_rate: (float) learning rate\n        :param cliprange: (float) Clipping factor\n        :param obs: (np.ndarray) The current observation of the environment\n        :param returns: (np.ndarray) the rewards\n        :param masks: (np.ndarray) The last masks for done episodes (used in recurent policies)\n        :param actions: (np.ndarray) the actions\n        :param values: (np.ndarray) the values\n        :param neglogpacs: (np.ndarray) Negative Log-likelihood probability of Actions\n        :param update: (int) the current step iteration\n        :param writer: (TensorFlow Summary.writer) the writer for tensorboard\n        :param states: (np.ndarray) For recurrent policies, the internal state of the recurrent model\n        :return: policy gradient loss, value function loss, policy entropy,\n                approximation of kl divergence, updated clipping range, training update operation\n        :param cliprange_vf: (float) Clipping factor for the value function\n        \"\"\"", "\n", "advs", "=", "returns", "-", "values", "\n", "advs", "=", "(", "advs", "-", "advs", ".", "mean", "(", ")", ")", "/", "(", "advs", ".", "std", "(", ")", "+", "1e-8", ")", "\n", "\n", "td_map", "=", "{", "self", ".", "train_model", ".", "obs_ph", ":", "obs", ",", "self", ".", "action_ph", ":", "actions", ",", "\n", "self", ".", "advs_ph", ":", "advs", ",", "self", ".", "rewards_ph", ":", "returns", ",", "\n", "self", ".", "learning_rate_ph", ":", "learning_rate", ",", "self", ".", "clip_range_ph", ":", "cliprange", ",", "\n", "self", ".", "agac_c_ph", ":", "agac_c_now", ",", "\n", "self", ".", "old_pi_neglogpac_ph", ":", "neglogpacs", ",", "self", ".", "old_vpred_ph", ":", "values", ",", "\n", "self", ".", "true_rewards_ph", ":", "true_returns", ",", "\n", "self", ".", "old_pi_probas_ph", ":", "pi_probas", ",", "\n", "self", ".", "old_pi_adv_logits_ph", ":", "pi_adv_logits", "\n", "}", "\n", "\n", "if", "states", "is", "not", "None", ":", "\n", "            ", "td_map", "[", "self", ".", "train_model", ".", "states_ph", "]", "=", "states", "\n", "td_map", "[", "self", ".", "train_model", ".", "dones_ph", "]", "=", "masks", "\n", "\n", "", "if", "cliprange_vf", "is", "not", "None", "and", "cliprange_vf", ">=", "0", ":", "\n", "            ", "td_map", "[", "self", ".", "clip_range_vf_ph", "]", "=", "cliprange_vf", "\n", "\n", "", "if", "states", "is", "None", ":", "\n", "            ", "update_fac", "=", "self", ".", "n_batch", "//", "self", ".", "nminibatches", "//", "self", ".", "noptepochs", "+", "1", "\n", "", "else", ":", "\n", "            ", "update_fac", "=", "self", ".", "n_batch", "//", "self", ".", "nminibatches", "//", "self", ".", "noptepochs", "//", "self", ".", "n_steps", "+", "1", "\n", "\n", "", "if", "writer", "is", "not", "None", ":", "\n", "# Run loss backprop with summary, but once every 10 runs save the metadata (memory, compute time, ...)", "\n", "            ", "if", "self", ".", "full_tensorboard_log", "and", "(", "1", "+", "update", ")", "%", "10", "==", "0", ":", "\n", "                ", "run_options", "=", "tf", ".", "RunOptions", "(", "trace_level", "=", "tf", ".", "RunOptions", ".", "FULL_TRACE", ")", "\n", "run_metadata", "=", "tf", ".", "RunMetadata", "(", ")", "\n", "summary", ",", "policy_loss", ",", "value_loss", ",", "policy_entropy", ",", "approxkl", ",", "clipfrac", ",", "_", "=", "self", ".", "sess", ".", "run", "(", "\n", "[", "self", ".", "summary", ",", "self", ".", "pg_loss", ",", "self", ".", "vf_loss", ",", "self", ".", "entropy", ",", "self", ".", "approxkl", ",", "self", ".", "clipfrac", ",", "\n", "self", ".", "_train", "]", ",", "\n", "td_map", ",", "options", "=", "run_options", ",", "run_metadata", "=", "run_metadata", ")", "\n", "writer", ".", "add_run_metadata", "(", "run_metadata", ",", "'step%d'", "%", "(", "update", "*", "update_fac", ")", ")", "\n", "", "else", ":", "\n", "                ", "summary", ",", "policy_loss", ",", "value_loss", ",", "policy_entropy", ",", "approxkl", ",", "clipfrac", ",", "_", "=", "self", ".", "sess", ".", "run", "(", "\n", "[", "self", ".", "summary", ",", "self", ".", "pg_loss", ",", "self", ".", "vf_loss", ",", "self", ".", "entropy", ",", "self", ".", "approxkl", ",", "self", ".", "clipfrac", ",", "\n", "self", ".", "_train", "]", ",", "\n", "td_map", ")", "\n", "", "writer", ".", "add_summary", "(", "summary", ",", "(", "update", "*", "update_fac", ")", ")", "\n", "", "else", ":", "\n", "            ", "policy_loss", ",", "value_loss", ",", "policy_entropy", ",", "approxkl", ",", "clipfrac", ",", "_", "=", "self", ".", "sess", ".", "run", "(", "\n", "[", "self", ".", "pg_loss", ",", "self", ".", "vf_loss", ",", "self", ".", "entropy", ",", "self", ".", "approxkl", ",", "self", ".", "clipfrac", ",", "\n", "self", ".", "_train", "]", ",", "td_map", ")", "\n", "\n", "", "return", "policy_loss", ",", "value_loss", ",", "policy_entropy", ",", "approxkl", ",", "clipfrac", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.agac.AGAC.learn": [[324, 435], ["core.tf_util.get_schedule_fn", "core.tf_util.get_schedule_fn", "core.tf_util.get_schedule_fn", "core.tf_util.get_schedule_fn", "agac.AGAC._init_num_timesteps", "agac.AGAC._init_callback", "core.base_class.SetVerbosity", "core.base_class.TensorboardWriter", "agac.AGAC._setup_learn", "time.time", "agac.AGAC.on_training_start", "range", "agac.AGAC.on_training_end", "locals", "globals", "time.time", "agac.AGAC.learning_rate", "agac.AGAC.agac_c", "agac.AGAC.cliprange", "core.tf_util.get_schedule_fn.", "agac.AGAC.on_rollout_start", "agac.AGAC.runner.run", "agac.AGAC.on_rollout_end", "agac.AGAC.ep_info_buf.extend", "numpy.mean", "time.time", "int", "numpy.arange", "range", "numpy.arange", "numpy.arange().reshape", "range", "core.tf_util.total_episode_reward_logger", "core.math_util.explained_variance", "core.logger.logkv", "core.logger.logkv", "core.logger.logkv", "core.logger.logkv", "core.logger.logkv", "core.logger.logkv", "core.logger.logkv", "zip", "core.logger.dumpkvs", "numpy.random.shuffle", "range", "numpy.random.shuffle", "range", "undiscounted_reward.reshape", "masks.reshape", "float", "core.logger.logkv", "core.logger.logkv", "core.logger.logkv", "mb_loss_vals.append", "numpy.arange", "flat_indices[].ravel", "mb_loss_vals.append", "len", "len", "core.math_util.safe_mean", "core.math_util.safe_mean", "agac.AGAC._train_step", "agac.AGAC._train_step"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util.get_schedule_fn", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util.get_schedule_fn", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util.get_schedule_fn", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util.get_schedule_fn", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.BaseRLModel._init_num_timesteps", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.EvalCallback._init_callback", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.BaseRLModel._setup_learn", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.BaseCallback.on_training_start", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.BaseCallback.on_training_end", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.BaseCallback.on_rollout_start", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.runners.AbstractEnvRunner.run", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.BaseCallback.on_rollout_end", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util.total_episode_reward_logger", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.math_util.explained_variance", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.logkv", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.logkv", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.logkv", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.logkv", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.logkv", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.logkv", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.logkv", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.dumpkvs", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.logkv", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.logkv", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.logkv", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.math_util.safe_mean", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.math_util.safe_mean", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.agac.AGAC._train_step", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.agac.AGAC._train_step"], ["", "def", "learn", "(", "self", ",", "total_timesteps", ",", "callback", "=", "None", ",", "log_interval", "=", "1", ",", "tb_log_name", "=", "\"PPO2\"", ",", "\n", "reset_num_timesteps", "=", "True", ")", ":", "\n", "# Transform to callable if needed", "\n", "        ", "self", ".", "learning_rate", "=", "get_schedule_fn", "(", "self", ".", "learning_rate", ")", "\n", "self", ".", "agac_c", "=", "get_schedule_fn", "(", "self", ".", "agac_c", ")", "\n", "self", ".", "cliprange", "=", "get_schedule_fn", "(", "self", ".", "cliprange", ")", "\n", "cliprange_vf", "=", "get_schedule_fn", "(", "self", ".", "cliprange_vf", ")", "\n", "\n", "new_tb_log", "=", "self", ".", "_init_num_timesteps", "(", "reset_num_timesteps", ")", "\n", "callback", "=", "self", ".", "_init_callback", "(", "callback", ")", "\n", "\n", "with", "SetVerbosity", "(", "self", ".", "verbose", ")", ",", "TensorboardWriter", "(", "self", ".", "graph", ",", "self", ".", "tensorboard_log", ",", "tb_log_name", ",", "new_tb_log", ")", "as", "writer", ":", "\n", "            ", "self", ".", "_setup_learn", "(", ")", "\n", "\n", "t_first_start", "=", "time", ".", "time", "(", ")", "\n", "n_updates", "=", "total_timesteps", "//", "self", ".", "n_batch", "\n", "\n", "callback", ".", "on_training_start", "(", "locals", "(", ")", ",", "globals", "(", ")", ")", "\n", "\n", "for", "update", "in", "range", "(", "1", ",", "n_updates", "+", "1", ")", ":", "\n", "                ", "assert", "self", ".", "n_batch", "%", "self", ".", "nminibatches", "==", "0", ",", "(", "\"The number of minibatches (`nminibatches`) \"", "\n", "\"is not a factor of the total number of samples \"", "\n", "\"collected per rollout (`n_batch`), \"", "\n", "\"some samples won't be used.\"", "\n", ")", "\n", "batch_size", "=", "self", ".", "n_batch", "//", "self", ".", "nminibatches", "\n", "t_start", "=", "time", ".", "time", "(", ")", "\n", "frac", "=", "1.0", "-", "(", "update", "-", "1.0", ")", "/", "n_updates", "\n", "lr_now", "=", "self", ".", "learning_rate", "(", "frac", ")", "\n", "agac_c_now", "=", "self", ".", "agac_c", "(", "frac", ")", "\n", "cliprange_now", "=", "self", ".", "cliprange", "(", "frac", ")", "\n", "cliprange_vf_now", "=", "cliprange_vf", "(", "frac", ")", "\n", "\n", "callback", ".", "on_rollout_start", "(", ")", "\n", "rollout", "=", "self", ".", "runner", ".", "run", "(", "callback", ")", "\n", "# Unpack", "\n", "obs", ",", "returns", ",", "true_returns", ",", "masks", ",", "actions", ",", "values", ",", "neglogpacs", ",", "pi_probas", ",", "pi_adv_logits", ",", "states", ",", "ep_infos", ",", "undiscounted_reward", "=", "rollout", "\n", "callback", ".", "on_rollout_end", "(", ")", "\n", "\n", "# Early stopping due to the callback", "\n", "if", "not", "self", ".", "runner", ".", "continue_training", ":", "\n", "                    ", "break", "\n", "\n", "", "self", ".", "ep_info_buf", ".", "extend", "(", "ep_infos", ")", "\n", "mb_loss_vals", "=", "[", "]", "\n", "if", "states", "is", "None", ":", "# nonrecurrent version", "\n", "                    ", "update_fac", "=", "self", ".", "n_batch", "//", "self", ".", "nminibatches", "//", "self", ".", "noptepochs", "+", "1", "\n", "inds", "=", "np", ".", "arange", "(", "self", ".", "n_batch", ")", "\n", "for", "epoch_num", "in", "range", "(", "self", ".", "noptepochs", ")", ":", "\n", "                        ", "np", ".", "random", ".", "shuffle", "(", "inds", ")", "\n", "for", "start", "in", "range", "(", "0", ",", "self", ".", "n_batch", ",", "batch_size", ")", ":", "\n", "                            ", "timestep", "=", "self", ".", "num_timesteps", "//", "update_fac", "+", "(", "(", "self", ".", "noptepochs", "*", "self", ".", "n_batch", "+", "epoch_num", "*", "\n", "self", ".", "n_batch", "+", "start", ")", "//", "batch_size", ")", "\n", "end", "=", "start", "+", "batch_size", "\n", "mbinds", "=", "inds", "[", "start", ":", "end", "]", "\n", "slices", "=", "(", "arr", "[", "mbinds", "]", "for", "arr", "in", "(", "\n", "obs", ",", "returns", ",", "true_returns", ",", "masks", ",", "actions", ",", "values", ",", "neglogpacs", ",", "pi_probas", ",", "\n", "pi_adv_logits", ")", ")", "\n", "mb_loss_vals", ".", "append", "(", "\n", "self", ".", "_train_step", "(", "lr_now", ",", "agac_c_now", ",", "cliprange_now", ",", "*", "slices", ",", "writer", "=", "writer", ",", "\n", "update", "=", "timestep", ",", "cliprange_vf", "=", "cliprange_vf_now", ")", ")", "\n", "\n", "", "", "", "else", ":", "# recurrent version", "\n", "                    ", "update_fac", "=", "self", ".", "n_batch", "//", "self", ".", "nminibatches", "//", "self", ".", "noptepochs", "//", "self", ".", "n_steps", "+", "1", "\n", "assert", "self", ".", "n_envs", "%", "self", ".", "nminibatches", "==", "0", "\n", "env_indices", "=", "np", ".", "arange", "(", "self", ".", "n_envs", ")", "\n", "flat_indices", "=", "np", ".", "arange", "(", "self", ".", "n_envs", "*", "self", ".", "n_steps", ")", ".", "reshape", "(", "self", ".", "n_envs", ",", "self", ".", "n_steps", ")", "\n", "envs_per_batch", "=", "batch_size", "//", "self", ".", "n_steps", "\n", "for", "epoch_num", "in", "range", "(", "self", ".", "noptepochs", ")", ":", "\n", "                        ", "np", ".", "random", ".", "shuffle", "(", "env_indices", ")", "\n", "for", "start", "in", "range", "(", "0", ",", "self", ".", "n_envs", ",", "envs_per_batch", ")", ":", "\n", "                            ", "timestep", "=", "self", ".", "num_timesteps", "//", "update_fac", "+", "(", "(", "self", ".", "noptepochs", "*", "self", ".", "n_envs", "+", "epoch_num", "*", "\n", "self", ".", "n_envs", "+", "start", ")", "//", "envs_per_batch", ")", "\n", "end", "=", "start", "+", "envs_per_batch", "\n", "mb_env_inds", "=", "env_indices", "[", "start", ":", "end", "]", "\n", "mb_flat_inds", "=", "flat_indices", "[", "mb_env_inds", "]", ".", "ravel", "(", ")", "\n", "slices", "=", "(", "arr", "[", "mb_flat_inds", "]", "for", "arr", "in", "(", "obs", ",", "returns", ",", "masks", ",", "actions", ",", "values", ",", "neglogpacs", ")", ")", "\n", "mb_states", "=", "states", "[", "mb_env_inds", "]", "\n", "mb_loss_vals", ".", "append", "(", "self", ".", "_train_step", "(", "lr_now", ",", "cliprange_now", ",", "*", "slices", ",", "update", "=", "timestep", ",", "\n", "writer", "=", "writer", ",", "states", "=", "mb_states", ",", "\n", "cliprange_vf", "=", "cliprange_vf_now", ")", ")", "\n", "\n", "", "", "", "loss_vals", "=", "np", ".", "mean", "(", "mb_loss_vals", ",", "axis", "=", "0", ")", "\n", "t_now", "=", "time", ".", "time", "(", ")", "\n", "fps", "=", "int", "(", "self", ".", "n_batch", "/", "(", "t_now", "-", "t_start", ")", ")", "\n", "\n", "if", "writer", "is", "not", "None", ":", "\n", "                    ", "total_episode_reward_logger", "(", "self", ".", "episode_reward", ",", "\n", "undiscounted_reward", ".", "reshape", "(", "(", "self", ".", "n_envs", ",", "self", ".", "n_steps", ")", ")", ",", "\n", "masks", ".", "reshape", "(", "(", "self", ".", "n_envs", ",", "self", ".", "n_steps", ")", ")", ",", "\n", "writer", ",", "self", ".", "num_timesteps", ")", "\n", "\n", "", "if", "self", ".", "verbose", ">=", "1", "and", "(", "update", "%", "log_interval", "==", "0", "or", "update", "==", "1", ")", ":", "\n", "                    ", "explained_var", "=", "explained_variance", "(", "values", ",", "returns", ")", "\n", "logger", ".", "logkv", "(", "\"serial_timesteps\"", ",", "update", "*", "self", ".", "n_steps", ")", "\n", "logger", ".", "logkv", "(", "\"n_updates\"", ",", "update", ")", "\n", "logger", ".", "logkv", "(", "\"total_timesteps\"", ",", "self", ".", "num_timesteps", ")", "\n", "logger", ".", "logkv", "(", "\"fps\"", ",", "fps", ")", "\n", "logger", ".", "logkv", "(", "\"agac_c\"", ",", "agac_c_now", ")", "\n", "logger", ".", "logkv", "(", "\"explained_variance\"", ",", "float", "(", "explained_var", ")", ")", "\n", "if", "len", "(", "self", ".", "ep_info_buf", ")", ">", "0", "and", "len", "(", "self", ".", "ep_info_buf", "[", "0", "]", ")", ">", "0", ":", "\n", "                        ", "logger", ".", "logkv", "(", "'ep_reward_mean'", ",", "safe_mean", "(", "[", "ep_info", "[", "'r'", "]", "for", "ep_info", "in", "self", ".", "ep_info_buf", "]", ")", ")", "\n", "logger", ".", "logkv", "(", "'ep_len_mean'", ",", "safe_mean", "(", "[", "ep_info", "[", "'l'", "]", "for", "ep_info", "in", "self", ".", "ep_info_buf", "]", ")", ")", "\n", "", "logger", ".", "logkv", "(", "'time_elapsed'", ",", "t_start", "-", "t_first_start", ")", "\n", "for", "(", "loss_val", ",", "loss_name", ")", "in", "zip", "(", "loss_vals", ",", "self", ".", "loss_names", ")", ":", "\n", "                        ", "logger", ".", "logkv", "(", "loss_name", ",", "loss_val", ")", "\n", "", "logger", ".", "dumpkvs", "(", ")", "\n", "\n", "", "", "callback", ".", "on_training_end", "(", ")", "\n", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.agac.AGAC.save": [[436, 463], ["agac.AGAC.get_parameters", "agac.AGAC._save_to_file"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.BaseRLModel.get_parameters", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.BaseRLModel._save_to_file"], ["", "", "def", "save", "(", "self", ",", "save_path", ",", "cloudpickle", "=", "False", ")", ":", "\n", "        ", "data", "=", "{", "\n", "\"gamma\"", ":", "self", ".", "gamma", ",", "\n", "\"n_steps\"", ":", "self", ".", "n_steps", ",", "\n", "\"vf_coef\"", ":", "self", ".", "vf_coef", ",", "\n", "\"ent_coef\"", ":", "self", ".", "ent_coef", ",", "\n", "\"max_grad_norm\"", ":", "self", ".", "max_grad_norm", ",", "\n", "\"learning_rate\"", ":", "self", ".", "learning_rate", ",", "\n", "\"lam\"", ":", "self", ".", "lam", ",", "\n", "\"nminibatches\"", ":", "self", ".", "nminibatches", ",", "\n", "\"noptepochs\"", ":", "self", ".", "noptepochs", ",", "\n", "\"cliprange\"", ":", "self", ".", "cliprange", ",", "\n", "\"cliprange_vf\"", ":", "self", ".", "cliprange_vf", ",", "\n", "\"verbose\"", ":", "self", ".", "verbose", ",", "\n", "\"policy\"", ":", "self", ".", "policy", ",", "\n", "\"observation_space\"", ":", "self", ".", "observation_space", ",", "\n", "\"action_space\"", ":", "self", ".", "action_space", ",", "\n", "\"n_envs\"", ":", "self", ".", "n_envs", ",", "\n", "\"n_cpu_tf_sess\"", ":", "self", ".", "n_cpu_tf_sess", ",", "\n", "\"seed\"", ":", "self", ".", "seed", ",", "\n", "\"_vectorize_action\"", ":", "self", ".", "_vectorize_action", ",", "\n", "\"policy_kwargs\"", ":", "self", ".", "policy_kwargs", "\n", "}", "\n", "\n", "params_to_save", "=", "self", ".", "get_parameters", "(", ")", "\n", "\n", "self", ".", "_save_to_file", "(", "save_path", ",", "data", "=", "data", ",", "params", "=", "params_to_save", ",", "cloudpickle", "=", "cloudpickle", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.agac.Runner.__init__": [[466, 479], ["core.runners.AbstractEnvRunner.__init__"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", ",", "env", ",", "model", ",", "n_steps", ",", "episodic_count", ",", "gamma", ",", "lam", ")", ":", "\n", "        ", "\"\"\"\n        A runner to learn the policy of an environment for a model\n\n        :param env: (Gym environment) The environment to learn from\n        :param model: (Model) The model to learn\n        :param n_steps: (int) The number of steps to run for each environment\n        :param gamma: (float) Discount factor\n        :param lam: (float) Factor for trade-off of bias vs variance for Generalized Advantage Estimator\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", "env", "=", "env", ",", "model", "=", "model", ",", "n_steps", "=", "n_steps", ",", "episodic_count", "=", "episodic_count", ")", "\n", "self", ".", "lam", "=", "lam", "\n", "self", ".", "gamma", "=", "gamma", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.agac.Runner._run": [[480, 580], ["range", "numpy.asarray", "numpy.asarray", "numpy.asarray", "numpy.asarray", "numpy.asarray", "numpy.asarray", "numpy.asarray", "numpy.asarray", "numpy.asarray", "agac.Runner.model.value", "numpy.expand_dims", "numpy.copy", "numpy.zeros_like", "numpy.zeros_like", "reversed", "map", "agac.Runner.model.step", "numpy.asarray.append", "numpy.asarray.append", "numpy.asarray.append", "numpy.asarray.append", "numpy.asarray.append", "numpy.asarray.append", "numpy.asarray.append", "isinstance", "agac.Runner.env.step", "numpy.asarray.append", "range", "agac.Runner.obs.copy", "tuple", "numpy.expand_dims.append", "numpy.clip", "info.get", "episode_state_count_dict.update", "agac.Runner.callback.on_step", "ep_infos.append", "numpy.sqrt", "episode_state_count_dict.get"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.BaseRLModel.value", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnv.step", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnv.step", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.math_util.RunningMeanStd.update", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.BaseCallback.on_step"], ["", "def", "_run", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Run a learning step of the model\n\n        :return:\n            - observations: (np.ndarray) the observations\n            - rewards: (np.ndarray) the rewards\n            - masks: (numpy bool) whether an episode is over or not\n            - actions: (np.ndarray) the actions\n            - values: (np.ndarray) the value function output\n            - negative log probabilities: (np.ndarray)\n            - states: (np.ndarray) the internal states of the recurrent policies\n            - infos: (dict) the extra information of the model\n        \"\"\"", "\n", "# mb stands for minibatch", "\n", "mb_obs", ",", "mb_rewards", ",", "mb_actions", ",", "mb_values", ",", "mb_dones", ",", "mb_neglogpacs", ",", "mb_pi_probas", ",", "mb_state_count", ",", "mb_pi_adv_logits", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "mb_states", "=", "self", ".", "states", "\n", "ep_infos", "=", "[", "]", "\n", "episode_state_count_dict", "=", "{", "}", "\n", "for", "_", "in", "range", "(", "self", ".", "n_steps", ")", ":", "\n", "            ", "actions", ",", "values", ",", "self", ".", "states", ",", "neglogpacs", ",", "pi_probas", ",", "pi_adv_logits", "=", "self", ".", "model", ".", "step", "(", "self", ".", "obs", ",", "\n", "self", ".", "states", ",", "\n", "self", ".", "dones", ")", "\n", "mb_obs", ".", "append", "(", "self", ".", "obs", ".", "copy", "(", ")", ")", "\n", "if", "self", ".", "episodic_count", ":", "\n", "                ", "episode_state_key", "=", "tuple", "(", "self", ".", "env", ".", "envs", "[", "0", "]", ".", "agent_pos", ")", "\n", "if", "episode_state_key", "in", "episode_state_count_dict", ":", "\n", "                    ", "episode_state_count_dict", "[", "episode_state_key", "]", "+=", "1", "\n", "", "else", ":", "\n", "                    ", "episode_state_count_dict", ".", "update", "(", "{", "episode_state_key", ":", "1", "}", ")", "\n", "", "mb_state_count", ".", "append", "(", "1", "/", "np", ".", "sqrt", "(", "episode_state_count_dict", ".", "get", "(", "episode_state_key", ")", ")", ")", "\n", "", "mb_actions", ".", "append", "(", "actions", ")", "\n", "mb_values", ".", "append", "(", "values", ")", "\n", "mb_neglogpacs", ".", "append", "(", "neglogpacs", ")", "\n", "mb_pi_probas", ".", "append", "(", "pi_probas", ")", "\n", "mb_pi_adv_logits", ".", "append", "(", "pi_adv_logits", ")", "\n", "mb_dones", ".", "append", "(", "self", ".", "dones", ")", "\n", "clipped_actions", "=", "actions", "\n", "# Clip the actions to avoid out of bound error", "\n", "if", "isinstance", "(", "self", ".", "env", ".", "action_space", ",", "gym", ".", "spaces", ".", "Box", ")", ":", "\n", "                ", "clipped_actions", "=", "np", ".", "clip", "(", "actions", ",", "self", ".", "env", ".", "action_space", ".", "low", ",", "self", ".", "env", ".", "action_space", ".", "high", ")", "\n", "", "self", ".", "obs", "[", ":", "]", ",", "rewards", ",", "self", ".", "dones", ",", "infos", "=", "self", ".", "env", ".", "step", "(", "clipped_actions", ")", "\n", "\n", "self", ".", "model", ".", "num_timesteps", "+=", "self", ".", "n_envs", "\n", "\n", "if", "self", ".", "callback", "is", "not", "None", ":", "\n", "# Abort training early", "\n", "                ", "if", "self", ".", "callback", ".", "on_step", "(", ")", "is", "False", ":", "\n", "                    ", "self", ".", "continue_training", "=", "False", "\n", "# Return dummy values", "\n", "return", "[", "None", "]", "*", "9", "\n", "\n", "", "", "for", "info", "in", "infos", ":", "\n", "                ", "maybe_ep_info", "=", "info", ".", "get", "(", "'episode'", ")", "\n", "if", "maybe_ep_info", "is", "not", "None", ":", "\n", "                    ", "ep_infos", ".", "append", "(", "maybe_ep_info", ")", "\n", "", "", "mb_rewards", ".", "append", "(", "rewards", ")", "\n", "# Batch of steps to batch of rollouts", "\n", "", "mb_obs", "=", "np", ".", "asarray", "(", "mb_obs", ",", "dtype", "=", "self", ".", "obs", ".", "dtype", ")", "\n", "mb_rewards", "=", "np", ".", "asarray", "(", "mb_rewards", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "mb_state_count", "=", "np", ".", "asarray", "(", "mb_state_count", ",", "dtype", "=", "self", ".", "obs", ".", "dtype", ")", "\n", "mb_actions", "=", "np", ".", "asarray", "(", "mb_actions", ")", "\n", "mb_values", "=", "np", ".", "asarray", "(", "mb_values", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "mb_neglogpacs", "=", "np", ".", "asarray", "(", "mb_neglogpacs", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "mb_pi_probas", "=", "np", ".", "asarray", "(", "mb_pi_probas", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "mb_pi_adv_logits", "=", "np", ".", "asarray", "(", "mb_pi_adv_logits", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "mb_dones", "=", "np", ".", "asarray", "(", "mb_dones", ",", "dtype", "=", "np", ".", "bool", ")", "\n", "last_values", "=", "self", ".", "model", ".", "value", "(", "self", ".", "obs", ",", "self", ".", "states", ",", "self", ".", "dones", ")", "\n", "\n", "# Counting intrinsic rewards", "\n", "mb_state_count", "=", "np", ".", "expand_dims", "(", "mb_state_count", ",", "axis", "=", "1", ")", "\n", "true_reward", "=", "np", ".", "copy", "(", "mb_rewards", ")", "\n", "if", "self", ".", "episodic_count", ":", "\n", "            ", "mb_rewards", "=", "mb_rewards", "+", "mb_state_count", "*", "0.1", "\n", "\n", "# discount/bootstrap off value fn", "\n", "", "mb_advs", "=", "np", ".", "zeros_like", "(", "mb_rewards", ")", "\n", "mb_true_advs", "=", "np", ".", "zeros_like", "(", "true_reward", ")", "\n", "last_gae_lam", "=", "0", "\n", "true_last_gae_lam", "=", "0", "\n", "for", "step", "in", "reversed", "(", "range", "(", "self", ".", "n_steps", ")", ")", ":", "\n", "            ", "if", "step", "==", "self", ".", "n_steps", "-", "1", ":", "\n", "                ", "nextnonterminal", "=", "1.0", "-", "self", ".", "dones", "\n", "nextvalues", "=", "last_values", "\n", "", "else", ":", "\n", "                ", "nextnonterminal", "=", "1.0", "-", "mb_dones", "[", "step", "+", "1", "]", "\n", "nextvalues", "=", "mb_values", "[", "step", "+", "1", "]", "\n", "", "delta", "=", "mb_rewards", "[", "step", "]", "+", "self", ".", "gamma", "*", "nextvalues", "*", "nextnonterminal", "-", "mb_values", "[", "step", "]", "\n", "true_delta", "=", "true_reward", "[", "step", "]", "+", "self", ".", "gamma", "*", "nextvalues", "*", "nextnonterminal", "-", "mb_values", "[", "step", "]", "\n", "mb_advs", "[", "step", "]", "=", "last_gae_lam", "=", "delta", "+", "self", ".", "gamma", "*", "self", ".", "lam", "*", "nextnonterminal", "*", "last_gae_lam", "\n", "mb_true_advs", "[", "\n", "step", "]", "=", "true_last_gae_lam", "=", "true_delta", "+", "self", ".", "gamma", "*", "self", ".", "lam", "*", "nextnonterminal", "*", "true_last_gae_lam", "\n", "", "mb_returns", "=", "mb_advs", "+", "mb_values", "\n", "mb_true_returns", "=", "mb_true_advs", "+", "mb_values", "\n", "mb_obs", ",", "mb_returns", ",", "mb_true_returns", ",", "mb_dones", ",", "mb_actions", ",", "mb_values", ",", "mb_neglogpacs", ",", "mb_pi_probas", ",", "mb_pi_adv_logits", ",", "true_reward", "=", "map", "(", "swap_and_flatten", ",", "(", "\n", "mb_obs", ",", "mb_returns", ",", "mb_true_returns", ",", "mb_dones", ",", "mb_actions", ",", "mb_values", ",", "mb_neglogpacs", ",", "mb_pi_probas", ",", "\n", "mb_pi_adv_logits", ",", "true_reward", ")", ")", "\n", "\n", "return", "mb_obs", ",", "mb_returns", ",", "mb_true_returns", ",", "mb_dones", ",", "mb_actions", ",", "mb_values", ",", "mb_neglogpacs", ",", "mb_pi_probas", ",", "mb_pi_adv_logits", ",", "mb_states", ",", "ep_infos", ",", "true_reward", "\n", "", "", ""]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.logger.Logger.__init__": [[31, 67], ["time.time", "logger.Logger._create_dirs", "yaml.dump", "logger.Logger._summary_writer.add_text", "logger.Logger.print_experiment_info", "neptune.init", "dataclasses.asdict", "logger.Logger._neptune_run[].add", "open", "yaml.dump", "dataclasses.asdict", "dataclasses.asdict"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.logger.Logger._create_dirs", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.logger.Logger.print_experiment_info", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.replay_buffer.ReplayBuffer.add"], ["def", "__init__", "(", "self", ",", "config", ":", "ExperimentConfig", ")", ":", "\n", "        ", "self", ".", "start_time", "=", "time", ".", "time", "(", ")", "\n", "self", ".", "_config", "=", "config", "\n", "\n", "# Get params", "\n", "logging_config", "=", "config", ".", "logging", "\n", "self", ".", "dir", "=", "\".\"", "\n", "self", ".", "_use_neptune", "=", "logging_config", ".", "use_neptune", "\n", "\n", "# Initialize neptune", "\n", "if", "self", ".", "_use_neptune", ":", "\n", "            ", "user_name", "=", "logging_config", ".", "neptune_user_name", "\n", "project_name", "=", "logging_config", ".", "neptune_project_name", "\n", "self", ".", "_neptune_run", "=", "neptune", ".", "init", "(", "user_name", "+", "\"/\"", "+", "project_name", ")", "\n", "self", ".", "_neptune_run", "[", "\"parameters\"", "]", "=", "dataclasses", ".", "asdict", "(", "config", ")", "\n", "tags", "=", "[", "logging_config", ".", "experiment_name", "]", "+", "[", "config", ".", "algorithm", ".", "env_name", "]", "\n", "self", ".", "_neptune_run", "[", "\"sys/tags\"", "]", ".", "add", "(", "tags", ")", "\n", "\n", "# Create directories", "\n", "", "self", ".", "_create_dirs", "(", "\n", "config", ".", "algorithm", ".", "env_name", ",", "\n", "config", ".", "algorithm", ".", "seed", ",", "\n", "config", ".", "logging", ".", "save_models", ",", "\n", ")", "\n", "\n", "# Dump config file", "\n", "with", "open", "(", "self", ".", "_save_dir", "+", "\"/\"", "+", "\"config.yaml\"", ",", "\"w\"", ")", "as", "cfg", ":", "\n", "            ", "yaml", ".", "dump", "(", "dataclasses", ".", "asdict", "(", "config", ")", ",", "cfg", ",", "default_flow_style", "=", "True", ")", "\n", "\n", "# Dump config", "\n", "", "config_str", "=", "yaml", ".", "dump", "(", "dataclasses", ".", "asdict", "(", "config", ")", ",", "default_flow_style", "=", "True", ")", "\n", "self", ".", "_summary_writer", ".", "add_text", "(", "\"config\"", ",", "config_str", ",", "0", ")", "\n", "\n", "self", ".", "_logs_df", "=", "None", "\n", "\n", "self", ".", "print_experiment_info", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.logger.Logger.print_experiment_info": [[68, 86], ["print", "print", "print", "print", "print", "print", "print", "print", "print", "print"], "methods", ["None"], ["", "def", "print_experiment_info", "(", "self", ")", ":", "\n", "        ", "print", "(", "\"---------------------------------\"", ")", "\n", "env_name", "=", "self", ".", "_config", ".", "algorithm", ".", "env_name", "\n", "print", "(", "\"Env: {}\"", ".", "format", "(", "env_name", ")", ",", "flush", "=", "True", ")", "\n", "print", "(", "\"Agent: AGAC PPO\"", ",", "flush", "=", "True", ")", "\n", "print", "(", "\"Seed: {}\"", ".", "format", "(", "self", ".", "_config", ".", "algorithm", ".", "seed", ")", ",", "flush", "=", "True", ")", "\n", "print", "(", "\"---------------------------------\"", ")", "\n", "\n", "rl_config", "=", "self", ".", "_config", ".", "reinforcement_learning", "\n", "learning_rate", "=", "rl_config", ".", "actor_learning_rate", "\n", "print", "(", "\" actor lr: {}\"", ".", "format", "(", "learning_rate", ")", ",", "flush", "=", "True", ")", "\n", "learning_rate", "=", "rl_config", ".", "critic_learning_rate", "\n", "print", "(", "\" critic lr: {}\"", ".", "format", "(", "learning_rate", ")", ",", "flush", "=", "True", ")", "\n", "learning_rate", "=", "rl_config", ".", "adversary_learning_rate", "\n", "print", "(", "\" adversary lr: {}\"", ".", "format", "(", "learning_rate", ")", ",", "flush", "=", "True", ")", "\n", "layers_dim", "=", "rl_config", ".", "layers_dim", "\n", "print", "(", "\"layers_dim: {}\"", ".", "format", "(", "layers_dim", ")", ",", "flush", "=", "True", ")", "\n", "print", "(", "\"---------------------------------\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.logger.Logger._create_dirs": [[87, 99], ["datetime.datetime.datetime.now().strftime", "torch.utils.tensorboard.SummaryWriter", "os.path.exists", "os.makedirs", "os.makedirs", "datetime.datetime.datetime.now", "os.path.exists"], "methods", ["None"], ["", "def", "_create_dirs", "(", "self", ",", "env_name", ",", "seed", ",", "save_models", ")", ":", "\n", "# create directories", "\n", "        ", "save_dir", "=", "\"./runs/\"", "+", "\"%s_%s_\"", "%", "(", "\"AGAC\"", ",", "env_name", ")", "\n", "save_dir", "+=", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "\"%b%d_%H-%M-%S\"", ")", "\n", "save_dir", "+=", "\"_%s\"", "%", "seed", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "save_dir", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "save_dir", ")", "\n", "", "self", ".", "_save_dir", "=", "save_dir", "\n", "self", ".", "_summary_writer", "=", "SummaryWriter", "(", "self", ".", "_save_dir", ")", "\n", "\n", "if", "save_models", "and", "not", "os", ".", "path", ".", "exists", "(", "self", ".", "_save_dir", "+", "\"/models\"", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "self", ".", "_save_dir", "+", "\"/models\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.logger.Logger.log": [[100, 117], ["logger.Logger._log_in_tensorboard", "logger.Logger._log_in_file", "logger.Logger._log_in_neptune", "logger.Logger._save_actor_weights"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.logger.Logger._log_in_tensorboard", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.logger.Logger._log_in_file", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.logger.Logger._log_in_neptune", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.logger.Logger._save_actor_weights"], ["", "", "def", "log", "(", "self", ",", "logs", ":", "List", "[", "LogData", "]", ")", ":", "\n", "        ", "\"\"\"\n        Log data to different channels and save model if required.\n        \"\"\"", "\n", "# get number of steps", "\n", "steps", "=", "[", "log", ".", "value", "for", "log", "in", "logs", "if", "log", ".", "name", "==", "\"total_steps\"", "]", "[", "0", "]", "\n", "\n", "# log", "\n", "self", ".", "_log_in_tensorboard", "(", "steps", ",", "logs", ")", "\n", "self", ".", "_log_in_file", "(", "logs", ")", "\n", "if", "self", ".", "_use_neptune", ":", "\n", "            ", "self", ".", "_log_in_neptune", "(", "logs", ")", "\n", "\n", "# save actors if needed", "\n", "", "if", "self", ".", "_config", ".", "logging", ".", "save_models", ":", "\n", "            ", "weights", "=", "[", "log", ".", "value", "for", "log", "in", "logs", "if", "log", ".", "name", "==", "\"actor_weights\"", "]", "[", "0", "]", "\n", "self", ".", "_save_actor_weights", "(", "weights", ",", "steps", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.logger.Logger._log_in_tensorboard": [[118, 128], ["logger.Logger._summary_writer.add_scalar", "logger.Logger._summary_writer.add_image"], "methods", ["None"], ["", "", "def", "_log_in_tensorboard", "(", "self", ",", "steps", ":", "int", ",", "logs", ":", "List", "[", "LogData", "]", ")", ":", "\n", "        ", "\"\"\"\n        Log data in tensorboard.\n        \"\"\"", "\n", "\n", "for", "log", "in", "logs", ":", "\n", "            ", "if", "log", ".", "type", "==", "\"scalar\"", ":", "\n", "                ", "self", ".", "_summary_writer", ".", "add_scalar", "(", "log", ".", "name", ",", "log", ".", "value", ",", "steps", ")", "\n", "", "if", "log", ".", "type", "==", "\"image\"", ":", "\n", "                ", "self", ".", "_summary_writer", ".", "add_image", "(", "log", ".", "name", ",", "log", ".", "value", ",", "steps", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.logger.Logger._log_in_neptune": [[129, 153], ["print", "logger.Logger._neptune_run[].log", "logger.Logger._neptune_run[].log", "logger.Logger._neptune_run[].log", "neptune.new.types.File.as_image", "neptune.new.types.File.as_image", "log.value.transpose"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.log", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.log", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.log"], ["", "", "", "def", "_log_in_neptune", "(", "self", ",", "logs", ":", "List", "[", "LogData", "]", ")", ":", "\n", "        ", "\"\"\"\n        Log data in neptune.\n        \"\"\"", "\n", "# log scalar metrics", "\n", "try", ":", "\n", "            ", "for", "log", "in", "logs", ":", "\n", "                ", "if", "log", ".", "type", "==", "\"scalar\"", ":", "\n", "                    ", "self", ".", "_neptune_run", "[", "log", ".", "name", "]", ".", "log", "(", "log", ".", "value", ")", "\n", "", "if", "log", ".", "type", "==", "\"image\"", ":", "\n", "                    ", "if", "(", "log", ".", "value", ".", "ndim", "==", "3", ")", "and", "(", "log", ".", "value", ".", "shape", "[", "-", "1", "]", "!=", "3", ")", ":", "\n", "                        ", "self", ".", "_neptune_run", "[", "log", ".", "name", "]", ".", "log", "(", "\n", "File", ".", "as_image", "(", "log", ".", "value", ".", "transpose", "(", "1", ",", "2", ",", "0", ")", ")", "\n", ")", "\n", "", "else", ":", "\n", "                        ", "self", ".", "_neptune_run", "[", "log", ".", "name", "]", ".", "log", "(", "File", ".", "as_image", "(", "log", ".", "value", ")", ")", "\n", "\n", "# log also csv and pkl files", "\n", "# self._neptune_run[\"logs.csv\"].upload(self._save_dir + \"/logs.csv\")", "\n", "# self._neptune_run[\"logs.pkl\"].upload(self._save_dir + \"/logs.pkl\")", "\n", "# neptune.log_artifact(self._save_dir + \"/logs.csv\")", "\n", "# neptune.log_artifact(self._save_dir + \"/logs.pkl\")", "\n", "", "", "", "", "except", "RuntimeError", ":", "\n", "            ", "print", "(", "\"WARNING: failed to log in Neptune\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.logger.Logger._log_in_file": [[154, 180], ["logger.Logger._logs_df.to_csv", "logger.Logger._logs_df.to_pickle", "df_dict.items", "pandas.DataFrame", "logger.Logger._logs_df.append"], "methods", ["None"], ["", "", "def", "_log_in_file", "(", "self", ",", "logs", ":", "List", "[", "LogData", "]", ")", ":", "\n", "        ", "\"\"\"\n        Log data in tensorboard.\n        \"\"\"", "\n", "df_dict", "=", "{", "}", "\n", "for", "log", "in", "logs", ":", "\n", "            ", "if", "log", ".", "type", "==", "\"scalar\"", ":", "\n", "                ", "df_dict", "[", "log", ".", "name", "]", "=", "log", ".", "value", "\n", "", "elif", "log", ".", "type", "==", "\"array\"", ":", "\n", "                ", "if", "\"weights\"", "not", "in", "log", ".", "name", ":", "\n", "                    ", "df_dict", "[", "log", ".", "name", "]", "=", "log", ".", "value", "\n", "\n", "", "", "", "if", "self", ".", "_logs_df", "is", "None", ":", "\n", "# if not already done, create dataframe", "\n", "            ", "for", "key", ",", "value", "in", "df_dict", ".", "items", "(", ")", ":", "\n", "                ", "df_dict", "[", "key", "]", "=", "[", "value", "]", "\n", "", "self", ".", "_logs_df", "=", "pd", ".", "DataFrame", "(", "data", "=", "df_dict", ")", "\n", "\n", "", "else", ":", "\n", "# otherwise, add data in dataframe", "\n", "            ", "self", ".", "_logs_df", "=", "self", ".", "_logs_df", ".", "append", "(", "df_dict", ",", "ignore_index", "=", "True", ")", "\n", "\n", "# save in csv format", "\n", "", "self", ".", "_logs_df", ".", "to_csv", "(", "self", ".", "_save_dir", "+", "\"/logs.csv\"", ")", "\n", "# save in pickle format", "\n", "self", ".", "_logs_df", ".", "to_pickle", "(", "self", ".", "_save_dir", "+", "\"/logs.pkl\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.logger.Logger._save_actor_weights": [[181, 187], ["numpy.save"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.VecNormalize.save"], ["", "def", "_save_actor_weights", "(", "self", ",", "actor_weights", ",", "steps", ")", ":", "\n", "        ", "\"\"\"\n        Save the weights of the population actors.\n        \"\"\"", "\n", "path", "=", "self", ".", "_save_dir", "+", "\"/models/agent_steps_{}\"", ".", "format", "(", "steps", ")", "\n", "np", ".", "save", "(", "path", ",", "actor_weights", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.agac_trainer.AGAC.__init__": [[24, 83], ["copy.deepcopy", "agac.memory.Memory", "agac.agac_ppo.PPO", "welford.Welford", "welford.Welford", "bool", "TypeError", "TypeError", "agac.logger.Logger", "agac_trainer.AGAC._set_seed", "re.search", "agac.utils.DiscreteGrid", "isinstance", "isinstance", "hasattr", "AttributeError", "env.spec.id.lower"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.agac_trainer.AGAC._set_seed"], ["    ", "def", "__init__", "(", "self", ",", "config", ":", "ExperimentConfig", ",", "env", ":", "gym", ".", "Env", ")", ":", "\n", "        ", "self", ".", "_config", "=", "config", "\n", "self", ".", "_env", "=", "env", "\n", "self", ".", "_evaluation_env", "=", "deepcopy", "(", "env", ")", "\n", "self", ".", "_discrete", "=", "config", ".", "algorithm", ".", "discrete", "\n", "if", "self", ".", "_discrete", "and", "(", "not", "isinstance", "(", "env", ".", "action_space", ",", "Discrete", ")", ")", ":", "\n", "            ", "raise", "TypeError", "(", "\"`env.action_space` must be of `Discrete` type\"", ")", "\n", "", "if", "(", "not", "self", ".", "_discrete", ")", "and", "(", "not", "isinstance", "(", "env", ".", "action_space", ",", "Box", ")", ")", ":", "\n", "            ", "raise", "TypeError", "(", "\"`env.action_space` must be of `Box` type\"", ")", "\n", "\n", "", "self", ".", "_memory", "=", "Memory", "(", ")", "\n", "self", ".", "_ppo", "=", "PPO", "(", "env", ".", "observation_space", ",", "env", ".", "action_space", ",", "config", ")", "\n", "\n", "if", "rank", "==", "0", ":", "\n", "            ", "self", ".", "_logger", "=", "Logger", "(", "config", ")", "\n", "", "self", ".", "_intrinsic_returns_stats", "=", "Welford", "(", ")", "\n", "self", ".", "_extrinsic_returns_stats", "=", "Welford", "(", ")", "\n", "\n", "if", "config", ".", "algorithm", ".", "seed", "!=", "-", "1", ":", "\n", "            ", "self", ".", "_set_seed", "(", "config", ".", "algorithm", ".", "seed", "+", "rank", ")", "\n", "\n", "", "self", ".", "_train_freq", "=", "config", ".", "algorithm", ".", "train_freq", "//", "num_workers", "\n", "self", ".", "_eval_freq", "=", "config", ".", "algorithm", ".", "eval_freq", "//", "num_workers", "\n", "self", ".", "_max_steps", "=", "config", ".", "algorithm", ".", "max_steps", "//", "num_workers", "\n", "self", ".", "_num_epochs", "=", "config", ".", "algorithm", ".", "num_epochs", "\n", "if", "not", "self", ".", "_discrete", ":", "\n", "            ", "self", ".", "_max_action", "=", "env", ".", "action_space", ".", "high", "[", "0", "]", "\n", "", "self", ".", "_max_updates", "=", "self", ".", "_max_steps", "//", "self", ".", "_train_freq", "\n", "\n", "rl_config", "=", "config", ".", "reinforcement_learning", "\n", "self", ".", "_discount", "=", "rl_config", ".", "discount", "\n", "self", ".", "_lambda_gae", "=", "rl_config", ".", "lambda_gae", "\n", "self", ".", "_batch_size", "=", "rl_config", ".", "batch_size", "\n", "self", ".", "_intrinsic_reward_coefficient", "=", "rl_config", ".", "intrinsic_reward_coefficient", "\n", "self", ".", "_current_intrinsic_reward_coefficient", "=", "(", "\n", "rl_config", ".", "intrinsic_reward_coefficient", "\n", ")", "\n", "self", ".", "_episodic_count", "=", "rl_config", ".", "episodic_count_coefficient", ">", "0", "\n", "self", ".", "_episodic_count_coefficient", "=", "rl_config", ".", "episodic_count_coefficient", "\n", "if", "self", ".", "_episodic_count", ":", "\n", "            ", "if", "not", "hasattr", "(", "env", ",", "\"state_key_extraction\"", ")", ":", "\n", "                ", "raise", "AttributeError", "(", "\n", "f\"To use episodic counts, the environment must\"", "\n", "\"have a `state_key_extraction` method\"", "\n", ")", "\n", "# TODO: pass function in config", "\n", "", "self", ".", "_state_key_extraction", "=", "env", ".", "state_key_extraction", "\n", "\n", "# logging grid", "\n", "", "is_minigrid", "=", "bool", "(", "re", ".", "search", "(", "\"minigrid\"", ",", "env", ".", "spec", ".", "id", ".", "lower", "(", ")", ")", ")", "\n", "self", ".", "_display_grid", "=", "(", "\n", "DiscreteGrid", "(", "self", ".", "_evaluation_env", ")", "\n", "if", "(", "config", ".", "logging", ".", "log_grid", "and", "is_minigrid", ")", "\n", "else", "None", "\n", ")", "\n", "\n", "self", ".", "_total_timesteps", "=", "0", "\n", "self", ".", "_total_updates", "=", "0", "\n", "self", ".", "_episode_num", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.agac_trainer.AGAC._set_seed": [[84, 89], ["agac_trainer.AGAC._env.seed", "agac_trainer.AGAC._env.action_space.seed", "torch.manual_seed", "numpy.random.seed"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.seed", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.seed", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.seed"], ["", "def", "_set_seed", "(", "self", ",", "seed", ":", "int", ")", ":", "\n", "        ", "self", ".", "_env", ".", "seed", "(", "seed", ")", "\n", "self", ".", "_env", ".", "action_space", ".", "seed", "(", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.agac_trainer.AGAC.train": [[90, 274], ["agac_trainer.AGAC._ppo.select_action", "float", "float", "agac_trainer.AGAC._env.step", "observations.append", "next_observations.append", "actions.append", "rewards.append", "log_pis.append", "adv_log_pis.append", "logits_pis.append", "adv_logits_pis.append", "dones.append", "copy.deepcopy", "agac_trainer.AGAC._env.reset", "agac_trainer.AGAC.evaluate", "comm.Gather", "comm.gather", "agac_trainer.AGAC._state_key_extraction", "state_counter.get", "state_counts.append", "numpy.clip", "agac_trainer.AGAC.copy", "new_observation.copy", "numpy.clip.copy", "float", "numpy.empty", "numpy.empty.flatten", "numpy.array().flatten", "numpy.mean", "numpy.mean", "agac.logger.LogData", "agac.logger.LogData", "agac.logger.LogData", "agac.logger.LogData", "agac.logger.LogData", "agac_trainer.AGAC._logger.log", "agac_trainer.AGAC._process_trajectory", "range", "agac_trainer.AGAC._memory.reset", "agac.logger.LogData", "agac.logger.LogData", "agac_trainer.AGAC._memory.get_epoch_batches", "numpy.array", "agac_trainer.AGAC._ppo.get_actor_weights", "float", "agac_trainer.AGAC._ppo.train_on_batch", "float", "float", "agac_trainer.AGAC._extrinsic_returns_stats.mean.copy", "agac_trainer.AGAC._intrinsic_returns_stats.mean.copy"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.networks.actors.DiscreteActor.select_action", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnv.step", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.reset", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.agac_trainer.AGAC.evaluate", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.log", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.agac_trainer.AGAC._process_trajectory", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.reset", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.memory.Memory.get_epoch_batches", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.agac_ppo.PPO.get_actor_weights", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.agac_ppo.PPO.train_on_batch"], ["", "def", "train", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Train the AGAC agent.\n        \"\"\"", "\n", "\n", "# Start training loop", "\n", "done", "=", "True", "\n", "\n", "actions", "=", "[", "]", "\n", "observations", "=", "[", "]", "\n", "next_observations", "=", "[", "]", "\n", "rewards", "=", "[", "]", "\n", "log_pis", "=", "[", "]", "\n", "dones", "=", "[", "]", "\n", "state_counts", "=", "[", "]", "\n", "state_counter", "=", "{", "}", "\n", "\n", "while", "self", ".", "_total_timesteps", "<", "self", ".", "_max_steps", ":", "\n", "\n", "            ", "if", "done", ":", "\n", "# Reset environment", "\n", "                ", "observation", "=", "self", ".", "_env", ".", "reset", "(", ")", "\n", "self", ".", "_episode_num", "+=", "1", "\n", "\n", "# Reset lists", "\n", "actions", "=", "[", "]", "\n", "observations", "=", "[", "]", "\n", "next_observations", "=", "[", "]", "\n", "rewards", "=", "[", "]", "\n", "log_pis", "=", "[", "]", "\n", "adv_log_pis", "=", "[", "]", "\n", "logits_pis", "=", "[", "]", "\n", "adv_logits_pis", "=", "[", "]", "\n", "dones", "=", "[", "]", "\n", "state_counts", "=", "[", "]", "\n", "state_counter", "=", "{", "}", "\n", "\n", "# Evaluation", "\n", "", "if", "self", ".", "_total_timesteps", "%", "self", ".", "_eval_freq", "==", "0", ":", "\n", "# Run evaluation", "\n", "                ", "evaluation_returns", ",", "last_rewards", "=", "self", ".", "evaluate", "(", ")", "\n", "recvbuf", "=", "None", "\n", "if", "rank", "==", "0", ":", "\n", "                    ", "num_episodes_eval", "=", "self", ".", "_config", ".", "algorithm", ".", "num_episodes_eval", "\n", "recvbuf", "=", "np", ".", "empty", "(", "\n", "[", "num_workers", ",", "num_episodes_eval", "]", ",", "dtype", "=", "np", ".", "float32", "\n", ")", "\n", "", "comm", ".", "Gather", "(", "evaluation_returns", ",", "recvbuf", ",", "root", "=", "0", ")", "\n", "last_rewards", "=", "comm", ".", "gather", "(", "last_rewards", ",", "root", "=", "0", ")", "\n", "\n", "if", "rank", "==", "0", ":", "\n", "                    ", "evaluation_returns", "=", "recvbuf", ".", "flatten", "(", ")", "\n", "last_rewards", "=", "np", ".", "array", "(", "last_rewards", ")", ".", "flatten", "(", ")", "\n", "\n", "# Log stats", "\n", "mean_evaluation_return", "=", "np", ".", "mean", "(", "evaluation_returns", ")", "\n", "mean_last_reward", "=", "np", ".", "mean", "(", "last_rewards", ")", "\n", "\n", "mean_return_log", "=", "LogData", "(", "\n", "name", "=", "\"evaluation_mean_return\"", ",", "\n", "type", "=", "\"scalar\"", ",", "\n", "value", "=", "mean_evaluation_return", ",", "\n", ")", "\n", "mean_last_reward_log", "=", "LogData", "(", "\n", "name", "=", "\"evaluation_mean_last_reward\"", ",", "\n", "type", "=", "\"scalar\"", ",", "\n", "value", "=", "mean_last_reward", ",", "\n", ")", "\n", "steps_log", "=", "LogData", "(", "\n", "name", "=", "\"total_steps\"", ",", "\n", "type", "=", "\"scalar\"", ",", "\n", "value", "=", "self", ".", "_total_timesteps", "*", "num_workers", ",", "\n", ")", "\n", "\n", "actor_weights_log", "=", "LogData", "(", "\n", "name", "=", "\"actor_weights\"", ",", "\n", "type", "=", "\"array\"", ",", "\n", "value", "=", "self", ".", "_ppo", ".", "get_actor_weights", "(", ")", ",", "\n", ")", "\n", "current_intrinsic_reward_coefficient_log", "=", "LogData", "(", "\n", "name", "=", "\"training_intrinsic_reward_coefficient\"", ",", "\n", "type", "=", "\"scalar\"", ",", "\n", "value", "=", "float", "(", "self", ".", "_current_intrinsic_reward_coefficient", ")", ",", "\n", ")", "\n", "\n", "logs", "=", "[", "\n", "mean_return_log", ",", "\n", "mean_last_reward_log", ",", "\n", "steps_log", ",", "\n", "actor_weights_log", ",", "\n", "current_intrinsic_reward_coefficient_log", ",", "\n", "]", "\n", "\n", "if", "self", ".", "_extrinsic_returns_stats", ".", "mean", "is", "not", "None", ":", "\n", "                        ", "mean_extrinsic_returns_log", "=", "LogData", "(", "\n", "name", "=", "\"training_mean_return\"", ",", "\n", "type", "=", "\"scalar\"", ",", "\n", "value", "=", "float", "(", "self", ".", "_extrinsic_returns_stats", ".", "mean", ".", "copy", "(", ")", ")", ",", "\n", ")", "\n", "mean_intrinsic_returns_log", "=", "LogData", "(", "\n", "name", "=", "\"training_mean_intrinsic_return\"", ",", "\n", "type", "=", "\"scalar\"", ",", "\n", "value", "=", "float", "(", "self", ".", "_intrinsic_returns_stats", ".", "mean", ".", "copy", "(", ")", ")", ",", "\n", ")", "\n", "\n", "logs", "+=", "[", "mean_extrinsic_returns_log", ",", "mean_intrinsic_returns_log", "]", "\n", "if", "self", ".", "_display_grid", ":", "\n", "                            ", "logs", "+=", "self", ".", "_display_grid", ".", "logs", "\n", "\n", "", "", "if", "self", ".", "_total_updates", ">", "1", ":", "\n", "                        ", "logs", "+=", "self", ".", "_ppo", ".", "logs", "\n", "\n", "", "self", ".", "_logger", ".", "log", "(", "logs", ")", "\n", "\n", "# Select action randomly or according to policy", "\n", "", "", "action", ",", "log_pi", ",", "adv_log_pi", ",", "logits_pi", ",", "logits_adv", "=", "self", ".", "_ppo", ".", "select_action", "(", "\n", "observation", ",", "deterministic", "=", "False", "\n", ")", "\n", "log_pi", "=", "float", "(", "log_pi", ")", "\n", "adv_log_pi", "=", "float", "(", "adv_log_pi", ")", "\n", "\n", "# Retrieve state count if necessary", "\n", "if", "self", ".", "_episodic_count", ":", "\n", "                ", "state_key", "=", "self", ".", "_state_key_extraction", "(", "self", ".", "_env", ")", "\n", "if", "state_key", "in", "state_counter", ":", "\n", "                    ", "state_counter", "[", "state_key", "]", "+=", "1", "\n", "", "else", ":", "\n", "                    ", "state_counter", "[", "state_key", "]", "=", "1", "\n", "", "state_count", "=", "state_counter", ".", "get", "(", "state_key", ")", "\n", "state_counts", ".", "append", "(", "state_count", ")", "\n", "\n", "# Perform action", "\n", "", "if", "not", "self", ".", "_discrete", ":", "\n", "                ", "action", "=", "np", ".", "clip", "(", "action", ",", "-", "self", ".", "_max_action", ",", "self", ".", "_max_action", ")", "\n", "", "new_observation", ",", "reward", ",", "done", ",", "_", "=", "self", ".", "_env", ".", "step", "(", "action", ")", "\n", "\n", "# Save transition", "\n", "observations", ".", "append", "(", "observation", ".", "copy", "(", ")", ")", "\n", "next_observations", ".", "append", "(", "new_observation", ".", "copy", "(", ")", ")", "\n", "actions", ".", "append", "(", "action", ".", "copy", "(", ")", ")", "\n", "rewards", ".", "append", "(", "reward", ")", "\n", "log_pis", ".", "append", "(", "log_pi", ")", "\n", "adv_log_pis", ".", "append", "(", "adv_log_pi", ")", "\n", "logits_pis", ".", "append", "(", "logits_pi", ")", "\n", "adv_logits_pis", ".", "append", "(", "logits_adv", ")", "\n", "dones", ".", "append", "(", "float", "(", "done", ")", ")", "\n", "\n", "# next observation becomes current observation", "\n", "observation", "=", "deepcopy", "(", "new_observation", ")", "\n", "\n", "# when episode done or T timesteps reached, process transitions", "\n", "# and fill memory", "\n", "if", "self", ".", "_total_timesteps", ">", "0", ":", "\n", "                ", "if", "done", "or", "self", ".", "_total_timesteps", "%", "self", ".", "_train_freq", "==", "0", ":", "\n", "                    ", "self", ".", "_process_trajectory", "(", "\n", "observations", ",", "\n", "next_observations", ",", "\n", "actions", ",", "\n", "rewards", ",", "\n", "log_pis", ",", "\n", "adv_log_pis", ",", "\n", "logits_pis", ",", "\n", "adv_logits_pis", ",", "\n", "dones", ",", "\n", "state_counts", ",", "\n", ")", "\n", "\n", "# Update", "\n", "", "if", "self", ".", "_total_timesteps", "%", "self", ".", "_config", ".", "algorithm", ".", "train_freq", "==", "0", ":", "\n", "                    ", "for", "_", "in", "range", "(", "self", ".", "_num_epochs", ")", ":", "\n", "                        ", "batches", "=", "self", ".", "_memory", ".", "get_epoch_batches", "(", "self", ".", "_batch_size", ")", "\n", "for", "batch", "in", "batches", ":", "\n", "                            ", "self", ".", "_ppo", ".", "train_on_batch", "(", "\n", "batch", ",", "self", ".", "_current_intrinsic_reward_coefficient", "\n", ")", "\n", "\n", "# Flush memory", "\n", "", "", "self", ".", "_memory", ".", "reset", "(", ")", "\n", "\n", "# Increment updates variables", "\n", "self", ".", "_total_updates", "+=", "1", "\n", "\n", "# Increment steps variables", "\n", "", "", "self", ".", "_total_timesteps", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.agac_trainer.AGAC.evaluate": [[275, 300], ["range", "agac_trainer.AGAC._evaluation_env.reset", "last_rewards.append", "episodes_returns.append", "numpy.asarray().astype", "numpy.asarray", "agac_trainer.AGAC._evaluation_env.step", "agac_trainer.AGAC._ppo.select_action", "numpy.clip", "agac_trainer.AGAC._display_grid.add", "agac_trainer.AGAC._display_grid.reset", "numpy.asarray"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.reset", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnv.step", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.networks.actors.DiscreteActor.select_action", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.replay_buffer.ReplayBuffer.add", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.reset"], ["", "", "def", "evaluate", "(", "self", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "np", ".", "ndarray", "]", ":", "\n", "        ", "\"\"\"\n        Play episodes in deterministic modes and return mean return.\n        \"\"\"", "\n", "episodes_returns", "=", "[", "]", "\n", "last_rewards", "=", "[", "]", "\n", "for", "_", "in", "range", "(", "self", ".", "_config", ".", "algorithm", ".", "num_episodes_eval", ")", ":", "\n", "            ", "observation", "=", "self", ".", "_evaluation_env", ".", "reset", "(", ")", "\n", "# Update grid if display grid", "\n", "episode_return", "=", "0.0", "\n", "done", "=", "False", "\n", "while", "not", "done", ":", "\n", "                ", "action", "=", "self", ".", "_ppo", ".", "select_action", "(", "observation", ",", "deterministic", "=", "True", ")", "[", "0", "]", "\n", "if", "not", "self", ".", "_discrete", ":", "\n", "                    ", "action", "=", "np", ".", "clip", "(", "action", ",", "-", "self", ".", "_max_action", ",", "self", ".", "_max_action", ")", "\n", "", "if", "self", ".", "_display_grid", ":", "\n", "                    ", "self", ".", "_display_grid", ".", "add", "(", ")", "\n", "", "new_observation", ",", "reward", ",", "done", ",", "_", "=", "self", ".", "_evaluation_env", ".", "step", "(", "action", ")", "\n", "episode_return", "+=", "reward", "\n", "observation", "=", "new_observation", "\n", "if", "done", "and", "self", ".", "_display_grid", ":", "\n", "                    ", "self", ".", "_display_grid", ".", "reset", "(", ")", "\n", "", "", "last_rewards", ".", "append", "(", "reward", ")", "\n", "episodes_returns", ".", "append", "(", "episode_return", ")", "\n", "", "return", "np", ".", "asarray", "(", "episodes_returns", ")", ".", "astype", "(", "np", ".", "float32", ")", ",", "np", ".", "asarray", "(", "last_rewards", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.agac_trainer.AGAC._process_trajectory": [[301, 396], ["numpy.asarray().astype", "numpy.asarray().astype().flatten", "numpy.asarray().astype().flatten", "numpy.asarray().astype().flatten", "numpy.asarray().astype().flatten", "numpy.asarray().astype", "numpy.asarray().astype", "agac_trainer.AGAC._ppo.compute_values", "agac_trainer.AGAC._intrinsic_returns_stats.add", "agac_trainer.AGAC._extrinsic_returns_stats.add", "agac.utils.compute_advantages_and_returns", "range", "numpy.asarray().astype().flatten", "numpy.floor", "numpy.sum", "numpy.sum", "numpy.asarray().astype", "agac_trainer.AGAC._ppo.compute_values", "float", "agac.utils.compute_advantages_and_returns", "len", "agac.memory.Transition", "agac_trainer.AGAC._memory.add", "numpy.asarray", "numpy.asarray().astype", "numpy.asarray().astype", "numpy.asarray().astype", "numpy.asarray().astype", "numpy.asarray", "numpy.asarray", "numpy.asarray().astype", "numpy.sqrt", "numpy.asarray", "numpy.asarray", "numpy.asarray", "numpy.asarray", "numpy.asarray", "numpy.asarray"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.agac_ppo.PPO.compute_values", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.replay_buffer.ReplayBuffer.add", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.replay_buffer.ReplayBuffer.add", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.utils.compute_advantages_and_returns", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.agac_ppo.PPO.compute_values", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.utils.compute_advantages_and_returns", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.replay_buffer.ReplayBuffer.add"], ["", "def", "_process_trajectory", "(", "\n", "self", ",", "\n", "observations", ":", "List", "[", "np", ".", "ndarray", "]", ",", "\n", "next_observations", ":", "List", "[", "np", ".", "ndarray", "]", ",", "\n", "actions", ":", "List", "[", "np", ".", "ndarray", "]", ",", "\n", "rewards", ":", "List", "[", "float", "]", ",", "\n", "log_pis", ":", "List", "[", "np", ".", "ndarray", "]", ",", "\n", "adv_log_pis", ":", "List", "[", "np", ".", "ndarray", "]", ",", "\n", "logits_pis", ":", "List", "[", "np", ".", "ndarray", "]", ",", "\n", "adv_logits_pis", ":", "List", "[", "np", ".", "ndarray", "]", ",", "\n", "dones", ":", "List", "[", "np", ".", "ndarray", "]", ",", "\n", "state_counts", ":", "List", "[", "float", "]", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Compute values, returns and advantages when a trajectory is completed\n        and add the transitions in the memory.\n        \"\"\"", "\n", "observations", "=", "np", ".", "asarray", "(", "observations", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "dones", "=", "np", ".", "asarray", "(", "dones", ")", ".", "astype", "(", "np", ".", "float32", ")", ".", "flatten", "(", ")", "\n", "extrinsic_rewards", "=", "np", ".", "asarray", "(", "rewards", ")", ".", "astype", "(", "np", ".", "float32", ")", ".", "flatten", "(", ")", "\n", "log_pis", "=", "np", ".", "asarray", "(", "log_pis", ")", ".", "astype", "(", "np", ".", "float32", ")", ".", "flatten", "(", ")", "\n", "adv_log_pis", "=", "np", ".", "asarray", "(", "adv_log_pis", ")", ".", "astype", "(", "np", ".", "float32", ")", ".", "flatten", "(", ")", "\n", "logits_pis", "=", "np", ".", "asarray", "(", "logits_pis", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "adv_logits_pis", "=", "np", ".", "asarray", "(", "adv_logits_pis", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "if", "self", ".", "_episodic_count", ":", "\n", "            ", "state_counts", "=", "np", ".", "asarray", "(", "state_counts", ")", ".", "astype", "(", "np", ".", "float32", ")", ".", "flatten", "(", ")", "\n", "state_count_rewards", "=", "np", ".", "floor", "(", "1", "/", "np", ".", "sqrt", "(", "state_counts", ")", ")", "\n", "\n", "", "values", "=", "self", ".", "_ppo", ".", "compute_values", "(", "observations", ")", "\n", "\n", "# Anneal intrinsic rewards coef (linearly annealed to 0):", "\n", "progress_fraction", "=", "1.0", "-", "self", ".", "_total_updates", "/", "self", ".", "_max_updates", "\n", "self", ".", "_current_intrinsic_reward_coefficient", "=", "(", "\n", "self", ".", "_intrinsic_reward_coefficient", "*", "progress_fraction", "\n", ")", "\n", "\n", "# compute intrinsic rewards", "\n", "intrinsic_rewards", "=", "log_pis", "-", "adv_log_pis", "\n", "\n", "# update returns stats", "\n", "self", ".", "_intrinsic_returns_stats", ".", "add", "(", "np", ".", "sum", "(", "intrinsic_rewards", ")", ")", "\n", "self", ".", "_extrinsic_returns_stats", ".", "add", "(", "np", ".", "sum", "(", "extrinsic_rewards", ")", ")", "\n", "\n", "# scale intrinsic rewards", "\n", "intrinsic_rewards", "*=", "self", ".", "_current_intrinsic_reward_coefficient", "\n", "\n", "# compute advantages", "\n", "if", "dones", "[", "-", "1", "]", "==", "1", ":", "\n", "            ", "last_r", "=", "0.0", "\n", "", "else", ":", "\n", "            ", "last_next_obs", "=", "np", ".", "asarray", "(", "[", "next_observations", "[", "-", "1", "]", "]", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "last_r", "=", "self", ".", "_ppo", ".", "compute_values", "(", "last_next_obs", ")", "\n", "last_r", "=", "float", "(", "last_r", ")", "\n", "\n", "", "advantages", ",", "returns", "=", "compute_advantages_and_returns", "(", "\n", "extrinsic_rewards", ",", "\n", "values", ",", "\n", "last_r", ",", "\n", "self", ".", "_discount", ",", "\n", "self", ".", "_lambda_gae", ",", "\n", ")", "\n", "\n", "if", "self", ".", "_episodic_count", ":", "\n", "            ", "agac_advantages", ",", "_", "=", "compute_advantages_and_returns", "(", "\n", "(", "\n", "extrinsic_rewards", "\n", "+", "self", ".", "_episodic_count_coefficient", "*", "state_count_rewards", "\n", ")", ",", "\n", "values", ",", "\n", "last_r", ",", "\n", "self", ".", "_discount", ",", "\n", "self", ".", "_lambda_gae", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "agac_advantages", "=", "advantages", "\n", "\n", "# Add Agac discrepancy", "\n", "", "agac_advantages", "=", "agac_advantages", "+", "intrinsic_rewards", "\n", "\n", "# create transitions and add them in memory", "\n", "for", "i", "in", "range", "(", "len", "(", "rewards", ")", ")", ":", "\n", "            ", "transition", "=", "Transition", "(", "\n", "observation", "=", "observations", "[", "i", "]", ",", "\n", "action", "=", "actions", "[", "i", "]", ",", "\n", "extrinsic_return", "=", "returns", "[", "i", "]", ",", "\n", "advantage", "=", "advantages", "[", "i", "]", ",", "\n", "agac_advantage", "=", "agac_advantages", "[", "i", "]", ",", "\n", "value", "=", "values", "[", "i", "]", ",", "\n", "log_pi", "=", "log_pis", "[", "i", "]", ",", "\n", "adv_log_pi", "=", "adv_log_pis", "[", "i", "]", ",", "\n", "logits_pi", "=", "logits_pis", "[", "i", "]", ",", "\n", "adv_logits_pi", "=", "adv_logits_pis", "[", "i", "]", ",", "\n", "done", "=", "dones", "[", "i", "]", ",", "\n", ")", "\n", "self", ".", "_memory", ".", "add", "(", "transition", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.agac_ppo.PPO.__init__": [[25, 101], ["agac_ppo.PPO._actor.to", "agac_ppo.PPO._critic.to", "agac_ppo.PPO._adversary.to", "core.utils.mpi_utils.sync_networks", "core.utils.mpi_utils.sync_networks", "core.utils.mpi_utils.sync_networks", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "dict", "torch.device", "torch.device", "torch.device", "torch.device", "core.networks.actors.DiscreteActor", "core.networks.actors.DiscreteActor", "core.networks.actors.GaussianActor", "core.networks.actors.GaussianActor", "core.networks.critics.CNNContinuousVNetwork", "core.networks.critics.ContinuousVNetwork", "agac_ppo.PPO._actor.parameters", "agac_ppo.PPO._adversary.parameters", "agac_ppo.PPO._critic.parameters", "str"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.mpi_utils.sync_networks", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.mpi_utils.sync_networks", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.mpi_utils.sync_networks"], ["    ", "def", "__init__", "(", "self", ",", "obs_space", ":", "Box", ",", "action_space", ":", "Space", ",", "config", ":", "ExperimentConfig", ")", ":", "\n", "        ", "rl_config", "=", "config", ".", "reinforcement_learning", "\n", "layers_dim", "=", "rl_config", ".", "layers_dim", "\n", "adv_layers_dim", "=", "rl_config", ".", "adv_layers_dim", "\n", "actor_lr", "=", "rl_config", ".", "actor_learning_rate", "\n", "critic_lr", "=", "rl_config", ".", "critic_learning_rate", "\n", "\n", "self", ".", "_discrete", "=", "config", ".", "algorithm", ".", "discrete", "\n", "self", ".", "_clipping_epsilon", "=", "rl_config", ".", "clipping_epsilon", "\n", "self", ".", "_value_loss_clip", "=", "rl_config", ".", "value_loss_clip", "\n", "self", ".", "_value_loss_coeff", "=", "rl_config", ".", "value_loss_coeff", "\n", "self", ".", "_adv_loss_coeff", "=", "rl_config", ".", "adversary_loss_coeff", "\n", "self", ".", "_entropy_coeff", "=", "rl_config", ".", "entropy_coefficient", "\n", "self", ".", "_clip_grad_norm", "=", "rl_config", ".", "clip_grad_norm", "\n", "if", "config", ".", "algorithm", ".", "gpu", "!=", "-", "1", ":", "\n", "            ", "self", ".", "_device", "=", "torch", ".", "device", "(", "\"cuda:\"", "+", "str", "(", "config", ".", "algorithm", ".", "gpu", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_device", "=", "\"cpu\"", "\n", "", "observation_dim", "=", "obs_space", ".", "shape", "\n", "\n", "if", "self", ".", "_discrete", ":", "\n", "            ", "action_dim", "=", "action_space", ".", "n", "\n", "self", ".", "_actor", "=", "DiscreteActor", "(", "\n", "observation_dim", ",", "\n", "action_dim", ",", "\n", "layers_dim", ",", "\n", "cnn_extractor", "=", "rl_config", ".", "cnn_extractor", ",", "\n", "layers_num_channels", "=", "rl_config", ".", "layers_num_channels", ",", "\n", ")", "\n", "self", ".", "_adversary", "=", "DiscreteActor", "(", "\n", "observation_dim", ",", "\n", "action_dim", ",", "\n", "adv_layers_dim", ",", "\n", "cnn_extractor", "=", "rl_config", ".", "cnn_extractor", ",", "\n", "layers_num_channels", "=", "rl_config", ".", "layers_num_channels", ",", "\n", ")", "\n", "", "else", ":", "\n", "# Continuous action space", "\n", "            ", "observation_dim", "=", "observation_dim", "[", "0", "]", "\n", "action_dim", "=", "action_space", ".", "shape", "[", "0", "]", "\n", "max_action", "=", "action_space", ".", "high", "[", "0", "]", "\n", "self", ".", "_actor", "=", "GaussianActor", "(", "\n", "observation_dim", ",", "action_dim", ",", "max_action", ",", "layers_dim", "\n", ")", "\n", "self", ".", "_adversary", "=", "GaussianActor", "(", "\n", "observation_dim", ",", "action_dim", ",", "max_action", ",", "adv_layers_dim", "\n", ")", "\n", "\n", "", "if", "rl_config", ".", "cnn_extractor", ":", "\n", "            ", "self", ".", "_critic", "=", "CNNContinuousVNetwork", "(", "\n", "observation_dim", ",", "layers_dim", ",", "rl_config", ".", "layers_num_channels", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_critic", "=", "ContinuousVNetwork", "(", "observation_dim", ",", "layers_dim", ")", "\n", "\n", "# To device", "\n", "", "self", ".", "_actor", ".", "to", "(", "self", ".", "_device", ")", "\n", "self", ".", "_critic", ".", "to", "(", "self", ".", "_device", ")", "\n", "self", ".", "_adversary", ".", "to", "(", "self", ".", "_device", ")", "\n", "\n", "sync_networks", "(", "self", ".", "_actor", ",", "comm", ")", "\n", "sync_networks", "(", "self", ".", "_critic", ",", "comm", ")", "\n", "sync_networks", "(", "self", ".", "_adversary", ",", "comm", ")", "\n", "\n", "self", ".", "_actor_optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "\n", "self", ".", "_actor", ".", "parameters", "(", ")", ",", "lr", "=", "actor_lr", ",", "eps", "=", "1e-5", "\n", ")", "\n", "self", ".", "_adversary_optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "\n", "self", ".", "_adversary", ".", "parameters", "(", ")", ",", "lr", "=", "actor_lr", ",", "eps", "=", "1e-5", "\n", ")", "\n", "self", ".", "_critic_optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "\n", "self", ".", "_critic", ".", "parameters", "(", ")", ",", "lr", "=", "critic_lr", ",", "eps", "=", "1e-5", "\n", ")", "\n", "\n", "# Monitoring", "\n", "self", ".", "_monitor", "=", "dict", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.agac_ppo.PPO.get_actor_weights": [[102, 107], ["agac_ppo.PPO._actor.get_params"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.networks.rl_model.RLModel.get_params"], ["", "def", "get_actor_weights", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Returns actor weights.\n        \"\"\"", "\n", "self", ".", "_actor", ".", "get_params", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.agac_ppo.PPO.set_actor_weights": [[108, 113], ["agac_ppo.PPO._actor.set_params"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.networks.rl_model.RLModel.set_params"], ["", "def", "set_actor_weights", "(", "self", ",", "params", ")", ":", "\n", "        ", "\"\"\"\n        Set actor weights.\n        \"\"\"", "\n", "self", ".", "_actor", ".", "set_params", "(", "params", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.agac_ppo.PPO.select_action": [[114, 147], ["torch.from_numpy().float().to().unsqueeze", "torch.from_numpy().float().to().unsqueeze", "torch.from_numpy().float().to().unsqueeze", "torch.from_numpy().float().to().unsqueeze", "agac_ppo.PPO._actor.forward", "agac_ppo.PPO._adversary.compute_log_prob", "action.detach().cpu().numpy().flatten.detach().cpu().numpy().flatten.detach().cpu().numpy().flatten", "log_pi.detach().cpu().numpy().flatten.detach().cpu().numpy().flatten.detach().cpu().numpy().flatten", "adv_log_pi.detach().cpu().numpy().flatten.detach().cpu().numpy().flatten.detach().cpu().numpy().flatten", "agac_ppo.PPO._actor.compute_distribution", "agac_ppo.PPO._adversary.compute_distribution", "numpy.concatenate.detach().cpu().numpy().flatten", "numpy.concatenate.detach().cpu().numpy().flatten", "agac_ppo.PPO.mean.detach().cpu().numpy().flatten", "agac_ppo.PPO.scale.detach().cpu().numpy().flatten", "numpy.concatenate", "agac_ppo.PPO.mean.detach().cpu().numpy().flatten", "agac_ppo.PPO.scale.detach().cpu().numpy().flatten", "numpy.concatenate", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "action.detach().cpu().numpy().flatten.detach().cpu().numpy().flatten.detach().cpu().numpy", "log_pi.detach().cpu().numpy().flatten.detach().cpu().numpy().flatten.detach().cpu().numpy", "adv_log_pi.detach().cpu().numpy().flatten.detach().cpu().numpy().flatten.detach().cpu().numpy", "numpy.concatenate.detach().cpu().numpy", "numpy.concatenate.detach().cpu().numpy", "agac_ppo.PPO.mean.detach().cpu().numpy", "agac_ppo.PPO.scale.detach().cpu().numpy", "agac_ppo.PPO.mean.detach().cpu().numpy", "agac_ppo.PPO.scale.detach().cpu().numpy", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "action.detach().cpu().numpy().flatten.detach().cpu().numpy().flatten.detach().cpu", "log_pi.detach().cpu().numpy().flatten.detach().cpu().numpy().flatten.detach().cpu", "adv_log_pi.detach().cpu().numpy().flatten.detach().cpu().numpy().flatten.detach().cpu", "numpy.concatenate.detach().cpu", "numpy.concatenate.detach().cpu", "agac_ppo.PPO.mean.detach().cpu", "agac_ppo.PPO.scale.detach().cpu", "agac_ppo.PPO.mean.detach().cpu", "agac_ppo.PPO.scale.detach().cpu", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "action.detach().cpu().numpy().flatten.detach().cpu().numpy().flatten.detach", "log_pi.detach().cpu().numpy().flatten.detach().cpu().numpy().flatten.detach", "adv_log_pi.detach().cpu().numpy().flatten.detach().cpu().numpy().flatten.detach", "numpy.concatenate.detach", "numpy.concatenate.detach", "agac_ppo.PPO.mean.detach", "agac_ppo.PPO.scale.detach", "agac_ppo.PPO.mean.detach", "agac_ppo.PPO.scale.detach"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.networks.cnn.CNN.forward", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.networks.actors.DiscreteActor.compute_log_prob", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.networks.actors.DiscreteActor.compute_distribution", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.networks.actors.DiscreteActor.compute_distribution"], ["", "def", "select_action", "(", "\n", "self", ",", "observation", ":", "Observation", ",", "deterministic", ":", "bool", "=", "False", "\n", ")", "->", "Action", ":", "\n", "        ", "\"\"\"\n        Select an action given an observation. Returns the selected action\n        and the corresponding log probability.\n        \"\"\"", "\n", "observation", "=", "(", "\n", "torch", ".", "from_numpy", "(", "observation", ")", ".", "float", "(", ")", ".", "to", "(", "self", ".", "_device", ")", ".", "unsqueeze", "(", "0", ")", "\n", ")", "\n", "outs", "=", "self", ".", "_actor", ".", "forward", "(", "observation", ",", "deterministic", ",", "return_log_prob", "=", "True", ")", "\n", "action", ",", "log_pi", "=", "outs", "[", "0", "]", ",", "outs", "[", "1", "]", "\n", "adv_log_pi", "=", "self", ".", "_adversary", ".", "compute_log_prob", "(", "observation", ",", "action", ")", "\n", "action", "=", "action", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "flatten", "(", ")", "\n", "log_pi", "=", "log_pi", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "flatten", "(", ")", "\n", "adv_log_pi", "=", "adv_log_pi", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "flatten", "(", ")", "\n", "\n", "# compute additional distribution information", "\n", "actor_distrib", "=", "self", ".", "_actor", ".", "compute_distribution", "(", "observation", ")", "\n", "adversary_distrib", "=", "self", ".", "_adversary", ".", "compute_distribution", "(", "observation", ")", "\n", "if", "self", ".", "_discrete", ":", "\n", "            ", "params_pi", "=", "outs", "[", "-", "1", "]", "\n", "params_adv", "=", "adversary_distrib", ".", "logits", "\n", "params_pi", "=", "params_pi", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "flatten", "(", ")", "\n", "params_adv", "=", "params_adv", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "flatten", "(", ")", "\n", "", "else", ":", "\n", "            ", "mean", "=", "actor_distrib", ".", "mean", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "flatten", "(", ")", "\n", "scale", "=", "actor_distrib", ".", "scale", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "flatten", "(", ")", "\n", "params_pi", "=", "np", ".", "concatenate", "(", "[", "mean", ",", "scale", "]", ",", "-", "1", ")", "\n", "mean", "=", "adversary_distrib", ".", "mean", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "flatten", "(", ")", "\n", "scale", "=", "adversary_distrib", ".", "scale", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "flatten", "(", ")", "\n", "params_adv", "=", "np", ".", "concatenate", "(", "[", "mean", ",", "scale", "]", ",", "-", "1", ")", "\n", "", "return", "action", ",", "log_pi", ",", "adv_log_pi", ",", "params_pi", ",", "params_adv", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.agac_ppo.PPO.compute_values": [[148, 160], ["torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "agac_ppo.PPO._critic().detach().cpu().numpy().flatten().astype", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "agac_ppo.PPO._critic().detach().cpu().numpy().flatten", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "agac_ppo.PPO._critic().detach().cpu().numpy", "agac_ppo.PPO._critic().detach().cpu", "agac_ppo.PPO._critic().detach", "agac_ppo.PPO._critic"], "methods", ["None"], ["", "def", "compute_values", "(", "self", ",", "observations", ":", "Observation", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"\n        Compute value estimations for a batch of observations.\n        \"\"\"", "\n", "observations", "=", "torch", ".", "from_numpy", "(", "observations", ")", ".", "float", "(", ")", ".", "to", "(", "self", ".", "_device", ")", "\n", "return", "(", "\n", "self", ".", "_critic", "(", "observations", ")", "\n", ".", "detach", "(", ")", "\n", ".", "cpu", "(", ")", "\n", ".", "numpy", "(", ")", "\n", ".", "flatten", "(", ")", "\n", ".", "astype", "(", "np", ".", "float32", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.agac_ppo.PPO.compute_distributions": [[162, 168], ["torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "agac_ppo.PPO._actor.compute_distribution", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.networks.actors.DiscreteActor.compute_distribution"], ["", "def", "compute_distributions", "(", "self", ",", "observations", ":", "Observation", ")", "->", "Distribution", ":", "\n", "        ", "\"\"\"\n        Compute action distributions conditional on observations\n        \"\"\"", "\n", "observations", "=", "torch", ".", "from_numpy", "(", "observations", ")", ".", "float", "(", ")", ".", "to", "(", "self", ".", "_device", ")", "\n", "return", "self", ".", "_actor", ".", "compute_distribution", "(", "observations", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.agac_ppo.PPO.train_on_batch": [[169, 313], ["torch.tensor().squeeze", "torch.tensor().squeeze", "torch.tensor().squeeze", "torch.tensor().squeeze", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "float", "float", "float", "float", "float", "agac_ppo.PPO._actor.compute_distribution", "agac_ppo.PPO.log_prob", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "float", "agac_ppo.PPO.entropy", "entropy.sum().flatten.sum().flatten.mean", "float", "agac_ppo.PPO._critic().flatten", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "float", "agac_ppo.PPO._adversary.compute_distribution", "torch.distributions.kl.kl_divergence().sum.mean", "float", "agac_ppo.PPO._actor_optimizer.zero_grad", "policy_loss.backward", "agac_ppo.PPO._adversary.zero_grad", "torch.distributions.kl.kl_divergence().sum.backward", "agac_ppo.PPO._critic_optimizer.zero_grad", "value_loss.backward", "core.utils.mpi_utils.sync_grads", "agac_ppo.PPO._actor_optimizer.step", "core.utils.mpi_utils.sync_grads", "agac_ppo.PPO._adversary_optimizer.step", "core.utils.mpi_utils.sync_grads", "agac_ppo.PPO._critic_optimizer.step", "returns.mean", "torch.tensor.mean", "torch.tensor.mean", "torch.tensor.mean", "torch.tensor.mean", "torch.tensor.mean", "torch.tensor.mean", "torch.tensor.mean", "torch.tensor.mean", "log_pi.sum().flatten.sum().flatten.sum().flatten", "torch.min().mean", "torch.min().mean", "torch.min().mean", "torch.min().mean", "entropy.sum().flatten.sum().flatten.sum().flatten", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.distributions.kl.kl_divergence", "torch.distributions.kl.kl_divergence", "isinstance", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.max().mean", "torch.max().mean", "torch.max().mean", "torch.max().mean", "torch.distributions.kl.kl_divergence", "torch.distributions.kl.kl_divergence", "torch.distributions.kl.kl_divergence().sum", "torch.distributions.kl.kl_divergence().sum", "torch.utils.clip_grad_norm_", "torch.utils.clip_grad_norm_", "torch.utils.clip_grad_norm_", "torch.utils.clip_grad_norm_", "torch.utils.clip_grad_norm_", "torch.utils.clip_grad_norm_", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.distributions.Categorical", "torch.distributions.Categorical", "torch.distributions.Categorical", "torch.distributions.Categorical", "torch.distributions.Normal", "torch.distributions.Normal", "torch.distributions.Normal", "torch.distributions.Normal", "pi_adv_kl.sum.sum.sum", "agac_ppo.PPO._critic", "agac_ppo.PPO._actor.parameters", "agac_ppo.PPO._critic.parameters", "agac_ppo.PPO._adversary.parameters", "log_pi.sum().flatten.sum().flatten.sum", "torch.min", "torch.min", "torch.min", "torch.min", "entropy.sum().flatten.sum().flatten.sum", "torch.Softmax", "torch.Softmax", "torch.Softmax", "torch.Softmax", "torch.tensor.squeeze", "torch.tensor.squeeze", "torch.tensor.squeeze", "torch.tensor.squeeze", "torch.tensor.squeeze", "torch.tensor.squeeze", "torch.tensor.squeeze", "torch.tensor.squeeze", "torch.max", "torch.max", "torch.max", "torch.max", "torch.distributions.kl.kl_divergence", "torch.distributions.kl.kl_divergence", "probs_pi.squeeze", "adv_probs_pi.squeeze"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.networks.actors.DiscreteActor.compute_distribution", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.BernoulliProbabilityDistribution.entropy", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.networks.actors.DiscreteActor.compute_distribution", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.mpi_utils.sync_grads", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnv.step", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.mpi_utils.sync_grads", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnv.step", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.mpi_utils.sync_grads", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnv.step"], ["", "def", "train_on_batch", "(", "self", ",", "batch", ":", "Batch", ",", "intrinsic_coef", ":", "float", ")", ":", "\n", "        ", "\"\"\"\n        Update actor and critic networks on batch of transitions.\n        \"\"\"", "\n", "\n", "# read batch", "\n", "actions", "=", "torch", ".", "tensor", "(", "\n", "batch", ".", "actions", ",", "device", "=", "self", ".", "_device", ",", "dtype", "=", "torch", ".", "float32", "\n", ")", ".", "squeeze", "(", ")", "\n", "observations", "=", "torch", ".", "tensor", "(", "\n", "batch", ".", "observations", ",", "device", "=", "self", ".", "_device", ",", "dtype", "=", "torch", ".", "float32", "\n", ")", "\n", "log_pi_old", "=", "torch", ".", "tensor", "(", "\n", "batch", ".", "log_pis", ",", "device", "=", "self", ".", "_device", ",", "dtype", "=", "torch", ".", "float32", "\n", ")", "\n", "adv_log_pi_old", "=", "torch", ".", "tensor", "(", "\n", "batch", ".", "adv_log_pis", ",", "device", "=", "self", ".", "_device", ",", "dtype", "=", "torch", ".", "float32", "\n", ")", "\n", "advantages", "=", "torch", ".", "tensor", "(", "\n", "batch", ".", "advantages", ",", "device", "=", "self", ".", "_device", ",", "dtype", "=", "torch", ".", "float32", "\n", ")", "\n", "agac_advantages", "=", "torch", ".", "tensor", "(", "\n", "batch", ".", "agac_advantages", ",", "device", "=", "self", ".", "_device", ",", "dtype", "=", "torch", ".", "float32", "\n", ")", "\n", "values_old", "=", "torch", ".", "tensor", "(", "\n", "batch", ".", "values", ",", "device", "=", "self", ".", "_device", ",", "dtype", "=", "torch", ".", "float32", "\n", ")", "\n", "params_pi", "=", "torch", ".", "tensor", "(", "\n", "batch", ".", "logits_pi", ",", "device", "=", "self", ".", "_device", ",", "dtype", "=", "torch", ".", "float32", "\n", ")", "\n", "adv_params_pi", "=", "torch", ".", "tensor", "(", "\n", "batch", ".", "adv_logits_pi", ",", "device", "=", "self", ".", "_device", ",", "dtype", "=", "torch", ".", "float32", "\n", ")", "\n", "\n", "returns", "=", "values_old", "+", "advantages", "\n", "self", ".", "_monitor", "[", "\"estimated_returns\"", "]", "=", "float", "(", "returns", ".", "mean", "(", ")", ")", "\n", "self", ".", "_monitor", "[", "\"log_pis\"", "]", "=", "float", "(", "log_pi_old", ".", "mean", "(", ")", ")", "\n", "self", ".", "_monitor", "[", "\"adv_log_pis\"", "]", "=", "float", "(", "adv_log_pi_old", ".", "mean", "(", ")", ")", "\n", "self", ".", "_monitor", "[", "\"advantages\"", "]", "=", "float", "(", "advantages", ".", "mean", "(", ")", ")", "\n", "self", ".", "_monitor", "[", "\"agac_advantages\"", "]", "=", "float", "(", "agac_advantages", ".", "mean", "(", ")", ")", "\n", "\n", "# get distributions", "\n", "pi_distribution", "=", "self", ".", "_actor", ".", "compute_distribution", "(", "observations", ")", "\n", "log_pi", "=", "pi_distribution", ".", "log_prob", "(", "actions", ")", "\n", "if", "not", "self", ".", "_discrete", ":", "\n", "# Summing independent univariate normal logits for Gaussian Actor", "\n", "            ", "log_pi", "=", "log_pi", ".", "sum", "(", "axis", "=", "-", "1", ")", ".", "flatten", "(", ")", "\n", "\n", "# compute surrogate loss", "\n", "", "probability_ratio", "=", "torch", ".", "exp", "(", "log_pi", "-", "log_pi_old", ")", "\n", "clipped_ratio", "=", "torch", ".", "clamp", "(", "\n", "probability_ratio", ",", "\n", "min", "=", "1.0", "-", "self", ".", "_clipping_epsilon", ",", "\n", "max", "=", "1.0", "+", "self", ".", "_clipping_epsilon", ",", "\n", ")", "\n", "\n", "policy_loss", "=", "-", "torch", ".", "min", "(", "\n", "probability_ratio", "*", "agac_advantages", ",", "\n", "clipped_ratio", "*", "agac_advantages", ",", "\n", ")", ".", "mean", "(", ")", "\n", "self", ".", "_monitor", "[", "\"policy_loss\"", "]", "=", "float", "(", "policy_loss", ")", "\n", "\n", "# add entropy bonus", "\n", "entropy", "=", "pi_distribution", ".", "entropy", "(", ")", "\n", "if", "not", "self", ".", "_discrete", ":", "\n", "# Summing independent univariate normal entropies for Gaussian Actor", "\n", "            ", "entropy", "=", "entropy", ".", "sum", "(", "axis", "=", "-", "1", ")", ".", "flatten", "(", ")", "\n", "", "entropy", "=", "entropy", ".", "mean", "(", ")", "\n", "self", ".", "_monitor", "[", "\"entropy\"", "]", "=", "float", "(", "entropy", ")", "\n", "policy_loss", "-=", "self", ".", "_entropy_coeff", "*", "entropy", "\n", "\n", "# compute kl", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "if", "self", ".", "_discrete", ":", "\n", "                ", "probs_pi", "=", "nn", ".", "Softmax", "(", "-", "1", ")", "(", "params_pi", ")", "\n", "adv_probs_pi", "=", "nn", ".", "Softmax", "(", "-", "1", ")", "(", "adv_params_pi", ")", "\n", "old_pi_distribution", "=", "Categorical", "(", "probs", "=", "probs_pi", ".", "squeeze", "(", ")", ")", "\n", "adv_old_pi_distribution", "=", "Categorical", "(", "\n", "probs", "=", "adv_probs_pi", ".", "squeeze", "(", ")", "+", "1e-8", "\n", ")", "\n", "", "else", ":", "\n", "                ", "dim", "=", "pi_distribution", ".", "loc", ".", "shape", "[", "-", "1", "]", "\n", "old_pi_mean", "=", "params_pi", ".", "squeeze", "(", ")", "[", ":", ",", ":", "dim", "]", "\n", "old_pi_std", "=", "adv_params_pi", ".", "squeeze", "(", ")", "[", ":", ",", "dim", ":", "]", "\n", "adv_old_pi_mean", "=", "adv_params_pi", ".", "squeeze", "(", ")", "[", ":", ",", ":", "dim", "]", "\n", "adv_old_pi_std", "=", "adv_params_pi", ".", "squeeze", "(", ")", "[", ":", ",", "dim", ":", "]", "\n", "old_pi_distribution", "=", "Normal", "(", "loc", "=", "old_pi_mean", ",", "scale", "=", "old_pi_std", ")", "\n", "adv_old_pi_distribution", "=", "Normal", "(", "\n", "loc", "=", "adv_old_pi_mean", ",", "scale", "=", "adv_old_pi_std", "+", "1e-8", "\n", ")", "\n", "\n", "# compute kl", "\n", "", "pi_adv_kl", "=", "kl_divergence", "(", "old_pi_distribution", ",", "adv_old_pi_distribution", ")", "\n", "pi_adv_kl", "=", "pi_adv_kl", "[", ":", ",", "None", "]", "\n", "if", "isinstance", "(", "pi_distribution", ",", "Normal", ")", ":", "\n", "                ", "pi_adv_kl", "=", "pi_adv_kl", ".", "sum", "(", "axis", "=", "-", "1", ")", "\n", "", "pi_adv_kl", "*=", "intrinsic_coef", "\n", "\n", "# compute value losses", "\n", "", "pred_values", "=", "self", ".", "_critic", "(", "observations", ")", ".", "flatten", "(", ")", "\n", "# clipped Values", "\n", "pred_values_clipped", "=", "values_old", "+", "torch", ".", "clamp", "(", "\n", "pred_values", "-", "values_old", ",", "\n", "min", "=", "-", "self", ".", "_value_loss_clip", ",", "\n", "max", "=", "self", ".", "_value_loss_clip", ",", "\n", ")", "\n", "vloss1", "=", "torch", ".", "pow", "(", "pred_values", "-", "returns", "-", "pi_adv_kl", ",", "2", ")", "\n", "vloss2", "=", "torch", ".", "pow", "(", "pred_values_clipped", "-", "returns", "-", "pi_adv_kl", ",", "2", ")", "\n", "value_loss", "=", "self", ".", "_value_loss_coeff", "*", "torch", ".", "max", "(", "vloss1", ",", "vloss2", ")", ".", "mean", "(", ")", "\n", "self", ".", "_monitor", "[", "\"value_loss\"", "]", "=", "float", "(", "value_loss", ")", "\n", "\n", "# adversary loss", "\n", "adv_pi_distribution", "=", "self", ".", "_adversary", ".", "compute_distribution", "(", "observations", ")", "\n", "if", "self", ".", "_discrete", ":", "\n", "            ", "adv_pi_distribution", ".", "probs", "=", "adv_pi_distribution", ".", "probs", "+", "1e-8", "\n", "adv_loss", "=", "kl_divergence", "(", "old_pi_distribution", ",", "adv_pi_distribution", ")", "\n", "", "else", ":", "\n", "            ", "adv_pi_distribution", ".", "scale", "=", "adv_pi_distribution", ".", "scale", "+", "1e-8", "\n", "adv_loss", "=", "kl_divergence", "(", "old_pi_distribution", ",", "adv_pi_distribution", ")", ".", "sum", "(", "\n", "axis", "=", "-", "1", "\n", ")", "\n", "", "adv_loss", "=", "adv_loss", ".", "mean", "(", ")", "\n", "self", ".", "_monitor", "[", "\"adversary_loss\"", "]", "=", "float", "(", "adv_loss", ")", "\n", "adv_loss", "*=", "self", ".", "_adv_loss_coeff", "\n", "\n", "# backpropagation", "\n", "self", ".", "_actor_optimizer", ".", "zero_grad", "(", ")", "\n", "policy_loss", ".", "backward", "(", ")", "\n", "self", ".", "_adversary", ".", "zero_grad", "(", ")", "\n", "adv_loss", ".", "backward", "(", ")", "\n", "self", ".", "_critic_optimizer", ".", "zero_grad", "(", ")", "\n", "value_loss", ".", "backward", "(", ")", "\n", "\n", "if", "self", ".", "_clip_grad_norm", "!=", "-", "1", ":", "\n", "            ", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "_actor", ".", "parameters", "(", ")", ",", "self", ".", "_clip_grad_norm", ")", "\n", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "_critic", ".", "parameters", "(", ")", ",", "self", ".", "_clip_grad_norm", ")", "\n", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "_adversary", ".", "parameters", "(", ")", ",", "self", ".", "_clip_grad_norm", ")", "\n", "\n", "", "sync_grads", "(", "self", ".", "_actor", ",", "comm", ")", "\n", "self", ".", "_actor_optimizer", ".", "step", "(", ")", "\n", "sync_grads", "(", "self", ".", "_adversary", ",", "comm", ")", "\n", "self", ".", "_adversary_optimizer", ".", "step", "(", ")", "\n", "sync_grads", "(", "self", ".", "_critic", ",", "comm", ")", "\n", "self", ".", "_critic_optimizer", ".", "step", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.agac_ppo.PPO.logs": [[314, 371], ["logs.append", "logs.append", "logs.append", "logs.append", "logs.append", "logs.append", "logs.append", "logs.append", "logs.append", "agac.logger.LogData", "agac.logger.LogData", "agac.logger.LogData", "agac.logger.LogData", "agac.logger.LogData", "agac.logger.LogData", "agac.logger.LogData", "agac.logger.LogData", "agac.logger.LogData"], "methods", ["None"], ["", "@", "property", "\n", "def", "logs", "(", "self", ")", "->", "List", "[", "LogData", "]", ":", "\n", "        ", "logs", "=", "[", "]", "\n", "logs", ".", "append", "(", "\n", "LogData", "(", "\n", "name", "=", "\"ppo/actor_loss\"", ",", "value", "=", "self", ".", "_monitor", "[", "\"policy_loss\"", "]", ",", "type", "=", "\"scalar\"", "\n", ")", "\n", ")", "\n", "logs", ".", "append", "(", "\n", "LogData", "(", "\n", "name", "=", "\"ppo/value_loss\"", ",", "value", "=", "self", ".", "_monitor", "[", "\"value_loss\"", "]", ",", "type", "=", "\"scalar\"", "\n", ")", "\n", ")", "\n", "logs", ".", "append", "(", "\n", "LogData", "(", "\n", "name", "=", "\"ppo/actor_entropy\"", ",", "value", "=", "self", ".", "_monitor", "[", "\"entropy\"", "]", ",", "type", "=", "\"scalar\"", "\n", ")", "\n", ")", "\n", "logs", ".", "append", "(", "\n", "LogData", "(", "name", "=", "\"ppo/log_pis\"", ",", "value", "=", "self", ".", "_monitor", "[", "\"log_pis\"", "]", ",", "type", "=", "\"scalar\"", ")", "\n", ")", "\n", "logs", ".", "append", "(", "\n", "LogData", "(", "\n", "name", "=", "\"ppo/adv_log_pis\"", ",", "\n", "value", "=", "self", ".", "_monitor", "[", "\"adv_log_pis\"", "]", ",", "\n", "type", "=", "\"scalar\"", ",", "\n", ")", "\n", ")", "\n", "logs", ".", "append", "(", "\n", "LogData", "(", "\n", "name", "=", "\"model/estimated_returns\"", ",", "\n", "value", "=", "self", ".", "_monitor", "[", "\"estimated_returns\"", "]", ",", "\n", "type", "=", "\"scalar\"", ",", "\n", ")", "\n", ")", "\n", "logs", ".", "append", "(", "\n", "LogData", "(", "\n", "name", "=", "\"model/agac_advantages\"", ",", "\n", "value", "=", "self", ".", "_monitor", "[", "\"agac_advantages\"", "]", ",", "\n", "type", "=", "\"scalar\"", ",", "\n", ")", "\n", ")", "\n", "logs", ".", "append", "(", "\n", "LogData", "(", "\n", "name", "=", "\"model/advantages\"", ",", "\n", "value", "=", "self", ".", "_monitor", "[", "\"advantages\"", "]", ",", "\n", "type", "=", "\"scalar\"", ",", "\n", ")", "\n", ")", "\n", "logs", ".", "append", "(", "\n", "LogData", "(", "\n", "name", "=", "\"adversary/loss\"", ",", "\n", "value", "=", "self", ".", "_monitor", "[", "\"adversary_loss\"", "]", ",", "\n", "type", "=", "\"scalar\"", ",", "\n", ")", "\n", ")", "\n", "return", "logs", "\n", "", "", ""]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.utils.DiscreteGrid.__init__": [[26, 34], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "env", ":", "gym_minigrid", ".", "minigrid", ".", "MiniGridEnv", ")", ":", "\n", "        ", "self", ".", "_env", "=", "env", "\n", "self", ".", "_tile_size", "=", "32", "\n", "self", ".", "_visit_weight", "=", "20", "\n", "self", ".", "_agent_positions", "=", "[", "]", "\n", "\n", "self", ".", "_display_grids", "=", "[", "]", "\n", "self", ".", "_grid", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.utils.DiscreteGrid.add": [[35, 39], ["utils.DiscreteGrid._agent_positions.append", "utils.DiscreteGrid._env.grid.render"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.render"], ["", "def", "add", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_grid", "is", "None", ":", "\n", "            ", "self", ".", "_grid", "=", "self", ".", "_env", ".", "grid", ".", "render", "(", "self", ".", "_tile_size", ")", "\n", "", "self", ".", "_agent_positions", ".", "append", "(", "self", ".", "_env", ".", "agent_pos", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.utils.DiscreteGrid.reset": [[40, 44], ["utils.DiscreteGrid._display_grids.append"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "_display_grids", ".", "append", "(", "(", "self", ".", "_grid", ",", "self", ".", "_agent_positions", ")", ")", "\n", "self", ".", "_grid", "=", "None", "\n", "self", ".", "_agent_positions", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.utils.DiscreteGrid._grid_coverage": [[45, 61], ["img.transpose"], "methods", ["None"], ["", "def", "_grid_coverage", "(", "\n", "self", ",", "grid", ":", "np", ".", "ndarray", ",", "positions", ":", "List", "[", "Tuple", "[", "int", "]", "]", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"\n        Returns an image representing the grid coverage over 1 episode.\n        \"\"\"", "\n", "img", "=", "grid", "\n", "for", "pos", "in", "positions", ":", "\n", "            ", "img", "[", "\n", "pos", "[", "1", "]", "*", "self", ".", "_tile_size", ":", "(", "pos", "[", "1", "]", "+", "1", ")", "*", "self", ".", "_tile_size", ",", "\n", "pos", "[", "0", "]", "*", "self", ".", "_tile_size", ":", "(", "pos", "[", "0", "]", "+", "1", ")", "*", "self", ".", "_tile_size", ",", "\n", "2", ",", "\n", "]", "+=", "self", ".", "_visit_weight", "\n", "\n", "", "img", "[", "img", ">", "255", "]", "=", "255", "\n", "return", "img", ".", "transpose", "(", "-", "1", ",", "0", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.utils.DiscreteGrid.logs": [[62, 74], ["agac.logger.LogData", "utils.DiscreteGrid._grid_coverage"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.utils.DiscreteGrid._grid_coverage"], ["", "@", "property", "\n", "def", "logs", "(", "self", ")", "->", "List", "[", "LogData", "]", ":", "\n", "        ", "logs", "=", "[", "\n", "LogData", "(", "\n", "name", "=", "\"grid_coverage\"", ",", "\n", "value", "=", "self", ".", "_grid_coverage", "(", "grid", ",", "positions", ")", ",", "\n", "type", "=", "\"image\"", ",", "\n", ")", "\n", "for", "grid", ",", "positions", "in", "self", ".", "_display_grids", "\n", "]", "\n", "self", ".", "_display_grids", "=", "[", "]", "\n", "return", "logs", "\n", "", "", ""]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.utils.discount": [[10, 12], ["scipy.signal.lfilter", "numpy.np.float32"], "function", ["None"], ["def", "discount", "(", "x", ":", "np", ".", "ndarray", ",", "gamma", ":", "float", ")", ":", "\n", "    ", "return", "scipy", ".", "signal", ".", "lfilter", "(", "[", "1", "]", ",", "[", "1", ",", "-", "gamma", "]", ",", "x", "[", ":", ":", "-", "1", "]", ",", "axis", "=", "0", ")", "[", ":", ":", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.utils.compute_advantages_and_returns": [[14, 23], ["numpy.append", "numpy.append", "utils.discount", "utils.discount", "discount.copy().astype", "returns.copy().astype", "discount.copy", "returns.copy"], "function", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.utils.discount", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.utils.discount"], ["", "def", "compute_advantages_and_returns", "(", "rewards", ",", "values", ",", "last_r", ",", "gamma", ",", "lambda_gae", ")", ":", "\n", "    ", "values", "=", "np", ".", "append", "(", "values", ",", "last_r", ")", "\n", "rewards", "=", "np", ".", "append", "(", "rewards", ",", "last_r", ")", "\n", "delta_t", "=", "rewards", "[", ":", "-", "1", "]", "+", "gamma", "*", "values", "[", "1", ":", "]", "-", "values", "[", ":", "-", "1", "]", "\n", "# This formula for the advantage comes from:", "\n", "# \"Generalized Advantage Estimation\": https://arxiv.org/abs/1506.02438", "\n", "advantages", "=", "discount", "(", "delta_t", ",", "gamma", "*", "lambda_gae", ")", "\n", "returns", "=", "discount", "(", "rewards", ",", "gamma", ")", "[", ":", "-", "1", "]", "\n", "return", "advantages", ".", "copy", "(", ")", ".", "astype", "(", "np", ".", "float32", ")", ",", "returns", ".", "copy", "(", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.configs.ExperimentConfig.is_valid": [[39, 44], ["configs.ExperimentConfig.algorithm.is_valid", "configs.ExperimentConfig.reinforcement_learning.is_valid", "configs.ExperimentConfig.logging.is_valid"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.configs.ReinforcementLearningConfig.is_valid", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.configs.ReinforcementLearningConfig.is_valid", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.configs.ReinforcementLearningConfig.is_valid"], ["def", "is_valid", "(", "self", ")", "->", "bool", ":", "\n", "        ", "cond", "=", "self", ".", "algorithm", ".", "is_valid", "(", ")", "\n", "cond", "&=", "self", ".", "reinforcement_learning", ".", "is_valid", "(", ")", "\n", "cond", "&=", "self", ".", "logging", ".", "is_valid", "(", ")", "\n", "return", "cond", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.configs.LoggingConfig.is_valid": [[58, 60], ["None"], "methods", ["None"], ["def", "is_valid", "(", "self", ")", "->", "bool", ":", "\n", "        ", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.configs.AlgorithmConfig.is_valid": [[76, 78], ["None"], "methods", ["None"], ["def", "is_valid", "(", "self", ")", "->", "bool", ":", "\n", "        ", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.configs.ReinforcementLearningConfig.is_valid": [[104, 106], ["None"], "methods", ["None"], ["def", "is_valid", "(", "self", ")", "->", "bool", ":", "\n", "        ", "return", "True", "\n", "", "", ""]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.configs.get_config_from_yaml": [[8, 29], ["configs.AlgorithmConfig", "configs.ReinforcementLearningConfig", "configs.LoggingConfig", "configs.ExperimentConfig", "open", "yaml.safe_load", "configs.ExperimentConfig.is_valid", "ValueError"], "function", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.configs.ReinforcementLearningConfig.is_valid"], ["def", "get_config_from_yaml", "(", "config_path", ":", "Path", ")", "->", "\"ExperimentConfig\"", ":", "\n", "    ", "\"\"\"Read yaml file and returns corresponding config object.\"\"\"", "\n", "with", "open", "(", "config_path", ",", "\"r\"", ")", "as", "file", ":", "\n", "        ", "parameters", "=", "yaml", ".", "safe_load", "(", "file", ")", "\n", "\n", "# instantiate config object", "\n", "", "algo_config", "=", "AlgorithmConfig", "(", "**", "parameters", "[", "\"algorithm\"", "]", ")", "\n", "rl_config", "=", "ReinforcementLearningConfig", "(", "**", "parameters", "[", "\"reinforcement_learning\"", "]", ")", "\n", "logging_config", "=", "LoggingConfig", "(", "**", "parameters", "[", "\"logging\"", "]", ")", "\n", "\n", "experiment_config", "=", "ExperimentConfig", "(", "\n", "algorithm", "=", "algo_config", ",", "\n", "reinforcement_learning", "=", "rl_config", ",", "\n", "logging", "=", "logging_config", ",", "\n", ")", "\n", "\n", "# checks if the obtained config is valid", "\n", "if", "not", "experiment_config", ".", "is_valid", "(", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"Tries to instantiate invalid experiment config.\"", ")", "\n", "\n", "", "return", "experiment_config", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.memory.Memory.__init__": [[49, 53], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "max_size", "=", "1e6", ")", ":", "\n", "        ", "self", ".", "_storage", "=", "[", "]", "\n", "self", ".", "_max_size", "=", "max_size", "\n", "self", ".", "_pointer_position", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.memory.Memory.num_elements": [[54, 57], ["len"], "methods", ["None"], ["", "@", "property", "\n", "def", "num_elements", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "_storage", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.memory.Memory.add": [[58, 64], ["len", "memory.Memory._storage.append", "int"], "methods", ["None"], ["", "def", "add", "(", "self", ",", "transition", ":", "Transition", ")", ":", "\n", "        ", "if", "len", "(", "self", ".", "_storage", ")", "==", "self", ".", "_max_size", ":", "\n", "            ", "self", ".", "_storage", "[", "int", "(", "self", ".", "_pointer_position", ")", "]", "=", "transition", "\n", "self", ".", "_pointer_position", "=", "(", "self", ".", "_pointer_position", "+", "1", ")", "%", "self", ".", "_max_size", "\n", "", "else", ":", "\n", "            ", "self", ".", "_storage", ".", "append", "(", "transition", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.memory.Memory.reset": [[65, 68], ["None"], "methods", ["None"], ["", "", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "_storage", "=", "[", "]", "\n", "self", ".", "_pointer_position", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.memory.Memory.sample": [[69, 106], ["len", "ValueError", "numpy.random.randint", "operator.itemgetter", "list", "Batch", "len", "zip", "numpy.array().copy", "numpy.array().copy", "numpy.array().copy", "numpy.array().copy", "numpy.array().copy", "numpy.array().copy", "numpy.array().copy", "numpy.array().copy", "numpy.array().copy", "numpy.array().copy", "numpy.array().copy", "operator.itemgetter.", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array"], "methods", ["None"], ["", "def", "sample", "(", "self", ",", "batch_size", ":", "int", ")", "->", "Batch", ":", "\n", "\n", "        ", "if", "len", "(", "self", ".", "_storage", ")", "<", "batch_size", ":", "\n", "            ", "raise", "ValueError", "(", "\"Not enough data in replay buffer to sample a batch.\"", ")", "\n", "\n", "", "else", ":", "\n", "            ", "ind", "=", "np", ".", "random", ".", "randint", "(", "0", ",", "len", "(", "self", ".", "_storage", ")", ",", "size", "=", "batch_size", ")", "\n", "op", "=", "operator", ".", "itemgetter", "(", "*", "ind", ")", "\n", "(", "\n", "observation", ",", "\n", "action", ",", "\n", "extrinsic_return", ",", "\n", "advantage", ",", "\n", "agac_advantage", ",", "\n", "value", ",", "\n", "log_pi", ",", "\n", "adv_log_pi", ",", "\n", "logits_pi", ",", "\n", "adv_logits_pi", ",", "\n", "done", ",", "\n", ")", "=", "list", "(", "zip", "(", "*", "op", "(", "self", ".", "_storage", ")", ")", ")", "\n", "\n", "batch", "=", "Batch", "(", "\n", "observations", "=", "np", ".", "array", "(", "observation", ")", ".", "copy", "(", ")", ",", "\n", "actions", "=", "np", ".", "array", "(", "action", ")", ".", "copy", "(", ")", ",", "\n", "extrinsic_returns", "=", "np", ".", "array", "(", "extrinsic_return", ")", ".", "copy", "(", ")", ",", "\n", "advantages", "=", "np", ".", "array", "(", "advantage", ")", ".", "copy", "(", ")", ",", "\n", "agac_advantages", "=", "np", ".", "array", "(", "agac_advantage", ")", ".", "copy", "(", ")", ",", "\n", "values", "=", "np", ".", "array", "(", "value", ")", ".", "copy", "(", ")", ",", "\n", "log_pis", "=", "np", ".", "array", "(", "log_pi", ")", ".", "copy", "(", ")", ",", "\n", "adv_log_pis", "=", "np", ".", "array", "(", "adv_log_pi", ")", ".", "copy", "(", ")", ",", "\n", "logits_pi", "=", "np", ".", "array", "(", "logits_pi", ")", ".", "copy", "(", ")", ",", "\n", "adv_logits_pi", "=", "np", ".", "array", "(", "adv_logits_pi", ")", ".", "copy", "(", ")", ",", "\n", "dones", "=", "np", ".", "array", "(", "done", ")", ".", "copy", "(", ")", ",", "\n", ")", "\n", "\n", "return", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.memory.Memory.get_advantages_stats": [[107, 126], ["list", "numpy.array().copy", "numpy.array().copy", "comm.allgather", "numpy.concatenate", "comm.allgather", "numpy.concatenate", "zip", "tuple", "tuple", "numpy.concatenate.mean", "numpy.concatenate.std", "numpy.concatenate.mean", "numpy.concatenate.std", "numpy.array", "numpy.array"], "methods", ["None"], ["", "", "def", "get_advantages_stats", "(", "self", ")", ":", "\n", "        ", "_", ",", "_", ",", "_", ",", "advantages", ",", "agac_advantages", ",", "_", ",", "_", ",", "_", ",", "_", ",", "_", ",", "_", "=", "list", "(", "\n", "zip", "(", "*", "self", ".", "_storage", ")", "\n", ")", "\n", "advantages", "=", "np", ".", "array", "(", "advantages", ")", ".", "copy", "(", ")", "\n", "agac_advantages", "=", "np", ".", "array", "(", "agac_advantages", ")", ".", "copy", "(", ")", "\n", "\n", "# Share advantages between processes to get more accurate stats", "\n", "advantages", "=", "comm", ".", "allgather", "(", "advantages", ")", "\n", "advantages", "=", "np", ".", "concatenate", "(", "tuple", "(", "advantages", ")", ")", "\n", "\n", "agac_advantages", "=", "comm", ".", "allgather", "(", "agac_advantages", ")", "\n", "agac_advantages", "=", "np", ".", "concatenate", "(", "tuple", "(", "agac_advantages", ")", ")", "\n", "\n", "return", "(", "\n", "advantages", ".", "mean", "(", ")", ",", "\n", "advantages", ".", "std", "(", ")", ",", "\n", "agac_advantages", ".", "mean", "(", ")", ",", "\n", "agac_advantages", ".", "std", "(", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.memory.Memory.get_epoch_batches": [[128, 180], ["len", "ValueError", "numpy.arange", "numpy.random.shuffle", "numpy.min", "memory.Memory.get_advantages_stats", "range", "len", "len", "len", "comm.allgather", "operator.itemgetter", "list", "Batch", "batches.append", "zip", "numpy.array().copy", "numpy.array().copy", "numpy.array().copy", "numpy.array().copy", "numpy.array().copy", "numpy.array().copy", "numpy.array().copy", "numpy.array().copy", "numpy.array().copy", "numpy.array().copy", "operator.itemgetter.", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array().copy", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.memory.Memory.get_advantages_stats"], ["", "def", "get_epoch_batches", "(", "self", ",", "batch_size", ":", "int", ")", "->", "List", "[", "Batch", "]", ":", "\n", "\n", "        ", "if", "len", "(", "self", ".", "_storage", ")", "<", "batch_size", ":", "\n", "            ", "raise", "ValueError", "(", "\"Not enough data in replay buffer to sample a batch.\"", ")", "\n", "\n", "", "else", ":", "\n", "            ", "idxes", "=", "np", ".", "arange", "(", "len", "(", "self", ".", "_storage", ")", ")", "\n", "np", ".", "random", ".", "shuffle", "(", "idxes", ")", "\n", "num_batches", "=", "len", "(", "self", ".", "_storage", ")", "//", "batch_size", "\n", "if", "len", "(", "self", ".", "_storage", ")", ">", "num_batches", "*", "batch_size", ":", "\n", "                ", "num_batches", "+=", "1", "\n", "", "num_batches", "=", "np", ".", "min", "(", "comm", ".", "allgather", "(", "num_batches", ")", ")", "\n", "idxes", "=", "idxes", "[", ":", "(", "num_batches", "*", "batch_size", ")", "]", "\n", "\n", "adv_mean", ",", "adv_std", ",", "agac_adv_mean", ",", "agac_adv_std", "=", "self", ".", "get_advantages_stats", "(", ")", "\n", "\n", "batches", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_batches", ")", ":", "\n", "                ", "ind", "=", "idxes", "[", "i", ":", ":", "num_batches", "]", "\n", "op", "=", "operator", ".", "itemgetter", "(", "*", "ind", ")", "\n", "(", "\n", "observation", ",", "\n", "action", ",", "\n", "extrinsic_return", ",", "\n", "advantage", ",", "\n", "agac_advantage", ",", "\n", "value", ",", "\n", "log_pi", ",", "\n", "adv_log_pi", ",", "\n", "logits_pi", ",", "\n", "adv_logits_pi", ",", "\n", "done", ",", "\n", ")", "=", "list", "(", "zip", "(", "*", "op", "(", "self", ".", "_storage", ")", ")", ")", "\n", "\n", "batch", "=", "Batch", "(", "\n", "observations", "=", "np", ".", "array", "(", "observation", ")", ".", "copy", "(", ")", ",", "\n", "actions", "=", "np", ".", "array", "(", "action", ")", ".", "copy", "(", ")", ",", "\n", "extrinsic_returns", "=", "np", ".", "array", "(", "extrinsic_return", ")", ".", "copy", "(", ")", ",", "\n", "advantages", "=", "np", ".", "array", "(", "advantage", ")", ".", "copy", "(", ")", ",", "# unnormalized", "\n", "agac_advantages", "=", "(", "np", ".", "array", "(", "agac_advantage", ")", ".", "copy", "(", ")", "-", "agac_adv_mean", ")", "\n", "/", "agac_adv_std", ",", "\n", "values", "=", "np", ".", "array", "(", "value", ")", ".", "copy", "(", ")", ",", "\n", "log_pis", "=", "np", ".", "array", "(", "log_pi", ")", ".", "copy", "(", ")", ",", "\n", "adv_log_pis", "=", "np", ".", "array", "(", "adv_log_pi", ")", ".", "copy", "(", ")", ",", "\n", "logits_pi", "=", "np", ".", "array", "(", "logits_pi", ")", ".", "copy", "(", ")", ",", "\n", "adv_logits_pi", "=", "np", ".", "array", "(", "adv_logits_pi", ")", ".", "copy", "(", ")", ",", "\n", "dones", "=", "np", ".", "array", "(", "done", ")", ".", "copy", "(", ")", ",", "\n", ")", "\n", "\n", "batches", ".", "append", "(", "batch", ")", "\n", "\n", "", "return", "batches", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac.memory.Memory.save": [[181, 184], ["numpy.save", "print"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.VecNormalize.save"], ["", "", "def", "save", "(", "self", ",", "outfile", ")", ":", "\n", "        ", "np", ".", "save", "(", "outfile", ",", "self", ".", "_storage", ")", "\n", "print", "(", "f\"* {outfile} succesfully saved..\"", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.agac_torch.run_agac_experiment.get_parser_args": [[11, 28], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["def", "get_parser_args", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--config-path\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "\"agac_torch/configs/minigrid.yaml\"", ",", "\n", "help", "=", "\"config\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--env-name\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "\"MiniGrid-MultiRoom-N10-S10-v0\"", ",", "\n", "help", "=", "\"gym env name\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\"--seed\"", ",", "type", "=", "int", ",", "default", "=", "123", ",", "help", "=", "\"seed number\"", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_layers.ortho_init": [[5, 43], ["tuple", "numpy.random.normal", "numpy.linalg.svd", "weights.reshape.reshape", "len", "len", "numpy.prod"], "function", ["None"], ["def", "ortho_init", "(", "scale", "=", "1.0", ")", ":", "\n", "    ", "\"\"\"\n    Orthogonal initialization for the policy weights\n\n    :param scale: (float) Scaling factor for the weights.\n    :return: (function) an initialization function for the weights\n    \"\"\"", "\n", "\n", "# _ortho_init(shape, dtype, partition_info=None)", "\n", "def", "_ortho_init", "(", "shape", ",", "*", "_", ",", "**", "_kwargs", ")", ":", "\n", "        ", "\"\"\"Intialize weights as Orthogonal matrix.\n\n        Orthogonal matrix initialization [1]_. For n-dimensional shapes where\n        n > 2, the n-1 trailing axes are flattened. For convolutional layers, this\n        corresponds to the fan-in, so this makes the initialization usable for\n        both dense and convolutional layers.\n\n        References\n        ----------\n        .. [1] Saxe, Andrew M., James L. McClelland, and Surya Ganguli.\n               \"Exact solutions to the nonlinear dynamics of learning in deep\n               linear\n        \"\"\"", "\n", "# lasagne ortho init for tf", "\n", "shape", "=", "tuple", "(", "shape", ")", "\n", "if", "len", "(", "shape", ")", "==", "2", ":", "\n", "            ", "flat_shape", "=", "shape", "\n", "", "elif", "len", "(", "shape", ")", "==", "4", ":", "# assumes NHWC", "\n", "            ", "flat_shape", "=", "(", "np", ".", "prod", "(", "shape", "[", ":", "-", "1", "]", ")", ",", "shape", "[", "-", "1", "]", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "", "gaussian_noise", "=", "np", ".", "random", ".", "normal", "(", "0.0", ",", "1.0", ",", "flat_shape", ")", "\n", "u", ",", "_", ",", "v", "=", "np", ".", "linalg", ".", "svd", "(", "gaussian_noise", ",", "full_matrices", "=", "False", ")", "\n", "weights", "=", "u", "if", "u", ".", "shape", "==", "flat_shape", "else", "v", "# pick the one with the correct shape", "\n", "weights", "=", "weights", ".", "reshape", "(", "shape", ")", "\n", "return", "(", "scale", "*", "weights", "[", ":", "shape", "[", "0", "]", ",", ":", "shape", "[", "1", "]", "]", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "\n", "", "return", "_ortho_init", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_layers.mlp": [[45, 62], ["enumerate", "tensorflow.layers.dense", "activ_fn", "tensorflow.contrib.layers.layer_norm", "str"], "function", ["None"], ["", "def", "mlp", "(", "input_tensor", ",", "layers", ",", "activ_fn", "=", "tf", ".", "nn", ".", "relu", ",", "layer_norm", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Create a multi-layer fully connected neural network.\n\n    :param input_tensor: (tf.placeholder)\n    :param layers: ([int]) Network architecture\n    :param activ_fn: (tf.function) Activation function\n    :param layer_norm: (bool) Whether to apply layer normalization or not\n    :return: (tf.Tensor)\n    \"\"\"", "\n", "output", "=", "input_tensor", "\n", "for", "i", ",", "layer_size", "in", "enumerate", "(", "layers", ")", ":", "\n", "        ", "output", "=", "tf", ".", "layers", ".", "dense", "(", "output", ",", "layer_size", ",", "name", "=", "'fc'", "+", "str", "(", "i", ")", ")", "\n", "if", "layer_norm", ":", "\n", "            ", "output", "=", "tf", ".", "contrib", ".", "layers", ".", "layer_norm", "(", "output", ",", "center", "=", "True", ",", "scale", "=", "True", ")", "\n", "", "output", "=", "activ_fn", "(", "output", ")", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_layers.conv": [[64, 108], ["isinstance", "isinstance", "input_tensor.get_shape", "tensorflow.compat.v1.variable_scope", "tensorflow.compat.v1.get_variable", "tensorflow.compat.v1.get_variable", "len", "len", "tensorflow.reshape", "tensorflow.nn.conv2d", "tf_layers.ortho_init", "tensorflow.constant_initializer"], "function", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_layers.ortho_init"], ["", "def", "conv", "(", "input_tensor", ",", "scope", ",", "*", ",", "n_filters", ",", "filter_size", ",", "stride", ",", "\n", "pad", "=", "'SAME'", ",", "init_scale", "=", "1.0", ",", "data_format", "=", "'NHWC'", ",", "one_dim_bias", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Creates a 2d convolutional layer for TensorFlow\n\n    :param input_tensor: (TensorFlow Tensor) The input tensor for the convolution\n    :param scope: (str) The TensorFlow variable scope\n    :param n_filters: (int) The number of filters\n    :param filter_size:  (Union[int, [int], tuple<int, int>]) The filter size for the squared kernel matrix,\n    or the height and width of kernel filter if the input is a list or tuple\n    :param stride: (int) The stride of the convolution\n    :param pad: (str) The padding type ('VALID' or 'SAME')\n    :param init_scale: (int) The initialization scale\n    :param data_format: (str) The data format for the convolution weights\n    :param one_dim_bias: (bool) If the bias should be one dimentional or not\n    :return: (TensorFlow Tensor) 2d convolutional layer\n    \"\"\"", "\n", "if", "isinstance", "(", "filter_size", ",", "list", ")", "or", "isinstance", "(", "filter_size", ",", "tuple", ")", ":", "\n", "        ", "assert", "len", "(", "filter_size", ")", "==", "2", ",", "\"Filter size must have 2 elements (height, width), {} were given\"", ".", "format", "(", "len", "(", "filter_size", ")", ")", "\n", "filter_height", "=", "filter_size", "[", "0", "]", "\n", "filter_width", "=", "filter_size", "[", "1", "]", "\n", "", "else", ":", "\n", "        ", "filter_height", "=", "filter_size", "\n", "filter_width", "=", "filter_size", "\n", "", "if", "data_format", "==", "'NHWC'", ":", "\n", "        ", "channel_ax", "=", "3", "\n", "strides", "=", "[", "1", ",", "stride", ",", "stride", ",", "1", "]", "\n", "bshape", "=", "[", "1", ",", "1", ",", "1", ",", "n_filters", "]", "\n", "", "elif", "data_format", "==", "'NCHW'", ":", "\n", "        ", "channel_ax", "=", "1", "\n", "strides", "=", "[", "1", ",", "1", ",", "stride", ",", "stride", "]", "\n", "bshape", "=", "[", "1", ",", "n_filters", ",", "1", ",", "1", "]", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "", "bias_var_shape", "=", "[", "n_filters", "]", "if", "one_dim_bias", "else", "[", "1", ",", "n_filters", ",", "1", ",", "1", "]", "\n", "n_input", "=", "input_tensor", ".", "get_shape", "(", ")", "[", "channel_ax", "]", "\n", "wshape", "=", "[", "filter_height", ",", "filter_width", ",", "n_input", ",", "n_filters", "]", "\n", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "scope", ")", ":", "\n", "        ", "weight", "=", "tf", ".", "compat", ".", "v1", ".", "get_variable", "(", "\"w\"", ",", "wshape", ",", "initializer", "=", "ortho_init", "(", "init_scale", ")", ")", "\n", "bias", "=", "tf", ".", "compat", ".", "v1", ".", "get_variable", "(", "\"b\"", ",", "bias_var_shape", ",", "initializer", "=", "tf", ".", "constant_initializer", "(", "0.0", ")", ")", "\n", "if", "not", "one_dim_bias", "and", "data_format", "==", "'NHWC'", ":", "\n", "            ", "bias", "=", "tf", ".", "reshape", "(", "bias", ",", "bshape", ")", "\n", "", "return", "bias", "+", "tf", ".", "nn", ".", "conv2d", "(", "input_tensor", ",", "weight", ",", "strides", "=", "strides", ",", "padding", "=", "pad", ",", "data_format", "=", "data_format", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_layers.linear": [[110, 126], ["tensorflow.compat.v1.variable_scope", "tensorflow.compat.v1.get_variable", "tensorflow.compat.v1.get_variable", "input_tensor.get_shape", "tensorflow.matmul", "tf_layers.ortho_init", "tensorflow.constant_initializer"], "function", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_layers.ortho_init"], ["", "", "def", "linear", "(", "input_tensor", ",", "scope", ",", "n_hidden", ",", "*", ",", "init_scale", "=", "1.0", ",", "init_bias", "=", "0.0", ")", ":", "\n", "    ", "\"\"\"\n    Creates a fully connected layer for TensorFlow\n\n    :param input_tensor: (TensorFlow Tensor) The input tensor for the fully connected layer\n    :param scope: (str) The TensorFlow variable scope\n    :param n_hidden: (int) The number of hidden neurons\n    :param init_scale: (int) The initialization scale\n    :param init_bias: (int) The initialization offset bias\n    :return: (TensorFlow Tensor) fully connected layer\n    \"\"\"", "\n", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "scope", ")", ":", "\n", "        ", "n_input", "=", "input_tensor", ".", "get_shape", "(", ")", "[", "1", "]", "\n", "weight", "=", "tf", ".", "compat", ".", "v1", ".", "get_variable", "(", "\"w\"", ",", "[", "n_input", ",", "n_hidden", "]", ",", "initializer", "=", "ortho_init", "(", "init_scale", ")", ")", "\n", "bias", "=", "tf", ".", "compat", ".", "v1", ".", "get_variable", "(", "\"b\"", ",", "[", "n_hidden", "]", ",", "initializer", "=", "tf", ".", "constant_initializer", "(", "init_bias", ")", ")", "\n", "return", "tf", ".", "matmul", "(", "input_tensor", ",", "weight", ")", "+", "bias", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_layers.minibatchAC": [[128, 139], ["tensorflow.compat.v1.variable_scope", "tensorflow.compat.v1.get_variable", "tensorflow.reshape", "input_tensor.get_shape", "tensorflow.keras.backend.dot"], "function", ["None"], ["", "", "def", "minibatchAC", "(", "input_tensor", ",", "scope", ",", "nb_kernels", ",", "kernel_dim", ")", ":", "\n", "    ", "\"\"\"\n    Creates a\n    \"\"\"", "\n", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "scope", ")", ":", "\n", "        ", "n_input", "=", "input_tensor", ".", "get_shape", "(", ")", "[", "1", "]", "\n", "weight", "=", "tf", ".", "compat", ".", "v1", ".", "get_variable", "(", "shape", "=", "(", "nb_kernels", ",", "n_input", ",", "kernel_dim", ")", ",", "\n", "initializer", "=", "tf", ".", "compat", ".", "v1", ".", "keras", ".", "initializers", ".", "glorot_uniform", ",", "\n", "name", "=", "'kernel'", ",", "\n", "trainable", "=", "True", ")", "\n", "return", "tf", ".", "reshape", "(", "tf", ".", "keras", ".", "backend", ".", "dot", "(", "input_tensor", ",", "weight", ")", ",", "(", "-", "1", ",", "nb_kernels", ",", "kernel_dim", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_layers.transformer": [[141, 152], ["tensorflow.compat.v1.variable_scope", "tensorflow.compat.v1.get_variable", "tensorflow.reshape", "input_tensor.get_shape", "tensorflow.keras.backend.dot"], "function", ["None"], ["", "", "def", "transformer", "(", "input_tensor", ",", "scope", ",", "kernel_dim", ")", ":", "\n", "    ", "\"\"\"\n    Creates a\n    \"\"\"", "\n", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "scope", ")", ":", "\n", "        ", "n_input", "=", "input_tensor", ".", "get_shape", "(", ")", "[", "1", "]", "\n", "weight", "=", "tf", ".", "compat", ".", "v1", ".", "get_variable", "(", "shape", "=", "(", "n_input", ",", "kernel_dim", ")", ",", "\n", "initializer", "=", "tf", ".", "compat", ".", "v1", ".", "keras", ".", "initializers", ".", "glorot_uniform", ",", "\n", "name", "=", "'kernel'", ",", "\n", "trainable", "=", "True", ")", "\n", "return", "tf", ".", "reshape", "(", "tf", ".", "keras", ".", "backend", ".", "dot", "(", "input_tensor", ",", "weight", ")", ",", "(", "-", "1", ",", "kernel_dim", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_layers.lstm": [[154, 205], ["tensorflow.split", "enumerate", "tensorflow.concat", "tensorflow.compat.v1.variable_scope", "tensorflow.compat.v1.get_variable", "tensorflow.compat.v1.get_variable", "tensorflow.compat.v1.get_variable", "zip", "tensorflow.split", "tensorflow.nn.sigmoid", "tensorflow.nn.sigmoid", "tensorflow.nn.sigmoid", "tensorflow.tanh", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.get_variable", "tf_layers.ortho_init", "tf_layers.ortho_init", "tensorflow.constant_initializer", "tensorflow.tanh", "tensorflow.tanh", "tensorflow.constant_initializer", "tensorflow.constant_initializer", "tensorflow.constant_initializer", "tensorflow.constant_initializer", "tensorflow.constant_initializer", "tensorflow.constant_initializer", "tf_layers._ln", "tf_layers._ln", "tensorflow.matmul", "tensorflow.matmul", "tf_layers._ln", "tensorflow.matmul", "tensorflow.matmul"], "function", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_layers.ortho_init", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_layers.ortho_init", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_layers._ln", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_layers._ln", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_layers._ln"], ["", "", "def", "lstm", "(", "input_tensor", ",", "mask_tensor", ",", "cell_state_hidden", ",", "scope", ",", "n_input", ",", "n_hidden", ",", "init_scale", "=", "1.0", ",", "layer_norm", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Creates an Long Short Term Memory (LSTM) cell for TensorFlow\n\n    :param input_tensor: (TensorFlow Tensor) The input tensor for the LSTM cell\n    :param mask_tensor: (TensorFlow Tensor) The mask tensor for the LSTM cell\n    :param cell_state_hidden: (TensorFlow Tensor) The state tensor for the LSTM cell\n    :param scope: (str) The TensorFlow variable scope\n    :param n_hidden: (int) The number of hidden neurons\n    :param init_scale: (int) The initialization scale\n    :param layer_norm: (bool) Whether to apply Layer Normalization or not\n    :return: (TensorFlow Tensor) LSTM cell\n    \"\"\"", "\n", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "scope", ")", ":", "\n", "        ", "weight_x", "=", "tf", ".", "compat", ".", "v1", ".", "get_variable", "(", "\"wx\"", ",", "[", "n_input", ",", "n_hidden", "*", "4", "]", ",", "initializer", "=", "ortho_init", "(", "init_scale", ")", ")", "\n", "weight_h", "=", "tf", ".", "compat", ".", "v1", ".", "get_variable", "(", "\"wh\"", ",", "[", "n_hidden", ",", "n_hidden", "*", "4", "]", ",", "initializer", "=", "ortho_init", "(", "init_scale", ")", ")", "\n", "bias", "=", "tf", ".", "compat", ".", "v1", ".", "get_variable", "(", "\"b\"", ",", "[", "n_hidden", "*", "4", "]", ",", "initializer", "=", "tf", ".", "constant_initializer", "(", "0.0", ")", ")", "\n", "\n", "if", "layer_norm", ":", "\n", "# Gain and bias of layer norm", "\n", "            ", "gain_x", "=", "tf", ".", "get_variable", "(", "\"gx\"", ",", "[", "n_hidden", "*", "4", "]", ",", "initializer", "=", "tf", ".", "constant_initializer", "(", "1.0", ")", ")", "\n", "bias_x", "=", "tf", ".", "get_variable", "(", "\"bx\"", ",", "[", "n_hidden", "*", "4", "]", ",", "initializer", "=", "tf", ".", "constant_initializer", "(", "0.0", ")", ")", "\n", "\n", "gain_h", "=", "tf", ".", "get_variable", "(", "\"gh\"", ",", "[", "n_hidden", "*", "4", "]", ",", "initializer", "=", "tf", ".", "constant_initializer", "(", "1.0", ")", ")", "\n", "bias_h", "=", "tf", ".", "get_variable", "(", "\"bh\"", ",", "[", "n_hidden", "*", "4", "]", ",", "initializer", "=", "tf", ".", "constant_initializer", "(", "0.0", ")", ")", "\n", "\n", "gain_c", "=", "tf", ".", "get_variable", "(", "\"gc\"", ",", "[", "n_hidden", "]", ",", "initializer", "=", "tf", ".", "constant_initializer", "(", "1.0", ")", ")", "\n", "bias_c", "=", "tf", ".", "get_variable", "(", "\"bc\"", ",", "[", "n_hidden", "]", ",", "initializer", "=", "tf", ".", "constant_initializer", "(", "0.0", ")", ")", "\n", "\n", "", "", "cell_state", ",", "hidden", "=", "tf", ".", "split", "(", "axis", "=", "1", ",", "num_or_size_splits", "=", "2", ",", "value", "=", "cell_state_hidden", ")", "\n", "for", "idx", ",", "(", "_input", ",", "mask", ")", "in", "enumerate", "(", "zip", "(", "input_tensor", ",", "mask_tensor", ")", ")", ":", "\n", "        ", "cell_state", "=", "cell_state", "*", "(", "1", "-", "mask", ")", "\n", "hidden", "=", "hidden", "*", "(", "1", "-", "mask", ")", "\n", "if", "layer_norm", ":", "\n", "            ", "gates", "=", "_ln", "(", "tf", ".", "matmul", "(", "_input", ",", "weight_x", ")", ",", "gain_x", ",", "bias_x", ")", "+", "_ln", "(", "tf", ".", "matmul", "(", "hidden", ",", "weight_h", ")", ",", "gain_h", ",", "bias_h", ")", "+", "bias", "\n", "", "else", ":", "\n", "            ", "gates", "=", "tf", ".", "matmul", "(", "_input", ",", "weight_x", ")", "+", "tf", ".", "matmul", "(", "hidden", ",", "weight_h", ")", "+", "bias", "\n", "", "in_gate", ",", "forget_gate", ",", "out_gate", ",", "cell_candidate", "=", "tf", ".", "split", "(", "axis", "=", "1", ",", "num_or_size_splits", "=", "4", ",", "value", "=", "gates", ")", "\n", "in_gate", "=", "tf", ".", "nn", ".", "sigmoid", "(", "in_gate", ")", "\n", "forget_gate", "=", "tf", ".", "nn", ".", "sigmoid", "(", "forget_gate", ")", "\n", "out_gate", "=", "tf", ".", "nn", ".", "sigmoid", "(", "out_gate", ")", "\n", "cell_candidate", "=", "tf", ".", "tanh", "(", "cell_candidate", ")", "\n", "cell_state", "=", "forget_gate", "*", "cell_state", "+", "in_gate", "*", "cell_candidate", "\n", "if", "layer_norm", ":", "\n", "            ", "hidden", "=", "out_gate", "*", "tf", ".", "tanh", "(", "_ln", "(", "cell_state", ",", "gain_c", ",", "bias_c", ")", ")", "\n", "", "else", ":", "\n", "            ", "hidden", "=", "out_gate", "*", "tf", ".", "tanh", "(", "cell_state", ")", "\n", "", "input_tensor", "[", "idx", "]", "=", "hidden", "\n", "", "cell_state_hidden", "=", "tf", ".", "concat", "(", "axis", "=", "1", ",", "values", "=", "[", "cell_state", ",", "hidden", "]", ")", "\n", "return", "input_tensor", ",", "cell_state_hidden", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_layers._ln": [[207, 224], ["tensorflow.nn.moments", "tensorflow.sqrt"], "function", ["None"], ["", "def", "_ln", "(", "input_tensor", ",", "gain", ",", "bias", ",", "epsilon", "=", "1e-5", ",", "axes", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Apply layer normalisation.\n\n    :param input_tensor: (TensorFlow Tensor) The input tensor for the Layer normalization\n    :param gain: (TensorFlow Tensor) The scale tensor for the Layer normalization\n    :param bias: (TensorFlow Tensor) The bias tensor for the Layer normalization\n    :param epsilon: (float) The epsilon value for floating point calculations\n    :param axes: (tuple, list or int) The axes to apply the mean and variance calculation\n    :return: (TensorFlow Tensor) a normalizing layer\n    \"\"\"", "\n", "if", "axes", "is", "None", ":", "\n", "        ", "axes", "=", "[", "1", "]", "\n", "", "mean", ",", "variance", "=", "tf", ".", "nn", ".", "moments", "(", "input_tensor", ",", "axes", "=", "axes", ",", "keep_dims", "=", "True", ")", "\n", "input_tensor", "=", "(", "input_tensor", "-", "mean", ")", "/", "tf", ".", "sqrt", "(", "variance", "+", "epsilon", ")", "\n", "input_tensor", "=", "input_tensor", "*", "gain", "+", "bias", "\n", "return", "input_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_layers.lnlstm": [[226, 239], ["tf_layers.lstm"], "function", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_layers.lstm"], ["", "def", "lnlstm", "(", "input_tensor", ",", "mask_tensor", ",", "cell_state", ",", "scope", ",", "n_hidden", ",", "init_scale", "=", "1.0", ")", ":", "\n", "    ", "\"\"\"\n    Creates a LSTM with Layer Normalization (lnlstm) cell for TensorFlow\n\n    :param input_tensor: (TensorFlow Tensor) The input tensor for the LSTM cell\n    :param mask_tensor: (TensorFlow Tensor) The mask tensor for the LSTM cell\n    :param cell_state: (TensorFlow Tensor) The state tensor for the LSTM cell\n    :param scope: (str) The TensorFlow variable scope\n    :param n_hidden: (int) The number of hidden neurons\n    :param init_scale: (int) The initialization scale\n    :return: (TensorFlow Tensor) lnlstm cell\n    \"\"\"", "\n", "return", "lstm", "(", "input_tensor", ",", "mask_tensor", ",", "cell_state", ",", "scope", ",", "n_hidden", ",", "init_scale", ",", "layer_norm", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_layers.conv_to_fc": [[241, 251], ["numpy.prod", "tensorflow.reshape", "tf.reshape.get_shape"], "function", ["None"], ["", "def", "conv_to_fc", "(", "input_tensor", ")", ":", "\n", "    ", "\"\"\"\n    Reshapes a Tensor from a convolutional network to a Tensor for a fully connected network\n\n    :param input_tensor: (TensorFlow Tensor) The convolutional input tensor\n    :return: (TensorFlow Tensor) The fully connected output tensor\n    \"\"\"", "\n", "n_hidden", "=", "np", ".", "prod", "(", "input_tensor", ".", "get_shape", "(", ")", "[", "1", ":", "]", ")", "\n", "input_tensor", "=", "tf", ".", "reshape", "(", "input_tensor", ",", "[", "-", "1", ",", "n_hidden", "]", ")", "\n", "return", "input_tensor", "\n", "", ""]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.math_util.RunningMeanStd.__init__": [[5, 16], ["numpy.zeros", "numpy.ones"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "epsilon", "=", "1e-4", ",", "shape", "=", "(", ")", ")", ":", "\n", "        ", "\"\"\"\n        calulates the running mean and std of a data stream\n        https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Parallel_algorithm\n\n        :param epsilon: (float) helps with arithmetic issues\n        :param shape: (tuple) the shape of the data stream's output\n        \"\"\"", "\n", "self", ".", "mean", "=", "np", ".", "zeros", "(", "shape", ",", "'float64'", ")", "\n", "self", ".", "var", "=", "np", ".", "ones", "(", "shape", ",", "'float64'", ")", "\n", "self", ".", "count", "=", "epsilon", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.math_util.RunningMeanStd.update": [[17, 22], ["numpy.mean", "numpy.var", "math_util.RunningMeanStd.update_from_moments"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.math_util.RunningMeanStd.update_from_moments"], ["", "def", "update", "(", "self", ",", "arr", ")", ":", "\n", "        ", "batch_mean", "=", "np", ".", "mean", "(", "arr", ",", "axis", "=", "0", ")", "\n", "batch_var", "=", "np", ".", "var", "(", "arr", ",", "axis", "=", "0", ")", "\n", "batch_count", "=", "arr", ".", "shape", "[", "0", "]", "\n", "self", ".", "update_from_moments", "(", "batch_mean", ",", "batch_var", ",", "batch_count", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.math_util.RunningMeanStd.update_from_moments": [[23, 38], ["numpy.square"], "methods", ["None"], ["", "def", "update_from_moments", "(", "self", ",", "batch_mean", ",", "batch_var", ",", "batch_count", ")", ":", "\n", "        ", "delta", "=", "batch_mean", "-", "self", ".", "mean", "\n", "tot_count", "=", "self", ".", "count", "+", "batch_count", "\n", "\n", "new_mean", "=", "self", ".", "mean", "+", "delta", "*", "batch_count", "/", "tot_count", "\n", "m_a", "=", "self", ".", "var", "*", "self", ".", "count", "\n", "m_b", "=", "batch_var", "*", "batch_count", "\n", "m_2", "=", "m_a", "+", "m_b", "+", "np", ".", "square", "(", "delta", ")", "*", "self", ".", "count", "*", "batch_count", "/", "(", "self", ".", "count", "+", "batch_count", ")", "\n", "new_var", "=", "m_2", "/", "(", "self", ".", "count", "+", "batch_count", ")", "\n", "\n", "new_count", "=", "batch_count", "+", "self", ".", "count", "\n", "\n", "self", ".", "mean", "=", "new_mean", "\n", "self", ".", "var", "=", "new_var", "\n", "self", ".", "count", "=", "new_count", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.math_util.safe_mean": [[40, 49], ["numpy.mean", "len"], "function", ["None"], ["", "", "def", "safe_mean", "(", "arr", ")", ":", "\n", "    ", "\"\"\"\n    Compute the mean of an array if there is at least one element.\n    For empty array, return nan. It is used for logging only.\n\n    :param arr: (np.ndarray)\n    :return: (float)\n    \"\"\"", "\n", "return", "np", ".", "nan", "if", "len", "(", "arr", ")", "==", "0", "else", "np", ".", "mean", "(", "arr", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.math_util.explained_variance": [[51, 68], ["numpy.var", "numpy.var"], "function", ["None"], ["", "def", "explained_variance", "(", "y_pred", ",", "y_true", ")", ":", "\n", "    ", "\"\"\"\n    Computes fraction of variance that ypred explains about y.\n    Returns 1 - Var[y-ypred] / Var[y]\n\n    interpretation:\n        ev=0  =>  might as well have predicted zero\n        ev=1  =>  perfect prediction\n        ev<0  =>  worse than just predicting zero\n\n    :param y_pred: (np.ndarray) the prediction\n    :param y_true: (np.ndarray) the expected value\n    :return: (float) explained variance of ypred and y\n    \"\"\"", "\n", "assert", "y_true", ".", "ndim", "==", "1", "and", "y_pred", ".", "ndim", "==", "1", "\n", "var_y", "=", "np", ".", "var", "(", "y_true", ")", "\n", "return", "np", ".", "nan", "if", "var_y", "==", "0", "else", "1", "-", "np", ".", "var", "(", "y_true", "-", "y_pred", ")", "/", "var_y", "\n", "", ""]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.input.observation_input": [[6, 51], ["isinstance", "tensorflow.placeholder", "tensorflow.cast", "isinstance", "tensorflow.one_hot", "tensorflow.compat.v1.placeholder", "tensorflow.cast", "isinstance", "numpy.any", "tensorflow.compat.v1.placeholder", "tensorflow.cast", "isinstance", "numpy.any", "numpy.any", "tensorflow.compat.v1.placeholder", "tensorflow.concat", "NotImplementedError", "numpy.isinf", "numpy.isinf", "tensorflow.cast", "len", "tensorflow.one_hot", "enumerate", "type", "tensorflow.split", "len"], "function", ["None"], ["def", "observation_input", "(", "ob_space", ",", "batch_size", "=", "None", ",", "name", "=", "'Ob'", ",", "scale", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Build observation input with encoding depending on the observation space type\n\n    When using Box ob_space, the input will be normalized between [1, 0] on the bounds ob_space.low and ob_space.high.\n\n    :param ob_space: (Gym Space) The observation space\n    :param batch_size: (int) batch size for input\n                       (default is None, so that resulting input placeholder can take tensors with any batch size)\n    :param name: (str) tensorflow variable name for input placeholder\n    :param scale: (bool) whether or not to scale the input\n    :return: (TensorFlow Tensor, TensorFlow Tensor) input_placeholder, processed_input_tensor\n    \"\"\"", "\n", "if", "isinstance", "(", "ob_space", ",", "Discrete", ")", ":", "\n", "        ", "observation_ph", "=", "tf", ".", "placeholder", "(", "shape", "=", "(", "batch_size", ",", ")", ",", "dtype", "=", "tf", ".", "int32", ",", "name", "=", "name", ")", "\n", "processed_observations", "=", "tf", ".", "cast", "(", "tf", ".", "one_hot", "(", "observation_ph", ",", "ob_space", ".", "n", ")", ",", "tf", ".", "float32", ")", "\n", "return", "observation_ph", ",", "processed_observations", "\n", "\n", "", "elif", "isinstance", "(", "ob_space", ",", "Box", ")", ":", "\n", "        ", "observation_ph", "=", "tf", ".", "compat", ".", "v1", ".", "placeholder", "(", "shape", "=", "(", "batch_size", ",", ")", "+", "ob_space", ".", "shape", ",", "dtype", "=", "ob_space", ".", "dtype", ",", "name", "=", "name", ")", "\n", "processed_observations", "=", "tf", ".", "cast", "(", "observation_ph", ",", "tf", ".", "float32", ")", "\n", "# rescale to [1, 0] if the bounds are defined", "\n", "if", "(", "scale", "and", "\n", "not", "np", ".", "any", "(", "np", ".", "isinf", "(", "ob_space", ".", "low", ")", ")", "and", "not", "np", ".", "any", "(", "np", ".", "isinf", "(", "ob_space", ".", "high", ")", ")", "and", "\n", "np", ".", "any", "(", "(", "ob_space", ".", "high", "-", "ob_space", ".", "low", ")", "!=", "0", ")", ")", ":", "\n", "# equivalent to processed_observations / 255.0 when bounds are set to [255, 0]", "\n", "            ", "processed_observations", "=", "(", "(", "processed_observations", "-", "ob_space", ".", "low", ")", "/", "(", "ob_space", ".", "high", "-", "ob_space", ".", "low", ")", ")", "\n", "", "return", "observation_ph", ",", "processed_observations", "\n", "\n", "", "elif", "isinstance", "(", "ob_space", ",", "MultiBinary", ")", ":", "\n", "        ", "observation_ph", "=", "tf", ".", "compat", ".", "v1", ".", "placeholder", "(", "shape", "=", "(", "batch_size", ",", "ob_space", ".", "n", ")", ",", "dtype", "=", "tf", ".", "int32", ",", "name", "=", "name", ")", "\n", "processed_observations", "=", "tf", ".", "cast", "(", "observation_ph", ",", "tf", ".", "float32", ")", "\n", "return", "observation_ph", ",", "processed_observations", "\n", "\n", "", "elif", "isinstance", "(", "ob_space", ",", "MultiDiscrete", ")", ":", "\n", "        ", "observation_ph", "=", "tf", ".", "compat", ".", "v1", ".", "placeholder", "(", "shape", "=", "(", "batch_size", ",", "len", "(", "ob_space", ".", "nvec", ")", ")", ",", "dtype", "=", "tf", ".", "int32", ",", "name", "=", "name", ")", "\n", "processed_observations", "=", "tf", ".", "concat", "(", "[", "\n", "tf", ".", "cast", "(", "tf", ".", "one_hot", "(", "input_split", ",", "ob_space", ".", "nvec", "[", "i", "]", ")", ",", "tf", ".", "float32", ")", "for", "i", ",", "input_split", "\n", "in", "enumerate", "(", "tf", ".", "split", "(", "observation_ph", ",", "len", "(", "ob_space", ".", "nvec", ")", ",", "axis", "=", "-", "1", ")", ")", "\n", "]", ",", "axis", "=", "-", "1", ")", "\n", "return", "observation_ph", ",", "processed_observations", "\n", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\"Error: the model does not support input space of type {}\"", ".", "format", "(", "\n", "type", "(", "ob_space", ")", ".", "__name__", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.policies.BasePolicy.__init__": [[132, 151], ["tensorflow.compat.v1.variable_scope", "core.input.observation_input", "tensorflow.placeholder"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.input.observation_input"], ["def", "__init__", "(", "self", ",", "sess", ",", "ob_space", ",", "ac_space", ",", "n_env", ",", "n_steps", ",", "n_batch", ",", "reuse", "=", "False", ",", "scale", "=", "False", ",", "\n", "obs_phs", "=", "None", ",", "add_action_ph", "=", "False", ")", ":", "\n", "        ", "self", ".", "n_env", "=", "n_env", "\n", "self", ".", "n_steps", "=", "n_steps", "\n", "self", ".", "n_batch", "=", "n_batch", "\n", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "\"input\"", ",", "reuse", "=", "False", ")", ":", "\n", "            ", "if", "obs_phs", "is", "None", ":", "\n", "                ", "self", ".", "_obs_ph", ",", "self", ".", "_processed_obs", "=", "observation_input", "(", "ob_space", ",", "n_batch", ",", "scale", "=", "scale", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "_obs_ph", ",", "self", ".", "_processed_obs", "=", "obs_phs", "\n", "\n", "", "self", ".", "_action_ph", "=", "None", "\n", "if", "add_action_ph", ":", "\n", "                ", "self", ".", "_action_ph", "=", "tf", ".", "placeholder", "(", "dtype", "=", "ac_space", ".", "dtype", ",", "shape", "=", "(", "n_batch", ",", ")", "+", "ac_space", ".", "shape", ",", "\n", "name", "=", "\"action_ph\"", ")", "\n", "", "", "self", ".", "sess", "=", "sess", "\n", "self", ".", "reuse", "=", "reuse", "\n", "self", ".", "ob_space", "=", "ob_space", "\n", "self", ".", "ac_space", "=", "ac_space", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.policies.BasePolicy.is_discrete": [[152, 156], ["isinstance"], "methods", ["None"], ["", "@", "property", "\n", "def", "is_discrete", "(", "self", ")", ":", "\n", "        ", "\"\"\"bool: is action space discrete.\"\"\"", "\n", "return", "isinstance", "(", "self", ".", "ac_space", ",", "Discrete", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.policies.BasePolicy.initial_state": [[157, 165], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "initial_state", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        The initial state of the policy. For feedforward policies, None. For a recurrent policy,\n        a NumPy array of shape (self.n_env, ) + state_shape.\n        \"\"\"", "\n", "assert", "not", "self", ".", "recurrent", ",", "\"When using recurrent policies, you must overwrite `initial_state()` method\"", "\n", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.policies.BasePolicy.obs_ph": [[166, 170], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "obs_ph", "(", "self", ")", ":", "\n", "        ", "\"\"\"tf.Tensor: placeholder for observations, shape (self.n_batch, ) + self.ob_space.shape.\"\"\"", "\n", "return", "self", ".", "_obs_ph", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.policies.BasePolicy.processed_obs": [[171, 178], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "processed_obs", "(", "self", ")", ":", "\n", "        ", "\"\"\"tf.Tensor: processed observations, shape (self.n_batch, ) + self.ob_space.shape.\n\n        The form of processing depends on the type of the observation space, and the parameters\n        whether scale is passed to the constructor; see observation_input for more information.\"\"\"", "\n", "return", "self", ".", "_processed_obs", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.policies.BasePolicy.action_ph": [[179, 183], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "action_ph", "(", "self", ")", ":", "\n", "        ", "\"\"\"tf.Tensor: placeholder for actions, shape (self.n_batch, ) + self.ac_space.shape.\"\"\"", "\n", "return", "self", ".", "_action_ph", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.policies.BasePolicy._kwargs_check": [[184, 201], ["ValueError", "len"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_kwargs_check", "(", "feature_extraction", ",", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Ensure that the user is not passing wrong keywords\n        when using policy_kwargs.\n\n        :param feature_extraction: (str)\n        :param kwargs: (dict)\n        \"\"\"", "\n", "# When using policy_kwargs parameter on model creation,", "\n", "# all keywords arguments must be consumed by the policy constructor except", "\n", "# the ones for the cnn_extractor network (cf nature_cnn()), where the keywords arguments", "\n", "# are not passed explicitly (using **kwargs to forward the arguments)", "\n", "# that's why there should be not kwargs left when using the mlp_extractor", "\n", "# (in that case the keywords arguments are passed explicitly)", "\n", "if", "feature_extraction", "==", "'mlp'", "and", "len", "(", "kwargs", ")", ">", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unknown keywords for policy: {}\"", ".", "format", "(", "kwargs", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.policies.BasePolicy.step": [[202, 213], ["None"], "methods", ["None"], ["", "", "@", "abstractmethod", "\n", "def", "step", "(", "self", ",", "obs", ",", "state", "=", "None", ",", "mask", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Returns the policy for a single step\n\n        :param obs: ([float] or [int]) The current observation of the environment\n        :param state: ([float]) The last states (used in recurrent policies)\n        :param mask: ([float]) The last masks (used in recurrent policies)\n        :return: ([float], [float], [float], [float]) actions, values, states, neglogp\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.policies.BasePolicy.proba_step": [[214, 225], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "proba_step", "(", "self", ",", "obs", ",", "state", "=", "None", ",", "mask", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Returns the action probability for a single step\n\n        :param obs: ([float] or [int]) The current observation of the environment\n        :param state: ([float]) The last states (used in recurrent policies)\n        :param mask: ([float]) The last masks (used in recurrent policies)\n        :return: ([float]) the action probability\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.policies.ActorCriticPolicy.__init__": [[241, 251], ["policies.BasePolicy.__init__", "core.distributions.make_proba_dist_type"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__init__", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.make_proba_dist_type"], ["def", "__init__", "(", "self", ",", "sess", ",", "ob_space", ",", "ac_space", ",", "n_env", ",", "n_steps", ",", "n_batch", ",", "reuse", "=", "False", ",", "scale", "=", "False", ")", ":", "\n", "        ", "super", "(", "ActorCriticPolicy", ",", "self", ")", ".", "__init__", "(", "sess", ",", "ob_space", ",", "ac_space", ",", "n_env", ",", "n_steps", ",", "n_batch", ",", "reuse", "=", "reuse", ",", "\n", "scale", "=", "scale", ")", "\n", "self", ".", "_pdtype", "=", "make_proba_dist_type", "(", "ac_space", ")", "\n", "self", ".", "_policy", "=", "None", "\n", "self", ".", "_proba_distribution", "=", "None", "\n", "self", ".", "_value_fn", "=", "None", "\n", "self", ".", "pi_adv_logits", "=", "None", "\n", "self", ".", "_action", "=", "None", "\n", "self", ".", "_deterministic_action", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.policies.ActorCriticPolicy._setup_init": [[252, 271], ["tensorflow.compat.v1.variable_scope", "policies.ActorCriticPolicy.proba_distribution.sample", "policies.ActorCriticPolicy.proba_distribution.mode", "policies.ActorCriticPolicy.proba_distribution.neglogp", "isinstance", "tensorflow.nn.softmax", "isinstance", "isinstance", "tensorflow.nn.sigmoid", "isinstance", "tensorflow.nn.softmax", "categorical.flatparam"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.replay_buffer.ReplayBuffer.sample", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.BernoulliProbabilityDistribution.mode", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.BernoulliProbabilityDistribution.neglogp", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.BernoulliProbabilityDistribution.flatparam"], ["", "def", "_setup_init", "(", "self", ")", ":", "\n", "        ", "\"\"\"Sets up the distributions, actions, and value.\"\"\"", "\n", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "\"output\"", ",", "reuse", "=", "True", ")", ":", "\n", "            ", "assert", "self", ".", "policy", "is", "not", "None", "and", "self", ".", "proba_distribution", "is", "not", "None", "and", "self", ".", "value_fn", "is", "not", "None", "\n", "self", ".", "_action", "=", "self", ".", "proba_distribution", ".", "sample", "(", ")", "\n", "self", ".", "_deterministic_action", "=", "self", ".", "proba_distribution", ".", "mode", "(", ")", "\n", "self", ".", "_neglogp", "=", "self", ".", "proba_distribution", ".", "neglogp", "(", "self", ".", "action", ")", "\n", "if", "isinstance", "(", "self", ".", "proba_distribution", ",", "CategoricalProbabilityDistribution", ")", ":", "\n", "                ", "self", ".", "_policy_proba", "=", "tf", ".", "nn", ".", "softmax", "(", "self", ".", "policy", ")", "\n", "", "elif", "isinstance", "(", "self", ".", "proba_distribution", ",", "DiagGaussianProbabilityDistribution", ")", ":", "\n", "                ", "self", ".", "_policy_proba", "=", "[", "self", ".", "proba_distribution", ".", "mean", ",", "self", ".", "proba_distribution", ".", "std", "]", "\n", "", "elif", "isinstance", "(", "self", ".", "proba_distribution", ",", "BernoulliProbabilityDistribution", ")", ":", "\n", "                ", "self", ".", "_policy_proba", "=", "tf", ".", "nn", ".", "sigmoid", "(", "self", ".", "policy", ")", "\n", "", "elif", "isinstance", "(", "self", ".", "proba_distribution", ",", "MultiCategoricalProbabilityDistribution", ")", ":", "\n", "                ", "self", ".", "_policy_proba", "=", "[", "tf", ".", "nn", ".", "softmax", "(", "categorical", ".", "flatparam", "(", ")", ")", "\n", "for", "categorical", "in", "self", ".", "proba_distribution", ".", "categoricals", "]", "\n", "", "else", ":", "\n", "                ", "self", ".", "_policy_proba", "=", "[", "]", "# it will return nothing, as it is not implemented", "\n", "", "self", ".", "_value_flat", "=", "self", ".", "value_fn", "[", ":", ",", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.policies.ActorCriticPolicy.pdtype": [[272, 276], ["None"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "pdtype", "(", "self", ")", ":", "\n", "        ", "\"\"\"ProbabilityDistributionType: type of the distribution for stochastic actions.\"\"\"", "\n", "return", "self", ".", "_pdtype", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.policies.ActorCriticPolicy.policy": [[277, 281], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "policy", "(", "self", ")", ":", "\n", "        ", "\"\"\"tf.Tensor: policy output, e.g. logits.\"\"\"", "\n", "return", "self", ".", "_policy", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.policies.ActorCriticPolicy.proba_distribution": [[282, 286], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "proba_distribution", "(", "self", ")", ":", "\n", "        ", "\"\"\"ProbabilityDistribution: distribution of stochastic actions.\"\"\"", "\n", "return", "self", ".", "_proba_distribution", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.policies.ActorCriticPolicy.value_fn": [[287, 291], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "value_fn", "(", "self", ")", ":", "\n", "        ", "\"\"\"tf.Tensor: value estimate, of shape (self.n_batch, 1)\"\"\"", "\n", "return", "self", ".", "_value_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.policies.ActorCriticPolicy.value_flat": [[292, 296], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "value_flat", "(", "self", ")", ":", "\n", "        ", "\"\"\"tf.Tensor: value estimate, of shape (self.n_batch, )\"\"\"", "\n", "return", "self", ".", "_value_flat", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.policies.ActorCriticPolicy.action": [[297, 301], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "action", "(", "self", ")", ":", "\n", "        ", "\"\"\"tf.Tensor: stochastic action, of shape (self.n_batch, ) + self.ac_space.shape.\"\"\"", "\n", "return", "self", ".", "_action", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.policies.ActorCriticPolicy.deterministic_action": [[302, 306], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "deterministic_action", "(", "self", ")", ":", "\n", "        ", "\"\"\"tf.Tensor: deterministic action, of shape (self.n_batch, ) + self.ac_space.shape.\"\"\"", "\n", "return", "self", ".", "_deterministic_action", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.policies.ActorCriticPolicy.neglogp": [[307, 311], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "neglogp", "(", "self", ")", ":", "\n", "        ", "\"\"\"tf.Tensor: negative log likelihood of the action sampled by self.action.\"\"\"", "\n", "return", "self", ".", "_neglogp", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.policies.ActorCriticPolicy.policy_proba": [[312, 316], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "policy_proba", "(", "self", ")", ":", "\n", "        ", "\"\"\"tf.Tensor: parameters of the probability distribution. Depends on pdtype.\"\"\"", "\n", "return", "self", ".", "_policy_proba", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.policies.ActorCriticPolicy.step": [[317, 329], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "step", "(", "self", ",", "obs", ",", "state", "=", "None", ",", "mask", "=", "None", ",", "deterministic", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Returns the policy for a single step\n\n        :param obs: ([float] or [int]) The current observation of the environment\n        :param state: ([float]) The last states (used in recurrent policies)\n        :param mask: ([float]) The last masks (used in recurrent policies)\n        :param deterministic: (bool) Whether or not to return deterministic actions.\n        :return: ([float], [float], [float], [float]) actions, values, states, neglogp\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.policies.ActorCriticPolicy.value": [[330, 341], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "value", "(", "self", ",", "obs", ",", "state", "=", "None", ",", "mask", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Returns the value for a single step\n\n        :param obs: ([float] or [int]) The current observation of the environment\n        :param state: ([float]) The last states (used in recurrent policies)\n        :param mask: ([float]) The last masks (used in recurrent policies)\n        :return: ([float]) The associated value of the action\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.policies.FeedForwardPolicy.__init__": [[364, 407], ["policies.ActorCriticPolicy.__init__", "policies.FeedForwardPolicy._kwargs_check", "policies.FeedForwardPolicy._setup_init", "warnings.warn", "tensorflow.compat.v1.variable_scope", "core.tf_layers.linear", "policies.FeedForwardPolicy.pdtype.proba_distribution_from_latent", "warnings.warn", "dict", "policies.cnn_extractor", "tensorflow.nn.elu", "core.tf_layers.linear", "policies.FeedForwardPolicy.pdtype.probability_distribution_class", "core.tf_layers.linear", "policies.FeedForwardPolicy.pdtype.probability_distribution_class", "numpy.sqrt", "policies.FeedForwardPolicy.pdtype.probability_distribution_class"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__init__", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.policies.BasePolicy._kwargs_check", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.policies.ActorCriticPolicy._setup_init", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.warn", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_layers.linear", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.BernoulliProbabilityDistributionType.proba_distribution_from_latent", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.warn", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.policies.cnn_extractor", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_layers.linear", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.BernoulliProbabilityDistributionType.probability_distribution_class", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_layers.linear", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.BernoulliProbabilityDistributionType.probability_distribution_class", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.BernoulliProbabilityDistributionType.probability_distribution_class"], ["def", "__init__", "(", "self", ",", "sess", ",", "ob_space", ",", "ac_space", ",", "n_env", ",", "n_steps", ",", "n_batch", ",", "reuse", "=", "False", ",", "layers", "=", "None", ",", "net_arch", "=", "None", ",", "\n", "act_fun", "=", "tf", ".", "tanh", ",", "feature_extraction", "=", "\"cnn\"", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "FeedForwardPolicy", ",", "self", ")", ".", "__init__", "(", "sess", ",", "ob_space", ",", "ac_space", ",", "n_env", ",", "n_steps", ",", "n_batch", ",", "reuse", "=", "reuse", ",", "\n", "scale", "=", "(", "feature_extraction", "==", "\"cnn\"", ")", ")", "\n", "\n", "self", ".", "_kwargs_check", "(", "feature_extraction", ",", "kwargs", ")", "\n", "\n", "if", "layers", "is", "not", "None", ":", "\n", "            ", "warnings", ".", "warn", "(", "\"Usage of the `layers` parameter is deprecated! Use net_arch instead \"", "\n", "\"(it has a different semantics though).\"", ",", "DeprecationWarning", ")", "\n", "if", "net_arch", "is", "not", "None", ":", "\n", "                ", "warnings", ".", "warn", "(", "\"The new `net_arch` parameter overrides the deprecated `layers` parameter!\"", ",", "\n", "DeprecationWarning", ")", "\n", "\n", "", "", "if", "net_arch", "is", "None", ":", "\n", "            ", "if", "layers", "is", "None", ":", "\n", "                ", "layers", "=", "[", "64", ",", "64", "]", "\n", "", "net_arch", "=", "[", "dict", "(", "vf", "=", "layers", ",", "pi", "=", "layers", ")", "]", "\n", "\n", "", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "\"model\"", ",", "reuse", "=", "reuse", ")", ":", "\n", "            ", "if", "feature_extraction", "==", "\"cnn\"", "and", "self", ".", "pdtype", ".", "probability_distribution_class", "(", ")", "==", "CategoricalProbabilityDistribution", ":", "\n", "                ", "pi_latent", ",", "vf_latent", ",", "pi_adv_latent", "=", "cnn_extractor", "(", "self", ".", "processed_obs", ",", "**", "kwargs", ")", "\n", "\n", "pi_adv_latent_2", "=", "tf", ".", "nn", ".", "elu", "(", "\n", "linear", "(", "pi_adv_latent", ",", "'pi_adv_latent'", ",", "n_hidden", "=", "128", ",", "init_scale", "=", "np", ".", "sqrt", "(", "2", ")", ")", ")", "\n", "\n", "self", ".", "pi_adv_logits", "=", "linear", "(", "pi_adv_latent_2", ",", "'pi_adv_latent_2'", ",", "ac_space", ".", "n", ",", "\n", "init_scale", "=", "0.01", ",", "init_bias", "=", "0.0", ")", "\n", "\n", "", "elif", "feature_extraction", "==", "\"mlp\"", "and", "self", ".", "pdtype", ".", "probability_distribution_class", "(", ")", "==", "BernoulliProbabilityDistribution", ":", "\n", "# TODO make a mlp version", "\n", "                ", "raise", "NotImplementedError", "\n", "\n", "", "elif", "feature_extraction", "==", "\"mlp\"", "and", "self", ".", "pdtype", ".", "probability_distribution_class", "(", ")", "==", "DiagGaussianProbabilityDistribution", ":", "\n", "# TODO make a mlp version", "\n", "                ", "raise", "NotImplementedError", "\n", "\n", "", "self", ".", "_value_fn", "=", "linear", "(", "vf_latent", ",", "'vf'", ",", "1", ")", "\n", "\n", "self", ".", "_proba_distribution", ",", "self", ".", "_policy", "=", "self", ".", "pdtype", ".", "proba_distribution_from_latent", "(", "pi_latent", ",", "vf_latent", ",", "init_scale", "=", "0.01", ")", "\n", "\n", "", "self", ".", "_setup_init", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.policies.FeedForwardPolicy.step": [[408, 417], ["policies.FeedForwardPolicy.sess.run", "policies.FeedForwardPolicy.sess.run"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.runners.AbstractEnvRunner.run", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.runners.AbstractEnvRunner.run"], ["", "def", "step", "(", "self", ",", "obs", ",", "state", "=", "None", ",", "mask", "=", "None", ",", "deterministic", "=", "False", ")", ":", "\n", "        ", "if", "deterministic", ":", "\n", "            ", "action", ",", "value", ",", "neglogp", "=", "self", ".", "sess", ".", "run", "(", "[", "self", ".", "deterministic_action", ",", "self", ".", "value_flat", ",", "self", ".", "neglogp", "]", ",", "\n", "{", "self", ".", "obs_ph", ":", "obs", "}", ")", "\n", "", "else", ":", "\n", "            ", "action", ",", "value", ",", "neglogp", ",", "pi_proba", ",", "pi_adv_logits", "=", "self", ".", "sess", ".", "run", "(", "\n", "[", "self", ".", "action", ",", "self", ".", "value_flat", ",", "self", ".", "neglogp", ",", "self", ".", "policy_proba", ",", "self", ".", "pi_adv_logits", "]", ",", "\n", "{", "self", ".", "obs_ph", ":", "obs", "}", ")", "\n", "", "return", "action", ",", "value", ",", "self", ".", "initial_state", ",", "neglogp", ",", "pi_proba", ",", "pi_adv_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.policies.FeedForwardPolicy.proba_step": [[418, 420], ["policies.FeedForwardPolicy.sess.run"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.runners.AbstractEnvRunner.run"], ["", "def", "proba_step", "(", "self", ",", "obs", ",", "state", "=", "None", ",", "mask", "=", "None", ")", ":", "\n", "        ", "return", "self", ".", "sess", ".", "run", "(", "self", ".", "policy_proba", ",", "{", "self", ".", "obs_ph", ":", "obs", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.policies.FeedForwardPolicy.value": [[421, 423], ["policies.FeedForwardPolicy.sess.run"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.runners.AbstractEnvRunner.run"], ["", "def", "value", "(", "self", ",", "obs", ",", "state", "=", "None", ",", "mask", "=", "None", ")", ":", "\n", "        ", "return", "self", ".", "sess", ".", "run", "(", "self", ".", "value_flat", ",", "{", "self", ".", "obs_ph", ":", "obs", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.policies.CnnPolicy.__init__": [[439, 442], ["policies.FeedForwardPolicy.__init__"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__init__"], ["def", "__init__", "(", "self", ",", "sess", ",", "ob_space", ",", "ac_space", ",", "n_env", ",", "n_steps", ",", "n_batch", ",", "reuse", "=", "False", ",", "**", "_kwargs", ")", ":", "\n", "        ", "super", "(", "CnnPolicy", ",", "self", ")", ".", "__init__", "(", "sess", ",", "ob_space", ",", "ac_space", ",", "n_env", ",", "n_steps", ",", "n_batch", ",", "reuse", ",", "\n", "feature_extraction", "=", "\"cnn\"", ",", "**", "_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.policies.MlpPolicy.__init__": [[458, 461], ["policies.FeedForwardPolicy.__init__"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__init__"], ["def", "__init__", "(", "self", ",", "sess", ",", "ob_space", ",", "ac_space", ",", "n_env", ",", "n_steps", ",", "n_batch", ",", "reuse", "=", "False", ",", "**", "_kwargs", ")", ":", "\n", "        ", "super", "(", "MlpPolicy", ",", "self", ")", ".", "__init__", "(", "sess", ",", "ob_space", ",", "ac_space", ",", "n_env", ",", "n_steps", ",", "n_batch", ",", "reuse", ",", "\n", "feature_extraction", "=", "\"mlp\"", ",", "**", "_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.policies.cnn_extractor": [[15, 51], ["activ", "activ", "activ", "core.tf_layers.conv_to_fc", "activ", "activ", "activ", "core.tf_layers.conv_to_fc", "activ", "activ", "activ", "core.tf_layers.conv_to_fc", "core.tf_layers.conv", "core.tf_layers.conv", "core.tf_layers.conv", "core.tf_layers.conv", "core.tf_layers.conv", "core.tf_layers.conv", "core.tf_layers.conv", "core.tf_layers.conv", "core.tf_layers.conv", "activ", "activ", "activ", "core.tf_layers.linear", "core.tf_layers.linear", "core.tf_layers.linear", "numpy.sqrt", "numpy.sqrt", "numpy.sqrt", "numpy.sqrt", "numpy.sqrt", "numpy.sqrt", "numpy.sqrt", "numpy.sqrt", "numpy.sqrt", "numpy.sqrt", "numpy.sqrt", "numpy.sqrt"], "function", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_layers.conv_to_fc", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_layers.conv_to_fc", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_layers.conv_to_fc", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_layers.conv", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_layers.conv", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_layers.conv", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_layers.conv", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_layers.conv", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_layers.conv", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_layers.conv", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_layers.conv", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_layers.conv", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_layers.linear", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_layers.linear", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_layers.linear"], ["def", "cnn_extractor", "(", "scaled_images", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"\n    Constructs a CNN for AGAC as depicted in Fig. 11 in Appendix C of the paper.\n\n    :param scaled_images: (TensorFlow Tensor) Image input placeholder\n    :param kwargs: (dict) Extra keywords parameters for the convolutional layers of the CNN\n    :return: (TensorFlow Tensor) The CNN output layer\n    \"\"\"", "\n", "activ", "=", "tf", ".", "nn", ".", "elu", "\n", "layer_1_pi", "=", "activ", "(", "\n", "conv", "(", "scaled_images", ",", "'c1_pi'", ",", "n_filters", "=", "32", ",", "filter_size", "=", "3", ",", "stride", "=", "2", ",", "init_scale", "=", "np", ".", "sqrt", "(", "2", ")", ",", "**", "kwargs", ")", ")", "\n", "layer_2_pi", "=", "activ", "(", "\n", "conv", "(", "layer_1_pi", ",", "'c2_pi'", ",", "n_filters", "=", "32", ",", "filter_size", "=", "3", ",", "stride", "=", "2", ",", "init_scale", "=", "np", ".", "sqrt", "(", "2", ")", ",", "**", "kwargs", ")", ")", "\n", "layer_3_pi", "=", "activ", "(", "\n", "conv", "(", "layer_2_pi", ",", "'c3_pi'", ",", "n_filters", "=", "32", ",", "filter_size", "=", "3", ",", "stride", "=", "2", ",", "init_scale", "=", "np", ".", "sqrt", "(", "2", ")", ",", "**", "kwargs", ")", ")", "\n", "layer_3_pi", "=", "conv_to_fc", "(", "layer_3_pi", ")", "\n", "\n", "layer_1_vf", "=", "activ", "(", "\n", "conv", "(", "scaled_images", ",", "'c1_vf'", ",", "n_filters", "=", "32", ",", "filter_size", "=", "3", ",", "stride", "=", "2", ",", "init_scale", "=", "np", ".", "sqrt", "(", "2", ")", ",", "**", "kwargs", ")", ")", "\n", "layer_2_vf", "=", "activ", "(", "\n", "conv", "(", "layer_1_vf", ",", "'c2_vf'", ",", "n_filters", "=", "32", ",", "filter_size", "=", "3", ",", "stride", "=", "2", ",", "init_scale", "=", "np", ".", "sqrt", "(", "2", ")", ",", "**", "kwargs", ")", ")", "\n", "layer_3_vf", "=", "activ", "(", "\n", "conv", "(", "layer_2_vf", ",", "'c3_vf'", ",", "n_filters", "=", "32", ",", "filter_size", "=", "3", ",", "stride", "=", "2", ",", "init_scale", "=", "np", ".", "sqrt", "(", "2", ")", ",", "**", "kwargs", ")", ")", "\n", "layer_3_vf", "=", "conv_to_fc", "(", "layer_3_vf", ")", "\n", "\n", "layer_1_adv", "=", "activ", "(", "\n", "conv", "(", "scaled_images", ",", "'c1_adv'", ",", "n_filters", "=", "32", ",", "filter_size", "=", "3", ",", "stride", "=", "2", ",", "init_scale", "=", "np", ".", "sqrt", "(", "2", ")", ",", "**", "kwargs", ")", ")", "\n", "layer_2_adv", "=", "activ", "(", "\n", "conv", "(", "layer_1_adv", ",", "'c2_adv'", ",", "n_filters", "=", "32", ",", "filter_size", "=", "3", ",", "stride", "=", "2", ",", "init_scale", "=", "np", ".", "sqrt", "(", "2", ")", ",", "**", "kwargs", ")", ")", "\n", "layer_3_adv", "=", "activ", "(", "\n", "conv", "(", "layer_2_adv", ",", "'c3_adv'", ",", "n_filters", "=", "32", ",", "filter_size", "=", "3", ",", "stride", "=", "2", ",", "init_scale", "=", "np", ".", "sqrt", "(", "2", ")", ",", "**", "kwargs", ")", ")", "\n", "layer_3_adv", "=", "conv_to_fc", "(", "layer_3_adv", ")", "\n", "\n", "return", "activ", "(", "linear", "(", "layer_3_pi", ",", "'fc1_pi'", ",", "n_hidden", "=", "256", ",", "init_scale", "=", "np", ".", "sqrt", "(", "2", ")", ")", ")", ",", "activ", "(", "linear", "(", "layer_3_vf", ",", "'fc1_vf'", ",", "n_hidden", "=", "256", ",", "init_scale", "=", "np", ".", "sqrt", "(", "2", ")", ")", ")", ",", "activ", "(", "linear", "(", "layer_3_adv", ",", "'fc1_adv'", ",", "n_hidden", "=", "256", ",", "init_scale", "=", "np", ".", "sqrt", "(", "2", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.policies.mlp_extractor": [[53, 111], ["enumerate", "enumerate", "isinstance", "itertools.zip_longest", "act_fun", "isinstance", "isinstance", "act_fun", "isinstance", "act_fun", "core.tf_layers.linear", "isinstance", "isinstance", "core.tf_layers.linear", "core.tf_layers.linear", "numpy.sqrt", "numpy.sqrt", "numpy.sqrt"], "function", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_layers.linear", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_layers.linear", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_layers.linear"], ["", "def", "mlp_extractor", "(", "flat_observations", ",", "net_arch", ",", "act_fun", ")", ":", "\n", "    ", "\"\"\"\n    Constructs an MLP that receives observations as an input and outputs a latent representation for the policy and\n    a value network. The ``net_arch`` parameter allows to specify the amount and size of the hidden layers and how many\n    of them are shared between the policy network and the value network. It is assumed to be a list with the following\n    structure:\n\n    1. An arbitrary length (zero allowed) number of integers each specifying the number of units in a shared layer.\n       If the number of ints is zero, there will be no shared layers.\n    2. An optional dict, to specify the following non-shared layers for the value network and the policy network.\n       It is formatted like ``dict(vf=[<value layer sizes>], pi=[<policy layer sizes>])``.\n       If it is missing any of the keys (pi or vf), no non-shared layers (empty list) is assumed.\n\n    For example to construct a network with one shared layer of size 55 followed by two non-shared layers for the value\n    network of size 255 and a single non-shared layer of size 128 for the policy network, the following layers_spec\n    would be used: ``[55, dict(vf=[255, 255], pi=[128])]``. A simple shared network topology with two layers of size 128\n    would be specified as [128, 128].\n\n    :param flat_observations: (tf.Tensor) The observations to base policy and value function on.\n    :param net_arch: ([int or dict]) The specification of the policy and value networks.\n        See above for details on its formatting.\n    :param act_fun: (tf function) The activation function to use for the networks.\n    :return: (tf.Tensor, tf.Tensor) latent_policy, latent_value of the specified network.\n        If all layers are shared, then ``latent_policy == latent_value``\n    \"\"\"", "\n", "latent", "=", "flat_observations", "\n", "policy_only_layers", "=", "[", "]", "# Layer sizes of the network that only belongs to the policy network", "\n", "value_only_layers", "=", "[", "]", "# Layer sizes of the network that only belongs to the value network", "\n", "\n", "# Iterate through the shared layers and build the shared parts of the network", "\n", "for", "idx", ",", "layer", "in", "enumerate", "(", "net_arch", ")", ":", "\n", "        ", "if", "isinstance", "(", "layer", ",", "int", ")", ":", "# Check that this is a shared layer", "\n", "            ", "layer_size", "=", "layer", "\n", "latent", "=", "act_fun", "(", "linear", "(", "latent", ",", "\"shared_fc{}\"", ".", "format", "(", "idx", ")", ",", "layer_size", ",", "init_scale", "=", "np", ".", "sqrt", "(", "2", ")", ")", ")", "\n", "", "else", ":", "\n", "            ", "assert", "isinstance", "(", "layer", ",", "dict", ")", ",", "\"Error: the net_arch list can only contain ints and dicts\"", "\n", "if", "'pi'", "in", "layer", ":", "\n", "                ", "assert", "isinstance", "(", "layer", "[", "'pi'", "]", ",", "list", ")", ",", "\"Error: net_arch[-1]['pi'] must contain a list of integers.\"", "\n", "policy_only_layers", "=", "layer", "[", "'pi'", "]", "\n", "\n", "", "if", "'vf'", "in", "layer", ":", "\n", "                ", "assert", "isinstance", "(", "layer", "[", "'vf'", "]", ",", "list", ")", ",", "\"Error: net_arch[-1]['vf'] must contain a list of integers.\"", "\n", "value_only_layers", "=", "layer", "[", "'vf'", "]", "\n", "", "break", "# From here on the network splits up in policy and value network", "\n", "\n", "# Build the non-shared part of the network", "\n", "", "", "latent_policy", "=", "latent", "\n", "latent_value", "=", "latent", "\n", "for", "idx", ",", "(", "pi_layer_size", ",", "vf_layer_size", ")", "in", "enumerate", "(", "zip_longest", "(", "policy_only_layers", ",", "value_only_layers", ")", ")", ":", "\n", "        ", "if", "pi_layer_size", "is", "not", "None", ":", "\n", "            ", "assert", "isinstance", "(", "pi_layer_size", ",", "int", ")", ",", "\"Error: net_arch[-1]['pi'] must only contain integers.\"", "\n", "latent_policy", "=", "act_fun", "(", "linear", "(", "latent_policy", ",", "\"pi_fc{}\"", ".", "format", "(", "idx", ")", ",", "pi_layer_size", ",", "init_scale", "=", "np", ".", "sqrt", "(", "2", ")", ")", ")", "\n", "\n", "", "if", "vf_layer_size", "is", "not", "None", ":", "\n", "            ", "assert", "isinstance", "(", "vf_layer_size", ",", "int", ")", ",", "\"Error: net_arch[-1]['vf'] must only contain integers.\"", "\n", "latent_value", "=", "act_fun", "(", "linear", "(", "latent_value", ",", "\"vf_fc{}\"", ".", "format", "(", "idx", ")", ",", "vf_layer_size", ",", "init_scale", "=", "np", ".", "sqrt", "(", "2", ")", ")", ")", "\n", "\n", "", "", "return", "latent_policy", ",", "latent_value", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.policies.get_policy_from_name": [[471, 485], ["ValueError", "ValueError", "list", "_policy_registry[].keys"], "function", ["None"], ["def", "get_policy_from_name", "(", "base_policy_type", ",", "name", ")", ":", "\n", "    ", "\"\"\"\n    returns the registed policy from the base type and name\n\n    :param base_policy_type: (BasePolicy) the base policy object\n    :param name: (str) the policy name\n    :return: (base_policy_type) the policy\n    \"\"\"", "\n", "if", "base_policy_type", "not", "in", "_policy_registry", ":", "\n", "        ", "raise", "ValueError", "(", "\"Error: the policy type {} is not registered!\"", ".", "format", "(", "base_policy_type", ")", ")", "\n", "", "if", "name", "not", "in", "_policy_registry", "[", "base_policy_type", "]", ":", "\n", "        ", "raise", "ValueError", "(", "\"Error: unknown policy type {}, the only registed policy type are: {}!\"", "\n", ".", "format", "(", "name", ",", "list", "(", "_policy_registry", "[", "base_policy_type", "]", ".", "keys", "(", ")", ")", ")", ")", "\n", "", "return", "_policy_registry", "[", "base_policy_type", "]", "[", "name", "]", "\n", "", ""]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.runners.AbstractEnvRunner.__init__": [[16, 39], ["numpy.zeros", "env.reset", "range"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.reset"], ["    ", "def", "__init__", "(", "self", ",", "*", ",", "env", ":", "Union", "[", "gym", ".", "Env", ",", "VecEnv", "]", ",", "model", ":", "'BaseRLModel'", ",", "n_steps", ":", "int", ",", "episodic_count", ":", "bool", ")", ":", "\n", "        ", "\"\"\"\n        Collect experience by running `n_steps` in the environment.\n        Note: if this is a `VecEnv`, the total number of steps will\n        be `n_steps * n_envs`.\n\n        :param env: (Union[gym.Env, VecEnv]) The environment to learn from\n        :param model: (BaseRLModel) The model to learn\n        :param n_steps: (int) The number of steps to run for each environment\n        \"\"\"", "\n", "self", ".", "env", "=", "env", "\n", "self", ".", "model", "=", "model", "\n", "n_envs", "=", "env", ".", "num_envs", "\n", "self", ".", "batch_ob_shape", "=", "(", "n_envs", "*", "n_steps", ",", ")", "+", "env", ".", "observation_space", ".", "shape", "\n", "self", ".", "obs", "=", "np", ".", "zeros", "(", "(", "n_envs", ",", ")", "+", "env", ".", "observation_space", ".", "shape", ",", "dtype", "=", "env", ".", "observation_space", ".", "dtype", ".", "name", ")", "\n", "self", ".", "obs", "[", ":", "]", "=", "env", ".", "reset", "(", ")", "\n", "self", ".", "n_steps", "=", "n_steps", "\n", "self", ".", "episodic_count", "=", "episodic_count", "\n", "self", ".", "states", "=", "model", ".", "initial_state", "\n", "self", ".", "dones", "=", "[", "False", "for", "_", "in", "range", "(", "n_envs", ")", "]", "\n", "self", ".", "callback", "=", "None", "# type: Optional[BaseCallback]", "\n", "self", ".", "continue_training", "=", "True", "\n", "self", ".", "n_envs", "=", "n_envs", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.runners.AbstractEnvRunner.run": [[40, 50], ["runners.AbstractEnvRunner._run"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.runners.AbstractEnvRunner._run"], ["", "def", "run", "(", "self", ",", "callback", ":", "Optional", "[", "BaseCallback", "]", "=", "None", ")", "->", "Any", ":", "\n", "        ", "\"\"\"\n        Collect experience.\n\n        :param callback: (Optional[BaseCallback]) The callback that will be called\n            at each environment step.\n        \"\"\"", "\n", "self", ".", "callback", "=", "callback", "\n", "self", ".", "continue_training", "=", "True", "\n", "return", "self", ".", "_run", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.runners.AbstractEnvRunner._run": [[51, 57], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "_run", "(", "self", ")", "->", "Any", ":", "\n", "        ", "\"\"\"\n        This method must be overwritten by child class.\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "", "", ""]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.console_util.fmt_row": [[11, 24], ["console_util.fmt_item", "len"], "function", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.console_util.fmt_item"], ["def", "fmt_row", "(", "width", ",", "row", ",", "header", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    fits a list of items to at least a certain length\n\n    :param width: (int) the minimum width of the string\n    :param row: ([Any]) a list of object you wish to get the string representation\n    :param header: (bool) whether or not to return the string as a header\n    :return: (str) the string representation of all the elements in 'row', of length >= 'width'\n    \"\"\"", "\n", "out", "=", "\" | \"", ".", "join", "(", "fmt_item", "(", "x", ",", "width", ")", "for", "x", "in", "row", ")", "\n", "if", "header", ":", "\n", "        ", "out", "=", "out", "+", "\"\\n\"", "+", "\"-\"", "*", "len", "(", "out", ")", "\n", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.console_util.fmt_item": [[26, 46], ["isinstance", "isinstance", "item.item.item", "abs", "str", "len"], "function", ["None"], ["", "def", "fmt_item", "(", "item", ",", "min_width", ")", ":", "\n", "    ", "\"\"\"\n    fits items to a given string length\n\n    :param item: (Any) the item you wish to get the string representation\n    :param min_width: (int) the minimum width of the string\n    :return: (str) the string representation of 'x' of length >= 'l'\n    \"\"\"", "\n", "if", "isinstance", "(", "item", ",", "np", ".", "ndarray", ")", ":", "\n", "        ", "assert", "item", ".", "ndim", "==", "0", "\n", "item", "=", "item", ".", "item", "(", ")", "\n", "", "if", "isinstance", "(", "item", ",", "(", "float", ",", "np", ".", "float32", ",", "np", ".", "float64", ")", ")", ":", "\n", "        ", "value", "=", "abs", "(", "item", ")", "\n", "if", "(", "value", "<", "1e-4", "or", "value", ">", "1e+4", ")", "and", "value", ">", "0", ":", "\n", "            ", "rep", "=", "\"%7.2e\"", "%", "item", "\n", "", "else", ":", "\n", "            ", "rep", "=", "\"%7.5f\"", "%", "item", "\n", "", "", "else", ":", "\n", "        ", "rep", "=", "str", "(", "item", ")", "\n", "", "return", "\" \"", "*", "(", "min_width", "-", "len", "(", "rep", ")", ")", "+", "rep", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.console_util.colorize": [[61, 79], ["attr.append", "str", "attr.append"], "function", ["None"], ["def", "colorize", "(", "string", ",", "color", ",", "bold", "=", "False", ",", "highlight", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Colorize, bold and/or highlight a string for terminal print\n\n    :param string: (str) input string\n    :param color: (str) the color, the lookup table is the dict at console_util.color2num\n    :param bold: (bool) if the string should be bold or not\n    :param highlight: (bool) if the string should be highlighted or not\n    :return: (str) the stylized output string\n    \"\"\"", "\n", "attr", "=", "[", "]", "\n", "num", "=", "COLOR_TO_NUM", "[", "color", "]", "\n", "if", "highlight", ":", "\n", "        ", "num", "+=", "10", "\n", "", "attr", ".", "append", "(", "str", "(", "num", ")", ")", "\n", "if", "bold", ":", "\n", "        ", "attr", ".", "append", "(", "'1'", ")", "\n", "", "return", "'\\x1b[%sm%s\\x1b[0m'", "%", "(", "';'", ".", "join", "(", "attr", ")", ",", "string", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.NoopResetEnv.__init__": [[318, 331], ["gym.Wrapper.__init__", "env.unwrapped.get_action_meanings"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", ",", "noop_max", "=", "30", ")", ":", "\n", "        ", "\"\"\"\n        Sample initial states by taking random number of no-ops on reset.\n        No-op is assumed to be action 0.\n\n        :param env: (Gym Environment) the environment to wrap\n        :param noop_max: (int) the maximum value of no-ops to run\n        \"\"\"", "\n", "gym", ".", "Wrapper", ".", "__init__", "(", "self", ",", "env", ")", "\n", "self", ".", "noop_max", "=", "noop_max", "\n", "self", ".", "override_num_noops", "=", "None", "\n", "self", ".", "noop_action", "=", "0", "\n", "assert", "env", ".", "unwrapped", ".", "get_action_meanings", "(", ")", "[", "0", "]", "==", "'NOOP'", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.NoopResetEnv.reset": [[332, 345], ["cmd_util.NoopResetEnv.env.reset", "range", "cmd_util.NoopResetEnv.unwrapped.np_random.randint", "cmd_util.NoopResetEnv.env.step", "cmd_util.NoopResetEnv.env.reset"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.reset", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnv.step", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.reset"], ["", "def", "reset", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "env", ".", "reset", "(", "**", "kwargs", ")", "\n", "if", "self", ".", "override_num_noops", "is", "not", "None", ":", "\n", "            ", "noops", "=", "self", ".", "override_num_noops", "\n", "", "else", ":", "\n", "            ", "noops", "=", "self", ".", "unwrapped", ".", "np_random", ".", "randint", "(", "1", ",", "self", ".", "noop_max", "+", "1", ")", "\n", "", "assert", "noops", ">", "0", "\n", "obs", "=", "None", "\n", "for", "_", "in", "range", "(", "noops", ")", ":", "\n", "            ", "obs", ",", "_", ",", "done", ",", "_", "=", "self", ".", "env", ".", "step", "(", "self", ".", "noop_action", ")", "\n", "if", "done", ":", "\n", "                ", "obs", "=", "self", ".", "env", ".", "reset", "(", "**", "kwargs", ")", "\n", "", "", "return", "obs", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.NoopResetEnv.step": [[346, 348], ["cmd_util.NoopResetEnv.env.step"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnv.step"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "return", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.FireResetEnv.__init__": [[351, 360], ["gym.Wrapper.__init__", "len", "env.unwrapped.get_action_meanings", "env.unwrapped.get_action_meanings"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", ")", ":", "\n", "        ", "\"\"\"\n        Take action on reset for environments that are fixed until firing.\n\n        :param env: (Gym Environment) the environment to wrap\n        \"\"\"", "\n", "gym", ".", "Wrapper", ".", "__init__", "(", "self", ",", "env", ")", "\n", "assert", "env", ".", "unwrapped", ".", "get_action_meanings", "(", ")", "[", "1", "]", "==", "'FIRE'", "\n", "assert", "len", "(", "env", ".", "unwrapped", ".", "get_action_meanings", "(", ")", ")", ">=", "3", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.FireResetEnv.reset": [[361, 370], ["cmd_util.FireResetEnv.env.reset", "cmd_util.FireResetEnv.env.step", "cmd_util.FireResetEnv.env.step", "cmd_util.FireResetEnv.env.reset", "cmd_util.FireResetEnv.env.reset"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.reset", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnv.step", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnv.step", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.reset", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.reset"], ["", "def", "reset", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "env", ".", "reset", "(", "**", "kwargs", ")", "\n", "obs", ",", "_", ",", "done", ",", "_", "=", "self", ".", "env", ".", "step", "(", "1", ")", "\n", "if", "done", ":", "\n", "            ", "self", ".", "env", ".", "reset", "(", "**", "kwargs", ")", "\n", "", "obs", ",", "_", ",", "done", ",", "_", "=", "self", ".", "env", ".", "step", "(", "2", ")", "\n", "if", "done", ":", "\n", "            ", "self", ".", "env", ".", "reset", "(", "**", "kwargs", ")", "\n", "", "return", "obs", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.FireResetEnv.step": [[371, 373], ["cmd_util.FireResetEnv.env.step"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnv.step"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "return", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.EpisodicLifeEnv.__init__": [[376, 386], ["gym.Wrapper.__init__"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", ")", ":", "\n", "        ", "\"\"\"\n        Make end-of-life == end-of-episode, but only reset on true game over.\n        Done by DeepMind for the DQN and co. since it helps value estimation.\n\n        :param env: (Gym Environment) the environment to wrap\n        \"\"\"", "\n", "gym", ".", "Wrapper", ".", "__init__", "(", "self", ",", "env", ")", "\n", "self", ".", "lives", "=", "0", "\n", "self", ".", "was_real_done", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.EpisodicLifeEnv.step": [[387, 400], ["cmd_util.EpisodicLifeEnv.env.step", "cmd_util.EpisodicLifeEnv.env.unwrapped.ale.lives"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnv.step"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "obs", ",", "reward", ",", "done", ",", "info", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "self", ".", "was_real_done", "=", "done", "\n", "# check current lives, make loss of life terminal,", "\n", "# then update lives to handle bonus lives", "\n", "lives", "=", "self", ".", "env", ".", "unwrapped", ".", "ale", ".", "lives", "(", ")", "\n", "if", "0", "<", "lives", "<", "self", ".", "lives", ":", "\n", "# for Qbert sometimes we stay in lives == 0 condtion for a few frames", "\n", "# so its important to keep lives > 0, so that we only reset once", "\n", "# the environment advertises done.", "\n", "            ", "done", "=", "True", "\n", "", "self", ".", "lives", "=", "lives", "\n", "return", "obs", ",", "reward", ",", "done", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.EpisodicLifeEnv.reset": [[401, 417], ["cmd_util.EpisodicLifeEnv.env.unwrapped.ale.lives", "cmd_util.EpisodicLifeEnv.env.reset", "cmd_util.EpisodicLifeEnv.env.step"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.reset", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnv.step"], ["", "def", "reset", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Calls the Gym environment reset, only when lives are exhausted.\n        This way all states are still reachable even though lives are episodic,\n        and the learner need not know about any of this behind-the-scenes.\n\n        :param kwargs: Extra keywords passed to env.reset() call\n        :return: ([int] or [float]) the first observation of the environment\n        \"\"\"", "\n", "if", "self", ".", "was_real_done", ":", "\n", "            ", "obs", "=", "self", ".", "env", ".", "reset", "(", "**", "kwargs", ")", "\n", "", "else", ":", "\n", "# no-op step to advance from terminal/lost life state", "\n", "            ", "obs", ",", "_", ",", "_", ",", "_", "=", "self", ".", "env", ".", "step", "(", "0", ")", "\n", "", "self", ".", "lives", "=", "self", ".", "env", ".", "unwrapped", ".", "ale", ".", "lives", "(", ")", "\n", "return", "obs", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.MaxAndSkipEnv.__init__": [[420, 431], ["gym.Wrapper.__init__", "numpy.zeros"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", ",", "skip", "=", "4", ")", ":", "\n", "        ", "\"\"\"\n        Return only every `skip`-th frame (frameskipping)\n\n        :param env: (Gym Environment) the environment\n        :param skip: (int) number of `skip`-th frame\n        \"\"\"", "\n", "gym", ".", "Wrapper", ".", "__init__", "(", "self", ",", "env", ")", "\n", "# most recent raw observations (for max pooling across time steps)", "\n", "self", ".", "_obs_buffer", "=", "np", ".", "zeros", "(", "(", "2", ",", ")", "+", "env", ".", "observation_space", ".", "shape", ",", "dtype", "=", "env", ".", "observation_space", ".", "dtype", ")", "\n", "self", ".", "_skip", "=", "skip", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.MaxAndSkipEnv.step": [[432, 456], ["range", "cmd_util.MaxAndSkipEnv._obs_buffer.max", "cmd_util.MaxAndSkipEnv.env.step"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnv.step"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "\"\"\"\n        Step the environment with the given action\n        Repeat action, sum reward, and max over last observations.\n\n        :param action: ([int] or [float]) the action\n        :return: ([int] or [float], [float], [bool], dict) observation, reward, done, information\n        \"\"\"", "\n", "total_reward", "=", "0.0", "\n", "done", "=", "None", "\n", "for", "i", "in", "range", "(", "self", ".", "_skip", ")", ":", "\n", "            ", "obs", ",", "reward", ",", "done", ",", "info", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "if", "i", "==", "self", ".", "_skip", "-", "2", ":", "\n", "                ", "self", ".", "_obs_buffer", "[", "0", "]", "=", "obs", "\n", "", "if", "i", "==", "self", ".", "_skip", "-", "1", ":", "\n", "                ", "self", ".", "_obs_buffer", "[", "1", "]", "=", "obs", "\n", "", "total_reward", "+=", "reward", "\n", "if", "done", ":", "\n", "                ", "break", "\n", "# Note that the observation on the done=True frame", "\n", "# doesn't matter", "\n", "", "", "max_frame", "=", "self", ".", "_obs_buffer", ".", "max", "(", "axis", "=", "0", ")", "\n", "\n", "return", "max_frame", ",", "total_reward", ",", "done", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.MaxAndSkipEnv.reset": [[457, 459], ["cmd_util.MaxAndSkipEnv.env.reset"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.reset"], ["", "def", "reset", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "self", ".", "env", ".", "reset", "(", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.ClipRewardEnv.__init__": [[462, 469], ["gym.RewardWrapper.__init__"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", ")", ":", "\n", "        ", "\"\"\"\n        clips the reward to {+1, 0, -1} by its sign.\n\n        :param env: (Gym Environment) the environment\n        \"\"\"", "\n", "gym", ".", "RewardWrapper", ".", "__init__", "(", "self", ",", "env", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.ClipRewardEnv.reward": [[470, 477], ["numpy.sign"], "methods", ["None"], ["", "def", "reward", "(", "self", ",", "reward", ")", ":", "\n", "        ", "\"\"\"\n        Bin reward to {+1, 0, -1} by its sign.\n\n        :param reward: (float)\n        \"\"\"", "\n", "return", "np", ".", "sign", "(", "reward", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.WarpFrame.__init__": [[480, 491], ["gym.ObservationWrapper.__init__", "gym.spaces.Box"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", ",", "width", ",", "height", ")", ":", "\n", "        ", "\"\"\"\n        Warp frames to 84x84 as done in the Nature paper and later work.\n\n        :param env: (Gym Environment) the environment\n        \"\"\"", "\n", "gym", ".", "ObservationWrapper", ".", "__init__", "(", "self", ",", "env", ")", "\n", "self", ".", "width", "=", "width", "\n", "self", ".", "height", "=", "height", "\n", "self", ".", "observation_space", "=", "spaces", ".", "Box", "(", "low", "=", "0", ",", "high", "=", "255", ",", "shape", "=", "(", "self", ".", "height", ",", "self", ".", "width", ",", "1", ")", ",", "\n", "dtype", "=", "env", ".", "observation_space", ".", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.WarpFrame.observation": [[492, 502], ["cv2.cvtColor", "cv2.resize"], "methods", ["None"], ["", "def", "observation", "(", "self", ",", "frame", ")", ":", "\n", "        ", "\"\"\"\n        returns the current observation from a frame\n\n        :param frame: ([int] or [float]) environment frame\n        :return: ([int] or [float]) the observation\n        \"\"\"", "\n", "frame", "=", "cv2", ".", "cvtColor", "(", "frame", ",", "cv2", ".", "COLOR_RGB2GRAY", ")", "\n", "frame", "=", "cv2", ".", "resize", "(", "frame", ",", "(", "self", ".", "width", ",", "self", ".", "height", ")", ",", "interpolation", "=", "cv2", ".", "INTER_AREA", ")", "\n", "return", "frame", "[", ":", ",", ":", ",", "None", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.FrameStack.__init__": [[505, 519], ["gym.Wrapper.__init__", "collections.deque", "gym.spaces.Box"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", ",", "n_frames", ")", ":", "\n", "        ", "\"\"\"Stack n_frames last frames.\n\n        Returns lazy array, which is much more memory efficient.\n\n        :param env: (Gym Environment) the environment\n        :param n_frames: (int) the number of frames to stack\n        \"\"\"", "\n", "gym", ".", "Wrapper", ".", "__init__", "(", "self", ",", "env", ")", "\n", "self", ".", "n_frames", "=", "n_frames", "\n", "self", ".", "frames", "=", "deque", "(", "[", "]", ",", "maxlen", "=", "n_frames", ")", "\n", "shp", "=", "env", ".", "observation_space", ".", "shape", "\n", "self", ".", "observation_space", "=", "spaces", ".", "Box", "(", "low", "=", "0", ",", "high", "=", "255", ",", "shape", "=", "(", "shp", "[", "0", "]", ",", "shp", "[", "1", "]", ",", "shp", "[", "2", "]", "*", "n_frames", ")", ",", "\n", "dtype", "=", "env", ".", "observation_space", ".", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.FrameStack.reset": [[520, 525], ["cmd_util.FrameStack.env.reset", "range", "cmd_util.FrameStack._get_ob", "cmd_util.FrameStack.frames.append"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.reset", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.FrameStack._get_ob"], ["", "def", "reset", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "obs", "=", "self", ".", "env", ".", "reset", "(", "**", "kwargs", ")", "\n", "for", "_", "in", "range", "(", "self", ".", "n_frames", ")", ":", "\n", "            ", "self", ".", "frames", ".", "append", "(", "obs", ")", "\n", "", "return", "self", ".", "_get_ob", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.FrameStack.step": [[526, 530], ["cmd_util.FrameStack.env.step", "cmd_util.FrameStack.frames.append", "cmd_util.FrameStack._get_ob"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnv.step", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.FrameStack._get_ob"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "obs", ",", "reward", ",", "done", ",", "info", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "self", ".", "frames", ".", "append", "(", "obs", ")", "\n", "return", "self", ".", "_get_ob", "(", ")", ",", "reward", ",", "done", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.FrameStack._get_ob": [[531, 534], ["cmd_util.LazyFrames", "len", "list"], "methods", ["None"], ["", "def", "_get_ob", "(", "self", ")", ":", "\n", "        ", "assert", "len", "(", "self", ".", "frames", ")", "==", "self", ".", "n_frames", "\n", "return", "LazyFrames", "(", "list", "(", "self", ".", "frames", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.ScaledFloatFrame.__init__": [[537, 540], ["gym.ObservationWrapper.__init__", "gym.spaces.Box"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", ")", ":", "\n", "        ", "gym", ".", "ObservationWrapper", ".", "__init__", "(", "self", ",", "env", ")", "\n", "self", ".", "observation_space", "=", "spaces", ".", "Box", "(", "low", "=", "0", ",", "high", "=", "1.0", ",", "shape", "=", "env", ".", "observation_space", ".", "shape", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.ScaledFloatFrame.observation": [[541, 545], ["numpy.array().astype", "numpy.array"], "methods", ["None"], ["", "def", "observation", "(", "self", ",", "observation", ")", ":", "\n", "# careful! This undoes the memory optimization, use", "\n", "# with smaller replay buffers only.", "\n", "        ", "return", "np", ".", "array", "(", "observation", ")", ".", "astype", "(", "np", ".", "float32", ")", "/", "255.0", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.LazyFrames.__init__": [[548, 560], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "frames", ")", ":", "\n", "        ", "\"\"\"\n        This object ensures that core frames between the observations are only stored once.\n        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\n        buffers.\n\n        This object should only be converted to np.ndarray before being passed to the model.\n\n        :param frames: ([int] or [float]) environment frames\n        \"\"\"", "\n", "self", ".", "_frames", "=", "frames", "\n", "self", ".", "_out", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.LazyFrames._force": [[561, 566], ["numpy.concatenate"], "methods", ["None"], ["", "def", "_force", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_out", "is", "None", ":", "\n", "            ", "self", ".", "_out", "=", "np", ".", "concatenate", "(", "self", ".", "_frames", ",", "axis", "=", "2", ")", "\n", "self", ".", "_frames", "=", "None", "\n", "", "return", "self", ".", "_out", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.LazyFrames.__array__": [[567, 572], ["cmd_util.LazyFrames._force", "out.astype.astype.astype"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.LazyFrames._force"], ["", "def", "__array__", "(", "self", ",", "dtype", "=", "None", ")", ":", "\n", "        ", "out", "=", "self", ".", "_force", "(", ")", "\n", "if", "dtype", "is", "not", "None", ":", "\n", "            ", "out", "=", "out", ".", "astype", "(", "dtype", ")", "\n", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.LazyFrames.__len__": [[573, 575], ["len", "cmd_util.LazyFrames._force"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.LazyFrames._force"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "_force", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.LazyFrames.__getitem__": [[576, 578], ["cmd_util.LazyFrames._force"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.LazyFrames._force"], ["", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "return", "self", ".", "_force", "(", ")", "[", "i", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.make_vec_env": [[19, 76], ["core.vec_env.VecFrameStack", "isinstance", "core.monitor.Monitor", "core.vec_env.DummyVecEnv", "gym.make", "env_id", "wrapper_class.seed", "wrapper_class.action_space.seed", "os.path.join", "os.makedirs", "wrapper_class", "len", "warnings.warn", "str", "cmd_util.make_vec_env.make_env"], "function", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.seed", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.seed", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.warn"], ["def", "make_vec_env", "(", "env_id", ",", "n_envs", "=", "1", ",", "seed", "=", "None", ",", "start_index", "=", "0", ",", "\n", "monitor_dir", "=", "None", ",", "wrapper_class", "=", "None", ",", "\n", "env_kwargs", "=", "None", ",", "vec_env_cls", "=", "None", ",", "vec_env_kwargs", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Create a wrapped, monitored `VecEnv`.\n    By default it uses a `DummyVecEnv` which is usually faster\n    than a `SubprocVecEnv`.\n\n    :param env_id: (str or Type[gym.Env]) the environment ID or the environment class\n    :param n_envs: (int) the number of environments you wish to have in parallel\n    :param seed: (int) the initial seed for the random number generator\n    :param start_index: (int) start rank index\n    :param monitor_dir: (str) Path to a folder where the monitor files will be saved.\n        If None, no file will be written, however, the env will still be wrapped\n        in a Monitor wrapper to provide additional information about training.\n    :param wrapper_class: (gym.Wrapper or callable) Additional wrapper to use on the environment.\n        This can also be a function with single argument that wraps the environment in many things.\n    :param env_kwargs: (dict) Optional keyword argument to pass to the env constructor\n    :param vec_env_cls: (Type[VecEnv]) A custom `VecEnv` class constructor. Default: None.\n    :param vec_env_kwargs: (dict) Keyword arguments to pass to the `VecEnv` class constructor.\n    :return: (VecEnv) The wrapped environment\n    \"\"\"", "\n", "env_kwargs", "=", "{", "}", "if", "env_kwargs", "is", "None", "else", "env_kwargs", "\n", "vec_env_kwargs", "=", "{", "}", "if", "vec_env_kwargs", "is", "None", "else", "vec_env_kwargs", "\n", "\n", "def", "make_env", "(", "rank", ")", ":", "\n", "        ", "def", "_init", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "env_id", ",", "str", ")", ":", "\n", "                ", "env", "=", "gym", ".", "make", "(", "env_id", ")", "\n", "if", "len", "(", "env_kwargs", ")", ">", "0", ":", "\n", "                    ", "warnings", ".", "warn", "(", "\"No environment class was passed (only an env ID) so `env_kwargs` will be ignored\"", ")", "\n", "", "", "else", ":", "\n", "                ", "env", "=", "env_id", "(", "**", "env_kwargs", ")", "\n", "", "if", "seed", "is", "not", "None", ":", "\n", "                ", "env", ".", "seed", "(", "seed", "+", "rank", ")", "\n", "env", ".", "action_space", ".", "seed", "(", "seed", "+", "rank", ")", "\n", "# Wrap the env in a Monitor wrapper", "\n", "# to have additional training information", "\n", "", "monitor_path", "=", "os", ".", "path", ".", "join", "(", "monitor_dir", ",", "str", "(", "rank", ")", ")", "if", "monitor_dir", "is", "not", "None", "else", "None", "\n", "# Create the monitor folder if needed", "\n", "if", "monitor_path", "is", "not", "None", ":", "\n", "                ", "os", ".", "makedirs", "(", "monitor_dir", ",", "exist_ok", "=", "True", ")", "\n", "", "env", "=", "Monitor", "(", "env", ",", "filename", "=", "monitor_path", ")", "\n", "# Optionally, wrap the environment with the provided wrapper", "\n", "if", "wrapper_class", "is", "not", "None", ":", "\n", "                ", "env", "=", "wrapper_class", "(", "env", ")", "\n", "", "return", "env", "\n", "\n", "", "return", "_init", "\n", "\n", "# No custom VecEnv is passed", "\n", "", "if", "vec_env_cls", "is", "None", ":", "\n", "# Default: use a DummyVecEnv", "\n", "        ", "vec_env_cls", "=", "DummyVecEnv", "\n", "\n", "", "return", "VecFrameStack", "(", "venv", "=", "DummyVecEnv", "(", "[", "make_env", "(", "i", "+", "start_index", ")", "for", "i", "in", "range", "(", "n_envs", ")", "]", ",", "**", "vec_env_kwargs", ")", ",", "\n", "n_stack", "=", "4", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.make_bullet_env": [[78, 134], ["core.vec_env.DummyVecEnv", "isinstance", "core.monitor.Monitor", "cmd_util.make_vec_env.make_env"], "function", ["None"], ["", "def", "make_bullet_env", "(", "env_id", ",", "n_envs", "=", "1", ",", "seed", "=", "None", ",", "start_index", "=", "0", ",", "\n", "monitor_dir", "=", "None", ",", "wrapper_class", "=", "None", ",", "\n", "env_kwargs", "=", "None", ",", "vec_env_cls", "=", "None", ",", "vec_env_kwargs", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Create a wrapped, monitored `VecEnv`.\n    By default it uses a `DummyVecEnv` which is usually faster\n    than a `SubprocVecEnv`.\n\n    :param env_id: (str or Type[gym.Env]) the environment ID or the environment class\n    :param n_envs: (int) the number of environments you wish to have in parallel\n    :param seed: (int) the initial seed for the random number generator\n    :param start_index: (int) start rank index\n    :param monitor_dir: (str) Path to a folder where the monitor files will be saved.\n        If None, no file will be written, however, the env will still be wrapped\n        in a Monitor wrapper to provide additional information about training.\n    :param wrapper_class: (gym.Wrapper or callable) Additional wrapper to use on the environment.\n        This can also be a function with single argument that wraps the environment in many things.\n    :param env_kwargs: (dict) Optional keyword argument to pass to the env constructor\n    :param vec_env_cls: (Type[VecEnv]) A custom `VecEnv` class constructor. Default: None.\n    :param vec_env_kwargs: (dict) Keyword arguments to pass to the `VecEnv` class constructor.\n    :return: (VecEnv) The wrapped environment\n    \"\"\"", "\n", "env_kwargs", "=", "{", "}", "if", "env_kwargs", "is", "None", "else", "env_kwargs", "\n", "vec_env_kwargs", "=", "{", "}", "if", "vec_env_kwargs", "is", "None", "else", "vec_env_kwargs", "\n", "\n", "def", "make_env", "(", "rank", ")", ":", "\n", "        ", "def", "_init", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "env_id", ",", "str", ")", ":", "\n", "                ", "env", "=", "gym", ".", "make", "(", "env_id", ")", "\n", "if", "len", "(", "env_kwargs", ")", ">", "0", ":", "\n", "                    ", "warnings", ".", "warn", "(", "\"No environment class was passed (only an env ID) so `env_kwargs` will be ignored\"", ")", "\n", "", "", "else", ":", "\n", "                ", "env", "=", "env_id", "(", "**", "env_kwargs", ")", "\n", "", "if", "seed", "is", "not", "None", ":", "\n", "                ", "env", ".", "seed", "(", "seed", "+", "rank", ")", "\n", "env", ".", "action_space", ".", "seed", "(", "seed", "+", "rank", ")", "\n", "# Wrap the env in a Monitor wrapper", "\n", "# to have additional training information", "\n", "", "monitor_path", "=", "os", ".", "path", ".", "join", "(", "monitor_dir", ",", "str", "(", "rank", ")", ")", "if", "monitor_dir", "is", "not", "None", "else", "None", "\n", "# Create the monitor folder if needed", "\n", "if", "monitor_path", "is", "not", "None", ":", "\n", "                ", "os", ".", "makedirs", "(", "monitor_dir", ",", "exist_ok", "=", "True", ")", "\n", "", "env", "=", "Monitor", "(", "env", ",", "filename", "=", "monitor_path", ")", "\n", "# Optionally, wrap the environment with the provided wrapper", "\n", "if", "wrapper_class", "is", "not", "None", ":", "\n", "                ", "env", "=", "wrapper_class", "(", "env", ")", "\n", "", "return", "env", "\n", "\n", "", "return", "_init", "\n", "\n", "# No custom VecEnv is passed", "\n", "", "if", "vec_env_cls", "is", "None", ":", "\n", "# Default: use a DummyVecEnv", "\n", "        ", "vec_env_cls", "=", "DummyVecEnv", "\n", "\n", "", "return", "DummyVecEnv", "(", "[", "make_env", "(", "i", "+", "start_index", ")", "for", "i", "in", "range", "(", "n_envs", ")", "]", ",", "**", "vec_env_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.make_atari_env": [[136, 175], ["cmd_util.set_global_seeds", "core.vec_env.SubprocVecEnv", "core.vec_env.DummyVecEnv", "cmd_util.make_atari", "core.monitor.Monitor.seed", "core.monitor.Monitor", "cmd_util.wrap_deepmind", "cmd_util.make_vec_env.make_env"], "function", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.set_global_seeds", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.make_atari", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.seed", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.wrap_deepmind"], ["", "def", "make_atari_env", "(", "env_id", ",", "num_env", ",", "seed", ",", "wrapper_kwargs", "=", "None", ",", "\n", "start_index", "=", "0", ",", "allow_early_resets", "=", "True", ",", "\n", "start_method", "=", "None", ",", "use_subprocess", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Create a wrapped, monitored VecEnv for Atari.\n\n    :param env_id: (str) the environment ID\n    :param num_env: (int) the number of environment you wish to have in subprocesses\n    :param seed: (int) the initial seed for RNG\n    :param wrapper_kwargs: (dict) the parameters for wrap_deepmind function\n    :param start_index: (int) start rank index\n    :param allow_early_resets: (bool) allows early reset of the environment\n    :param start_method: (str) method used to start the subprocesses.\n        See SubprocVecEnv doc for more information\n    :param use_subprocess: (bool) Whether to use `SubprocVecEnv` or `DummyVecEnv` when\n        `num_env` > 1, `DummyVecEnv` is usually faster. Default: False\n    :return: (VecEnv) The atari environment\n    \"\"\"", "\n", "if", "wrapper_kwargs", "is", "None", ":", "\n", "        ", "wrapper_kwargs", "=", "{", "}", "\n", "\n", "", "def", "make_env", "(", "rank", ")", ":", "\n", "        ", "def", "_thunk", "(", ")", ":", "\n", "            ", "env", "=", "make_atari", "(", "env_id", ")", "\n", "env", ".", "seed", "(", "seed", "+", "rank", ")", "\n", "env", "=", "Monitor", "(", "env", ",", "logger", ".", "get_dir", "(", ")", "and", "os", ".", "path", ".", "join", "(", "logger", ".", "get_dir", "(", ")", ",", "str", "(", "rank", ")", ")", ",", "\n", "allow_early_resets", "=", "allow_early_resets", ")", "\n", "return", "wrap_deepmind", "(", "env", ",", "**", "wrapper_kwargs", ")", "\n", "\n", "", "return", "_thunk", "\n", "\n", "", "set_global_seeds", "(", "seed", ")", "\n", "\n", "# When using one environment, no need to start subprocesses", "\n", "if", "num_env", "==", "1", "or", "not", "use_subprocess", ":", "\n", "        ", "return", "DummyVecEnv", "(", "[", "make_env", "(", "i", "+", "start_index", ")", "for", "i", "in", "range", "(", "num_env", ")", "]", ")", "\n", "\n", "", "return", "SubprocVecEnv", "(", "[", "make_env", "(", "i", "+", "start_index", ")", "for", "i", "in", "range", "(", "num_env", ")", "]", ",", "\n", "start_method", "=", "start_method", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.make_doom_env": [[177, 226], ["cmd_util.set_global_seeds", "core.vec_env.SubprocVecEnv", "core.vec_env.DummyVecEnv", "os.path.join", "os.makedirs", "cmd_util.make_doom", "core.monitor.Monitor.seed", "core.monitor.Monitor", "cmd_util.make_vec_env.make_env"], "function", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.set_global_seeds", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.make_doom", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.seed"], ["", "def", "make_doom_env", "(", "env_id", ",", "num_env", ",", "seed", ",", "wrapper_kwargs", "=", "None", ",", "\n", "start_index", "=", "0", ",", "allow_early_resets", "=", "True", ",", "monitor_dir", "=", "None", ",", "\n", "start_method", "=", "None", ",", "use_subprocess", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Create a wrapped, monitored VecEnv for Atari.\n\n    :param env_id: (str) the environment ID\n    :param num_env: (int) the number of environment you wish to have in subprocesses\n    :param seed: (int) the initial seed for RNG\n    :param wrapper_kwargs: (dict) the parameters for wrap_deepmind function\n    :param start_index: (int) start rank index\n    :param allow_early_resets: (bool) allows early reset of the environment\n    :param start_method: (str) method used to start the subprocesses.\n        See SubprocVecEnv doc for more information\n    :param use_subprocess: (bool) Whether to use `SubprocVecEnv` or `DummyVecEnv` when\n        `num_env` > 1, `DummyVecEnv` is usually faster. Default: False\n    :return: (VecEnv) The atari environment\n    \"\"\"", "\n", "if", "wrapper_kwargs", "is", "None", ":", "\n", "        ", "wrapper_kwargs", "=", "{", "}", "\n", "\n", "", "def", "make_env", "(", "rank", ")", ":", "\n", "        ", "monitor_path", "=", "os", ".", "path", ".", "join", "(", "monitor_dir", ",", "str", "(", "rank", ")", ")", "if", "monitor_dir", "is", "not", "None", "else", "None", "\n", "# Create the monitor folder if needed", "\n", "if", "monitor_path", "is", "not", "None", ":", "\n", "            ", "os", ".", "makedirs", "(", "monitor_dir", ",", "exist_ok", "=", "True", ")", "\n", "\n", "", "def", "_thunk", "(", ")", ":", "\n", "            ", "import", "vizdoomgym", "\n", "env", "=", "make_doom", "(", "env_id", ",", "episode_life", "=", "False", ",", "\n", "clip_rewards", "=", "False", ",", "\n", "frame_stack", "=", "True", ",", "\n", "scale", "=", "False", ",", "\n", "fire", "=", "False", ")", "\n", "env", ".", "seed", "(", "seed", "+", "rank", ")", "\n", "env", "=", "Monitor", "(", "env", ",", "filename", "=", "monitor_path", ",", "\n", "allow_early_resets", "=", "allow_early_resets", ")", "\n", "return", "env", "\n", "\n", "", "return", "_thunk", "\n", "\n", "", "set_global_seeds", "(", "seed", ")", "\n", "\n", "# When using one environment, no need to start subprocesses", "\n", "if", "num_env", "==", "1", "or", "not", "use_subprocess", ":", "\n", "        ", "return", "DummyVecEnv", "(", "[", "make_env", "(", "i", "+", "start_index", ")", "for", "i", "in", "range", "(", "num_env", ")", "]", ")", "\n", "\n", "", "return", "SubprocVecEnv", "(", "[", "make_env", "(", "i", "+", "start_index", ")", "for", "i", "in", "range", "(", "num_env", ")", "]", ",", "\n", "start_method", "=", "start_method", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.make_mujoco_env": [[228, 242], ["cmd_util.set_global_seeds", "gym.make", "core.monitor.Monitor", "core.monitor.Monitor.seed", "os.path.join", "core.logger.get_dir", "cmd_util.mpi_rank_or_zero"], "function", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.set_global_seeds", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.seed", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.get_dir", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.mpi_rank_or_zero"], ["", "def", "make_mujoco_env", "(", "env_id", ",", "seed", ",", "allow_early_resets", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    Create a wrapped, monitored gym.Env for MuJoCo.\n\n    :param env_id: (str) the environment ID\n    :param seed: (int) the initial seed for RNG\n    :param allow_early_resets: (bool) allows early reset of the environment\n    :return: (Gym Environment) The mujoco environment\n    \"\"\"", "\n", "set_global_seeds", "(", "seed", "+", "10000", "*", "mpi_rank_or_zero", "(", ")", ")", "\n", "env", "=", "gym", ".", "make", "(", "env_id", ")", "\n", "env", "=", "Monitor", "(", "env", ",", "os", ".", "path", ".", "join", "(", "logger", ".", "get_dir", "(", ")", ",", "'0'", ")", ",", "allow_early_resets", "=", "allow_early_resets", ")", "\n", "env", ".", "seed", "(", "seed", ")", "\n", "return", "env", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.arg_parser": [[244, 252], ["argparse.ArgumentParser"], "function", ["None"], ["", "def", "arg_parser", "(", ")", ":", "\n", "    ", "\"\"\"\n    Create an empty argparse.ArgumentParser.\n\n    :return: (ArgumentParser)\n    \"\"\"", "\n", "import", "argparse", "\n", "return", "argparse", ".", "ArgumentParser", "(", "formatter_class", "=", "argparse", ".", "ArgumentDefaultsHelpFormatter", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.atari_arg_parser": [[254, 265], ["cmd_util.arg_parser", "arg_parser.add_argument", "arg_parser.add_argument", "arg_parser.add_argument", "int"], "function", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.arg_parser"], ["", "def", "atari_arg_parser", "(", ")", ":", "\n", "    ", "\"\"\"\n    Create an argparse.ArgumentParser for run_atari.py.\n\n    :return: (ArgumentParser) parser {'--env': 'BreakoutNoFrameskip-v4', '--seed': 0, '--num-timesteps': int(1e7)}\n    \"\"\"", "\n", "parser", "=", "arg_parser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'--env'", ",", "help", "=", "'environment ID'", ",", "default", "=", "'BreakoutNoFrameskip-v4'", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "help", "=", "'RNG seed'", ",", "type", "=", "int", ",", "default", "=", "0", ")", "\n", "parser", ".", "add_argument", "(", "'--num-timesteps'", ",", "type", "=", "int", ",", "default", "=", "int", "(", "1e7", ")", ")", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.mujoco_arg_parser": [[267, 279], ["cmd_util.arg_parser", "arg_parser.add_argument", "arg_parser.add_argument", "arg_parser.add_argument", "arg_parser.add_argument", "int"], "function", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.arg_parser"], ["", "def", "mujoco_arg_parser", "(", ")", ":", "\n", "    ", "\"\"\"\n    Create an argparse.ArgumentParser for run_mujoco.py.\n\n    :return:  (ArgumentParser) parser {'--env': 'Reacher-v2', '--seed': 0, '--num-timesteps': int(1e6), '--play': False}\n    \"\"\"", "\n", "parser", "=", "arg_parser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'--env'", ",", "help", "=", "'environment ID'", ",", "type", "=", "str", ",", "default", "=", "'Reacher-v2'", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "help", "=", "'RNG seed'", ",", "type", "=", "int", ",", "default", "=", "0", ")", "\n", "parser", ".", "add_argument", "(", "'--num-timesteps'", ",", "type", "=", "int", ",", "default", "=", "int", "(", "1e6", ")", ")", "\n", "parser", ".", "add_argument", "(", "'--play'", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ")", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.set_global_seeds": [[281, 293], ["tensorflow.compat.v1.set_random_seed", "numpy.random.seed", "random.seed", "hasattr", "gym.spaces.prng.seed"], "function", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.BaseRLModel.set_random_seed", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.seed", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.seed", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.seed"], ["", "def", "set_global_seeds", "(", "seed", ")", ":", "\n", "    ", "\"\"\"\n    set the seed for python random, tensorflow, numpy and gym spaces\n\n    :param seed: (int) the seed\n    \"\"\"", "\n", "tf", ".", "compat", ".", "v1", ".", "set_random_seed", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "random", ".", "seed", "(", "seed", ")", "\n", "# prng was removed in latest gym version", "\n", "if", "hasattr", "(", "gym", ".", "spaces", ",", "'prng'", ")", ":", "\n", "        ", "gym", ".", "spaces", ".", "prng", ".", "seed", "(", "seed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.mpi_rank_or_zero": [[295, 305], ["mpi4py.MPI.COMM_WORLD.Get_rank"], "function", ["None"], ["", "", "def", "mpi_rank_or_zero", "(", ")", ":", "\n", "    ", "\"\"\"\n    Return the MPI rank if mpi is installed. Otherwise, return 0.\n    :return: (int)\n    \"\"\"", "\n", "try", ":", "\n", "        ", "import", "mpi4py", "\n", "return", "mpi4py", ".", "MPI", ".", "COMM_WORLD", ".", "Get_rank", "(", ")", "\n", "", "except", "ImportError", ":", "\n", "        ", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.flatten_lists": [[307, 315], ["None"], "function", ["None"], ["", "", "def", "flatten_lists", "(", "listoflists", ")", ":", "\n", "    ", "\"\"\"\n    Flatten a python list of list\n\n    :param listoflists: (list(list))\n    :return: (list)\n    \"\"\"", "\n", "return", "[", "el", "for", "list_", "in", "listoflists", "for", "el", "in", "list_", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.make_atari": [[580, 592], ["gym.make", "cmd_util.NoopResetEnv", "cmd_util.MaxAndSkipEnv"], "function", ["None"], ["", "", "def", "make_atari", "(", "env_id", ")", ":", "\n", "    ", "\"\"\"\n    Create a wrapped atari Environment\n\n    :param env_id: (str) the environment ID\n    :return: (Gym Environment) the wrapped atari environment\n    \"\"\"", "\n", "env", "=", "gym", ".", "make", "(", "env_id", ")", "\n", "assert", "'NoFrameskip'", "in", "env", ".", "spec", ".", "id", "\n", "env", "=", "NoopResetEnv", "(", "env", ",", "noop_max", "=", "30", ")", "\n", "env", "=", "MaxAndSkipEnv", "(", "env", ",", "skip", "=", "4", ")", "\n", "return", "env", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.make_doom": [[594, 616], ["gym.make", "cmd_util.MaxAndSkipEnv", "cmd_util.WarpFrame", "cmd_util.EpisodicLifeEnv", "cmd_util.ScaledFloatFrame", "cmd_util.ClipRewardEnv", "cmd_util.FrameStack", "FireResetEnv.unwrapped.get_action_meanings", "cmd_util.FireResetEnv"], "function", ["None"], ["", "def", "make_doom", "(", "env_id", ",", "\n", "episode_life", "=", "False", ",", "\n", "clip_rewards", "=", "True", ",", "\n", "frame_stack", "=", "False", ",", "\n", "scale", "=", "True", ",", "\n", "fire", "=", "False", ")", ":", "# FYI scale=False in openai/baselines", "\n", "    ", "env", "=", "gym", ".", "make", "(", "env_id", ")", "\n", "env", "=", "MaxAndSkipEnv", "(", "env", ",", "skip", "=", "6", ")", "\n", "\n", "if", "episode_life", ":", "\n", "        ", "env", "=", "EpisodicLifeEnv", "(", "env", ")", "\n", "", "if", "fire", ":", "\n", "        ", "if", "'FIRE'", "in", "env", ".", "unwrapped", ".", "get_action_meanings", "(", ")", ":", "\n", "            ", "env", "=", "FireResetEnv", "(", "env", ")", "\n", "", "", "env", "=", "WarpFrame", "(", "env", ",", "width", "=", "42", ",", "height", "=", "42", ")", "\n", "if", "scale", ":", "\n", "        ", "env", "=", "ScaledFloatFrame", "(", "env", ")", "\n", "", "if", "clip_rewards", ":", "\n", "        ", "env", "=", "ClipRewardEnv", "(", "env", ")", "\n", "", "if", "frame_stack", ":", "\n", "        ", "env", "=", "FrameStack", "(", "env", ",", "4", ")", "\n", "", "return", "env", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.wrap_deepmind": [[618, 641], ["cmd_util.WarpFrame", "cmd_util.EpisodicLifeEnv", "FrameStack.unwrapped.get_action_meanings", "cmd_util.FireResetEnv", "cmd_util.ScaledFloatFrame", "cmd_util.ClipRewardEnv", "cmd_util.FrameStack"], "function", ["None"], ["", "def", "wrap_deepmind", "(", "env", ",", "episode_life", "=", "True", ",", "clip_rewards", "=", "True", ",", "frame_stack", "=", "False", ",", "scale", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Configure environment for DeepMind-style Atari.\n\n    :param env: (Gym Environment) the atari environment\n    :param episode_life: (bool) wrap the episode life wrapper\n    :param clip_rewards: (bool) wrap the reward clipping wrapper\n    :param frame_stack: (bool) wrap the frame stacking wrapper\n    :param scale: (bool) wrap the scaling observation wrapper\n    :return: (Gym Environment) the wrapped atari environment\n    \"\"\"", "\n", "if", "episode_life", ":", "\n", "        ", "env", "=", "EpisodicLifeEnv", "(", "env", ")", "\n", "", "if", "'FIRE'", "in", "env", ".", "unwrapped", ".", "get_action_meanings", "(", ")", ":", "\n", "        ", "env", "=", "FireResetEnv", "(", "env", ")", "\n", "", "env", "=", "WarpFrame", "(", "env", ")", "\n", "if", "scale", ":", "\n", "        ", "env", "=", "ScaledFloatFrame", "(", "env", ")", "\n", "", "if", "clip_rewards", ":", "\n", "        ", "env", "=", "ClipRewardEnv", "(", "env", ")", "\n", "", "if", "frame_stack", ":", "\n", "        ", "env", "=", "FrameStack", "(", "env", ",", "4", ")", "\n", "", "return", "env", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.swap_and_flatten": [[643, 652], ["arr.swapaxes().reshape", "arr.swapaxes"], "function", ["None"], ["", "def", "swap_and_flatten", "(", "arr", ")", ":", "\n", "    ", "\"\"\"\n    swap and then flatten axes 0 and 1\n\n    :param arr: (np.ndarray)\n    :return: (np.ndarray)\n    \"\"\"", "\n", "shape", "=", "arr", ".", "shape", "\n", "return", "arr", ".", "swapaxes", "(", "0", ",", "1", ")", ".", "reshape", "(", "shape", "[", "0", "]", "*", "shape", "[", "1", "]", ",", "*", "shape", "[", "2", ":", "]", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.save_util.is_json_serializable": [[11, 25], ["json.dumps"], "function", ["None"], ["def", "is_json_serializable", "(", "item", ")", ":", "\n", "    ", "\"\"\"\n    Test if an object is serializable into JSON\n\n    :param item: (object) The object to be tested for JSON serialization.\n    :return: (bool) True if object is JSON serializable, false otherwise.\n    \"\"\"", "\n", "# Try with try-except struct.", "\n", "json_serializable", "=", "True", "\n", "try", ":", "\n", "        ", "_", "=", "json", ".", "dumps", "(", "item", ")", "\n", "", "except", "TypeError", ":", "\n", "        ", "json_serializable", "=", "False", "\n", "", "return", "json_serializable", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.save_util.data_to_json": [[27, 84], ["data.items", "json.dumps", "save_util.is_json_serializable", "base64.b64encode().decode", "str", "hasattr", "isinstance", "item_generator", "base64.b64encode", "type", "isinstance", "save_util.is_json_serializable", "cloudpickle.dumps", "str"], "function", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.save_util.is_json_serializable", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.save_util.is_json_serializable"], ["", "def", "data_to_json", "(", "data", ")", ":", "\n", "    ", "\"\"\"\n    Turn data (class parameters) into a JSON string for storing\n\n    :param data: (Dict) Dictionary of class parameters to be\n        stored. Items that are not JSON serializable will be\n        pickled with Cloudpickle and stored as bytearray in\n        the JSON file\n    :return: (str) JSON string of the data serialized.\n    \"\"\"", "\n", "# First, check what elements can not be JSONfied,", "\n", "# and turn them into byte-strings", "\n", "serializable_data", "=", "{", "}", "\n", "for", "data_key", ",", "data_item", "in", "data", ".", "items", "(", ")", ":", "\n", "# See if object is JSON serializable", "\n", "        ", "if", "is_json_serializable", "(", "data_item", ")", ":", "\n", "# All good, store as it is", "\n", "            ", "serializable_data", "[", "data_key", "]", "=", "data_item", "\n", "", "else", ":", "\n", "# Not serializable, cloudpickle it into", "\n", "# bytes and convert to base64 string for storing.", "\n", "# Also store type of the class for consumption", "\n", "# from other languages/humans, so we have an", "\n", "# idea what was being stored.", "\n", "            ", "base64_encoded", "=", "base64", ".", "b64encode", "(", "\n", "cloudpickle", ".", "dumps", "(", "data_item", ")", "\n", ")", ".", "decode", "(", ")", "\n", "\n", "# Use \":\" to make sure we do", "\n", "# not override these keys", "\n", "# when we include variables of the object later", "\n", "cloudpickle_serialization", "=", "{", "\n", "\":type:\"", ":", "str", "(", "type", "(", "data_item", ")", ")", ",", "\n", "\":serialized:\"", ":", "base64_encoded", "\n", "}", "\n", "\n", "# Add first-level JSON-serializable items of the", "\n", "# object for further details (but not deeper than this to", "\n", "# avoid deep nesting).", "\n", "# First we check that object has attributes (not all do,", "\n", "# e.g. numpy scalars)", "\n", "if", "hasattr", "(", "data_item", ",", "\"__dict__\"", ")", "or", "isinstance", "(", "data_item", ",", "dict", ")", ":", "\n", "# Take elements from __dict__ for custom classes", "\n", "                ", "item_generator", "=", "(", "\n", "data_item", ".", "items", "if", "isinstance", "(", "data_item", ",", "dict", ")", "else", "data_item", ".", "__dict__", ".", "items", "\n", ")", "\n", "for", "variable_name", ",", "variable_item", "in", "item_generator", "(", ")", ":", "\n", "# Check if serializable. If not, just include the", "\n", "# string-representation of the object.", "\n", "                    ", "if", "is_json_serializable", "(", "variable_item", ")", ":", "\n", "                        ", "cloudpickle_serialization", "[", "variable_name", "]", "=", "variable_item", "\n", "", "else", ":", "\n", "                        ", "cloudpickle_serialization", "[", "variable_name", "]", "=", "str", "(", "variable_item", ")", "\n", "\n", "", "", "", "serializable_data", "[", "data_key", "]", "=", "cloudpickle_serialization", "\n", "", "", "json_string", "=", "json", ".", "dumps", "(", "serializable_data", ",", "indent", "=", "4", ")", "\n", "return", "json_string", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.save_util.json_to_data": [[86, 133], ["json.loads", "json.loads.items", "ValueError", "isinstance", "custom_objects.keys", "isinstance", "data_item.keys", "cloudpickle.loads", "base64.b64decode", "RuntimeError", "serialization.encode"], "function", ["None"], ["", "def", "json_to_data", "(", "json_string", ",", "custom_objects", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Turn JSON serialization of class-parameters back into dictionary.\n\n    :param json_string: (str) JSON serialization of the class-parameters\n        that should be loaded.\n    :param custom_objects: (dict) Dictionary of objects to replace\n        upon loading. If a variable is present in this dictionary as a\n        key, it will not be deserialized and the corresponding item\n        will be used instead. Similar to custom_objects in\n        `keras.models.load_model`. Useful when you have an object in\n        file that can not be deserialized.\n    :return: (dict) Loaded class parameters.\n    \"\"\"", "\n", "if", "custom_objects", "is", "not", "None", "and", "not", "isinstance", "(", "custom_objects", ",", "dict", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"custom_objects argument must be a dict or None\"", ")", "\n", "\n", "", "json_dict", "=", "json", ".", "loads", "(", "json_string", ")", "\n", "# This will be filled with deserialized data", "\n", "return_data", "=", "{", "}", "\n", "for", "data_key", ",", "data_item", "in", "json_dict", ".", "items", "(", ")", ":", "\n", "        ", "if", "custom_objects", "is", "not", "None", "and", "data_key", "in", "custom_objects", ".", "keys", "(", ")", ":", "\n", "# If item is provided in custom_objects, replace", "\n", "# the one from JSON with the one in custom_objects", "\n", "            ", "return_data", "[", "data_key", "]", "=", "custom_objects", "[", "data_key", "]", "\n", "", "elif", "isinstance", "(", "data_item", ",", "dict", ")", "and", "\":serialized:\"", "in", "data_item", ".", "keys", "(", ")", ":", "\n", "# If item is dictionary with \":serialized:\"", "\n", "# key, this means it is serialized with cloudpickle.", "\n", "            ", "serialization", "=", "data_item", "[", "\":serialized:\"", "]", "\n", "# Try-except deserialization in case we run into", "\n", "# errors. If so, we can tell bit more information to", "\n", "# user.", "\n", "try", ":", "\n", "                ", "deserialized_object", "=", "cloudpickle", ".", "loads", "(", "\n", "base64", ".", "b64decode", "(", "serialization", ".", "encode", "(", ")", ")", "\n", ")", "\n", "", "except", "pickle", ".", "UnpicklingError", ":", "\n", "                ", "raise", "RuntimeError", "(", "\n", "\"Could not deserialize object {}. \"", ".", "format", "(", "data_key", ")", "+", "\n", "\"Consider using `custom_objects` argument to replace \"", "+", "\n", "\"this object.\"", "\n", ")", "\n", "", "return_data", "[", "data_key", "]", "=", "deserialized_object", "\n", "", "else", ":", "\n", "# Read as it is", "\n", "            ", "return_data", "[", "data_key", "]", "=", "data_item", "\n", "", "", "return", "return_data", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.save_util.params_to_bytes": [[135, 153], ["io.BytesIO", "numpy.savez", "io.BytesIO.getvalue"], "function", ["None"], ["", "def", "params_to_bytes", "(", "params", ")", ":", "\n", "    ", "\"\"\"\n    Turn params (OrderedDict of variable name -> ndarray) into\n    serialized bytes for storing.\n\n    Note: `numpy.savez` does not save the ordering.\n\n    :param params: (OrderedDict) Dictionary mapping variable\n        names to numpy arrays of the current parameters of the\n        model.\n    :return: (bytes) Bytes object of the serialized content.\n    \"\"\"", "\n", "# Create byte-buffer and save params with", "\n", "# savez function, and return the bytes.", "\n", "byte_file", "=", "io", ".", "BytesIO", "(", ")", "\n", "np", ".", "savez", "(", "byte_file", ",", "**", "params", ")", "\n", "serialized_params", "=", "byte_file", ".", "getvalue", "(", ")", "\n", "return", "serialized_params", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.save_util.bytes_to_params": [[155, 174], ["io.BytesIO", "numpy.load", "collections.OrderedDict"], "function", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.VecNormalize.load"], ["", "def", "bytes_to_params", "(", "serialized_params", ",", "param_list", ")", ":", "\n", "    ", "\"\"\"\n    Turn serialized parameters (bytes) back into OrderedDictionary.\n\n    :param serialized_params: (byte) Serialized parameters\n        with `numpy.savez`.\n    :param param_list: (list) List of strings, representing\n        the order of parameters in which they should be returned\n    :return: (OrderedDict) Dictionary mapping variable name to\n        numpy array of the parameters.\n    \"\"\"", "\n", "byte_file", "=", "io", ".", "BytesIO", "(", "serialized_params", ")", "\n", "params", "=", "np", ".", "load", "(", "byte_file", ")", "\n", "return_dictionary", "=", "OrderedDict", "(", ")", "\n", "# Assign parameters to return_dictionary", "\n", "# in the order specified by param_list", "\n", "for", "param_name", "in", "param_list", ":", "\n", "        ", "return_dictionary", "[", "param_name", "]", "=", "params", "[", "param_name", "]", "\n", "", "return", "return_dictionary", "\n", "", ""]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.KVWriter.writekvs": [[32, 39], ["None"], "methods", ["None"], ["        ", "self", ".", "start_time", "=", "time", ".", "time", "(", ")", "\n", "self", ".", "_config", "=", "config", "\n", "\n", "# Get params", "\n", "logging_config", "=", "config", ".", "logging", "\n", "self", ".", "dir", "=", "\".\"", "\n", "self", ".", "_use_neptune", "=", "logging_config", ".", "use_neptune", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.SeqWriter.writeseq": [[46, 53], ["None"], "methods", ["None"], ["tags", "=", "[", "logging_config", ".", "experiment_name", "]", "+", "[", "config", ".", "algorithm", ".", "env_name", "]", "\n", "self", ".", "_neptune_run", "[", "\"sys/tags\"", "]", ".", "add", "(", "tags", ")", "\n", "\n", "# Create directories", "\n", "", "self", ".", "_create_dirs", "(", "\n", "config", ".", "algorithm", ".", "env_name", ",", "\n", "config", ".", "algorithm", ".", "seed", ",", "\n", "config", ".", "logging", ".", "save_models", ",", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.HumanOutputFormat.__init__": [[56, 69], ["isinstance", "open", "hasattr"], "methods", ["None"], ["# Dump config file", "\n", "with", "open", "(", "self", ".", "_save_dir", "+", "\"/\"", "+", "\"config.yaml\"", ",", "\"w\"", ")", "as", "cfg", ":", "\n", "            ", "yaml", ".", "dump", "(", "dataclasses", ".", "asdict", "(", "config", ")", ",", "cfg", ",", "default_flow_style", "=", "True", ")", "\n", "\n", "# Dump config", "\n", "", "config_str", "=", "yaml", ".", "dump", "(", "dataclasses", ".", "asdict", "(", "config", ")", ",", "default_flow_style", "=", "True", ")", "\n", "self", ".", "_summary_writer", ".", "add_text", "(", "\"config\"", ",", "config_str", ",", "0", ")", "\n", "\n", "self", ".", "_logs_df", "=", "None", "\n", "\n", "self", ".", "print_experiment_info", "(", ")", "\n", "\n", "", "def", "print_experiment_info", "(", "self", ")", ":", "\n", "        ", "print", "(", "\"---------------------------------\"", ")", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.HumanOutputFormat.writekvs": [[70, 103], ["sorted", "sorted", "lines.append", "logger.HumanOutputFormat.file.write", "logger.HumanOutputFormat.file.flush", "kvs.items", "isinstance", "logger.HumanOutputFormat._truncate", "len", "warnings.warn", "max", "max", "key2str.items", "lines.append", "str", "map", "map", "logger.HumanOutputFormat._truncate", "key2str.keys", "key2str.values", "len", "len"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.HumanOutputFormat._truncate", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.warn", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.HumanOutputFormat._truncate"], ["env_name", "=", "self", ".", "_config", ".", "algorithm", ".", "env_name", "\n", "print", "(", "\"Env: {}\"", ".", "format", "(", "env_name", ")", ",", "flush", "=", "True", ")", "\n", "print", "(", "\"Agent: AGAC PPO\"", ",", "flush", "=", "True", ")", "\n", "print", "(", "\"Seed: {}\"", ".", "format", "(", "self", ".", "_config", ".", "algorithm", ".", "seed", ")", ",", "flush", "=", "True", ")", "\n", "print", "(", "\"---------------------------------\"", ")", "\n", "\n", "rl_config", "=", "self", ".", "_config", ".", "reinforcement_learning", "\n", "learning_rate", "=", "rl_config", ".", "actor_learning_rate", "\n", "print", "(", "\" actor lr: {}\"", ".", "format", "(", "learning_rate", ")", ",", "flush", "=", "True", ")", "\n", "learning_rate", "=", "rl_config", ".", "critic_learning_rate", "\n", "print", "(", "\" critic lr: {}\"", ".", "format", "(", "learning_rate", ")", ",", "flush", "=", "True", ")", "\n", "learning_rate", "=", "rl_config", ".", "adversary_learning_rate", "\n", "print", "(", "\" adversary lr: {}\"", ".", "format", "(", "learning_rate", ")", ",", "flush", "=", "True", ")", "\n", "layers_dim", "=", "rl_config", ".", "layers_dim", "\n", "print", "(", "\"layers_dim: {}\"", ".", "format", "(", "layers_dim", ")", ",", "flush", "=", "True", ")", "\n", "print", "(", "\"---------------------------------\"", ")", "\n", "\n", "", "def", "_create_dirs", "(", "self", ",", "env_name", ",", "seed", ",", "save_models", ")", ":", "\n", "# create directories", "\n", "        ", "save_dir", "=", "\"./runs/\"", "+", "\"%s_%s_\"", "%", "(", "\"AGAC\"", ",", "env_name", ")", "\n", "save_dir", "+=", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "\"%b%d_%H-%M-%S\"", ")", "\n", "save_dir", "+=", "\"_%s\"", "%", "seed", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "save_dir", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "save_dir", ")", "\n", "", "self", ".", "_save_dir", "=", "save_dir", "\n", "self", ".", "_summary_writer", "=", "SummaryWriter", "(", "self", ".", "_save_dir", ")", "\n", "\n", "if", "save_models", "and", "not", "os", ".", "path", ".", "exists", "(", "self", ".", "_save_dir", "+", "\"/models\"", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "self", ".", "_save_dir", "+", "\"/models\"", ")", "\n", "\n", "", "", "def", "log", "(", "self", ",", "logs", ":", "List", "[", "LogData", "]", ")", ":", "\n", "        ", "\"\"\"\n        Log data to different channels and save model if required.\n        \"\"\"", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.HumanOutputFormat._truncate": [[104, 107], ["len"], "methods", ["None"], ["# get number of steps", "\n", "steps", "=", "[", "log", ".", "value", "for", "log", "in", "logs", "if", "log", ".", "name", "==", "\"total_steps\"", "]", "[", "0", "]", "\n", "\n", "# log", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.HumanOutputFormat.writeseq": [[108, 116], ["list", "enumerate", "logger.HumanOutputFormat.file.write", "logger.HumanOutputFormat.file.flush", "logger.HumanOutputFormat.file.write", "logger.HumanOutputFormat.file.write", "len"], "methods", ["None"], ["self", ".", "_log_in_tensorboard", "(", "steps", ",", "logs", ")", "\n", "self", ".", "_log_in_file", "(", "logs", ")", "\n", "if", "self", ".", "_use_neptune", ":", "\n", "            ", "self", ".", "_log_in_neptune", "(", "logs", ")", "\n", "\n", "# save actors if needed", "\n", "", "if", "self", ".", "_config", ".", "logging", ".", "save_models", ":", "\n", "            ", "weights", "=", "[", "log", ".", "value", "for", "log", "in", "logs", "if", "log", ".", "name", "==", "\"actor_weights\"", "]", "[", "0", "]", "\n", "self", ".", "_save_actor_weights", "(", "weights", ",", "steps", ")", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.HumanOutputFormat.close": [[117, 123], ["logger.HumanOutputFormat.file.close"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.close"], ["\n", "", "", "def", "_log_in_tensorboard", "(", "self", ",", "steps", ":", "int", ",", "logs", ":", "List", "[", "LogData", "]", ")", ":", "\n", "        ", "\"\"\"\n        Log data in tensorboard.\n        \"\"\"", "\n", "\n", "for", "log", "in", "logs", ":", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.JSONOutputFormat.__init__": [[126, 133], ["open"], "methods", ["None"], ["", "if", "log", ".", "type", "==", "\"image\"", ":", "\n", "                ", "self", ".", "_summary_writer", ".", "add_image", "(", "log", ".", "name", ",", "log", ".", "value", ",", "steps", ")", "\n", "\n", "", "", "", "def", "_log_in_neptune", "(", "self", ",", "logs", ":", "List", "[", "LogData", "]", ")", ":", "\n", "        ", "\"\"\"\n        Log data in neptune.\n        \"\"\"", "\n", "# log scalar metrics", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.JSONOutputFormat.writekvs": [[134, 145], ["sorted", "logger.JSONOutputFormat.file.write", "logger.JSONOutputFormat.file.flush", "kvs.items", "hasattr", "json.dumps", "float", "value.tolist", "len"], "methods", ["None"], ["try", ":", "\n", "            ", "for", "log", "in", "logs", ":", "\n", "                ", "if", "log", ".", "type", "==", "\"scalar\"", ":", "\n", "                    ", "self", ".", "_neptune_run", "[", "log", ".", "name", "]", ".", "log", "(", "log", ".", "value", ")", "\n", "", "if", "log", ".", "type", "==", "\"image\"", ":", "\n", "                    ", "if", "(", "log", ".", "value", ".", "ndim", "==", "3", ")", "and", "(", "log", ".", "value", ".", "shape", "[", "-", "1", "]", "!=", "3", ")", ":", "\n", "                        ", "self", ".", "_neptune_run", "[", "log", ".", "name", "]", ".", "log", "(", "\n", "File", ".", "as_image", "(", "log", ".", "value", ".", "transpose", "(", "1", ",", "2", ",", "0", ")", ")", "\n", ")", "\n", "", "else", ":", "\n", "                        ", "self", ".", "_neptune_run", "[", "log", ".", "name", "]", ".", "log", "(", "File", ".", "as_image", "(", "log", ".", "value", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.JSONOutputFormat.close": [[146, 151], ["logger.JSONOutputFormat.file.close"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.close"], ["# log also csv and pkl files", "\n", "# self._neptune_run[\"logs.csv\"].upload(self._save_dir + \"/logs.csv\")", "\n", "# self._neptune_run[\"logs.pkl\"].upload(self._save_dir + \"/logs.pkl\")", "\n", "# neptune.log_artifact(self._save_dir + \"/logs.csv\")", "\n", "# neptune.log_artifact(self._save_dir + \"/logs.pkl\")", "\n", "", "", "", "", "except", "RuntimeError", ":", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.CSVOutputFormat.__init__": [[154, 163], ["open"], "methods", ["None"], ["", "", "def", "_log_in_file", "(", "self", ",", "logs", ":", "List", "[", "LogData", "]", ")", ":", "\n", "        ", "\"\"\"\n        Log data in tensorboard.\n        \"\"\"", "\n", "df_dict", "=", "{", "}", "\n", "for", "log", "in", "logs", ":", "\n", "            ", "if", "log", ".", "type", "==", "\"scalar\"", ":", "\n", "                ", "df_dict", "[", "log", ".", "name", "]", "=", "log", ".", "value", "\n", "", "elif", "log", ".", "type", "==", "\"array\"", ":", "\n", "                ", "if", "\"weights\"", "not", "in", "log", ".", "name", ":", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.CSVOutputFormat.writekvs": [[164, 189], ["enumerate", "logger.CSVOutputFormat.file.write", "logger.CSVOutputFormat.file.flush", "kvs.keys", "logger.CSVOutputFormat.keys.extend", "logger.CSVOutputFormat.file.seek", "logger.CSVOutputFormat.file.readlines", "logger.CSVOutputFormat.file.seek", "enumerate", "logger.CSVOutputFormat.file.write", "kvs.get", "logger.CSVOutputFormat.file.write", "logger.CSVOutputFormat.file.write", "logger.CSVOutputFormat.file.write", "logger.CSVOutputFormat.file.write", "logger.CSVOutputFormat.file.write", "logger.CSVOutputFormat.file.write", "logger.CSVOutputFormat.file.write", "str", "len"], "methods", ["None"], ["                    ", "df_dict", "[", "log", ".", "name", "]", "=", "log", ".", "value", "\n", "\n", "", "", "", "if", "self", ".", "_logs_df", "is", "None", ":", "\n", "# if not already done, create dataframe", "\n", "            ", "for", "key", ",", "value", "in", "df_dict", ".", "items", "(", ")", ":", "\n", "                ", "df_dict", "[", "key", "]", "=", "[", "value", "]", "\n", "", "self", ".", "_logs_df", "=", "pd", ".", "DataFrame", "(", "data", "=", "df_dict", ")", "\n", "\n", "", "else", ":", "\n", "# otherwise, add data in dataframe", "\n", "            ", "self", ".", "_logs_df", "=", "self", ".", "_logs_df", ".", "append", "(", "df_dict", ",", "ignore_index", "=", "True", ")", "\n", "\n", "# save in csv format", "\n", "", "self", ".", "_logs_df", ".", "to_csv", "(", "self", ".", "_save_dir", "+", "\"/logs.csv\"", ")", "\n", "# save in pickle format", "\n", "self", ".", "_logs_df", ".", "to_pickle", "(", "self", ".", "_save_dir", "+", "\"/logs.pkl\"", ")", "\n", "\n", "", "def", "_save_actor_weights", "(", "self", ",", "actor_weights", ",", "steps", ")", ":", "\n", "        ", "\"\"\"\n        Save the weights of the population actors.\n        \"\"\"", "\n", "path", "=", "self", ".", "_save_dir", "+", "\"/models/agent_steps_{}\"", ".", "format", "(", "steps", ")", "\n", "np", ".", "save", "(", "path", ",", "actor_weights", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.CSVOutputFormat.close": [[190, 195], ["logger.CSVOutputFormat.file.close"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.close"], []], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.TensorBoardOutputFormat.__init__": [[221, 233], ["os.makedirs", "os.path.join", "tensorflow.python.pywrap_tensorflow.EventsWriter", "os.path.abspath", "tensorflow.python.util.compat.as_bytes"], "methods", ["None"], []], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.TensorBoardOutputFormat.writekvs": [[234, 243], ["tensorflow.Summary", "tensorflow.core.util.event_pb2.Event", "logger.TensorBoardOutputFormat.writer.WriteEvent", "logger.TensorBoardOutputFormat.writer.Flush", "ValueError", "time.time", "logger.summary_val", "kvs.items", "logger.valid_float_value"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.summary_val", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.valid_float_value"], []], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.TensorBoardOutputFormat.close": [[244, 251], ["logger.TensorBoardOutputFormat.writer.Close"], "methods", ["None"], []], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.ProfileKV.__init__": [[419, 428], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.ProfileKV.__enter__": [[429, 431], ["time.time"], "methods", ["None"], []], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.ProfileKV.__exit__": [[432, 434], ["time.time"], "methods", ["None"], []], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.Logger.__init__": [[467, 479], ["collections.defaultdict", "collections.defaultdict"], "methods", ["None"], []], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.Logger.logkv": [[482, 492], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.Logger.logkv_mean": [[493, 506], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.Logger.dumpkvs": [[507, 518], ["logger.Logger.name2val.clear", "logger.Logger.name2cnt.clear", "isinstance", "fmt.writekvs"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.TensorBoardOutputFormat.writekvs"], []], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.Logger.log": [[519, 532], ["logger.Logger._do_log"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.Logger._do_log"], []], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.Logger.set_level": [[535, 542], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.Logger.get_dir": [[543, 551], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.Logger.close": [[552, 558], ["fmt.close"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.close"], []], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.Logger._do_log": [[561, 570], ["isinstance", "fmt.writeseq", "map"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.HumanOutputFormat.writeseq"], []], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.ScopedConfigure.__init__": [[616, 630], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.ScopedConfigure.__enter__": [[631, 634], ["logger.configure"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.configure"], []], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.ScopedConfigure.__exit__": [[635, 638], ["Logger.CURRENT.close"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.close"], []], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.summary_val": [[197, 204], ["tensorflow.Summary.Value", "float"], "function", ["None"], []], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.valid_float_value": [[206, 218], ["float"], "function", ["None"], []], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.make_output_format": [[253, 275], ["os.makedirs", "logger.HumanOutputFormat", "logger.HumanOutputFormat", "os.path.join", "logger.JSONOutputFormat", "os.path.join", "logger.CSVOutputFormat", "os.path.join", "logger.TensorBoardOutputFormat", "ValueError", "os.path.join"], "function", ["None"], []], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.logkv": [[281, 291], ["Logger.CURRENT.logkv"], "function", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.logkv"], []], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.logkv_mean": [[293, 301], ["Logger.CURRENT.logkv_mean"], "function", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.logkv_mean"], []], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.logkvs": [[303, 311], ["key_values.items", "logger.logkv"], "function", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.logkv"], []], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.dumpkvs": [[313, 318], ["Logger.CURRENT.dumpkvs"], "function", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.dumpkvs"], []], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.getkvs": [[320, 327], ["None"], "function", ["None"], []], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.log": [[329, 341], ["Logger.CURRENT.log"], "function", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.log"], []], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.debug": [[343, 352], ["logger.log"], "function", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.log"], []], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.info": [[354, 363], ["logger.log"], "function", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.log"], []], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.warn": [[365, 374], ["logger.log"], "function", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.log"], []], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.error": [[376, 385], ["logger.log"], "function", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.log"], []], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.set_level": [[387, 394], ["Logger.CURRENT.set_level"], "function", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.set_level"], []], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.get_level": [[396, 402], ["None"], "function", ["None"], []], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.get_dir": [[404, 412], ["Logger.CURRENT.get_dir"], "function", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.get_dir"], []], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.profile": [[436, 454], ["logger.ProfileKV", "func"], "function", ["None"], []], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.configure": [[575, 603], ["isinstance", "os.makedirs", "core.cmd_util.mpi_rank_or_zero", "filter", "logger.Logger", "logger.log", "os.getenv", "os.path.join", "logger.make_output_format", "tempfile.gettempdir", "datetime.datetime.now().strftime", "os.getenv().split", "os.getenv().split", "datetime.datetime.now", "os.getenv", "os.getenv"], "function", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.mpi_rank_or_zero", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.log", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.make_output_format"], []], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.reset": [[605, 613], ["Logger.CURRENT.close", "logger.log"], "function", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.close", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.log"], []], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger._demo": [[642, 678], ["logger.info", "logger.debug", "logger.set_level", "logger.debug", "os.path.exists", "logger.configure", "logger.logkv", "logger.logkv", "logger.dumpkvs", "logger.logkv", "logger.logkv", "logger.dumpkvs", "logger.info", "logger.logkv_mean", "logger.logkv_mean", "logger.logkv", "logger.dumpkvs", "logger.reset", "logger.logkv", "logger.dumpkvs", "logger.warn", "logger.error", "logger.logkvs", "shutil.rmtree", "logger.ScopedConfigure", "logger.info", "logger.ScopedConfigure", "logger.logkv", "logger.dumpkvs"], "function", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.info", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.debug", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.set_level", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.debug", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.configure", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.logkv", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.logkv", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.dumpkvs", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.logkv", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.logkv", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.dumpkvs", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.info", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.logkv_mean", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.logkv_mean", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.logkv", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.dumpkvs", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.reset", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.logkv", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.dumpkvs", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.warn", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.error", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.logkvs", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.info", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.logkv", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.dumpkvs"], []], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.read_json": [[684, 697], ["pandas.DataFrame", "open", "data.append", "json.loads"], "function", ["None"], []], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.read_csv": [[699, 708], ["pandas.read_csv"], "function", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.read_csv"], []], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.read_tb": [[710, 745], ["os.path.isdir", "collections.defaultdict", "np.empty", "sorted", "enumerate", "pandas.DataFrame", "glob", "os.path.basename().startswith", "tensorflow.train.summary_iterator", "collections.defaultdict.keys", "os.path.join", "NotImplementedError", "len", "os.path.basename", "max", "tag2pairs[].append"], "function", ["None"], []], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.dataset.Dataset.__init__": [[5, 20], ["dataset.Dataset.shuffle_dataset", "next", "iter", "data_map.values"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.dataset.Dataset.shuffle_dataset"], ["    ", "def", "__init__", "(", "self", ",", "data_map", ",", "shuffle", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Data loader that handles batches and shuffling.\n        WARNING: this will alter the given data_map ordering, as dicts are mutable\n\n        :param data_map: (dict) the input data, where every column is a key\n        :param shuffle: (bool) Whether to shuffle or not the dataset\n            Important: this should be disabled for recurrent policies\n        \"\"\"", "\n", "self", ".", "data_map", "=", "data_map", "\n", "self", ".", "shuffle", "=", "shuffle", "\n", "self", ".", "n_samples", "=", "next", "(", "iter", "(", "data_map", ".", "values", "(", ")", ")", ")", ".", "shape", "[", "0", "]", "\n", "self", ".", "_next_id", "=", "0", "\n", "if", "self", ".", "shuffle", ":", "\n", "            ", "self", ".", "shuffle_dataset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.dataset.Dataset.shuffle_dataset": [[21, 30], ["numpy.arange", "numpy.random.shuffle"], "methods", ["None"], ["", "", "def", "shuffle_dataset", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Shuffles the data_map\n        \"\"\"", "\n", "perm", "=", "np", ".", "arange", "(", "self", ".", "n_samples", ")", "\n", "np", ".", "random", ".", "shuffle", "(", "perm", ")", "\n", "\n", "for", "key", "in", "self", ".", "data_map", ":", "\n", "            ", "self", ".", "data_map", "[", "key", "]", "=", "self", ".", "data_map", "[", "key", "]", "[", "perm", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.dataset.Dataset.next_batch": [[31, 51], ["min", "dict", "dataset.Dataset.shuffle_dataset"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.dataset.Dataset.shuffle_dataset"], ["", "", "def", "next_batch", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "\"\"\"\n        returns a batch of data of a given size\n\n        :param batch_size: (int) the size of the batch\n        :return: (dict) a batch of the input data of size 'batch_size'\n        \"\"\"", "\n", "if", "self", ".", "_next_id", ">=", "self", ".", "n_samples", ":", "\n", "            ", "self", ".", "_next_id", "=", "0", "\n", "if", "self", ".", "shuffle", ":", "\n", "                ", "self", ".", "shuffle_dataset", "(", ")", "\n", "\n", "", "", "cur_id", "=", "self", ".", "_next_id", "\n", "cur_batch_size", "=", "min", "(", "batch_size", ",", "self", ".", "n_samples", "-", "self", ".", "_next_id", ")", "\n", "self", ".", "_next_id", "+=", "cur_batch_size", "\n", "\n", "data_map", "=", "dict", "(", ")", "\n", "for", "key", "in", "self", ".", "data_map", ":", "\n", "            ", "data_map", "[", "key", "]", "=", "self", ".", "data_map", "[", "key", "]", "[", "cur_id", ":", "cur_id", "+", "cur_batch_size", "]", "\n", "", "return", "data_map", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.dataset.Dataset.iterate_once": [[52, 65], ["dataset.Dataset.shuffle_dataset", "dataset.Dataset.next_batch"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.dataset.Dataset.shuffle_dataset", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.dataset.Dataset.next_batch"], ["", "def", "iterate_once", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "\"\"\"\n        generator that iterates over the dataset\n\n        :param batch_size: (int) the size of the batch\n        :return: (dict) a batch of the input data of size 'batch_size'\n        \"\"\"", "\n", "if", "self", ".", "shuffle", ":", "\n", "            ", "self", ".", "shuffle_dataset", "(", ")", "\n", "\n", "", "while", "self", ".", "_next_id", "<=", "self", ".", "n_samples", "-", "batch_size", ":", "\n", "            ", "yield", "self", ".", "next_batch", "(", "batch_size", ")", "\n", "", "self", ".", "_next_id", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.dataset.Dataset.subset": [[66, 78], ["dict", "dataset.Dataset"], "methods", ["None"], ["", "def", "subset", "(", "self", ",", "num_elements", ",", "shuffle", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Return a subset of the current dataset\n\n        :param num_elements: (int) the number of element you wish to have in the subset\n        :param shuffle: (bool) Whether to shuffle or not the dataset\n        :return: (Dataset) a new subset of the current Dataset object\n        \"\"\"", "\n", "data_map", "=", "dict", "(", ")", "\n", "for", "key", "in", "self", ".", "data_map", ":", "\n", "            ", "data_map", "[", "key", "]", "=", "self", ".", "data_map", "[", "key", "]", "[", ":", "num_elements", "]", "\n", "", "return", "Dataset", "(", "data_map", ",", "shuffle", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.dataset.iterbatches": [[80, 102], ["tuple", "all", "numpy.arange", "numpy.array_split", "map", "numpy.random.shuffle", "numpy.arange", "len", "tuple"], "function", ["None"], ["", "", "def", "iterbatches", "(", "arrays", ",", "*", ",", "num_batches", "=", "None", ",", "batch_size", "=", "None", ",", "shuffle", "=", "True", ",", "include_final_partial_batch", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    Iterates over arrays in batches, must provide either num_batches or batch_size, the other must be None.\n\n    :param arrays: (tuple) a tuple of arrays\n    :param num_batches: (int) the number of batches, must be None is batch_size is defined\n    :param batch_size: (int) the size of the batch, must be None is num_batches is defined\n    :param shuffle: (bool) enable auto shuffle\n    :param include_final_partial_batch: (bool) add the last batch if not the same size as the batch_size\n    :return: (tuples) a tuple of a batch of the arrays\n    \"\"\"", "\n", "assert", "(", "num_batches", "is", "None", ")", "!=", "(", "batch_size", "is", "None", ")", ",", "'Provide num_batches or batch_size, but not both'", "\n", "arrays", "=", "tuple", "(", "map", "(", "np", ".", "asarray", ",", "arrays", ")", ")", "\n", "n_samples", "=", "arrays", "[", "0", "]", ".", "shape", "[", "0", "]", "\n", "assert", "all", "(", "a", ".", "shape", "[", "0", "]", "==", "n_samples", "for", "a", "in", "arrays", "[", "1", ":", "]", ")", "\n", "inds", "=", "np", ".", "arange", "(", "n_samples", ")", "\n", "if", "shuffle", ":", "\n", "        ", "np", ".", "random", ".", "shuffle", "(", "inds", ")", "\n", "", "sections", "=", "np", ".", "arange", "(", "0", ",", "n_samples", ",", "batch_size", ")", "[", "1", ":", "]", "if", "num_batches", "is", "None", "else", "num_batches", "\n", "for", "batch_inds", "in", "np", ".", "array_split", "(", "inds", ",", "sections", ")", ":", "\n", "        ", "if", "include_final_partial_batch", "or", "len", "(", "batch_inds", ")", "==", "batch_size", ":", "\n", "            ", "yield", "tuple", "(", "a", "[", "batch_inds", "]", "for", "a", "in", "arrays", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.ProbabilityDistribution.__init__": [[17, 19], ["object.__init__"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "ProbabilityDistribution", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.ProbabilityDistribution.flatparam": [[20, 27], ["None"], "methods", ["None"], ["", "def", "flatparam", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Return the direct probabilities\n\n        :return: ([float]) the probabilities\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.ProbabilityDistribution.mode": [[28, 35], ["None"], "methods", ["None"], ["", "def", "mode", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Returns the probability\n\n        :return: (Tensorflow Tensor) the deterministic action\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.ProbabilityDistribution.neglogp": [[36, 45], ["None"], "methods", ["None"], ["", "def", "neglogp", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"\n        returns the of the negative log likelihood\n\n        :param x: (str) the labels of each index\n        :return: ([float]) The negative log likelihood of the distribution\n        \"\"\"", "\n", "# Usually it's easier to define the negative logprob", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.ProbabilityDistribution.kl": [[46, 54], ["None"], "methods", ["None"], ["", "def", "kl", "(", "self", ",", "other", ")", ":", "\n", "        ", "\"\"\"\n        Calculates the Kullback-Leibler divergence from the given probability distribution\n\n        :param other: ([float]) the distribution to compare with\n        :return: (float) the KL divergence of the two distributions\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.ProbabilityDistribution.entropy": [[55, 62], ["None"], "methods", ["None"], ["", "def", "entropy", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Returns Shannon's entropy of the probability\n\n        :return: (float) the entropy\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.ProbabilityDistribution.sample": [[63, 70], ["None"], "methods", ["None"], ["", "def", "sample", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        returns a sample from the probability distribution\n\n        :return: (Tensorflow Tensor) the stochastic action\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.ProbabilityDistribution.logp": [[71, 79], ["distributions.ProbabilityDistribution.neglogp"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.BernoulliProbabilityDistribution.neglogp"], ["", "def", "logp", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"\n        returns the of the log likelihood\n\n        :param x: (str) the labels of each index\n        :return: ([float]) The log likelihood of the distribution\n        \"\"\"", "\n", "return", "-", "self", ".", "neglogp", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.ProbabilityDistributionType.probability_distribution_class": [[86, 93], ["None"], "methods", ["None"], ["def", "probability_distribution_class", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        returns the ProbabilityDistribution class of this type\n\n        :return: (Type ProbabilityDistribution) the probability distribution class associated\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.ProbabilityDistributionType.proba_distribution_from_flat": [[94, 103], ["distributions.ProbabilityDistributionType.probability_distribution_class"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.BernoulliProbabilityDistributionType.probability_distribution_class"], ["", "def", "proba_distribution_from_flat", "(", "self", ",", "flat", ")", ":", "\n", "        ", "\"\"\"\n        Returns the probability distribution from flat probabilities\n        flat: flattened vector of parameters of probability distribution\n\n        :param flat: ([float]) the flat probabilities\n        :return: (ProbabilityDistribution) the instance of the ProbabilityDistribution associated\n        \"\"\"", "\n", "return", "self", ".", "probability_distribution_class", "(", ")", "(", "flat", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.ProbabilityDistributionType.proba_distribution_from_latent": [[104, 115], ["None"], "methods", ["None"], ["", "def", "proba_distribution_from_latent", "(", "self", ",", "pi_latent_vector", ",", "vf_latent_vector", ",", "init_scale", "=", "1.0", ",", "init_bias", "=", "0.0", ")", ":", "\n", "        ", "\"\"\"\n        returns the probability distribution from latent values\n\n        :param pi_latent_vector: ([float]) the latent pi values\n        :param vf_latent_vector: ([float]) the latent vf values\n        :param init_scale: (float) the initial scale of the distribution\n        :param init_bias: (float) the initial bias of the distribution\n        :return: (ProbabilityDistribution) the instance of the ProbabilityDistribution associated\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.ProbabilityDistributionType.param_shape": [[116, 123], ["None"], "methods", ["None"], ["", "def", "param_shape", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        returns the shape of the input parameters\n\n        :return: ([int]) the shape\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.ProbabilityDistributionType.sample_shape": [[124, 131], ["None"], "methods", ["None"], ["", "def", "sample_shape", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        returns the shape of the sampling\n\n        :return: ([int]) the shape\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.ProbabilityDistributionType.sample_dtype": [[132, 139], ["None"], "methods", ["None"], ["", "def", "sample_dtype", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        returns the type of the sampling\n\n        :return: (type) the type\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.ProbabilityDistributionType.param_placeholder": [[140, 149], ["tensorflow.placeholder", "distributions.ProbabilityDistributionType.param_shape"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.BernoulliProbabilityDistributionType.param_shape"], ["", "def", "param_placeholder", "(", "self", ",", "prepend_shape", ",", "name", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        returns the TensorFlow placeholder for the input parameters\n\n        :param prepend_shape: ([int]) the prepend shape\n        :param name: (str) the placeholder name\n        :return: (TensorFlow Tensor) the placeholder\n        \"\"\"", "\n", "return", "tf", ".", "placeholder", "(", "dtype", "=", "tf", ".", "float32", ",", "shape", "=", "prepend_shape", "+", "self", ".", "param_shape", "(", ")", ",", "name", "=", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.ProbabilityDistributionType.sample_placeholder": [[150, 159], ["tensorflow.compat.v1.placeholder", "distributions.ProbabilityDistributionType.sample_dtype", "distributions.ProbabilityDistributionType.sample_shape"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.BernoulliProbabilityDistributionType.sample_dtype", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.BernoulliProbabilityDistributionType.sample_shape"], ["", "def", "sample_placeholder", "(", "self", ",", "prepend_shape", ",", "name", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        returns the TensorFlow placeholder for the sampling\n\n        :param prepend_shape: ([int]) the prepend shape\n        :param name: (str) the placeholder name\n        :return: (TensorFlow Tensor) the placeholder\n        \"\"\"", "\n", "return", "tf", ".", "compat", ".", "v1", ".", "placeholder", "(", "dtype", "=", "self", ".", "sample_dtype", "(", ")", ",", "shape", "=", "prepend_shape", "+", "self", ".", "sample_shape", "(", ")", ",", "name", "=", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.CategoricalProbabilityDistributionType.__init__": [[162, 169], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "n_cat", ")", ":", "\n", "        ", "\"\"\"\n        The probability distribution type for categorical input\n\n        :param n_cat: (int) the number of categories\n        \"\"\"", "\n", "self", ".", "n_cat", "=", "n_cat", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.CategoricalProbabilityDistributionType.probability_distribution_class": [[170, 172], ["None"], "methods", ["None"], ["", "def", "probability_distribution_class", "(", "self", ")", ":", "\n", "        ", "return", "CategoricalProbabilityDistribution", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.CategoricalProbabilityDistributionType.proba_distribution_from_latent": [[173, 176], ["core.tf_layers.linear", "distributions.CategoricalProbabilityDistributionType.proba_distribution_from_flat"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_layers.linear", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.DiagGaussianProbabilityDistributionType.proba_distribution_from_flat"], ["", "def", "proba_distribution_from_latent", "(", "self", ",", "pi_latent_vector", ",", "vf_latent_vector", ",", "init_scale", "=", "1.0", ",", "init_bias", "=", "0.0", ")", ":", "\n", "        ", "pdparam", "=", "linear", "(", "pi_latent_vector", ",", "'pi'", ",", "self", ".", "n_cat", ",", "init_scale", "=", "init_scale", ",", "init_bias", "=", "init_bias", ")", "\n", "return", "self", ".", "proba_distribution_from_flat", "(", "pdparam", ")", ",", "pdparam", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.CategoricalProbabilityDistributionType.param_shape": [[177, 179], ["None"], "methods", ["None"], ["", "def", "param_shape", "(", "self", ")", ":", "\n", "        ", "return", "[", "self", ".", "n_cat", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.CategoricalProbabilityDistributionType.sample_shape": [[180, 182], ["None"], "methods", ["None"], ["", "def", "sample_shape", "(", "self", ")", ":", "\n", "        ", "return", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.CategoricalProbabilityDistributionType.sample_dtype": [[183, 185], ["None"], "methods", ["None"], ["", "def", "sample_dtype", "(", "self", ")", ":", "\n", "        ", "return", "tf", ".", "int64", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.MultiCategoricalProbabilityDistributionType.__init__": [[188, 198], ["n_vec.astype"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "n_vec", ")", ":", "\n", "        ", "\"\"\"\n        The probability distribution type for multiple categorical input\n\n        :param n_vec: ([int]) the vectors\n        \"\"\"", "\n", "# Cast the variable because tf does not allow uint32", "\n", "self", ".", "n_vec", "=", "n_vec", ".", "astype", "(", "np", ".", "int32", ")", "\n", "# Check that the cast was valid", "\n", "assert", "(", "self", ".", "n_vec", ">", "0", ")", ".", "all", "(", ")", ",", "\"Casting uint32 to int32 was invalid\"", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.MultiCategoricalProbabilityDistributionType.probability_distribution_class": [[199, 201], ["None"], "methods", ["None"], ["", "def", "probability_distribution_class", "(", "self", ")", ":", "\n", "        ", "return", "MultiCategoricalProbabilityDistribution", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.MultiCategoricalProbabilityDistributionType.proba_distribution_from_flat": [[202, 204], ["distributions.MultiCategoricalProbabilityDistribution"], "methods", ["None"], ["", "def", "proba_distribution_from_flat", "(", "self", ",", "flat", ")", ":", "\n", "        ", "return", "MultiCategoricalProbabilityDistribution", "(", "self", ".", "n_vec", ",", "flat", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.MultiCategoricalProbabilityDistributionType.proba_distribution_from_latent": [[205, 209], ["core.tf_layers.linear", "core.tf_layers.linear", "sum", "sum", "distributions.MultiCategoricalProbabilityDistributionType.proba_distribution_from_flat"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_layers.linear", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_layers.linear", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.DiagGaussianProbabilityDistributionType.proba_distribution_from_flat"], ["", "def", "proba_distribution_from_latent", "(", "self", ",", "pi_latent_vector", ",", "vf_latent_vector", ",", "init_scale", "=", "1.0", ",", "init_bias", "=", "0.0", ")", ":", "\n", "        ", "pdparam", "=", "linear", "(", "pi_latent_vector", ",", "'pi'", ",", "sum", "(", "self", ".", "n_vec", ")", ",", "init_scale", "=", "init_scale", ",", "init_bias", "=", "init_bias", ")", "\n", "q_values", "=", "linear", "(", "vf_latent_vector", ",", "'q'", ",", "sum", "(", "self", ".", "n_vec", ")", ",", "init_scale", "=", "init_scale", ",", "init_bias", "=", "init_bias", ")", "\n", "return", "self", ".", "proba_distribution_from_flat", "(", "pdparam", ")", ",", "pdparam", ",", "q_values", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.MultiCategoricalProbabilityDistributionType.param_shape": [[210, 212], ["sum"], "methods", ["None"], ["", "def", "param_shape", "(", "self", ")", ":", "\n", "        ", "return", "[", "sum", "(", "self", ".", "n_vec", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.MultiCategoricalProbabilityDistributionType.sample_shape": [[213, 215], ["len"], "methods", ["None"], ["", "def", "sample_shape", "(", "self", ")", ":", "\n", "        ", "return", "[", "len", "(", "self", ".", "n_vec", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.MultiCategoricalProbabilityDistributionType.sample_dtype": [[216, 218], ["None"], "methods", ["None"], ["", "def", "sample_dtype", "(", "self", ")", ":", "\n", "        ", "return", "tf", ".", "int64", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.DiagGaussianProbabilityDistributionType.__init__": [[221, 228], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "size", ")", ":", "\n", "        ", "\"\"\"\n        The probability distribution type for multivariate Gaussian input\n\n        :param size: (int) the number of dimensions of the multivariate gaussian\n        \"\"\"", "\n", "self", ".", "size", "=", "size", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.DiagGaussianProbabilityDistributionType.probability_distribution_class": [[229, 231], ["None"], "methods", ["None"], ["", "def", "probability_distribution_class", "(", "self", ")", ":", "\n", "        ", "return", "DiagGaussianProbabilityDistribution", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.DiagGaussianProbabilityDistributionType.proba_distribution_from_flat": [[232, 240], ["distributions.DiagGaussianProbabilityDistributionType.probability_distribution_class"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.BernoulliProbabilityDistributionType.probability_distribution_class"], ["", "def", "proba_distribution_from_flat", "(", "self", ",", "flat", ")", ":", "\n", "        ", "\"\"\"\n        returns the probability distribution from flat probabilities\n\n        :param flat: ([float]) the flat probabilities\n        :return: (ProbabilityDistribution) the instance of the ProbabilityDistribution associated\n        \"\"\"", "\n", "return", "self", ".", "probability_distribution_class", "(", ")", "(", "flat", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.DiagGaussianProbabilityDistributionType.proba_distribution_from_latent": [[241, 248], ["core.tf_layers.linear", "tensorflow.compat.v1.get_variable", "tensorflow.concat", "core.tf_layers.linear", "distributions.DiagGaussianProbabilityDistributionType.proba_distribution_from_flat", "tensorflow.zeros_initializer"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_layers.linear", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_layers.linear", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.DiagGaussianProbabilityDistributionType.proba_distribution_from_flat"], ["", "def", "proba_distribution_from_latent", "(", "self", ",", "pi_latent_vector", ",", "vf_latent_vector", ",", "init_scale", "=", "1.0", ",", "init_bias", "=", "0.0", ")", ":", "\n", "        ", "mean", "=", "linear", "(", "pi_latent_vector", ",", "'pi'", ",", "self", ".", "size", ",", "init_scale", "=", "init_scale", ",", "init_bias", "=", "init_bias", ")", "\n", "logstd", "=", "tf", ".", "compat", ".", "v1", ".", "get_variable", "(", "name", "=", "'pi/logstd'", ",", "shape", "=", "[", "1", ",", "self", ".", "size", "]", ",", "initializer", "=", "tf", ".", "zeros_initializer", "(", ")", ")", "\n", "# logstd = linear(pi_latent_vector, 'pi/logstd', self.size, init_scale=init_scale, init_bias=init_bias)", "\n", "pdparam", "=", "tf", ".", "concat", "(", "[", "mean", ",", "mean", "*", "0.0", "+", "logstd", "]", ",", "axis", "=", "1", ")", "\n", "q_values", "=", "linear", "(", "vf_latent_vector", ",", "'q'", ",", "self", ".", "size", ",", "init_scale", "=", "init_scale", ",", "init_bias", "=", "init_bias", ")", "\n", "return", "self", ".", "proba_distribution_from_flat", "(", "pdparam", ")", ",", "mean", ",", "q_values", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.DiagGaussianProbabilityDistributionType.param_shape": [[249, 251], ["None"], "methods", ["None"], ["", "def", "param_shape", "(", "self", ")", ":", "\n", "        ", "return", "[", "2", "*", "self", ".", "size", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.DiagGaussianProbabilityDistributionType.sample_shape": [[252, 254], ["None"], "methods", ["None"], ["", "def", "sample_shape", "(", "self", ")", ":", "\n", "        ", "return", "[", "self", ".", "size", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.DiagGaussianProbabilityDistributionType.sample_dtype": [[255, 257], ["None"], "methods", ["None"], ["", "def", "sample_dtype", "(", "self", ")", ":", "\n", "        ", "return", "tf", ".", "float32", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.BernoulliProbabilityDistributionType.__init__": [[260, 267], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "size", ")", ":", "\n", "        ", "\"\"\"\n        The probability distribution type for Bernoulli input\n\n        :param size: (int) the number of dimensions of the Bernoulli distribution\n        \"\"\"", "\n", "self", ".", "size", "=", "size", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.BernoulliProbabilityDistributionType.probability_distribution_class": [[268, 270], ["None"], "methods", ["None"], ["", "def", "probability_distribution_class", "(", "self", ")", ":", "\n", "        ", "return", "BernoulliProbabilityDistribution", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.BernoulliProbabilityDistributionType.proba_distribution_from_latent": [[271, 275], ["core.tf_layers.linear", "core.tf_layers.linear", "distributions.BernoulliProbabilityDistributionType.proba_distribution_from_flat"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_layers.linear", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_layers.linear", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.DiagGaussianProbabilityDistributionType.proba_distribution_from_flat"], ["", "def", "proba_distribution_from_latent", "(", "self", ",", "pi_latent_vector", ",", "vf_latent_vector", ",", "init_scale", "=", "1.0", ",", "init_bias", "=", "0.0", ")", ":", "\n", "        ", "pdparam", "=", "linear", "(", "pi_latent_vector", ",", "'pi'", ",", "self", ".", "size", ",", "init_scale", "=", "init_scale", ",", "init_bias", "=", "init_bias", ")", "\n", "q_values", "=", "linear", "(", "vf_latent_vector", ",", "'q'", ",", "self", ".", "size", ",", "init_scale", "=", "init_scale", ",", "init_bias", "=", "init_bias", ")", "\n", "return", "self", ".", "proba_distribution_from_flat", "(", "pdparam", ")", ",", "pdparam", ",", "q_values", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.BernoulliProbabilityDistributionType.param_shape": [[276, 278], ["None"], "methods", ["None"], ["", "def", "param_shape", "(", "self", ")", ":", "\n", "        ", "return", "[", "self", ".", "size", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.BernoulliProbabilityDistributionType.sample_shape": [[279, 281], ["None"], "methods", ["None"], ["", "def", "sample_shape", "(", "self", ")", ":", "\n", "        ", "return", "[", "self", ".", "size", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.BernoulliProbabilityDistributionType.sample_dtype": [[282, 284], ["None"], "methods", ["None"], ["", "def", "sample_dtype", "(", "self", ")", ":", "\n", "        ", "return", "tf", ".", "int32", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.CategoricalProbabilityDistribution.__init__": [[287, 295], ["distributions.ProbabilityDistribution.__init__"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__init__"], ["    ", "def", "__init__", "(", "self", ",", "logits", ")", ":", "\n", "        ", "\"\"\"\n        Probability distributions from categorical input\n\n        :param logits: ([float]) the categorical logits input\n        \"\"\"", "\n", "self", ".", "logits", "=", "logits", "\n", "super", "(", "CategoricalProbabilityDistribution", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.CategoricalProbabilityDistribution.flatparam": [[296, 298], ["None"], "methods", ["None"], ["", "def", "flatparam", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.CategoricalProbabilityDistribution.mode": [[299, 301], ["tensorflow.argmax"], "methods", ["None"], ["", "def", "mode", "(", "self", ")", ":", "\n", "        ", "return", "tf", ".", "argmax", "(", "self", ".", "logits", ",", "axis", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.CategoricalProbabilityDistribution.neglogp": [[302, 309], ["tensorflow.one_hot", "tensorflow.nn.softmax_cross_entropy_with_logits_v2", "distributions.CategoricalProbabilityDistribution.logits.get_shape().as_list", "tensorflow.stop_gradient", "distributions.CategoricalProbabilityDistribution.logits.get_shape"], "methods", ["None"], ["", "def", "neglogp", "(", "self", ",", "x", ")", ":", "\n", "# Note: we can't use sparse_softmax_cross_entropy_with_logits because", "\n", "#       the implementation does not allow second-order derivatives...", "\n", "        ", "one_hot_actions", "=", "tf", ".", "one_hot", "(", "x", ",", "self", ".", "logits", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "-", "1", "]", ")", "\n", "return", "tf", ".", "nn", ".", "softmax_cross_entropy_with_logits_v2", "(", "\n", "logits", "=", "self", ".", "logits", ",", "\n", "labels", "=", "tf", ".", "stop_gradient", "(", "one_hot_actions", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.CategoricalProbabilityDistribution.kl": [[310, 319], ["tensorflow.exp", "tensorflow.exp", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.reduce_max", "tensorflow.reduce_max", "tensorflow.log", "tensorflow.log"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.log", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.log"], ["", "def", "kl", "(", "self", ",", "other", ")", ":", "\n", "        ", "a_0", "=", "self", ".", "logits", "-", "tf", ".", "reduce_max", "(", "self", ".", "logits", ",", "axis", "=", "-", "1", ",", "keepdims", "=", "True", ")", "\n", "a_1", "=", "other", ".", "logits", "-", "tf", ".", "reduce_max", "(", "other", ".", "logits", ",", "axis", "=", "-", "1", ",", "keepdims", "=", "True", ")", "\n", "exp_a_0", "=", "tf", ".", "exp", "(", "a_0", ")", "\n", "exp_a_1", "=", "tf", ".", "exp", "(", "a_1", ")", "\n", "z_0", "=", "tf", ".", "reduce_sum", "(", "exp_a_0", ",", "axis", "=", "-", "1", ",", "keepdims", "=", "True", ")", "\n", "z_1", "=", "tf", ".", "reduce_sum", "(", "exp_a_1", ",", "axis", "=", "-", "1", ",", "keepdims", "=", "True", ")", "\n", "p_0", "=", "exp_a_0", "/", "z_0", "\n", "return", "tf", ".", "reduce_sum", "(", "p_0", "*", "(", "a_0", "-", "tf", ".", "log", "(", "z_0", ")", "-", "a_1", "+", "tf", ".", "log", "(", "z_1", ")", ")", ",", "axis", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.CategoricalProbabilityDistribution.entropy": [[320, 326], ["tensorflow.exp", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.reduce_max", "tensorflow.math.log"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.log"], ["", "def", "entropy", "(", "self", ")", ":", "\n", "        ", "a_0", "=", "self", ".", "logits", "-", "tf", ".", "reduce_max", "(", "self", ".", "logits", ",", "axis", "=", "-", "1", ",", "keepdims", "=", "True", ")", "\n", "exp_a_0", "=", "tf", ".", "exp", "(", "a_0", ")", "\n", "z_0", "=", "tf", ".", "reduce_sum", "(", "exp_a_0", ",", "axis", "=", "-", "1", ",", "keepdims", "=", "True", ")", "\n", "p_0", "=", "exp_a_0", "/", "z_0", "\n", "return", "tf", ".", "reduce_sum", "(", "p_0", "*", "(", "tf", ".", "math", ".", "log", "(", "z_0", ")", "-", "a_0", ")", ",", "axis", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.CategoricalProbabilityDistribution.sample": [[327, 332], ["tensorflow.random.uniform", "tensorflow.argmax", "tensorflow.shape", "tensorflow.math.log", "tensorflow.math.log"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.log", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.log"], ["", "def", "sample", "(", "self", ")", ":", "\n", "# Gumbel-max trick to sample", "\n", "# a categorical distribution (see http://amid.fish/humble-gumbel)", "\n", "        ", "uniform", "=", "tf", ".", "random", ".", "uniform", "(", "tf", ".", "shape", "(", "self", ".", "logits", ")", ",", "dtype", "=", "self", ".", "logits", ".", "dtype", ")", "\n", "return", "tf", ".", "argmax", "(", "self", ".", "logits", "-", "tf", ".", "math", ".", "log", "(", "-", "tf", ".", "math", ".", "log", "(", "uniform", ")", ")", ",", "axis", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.CategoricalProbabilityDistribution.fromflat": [[333, 342], ["cls"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "fromflat", "(", "cls", ",", "flat", ")", ":", "\n", "        ", "\"\"\"\n        Create an instance of this from new logits values\n\n        :param flat: ([float]) the categorical logits input\n        :return: (ProbabilityDistribution) the instance from the given categorical input\n        \"\"\"", "\n", "return", "cls", "(", "flat", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.MultiCategoricalProbabilityDistribution.__init__": [[345, 355], ["list", "distributions.ProbabilityDistribution.__init__", "map", "tensorflow.split"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__init__"], ["    ", "def", "__init__", "(", "self", ",", "nvec", ",", "flat", ")", ":", "\n", "        ", "\"\"\"\n        Probability distributions from multicategorical input\n\n        :param nvec: ([int]) the sizes of the different categorical inputs\n        :param flat: ([float]) the categorical logits input\n        \"\"\"", "\n", "self", ".", "flat", "=", "flat", "\n", "self", ".", "categoricals", "=", "list", "(", "map", "(", "CategoricalProbabilityDistribution", ",", "tf", ".", "split", "(", "flat", ",", "nvec", ",", "axis", "=", "-", "1", ")", ")", ")", "\n", "super", "(", "MultiCategoricalProbabilityDistribution", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.MultiCategoricalProbabilityDistribution.flatparam": [[356, 358], ["None"], "methods", ["None"], ["", "def", "flatparam", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "flat", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.MultiCategoricalProbabilityDistribution.mode": [[359, 361], ["tensorflow.stack", "p.mode"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.BernoulliProbabilityDistribution.mode"], ["", "def", "mode", "(", "self", ")", ":", "\n", "        ", "return", "tf", ".", "stack", "(", "[", "p", ".", "mode", "(", ")", "for", "p", "in", "self", ".", "categoricals", "]", ",", "axis", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.MultiCategoricalProbabilityDistribution.neglogp": [[362, 364], ["tensorflow.add_n", "p.neglogp", "zip", "tensorflow.unstack"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.BernoulliProbabilityDistribution.neglogp"], ["", "def", "neglogp", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "tf", ".", "add_n", "(", "[", "p", ".", "neglogp", "(", "px", ")", "for", "p", ",", "px", "in", "zip", "(", "self", ".", "categoricals", ",", "tf", ".", "unstack", "(", "x", ",", "axis", "=", "-", "1", ")", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.MultiCategoricalProbabilityDistribution.kl": [[365, 367], ["tensorflow.add_n", "p.kl", "zip"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.BernoulliProbabilityDistribution.kl"], ["", "def", "kl", "(", "self", ",", "other", ")", ":", "\n", "        ", "return", "tf", ".", "add_n", "(", "[", "p", ".", "kl", "(", "q", ")", "for", "p", ",", "q", "in", "zip", "(", "self", ".", "categoricals", ",", "other", ".", "categoricals", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.MultiCategoricalProbabilityDistribution.entropy": [[368, 370], ["tensorflow.add_n", "p.entropy"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.BernoulliProbabilityDistribution.entropy"], ["", "def", "entropy", "(", "self", ")", ":", "\n", "        ", "return", "tf", ".", "add_n", "(", "[", "p", ".", "entropy", "(", ")", "for", "p", "in", "self", ".", "categoricals", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.MultiCategoricalProbabilityDistribution.sample": [[371, 373], ["tensorflow.stack", "p.sample"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.replay_buffer.ReplayBuffer.sample"], ["", "def", "sample", "(", "self", ")", ":", "\n", "        ", "return", "tf", ".", "stack", "(", "[", "p", ".", "sample", "(", ")", "for", "p", "in", "self", ".", "categoricals", "]", ",", "axis", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.MultiCategoricalProbabilityDistribution.fromflat": [[374, 383], ["None"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "fromflat", "(", "cls", ",", "flat", ")", ":", "\n", "        ", "\"\"\"\n        Create an instance of this from new logits values\n\n        :param flat: ([float]) the multi categorical logits input\n        :return: (ProbabilityDistribution) the instance from the given multi categorical input\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.DiagGaussianProbabilityDistribution.__init__": [[386, 402], ["tensorflow.split", "tensorflow.exp", "distributions.ProbabilityDistribution.__init__", "tensorflow.reduce_mean", "tensorflow.reduce_mean", "tensorflow.cast", "tensorflow.cast", "len", "tensorflow.greater", "tensorflow.greater"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__init__"], ["    ", "def", "__init__", "(", "self", ",", "flat", ")", ":", "\n", "        ", "\"\"\"\n        Probability distributions from multivariate Gaussian input\n\n        :param flat: ([float]) the multivariate Gaussian input data\n        \"\"\"", "\n", "self", ".", "flat", "=", "flat", "\n", "mean", ",", "logstd", "=", "tf", ".", "split", "(", "axis", "=", "len", "(", "flat", ".", "shape", ")", "-", "1", ",", "num_or_size_splits", "=", "2", ",", "value", "=", "flat", ")", "\n", "self", ".", "mean", "=", "mean", "\n", "# Clip logstd", "\n", "self", ".", "clipfrac", "=", "tf", ".", "reduce_mean", "(", "tf", ".", "cast", "(", "tf", ".", "greater", "(", "LOG_STD_MIN", ",", "logstd", ")", ",", "tf", ".", "float32", ")", ")", "+", "tf", ".", "reduce_mean", "(", "tf", ".", "cast", "(", "tf", ".", "greater", "(", "logstd", ",", "LOG_STD_MAX", ")", ",", "tf", ".", "float32", ")", ")", "\n", "# logstd = tf.clip_by_value(logstd, LOG_STD_MIN, LOG_STD_MAX)", "\n", "self", ".", "logstd", "=", "logstd", "\n", "self", ".", "std", "=", "tf", ".", "exp", "(", "logstd", ")", "\n", "super", "(", "DiagGaussianProbabilityDistribution", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.DiagGaussianProbabilityDistribution.flatparam": [[403, 405], ["None"], "methods", ["None"], ["", "def", "flatparam", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "flat", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.DiagGaussianProbabilityDistribution.mode": [[406, 409], ["None"], "methods", ["None"], ["", "def", "mode", "(", "self", ")", ":", "\n", "# Bounds are taken into account outside this class (during training only)", "\n", "        ", "return", "self", ".", "mean", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.DiagGaussianProbabilityDistribution.neglogp": [[410, 414], ["tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.cast", "tensorflow.square", "numpy.log", "tensorflow.shape"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.log"], ["", "def", "neglogp", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "0.5", "*", "tf", ".", "reduce_sum", "(", "tf", ".", "square", "(", "(", "x", "-", "self", ".", "mean", ")", "/", "self", ".", "std", ")", ",", "axis", "=", "-", "1", ")", "+", "0.5", "*", "np", ".", "log", "(", "2.0", "*", "np", ".", "pi", ")", "*", "tf", ".", "cast", "(", "tf", ".", "shape", "(", "x", ")", "[", "-", "1", "]", ",", "tf", ".", "float32", ")", "+", "tf", ".", "reduce_sum", "(", "self", ".", "logstd", ",", "axis", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.DiagGaussianProbabilityDistribution.kl": [[415, 419], ["isinstance", "tensorflow.reduce_sum", "tensorflow.square", "tensorflow.square", "tensorflow.square"], "methods", ["None"], ["", "def", "kl", "(", "self", ",", "other", ")", ":", "\n", "        ", "assert", "isinstance", "(", "other", ",", "DiagGaussianProbabilityDistribution", ")", "\n", "return", "tf", ".", "reduce_sum", "(", "other", ".", "logstd", "-", "self", ".", "logstd", "+", "(", "tf", ".", "square", "(", "self", ".", "std", ")", "+", "tf", ".", "square", "(", "self", ".", "mean", "-", "other", ".", "mean", ")", ")", "/", "\n", "(", "2.0", "*", "tf", ".", "square", "(", "other", ".", "std", ")", ")", "-", "0.5", ",", "axis", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.DiagGaussianProbabilityDistribution.entropy": [[420, 422], ["tensorflow.reduce_sum", "numpy.log"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.log"], ["", "def", "entropy", "(", "self", ")", ":", "\n", "        ", "return", "tf", ".", "reduce_sum", "(", "self", ".", "logstd", "+", ".5", "*", "np", ".", "log", "(", "2.0", "*", "np", ".", "pi", "*", "np", ".", "e", ")", ",", "axis", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.DiagGaussianProbabilityDistribution.logstdvalue": [[423, 425], ["None"], "methods", ["None"], ["", "def", "logstdvalue", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "logstd", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.DiagGaussianProbabilityDistribution.sample": [[426, 431], ["tensorflow.compat.v1.random_normal", "tensorflow.shape"], "methods", ["None"], ["", "def", "sample", "(", "self", ")", ":", "\n", "# Bounds are taken into acount outside this class (during training only)", "\n", "# Otherwise, it changes the distribution and breaks PPO2 for instance", "\n", "        ", "return", "self", ".", "mean", "+", "self", ".", "std", "*", "tf", ".", "compat", ".", "v1", ".", "random_normal", "(", "tf", ".", "shape", "(", "self", ".", "mean", ")", ",", "\n", "dtype", "=", "self", ".", "mean", ".", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.DiagGaussianProbabilityDistribution.fromflat": [[432, 441], ["cls"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "fromflat", "(", "cls", ",", "flat", ")", ":", "\n", "        ", "\"\"\"\n        Create an instance of this from new multivariate Gaussian input\n\n        :param flat: ([float]) the multivariate Gaussian input data\n        :return: (ProbabilityDistribution) the instance from the given multivariate Gaussian input data\n        \"\"\"", "\n", "return", "cls", "(", "flat", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.BernoulliProbabilityDistribution.__init__": [[444, 453], ["tensorflow.sigmoid", "distributions.ProbabilityDistribution.__init__"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__init__"], ["    ", "def", "__init__", "(", "self", ",", "logits", ")", ":", "\n", "        ", "\"\"\"\n        Probability distributions from Bernoulli input\n\n        :param logits: ([float]) the Bernoulli input data\n        \"\"\"", "\n", "self", ".", "logits", "=", "logits", "\n", "self", ".", "probabilities", "=", "tf", ".", "sigmoid", "(", "logits", ")", "\n", "super", "(", "BernoulliProbabilityDistribution", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.BernoulliProbabilityDistribution.flatparam": [[454, 456], ["None"], "methods", ["None"], ["", "def", "flatparam", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.BernoulliProbabilityDistribution.mode": [[457, 459], ["tensorflow.round"], "methods", ["None"], ["", "def", "mode", "(", "self", ")", ":", "\n", "        ", "return", "tf", ".", "round", "(", "self", ".", "probabilities", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.BernoulliProbabilityDistribution.neglogp": [[460, 464], ["tensorflow.reduce_sum", "tensorflow.nn.sigmoid_cross_entropy_with_logits", "tensorflow.cast"], "methods", ["None"], ["", "def", "neglogp", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "tf", ".", "reduce_sum", "(", "tf", ".", "nn", ".", "sigmoid_cross_entropy_with_logits", "(", "logits", "=", "self", ".", "logits", ",", "\n", "labels", "=", "tf", ".", "cast", "(", "x", ",", "tf", ".", "float32", ")", ")", ",", "\n", "axis", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.BernoulliProbabilityDistribution.kl": [[465, 470], ["tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.nn.sigmoid_cross_entropy_with_logits", "tensorflow.nn.sigmoid_cross_entropy_with_logits"], "methods", ["None"], ["", "def", "kl", "(", "self", ",", "other", ")", ":", "\n", "        ", "return", "tf", ".", "reduce_sum", "(", "tf", ".", "nn", ".", "sigmoid_cross_entropy_with_logits", "(", "logits", "=", "other", ".", "logits", ",", "\n", "labels", "=", "self", ".", "probabilities", ")", ",", "axis", "=", "-", "1", ")", "-", "tf", ".", "reduce_sum", "(", "tf", ".", "nn", ".", "sigmoid_cross_entropy_with_logits", "(", "logits", "=", "self", ".", "logits", ",", "\n", "labels", "=", "self", ".", "probabilities", ")", ",", "axis", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.BernoulliProbabilityDistribution.entropy": [[471, 474], ["tensorflow.reduce_sum", "tensorflow.nn.sigmoid_cross_entropy_with_logits"], "methods", ["None"], ["", "def", "entropy", "(", "self", ")", ":", "\n", "        ", "return", "tf", ".", "reduce_sum", "(", "tf", ".", "nn", ".", "sigmoid_cross_entropy_with_logits", "(", "logits", "=", "self", ".", "logits", ",", "\n", "labels", "=", "self", ".", "probabilities", ")", ",", "axis", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.BernoulliProbabilityDistribution.sample": [[475, 478], ["tensorflow.random.uniform", "tensorflow.cast", "tensorflow.shape", "tensorflow.python.ops.math_ops.less"], "methods", ["None"], ["", "def", "sample", "(", "self", ")", ":", "\n", "        ", "samples_from_uniform", "=", "tf", ".", "random", ".", "uniform", "(", "tf", ".", "shape", "(", "self", ".", "probabilities", ")", ")", "\n", "return", "tf", ".", "cast", "(", "math_ops", ".", "less", "(", "samples_from_uniform", ",", "self", ".", "probabilities", ")", ",", "tf", ".", "float32", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.BernoulliProbabilityDistribution.fromflat": [[479, 488], ["cls"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "fromflat", "(", "cls", ",", "flat", ")", ":", "\n", "        ", "\"\"\"\n        Create an instance of this from new Bernoulli input\n\n        :param flat: ([float]) the Bernoulli input data\n        :return: (ProbabilityDistribution) the instance from the given Bernoulli input data\n        \"\"\"", "\n", "return", "cls", "(", "flat", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.make_proba_dist_type": [[490, 510], ["isinstance", "distributions.DiagGaussianProbabilityDistributionType", "isinstance", "len", "distributions.CategoricalProbabilityDistributionType", "isinstance", "distributions.MultiCategoricalProbabilityDistributionType", "isinstance", "distributions.BernoulliProbabilityDistributionType", "NotImplementedError", "type"], "function", ["None"], ["", "", "def", "make_proba_dist_type", "(", "ac_space", ")", ":", "\n", "    ", "\"\"\"\n    return an instance of ProbabilityDistributionType for the correct type of action space\n\n    :param ac_space: (Gym Space) the input action space\n    :return: (ProbabilityDistributionType) the appropriate instance of a ProbabilityDistributionType\n    \"\"\"", "\n", "if", "isinstance", "(", "ac_space", ",", "spaces", ".", "Box", ")", ":", "\n", "        ", "assert", "len", "(", "ac_space", ".", "shape", ")", "==", "1", ",", "\"Error: the action space must be a vector\"", "\n", "return", "DiagGaussianProbabilityDistributionType", "(", "ac_space", ".", "shape", "[", "0", "]", ")", "\n", "", "elif", "isinstance", "(", "ac_space", ",", "spaces", ".", "Discrete", ")", ":", "\n", "        ", "return", "CategoricalProbabilityDistributionType", "(", "ac_space", ".", "n", ")", "\n", "", "elif", "isinstance", "(", "ac_space", ",", "spaces", ".", "MultiDiscrete", ")", ":", "\n", "        ", "return", "MultiCategoricalProbabilityDistributionType", "(", "ac_space", ".", "nvec", ")", "\n", "", "elif", "isinstance", "(", "ac_space", ",", "spaces", ".", "MultiBinary", ")", ":", "\n", "        ", "return", "BernoulliProbabilityDistributionType", "(", "ac_space", ".", "n", ")", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\"Error: probability distribution, not implemented for action space of type {}.\"", "\n", ".", "format", "(", "type", "(", "ac_space", ")", ")", "+", "\n", "\" Must be of type Gym Spaces: Box, Discrete, MultiDiscrete or MultiBinary.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.distributions.shape_el": [[512, 525], ["tensor.get_shape", "tensorflow.shape"], "function", ["None"], ["", "", "def", "shape_el", "(", "tensor", ",", "index", ")", ":", "\n", "    ", "\"\"\"\n    get the shape of a TensorFlow Tensor element\n\n    :param tensor: (TensorFlow Tensor) the input tensor\n    :param index: (int) the element\n    :return: ([int]) the shape\n    \"\"\"", "\n", "maybe", "=", "tensor", ".", "get_shape", "(", ")", "[", "index", "]", "\n", "if", "maybe", "is", "not", "None", ":", "\n", "        ", "return", "maybe", "\n", "", "else", ":", "\n", "        ", "return", "tf", ".", "shape", "(", "tensor", ")", "[", "index", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util._Function.__init__": [[346, 366], ["tensorflow.group", "list", "hasattr", "isinstance", "len"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "inputs", ",", "outputs", ",", "updates", ",", "givens", ")", ":", "\n", "        ", "\"\"\"\n        Theano like function\n\n        :param inputs: (TensorFlow Tensor or Object with make_feed_dict) list of input arguments\n        :param outputs: (TensorFlow Tensor) list of outputs or a single output to be returned from function. Returned\n            value will also have the same shape.\n        :param updates: ([tf.Operation] or tf.Operation)\n        list of update functions or single update function that will be run whenever\n        the function is called. The return is ignored.\n        :param givens: (dict) the values known for the output\n        \"\"\"", "\n", "for", "inpt", "in", "inputs", ":", "\n", "            ", "if", "not", "hasattr", "(", "inpt", ",", "'make_feed_dict'", ")", "and", "not", "(", "isinstance", "(", "inpt", ",", "tf", ".", "Tensor", ")", "and", "len", "(", "inpt", ".", "op", ".", "inputs", ")", "==", "0", ")", ":", "\n", "                ", "assert", "False", ",", "\"inputs should all be placeholders, constants, or have a make_feed_dict method\"", "\n", "", "", "self", ".", "inputs", "=", "inputs", "\n", "updates", "=", "updates", "or", "[", "]", "\n", "self", ".", "update_group", "=", "tf", ".", "group", "(", "*", "updates", ")", "\n", "self", ".", "outputs_update", "=", "list", "(", "outputs", ")", "+", "[", "self", ".", "update_group", "]", "\n", "self", ".", "givens", "=", "{", "}", "if", "givens", "is", "None", "else", "givens", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util._Function._feed_input": [[367, 373], ["hasattr", "feed_dict.update", "inpt.make_feed_dict"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.math_util.RunningMeanStd.update"], ["", "@", "classmethod", "\n", "def", "_feed_input", "(", "cls", ",", "feed_dict", ",", "inpt", ",", "value", ")", ":", "\n", "        ", "if", "hasattr", "(", "inpt", ",", "'make_feed_dict'", ")", ":", "\n", "            ", "feed_dict", ".", "update", "(", "inpt", ".", "make_feed_dict", "(", "value", ")", ")", "\n", "", "else", ":", "\n", "            ", "feed_dict", "[", "inpt", "]", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util._Function.__call__": [[374, 387], ["zip", "len", "len", "tensorflow.get_default_session", "tf_util._Function._feed_input", "feed_dict.get", "tensorflow.get_default_session.run"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util._Function._feed_input", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.runners.AbstractEnvRunner.run"], ["", "", "def", "__call__", "(", "self", ",", "*", "args", ",", "sess", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "assert", "len", "(", "args", ")", "<=", "len", "(", "self", ".", "inputs", ")", ",", "\"Too many arguments provided\"", "\n", "if", "sess", "is", "None", ":", "\n", "            ", "sess", "=", "tf", ".", "get_default_session", "(", ")", "\n", "", "feed_dict", "=", "{", "}", "\n", "# Update the args", "\n", "for", "inpt", ",", "value", "in", "zip", "(", "self", ".", "inputs", ",", "args", ")", ":", "\n", "            ", "self", ".", "_feed_input", "(", "feed_dict", ",", "inpt", ",", "value", ")", "\n", "# Update feed dict with givens.", "\n", "", "for", "inpt", "in", "self", ".", "givens", ":", "\n", "            ", "feed_dict", "[", "inpt", "]", "=", "feed_dict", ".", "get", "(", "inpt", ",", "self", ".", "givens", "[", "inpt", "]", ")", "\n", "", "results", "=", "sess", ".", "run", "(", "self", ".", "outputs_update", ",", "feed_dict", "=", "feed_dict", ",", "**", "kwargs", ")", "[", ":", "-", "1", "]", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util.SetFromFlat.__init__": [[446, 466], ["list", "numpy.sum", "tensorflow.placeholder", "zip", "tensorflow.group", "map", "tf_util.intprod", "assigns.append", "tf_util.intprod", "tensorflow.assign", "tensorflow.reshape"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util.intprod", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util.intprod"], ["    ", "def", "__init__", "(", "self", ",", "var_list", ",", "dtype", "=", "tf", ".", "float32", ",", "sess", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Set the parameters from a flat vector\n\n        :param var_list: ([TensorFlow Tensor]) the variables\n        :param dtype: (type) the type for the placeholder\n        :param sess: (TensorFlow Session)\n        \"\"\"", "\n", "shapes", "=", "list", "(", "map", "(", "var_shape", ",", "var_list", ")", ")", "\n", "total_size", "=", "np", ".", "sum", "(", "[", "intprod", "(", "shape", ")", "for", "shape", "in", "shapes", "]", ")", "\n", "\n", "self", ".", "theta", "=", "theta", "=", "tf", ".", "placeholder", "(", "dtype", ",", "[", "total_size", "]", ")", "\n", "start", "=", "0", "\n", "assigns", "=", "[", "]", "\n", "for", "(", "shape", ",", "_var", ")", "in", "zip", "(", "shapes", ",", "var_list", ")", ":", "\n", "            ", "size", "=", "intprod", "(", "shape", ")", "\n", "assigns", ".", "append", "(", "tf", ".", "assign", "(", "_var", ",", "tf", ".", "reshape", "(", "theta", "[", "start", ":", "start", "+", "size", "]", ",", "shape", ")", ")", ")", "\n", "start", "+=", "size", "\n", "", "self", ".", "operation", "=", "tf", ".", "group", "(", "*", "assigns", ")", "\n", "self", ".", "sess", "=", "sess", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util.SetFromFlat.__call__": [[467, 472], ["tensorflow.get_default_session().run", "tf_util.SetFromFlat.sess.run", "tensorflow.get_default_session"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.runners.AbstractEnvRunner.run", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.runners.AbstractEnvRunner.run"], ["", "def", "__call__", "(", "self", ",", "theta", ")", ":", "\n", "        ", "if", "self", ".", "sess", "is", "None", ":", "\n", "            ", "return", "tf", ".", "get_default_session", "(", ")", ".", "run", "(", "self", ".", "operation", ",", "feed_dict", "=", "{", "self", ".", "theta", ":", "theta", "}", ")", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "sess", ".", "run", "(", "self", ".", "operation", ",", "feed_dict", "=", "{", "self", ".", "theta", ":", "theta", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util.GetFlat.__init__": [[475, 484], ["tensorflow.concat", "tensorflow.reshape", "tf_util.numel"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util.numel"], ["    ", "def", "__init__", "(", "self", ",", "var_list", ",", "sess", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Get the parameters as a flat vector\n\n        :param var_list: ([TensorFlow Tensor]) the variables\n        :param sess: (TensorFlow Session)\n        \"\"\"", "\n", "self", ".", "operation", "=", "tf", ".", "concat", "(", "axis", "=", "0", ",", "values", "=", "[", "tf", ".", "reshape", "(", "v", ",", "[", "numel", "(", "v", ")", "]", ")", "for", "v", "in", "var_list", "]", ")", "\n", "self", ".", "sess", "=", "sess", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util.GetFlat.__call__": [[485, 490], ["tensorflow.get_default_session().run", "tf_util.GetFlat.sess.run", "tensorflow.get_default_session"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.runners.AbstractEnvRunner.run", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.runners.AbstractEnvRunner.run"], ["", "def", "__call__", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "sess", "is", "None", ":", "\n", "            ", "return", "tf", ".", "get_default_session", "(", ")", ".", "run", "(", "self", ".", "operation", ")", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "sess", ".", "run", "(", "self", ".", "operation", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util.is_image": [[11, 22], ["len"], "function", ["None"], ["def", "is_image", "(", "tensor", ")", ":", "\n", "    ", "\"\"\"\n    Check if a tensor has the shape of\n    a valid image for tensorboard logging.\n    Valid image: RGB, RGBD, GrayScale\n\n    :param tensor: (np.ndarray or tf.placeholder)\n    :return: (bool)\n    \"\"\"", "\n", "\n", "return", "len", "(", "tensor", ".", "shape", ")", "==", "3", "and", "tensor", ".", "shape", "[", "-", "1", "]", "in", "[", "1", ",", "3", ",", "4", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util.linear_schedule": [[24, 43], ["isinstance", "float"], "function", ["None"], ["", "def", "linear_schedule", "(", "initial_value", ")", ":", "\n", "    ", "\"\"\"\n    Linear learning rate schedule.\n\n    :param initial_value: (float or str)\n    :return: (function)\n    \"\"\"", "\n", "if", "isinstance", "(", "initial_value", ",", "str", ")", ":", "\n", "        ", "initial_value", "=", "float", "(", "initial_value", ")", "\n", "\n", "", "def", "func", "(", "progress", ")", ":", "\n", "        ", "\"\"\"\n        Progress will decrease from 1 (beginning) to 0\n        :param progress: (float)\n        :return: (float)\n        \"\"\"", "\n", "return", "progress", "*", "initial_value", "\n", "\n", "", "return", "func", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util.constfn": [[45, 58], ["None"], "function", ["None"], ["", "def", "constfn", "(", "val", ")", ":", "\n", "    ", "\"\"\"\n    Create a function that returns a constant\n    It is useful for learning rate schedule (to avoid code duplication)\n\n    :param val: (float)\n    :return: (function)\n    \"\"\"", "\n", "\n", "def", "func", "(", "_", ")", ":", "\n", "        ", "return", "val", "\n", "\n", "", "return", "func", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util.get_schedule_fn": [[60, 76], ["isinstance", "tf_util.constfn", "callable", "float"], "function", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util.constfn"], ["", "def", "get_schedule_fn", "(", "value_schedule", ")", ":", "\n", "    ", "\"\"\"\n    Transform (if needed) learning rate and clip range\n    to callable.\n\n    :param value_schedule: (callable or float)\n    :return: (function)\n    \"\"\"", "\n", "# If the passed schedule is a float", "\n", "# create a constant function", "\n", "if", "isinstance", "(", "value_schedule", ",", "(", "float", ",", "int", ")", ")", ":", "\n", "# Cast to float to avoid errors", "\n", "        ", "value_schedule", "=", "constfn", "(", "float", "(", "value_schedule", ")", ")", "\n", "", "else", ":", "\n", "        ", "assert", "callable", "(", "value_schedule", ")", "\n", "", "return", "value_schedule", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util.batch_to_seq": [[78, 93], ["tensorflow.reshape", "tensorflow.reshape", "tensorflow.squeeze", "tensorflow.split"], "function", ["None"], ["", "def", "batch_to_seq", "(", "tensor_batch", ",", "n_batch", ",", "n_steps", ",", "flat", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Transform a batch of Tensors, into a sequence of Tensors for recurrent policies\n\n    :param tensor_batch: (TensorFlow Tensor) The input tensor to unroll\n    :param n_batch: (int) The number of batch to run (n_envs * n_steps)\n    :param n_steps: (int) The number of steps to run for each environment\n    :param flat: (bool) If the input Tensor is flat\n    :return: (TensorFlow Tensor) sequence of Tensors for recurrent policies\n    \"\"\"", "\n", "if", "flat", ":", "\n", "        ", "tensor_batch", "=", "tf", ".", "reshape", "(", "tensor_batch", ",", "[", "n_batch", ",", "n_steps", "]", ")", "\n", "", "else", ":", "\n", "        ", "tensor_batch", "=", "tf", ".", "reshape", "(", "tensor_batch", ",", "[", "n_batch", ",", "n_steps", ",", "-", "1", "]", ")", "\n", "", "return", "[", "tf", ".", "squeeze", "(", "v", ",", "[", "1", "]", ")", "for", "v", "in", "tf", ".", "split", "(", "axis", "=", "1", ",", "num_or_size_splits", "=", "n_steps", ",", "value", "=", "tensor_batch", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util.seq_to_batch": [[95, 110], ["tensor_sequence[].get_shape().as_list", "tensorflow.reshape", "tensorflow.reshape", "tensor_sequence[].get_shape", "len", "tensorflow.concat", "tensorflow.stack", "tensor_sequence[].get_shape"], "function", ["None"], ["", "def", "seq_to_batch", "(", "tensor_sequence", ",", "flat", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Transform a sequence of Tensors, into a batch of Tensors for recurrent policies\n\n    :param tensor_sequence: (TensorFlow Tensor) The input tensor to batch\n    :param flat: (bool) If the input Tensor is flat\n    :return: (TensorFlow Tensor) batch of Tensors for recurrent policies\n    \"\"\"", "\n", "shape", "=", "tensor_sequence", "[", "0", "]", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "\n", "if", "not", "flat", ":", "\n", "        ", "assert", "len", "(", "shape", ")", ">", "1", "\n", "n_hidden", "=", "tensor_sequence", "[", "0", "]", ".", "get_shape", "(", ")", "[", "-", "1", "]", ".", "value", "\n", "return", "tf", ".", "reshape", "(", "tf", ".", "concat", "(", "axis", "=", "1", ",", "values", "=", "tensor_sequence", ")", ",", "[", "-", "1", ",", "n_hidden", "]", ")", "\n", "", "else", ":", "\n", "        ", "return", "tf", ".", "reshape", "(", "tf", ".", "stack", "(", "values", "=", "tensor_sequence", ",", "axis", "=", "1", ")", ",", "[", "-", "1", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util.check_shape": [[112, 123], ["zip", "tensor.get_shape().as_list", "str", "str", "tensor.get_shape", "tensor.get_shape", "str"], "function", ["None"], ["", "", "def", "check_shape", "(", "tensors", ",", "shapes", ")", ":", "\n", "    ", "\"\"\"\n    Verifies the tensors match the given shape, will raise an error if the shapes do not match\n\n    :param tensors: ([TensorFlow Tensor]) The tensors that should be checked\n    :param shapes: ([list]) The list of shapes for each tensor\n    \"\"\"", "\n", "i", "=", "0", "\n", "for", "(", "tensor", ",", "shape", ")", "in", "zip", "(", "tensors", ",", "shapes", ")", ":", "\n", "        ", "assert", "tensor", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "==", "shape", ",", "\"id \"", "+", "str", "(", "i", ")", "+", "\" shape \"", "+", "str", "(", "tensor", ".", "get_shape", "(", ")", ")", "+", "str", "(", "shape", ")", "\n", "i", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util.huber_loss": [[130, 142], ["tensorflow.where", "tensorflow.abs", "tensorflow.square", "tensorflow.abs"], "function", ["None"], ["", "", "def", "huber_loss", "(", "tensor", ",", "delta", "=", "1.0", ")", ":", "\n", "    ", "\"\"\"\n    Reference: https://en.wikipedia.org/wiki/Huber_loss\n\n    :param tensor: (TensorFlow Tensor) the input value\n    :param delta: (float) Huber loss delta value\n    :return: (TensorFlow Tensor) Huber loss output\n    \"\"\"", "\n", "return", "tf", ".", "where", "(", "\n", "tf", ".", "abs", "(", "tensor", ")", "<", "delta", ",", "\n", "tf", ".", "square", "(", "tensor", ")", "*", "0.5", ",", "\n", "delta", "*", "(", "tf", ".", "abs", "(", "tensor", ")", "-", "0.5", "*", "delta", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util.sample": [[145, 156], ["tensorflow.random_uniform", "tensorflow.argmax", "tensorflow.shape", "tensorflow.log", "tensorflow.log"], "function", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.log", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.log"], ["", "def", "sample", "(", "logits", ")", ":", "\n", "    ", "\"\"\"\n    Creates a sampling Tensor for non deterministic policies\n    when using categorical distribution.\n    It uses the Gumbel-max trick: http://amid.fish/humble-gumbel\n\n    :param logits: (TensorFlow Tensor) The input probability for each action\n    :return: (TensorFlow Tensor) The sampled action\n    \"\"\"", "\n", "noise", "=", "tf", ".", "random_uniform", "(", "tf", ".", "shape", "(", "logits", ")", ")", "\n", "return", "tf", ".", "argmax", "(", "logits", "-", "tf", ".", "log", "(", "-", "tf", ".", "log", "(", "noise", ")", ")", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util.calc_entropy": [[158, 171], ["tensorflow.exp", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.reduce_max", "tensorflow.log"], "function", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.log"], ["", "def", "calc_entropy", "(", "logits", ")", ":", "\n", "    ", "\"\"\"\n    Calculates the entropy of the output values of the network\n\n    :param logits: (TensorFlow Tensor) The input probability for each action\n    :return: (TensorFlow Tensor) The Entropy of the output values of the network\n    \"\"\"", "\n", "# Compute softmax", "\n", "a_0", "=", "logits", "-", "tf", ".", "reduce_max", "(", "logits", ",", "1", ",", "keepdims", "=", "True", ")", "\n", "exp_a_0", "=", "tf", ".", "exp", "(", "a_0", ")", "\n", "z_0", "=", "tf", ".", "reduce_sum", "(", "exp_a_0", ",", "1", ",", "keepdims", "=", "True", ")", "\n", "p_0", "=", "exp_a_0", "/", "z_0", "\n", "return", "tf", ".", "reduce_sum", "(", "p_0", "*", "(", "tf", ".", "log", "(", "z_0", ")", "-", "a_0", ")", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util.mse": [[173, 182], ["tensorflow.reduce_mean", "tensorflow.square"], "function", ["None"], ["", "def", "mse", "(", "pred", ",", "target", ")", ":", "\n", "    ", "\"\"\"\n    Returns the Mean squared error between prediction and target\n\n    :param pred: (TensorFlow Tensor) The predicted value\n    :param target: (TensorFlow Tensor) The target value\n    :return: (TensorFlow Tensor) The Mean squared error between prediction and target\n    \"\"\"", "\n", "return", "tf", ".", "reduce_mean", "(", "tf", ".", "square", "(", "pred", "-", "target", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util.avg_norm": [[184, 192], ["tensorflow.reduce_mean", "tensorflow.sqrt", "tensorflow.reduce_sum", "tensorflow.square"], "function", ["None"], ["", "def", "avg_norm", "(", "tensor", ")", ":", "\n", "    ", "\"\"\"\n    Return an average of the L2 normalization of the batch\n\n    :param tensor: (TensorFlow Tensor) The input tensor\n    :return: (TensorFlow Tensor) Average L2 normalization of the batch\n    \"\"\"", "\n", "return", "tf", ".", "reduce_mean", "(", "tf", ".", "sqrt", "(", "tf", ".", "reduce_sum", "(", "tf", ".", "square", "(", "tensor", ")", ",", "axis", "=", "-", "1", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util.gradient_add": [[194, 214], ["print"], "function", ["None"], ["", "def", "gradient_add", "(", "grad_1", ",", "grad_2", ",", "param", ",", "verbose", "=", "0", ")", ":", "\n", "    ", "\"\"\"\n    Sum two gradients\n\n    :param grad_1: (TensorFlow Tensor) The first gradient\n    :param grad_2: (TensorFlow Tensor) The second gradient\n    :param param: (TensorFlow parameters) The trainable parameters\n    :param verbose: (int) verbosity level\n    :return: (TensorFlow Tensor) the sum of the gradients\n    \"\"\"", "\n", "if", "verbose", ">", "1", ":", "\n", "        ", "print", "(", "[", "grad_1", ",", "grad_2", ",", "param", ".", "name", "]", ")", "\n", "", "if", "grad_1", "is", "None", "and", "grad_2", "is", "None", ":", "\n", "        ", "return", "None", "\n", "", "elif", "grad_1", "is", "None", ":", "\n", "        ", "return", "grad_2", "\n", "", "elif", "grad_2", "is", "None", ":", "\n", "        ", "return", "grad_1", "\n", "", "else", ":", "\n", "        ", "return", "grad_1", "+", "grad_2", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util.q_explained_variance": [[216, 228], ["tensorflow.nn.moments", "tensorflow.nn.moments", "tf_util.check_shape"], "function", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util.check_shape"], ["", "", "def", "q_explained_variance", "(", "q_pred", ",", "q_true", ")", ":", "\n", "    ", "\"\"\"\n    Calculates the explained variance of the Q value\n\n    :param q_pred: (TensorFlow Tensor) The predicted Q value\n    :param q_true: (TensorFlow Tensor) The expected Q value\n    :return: (TensorFlow Tensor) the explained variance of the Q value\n    \"\"\"", "\n", "_", ",", "var_y", "=", "tf", ".", "nn", ".", "moments", "(", "q_true", ",", "axes", "=", "[", "0", ",", "1", "]", ")", "\n", "_", ",", "var_pred", "=", "tf", ".", "nn", ".", "moments", "(", "q_true", "-", "q_pred", ",", "axes", "=", "[", "0", ",", "1", "]", ")", "\n", "check_shape", "(", "[", "var_y", ",", "var_pred", "]", ",", "[", "[", "]", "]", "*", "2", ")", "\n", "return", "1.0", "-", "(", "var_pred", "/", "var_y", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util.make_session": [[235, 256], ["tensorflow.compat.v1.ConfigProto", "int", "tensorflow.InteractiveSession", "tensorflow.compat.v1.Session", "os.getenv", "multiprocessing.cpu_count"], "function", ["None"], ["", "def", "make_session", "(", "num_cpu", "=", "None", ",", "make_default", "=", "False", ",", "graph", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Returns a session that will use <num_cpu> CPU's only\n\n    :param num_cpu: (int) number of CPUs to use for TensorFlow\n    :param make_default: (bool) if this should return an InteractiveSession or a normal Session\n    :param graph: (TensorFlow Graph) the graph of the session\n    :return: (TensorFlow session)\n    \"\"\"", "\n", "if", "num_cpu", "is", "None", ":", "\n", "        ", "num_cpu", "=", "int", "(", "os", ".", "getenv", "(", "'RCALL_NUM_CPU'", ",", "multiprocessing", ".", "cpu_count", "(", ")", ")", ")", "\n", "", "tf_config", "=", "tf", ".", "compat", ".", "v1", ".", "ConfigProto", "(", "\n", "allow_soft_placement", "=", "True", ",", "\n", "inter_op_parallelism_threads", "=", "num_cpu", ",", "\n", "intra_op_parallelism_threads", "=", "num_cpu", ")", "\n", "# Prevent tensorflow from taking all the gpu memory", "\n", "tf_config", ".", "gpu_options", ".", "allow_growth", "=", "True", "\n", "if", "make_default", ":", "\n", "        ", "return", "tf", ".", "InteractiveSession", "(", "config", "=", "tf_config", ",", "graph", "=", "graph", ")", "\n", "", "else", ":", "\n", "        ", "return", "tf", ".", "compat", ".", "v1", ".", "Session", "(", "config", "=", "tf_config", ",", "graph", "=", "graph", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util.single_threaded_session": [[258, 267], ["tf_util.make_session"], "function", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util.make_session"], ["", "", "def", "single_threaded_session", "(", "make_default", "=", "False", ",", "graph", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Returns a session which will only use a single CPU\n\n    :param make_default: (bool) if this should return an InteractiveSession or a normal Session\n    :param graph: (TensorFlow Graph) the graph of the session\n    :return: (TensorFlow session)\n    \"\"\"", "\n", "return", "make_session", "(", "num_cpu", "=", "1", ",", "make_default", "=", "make_default", ",", "graph", "=", "graph", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util.in_session": [[269, 283], ["functools.wraps", "tensorflow.Session", "tf_util.linear_schedule.func"], "function", ["None"], ["", "def", "in_session", "(", "func", ")", ":", "\n", "    ", "\"\"\"\n    Wraps a function so that it is in a TensorFlow Session\n\n    :param func: (function) the function to wrap\n    :return: (function)\n    \"\"\"", "\n", "\n", "@", "functools", ".", "wraps", "(", "func", ")", "\n", "def", "newfunc", "(", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "with", "tf", ".", "Session", "(", ")", ":", "\n", "            ", "func", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n", "", "", "return", "newfunc", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util.initialize": [[288, 299], ["tf.get_default_session.run", "ALREADY_INITIALIZED.update", "tensorflow.get_default_session", "set", "tensorflow.variables_initializer", "tensorflow.global_variables"], "function", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.runners.AbstractEnvRunner.run", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.math_util.RunningMeanStd.update"], ["def", "initialize", "(", "sess", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Initialize all the uninitialized variables in the global scope.\n\n    :param sess: (TensorFlow Session)\n    \"\"\"", "\n", "if", "sess", "is", "None", ":", "\n", "        ", "sess", "=", "tf", ".", "get_default_session", "(", ")", "\n", "", "new_variables", "=", "set", "(", "tf", ".", "global_variables", "(", ")", ")", "-", "ALREADY_INITIALIZED", "\n", "sess", ".", "run", "(", "tf", ".", "variables_initializer", "(", "new_variables", ")", ")", "\n", "ALREADY_INITIALIZED", ".", "update", "(", "new_variables", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util.function": [[306, 343], ["isinstance", "tf_util._Function", "isinstance", "tf_util._Function", "tf_util._Function", "outputs.values", "type", "zip", "_Function.", "outputs.keys", "_Function."], "function", ["None"], ["", "def", "function", "(", "inputs", ",", "outputs", ",", "updates", "=", "None", ",", "givens", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Take a bunch of tensorflow placeholders and expressions\n    computed based on those placeholders and produces f(inputs) -> outputs. Function f takes\n    values to be fed to the input's placeholders and produces the values of the expressions\n    in outputs. Just like a Theano function.\n\n    Input values can be passed in the same order as inputs or can be provided as kwargs based\n    on placeholder name (passed to constructor or accessible via placeholder.op.name).\n\n    Example:\n       >>> x = tf.placeholder(tf.int32, (), name=\"x\")\n       >>> y = tf.placeholder(tf.int32, (), name=\"y\")\n       >>> z = 3 * x + 2 * y\n       >>> lin = function([x, y], z, givens={y: 0})\n       >>> with single_threaded_session():\n       >>>     initialize()\n       >>>     assert lin(2) == 6\n       >>>     assert lin(x=3) == 9\n       >>>     assert lin(2, 2) == 10\n\n    :param inputs: (TensorFlow Tensor or Object with make_feed_dict) list of input arguments\n    :param outputs: (TensorFlow Tensor) list of outputs or a single output to be returned from function. Returned\n        value will also have the same shape.\n    :param updates: ([tf.Operation] or tf.Operation)\n        list of update functions or single update function that will be run whenever\n        the function is called. The return is ignored.\n    :param givens: (dict) the values known for the output\n    \"\"\"", "\n", "if", "isinstance", "(", "outputs", ",", "list", ")", ":", "\n", "        ", "return", "_Function", "(", "inputs", ",", "outputs", ",", "updates", ",", "givens", "=", "givens", ")", "\n", "", "elif", "isinstance", "(", "outputs", ",", "(", "dict", ",", "collections", ".", "OrderedDict", ")", ")", ":", "\n", "        ", "func", "=", "_Function", "(", "inputs", ",", "outputs", ".", "values", "(", ")", ",", "updates", ",", "givens", "=", "givens", ")", "\n", "return", "lambda", "*", "args", ",", "**", "kwargs", ":", "type", "(", "outputs", ")", "(", "zip", "(", "outputs", ".", "keys", "(", ")", ",", "func", "(", "*", "args", ",", "**", "kwargs", ")", ")", ")", "\n", "", "else", ":", "\n", "        ", "func", "=", "_Function", "(", "inputs", ",", "[", "outputs", "]", ",", "updates", ",", "givens", "=", "givens", ")", "\n", "return", "lambda", "*", "args", ",", "**", "kwargs", ":", "func", "(", "*", "args", ",", "**", "kwargs", ")", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util.var_shape": [[394, 405], ["tensor.get_shape().as_list", "all", "tensor.get_shape", "isinstance"], "function", ["None"], ["", "", "def", "var_shape", "(", "tensor", ")", ":", "\n", "    ", "\"\"\"\n    get TensorFlow Tensor shape\n\n    :param tensor: (TensorFlow Tensor) the input tensor\n    :return: ([int]) the shape\n    \"\"\"", "\n", "out", "=", "tensor", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "\n", "assert", "all", "(", "isinstance", "(", "a", ",", "int", ")", "for", "a", "in", "out", ")", ",", "\"shape function assumes that shape is fully known\"", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util.numel": [[407, 415], ["tf_util.intprod", "tf_util.var_shape"], "function", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util.intprod", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util.var_shape"], ["", "def", "numel", "(", "tensor", ")", ":", "\n", "    ", "\"\"\"\n    get TensorFlow Tensor's number of elements\n\n    :param tensor: (TensorFlow Tensor) the input tensor\n    :return: (int) the number of elements\n    \"\"\"", "\n", "return", "intprod", "(", "var_shape", "(", "tensor", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util.intprod": [[417, 425], ["int", "numpy.prod"], "function", ["None"], ["", "def", "intprod", "(", "tensor", ")", ":", "\n", "    ", "\"\"\"\n    calculates the product of all the elements in a list\n\n    :param tensor: ([Number]) the list of elements\n    :return: (int) the product truncated\n    \"\"\"", "\n", "return", "int", "(", "np", ".", "prod", "(", "tensor", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util.flatgrad": [[427, 442], ["tensorflow.gradients", "tensorflow.concat", "tensorflow.clip_by_norm", "tensorflow.reshape", "zip", "tensorflow.zeros_like", "tf_util.numel"], "function", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util.numel"], ["", "def", "flatgrad", "(", "loss", ",", "var_list", ",", "clip_norm", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    calculates the gradient and flattens it\n\n    :param loss: (float) the loss value\n    :param var_list: ([TensorFlow Tensor]) the variables\n    :param clip_norm: (float) clip the gradients (disabled if None)\n    :return: ([TensorFlow Tensor]) flattened gradient\n    \"\"\"", "\n", "grads", "=", "tf", ".", "gradients", "(", "loss", ",", "var_list", ")", "\n", "if", "clip_norm", "is", "not", "None", ":", "\n", "        ", "grads", "=", "[", "tf", ".", "clip_by_norm", "(", "grad", ",", "clip_norm", "=", "clip_norm", ")", "for", "grad", "in", "grads", "]", "\n", "", "return", "tf", ".", "concat", "(", "axis", "=", "0", ",", "values", "=", "[", "\n", "tf", ".", "reshape", "(", "grad", "if", "grad", "is", "not", "None", "else", "tf", ".", "zeros_like", "(", "v", ")", ",", "[", "numel", "(", "v", ")", "]", ")", "\n", "for", "(", "v", ",", "grad", ")", "in", "zip", "(", "var_list", ",", "grads", ")", "\n", "]", ")", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util.get_trainable_vars": [[497, 505], ["tensorflow.compat.v1.get_collection"], "function", ["None"], ["", "", "", "def", "get_trainable_vars", "(", "name", ")", ":", "\n", "    ", "\"\"\"\n    returns the trainable variables\n\n    :param name: (str) the scope\n    :return: ([TensorFlow Variable])\n    \"\"\"", "\n", "return", "tf", ".", "compat", ".", "v1", ".", "get_collection", "(", "tf", ".", "compat", ".", "v1", ".", "GraphKeys", ".", "TRAINABLE_VARIABLES", ",", "scope", "=", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util.get_globals_vars": [[507, 515], ["tensorflow.compat.v1.get_collection"], "function", ["None"], ["", "def", "get_globals_vars", "(", "name", ")", ":", "\n", "    ", "\"\"\"\n    returns the trainable variables\n\n    :param name: (str) the scope\n    :return: ([TensorFlow Variable])\n    \"\"\"", "\n", "return", "tf", ".", "compat", ".", "v1", ".", "get_collection", "(", "tf", ".", "compat", ".", "v1", ".", "GraphKeys", ".", "GLOBAL_VARIABLES", ",", "scope", "=", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util.outer_scope_getter": [[517, 532], ["name.replace.replace", "getter"], "function", ["None"], ["", "def", "outer_scope_getter", "(", "scope", ",", "new_scope", "=", "\"\"", ")", ":", "\n", "    ", "\"\"\"\n    remove a scope layer for the getter\n\n    :param scope: (str) the layer to remove\n    :param new_scope: (str) optional replacement name\n    :return: (function (function, str, ``*args``, ``**kwargs``): Tensorflow Tensor)\n    \"\"\"", "\n", "\n", "def", "_getter", "(", "getter", ",", "name", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "name", "=", "name", ".", "replace", "(", "scope", "+", "\"/\"", ",", "new_scope", ",", "1", ")", "\n", "val", "=", "getter", "(", "name", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "return", "val", "\n", "\n", "", "return", "_getter", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util.total_episode_reward_logger": [[539, 568], ["tensorflow.compat.v1.variable_scope", "range", "numpy.sort", "numpy.argwhere", "len", "sum", "sum", "tensorflow.compat.v1.Summary", "writer.add_summary", "range", "sum", "len", "sum", "tensorflow.compat.v1.Summary", "writer.add_summary", "tensorflow.compat.v1.Summary.Value", "tensorflow.compat.v1.Summary.Value"], "function", ["None"], ["", "def", "total_episode_reward_logger", "(", "rew", ",", "rewards", ",", "masks", ",", "writer", ",", "steps", ")", ":", "\n", "    ", "\"\"\"\n    calculates the cumulated episode reward, and prints to tensorflow log the output\n\n    :param rew: (np.array float) the total running reward\n    :param rewards: (np.array float) the rewards\n    :param masks: (np.array bool) the end of episodes\n    :param writer: (TensorFlow Session.writer) the writer to log to\n    :param steps: (int) the current timestep\n    :return: (np.array float) the updated total running reward\n    :return: (np.array float) the updated total running reward\n    \"\"\"", "\n", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "\"environment_info\"", ",", "reuse", "=", "True", ")", ":", "\n", "        ", "for", "env_idx", "in", "range", "(", "rewards", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "dones_idx", "=", "np", ".", "sort", "(", "np", ".", "argwhere", "(", "masks", "[", "env_idx", "]", ")", ")", "\n", "\n", "if", "len", "(", "dones_idx", ")", "==", "0", ":", "\n", "                ", "rew", "[", "env_idx", "]", "+=", "sum", "(", "rewards", "[", "env_idx", "]", ")", "\n", "", "else", ":", "\n", "                ", "rew", "[", "env_idx", "]", "+=", "sum", "(", "rewards", "[", "env_idx", ",", ":", "dones_idx", "[", "0", ",", "0", "]", "]", ")", "\n", "summary", "=", "tf", ".", "compat", ".", "v1", ".", "Summary", "(", "value", "=", "[", "tf", ".", "compat", ".", "v1", ".", "Summary", ".", "Value", "(", "tag", "=", "\"episode_reward\"", ",", "simple_value", "=", "rew", "[", "env_idx", "]", ")", "]", ")", "\n", "writer", ".", "add_summary", "(", "summary", ",", "steps", "+", "dones_idx", "[", "0", ",", "0", "]", ")", "\n", "for", "k", "in", "range", "(", "1", ",", "len", "(", "dones_idx", "[", ":", ",", "0", "]", ")", ")", ":", "\n", "                    ", "rew", "[", "env_idx", "]", "=", "sum", "(", "rewards", "[", "env_idx", ",", "dones_idx", "[", "k", "-", "1", ",", "0", "]", ":", "dones_idx", "[", "k", ",", "0", "]", "]", ")", "\n", "summary", "=", "tf", ".", "compat", ".", "v1", ".", "Summary", "(", "value", "=", "[", "tf", ".", "compat", ".", "v1", ".", "Summary", ".", "Value", "(", "tag", "=", "\"episode_reward\"", ",", "simple_value", "=", "rew", "[", "env_idx", "]", ")", "]", ")", "\n", "writer", ".", "add_summary", "(", "summary", ",", "steps", "+", "dones_idx", "[", "k", ",", "0", "]", ")", "\n", "", "rew", "[", "env_idx", "]", "=", "sum", "(", "rewards", "[", "env_idx", ",", "dones_idx", "[", "-", "1", ",", "0", "]", ":", "]", ")", "\n", "\n", "", "", "", "return", "rew", "\n", "", ""]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.BaseRLModel.__init__": [[43, 97], ["core.vec_env.unwrap_vec_normalize", "isinstance", "core.policies.get_policy_from_name", "isinstance", "core.vec_env.all_vec_env.DummyVecEnv", "isinstance", "isinstance", "print", "core.vec_env.all_vec_env.DummyVecEnv", "print", "base_class._UnvecWrapper", "ValueError", "gym.make"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.__init__.unwrap_vec_normalize", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.policies.get_policy_from_name"], ["def", "__init__", "(", "self", ",", "policy", ",", "env", ",", "verbose", "=", "0", ",", "*", ",", "requires_vec_env", ",", "policy_base", ",", "\n", "policy_kwargs", "=", "None", ",", "seed", "=", "None", ",", "n_cpu_tf_sess", "=", "None", ")", ":", "\n", "        ", "if", "isinstance", "(", "policy", ",", "str", ")", "and", "policy_base", "is", "not", "None", ":", "\n", "            ", "self", ".", "policy", "=", "get_policy_from_name", "(", "policy_base", ",", "policy", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "policy", "=", "policy", "\n", "", "self", ".", "env", "=", "env", "\n", "self", ".", "verbose", "=", "verbose", "\n", "self", ".", "_requires_vec_env", "=", "requires_vec_env", "\n", "self", ".", "policy_kwargs", "=", "{", "}", "if", "policy_kwargs", "is", "None", "else", "policy_kwargs", "\n", "self", ".", "observation_space", "=", "None", "\n", "self", ".", "action_space", "=", "None", "\n", "self", ".", "n_envs", "=", "None", "\n", "self", ".", "_vectorize_action", "=", "False", "\n", "self", ".", "num_timesteps", "=", "0", "\n", "self", ".", "graph", "=", "None", "\n", "self", ".", "sess", "=", "None", "\n", "self", ".", "params", "=", "None", "\n", "self", ".", "seed", "=", "seed", "\n", "self", ".", "_param_load_ops", "=", "None", "\n", "self", ".", "n_cpu_tf_sess", "=", "n_cpu_tf_sess", "\n", "self", ".", "episode_reward", "=", "None", "\n", "self", ".", "ep_info_buf", "=", "None", "\n", "\n", "if", "env", "is", "not", "None", ":", "\n", "            ", "if", "isinstance", "(", "env", ",", "str", ")", ":", "\n", "                ", "if", "self", ".", "verbose", ">=", "1", ":", "\n", "                    ", "print", "(", "\"Creating environment from the given name, wrapped in a DummyVecEnv.\"", ")", "\n", "", "self", ".", "env", "=", "env", "=", "DummyVecEnv", "(", "[", "lambda", ":", "gym", ".", "make", "(", "env", ")", "]", ")", "\n", "\n", "", "self", ".", "observation_space", "=", "env", ".", "observation_space", "\n", "self", ".", "action_space", "=", "env", ".", "action_space", "\n", "if", "requires_vec_env", ":", "\n", "                ", "if", "isinstance", "(", "env", ",", "VecEnv", ")", ":", "\n", "                    ", "self", ".", "n_envs", "=", "env", ".", "num_envs", "\n", "", "else", ":", "\n", "# The model requires a VecEnv", "\n", "# wrap it in a DummyVecEnv to avoid error", "\n", "                    ", "self", ".", "env", "=", "DummyVecEnv", "(", "[", "lambda", ":", "env", "]", ")", "\n", "if", "self", ".", "verbose", ">=", "1", ":", "\n", "                        ", "print", "(", "\"Wrapping the env in a DummyVecEnv.\"", ")", "\n", "", "self", ".", "n_envs", "=", "1", "\n", "", "", "else", ":", "\n", "                ", "if", "isinstance", "(", "env", ",", "VecEnv", ")", ":", "\n", "                    ", "if", "env", ".", "num_envs", "==", "1", ":", "\n", "                        ", "self", ".", "env", "=", "_UnvecWrapper", "(", "env", ")", "\n", "self", ".", "_vectorize_action", "=", "True", "\n", "", "else", ":", "\n", "                        ", "raise", "ValueError", "(", "\"Error: the model requires a non vectorized environment or a single vectorized\"", "\n", "\" environment.\"", ")", "\n", "", "", "self", ".", "n_envs", "=", "1", "\n", "\n", "# Get VecNormalize object if it exists", "\n", "", "", "self", ".", "_vec_normalize_env", "=", "unwrap_vec_normalize", "(", "self", ".", "env", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.BaseRLModel.get_env": [[98, 105], ["None"], "methods", ["None"], ["", "def", "get_env", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        returns the current environment (can be None if not defined)\n\n        :return: (Gym Environment) The current environment\n        \"\"\"", "\n", "return", "self", ".", "env", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.BaseRLModel.get_vec_normalize_env": [[106, 114], ["None"], "methods", ["None"], ["", "def", "get_vec_normalize_env", "(", "self", ")", "->", "Optional", "[", "VecNormalize", "]", ":", "\n", "        ", "\"\"\"\n        Return the ``VecNormalize`` wrapper of the training env\n        if it exists.\n\n        :return: Optional[VecNormalize] The ``VecNormalize`` env.\n        \"\"\"", "\n", "return", "self", ".", "_vec_normalize_env", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.BaseRLModel.set_env": [[115, 163], ["core.vec_env.unwrap_vec_normalize", "isinstance", "isinstance", "print", "ValueError", "base_class._UnvecWrapper", "ValueError"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.__init__.unwrap_vec_normalize"], ["", "def", "set_env", "(", "self", ",", "env", ")", ":", "\n", "        ", "\"\"\"\n        Checks the validity of the environment, and if it is coherent, set it as the current environment.\n\n        :param env: (Gym Environment) The environment for learning a policy\n        \"\"\"", "\n", "if", "env", "is", "None", "and", "self", ".", "env", "is", "None", ":", "\n", "            ", "if", "self", ".", "verbose", ">=", "1", ":", "\n", "                ", "print", "(", "\"Loading a model without an environment, \"", "\n", "\"this model cannot be trained until it has a valid environment.\"", ")", "\n", "", "return", "\n", "", "elif", "env", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"Error: trying to replace the current environment with None\"", ")", "\n", "\n", "# sanity checking the environment", "\n", "", "assert", "self", ".", "observation_space", "==", "env", ".", "observation_space", ",", "\"Error: the environment passed must have at least the same observation space as the model was trained on.\"", "\n", "assert", "self", ".", "action_space", "==", "env", ".", "action_space", ",", "\"Error: the environment passed must have at least the same action space as the model was trained on.\"", "\n", "if", "self", ".", "_requires_vec_env", ":", "\n", "            ", "assert", "isinstance", "(", "env", ",", "VecEnv", ")", ",", "\"Error: the environment passed is not a vectorized environment, however {} requires it\"", ".", "format", "(", "\n", "self", ".", "__class__", ".", "__name__", ")", "\n", "assert", "not", "self", ".", "policy", ".", "recurrent", "or", "self", ".", "n_envs", "==", "env", ".", "num_envs", ",", "\"Error: the environment passed must have the same number of environments as the model was trained on.\"", "\"This is due to the Lstm policy not being capable of changing the number of environments.\"", "\n", "self", ".", "n_envs", "=", "env", ".", "num_envs", "\n", "", "else", ":", "\n", "# for models that dont want vectorized environment, check if they make sense and adapt them.", "\n", "# Otherwise tell the user about this issue", "\n", "            ", "if", "isinstance", "(", "env", ",", "VecEnv", ")", ":", "\n", "                ", "if", "env", ".", "num_envs", "==", "1", ":", "\n", "                    ", "env", "=", "_UnvecWrapper", "(", "env", ")", "\n", "self", ".", "_vectorize_action", "=", "True", "\n", "", "else", ":", "\n", "                    ", "raise", "ValueError", "(", "\"Error: the model requires a non vectorized environment or a single vectorized \"", "\n", "\"environment.\"", ")", "\n", "", "", "else", ":", "\n", "                ", "self", ".", "_vectorize_action", "=", "False", "\n", "\n", "", "self", ".", "n_envs", "=", "1", "\n", "\n", "", "self", ".", "env", "=", "env", "\n", "self", ".", "_vec_normalize_env", "=", "unwrap_vec_normalize", "(", "env", ")", "\n", "\n", "# Invalidated by environment change.", "\n", "self", ".", "episode_reward", "=", "None", "\n", "self", ".", "ep_info_buf", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.BaseRLModel._init_num_timesteps": [[164, 178], ["None"], "methods", ["None"], ["", "def", "_init_num_timesteps", "(", "self", ",", "reset_num_timesteps", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Initialize and resets num_timesteps (total timesteps since beginning of training)\n        if needed. Mainly used logging and plotting (tensorboard).\n\n        :param reset_num_timesteps: (bool) Set it to false when continuing training\n            to not create new plotting curves in tensorboard.\n        :return: (bool) Whether a new tensorboard log needs to be created\n        \"\"\"", "\n", "if", "reset_num_timesteps", ":", "\n", "            ", "self", ".", "num_timesteps", "=", "0", "\n", "\n", "", "new_tb_log", "=", "self", ".", "num_timesteps", "==", "0", "\n", "return", "new_tb_log", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.BaseRLModel.setup_model": [[179, 185], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "setup_model", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Create all the functions and tensorflow graphs necessary to train the model\n        \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.BaseRLModel._init_callback": [[186, 202], ["isinstance", "core.callbacks.ConvertCallback.init_callback", "core.callbacks.CallbackList", "isinstance", "core.callbacks.ConvertCallback"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.EventCallback.init_callback"], ["", "def", "_init_callback", "(", "self", ",", "\n", "callback", ":", "Union", "[", "None", ",", "Callable", ",", "List", "[", "BaseCallback", "]", ",", "BaseCallback", "]", "\n", ")", "->", "BaseCallback", ":", "\n", "        ", "\"\"\"\n        :param callback: (Union[None, Callable, List[BaseCallback], BaseCallback])\n        :return: (BaseCallback)\n        \"\"\"", "\n", "# Convert a list of callbacks into a callback", "\n", "if", "isinstance", "(", "callback", ",", "list", ")", ":", "\n", "            ", "callback", "=", "CallbackList", "(", "callback", ")", "\n", "# Convert functional callback to object", "\n", "", "if", "not", "isinstance", "(", "callback", ",", "BaseCallback", ")", ":", "\n", "            ", "callback", "=", "ConvertCallback", "(", "callback", ")", "\n", "\n", "", "callback", ".", "init_callback", "(", "self", ")", "\n", "return", "callback", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.BaseRLModel.set_random_seed": [[203, 219], ["core.cmd_util.set_global_seeds", "base_class.BaseRLModel.action_space.seed", "base_class.BaseRLModel.env.seed", "base_class.BaseRLModel.env.action_space.seed"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.cmd_util.set_global_seeds", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.seed", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.seed", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.seed"], ["", "def", "set_random_seed", "(", "self", ",", "seed", ":", "Optional", "[", "int", "]", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        :param seed: (Optional[int]) Seed for the pseudo-random generators. If None,\n            do not change the seeds.\n        \"\"\"", "\n", "# Ignore if the seed is None", "\n", "if", "seed", "is", "None", ":", "\n", "            ", "return", "\n", "# Seed python, numpy and tf random generator", "\n", "", "set_global_seeds", "(", "seed", ")", "\n", "if", "self", ".", "env", "is", "not", "None", ":", "\n", "            ", "self", ".", "env", ".", "seed", "(", "seed", ")", "\n", "# Seed the action space", "\n", "# useful when selecting random actions", "\n", "self", ".", "env", ".", "action_space", ".", "seed", "(", "seed", ")", "\n", "", "self", ".", "action_space", ".", "seed", "(", "seed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.BaseRLModel._setup_learn": [[220, 231], ["ValueError", "numpy.zeros", "collections.deque"], "methods", ["None"], ["", "def", "_setup_learn", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Check the environment.\n        \"\"\"", "\n", "if", "self", ".", "env", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"Error: cannot train the model without a valid environment, please set an environment with\"", "\n", "\"set_env(self, env) method.\"", ")", "\n", "", "if", "self", ".", "episode_reward", "is", "None", ":", "\n", "            ", "self", ".", "episode_reward", "=", "np", ".", "zeros", "(", "(", "self", ".", "n_envs", ",", ")", ")", "\n", "", "if", "self", ".", "ep_info_buf", "is", "None", ":", "\n", "            ", "self", ".", "ep_info_buf", "=", "deque", "(", "maxlen", "=", "100", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.BaseRLModel.get_parameter_list": [[232, 242], ["None"], "methods", ["None"], ["", "", "@", "abstractmethod", "\n", "def", "get_parameter_list", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Get tensorflow Variables of model's parameters\n\n        This includes all variables necessary for continuing training (saving / loading).\n\n        :return: (list) List of tensorflow Variables\n        \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.BaseRLModel.get_parameters": [[243, 253], ["base_class.BaseRLModel.get_parameter_list", "base_class.BaseRLModel.sess.run", "collections.OrderedDict", "zip"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.ActorCriticRLModel.get_parameter_list", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.runners.AbstractEnvRunner.run"], ["", "def", "get_parameters", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Get current model parameters as dictionary of variable name -> ndarray.\n\n        :return: (OrderedDict) Dictionary of variable name -> ndarray of model's parameters.\n        \"\"\"", "\n", "parameters", "=", "self", ".", "get_parameter_list", "(", ")", "\n", "parameter_values", "=", "self", ".", "sess", ".", "run", "(", "parameters", ")", "\n", "return_dictionary", "=", "OrderedDict", "(", "(", "param", ".", "name", ",", "value", ")", "for", "param", ",", "value", "in", "zip", "(", "parameters", ",", "parameter_values", ")", ")", "\n", "return", "return_dictionary", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.BaseRLModel._setup_load_operations": [[254, 274], ["base_class.BaseRLModel.get_parameter_list", "collections.OrderedDict", "RuntimeError", "base_class.BaseRLModel.graph.as_default", "tensorflow.placeholder", "param.assign"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.ActorCriticRLModel.get_parameter_list"], ["", "def", "_setup_load_operations", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Create tensorflow operations for loading model parameters\n        \"\"\"", "\n", "# Assume tensorflow graphs are static -> check", "\n", "# that we only call this function once", "\n", "if", "self", ".", "_param_load_ops", "is", "not", "None", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"Parameter load operations have already been created\"", ")", "\n", "# For each loadable parameter, create appropiate", "\n", "# placeholder and an assign op, and store them to", "\n", "# self.load_param_ops as dict of variable.name -> (placeholder, assign)", "\n", "", "loadable_parameters", "=", "self", ".", "get_parameter_list", "(", ")", "\n", "# Use OrderedDict to store order for backwards compatibility with", "\n", "# list-based params", "\n", "self", ".", "_param_load_ops", "=", "OrderedDict", "(", ")", "\n", "with", "self", ".", "graph", ".", "as_default", "(", ")", ":", "\n", "            ", "for", "param", "in", "loadable_parameters", ":", "\n", "                ", "placeholder", "=", "tf", ".", "placeholder", "(", "dtype", "=", "param", ".", "dtype", ",", "shape", "=", "param", ".", "shape", ")", "\n", "# param.name is unique (tensorflow variables have unique names)", "\n", "self", ".", "_param_load_ops", "[", "param", ".", "name", "]", "=", "(", "placeholder", ",", "param", ".", "assign", "(", "placeholder", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.BaseRLModel._get_pretrain_placeholders": [[275, 288], ["None"], "methods", ["None"], ["", "", "", "@", "abstractmethod", "\n", "def", "_get_pretrain_placeholders", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Return the placeholders needed for the pretraining:\n        - obs_ph: observation placeholder\n        - actions_ph will be population with an action from the environment\n            (from the expert dataset)\n        - deterministic_actions_ph: e.g., in the case of a Gaussian policy,\n            the mean.\n\n        :return: ((tf.placeholder)) (obs_ph, actions_ph, deterministic_actions_ph)\n        \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.BaseRLModel.pretrain": [[289, 376], ["isinstance", "isinstance", "range", "base_class.BaseRLModel.graph.as_default", "base_class.BaseRLModel.sess.run", "print", "int", "range", "len", "print", "int", "tensorflow.variable_scope", "tensorflow.train.AdamOptimizer", "tensorflow.train.AdamOptimizer.minimize", "tensorflow.global_variables_initializer", "len", "dataset.get_next_batch", "base_class.BaseRLModel.sess.run", "range", "len", "base_class.BaseRLModel._get_pretrain_placeholders", "tensorflow.reduce_mean", "base_class.BaseRLModel._get_pretrain_placeholders", "tensorflow.expand_dims", "tensorflow.one_hot", "tensorflow.nn.softmax_cross_entropy_with_logits_v2", "tensorflow.reduce_mean", "len", "dataset.get_next_batch", "base_class.BaseRLModel.sess.run", "print", "print", "print", "print", "tensorflow.square", "tensorflow.stop_gradient"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.runners.AbstractEnvRunner.run", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.runners.AbstractEnvRunner.run", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.BaseRLModel._get_pretrain_placeholders", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.BaseRLModel._get_pretrain_placeholders", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.runners.AbstractEnvRunner.run"], ["", "def", "pretrain", "(", "self", ",", "dataset", ",", "n_epochs", "=", "10", ",", "learning_rate", "=", "1e-4", ",", "\n", "adam_epsilon", "=", "1e-8", ",", "val_interval", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Pretrain a model using behavior cloning:\n        supervised learning given an expert dataset.\n\n        NOTE: only Box and Discrete spaces are supported for now.\n\n        :param dataset: (ExpertDataset) Dataset manager\n        :param n_epochs: (int) Number of iterations on the training set\n        :param learning_rate: (float) Learning rate\n        :param adam_epsilon: (float) the epsilon value for the adam optimizer\n        :param val_interval: (int) Report training and validation losses every n epochs.\n            By default, every 10th of the maximum number of epochs.\n        :return: (BaseRLModel) the pretrained model\n        \"\"\"", "\n", "continuous_actions", "=", "isinstance", "(", "self", ".", "action_space", ",", "gym", ".", "spaces", ".", "Box", ")", "\n", "discrete_actions", "=", "isinstance", "(", "self", ".", "action_space", ",", "gym", ".", "spaces", ".", "Discrete", ")", "\n", "\n", "assert", "discrete_actions", "or", "continuous_actions", ",", "'Only Discrete and Box action spaces are supported'", "\n", "\n", "# Validate the model every 10% of the total number of iteration", "\n", "if", "val_interval", "is", "None", ":", "\n", "# Prevent modulo by zero", "\n", "            ", "if", "n_epochs", "<", "10", ":", "\n", "                ", "val_interval", "=", "1", "\n", "", "else", ":", "\n", "                ", "val_interval", "=", "int", "(", "n_epochs", "/", "10", ")", "\n", "\n", "", "", "with", "self", ".", "graph", ".", "as_default", "(", ")", ":", "\n", "            ", "with", "tf", ".", "variable_scope", "(", "'pretrain'", ")", ":", "\n", "                ", "if", "continuous_actions", ":", "\n", "                    ", "obs_ph", ",", "actions_ph", ",", "deterministic_actions_ph", "=", "self", ".", "_get_pretrain_placeholders", "(", ")", "\n", "loss", "=", "tf", ".", "reduce_mean", "(", "tf", ".", "square", "(", "actions_ph", "-", "deterministic_actions_ph", ")", ")", "\n", "", "else", ":", "\n", "                    ", "obs_ph", ",", "actions_ph", ",", "actions_logits_ph", "=", "self", ".", "_get_pretrain_placeholders", "(", ")", "\n", "# actions_ph has a shape if (n_batch,), we reshape it to (n_batch, 1)", "\n", "# so no additional changes is needed in the dataloader", "\n", "actions_ph", "=", "tf", ".", "expand_dims", "(", "actions_ph", ",", "axis", "=", "1", ")", "\n", "one_hot_actions", "=", "tf", ".", "one_hot", "(", "actions_ph", ",", "self", ".", "action_space", ".", "n", ")", "\n", "loss", "=", "tf", ".", "nn", ".", "softmax_cross_entropy_with_logits_v2", "(", "\n", "logits", "=", "actions_logits_ph", ",", "\n", "labels", "=", "tf", ".", "stop_gradient", "(", "one_hot_actions", ")", "\n", ")", "\n", "loss", "=", "tf", ".", "reduce_mean", "(", "loss", ")", "\n", "", "optimizer", "=", "tf", ".", "train", ".", "AdamOptimizer", "(", "learning_rate", "=", "learning_rate", ",", "epsilon", "=", "adam_epsilon", ")", "\n", "optim_op", "=", "optimizer", ".", "minimize", "(", "loss", ",", "var_list", "=", "self", ".", "params", ")", "\n", "\n", "", "self", ".", "sess", ".", "run", "(", "tf", ".", "global_variables_initializer", "(", ")", ")", "\n", "\n", "", "if", "self", ".", "verbose", ">", "0", ":", "\n", "            ", "print", "(", "\"Pretraining with Behavior Cloning...\"", ")", "\n", "\n", "", "for", "epoch_idx", "in", "range", "(", "int", "(", "n_epochs", ")", ")", ":", "\n", "            ", "train_loss", "=", "0.0", "\n", "# Full pass on the training set", "\n", "for", "_", "in", "range", "(", "len", "(", "dataset", ".", "train_loader", ")", ")", ":", "\n", "                ", "expert_obs", ",", "expert_actions", "=", "dataset", ".", "get_next_batch", "(", "'train'", ")", "\n", "feed_dict", "=", "{", "\n", "obs_ph", ":", "expert_obs", ",", "\n", "actions_ph", ":", "expert_actions", ",", "\n", "}", "\n", "train_loss_", ",", "_", "=", "self", ".", "sess", ".", "run", "(", "[", "loss", ",", "optim_op", "]", ",", "feed_dict", ")", "\n", "train_loss", "+=", "train_loss_", "\n", "\n", "", "train_loss", "/=", "len", "(", "dataset", ".", "train_loader", ")", "\n", "\n", "if", "self", ".", "verbose", ">", "0", "and", "(", "epoch_idx", "+", "1", ")", "%", "val_interval", "==", "0", ":", "\n", "                ", "val_loss", "=", "0.0", "\n", "# Full pass on the validation set", "\n", "for", "_", "in", "range", "(", "len", "(", "dataset", ".", "val_loader", ")", ")", ":", "\n", "                    ", "expert_obs", ",", "expert_actions", "=", "dataset", ".", "get_next_batch", "(", "'val'", ")", "\n", "val_loss_", ",", "=", "self", ".", "sess", ".", "run", "(", "[", "loss", "]", ",", "{", "obs_ph", ":", "expert_obs", ",", "\n", "actions_ph", ":", "expert_actions", "}", ")", "\n", "val_loss", "+=", "val_loss_", "\n", "\n", "", "val_loss", "/=", "len", "(", "dataset", ".", "val_loader", ")", "\n", "if", "self", ".", "verbose", ">", "0", ":", "\n", "                    ", "print", "(", "\"==== Training progress {:.2f}% ====\"", ".", "format", "(", "100", "*", "(", "epoch_idx", "+", "1", ")", "/", "n_epochs", ")", ")", "\n", "print", "(", "'Epoch {}'", ".", "format", "(", "epoch_idx", "+", "1", ")", ")", "\n", "print", "(", "\"Training loss: {:.6f}, Validation loss: {:.6f}\"", ".", "format", "(", "train_loss", ",", "val_loss", ")", ")", "\n", "print", "(", ")", "\n", "# Free memory", "\n", "", "", "del", "expert_obs", ",", "expert_actions", "\n", "", "if", "self", ".", "verbose", ">", "0", ":", "\n", "            ", "print", "(", "\"Pretraining done.\"", ")", "\n", "", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.BaseRLModel.learn": [[377, 396], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "learn", "(", "self", ",", "total_timesteps", ",", "callback", "=", "None", ",", "log_interval", "=", "100", ",", "tb_log_name", "=", "\"run\"", ",", "\n", "reset_num_timesteps", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Return a trained model.\n\n        :param total_timesteps: (int) The total number of samples to train on\n        :param callback: (Union[callable, [callable], BaseCallback])\n            function called at every steps with state of the algorithm.\n            It takes the local and global variables. If it returns False, training is aborted.\n            When the callback inherits from BaseCallback, you will have access\n            to additional stages of the training (training start/end),\n            please read the documentation for more details.\n        :param log_interval: (int) The number of timesteps before logging.\n        :param tb_log_name: (str) the name of the run for tensorboard log\n        :param reset_num_timesteps: (bool) whether or not to reset the current timestep number (used in logging)\n        :return: (BaseRLModel) the trained model\n        \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.BaseRLModel.predict": [[397, 409], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "predict", "(", "self", ",", "observation", ",", "state", "=", "None", ",", "mask", "=", "None", ",", "deterministic", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Get the model's action from an observation\n\n        :param observation: (np.ndarray) the input observation\n        :param state: (np.ndarray) The last states (can be None, used in recurrent policies)\n        :param mask: (np.ndarray) The last masks (can be None, used in recurrent policies)\n        :param deterministic: (bool) Whether or not to return deterministic actions.\n        :return: (np.ndarray, np.ndarray) the model's action and the next state (used in recurrent policies)\n        \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.BaseRLModel.action_probability": [[410, 436], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "action_probability", "(", "self", ",", "observation", ",", "state", "=", "None", ",", "mask", "=", "None", ",", "actions", "=", "None", ",", "logp", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        If ``actions`` is ``None``, then get the model's action probability distribution from a given observation.\n\n        Depending on the action space the output is:\n            - Discrete: probability for each possible action\n            - Box: mean and standard deviation of the action output\n\n        However if ``actions`` is not ``None``, this function will return the probability that the given actions are\n        taken with the given parameters (observation, state, ...) on this model. For discrete action spaces, it\n        returns the probability mass; for continuous action spaces, the probability density. This is since the\n        probability mass will always be zero in continuous spaces, see http://blog.christianperone.com/2019/01/\n        for a good explanation\n\n        :param observation: (np.ndarray) the input observation\n        :param state: (np.ndarray) The last states (can be None, used in recurrent policies)\n        :param mask: (np.ndarray) The last masks (can be None, used in recurrent policies)\n        :param actions: (np.ndarray) (OPTIONAL) For calculating the likelihood that the given actions are chosen by\n            the model for each of the given parameters. Must have the same number of actions and observations.\n            (set to None to return the complete action probability distribution)\n        :param logp: (bool) (OPTIONAL) When specified with actions, returns probability in log-space.\n            This has no effect if actions is None.\n        :return: (np.ndarray) the model's (log) action probability\n        \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.BaseRLModel.load_parameters": [[437, 502], ["isinstance", "set", "dict.items", "base_class.BaseRLModel.sess.run", "base_class.BaseRLModel._setup_load_operations", "isinstance", "base_class.BaseRLModel._param_load_ops.keys", "param_update_ops.append", "set.remove", "RuntimeError", "warnings.warn", "dict", "enumerate", "base_class.BaseRLModel._load_from_file", "dict", "len", "base_class.BaseRLModel._param_load_ops.keys"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.runners.AbstractEnvRunner.run", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.BaseRLModel._setup_load_operations", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.warn", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.BaseRLModel._load_from_file"], ["", "def", "load_parameters", "(", "self", ",", "load_path_or_dict", ",", "exact_match", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Load model parameters from a file or a dictionary\n\n        Dictionary keys should be tensorflow variable names, which can be obtained\n        with ``get_parameters`` function. If ``exact_match`` is True, dictionary\n        should contain keys for all model's parameters, otherwise RunTimeError\n        is raised. If False, only variables included in the dictionary will be updated.\n\n        This does not load agent's hyper-parameters.\n\n        .. warning::\n            This function does not update trainer/optimizer variables (e.g. momentum).\n            As such training after using this function may lead to less-than-optimal results.\n\n        :param load_path_or_dict: (str or file-like or dict) Save parameter location\n            or dict of parameters as variable.name -> ndarrays to be loaded.\n        :param exact_match: (bool) If True, expects load dictionary to contain keys for\n            all variables in the model. If False, loads parameters only for variables\n            mentioned in the dictionary. Defaults to True.\n        \"\"\"", "\n", "# Make sure we have assign ops", "\n", "if", "self", ".", "_param_load_ops", "is", "None", ":", "\n", "            ", "self", ".", "_setup_load_operations", "(", ")", "\n", "\n", "", "if", "isinstance", "(", "load_path_or_dict", ",", "dict", ")", ":", "\n", "# Assume `load_path_or_dict` is dict of variable.name -> ndarrays we want to load", "\n", "            ", "params", "=", "load_path_or_dict", "\n", "", "elif", "isinstance", "(", "load_path_or_dict", ",", "list", ")", ":", "\n", "            ", "warnings", ".", "warn", "(", "\"Loading model parameters from a list. This has been replaced \"", "+", "\n", "\"with parameter dictionaries with variable names and parameters. \"", "+", "\n", "\"If you are loading from a file, consider re-saving the file.\"", ",", "\n", "DeprecationWarning", ")", "\n", "# Assume `load_path_or_dict` is list of ndarrays.", "\n", "# Create param dictionary assuming the parameters are in same order", "\n", "# as `get_parameter_list` returns them.", "\n", "params", "=", "dict", "(", ")", "\n", "for", "i", ",", "param_name", "in", "enumerate", "(", "self", ".", "_param_load_ops", ".", "keys", "(", ")", ")", ":", "\n", "                ", "params", "[", "param_name", "]", "=", "load_path_or_dict", "[", "i", "]", "\n", "", "", "else", ":", "\n", "# Assume a filepath or file-like.", "\n", "# Use existing deserializer to load the parameters.", "\n", "# We only need the parameters part of the file, so", "\n", "# only load that part.", "\n", "            ", "_", ",", "params", "=", "BaseRLModel", ".", "_load_from_file", "(", "load_path_or_dict", ",", "load_data", "=", "False", ")", "\n", "params", "=", "dict", "(", "params", ")", "\n", "\n", "", "feed_dict", "=", "{", "}", "\n", "param_update_ops", "=", "[", "]", "\n", "# Keep track of not-updated variables", "\n", "not_updated_variables", "=", "set", "(", "self", ".", "_param_load_ops", ".", "keys", "(", ")", ")", "\n", "for", "param_name", ",", "param_value", "in", "params", ".", "items", "(", ")", ":", "\n", "            ", "placeholder", ",", "assign_op", "=", "self", ".", "_param_load_ops", "[", "param_name", "]", "\n", "feed_dict", "[", "placeholder", "]", "=", "param_value", "\n", "# Create list of tf.assign operations for sess.run", "\n", "param_update_ops", ".", "append", "(", "assign_op", ")", "\n", "# Keep track which variables are updated", "\n", "not_updated_variables", ".", "remove", "(", "param_name", ")", "\n", "\n", "# Check that we updated all parameters if exact_match=True", "\n", "", "if", "exact_match", "and", "len", "(", "not_updated_variables", ")", ">", "0", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"Load dictionary did not contain all variables. \"", "+", "\n", "\"Missing variables: {}\"", ".", "format", "(", "\", \"", ".", "join", "(", "not_updated_variables", ")", ")", ")", "\n", "\n", "", "self", ".", "sess", ".", "run", "(", "param_update_ops", ",", "feed_dict", "=", "feed_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.BaseRLModel.save": [[503, 512], ["NotImplementedError"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "save", "(", "self", ",", "save_path", ",", "cloudpickle", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Save the current parameters to file\n\n        :param save_path: (str or file-like) The save location\n        :param cloudpickle: (bool) Use older cloudpickle format instead of zip-archives.\n        \"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.BaseRLModel.load": [[513, 531], ["NotImplementedError"], "methods", ["None"], ["", "@", "classmethod", "\n", "@", "abstractmethod", "\n", "def", "load", "(", "cls", ",", "load_path", ",", "env", "=", "None", ",", "custom_objects", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Load the model from file\n\n        :param load_path: (str or file-like) the saved parameter location\n        :param env: (Gym Environment) the new environment to run the loaded model on\n            (can be None if you only need prediction from a trained model)\n        :param custom_objects: (dict) Dictionary of objects to replace\n            upon loading. If a variable is present in this dictionary as a\n            key, it will not be deserialized and the corresponding item\n            will be used instead. Similar to custom_objects in\n            `keras.models.load_model`. Useful when you have an object in\n            file that can not be deserialized.\n        :param kwargs: extra arguments to change the model when loading\n        \"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.BaseRLModel._save_to_file_cloudpickle": [[532, 550], ["isinstance", "os.path.splitext", "cloudpickle.dump", "open", "cloudpickle.dump"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_save_to_file_cloudpickle", "(", "save_path", ",", "data", "=", "None", ",", "params", "=", "None", ")", ":", "\n", "        ", "\"\"\"Legacy code for saving models with cloudpickle\n\n        :param save_path: (str or file-like) Where to store the model\n        :param data: (OrderedDict) Class parameters being stored\n        :param params: (OrderedDict) Model parameters being stored\n        \"\"\"", "\n", "if", "isinstance", "(", "save_path", ",", "str", ")", ":", "\n", "            ", "_", ",", "ext", "=", "os", ".", "path", ".", "splitext", "(", "save_path", ")", "\n", "if", "ext", "==", "\"\"", ":", "\n", "                ", "save_path", "+=", "\".pkl\"", "\n", "\n", "", "with", "open", "(", "save_path", ",", "\"wb\"", ")", "as", "file_", ":", "\n", "                ", "cloudpickle", ".", "dump", "(", "(", "data", ",", "params", ")", ",", "file_", ")", "\n", "", "", "else", ":", "\n", "# Here save_path is a file-like object, not a path", "\n", "            ", "cloudpickle", ".", "dump", "(", "(", "data", ",", "params", ")", ",", "save_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.BaseRLModel._save_to_file_zip": [[551, 590], ["isinstance", "core.save_util.data_to_json", "core.save_util.params_to_bytes", "json.dumps", "os.path.splitext", "zipfile.ZipFile", "list", "file_.writestr", "file_.writestr", "file_.writestr", "params.keys"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.save_util.data_to_json", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.save_util.params_to_bytes"], ["", "", "@", "staticmethod", "\n", "def", "_save_to_file_zip", "(", "save_path", ",", "data", "=", "None", ",", "params", "=", "None", ")", ":", "\n", "        ", "\"\"\"Save model to a .zip archive\n\n        :param save_path: (str or file-like) Where to store the model\n        :param data: (OrderedDict) Class parameters being stored\n        :param params: (OrderedDict) Model parameters being stored\n        \"\"\"", "\n", "# data/params can be None, so do not", "\n", "# try to serialize them blindly", "\n", "if", "data", "is", "not", "None", ":", "\n", "            ", "serialized_data", "=", "data_to_json", "(", "data", ")", "\n", "", "if", "params", "is", "not", "None", ":", "\n", "            ", "serialized_params", "=", "params_to_bytes", "(", "params", ")", "\n", "# We also have to store list of the parameters", "\n", "# to store the ordering for OrderedDict.", "\n", "# We can trust these to be strings as they", "\n", "# are taken from the Tensorflow graph.", "\n", "serialized_param_list", "=", "json", ".", "dumps", "(", "\n", "list", "(", "params", ".", "keys", "(", ")", ")", ",", "\n", "indent", "=", "4", "\n", ")", "\n", "\n", "# Check postfix if save_path is a string", "\n", "", "if", "isinstance", "(", "save_path", ",", "str", ")", ":", "\n", "            ", "_", ",", "ext", "=", "os", ".", "path", ".", "splitext", "(", "save_path", ")", "\n", "if", "ext", "==", "\"\"", ":", "\n", "                ", "save_path", "+=", "\".zip\"", "\n", "\n", "# Create a zip-archive and write our objects", "\n", "# there. This works when save_path", "\n", "# is either str or a file-like", "\n", "", "", "with", "zipfile", ".", "ZipFile", "(", "save_path", ",", "\"w\"", ")", "as", "file_", ":", "\n", "# Do not try to save \"None\" elements", "\n", "            ", "if", "data", "is", "not", "None", ":", "\n", "                ", "file_", ".", "writestr", "(", "\"data\"", ",", "serialized_data", ")", "\n", "", "if", "params", "is", "not", "None", ":", "\n", "                ", "file_", ".", "writestr", "(", "\"parameters\"", ",", "serialized_params", ")", "\n", "file_", ".", "writestr", "(", "\"parameter_list\"", ",", "serialized_param_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.BaseRLModel._save_to_file": [[591, 604], ["base_class.BaseRLModel._save_to_file_cloudpickle", "base_class.BaseRLModel._save_to_file_zip"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.BaseRLModel._save_to_file_cloudpickle", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.BaseRLModel._save_to_file_zip"], ["", "", "", "@", "staticmethod", "\n", "def", "_save_to_file", "(", "save_path", ",", "data", "=", "None", ",", "params", "=", "None", ",", "cloudpickle", "=", "False", ")", ":", "\n", "        ", "\"\"\"Save model to a zip archive or cloudpickle file.\n\n        :param save_path: (str or file-like) Where to store the model\n        :param data: (OrderedDict) Class parameters being stored\n        :param params: (OrderedDict) Model parameters being stored\n        :param cloudpickle: (bool) Use old cloudpickle format instead of a zip archive.\n        \"\"\"", "\n", "if", "cloudpickle", ":", "\n", "            ", "BaseRLModel", ".", "_save_to_file_cloudpickle", "(", "save_path", ",", "data", ",", "params", ")", "\n", "", "else", ":", "\n", "            ", "BaseRLModel", ".", "_save_to_file_zip", "(", "save_path", ",", "data", ",", "params", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.BaseRLModel._load_from_file_cloudpickle": [[605, 626], ["isinstance", "cloudpickle.load", "os.path.exists", "os.path.exists", "open", "cloudpickle.load", "ValueError"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.VecNormalize.load", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.VecNormalize.load"], ["", "", "@", "staticmethod", "\n", "def", "_load_from_file_cloudpickle", "(", "load_path", ")", ":", "\n", "        ", "\"\"\"Legacy code for loading older models stored with cloudpickle\n\n        :param load_path: (str or file-like) where from to load the file\n        :return: (dict, OrderedDict) Class parameters and model parameters\n        \"\"\"", "\n", "if", "isinstance", "(", "load_path", ",", "str", ")", ":", "\n", "            ", "if", "not", "os", ".", "path", ".", "exists", "(", "load_path", ")", ":", "\n", "                ", "if", "os", ".", "path", ".", "exists", "(", "load_path", "+", "\".pkl\"", ")", ":", "\n", "                    ", "load_path", "+=", "\".pkl\"", "\n", "", "else", ":", "\n", "                    ", "raise", "ValueError", "(", "\"Error: the file {} could not be found\"", ".", "format", "(", "load_path", ")", ")", "\n", "\n", "", "", "with", "open", "(", "load_path", ",", "\"rb\"", ")", "as", "file_", ":", "\n", "                ", "data", ",", "params", "=", "cloudpickle", ".", "load", "(", "file_", ")", "\n", "", "", "else", ":", "\n", "# Here load_path is a file-like object, not a path", "\n", "            ", "data", ",", "params", "=", "cloudpickle", ".", "load", "(", "load_path", ")", "\n", "\n", "", "return", "data", ",", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.BaseRLModel._load_from_file": [[627, 689], ["isinstance", "os.path.exists", "os.path.exists", "zipfile.ZipFile", "file_.namelist", "warnings.warn", "base_class.BaseRLModel._load_from_file_cloudpickle", "ValueError", "file_.read().decode", "core.save_util.json_to_data", "file_.read().decode", "json.loads", "file_.read", "core.save_util.bytes_to_params", "isinstance", "load_path.seek", "file_.read", "file_.read"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.warn", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.BaseRLModel._load_from_file_cloudpickle", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.save_util.json_to_data", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.save_util.bytes_to_params"], ["", "@", "staticmethod", "\n", "def", "_load_from_file", "(", "load_path", ",", "load_data", "=", "True", ",", "custom_objects", "=", "None", ")", ":", "\n", "        ", "\"\"\"Load model data from a .zip archive\n\n        :param load_path: (str or file-like) Where to load model from\n        :param load_data: (bool) Whether we should load and return data\n            (class parameters). Mainly used by `load_parameters` to\n            only load model parameters (weights).\n        :param custom_objects: (dict) Dictionary of objects to replace\n            upon loading. If a variable is present in this dictionary as a\n            key, it will not be deserialized and the corresponding item\n            will be used instead. Similar to custom_objects in\n            `keras.models.load_model`. Useful when you have an object in\n            file that can not be deserialized.\n        :return: (dict, OrderedDict) Class parameters and model parameters\n        \"\"\"", "\n", "# Check if file exists if load_path is", "\n", "# a string", "\n", "if", "isinstance", "(", "load_path", ",", "str", ")", ":", "\n", "            ", "if", "not", "os", ".", "path", ".", "exists", "(", "load_path", ")", ":", "\n", "                ", "if", "os", ".", "path", ".", "exists", "(", "load_path", "+", "\".zip\"", ")", ":", "\n", "                    ", "load_path", "+=", "\".zip\"", "\n", "", "else", ":", "\n", "                    ", "raise", "ValueError", "(", "\"Error: the file {} could not be found\"", ".", "format", "(", "load_path", ")", ")", "\n", "\n", "# Open the zip archive and load data.", "\n", "", "", "", "try", ":", "\n", "            ", "with", "zipfile", ".", "ZipFile", "(", "load_path", ",", "\"r\"", ")", "as", "file_", ":", "\n", "                ", "namelist", "=", "file_", ".", "namelist", "(", ")", "\n", "# If data or parameters is not in the", "\n", "# zip archive, assume they were stored", "\n", "# as None (_save_to_file allows this).", "\n", "data", "=", "None", "\n", "params", "=", "None", "\n", "if", "\"data\"", "in", "namelist", "and", "load_data", ":", "\n", "# Load class parameters and convert to string", "\n", "# (Required for json library in Python 3.5)", "\n", "                    ", "json_data", "=", "file_", ".", "read", "(", "\"data\"", ")", ".", "decode", "(", ")", "\n", "data", "=", "json_to_data", "(", "json_data", ",", "custom_objects", "=", "custom_objects", ")", "\n", "\n", "", "if", "\"parameters\"", "in", "namelist", ":", "\n", "# Load parameter list and and parameters", "\n", "                    ", "parameter_list_json", "=", "file_", ".", "read", "(", "\"parameter_list\"", ")", ".", "decode", "(", ")", "\n", "parameter_list", "=", "json", ".", "loads", "(", "parameter_list_json", ")", "\n", "serialized_params", "=", "file_", ".", "read", "(", "\"parameters\"", ")", "\n", "params", "=", "bytes_to_params", "(", "\n", "serialized_params", ",", "parameter_list", "\n", ")", "\n", "", "", "", "except", "zipfile", ".", "BadZipFile", ":", "\n", "# load_path wasn't a zip file. Possibly a cloudpickle", "\n", "# file. Show a warning and fall back to loading cloudpickle.", "\n", "            ", "warnings", ".", "warn", "(", "\"It appears you are loading from a file with old format. \"", "+", "\n", "\"Older cloudpickle format has been replaced with zip-archived \"", "+", "\n", "\"models. Consider saving the model with new format.\"", ",", "\n", "DeprecationWarning", ")", "\n", "# Attempt loading with the cloudpickle format.", "\n", "# If load_path is file-like, seek back to beginning of file", "\n", "if", "not", "isinstance", "(", "load_path", ",", "str", ")", ":", "\n", "                ", "load_path", ".", "seek", "(", "0", ")", "\n", "", "data", ",", "params", "=", "BaseRLModel", ".", "_load_from_file_cloudpickle", "(", "load_path", ")", "\n", "\n", "", "return", "data", ",", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.BaseRLModel._softmax": [[690, 700], ["numpy.exp", "numpy.max", "numpy.exp.sum"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_softmax", "(", "x_input", ")", ":", "\n", "        ", "\"\"\"\n        An implementation of softmax.\n\n        :param x_input: (numpy float) input vector\n        :return: (numpy float) output vector\n        \"\"\"", "\n", "x_exp", "=", "np", ".", "exp", "(", "x_input", ".", "T", "-", "np", ".", "max", "(", "x_input", ".", "T", ",", "axis", "=", "0", ")", ")", "\n", "return", "(", "x_exp", "/", "x_exp", ".", "sum", "(", "axis", "=", "0", ")", ")", ".", "T", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.BaseRLModel._is_vectorized_observation": [[701, 750], ["isinstance", "isinstance", "isinstance", "ValueError", "isinstance", "len", "ValueError", "ValueError", "len", "ValueError", "len", "len", "ValueError", "map", "len", "len", "len"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_is_vectorized_observation", "(", "observation", ",", "observation_space", ")", ":", "\n", "        ", "\"\"\"\n        For every observation type, detects and validates the shape,\n        then returns whether or not the observation is vectorized.\n\n        :param observation: (np.ndarray) the input observation to validate\n        :param observation_space: (gym.spaces) the observation space\n        :return: (bool) whether the given observation is vectorized or not\n        \"\"\"", "\n", "if", "isinstance", "(", "observation_space", ",", "gym", ".", "spaces", ".", "Box", ")", ":", "\n", "            ", "if", "observation", ".", "shape", "==", "observation_space", ".", "shape", ":", "\n", "                ", "return", "False", "\n", "", "elif", "observation", ".", "shape", "[", "1", ":", "]", "==", "observation_space", ".", "shape", ":", "\n", "                ", "return", "True", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"Error: Unexpected observation shape {} for \"", ".", "format", "(", "observation", ".", "shape", ")", "+", "\n", "\"Box environment, please use {} \"", ".", "format", "(", "observation_space", ".", "shape", ")", "+", "\n", "\"or (n_env, {}) for the observation shape.\"", "\n", ".", "format", "(", "\", \"", ".", "join", "(", "map", "(", "str", ",", "observation_space", ".", "shape", ")", ")", ")", ")", "\n", "", "", "elif", "isinstance", "(", "observation_space", ",", "gym", ".", "spaces", ".", "Discrete", ")", ":", "\n", "            ", "if", "observation", ".", "shape", "==", "(", ")", ":", "# A numpy array of a number, has shape empty tuple '()'", "\n", "                ", "return", "False", "\n", "", "elif", "len", "(", "observation", ".", "shape", ")", "==", "1", ":", "\n", "                ", "return", "True", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"Error: Unexpected observation shape {} for \"", ".", "format", "(", "observation", ".", "shape", ")", "+", "\n", "\"Discrete environment, please use (1,) or (n_env, 1) for the observation shape.\"", ")", "\n", "", "", "elif", "isinstance", "(", "observation_space", ",", "gym", ".", "spaces", ".", "MultiDiscrete", ")", ":", "\n", "            ", "if", "observation", ".", "shape", "==", "(", "len", "(", "observation_space", ".", "nvec", ")", ",", ")", ":", "\n", "                ", "return", "False", "\n", "", "elif", "len", "(", "observation", ".", "shape", ")", "==", "2", "and", "observation", ".", "shape", "[", "1", "]", "==", "len", "(", "observation_space", ".", "nvec", ")", ":", "\n", "                ", "return", "True", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"Error: Unexpected observation shape {} for MultiDiscrete \"", ".", "format", "(", "observation", ".", "shape", ")", "+", "\n", "\"environment, please use ({},) or \"", ".", "format", "(", "len", "(", "observation_space", ".", "nvec", ")", ")", "+", "\n", "\"(n_env, {}) for the observation shape.\"", ".", "format", "(", "len", "(", "observation_space", ".", "nvec", ")", ")", ")", "\n", "", "", "elif", "isinstance", "(", "observation_space", ",", "gym", ".", "spaces", ".", "MultiBinary", ")", ":", "\n", "            ", "if", "observation", ".", "shape", "==", "(", "observation_space", ".", "n", ",", ")", ":", "\n", "                ", "return", "False", "\n", "", "elif", "len", "(", "observation", ".", "shape", ")", "==", "2", "and", "observation", ".", "shape", "[", "1", "]", "==", "observation_space", ".", "n", ":", "\n", "                ", "return", "True", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"Error: Unexpected observation shape {} for MultiBinary \"", ".", "format", "(", "observation", ".", "shape", ")", "+", "\n", "\"environment, please use ({},) or \"", ".", "format", "(", "observation_space", ".", "n", ")", "+", "\n", "\"(n_env, {}) for the observation shape.\"", ".", "format", "(", "observation_space", ".", "n", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Error: Cannot determine if the observation is vectorized with the space type {}.\"", "\n", ".", "format", "(", "observation_space", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.BaseRLModel.step": [[751, 753], ["None"], "methods", ["None"], ["", "", "def", "step", "(", "self", ",", "obs", ",", "states", ",", "dones", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.BaseRLModel.value": [[754, 756], ["None"], "methods", ["None"], ["", "def", "value", "(", "self", ",", "obs", ",", "states", ",", "dones", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.ActorCriticRLModel.__init__": [[776, 788], ["base_class.BaseRLModel.__init__"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__init__"], ["def", "__init__", "(", "self", ",", "policy", ",", "env", ",", "_init_setup_model", ",", "verbose", "=", "0", ",", "policy_base", "=", "ActorCriticPolicy", ",", "\n", "requires_vec_env", "=", "False", ",", "policy_kwargs", "=", "None", ",", "seed", "=", "None", ",", "n_cpu_tf_sess", "=", "None", ")", ":", "\n", "        ", "super", "(", "ActorCriticRLModel", ",", "self", ")", ".", "__init__", "(", "policy", ",", "env", ",", "verbose", "=", "verbose", ",", "requires_vec_env", "=", "requires_vec_env", ",", "\n", "policy_base", "=", "policy_base", ",", "policy_kwargs", "=", "policy_kwargs", ",", "\n", "seed", "=", "seed", ",", "n_cpu_tf_sess", "=", "n_cpu_tf_sess", ")", "\n", "\n", "self", ".", "sess", "=", "None", "\n", "self", ".", "initial_state", "=", "None", "\n", "self", ".", "step", "=", "None", "\n", "self", ".", "proba_step", "=", "None", "\n", "self", ".", "params", "=", "None", "\n", "self", ".", "_runner", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.ActorCriticRLModel._make_runner": [[789, 795], ["NotImplementedError"], "methods", ["None"], ["", "def", "_make_runner", "(", "self", ")", "->", "AbstractEnvRunner", ":", "\n", "        ", "\"\"\"Builds a new Runner.\n\n        Lazily called whenever `self.runner` is accessed and `self._runner is None`.\n        \"\"\"", "\n", "raise", "NotImplementedError", "(", "\"This model is not configured to use a Runner\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.ActorCriticRLModel.runner": [[796, 801], ["base_class.ActorCriticRLModel._make_runner"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.ActorCriticRLModel._make_runner"], ["", "@", "property", "\n", "def", "runner", "(", "self", ")", "->", "AbstractEnvRunner", ":", "\n", "        ", "if", "self", ".", "_runner", "is", "None", ":", "\n", "            ", "self", ".", "_runner", "=", "self", ".", "_make_runner", "(", ")", "\n", "", "return", "self", ".", "_runner", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.ActorCriticRLModel.set_env": [[802, 805], ["base_class.BaseRLModel.set_env"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.ActorCriticRLModel.set_env"], ["", "def", "set_env", "(", "self", ",", "env", ")", ":", "\n", "        ", "self", ".", "_runner", "=", "None", "# New environment invalidates `self._runner`.", "\n", "super", "(", ")", ".", "set_env", "(", "env", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.ActorCriticRLModel.setup_model": [[806, 809], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "setup_model", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.ActorCriticRLModel.learn": [[810, 814], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "learn", "(", "self", ",", "total_timesteps", ",", "callback", "=", "None", ",", "\n", "log_interval", "=", "100", ",", "tb_log_name", "=", "\"run\"", ",", "reset_num_timesteps", "=", "True", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.ActorCriticRLModel.predict": [[815, 837], ["numpy.array", "base_class.ActorCriticRLModel._is_vectorized_observation", "observation.reshape.reshape.reshape", "base_class.ActorCriticRLModel.step", "isinstance", "numpy.clip", "ValueError", "range"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.BaseRLModel._is_vectorized_observation", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnv.step"], ["", "def", "predict", "(", "self", ",", "observation", ",", "state", "=", "None", ",", "mask", "=", "None", ",", "deterministic", "=", "False", ")", ":", "\n", "        ", "if", "state", "is", "None", ":", "\n", "            ", "state", "=", "self", ".", "initial_state", "\n", "", "if", "mask", "is", "None", ":", "\n", "            ", "mask", "=", "[", "False", "for", "_", "in", "range", "(", "self", ".", "n_envs", ")", "]", "\n", "", "observation", "=", "np", ".", "array", "(", "observation", ")", "\n", "vectorized_env", "=", "self", ".", "_is_vectorized_observation", "(", "observation", ",", "self", ".", "observation_space", ")", "\n", "\n", "observation", "=", "observation", ".", "reshape", "(", "(", "-", "1", ",", ")", "+", "self", ".", "observation_space", ".", "shape", ")", "\n", "actions", ",", "_", ",", "states", ",", "_", "=", "self", ".", "step", "(", "observation", ",", "state", ",", "mask", ",", "deterministic", "=", "deterministic", ")", "\n", "\n", "clipped_actions", "=", "actions", "\n", "# Clip the actions to avoid out of bound error", "\n", "if", "isinstance", "(", "self", ".", "action_space", ",", "gym", ".", "spaces", ".", "Box", ")", ":", "\n", "            ", "clipped_actions", "=", "np", ".", "clip", "(", "actions", ",", "self", ".", "action_space", ".", "low", ",", "self", ".", "action_space", ".", "high", ")", "\n", "\n", "", "if", "not", "vectorized_env", ":", "\n", "            ", "if", "state", "is", "not", "None", ":", "\n", "                ", "raise", "ValueError", "(", "\"Error: The environment must be vectorized when using recurrent policies.\"", ")", "\n", "", "clipped_actions", "=", "clipped_actions", "[", "0", "]", "\n", "\n", "", "return", "clipped_actions", ",", "states", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.ActorCriticRLModel.action_probability": [[838, 917], ["numpy.array", "base_class.ActorCriticRLModel._is_vectorized_observation", "observation.reshape.reshape.reshape", "base_class.ActorCriticRLModel.proba_step", "len", "warnings.warn", "numpy.array", "isinstance", "ret.reshape.reshape.reshape", "actions.reshape.reshape.reshape", "isinstance", "ValueError", "range", "actions.reshape.reshape.reshape", "numpy.swapaxes", "numpy.prod", "isinstance", "numpy.log", "numpy.exp", "type", "actions.reshape.reshape.reshape", "numpy.prod", "isinstance", "numpy.arange", "len", "actions.reshape.reshape.reshape", "numpy.exp", "numpy.prod", "warnings.warn", "zip", "numpy.log", "numpy.sum", "numpy.sum", "numpy.arange", "type", "numpy.square"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.BaseRLModel._is_vectorized_observation", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.policies.FeedForwardPolicy.proba_step", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.warn", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.log", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.warn", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.log"], ["", "def", "action_probability", "(", "self", ",", "observation", ",", "state", "=", "None", ",", "mask", "=", "None", ",", "actions", "=", "None", ",", "logp", "=", "False", ")", ":", "\n", "        ", "if", "state", "is", "None", ":", "\n", "            ", "state", "=", "self", ".", "initial_state", "\n", "", "if", "mask", "is", "None", ":", "\n", "            ", "mask", "=", "[", "False", "for", "_", "in", "range", "(", "self", ".", "n_envs", ")", "]", "\n", "", "observation", "=", "np", ".", "array", "(", "observation", ")", "\n", "vectorized_env", "=", "self", ".", "_is_vectorized_observation", "(", "observation", ",", "self", ".", "observation_space", ")", "\n", "\n", "observation", "=", "observation", ".", "reshape", "(", "(", "-", "1", ",", ")", "+", "self", ".", "observation_space", ".", "shape", ")", "\n", "actions_proba", "=", "self", ".", "proba_step", "(", "observation", ",", "state", ",", "mask", ")", "\n", "\n", "if", "len", "(", "actions_proba", ")", "==", "0", ":", "# empty list means not implemented", "\n", "            ", "warnings", ".", "warn", "(", "\"Warning: action probability is not implemented for {} action space. Returning None.\"", "\n", ".", "format", "(", "type", "(", "self", ".", "action_space", ")", ".", "__name__", ")", ")", "\n", "return", "None", "\n", "\n", "", "if", "actions", "is", "not", "None", ":", "# comparing the action distribution, to given actions", "\n", "            ", "prob", "=", "None", "\n", "logprob", "=", "None", "\n", "actions", "=", "np", ".", "array", "(", "[", "actions", "]", ")", "\n", "if", "isinstance", "(", "self", ".", "action_space", ",", "gym", ".", "spaces", ".", "Discrete", ")", ":", "\n", "                ", "actions", "=", "actions", ".", "reshape", "(", "(", "-", "1", ",", ")", ")", "\n", "assert", "observation", ".", "shape", "[", "0", "]", "==", "actions", ".", "shape", "[", "0", "]", ",", "\"Error: batch sizes differ for actions and observations.\"", "\n", "prob", "=", "actions_proba", "[", "np", ".", "arange", "(", "actions", ".", "shape", "[", "0", "]", ")", ",", "actions", "]", "\n", "\n", "", "elif", "isinstance", "(", "self", ".", "action_space", ",", "gym", ".", "spaces", ".", "MultiDiscrete", ")", ":", "\n", "                ", "actions", "=", "actions", ".", "reshape", "(", "(", "-", "1", ",", "len", "(", "self", ".", "action_space", ".", "nvec", ")", ")", ")", "\n", "assert", "observation", ".", "shape", "[", "0", "]", "==", "actions", ".", "shape", "[", "0", "]", ",", "\"Error: batch sizes differ for actions and observations.\"", "\n", "# Discrete action probability, over multiple categories", "\n", "actions", "=", "np", ".", "swapaxes", "(", "actions", ",", "0", ",", "1", ")", "# swap axis for easier categorical split", "\n", "prob", "=", "np", ".", "prod", "(", "[", "proba", "[", "np", ".", "arange", "(", "act", ".", "shape", "[", "0", "]", ")", ",", "act", "]", "\n", "for", "proba", ",", "act", "in", "zip", "(", "actions_proba", ",", "actions", ")", "]", ",", "axis", "=", "0", ")", "\n", "\n", "", "elif", "isinstance", "(", "self", ".", "action_space", ",", "gym", ".", "spaces", ".", "MultiBinary", ")", ":", "\n", "                ", "actions", "=", "actions", ".", "reshape", "(", "(", "-", "1", ",", "self", ".", "action_space", ".", "n", ")", ")", "\n", "assert", "observation", ".", "shape", "[", "0", "]", "==", "actions", ".", "shape", "[", "0", "]", ",", "\"Error: batch sizes differ for actions and observations.\"", "\n", "# Bernoulli action probability, for every action", "\n", "prob", "=", "np", ".", "prod", "(", "actions_proba", "*", "actions", "+", "(", "1", "-", "actions_proba", ")", "*", "(", "1", "-", "actions", ")", ",", "axis", "=", "1", ")", "\n", "\n", "", "elif", "isinstance", "(", "self", ".", "action_space", ",", "gym", ".", "spaces", ".", "Box", ")", ":", "\n", "                ", "actions", "=", "actions", ".", "reshape", "(", "(", "-", "1", ",", ")", "+", "self", ".", "action_space", ".", "shape", ")", "\n", "mean", ",", "logstd", "=", "actions_proba", "\n", "std", "=", "np", ".", "exp", "(", "logstd", ")", "\n", "\n", "n_elts", "=", "np", ".", "prod", "(", "mean", ".", "shape", "[", "1", ":", "]", ")", "# first dimension is batch size", "\n", "log_normalizer", "=", "n_elts", "/", "2", "*", "np", ".", "log", "(", "2", "*", "np", ".", "pi", ")", "+", "1", "/", "2", "*", "np", ".", "sum", "(", "logstd", ",", "axis", "=", "1", ")", "\n", "\n", "# Diagonal Gaussian action probability, for every action", "\n", "logprob", "=", "-", "np", ".", "sum", "(", "np", ".", "square", "(", "actions", "-", "mean", ")", "/", "(", "2", "*", "std", ")", ",", "axis", "=", "1", ")", "-", "log_normalizer", "\n", "\n", "", "else", ":", "\n", "                ", "warnings", ".", "warn", "(", "\"Warning: action_probability not implemented for {} actions space. Returning None.\"", "\n", ".", "format", "(", "type", "(", "self", ".", "action_space", ")", ".", "__name__", ")", ")", "\n", "return", "None", "\n", "\n", "# Return in space (log or normal) requested by user, converting if necessary", "\n", "", "if", "logp", ":", "\n", "                ", "if", "logprob", "is", "None", ":", "\n", "                    ", "logprob", "=", "np", ".", "log", "(", "prob", ")", "\n", "", "ret", "=", "logprob", "\n", "", "else", ":", "\n", "                ", "if", "prob", "is", "None", ":", "\n", "                    ", "prob", "=", "np", ".", "exp", "(", "logprob", ")", "\n", "", "ret", "=", "prob", "\n", "\n", "# normalize action proba shape for the different gym spaces", "\n", "", "ret", "=", "ret", ".", "reshape", "(", "(", "-", "1", ",", "1", ")", ")", "\n", "", "else", ":", "\n", "            ", "ret", "=", "actions_proba", "\n", "\n", "", "if", "not", "vectorized_env", ":", "\n", "            ", "if", "state", "is", "not", "None", ":", "\n", "                ", "raise", "ValueError", "(", "\"Error: The environment must be vectorized when using recurrent policies.\"", ")", "\n", "", "ret", "=", "ret", "[", "0", "]", "\n", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.ActorCriticRLModel.get_parameter_list": [[918, 920], ["None"], "methods", ["None"], ["", "def", "get_parameter_list", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.ActorCriticRLModel.save": [[921, 924], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "save", "(", "self", ",", "save_path", ",", "cloudpickle", "=", "False", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.ActorCriticRLModel.load": [[925, 957], ["cls._load_from_file", "cls", "cls.__dict__.update", "cls.__dict__.update", "cls.set_env", "cls.setup_model", "cls.load_parameters", "ValueError"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.BaseRLModel._load_from_file", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.math_util.RunningMeanStd.update", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.math_util.RunningMeanStd.update", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.ActorCriticRLModel.set_env", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.OffPolicyRLModel.setup_model", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.BaseRLModel.load_parameters"], ["", "@", "classmethod", "\n", "def", "load", "(", "cls", ",", "load_path", ",", "env", "=", "None", ",", "custom_objects", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Load the model from file\n\n        :param load_path: (str or file-like) the saved parameter location\n        :param env: (Gym Environment) the new environment to run the loaded model on\n            (can be None if you only need prediction from a trained model)\n        :param custom_objects: (dict) Dictionary of objects to replace\n            upon loading. If a variable is present in this dictionary as a\n            key, it will not be deserialized and the corresponding item\n            will be used instead. Similar to custom_objects in\n            `keras.models.load_model`. Useful when you have an object in\n            file that can not be deserialized.\n        :param kwargs: extra arguments to change the model when loading\n        \"\"\"", "\n", "data", ",", "params", "=", "cls", ".", "_load_from_file", "(", "load_path", ",", "custom_objects", "=", "custom_objects", ")", "\n", "\n", "if", "'policy_kwargs'", "in", "kwargs", "and", "kwargs", "[", "'policy_kwargs'", "]", "!=", "data", "[", "'policy_kwargs'", "]", ":", "\n", "            ", "raise", "ValueError", "(", "\"The specified policy kwargs do not equal the stored policy kwargs. \"", "\n", "\"Stored kwargs: {}, specified kwargs: {}\"", ".", "format", "(", "data", "[", "'policy_kwargs'", "]", ",", "\n", "kwargs", "[", "'policy_kwargs'", "]", ")", ")", "\n", "\n", "", "model", "=", "cls", "(", "policy", "=", "data", "[", "\"policy\"", "]", ",", "env", "=", "None", ",", "_init_setup_model", "=", "False", ")", "\n", "model", ".", "__dict__", ".", "update", "(", "data", ")", "\n", "model", ".", "__dict__", ".", "update", "(", "kwargs", ")", "\n", "model", ".", "set_env", "(", "env", ")", "\n", "model", ".", "setup_model", "(", ")", "\n", "\n", "model", ".", "load_parameters", "(", "params", ")", "\n", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.OffPolicyRLModel.__init__": [[978, 986], ["base_class.BaseRLModel.__init__"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__init__"], ["def", "__init__", "(", "self", ",", "policy", ",", "env", ",", "replay_buffer", "=", "None", ",", "_init_setup_model", "=", "False", ",", "verbose", "=", "0", ",", "*", ",", "\n", "requires_vec_env", "=", "False", ",", "policy_base", "=", "None", ",", "\n", "policy_kwargs", "=", "None", ",", "seed", "=", "None", ",", "n_cpu_tf_sess", "=", "None", ")", ":", "\n", "        ", "super", "(", "OffPolicyRLModel", ",", "self", ")", ".", "__init__", "(", "policy", ",", "env", ",", "verbose", "=", "verbose", ",", "requires_vec_env", "=", "requires_vec_env", ",", "\n", "policy_base", "=", "policy_base", ",", "policy_kwargs", "=", "policy_kwargs", ",", "\n", "seed", "=", "seed", ",", "n_cpu_tf_sess", "=", "n_cpu_tf_sess", ")", "\n", "\n", "self", ".", "replay_buffer", "=", "replay_buffer", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.OffPolicyRLModel.setup_model": [[987, 990], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "setup_model", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.OffPolicyRLModel.learn": [[991, 995], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "learn", "(", "self", ",", "total_timesteps", ",", "callback", "=", "None", ",", "\n", "log_interval", "=", "100", ",", "tb_log_name", "=", "\"run\"", ",", "reset_num_timesteps", "=", "True", ",", "replay_wrapper", "=", "None", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.OffPolicyRLModel.predict": [[996, 999], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "predict", "(", "self", ",", "observation", ",", "state", "=", "None", ",", "mask", "=", "None", ",", "deterministic", "=", "False", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.OffPolicyRLModel.action_probability": [[1000, 1003], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "action_probability", "(", "self", ",", "observation", ",", "state", "=", "None", ",", "mask", "=", "None", ",", "actions", "=", "None", ",", "logp", "=", "False", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.OffPolicyRLModel.save": [[1004, 1007], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "save", "(", "self", ",", "save_path", ",", "cloudpickle", "=", "False", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.OffPolicyRLModel.load": [[1008, 1040], ["cls._load_from_file", "cls", "cls.__dict__.update", "cls.__dict__.update", "cls.set_env", "cls.setup_model", "cls.load_parameters", "ValueError"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.BaseRLModel._load_from_file", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.math_util.RunningMeanStd.update", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.math_util.RunningMeanStd.update", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.ActorCriticRLModel.set_env", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.OffPolicyRLModel.setup_model", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.BaseRLModel.load_parameters"], ["", "@", "classmethod", "\n", "def", "load", "(", "cls", ",", "load_path", ",", "env", "=", "None", ",", "custom_objects", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Load the model from file\n\n        :param load_path: (str or file-like) the saved parameter location\n        :param env: (Gym Environment) the new environment to run the loaded model on\n            (can be None if you only need prediction from a trained model)\n        :param custom_objects: (dict) Dictionary of objects to replace\n            upon loading. If a variable is present in this dictionary as a\n            key, it will not be deserialized and the corresponding item\n            will be used instead. Similar to custom_objects in\n            `keras.models.load_model`. Useful when you have an object in\n            file that can not be deserialized.\n        :param kwargs: extra arguments to change the model when loading\n        \"\"\"", "\n", "data", ",", "params", "=", "cls", ".", "_load_from_file", "(", "load_path", ",", "custom_objects", "=", "custom_objects", ")", "\n", "\n", "if", "'policy_kwargs'", "in", "kwargs", "and", "kwargs", "[", "'policy_kwargs'", "]", "!=", "data", "[", "'policy_kwargs'", "]", ":", "\n", "            ", "raise", "ValueError", "(", "\"The specified policy kwargs do not equal the stored policy kwargs. \"", "\n", "\"Stored kwargs: {}, specified kwargs: {}\"", ".", "format", "(", "data", "[", "'policy_kwargs'", "]", ",", "\n", "kwargs", "[", "'policy_kwargs'", "]", ")", ")", "\n", "\n", "", "model", "=", "cls", "(", "policy", "=", "data", "[", "\"policy\"", "]", ",", "env", "=", "None", ",", "_init_setup_model", "=", "False", ")", "\n", "model", ".", "__dict__", ".", "update", "(", "data", ")", "\n", "model", ".", "__dict__", ".", "update", "(", "kwargs", ")", "\n", "model", ".", "set_env", "(", "env", ")", "\n", "model", ".", "setup_model", "(", ")", "\n", "\n", "model", ".", "load_parameters", "(", "params", ")", "\n", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class._UnvecWrapper.__init__": [[1043, 1051], ["core.vec_env.all_vec_env.VecEnvWrapper.__init__"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__init__"], ["    ", "def", "__init__", "(", "self", ",", "venv", ")", ":", "\n", "        ", "\"\"\"\n        Unvectorize a vectorized environment, for vectorized environment that only have one environment\n\n        :param venv: (VecEnv) the vectorized environment to wrap\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", "venv", ")", "\n", "assert", "venv", ".", "num_envs", "==", "1", ",", "\"Error: cannot unwrap a environment wrapper that has more than one environment.\"", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class._UnvecWrapper.seed": [[1052, 1054], ["base_class._UnvecWrapper.venv.env_method"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.env_method"], ["", "def", "seed", "(", "self", ",", "seed", "=", "None", ")", ":", "\n", "        ", "return", "self", ".", "venv", ".", "env_method", "(", "'seed'", ",", "seed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class._UnvecWrapper.__getattr__": [[1055, 1059], ["getattr", "getattr"], "methods", ["None"], ["", "def", "__getattr__", "(", "self", ",", "attr", ")", ":", "\n", "        ", "if", "attr", "in", "self", ".", "__dict__", ":", "\n", "            ", "return", "getattr", "(", "self", ",", "attr", ")", "\n", "", "return", "getattr", "(", "self", ".", "venv", ",", "attr", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class._UnvecWrapper.__set_attr__": [[1060, 1065], ["setattr", "setattr"], "methods", ["None"], ["", "def", "__set_attr__", "(", "self", ",", "attr", ",", "value", ")", ":", "\n", "        ", "if", "attr", "in", "self", ".", "__dict__", ":", "\n", "            ", "setattr", "(", "self", ",", "attr", ",", "value", ")", "\n", "", "else", ":", "\n", "            ", "setattr", "(", "self", ".", "venv", ",", "attr", ",", "value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class._UnvecWrapper.compute_reward": [[1066, 1068], ["float", "base_class._UnvecWrapper.venv.env_method"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.env_method"], ["", "", "def", "compute_reward", "(", "self", ",", "achieved_goal", ",", "desired_goal", ",", "_info", ")", ":", "\n", "        ", "return", "float", "(", "self", ".", "venv", ".", "env_method", "(", "'compute_reward'", ",", "achieved_goal", ",", "desired_goal", ",", "_info", ")", "[", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class._UnvecWrapper.unvec_obs": [[1069, 1082], ["collections.OrderedDict", "obs.keys", "isinstance"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "unvec_obs", "(", "obs", ")", ":", "\n", "        ", "\"\"\"\n        :param obs: (Union[np.ndarray, dict])\n        :return: (Union[np.ndarray, dict])\n        \"\"\"", "\n", "if", "not", "isinstance", "(", "obs", ",", "dict", ")", ":", "\n", "            ", "return", "obs", "[", "0", "]", "\n", "", "obs_", "=", "OrderedDict", "(", ")", "\n", "for", "key", "in", "obs", ".", "keys", "(", ")", ":", "\n", "            ", "obs_", "[", "key", "]", "=", "obs", "[", "key", "]", "[", "0", "]", "\n", "", "del", "obs", "\n", "return", "obs_", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class._UnvecWrapper.reset": [[1083, 1085], ["base_class._UnvecWrapper.unvec_obs", "base_class._UnvecWrapper.venv.reset"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class._UnvecWrapper.unvec_obs", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.reset"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "unvec_obs", "(", "self", ".", "venv", ".", "reset", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class._UnvecWrapper.step_async": [[1086, 1088], ["base_class._UnvecWrapper.venv.step_async"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.step_async"], ["", "def", "step_async", "(", "self", ",", "actions", ")", ":", "\n", "        ", "self", ".", "venv", ".", "step_async", "(", "[", "actions", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class._UnvecWrapper.step_wait": [[1089, 1092], ["base_class._UnvecWrapper.venv.step_wait", "base_class._UnvecWrapper.unvec_obs", "float"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.step_wait", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class._UnvecWrapper.unvec_obs"], ["", "def", "step_wait", "(", "self", ")", ":", "\n", "        ", "obs", ",", "rewards", ",", "dones", ",", "information", "=", "self", ".", "venv", ".", "step_wait", "(", ")", "\n", "return", "self", ".", "unvec_obs", "(", "obs", ")", ",", "float", "(", "rewards", "[", "0", "]", ")", ",", "dones", "[", "0", "]", ",", "information", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class._UnvecWrapper.render": [[1093, 1095], ["base_class._UnvecWrapper.venv.render"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.render"], ["", "def", "render", "(", "self", ",", "mode", "=", "'human'", ")", ":", "\n", "        ", "return", "self", ".", "venv", ".", "render", "(", "mode", "=", "mode", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.SetVerbosity.__init__": [[1098, 1105], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "verbose", "=", "0", ")", ":", "\n", "        ", "\"\"\"\n        define a region of code for certain level of verbosity\n\n        :param verbose: (int) the verbosity level: 0 none, 1 training information, 2 tensorflow debug\n        \"\"\"", "\n", "self", ".", "verbose", "=", "verbose", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.SetVerbosity.__enter__": [[1106, 1117], ["os.environ.get", "core.logger.get_level", "core.logger.set_level", "gym.logger.set_level"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.get_level", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.set_level", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.set_level"], ["", "def", "__enter__", "(", "self", ")", ":", "\n", "        ", "self", ".", "tf_level", "=", "os", ".", "environ", ".", "get", "(", "'TF_CPP_MIN_LOG_LEVEL'", ",", "'0'", ")", "\n", "self", ".", "log_level", "=", "logger", ".", "get_level", "(", ")", "\n", "self", ".", "gym_level", "=", "gym", ".", "logger", ".", "MIN_LEVEL", "\n", "\n", "if", "self", ".", "verbose", "<=", "1", ":", "\n", "            ", "os", ".", "environ", "[", "'TF_CPP_MIN_LOG_LEVEL'", "]", "=", "'3'", "\n", "\n", "", "if", "self", ".", "verbose", "<=", "0", ":", "\n", "            ", "logger", ".", "set_level", "(", "logger", ".", "DISABLED", ")", "\n", "gym", ".", "logger", ".", "set_level", "(", "gym", ".", "logger", ".", "DISABLED", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.SetVerbosity.__exit__": [[1118, 1125], ["core.logger.set_level", "gym.logger.set_level"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.set_level", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.set_level"], ["", "", "def", "__exit__", "(", "self", ",", "exc_type", ",", "exc_val", ",", "exc_tb", ")", ":", "\n", "        ", "if", "self", ".", "verbose", "<=", "1", ":", "\n", "            ", "os", ".", "environ", "[", "'TF_CPP_MIN_LOG_LEVEL'", "]", "=", "self", ".", "tf_level", "\n", "\n", "", "if", "self", ".", "verbose", "<=", "0", ":", "\n", "            ", "logger", ".", "set_level", "(", "self", ".", "log_level", ")", "\n", "gym", ".", "logger", ".", "set_level", "(", "self", ".", "gym_level", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.TensorboardWriter.__init__": [[1128, 1142], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "graph", ",", "tensorboard_log_path", ",", "tb_log_name", ",", "new_tb_log", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Create a Tensorboard writer for a code segment, and saves it to the log directory as its own run\n\n        :param graph: (Tensorflow Graph) the model graph\n        :param tensorboard_log_path: (str) the save path for the log (can be None for no logging)\n        :param tb_log_name: (str) the name of the run for tensorboard log\n        :param new_tb_log: (bool) whether or not to create a new logging folder for tensorbaord\n        \"\"\"", "\n", "self", ".", "graph", "=", "graph", "\n", "self", ".", "tensorboard_log_path", "=", "tensorboard_log_path", "\n", "self", ".", "tb_log_name", "=", "tb_log_name", "\n", "self", ".", "writer", "=", "None", "\n", "self", ".", "new_tb_log", "=", "new_tb_log", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.TensorboardWriter.__enter__": [[1143, 1151], ["base_class.TensorboardWriter._get_latest_run_id", "os.path.join", "tensorflow.compat.v1.summary.FileWriter"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.TensorboardWriter._get_latest_run_id"], ["", "def", "__enter__", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "tensorboard_log_path", "is", "not", "None", ":", "\n", "            ", "latest_run_id", "=", "self", ".", "_get_latest_run_id", "(", ")", "\n", "if", "self", ".", "new_tb_log", ":", "\n", "                ", "latest_run_id", "=", "latest_run_id", "+", "1", "\n", "", "save_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "tensorboard_log_path", ",", "\"{}_{}\"", ".", "format", "(", "self", ".", "tb_log_name", ",", "latest_run_id", ")", ")", "\n", "self", ".", "writer", "=", "tf", ".", "compat", ".", "v1", ".", "summary", ".", "FileWriter", "(", "save_path", ",", "graph", "=", "self", ".", "graph", ")", "\n", "", "return", "self", ".", "writer", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.TensorboardWriter._get_latest_run_id": [[1152, 1166], ["glob.glob", "path.split", "file_name.split", "ext.isdigit", "int", "int", "file_name.split"], "methods", ["None"], ["", "def", "_get_latest_run_id", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        returns the latest run number for the given log name and log path,\n        by finding the greatest number in the directories.\n\n        :return: (int) latest run number\n        \"\"\"", "\n", "max_run_id", "=", "0", "\n", "for", "path", "in", "glob", ".", "glob", "(", "\"{}/{}_[0-9]*\"", ".", "format", "(", "self", ".", "tensorboard_log_path", ",", "self", ".", "tb_log_name", ")", ")", ":", "\n", "            ", "file_name", "=", "path", ".", "split", "(", "os", ".", "sep", ")", "[", "-", "1", "]", "\n", "ext", "=", "file_name", ".", "split", "(", "\"_\"", ")", "[", "-", "1", "]", "\n", "if", "self", ".", "tb_log_name", "==", "\"_\"", ".", "join", "(", "file_name", ".", "split", "(", "\"_\"", ")", "[", ":", "-", "1", "]", ")", "and", "ext", ".", "isdigit", "(", ")", "and", "int", "(", "ext", ")", ">", "max_run_id", ":", "\n", "                ", "max_run_id", "=", "int", "(", "ext", ")", "\n", "", "", "return", "max_run_id", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.TensorboardWriter.__exit__": [[1167, 1171], ["base_class.TensorboardWriter.writer.add_graph", "base_class.TensorboardWriter.writer.flush"], "methods", ["None"], ["", "def", "__exit__", "(", "self", ",", "exc_type", ",", "exc_val", ",", "exc_tb", ")", ":", "\n", "        ", "if", "self", ".", "writer", "is", "not", "None", ":", "\n", "            ", "self", ".", "writer", ".", "add_graph", "(", "self", ".", "graph", ")", "\n", "self", ".", "writer", ".", "flush", "(", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.BaseCallback.__init__": [[25, 42], ["abc.ABC.__init__"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__init__"], ["def", "__init__", "(", "self", ",", "verbose", ":", "int", "=", "0", ")", ":", "\n", "        ", "super", "(", "BaseCallback", ",", "self", ")", ".", "__init__", "(", ")", "\n", "# The RL model", "\n", "self", ".", "model", "=", "None", "# type: Optional[BaseRLModel]", "\n", "# An alias for self.model.get_env(), the environment used for training", "\n", "self", ".", "training_env", "=", "None", "# type: Union[gym.Env, VecEnv, None]", "\n", "# Number of time the callback was called", "\n", "self", ".", "n_calls", "=", "0", "# type: int", "\n", "# n_envs * n times env.step() was called", "\n", "self", ".", "num_timesteps", "=", "0", "# type: int", "\n", "self", ".", "verbose", "=", "verbose", "\n", "self", ".", "locals", "=", "None", "# type: Optional[Dict[str, Any]]", "\n", "self", ".", "globals", "=", "None", "# type: Optional[Dict[str, Any]]", "\n", "self", ".", "logger", "=", "None", "# type: Optional[logger.Logger]", "\n", "# Sometimes, for event callback, it is useful", "\n", "# to have access to the parent object", "\n", "self", ".", "parent", "=", "None", "# type: Optional[BaseCallback]", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.BaseCallback.init_callback": [[44, 53], ["model.get_env", "callbacks.BaseCallback._init_callback"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.BaseRLModel.get_env", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.EvalCallback._init_callback"], ["", "def", "init_callback", "(", "self", ",", "model", ":", "'BaseRLModel'", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Initialize the callback by saving references to the\n        RL model and the training environment for convenience.\n        \"\"\"", "\n", "self", ".", "model", "=", "model", "\n", "self", ".", "training_env", "=", "model", ".", "get_env", "(", ")", "\n", "self", ".", "logger", "=", "logger", ".", "Logger", ".", "CURRENT", "\n", "self", ".", "_init_callback", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.BaseCallback._init_callback": [[54, 56], ["None"], "methods", ["None"], ["", "def", "_init_callback", "(", "self", ")", "->", "None", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.BaseCallback.on_training_start": [[57, 62], ["callbacks.BaseCallback._on_training_start"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.CallbackList._on_training_start"], ["", "def", "on_training_start", "(", "self", ",", "locals_", ":", "Dict", "[", "str", ",", "Any", "]", ",", "globals_", ":", "Dict", "[", "str", ",", "Any", "]", ")", "->", "None", ":", "\n", "# Those are reference and will be updated automatically", "\n", "        ", "self", ".", "locals", "=", "locals_", "\n", "self", ".", "globals", "=", "globals_", "\n", "self", ".", "_on_training_start", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.BaseCallback._on_training_start": [[63, 65], ["None"], "methods", ["None"], ["", "def", "_on_training_start", "(", "self", ")", "->", "None", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.BaseCallback.on_rollout_start": [[66, 68], ["callbacks.BaseCallback._on_rollout_start"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.CallbackList._on_rollout_start"], ["", "def", "on_rollout_start", "(", "self", ")", "->", "None", ":", "\n", "        ", "self", ".", "_on_rollout_start", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.BaseCallback._on_rollout_start": [[69, 71], ["None"], "methods", ["None"], ["", "def", "_on_rollout_start", "(", "self", ")", "->", "None", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.BaseCallback._on_step": [[72, 77], ["None"], "methods", ["None"], ["", "def", "_on_step", "(", "self", ")", "->", "bool", ":", "\n", "        ", "\"\"\"\n        :return: (bool) If the callback returns False, training is aborted early.\n        \"\"\"", "\n", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.BaseCallback.on_step": [[78, 91], ["callbacks.BaseCallback._on_step"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.EveryNTimesteps._on_step"], ["", "def", "on_step", "(", "self", ")", "->", "bool", ":", "\n", "        ", "\"\"\"\n        This method will be called by the model after each call to `env.step()`.\n\n        For child callback (of an `EventCallback`), this will be called\n        when the event is triggered.\n\n        :return: (bool) If the callback returns False, training is aborted early.\n        \"\"\"", "\n", "self", ".", "n_calls", "+=", "1", "\n", "self", ".", "num_timesteps", "=", "self", ".", "model", ".", "num_timesteps", "\n", "\n", "return", "self", ".", "_on_step", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.BaseCallback.on_training_end": [[92, 94], ["callbacks.BaseCallback._on_training_end"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.CallbackList._on_training_end"], ["", "def", "on_training_end", "(", "self", ")", "->", "None", ":", "\n", "        ", "self", ".", "_on_training_end", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.BaseCallback._on_training_end": [[95, 97], ["None"], "methods", ["None"], ["", "def", "_on_training_end", "(", "self", ")", "->", "None", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.BaseCallback.on_rollout_end": [[98, 100], ["callbacks.BaseCallback._on_rollout_end"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.CallbackList._on_rollout_end"], ["", "def", "on_rollout_end", "(", "self", ")", "->", "None", ":", "\n", "        ", "self", ".", "_on_rollout_end", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.BaseCallback._on_rollout_end": [[101, 103], ["None"], "methods", ["None"], ["", "def", "_on_rollout_end", "(", "self", ")", "->", "None", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.EventCallback.__init__": [[114, 120], ["callbacks.BaseCallback.__init__"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__init__"], ["def", "__init__", "(", "self", ",", "callback", ":", "Optional", "[", "BaseCallback", "]", "=", "None", ",", "verbose", ":", "int", "=", "0", ")", ":", "\n", "        ", "super", "(", "EventCallback", ",", "self", ")", ".", "__init__", "(", "verbose", "=", "verbose", ")", "\n", "self", ".", "callback", "=", "callback", "\n", "# Give access to the parent", "\n", "if", "callback", "is", "not", "None", ":", "\n", "            ", "self", ".", "callback", ".", "parent", "=", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.EventCallback.init_callback": [[121, 125], ["callbacks.BaseCallback.init_callback", "callbacks.EventCallback.callback.init_callback"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.EventCallback.init_callback", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.EventCallback.init_callback"], ["", "", "def", "init_callback", "(", "self", ",", "model", ":", "'BaseRLModel'", ")", "->", "None", ":", "\n", "        ", "super", "(", "EventCallback", ",", "self", ")", ".", "init_callback", "(", "model", ")", "\n", "if", "self", ".", "callback", "is", "not", "None", ":", "\n", "            ", "self", ".", "callback", ".", "init_callback", "(", "self", ".", "model", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.EventCallback._on_training_start": [[126, 129], ["callbacks.EventCallback.callback.on_training_start"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.BaseCallback.on_training_start"], ["", "", "def", "_on_training_start", "(", "self", ")", "->", "None", ":", "\n", "        ", "if", "self", ".", "callback", "is", "not", "None", ":", "\n", "            ", "self", ".", "callback", ".", "on_training_start", "(", "self", ".", "locals", ",", "self", ".", "globals", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.EventCallback._on_event": [[130, 134], ["callbacks.EventCallback.callback.on_step"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.BaseCallback.on_step"], ["", "", "def", "_on_event", "(", "self", ")", "->", "bool", ":", "\n", "        ", "if", "self", ".", "callback", "is", "not", "None", ":", "\n", "            ", "return", "self", ".", "callback", ".", "on_step", "(", ")", "\n", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.EventCallback._on_step": [[135, 137], ["None"], "methods", ["None"], ["", "def", "_on_step", "(", "self", ")", "->", "bool", ":", "\n", "        ", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.CallbackList.__init__": [[147, 151], ["callbacks.BaseCallback.__init__", "isinstance"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__init__"], ["def", "__init__", "(", "self", ",", "callbacks", ":", "List", "[", "BaseCallback", "]", ")", ":", "\n", "        ", "super", "(", "CallbackList", ",", "self", ")", ".", "__init__", "(", ")", "\n", "assert", "isinstance", "(", "callbacks", ",", "list", ")", "\n", "self", ".", "callbacks", "=", "callbacks", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.CallbackList._init_callback": [[152, 155], ["callback.init_callback"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.EventCallback.init_callback"], ["", "def", "_init_callback", "(", "self", ")", "->", "None", ":", "\n", "        ", "for", "callback", "in", "self", ".", "callbacks", ":", "\n", "            ", "callback", ".", "init_callback", "(", "self", ".", "model", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.CallbackList._on_training_start": [[156, 159], ["callback.on_training_start"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.BaseCallback.on_training_start"], ["", "", "def", "_on_training_start", "(", "self", ")", "->", "None", ":", "\n", "        ", "for", "callback", "in", "self", ".", "callbacks", ":", "\n", "            ", "callback", ".", "on_training_start", "(", "self", ".", "locals", ",", "self", ".", "globals", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.CallbackList._on_rollout_start": [[160, 163], ["callback.on_rollout_start"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.BaseCallback.on_rollout_start"], ["", "", "def", "_on_rollout_start", "(", "self", ")", "->", "None", ":", "\n", "        ", "for", "callback", "in", "self", ".", "callbacks", ":", "\n", "            ", "callback", ".", "on_rollout_start", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.CallbackList._on_step": [[164, 170], ["callback.on_step"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.BaseCallback.on_step"], ["", "", "def", "_on_step", "(", "self", ")", "->", "bool", ":", "\n", "        ", "continue_training", "=", "True", "\n", "for", "callback", "in", "self", ".", "callbacks", ":", "\n", "# Return False (stop training) if at least one callback returns False", "\n", "            ", "continue_training", "=", "callback", ".", "on_step", "(", ")", "and", "continue_training", "\n", "", "return", "continue_training", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.CallbackList._on_rollout_end": [[171, 174], ["callback.on_rollout_end"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.BaseCallback.on_rollout_end"], ["", "def", "_on_rollout_end", "(", "self", ")", "->", "None", ":", "\n", "        ", "for", "callback", "in", "self", ".", "callbacks", ":", "\n", "            ", "callback", ".", "on_rollout_end", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.CallbackList._on_training_end": [[175, 178], ["callback.on_training_end"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.BaseCallback.on_training_end"], ["", "", "def", "_on_training_end", "(", "self", ")", "->", "None", ":", "\n", "        ", "for", "callback", "in", "self", ".", "callbacks", ":", "\n", "            ", "callback", ".", "on_training_end", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.CheckpointCallback.__init__": [[189, 194], ["callbacks.BaseCallback.__init__"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__init__"], ["def", "__init__", "(", "self", ",", "save_freq", ":", "int", ",", "save_path", ":", "str", ",", "name_prefix", "=", "'rl_model'", ",", "verbose", "=", "0", ")", ":", "\n", "        ", "super", "(", "CheckpointCallback", ",", "self", ")", ".", "__init__", "(", "verbose", ")", "\n", "self", ".", "save_freq", "=", "save_freq", "\n", "self", ".", "save_path", "=", "save_path", "\n", "self", ".", "name_prefix", "=", "name_prefix", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.CheckpointCallback._init_callback": [[195, 199], ["os.makedirs"], "methods", ["None"], ["", "def", "_init_callback", "(", "self", ")", "->", "None", ":", "\n", "# Create folder if needed", "\n", "        ", "if", "self", ".", "save_path", "is", "not", "None", ":", "\n", "            ", "os", ".", "makedirs", "(", "self", ".", "save_path", ",", "exist_ok", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.CheckpointCallback._on_step": [[200, 207], ["os.path.join", "callbacks.CheckpointCallback.model.save", "print"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.VecNormalize.save"], ["", "", "def", "_on_step", "(", "self", ")", "->", "bool", ":", "\n", "        ", "if", "self", ".", "n_calls", "%", "self", ".", "save_freq", "==", "0", ":", "\n", "            ", "path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "save_path", ",", "'{}_{}_steps'", ".", "format", "(", "self", ".", "name_prefix", ",", "self", ".", "num_timesteps", ")", ")", "\n", "self", ".", "model", ".", "save", "(", "path", ")", "\n", "if", "self", ".", "verbose", ">", "1", ":", "\n", "                ", "print", "(", "\"Saving model checkpoint to {}\"", ".", "format", "(", "path", ")", ")", "\n", "", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.ConvertCallback.__init__": [[217, 220], ["callbacks.BaseCallback.__init__"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__init__"], ["def", "__init__", "(", "self", ",", "callback", ",", "verbose", "=", "0", ")", ":", "\n", "        ", "super", "(", "ConvertCallback", ",", "self", ")", ".", "__init__", "(", "verbose", ")", "\n", "self", ".", "callback", "=", "callback", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.ConvertCallback._on_step": [[221, 225], ["callbacks.ConvertCallback.callback"], "methods", ["None"], ["", "def", "_on_step", "(", "self", ")", "->", "bool", ":", "\n", "        ", "if", "self", ".", "callback", "is", "not", "None", ":", "\n", "            ", "return", "self", ".", "callback", "(", "self", ".", "locals", ",", "self", ".", "globals", ")", "\n", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.EvalCallback.__init__": [[246, 278], ["callbacks.EventCallback.__init__", "isinstance", "core.vec_env.DummyVecEnv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__init__"], ["def", "__init__", "(", "self", ",", "eval_env", ":", "Union", "[", "gym", ".", "Env", ",", "VecEnv", "]", ",", "\n", "callback_on_new_best", ":", "Optional", "[", "BaseCallback", "]", "=", "None", ",", "\n", "n_eval_episodes", ":", "int", "=", "5", ",", "\n", "eval_freq", ":", "int", "=", "10000", ",", "\n", "log_path", ":", "str", "=", "None", ",", "\n", "best_model_save_path", ":", "str", "=", "None", ",", "\n", "deterministic", ":", "bool", "=", "True", ",", "\n", "render", ":", "bool", "=", "False", ",", "\n", "verbose", ":", "int", "=", "1", ")", ":", "\n", "        ", "super", "(", "EvalCallback", ",", "self", ")", ".", "__init__", "(", "callback_on_new_best", ",", "verbose", "=", "verbose", ")", "\n", "self", ".", "n_eval_episodes", "=", "n_eval_episodes", "\n", "self", ".", "eval_freq", "=", "eval_freq", "\n", "self", ".", "best_mean_reward", "=", "-", "np", ".", "inf", "\n", "self", ".", "last_mean_reward", "=", "-", "np", ".", "inf", "\n", "self", ".", "deterministic", "=", "deterministic", "\n", "self", ".", "render", "=", "render", "\n", "\n", "# Convert to VecEnv for consistency", "\n", "if", "not", "isinstance", "(", "eval_env", ",", "VecEnv", ")", ":", "\n", "            ", "eval_env", "=", "DummyVecEnv", "(", "[", "lambda", ":", "eval_env", "]", ")", "\n", "\n", "", "assert", "eval_env", ".", "num_envs", "==", "1", ",", "\"You must pass only one environment for evaluation\"", "\n", "\n", "self", ".", "eval_env", "=", "eval_env", "\n", "self", ".", "best_model_save_path", "=", "best_model_save_path", "\n", "# Logs will be written in `evaluations.npz`", "\n", "if", "log_path", "is", "not", "None", ":", "\n", "            ", "log_path", "=", "os", ".", "path", ".", "join", "(", "log_path", ",", "'evaluations'", ")", "\n", "", "self", ".", "log_path", "=", "log_path", "\n", "self", ".", "evaluations_results", "=", "[", "]", "\n", "self", ".", "evaluations_timesteps", "=", "[", "]", "\n", "self", ".", "evaluations_length", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.EvalCallback._init_callback": [[279, 290], ["warnings.warn", "os.makedirs", "os.makedirs", "type", "type", "os.path.dirname"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.warn"], ["", "def", "_init_callback", "(", "self", ")", ":", "\n", "# Does not work in some corner cases, where the wrapper is not the same", "\n", "        ", "if", "not", "type", "(", "self", ".", "training_env", ")", "is", "type", "(", "self", ".", "eval_env", ")", ":", "\n", "            ", "warnings", ".", "warn", "(", "\"Training and eval env are not of the same type\"", "\n", "\"{} != {}\"", ".", "format", "(", "self", ".", "training_env", ",", "self", ".", "eval_env", ")", ")", "\n", "\n", "# Create folders if needed", "\n", "", "if", "self", ".", "best_model_save_path", "is", "not", "None", ":", "\n", "            ", "os", ".", "makedirs", "(", "self", ".", "best_model_save_path", ",", "exist_ok", "=", "True", ")", "\n", "", "if", "self", ".", "log_path", "is", "not", "None", ":", "\n", "            ", "os", ".", "makedirs", "(", "os", ".", "path", ".", "dirname", "(", "self", ".", "log_path", ")", ",", "exist_ok", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.EvalCallback._on_step": [[291, 331], ["core.vec_env.sync_envs_normalization", "core.evaluation.evaluate_policy", "callbacks.EvalCallback.evaluations_timesteps.append", "callbacks.EvalCallback.evaluations_results.append", "callbacks.EvalCallback.evaluations_length.append", "numpy.savez", "numpy.mean", "numpy.std", "numpy.mean", "numpy.std", "print", "print", "print", "callbacks.EvalCallback.model.save", "callbacks.EvalCallback._on_event", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.__init__.sync_envs_normalization", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.evaluation.evaluate_policy", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.VecNormalize.save", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.EventCallback._on_event"], ["", "", "def", "_on_step", "(", "self", ")", "->", "bool", ":", "\n", "\n", "        ", "if", "self", ".", "eval_freq", ">", "0", "and", "self", ".", "n_calls", "%", "self", ".", "eval_freq", "==", "0", ":", "\n", "# Sync training and eval env if there is VecNormalize", "\n", "            ", "sync_envs_normalization", "(", "self", ".", "training_env", ",", "self", ".", "eval_env", ")", "\n", "\n", "episode_rewards", ",", "episode_lengths", "=", "evaluate_policy", "(", "self", ".", "model", ",", "self", ".", "eval_env", ",", "\n", "n_eval_episodes", "=", "self", ".", "n_eval_episodes", ",", "\n", "render", "=", "self", ".", "render", ",", "\n", "deterministic", "=", "self", ".", "deterministic", ",", "\n", "return_episode_rewards", "=", "True", ")", "\n", "\n", "if", "self", ".", "log_path", "is", "not", "None", ":", "\n", "                ", "self", ".", "evaluations_timesteps", ".", "append", "(", "self", ".", "num_timesteps", ")", "\n", "self", ".", "evaluations_results", ".", "append", "(", "episode_rewards", ")", "\n", "self", ".", "evaluations_length", ".", "append", "(", "episode_lengths", ")", "\n", "np", ".", "savez", "(", "self", ".", "log_path", ",", "timesteps", "=", "self", ".", "evaluations_timesteps", ",", "\n", "results", "=", "self", ".", "evaluations_results", ",", "ep_lengths", "=", "self", ".", "evaluations_length", ")", "\n", "\n", "", "mean_reward", ",", "std_reward", "=", "np", ".", "mean", "(", "episode_rewards", ")", ",", "np", ".", "std", "(", "episode_rewards", ")", "\n", "mean_ep_length", ",", "std_ep_length", "=", "np", ".", "mean", "(", "episode_lengths", ")", ",", "np", ".", "std", "(", "episode_lengths", ")", "\n", "# Keep track of the last evaluation, useful for classes that derive from this callback", "\n", "self", ".", "last_mean_reward", "=", "mean_reward", "\n", "\n", "if", "self", ".", "verbose", ">", "0", ":", "\n", "                ", "print", "(", "\"Eval num_timesteps={}, \"", "\n", "\"episode_reward={:.2f} +/- {:.2f}\"", ".", "format", "(", "self", ".", "num_timesteps", ",", "mean_reward", ",", "std_reward", ")", ")", "\n", "print", "(", "\"Episode length: {:.2f} +/- {:.2f}\"", ".", "format", "(", "mean_ep_length", ",", "std_ep_length", ")", ")", "\n", "\n", "", "if", "mean_reward", ">", "self", ".", "best_mean_reward", ":", "\n", "                ", "if", "self", ".", "verbose", ">", "0", ":", "\n", "                    ", "print", "(", "\"New best mean reward!\"", ")", "\n", "", "if", "self", ".", "best_model_save_path", "is", "not", "None", ":", "\n", "                    ", "self", ".", "model", ".", "save", "(", "os", ".", "path", ".", "join", "(", "self", ".", "best_model_save_path", ",", "'best_model'", ")", ")", "\n", "", "self", ".", "best_mean_reward", "=", "mean_reward", "\n", "# Trigger callback if needed", "\n", "if", "self", ".", "callback", "is", "not", "None", ":", "\n", "                    ", "return", "self", ".", "_on_event", "(", ")", "\n", "\n", "", "", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.StopTrainingOnRewardThreshold.__init__": [[345, 348], ["callbacks.BaseCallback.__init__"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__init__"], ["def", "__init__", "(", "self", ",", "reward_threshold", ":", "float", ",", "verbose", ":", "int", "=", "0", ")", ":", "\n", "        ", "super", "(", "StopTrainingOnRewardThreshold", ",", "self", ")", ".", "__init__", "(", "verbose", "=", "verbose", ")", "\n", "self", ".", "reward_threshold", "=", "reward_threshold", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.StopTrainingOnRewardThreshold._on_step": [[349, 358], ["bool", "print"], "methods", ["None"], ["", "def", "_on_step", "(", "self", ")", "->", "bool", ":", "\n", "        ", "assert", "self", ".", "parent", "is", "not", "None", ",", "(", "\"`StopTrainingOnRewardThreshold` callback must be used \"", "\n", "\"with an `EvalCallback`\"", ")", "\n", "# Convert np.bool to bool, otherwise callback.on_step() is False won't work", "\n", "continue_training", "=", "bool", "(", "self", ".", "parent", ".", "best_mean_reward", "<", "self", ".", "reward_threshold", ")", "\n", "if", "self", ".", "verbose", ">", "0", "and", "not", "continue_training", ":", "\n", "            ", "print", "(", "\"Stopping training because the mean reward {:.2f} \"", "\n", "\" is above the threshold {}\"", ".", "format", "(", "self", ".", "parent", ".", "best_mean_reward", ",", "self", ".", "reward_threshold", ")", ")", "\n", "", "return", "continue_training", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.EveryNTimesteps.__init__": [[369, 373], ["callbacks.EventCallback.__init__"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__init__"], ["def", "__init__", "(", "self", ",", "n_steps", ":", "int", ",", "callback", ":", "BaseCallback", ")", ":", "\n", "        ", "super", "(", "EveryNTimesteps", ",", "self", ")", ".", "__init__", "(", "callback", ")", "\n", "self", ".", "n_steps", "=", "n_steps", "\n", "self", ".", "last_time_trigger", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.EveryNTimesteps._on_step": [[374, 379], ["callbacks.EveryNTimesteps._on_event"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.callbacks.EventCallback._on_event"], ["", "def", "_on_step", "(", "self", ")", "->", "bool", ":", "\n", "        ", "if", "(", "self", ".", "num_timesteps", "-", "self", ".", "last_time_trigger", ")", ">=", "self", ".", "n_steps", ":", "\n", "            ", "self", ".", "last_time_trigger", "=", "self", ".", "num_timesteps", "\n", "return", "self", ".", "_on_event", "(", ")", "\n", "", "return", "True", "\n", "", "", ""]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.monitor.Monitor.__init__": [[19, 61], ["gym.Wrapper.__init__", "time.time", "open", "monitor.Monitor.file_handler.write", "csv.DictWriter", "monitor.Monitor.logger.writeheader", "monitor.Monitor.file_handler.flush", "os.path.join.endswith", "os.path.isdir", "os.path.join", "json.dumps"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__init__"], ["def", "__init__", "(", "self", ",", "\n", "env", ":", "gym", ".", "Env", ",", "\n", "filename", ":", "Optional", "[", "str", "]", ",", "\n", "allow_early_resets", ":", "bool", "=", "True", ",", "\n", "reset_keywords", "=", "(", ")", ",", "\n", "info_keywords", "=", "(", ")", ")", ":", "\n", "        ", "\"\"\"\n        A monitor wrapper for Gym environments, it is used to know the episode reward, length, time and other data.\n        :param env: (gym.Env) The environment\n        :param filename: (Optional[str]) the location to save a log file, can be None for no log\n        :param allow_early_resets: (bool) allows the reset of the environment before it is done\n        :param reset_keywords: (tuple) extra keywords for the reset call, if extra parameters are needed at reset\n        :param info_keywords: (tuple) extra information to log, from the information return of environment.step\n        \"\"\"", "\n", "super", "(", "Monitor", ",", "self", ")", ".", "__init__", "(", "env", "=", "env", ")", "\n", "self", ".", "t_start", "=", "time", ".", "time", "(", ")", "\n", "if", "filename", "is", "None", ":", "\n", "            ", "self", ".", "file_handler", "=", "None", "\n", "self", ".", "logger", "=", "None", "\n", "", "else", ":", "\n", "            ", "if", "not", "filename", ".", "endswith", "(", "Monitor", ".", "EXT", ")", ":", "\n", "                ", "if", "os", ".", "path", ".", "isdir", "(", "filename", ")", ":", "\n", "                    ", "filename", "=", "os", ".", "path", ".", "join", "(", "filename", ",", "Monitor", ".", "EXT", ")", "\n", "", "else", ":", "\n", "                    ", "filename", "=", "filename", "+", "\".\"", "+", "Monitor", ".", "EXT", "\n", "", "", "self", ".", "file_handler", "=", "open", "(", "filename", ",", "\"wt\"", ")", "\n", "self", ".", "file_handler", ".", "write", "(", "'#%s\\n'", "%", "json", ".", "dumps", "(", "{", "\"t_start\"", ":", "self", ".", "t_start", ",", "'env_id'", ":", "env", ".", "spec", "and", "env", ".", "spec", ".", "id", "}", ")", ")", "\n", "self", ".", "logger", "=", "csv", ".", "DictWriter", "(", "self", ".", "file_handler", ",", "\n", "fieldnames", "=", "(", "'r'", ",", "'l'", ",", "'t'", ")", "+", "reset_keywords", "+", "info_keywords", ")", "\n", "self", ".", "logger", ".", "writeheader", "(", ")", "\n", "self", ".", "file_handler", ".", "flush", "(", ")", "\n", "\n", "", "self", ".", "reset_keywords", "=", "reset_keywords", "\n", "self", ".", "info_keywords", "=", "info_keywords", "\n", "self", ".", "allow_early_resets", "=", "allow_early_resets", "\n", "self", ".", "rewards", "=", "None", "\n", "self", ".", "needs_reset", "=", "True", "\n", "self", ".", "episode_rewards", "=", "[", "]", "\n", "self", ".", "episode_lengths", "=", "[", "]", "\n", "self", ".", "episode_times", "=", "[", "]", "\n", "self", ".", "total_steps", "=", "0", "\n", "self", ".", "current_reset_info", "=", "{", "}", "# extra info about the current episode, that was passed in during reset()", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.monitor.Monitor.reset": [[62, 79], ["monitor.Monitor.env.reset", "RuntimeError", "kwargs.get", "ValueError"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.reset"], ["", "def", "reset", "(", "self", ",", "**", "kwargs", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"\n        Calls the Gym environment reset. Can only be called if the environment is over, or if allow_early_resets is True\n        :param kwargs: Extra keywords saved for the next episode. only if defined by reset_keywords\n        :return: (np.ndarray) the first observation of the environment\n        \"\"\"", "\n", "if", "not", "self", ".", "allow_early_resets", "and", "not", "self", ".", "needs_reset", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"Tried to reset an environment before done. If you want to allow early resets, \"", "\n", "\"wrap your env with Monitor(env, path, allow_early_resets=True)\"", ")", "\n", "", "self", ".", "rewards", "=", "[", "]", "\n", "self", ".", "needs_reset", "=", "False", "\n", "for", "key", "in", "self", ".", "reset_keywords", ":", "\n", "            ", "value", "=", "kwargs", ".", "get", "(", "key", ")", "\n", "if", "value", "is", "None", ":", "\n", "                ", "raise", "ValueError", "(", "'Expected you to pass kwarg {} into reset'", ".", "format", "(", "key", ")", ")", "\n", "", "self", ".", "current_reset_info", "[", "key", "]", "=", "value", "\n", "", "return", "self", ".", "env", ".", "reset", "(", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.monitor.Monitor.step": [[80, 107], ["monitor.Monitor.env.step", "monitor.Monitor.rewards.append", "RuntimeError", "sum", "len", "monitor.Monitor.episode_rewards.append", "monitor.Monitor.episode_lengths.append", "monitor.Monitor.episode_times.append", "ep_info.update", "round", "round", "monitor.Monitor.logger.writerow", "monitor.Monitor.file_handler.flush", "time.time", "time.time"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnv.step", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.math_util.RunningMeanStd.update"], ["", "def", "step", "(", "self", ",", "action", ":", "np", ".", "ndarray", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "float", ",", "bool", ",", "Dict", "[", "Any", ",", "Any", "]", "]", ":", "\n", "        ", "\"\"\"\n        Step the environment with the given action\n        :param action: (np.ndarray) the action\n        :return: (Tuple[np.ndarray, float, bool, Dict[Any, Any]]) observation, reward, done, information\n        \"\"\"", "\n", "if", "self", ".", "needs_reset", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"Tried to step environment that needs reset\"", ")", "\n", "", "observation", ",", "reward", ",", "done", ",", "info", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "self", ".", "rewards", ".", "append", "(", "reward", ")", "\n", "if", "done", ":", "\n", "            ", "self", ".", "needs_reset", "=", "True", "\n", "ep_rew", "=", "sum", "(", "self", ".", "rewards", ")", "\n", "eplen", "=", "len", "(", "self", ".", "rewards", ")", "\n", "ep_info", "=", "{", "\"r\"", ":", "round", "(", "ep_rew", ",", "6", ")", ",", "\"l\"", ":", "eplen", ",", "\"t\"", ":", "round", "(", "time", ".", "time", "(", ")", "-", "self", ".", "t_start", ",", "6", ")", "}", "\n", "for", "key", "in", "self", ".", "info_keywords", ":", "\n", "                ", "ep_info", "[", "key", "]", "=", "info", "[", "key", "]", "\n", "", "self", ".", "episode_rewards", ".", "append", "(", "ep_rew", ")", "\n", "self", ".", "episode_lengths", ".", "append", "(", "eplen", ")", "\n", "self", ".", "episode_times", ".", "append", "(", "time", ".", "time", "(", ")", "-", "self", ".", "t_start", ")", "\n", "ep_info", ".", "update", "(", "self", ".", "current_reset_info", ")", "\n", "if", "self", ".", "logger", ":", "\n", "                ", "self", ".", "logger", ".", "writerow", "(", "ep_info", ")", "\n", "self", ".", "file_handler", ".", "flush", "(", ")", "\n", "", "info", "[", "'episode'", "]", "=", "ep_info", "\n", "", "self", ".", "total_steps", "+=", "1", "\n", "return", "observation", ",", "reward", ",", "done", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.monitor.Monitor.close": [[108, 115], ["super().close", "monitor.Monitor.file_handler.close"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.close", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.close"], ["", "def", "close", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Closes the environment\n        \"\"\"", "\n", "super", "(", "Monitor", ",", "self", ")", ".", "close", "(", ")", "\n", "if", "self", ".", "file_handler", "is", "not", "None", ":", "\n", "            ", "self", ".", "file_handler", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.monitor.Monitor.get_total_steps": [[116, 122], ["None"], "methods", ["None"], ["", "", "def", "get_total_steps", "(", "self", ")", "->", "int", ":", "\n", "        ", "\"\"\"\n        Returns the total number of timesteps\n        :return: (int)\n        \"\"\"", "\n", "return", "self", ".", "total_steps", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.monitor.Monitor.get_episode_rewards": [[123, 129], ["None"], "methods", ["None"], ["", "def", "get_episode_rewards", "(", "self", ")", "->", "List", "[", "float", "]", ":", "\n", "        ", "\"\"\"\n        Returns the rewards of all the episodes\n        :return: ([float])\n        \"\"\"", "\n", "return", "self", ".", "episode_rewards", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.monitor.Monitor.get_episode_lengths": [[130, 136], ["None"], "methods", ["None"], ["", "def", "get_episode_lengths", "(", "self", ")", "->", "List", "[", "int", "]", ":", "\n", "        ", "\"\"\"\n        Returns the number of timesteps of all the episodes\n        :return: ([int])\n        \"\"\"", "\n", "return", "self", ".", "episode_lengths", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.monitor.Monitor.get_episode_times": [[137, 143], ["None"], "methods", ["None"], ["", "def", "get_episode_times", "(", "self", ")", "->", "List", "[", "float", "]", ":", "\n", "        ", "\"\"\"\n        Returns the runtime in seconds of all the episodes\n        :return: ([float])\n        \"\"\"", "\n", "return", "self", ".", "episode_times", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.monitor.get_monitor_files": [[152, 159], ["glob.glob", "os.path.join"], "function", ["None"], ["", "def", "get_monitor_files", "(", "path", ":", "str", ")", "->", "List", "[", "str", "]", ":", "\n", "    ", "\"\"\"\n    get all the monitor files in the given path\n    :param path: (str) the logging folder\n    :return: ([str]) the log files\n    \"\"\"", "\n", "return", "glob", "(", "os", ".", "path", ".", "join", "(", "path", ",", "\"*\"", "+", "Monitor", ".", "EXT", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.monitor.load_results": [[161, 200], ["pandas.concat", "pandas.DataFrame.sort_values", "pandas.DataFrame.reset_index", "min", "glob.glob", "monitor.get_monitor_files", "monitor.LoadMonitorResultsError", "data_frames.append", "os.path.join", "open", "file_name.endswith", "file_handler.readline", "json.loads", "pandas.read_csv", "headers.append", "file_name.endswith", "file_handler.readlines", "json.loads", "headers.append", "pandas.DataFrame", "json.loads", "episodes.append"], "function", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.monitor.get_monitor_files", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.read_csv"], ["", "def", "load_results", "(", "path", ":", "str", ")", "->", "pandas", ".", "DataFrame", ":", "\n", "    ", "\"\"\"\n    Load all Monitor logs from a given directory path matching ``*monitor.csv`` and ``*monitor.json``\n    :param path: (str) the directory path containing the log file(s)\n    :return: (pandas.DataFrame) the logged data\n    \"\"\"", "\n", "# get both csv and (old) json files", "\n", "monitor_files", "=", "(", "glob", "(", "os", ".", "path", ".", "join", "(", "path", ",", "\"*monitor.json\"", ")", ")", "+", "get_monitor_files", "(", "path", ")", ")", "\n", "if", "not", "monitor_files", ":", "\n", "        ", "raise", "LoadMonitorResultsError", "(", "\"no monitor files of the form *%s found in %s\"", "%", "(", "Monitor", ".", "EXT", ",", "path", ")", ")", "\n", "", "data_frames", "=", "[", "]", "\n", "headers", "=", "[", "]", "\n", "for", "file_name", "in", "monitor_files", ":", "\n", "        ", "with", "open", "(", "file_name", ",", "'rt'", ")", "as", "file_handler", ":", "\n", "            ", "if", "file_name", ".", "endswith", "(", "'csv'", ")", ":", "\n", "                ", "first_line", "=", "file_handler", ".", "readline", "(", ")", "\n", "assert", "first_line", "[", "0", "]", "==", "'#'", "\n", "header", "=", "json", ".", "loads", "(", "first_line", "[", "1", ":", "]", ")", "\n", "data_frame", "=", "pandas", ".", "read_csv", "(", "file_handler", ",", "index_col", "=", "None", ")", "\n", "headers", ".", "append", "(", "header", ")", "\n", "", "elif", "file_name", ".", "endswith", "(", "'json'", ")", ":", "# Deprecated json format", "\n", "                ", "episodes", "=", "[", "]", "\n", "lines", "=", "file_handler", ".", "readlines", "(", ")", "\n", "header", "=", "json", ".", "loads", "(", "lines", "[", "0", "]", ")", "\n", "headers", ".", "append", "(", "header", ")", "\n", "for", "line", "in", "lines", "[", "1", ":", "]", ":", "\n", "                    ", "episode", "=", "json", ".", "loads", "(", "line", ")", "\n", "episodes", ".", "append", "(", "episode", ")", "\n", "", "data_frame", "=", "pandas", ".", "DataFrame", "(", "episodes", ")", "\n", "", "else", ":", "\n", "                ", "assert", "0", ",", "'unreachable'", "\n", "", "data_frame", "[", "'t'", "]", "+=", "header", "[", "'t_start'", "]", "\n", "", "data_frames", ".", "append", "(", "data_frame", ")", "\n", "", "data_frame", "=", "pandas", ".", "concat", "(", "data_frames", ")", "\n", "data_frame", ".", "sort_values", "(", "'t'", ",", "inplace", "=", "True", ")", "\n", "data_frame", ".", "reset_index", "(", "inplace", "=", "True", ")", "\n", "data_frame", "[", "'t'", "]", "-=", "min", "(", "header", "[", "'t_start'", "]", "for", "header", "in", "headers", ")", "\n", "# data_frame.headers = headers  # HACK to preserve backwards compatibility", "\n", "return", "data_frame", "\n", "", ""]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.evaluation.evaluate_policy": [[6, 58], ["isinstance", "range", "numpy.mean", "numpy.std", "env.reset", "episode_rewards.append", "episode_lengths.append", "model.predict", "env.step", "callback", "env.render", "locals", "globals"], "function", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.reset", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.base_class.OffPolicyRLModel.predict", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnv.step", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.render"], ["def", "evaluate_policy", "(", "model", ",", "env", ",", "n_eval_episodes", "=", "10", ",", "deterministic", "=", "True", ",", "\n", "render", "=", "False", ",", "callback", "=", "None", ",", "reward_threshold", "=", "None", ",", "\n", "return_episode_rewards", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Runs policy for `n_eval_episodes` episodes and returns average reward.\n    This is made to work only with one env.\n\n    :param model: (BaseRLModel) The RL agent you want to evaluate.\n    :param env: (gym.Env or VecEnv) The gym environment. In the case of a `VecEnv`\n        this must contain only one environment.\n    :param n_eval_episodes: (int) Number of episode to evaluate the agent\n    :param deterministic: (bool) Whether to use deterministic or stochastic actions\n    :param render: (bool) Whether to render the environment or not\n    :param callback: (callable) callback function to do additional checks,\n        called after each step.\n    :param reward_threshold: (float) Minimum expected reward per episode,\n        this will raise an error if the performance is not met\n    :param return_episode_rewards: (bool) If True, a list of reward per episode\n        will be returned instead of the mean.\n    :return: (float, float) Mean reward per episode, std of reward per episode\n        returns ([float], [int]) when `return_episode_rewards` is True\n    \"\"\"", "\n", "if", "isinstance", "(", "env", ",", "VecEnv", ")", ":", "\n", "        ", "assert", "env", ".", "num_envs", "==", "1", ",", "\"You must pass only one environment when using this function\"", "\n", "\n", "", "episode_rewards", ",", "episode_lengths", "=", "[", "]", ",", "[", "]", "\n", "for", "_", "in", "range", "(", "n_eval_episodes", ")", ":", "\n", "        ", "obs", "=", "env", ".", "reset", "(", ")", "\n", "done", ",", "state", "=", "False", ",", "None", "\n", "episode_reward", "=", "0.0", "\n", "episode_length", "=", "0", "\n", "while", "not", "done", ":", "\n", "            ", "action", ",", "state", "=", "model", ".", "predict", "(", "obs", ",", "state", "=", "state", ",", "deterministic", "=", "deterministic", ")", "\n", "obs", ",", "reward", ",", "done", ",", "_info", "=", "env", ".", "step", "(", "action", ")", "\n", "episode_reward", "+=", "reward", "\n", "if", "callback", "is", "not", "None", ":", "\n", "                ", "callback", "(", "locals", "(", ")", ",", "globals", "(", ")", ")", "\n", "", "episode_length", "+=", "1", "\n", "if", "render", ":", "\n", "                ", "env", ".", "render", "(", ")", "\n", "", "", "episode_rewards", ".", "append", "(", "episode_reward", ")", "\n", "episode_lengths", ".", "append", "(", "episode_length", ")", "\n", "\n", "", "mean_reward", "=", "np", ".", "mean", "(", "episode_rewards", ")", "\n", "std_reward", "=", "np", ".", "std", "(", "episode_rewards", ")", "\n", "\n", "if", "reward_threshold", "is", "not", "None", ":", "\n", "        ", "assert", "mean_reward", ">", "reward_threshold", ",", "'Mean reward below threshold: '", "'{:.2f} < {:.2f}'", ".", "format", "(", "mean_reward", ",", "reward_threshold", ")", "\n", "", "if", "return_episode_rewards", ":", "\n", "        ", "return", "episode_rewards", ",", "episode_lengths", "\n", "", "return", "mean_reward", ",", "std_reward", "\n", "", ""]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.mpi_utils.gather_global_data": [[10, 30], ["range", "numpy.empty", "len", "comm.Allgather", "len", "tuple", "numpy.array", "range", "type", "comm.Get_size", "len"], "function", ["None"], ["def", "gather_global_data", "(", "comm", ",", "*", "args", ")", ":", "\n", "    ", "\"\"\"Takes an arbitrary number (say N) of variables (*args), aggregates each variable\n    among all workers and outputs N numpy arrays being aggregations of these variables\n    Ex:\n    Worker 1 has x = 20, y = 40\n    Worker 2 has x = 21, y = 41\n    global_x, global_y = gather_global_data(comm, x, y)\n    All workers have global_x = [[20],[21]], global_y = [[40],[41]]\n    \"\"\"", "\n", "locals", "=", "[", "np", ".", "array", "(", "[", "arg", "]", ")", "if", "type", "(", "arg", ")", "is", "not", "np", ".", "ndarray", "else", "arg", "for", "arg", "in", "args", "]", "\n", "globals", "=", "[", "\n", "np", ".", "empty", "(", "(", "comm", ".", "Get_size", "(", ")", ",", "*", "locals", "[", "i", "]", ".", "shape", ")", ",", "dtype", "=", "locals", "[", "i", "]", ".", "dtype", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "locals", ")", ")", "\n", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "locals", ")", ")", ":", "\n", "        ", "comm", ".", "Allgather", "(", "locals", "[", "i", "]", ",", "globals", "[", "i", "]", ")", "\n", "", "if", "len", "(", "globals", ")", "==", "1", ":", "\n", "        ", "return", "globals", "[", "0", "]", "\n", "", "else", ":", "\n", "        ", "return", "tuple", "(", "globals", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.mpi_utils.sync_tensor_grads": [[32, 42], ["tensor.grad.numpy().flatten", "numpy.zeros_like", "comm.Allreduce", "comm.Get_size", "torch.tensor", "tensor.grad.numpy"], "function", ["None"], ["", "", "def", "sync_tensor_grads", "(", "tensor", ",", "comm", "=", "None", ")", ":", "\n", "    ", "\"\"\"Sync for scalar tensors of shape (1,)\"\"\"", "\n", "assert", "tensor", ".", "shape", "==", "(", "1", ",", ")", ",", "\"non supported tensor shape\"", "\n", "if", "comm", "is", "None", ":", "\n", "        ", "comm", "=", "MPI", ".", "COMM_WORLD", "\n", "", "flat_grads", "=", "tensor", ".", "grad", ".", "numpy", "(", ")", ".", "flatten", "(", ")", "\n", "global_grads", "=", "np", ".", "zeros_like", "(", "flat_grads", ")", "\n", "comm", ".", "Allreduce", "(", "flat_grads", ",", "global_grads", ",", "op", "=", "MPI", ".", "SUM", ")", "\n", "global_grads", "/=", "comm", ".", "Get_size", "(", ")", "\n", "tensor", ".", "grad", "[", "0", "]", "=", "torch", ".", "tensor", "(", "global_grads", "[", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.mpi_utils.sync_networks": [[44, 52], ["mpi_utils._get_flat_params_or_grads", "comm.Bcast", "mpi_utils._set_flat_params_or_grads"], "function", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.mpi_utils._get_flat_params_or_grads", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.mpi_utils._set_flat_params_or_grads"], ["", "def", "sync_networks", "(", "network", ",", "comm", "=", "None", ")", ":", "\n", "    ", "\"\"\"Synchronize neural nets between threads\"\"\"", "\n", "if", "comm", "is", "None", ":", "\n", "        ", "comm", "=", "MPI", ".", "COMM_WORLD", "\n", "", "flat_params", "=", "_get_flat_params_or_grads", "(", "network", ",", "mode", "=", "\"params\"", ")", "\n", "comm", ".", "Bcast", "(", "flat_params", ",", "root", "=", "0", ")", "\n", "# set the flat params back to the network", "\n", "_set_flat_params_or_grads", "(", "network", ",", "flat_params", ",", "mode", "=", "\"params\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.mpi_utils.sync_grads": [[54, 63], ["mpi_utils._get_flat_params_or_grads", "numpy.zeros_like", "comm.Allreduce", "comm.Get_size", "mpi_utils._set_flat_params_or_grads"], "function", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.mpi_utils._get_flat_params_or_grads", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.mpi_utils._set_flat_params_or_grads"], ["", "def", "sync_grads", "(", "network", ",", "comm", "=", "None", ")", ":", "\n", "    ", "\"\"\"Synchronize gradients between threads\"\"\"", "\n", "if", "comm", "is", "None", ":", "\n", "        ", "comm", "=", "MPI", ".", "COMM_WORLD", "\n", "", "flat_grads", "=", "_get_flat_params_or_grads", "(", "network", ",", "mode", "=", "\"grads\"", ")", "\n", "global_grads", "=", "np", ".", "zeros_like", "(", "flat_grads", ")", "\n", "comm", ".", "Allreduce", "(", "flat_grads", ",", "global_grads", ",", "op", "=", "MPI", ".", "SUM", ")", "\n", "global_grads", "/=", "comm", ".", "Get_size", "(", ")", "\n", "_set_flat_params_or_grads", "(", "network", ",", "global_grads", ",", "mode", "=", "\"grads\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.mpi_utils.print_from_master_node": [[65, 71], ["mpi4py.MPI.COMM_WORLD.Get_rank", "print", "print"], "function", ["None"], ["", "def", "print_from_master_node", "(", "s", ",", "end", "=", "None", ")", ":", "\n", "    ", "if", "MPI", ".", "COMM_WORLD", ".", "Get_rank", "(", ")", "==", "0", ":", "\n", "        ", "if", "end", "is", "not", "None", ":", "\n", "            ", "print", "(", "s", ",", "end", "=", "end", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "s", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.mpi_utils._get_flat_params_or_grads": [[73, 81], ["numpy.concatenate", "getattr", "p.cpu().numpy().flatten", "network.parameters", "p.cpu().numpy", "p.cpu"], "function", ["None"], ["", "", "", "def", "_get_flat_params_or_grads", "(", "network", ",", "mode", "=", "\"params\"", ")", ":", "\n", "    ", "\"\"\"include two kinds: grads and params\"\"\"", "\n", "attr", "=", "\"data\"", "if", "mode", "==", "\"params\"", "else", "\"grad\"", "\n", "params_or_grads_list", "=", "[", "getattr", "(", "param", ",", "attr", ")", "for", "param", "in", "network", ".", "parameters", "(", ")", "]", "\n", "params_or_grads_list", "=", "[", "\n", "p", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "flatten", "(", ")", "for", "p", "in", "params_or_grads_list", "if", "p", "is", "not", "None", "\n", "]", "\n", "return", "np", ".", "concatenate", "(", "params_or_grads_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.mpi_utils._set_flat_params_or_grads": [[83, 96], ["network.parameters", "getattr", "getattr().copy_", "param.data.numel", "torch.tensor().view_as", "getattr", "torch.tensor", "param.data.numel"], "function", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util.numel", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.tf_util.numel"], ["", "def", "_set_flat_params_or_grads", "(", "network", ",", "flat_params", ",", "mode", "=", "\"params\"", ")", ":", "\n", "    ", "\"\"\"include two kinds: grads and params\"\"\"", "\n", "attr", "=", "\"data\"", "if", "mode", "==", "\"params\"", "else", "\"grad\"", "\n", "# the pointer", "\n", "pointer", "=", "0", "\n", "for", "param", "in", "network", ".", "parameters", "(", ")", ":", "\n", "        ", "if", "getattr", "(", "param", ",", "attr", ")", "is", "not", "None", ":", "\n", "            ", "getattr", "(", "param", ",", "attr", ")", ".", "copy_", "(", "\n", "torch", ".", "tensor", "(", "\n", "flat_params", "[", "pointer", ":", "pointer", "+", "param", ".", "data", ".", "numel", "(", ")", "]", "\n", ")", ".", "view_as", "(", "param", ".", "data", ")", "\n", ")", "\n", "pointer", "+=", "param", ".", "data", ".", "numel", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.mpi_utils.sync_buffers": [[98, 140], ["replay_buffer_per_worker.reset", "comm.allgather", "range", "range", "len", "len", "replay_buffer.add", "replay_buffer.add", "replay_buffer.add", "replay_buffer.add", "behavior.astype", "obs.astype", "n_obs.astype", "obs.astype", "n_obs.astype", "behavior.astype"], "function", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.reset", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.replay_buffer.ReplayBuffer.add", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.replay_buffer.ReplayBuffer.add", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.replay_buffer.ReplayBuffer.add", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.replay_buffer.ReplayBuffer.add"], ["", "", "", "def", "sync_buffers", "(", "\n", "replay_buffer_per_worker", ",", "replay_buffer", ",", "store_in_32", "=", "True", ",", "between_threads", "=", "True", "\n", ")", ":", "\n", "    ", "\"\"\"Synchronize buffers between threads\"\"\"", "\n", "if", "between_threads", ":", "\n", "        ", "comm", "=", "MPI", ".", "COMM_WORLD", "\n", "buffers", "=", "comm", ".", "allgather", "(", "replay_buffer_per_worker", ".", "_storage", ")", "\n", "for", "buffer", "in", "buffers", ":", "\n", "            ", "for", "i", "in", "range", "(", "len", "(", "buffer", ")", ")", ":", "\n", "                ", "if", "store_in_32", ":", "\n", "                    ", "obs", ",", "n_obs", ",", "action", ",", "reward", ",", "done_bool", ",", "behavior", "=", "buffer", "[", "i", "]", "\n", "replay_buffer", ".", "add", "(", "\n", "(", "\n", "obs", ".", "astype", "(", "\"f\"", ")", ",", "\n", "n_obs", ".", "astype", "(", "\"f\"", ")", ",", "\n", "action", ",", "\n", "reward", ",", "\n", "done_bool", ",", "\n", "behavior", ".", "astype", "(", "\"f\"", ")", ",", "\n", ")", "\n", ")", "\n", "", "else", ":", "\n", "                    ", "replay_buffer", ".", "add", "(", "buffer", "[", "i", "]", ")", "\n", "", "", "", "", "else", ":", "\n", "        ", "for", "i", "in", "range", "(", "len", "(", "replay_buffer_per_worker", ")", ")", ":", "\n", "            ", "if", "store_in_32", ":", "\n", "                ", "(", "\n", "obs", ",", "\n", "n_obs", ",", "\n", "action", ",", "\n", "reward", ",", "\n", "done_bool", ",", "\n", "behavior", ",", "\n", ")", "=", "replay_buffer_per_worker", "[", "i", "]", "\n", "replay_buffer", ".", "add", "(", "\n", "(", "obs", ".", "astype", "(", "\"f\"", ")", ",", "n_obs", ".", "astype", "(", "\"f\"", ")", ",", "action", ",", "reward", ",", "done_bool", ")", ",", "\n", "behavior", ".", "astype", "(", "\"f\"", ")", ",", "\n", ")", "\n", "", "else", ":", "\n", "                ", "replay_buffer", ".", "add", "(", "replay_buffer_per_worker", "[", "i", "]", ")", "\n", "\n", "", "", "", "replay_buffer_per_worker", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.mpi_utils.gather_buffers": [[142, 159], ["range", "len", "replay_buffer.add", "replay_buffer.add", "obs.astype", "n_obs.astype", "behavior.astype"], "function", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.replay_buffer.ReplayBuffer.add", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.replay_buffer.ReplayBuffer.add"], ["", "def", "gather_buffers", "(", "buffers", ",", "replay_buffer", ",", "store_in_32", "=", "False", ")", ":", "\n", "    ", "for", "buffer", "in", "buffers", ":", "\n", "        ", "for", "i", "in", "range", "(", "len", "(", "buffer", ")", ")", ":", "\n", "            ", "if", "store_in_32", ":", "\n", "                ", "obs", ",", "n_obs", ",", "action", ",", "reward", ",", "done_bool", ",", "behavior", "=", "buffer", "[", "i", "]", "\n", "replay_buffer", ".", "add", "(", "\n", "(", "\n", "obs", ".", "astype", "(", "\"f\"", ")", ",", "\n", "n_obs", ".", "astype", "(", "\"f\"", ")", ",", "\n", "action", ",", "\n", "reward", ",", "\n", "done_bool", ",", "\n", "behavior", ".", "astype", "(", "\"f\"", ")", ",", "\n", ")", "\n", ")", "\n", "", "else", ":", "\n", "                ", "replay_buffer", ".", "add", "(", "buffer", "[", "i", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.mpi_utils.sync_archive": [[161, 180], ["comm.allgather", "comm.allgather", "comm.allgather", "comm.allgather", "range", "len", "archive.add"], "function", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.replay_buffer.ReplayBuffer.add"], ["", "", "", "", "def", "sync_archive", "(", "\n", "archive", ",", "\n", "params_per_worker", ",", "\n", "behavior_per_worker", ",", "\n", "fitness_per_worker", ",", "\n", "from_novelty_per_worker", ",", "\n", ")", ":", "\n", "    ", "\"\"\"Synchronize archives between threads\"\"\"", "\n", "comm", "=", "MPI", ".", "COMM_WORLD", "\n", "all_params", "=", "comm", ".", "allgather", "(", "params_per_worker", ")", "\n", "all_behaviors", "=", "comm", ".", "allgather", "(", "behavior_per_worker", ")", "\n", "all_fitnesses", "=", "comm", ".", "allgather", "(", "fitness_per_worker", ")", "\n", "all_from_novelties", "=", "comm", ".", "allgather", "(", "from_novelty_per_worker", ")", "\n", "for", "ind", "in", "range", "(", "len", "(", "all_params", ")", ")", ":", "\n", "        ", "archive", ".", "add", "(", "\n", "all_params", "[", "ind", "]", ",", "\n", "all_behaviors", "[", "ind", "]", ",", "\n", "all_fitnesses", "[", "ind", "]", ",", "\n", "all_from_novelties", "[", "ind", "]", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.envs.MinigridWrapper.__init__": [[17, 57], ["gym.Wrapper.__init__", "gym_minigrid.wrappers.ImgObsWrapper", "gym.wrappers.frame_stack.FrameStack", "gym.wrappers.transform_observation.TransformObservation", "gym.spaces.Box", "obs[].transpose().reshape", "reshape_obs", "reshape_obs", "obs[].transpose"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "env", ":", "gym", ".", "Env", ",", "\n", "num_stack", ":", "int", "=", "4", ",", "\n", "seed", ":", "int", "=", "-", "1", ",", "\n", ")", ":", "\n", "        ", "super", "(", "MinigridWrapper", ",", "self", ")", ".", "__init__", "(", "env", ")", "\n", "\n", "self", ".", "_seed", "=", "seed", "if", "seed", "!=", "-", "1", "else", "None", "\n", "self", ".", "num_stack", "=", "num_stack", "\n", "\n", "# only keep image as state", "\n", "env", "=", "ImgObsWrapper", "(", "env", ")", "\n", "\n", "# define new state space shape", "\n", "ob_shape", "=", "env", ".", "observation_space", ".", "shape", "\n", "stacked_obs_shape", "=", "(", "ob_shape", "[", "-", "1", "]", "*", "num_stack", ",", ")", "+", "ob_shape", "[", ":", "-", "1", "]", "\n", "\n", "# stack frames", "\n", "env", "=", "FrameStack", "(", "env", "=", "env", ",", "num_stack", "=", "num_stack", ")", "\n", "\n", "# transpose and reshape observation", "\n", "reshape_obs", "=", "(", "\n", "lambda", "obs", ":", "obs", "[", ":", "]", ".", "transpose", "(", "0", ",", "-", "1", ",", "1", ",", "2", ")", ".", "reshape", "(", "stacked_obs_shape", ")", "\n", ")", "\n", "env", "=", "TransformObservation", "(", "env", ",", "f", "=", "reshape_obs", ")", "\n", "\n", "# set new env, action and observation spaces", "\n", "ob_space", "=", "env", ".", "observation_space", "\n", "env", ".", "observation_space", "=", "Box", "(", "\n", "low", "=", "reshape_obs", "(", "ob_space", ".", "low", ")", ",", "\n", "high", "=", "reshape_obs", "(", "ob_space", ".", "high", ")", ",", "\n", "shape", "=", "stacked_obs_shape", ",", "\n", "dtype", "=", "ob_space", ".", "dtype", ",", "\n", ")", "\n", "\n", "# set env", "\n", "self", ".", "env", "=", "env", "\n", "self", ".", "observation_space", "=", "env", ".", "observation_space", "\n", "self", ".", "action_space", "=", "env", ".", "action_space", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.envs.MinigridWrapper.reset": [[58, 65], ["envs.MinigridWrapper.env.reset", "envs.MinigridWrapper.env.seed"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.reset", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.seed"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_seed", ":", "\n", "            ", "self", ".", "env", ".", "seed", "(", "self", ".", "_seed", ")", "\n", "", "observation", "=", "self", ".", "env", ".", "reset", "(", ")", "\n", "# start with empty observations", "\n", "observation", "[", ":", "(", "self", ".", "num_stack", "-", "1", ")", "*", "3", "]", "=", "0", "\n", "return", "observation", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.envs.EpisodicCountWrapper.__init__": [[68, 71], ["gym.Wrapper.__init__"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", ":", "gym", ".", "Env", ",", "state_key_extraction", ":", "Callable", "[", "...", ",", "Iterable", "]", ")", ":", "\n", "        ", "super", "(", "EpisodicCountWrapper", ",", "self", ")", ".", "__init__", "(", "env", ")", "\n", "self", ".", "state_key_extraction", "=", "state_key_extraction", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.envs.MultiRoomEnvN10S10.__init__": [[74, 76], ["gym_minigrid.envs.multiroom.MultiRoomEnv.__init__"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "minNumRooms", "=", "10", ",", "maxNumRooms", "=", "10", ",", "maxRoomSize", "=", "10", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.envs.MultiRoomEnvN10S6.__init__": [[79, 81], ["gym_minigrid.envs.multiroom.MultiRoomEnv.__init__"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "minNumRooms", "=", "10", ",", "maxNumRooms", "=", "10", ",", "maxRoomSize", "=", "6", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.replay_buffer.ReplayBuffer.__init__": [[18, 22], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "max_size", "=", "1e6", ")", ":", "\n", "        ", "self", ".", "_storage", "=", "[", "]", "\n", "self", ".", "_max_size", "=", "max_size", "\n", "self", ".", "_pointer_position", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.replay_buffer.ReplayBuffer.num_elements": [[23, 26], ["len"], "methods", ["None"], ["", "@", "property", "\n", "def", "num_elements", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "_storage", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.replay_buffer.ReplayBuffer.add": [[27, 33], ["len", "replay_buffer.ReplayBuffer._storage.append", "int"], "methods", ["None"], ["", "def", "add", "(", "self", ",", "transition", ":", "Transition", ")", ":", "\n", "        ", "if", "len", "(", "self", ".", "_storage", ")", "==", "self", ".", "_max_size", ":", "\n", "            ", "self", ".", "_storage", "[", "int", "(", "self", ".", "_pointer_position", ")", "]", "=", "transition", "\n", "self", ".", "_pointer_position", "=", "(", "self", ".", "_pointer_position", "+", "1", ")", "%", "self", ".", "_max_size", "\n", "", "else", ":", "\n", "            ", "self", ".", "_storage", ".", "append", "(", "transition", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.replay_buffer.ReplayBuffer.reset": [[34, 37], ["None"], "methods", ["None"], ["", "", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "_storage", "=", "[", "]", "\n", "self", ".", "_pointer_position", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.replay_buffer.ReplayBuffer.sample": [[38, 58], ["len", "ValueError", "numpy.random.randint", "operator.itemgetter", "list", "Batch", "len", "zip", "numpy.array().copy", "numpy.array().copy", "numpy.array().copy", "numpy.array().copy", "numpy.array().copy", "numpy.array().copy", "operator.itemgetter.", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array"], "methods", ["None"], ["", "def", "sample", "(", "self", ",", "batch_size", ":", "int", ")", "->", "Batch", ":", "\n", "\n", "        ", "if", "len", "(", "self", ".", "_storage", ")", "<", "batch_size", ":", "\n", "            ", "raise", "ValueError", "(", "\"Not enough data in replay buffer to sample a batch.\"", ")", "\n", "\n", "", "else", ":", "\n", "            ", "ind", "=", "np", ".", "random", ".", "randint", "(", "0", ",", "len", "(", "self", ".", "_storage", ")", ",", "size", "=", "batch_size", ")", "\n", "op", "=", "operator", ".", "itemgetter", "(", "*", "ind", ")", "\n", "x", ",", "y", ",", "u", ",", "r", ",", "d", ",", "o", "=", "list", "(", "zip", "(", "*", "op", "(", "self", ".", "_storage", ")", ")", ")", "\n", "\n", "batch", "=", "Batch", "(", "\n", "observations", "=", "np", ".", "array", "(", "x", ")", ".", "copy", "(", ")", ",", "\n", "next_observations", "=", "np", ".", "array", "(", "y", ")", ".", "copy", "(", ")", ",", "\n", "actions", "=", "np", ".", "array", "(", "u", ")", ".", "copy", "(", ")", ",", "\n", "rewards", "=", "np", ".", "array", "(", "r", ")", ".", "copy", "(", ")", ",", "\n", "dones", "=", "np", ".", "array", "(", "d", ")", ".", "copy", "(", ")", ",", "\n", "descriptors", "=", "np", ".", "array", "(", "o", ")", ".", "copy", "(", ")", ",", "\n", ")", "\n", "\n", "return", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.replay_buffer.ReplayBuffer.save": [[59, 62], ["numpy.save", "print"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.VecNormalize.save"], ["", "", "def", "save", "(", "self", ",", "outfile", ")", ":", "\n", "        ", "np", ".", "save", "(", "outfile", ",", "self", ".", "_storage", ")", "\n", "print", "(", "f\"* {outfile} succesfully saved..\"", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.inits.fanin_init": [[4, 14], ["tensor.size", "tensor.data.uniform_", "len", "numpy.sqrt", "len", "numpy.prod", "Exception"], "function", ["None"], ["def", "fanin_init", "(", "tensor", ")", ":", "\n", "    ", "size", "=", "tensor", ".", "size", "(", ")", "\n", "if", "len", "(", "size", ")", "==", "2", ":", "\n", "        ", "fan_in", "=", "size", "[", "0", "]", "\n", "", "elif", "len", "(", "size", ")", ">", "2", ":", "\n", "        ", "fan_in", "=", "np", ".", "prod", "(", "size", "[", "1", ":", "]", ")", "\n", "", "else", ":", "\n", "        ", "raise", "Exception", "(", "\"Shape must be have dimension at least 2.\"", ")", "\n", "", "bound", "=", "1.0", "/", "np", ".", "sqrt", "(", "fan_in", ")", "\n", "return", "tensor", ".", "data", ".", "uniform_", "(", "-", "bound", ",", "bound", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.inits.initialize_hidden_layer": [[16, 19], ["inits.fanin_init", "layer.bias.data.fill_"], "function", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.inits.fanin_init"], ["", "def", "initialize_hidden_layer", "(", "layer", ",", "b_init_value", "=", "0.1", ")", ":", "\n", "    ", "fanin_init", "(", "layer", ".", "weight", ")", "\n", "layer", ".", "bias", ".", "data", ".", "fill_", "(", "b_init_value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.inits.initialize_last_layer": [[21, 24], ["layer.weight.data.uniform_", "layer.bias.data.uniform_"], "function", ["None"], ["", "def", "initialize_last_layer", "(", "layer", ",", "init_w", "=", "1e-3", ")", ":", "\n", "    ", "layer", ".", "weight", ".", "data", ".", "uniform_", "(", "-", "init_w", ",", "init_w", ")", "\n", "layer", ".", "bias", ".", "data", ".", "uniform_", "(", "-", "init_w", ",", "init_w", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.networks.actors.GaussianActor.__init__": [[16, 30], ["core.networks.rl_model.RLModel.__init__", "core.networks.mlp.MLP", "torch.nn.Parameter", "len", "numpy.ones", "torch.as_tensor"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__init__"], ["    ", "def", "__init__", "(", "self", ",", "observation_dim", ",", "action_dim", ",", "max_action", ",", "layers_dim", ")", ":", "\n", "        ", "super", "(", "GaussianActor", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "assert", "(", "\n", "len", "(", "layers_dim", ")", ">", "1", "\n", ")", ",", "\"can not define continuous stochastic actor without hidden layers\"", "\n", "\n", "# define shared layers", "\n", "self", ".", "_mean_net", "=", "MLP", "(", "observation_dim", ",", "action_dim", ",", "layers_dim", ")", "\n", "\n", "log_std", "=", "-", "0.5", "*", "np", ".", "ones", "(", "action_dim", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "_log_std", "=", "torch", ".", "nn", ".", "Parameter", "(", "torch", ".", "as_tensor", "(", "log_std", ")", ")", "\n", "\n", "self", ".", "_max_action", "=", "max_action", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.networks.actors.GaussianActor.compute_distribution": [[31, 35], ["actors.GaussianActor._mean_net", "torch.exp", "torch.distributions.Normal"], "methods", ["None"], ["", "def", "compute_distribution", "(", "self", ",", "obs", ")", "->", "Normal", ":", "\n", "        ", "mu", "=", "self", ".", "_mean_net", "(", "obs", ")", "\n", "std", "=", "torch", ".", "exp", "(", "self", ".", "_log_std", ")", "\n", "return", "Normal", "(", "mu", ",", "std", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.networks.actors.GaussianActor.forward": [[36, 53], ["actors.GaussianActor.compute_distribution", "actors.GaussianActor.sample", "actors.GaussianActor.log_prob().sum", "actors.GaussianActor.log_prob"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.networks.actors.DiscreteActor.compute_distribution", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.replay_buffer.ReplayBuffer.sample"], ["", "def", "forward", "(", "\n", "self", ",", "x", ",", "deterministic", ":", "bool", "=", "False", ",", "return_log_prob", ":", "bool", "=", "False", "\n", ")", "->", "Tuple", "[", "Action", ",", "LogProb", "]", ":", "\n", "# forward pass", "\n", "        ", "distribution", "=", "self", ".", "compute_distribution", "(", "x", ")", "\n", "\n", "if", "deterministic", ":", "\n", "            ", "action", "=", "distribution", ".", "mean", "\n", "", "else", ":", "\n", "            ", "action", "=", "distribution", ".", "sample", "(", ")", "\n", "\n", "", "if", "return_log_prob", ":", "\n", "            ", "log_prob", "=", "distribution", ".", "log_prob", "(", "action", ")", ".", "sum", "(", "axis", "=", "-", "1", ")", "\n", "", "else", ":", "\n", "            ", "log_prob", "=", "None", "\n", "\n", "", "return", "action", ",", "log_prob", ",", "distribution", ".", "mean", ",", "distribution", ".", "scale", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.networks.actors.GaussianActor.compute_log_prob": [[54, 58], ["actors.GaussianActor.compute_distribution", "actors.GaussianActor.log_prob().sum", "actors.GaussianActor.log_prob"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.networks.actors.DiscreteActor.compute_distribution"], ["", "def", "compute_log_prob", "(", "self", ",", "observations", ":", "Observation", ",", "actions", ":", "Action", ")", "->", "LogProb", ":", "\n", "# forward pass", "\n", "        ", "distribution", "=", "self", ".", "compute_distribution", "(", "observations", ")", "\n", "return", "distribution", ".", "log_prob", "(", "actions", ")", ".", "sum", "(", "axis", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.networks.actors.GaussianActor.select_action": [[59, 63], ["action.detach().cpu().numpy().flatten.detach().cpu().numpy().flatten.detach().cpu().numpy().flatten", "actors.GaussianActor.forward", "action.detach().cpu().numpy().flatten.detach().cpu().numpy().flatten.detach().cpu().numpy", "action.detach().cpu().numpy().flatten.detach().cpu().numpy().flatten.detach().cpu", "action.detach().cpu().numpy().flatten.detach().cpu().numpy().flatten.detach"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.networks.cnn.CNN.forward"], ["", "def", "select_action", "(", "self", ",", "observation", ":", "np", ".", "ndarray", ",", "deterministic", ":", "bool", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "action", "=", "self", ".", "forward", "(", "observation", ",", "deterministic", ")", "[", "0", "]", "\n", "action", "=", "action", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "flatten", "(", ")", "\n", "return", "action", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.networks.actors.DiscreteActor.__init__": [[66, 98], ["core.networks.rl_model.RLModel.__init__", "core.utils.inits.initialize_last_layer", "core.networks.cnn.CNN", "core.networks.mlp.MLP", "torch.nn.Sequential", "core.networks.mlp.MLP", "core.utils.inits.initialize_hidden_layer", "torch.nn.Flatten"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__init__", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.inits.initialize_last_layer", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.inits.initialize_hidden_layer"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "observation_dim", ":", "Tuple", "[", "int", "]", ",", "\n", "action_dim", ":", "int", ",", "\n", "layers_dim", ":", "List", "[", "int", "]", ",", "\n", "cnn_extractor", ":", "bool", "=", "False", ",", "\n", "layers_num_channels", ":", "List", "[", "int", "]", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Simple Discrete Actor with or without CNN extractor.\n        observation_dim, Tuple: shape of environment's observation\n        \"\"\"", "\n", "super", "(", "DiscreteActor", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "if", "cnn_extractor", ":", "\n", "            ", "core_cnn", "=", "CNN", "(", "\n", "observation_dim", ",", "layers_num_channels", ",", "stride", "=", "2", ",", "kernel_size", "=", "3", "\n", ")", "\n", "flattened_dim", "=", "(", "\n", "observation_dim", "[", "1", "]", "*", "observation_dim", "[", "2", "]", "*", "layers_num_channels", "[", "-", "1", "]", "\n", ")", "\n", "core_mlp", "=", "MLP", "(", "flattened_dim", ",", "action_dim", ",", "layers_dim", ")", "\n", "layers", "=", "core_cnn", ".", "layers", "+", "core_mlp", ".", "layers", "\n", "self", ".", "_core", "=", "nn", ".", "Sequential", "(", "core_cnn", ",", "nn", ".", "Flatten", "(", ")", ",", "core_mlp", ")", "\n", "\n", "", "else", ":", "\n", "            ", "self", ".", "_core", "=", "MLP", "(", "observation_dim", "[", "0", "]", ",", "action_dim", ",", "layers_dim", ")", "\n", "layers", "=", "self", ".", "_core", ".", "layers", "\n", "\n", "", "for", "layer", "in", "layers", "[", ":", "-", "1", "]", ":", "\n", "            ", "initialize_hidden_layer", "(", "layer", ")", "\n", "", "initialize_last_layer", "(", "layers", "[", "-", "1", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.networks.actors.DiscreteActor.forward": [[99, 115], ["actors.DiscreteActor.compute_distribution", "torch.argmax", "actors.DiscreteActor.sample", "actors.DiscreteActor.log_prob"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.networks.actors.DiscreteActor.compute_distribution", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.utils.replay_buffer.ReplayBuffer.sample"], ["", "def", "forward", "(", "\n", "self", ",", "x", ",", "deterministic", ":", "bool", "=", "False", ",", "return_log_prob", ":", "bool", "=", "False", "\n", ")", "->", "Tuple", "[", "Action", ",", "LogProb", ",", "Prob", ",", "LogProb", "]", ":", "\n", "        ", "m", "=", "self", ".", "compute_distribution", "(", "x", ")", "\n", "if", "deterministic", ":", "\n", "            ", "action", "=", "torch", ".", "argmax", "(", "m", ".", "probs", ",", "dim", "=", "-", "1", ")", "\n", "", "else", ":", "\n", "            ", "action", "=", "m", ".", "sample", "(", ")", "\n", "\n", "", "if", "return_log_prob", ":", "\n", "            ", "log_prob", "=", "m", ".", "log_prob", "(", "action", ")", "\n", "probs", "=", "m", ".", "probs", "\n", "log_probs", "=", "m", ".", "logits", "\n", "", "else", ":", "\n", "            ", "log_probs", ",", "log_prob", "=", "None", ",", "None", "\n", "", "return", "action", ",", "log_prob", ",", "probs", ",", "log_probs", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.networks.actors.DiscreteActor.compute_distribution": [[116, 120], ["actors.DiscreteActor._core", "torch.distributions.Categorical", "torch.nn.Softmax"], "methods", ["None"], ["", "def", "compute_distribution", "(", "self", ",", "observations", ":", "Observation", ")", "->", "Categorical", ":", "\n", "        ", "logits", "=", "self", ".", "_core", "(", "observations", ")", "\n", "probs", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "(", "logits", ")", "\n", "return", "Categorical", "(", "probs", "=", "probs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.networks.actors.DiscreteActor.compute_log_prob": [[121, 125], ["actors.DiscreteActor.compute_distribution", "actors.DiscreteActor.log_prob"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.networks.actors.DiscreteActor.compute_distribution"], ["", "def", "compute_log_prob", "(", "self", ",", "observations", ":", "Observation", ",", "actions", ":", "Action", ")", "->", "LogProb", ":", "\n", "# forward pass", "\n", "        ", "distribution", "=", "self", ".", "compute_distribution", "(", "observations", ")", "\n", "return", "distribution", ".", "log_prob", "(", "actions", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.networks.actors.DiscreteActor.select_action": [[126, 136], ["[].cpu().data.numpy().flatten", "int", "[].cpu().data.numpy", "[].cpu", "actors.DiscreteActor."], "methods", ["None"], ["", "def", "select_action", "(", "\n", "self", ",", "observation", ":", "Observation", ",", "deterministic", ":", "bool", "=", "True", "\n", ")", "->", "Action", ":", "\n", "        ", "action", "=", "(", "\n", "self", "(", "observation", ",", "deterministic", "=", "deterministic", ")", "[", "0", "]", "\n", ".", "cpu", "(", ")", "\n", ".", "data", ".", "numpy", "(", ")", "\n", ".", "flatten", "(", ")", "\n", ")", "\n", "return", "int", "(", "action", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.networks.mlp.MLP.__init__": [[7, 22], ["torch.Module.__init__", "zip", "layers.pop", "torch.Sequential", "torch.Linear", "mlp.MLP._layers_without_activations.append", "layers.append", "layers.append", "list", "torch.ReLU"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "input_dim", ":", "List", "[", "int", "]", ",", "output_dim", ":", "int", ",", "layers_dim", ":", "Union", "[", "List", "[", "int", "]", ",", "int", "]", "\n", ")", ":", "\n", "        ", "nn", ".", "Module", ".", "__init__", "(", "self", ")", "\n", "num_neurons", "=", "[", "input_dim", "]", "+", "list", "(", "layers_dim", ")", "+", "[", "output_dim", "]", "\n", "num_neurons", "=", "zip", "(", "num_neurons", "[", ":", "-", "1", "]", ",", "num_neurons", "[", "1", ":", "]", ")", "\n", "layers", "=", "[", "]", "\n", "self", ".", "_layers_without_activations", "=", "[", "]", "\n", "for", "in_dim", ",", "out_dim", "in", "num_neurons", ":", "\n", "            ", "layer", "=", "nn", ".", "Linear", "(", "in_dim", ",", "out_dim", ")", "\n", "self", ".", "_layers_without_activations", ".", "append", "(", "layer", ")", "\n", "layers", ".", "append", "(", "layer", ")", "\n", "layers", ".", "append", "(", "nn", ".", "ReLU", "(", ")", ")", "\n", "", "layers", ".", "pop", "(", ")", "# remove last activation", "\n", "self", ".", "_mlp", "=", "nn", ".", "Sequential", "(", "*", "layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.networks.mlp.MLP.layers": [[23, 26], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "layers", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_layers_without_activations", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.networks.mlp.MLP.forward": [[27, 29], ["mlp.MLP._mlp"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "_mlp", "(", "x", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.networks.critics.ContinuousVNetwork.__init__": [[15, 20], ["core.networks.rl_model.RLModel.__init__", "core.networks.mlp.MLP"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__init__"], ["def", "__init__", "(", "self", ",", "observation_dim", ":", "int", ",", "layers_dim", ":", "List", "[", "int", "]", ")", ":", "\n", "        ", "super", "(", "ContinuousVNetwork", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# V network architecture", "\n", "self", ".", "_v_mlp", "=", "MLP", "(", "observation_dim", ",", "1", ",", "layers_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.networks.critics.ContinuousVNetwork.forward": [[21, 24], ["critics.ContinuousVNetwork._v_mlp"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "_v_mlp", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.networks.critics.CNNContinuousVNetwork.__init__": [[29, 43], ["core.networks.rl_model.RLModel.__init__", "core.networks.mlp.MLP", "core.networks.cnn.CNN", "torch.Sequential", "torch.Flatten"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "observation_dim", ":", "Tuple", "[", "int", "]", ",", "\n", "layers_dim", ":", "List", "[", "int", "]", ",", "\n", "layers_num_channels", ":", "List", "[", "int", "]", ",", "\n", ")", ":", "\n", "        ", "super", "(", "CNNContinuousVNetwork", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "flattened_dim", "=", "(", "\n", "observation_dim", "[", "1", "]", "*", "observation_dim", "[", "2", "]", "*", "layers_num_channels", "[", "-", "1", "]", "\n", ")", "\n", "v_mlp", "=", "MLP", "(", "flattened_dim", ",", "1", ",", "layers_dim", ")", "\n", "v_cnn", "=", "CNN", "(", "observation_dim", ",", "layers_num_channels", ",", "stride", "=", "2", ",", "kernel_size", "=", "3", ")", "\n", "self", ".", "_v", "=", "nn", ".", "Sequential", "(", "v_cnn", ",", "nn", ".", "Flatten", "(", ")", ",", "v_mlp", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.networks.critics.CNNContinuousVNetwork.forward": [[44, 47], ["critics.CNNContinuousVNetwork._v"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "_v", "(", "x", ")", "\n", "return", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.networks.rl_model.RLModel.__init__": [[12, 14], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "RLModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.networks.rl_model.RLModel.set_params": [[15, 18], ["rl_model.RLModel.load_state_dict"], "methods", ["None"], ["", "def", "set_params", "(", "self", ",", "params", ":", "Parameters", ")", ":", "\n", "        ", "\"\"\"Set the params of the network to the given parameters\"\"\"", "\n", "self", ".", "load_state_dict", "(", "params", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.networks.rl_model.RLModel.get_params": [[19, 22], ["v.cpu", "rl_model.RLModel.state_dict().items", "rl_model.RLModel.state_dict"], "methods", ["None"], ["", "def", "get_params", "(", "self", ")", "->", "Parameters", ":", "\n", "        ", "\"\"\"Return parameters of the actor\"\"\"", "\n", "return", "{", "k", ":", "v", ".", "cpu", "(", ")", "for", "k", ",", "v", "in", "self", ".", "state_dict", "(", ")", ".", "items", "(", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.networks.rl_model.RLModel.get_size": [[23, 26], ["rl_model.RLModel.get_params"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.networks.rl_model.RLModel.get_params"], ["", "def", "get_size", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the number of parameters of the network\"\"\"", "\n", "return", "self", ".", "get_params", "(", ")", ".", "shape", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.networks.rl_model.RLModel.get_grads": [[27, 36], ["rl_model.RLModel.parameters", "grads.append", "p.grad.data.cpu().numpy", "p.grad.data.cpu"], "methods", ["None"], ["", "def", "get_grads", "(", "self", ")", "->", "Gradients", ":", "\n", "        ", "\"\"\"\n        Get gradients.\n        \"\"\"", "\n", "grads", "=", "[", "]", "\n", "for", "p", "in", "self", ".", "parameters", "(", ")", ":", "\n", "            ", "grad", "=", "None", "if", "p", ".", "grad", "is", "None", "else", "p", ".", "grad", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "grads", ".", "append", "(", "grad", ")", "\n", "", "return", "grads", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.networks.rl_model.RLModel.set_grads": [[37, 42], ["zip", "rl_model.RLModel.parameters", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy"], "methods", ["None"], ["", "def", "set_grads", "(", "self", ",", "gradients", ":", "Gradients", ")", ":", "\n", "        ", "\"\"\"Set gradients\"\"\"", "\n", "for", "g", ",", "p", "in", "zip", "(", "gradients", ",", "self", ".", "parameters", "(", ")", ")", ":", "\n", "            ", "if", "g", "is", "not", "None", ":", "\n", "                ", "p", ".", "grad", "=", "torch", ".", "from_numpy", "(", "g", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.networks.rl_model.RLModel.load_model": [[43, 52], ["rl_model.RLModel.load_state_dict", "torch.load", "torch.load", "torch.load", "torch.load"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.VecNormalize.load", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.VecNormalize.load", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.VecNormalize.load", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.VecNormalize.load"], ["", "", "", "def", "load_model", "(", "self", ",", "filename", ",", "net_name", ")", ":", "\n", "        ", "\"\"\"Load the model\"\"\"", "\n", "if", "filename", "is", "None", ":", "\n", "            ", "return", "\n", "\n", "", "self", ".", "load_state_dict", "(", "\n", "torch", ".", "load", "(", "\n", "\"{}/{}.pkl\"", ".", "format", "(", "filename", ",", "net_name", ")", ",", "\n", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.networks.rl_model.RLModel.save_model": [[55, 58], ["torch.save", "torch.save", "torch.save", "torch.save", "rl_model.RLModel.state_dict"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.VecNormalize.save", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.VecNormalize.save", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.VecNormalize.save", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.VecNormalize.save"], ["", "def", "save_model", "(", "self", ",", "output", ",", "net_name", ")", ":", "\n", "        ", "\"\"\"Saves the model\"\"\"", "\n", "torch", ".", "save", "(", "self", ".", "state_dict", "(", ")", ",", "\"{}/{}.pkl\"", ".", "format", "(", "output", ",", "net_name", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.networks.cnn.CNN.__init__": [[8, 29], ["torch.Module.__init__", "math.ceil", "zip", "torch.Sequential", "torch.Conv2d", "cnn.CNN._layers_without_activations.append", "layers.append", "layers.append", "torch.ELU"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "input_dim", ":", "Union", "[", "List", "[", "int", "]", ",", "Tuple", "[", "int", "]", "]", ",", "\n", "layers_num_channels", ":", "List", "[", "int", "]", ",", "\n", "stride", ":", "int", "=", "2", ",", "\n", "kernel_size", ":", "int", "=", "3", ",", "\n", ")", ":", "\n", "        ", "nn", ".", "Module", ".", "__init__", "(", "self", ")", "\n", "# Same padding", "\n", "input_num_channels", ",", "input_h", ",", "input_w", "=", "input_dim", "\n", "padding", "=", "math", ".", "ceil", "(", "(", "input_h", "*", "(", "stride", "-", "1", ")", "+", "kernel_size", "-", "stride", ")", "/", "2", ")", "\n", "layers_num_channels", "=", "[", "input_num_channels", "]", "+", "layers_num_channels", "\n", "num_channels", "=", "zip", "(", "layers_num_channels", "[", ":", "-", "1", "]", ",", "layers_num_channels", "[", "1", ":", "]", ")", "\n", "layers", "=", "[", "]", "\n", "self", ".", "_layers_without_activations", "=", "[", "]", "\n", "for", "in_channels", ",", "out_channels", "in", "num_channels", ":", "\n", "            ", "layer", "=", "nn", ".", "Conv2d", "(", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "stride", ",", "padding", ")", "\n", "self", ".", "_layers_without_activations", ".", "append", "(", "layer", ")", "\n", "layers", ".", "append", "(", "layer", ")", "\n", "layers", ".", "append", "(", "nn", ".", "ELU", "(", ")", ")", "\n", "", "self", ".", "_cnn", "=", "nn", ".", "Sequential", "(", "*", "layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.networks.cnn.CNN.layers": [[30, 33], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "layers", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_layers_without_activations", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.networks.cnn.CNN.forward": [[34, 36], ["cnn.CNN._cnn"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "_cnn", "(", "x", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.VecNormalize.__init__": [[35, 51], ["core.vec_env.base_vec_env.VecEnvWrapper.__init__", "core.math_util.RunningMeanStd", "core.math_util.RunningMeanStd", "numpy.zeros"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__init__"], ["def", "__init__", "(", "self", ",", "venv", ",", "training", "=", "True", ",", "norm_obs", "=", "True", ",", "norm_reward", "=", "True", ",", "\n", "clip_obs", "=", "10.", ",", "clip_reward", "=", "10.", ",", "gamma", "=", "0.99", ",", "epsilon", "=", "1e-8", ")", ":", "\n", "        ", "VecEnvWrapper", ".", "__init__", "(", "self", ",", "venv", ")", "\n", "self", ".", "obs_rms", "=", "RunningMeanStd", "(", "shape", "=", "self", ".", "observation_space", ".", "shape", ")", "\n", "self", ".", "ret_rms", "=", "RunningMeanStd", "(", "shape", "=", "(", ")", ")", "\n", "self", ".", "clip_obs", "=", "clip_obs", "\n", "self", ".", "clip_reward", "=", "clip_reward", "\n", "# Returns: discounted rewards", "\n", "self", ".", "ret", "=", "np", ".", "zeros", "(", "self", ".", "num_envs", ")", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "epsilon", "=", "epsilon", "\n", "self", ".", "training", "=", "training", "\n", "self", ".", "norm_obs", "=", "norm_obs", "\n", "self", ".", "norm_reward", "=", "norm_reward", "\n", "self", ".", "old_obs", "=", "None", "\n", "self", ".", "old_rews", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.VecNormalize.__getstate__": [[52, 64], ["all_vec_env.VecNormalize.__dict__.copy"], "methods", ["None"], ["", "def", "__getstate__", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Gets state for pickling.\n\n        Excludes self.venv, as in general VecEnv's may not be pickleable.\"\"\"", "\n", "state", "=", "self", ".", "__dict__", ".", "copy", "(", ")", "\n", "# these attributes are not pickleable", "\n", "del", "state", "[", "'venv'", "]", "\n", "del", "state", "[", "'class_attributes'", "]", "\n", "# these attributes depend on the above and so we would prefer not to pickle", "\n", "del", "state", "[", "'ret'", "]", "\n", "return", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.VecNormalize.__setstate__": [[65, 75], ["all_vec_env.VecNormalize.__dict__.update"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.math_util.RunningMeanStd.update"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "\"\"\"\n        Restores pickled state.\n\n        User must call set_venv() after unpickling before using.\n\n        :param state: (dict)\"\"\"", "\n", "self", ".", "__dict__", ".", "update", "(", "state", ")", "\n", "assert", "'venv'", "not", "in", "state", "\n", "self", ".", "venv", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.VecNormalize.set_venv": [[76, 90], ["core.vec_env.base_vec_env.VecEnvWrapper.__init__", "numpy.zeros", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__init__"], ["", "def", "set_venv", "(", "self", ",", "venv", ")", ":", "\n", "        ", "\"\"\"\n        Sets the vector environment to wrap to venv.\n\n        Also sets attributes derived from this such as `num_env`.\n\n        :param venv: (VecEnv)\n        \"\"\"", "\n", "if", "self", ".", "venv", "is", "not", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"Trying to set venv of already initialized VecNormalize wrapper.\"", ")", "\n", "", "VecEnvWrapper", ".", "__init__", "(", "self", ",", "venv", ")", "\n", "if", "self", ".", "obs_rms", ".", "mean", ".", "shape", "!=", "self", ".", "observation_space", ".", "shape", ":", "\n", "            ", "raise", "ValueError", "(", "\"venv is incompatible with current statistics.\"", ")", "\n", "", "self", ".", "ret", "=", "np", ".", "zeros", "(", "self", ".", "num_envs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.VecNormalize.step_wait": [[91, 112], ["all_vec_env.VecNormalize.venv.step_wait", "all_vec_env.VecNormalize.normalize_obs", "all_vec_env.VecNormalize.normalize_reward", "all_vec_env.VecNormalize.obs_rms.update", "all_vec_env.VecNormalize._update_reward"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.step_wait", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.VecNormalize.normalize_obs", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.VecNormalize.normalize_reward", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.math_util.RunningMeanStd.update", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.VecNormalize._update_reward"], ["", "def", "step_wait", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Apply sequence of actions to sequence of environments\n        actions -> (observations, rewards, news)\n\n        where 'news' is a boolean vector indicating whether each element is new.\n        \"\"\"", "\n", "obs", ",", "rews", ",", "news", ",", "infos", "=", "self", ".", "venv", ".", "step_wait", "(", ")", "\n", "self", ".", "old_obs", "=", "obs", "\n", "self", ".", "old_rews", "=", "rews", "\n", "\n", "if", "self", ".", "training", ":", "\n", "            ", "self", ".", "obs_rms", ".", "update", "(", "obs", ")", "\n", "", "obs", "=", "self", ".", "normalize_obs", "(", "obs", ")", "\n", "\n", "if", "self", ".", "training", ":", "\n", "            ", "self", ".", "_update_reward", "(", "rews", ")", "\n", "", "rews", "=", "self", ".", "normalize_reward", "(", "rews", ")", "\n", "\n", "self", ".", "ret", "[", "news", "]", "=", "0", "\n", "return", "obs", ",", "rews", ",", "news", ",", "infos", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.VecNormalize._update_reward": [[113, 117], ["all_vec_env.VecNormalize.ret_rms.update"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.math_util.RunningMeanStd.update"], ["", "def", "_update_reward", "(", "self", ",", "reward", ":", "np", ".", "ndarray", ")", "->", "None", ":", "\n", "        ", "\"\"\"Update reward normalization statistics.\"\"\"", "\n", "self", ".", "ret", "=", "self", ".", "ret", "*", "self", ".", "gamma", "+", "reward", "\n", "self", ".", "ret_rms", ".", "update", "(", "self", ".", "ret", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.VecNormalize.normalize_obs": [[118, 128], ["numpy.clip", "numpy.sqrt"], "methods", ["None"], ["", "def", "normalize_obs", "(", "self", ",", "obs", ":", "np", ".", "ndarray", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"\n        Normalize observations using this VecNormalize's observations statistics.\n        Calling this method does not update statistics.\n        \"\"\"", "\n", "if", "self", ".", "norm_obs", ":", "\n", "            ", "obs", "=", "np", ".", "clip", "(", "(", "obs", "-", "self", ".", "obs_rms", ".", "mean", ")", "/", "np", ".", "sqrt", "(", "self", ".", "obs_rms", ".", "var", "+", "self", ".", "epsilon", ")", ",", "\n", "-", "self", ".", "clip_obs", ",", "\n", "self", ".", "clip_obs", ")", "\n", "", "return", "obs", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.VecNormalize.normalize_reward": [[129, 138], ["numpy.clip", "numpy.sqrt"], "methods", ["None"], ["", "def", "normalize_reward", "(", "self", ",", "reward", ":", "np", ".", "ndarray", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"\n        Normalize rewards using this VecNormalize's rewards statistics.\n        Calling this method does not update statistics.\n        \"\"\"", "\n", "if", "self", ".", "norm_reward", ":", "\n", "            ", "reward", "=", "np", ".", "clip", "(", "reward", "/", "np", ".", "sqrt", "(", "self", ".", "ret_rms", ".", "var", "+", "self", ".", "epsilon", ")", ",", "\n", "-", "self", ".", "clip_reward", ",", "self", ".", "clip_reward", ")", "\n", "", "return", "reward", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.VecNormalize.get_original_obs": [[139, 145], ["all_vec_env.VecNormalize.old_obs.copy"], "methods", ["None"], ["", "def", "get_original_obs", "(", "self", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"\n        Returns an unnormalized version of the observations from the most recent\n        step or reset.\n        \"\"\"", "\n", "return", "self", ".", "old_obs", ".", "copy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.VecNormalize.get_original_reward": [[146, 151], ["all_vec_env.VecNormalize.old_rews.copy"], "methods", ["None"], ["", "def", "get_original_reward", "(", "self", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"\n        Returns an unnormalized version of the rewards from the most recent step.\n        \"\"\"", "\n", "return", "self", ".", "old_rews", ".", "copy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.VecNormalize.reset": [[152, 162], ["all_vec_env.VecNormalize.venv.reset", "numpy.zeros", "all_vec_env.VecNormalize.normalize_obs", "all_vec_env.VecNormalize._update_reward"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.reset", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.VecNormalize.normalize_obs", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.VecNormalize._update_reward"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Reset all environments\n        \"\"\"", "\n", "obs", "=", "self", ".", "venv", ".", "reset", "(", ")", "\n", "self", ".", "old_obs", "=", "obs", "\n", "self", ".", "ret", "=", "np", ".", "zeros", "(", "self", ".", "num_envs", ")", "\n", "if", "self", ".", "training", ":", "\n", "            ", "self", ".", "_update_reward", "(", "self", ".", "ret", ")", "\n", "", "return", "self", ".", "normalize_obs", "(", "obs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.VecNormalize.load": [[163, 176], ["pickle.load.set_venv", "open", "pickle.load"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.VecNormalize.set_venv", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.VecNormalize.load"], ["", "@", "staticmethod", "\n", "def", "load", "(", "load_path", ",", "venv", ")", ":", "\n", "        ", "\"\"\"\n        Loads a saved VecNormalize object.\n\n        :param load_path: the path to load from.\n        :param venv: the VecEnv to wrap.\n        :return: (VecNormalize)\n        \"\"\"", "\n", "with", "open", "(", "load_path", ",", "\"rb\"", ")", "as", "file_handler", ":", "\n", "            ", "vec_normalize", "=", "pickle", ".", "load", "(", "file_handler", ")", "\n", "", "vec_normalize", ".", "set_venv", "(", "venv", ")", "\n", "return", "vec_normalize", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.VecNormalize.save": [[177, 180], ["open", "pickle.dump"], "methods", ["None"], ["", "def", "save", "(", "self", ",", "save_path", ")", ":", "\n", "        ", "with", "open", "(", "save_path", ",", "\"wb\"", ")", "as", "file_handler", ":", "\n", "            ", "pickle", ".", "dump", "(", "self", ",", "file_handler", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.VecNormalize.save_running_average": [[181, 193], ["warnings.warn", "zip", "open", "pickle.dump"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.warn"], ["", "", "def", "save_running_average", "(", "self", ",", "path", ")", ":", "\n", "        ", "\"\"\"\n        :param path: (str) path to log dir\n\n        .. deprecated:: 2.9.0\n            This function will be removed in a future version\n        \"\"\"", "\n", "warnings", ".", "warn", "(", "\"Usage of `save_running_average` is deprecated. Please \"", "\n", "\"use `save` or pickle instead.\"", ",", "DeprecationWarning", ")", "\n", "for", "rms", ",", "name", "in", "zip", "(", "[", "self", ".", "obs_rms", ",", "self", ".", "ret_rms", "]", ",", "[", "'obs_rms'", ",", "'ret_rms'", "]", ")", ":", "\n", "            ", "with", "open", "(", "\"{}/{}.pkl\"", ".", "format", "(", "path", ",", "name", ")", ",", "'wb'", ")", "as", "file_handler", ":", "\n", "                ", "pickle", ".", "dump", "(", "rms", ",", "file_handler", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.VecNormalize.load_running_average": [[194, 206], ["warnings.warn", "open", "setattr", "pickle.load"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.warn", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.VecNormalize.load"], ["", "", "", "def", "load_running_average", "(", "self", ",", "path", ")", ":", "\n", "        ", "\"\"\"\n        :param path: (str) path to log dir\n\n        .. deprecated:: 2.9.0\n            This function will be removed in a future version\n        \"\"\"", "\n", "warnings", ".", "warn", "(", "\"Usage of `load_running_average` is deprecated. Please \"", "\n", "\"use `load` or pickle instead.\"", ",", "DeprecationWarning", ")", "\n", "for", "name", "in", "[", "'obs_rms'", ",", "'ret_rms'", "]", ":", "\n", "            ", "with", "open", "(", "\"{}/{}.pkl\"", ".", "format", "(", "path", ",", "name", ")", ",", "'rb'", ")", "as", "file_handler", ":", "\n", "                ", "setattr", "(", "self", ",", "name", ",", "pickle", ".", "load", "(", "file_handler", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.VecFrameStack.__init__": [[216, 225], ["numpy.repeat", "numpy.repeat", "numpy.zeros", "gym.spaces.Box", "core.vec_env.base_vec_env.VecEnvWrapper.__init__"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__init__"], ["def", "__init__", "(", "self", ",", "venv", ",", "n_stack", ")", ":", "\n", "        ", "self", ".", "venv", "=", "venv", "\n", "self", ".", "n_stack", "=", "n_stack", "\n", "wrapped_obs_space", "=", "venv", ".", "observation_space", "\n", "low", "=", "np", ".", "repeat", "(", "wrapped_obs_space", ".", "low", ",", "self", ".", "n_stack", ",", "axis", "=", "-", "1", ")", "\n", "high", "=", "np", ".", "repeat", "(", "wrapped_obs_space", ".", "high", ",", "self", ".", "n_stack", ",", "axis", "=", "-", "1", ")", "\n", "self", ".", "stackedobs", "=", "np", ".", "zeros", "(", "(", "venv", ".", "num_envs", ",", ")", "+", "low", ".", "shape", ",", "low", ".", "dtype", ")", "\n", "observation_space", "=", "spaces", ".", "Box", "(", "low", "=", "low", ",", "high", "=", "high", ",", "dtype", "=", "venv", ".", "observation_space", ".", "dtype", ")", "\n", "VecEnvWrapper", ".", "__init__", "(", "self", ",", "venv", ",", "observation_space", "=", "observation_space", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.VecFrameStack.step_wait": [[226, 243], ["all_vec_env.VecFrameStack.venv.step_wait", "numpy.roll", "enumerate", "numpy.concatenate", "warnings.warn"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.step_wait", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.warn"], ["", "def", "step_wait", "(", "self", ")", ":", "\n", "        ", "observations", ",", "rewards", ",", "dones", ",", "infos", "=", "self", ".", "venv", ".", "step_wait", "(", ")", "\n", "last_ax_size", "=", "observations", ".", "shape", "[", "-", "1", "]", "\n", "self", ".", "stackedobs", "=", "np", ".", "roll", "(", "self", ".", "stackedobs", ",", "shift", "=", "-", "last_ax_size", ",", "axis", "=", "-", "1", ")", "\n", "for", "i", ",", "done", "in", "enumerate", "(", "dones", ")", ":", "\n", "            ", "if", "done", ":", "\n", "                ", "if", "'terminal_observation'", "in", "infos", "[", "i", "]", ":", "\n", "                    ", "old_terminal", "=", "infos", "[", "i", "]", "[", "'terminal_observation'", "]", "\n", "new_terminal", "=", "np", ".", "concatenate", "(", "\n", "(", "self", ".", "stackedobs", "[", "i", ",", "...", ",", ":", "-", "last_ax_size", "]", ",", "old_terminal", ")", ",", "axis", "=", "-", "1", ")", "\n", "infos", "[", "i", "]", "[", "'terminal_observation'", "]", "=", "new_terminal", "\n", "", "else", ":", "\n", "                    ", "warnings", ".", "warn", "(", "\n", "\"VecFrameStack wrapping a VecEnv without terminal_observation info\"", ")", "\n", "", "self", ".", "stackedobs", "[", "i", "]", "=", "0", "\n", "", "", "self", ".", "stackedobs", "[", "...", ",", "-", "observations", ".", "shape", "[", "-", "1", "]", ":", "]", "=", "observations", "\n", "return", "self", ".", "stackedobs", ",", "rewards", ",", "dones", ",", "infos", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.VecFrameStack.reset": [[244, 252], ["all_vec_env.VecFrameStack.venv.reset"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.reset"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Reset all environments\n        \"\"\"", "\n", "obs", "=", "self", ".", "venv", ".", "reset", "(", ")", "\n", "self", ".", "stackedobs", "[", "...", "]", "=", "0", "\n", "self", ".", "stackedobs", "[", "...", ",", "-", "obs", ".", "shape", "[", "-", "1", "]", ":", "]", "=", "obs", "\n", "return", "self", ".", "stackedobs", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.VecFrameStack.close": [[253, 255], ["all_vec_env.VecFrameStack.venv.close"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.close"], ["", "def", "close", "(", "self", ")", ":", "\n", "        ", "self", ".", "venv", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.DummyVecEnv.__init__": [[268, 283], ["core.vec_env.base_vec_env.VecEnv.__init__", "core.vec_env.util.obs_space_info", "collections.OrderedDict", "numpy.zeros", "numpy.zeros", "fn", "len", "range", "numpy.zeros", "tuple"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__init__", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.util.obs_space_info"], ["def", "__init__", "(", "self", ",", "env_fns", ")", ":", "\n", "        ", "self", ".", "envs", "=", "[", "fn", "(", ")", "for", "fn", "in", "env_fns", "]", "\n", "env", "=", "self", ".", "envs", "[", "0", "]", "\n", "VecEnv", ".", "__init__", "(", "self", ",", "len", "(", "env_fns", ")", ",", "env", ".", "observation_space", ",", "env", ".", "action_space", ")", "\n", "obs_space", "=", "env", ".", "observation_space", "\n", "self", ".", "keys", ",", "shapes", ",", "dtypes", "=", "obs_space_info", "(", "obs_space", ")", "\n", "\n", "self", ".", "buf_obs", "=", "OrderedDict", "(", "[", "\n", "(", "k", ",", "np", ".", "zeros", "(", "(", "self", ".", "num_envs", ",", ")", "+", "tuple", "(", "shapes", "[", "k", "]", ")", ",", "dtype", "=", "dtypes", "[", "k", "]", ")", ")", "\n", "for", "k", "in", "self", ".", "keys", "]", ")", "\n", "self", ".", "buf_dones", "=", "np", ".", "zeros", "(", "(", "self", ".", "num_envs", ",", ")", ",", "dtype", "=", "np", ".", "bool", ")", "\n", "self", ".", "buf_rews", "=", "np", ".", "zeros", "(", "(", "self", ".", "num_envs", ",", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "buf_infos", "=", "[", "{", "}", "for", "_", "in", "range", "(", "self", ".", "num_envs", ")", "]", "\n", "self", ".", "actions", "=", "None", "\n", "self", ".", "metadata", "=", "env", ".", "metadata", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.DummyVecEnv.step_async": [[284, 286], ["None"], "methods", ["None"], ["", "def", "step_async", "(", "self", ",", "actions", ")", ":", "\n", "        ", "self", ".", "actions", "=", "actions", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.DummyVecEnv.step_wait": [[287, 298], ["range", "all_vec_env.DummyVecEnv.envs[].step", "all_vec_env.DummyVecEnv._save_obs", "all_vec_env.DummyVecEnv._obs_from_buf", "numpy.copy", "numpy.copy", "all_vec_env.DummyVecEnv.buf_infos.copy", "all_vec_env.DummyVecEnv.envs[].reset"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnv.step", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.DummyVecEnv._save_obs", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.DummyVecEnv._obs_from_buf", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.reset"], ["", "def", "step_wait", "(", "self", ")", ":", "\n", "        ", "for", "env_idx", "in", "range", "(", "self", ".", "num_envs", ")", ":", "\n", "            ", "obs", ",", "self", ".", "buf_rews", "[", "env_idx", "]", ",", "self", ".", "buf_dones", "[", "env_idx", "]", ",", "self", ".", "buf_infos", "[", "env_idx", "]", "=", "self", ".", "envs", "[", "env_idx", "]", ".", "step", "(", "self", ".", "actions", "[", "env_idx", "]", ")", "\n", "if", "self", ".", "buf_dones", "[", "env_idx", "]", ":", "\n", "# save final observation where user can get it, then reset", "\n", "                ", "self", ".", "buf_infos", "[", "env_idx", "]", "[", "'terminal_observation'", "]", "=", "obs", "\n", "obs", "=", "self", ".", "envs", "[", "env_idx", "]", ".", "reset", "(", ")", "\n", "", "self", ".", "_save_obs", "(", "env_idx", ",", "obs", ")", "\n", "", "return", "(", "self", ".", "_obs_from_buf", "(", ")", ",", "np", ".", "copy", "(", "self", ".", "buf_rews", ")", ",", "np", ".", "copy", "(", "self", ".", "buf_dones", ")", ",", "\n", "self", ".", "buf_infos", ".", "copy", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.DummyVecEnv.seed": [[299, 304], ["list", "enumerate", "list.append", "env.seed"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.seed"], ["", "def", "seed", "(", "self", ",", "seed", "=", "None", ")", ":", "\n", "        ", "seeds", "=", "list", "(", ")", "\n", "for", "idx", ",", "env", "in", "enumerate", "(", "self", ".", "envs", ")", ":", "\n", "            ", "seeds", ".", "append", "(", "env", ".", "seed", "(", "seed", "+", "idx", ")", ")", "\n", "", "return", "seeds", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.DummyVecEnv.reset": [[305, 310], ["range", "all_vec_env.DummyVecEnv._obs_from_buf", "all_vec_env.DummyVecEnv.envs[].reset", "all_vec_env.DummyVecEnv._save_obs"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.DummyVecEnv._obs_from_buf", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.reset", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.DummyVecEnv._save_obs"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "for", "env_idx", "in", "range", "(", "self", ".", "num_envs", ")", ":", "\n", "            ", "obs", "=", "self", ".", "envs", "[", "env_idx", "]", ".", "reset", "(", ")", "\n", "self", ".", "_save_obs", "(", "env_idx", ",", "obs", ")", "\n", "", "return", "self", ".", "_obs_from_buf", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.DummyVecEnv.close": [[311, 314], ["env.close"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.close"], ["", "def", "close", "(", "self", ")", ":", "\n", "        ", "for", "env", "in", "self", ".", "envs", ":", "\n", "            ", "env", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.DummyVecEnv.get_images": [[315, 317], ["env.render"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.render"], ["", "", "def", "get_images", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", "->", "Sequence", "[", "np", ".", "ndarray", "]", ":", "\n", "        ", "return", "[", "env", ".", "render", "(", "*", "args", ",", "mode", "=", "'rgb_array'", ",", "**", "kwargs", ")", "for", "env", "in", "self", ".", "envs", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.DummyVecEnv.render": [[318, 334], ["all_vec_env.DummyVecEnv.envs[].render", "super().render"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.render", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.render"], ["", "def", "render", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Gym environment rendering. If there are multiple environments then\n        they are tiled together in one image via `BaseVecEnv.render()`.\n        Otherwise (if `self.num_envs == 1`), we pass the render call directly to the\n        underlying environment.\n\n        Therefore, some arguments such as `mode` will have values that are valid\n        only when `num_envs == 1`.\n\n        :param mode: The rendering type.\n        \"\"\"", "\n", "if", "self", ".", "num_envs", "==", "1", ":", "\n", "            ", "return", "self", ".", "envs", "[", "0", "]", ".", "render", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "", "else", ":", "\n", "            ", "return", "super", "(", ")", ".", "render", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.DummyVecEnv._save_obs": [[335, 341], ["None"], "methods", ["None"], ["", "", "def", "_save_obs", "(", "self", ",", "env_idx", ",", "obs", ")", ":", "\n", "        ", "for", "key", "in", "self", ".", "keys", ":", "\n", "            ", "if", "key", "is", "None", ":", "\n", "                ", "self", ".", "buf_obs", "[", "key", "]", "[", "env_idx", "]", "=", "obs", "\n", "", "else", ":", "\n", "                ", "self", ".", "buf_obs", "[", "key", "]", "[", "env_idx", "]", "=", "obs", "[", "key", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.DummyVecEnv._obs_from_buf": [[342, 344], ["core.vec_env.util.dict_to_obs", "core.vec_env.util.copy_obs_dict"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.util.dict_to_obs", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.util.copy_obs_dict"], ["", "", "", "def", "_obs_from_buf", "(", "self", ")", ":", "\n", "        ", "return", "dict_to_obs", "(", "self", ".", "observation_space", ",", "copy_obs_dict", "(", "self", ".", "buf_obs", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.DummyVecEnv.get_attr": [[345, 349], ["all_vec_env.DummyVecEnv._get_target_envs", "getattr"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.DummyVecEnv._get_target_envs"], ["", "def", "get_attr", "(", "self", ",", "attr_name", ",", "indices", "=", "None", ")", ":", "\n", "        ", "\"\"\"Return attribute from vectorized environment (see base class).\"\"\"", "\n", "target_envs", "=", "self", ".", "_get_target_envs", "(", "indices", ")", "\n", "return", "[", "getattr", "(", "env_i", ",", "attr_name", ")", "for", "env_i", "in", "target_envs", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.DummyVecEnv.set_attr": [[350, 355], ["all_vec_env.DummyVecEnv._get_target_envs", "setattr"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.DummyVecEnv._get_target_envs"], ["", "def", "set_attr", "(", "self", ",", "attr_name", ",", "value", ",", "indices", "=", "None", ")", ":", "\n", "        ", "\"\"\"Set attribute inside vectorized environments (see base class).\"\"\"", "\n", "target_envs", "=", "self", ".", "_get_target_envs", "(", "indices", ")", "\n", "for", "env_i", "in", "target_envs", ":", "\n", "            ", "setattr", "(", "env_i", ",", "attr_name", ",", "value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.DummyVecEnv.env_method": [[356, 360], ["all_vec_env.DummyVecEnv._get_target_envs", "getattr"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.DummyVecEnv._get_target_envs"], ["", "", "def", "env_method", "(", "self", ",", "method_name", ",", "*", "method_args", ",", "indices", "=", "None", ",", "**", "method_kwargs", ")", ":", "\n", "        ", "\"\"\"Call instance methods of vectorized environments.\"\"\"", "\n", "target_envs", "=", "self", ".", "_get_target_envs", "(", "indices", ")", "\n", "return", "[", "getattr", "(", "env_i", ",", "method_name", ")", "(", "*", "method_args", ",", "**", "method_kwargs", ")", "for", "env_i", "in", "target_envs", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.DummyVecEnv._get_target_envs": [[361, 364], ["all_vec_env.DummyVecEnv._get_indices"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnv._get_indices"], ["", "def", "_get_target_envs", "(", "self", ",", "indices", ")", ":", "\n", "        ", "indices", "=", "self", ".", "_get_indices", "(", "indices", ")", "\n", "return", "[", "self", ".", "envs", "[", "i", "]", "for", "i", "in", "indices", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.SubprocVecEnv.__init__": [[391, 417], ["len", "multiprocessing.get_context", "zip", "zip", "all_vec_env.SubprocVecEnv.remotes[].send", "all_vec_env.SubprocVecEnv.remotes[].recv", "core.vec_env.base_vec_env.VecEnv.__init__", "multiprocessing.get_context.Process", "multiprocessing.get_context.Process.start", "all_vec_env.SubprocVecEnv.processes.append", "work_remote.close", "len", "multiprocessing.get_all_start_methods", "core.vec_env.base_vec_env.CloudpickleWrapper", "multiprocessing.get_context.Pipe", "range"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__init__", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.close"], ["def", "__init__", "(", "self", ",", "env_fns", ",", "start_method", "=", "None", ")", ":", "\n", "        ", "self", ".", "waiting", "=", "False", "\n", "self", ".", "closed", "=", "False", "\n", "n_envs", "=", "len", "(", "env_fns", ")", "\n", "\n", "if", "start_method", "is", "None", ":", "\n", "# Fork is not a thread safe method (see issue #217)", "\n", "# but is more user friendly (does not require to wrap the code in", "\n", "# a `if __name__ == \"__main__\":`)", "\n", "            ", "forkserver_available", "=", "'forkserver'", "in", "multiprocessing", ".", "get_all_start_methods", "(", ")", "\n", "start_method", "=", "'forkserver'", "if", "forkserver_available", "else", "'spawn'", "\n", "", "ctx", "=", "multiprocessing", ".", "get_context", "(", "start_method", ")", "\n", "\n", "self", ".", "remotes", ",", "self", ".", "work_remotes", "=", "zip", "(", "*", "[", "ctx", ".", "Pipe", "(", "duplex", "=", "True", ")", "for", "_", "in", "range", "(", "n_envs", ")", "]", ")", "\n", "self", ".", "processes", "=", "[", "]", "\n", "for", "work_remote", ",", "remote", ",", "env_fn", "in", "zip", "(", "self", ".", "work_remotes", ",", "self", ".", "remotes", ",", "env_fns", ")", ":", "\n", "            ", "args", "=", "(", "work_remote", ",", "remote", ",", "CloudpickleWrapper", "(", "env_fn", ")", ")", "\n", "# daemon=True: if the main process crashes, we should not cause things to hang", "\n", "process", "=", "ctx", ".", "Process", "(", "target", "=", "_worker", ",", "args", "=", "args", ",", "daemon", "=", "True", ")", "# pytype:disable=attribute-error", "\n", "process", ".", "start", "(", ")", "\n", "self", ".", "processes", ".", "append", "(", "process", ")", "\n", "work_remote", ".", "close", "(", ")", "\n", "\n", "", "self", ".", "remotes", "[", "0", "]", ".", "send", "(", "(", "'get_spaces'", ",", "None", ")", ")", "\n", "observation_space", ",", "action_space", "=", "self", ".", "remotes", "[", "0", "]", ".", "recv", "(", ")", "\n", "VecEnv", ".", "__init__", "(", "self", ",", "len", "(", "env_fns", ")", ",", "observation_space", ",", "action_space", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.SubprocVecEnv.step_async": [[418, 422], ["zip", "remote.send"], "methods", ["None"], ["", "def", "step_async", "(", "self", ",", "actions", ")", ":", "\n", "        ", "for", "remote", ",", "action", "in", "zip", "(", "self", ".", "remotes", ",", "actions", ")", ":", "\n", "            ", "remote", ".", "send", "(", "(", "'step'", ",", "action", ")", ")", "\n", "", "self", ".", "waiting", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.SubprocVecEnv.step_wait": [[423, 428], ["zip", "remote.recv", "all_vec_env._flatten_obs", "numpy.stack", "numpy.stack"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env._flatten_obs"], ["", "def", "step_wait", "(", "self", ")", ":", "\n", "        ", "results", "=", "[", "remote", ".", "recv", "(", ")", "for", "remote", "in", "self", ".", "remotes", "]", "\n", "self", ".", "waiting", "=", "False", "\n", "obs", ",", "rews", ",", "dones", ",", "infos", "=", "zip", "(", "*", "results", ")", "\n", "return", "_flatten_obs", "(", "obs", ",", "self", ".", "observation_space", ")", ",", "np", ".", "stack", "(", "rews", ")", ",", "np", ".", "stack", "(", "dones", ")", ",", "infos", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.SubprocVecEnv.seed": [[429, 433], ["enumerate", "remote.send", "remote.recv"], "methods", ["None"], ["", "def", "seed", "(", "self", ",", "seed", "=", "None", ")", ":", "\n", "        ", "for", "idx", ",", "remote", "in", "enumerate", "(", "self", ".", "remotes", ")", ":", "\n", "            ", "remote", ".", "send", "(", "(", "'seed'", ",", "seed", "+", "idx", ")", ")", "\n", "", "return", "[", "remote", ".", "recv", "(", ")", "for", "remote", "in", "self", ".", "remotes", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.SubprocVecEnv.reset": [[434, 439], ["all_vec_env._flatten_obs", "remote.send", "remote.recv"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env._flatten_obs"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "for", "remote", "in", "self", ".", "remotes", ":", "\n", "            ", "remote", ".", "send", "(", "(", "'reset'", ",", "None", ")", ")", "\n", "", "obs", "=", "[", "remote", ".", "recv", "(", ")", "for", "remote", "in", "self", ".", "remotes", "]", "\n", "return", "_flatten_obs", "(", "obs", ",", "self", ".", "observation_space", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.SubprocVecEnv.close": [[440, 451], ["remote.send", "process.join", "remote.recv"], "methods", ["None"], ["", "def", "close", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "closed", ":", "\n", "            ", "return", "\n", "", "if", "self", ".", "waiting", ":", "\n", "            ", "for", "remote", "in", "self", ".", "remotes", ":", "\n", "                ", "remote", ".", "recv", "(", ")", "\n", "", "", "for", "remote", "in", "self", ".", "remotes", ":", "\n", "            ", "remote", ".", "send", "(", "(", "'close'", ",", "None", ")", ")", "\n", "", "for", "process", "in", "self", ".", "processes", ":", "\n", "            ", "process", ".", "join", "(", ")", "\n", "", "self", ".", "closed", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.SubprocVecEnv.get_images": [[452, 459], ["pipe.send", "pipe.recv"], "methods", ["None"], ["", "def", "get_images", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", "->", "Sequence", "[", "np", ".", "ndarray", "]", ":", "\n", "        ", "for", "pipe", "in", "self", ".", "remotes", ":", "\n", "# gather images from subprocesses", "\n", "# `mode` will be taken into account later", "\n", "            ", "pipe", ".", "send", "(", "(", "'render'", ",", "(", "args", ",", "{", "'mode'", ":", "'rgb_array'", ",", "**", "kwargs", "}", ")", ")", ")", "\n", "", "imgs", "=", "[", "pipe", ".", "recv", "(", ")", "for", "pipe", "in", "self", ".", "remotes", "]", "\n", "return", "imgs", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.SubprocVecEnv.get_attr": [[460, 466], ["all_vec_env.SubprocVecEnv._get_target_remotes", "remote.send", "remote.recv"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.SubprocVecEnv._get_target_remotes"], ["", "def", "get_attr", "(", "self", ",", "attr_name", ",", "indices", "=", "None", ")", ":", "\n", "        ", "\"\"\"Return attribute from vectorized environment (see base class).\"\"\"", "\n", "target_remotes", "=", "self", ".", "_get_target_remotes", "(", "indices", ")", "\n", "for", "remote", "in", "target_remotes", ":", "\n", "            ", "remote", ".", "send", "(", "(", "'get_attr'", ",", "attr_name", ")", ")", "\n", "", "return", "[", "remote", ".", "recv", "(", ")", "for", "remote", "in", "target_remotes", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.SubprocVecEnv.set_attr": [[467, 474], ["all_vec_env.SubprocVecEnv._get_target_remotes", "remote.send", "remote.recv"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.SubprocVecEnv._get_target_remotes"], ["", "def", "set_attr", "(", "self", ",", "attr_name", ",", "value", ",", "indices", "=", "None", ")", ":", "\n", "        ", "\"\"\"Set attribute inside vectorized environments (see base class).\"\"\"", "\n", "target_remotes", "=", "self", ".", "_get_target_remotes", "(", "indices", ")", "\n", "for", "remote", "in", "target_remotes", ":", "\n", "            ", "remote", ".", "send", "(", "(", "'set_attr'", ",", "(", "attr_name", ",", "value", ")", ")", ")", "\n", "", "for", "remote", "in", "target_remotes", ":", "\n", "            ", "remote", ".", "recv", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.SubprocVecEnv.env_method": [[475, 481], ["all_vec_env.SubprocVecEnv._get_target_remotes", "remote.send", "remote.recv"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.SubprocVecEnv._get_target_remotes"], ["", "", "def", "env_method", "(", "self", ",", "method_name", ",", "*", "method_args", ",", "indices", "=", "None", ",", "**", "method_kwargs", ")", ":", "\n", "        ", "\"\"\"Call instance methods of vectorized environments.\"\"\"", "\n", "target_remotes", "=", "self", ".", "_get_target_remotes", "(", "indices", ")", "\n", "for", "remote", "in", "target_remotes", ":", "\n", "            ", "remote", ".", "send", "(", "(", "'env_method'", ",", "(", "method_name", ",", "method_args", ",", "method_kwargs", ")", ")", ")", "\n", "", "return", "[", "remote", ".", "recv", "(", ")", "for", "remote", "in", "target_remotes", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env.SubprocVecEnv._get_target_remotes": [[482, 492], ["all_vec_env.SubprocVecEnv._get_indices"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnv._get_indices"], ["", "def", "_get_target_remotes", "(", "self", ",", "indices", ")", ":", "\n", "        ", "\"\"\"\n        Get the connection object needed to communicate with the wanted\n        envs that are in subprocesses.\n\n        :param indices: (None,int,Iterable) refers to indices of envs.\n        :return: ([multiprocessing.Connection]) Connection object to communicate between processes.\n        \"\"\"", "\n", "indices", "=", "self", ".", "_get_indices", "(", "indices", ")", "\n", "return", "[", "self", ".", "remotes", "[", "i", "]", "for", "i", "in", "indices", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env._worker": [[494, 530], ["parent_remote.close", "env_fn_wrapper.var", "remote.recv", "env_fn_wrapper.var.step", "remote.send", "env_fn_wrapper.var.reset", "remote.send", "env_fn_wrapper.var.seed", "env_fn_wrapper.var.reset", "remote.send", "remote.send", "env_fn_wrapper.var.render", "remote.close", "remote.send", "getattr", "remote.send", "getattr.", "remote.send", "getattr", "remote.send", "setattr"], "function", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.close", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnv.step", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.reset", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.seed", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.reset", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.render", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.close"], ["", "", "def", "_worker", "(", "remote", ",", "parent_remote", ",", "env_fn_wrapper", ")", ":", "\n", "    ", "parent_remote", ".", "close", "(", ")", "\n", "env", "=", "env_fn_wrapper", ".", "var", "(", ")", "\n", "while", "True", ":", "\n", "        ", "try", ":", "\n", "            ", "cmd", ",", "data", "=", "remote", ".", "recv", "(", ")", "\n", "if", "cmd", "==", "'step'", ":", "\n", "                ", "observation", ",", "reward", ",", "done", ",", "info", "=", "env", ".", "step", "(", "data", ")", "\n", "if", "done", ":", "\n", "# save final observation where user can get it, then reset", "\n", "                    ", "info", "[", "'terminal_observation'", "]", "=", "observation", "\n", "observation", "=", "env", ".", "reset", "(", ")", "\n", "", "remote", ".", "send", "(", "(", "observation", ",", "reward", ",", "done", ",", "info", ")", ")", "\n", "", "elif", "cmd", "==", "'seed'", ":", "\n", "                ", "remote", ".", "send", "(", "env", ".", "seed", "(", "data", ")", ")", "\n", "", "elif", "cmd", "==", "'reset'", ":", "\n", "                ", "observation", "=", "env", ".", "reset", "(", ")", "\n", "remote", ".", "send", "(", "observation", ")", "\n", "", "elif", "cmd", "==", "'render'", ":", "\n", "                ", "remote", ".", "send", "(", "env", ".", "render", "(", "*", "data", "[", "0", "]", ",", "**", "data", "[", "1", "]", ")", ")", "\n", "", "elif", "cmd", "==", "'close'", ":", "\n", "                ", "remote", ".", "close", "(", ")", "\n", "break", "\n", "", "elif", "cmd", "==", "'get_spaces'", ":", "\n", "                ", "remote", ".", "send", "(", "(", "env", ".", "observation_space", ",", "env", ".", "action_space", ")", ")", "\n", "", "elif", "cmd", "==", "'env_method'", ":", "\n", "                ", "method", "=", "getattr", "(", "env", ",", "data", "[", "0", "]", ")", "\n", "remote", ".", "send", "(", "method", "(", "*", "data", "[", "1", "]", ",", "**", "data", "[", "2", "]", ")", ")", "\n", "", "elif", "cmd", "==", "'get_attr'", ":", "\n", "                ", "remote", ".", "send", "(", "getattr", "(", "env", ",", "data", ")", ")", "\n", "", "elif", "cmd", "==", "'set_attr'", ":", "\n", "                ", "remote", ".", "send", "(", "setattr", "(", "env", ",", "data", "[", "0", "]", ",", "data", "[", "1", "]", ")", ")", "\n", "", "else", ":", "\n", "                ", "raise", "NotImplementedError", "\n", "", "", "except", "EOFError", ":", "\n", "            ", "break", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.all_vec_env._flatten_obs": [[532, 556], ["isinstance", "isinstance", "len", "isinstance", "isinstance", "collections.OrderedDict", "isinstance", "isinstance", "len", "tuple", "numpy.stack", "numpy.stack", "space.spaces.keys", "numpy.stack", "range"], "function", ["None"], ["", "", "", "def", "_flatten_obs", "(", "obs", ",", "space", ")", ":", "\n", "    ", "\"\"\"\n    Flatten observations, depending on the observation space.\n\n    :param obs: (list<X> or tuple<X> where X is dict<ndarray>, tuple<ndarray> or ndarray) observations.\n                A list or tuple of observations, one per environment.\n                Each environment observation may be a NumPy array, or a dict or tuple of NumPy arrays.\n    :return (OrderedDict<ndarray>, tuple<ndarray> or ndarray) flattened observations.\n            A flattened NumPy array or an OrderedDict or tuple of flattened numpy arrays.\n            Each NumPy array has the environment index as its first axis.\n    \"\"\"", "\n", "assert", "isinstance", "(", "obs", ",", "(", "list", ",", "tuple", ")", ")", ",", "\"expected list or tuple of observations per environment\"", "\n", "assert", "len", "(", "obs", ")", ">", "0", ",", "\"need observations from at least one environment\"", "\n", "\n", "if", "isinstance", "(", "space", ",", "gym", ".", "spaces", ".", "Dict", ")", ":", "\n", "        ", "assert", "isinstance", "(", "space", ".", "spaces", ",", "OrderedDict", ")", ",", "\"Dict space must have ordered subspaces\"", "\n", "assert", "isinstance", "(", "obs", "[", "0", "]", ",", "dict", ")", ",", "\"non-dict observation for environment with Dict observation space\"", "\n", "return", "OrderedDict", "(", "[", "(", "k", ",", "np", ".", "stack", "(", "[", "o", "[", "k", "]", "for", "o", "in", "obs", "]", ")", ")", "for", "k", "in", "space", ".", "spaces", ".", "keys", "(", ")", "]", ")", "\n", "", "elif", "isinstance", "(", "space", ",", "gym", ".", "spaces", ".", "Tuple", ")", ":", "\n", "        ", "assert", "isinstance", "(", "obs", "[", "0", "]", ",", "tuple", ")", ",", "\"non-tuple observation for environment with Tuple observation space\"", "\n", "obs_len", "=", "len", "(", "space", ".", "spaces", ")", "\n", "return", "tuple", "(", "(", "np", ".", "stack", "(", "[", "o", "[", "i", "]", "for", "o", "in", "obs", "]", ")", "for", "i", "in", "range", "(", "obs_len", ")", ")", ")", "\n", "", "else", ":", "\n", "        ", "return", "np", ".", "stack", "(", "obs", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.AlreadySteppingError.__init__": [[19, 22], ["Exception.__init__"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "msg", "=", "'already running an async step'", "\n", "Exception", ".", "__init__", "(", "self", ",", "msg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.NotSteppingError.__init__": [[30, 33], ["Exception.__init__"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "msg", "=", "'not running an async step'", "\n", "Exception", ".", "__init__", "(", "self", ",", "msg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnv.__init__": [[47, 51], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "num_envs", ",", "observation_space", ",", "action_space", ")", ":", "\n", "        ", "self", ".", "num_envs", "=", "num_envs", "\n", "self", ".", "observation_space", "=", "observation_space", "\n", "self", ".", "action_space", "=", "action_space", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnv.reset": [[52, 65], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "reset", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Reset all the environments and return an array of\n        observations, or a tuple of observation arrays.\n\n        If step_async is still doing work, that work will\n        be cancelled and step_wait() should not be called\n        until step_async() is invoked again.\n\n        :return: ([int] or [float]) observation\n        \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnv.step_async": [[66, 77], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "step_async", "(", "self", ",", "actions", ")", ":", "\n", "        ", "\"\"\"\n        Tell all the environments to start taking a step\n        with the given actions.\n        Call step_wait() to get the results of the step.\n\n        You should not call this if a step_async run is\n        already pending.\n        \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnv.step_wait": [[78, 86], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "step_wait", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Wait for the step taken with step_async().\n\n        :return: ([int] or [float], [float], [bool], dict) observation, reward, done, information\n        \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnv.close": [[87, 93], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "close", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Clean up the environment's resources.\n        \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnv.get_attr": [[94, 104], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "get_attr", "(", "self", ",", "attr_name", ",", "indices", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Return attribute from vectorized environment.\n\n        :param attr_name: (str) The name of the attribute whose value to return\n        :param indices: (list,int) Indices of envs to get attribute from\n        :return: (list) List of values of 'attr_name' in all environments\n        \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnv.set_attr": [[105, 116], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "set_attr", "(", "self", ",", "attr_name", ",", "value", ",", "indices", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Set attribute inside vectorized environments.\n\n        :param attr_name: (str) The name of attribute to assign new value\n        :param value: (obj) Value to assign to `attr_name`\n        :param indices: (list,int) Indices of envs to assign value\n        :return: (NoneType)\n        \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnv.env_method": [[117, 129], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "env_method", "(", "self", ",", "method_name", ",", "*", "method_args", ",", "indices", "=", "None", ",", "**", "method_kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Call instance methods of vectorized environments.\n\n        :param method_name: (str) The name of the environment method to invoke.\n        :param indices: (list,int) Indices of envs whose method to call\n        :param method_args: (tuple) Any positional arguments to provide in the call\n        :param method_kwargs: (dict) Any keyword arguments to provide in the call\n        :return: (list) List of items returned by the environment's method call\n        \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnv.seed": [[130, 141], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "seed", "(", "self", ",", "seed", ":", "Optional", "[", "int", "]", "=", "None", ")", "->", "List", "[", "Union", "[", "None", ",", "int", "]", "]", ":", "\n", "        ", "\"\"\"\n        Sets the random seeds for all environments, based on a given seed.\n        Each individual environment will still get its own seed, by incrementing the given seed.\n\n        :param seed: (Optional[int]) The random seed. May be None for completely random seeding.\n        :return: (List[Union[None, int]]) Returns a list containing the seeds for each individual env.\n            Note that all list elements may be None, if the env does not return anything when being seeded.\n        \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnv.step": [[142, 151], ["base_vec_env.VecEnv.step_async", "base_vec_env.VecEnv.step_wait"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.step_async", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.step_wait"], ["", "def", "step", "(", "self", ",", "actions", ")", ":", "\n", "        ", "\"\"\"\n        Step the environments with the given action\n\n        :param actions: ([int] or [float]) the action\n        :return: ([int] or [float], [float], [bool], dict) observation, reward, done, information\n        \"\"\"", "\n", "self", ".", "step_async", "(", "actions", ")", "\n", "return", "self", ".", "step_wait", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnv.get_images": [[152, 157], ["None"], "methods", ["None"], ["", "def", "get_images", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", "->", "Sequence", "[", "np", ".", "ndarray", "]", ":", "\n", "        ", "\"\"\"\n        Return RGB images from each environment\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnv.render": [[158, 180], ["core.vec_env.util.tile_images", "base_vec_env.VecEnv.get_images", "cv2.imshow", "cv2.waitKey", "core.logger.warn"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.util.tile_images", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.get_images", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.logger.warn"], ["", "def", "render", "(", "self", ",", "mode", ":", "str", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Gym environment rendering\n\n        :param mode: the rendering type\n        \"\"\"", "\n", "try", ":", "\n", "            ", "imgs", "=", "self", ".", "get_images", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "", "except", "NotImplementedError", ":", "\n", "            ", "logger", ".", "warn", "(", "'Render not defined for {}'", ".", "format", "(", "self", ")", ")", "\n", "return", "\n", "\n", "# Create a big image by tiling images from subprocesses", "\n", "", "bigimg", "=", "tile_images", "(", "imgs", ")", "\n", "if", "mode", "==", "'human'", ":", "\n", "            ", "import", "cv2", "# pytype:disable=import-error", "\n", "cv2", ".", "imshow", "(", "'vecenv'", ",", "bigimg", "[", ":", ",", ":", ",", ":", ":", "-", "1", "]", ")", "\n", "cv2", ".", "waitKey", "(", "1", ")", "\n", "", "elif", "mode", "==", "'rgb_array'", ":", "\n", "            ", "return", "bigimg", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnv.unwrapped": [[181, 187], ["isinstance"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "unwrapped", "(", "self", ")", ":", "\n", "        ", "if", "isinstance", "(", "self", ",", "VecEnvWrapper", ")", ":", "\n", "            ", "return", "self", ".", "venv", ".", "unwrapped", "\n", "", "else", ":", "\n", "            ", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnv.getattr_depth_check": [[188, 199], ["hasattr", "type", "type"], "methods", ["None"], ["", "", "def", "getattr_depth_check", "(", "self", ",", "name", ",", "already_found", ")", ":", "\n", "        ", "\"\"\"Check if an attribute reference is being hidden in a recursive call to __getattr__\n\n        :param name: (str) name of attribute to check for\n        :param already_found: (bool) whether this attribute has already been found in a wrapper\n        :return: (str or None) name of module whose attribute is being shadowed, if any.\n        \"\"\"", "\n", "if", "hasattr", "(", "self", ",", "name", ")", "and", "already_found", ":", "\n", "            ", "return", "\"{0}.{1}\"", ".", "format", "(", "type", "(", "self", ")", ".", "__module__", ",", "type", "(", "self", ")", ".", "__name__", ")", "\n", "", "else", ":", "\n", "            ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnv._get_indices": [[200, 212], ["range", "isinstance"], "methods", ["None"], ["", "", "def", "_get_indices", "(", "self", ",", "indices", ")", ":", "\n", "        ", "\"\"\"\n        Convert a flexibly-typed reference to environment indices to an implied list of indices.\n\n        :param indices: (None,int,Iterable) refers to indices of envs.\n        :return: (list) the implied list of indices.\n        \"\"\"", "\n", "if", "indices", "is", "None", ":", "\n", "            ", "indices", "=", "range", "(", "self", ".", "num_envs", ")", "\n", "", "elif", "isinstance", "(", "indices", ",", "int", ")", ":", "\n", "            ", "indices", "=", "[", "indices", "]", "\n", "", "return", "indices", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.__init__": [[223, 228], ["base_vec_env.VecEnv.__init__", "dict", "inspect.getmembers"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__init__"], ["def", "__init__", "(", "self", ",", "venv", ",", "observation_space", "=", "None", ",", "action_space", "=", "None", ")", ":", "\n", "        ", "self", ".", "venv", "=", "venv", "\n", "VecEnv", ".", "__init__", "(", "self", ",", "num_envs", "=", "venv", ".", "num_envs", ",", "observation_space", "=", "observation_space", "or", "venv", ".", "observation_space", ",", "\n", "action_space", "=", "action_space", "or", "venv", ".", "action_space", ")", "\n", "self", ".", "class_attributes", "=", "dict", "(", "inspect", ".", "getmembers", "(", "self", ".", "__class__", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.step_async": [[229, 231], ["base_vec_env.VecEnvWrapper.venv.step_async"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.step_async"], ["", "def", "step_async", "(", "self", ",", "actions", ")", ":", "\n", "        ", "self", ".", "venv", ".", "step_async", "(", "actions", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.reset": [[232, 235], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "reset", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.step_wait": [[236, 239], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "step_wait", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.seed": [[240, 242], ["base_vec_env.VecEnvWrapper.venv.seed"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.seed"], ["", "def", "seed", "(", "self", ",", "seed", "=", "None", ")", ":", "\n", "        ", "return", "self", ".", "venv", ".", "seed", "(", "seed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.close": [[243, 245], ["base_vec_env.VecEnvWrapper.venv.close"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.close"], ["", "def", "close", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "venv", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.render": [[246, 248], ["base_vec_env.VecEnvWrapper.venv.render"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.render"], ["", "def", "render", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "self", ".", "venv", ".", "render", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.get_images": [[249, 251], ["base_vec_env.VecEnvWrapper.venv.get_images"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.get_images"], ["", "def", "get_images", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "venv", ".", "get_images", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.get_attr": [[252, 254], ["base_vec_env.VecEnvWrapper.venv.get_attr"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.get_attr"], ["", "def", "get_attr", "(", "self", ",", "attr_name", ",", "indices", "=", "None", ")", ":", "\n", "        ", "return", "self", ".", "venv", ".", "get_attr", "(", "attr_name", ",", "indices", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.set_attr": [[255, 257], ["base_vec_env.VecEnvWrapper.venv.set_attr"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.set_attr"], ["", "def", "set_attr", "(", "self", ",", "attr_name", ",", "value", ",", "indices", "=", "None", ")", ":", "\n", "        ", "return", "self", ".", "venv", ".", "set_attr", "(", "attr_name", ",", "value", ",", "indices", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.env_method": [[258, 260], ["base_vec_env.VecEnvWrapper.venv.env_method"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.env_method"], ["", "def", "env_method", "(", "self", ",", "method_name", ",", "*", "method_args", ",", "indices", "=", "None", ",", "**", "method_kwargs", ")", ":", "\n", "        ", "return", "self", ".", "venv", ".", "env_method", "(", "method_name", ",", "*", "method_args", ",", "indices", "=", "indices", ",", "**", "method_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.__getattr__": [[261, 274], ["base_vec_env.VecEnvWrapper.getattr_depth_check", "base_vec_env.VecEnvWrapper.getattr_recursive", "AttributeError", "format_str.format", "type", "type"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.getattr_depth_check", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.getattr_recursive"], ["", "def", "__getattr__", "(", "self", ",", "name", ")", ":", "\n", "        ", "\"\"\"Find attribute from wrapped venv(s) if this wrapper does not have it.\n        Useful for accessing attributes from venvs which are wrapped with multiple wrappers\n        which have unique attributes of interest.\n        \"\"\"", "\n", "blocked_class", "=", "self", ".", "getattr_depth_check", "(", "name", ",", "already_found", "=", "False", ")", "\n", "if", "blocked_class", "is", "not", "None", ":", "\n", "            ", "own_class", "=", "\"{0}.{1}\"", ".", "format", "(", "type", "(", "self", ")", ".", "__module__", ",", "type", "(", "self", ")", ".", "__name__", ")", "\n", "format_str", "=", "(", "\"Error: Recursive attribute lookup for {0} from {1} is \"", "\n", "\"ambiguous and hides attribute from {2}\"", ")", "\n", "raise", "AttributeError", "(", "format_str", ".", "format", "(", "name", ",", "own_class", ",", "blocked_class", ")", ")", "\n", "\n", "", "return", "self", ".", "getattr_recursive", "(", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper._get_all_attributes": [[275, 283], ["base_vec_env.VecEnvWrapper.__dict__.copy", "base_vec_env.VecEnvWrapper.update"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.core.math_util.RunningMeanStd.update"], ["", "def", "_get_all_attributes", "(", "self", ")", ":", "\n", "        ", "\"\"\"Get all (inherited) instance and class attributes\n\n        :return: (dict<str, object>) all_attributes\n        \"\"\"", "\n", "all_attributes", "=", "self", ".", "__dict__", ".", "copy", "(", ")", "\n", "all_attributes", ".", "update", "(", "self", ".", "class_attributes", ")", "\n", "return", "all_attributes", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.getattr_recursive": [[284, 301], ["base_vec_env.VecEnvWrapper._get_all_attributes", "getattr", "hasattr", "base_vec_env.VecEnvWrapper.venv.getattr_recursive", "getattr"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper._get_all_attributes", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.getattr_recursive"], ["", "def", "getattr_recursive", "(", "self", ",", "name", ")", ":", "\n", "        ", "\"\"\"Recursively check wrappers to find attribute.\n\n        :param name (str) name of attribute to look for\n        :return: (object) attribute\n        \"\"\"", "\n", "all_attributes", "=", "self", ".", "_get_all_attributes", "(", ")", "\n", "if", "name", "in", "all_attributes", ":", "# attribute is present in this wrapper", "\n", "            ", "attr", "=", "getattr", "(", "self", ",", "name", ")", "\n", "", "elif", "hasattr", "(", "self", ".", "venv", ",", "'getattr_recursive'", ")", ":", "\n", "# Attribute not present, child is wrapper. Call getattr_recursive rather than getattr", "\n", "# to avoid a duplicate call to getattr_depth_check.", "\n", "            ", "attr", "=", "self", ".", "venv", ".", "getattr_recursive", "(", "name", ")", "\n", "", "else", ":", "# attribute not present, child is an unwrapped VecEnv", "\n", "            ", "attr", "=", "getattr", "(", "self", ".", "venv", ",", "name", ")", "\n", "\n", "", "return", "attr", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.getattr_depth_check": [[302, 319], ["base_vec_env.VecEnvWrapper._get_all_attributes", "base_vec_env.VecEnvWrapper.venv.getattr_depth_check", "base_vec_env.VecEnvWrapper.venv.getattr_depth_check", "type", "type"], "methods", ["home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper._get_all_attributes", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.getattr_depth_check", "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.VecEnvWrapper.getattr_depth_check"], ["", "def", "getattr_depth_check", "(", "self", ",", "name", ",", "already_found", ")", ":", "\n", "        ", "\"\"\"See base class.\n\n        :return: (str or None) name of module whose attribute is being shadowed, if any.\n        \"\"\"", "\n", "all_attributes", "=", "self", ".", "_get_all_attributes", "(", ")", "\n", "if", "name", "in", "all_attributes", "and", "already_found", ":", "\n", "# this venv's attribute is being hidden because of a higher venv.", "\n", "            ", "shadowed_wrapper_class", "=", "\"{0}.{1}\"", ".", "format", "(", "type", "(", "self", ")", ".", "__module__", ",", "type", "(", "self", ")", ".", "__name__", ")", "\n", "", "elif", "name", "in", "all_attributes", "and", "not", "already_found", ":", "\n", "# we have found the first reference to the attribute. Now check for duplicates.", "\n", "            ", "shadowed_wrapper_class", "=", "self", ".", "venv", ".", "getattr_depth_check", "(", "name", ",", "True", ")", "\n", "", "else", ":", "\n", "# this wrapper does not have the attribute. Keep searching.", "\n", "            ", "shadowed_wrapper_class", "=", "self", ".", "venv", ".", "getattr_depth_check", "(", "name", ",", "already_found", ")", "\n", "\n", "", "return", "shadowed_wrapper_class", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__init__": [[322, 329], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "var", ")", ":", "\n", "        ", "\"\"\"\n        Uses cloudpickle to serialize contents (otherwise multiprocessing tries to use pickle)\n\n        :param var: (Any) the variable you wish to wrap for pickling with cloudpickle\n        \"\"\"", "\n", "self", ".", "var", "=", "var", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__getstate__": [[330, 332], ["cloudpickle.dumps"], "methods", ["None"], ["", "def", "__getstate__", "(", "self", ")", ":", "\n", "        ", "return", "cloudpickle", ".", "dumps", "(", "self", ".", "var", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.base_vec_env.CloudpickleWrapper.__setstate__": [[333, 335], ["pickle.loads"], "methods", ["None"], ["", "def", "__setstate__", "(", "self", ",", "obs", ")", ":", "\n", "        ", "self", ".", "var", "=", "pickle", ".", "loads", "(", "obs", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.__init__.unwrap_vec_normalize": [[11, 22], ["isinstance", "isinstance"], "function", ["None"], []], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.__init__.sync_envs_normalization": [[25, 47], ["isinstance", "isinstance", "isinstance", "isinstance", "copy.deepcopy", "copy.deepcopy"], "function", ["None"], []], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.util.tile_images": [[11, 35], ["numpy.asarray", "int", "int", "numpy.array", "np.array.reshape", "out_image.reshape.transpose", "out_image.reshape.reshape", "numpy.ceil", "numpy.ceil", "numpy.sqrt", "list", "float", "range"], "function", ["None"], ["def", "tile_images", "(", "img_nhwc", ")", ":", "\n", "    ", "\"\"\"\n    Tile N images into one big PxQ image\n    (P,Q) are chosen to be as close as possible, and if N\n    is square, then P=Q.\n\n    :param img_nhwc: (list) list or array of images, ndim=4 once turned into array. img nhwc\n        n = batch index, h = height, w = width, c = channel\n    :return: (numpy float) img_HWc, ndim=3\n    \"\"\"", "\n", "img_nhwc", "=", "np", ".", "asarray", "(", "img_nhwc", ")", "\n", "n_images", ",", "height", ",", "width", ",", "n_channels", "=", "img_nhwc", ".", "shape", "\n", "# new_height was named H before", "\n", "new_height", "=", "int", "(", "np", ".", "ceil", "(", "np", ".", "sqrt", "(", "n_images", ")", ")", ")", "\n", "# new_width was named W before", "\n", "new_width", "=", "int", "(", "np", ".", "ceil", "(", "float", "(", "n_images", ")", "/", "new_height", ")", ")", "\n", "img_nhwc", "=", "np", ".", "array", "(", "list", "(", "img_nhwc", ")", "+", "[", "img_nhwc", "[", "0", "]", "*", "0", "for", "_", "in", "range", "(", "n_images", ",", "new_height", "*", "new_width", ")", "]", ")", "\n", "# img_HWhwc", "\n", "out_image", "=", "img_nhwc", ".", "reshape", "(", "new_height", ",", "new_width", ",", "height", ",", "width", ",", "n_channels", ")", "\n", "# img_HhWwc", "\n", "out_image", "=", "out_image", ".", "transpose", "(", "0", ",", "2", ",", "1", ",", "3", ",", "4", ")", "\n", "# img_Hh_Ww_c", "\n", "out_image", "=", "out_image", ".", "reshape", "(", "new_height", "*", "height", ",", "new_width", "*", "width", ",", "n_channels", ")", "\n", "return", "out_image", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.util.copy_obs_dict": [[37, 46], ["isinstance", "collections.OrderedDict", "type", "numpy.copy", "obs.items"], "function", ["None"], ["", "def", "copy_obs_dict", "(", "obs", ")", ":", "\n", "    ", "\"\"\"\n    Deep-copy a dict of numpy arrays.\n\n    :param obs: (OrderedDict<ndarray>): a dict of numpy arrays.\n    :return (OrderedDict<ndarray>) a dict of copied numpy arrays.\n    \"\"\"", "\n", "assert", "isinstance", "(", "obs", ",", "OrderedDict", ")", ",", "\"unexpected type for observations '{}'\"", ".", "format", "(", "type", "(", "obs", ")", ")", "\n", "return", "OrderedDict", "(", "[", "(", "k", ",", "np", ".", "copy", "(", "v", ")", ")", "for", "k", ",", "v", "in", "obs", ".", "items", "(", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.util.dict_to_obs": [[48, 68], ["isinstance", "isinstance", "tuple", "len", "len", "set", "obs_dict.keys", "range", "len"], "function", ["None"], ["", "def", "dict_to_obs", "(", "space", ",", "obs_dict", ")", ":", "\n", "    ", "\"\"\"\n    Convert an internal representation raw_obs into the appropriate type\n    specified by space.\n\n    :param space: (gym.spaces.Space) an observation space.\n    :param obs_dict: (OrderedDict<ndarray>) a dict of numpy arrays.\n    :return (ndarray, tuple<ndarray> or dict<ndarray>): returns an observation\n            of the same type as space. If space is Dict, function is identity;\n            if space is Tuple, converts dict to Tuple; otherwise, space is\n            unstructured and returns the value raw_obs[None].\n    \"\"\"", "\n", "if", "isinstance", "(", "space", ",", "gym", ".", "spaces", ".", "Dict", ")", ":", "\n", "        ", "return", "obs_dict", "\n", "", "elif", "isinstance", "(", "space", ",", "gym", ".", "spaces", ".", "Tuple", ")", ":", "\n", "        ", "assert", "len", "(", "obs_dict", ")", "==", "len", "(", "space", ".", "spaces", ")", ",", "\"size of observation does not match size of observation space\"", "\n", "return", "tuple", "(", "(", "obs_dict", "[", "i", "]", "for", "i", "in", "range", "(", "len", "(", "space", ".", "spaces", ")", ")", ")", ")", "\n", "", "else", ":", "\n", "        ", "assert", "set", "(", "obs_dict", ".", "keys", "(", ")", ")", "==", "{", "None", "}", ",", "\"multiple observation keys for unstructured observation space\"", "\n", "return", "obs_dict", "[", "None", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yfletberliac_adversarially-guided-actor-critic.vec_env.util.obs_space_info": [[70, 100], ["isinstance", "subspaces.items", "isinstance", "isinstance", "keys.append", "hasattr", "type", "enumerate"], "function", ["None"], ["", "", "def", "obs_space_info", "(", "obs_space", ")", ":", "\n", "    ", "\"\"\"\n    Get dict-structured information about a gym.Space.\n\n    Dict spaces are represented directly by their dict of subspaces.\n    Tuple spaces are converted into a dict with keys indexing into the tuple.\n    Unstructured spaces are represented by {None: obs_space}.\n\n    :param obs_space: (gym.spaces.Space) an observation space\n    :return (tuple) A tuple (keys, shapes, dtypes):\n        keys: a list of dict keys.\n        shapes: a dict mapping keys to shapes.\n        dtypes: a dict mapping keys to dtypes.\n    \"\"\"", "\n", "if", "isinstance", "(", "obs_space", ",", "gym", ".", "spaces", ".", "Dict", ")", ":", "\n", "        ", "assert", "isinstance", "(", "obs_space", ".", "spaces", ",", "OrderedDict", ")", ",", "\"Dict space must have ordered subspaces\"", "\n", "subspaces", "=", "obs_space", ".", "spaces", "\n", "", "elif", "isinstance", "(", "obs_space", ",", "gym", ".", "spaces", ".", "Tuple", ")", ":", "\n", "        ", "subspaces", "=", "{", "i", ":", "space", "for", "i", ",", "space", "in", "enumerate", "(", "obs_space", ".", "spaces", ")", "}", "\n", "", "else", ":", "\n", "        ", "assert", "not", "hasattr", "(", "obs_space", ",", "'spaces'", ")", ",", "\"Unsupported structured space '{}'\"", ".", "format", "(", "type", "(", "obs_space", ")", ")", "\n", "subspaces", "=", "{", "None", ":", "obs_space", "}", "\n", "", "keys", "=", "[", "]", "\n", "shapes", "=", "{", "}", "\n", "dtypes", "=", "{", "}", "\n", "for", "key", ",", "box", "in", "subspaces", ".", "items", "(", ")", ":", "\n", "        ", "keys", ".", "append", "(", "key", ")", "\n", "shapes", "[", "key", "]", "=", "box", ".", "shape", "\n", "dtypes", "[", "key", "]", "=", "box", ".", "dtype", "\n", "", "return", "keys", ",", "shapes", ",", "dtypes", "\n", "", ""]]}