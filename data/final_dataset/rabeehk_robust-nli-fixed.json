{"home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.InferSent.__init__": [[10, 33], ["torch.Module.__init__", "torch.LSTM", "torch.LSTM"], "methods", ["home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.ClassificationNet.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "InferSent", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "bsize", "=", "config", "[", "'bsize'", "]", "\n", "self", ".", "word_emb_dim", "=", "config", "[", "'word_emb_dim'", "]", "\n", "self", ".", "enc_lstm_dim", "=", "config", "[", "'enc_lstm_dim'", "]", "\n", "self", ".", "pool_type", "=", "config", "[", "'pool_type'", "]", "\n", "self", ".", "dpout_model", "=", "config", "[", "'dpout_model'", "]", "\n", "self", ".", "version", "=", "1", "if", "'version'", "not", "in", "config", "else", "config", "[", "'version'", "]", "\n", "\n", "self", ".", "enc_lstm", "=", "nn", ".", "LSTM", "(", "self", ".", "word_emb_dim", ",", "self", ".", "enc_lstm_dim", ",", "1", ",", "\n", "bidirectional", "=", "True", ",", "dropout", "=", "self", ".", "dpout_model", ")", "\n", "\n", "assert", "self", ".", "version", "in", "[", "1", ",", "2", "]", "\n", "if", "self", ".", "version", "==", "1", ":", "\n", "            ", "self", ".", "bos", "=", "'<s>'", "\n", "self", ".", "eos", "=", "'</s>'", "\n", "self", ".", "max_pad", "=", "True", "\n", "self", ".", "moses_tok", "=", "False", "\n", "", "elif", "self", ".", "version", "==", "2", ":", "\n", "            ", "self", ".", "bos", "=", "'<p>'", "\n", "self", ".", "eos", "=", "'</p>'", "\n", "self", ".", "max_pad", "=", "False", "\n", "self", ".", "moses_tok", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.InferSent.is_cuda": [[34, 37], ["None"], "methods", ["None"], ["", "", "def", "is_cuda", "(", "self", ")", ":", "\n", "# either all weights are on cpu or they are on gpu", "\n", "        ", "return", "self", ".", "enc_lstm", ".", "bias_hh_l0", ".", "data", ".", "is_cuda", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.InferSent.forward": [[38, 76], ["sent_len_sorted.copy.copy.copy", "numpy.argsort", "sent.index_select.index_select.index_select", "torch.utils.rnn.pack_padded_sequence", "torch.utils.rnn.pack_padded_sequence", "sent_output.index_select.index_select.index_select", "numpy.argsort", "models.InferSent.is_cuda", "torch.from_numpy().cuda", "torch.from_numpy().cuda", "torch.from_numpy().cuda", "torch.from_numpy().cuda", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "models.InferSent.enc_lstm", "torch.utils.rnn.pad_packed_sequence", "torch.utils.rnn.pad_packed_sequence", "models.InferSent.is_cuda", "torch.from_numpy().cuda", "torch.from_numpy().cuda", "torch.from_numpy().cuda", "torch.from_numpy().cuda", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.FloatTensor().unsqueeze().cuda", "torch.FloatTensor().unsqueeze().cuda", "torch.FloatTensor().unsqueeze().cuda", "torch.FloatTensor().unsqueeze().cuda", "torch.sum().squeeze", "torch.sum().squeeze", "torch.sum().squeeze", "torch.sum().squeeze", "numpy.sort", "torch.FloatTensor().unsqueeze().cuda.expand_as", "torch.FloatTensor().unsqueeze().cuda.expand_as", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.max", "torch.max", "torch.max", "torch.max", "emb.squeeze.squeeze.ndimension", "emb.squeeze.squeeze.squeeze", "emb.squeeze.squeeze.ndimension", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor().unsqueeze().cuda.copy", "torch.FloatTensor().unsqueeze().cuda.copy"], "methods", ["home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.InferSent.is_cuda", "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.InferSent.is_cuda"], ["", "def", "forward", "(", "self", ",", "sent_tuple", ")", ":", "\n", "# sent_len: [max_len, ..., min_len] (bsize)", "\n", "# sent: (seqlen x bsize x worddim)", "\n", "        ", "sent", ",", "sent_len", "=", "sent_tuple", "\n", "\n", "# Sort by length (keep idx)", "\n", "sent_len_sorted", ",", "idx_sort", "=", "np", ".", "sort", "(", "sent_len", ")", "[", ":", ":", "-", "1", "]", ",", "np", ".", "argsort", "(", "-", "sent_len", ")", "\n", "sent_len_sorted", "=", "sent_len_sorted", ".", "copy", "(", ")", "\n", "idx_unsort", "=", "np", ".", "argsort", "(", "idx_sort", ")", "\n", "\n", "idx_sort", "=", "torch", ".", "from_numpy", "(", "idx_sort", ")", ".", "cuda", "(", ")", "if", "self", ".", "is_cuda", "(", ")", "else", "torch", ".", "from_numpy", "(", "idx_sort", ")", "\n", "sent", "=", "sent", ".", "index_select", "(", "1", ",", "idx_sort", ")", "\n", "\n", "# Handling padding in Recurrent Networks", "\n", "sent_packed", "=", "nn", ".", "utils", ".", "rnn", ".", "pack_padded_sequence", "(", "sent", ",", "sent_len_sorted", ")", "\n", "sent_output", "=", "self", ".", "enc_lstm", "(", "sent_packed", ")", "[", "0", "]", "# seqlen x batch x 2*nhid", "\n", "sent_output", "=", "nn", ".", "utils", ".", "rnn", ".", "pad_packed_sequence", "(", "sent_output", ")", "[", "0", "]", "\n", "\n", "# Un-sort by length", "\n", "idx_unsort", "=", "torch", ".", "from_numpy", "(", "idx_unsort", ")", ".", "cuda", "(", ")", "if", "self", ".", "is_cuda", "(", ")", "else", "torch", ".", "from_numpy", "(", "idx_unsort", ")", "\n", "sent_output", "=", "sent_output", ".", "index_select", "(", "1", ",", "idx_unsort", ")", "\n", "\n", "# Pooling", "\n", "if", "self", ".", "pool_type", "==", "\"mean\"", ":", "\n", "            ", "sent_len", "=", "torch", ".", "FloatTensor", "(", "sent_len", ".", "copy", "(", ")", ")", ".", "unsqueeze", "(", "1", ")", ".", "cuda", "(", ")", "\n", "emb", "=", "torch", ".", "sum", "(", "sent_output", ",", "0", ")", ".", "squeeze", "(", "0", ")", "\n", "emb", "=", "emb", "/", "sent_len", ".", "expand_as", "(", "emb", ")", "\n", "", "elif", "self", ".", "pool_type", "==", "\"max\"", ":", "\n", "            ", "if", "not", "self", ".", "max_pad", ":", "\n", "                ", "sent_output", "[", "sent_output", "==", "0", "]", "=", "-", "1e9", "\n", "", "emb", "=", "torch", ".", "max", "(", "sent_output", ",", "0", ")", "[", "0", "]", "\n", "if", "emb", ".", "ndimension", "(", ")", "==", "3", ":", "\n", "                ", "emb", "=", "emb", ".", "squeeze", "(", "0", ")", "\n", "assert", "emb", ".", "ndimension", "(", ")", "==", "2", "\n", "\n", "", "", "return", "emb", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.InferSent.set_w2v_path": [[77, 79], ["None"], "methods", ["None"], ["", "def", "set_w2v_path", "(", "self", ",", "w2v_path", ")", ":", "\n", "        ", "self", ".", "w2v_path", "=", "w2v_path", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.InferSent.get_word_dict": [[80, 91], ["s.split", "models.InferSent.tokenize"], "methods", ["home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.InferSent.tokenize"], ["", "def", "get_word_dict", "(", "self", ",", "sentences", ",", "tokenize", "=", "True", ")", ":", "\n", "# create vocab of words", "\n", "        ", "word_dict", "=", "{", "}", "\n", "sentences", "=", "[", "s", ".", "split", "(", ")", "if", "not", "tokenize", "else", "self", ".", "tokenize", "(", "s", ")", "for", "s", "in", "sentences", "]", "\n", "for", "sent", "in", "sentences", ":", "\n", "            ", "for", "word", "in", "sent", ":", "\n", "                ", "if", "word", "not", "in", "word_dict", ":", "\n", "                    ", "word_dict", "[", "word", "]", "=", "''", "\n", "", "", "", "word_dict", "[", "self", ".", "bos", "]", "=", "''", "\n", "word_dict", "[", "self", ".", "eos", "]", "=", "''", "\n", "return", "word_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.InferSent.get_w2v": [[92, 103], ["hasattr", "print", "open", "line.split", "numpy.fromstring", "len", "len"], "methods", ["None"], ["", "def", "get_w2v", "(", "self", ",", "word_dict", ")", ":", "\n", "        ", "assert", "hasattr", "(", "self", ",", "'w2v_path'", ")", ",", "'w2v path not set'", "\n", "# create word_vec with w2v vectors", "\n", "word_vec", "=", "{", "}", "\n", "with", "open", "(", "self", ".", "w2v_path", ")", "as", "f", ":", "\n", "            ", "for", "line", "in", "f", ":", "\n", "                ", "word", ",", "vec", "=", "line", ".", "split", "(", "' '", ",", "1", ")", "\n", "if", "word", "in", "word_dict", ":", "\n", "                    ", "word_vec", "[", "word", "]", "=", "np", ".", "fromstring", "(", "vec", ",", "sep", "=", "' '", ")", "\n", "", "", "", "print", "(", "'Found %s(/%s) words with w2v vectors'", "%", "(", "len", "(", "word_vec", ")", ",", "len", "(", "word_dict", ")", ")", ")", "\n", "return", "word_vec", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.InferSent.get_w2v_k": [[104, 122], ["hasattr", "open", "line.split", "numpy.fromstring", "all", "numpy.fromstring"], "methods", ["None"], ["", "def", "get_w2v_k", "(", "self", ",", "K", ")", ":", "\n", "        ", "assert", "hasattr", "(", "self", ",", "'w2v_path'", ")", ",", "'w2v path not set'", "\n", "# create word_vec with k first w2v vectors", "\n", "k", "=", "0", "\n", "word_vec", "=", "{", "}", "\n", "with", "open", "(", "self", ".", "w2v_path", ")", "as", "f", ":", "\n", "            ", "for", "line", "in", "f", ":", "\n", "                ", "word", ",", "vec", "=", "line", ".", "split", "(", "' '", ",", "1", ")", "\n", "if", "k", "<=", "K", ":", "\n", "                    ", "word_vec", "[", "word", "]", "=", "np", ".", "fromstring", "(", "vec", ",", "sep", "=", "' '", ")", "\n", "k", "+=", "1", "\n", "", "if", "k", ">", "K", ":", "\n", "                    ", "if", "word", "in", "[", "self", ".", "bos", ",", "self", ".", "eos", "]", ":", "\n", "                        ", "word_vec", "[", "word", "]", "=", "np", ".", "fromstring", "(", "vec", ",", "sep", "=", "' '", ")", "\n", "\n", "", "", "if", "k", ">", "K", "and", "all", "(", "[", "w", "in", "word_vec", "for", "w", "in", "[", "self", ".", "bos", ",", "self", ".", "eos", "]", "]", ")", ":", "\n", "                    ", "break", "\n", "", "", "", "return", "word_vec", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.InferSent.build_vocab": [[123, 128], ["hasattr", "models.InferSent.get_word_dict", "models.InferSent.get_w2v", "print", "len"], "methods", ["home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.data.get_word_dict", "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.InferSent.get_w2v"], ["", "def", "build_vocab", "(", "self", ",", "sentences", ",", "tokenize", "=", "True", ")", ":", "\n", "        ", "assert", "hasattr", "(", "self", ",", "'w2v_path'", ")", ",", "'w2v path not set'", "\n", "word_dict", "=", "self", ".", "get_word_dict", "(", "sentences", ",", "tokenize", ")", "\n", "self", ".", "word_vec", "=", "self", ".", "get_w2v", "(", "word_dict", ")", "\n", "print", "(", "'Vocab size : %s'", "%", "(", "len", "(", "self", ".", "word_vec", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.InferSent.build_vocab_k_words": [[130, 134], ["hasattr", "models.InferSent.get_w2v_k", "print"], "methods", ["home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.InferSent.get_w2v_k"], ["", "def", "build_vocab_k_words", "(", "self", ",", "K", ")", ":", "\n", "        ", "assert", "hasattr", "(", "self", ",", "'w2v_path'", ")", ",", "'w2v path not set'", "\n", "self", ".", "word_vec", "=", "self", ".", "get_w2v_k", "(", "K", ")", "\n", "print", "(", "'Vocab size : %s'", "%", "(", "K", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.InferSent.update_vocab": [[135, 152], ["hasattr", "hasattr", "models.InferSent.get_word_dict", "print", "models.InferSent.get_w2v", "models.InferSent.word_vec.update", "len", "len"], "methods", ["home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.data.get_word_dict", "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.InferSent.get_w2v"], ["", "def", "update_vocab", "(", "self", ",", "sentences", ",", "tokenize", "=", "True", ")", ":", "\n", "        ", "assert", "hasattr", "(", "self", ",", "'w2v_path'", ")", ",", "'warning : w2v path not set'", "\n", "assert", "hasattr", "(", "self", ",", "'word_vec'", ")", ",", "'build_vocab before updating it'", "\n", "word_dict", "=", "self", ".", "get_word_dict", "(", "sentences", ",", "tokenize", ")", "\n", "\n", "# keep only new words", "\n", "for", "word", "in", "self", ".", "word_vec", ":", "\n", "            ", "if", "word", "in", "word_dict", ":", "\n", "                ", "del", "word_dict", "[", "word", "]", "\n", "\n", "# udpate vocabulary", "\n", "", "", "if", "word_dict", ":", "\n", "            ", "new_word_vec", "=", "self", ".", "get_w2v", "(", "word_dict", ")", "\n", "self", ".", "word_vec", ".", "update", "(", "new_word_vec", ")", "\n", "", "else", ":", "\n", "            ", "new_word_vec", "=", "[", "]", "\n", "", "print", "(", "'New vocab size : %s (added %s words)'", "%", "(", "len", "(", "self", ".", "word_vec", ")", ",", "len", "(", "new_word_vec", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.InferSent.get_batch": [[153, 163], ["numpy.zeros", "range", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "len", "range", "len", "len", "len"], "methods", ["None"], ["", "def", "get_batch", "(", "self", ",", "batch", ")", ":", "\n", "# sent in batch in decreasing order of lengths", "\n", "# batch: (bsize, max_len, word_dim)", "\n", "        ", "embed", "=", "np", ".", "zeros", "(", "(", "len", "(", "batch", "[", "0", "]", ")", ",", "len", "(", "batch", ")", ",", "self", ".", "word_emb_dim", ")", ")", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "batch", ")", ")", ":", "\n", "            ", "for", "j", "in", "range", "(", "len", "(", "batch", "[", "i", "]", ")", ")", ":", "\n", "                ", "embed", "[", "j", ",", "i", ",", ":", "]", "=", "self", ".", "word_vec", "[", "batch", "[", "i", "]", "[", "j", "]", "]", "\n", "\n", "", "", "return", "torch", ".", "FloatTensor", "(", "embed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.InferSent.tokenize": [[164, 172], ["s.replace.replace.replace", "s.replace.replace.split", "word_tokenize", "word_tokenize"], "methods", ["None"], ["", "def", "tokenize", "(", "self", ",", "s", ")", ":", "\n", "        ", "from", "nltk", ".", "tokenize", "import", "word_tokenize", "\n", "if", "self", ".", "moses_tok", ":", "\n", "            ", "s", "=", "' '", ".", "join", "(", "word_tokenize", "(", "s", ")", ")", "\n", "s", "=", "s", ".", "replace", "(", "\" n't \"", ",", "\"n 't \"", ")", "# HACK to get ~MOSES tokenization", "\n", "return", "s", ".", "split", "(", ")", "\n", "", "else", ":", "\n", "            ", "return", "word_tokenize", "(", "s", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.InferSent.prepare_samples": [[173, 199], ["numpy.sum", "range", "numpy.array", "numpy.sum", "len", "print", "numpy.argsort", "numpy.array", "len", "warnings.warn", "len", "numpy.sort", "s.split", "models.InferSent.tokenize"], "methods", ["home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.InferSent.tokenize"], ["", "", "def", "prepare_samples", "(", "self", ",", "sentences", ",", "bsize", ",", "tokenize", ",", "verbose", ")", ":", "\n", "        ", "sentences", "=", "[", "[", "self", ".", "bos", "]", "+", "s", ".", "split", "(", ")", "+", "[", "self", ".", "eos", "]", "if", "not", "tokenize", "else", "\n", "[", "self", ".", "bos", "]", "+", "self", ".", "tokenize", "(", "s", ")", "+", "[", "self", ".", "eos", "]", "for", "s", "in", "sentences", "]", "\n", "n_w", "=", "np", ".", "sum", "(", "[", "len", "(", "x", ")", "for", "x", "in", "sentences", "]", ")", "\n", "\n", "# filters words without w2v vectors", "\n", "for", "i", "in", "range", "(", "len", "(", "sentences", ")", ")", ":", "\n", "            ", "s_f", "=", "[", "word", "for", "word", "in", "sentences", "[", "i", "]", "if", "word", "in", "self", ".", "word_vec", "]", "\n", "if", "not", "s_f", ":", "\n", "                ", "import", "warnings", "\n", "warnings", ".", "warn", "(", "'No words in \"%s\" (idx=%s) have w2v vectors. \\\n                               Replacing by \"</s>\"..'", "%", "(", "sentences", "[", "i", "]", ",", "i", ")", ")", "\n", "s_f", "=", "[", "self", ".", "eos", "]", "\n", "", "sentences", "[", "i", "]", "=", "s_f", "\n", "\n", "", "lengths", "=", "np", ".", "array", "(", "[", "len", "(", "s", ")", "for", "s", "in", "sentences", "]", ")", "\n", "n_wk", "=", "np", ".", "sum", "(", "lengths", ")", "\n", "if", "verbose", ":", "\n", "            ", "print", "(", "'Nb words kept : %s/%s (%.1f%s)'", "%", "(", "\n", "n_wk", ",", "n_w", ",", "100.0", "*", "n_wk", "/", "n_w", ",", "'%'", ")", ")", "\n", "\n", "# sort by decreasing length", "\n", "", "lengths", ",", "idx_sort", "=", "np", ".", "sort", "(", "lengths", ")", "[", ":", ":", "-", "1", "]", ",", "np", ".", "argsort", "(", "-", "lengths", ")", "\n", "sentences", "=", "np", ".", "array", "(", "sentences", ")", "[", "idx_sort", "]", "\n", "\n", "return", "sentences", ",", "lengths", ",", "idx_sort", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.InferSent.encode": [[200, 224], ["time.time", "models.InferSent.prepare_samples", "range", "numpy.vstack", "numpy.argsort", "len", "models.InferSent.get_batch", "models.InferSent.is_cuda", "numpy.vstack.append", "print", "models.InferSent.cuda", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "models.InferSent.forward().data.cpu().numpy", "models.InferSent.forward().data.cpu", "len", "models.InferSent.is_cuda", "time.time", "models.InferSent.forward"], "methods", ["home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.InferSent.prepare_samples", "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.data.get_batch", "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.InferSent.is_cuda", "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.InferSent.is_cuda", "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.ClassificationNet.forward"], ["", "def", "encode", "(", "self", ",", "sentences", ",", "bsize", "=", "64", ",", "tokenize", "=", "True", ",", "verbose", "=", "False", ")", ":", "\n", "        ", "tic", "=", "time", ".", "time", "(", ")", "\n", "sentences", ",", "lengths", ",", "idx_sort", "=", "self", ".", "prepare_samples", "(", "\n", "sentences", ",", "bsize", ",", "tokenize", ",", "verbose", ")", "\n", "\n", "embeddings", "=", "[", "]", "\n", "for", "stidx", "in", "range", "(", "0", ",", "len", "(", "sentences", ")", ",", "bsize", ")", ":", "\n", "            ", "batch", "=", "self", ".", "get_batch", "(", "sentences", "[", "stidx", ":", "stidx", "+", "bsize", "]", ")", "\n", "if", "self", ".", "is_cuda", "(", ")", ":", "\n", "                ", "batch", "=", "batch", ".", "cuda", "(", ")", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "batch", "=", "self", ".", "forward", "(", "(", "batch", ",", "lengths", "[", "stidx", ":", "stidx", "+", "bsize", "]", ")", ")", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "", "embeddings", ".", "append", "(", "batch", ")", "\n", "", "embeddings", "=", "np", ".", "vstack", "(", "embeddings", ")", "\n", "\n", "# unsort", "\n", "idx_unsort", "=", "np", ".", "argsort", "(", "idx_sort", ")", "\n", "embeddings", "=", "embeddings", "[", "idx_unsort", "]", "\n", "\n", "if", "verbose", ":", "\n", "            ", "print", "(", "'Speed : %.1f sentences/s (%s mode, bsize=%s)'", "%", "(", "\n", "len", "(", "embeddings", ")", "/", "(", "time", ".", "time", "(", ")", "-", "tic", ")", ",", "\n", "'gpu'", "if", "self", ".", "is_cuda", "(", ")", "else", "'cpu'", ",", "bsize", ")", ")", "\n", "", "return", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.InferSent.visualize": [[225, 255], ["models.InferSent.get_batch", "models.InferSent.is_cuda", "torch.max", "torch.max", "torch.max", "torch.max", "idxs.data.cpu().numpy.data.cpu().numpy.data.cpu().numpy", "range", "plt.xticks", "plt.bar", "plt.ylabel", "plt.title", "plt.show", "sent.split", "models.InferSent.tokenize", "warnings.warn", "batch.cuda.cuda.cuda", "models.InferSent.enc_lstm", "numpy.sum", "len", "idxs.data.cpu().numpy.data.cpu().numpy.data.cpu", "range", "numpy.sum", "len"], "methods", ["home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.data.get_batch", "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.InferSent.is_cuda", "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.InferSent.tokenize"], ["", "def", "visualize", "(", "self", ",", "sent", ",", "tokenize", "=", "True", ")", ":", "\n", "\n", "        ", "sent", "=", "sent", ".", "split", "(", ")", "if", "not", "tokenize", "else", "self", ".", "tokenize", "(", "sent", ")", "\n", "sent", "=", "[", "[", "self", ".", "bos", "]", "+", "[", "word", "for", "word", "in", "sent", "if", "word", "in", "self", ".", "word_vec", "]", "+", "[", "self", ".", "eos", "]", "]", "\n", "\n", "if", "' '", ".", "join", "(", "sent", "[", "0", "]", ")", "==", "'%s %s'", "%", "(", "self", ".", "bos", ",", "self", ".", "eos", ")", ":", "\n", "            ", "import", "warnings", "\n", "warnings", ".", "warn", "(", "'No words in \"%s\" have w2v vectors. Replacing \\\n                           by \"%s %s\"..'", "%", "(", "sent", ",", "self", ".", "bos", ",", "self", ".", "eos", ")", ")", "\n", "", "batch", "=", "self", ".", "get_batch", "(", "sent", ")", "\n", "\n", "if", "self", ".", "is_cuda", "(", ")", ":", "\n", "            ", "batch", "=", "batch", ".", "cuda", "(", ")", "\n", "", "output", "=", "self", ".", "enc_lstm", "(", "batch", ")", "[", "0", "]", "\n", "output", ",", "idxs", "=", "torch", ".", "max", "(", "output", ",", "0", ")", "\n", "# output, idxs = output.squeeze(), idxs.squeeze()", "\n", "idxs", "=", "idxs", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "argmaxs", "=", "[", "np", ".", "sum", "(", "(", "idxs", "==", "k", ")", ")", "for", "k", "in", "range", "(", "len", "(", "sent", "[", "0", "]", ")", ")", "]", "\n", "\n", "# visualize model", "\n", "import", "matplotlib", ".", "pyplot", "as", "plt", "\n", "x", "=", "range", "(", "len", "(", "sent", "[", "0", "]", ")", ")", "\n", "y", "=", "[", "100.0", "*", "n", "/", "np", ".", "sum", "(", "argmaxs", ")", "for", "n", "in", "argmaxs", "]", "\n", "plt", ".", "xticks", "(", "x", ",", "sent", "[", "0", "]", ",", "rotation", "=", "45", ")", "\n", "plt", ".", "bar", "(", "x", ",", "y", ")", "\n", "plt", ".", "ylabel", "(", "'%'", ")", "\n", "plt", ".", "title", "(", "'Visualisation of words importance'", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n", "return", "output", ",", "idxs", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.NLINet.__init__": [[262, 295], ["torch.Module.__init__", "eval", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Tanh", "torch.Tanh", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Tanh", "torch.Tanh", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.ClassificationNet.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "NLINet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# classifier", "\n", "self", ".", "nonlinear_fc", "=", "config", "[", "'nonlinear_fc'", "]", "\n", "self", ".", "fc_dim", "=", "config", "[", "'fc_dim'", "]", "\n", "self", ".", "n_classes", "=", "config", "[", "'n_classes'", "]", "\n", "self", ".", "enc_lstm_dim", "=", "config", "[", "'enc_lstm_dim'", "]", "\n", "self", ".", "encoder_type", "=", "config", "[", "'encoder_type'", "]", "\n", "self", ".", "dpout_fc", "=", "config", "[", "'dpout_fc'", "]", "\n", "\n", "self", ".", "encoder", "=", "eval", "(", "self", ".", "encoder_type", ")", "(", "config", ")", "\n", "self", ".", "inputdim", "=", "4", "*", "2", "*", "self", ".", "enc_lstm_dim", "\n", "self", ".", "inputdim", "=", "4", "*", "self", ".", "inputdim", "if", "self", ".", "encoder_type", "in", "[", "\"ConvNetEncoder\"", ",", "\"InnerAttentionMILAEncoder\"", "]", "else", "self", ".", "inputdim", "\n", "self", ".", "inputdim", "=", "self", ".", "inputdim", "/", "2", "if", "self", ".", "encoder_type", "==", "\"LSTMEncoder\"", "else", "self", ".", "inputdim", "\n", "if", "self", ".", "nonlinear_fc", ":", "\n", "            ", "self", ".", "classifier", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Dropout", "(", "p", "=", "self", ".", "dpout_fc", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "inputdim", ",", "self", ".", "fc_dim", ")", ",", "\n", "nn", ".", "Tanh", "(", ")", ",", "\n", "nn", ".", "Dropout", "(", "p", "=", "self", ".", "dpout_fc", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "fc_dim", ",", "self", ".", "fc_dim", ")", ",", "\n", "nn", ".", "Tanh", "(", ")", ",", "\n", "nn", ".", "Dropout", "(", "p", "=", "self", ".", "dpout_fc", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "fc_dim", ",", "self", ".", "n_classes", ")", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "classifier", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "self", ".", "inputdim", ",", "self", ".", "fc_dim", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "fc_dim", ",", "self", ".", "fc_dim", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "fc_dim", ",", "self", ".", "n_classes", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.NLINet.forward": [[297, 305], ["models.NLINet.encoder", "models.NLINet.encoder", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "models.NLINet.classifier", "torch.abs", "torch.abs", "torch.abs", "torch.abs"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "s1", ",", "s2", ")", ":", "\n", "# s1 : (s1, s1_len)", "\n", "        ", "u", "=", "self", ".", "encoder", "(", "s1", ")", "\n", "v", "=", "self", ".", "encoder", "(", "s2", ")", "\n", "\n", "features", "=", "torch", ".", "cat", "(", "(", "u", ",", "v", ",", "torch", ".", "abs", "(", "u", "-", "v", ")", ",", "u", "*", "v", ")", ",", "1", ")", "\n", "output", "=", "self", ".", "classifier", "(", "features", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.NLINet.encode": [[306, 309], ["models.NLINet.encoder"], "methods", ["None"], ["", "def", "encode", "(", "self", ",", "s1", ")", ":", "\n", "        ", "emb", "=", "self", ".", "encoder", "(", "s1", ")", "\n", "return", "emb", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.NLI_HYPOTHS_Net.__init__": [[315, 349], ["torch.Module.__init__", "eval", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Tanh", "torch.Tanh", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Tanh", "torch.Tanh", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.ClassificationNet.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "NLI_HYPOTHS_Net", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# classifier", "\n", "self", ".", "nonlinear_fc", "=", "config", "[", "'nonlinear_fc'", "]", "\n", "self", ".", "fc_dim", "=", "config", "[", "'fc_dim'", "]", "\n", "self", ".", "n_classes", "=", "config", "[", "'n_classes'", "]", "\n", "self", ".", "enc_lstm_dim", "=", "config", "[", "'enc_lstm_dim'", "]", "\n", "self", ".", "encoder_type", "=", "config", "[", "'encoder_type'", "]", "\n", "self", ".", "dpout_fc", "=", "config", "[", "'dpout_fc'", "]", "\n", "\n", "self", ".", "encoder", "=", "eval", "(", "self", ".", "encoder_type", ")", "(", "config", ")", "\n", "self", ".", "inputdim", "=", "2", "*", "self", ".", "enc_lstm_dim", "\n", "#self.inputdim = 4*2*self.enc_lstm_dim", "\n", "self", ".", "inputdim", "=", "4", "*", "self", ".", "inputdim", "if", "self", ".", "encoder_type", "in", "[", "\"ConvNetEncoder\"", ",", "\"InnerAttentionMILAEncoder\"", "]", "else", "self", ".", "inputdim", "\n", "self", ".", "inputdim", "=", "self", ".", "inputdim", "/", "2", "if", "self", ".", "encoder_type", "==", "\"LSTMEncoder\"", "else", "self", ".", "inputdim", "\n", "if", "self", ".", "nonlinear_fc", ":", "\n", "            ", "self", ".", "classifier", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Dropout", "(", "p", "=", "self", ".", "dpout_fc", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "inputdim", ",", "self", ".", "fc_dim", ")", ",", "\n", "nn", ".", "Tanh", "(", ")", ",", "\n", "nn", ".", "Dropout", "(", "p", "=", "self", ".", "dpout_fc", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "fc_dim", ",", "self", ".", "fc_dim", ")", ",", "\n", "nn", ".", "Tanh", "(", ")", ",", "\n", "nn", ".", "Dropout", "(", "p", "=", "self", ".", "dpout_fc", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "fc_dim", ",", "self", ".", "n_classes", ")", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "classifier", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "self", ".", "inputdim", ",", "self", ".", "fc_dim", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "fc_dim", ",", "self", ".", "fc_dim", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "fc_dim", ",", "self", ".", "n_classes", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.NLI_HYPOTHS_Net.forward": [[351, 358], ["models.NLI_HYPOTHS_Net.encoder", "models.NLI_HYPOTHS_Net.classifier"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "s2", ")", ":", "\n", "# s1 : (s1, s1_len)", "\n", "        ", "v", "=", "self", ".", "encoder", "(", "s2", ")", "\n", "\n", "features", "=", "v", "\n", "output", "=", "self", ".", "classifier", "(", "features", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.NLI_HYPOTHS_Net.encode": [[359, 362], ["models.NLI_HYPOTHS_Net.encoder"], "methods", ["None"], ["", "def", "encode", "(", "self", ",", "s1", ")", ":", "\n", "        ", "emb", "=", "self", ".", "encoder", "(", "s1", ")", "\n", "return", "emb", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.GradReverse.forward": [[368, 372], ["x.view_as"], "methods", ["None"], ["    ", "@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "x", ",", "lambd", ")", ":", "\n", "        ", "ctx", ".", "lambd", "=", "lambd", "\n", "return", "x", ".", "view_as", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.GradReverse.backward": [[373, 376], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "grad_output", ")", ":", "\n", "        ", "return", "(", "grad_output", "*", "-", "ctx", ".", "lambd", ")", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.SharedHypothNet.__init__": [[386, 422], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Tanh", "torch.Tanh", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Tanh", "torch.Tanh", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.ClassificationNet.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "encoder", ")", ":", "\n", "        ", "super", "(", "SharedHypothNet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# classifier", "\n", "self", ".", "nonlinear_fc", "=", "config", "[", "'nonlinear_fc'", "]", "\n", "self", ".", "fc_dim", "=", "config", "[", "'fc_dim'", "]", "\n", "self", ".", "n_classes", "=", "config", "[", "'n_classes'", "]", "\n", "self", ".", "enc_lstm_dim", "=", "config", "[", "'enc_lstm_dim'", "]", "\n", "self", ".", "encoder_type", "=", "config", "[", "'encoder_type'", "]", "\n", "self", ".", "dpout_fc", "=", "config", "[", "'dpout_fc'", "]", "\n", "self", ".", "adv_hyp_encoder_lambda", "=", "config", "[", "'adv_hyp_encoder_lambda'", "]", "\n", "\n", "#self.encoder = eval(self.encoder_type)(config)", "\n", "self", ".", "encoder", "=", "encoder", "\n", "self", ".", "inputdim", "=", "2", "*", "self", ".", "enc_lstm_dim", "\n", "#self.inputdim = 4*2*self.enc_lstm_dim", "\n", "self", ".", "inputdim", "=", "4", "*", "self", ".", "inputdim", "if", "self", ".", "encoder_type", "in", "[", "\"ConvNetEncoder\"", ",", "\"InnerAttentionMILAEncoder\"", "]", "else", "self", ".", "inputdim", "\n", "self", ".", "inputdim", "=", "self", ".", "inputdim", "/", "2", "if", "self", ".", "encoder_type", "==", "\"LSTMEncoder\"", "else", "self", ".", "inputdim", "\n", "if", "self", ".", "nonlinear_fc", ":", "\n", "            ", "self", ".", "classifier", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Dropout", "(", "p", "=", "self", ".", "dpout_fc", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "inputdim", ",", "self", ".", "fc_dim", ")", ",", "\n", "nn", ".", "Tanh", "(", ")", ",", "\n", "nn", ".", "Dropout", "(", "p", "=", "self", ".", "dpout_fc", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "fc_dim", ",", "self", ".", "fc_dim", ")", ",", "\n", "nn", ".", "Tanh", "(", ")", ",", "\n", "nn", ".", "Dropout", "(", "p", "=", "self", ".", "dpout_fc", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "fc_dim", ",", "self", ".", "n_classes", ")", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "classifier", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "self", ".", "inputdim", ",", "self", ".", "fc_dim", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "fc_dim", ",", "self", ".", "fc_dim", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "fc_dim", ",", "self", ".", "n_classes", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.SharedHypothNet.forward": [[424, 433], ["models.SharedHypothNet.encoder", "models.grad_reverse", "models.SharedHypothNet.classifier"], "methods", ["home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.grad_reverse"], ["", "", "def", "forward", "(", "self", ",", "s2", ")", ":", "\n", "# s1 : (s1, s1_len)", "\n", "        ", "v", "=", "self", ".", "encoder", "(", "s2", ")", "\n", "# reverse gradients going into the encoder", "\n", "v", "=", "grad_reverse", "(", "v", ",", "self", ".", "adv_hyp_encoder_lambda", ")", "\n", "\n", "features", "=", "v", "\n", "output", "=", "self", ".", "classifier", "(", "features", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.SharedHypothNet.encode": [[434, 437], ["models.SharedHypothNet.encoder"], "methods", ["None"], ["", "def", "encode", "(", "self", ",", "s1", ")", ":", "\n", "        ", "emb", "=", "self", ".", "encoder", "(", "s1", ")", "\n", "return", "emb", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.SharedNLINet.__init__": [[440, 476], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Tanh", "torch.Tanh", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Tanh", "torch.Tanh", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.ClassificationNet.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "encoder_premise", ",", "encoder_hypoth", ")", ":", "\n", "# Assumes encoder_premise and encoder_hypoth are of same config['encoder_type']", "\n", "        ", "super", "(", "SharedNLINet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# classifier", "\n", "self", ".", "nonlinear_fc", "=", "config", "[", "'nonlinear_fc'", "]", "\n", "self", ".", "fc_dim", "=", "config", "[", "'fc_dim'", "]", "\n", "self", ".", "n_classes", "=", "config", "[", "'n_classes'", "]", "\n", "self", ".", "enc_lstm_dim", "=", "config", "[", "'enc_lstm_dim'", "]", "\n", "self", ".", "encoder_type", "=", "config", "[", "'encoder_type'", "]", "\n", "self", ".", "dpout_fc", "=", "config", "[", "'dpout_fc'", "]", "\n", "self", ".", "adv_hyp_encoder_lambda", "=", "config", "[", "'nli_net_adv_hyp_encoder_lambda'", "]", "\n", "\n", "self", ".", "encoder_premise", "=", "encoder_premise", "\n", "self", ".", "encoder_hypoth", "=", "encoder_hypoth", "\n", "self", ".", "inputdim", "=", "4", "*", "2", "*", "self", ".", "enc_lstm_dim", "\n", "self", ".", "inputdim", "=", "4", "*", "self", ".", "inputdim", "if", "self", ".", "encoder_type", "in", "[", "\"ConvNetEncoder\"", ",", "\"InnerAttentionMILAEncoder\"", "]", "else", "self", ".", "inputdim", "\n", "self", ".", "inputdim", "=", "self", ".", "inputdim", "/", "2", "if", "self", ".", "encoder_type", "==", "\"LSTMEncoder\"", "else", "self", ".", "inputdim", "\n", "if", "self", ".", "nonlinear_fc", ":", "\n", "            ", "self", ".", "classifier", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Dropout", "(", "p", "=", "self", ".", "dpout_fc", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "inputdim", ",", "self", ".", "fc_dim", ")", ",", "\n", "nn", ".", "Tanh", "(", ")", ",", "\n", "nn", ".", "Dropout", "(", "p", "=", "self", ".", "dpout_fc", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "fc_dim", ",", "self", ".", "fc_dim", ")", ",", "\n", "nn", ".", "Tanh", "(", ")", ",", "\n", "nn", ".", "Dropout", "(", "p", "=", "self", ".", "dpout_fc", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "fc_dim", ",", "self", ".", "n_classes", ")", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "classifier", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "self", ".", "inputdim", ",", "self", ".", "fc_dim", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "fc_dim", ",", "self", ".", "fc_dim", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "fc_dim", ",", "self", ".", "n_classes", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.SharedNLINet.forward": [[478, 491], ["models.SharedNLINet.encoder_premise", "models.SharedNLINet.encoder_hypoth", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "models.SharedNLINet.classifier", "models.grad_reverse", "models.grad_reverse", "torch.abs", "torch.abs", "torch.abs", "torch.abs"], "methods", ["home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.grad_reverse", "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.grad_reverse"], ["", "", "def", "forward", "(", "self", ",", "s1", ",", "s2", ",", "random_premise", "=", "False", ")", ":", "\n", "# s1 : (s1, s1_len)", "\n", "        ", "u", "=", "self", ".", "encoder_premise", "(", "s1", ")", "\n", "v", "=", "self", ".", "encoder_hypoth", "(", "s2", ")", "\n", "if", "random_premise", ":", "\n", "# block gradients from back-propagating to the premise encoder ", "\n", "            ", "u", "=", "grad_reverse", "(", "u", ",", "0.0", ")", "\n", "# reverse gradients when back-propagating to the hypothesis encoder", "\n", "v", "=", "grad_reverse", "(", "v", ",", "self", ".", "adv_hyp_encoder_lambda", ")", "\n", "\n", "", "features", "=", "torch", ".", "cat", "(", "(", "u", ",", "v", ",", "torch", ".", "abs", "(", "u", "-", "v", ")", ",", "u", "*", "v", ")", ",", "1", ")", "\n", "output", "=", "self", ".", "classifier", "(", "features", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.SharedNLINet.encode": [[492, 495], ["models.SharedNLINet.encoder_premise"], "methods", ["None"], ["", "def", "encode", "(", "self", ",", "s1", ")", ":", "\n", "        ", "emb", "=", "self", ".", "encoder_premise", "(", "s1", ")", "\n", "return", "emb", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.ClassificationNet.__init__": [[500, 518], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "eval", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.ClassificationNet.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "ClassificationNet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# classifier", "\n", "self", ".", "nonlinear_fc", "=", "config", "[", "'nonlinear_fc'", "]", "\n", "self", ".", "fc_dim", "=", "config", "[", "'fc_dim'", "]", "\n", "self", ".", "n_classes", "=", "config", "[", "'n_classes'", "]", "\n", "self", ".", "enc_lstm_dim", "=", "config", "[", "'enc_lstm_dim'", "]", "\n", "self", ".", "encoder_type", "=", "config", "[", "'encoder_type'", "]", "\n", "self", ".", "dpout_fc", "=", "config", "[", "'dpout_fc'", "]", "\n", "\n", "self", ".", "encoder", "=", "eval", "(", "self", ".", "encoder_type", ")", "(", "config", ")", "\n", "self", ".", "inputdim", "=", "2", "*", "self", ".", "enc_lstm_dim", "\n", "self", ".", "inputdim", "=", "4", "*", "self", ".", "inputdim", "if", "self", ".", "encoder_type", "==", "\"ConvNetEncoder\"", "else", "self", ".", "inputdim", "\n", "self", ".", "inputdim", "=", "self", ".", "enc_lstm_dim", "if", "self", ".", "encoder_type", "==", "\"LSTMEncoder\"", "else", "self", ".", "inputdim", "\n", "self", ".", "classifier", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "self", ".", "inputdim", ",", "512", ")", ",", "\n", "nn", ".", "Linear", "(", "512", ",", "self", ".", "n_classes", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.ClassificationNet.forward": [[520, 526], ["models.ClassificationNet.encoder", "models.ClassificationNet.classifier"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "s1", ")", ":", "\n", "# s1 : (s1, s1_len)", "\n", "        ", "u", "=", "self", ".", "encoder", "(", "s1", ")", "\n", "\n", "output", "=", "self", ".", "classifier", "(", "u", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.ClassificationNet.encode": [[527, 530], ["models.ClassificationNet.encoder"], "methods", ["None"], ["", "def", "encode", "(", "self", ",", "s1", ")", ":", "\n", "        ", "emb", "=", "self", ".", "encoder", "(", "s1", ")", "\n", "return", "emb", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.grad_reverse": [[377, 379], ["GradReverse.apply"], "function", ["None"], ["", "", "def", "grad_reverse", "(", "x", ",", "lambd", "=", "1.0", ")", ":", "\n", "    ", "return", "GradReverse", ".", "apply", "(", "x", ",", "lambd", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.eval.get_args": [[13, 57], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_known_args", "print", "print"], "function", ["None"], ["def", "get_args", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Training NLI model based on just hypothesis sentence'", ")", "\n", "\n", "# paths", "\n", "parser", ".", "add_argument", "(", "\"--outputfile\"", ",", "type", "=", "str", ",", "default", "=", "\"results.csv\"", ",", "help", "=", "\"writes the results on this file.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--word_emb_dim\"", ",", "type", "=", "int", ",", "default", "=", "300", ")", "\n", "parser", ".", "add_argument", "(", "\"--embdfile\"", ",", "type", "=", "str", ",", "default", "=", "'../data/embds/glove.840B.300d.txt'", ",", "\n", "help", "=", "\"File containin the word embeddings\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--outputdir\"", ",", "type", "=", "str", ",", "default", "=", "'savedir/'", ",", "help", "=", "\"Output directory\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--model\"", ",", "type", "=", "str", ",", "help", "=", "\"Input model that has already been trained\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--pred_file\"", ",", "type", "=", "str", ",", "default", "=", "'preds'", ",", "help", "=", "\"Suffix for the prediction files\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--test_path\"", ",", "type", "=", "str", ",", "required", "=", "True", ",", "help", "=", "\"The path of the test dataset.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--train_path\"", ",", "type", "=", "str", ",", "required", "=", "True", ",", "help", "=", "\"The path of the train dataset the model is trained on.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--test_data\"", ",", "type", "=", "str", ",", "required", "=", "True", ",", "help", "=", "\"The name of the test dataset\"", ",", "choices", "=", "[", "'SNLI'", ",", "\n", "'SNLIHard'", ",", "'MNLIMatched'", ",", "'MNLIMismatched'", ",", "'JOCI'", ",", "'SICK-E'", ",", "'AddOneRTE'", ",", "'DPR'", ",", "'FNPLUS'", ",", "'SciTail'", ",", "'SPR'", ",", "'MPE'", ",", "'QQP'", ",", "'GLUEDiagnostic'", "]", ")", "\n", "parser", ".", "add_argument", "(", "\"--train_data\"", ",", "type", "=", "str", ",", "required", "=", "True", ",", "help", "=", "\"The name of the train dataset\"", ",", "choices", "=", "[", "'SNLI'", ",", "\n", "'SNLIHard'", ",", "'MNLIMatched'", ",", "'MNLIMismatched'", ",", "'JOCI'", ",", "'SICK-E'", ",", "'AddOneRTE'", ",", "'DPR'", ",", "'FNPLUS'", ",", "'SciTail'", ",", "'SPR'", ",", "'MPE'", ",", "'QQP'", ",", "'GLUEDiagnostic'", "]", ")", "\n", "\n", "# data", "\n", "parser", ".", "add_argument", "(", "\"--max_train_sents\"", ",", "type", "=", "int", ",", "default", "=", "10000000", ",", "help", "=", "\"Maximum number of training examples\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_val_sents\"", ",", "type", "=", "int", ",", "default", "=", "10000000", ",", "help", "=", "\"Maximum number of validation/dev examples\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_test_sents\"", ",", "type", "=", "int", ",", "default", "=", "10000000", ",", "help", "=", "\"Maximum number of test examples\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--lorelei_embds\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Whether to use multilingual embeddings released for LORELEI. This requires cleaning up words since wordsare are prefixed with the language. 0 for no, 1 for yes (Default is 0)\"", ")", "\n", "\n", "# model", "\n", "parser", ".", "add_argument", "(", "\"--batch_size\"", ",", "type", "=", "int", ",", "default", "=", "64", ")", "\n", "\n", "# gpu", "\n", "parser", ".", "add_argument", "(", "\"--gpu_id\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "help", "=", "\"GPU ID\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--seed\"", ",", "type", "=", "int", ",", "default", "=", "1234", ",", "help", "=", "\"seed\"", ")", "\n", "\n", "# misc", "\n", "parser", ".", "add_argument", "(", "\"--verbose\"", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "\"Verbose output\"", ")", "\n", "\n", "params", ",", "_", "=", "parser", ".", "parse_known_args", "(", ")", "\n", "\n", "# print parameters passed, and all parameters", "\n", "print", "(", "'\\ntogrep : {0}\\n'", ".", "format", "(", "sys", ".", "argv", "[", "1", ":", "]", ")", ")", "\n", "print", "(", "params", ")", "\n", "\n", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.eval.compute_score_with_logits": [[59, 65], ["pred.long().eq().cpu().sum", "logits.data.max", "pred.long().eq().cpu", "pred.long().eq", "labels.data.long", "pred.long"], "function", ["None"], ["", "def", "compute_score_with_logits", "(", "logits", ",", "labels", ",", "n_classes", ")", ":", "\n", "    ", "pred", "=", "logits", ".", "data", ".", "max", "(", "1", ")", "[", "1", "]", "\n", "if", "n_classes", "==", "2", ":", "\n", "        ", "pred", "[", "pred", "==", "2", "]", "=", "1", "\n", "", "correct", "=", "pred", ".", "long", "(", ")", ".", "eq", "(", "labels", ".", "data", ".", "long", "(", ")", ")", ".", "cpu", "(", ")", ".", "sum", "(", ")", "\n", "return", "correct", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.eval.get_nli_split": [[67, 77], ["numpy.array", "line.rstrip", "line.rstrip", "open", "open", "os.path.join", "os.path.join", "open", "line.rstrip", "os.path.join"], "function", ["None"], ["", "def", "get_nli_split", "(", "path", ",", "n_classes", ",", "split", "=", "\"test\"", ")", ":", "\n", "    ", "data", "=", "{", "}", "\n", "if", "n_classes", "==", "3", ":", "\n", "        ", "dico_labels", "=", "{", "'entailment'", ":", "0", ",", "'neutral'", ":", "1", ",", "'contradiction'", ":", "2", ",", "'hidden'", ":", "3", "}", "\n", "", "else", ":", "\n", "        ", "dico_labels", "=", "{", "'entailment'", ":", "0", ",", "'neutral'", ":", "1", ",", "'contradiction'", ":", "1", ",", "'hidden'", ":", "3", "}", "\n", "", "data", "[", "'s1'", "]", "=", "[", "line", ".", "rstrip", "(", ")", "for", "line", "in", "open", "(", "join", "(", "path", ",", "'s1.'", "+", "split", ")", ",", "'r'", ")", "]", "\n", "data", "[", "'s2'", "]", "=", "[", "line", ".", "rstrip", "(", ")", "for", "line", "in", "open", "(", "join", "(", "path", ",", "'s2.'", "+", "split", ")", ",", "'r'", ")", "]", "\n", "data", "[", "'labels'", "]", "=", "np", ".", "array", "(", "[", "dico_labels", "[", "line", ".", "rstrip", "(", "'\\n'", ")", "]", "for", "line", "in", "open", "(", "join", "(", "path", ",", "'labels.'", "+", "split", ")", ",", "'r'", ")", "]", ")", "\n", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.eval.evaluate": [[79, 107], ["eval.get_nli_split", "range", "round", "print", "numpy.array", "len", "data.get_batch", "data.get_batch", "torch.autograd.Variable().cuda", "nli_net", "outputs.extend", "eval.compute_score_with_logits", "torch.autograd.Variable", "torch.autograd.Variable", "[].cpu().numpy", "len", "s1_batch.cuda", "s2_batch.cuda", "torch.autograd.Variable", "correct.item", "torch.LongTensor", "[].cpu", "sent.split", "nli_net.data.max"], "function", ["home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.eval.get_nli_split", "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.data.get_batch", "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.data.get_batch", "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.eval.compute_score_with_logits"], ["", "def", "evaluate", "(", "args", ",", "nli_net", ",", "test_nlipath", ",", "n_classes", ",", "word_vec", ",", "split", "=", "\"test\"", ")", ":", "\n", "    ", "test", "=", "get_nli_split", "(", "test_nlipath", ",", "n_classes", ",", "split", ")", "\n", "for", "split", "in", "[", "'s1'", ",", "'s2'", "]", ":", "\n", "        ", "test", "[", "split", "]", "=", "np", ".", "array", "(", "[", "[", "'<s>'", "]", "+", "\n", "[", "word", "for", "word", "in", "sent", ".", "split", "(", ")", "if", "word", "in", "word_vec", "]", "+", "\n", "[", "'</s>'", "]", "for", "sent", "in", "test", "[", "split", "]", "]", ")", "\n", "\n", "# Evaluates on the test set.", "\n", "", "correct", "=", "0.", "\n", "s1", "=", "test", "[", "'s1'", "]", "\n", "s2", "=", "test", "[", "'s2'", "]", "\n", "target", "=", "test", "[", "'labels'", "]", "\n", "outputs", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "s1", ")", ",", "args", ".", "batch_size", ")", ":", "\n", "# prepare batch", "\n", "        ", "s1_batch", ",", "s1_len", "=", "get_batch", "(", "s1", "[", "i", ":", "i", "+", "args", ".", "batch_size", "]", ",", "word_vec", ",", "args", ".", "word_emb_dim", ")", "\n", "\n", "s2_batch", ",", "s2_len", "=", "get_batch", "(", "s2", "[", "i", ":", "i", "+", "args", ".", "batch_size", "]", ",", "word_vec", ",", "args", ".", "word_emb_dim", ")", "\n", "\n", "s1_batch", ",", "s2_batch", "=", "Variable", "(", "s1_batch", ".", "cuda", "(", ")", ")", ",", "Variable", "(", "s2_batch", ".", "cuda", "(", ")", ")", "\n", "tgt_batch", "=", "Variable", "(", "torch", ".", "LongTensor", "(", "target", "[", "i", ":", "i", "+", "args", ".", "batch_size", "]", ")", ")", ".", "cuda", "(", ")", "\n", "output", "=", "nli_net", "(", "(", "s1_batch", ",", "s1_len", ")", ",", "(", "s2_batch", ",", "s2_len", ")", ")", "\n", "outputs", ".", "extend", "(", "output", ".", "data", ".", "max", "(", "1", ")", "[", "1", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "correct", "+=", "compute_score_with_logits", "(", "output", ",", "tgt_batch", ",", "n_classes", ")", "\n", "\n", "", "eval_acc", "=", "round", "(", "100", "*", "correct", ".", "item", "(", ")", "/", "len", "(", "s1", ")", ",", "2", ")", "\n", "print", "(", "'evaluation accuracy is {0}'", ".", "format", "(", "eval_acc", ")", ")", "\n", "return", "eval_acc", ",", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.eval.get_vocab": [[109, 127], ["data.build_vocab", "data.get_nli", "eval", "[].extend", "eval", "eval"], "function", ["home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.data.build_vocab", "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.data.get_nli"], ["", "def", "get_vocab", "(", "args", ")", ":", "\n", "# build a vocabulary from all train,dev,test set of the actual snli plus the test set of the", "\n", "# all the transfer tasks.", "\n", "    ", "train", ",", "valid", ",", "test", "=", "{", "}", ",", "{", "}", ",", "{", "}", "\n", "for", "split", "in", "[", "'test'", ",", "'valid'", ",", "'train'", "]", ":", "\n", "        ", "for", "s", "in", "[", "'s1'", ",", "'s2'", "]", ":", "\n", "            ", "eval", "(", "split", ")", "[", "s", "]", "=", "[", "]", "\n", "", "", "for", "datapath", ",", "n_classes", "in", "[", "(", "args", ".", "test_path", ",", "args", ".", "data_to_n_classes", "[", "args", ".", "test_data", "]", ")", ",", "\n", "(", "args", ".", "train_path", ",", "args", ".", "data_to_n_classes", "[", "args", ".", "train_data", "]", ")", "]", ":", "\n", "        ", "transfer_train", ",", "transfer_valid", ",", "transfer_test", "=", "get_nli", "(", "datapath", ",", "n_classes", ")", "\n", "for", "split", "in", "[", "'test'", ",", "'valid'", ",", "'train'", "]", ":", "\n", "            ", "for", "s", "in", "[", "'s1'", ",", "'s2'", "]", ":", "\n", "                ", "eval", "(", "split", ")", "[", "s", "]", ".", "extend", "(", "eval", "(", "\"transfer_\"", "+", "split", ")", "[", "s", "]", ")", "\n", "\n", "", "", "", "word_vec", "=", "build_vocab", "(", "train", "[", "'s1'", "]", "+", "train", "[", "'s2'", "]", "+", "\n", "valid", "[", "'s1'", "]", "+", "valid", "[", "'s2'", "]", "+", "\n", "test", "[", "'s1'", "]", "+", "test", "[", "'s2'", "]", ",", "args", ".", "embdfile", ")", "\n", "return", "word_vec", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.eval.main": [[129, 151], ["numpy.random.seed", "torch.manual_seed", "torch.cuda.manual_seed", "eval.get_vocab", "torch.load().eval().cuda", "mutils.write_to_csv", "eval.evaluate", "eval.evaluate", "torch.load().eval", "torch.load"], "function", ["home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.eval.get_vocab", "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.mutils.write_to_csv", "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.train.evaluate", "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.train.evaluate"], ["", "def", "main", "(", "args", ")", ":", "\n", "# sets seed.", "\n", "    ", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "cuda", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "\n", "args", ".", "data_to_n_classes", "=", "{", "\n", "'SNLI'", ":", "3", ",", "'SNLIHard'", ":", "3", ",", "'MNLIMatched'", ":", "3", ",", "'MNLIMismatched'", ":", "3", ",", "'JOCI'", ":", "3", ",", "'SICK-E'", ":", "3", ",", "'AddOneRTE'", ":", "2", ",", "'DPR'", ":", "2", ",", "'FNPLUS'", ":", "2", ",", "'SciTail'", ":", "2", ",", "'SPR'", ":", "2", ",", "'MPE'", ":", "3", ",", "'QQP'", ":", "2", ",", "'GLUEDiagnostic'", ":", "3", "\n", "}", "\n", "\n", "# builds vocabulary from the all datasets.", "\n", "word_vec", "=", "get_vocab", "(", "args", ")", "\n", "shared_nli_net", "=", "torch", ".", "load", "(", "args", ".", "model", ")", ".", "eval", "(", ")", ".", "cuda", "(", ")", "\n", "\n", "eval_accs", "=", "{", "}", "\n", "eval_accs", "[", "\"test\"", "]", "=", "evaluate", "(", "args", ",", "shared_nli_net", ",", "args", ".", "test_path", ",", "args", ".", "data_to_n_classes", "[", "args", ".", "test_data", "]", ",", "word_vec", ",", "split", "=", "\"test\"", ")", "[", "0", "]", "\n", "eval_accs", "[", "\"dev\"", "]", "=", "evaluate", "(", "args", ",", "shared_nli_net", ",", "args", ".", "test_path", ",", "args", ".", "data_to_n_classes", "[", "args", ".", "test_data", "]", ",", "word_vec", ",", "split", "=", "\"dev\"", ")", "[", "0", "]", "\n", "write_to_csv", "(", "eval_accs", ",", "args", ",", "args", ".", "outputfile", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.mutils.write_to_csv": [[15, 50], ["open", "vars", "open.close", "vars.keys", "enumerate", "open.write", "open", "csv.reader", "next", "open", "enumerate", "f.write", "os.stat", "open.write", "scores.keys", "open.write", "f.write", "f.write", "AssertionError", "len", "len", "str", "scores.keys", "str"], "function", ["None"], ["def", "write_to_csv", "(", "scores", ",", "params", ",", "outputfile", ")", ":", "\n", "    ", "\"\"\"\n    This function writes the parameters and the scores with their names in a\n    csv file.\n    \"\"\"", "\n", "# creates the file if not existing.", "\n", "file", "=", "open", "(", "outputfile", ",", "'a'", ")", "\n", "# If file is empty writes the keys to the file.", "\n", "params_dict", "=", "vars", "(", "params", ")", "\n", "if", "os", ".", "stat", "(", "outputfile", ")", ".", "st_size", "==", "0", ":", "\n", "# Writes the configuration parameters", "\n", "        ", "for", "key", "in", "params_dict", ".", "keys", "(", ")", ":", "\n", "            ", "file", ".", "write", "(", "key", "+", "\";\"", ")", "\n", "", "for", "i", ",", "key", "in", "enumerate", "(", "scores", ".", "keys", "(", ")", ")", ":", "\n", "            ", "ending", "=", "\";\"", "if", "i", "<", "len", "(", "scores", ".", "keys", "(", ")", ")", "-", "1", "else", "\"\"", "\n", "file", ".", "write", "(", "key", "+", "ending", ")", "\n", "", "file", ".", "write", "(", "\"\\n\"", ")", "\n", "", "file", ".", "close", "(", ")", "\n", "\n", "# Writes the values to each corresponding column.", "\n", "with", "open", "(", "outputfile", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "reader", "=", "csv", ".", "reader", "(", "f", ",", "delimiter", "=", "';'", ")", "\n", "headers", "=", "next", "(", "reader", ")", "\n", "\n", "# Iterates over the header names and write the corresponding values.", "\n", "", "with", "open", "(", "outputfile", ",", "'a'", ")", "as", "f", ":", "\n", "        ", "for", "i", ",", "key", "in", "enumerate", "(", "headers", ")", ":", "\n", "            ", "ending", "=", "\";\"", "if", "i", "<", "len", "(", "headers", ")", "-", "1", "else", "\"\"", "\n", "if", "key", "in", "params_dict", ":", "\n", "                ", "f", ".", "write", "(", "str", "(", "params_dict", "[", "key", "]", ")", "+", "ending", ")", "\n", "", "elif", "key", "in", "scores", ":", "\n", "                ", "f", ".", "write", "(", "str", "(", "scores", "[", "key", "]", ")", "+", "ending", ")", "\n", "", "else", ":", "\n", "                ", "raise", "AssertionError", "(", "\"Key not found in the given dictionary\"", ")", "\n", "", "", "f", ".", "write", "(", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.mutils.get_optimizer": [[52, 99], ["s[].split", "inspect.getargspec", "all", "Exception", "x.split", "float", "s.find", "len", "re.match", "optim_params.keys", "str", "str", "optim_params.keys", "s.find", "Exception"], "function", ["None"], ["", "", "def", "get_optimizer", "(", "s", ")", ":", "\n", "    ", "\"\"\"\n    Parse optimizer parameters.\n    Input should be of the form:\n        - \"sgd,lr=0.01\"\n        - \"adagrad,lr=0.1,lr_decay=0.05\"\n    \"\"\"", "\n", "if", "\",\"", "in", "s", ":", "\n", "        ", "method", "=", "s", "[", ":", "s", ".", "find", "(", "','", ")", "]", "\n", "optim_params", "=", "{", "}", "\n", "for", "x", "in", "s", "[", "s", ".", "find", "(", "','", ")", "+", "1", ":", "]", ".", "split", "(", "','", ")", ":", "\n", "            ", "split", "=", "x", ".", "split", "(", "'='", ")", "\n", "assert", "len", "(", "split", ")", "==", "2", "\n", "assert", "re", ".", "match", "(", "\"^[+-]?(\\d+(\\.\\d*)?|\\.\\d+)$\"", ",", "split", "[", "1", "]", ")", "is", "not", "None", "\n", "optim_params", "[", "split", "[", "0", "]", "]", "=", "float", "(", "split", "[", "1", "]", ")", "\n", "", "", "else", ":", "\n", "        ", "method", "=", "s", "\n", "optim_params", "=", "{", "}", "\n", "\n", "", "if", "method", "==", "'adadelta'", ":", "\n", "        ", "optim_fn", "=", "optim", ".", "Adadelta", "\n", "", "elif", "method", "==", "'adagrad'", ":", "\n", "        ", "optim_fn", "=", "optim", ".", "Adagrad", "\n", "", "elif", "method", "==", "'adam'", ":", "\n", "        ", "optim_fn", "=", "optim", ".", "Adam", "\n", "", "elif", "method", "==", "'adamax'", ":", "\n", "        ", "optim_fn", "=", "optim", ".", "Adamax", "\n", "", "elif", "method", "==", "'asgd'", ":", "\n", "        ", "optim_fn", "=", "optim", ".", "ASGD", "\n", "", "elif", "method", "==", "'rmsprop'", ":", "\n", "        ", "optim_fn", "=", "optim", ".", "RMSprop", "\n", "", "elif", "method", "==", "'rprop'", ":", "\n", "        ", "optim_fn", "=", "optim", ".", "Rprop", "\n", "", "elif", "method", "==", "'sgd'", ":", "\n", "        ", "optim_fn", "=", "optim", ".", "SGD", "\n", "assert", "'lr'", "in", "optim_params", "\n", "", "else", ":", "\n", "        ", "raise", "Exception", "(", "'Unknown optimization method: \"%s\"'", "%", "method", ")", "\n", "\n", "# check that we give good parameters to the optimizer", "\n", "", "expected_args", "=", "inspect", ".", "getargspec", "(", "optim_fn", ".", "__init__", ")", "[", "0", "]", "\n", "assert", "expected_args", "[", ":", "2", "]", "==", "[", "'self'", ",", "'params'", "]", "\n", "if", "not", "all", "(", "k", "in", "expected_args", "[", "2", ":", "]", "for", "k", "in", "optim_params", ".", "keys", "(", ")", ")", ":", "\n", "        ", "raise", "Exception", "(", "'Unexpected parameters: expected \"%s\", got \"%s\"'", "%", "(", "\n", "str", "(", "expected_args", "[", "2", ":", "]", ")", ",", "str", "(", "optim_params", ".", "keys", "(", ")", ")", ")", ")", "\n", "\n", "", "return", "optim_fn", ",", "optim_params", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.mutils.batcher": [[106, 114], ["params.infersent.encode"], "function", ["home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.ClassificationNet.encode"], ["def", "batcher", "(", "batch", ",", "params", ")", ":", "\n", "# batch contains list of words", "\n", "    ", "batch", "=", "[", "[", "'<s>'", "]", "+", "s", "+", "[", "'</s>'", "]", "for", "s", "in", "batch", "]", "\n", "sentences", "=", "[", "' '", ".", "join", "(", "s", ")", "for", "s", "in", "batch", "]", "\n", "embeddings", "=", "params", ".", "infersent", ".", "encode", "(", "sentences", ",", "bsize", "=", "params", ".", "batch_size", ",", "\n", "tokenize", "=", "False", ")", "\n", "\n", "return", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.mutils.prepare": [[116, 119], ["params.infersent.build_vocab"], "function", ["home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.data.build_vocab"], ["", "def", "prepare", "(", "params", ",", "samples", ")", ":", "\n", "    ", "params", ".", "infersent", ".", "build_vocab", "(", "[", "' '", ".", "join", "(", "s", ")", "for", "s", "in", "samples", "]", ",", "\n", "params", ".", "glove_path", ",", "tokenize", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.train.get_args": [[16, 73], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_known_args", "print", "print"], "function", ["None"], ["def", "get_args", "(", ")", ":", "\n", "  ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Training NLI model based on just hypothesis sentence'", ")", "\n", "\n", "# paths", "\n", "parser", ".", "add_argument", "(", "\"--use_early_stopping\"", ",", "action", "=", "\"store_true\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--version\"", ",", "type", "=", "int", ",", "default", "=", "2", ",", "help", "=", "\"Defines the version of the model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--embdfile\"", ",", "type", "=", "str", ",", "default", "=", "'../data/embds/glove.840B.300d.txt'", ",", "help", "=", "\"File containin the word embeddings\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--outputdir\"", ",", "type", "=", "str", ",", "default", "=", "'savedir/'", ",", "help", "=", "\"Output directory\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--outputmodelname\"", ",", "type", "=", "str", ",", "default", "=", "'model.pickle'", ")", "\n", "parser", ".", "add_argument", "(", "\"--outputmodelname_adversary\"", ",", "type", "=", "str", ",", "default", "=", "'model_adv.pickle'", ")", "\n", "parser", ".", "add_argument", "(", "\"--nlipath\"", ",", "type", "=", "str", ",", "required", "=", "True", ",", "help", "=", "\"specifies the folder of the dataset\"", ")", "\n", "\n", "# data", "\n", "parser", ".", "add_argument", "(", "\"--max_train_sents\"", ",", "type", "=", "int", ",", "default", "=", "10000000", ",", "help", "=", "\"Maximum number of training examples\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_val_sents\"", ",", "type", "=", "int", ",", "default", "=", "10000000", ",", "help", "=", "\"Maximum number of validation/dev examples\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_test_sents\"", ",", "type", "=", "int", ",", "default", "=", "10000000", ",", "help", "=", "\"Maximum number of test examples\"", ")", "\n", "# training", "\n", "parser", ".", "add_argument", "(", "\"--n_epochs\"", ",", "type", "=", "int", ",", "default", "=", "20", ")", "\n", "parser", ".", "add_argument", "(", "\"--batch_size\"", ",", "type", "=", "int", ",", "default", "=", "64", ")", "\n", "parser", ".", "add_argument", "(", "\"--dpout_model\"", ",", "type", "=", "float", ",", "default", "=", "0.", ",", "help", "=", "\"encoder dropout\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--dpout_fc\"", ",", "type", "=", "float", ",", "default", "=", "0.", ",", "help", "=", "\"classifier dropout\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--nonlinear_fc\"", ",", "type", "=", "float", ",", "default", "=", "0", ",", "help", "=", "\"use nonlinearity in fc\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--optimizer\"", ",", "type", "=", "str", ",", "default", "=", "\"sgd,lr=0.1\"", ",", "help", "=", "\"adam or sgd,lr=0.1\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--lrshrink\"", ",", "type", "=", "float", ",", "default", "=", "5", ",", "help", "=", "\"shrink factor for sgd\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--decay\"", ",", "type", "=", "float", ",", "default", "=", "0.99", ",", "help", "=", "\"lr decay\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--minlr\"", ",", "type", "=", "float", ",", "default", "=", "1e-5", ",", "help", "=", "\"minimum lr\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_norm\"", ",", "type", "=", "float", ",", "default", "=", "5.", ",", "help", "=", "\"max norm (grad clipping)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--adv_lambda\"", ",", "type", "=", "float", ",", "default", "=", "1.", ",", "help", "=", "\"adversarial loss weight for hyp only classifier\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--adv_hyp_encoder_lambda\"", ",", "type", "=", "float", ",", "default", "=", "1.", ",", "help", "=", "\"adversarial weight for hypothesis only encoder\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--nli_net_adv_hyp_encoder_lambda\"", ",", "type", "=", "float", ",", "default", "=", "1.", ",", "help", "=", "\"adversarial weight for hypothesis encoder in NLI net\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--random_premise_frac\"", ",", "type", "=", "float", ",", "default", "=", "0.", ",", "help", "=", "\"fraction of random premises to use in NLI net\"", ")", "\n", "\n", "# model", "\n", "parser", ".", "add_argument", "(", "\"--encoder_type\"", ",", "type", "=", "str", ",", "default", "=", "'InferSent'", ",", "help", "=", "\"see list of encoders\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--enc_lstm_dim\"", ",", "type", "=", "int", ",", "default", "=", "2048", ",", "help", "=", "\"encoder nhid dimension\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--n_enc_layers\"", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "\"encoder num layers\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--fc_dim\"", ",", "type", "=", "int", ",", "default", "=", "512", ",", "help", "=", "\"nhid of fc layers\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--n_classes\"", ",", "type", "=", "int", ",", "default", "=", "3", ",", "required", "=", "True", ",", "help", "=", "\"entailment/neutral/contradiction\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--pool_type\"", ",", "type", "=", "str", ",", "default", "=", "'max'", ",", "help", "=", "\"max or mean\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--pre_trained_model\"", ",", "type", "=", "str", ",", "default", "=", "''", ",", "help", "=", "\"Path to pre-trained model\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--pre_trained_adv_model\"", ",", "type", "=", "str", ",", "default", "=", "''", ",", "help", "=", "\"Path to pre-trained hypothesis model\"", ")", "\n", "\n", "# gpu", "\n", "parser", ".", "add_argument", "(", "\"--gpu_id\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "help", "=", "\"GPU ID\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--seed\"", ",", "type", "=", "int", ",", "default", "=", "1234", ",", "help", "=", "\"seed\"", ")", "\n", "\n", "\n", "#misc", "\n", "parser", ".", "add_argument", "(", "\"--verbose\"", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "\"Verbose output\"", ")", "\n", "\n", "params", ",", "_", "=", "parser", ".", "parse_known_args", "(", ")", "\n", "\n", "# print parameters passed, and all parameters", "\n", "print", "(", "'\\ntogrep : {0}\\n'", ".", "format", "(", "sys", ".", "argv", "[", "1", ":", "]", ")", ")", "\n", "print", "(", "params", ")", "\n", "\n", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.train.get_model_configs": [[74, 105], ["str"], "function", ["None"], ["", "def", "get_model_configs", "(", "params", ",", "n_words", ")", ":", "\n", "  ", "\"\"\"\n  MODEL\n  \"\"\"", "\n", "# model config", "\n", "config_nli_model", "=", "{", "\n", "'n_words'", ":", "n_words", ",", "\n", "'word_emb_dim'", ":", "params", ".", "word_emb_dim", ",", "\n", "'enc_lstm_dim'", ":", "params", ".", "enc_lstm_dim", ",", "\n", "'n_enc_layers'", ":", "params", ".", "n_enc_layers", ",", "\n", "'dpout_model'", ":", "params", ".", "dpout_model", ",", "\n", "'dpout_fc'", ":", "params", ".", "dpout_fc", ",", "\n", "'fc_dim'", ":", "params", ".", "fc_dim", ",", "\n", "'bsize'", ":", "params", ".", "batch_size", ",", "\n", "'n_classes'", ":", "params", ".", "n_classes", ",", "\n", "'pool_type'", ":", "params", ".", "pool_type", ",", "\n", "'nonlinear_fc'", ":", "params", ".", "nonlinear_fc", ",", "\n", "'encoder_type'", ":", "params", ".", "encoder_type", ",", "\n", "'use_cuda'", ":", "params", ".", "gpu_id", ">", "-", "1", ",", "\n", "'verbose'", ":", "params", ".", "verbose", ">", "0", ",", "\n", "'adv_hyp_encoder_lambda'", ":", "params", ".", "adv_hyp_encoder_lambda", ",", "\n", "'nli_net_adv_hyp_encoder_lambda'", ":", "params", ".", "nli_net_adv_hyp_encoder_lambda", ",", "\n", "'random_premise_frac'", ":", "params", ".", "random_premise_frac", ",", "\n", "'version'", ":", "params", ".", "version", ",", "\n", "}", "\n", "\n", "# model", "\n", "encoder_types", "=", "[", "'InferSent'", "]", "\n", "assert", "params", ".", "encoder_type", "in", "encoder_types", ",", "\"encoder_type must be in \"", "+", "str", "(", "encoder_types", ")", "\n", "return", "config_nli_model", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.train.trainepoch": [[106, 229], ["print", "shared_nli_net.train", "shared_hypoth_net.train", "time.time", "numpy.random.permutation", "print", "print", "time.time", "range", "round", "round", "print", "len", "len", "data.get_batch", "torch.autograd.Variable.size", "shared_nli_net", "pred_nli.long().eq().cpu().sum().item", "loss_fn_nli", "all_costs_nli.append", "optimizer_nli.zero_grad", "loss_fn_nli.backward", "train.trainepoch.clip_gradients_and_step"], "function", ["home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.data.get_batch", "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.models.GradReverse.backward"], ["", "def", "trainepoch", "(", "epoch", ",", "train", ",", "optimizer_nli", ",", "optimizer_hypoth", ",", "params", ",", "word_vec", ",", "shared_nli_net", ",", "shared_hypoth_net", ",", "loss_fn_nli", ",", "loss_fn_hypoth", ",", "adv_lambda", ",", "adv_hyp_encoder_lambda", ")", ":", "\n", "  ", "print", "(", "'\\nTRAINING : Epoch '", "+", "str", "(", "epoch", ")", ")", "\n", "shared_nli_net", ".", "train", "(", ")", "\n", "shared_hypoth_net", ".", "train", "(", ")", "\n", "all_costs_nli", ",", "all_costs_hypoth", "=", "[", "]", ",", "[", "]", "\n", "words_count", "=", "0", "\n", "\n", "last_time", "=", "time", ".", "time", "(", ")", "\n", "correct_nli", ",", "correct_hypoth", "=", "0.", ",", "0.", "\n", "\n", "# shuffle the data", "\n", "permutation", "=", "np", ".", "random", ".", "permutation", "(", "len", "(", "train", "[", "'s1'", "]", ")", ")", "\n", "premises", "=", "train", "[", "'s1'", "]", "[", "permutation", "]", "\n", "hypoths", "=", "train", "[", "'s2'", "]", "[", "permutation", "]", "\n", "target", "=", "train", "[", "'label'", "]", "[", "permutation", "]", "\n", "\n", "optimizer_nli", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "=", "optimizer_nli", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "*", "params", ".", "decay", "if", "epoch", ">", "1", "and", "'sgd'", "in", "params", ".", "optimizer", "else", "optimizer_nli", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "\n", "print", "(", "'Learning rate (nli) : {0}'", ".", "format", "(", "optimizer_nli", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", ")", ")", "\n", "optimizer_hypoth", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "=", "optimizer_hypoth", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "*", "params", ".", "decay", "if", "epoch", ">", "1", "and", "'sgd'", "in", "params", ".", "optimizer", "else", "optimizer_hypoth", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "\n", "print", "(", "'Learning rate (hypoth) : {0}'", ".", "format", "(", "optimizer_hypoth", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", ")", ")", "\n", "\n", "#trained_sents = 0", "\n", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "for", "stidx", "in", "range", "(", "0", ",", "len", "(", "hypoths", ")", ",", "params", ".", "batch_size", ")", ":", "\n", "# prepare batch", "\n", "    ", "hypoths_batch", ",", "hypoths_len", "=", "get_batch", "(", "hypoths", "[", "stidx", ":", "stidx", "+", "params", ".", "batch_size", "]", ",", "word_vec", ")", "\n", "if", "np", ".", "random", ".", "random", "(", ")", ">", "params", ".", "random_premise_frac", ":", "\n", "      ", "random_premise", "=", "False", "\n", "premises_batch", ",", "premises_len", "=", "get_batch", "(", "premises", "[", "stidx", ":", "stidx", "+", "params", ".", "batch_size", "]", ",", "word_vec", ")", "\n", "", "else", ":", "\n", "      ", "random_premise", "=", "True", "\n", "premises_batch", ",", "premises_len", "=", "get_batch", "(", "np", ".", "random", ".", "choice", "(", "premises", ",", "len", "(", "hypoths_len", ")", ",", "replace", "=", "False", ")", ",", "word_vec", ")", "\n", "\n", "", "tgt_batch", "=", "None", "\n", "if", "params", ".", "gpu_id", ">", "-", "1", ":", "\n", "      ", "hypoths_batch", "=", "Variable", "(", "hypoths_batch", ".", "cuda", "(", ")", ")", "\n", "premises_batch", "=", "Variable", "(", "premises_batch", ".", "cuda", "(", ")", ")", "\n", "tgt_batch", "=", "Variable", "(", "torch", ".", "LongTensor", "(", "target", "[", "stidx", ":", "stidx", "+", "params", ".", "batch_size", "]", ")", ")", ".", "cuda", "(", ")", "\n", "", "else", ":", "\n", "      ", "hypoths_batch", "=", "Variable", "(", "hypoths_batch", ")", "\n", "premises_batch", "=", "Variable", "(", "premises_batch", ")", "\n", "tgt_batch", "=", "Variable", "(", "torch", ".", "LongTensor", "(", "target", "[", "stidx", ":", "stidx", "+", "params", ".", "batch_size", "]", ")", ")", "\n", "\n", "", "k", "=", "hypoths_batch", ".", "size", "(", "1", ")", "# actual batch size", "\n", "\n", "# model forward", "\n", "output_nli", "=", "shared_nli_net", "(", "(", "premises_batch", ",", "premises_len", ")", ",", "(", "hypoths_batch", ",", "hypoths_len", ")", ",", "random_premise", ")", "\n", "pred_nli", "=", "output_nli", ".", "data", ".", "max", "(", "1", ")", "[", "1", "]", "\n", "correct_nli", "+=", "pred_nli", ".", "long", "(", ")", ".", "eq", "(", "tgt_batch", ".", "data", ".", "long", "(", ")", ")", ".", "cpu", "(", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "assert", "len", "(", "pred_nli", ")", "==", "len", "(", "hypoths", "[", "stidx", ":", "stidx", "+", "params", ".", "batch_size", "]", ")", "\n", "\n", "if", "adv_lambda", ">", "0", ":", "\n", "      ", "output_hypoth", "=", "shared_hypoth_net", "(", "(", "hypoths_batch", ",", "hypoths_len", ")", ")", "\n", "pred_hypoth", "=", "output_hypoth", ".", "data", ".", "max", "(", "1", ")", "[", "1", "]", "\n", "correct_hypoth", "+=", "pred_hypoth", ".", "long", "(", ")", ".", "eq", "(", "tgt_batch", ".", "data", ".", "long", "(", ")", ")", ".", "cpu", "(", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "assert", "len", "(", "pred_hypoth", ")", "==", "len", "(", "hypoths", "[", "stidx", ":", "stidx", "+", "params", ".", "batch_size", "]", ")", "\n", "\n", "# loss", "\n", "", "loss_nli", "=", "loss_fn_nli", "(", "output_nli", ",", "tgt_batch", ")", "\n", "all_costs_nli", ".", "append", "(", "loss_nli", ".", "item", "(", ")", ")", "\n", "if", "adv_lambda", ">", "0", ":", "\n", "      ", "loss_hypoth", "=", "loss_fn_hypoth", "(", "output_hypoth", ",", "tgt_batch", ")", "\n", "loss_hypoth", "*=", "adv_lambda", "\n", "all_costs_hypoth", ".", "append", "(", "loss_hypoth", ".", "item", "(", ")", ")", "\n", "", "words_count", "+=", "hypoths_batch", ".", "nelement", "(", ")", "/", "params", ".", "word_emb_dim", "\n", "\n", "# backward", "\n", "optimizer_nli", ".", "zero_grad", "(", ")", "\n", "if", "adv_lambda", ">", "0", ":", "\n", "      ", "optimizer_hypoth", ".", "zero_grad", "(", ")", "\n", "", "loss_nli", ".", "backward", "(", ")", "\n", "if", "adv_lambda", ">", "0", ":", "\n", "      ", "loss_hypoth", ".", "backward", "(", ")", "\n", "\n", "\n", "", "def", "clip_gradients_and_step", "(", "net", ",", "k", ",", "optimizer", ")", ":", "\n", "\n", "# gradient clipping (off by default)", "\n", "      ", "shrink_factor", "=", "1", "\n", "total_norm", "=", "0", "\n", "\n", "for", "p", "in", "net", ".", "parameters", "(", ")", ":", "\n", "        ", "if", "p", ".", "requires_grad", ":", "\n", "#p.grad.data.div_(k)  # divide by the actual batch size", "\n", "          ", "total_norm", "+=", "p", ".", "grad", ".", "data", ".", "norm", "(", ")", "**", "2", "\n", "", "", "total_norm", "=", "np", ".", "sqrt", "(", "total_norm", ".", "item", "(", ")", ")", "\n", "\n", "if", "total_norm", ">", "params", ".", "max_norm", ":", "\n", "          ", "shrink_factor", "=", "params", ".", "max_norm", "/", "total_norm", "\n", "", "current_lr", "=", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "# current lr (no external \"lr\", for adam)", "\n", "\n", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "=", "current_lr", "*", "shrink_factor", "# just for update", "\n", "\n", "# optimizer step", "\n", "optimizer", ".", "step", "(", ")", "\n", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "=", "current_lr", "\n", "\n", "\n", "", "clip_gradients_and_step", "(", "shared_nli_net", ",", "k", ",", "optimizer_nli", ")", "\n", "if", "adv_lambda", ">", "0", ":", "\n", "      ", "clip_gradients_and_step", "(", "shared_hypoth_net", ",", "k", ",", "optimizer_hypoth", ")", "\n", "\n", "", "if", "len", "(", "all_costs_nli", ")", "==", "100", ":", "\n", "      ", "print", "(", "'{0} ; loss nli {1} ; loss hypoth {2} ; sentence/s {3} ; words/s {4} ; accuracy train nli {5} ; accuracy train hypoth {6}'", ".", "format", "(", "\n", "stidx", ",", "round", "(", "np", ".", "mean", "(", "all_costs_nli", ")", ",", "2", ")", ",", "\n", "round", "(", "np", ".", "mean", "(", "all_costs_hypoth", ")", ",", "2", ")", ",", "\n", "int", "(", "len", "(", "all_costs_nli", ")", "*", "params", ".", "batch_size", "/", "(", "time", ".", "time", "(", ")", "-", "last_time", ")", ")", ",", "\n", "int", "(", "words_count", "*", "1.0", "/", "(", "time", ".", "time", "(", ")", "-", "last_time", ")", ")", ",", "\n", "round", "(", "100.", "*", "correct_nli", "/", "(", "stidx", "+", "k", ")", ",", "2", ")", ",", "\n", "round", "(", "100.", "*", "correct_hypoth", "/", "(", "stidx", "+", "k", ")", ",", "2", ")", ")", ")", "\n", "last_time", "=", "time", ".", "time", "(", ")", "\n", "words_count", "=", "0", "\n", "all_costs_nli", ",", "all_costs_hypoth", "=", "[", "]", ",", "[", "]", "\n", "\n", "", "", "train_acc_nli", "=", "round", "(", "100.", "*", "correct_nli", "/", "len", "(", "hypoths", ")", ",", "2", ")", "\n", "train_acc_hypoth", "=", "round", "(", "100.", "*", "correct_hypoth", "/", "len", "(", "hypoths", ")", ",", "2", ")", "\n", "print", "(", "'results : epoch {0} ; mean accuracy train nli : {1}, loss nli : {2} ; mean accuracy train hypoth : {3}, loss hypoth : {4}'", "\n", ".", "format", "(", "epoch", ",", "train_acc_nli", ",", "round", "(", "np", ".", "mean", "(", "all_costs_nli", ")", ",", "2", ")", ",", "train_acc_hypoth", ",", "round", "(", "np", ".", "mean", "(", "all_costs_hypoth", ")", ",", "2", ")", ")", ")", "\n", "return", "train_acc_nli", ",", "train_acc_hypoth", ",", "shared_nli_net", ",", "shared_hypoth_net", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.train.evaluate": [[230, 297], ["shared_nli_net.eval", "shared_hypoth_net.eval", "range", "round", "round", "print", "len", "data.get_batch", "data.get_batch", "shared_nli_net", "pred_nli.long().eq().cpu().sum().item", "print", "print", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable().cuda", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "shared_nli_net.data.max", "shared_hypoth_net", "pred_hypoth.long().eq().cpu().sum().item", "len", "len", "print", "torch.save", "torch.save", "torch.save", "torch.save", "torch.autograd.Variable.cuda", "torch.autograd.Variable.cuda", "torch.LongTensor", "torch.LongTensor", "pred_nli.long().eq().cpu().sum", "shared_hypoth_net.data.max", "os.path.exists", "os.makedirs", "os.path.join", "os.path.join", "print", "print", "torch.autograd.Variable", "pred_hypoth.long().eq().cpu().sum", "torch.LongTensor", "torch.LongTensor", "pred_nli.long().eq().cpu", "pred_hypoth.long().eq().cpu", "pred_nli.long().eq", "torch.autograd.Variable.data.long", "pred_hypoth.long().eq", "pred_nli.long", "torch.autograd.Variable.data.long", "pred_hypoth.long"], "function", ["home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.data.get_batch", "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.data.get_batch"], ["", "def", "evaluate", "(", "epoch", ",", "valid", ",", "optimizer_nli", ",", "optimizer_hypoth", ",", "params", ",", "word_vec", ",", "shared_nli_net", ",", "shared_hypoth_net", ",", "eval_type", "=", "'valid'", ",", "final_eval", "=", "False", ",", "adv_lambda", "=", "1.", ")", ":", "\n", "  ", "shared_nli_net", ".", "eval", "(", ")", "\n", "shared_hypoth_net", ".", "eval", "(", ")", "\n", "correct_nli", ",", "correct_hypoth", "=", "0.", ",", "0.", "\n", "global", "val_acc_best", ",", "lr", ",", "stop_training", ",", "adam_stop", "\n", "\n", "if", "eval_type", "==", "'valid'", ":", "\n", "    ", "print", "(", "'\\n{0} : Epoch {1}'", ".", "format", "(", "eval_type", ",", "epoch", ")", ")", "\n", "\n", "", "hypoths", "=", "valid", "[", "'s2'", "]", "\n", "premises", "=", "valid", "[", "'s1'", "]", "\n", "target", "=", "valid", "[", "'label'", "]", "\n", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "hypoths", ")", ",", "params", ".", "batch_size", ")", ":", "\n", "# prepare batch", "\n", "    ", "hypoths_batch", ",", "hypoths_len", "=", "get_batch", "(", "hypoths", "[", "i", ":", "i", "+", "params", ".", "batch_size", "]", ",", "word_vec", ")", "\n", "premises_batch", ",", "premises_len", "=", "get_batch", "(", "premises", "[", "i", ":", "i", "+", "params", ".", "batch_size", "]", ",", "word_vec", ")", "\n", "tgt_batch", "=", "None", "\n", "if", "params", ".", "gpu_id", ">", "-", "1", ":", "\n", "      ", "hypoths_batch", "=", "Variable", "(", "hypoths_batch", ".", "cuda", "(", ")", ")", "\n", "premises_batch", "=", "Variable", "(", "premises_batch", ".", "cuda", "(", ")", ")", "\n", "tgt_batch", "=", "Variable", "(", "torch", ".", "LongTensor", "(", "target", "[", "i", ":", "i", "+", "params", ".", "batch_size", "]", ")", ")", ".", "cuda", "(", ")", "\n", "", "else", ":", "\n", "      ", "hypoths_batch", "=", "Variable", "(", "hypoths_batch", ")", "\n", "premises_batch", "=", "Variable", "(", "premises_batch", ")", "\n", "tgt_batch", "=", "Variable", "(", "torch", ".", "LongTensor", "(", "target", "[", "i", ":", "i", "+", "params", ".", "batch_size", "]", ")", ")", "\n", "\n", "# model forward", "\n", "", "output_nli", "=", "shared_nli_net", "(", "(", "premises_batch", ",", "premises_len", ")", ",", "(", "hypoths_batch", ",", "hypoths_len", ")", ")", "\n", "pred_nli", "=", "output_nli", ".", "data", ".", "max", "(", "1", ")", "[", "1", "]", "\n", "correct_nli", "+=", "pred_nli", ".", "long", "(", ")", ".", "eq", "(", "tgt_batch", ".", "data", ".", "long", "(", ")", ")", ".", "cpu", "(", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "if", "adv_lambda", ">", "0", ":", "\n", "      ", "output_hypoth", "=", "shared_hypoth_net", "(", "(", "hypoths_batch", ",", "hypoths_len", ")", ")", "\n", "pred_hypoth", "=", "output_hypoth", ".", "data", ".", "max", "(", "1", ")", "[", "1", "]", "\n", "correct_hypoth", "+=", "pred_hypoth", ".", "long", "(", ")", ".", "eq", "(", "tgt_batch", ".", "data", ".", "long", "(", ")", ")", ".", "cpu", "(", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "\n", "# save model", "\n", "", "", "eval_acc_nli", "=", "round", "(", "100.", "*", "correct_nli", "/", "len", "(", "hypoths", ")", ",", "2", ")", "\n", "eval_acc_hypoth", "=", "round", "(", "100.", "*", "correct_hypoth", "/", "len", "(", "hypoths", ")", ",", "2", ")", "\n", "if", "final_eval", ":", "\n", "    ", "print", "(", "'finalgrep : accuracy {0} : nli {1}, hypoth {2}'", ".", "format", "(", "eval_type", ",", "eval_acc_nli", ",", "eval_acc_hypoth", ")", ")", "\n", "", "else", ":", "\n", "    ", "print", "(", "'togrep : results : epoch {0} ; mean accuracy {1} : nli {2}, hypoth {3}'", ".", "format", "(", "epoch", ",", "eval_type", ",", "eval_acc_nli", ",", "eval_acc_hypoth", ")", ")", "\n", "\n", "", "if", "eval_type", "==", "'valid'", "and", "epoch", "<=", "params", ".", "n_epochs", ":", "\n", "    ", "if", "eval_acc_nli", ">", "val_acc_best", ":", "\n", "      ", "print", "(", "'saving model at epoch {0}'", ".", "format", "(", "epoch", ")", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "params", ".", "outputdir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "params", ".", "outputdir", ")", "\n", "", "torch", ".", "save", "(", "shared_nli_net", ",", "os", ".", "path", ".", "join", "(", "params", ".", "outputdir", ",", "params", ".", "outputmodelname", ")", ")", "\n", "torch", ".", "save", "(", "shared_hypoth_net", ",", "os", ".", "path", ".", "join", "(", "params", ".", "outputdir", ",", "params", ".", "outputmodelname_adversary", ")", ")", "\n", "val_acc_best", "=", "eval_acc_nli", "\n", "", "else", ":", "\n", "      ", "if", "'sgd'", "in", "params", ".", "optimizer", ":", "\n", "        ", "optimizer_nli", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "=", "optimizer_nli", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "/", "params", ".", "lrshrink", "\n", "print", "(", "'Shrinking nli lr by : {0}. New lr = {1}'", ".", "format", "(", "params", ".", "lrshrink", ",", "\n", "optimizer_nli", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", ")", ")", "\n", "optimizer_hypoth", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "=", "optimizer_hypoth", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "/", "params", ".", "lrshrink", "\n", "print", "(", "'Shrinking hypoth lr by : {0}. New lr = {1}'", ".", "format", "(", "params", ".", "lrshrink", ",", "\n", "optimizer_hypoth", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", ")", ")", "\n", "if", "optimizer_nli", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "<", "params", ".", "minlr", "and", "params", ".", "use_early_stopping", ":", "\n", "          ", "stop_training", "=", "True", "\n", "", "", "if", "'adam'", "in", "params", ".", "optimizer", "and", "params", ".", "use_early_stopping", ":", "\n", "# early stopping (at 2nd decrease in accuracy)", "\n", "        ", "stop_training", "=", "adam_stop", "\n", "adam_stop", "=", "True", "\n", "", "", "", "return", "eval_acc_nli", ",", "eval_acc_hypoth", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.train.main": [[298, 408], ["numpy.random.seed", "torch.manual_seed", "torch.manual_seed", "data.get_nli", "data.build_vocab", "len", "train.get_model_configs", "models.SharedNLINet", "models.SharedHypothNet", "print", "print", "print", "print", "torch.FloatTensor().fill_", "torch.FloatTensor().fill_", "torch.CrossEntropyLoss", "torch.FloatTensor().fill_", "torch.FloatTensor().fill_", "torch.CrossEntropyLoss", "mutils.get_optimizer", "optim_fn", "optim_fn", "torch.cuda.manual_seed", "torch.cuda.manual_seed", "len", "eval", "eval", "print", "torch.load", "torch.load", "models.SharedNLINet.state_dict", "torch.load.state_dict", "shared_nli_net.state_dict.items", "models.SharedNLINet.load_state_dict", "print", "torch.load", "torch.load", "models.SharedHypothNet.state_dict", "torch.load.state_dict", "nli_hypoth_params.items", "models.SharedHypothNet.load_state_dict", "models.SharedNLINet.parameters", "models.SharedHypothNet.classifier.parameters", "models.SharedNLINet.cuda", "models.SharedHypothNet.cuda", "nn.CrossEntropyLoss.cuda", "nn.CrossEntropyLoss.cuda", "train.trainepoch", "train.evaluate", "numpy.array", "shared_nli_net.state_dict.keys", "pre_trained_model.state_dict.keys", "shared_hypoth_net.state_dict.keys", "pre_trained_model.state_dict.keys", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "eval", "parameters.size", "pre_trained_params[].size", "parameters.size", "pre_trained_params[].size", "list", "data.build_vocab.keys", "eval", "sent.split"], "function", ["home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.data.get_nli", "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.data.build_vocab", "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.train.get_model_configs", "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.mutils.get_optimizer", "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.train.trainepoch", "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.train.evaluate"], ["", "def", "main", "(", "args", ")", ":", "\n", "\n", "  ", "\"\"\"\n  SEED\n  \"\"\"", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "if", "args", ".", "gpu_id", ">", "-", "1", ":", "\n", "    ", "torch", ".", "cuda", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "\n", "", "\"\"\"\n  DATA\n  \"\"\"", "\n", "train", ",", "valid", ",", "test", "=", "get_nli", "(", "args", ".", "nlipath", ",", "args", ".", "n_classes", ")", "\n", "word_vecs", "=", "build_vocab", "(", "train", "[", "'s1'", "]", "+", "train", "[", "'s2'", "]", "+", "\n", "valid", "[", "'s1'", "]", "+", "valid", "[", "'s2'", "]", "+", "\n", "test", "[", "'s1'", "]", "+", "test", "[", "'s2'", "]", ",", "args", ".", "embdfile", ")", "\n", "\n", "for", "split", "in", "[", "'s1'", ",", "'s2'", "]", ":", "\n", "    ", "for", "data_type", "in", "[", "'train'", ",", "'valid'", ",", "'test'", "]", ":", "\n", "        ", "eval", "(", "data_type", ")", "[", "split", "]", "=", "np", ".", "array", "(", "[", "[", "'<s>'", "]", "+", "\n", "[", "word", "for", "word", "in", "sent", ".", "split", "(", ")", "if", "word", "in", "word_vecs", "]", "+", "\n", "[", "'</s>'", "]", "for", "sent", "in", "eval", "(", "data_type", ")", "[", "split", "]", "]", ")", "\n", "\n", "\n", "", "", "args", ".", "word_emb_dim", "=", "len", "(", "word_vecs", "[", "list", "(", "word_vecs", ".", "keys", "(", ")", ")", "[", "0", "]", "]", ")", "\n", "\n", "nli_model_configs", "=", "get_model_configs", "(", "args", ",", "len", "(", "word_vecs", ")", ")", "\n", "\n", "\n", "nli_model_configs", "[", "\"n_classes\"", "]", "=", "args", ".", "n_classes", "\n", "\n", "# define premise and hypoth encoders", "\n", "premise_encoder", "=", "eval", "(", "nli_model_configs", "[", "'encoder_type'", "]", ")", "(", "nli_model_configs", ")", "\n", "hypoth_encoder", "=", "eval", "(", "nli_model_configs", "[", "'encoder_type'", "]", ")", "(", "nli_model_configs", ")", "\n", "shared_nli_net", "=", "SharedNLINet", "(", "nli_model_configs", ",", "premise_encoder", ",", "hypoth_encoder", ")", "\n", "shared_hypoth_net", "=", "SharedHypothNet", "(", "nli_model_configs", ",", "hypoth_encoder", ")", "\n", "print", "(", "shared_nli_net", ")", "\n", "print", "(", "shared_hypoth_net", ")", "\n", "\n", "if", "args", ".", "pre_trained_model", ":", "\n", "    ", "print", "(", "\"Pre_trained_model: \"", "+", "args", ".", "pre_trained_model", ")", "\n", "pre_trained_model", "=", "torch", ".", "load", "(", "args", ".", "pre_trained_model", ")", "\n", "\n", "shared_nli_net_params", "=", "shared_nli_net", ".", "state_dict", "(", ")", "\n", "pre_trained_params", "=", "pre_trained_model", ".", "state_dict", "(", ")", "\n", "assert", "shared_nli_net_params", ".", "keys", "(", ")", "==", "pre_trained_params", ".", "keys", "(", ")", ",", "\"load model has different parameter state names that NLI_HYPOTHS_NET\"", "\n", "for", "key", ",", "parameters", "in", "shared_nli_net_params", ".", "items", "(", ")", ":", "\n", "      ", "if", "parameters", ".", "size", "(", ")", "==", "pre_trained_params", "[", "key", "]", ".", "size", "(", ")", ":", "\n", "        ", "shared_nli_net_params", "[", "key", "]", "=", "pre_trained_params", "[", "key", "]", "\n", "", "", "shared_nli_net", ".", "load_state_dict", "(", "shared_nli_net_params", ")", "\n", "\n", "", "print", "(", "shared_nli_net", ")", "\n", "\n", "if", "args", ".", "pre_trained_adv_model", ":", "\n", "    ", "print", "(", "\"Pre_trained_adv_model: \"", "+", "args", ".", "pre_trained_adv_model", ")", "\n", "pre_trained_model", "=", "torch", ".", "load", "(", "args", ".", "pre_trained_adv_model", ")", "\n", "\n", "shared_hypoth_net_params", "=", "shared_hypoth_net", ".", "state_dict", "(", ")", "\n", "pre_trained_params", "=", "pre_trained_model", ".", "state_dict", "(", ")", "\n", "assert", "shared_hypoth_net_params", ".", "keys", "(", ")", "==", "pre_trained_params", ".", "keys", "(", ")", ",", "\"load model has different parameter state names that NLI_HYPOTHS_NET\"", "\n", "for", "key", ",", "parameters", "in", "nli_hypoth_params", ".", "items", "(", ")", ":", "\n", "      ", "if", "parameters", ".", "size", "(", ")", "==", "pre_trained_params", "[", "key", "]", ".", "size", "(", ")", ":", "\n", "        ", "shared_hypoth_net_params", "[", "key", "]", "=", "pre_trained_params", "[", "key", "]", "\n", "", "", "shared_hypoth_net", ".", "load_state_dict", "(", "shared_hypoth_net_params", ")", "\n", "\n", "", "print", "(", "shared_hypoth_net", ")", "\n", "\n", "\n", "# nli loss", "\n", "weight", "=", "torch", ".", "FloatTensor", "(", "args", ".", "n_classes", ")", ".", "fill_", "(", "1", ")", "\n", "loss_fn_nli", "=", "nn", ".", "CrossEntropyLoss", "(", "weight", "=", "weight", ")", "\n", "loss_fn_nli", ".", "size_average", "=", "False", "\n", "\n", "# hypoth (adversarial) loss", "\n", "weight", "=", "torch", ".", "FloatTensor", "(", "args", ".", "n_classes", ")", ".", "fill_", "(", "1", ")", "\n", "loss_fn_hypoth", "=", "nn", ".", "CrossEntropyLoss", "(", "weight", "=", "weight", ")", "\n", "loss_fn_hypoth", ".", "size_average", "=", "False", "\n", "\n", "# optimizer", "\n", "optim_fn", ",", "optim_params", "=", "get_optimizer", "(", "args", ".", "optimizer", ")", "\n", "optimizer_nli", "=", "optim_fn", "(", "shared_nli_net", ".", "parameters", "(", ")", ",", "**", "optim_params", ")", "\n", "#optimizer_hypoth = optim_fn(shared_hypoth_net.parameters(), **optim_params)", "\n", "# only pass hypoth classifier params to avoid updating shared encoder params twice ", "\n", "optimizer_hypoth", "=", "optim_fn", "(", "shared_hypoth_net", ".", "classifier", ".", "parameters", "(", ")", ",", "**", "optim_params", ")", "\n", "\n", "if", "args", ".", "gpu_id", ">", "-", "1", ":", "\n", "    ", "shared_nli_net", ".", "cuda", "(", ")", "\n", "shared_hypoth_net", ".", "cuda", "(", ")", "\n", "loss_fn_nli", ".", "cuda", "(", ")", "\n", "loss_fn_hypoth", ".", "cuda", "(", ")", "\n", "\n", "", "\"\"\"\n  TRAIN\n  \"\"\"", "\n", "global", "val_acc_best", ",", "lr", ",", "stop_training", ",", "adam_stop", "\n", "val_acc_best", "=", "-", "1e10", "\n", "adam_stop", "=", "False", "\n", "stop_training", "=", "False", "\n", "lr", "=", "optim_params", "[", "'lr'", "]", "if", "'sgd'", "in", "args", ".", "optimizer", "else", "None", "\n", "\n", "\"\"\"\n  Train model on Natural Language Inference task\n  \"\"\"", "\n", "epoch", "=", "1", "\n", "\n", "while", "not", "stop_training", "and", "epoch", "<=", "args", ".", "n_epochs", ":", "\n", "    ", "train_acc_nli", ",", "train_acc_hypoth", ",", "shared_nli_net", ",", "shared_hypoth_net", "=", "trainepoch", "(", "epoch", ",", "train", ",", "optimizer_nli", ",", "optimizer_hypoth", ",", "args", ",", "word_vecs", ",", "shared_nli_net", ",", "shared_hypoth_net", ",", "loss_fn_nli", ",", "loss_fn_hypoth", ",", "args", ".", "adv_lambda", ",", "args", ".", "adv_hyp_encoder_lambda", ")", "\n", "eval_acc_nli", ",", "eval_acc_hypoth", "=", "evaluate", "(", "epoch", ",", "valid", ",", "optimizer_nli", ",", "optimizer_hypoth", ",", "args", ",", "word_vecs", ",", "shared_nli_net", ",", "shared_hypoth_net", ",", "'valid'", ",", "adv_lambda", "=", "args", ".", "adv_lambda", ")", "\n", "epoch", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.data.get_batch": [[6, 17], ["numpy.array", "numpy.max", "numpy.zeros", "range", "len", "range", "torch.from_numpy().float", "len", "len", "len", "torch.from_numpy"], "function", ["None"], ["def", "get_batch", "(", "batch", ",", "word_vec", ",", "emb_dim", "=", "300", ")", ":", "\n", "# sent in batch in decreasing order of lengths (bsize, max_len, word_dim)", "\n", "    ", "lengths", "=", "np", ".", "array", "(", "[", "len", "(", "x", ")", "for", "x", "in", "batch", "]", ")", "\n", "max_len", "=", "np", ".", "max", "(", "lengths", ")", "\n", "embed", "=", "np", ".", "zeros", "(", "(", "max_len", ",", "len", "(", "batch", ")", ",", "emb_dim", ")", ")", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "batch", ")", ")", ":", "\n", "        ", "for", "j", "in", "range", "(", "len", "(", "batch", "[", "i", "]", ")", ")", ":", "\n", "            ", "embed", "[", "j", ",", "i", ",", ":", "]", "=", "word_vec", "[", "batch", "[", "i", "]", "[", "j", "]", "]", "\n", "\n", "", "", "return", "torch", ".", "from_numpy", "(", "embed", ")", ".", "float", "(", ")", ",", "lengths", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.data.get_word_dict": [[19, 30], ["sent.split"], "function", ["None"], ["", "def", "get_word_dict", "(", "sentences", ")", ":", "\n", "# create vocab of words", "\n", "    ", "word_dict", "=", "{", "}", "\n", "for", "sent", "in", "sentences", ":", "\n", "        ", "for", "word", "in", "sent", ".", "split", "(", ")", ":", "\n", "            ", "if", "word", "not", "in", "word_dict", ":", "\n", "                ", "word_dict", "[", "word", "]", "=", "''", "\n", "", "", "", "word_dict", "[", "'<s>'", "]", "=", "''", "\n", "word_dict", "[", "'</s>'", "]", "=", "''", "\n", "word_dict", "[", "'<p>'", "]", "=", "''", "\n", "return", "word_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.data.get_glove": [[32, 43], ["print", "open", "line.split", "len", "len", "numpy.array", "list", "map", "vec.split"], "function", ["None"], ["", "def", "get_glove", "(", "word_dict", ",", "glove_path", ")", ":", "\n", "# create word_vec with glove vectors", "\n", "    ", "word_vec", "=", "{", "}", "\n", "with", "open", "(", "glove_path", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "word", ",", "vec", "=", "line", ".", "split", "(", "' '", ",", "1", ")", "\n", "if", "word", "in", "word_dict", ":", "\n", "                ", "word_vec", "[", "word", "]", "=", "np", ".", "array", "(", "list", "(", "map", "(", "float", ",", "vec", ".", "split", "(", ")", ")", ")", ")", "\n", "", "", "", "print", "(", "'Found {0}(/{1}) words with glove vectors'", ".", "format", "(", "\n", "len", "(", "word_vec", ")", ",", "len", "(", "word_dict", ")", ")", ")", "\n", "return", "word_vec", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.data.build_vocab": [[45, 50], ["data.get_word_dict", "data.get_glove", "print", "len"], "function", ["home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.data.get_word_dict", "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.data.get_glove"], ["", "def", "build_vocab", "(", "sentences", ",", "glove_path", ")", ":", "\n", "    ", "word_dict", "=", "get_word_dict", "(", "sentences", ")", "\n", "word_vec", "=", "get_glove", "(", "word_dict", ",", "glove_path", ")", "\n", "print", "(", "'Vocab size : {0}'", ".", "format", "(", "len", "(", "word_vec", ")", ")", ")", "\n", "return", "word_vec", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.src.data.get_nli": [[53, 90], ["os.path.join", "os.path.join", "os.path.join", "numpy.array", "print", "line.rstrip", "line.rstrip", "len", "len", "len", "open", "open", "data_type.upper", "len", "open", "line.rstrip"], "function", ["None"], ["", "def", "get_nli", "(", "data_path", ",", "n_classes", ")", ":", "\n", "    ", "s1", "=", "{", "}", "\n", "s2", "=", "{", "}", "\n", "target", "=", "{", "}", "\n", "\n", "if", "n_classes", "==", "3", ":", "\n", "        ", "dico_label", "=", "{", "'entailment'", ":", "0", ",", "'neutral'", ":", "1", ",", "'contradiction'", ":", "2", ",", "'hidden'", ":", "0", "}", "\n", "", "else", ":", "\n", "        ", "dico_label", "=", "{", "'entailment'", ":", "0", ",", "'neutral'", ":", "1", ",", "'contradiction'", ":", "1", ",", "'hidden'", ":", "0", "}", "\n", "\n", "", "for", "data_type", "in", "[", "'train'", ",", "'dev'", ",", "'test'", "]", ":", "\n", "        ", "s1", "[", "data_type", "]", ",", "s2", "[", "data_type", "]", ",", "target", "[", "data_type", "]", "=", "{", "}", ",", "{", "}", ",", "{", "}", "\n", "s1", "[", "data_type", "]", "[", "'path'", "]", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "'s1.'", "+", "data_type", ")", "\n", "s2", "[", "data_type", "]", "[", "'path'", "]", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "'s2.'", "+", "data_type", ")", "\n", "target", "[", "data_type", "]", "[", "'path'", "]", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "\n", "'labels.'", "+", "data_type", ")", "\n", "\n", "s1", "[", "data_type", "]", "[", "'sent'", "]", "=", "[", "line", ".", "rstrip", "(", ")", "for", "line", "in", "\n", "open", "(", "s1", "[", "data_type", "]", "[", "'path'", "]", ",", "'r'", ")", "]", "\n", "s2", "[", "data_type", "]", "[", "'sent'", "]", "=", "[", "line", ".", "rstrip", "(", ")", "for", "line", "in", "\n", "open", "(", "s2", "[", "data_type", "]", "[", "'path'", "]", ",", "'r'", ")", "]", "\n", "target", "[", "data_type", "]", "[", "'data'", "]", "=", "np", ".", "array", "(", "[", "dico_label", "[", "line", ".", "rstrip", "(", "'\\n'", ")", "]", "\n", "for", "line", "in", "open", "(", "target", "[", "data_type", "]", "[", "'path'", "]", ",", "'r'", ")", "]", ")", "\n", "\n", "assert", "len", "(", "s1", "[", "data_type", "]", "[", "'sent'", "]", ")", "==", "len", "(", "s2", "[", "data_type", "]", "[", "'sent'", "]", ")", "==", "len", "(", "target", "[", "data_type", "]", "[", "'data'", "]", ")", "\n", "\n", "print", "(", "'** {0} DATA : Found {1} pairs of {2} sentences.'", ".", "format", "(", "\n", "data_type", ".", "upper", "(", ")", ",", "len", "(", "s1", "[", "data_type", "]", "[", "'sent'", "]", ")", ",", "data_type", ")", ")", "\n", "\n", "", "train", "=", "{", "'s1'", ":", "s1", "[", "'train'", "]", "[", "'sent'", "]", ",", "'s2'", ":", "s2", "[", "'train'", "]", "[", "'sent'", "]", ",", "\n", "'label'", ":", "target", "[", "'train'", "]", "[", "'data'", "]", "}", "\n", "dev", "=", "{", "'s1'", ":", "s1", "[", "'dev'", "]", "[", "'sent'", "]", ",", "'s2'", ":", "s2", "[", "'dev'", "]", "[", "'sent'", "]", ",", "\n", "'label'", ":", "target", "[", "'dev'", "]", "[", "'data'", "]", "}", "\n", "test", "=", "{", "'s1'", ":", "s1", "[", "'test'", "]", "[", "'sent'", "]", ",", "'s2'", ":", "s2", "[", "'test'", "]", "[", "'sent'", "]", ",", "\n", "'label'", ":", "target", "[", "'test'", "]", "[", "'data'", "]", "}", "\n", "return", "train", ",", "dev", ",", "test", "\n", "", ""]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.scripts.run-transfer.submit_job": [[40, 45], ["os.system", "open", "f.write", "os.path.join"], "function", ["None"], ["def", "submit_job", "(", "curr_job", ",", "filename", ",", "outpath_base", ")", ":", "\n", "    ", "job_name", "=", "\"template.job\"", "\n", "with", "open", "(", "job_name", ",", "\"w\"", ")", "as", "f", ":", "\n", "       ", "f", ".", "write", "(", "curr_job", ")", "\n", "", "os", ".", "system", "(", "\"qsub -V -N {0} -e {1}.err -o {1}.out template.job\"", ".", "format", "(", "filename", ",", "os", ".", "path", ".", "join", "(", "outpath_base", ",", "filename", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.scripts.run-finetune.submit_job": [[21, 26], ["os.system", "open", "f.write", "os.path.join"], "function", ["None"], ["def", "submit_job", "(", "curr_job", ",", "filename", ",", "outpath_base", ")", ":", "\n", "    ", "job_name", "=", "\"template.job\"", "\n", "with", "open", "(", "job_name", ",", "\"w\"", ")", "as", "f", ":", "\n", "       ", "f", ".", "write", "(", "curr_job", ")", "\n", "", "os", ".", "system", "(", "\"qsub -V -N {0} -e {1}.err -o {1}.out template.job\"", ".", "format", "(", "filename", ",", "os", ".", "path", ".", "join", "(", "outpath_base", ",", "filename", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.data.convert_recast_data.main": [[5, 74], ["glob.glob", "open", "open", "open", "open", "open", "open", "open", "open", "open", "open", "[].close", "[].close", "[].split", "line.startswith", "line.startswith", "line.startswith", "file.split", "line.startswith", "[].strip", "line.startswith", "[].strip", "line.split", "line.strip", "[].write", "[].write", "[].write", "line.split", "line.split", "line.split", "str", "[].split", "file.split"], "function", ["None"], ["def", "main", "(", ")", ":", "\n", "  ", "train_count", "=", "0", "\n", "val_count", "=", "0", "\n", "test_count", "=", "0", "\n", "input_files", "=", "glob", ".", "glob", "(", "\"./recast/*_data.txt\"", ")", "\n", "\n", "f_train_orig_data", "=", "open", "(", "\"recast/cl_train_orig_dataset_file\"", ",", "\"wb\"", ")", "\n", "f_val_orig_data", "=", "open", "(", "\"recast/cl_val_orig_dataset_file\"", ",", "\"wb\"", ")", "\n", "f_test_orig_data", "=", "open", "(", "\"recast/cl_test_orig_dataset_file\"", ",", "\"wb\"", ")", "\n", "for", "file", "in", "input_files", ":", "\n", "\n", "    ", "f_type", "=", "file", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", ".", "split", "(", "\"_\"", ")", "[", "0", "]", "\n", "f_train_lbl", "=", "open", "(", "\"recast/cl_\"", "+", "f_type", "+", "\"_train_lbl_file\"", ",", "\"wb\"", ")", "\n", "f_dev_lbl", "=", "open", "(", "\"recast/cl_\"", "+", "f_type", "+", "\"_val_lbl_file\"", ",", "\"wb\"", ")", "\n", "f_test_lbl", "=", "open", "(", "\"recast/cl_\"", "+", "f_type", "+", "\"_test_lbl_file\"", ",", "\"wb\"", ")", "\n", "\n", "f_train_source", "=", "open", "(", "\"recast/cl_\"", "+", "f_type", "+", "\"_train_source_file\"", ",", "\"wb\"", ")", "\n", "f_dev_source", "=", "open", "(", "\"recast/cl_\"", "+", "f_type", "+", "\"_val_source_file\"", ",", "\"wb\"", ")", "\n", "f_test_source", "=", "open", "(", "\"recast/cl_\"", "+", "f_type", "+", "\"_test_source_file\"", ",", "\"wb\"", ")", "\n", "\n", "\n", "out_files", "=", "{", "\"train\"", ":", "[", "f_train_lbl", ",", "f_train_source", ",", "f_train_orig_data", "]", ",", "\"dev\"", ":", "[", "f_dev_lbl", ",", "f_dev_source", ",", "f_val_orig_data", "]", ",", "\"test\"", ":", "[", "f_test_lbl", ",", "f_test_source", ",", "f_test_orig_data", "]", "}", "\n", "\n", "\n", "orig_sent", ",", "hyp_sent", ",", "data_split", ",", "src", ",", "label", "=", "None", ",", "None", ",", "None", ",", "None", ",", "None", "\n", "for", "line", "in", "open", "(", "file", ")", ":", "\n", "      ", "if", "line", ".", "startswith", "(", "\"entailed: \"", ")", ":", "\n", "        ", "label", "=", "\"entailed\"", "\n", "if", "\"not-entailed\"", "in", "line", ":", "\n", "          ", "label", "=", "\"not-entailed\"", "\n", "", "", "elif", "line", ".", "startswith", "(", "\"text: \"", ")", ":", "\n", "        ", "orig_sent", "=", "\" \"", ".", "join", "(", "line", ".", "split", "(", "\"text: \"", ")", "[", "1", ":", "]", ")", ".", "strip", "(", ")", "\n", "", "elif", "line", ".", "startswith", "(", "\"hypothesis: \"", ")", ":", "\n", "        ", "hyp_sent", "=", "\" \"", ".", "join", "(", "line", ".", "split", "(", "\"hypothesis: \"", ")", "[", "1", ":", "]", ")", ".", "strip", "(", ")", "\n", "", "elif", "line", ".", "startswith", "(", "\"partof: \"", ")", ":", "\n", "        ", "data_split", "=", "line", ".", "split", "(", "\"partof: \"", ")", "[", "-", "1", "]", ".", "strip", "(", ")", "\n", "", "elif", "line", ".", "startswith", "(", "\"provenance: \"", ")", ":", "\n", "        ", "src", "=", "line", ".", "split", "(", "\"provenance: \"", ")", "[", "-", "1", "]", ".", "strip", "(", ")", "\n", "", "elif", "not", "line", ".", "strip", "(", ")", ":", "\n", "        ", "assert", "orig_sent", "!=", "None", "\n", "assert", "hyp_sent", "!=", "None", "\n", "assert", "data_split", "!=", "None", "\n", "assert", "src", "!=", "None", "\n", "assert", "label", "!=", "None", "\n", "'''if data_split == 'train':  and train_count > 1000:\n          continue\n        elif data_split == 'train':\n          train_count += 1\n        elif data_split == 'dev' and val_count > 1000:\n          continue\n        elif data_split == 'dev':\n          val_count += 1 \n        elif data_split == 'test' and test_count > 100:\n          continue\n        elif data_split == 'test':\n          test_count += 1\n        '''", "\n", "#print orig_sent, hyp_sent, data_split, src, label", "\n", "out_files", "[", "data_split", "]", "[", "0", "]", ".", "write", "(", "str", "(", "label", ")", "+", "\"\\n\"", ")", "\n", "out_files", "[", "data_split", "]", "[", "1", "]", ".", "write", "(", "orig_sent", "+", "\"|||\"", "+", "hyp_sent", "+", "\"\\n\"", ")", "\n", "out_files", "[", "data_split", "]", "[", "2", "]", ".", "write", "(", "file", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", ".", "split", "(", "\"_\"", ")", "[", "0", "]", "+", "\"\\n\"", ")", "\n", "\n", "orig_sent", ",", "hyp_sent", ",", "data_split", ",", "src", ",", "label", "=", "None", ",", "None", ",", "None", ",", "None", ",", "None", "\n", "\n", "", "", "", "for", "data_type", "in", "out_files", ":", "\n", "    ", "out_files", "[", "data_type", "]", "[", "0", "]", ".", "close", "(", ")", "\n", "out_files", "[", "data_type", "]", "[", "1", "]", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rabeehk_robust-nli-fixed.data.convert_mpe-concat.combine_premises": [[5, 15], ["nltk.word_tokenize", "range", "row[].split"], "function", ["None"], ["def", "combine_premises", "(", "row", ")", ":", "\n", "#import pdb; pdb.set_trace()", "\n", "    ", "'''\n    Index([u'ID', u'premise1', u'premise2', u'premise3', u'premise4',\n       u'hypothesis', u'entailment_judgments', u'neutral_judgments',\n       u'contradiction_judgments', u'gold_label'],\n      dtype='object')\n    '''", "\n", "# return \" \".join([nltk.word_tokenize(row[\"premise%d\"%(i)].split('/')[1]) for i in range(1,5)])", "\n", "return", "\" \"", ".", "join", "(", "[", "\" \"", ".", "join", "(", "nltk", ".", "word_tokenize", "(", "row", "[", "\"premise%d\"", "%", "(", "i", ")", "]", ".", "split", "(", "'/'", ")", "[", "1", "]", ")", ")", "for", "i", "in", "range", "(", "1", ",", "5", ")", "]", ")", "\n", "\n"]]}