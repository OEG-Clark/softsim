{"home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.src.main.my_main": [[26, 36], ["main.config_copy", "numpy.random.seed", "torch.manual_seed", "run.run"], "function", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.src.main.config_copy", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt.seed", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner.run"], ["@", "ex", ".", "main", "\n", "def", "my_main", "(", "_run", ",", "_config", ",", "_log", ")", ":", "\n", "# Setting the random seed throughout the modules", "\n", "    ", "config", "=", "config_copy", "(", "_config", ")", "\n", "np", ".", "random", ".", "seed", "(", "config", "[", "\"seed\"", "]", ")", "\n", "th", ".", "manual_seed", "(", "config", "[", "\"seed\"", "]", ")", "\n", "config", "[", "'env_args'", "]", "[", "'seed'", "]", "=", "config", "[", "\"seed\"", "]", "\n", "\n", "# run the framework", "\n", "run", "(", "_run", ",", "config", ",", "_log", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.src.main._get_config": [[38, 53], ["enumerate", "open", "_v.split", "_v.split", "os.path.join", "yaml.load", "os.path.dirname"], "function", ["None"], ["", "def", "_get_config", "(", "params", ",", "arg_name", ",", "subfolder", ")", ":", "\n", "    ", "config_name", "=", "None", "\n", "for", "_i", ",", "_v", "in", "enumerate", "(", "params", ")", ":", "\n", "        ", "if", "_v", ".", "split", "(", "\"=\"", ")", "[", "0", "]", "==", "arg_name", ":", "\n", "            ", "config_name", "=", "_v", ".", "split", "(", "\"=\"", ")", "[", "1", "]", "\n", "del", "params", "[", "_i", "]", "\n", "break", "\n", "\n", "", "", "if", "config_name", "is", "not", "None", ":", "\n", "        ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "os", ".", "path", ".", "dirname", "(", "__file__", ")", ",", "\"config\"", ",", "subfolder", ",", "\"{}.yaml\"", ".", "format", "(", "config_name", ")", ")", ",", "\"r\"", ")", "as", "f", ":", "\n", "            ", "try", ":", "\n", "                ", "config_dict", "=", "yaml", ".", "load", "(", "f", ")", "\n", "", "except", "yaml", ".", "YAMLError", "as", "exc", ":", "\n", "                ", "assert", "False", ",", "\"{}.yaml error: {}\"", ".", "format", "(", "config_name", ",", "exc", ")", "\n", "", "", "return", "config_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.src.main.recursive_dict_update": [[55, 62], ["u.items", "isinstance", "main.recursive_dict_update", "d.get"], "function", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.src.main.recursive_dict_update"], ["", "", "def", "recursive_dict_update", "(", "d", ",", "u", ")", ":", "\n", "    ", "for", "k", ",", "v", "in", "u", ".", "items", "(", ")", ":", "\n", "        ", "if", "isinstance", "(", "v", ",", "collections", ".", "Mapping", ")", ":", "\n", "            ", "d", "[", "k", "]", "=", "recursive_dict_update", "(", "d", ".", "get", "(", "k", ",", "{", "}", ")", ",", "v", ")", "\n", "", "else", ":", "\n", "            ", "d", "[", "k", "]", "=", "v", "\n", "", "", "return", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.src.main.config_copy": [[64, 71], ["isinstance", "isinstance", "main.config_copy", "copy.deepcopy", "config.items", "main.config_copy"], "function", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.src.main.config_copy", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.src.main.config_copy"], ["", "def", "config_copy", "(", "config", ")", ":", "\n", "    ", "if", "isinstance", "(", "config", ",", "dict", ")", ":", "\n", "        ", "return", "{", "k", ":", "config_copy", "(", "v", ")", "for", "k", ",", "v", "in", "config", ".", "items", "(", ")", "}", "\n", "", "elif", "isinstance", "(", "config", ",", "list", ")", ":", "\n", "        ", "return", "[", "config_copy", "(", "v", ")", "for", "v", "in", "config", "]", "\n", "", "else", ":", "\n", "        ", "return", "deepcopy", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.src.run.run": [[21, 71], ["run.args_sanity_check", "types.SimpleNamespace", "utils.logging.Logger", "_log.info", "pprint.pformat", "_log.info", "utils.logging.Logger.setup_sacred", "run.run_sequential", "print", "print", "threading.enumerate", "print", "os._exit", "datetime.datetime.now().strftime", "os.path.join", "os.path.join().format", "utils.logging.Logger.setup_tb", "print", "pymongo_client.close", "print", "os.path.dirname", "print", "t.join", "print", "datetime.datetime.now", "os.path.dirname", "os.path.join", "os.path.abspath"], "function", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.src.run.args_sanity_check", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.setup_sacred", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.src.run.run_sequential", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.setup_tb", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt.close"], ["def", "run", "(", "_run", ",", "_config", ",", "_log", ",", "pymongo_client", "=", "None", ")", ":", "\n", "\n", "# check args sanity", "\n", "    ", "_config", "=", "args_sanity_check", "(", "_config", ",", "_log", ")", "\n", "\n", "args", "=", "SN", "(", "**", "_config", ")", "\n", "args", ".", "device", "=", "\"cuda\"", "if", "args", ".", "use_cuda", "else", "\"cpu\"", "\n", "\n", "# setup loggers", "\n", "logger", "=", "Logger", "(", "_log", ")", "\n", "\n", "_log", ".", "info", "(", "\"Experiment Parameters:\"", ")", "\n", "experiment_params", "=", "pprint", ".", "pformat", "(", "_config", ",", "\n", "indent", "=", "4", ",", "\n", "width", "=", "1", ")", "\n", "_log", ".", "info", "(", "\"\\n\\n\"", "+", "experiment_params", "+", "\"\\n\"", ")", "\n", "\n", "# configure tensorboard logger", "\n", "unique_token", "=", "\"{}__{}\"", ".", "format", "(", "args", ".", "name", ",", "datetime", ".", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "\"%Y-%m-%d_%H-%M-%S\"", ")", ")", "\n", "args", ".", "unique_token", "=", "unique_token", "\n", "if", "args", ".", "use_tensorboard", ":", "\n", "        ", "tb_logs_direc", "=", "os", ".", "path", ".", "join", "(", "dirname", "(", "dirname", "(", "abspath", "(", "__file__", ")", ")", ")", ",", "\"results\"", ",", "\"tb_logs\"", ")", "\n", "tb_exp_direc", "=", "os", ".", "path", ".", "join", "(", "tb_logs_direc", ",", "\"{}\"", ")", ".", "format", "(", "unique_token", ")", "\n", "logger", ".", "setup_tb", "(", "tb_exp_direc", ")", "\n", "\n", "# sacred is on by default", "\n", "", "logger", ".", "setup_sacred", "(", "_run", ")", "\n", "\n", "# Run and train", "\n", "run_sequential", "(", "args", "=", "args", ",", "logger", "=", "logger", ")", "\n", "\n", "# Clean up after finishing", "\n", "print", "(", "\"Exiting Main\"", ")", "\n", "\n", "if", "pymongo_client", "is", "not", "None", ":", "\n", "        ", "print", "(", "\"Attempting to close mongodb client\"", ")", "\n", "pymongo_client", ".", "close", "(", ")", "\n", "print", "(", "\"Mongodb client closed\"", ")", "\n", "\n", "", "print", "(", "\"Stopping all threads\"", ")", "\n", "for", "t", "in", "threading", ".", "enumerate", "(", ")", ":", "\n", "        ", "if", "t", ".", "name", "!=", "\"MainThread\"", ":", "\n", "            ", "print", "(", "\"Thread {} is alive! Is daemon: {}\"", ".", "format", "(", "t", ".", "name", ",", "t", ".", "daemon", ")", ")", "\n", "t", ".", "join", "(", "timeout", "=", "1", ")", "\n", "print", "(", "\"Thread joined\"", ")", "\n", "\n", "", "", "print", "(", "\"Exiting script\"", ")", "\n", "\n", "# Making sure framework really exits", "\n", "os", ".", "_exit", "(", "os", ".", "EX_OK", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.src.run.evaluate_sequential": [[73, 86], ["range", "runner.close_env", "runner.run", "torch.save", "runner.save_replay"], "function", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner.close_env", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner.run", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner.save_replay"], ["", "def", "evaluate_sequential", "(", "args", ",", "runner", ")", ":", "\n", "\n", "    ", "for", "_", "in", "range", "(", "args", ".", "test_nepisode", ")", ":", "\n", "        ", "batch", "=", "runner", ".", "run", "(", "test_mode", "=", "True", ")", "\n", "\n", "# YZ: save the last episode data", "\n", "", "if", "args", ".", "save_batch_path", "!=", "\"\"", ":", "\n", "        ", "th", ".", "save", "(", "batch", ",", "args", ".", "save_batch_path", ")", "\n", "\n", "", "if", "args", ".", "save_replay", ":", "\n", "        ", "runner", ".", "save_replay", "(", ")", "\n", "\n", "", "runner", ".", "close_env", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.src.run.run_sequential": [[87, 234], ["runner.get_env_info", "components.episode_buffer.ReplayBuffer", "runner.setup", "time.time", "logger.console_logger.info", "runner.close_env", "logger.console_logger.info", "learner.cuda", "os.listdir", "os.path.join", "logger.console_logger.info", "learner.load_models", "runner.run", "components.episode_buffer.ReplayBuffer.insert_episode_batch", "components.episode_buffer.ReplayBuffer.can_sample", "max", "os.path.isdir", "logger.console_logger.info", "os.path.join", "max", "min", "str", "range", "logger.console_logger.info", "logger.console_logger.info", "time.time", "range", "os.path.join", "os.makedirs", "logger.console_logger.info", "learner.save_models", "logger.log_stat", "logger.print_recent_stats", "components.transforms.OneHot", "os.path.isdir", "name.isdigit", "timesteps.append", "torch.load", "runner.cal_values", "torch.save", "run.evaluate_sequential", "components.episode_buffer.ReplayBuffer.sample", "buffer.sample.max_t_filled", "learner.train", "runner.run", "str", "int", "buffer.sample.to", "utils.timehelper.time_left", "utils.timehelper.time_str", "abs", "time.time"], "function", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner.get_env_info", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner.setup", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner.close_env", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.load_models", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner.run", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.ReplayBuffer.insert_episode_batch", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.ReplayBuffer.can_sample", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.save_models", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.print_recent_stats", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner.cal_values", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.src.run.evaluate_sequential", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.ReplayBuffer.sample", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.EpisodeBatch.max_t_filled", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.q_learner.QLearner.train", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner.run", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.EpisodeBatch.to", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.timehelper.time_left", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.timehelper.time_str"], ["", "def", "run_sequential", "(", "args", ",", "logger", ")", ":", "\n", "\n", "# Init runner so we can get env info", "\n", "    ", "runner", "=", "r_REGISTRY", "[", "args", ".", "runner", "]", "(", "args", "=", "args", ",", "logger", "=", "logger", ")", "\n", "\n", "# Set up schemes and groups here", "\n", "env_info", "=", "runner", ".", "get_env_info", "(", ")", "\n", "args", ".", "n_agents", "=", "env_info", "[", "\"n_agents\"", "]", "\n", "args", ".", "n_actions", "=", "env_info", "[", "\"n_actions\"", "]", "\n", "args", ".", "state_shape", "=", "env_info", "[", "\"state_shape\"", "]", "\n", "args", ".", "obs_shape", "=", "env_info", "[", "\"obs_shape\"", "]", "\n", "\n", "# Default/Base scheme", "\n", "scheme", "=", "{", "\n", "\"state\"", ":", "{", "\"vshape\"", ":", "env_info", "[", "\"state_shape\"", "]", "}", ",", "\n", "\"obs\"", ":", "{", "\"vshape\"", ":", "env_info", "[", "\"obs_shape\"", "]", ",", "\"group\"", ":", "\"agents\"", "}", ",", "\n", "\"actions\"", ":", "{", "\"vshape\"", ":", "(", "1", ",", ")", ",", "\"group\"", ":", "\"agents\"", ",", "\"dtype\"", ":", "th", ".", "long", "}", ",", "\n", "\"avail_actions\"", ":", "{", "\"vshape\"", ":", "(", "env_info", "[", "\"n_actions\"", "]", ",", ")", ",", "\"group\"", ":", "\"agents\"", ",", "\"dtype\"", ":", "th", ".", "int", "}", ",", "\n", "\"reward\"", ":", "{", "\"vshape\"", ":", "(", "1", ",", ")", "}", ",", "\n", "\"terminated\"", ":", "{", "\"vshape\"", ":", "(", "1", ",", ")", ",", "\"dtype\"", ":", "th", ".", "uint8", "}", ",", "\n", "}", "\n", "groups", "=", "{", "\n", "\"agents\"", ":", "args", ".", "n_agents", "\n", "}", "\n", "preprocess", "=", "{", "\n", "\"actions\"", ":", "(", "\"actions_onehot\"", ",", "[", "OneHot", "(", "out_dim", "=", "args", ".", "n_actions", ")", "]", ")", "\n", "}", "\n", "\n", "buffer", "=", "ReplayBuffer", "(", "scheme", ",", "groups", ",", "args", ".", "buffer_size", ",", "env_info", "[", "\"episode_limit\"", "]", "+", "1", ",", "\n", "preprocess", "=", "preprocess", ",", "\n", "device", "=", "\"cpu\"", "if", "args", ".", "buffer_cpu_only", "else", "args", ".", "device", ")", "\n", "\n", "# Setup multiagent controller here", "\n", "mac", "=", "mac_REGISTRY", "[", "args", ".", "mac", "]", "(", "buffer", ".", "scheme", ",", "groups", ",", "args", ")", "\n", "\n", "# Give runner the scheme", "\n", "runner", ".", "setup", "(", "scheme", "=", "scheme", ",", "groups", "=", "groups", ",", "preprocess", "=", "preprocess", ",", "mac", "=", "mac", ")", "\n", "\n", "# Learner", "\n", "learner", "=", "le_REGISTRY", "[", "args", ".", "learner", "]", "(", "mac", ",", "buffer", ".", "scheme", ",", "logger", ",", "args", ")", "\n", "\n", "if", "args", ".", "use_cuda", ":", "\n", "        ", "learner", ".", "cuda", "(", ")", "\n", "\n", "", "if", "args", ".", "checkpoint_path", "!=", "\"\"", ":", "\n", "\n", "        ", "timesteps", "=", "[", "]", "\n", "timestep_to_load", "=", "0", "\n", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "args", ".", "checkpoint_path", ")", ":", "\n", "            ", "logger", ".", "console_logger", ".", "info", "(", "\"Checkpoint directiory {} doesn't exist\"", ".", "format", "(", "args", ".", "checkpoint_path", ")", ")", "\n", "return", "\n", "\n", "# Go through all files in args.checkpoint_path", "\n", "", "for", "name", "in", "os", ".", "listdir", "(", "args", ".", "checkpoint_path", ")", ":", "\n", "            ", "full_name", "=", "os", ".", "path", ".", "join", "(", "args", ".", "checkpoint_path", ",", "name", ")", "\n", "# Check if they are dirs the names of which are numbers", "\n", "if", "os", ".", "path", ".", "isdir", "(", "full_name", ")", "and", "name", ".", "isdigit", "(", ")", ":", "\n", "                ", "timesteps", ".", "append", "(", "int", "(", "name", ")", ")", "\n", "\n", "", "", "if", "args", ".", "load_step", "==", "0", ":", "\n", "# choose the max timestep", "\n", "            ", "timestep_to_load", "=", "max", "(", "timesteps", ")", "\n", "", "else", ":", "\n", "# choose the timestep closest to load_step", "\n", "            ", "timestep_to_load", "=", "min", "(", "timesteps", ",", "key", "=", "lambda", "x", ":", "abs", "(", "x", "-", "args", ".", "load_step", ")", ")", "\n", "\n", "", "model_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "checkpoint_path", ",", "str", "(", "timestep_to_load", ")", ")", "\n", "\n", "logger", ".", "console_logger", ".", "info", "(", "\"Loading model from {}\"", ".", "format", "(", "model_path", ")", ")", "\n", "learner", ".", "load_models", "(", "model_path", ")", "\n", "runner", ".", "t_env", "=", "timestep_to_load", "\n", "\n", "if", "args", ".", "evaluate", "or", "args", ".", "save_replay", ":", "\n", "\n", "# YZ: save the last episode data", "\n", "            ", "if", "args", ".", "load_batch_path", "!=", "\"\"", ":", "\n", "                ", "batch", "=", "th", ".", "load", "(", "args", ".", "load_batch_path", ")", "\n", "values", "=", "runner", ".", "cal_values", "(", "batch", ")", "\n", "th", ".", "save", "(", "values", ",", "args", ".", "save_values_path", ")", "\n", "", "else", ":", "\n", "                ", "evaluate_sequential", "(", "args", ",", "runner", ")", "\n", "", "return", "\n", "\n", "# start training", "\n", "", "", "episode", "=", "0", "\n", "last_test_T", "=", "-", "args", ".", "test_interval", "-", "1", "\n", "last_log_T", "=", "0", "\n", "model_save_time", "=", "0", "\n", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "last_time", "=", "start_time", "\n", "\n", "logger", ".", "console_logger", ".", "info", "(", "\"Beginning training for {} timesteps\"", ".", "format", "(", "args", ".", "t_max", ")", ")", "\n", "\n", "while", "runner", ".", "t_env", "<=", "args", ".", "t_max", ":", "\n", "\n", "# Run for a whole episode at a time", "\n", "        ", "episode_batch", "=", "runner", ".", "run", "(", "test_mode", "=", "False", ")", "\n", "buffer", ".", "insert_episode_batch", "(", "episode_batch", ")", "\n", "\n", "if", "buffer", ".", "can_sample", "(", "args", ".", "batch_size", ")", ":", "\n", "            ", "for", "_", "in", "range", "(", "args", ".", "training_iters", ")", ":", "\n", "                ", "episode_sample", "=", "buffer", ".", "sample", "(", "args", ".", "batch_size", ")", "\n", "\n", "# Truncate batch to only filled timesteps", "\n", "max_ep_t", "=", "episode_sample", ".", "max_t_filled", "(", ")", "\n", "episode_sample", "=", "episode_sample", "[", ":", ",", ":", "max_ep_t", "]", "\n", "\n", "if", "episode_sample", ".", "device", "!=", "args", ".", "device", ":", "\n", "                    ", "episode_sample", ".", "to", "(", "args", ".", "device", ")", "\n", "\n", "", "learner", ".", "train", "(", "episode_sample", ",", "runner", ".", "t_env", ",", "episode", ")", "\n", "\n", "# Execute test runs once in a while", "\n", "", "", "n_test_runs", "=", "max", "(", "1", ",", "args", ".", "test_nepisode", "//", "runner", ".", "batch_size", ")", "\n", "if", "(", "runner", ".", "t_env", "-", "last_test_T", ")", "/", "args", ".", "test_interval", ">=", "1.0", ":", "\n", "\n", "            ", "logger", ".", "console_logger", ".", "info", "(", "\"t_env: {} / {}\"", ".", "format", "(", "runner", ".", "t_env", ",", "args", ".", "t_max", ")", ")", "\n", "logger", ".", "console_logger", ".", "info", "(", "\"Estimated time left: {}. Time passed: {}\"", ".", "format", "(", "\n", "time_left", "(", "last_time", ",", "last_test_T", ",", "runner", ".", "t_env", ",", "args", ".", "t_max", ")", ",", "time_str", "(", "time", ".", "time", "(", ")", "-", "start_time", ")", ")", ")", "\n", "last_time", "=", "time", ".", "time", "(", ")", "\n", "\n", "last_test_T", "=", "runner", ".", "t_env", "\n", "for", "_", "in", "range", "(", "n_test_runs", ")", ":", "\n", "                ", "runner", ".", "run", "(", "test_mode", "=", "True", ")", "\n", "\n", "", "", "if", "args", ".", "save_model", "and", "(", "runner", ".", "t_env", "-", "model_save_time", ">=", "args", ".", "save_model_interval", "or", "model_save_time", "==", "0", ")", ":", "\n", "            ", "model_save_time", "=", "runner", ".", "t_env", "\n", "save_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "local_results_path", ",", "\"models\"", ",", "args", ".", "unique_token", ",", "str", "(", "runner", ".", "t_env", ")", ")", "\n", "#\"results/models/{}\".format(unique_token)", "\n", "os", ".", "makedirs", "(", "save_path", ",", "exist_ok", "=", "True", ")", "\n", "logger", ".", "console_logger", ".", "info", "(", "\"Saving models to {}\"", ".", "format", "(", "save_path", ")", ")", "\n", "\n", "# learner should handle saving/loading -- delegate actor save/load to mac,", "\n", "# use appropriate filenames to do critics, optimizer states", "\n", "learner", ".", "save_models", "(", "save_path", ")", "\n", "\n", "", "episode", "+=", "args", ".", "batch_size_run", "\n", "\n", "if", "(", "runner", ".", "t_env", "-", "last_log_T", ")", ">=", "args", ".", "log_interval", ":", "\n", "            ", "logger", ".", "log_stat", "(", "\"episode\"", ",", "episode", ",", "runner", ".", "t_env", ")", "\n", "logger", ".", "print_recent_stats", "(", ")", "\n", "last_log_T", "=", "runner", ".", "t_env", "\n", "\n", "", "", "runner", ".", "close_env", "(", ")", "\n", "logger", ".", "console_logger", ".", "info", "(", "\"Finished Training\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.src.run.args_sanity_check": [[237, 261], ["_log.warning", "torch.cuda.is_available"], "function", ["None"], ["", "def", "args_sanity_check", "(", "config", ",", "_log", ")", ":", "\n", "\n", "# set CUDA flags", "\n", "# config[\"use_cuda\"] = True # Use cuda whenever possible!", "\n", "    ", "if", "config", "[", "\"use_cuda\"", "]", "and", "not", "th", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "        ", "config", "[", "\"use_cuda\"", "]", "=", "False", "\n", "_log", ".", "warning", "(", "\"CUDA flag use_cuda was switched OFF automatically because no CUDA devices are available!\"", ")", "\n", "\n", "", "if", "config", "[", "\"test_nepisode\"", "]", "<", "config", "[", "\"batch_size_run\"", "]", ":", "\n", "        ", "config", "[", "\"test_nepisode\"", "]", "=", "config", "[", "\"batch_size_run\"", "]", "\n", "", "else", ":", "\n", "        ", "config", "[", "\"test_nepisode\"", "]", "=", "(", "config", "[", "\"test_nepisode\"", "]", "//", "config", "[", "\"batch_size_run\"", "]", ")", "*", "config", "[", "\"batch_size_run\"", "]", "\n", "\n", "# assert (config[\"run_mode\"] in [\"parallel_subproc\"] and config[\"use_replay_buffer\"]) or (not config[\"run_mode\"] in [\"parallel_subproc\"]),  \\", "\n", "#     \"need to use replay buffer if running in parallel mode!\"", "\n", "\n", "# assert not (not config[\"use_replay_buffer\"] and (config[\"batch_size_run\"]!=config[\"batch_size\"]) ) , \"if not using replay buffer, require batch_size and batch_size_run to be the same.\"", "\n", "\n", "# if config[\"learner\"] == \"coma\":", "\n", "#    assert (config[\"run_mode\"] in [\"parallel_subproc\"]  and config[\"batch_size_run\"]==config[\"batch_size\"]) or \\", "\n", "#    (not config[\"run_mode\"] in [\"parallel_subproc\"]  and not config[\"use_replay_buffer\"]), \\", "\n", "#        \"cannot use replay buffer for coma, unless in parallel mode, when it needs to have exactly have size batch_size.\"", "\n", "\n", "", "return", "config", "\n", "", ""]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.rl_utils.build_td_lambda_targets__old": [[4, 20], ["rewards.size", "rewards.size", "rewards.new().zero_", "terminated.float.float", "reversed", "rewards.new().zero_", "range", "rewards.new", "rewards.new", "target_qs.size"], "function", ["None"], ["def", "build_td_lambda_targets__old", "(", "rewards", ",", "terminated", ",", "mask", ",", "target_qs", ",", "n_agents", ",", "gamma", ",", "td_lambda", ")", ":", "\n", "    ", "bs", "=", "rewards", ".", "size", "(", "0", ")", "\n", "max_t", "=", "rewards", ".", "size", "(", "1", ")", "\n", "targets", "=", "rewards", ".", "new", "(", "target_qs", ".", "size", "(", ")", ")", ".", "zero_", "(", ")", "[", ":", ",", ":", "-", "1", "]", "# Produce 1 less target than the inputted Q-Values", "\n", "running_target", "=", "rewards", ".", "new", "(", "bs", ",", "n_agents", ")", ".", "zero_", "(", ")", "\n", "terminated", "=", "terminated", ".", "float", "(", ")", "\n", "for", "t", "in", "reversed", "(", "range", "(", "max_t", ")", ")", ":", "\n", "        ", "if", "t", "==", "max_t", "-", "1", ":", "\n", "            ", "running_target", "=", "mask", "[", ":", ",", "t", "]", "*", "(", "rewards", "[", ":", ",", "t", "]", "+", "gamma", "*", "(", "1", "-", "terminated", "[", ":", ",", "t", "]", ")", "*", "target_qs", "[", ":", ",", "t", "]", ")", "\n", "", "else", ":", "\n", "            ", "running_target", "=", "mask", "[", ":", ",", "t", "]", "*", "(", "\n", "terminated", "[", ":", ",", "t", "]", "*", "rewards", "[", ":", ",", "t", "]", "# Just the reward if the env terminates", "\n", "+", "(", "1", "-", "terminated", "[", ":", ",", "t", "]", ")", "*", "(", "rewards", "[", ":", ",", "t", "]", "+", "gamma", "*", "(", "td_lambda", "*", "running_target", "+", "(", "1", "-", "td_lambda", ")", "*", "target_qs", "[", ":", ",", "t", "]", ")", ")", "\n", ")", "\n", "", "targets", "[", ":", ",", "t", ",", ":", "]", "=", "running_target", "\n", "", "return", "targets", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.rl_utils.build_td_lambda_targets": [[22, 33], ["target_qs.new_zeros", "range", "torch.sum"], "function", ["None"], ["", "def", "build_td_lambda_targets", "(", "rewards", ",", "terminated", ",", "mask", ",", "target_qs", ",", "n_agents", ",", "gamma", ",", "td_lambda", ")", ":", "\n", "# Assumes  <target_qs > in B*T*A and <reward >, <terminated >, <mask > in (at least) B*T-1*1", "\n", "# Initialise  last  lambda -return  for  not  terminated  episodes", "\n", "    ", "ret", "=", "target_qs", ".", "new_zeros", "(", "*", "target_qs", ".", "shape", ")", "\n", "ret", "[", ":", ",", "-", "1", "]", "=", "target_qs", "[", ":", ",", "-", "1", "]", "*", "(", "1", "-", "th", ".", "sum", "(", "terminated", ",", "dim", "=", "1", ")", ")", "\n", "# Backwards  recursive  update  of the \"forward  view\"", "\n", "for", "t", "in", "range", "(", "ret", ".", "shape", "[", "1", "]", "-", "2", ",", "-", "1", ",", "-", "1", ")", ":", "\n", "        ", "ret", "[", ":", ",", "t", "]", "=", "td_lambda", "*", "gamma", "*", "ret", "[", ":", ",", "t", "+", "1", "]", "+", "mask", "[", ":", ",", "t", "]", "*", "(", "rewards", "[", ":", ",", "t", "]", "+", "(", "1", "-", "td_lambda", ")", "*", "gamma", "*", "target_qs", "[", ":", ",", "t", "+", "1", "]", "*", "(", "1", "-", "terminated", "[", ":", ",", "t", "]", ")", ")", "\n", "# Returns lambda-return from t=0 to t=T-1, i.e. in B*T-1*A", "\n", "", "return", "ret", "[", ":", ",", "0", ":", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.__init__": [[6, 14], ["collections.defaultdict"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "console_logger", ")", ":", "\n", "        ", "self", ".", "console_logger", "=", "console_logger", "\n", "\n", "self", ".", "use_tb", "=", "False", "\n", "self", ".", "use_sacred", "=", "False", "\n", "self", ".", "use_hdf", "=", "False", "\n", "\n", "self", ".", "stats", "=", "defaultdict", "(", "lambda", ":", "[", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.setup_tb": [[15, 21], ["configure"], "methods", ["None"], ["", "def", "setup_tb", "(", "self", ",", "directory_name", ")", ":", "\n", "# Import here so it doesn't have to be installed if you don't use it", "\n", "        ", "from", "tensorboard_logger", "import", "configure", ",", "log_value", "\n", "configure", "(", "directory_name", ")", "\n", "self", ".", "tb_logger", "=", "log_value", "\n", "self", ".", "use_tb", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.setup_sacred": [[22, 25], ["None"], "methods", ["None"], ["", "def", "setup_sacred", "(", "self", ",", "sacred_run_dict", ")", ":", "\n", "        ", "self", ".", "sacred_info", "=", "sacred_run_dict", ".", "info", "\n", "self", ".", "use_sacred", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat": [[28, 41], ["logging.Logger.stats[].append", "logging.Logger.tb_logger", "logging.Logger.sacred_info[].append", "logging.Logger.sacred_info[].append"], "methods", ["None"], ["", "def", "log_stat", "(", "self", ",", "key", ",", "value", ",", "t", ",", "to_sacred", "=", "True", ")", ":", "\n", "        ", "self", ".", "stats", "[", "key", "]", ".", "append", "(", "(", "t", ",", "value", ")", ")", "\n", "\n", "if", "self", ".", "use_tb", ":", "\n", "            ", "self", ".", "tb_logger", "(", "key", ",", "value", ",", "t", ")", "\n", "\n", "", "if", "self", ".", "use_sacred", "and", "to_sacred", ":", "\n", "            ", "if", "key", "in", "self", ".", "sacred_info", ":", "\n", "                ", "self", ".", "sacred_info", "[", "\"{}_T\"", ".", "format", "(", "key", ")", "]", ".", "append", "(", "t", ")", "\n", "self", ".", "sacred_info", "[", "key", "]", ".", "append", "(", "value", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "sacred_info", "[", "\"{}_T\"", ".", "format", "(", "key", ")", "]", "=", "[", "t", "]", "\n", "self", ".", "sacred_info", "[", "key", "]", "=", "[", "value", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.print_recent_stats": [[42, 54], ["sorted", "logging.Logger.console_logger.info", "logging.Logger.stats.items", "numpy.mean"], "methods", ["None"], ["", "", "", "def", "print_recent_stats", "(", "self", ")", ":", "\n", "        ", "log_str", "=", "\"Recent Stats | t_env: {:>10} | Episode: {:>8}\\n\"", ".", "format", "(", "*", "self", ".", "stats", "[", "\"episode\"", "]", "[", "-", "1", "]", ")", "\n", "i", "=", "0", "\n", "for", "(", "k", ",", "v", ")", "in", "sorted", "(", "self", ".", "stats", ".", "items", "(", ")", ")", ":", "\n", "            ", "if", "k", "==", "\"episode\"", ":", "\n", "                ", "continue", "\n", "", "i", "+=", "1", "\n", "window", "=", "3", "if", "(", "k", "not", "in", "[", "\"epsilon\"", "]", ")", "else", "1", "\n", "item", "=", "\"{:.4f}\"", ".", "format", "(", "np", ".", "mean", "(", "[", "x", "[", "1", "]", "for", "x", "in", "self", ".", "stats", "[", "k", "]", "[", "-", "window", ":", "]", "]", ")", ")", "\n", "log_str", "+=", "\"{:<25}{:>8}\"", ".", "format", "(", "k", "+", "\":\"", ",", "item", ")", "\n", "log_str", "+=", "\"\\n\"", "if", "i", "%", "4", "==", "0", "else", "\"\\t\"", "\n", "", "self", ".", "console_logger", ".", "info", "(", "log_str", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.get_logger": [[56, 66], ["logging.getLogger", "logging.StreamHandler", "logging.Formatter", "logging.StreamHandler.setFormatter", "logging.getLogger.addHandler", "logging.getLogger.setLevel"], "function", ["None"], ["", "", "def", "get_logger", "(", ")", ":", "\n", "    ", "logger", "=", "logging", ".", "getLogger", "(", ")", "\n", "logger", ".", "handlers", "=", "[", "]", "\n", "ch", "=", "logging", ".", "StreamHandler", "(", ")", "\n", "formatter", "=", "logging", ".", "Formatter", "(", "'[%(levelname)s %(asctime)s] %(name)s %(message)s'", ",", "'%H:%M:%S'", ")", "\n", "ch", ".", "setFormatter", "(", "formatter", ")", "\n", "logger", ".", "addHandler", "(", "ch", ")", "\n", "logger", ".", "setLevel", "(", "'DEBUG'", ")", "\n", "\n", "return", "logger", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.dict2namedtuple.convert": [[2, 4], ["collections.namedtuple", "dictionary.keys"], "function", ["None"], ["def", "convert", "(", "dictionary", ")", ":", "\n", "    ", "return", "namedtuple", "(", "'GenericDict'", ",", "dictionary", ".", "keys", "(", ")", ")", "(", "**", "dictionary", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.timehelper.print_time": [[5, 15], ["max", "min", "print", "time.time", "len", "numpy.mean", "timehelper.time_str", "timehelper.time_str"], "function", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.timehelper.time_str", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.timehelper.time_str"], ["def", "print_time", "(", "start_time", ",", "T", ",", "t_max", ",", "episode", ",", "episode_rewards", ")", ":", "\n", "    ", "time_elapsed", "=", "time", ".", "time", "(", ")", "-", "start_time", "\n", "T", "=", "max", "(", "1", ",", "T", ")", "\n", "time_left", "=", "time_elapsed", "*", "(", "t_max", "-", "T", ")", "/", "T", "\n", "# Just in case its over 100 days", "\n", "time_left", "=", "min", "(", "time_left", ",", "60", "*", "60", "*", "24", "*", "100", ")", "\n", "last_reward", "=", "\"N\\A\"", "\n", "if", "len", "(", "episode_rewards", ")", ">", "5", ":", "\n", "        ", "last_reward", "=", "\"{:.2f}\"", ".", "format", "(", "np", ".", "mean", "(", "episode_rewards", "[", "-", "50", ":", "]", ")", ")", "\n", "", "print", "(", "\"\\033[F\\033[F\\x1b[KEp: {:,}, T: {:,}/{:,}, Reward: {}, \\n\\x1b[KElapsed: {}, Left: {}\\n\"", ".", "format", "(", "episode", ",", "T", ",", "t_max", ",", "last_reward", ",", "time_str", "(", "time_elapsed", ")", ",", "time_str", "(", "time_left", ")", ")", ",", "\" \"", "*", "10", ",", "end", "=", "\"\\r\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.timehelper.time_left": [[16, 25], ["max", "min", "timehelper.time_str", "time.time"], "function", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.timehelper.time_str"], ["", "def", "time_left", "(", "start_time", ",", "t_start", ",", "t_current", ",", "t_max", ")", ":", "\n", "    ", "if", "t_current", ">=", "t_max", ":", "\n", "        ", "return", "\"-\"", "\n", "", "time_elapsed", "=", "time", ".", "time", "(", ")", "-", "start_time", "\n", "t_current", "=", "max", "(", "1", ",", "t_current", ")", "\n", "time_left", "=", "time_elapsed", "*", "(", "t_max", "-", "t_current", ")", "/", "(", "t_current", "-", "t_start", ")", "\n", "# Just in case its over 100 days", "\n", "time_left", "=", "min", "(", "time_left", ",", "60", "*", "60", "*", "24", "*", "100", ")", "\n", "return", "time_str", "(", "time_left", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.timehelper.time_str": [[26, 42], ["divmod", "divmod", "divmod", "int", "int", "int", "int", "timehelper.time_left", "timehelper.time_left"], "function", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.timehelper.time_left", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.timehelper.time_left"], ["", "def", "time_str", "(", "s", ")", ":", "\n", "    ", "\"\"\"\n    Convert seconds to a nicer string showing days, hours, minutes and seconds\n    \"\"\"", "\n", "days", ",", "remainder", "=", "divmod", "(", "s", ",", "60", "*", "60", "*", "24", ")", "\n", "hours", ",", "remainder", "=", "divmod", "(", "remainder", ",", "60", "*", "60", ")", "\n", "minutes", ",", "seconds", "=", "divmod", "(", "remainder", ",", "60", ")", "\n", "string", "=", "\"\"", "\n", "if", "days", ">", "0", ":", "\n", "        ", "string", "+=", "\"{:d} days, \"", ".", "format", "(", "int", "(", "days", ")", ")", "\n", "", "if", "hours", ">", "0", ":", "\n", "        ", "string", "+=", "\"{:d} hours, \"", ".", "format", "(", "int", "(", "hours", ")", ")", "\n", "", "if", "minutes", ">", "0", ":", "\n", "        ", "string", "+=", "\"{:d} minutes, \"", ".", "format", "(", "int", "(", "minutes", ")", ")", "\n", "", "string", "+=", "\"{:d} seconds\"", ".", "format", "(", "int", "(", "seconds", ")", ")", "\n", "return", "string", "\n", "", ""]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.transforms.Transform.transform": [[5, 7], ["None"], "methods", ["None"], ["    ", "def", "transform", "(", "self", ",", "tensor", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.transforms.Transform.infer_output_info": [[8, 10], ["None"], "methods", ["None"], ["", "def", "infer_output_info", "(", "self", ",", "vshape_in", ",", "dtype_in", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.transforms.OneHot.__init__": [[12, 14], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "out_dim", ")", ":", "\n", "        ", "self", ".", "out_dim", "=", "out_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.transforms.OneHot.transform": [[15, 19], ["tensor.new().zero_", "tensor.new().zero_.scatter_", "tensor.new().zero_.float", "tensor.long", "tensor.new"], "methods", ["None"], ["", "def", "transform", "(", "self", ",", "tensor", ")", ":", "\n", "        ", "y_onehot", "=", "tensor", ".", "new", "(", "*", "tensor", ".", "shape", "[", ":", "-", "1", "]", ",", "self", ".", "out_dim", ")", ".", "zero_", "(", ")", "\n", "y_onehot", ".", "scatter_", "(", "-", "1", ",", "tensor", ".", "long", "(", ")", ",", "1", ")", "\n", "return", "y_onehot", ".", "float", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.transforms.OneHot.infer_output_info": [[20, 24], ["None"], "methods", ["None"], ["", "def", "infer_output_info", "(", "self", ",", "vshape_in", ",", "dtype_in", ")", ":", "\n", "# TODO: Check this shouldn't be here", "\n", "# assert vshape_in == (1,)", "\n", "        ", "return", "(", "self", ".", "out_dim", ",", ")", ",", "th", ".", "float32", "", "", "", ""]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.action_selectors.MultinomialActionSelector.__init__": [[12, 19], ["epsilon_schedules.DecayThenFlatSchedule", "action_selectors.MultinomialActionSelector.schedule.eval", "getattr"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.epsilon_schedules.DecayThenFlatSchedule.eval"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "self", ".", "args", "=", "args", "\n", "\n", "self", ".", "schedule", "=", "DecayThenFlatSchedule", "(", "args", ".", "epsilon_start", ",", "args", ".", "epsilon_finish", ",", "args", ".", "epsilon_anneal_time", ",", "\n", "decay", "=", "\"linear\"", ")", "\n", "self", ".", "epsilon", "=", "self", ".", "schedule", ".", "eval", "(", "0", ")", "\n", "self", ".", "test_greedy", "=", "getattr", "(", "args", ",", "\"test_greedy\"", ",", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.action_selectors.MultinomialActionSelector.select_action": [[20, 38], ["agent_inputs.clone", "action_selectors.MultinomialActionSelector.schedule.eval", "float", "torch.distributions.Categorical().sample().long", "agent_inputs.clone.max", "torch.distributions.Categorical().sample", "torch.distributions.Categorical"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.epsilon_schedules.DecayThenFlatSchedule.eval", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.ReplayBuffer.sample"], ["", "def", "select_action", "(", "self", ",", "agent_inputs", ",", "avail_actions", ",", "t_env", ",", "test_mode", "=", "False", ")", ":", "\n", "# assume the input is logits", "\n", "        ", "masked_policies", "=", "agent_inputs", ".", "clone", "(", ")", "\n", "# fix a bug over sample()", "\n", "masked_policies", "[", "avail_actions", "==", "0.0", "]", "=", "-", "float", "(", "\"inf\"", ")", "\n", "\n", "self", ".", "epsilon", "=", "self", ".", "schedule", ".", "eval", "(", "t_env", ")", "\n", "\n", "if", "test_mode", "and", "self", ".", "test_greedy", ":", "\n", "            ", "picked_actions", "=", "masked_policies", ".", "max", "(", "dim", "=", "2", ")", "[", "1", "]", "\n", "", "else", ":", "\n", "# noise = th.rand_like(masked_policies) * 0.1", "\n", "# noise[avail_actions == 0.0] = -float(\"inf\")", "\n", "# masked_policies += noise", "\n", "# fix the bug of categorical distribution in pytorch", "\n", "            ", "picked_actions", "=", "Categorical", "(", "logits", "=", "masked_policies", ")", ".", "sample", "(", ")", ".", "long", "(", ")", "\n", "\n", "", "return", "picked_actions", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.action_selectors.EpsilonGreedyActionSelector.__init__": [[44, 50], ["epsilon_schedules.DecayThenFlatSchedule", "action_selectors.EpsilonGreedyActionSelector.schedule.eval"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.epsilon_schedules.DecayThenFlatSchedule.eval"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "self", ".", "args", "=", "args", "\n", "\n", "# Was there so I used it", "\n", "self", ".", "schedule", "=", "DecayThenFlatSchedule", "(", "args", ".", "epsilon_start", ",", "args", ".", "epsilon_finish", ",", "args", ".", "epsilon_anneal_time", ",", "decay", "=", "\"linear\"", ")", "\n", "self", ".", "epsilon", "=", "self", ".", "schedule", ".", "eval", "(", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.action_selectors.EpsilonGreedyActionSelector.select_action": [[51, 72], ["action_selectors.EpsilonGreedyActionSelector.schedule.eval", "agent_inputs.clone", "torch.rand_like", "torch.distributions.Categorical().sample().long", "float", "torch.distributions.Categorical().sample", "agent_inputs.clone.max", "torch.distributions.Categorical", "avail_actions.float"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.epsilon_schedules.DecayThenFlatSchedule.eval", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.ReplayBuffer.sample"], ["", "def", "select_action", "(", "self", ",", "agent_inputs", ",", "avail_actions", ",", "t_env", ",", "test_mode", "=", "False", ")", ":", "\n", "\n", "# Assuming agent_inputs is a batch of Q-Values for each agent bav", "\n", "        ", "self", ".", "epsilon", "=", "self", ".", "schedule", ".", "eval", "(", "t_env", ")", "\n", "\n", "if", "test_mode", ":", "\n", "# Greedy action selection only", "\n", "            ", "self", ".", "epsilon", "=", "0.0", "\n", "if", "self", ".", "args", ".", "epsilon_test", ":", "\n", "                ", "self", ".", "epsilon", "=", "self", ".", "args", ".", "epsilon_test", "\n", "\n", "# mask actions that are excluded from selection", "\n", "", "", "masked_q_values", "=", "agent_inputs", ".", "clone", "(", ")", "\n", "masked_q_values", "[", "avail_actions", "==", "0.0", "]", "=", "-", "float", "(", "\"inf\"", ")", "# should never be selected!", "\n", "\n", "random_numbers", "=", "th", ".", "rand_like", "(", "agent_inputs", "[", ":", ",", ":", ",", "0", "]", ")", "\n", "pick_random", "=", "(", "random_numbers", "<", "self", ".", "epsilon", ")", ".", "long", "(", ")", "\n", "random_actions", "=", "Categorical", "(", "avail_actions", ".", "float", "(", ")", ")", ".", "sample", "(", ")", ".", "long", "(", ")", "\n", "\n", "picked_actions", "=", "pick_random", "*", "random_actions", "+", "(", "1", "-", "pick_random", ")", "*", "masked_q_values", ".", "max", "(", "dim", "=", "2", ")", "[", "1", "]", "\n", "return", "picked_actions", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.action_selectors.PolicyEpsilonGreedyActionSelector.__init__": [[77, 83], ["epsilon_schedules.DecayThenFlatSchedule", "action_selectors.PolicyEpsilonGreedyActionSelector.schedule.eval"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.epsilon_schedules.DecayThenFlatSchedule.eval"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "self", ".", "args", "=", "args", "\n", "\n", "# Was there so I used it", "\n", "self", ".", "schedule", "=", "DecayThenFlatSchedule", "(", "args", ".", "epsilon_start", ",", "args", ".", "epsilon_finish", ",", "args", ".", "epsilon_anneal_time", ",", "decay", "=", "\"linear\"", ")", "\n", "self", ".", "epsilon", "=", "self", ".", "schedule", ".", "eval", "(", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.action_selectors.PolicyEpsilonGreedyActionSelector.select_action": [[84, 107], ["action_selectors.PolicyEpsilonGreedyActionSelector.schedule.eval", "torch.rand_like", "torch.distributions.Categorical().sample().long", "agent_pis.clone", "agent_pis.clone.argmax", "float", "torch.distributions.Categorical().sample", "torch.distributions.Categorical", "avail_actions.float"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.epsilon_schedules.DecayThenFlatSchedule.eval", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.ReplayBuffer.sample"], ["", "def", "select_action", "(", "self", ",", "agent_qs", ",", "agent_pis", ",", "avail_actions", ",", "t_env", ",", "test_mode", "=", "False", ")", ":", "\n", "\n", "# Assuming agent_inputs is a batch of Q-Values for each agent bav", "\n", "        ", "self", ".", "epsilon", "=", "self", ".", "schedule", ".", "eval", "(", "t_env", ")", "\n", "\n", "if", "test_mode", ":", "\n", "# Greedy action selection only", "\n", "            ", "self", ".", "epsilon", "=", "0.0", "\n", "\n", "# mask actions that are excluded from selection", "\n", "# masked_q_values = agent_qs.clone()", "\n", "# masked_q_values[avail_actions == 0.0] = -float(\"inf\")  # should never be selected!", "\n", "\n", "", "random_numbers", "=", "th", ".", "rand_like", "(", "agent_qs", "[", ":", ",", ":", ",", "0", "]", ")", "\n", "pick_random", "=", "(", "random_numbers", "<", "self", ".", "epsilon", ")", ".", "long", "(", ")", "\n", "random_actions", "=", "Categorical", "(", "avail_actions", ".", "float", "(", ")", ")", ".", "sample", "(", ")", ".", "long", "(", ")", "\n", "\n", "# max_action = th.abs(masked_q_values - agent_pis).argmin(dim=2)", "\n", "masked_agent_pis", "=", "agent_pis", ".", "clone", "(", ")", "\n", "masked_agent_pis", "[", "avail_actions", "==", "0.0", "]", "=", "-", "float", "(", "\"inf\"", ")", "\n", "max_action", "=", "masked_agent_pis", ".", "argmax", "(", "dim", "=", "2", ")", "\n", "picked_actions", "=", "pick_random", "*", "random_actions", "+", "(", "1", "-", "pick_random", ")", "*", "max_action", "\n", "return", "picked_actions", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.epsilon_schedules.DecayThenFlatSchedule.__init__": [[5, 19], ["numpy.log"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "\n", "start", ",", "\n", "finish", ",", "\n", "time_length", ",", "\n", "decay", "=", "\"exp\"", ")", ":", "\n", "\n", "        ", "self", ".", "start", "=", "start", "\n", "self", ".", "finish", "=", "finish", "\n", "self", ".", "time_length", "=", "time_length", "\n", "self", ".", "delta", "=", "(", "self", ".", "start", "-", "self", ".", "finish", ")", "/", "self", ".", "time_length", "\n", "self", ".", "decay", "=", "decay", "\n", "\n", "if", "self", ".", "decay", "in", "[", "\"exp\"", "]", ":", "\n", "            ", "self", ".", "exp_scaling", "=", "(", "-", "1", ")", "*", "self", ".", "time_length", "/", "np", ".", "log", "(", "self", ".", "finish", ")", "if", "self", ".", "finish", ">", "0", "else", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.epsilon_schedules.DecayThenFlatSchedule.eval": [[20, 25], ["max", "min", "max", "numpy.exp"], "methods", ["None"], ["", "", "def", "eval", "(", "self", ",", "T", ")", ":", "\n", "        ", "if", "self", ".", "decay", "in", "[", "\"linear\"", "]", ":", "\n", "            ", "return", "max", "(", "self", ".", "finish", ",", "self", ".", "start", "-", "self", ".", "delta", "*", "T", ")", "\n", "", "elif", "self", ".", "decay", "in", "[", "\"exp\"", "]", ":", "\n", "            ", "return", "min", "(", "self", ".", "start", ",", "max", "(", "self", ".", "finish", ",", "np", ".", "exp", "(", "-", "T", "/", "self", ".", "exp_scaling", ")", ")", ")", "\n", "", "", "pass", ""]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.EpisodeBatch.__init__": [[7, 29], ["scheme.copy", "types.SimpleNamespace", "episode_buffer.EpisodeBatch._setup_data"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.EpisodeBatch._setup_data"], ["    ", "def", "__init__", "(", "self", ",", "\n", "scheme", ",", "\n", "groups", ",", "\n", "batch_size", ",", "\n", "max_seq_length", ",", "\n", "data", "=", "None", ",", "\n", "preprocess", "=", "None", ",", "\n", "device", "=", "\"cpu\"", ")", ":", "\n", "        ", "self", ".", "scheme", "=", "scheme", ".", "copy", "(", ")", "\n", "self", ".", "groups", "=", "groups", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "max_seq_length", "=", "max_seq_length", "\n", "self", ".", "preprocess", "=", "{", "}", "if", "preprocess", "is", "None", "else", "preprocess", "\n", "self", ".", "device", "=", "device", "\n", "\n", "if", "data", "is", "not", "None", ":", "\n", "            ", "self", ".", "data", "=", "data", "\n", "", "else", ":", "\n", "            ", "self", ".", "data", "=", "SN", "(", ")", "\n", "self", ".", "data", ".", "transition_data", "=", "{", "}", "\n", "self", ".", "data", ".", "episode_data", "=", "{", "}", "\n", "self", ".", "_setup_data", "(", "self", ".", "scheme", ",", "self", ".", "groups", ",", "batch_size", ",", "max_seq_length", ",", "self", ".", "preprocess", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.EpisodeBatch._setup_data": [[30, 76], ["scheme.update", "scheme.items", "field_info.get", "field_info.get", "field_info.get", "isinstance", "torch.zeros", "torch.zeros", "transform.infer_output_info"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.EpisodeBatch.update", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.transforms.OneHot.infer_output_info"], ["", "", "def", "_setup_data", "(", "self", ",", "scheme", ",", "groups", ",", "batch_size", ",", "max_seq_length", ",", "preprocess", ")", ":", "\n", "        ", "if", "preprocess", "is", "not", "None", ":", "\n", "            ", "for", "k", "in", "preprocess", ":", "\n", "                ", "assert", "k", "in", "scheme", "\n", "new_k", "=", "preprocess", "[", "k", "]", "[", "0", "]", "\n", "transforms", "=", "preprocess", "[", "k", "]", "[", "1", "]", "\n", "\n", "vshape", "=", "self", ".", "scheme", "[", "k", "]", "[", "\"vshape\"", "]", "\n", "dtype", "=", "self", ".", "scheme", "[", "k", "]", "[", "\"dtype\"", "]", "\n", "for", "transform", "in", "transforms", ":", "\n", "                    ", "vshape", ",", "dtype", "=", "transform", ".", "infer_output_info", "(", "vshape", ",", "dtype", ")", "\n", "\n", "", "self", ".", "scheme", "[", "new_k", "]", "=", "{", "\n", "\"vshape\"", ":", "vshape", ",", "\n", "\"dtype\"", ":", "dtype", "\n", "}", "\n", "if", "\"group\"", "in", "self", ".", "scheme", "[", "k", "]", ":", "\n", "                    ", "self", ".", "scheme", "[", "new_k", "]", "[", "\"group\"", "]", "=", "self", ".", "scheme", "[", "k", "]", "[", "\"group\"", "]", "\n", "", "if", "\"episode_const\"", "in", "self", ".", "scheme", "[", "k", "]", ":", "\n", "                    ", "self", ".", "scheme", "[", "new_k", "]", "[", "\"episode_const\"", "]", "=", "self", ".", "scheme", "[", "k", "]", "[", "\"episode_const\"", "]", "\n", "\n", "", "", "", "assert", "\"filled\"", "not", "in", "scheme", ",", "'\"filled\" is a reserved key for masking.'", "\n", "scheme", ".", "update", "(", "{", "\n", "\"filled\"", ":", "{", "\"vshape\"", ":", "(", "1", ",", ")", ",", "\"dtype\"", ":", "th", ".", "long", "}", ",", "\n", "}", ")", "\n", "\n", "for", "field_key", ",", "field_info", "in", "scheme", ".", "items", "(", ")", ":", "\n", "            ", "assert", "\"vshape\"", "in", "field_info", ",", "\"Scheme must define vshape for {}\"", ".", "format", "(", "field_key", ")", "\n", "vshape", "=", "field_info", "[", "\"vshape\"", "]", "\n", "episode_const", "=", "field_info", ".", "get", "(", "\"episode_const\"", ",", "False", ")", "\n", "group", "=", "field_info", ".", "get", "(", "\"group\"", ",", "None", ")", "\n", "dtype", "=", "field_info", ".", "get", "(", "\"dtype\"", ",", "th", ".", "float32", ")", "\n", "\n", "if", "isinstance", "(", "vshape", ",", "int", ")", ":", "\n", "                ", "vshape", "=", "(", "vshape", ",", ")", "\n", "\n", "", "if", "group", ":", "\n", "                ", "assert", "group", "in", "groups", ",", "\"Group {} must have its number of members defined in _groups_\"", ".", "format", "(", "group", ")", "\n", "shape", "=", "(", "groups", "[", "group", "]", ",", "*", "vshape", ")", "\n", "", "else", ":", "\n", "                ", "shape", "=", "vshape", "\n", "\n", "", "if", "episode_const", ":", "\n", "                ", "self", ".", "data", ".", "episode_data", "[", "field_key", "]", "=", "th", ".", "zeros", "(", "(", "batch_size", ",", "*", "shape", ")", ",", "dtype", "=", "dtype", ",", "device", "=", "self", ".", "device", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "data", ".", "transition_data", "[", "field_key", "]", "=", "th", ".", "zeros", "(", "(", "batch_size", ",", "max_seq_length", ",", "*", "shape", ")", ",", "dtype", "=", "dtype", ",", "device", "=", "self", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.EpisodeBatch.extend": [[77, 79], ["episode_buffer.EpisodeBatch._setup_data"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.EpisodeBatch._setup_data"], ["", "", "", "def", "extend", "(", "self", ",", "scheme", ",", "groups", "=", "None", ")", ":", "\n", "        ", "self", ".", "_setup_data", "(", "scheme", ",", "self", ".", "groups", "if", "groups", "is", "None", "else", "groups", ",", "self", ".", "batch_size", ",", "self", ".", "max_seq_length", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.EpisodeBatch.to": [[80, 86], ["episode_buffer.EpisodeBatch.data.transition_data.items", "episode_buffer.EpisodeBatch.data.episode_data.items", "v.to", "v.to"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.EpisodeBatch.to", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.EpisodeBatch.to"], ["", "def", "to", "(", "self", ",", "device", ")", ":", "\n", "        ", "for", "k", ",", "v", "in", "self", ".", "data", ".", "transition_data", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "data", ".", "transition_data", "[", "k", "]", "=", "v", ".", "to", "(", "device", ")", "\n", "", "for", "k", ",", "v", "in", "self", ".", "data", ".", "episode_data", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "data", ".", "episode_data", "[", "k", "]", "=", "v", ".", "to", "(", "device", ")", "\n", "", "self", ".", "device", "=", "device", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.EpisodeBatch.update": [[87, 113], ["slice", "slice", "episode_buffer.EpisodeBatch._parse_slices", "data.items", "episode_buffer.EpisodeBatch.scheme[].get", "torch.tensor", "episode_buffer.EpisodeBatch._check_safe_view", "transform.transform.view_as", "transform.transform.view_as", "KeyError", "transform.transform"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.EpisodeBatch._parse_slices", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.EpisodeBatch._check_safe_view", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.transforms.OneHot.transform"], ["", "def", "update", "(", "self", ",", "data", ",", "bs", "=", "slice", "(", "None", ")", ",", "ts", "=", "slice", "(", "None", ")", ",", "mark_filled", "=", "True", ")", ":", "\n", "        ", "slices", "=", "self", ".", "_parse_slices", "(", "(", "bs", ",", "ts", ")", ")", "\n", "for", "k", ",", "v", "in", "data", ".", "items", "(", ")", ":", "\n", "            ", "if", "k", "in", "self", ".", "data", ".", "transition_data", ":", "\n", "                ", "target", "=", "self", ".", "data", ".", "transition_data", "\n", "if", "mark_filled", ":", "\n", "                    ", "target", "[", "\"filled\"", "]", "[", "slices", "]", "=", "1", "\n", "mark_filled", "=", "False", "\n", "", "_slices", "=", "slices", "\n", "", "elif", "k", "in", "self", ".", "data", ".", "episode_data", ":", "\n", "                ", "target", "=", "self", ".", "data", ".", "episode_data", "\n", "_slices", "=", "slices", "[", "0", "]", "\n", "", "else", ":", "\n", "                ", "raise", "KeyError", "(", "\"{} not found in transition or episode data\"", ".", "format", "(", "k", ")", ")", "\n", "\n", "", "dtype", "=", "self", ".", "scheme", "[", "k", "]", ".", "get", "(", "\"dtype\"", ",", "th", ".", "float32", ")", "\n", "v", "=", "th", ".", "tensor", "(", "v", ",", "dtype", "=", "dtype", ",", "device", "=", "self", ".", "device", ")", "\n", "self", ".", "_check_safe_view", "(", "v", ",", "target", "[", "k", "]", "[", "_slices", "]", ")", "\n", "target", "[", "k", "]", "[", "_slices", "]", "=", "v", ".", "view_as", "(", "target", "[", "k", "]", "[", "_slices", "]", ")", "\n", "\n", "if", "k", "in", "self", ".", "preprocess", ":", "\n", "                ", "new_k", "=", "self", ".", "preprocess", "[", "k", "]", "[", "0", "]", "\n", "v", "=", "target", "[", "k", "]", "[", "_slices", "]", "\n", "for", "transform", "in", "self", ".", "preprocess", "[", "k", "]", "[", "1", "]", ":", "\n", "                    ", "v", "=", "transform", ".", "transform", "(", "v", ")", "\n", "", "target", "[", "new_k", "]", "[", "_slices", "]", "=", "v", ".", "view_as", "(", "target", "[", "new_k", "]", "[", "_slices", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.EpisodeBatch._check_safe_view": [[114, 122], ["len", "ValueError"], "methods", ["None"], ["", "", "", "def", "_check_safe_view", "(", "self", ",", "v", ",", "dest", ")", ":", "\n", "        ", "idx", "=", "len", "(", "v", ".", "shape", ")", "-", "1", "\n", "for", "s", "in", "dest", ".", "shape", "[", ":", ":", "-", "1", "]", ":", "\n", "            ", "if", "v", ".", "shape", "[", "idx", "]", "!=", "s", ":", "\n", "                ", "if", "s", "!=", "1", ":", "\n", "                    ", "raise", "ValueError", "(", "\"Unsafe reshape of {} to {}\"", ".", "format", "(", "v", ".", "shape", ",", "dest", ".", "shape", ")", ")", "\n", "", "", "else", ":", "\n", "                ", "idx", "-=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.EpisodeBatch.__getitem__": [[123, 160], ["isinstance", "isinstance", "all", "episode_buffer.EpisodeBatch._new_data_sn", "episode_buffer.EpisodeBatch", "episode_buffer.EpisodeBatch._parse_slices", "episode_buffer.EpisodeBatch._new_data_sn", "episode_buffer.EpisodeBatch.data.transition_data.items", "episode_buffer.EpisodeBatch.data.episode_data.items", "episode_buffer.EpisodeBatch._get_num_items", "episode_buffer.EpisodeBatch._get_num_items", "episode_buffer.EpisodeBatch", "isinstance", "KeyError"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.EpisodeBatch._new_data_sn", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.EpisodeBatch._parse_slices", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.EpisodeBatch._new_data_sn", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.EpisodeBatch._get_num_items", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.EpisodeBatch._get_num_items"], ["", "", "", "def", "__getitem__", "(", "self", ",", "item", ")", ":", "\n", "        ", "if", "isinstance", "(", "item", ",", "str", ")", ":", "\n", "            ", "if", "item", "in", "self", ".", "data", ".", "episode_data", ":", "\n", "                ", "return", "self", ".", "data", ".", "episode_data", "[", "item", "]", "\n", "", "elif", "item", "in", "self", ".", "data", ".", "transition_data", ":", "\n", "                ", "return", "self", ".", "data", ".", "transition_data", "[", "item", "]", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "\n", "", "", "elif", "isinstance", "(", "item", ",", "tuple", ")", "and", "all", "(", "[", "isinstance", "(", "it", ",", "str", ")", "for", "it", "in", "item", "]", ")", ":", "\n", "            ", "new_data", "=", "self", ".", "_new_data_sn", "(", ")", "\n", "for", "key", "in", "item", ":", "\n", "                ", "if", "key", "in", "self", ".", "data", ".", "transition_data", ":", "\n", "                    ", "new_data", ".", "transition_data", "[", "key", "]", "=", "self", ".", "data", ".", "transition_data", "[", "key", "]", "\n", "", "elif", "key", "in", "self", ".", "data", ".", "episode_data", ":", "\n", "                    ", "new_data", ".", "episode_data", "[", "key", "]", "=", "self", ".", "data", ".", "episode_data", "[", "key", "]", "\n", "", "else", ":", "\n", "                    ", "raise", "KeyError", "(", "\"Unrecognised key {}\"", ".", "format", "(", "key", ")", ")", "\n", "\n", "# Update the scheme to only have the requested keys", "\n", "", "", "new_scheme", "=", "{", "key", ":", "self", ".", "scheme", "[", "key", "]", "for", "key", "in", "item", "}", "\n", "new_groups", "=", "{", "self", ".", "scheme", "[", "key", "]", "[", "\"group\"", "]", ":", "self", ".", "groups", "[", "self", ".", "scheme", "[", "key", "]", "[", "\"group\"", "]", "]", "\n", "for", "key", "in", "item", "if", "\"group\"", "in", "self", ".", "scheme", "[", "key", "]", "}", "\n", "ret", "=", "EpisodeBatch", "(", "new_scheme", ",", "new_groups", ",", "self", ".", "batch_size", ",", "self", ".", "max_seq_length", ",", "data", "=", "new_data", ",", "device", "=", "self", ".", "device", ")", "\n", "return", "ret", "\n", "", "else", ":", "\n", "            ", "item", "=", "self", ".", "_parse_slices", "(", "item", ")", "\n", "new_data", "=", "self", ".", "_new_data_sn", "(", ")", "\n", "for", "k", ",", "v", "in", "self", ".", "data", ".", "transition_data", ".", "items", "(", ")", ":", "\n", "                ", "new_data", ".", "transition_data", "[", "k", "]", "=", "v", "[", "item", "]", "\n", "", "for", "k", ",", "v", "in", "self", ".", "data", ".", "episode_data", ".", "items", "(", ")", ":", "\n", "                ", "new_data", ".", "episode_data", "[", "k", "]", "=", "v", "[", "item", "[", "0", "]", "]", "\n", "\n", "", "ret_bs", "=", "self", ".", "_get_num_items", "(", "item", "[", "0", "]", ",", "self", ".", "batch_size", ")", "\n", "ret_max_t", "=", "self", ".", "_get_num_items", "(", "item", "[", "1", "]", ",", "self", ".", "max_seq_length", ")", "\n", "\n", "ret", "=", "EpisodeBatch", "(", "self", ".", "scheme", ",", "self", ".", "groups", ",", "ret_bs", ",", "ret_max_t", ",", "data", "=", "new_data", ",", "device", "=", "self", ".", "device", ")", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.EpisodeBatch._get_num_items": [[161, 167], ["isinstance", "isinstance", "len", "isinstance", "indexing_item.indices"], "methods", ["None"], ["", "", "def", "_get_num_items", "(", "self", ",", "indexing_item", ",", "max_size", ")", ":", "\n", "        ", "if", "isinstance", "(", "indexing_item", ",", "list", ")", "or", "isinstance", "(", "indexing_item", ",", "np", ".", "ndarray", ")", ":", "\n", "            ", "return", "len", "(", "indexing_item", ")", "\n", "", "elif", "isinstance", "(", "indexing_item", ",", "slice", ")", ":", "\n", "            ", "_range", "=", "indexing_item", ".", "indices", "(", "max_size", ")", "\n", "return", "1", "+", "(", "_range", "[", "1", "]", "-", "_range", "[", "0", "]", "-", "1", ")", "//", "_range", "[", "2", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.EpisodeBatch._new_data_sn": [[168, 173], ["types.SimpleNamespace"], "methods", ["None"], ["", "", "def", "_new_data_sn", "(", "self", ")", ":", "\n", "        ", "new_data", "=", "SN", "(", ")", "\n", "new_data", ".", "transition_data", "=", "{", "}", "\n", "new_data", ".", "episode_data", "=", "{", "}", "\n", "return", "new_data", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.EpisodeBatch._parse_slices": [[174, 196], ["isinstance", "isinstance", "isinstance", "isinstance", "IndexError", "isinstance", "slice", "parsed.append", "parsed.append", "slice"], "methods", ["None"], ["", "def", "_parse_slices", "(", "self", ",", "items", ")", ":", "\n", "        ", "parsed", "=", "[", "]", "\n", "# Only batch slice given, add full time slice", "\n", "if", "(", "isinstance", "(", "items", ",", "slice", ")", "# slice a:b", "\n", "or", "isinstance", "(", "items", ",", "int", ")", "# int i", "\n", "or", "(", "isinstance", "(", "items", ",", "(", "list", ",", "np", ".", "ndarray", ",", "th", ".", "LongTensor", ",", "th", ".", "cuda", ".", "LongTensor", ")", ")", ")", "# [a,b,c]", "\n", ")", ":", "\n", "            ", "items", "=", "(", "items", ",", "slice", "(", "None", ")", ")", "\n", "\n", "# Need the time indexing to be contiguous", "\n", "", "if", "isinstance", "(", "items", "[", "1", "]", ",", "list", ")", ":", "\n", "            ", "raise", "IndexError", "(", "\"Indexing across Time must be contiguous\"", ")", "\n", "\n", "", "for", "item", "in", "items", ":", "\n", "#TODO: stronger checks to ensure only supported options get through", "\n", "            ", "if", "isinstance", "(", "item", ",", "int", ")", ":", "\n", "# Convert single indices to slices", "\n", "                ", "parsed", ".", "append", "(", "slice", "(", "item", ",", "item", "+", "1", ")", ")", "\n", "", "else", ":", "\n", "# Leave slices and lists as is", "\n", "                ", "parsed", ".", "append", "(", "item", ")", "\n", "", "", "return", "parsed", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.EpisodeBatch.max_t_filled": [[197, 199], ["torch.sum().max", "torch.sum"], "methods", ["None"], ["", "def", "max_t_filled", "(", "self", ")", ":", "\n", "        ", "return", "th", ".", "sum", "(", "self", ".", "data", ".", "transition_data", "[", "\"filled\"", "]", ",", "1", ")", ".", "max", "(", "0", ")", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.EpisodeBatch.__repr__": [[200, 205], ["episode_buffer.EpisodeBatch.scheme.keys", "episode_buffer.EpisodeBatch.groups.keys"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "\"EpisodeBatch. Batch Size:{} Max_seq_len:{} Keys:{} Groups:{}\"", ".", "format", "(", "self", ".", "batch_size", ",", "\n", "self", ".", "max_seq_length", ",", "\n", "self", ".", "scheme", ".", "keys", "(", ")", ",", "\n", "self", ".", "groups", ".", "keys", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.ReplayBuffer.__init__": [[207, 212], ["episode_buffer.EpisodeBatch.__init__"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner.__init__"], ["    ", "def", "__init__", "(", "self", ",", "scheme", ",", "groups", ",", "buffer_size", ",", "max_seq_length", ",", "preprocess", "=", "None", ",", "device", "=", "\"cpu\"", ")", ":", "\n", "        ", "super", "(", "ReplayBuffer", ",", "self", ")", ".", "__init__", "(", "scheme", ",", "groups", ",", "buffer_size", ",", "max_seq_length", ",", "preprocess", "=", "preprocess", ",", "device", "=", "device", ")", "\n", "self", ".", "buffer_size", "=", "buffer_size", "# same as self.batch_size but more explicit", "\n", "self", ".", "buffer_index", "=", "0", "\n", "self", ".", "episodes_in_buffer", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.ReplayBuffer.insert_episode_batch": [[213, 229], ["episode_buffer.ReplayBuffer.update", "episode_buffer.ReplayBuffer.update", "max", "episode_buffer.ReplayBuffer.insert_episode_batch", "episode_buffer.ReplayBuffer.insert_episode_batch", "slice", "slice", "slice"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.EpisodeBatch.update", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.EpisodeBatch.update", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.ReplayBuffer.insert_episode_batch", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.ReplayBuffer.insert_episode_batch"], ["", "def", "insert_episode_batch", "(", "self", ",", "ep_batch", ")", ":", "\n", "        ", "if", "self", ".", "buffer_index", "+", "ep_batch", ".", "batch_size", "<=", "self", ".", "buffer_size", ":", "\n", "            ", "self", ".", "update", "(", "ep_batch", ".", "data", ".", "transition_data", ",", "\n", "slice", "(", "self", ".", "buffer_index", ",", "self", ".", "buffer_index", "+", "ep_batch", ".", "batch_size", ")", ",", "\n", "slice", "(", "0", ",", "ep_batch", ".", "max_seq_length", ")", ",", "\n", "mark_filled", "=", "False", ")", "\n", "self", ".", "update", "(", "ep_batch", ".", "data", ".", "episode_data", ",", "\n", "slice", "(", "self", ".", "buffer_index", ",", "self", ".", "buffer_index", "+", "ep_batch", ".", "batch_size", ")", ")", "\n", "self", ".", "buffer_index", "=", "(", "self", ".", "buffer_index", "+", "ep_batch", ".", "batch_size", ")", "\n", "self", ".", "episodes_in_buffer", "=", "max", "(", "self", ".", "episodes_in_buffer", ",", "self", ".", "buffer_index", ")", "\n", "self", ".", "buffer_index", "=", "self", ".", "buffer_index", "%", "self", ".", "buffer_size", "\n", "assert", "self", ".", "buffer_index", "<", "self", ".", "buffer_size", "\n", "", "else", ":", "\n", "            ", "buffer_left", "=", "self", ".", "buffer_size", "-", "self", ".", "buffer_index", "\n", "self", ".", "insert_episode_batch", "(", "ep_batch", "[", "0", ":", "buffer_left", ",", ":", "]", ")", "\n", "self", ".", "insert_episode_batch", "(", "ep_batch", "[", "buffer_left", ":", ",", ":", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.ReplayBuffer.can_sample": [[230, 232], ["None"], "methods", ["None"], ["", "", "def", "can_sample", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "return", "self", ".", "episodes_in_buffer", ">=", "batch_size", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.ReplayBuffer.sample": [[233, 241], ["episode_buffer.ReplayBuffer.can_sample", "numpy.random.choice"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.ReplayBuffer.can_sample"], ["", "def", "sample", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "assert", "self", ".", "can_sample", "(", "batch_size", ")", "\n", "if", "self", ".", "episodes_in_buffer", "==", "batch_size", ":", "\n", "            ", "return", "self", "[", ":", "batch_size", "]", "\n", "", "else", ":", "\n", "# Uniform sampling only atm", "\n", "            ", "ep_ids", "=", "np", ".", "random", ".", "choice", "(", "self", ".", "episodes_in_buffer", ",", "batch_size", ",", "replace", "=", "False", ")", "\n", "return", "self", "[", "ep_ids", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.ReplayBuffer.__repr__": [[242, 247], ["episode_buffer.ReplayBuffer.scheme.keys", "episode_buffer.ReplayBuffer.groups.keys"], "methods", ["None"], ["", "", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "\"ReplayBuffer. {}/{} episodes. Keys:{} Groups:{}\"", ".", "format", "(", "self", ".", "episodes_in_buffer", ",", "\n", "self", ".", "buffer_size", ",", "\n", "self", ".", "scheme", ".", "keys", "(", ")", ",", "\n", "self", ".", "groups", ".", "keys", "(", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.max_q_learner.MAXQLearner.__init__": [[14, 72], ["list", "list", "copy.deepcopy", "list", "copy.deepcopy", "torch.optim.RMSprop", "collections.deque", "mac.parameters", "max_q_learner.MAXQLearner.mac.parameters", "list", "list", "copy.deepcopy", "Exception", "max_q_learner.MAXQLearner.central_mixer.parameters", "modules.mixers.vdn.VDNMixer", "max_q_learner.MAXQLearner.mixer.parameters", "max_q_learner.MAXQLearner.mixer.parameters", "copy.deepcopy", "list", "modules.mixers.qmix.QMixer", "ValueError", "modules.mixers.qmix_central_no_hyper.QMixerCentralFF", "max_q_learner.MAXQLearner.central_mac.parameters", "modules.mixers.qmix_central_attention.QMixerCentralAtten", "Exception"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.parameters", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.parameters", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.parameters", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.parameters", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.parameters", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.parameters"], ["    ", "def", "__init__", "(", "self", ",", "mac", ",", "scheme", ",", "logger", ",", "args", ")", ":", "\n", "        ", "self", ".", "args", "=", "args", "\n", "self", ".", "mac", "=", "mac", "\n", "self", ".", "logger", "=", "logger", "\n", "\n", "self", ".", "mac_params", "=", "list", "(", "mac", ".", "parameters", "(", ")", ")", "\n", "self", ".", "params", "=", "list", "(", "self", ".", "mac", ".", "parameters", "(", ")", ")", "\n", "\n", "self", ".", "last_target_update_episode", "=", "0", "\n", "\n", "self", ".", "mixer", "=", "None", "\n", "assert", "args", ".", "mixer", "is", "not", "None", "\n", "if", "args", ".", "mixer", "is", "not", "None", ":", "\n", "            ", "if", "args", ".", "mixer", "==", "\"vdn\"", ":", "\n", "                ", "self", ".", "mixer", "=", "VDNMixer", "(", ")", "\n", "", "elif", "args", ".", "mixer", "==", "\"qmix\"", ":", "\n", "                ", "self", ".", "mixer", "=", "QMixer", "(", "args", ")", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"Mixer {} not recognised.\"", ".", "format", "(", "args", ".", "mixer", ")", ")", "\n", "", "self", ".", "mixer_params", "=", "list", "(", "self", ".", "mixer", ".", "parameters", "(", ")", ")", "\n", "self", ".", "params", "+=", "list", "(", "self", ".", "mixer", ".", "parameters", "(", ")", ")", "\n", "self", ".", "target_mixer", "=", "copy", ".", "deepcopy", "(", "self", ".", "mixer", ")", "\n", "\n", "# a little wasteful to deepcopy (e.g. duplicates action selector), but should work for any MAC", "\n", "", "self", ".", "target_mac", "=", "copy", ".", "deepcopy", "(", "mac", ")", "\n", "\n", "# Central Q", "\n", "# TODO: Clean this mess up!", "\n", "self", ".", "central_mac", "=", "None", "\n", "if", "self", ".", "args", ".", "central_mixer", "in", "[", "\"ff\"", ",", "\"atten\"", "]", ":", "\n", "            ", "if", "self", ".", "args", ".", "central_loss", "==", "0", ":", "\n", "                ", "self", ".", "central_mixer", "=", "self", ".", "mixer", "\n", "self", ".", "central_mac", "=", "self", ".", "mac", "\n", "self", ".", "target_central_mac", "=", "self", ".", "target_mac", "\n", "", "else", ":", "\n", "                ", "if", "self", ".", "args", ".", "central_mixer", "==", "\"ff\"", ":", "\n", "                    ", "self", ".", "central_mixer", "=", "QMixerCentralFF", "(", "args", ")", "# Feedforward network that takes state and agent utils as input", "\n", "", "elif", "self", ".", "args", ".", "central_mixer", "==", "\"atten\"", ":", "\n", "                    ", "self", ".", "central_mixer", "=", "QMixerCentralAtten", "(", "args", ")", "\n", "", "else", ":", "\n", "                    ", "raise", "Exception", "(", "\"Error with central_mixer\"", ")", "\n", "\n", "", "assert", "args", ".", "central_mac", "==", "\"basic_central_mac\"", "\n", "self", ".", "central_mac", "=", "mac_REGISTRY", "[", "args", ".", "central_mac", "]", "(", "scheme", ",", "args", ")", "# Groups aren't used in the CentralBasicController. Little hacky", "\n", "self", ".", "target_central_mac", "=", "copy", ".", "deepcopy", "(", "self", ".", "central_mac", ")", "\n", "self", ".", "params", "+=", "list", "(", "self", ".", "central_mac", ".", "parameters", "(", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "raise", "Exception", "(", "\"Error with qCentral\"", ")", "\n", "", "self", ".", "params", "+=", "list", "(", "self", ".", "central_mixer", ".", "parameters", "(", ")", ")", "\n", "self", ".", "target_central_mixer", "=", "copy", ".", "deepcopy", "(", "self", ".", "central_mixer", ")", "\n", "\n", "self", ".", "optimiser", "=", "RMSprop", "(", "params", "=", "self", ".", "params", ",", "lr", "=", "args", ".", "lr", ",", "alpha", "=", "args", ".", "optim_alpha", ",", "eps", "=", "args", ".", "optim_eps", ")", "\n", "\n", "self", ".", "log_stats_t", "=", "-", "self", ".", "args", ".", "learner_log_interval", "-", "1", "\n", "\n", "self", ".", "grad_norm", "=", "1", "\n", "self", ".", "mixer_norm", "=", "1", "\n", "self", ".", "mixer_norms", "=", "deque", "(", "[", "1", "]", ",", "maxlen", "=", "100", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.max_q_learner.MAXQLearner.train": [[73, 218], ["[].float", "[].float", "max_q_learner.MAXQLearner.mac.init_hidden", "range", "torch.stack", "torch.gather().squeeze", "max_q_learner.MAXQLearner.target_mac.init_hidden", "range", "torch.stack", "max_q_learner.MAXQLearner.central_mac.init_hidden", "range", "torch.stack", "torch.gather().squeeze", "max_q_learner.MAXQLearner.target_central_mac.init_hidden", "range", "torch.stack", "torch.gather().squeeze", "max_q_learner.MAXQLearner.mixer", "max_q_learner.MAXQLearner.target_central_mixer", "mask.expand_as.expand_as.expand_as", "max_q_learner.MAXQLearner.central_mixer", "mask.expand_as.expand_as.expand_as", "max_q_learner.MAXQLearner.optimiser.zero_grad", "loss.backward", "max_q_learner.MAXQLearner.mixer_norms.append", "torch.nn.utils.clip_grad_norm_", "max_q_learner.MAXQLearner.optimiser.step", "max_q_learner.MAXQLearner.mac.forward", "torch.stack.append", "max_q_learner.MAXQLearner.target_mac.forward", "torch.stack.append", "torch.stack.clone().detach", "mac_out_detach[].max", "torch.gather().squeeze", "Exception", "max_q_learner.MAXQLearner.central_mac.forward", "torch.stack.append", "max_q_learner.MAXQLearner.target_central_mac.forward", "torch.stack.append", "targets.detach", "targets.detach", "mask.expand_as.expand_as.sum", "torch.ones_like", "torch.where", "torch.where.mean().item", "max_q_learner.MAXQLearner.target_central_mixer", "torch.where", "torch.where.mean().item", "mask.expand_as.expand_as.sum", "p.grad.data.norm", "p.grad.data.norm", "max_q_learner.MAXQLearner._update_targets", "max_q_learner.MAXQLearner.logger.log_stat", "max_q_learner.MAXQLearner.logger.log_stat", "max_q_learner.MAXQLearner.logger.log_stat", "max_q_learner.MAXQLearner.logger.log_stat", "max_q_learner.MAXQLearner.logger.log_stat", "mask.expand_as.expand_as.sum().item", "max_q_learner.MAXQLearner.logger.log_stat", "max_q_learner.MAXQLearner.logger.log_stat", "max_q_learner.MAXQLearner.logger.log_stat", "max_q_learner.MAXQLearner.logger.log_stat", "max_q_learner.MAXQLearner.logger.log_stat", "torch.gather", "torch.gather", "torch.gather", "p.grad.data.norm.item", "p.grad.data.norm.item", "loss.item", "qmix_loss.item", "central_loss.item", "torch.stack.clone", "torch.gather", "cur_max_actions[].unsqueeze().repeat", "torch.ones_like", "torch.where.mean", "torch.ones_like", "torch.where.mean", "mask.expand_as.expand_as.sum", "masked_td_error.abs().sum().item", "actions.unsqueeze().repeat", "torch.where.detach", "cur_max_actions[].unsqueeze", "masked_td_error.abs().sum", "actions.unsqueeze", "masked_td_error.abs"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.agents.central_rnn_agent.CentralRNNAgent.init_hidden", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.agents.central_rnn_agent.CentralRNNAgent.init_hidden", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.agents.central_rnn_agent.CentralRNNAgent.init_hidden", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.agents.central_rnn_agent.CentralRNNAgent.init_hidden", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.step", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.sqddpg.SQDDPGMixer.forward", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.sqddpg.SQDDPGMixer.forward", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.sqddpg.SQDDPGMixer.forward", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.sqddpg.SQDDPGMixer.forward", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.q_learner.QLearner._update_targets", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat"], ["", "def", "train", "(", "self", ",", "batch", ":", "EpisodeBatch", ",", "t_env", ":", "int", ",", "episode_num", ":", "int", ")", ":", "\n", "# Get the relevant quantities", "\n", "        ", "rewards", "=", "batch", "[", "\"reward\"", "]", "[", ":", ",", ":", "-", "1", "]", "\n", "actions", "=", "batch", "[", "\"actions\"", "]", "[", ":", ",", ":", "-", "1", "]", "\n", "terminated", "=", "batch", "[", "\"terminated\"", "]", "[", ":", ",", ":", "-", "1", "]", ".", "float", "(", ")", "\n", "mask", "=", "batch", "[", "\"filled\"", "]", "[", ":", ",", ":", "-", "1", "]", ".", "float", "(", ")", "\n", "mask", "[", ":", ",", "1", ":", "]", "=", "mask", "[", ":", ",", "1", ":", "]", "*", "(", "1", "-", "terminated", "[", ":", ",", ":", "-", "1", "]", ")", "\n", "avail_actions", "=", "batch", "[", "\"avail_actions\"", "]", "\n", "\n", "# Calculate estimated Q-Values", "\n", "mac_out", "=", "[", "]", "\n", "self", ".", "mac", ".", "init_hidden", "(", "batch", ".", "batch_size", ")", "\n", "for", "t", "in", "range", "(", "batch", ".", "max_seq_length", ")", ":", "\n", "            ", "agent_outs", "=", "self", ".", "mac", ".", "forward", "(", "batch", ",", "t", "=", "t", ")", "\n", "mac_out", ".", "append", "(", "agent_outs", ")", "\n", "", "mac_out", "=", "th", ".", "stack", "(", "mac_out", ",", "dim", "=", "1", ")", "# Concat over time", "\n", "\n", "# Pick the Q-Values for the actions taken by each agent", "\n", "chosen_action_qvals_agents", "=", "th", ".", "gather", "(", "mac_out", "[", ":", ",", ":", "-", "1", "]", ",", "dim", "=", "3", ",", "index", "=", "actions", ")", ".", "squeeze", "(", "3", ")", "# Remove the last dim", "\n", "chosen_action_qvals", "=", "chosen_action_qvals_agents", "\n", "\n", "# Calculate the Q-Values necessary for the target", "\n", "target_mac_out", "=", "[", "]", "\n", "self", ".", "target_mac", ".", "init_hidden", "(", "batch", ".", "batch_size", ")", "\n", "for", "t", "in", "range", "(", "batch", ".", "max_seq_length", ")", ":", "\n", "            ", "target_agent_outs", "=", "self", ".", "target_mac", ".", "forward", "(", "batch", ",", "t", "=", "t", ")", "\n", "target_mac_out", ".", "append", "(", "target_agent_outs", ")", "\n", "\n", "# We don't need the first timesteps Q-Value estimate for calculating targets", "\n", "", "target_mac_out", "=", "th", ".", "stack", "(", "target_mac_out", "[", ":", "]", ",", "dim", "=", "1", ")", "# Concat across time", "\n", "\n", "# Mask out unavailable actions", "\n", "target_mac_out", "[", "avail_actions", "[", ":", ",", ":", "]", "==", "0", "]", "=", "-", "9999999", "# From OG deepmarl", "\n", "\n", "# Max over target Q-Values", "\n", "if", "self", ".", "args", ".", "double_q", ":", "\n", "# Get actions that maximise live Q (for double q-learning)", "\n", "            ", "mac_out_detach", "=", "mac_out", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "mac_out_detach", "[", "avail_actions", "==", "0", "]", "=", "-", "9999999", "\n", "cur_max_action_targets", ",", "cur_max_actions", "=", "mac_out_detach", "[", ":", ",", ":", "]", ".", "max", "(", "dim", "=", "3", ",", "keepdim", "=", "True", ")", "\n", "target_max_agent_qvals", "=", "th", ".", "gather", "(", "target_mac_out", "[", ":", ",", ":", "]", ",", "3", ",", "cur_max_actions", "[", ":", ",", ":", "]", ")", ".", "squeeze", "(", "3", ")", "\n", "", "else", ":", "\n", "            ", "raise", "Exception", "(", "\"Use double q\"", ")", "\n", "\n", "# Central MAC stuff", "\n", "", "central_mac_out", "=", "[", "]", "\n", "self", ".", "central_mac", ".", "init_hidden", "(", "batch", ".", "batch_size", ")", "\n", "for", "t", "in", "range", "(", "batch", ".", "max_seq_length", ")", ":", "\n", "            ", "agent_outs", "=", "self", ".", "central_mac", ".", "forward", "(", "batch", ",", "t", "=", "t", ")", "\n", "central_mac_out", ".", "append", "(", "agent_outs", ")", "\n", "", "central_mac_out", "=", "th", ".", "stack", "(", "central_mac_out", ",", "dim", "=", "1", ")", "# Concat over time", "\n", "central_chosen_action_qvals_agents", "=", "th", ".", "gather", "(", "central_mac_out", "[", ":", ",", ":", "-", "1", "]", ",", "dim", "=", "3", ",", "index", "=", "actions", ".", "unsqueeze", "(", "4", ")", ".", "repeat", "(", "1", ",", "1", ",", "1", ",", "1", ",", "self", ".", "args", ".", "central_action_embed", ")", ")", ".", "squeeze", "(", "3", ")", "# Remove the last dim", "\n", "\n", "central_target_mac_out", "=", "[", "]", "\n", "self", ".", "target_central_mac", ".", "init_hidden", "(", "batch", ".", "batch_size", ")", "\n", "for", "t", "in", "range", "(", "batch", ".", "max_seq_length", ")", ":", "\n", "            ", "target_agent_outs", "=", "self", ".", "target_central_mac", ".", "forward", "(", "batch", ",", "t", "=", "t", ")", "\n", "central_target_mac_out", ".", "append", "(", "target_agent_outs", ")", "\n", "", "central_target_mac_out", "=", "th", ".", "stack", "(", "central_target_mac_out", "[", ":", "]", ",", "dim", "=", "1", ")", "# Concat across time", "\n", "# Mask out unavailable actions", "\n", "central_target_mac_out", "[", "avail_actions", "[", ":", ",", ":", "]", "==", "0", "]", "=", "-", "9999999", "# From OG deepmarl", "\n", "# Use the Qmix max actions", "\n", "central_target_max_agent_qvals", "=", "th", ".", "gather", "(", "central_target_mac_out", "[", ":", ",", ":", "]", ",", "3", ",", "cur_max_actions", "[", ":", ",", ":", "]", ".", "unsqueeze", "(", "4", ")", ".", "repeat", "(", "1", ",", "1", ",", "1", ",", "1", ",", "self", ".", "args", ".", "central_action_embed", ")", ")", ".", "squeeze", "(", "3", ")", "\n", "# ---", "\n", "\n", "# Mix", "\n", "chosen_action_qvals", "=", "self", ".", "mixer", "(", "chosen_action_qvals", ",", "batch", "[", "\"state\"", "]", "[", ":", ",", ":", "-", "1", "]", ")", "\n", "target_max_qvals", "=", "self", ".", "target_central_mixer", "(", "central_target_max_agent_qvals", "[", ":", ",", "1", ":", "]", ",", "batch", "[", "\"state\"", "]", "[", ":", ",", "1", ":", "]", ")", "\n", "\n", "# Calculate 1-step Q-Learning targets", "\n", "targets", "=", "rewards", "+", "self", ".", "args", ".", "gamma", "*", "(", "1", "-", "terminated", ")", "*", "target_max_qvals", "\n", "\n", "# Td-error", "\n", "td_error", "=", "(", "chosen_action_qvals", "-", "(", "targets", ".", "detach", "(", ")", ")", ")", "\n", "\n", "mask", "=", "mask", ".", "expand_as", "(", "td_error", ")", "\n", "\n", "# 0-out the targets that came from padded data", "\n", "masked_td_error", "=", "td_error", "*", "mask", "\n", "\n", "# Training central Q", "\n", "central_chosen_action_qvals", "=", "self", ".", "central_mixer", "(", "central_chosen_action_qvals_agents", ",", "batch", "[", "\"state\"", "]", "[", ":", ",", ":", "-", "1", "]", ")", "\n", "central_td_error", "=", "(", "central_chosen_action_qvals", "-", "targets", ".", "detach", "(", ")", ")", "\n", "central_mask", "=", "mask", ".", "expand_as", "(", "central_td_error", ")", "\n", "central_masked_td_error", "=", "central_td_error", "*", "central_mask", "\n", "central_loss", "=", "(", "central_masked_td_error", "**", "2", ")", ".", "sum", "(", ")", "/", "mask", ".", "sum", "(", ")", "\n", "\n", "# QMIX loss with weighting", "\n", "ws", "=", "th", ".", "ones_like", "(", "td_error", ")", "*", "self", ".", "args", ".", "w", "\n", "if", "self", ".", "args", ".", "hysteretic_qmix", ":", "# OW-QMIX", "\n", "            ", "ws", "=", "th", ".", "where", "(", "td_error", "<", "0", ",", "th", ".", "ones_like", "(", "td_error", ")", "*", "1", ",", "ws", ")", "# Target is greater than current max", "\n", "w_to_use", "=", "ws", ".", "mean", "(", ")", ".", "item", "(", ")", "# For logging", "\n", "", "else", ":", "# CW-QMIX", "\n", "            ", "is_max_action", "=", "(", "actions", "==", "cur_max_actions", "[", ":", ",", ":", "-", "1", "]", ")", ".", "min", "(", "dim", "=", "2", ")", "[", "0", "]", "\n", "max_action_qtot", "=", "self", ".", "target_central_mixer", "(", "central_target_max_agent_qvals", "[", ":", ",", ":", "-", "1", "]", ",", "batch", "[", "\"state\"", "]", "[", ":", ",", ":", "-", "1", "]", ")", "\n", "qtot_larger", "=", "targets", ">", "max_action_qtot", "\n", "ws", "=", "th", ".", "where", "(", "is_max_action", "|", "qtot_larger", ",", "th", ".", "ones_like", "(", "td_error", ")", "*", "1", ",", "ws", ")", "# Target is greater than current max", "\n", "w_to_use", "=", "ws", ".", "mean", "(", ")", ".", "item", "(", ")", "# Average of ws for logging", "\n", "\n", "", "qmix_loss", "=", "(", "ws", ".", "detach", "(", ")", "*", "(", "masked_td_error", "**", "2", ")", ")", ".", "sum", "(", ")", "/", "mask", ".", "sum", "(", ")", "\n", "\n", "# The weightings for the different losses aren't used (they are always set to 1)", "\n", "loss", "=", "self", ".", "args", ".", "qmix_loss", "*", "qmix_loss", "+", "self", ".", "args", ".", "central_loss", "*", "central_loss", "\n", "\n", "# Optimise", "\n", "self", ".", "optimiser", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "\n", "# Logging", "\n", "agent_norm", "=", "0", "\n", "for", "p", "in", "self", ".", "mac_params", ":", "\n", "            ", "param_norm", "=", "p", ".", "grad", ".", "data", ".", "norm", "(", "2", ")", "\n", "agent_norm", "+=", "param_norm", ".", "item", "(", ")", "**", "2", "\n", "", "agent_norm", "=", "agent_norm", "**", "(", "1.", "/", "2", ")", "\n", "\n", "mixer_norm", "=", "0", "\n", "for", "p", "in", "self", ".", "mixer_params", ":", "\n", "            ", "param_norm", "=", "p", ".", "grad", ".", "data", ".", "norm", "(", "2", ")", "\n", "mixer_norm", "+=", "param_norm", ".", "item", "(", ")", "**", "2", "\n", "", "mixer_norm", "=", "mixer_norm", "**", "(", "1.", "/", "2", ")", "\n", "self", ".", "mixer_norm", "=", "mixer_norm", "\n", "self", ".", "mixer_norms", ".", "append", "(", "mixer_norm", ")", "\n", "\n", "grad_norm", "=", "th", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "params", ",", "self", ".", "args", ".", "grad_norm_clip", ")", "\n", "self", ".", "grad_norm", "=", "grad_norm", "\n", "\n", "self", ".", "optimiser", ".", "step", "(", ")", "\n", "\n", "if", "(", "episode_num", "-", "self", ".", "last_target_update_episode", ")", "/", "self", ".", "args", ".", "target_update_interval", ">=", "1.0", ":", "\n", "            ", "self", ".", "_update_targets", "(", ")", "\n", "self", ".", "last_target_update_episode", "=", "episode_num", "\n", "\n", "", "if", "t_env", "-", "self", ".", "log_stats_t", ">=", "self", ".", "args", ".", "learner_log_interval", ":", "\n", "            ", "self", ".", "logger", ".", "log_stat", "(", "\"loss\"", ",", "loss", ".", "item", "(", ")", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"qmix_loss\"", ",", "qmix_loss", ".", "item", "(", ")", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"grad_norm\"", ",", "grad_norm", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"mixer_norm\"", ",", "mixer_norm", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"agent_norm\"", ",", "agent_norm", ",", "t_env", ")", "\n", "mask_elems", "=", "mask", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"td_error_abs\"", ",", "(", "masked_td_error", ".", "abs", "(", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "/", "mask_elems", ")", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"q_taken_mean\"", ",", "(", "chosen_action_qvals", "*", "mask", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "/", "(", "mask_elems", "*", "self", ".", "args", ".", "n_agents", ")", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"target_mean\"", ",", "(", "targets", "*", "mask", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "/", "(", "mask_elems", "*", "self", ".", "args", ".", "n_agents", ")", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"central_loss\"", ",", "central_loss", ".", "item", "(", ")", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"w_to_use\"", ",", "w_to_use", ",", "t_env", ")", "\n", "self", ".", "log_stats_t", "=", "t_env", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.max_q_learner.MAXQLearner._update_targets": [[219, 227], ["max_q_learner.MAXQLearner.target_mac.load_state", "max_q_learner.MAXQLearner.target_central_mixer.load_state_dict", "max_q_learner.MAXQLearner.logger.console_logger.info", "max_q_learner.MAXQLearner.target_mixer.load_state_dict", "max_q_learner.MAXQLearner.target_central_mac.load_state", "max_q_learner.MAXQLearner.central_mixer.state_dict", "max_q_learner.MAXQLearner.mixer.state_dict"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.load_state", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.load_state"], ["", "", "def", "_update_targets", "(", "self", ")", ":", "\n", "        ", "self", ".", "target_mac", ".", "load_state", "(", "self", ".", "mac", ")", "\n", "if", "self", ".", "mixer", "is", "not", "None", ":", "\n", "            ", "self", ".", "target_mixer", ".", "load_state_dict", "(", "self", ".", "mixer", ".", "state_dict", "(", ")", ")", "\n", "", "if", "self", ".", "central_mac", "is", "not", "None", ":", "\n", "            ", "self", ".", "target_central_mac", ".", "load_state", "(", "self", ".", "central_mac", ")", "\n", "", "self", ".", "target_central_mixer", ".", "load_state_dict", "(", "self", ".", "central_mixer", ".", "state_dict", "(", ")", ")", "\n", "self", ".", "logger", ".", "console_logger", ".", "info", "(", "\"Updated target network\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.max_q_learner.MAXQLearner.cuda": [[228, 239], ["max_q_learner.MAXQLearner.mac.cuda", "max_q_learner.MAXQLearner.target_mac.cuda", "max_q_learner.MAXQLearner.central_mixer.cuda", "max_q_learner.MAXQLearner.target_central_mixer.cuda", "max_q_learner.MAXQLearner.mixer.cuda", "max_q_learner.MAXQLearner.target_mixer.cuda", "max_q_learner.MAXQLearner.central_mac.cuda", "max_q_learner.MAXQLearner.target_central_mac.cuda"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda"], ["", "def", "cuda", "(", "self", ")", ":", "\n", "        ", "self", ".", "mac", ".", "cuda", "(", ")", "\n", "self", ".", "target_mac", ".", "cuda", "(", ")", "\n", "if", "self", ".", "mixer", "is", "not", "None", ":", "\n", "            ", "self", ".", "mixer", ".", "cuda", "(", ")", "\n", "self", ".", "target_mixer", ".", "cuda", "(", ")", "\n", "", "if", "self", ".", "central_mac", "is", "not", "None", ":", "\n", "            ", "self", ".", "central_mac", ".", "cuda", "(", ")", "\n", "self", ".", "target_central_mac", ".", "cuda", "(", ")", "\n", "", "self", ".", "central_mixer", ".", "cuda", "(", ")", "\n", "self", ".", "target_central_mixer", ".", "cuda", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.max_q_learner.MAXQLearner.save_models": [[241, 246], ["max_q_learner.MAXQLearner.mac.save_models", "torch.save", "torch.save", "max_q_learner.MAXQLearner.optimiser.state_dict", "max_q_learner.MAXQLearner.mixer.state_dict"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.save_models"], ["", "def", "save_models", "(", "self", ",", "path", ")", ":", "\n", "        ", "self", ".", "mac", ".", "save_models", "(", "path", ")", "\n", "if", "self", ".", "mixer", "is", "not", "None", ":", "\n", "            ", "th", ".", "save", "(", "self", ".", "mixer", ".", "state_dict", "(", ")", ",", "\"{}/mixer.th\"", ".", "format", "(", "path", ")", ")", "\n", "", "th", ".", "save", "(", "self", ".", "optimiser", ".", "state_dict", "(", ")", ",", "\"{}/opt.th\"", ".", "format", "(", "path", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.max_q_learner.MAXQLearner.load_models": [[247, 254], ["max_q_learner.MAXQLearner.mac.load_models", "max_q_learner.MAXQLearner.target_mac.load_models", "max_q_learner.MAXQLearner.optimiser.load_state_dict", "max_q_learner.MAXQLearner.mixer.load_state_dict", "torch.load", "torch.load"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.load_models", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.load_models"], ["", "def", "load_models", "(", "self", ",", "path", ")", ":", "\n", "        ", "self", ".", "mac", ".", "load_models", "(", "path", ")", "\n", "# Not quite right but I don't want to save target networks", "\n", "self", ".", "target_mac", ".", "load_models", "(", "path", ")", "\n", "if", "self", ".", "mixer", "is", "not", "None", ":", "\n", "            ", "self", ".", "mixer", ".", "load_state_dict", "(", "th", ".", "load", "(", "\"{}/mixer.th\"", ".", "format", "(", "path", ")", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", ")", "\n", "", "self", ".", "optimiser", ".", "load_state_dict", "(", "th", ".", "load", "(", "\"{}/opt.th\"", ".", "format", "(", "path", ")", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.max_q_learner_ddpg.DDPGQLearner.__init__": [[13, 54], ["list", "list", "copy.deepcopy", "modules.mixers.qmix_central_no_hyper.QMixerCentralFF", "copy.deepcopy", "list", "list", "copy.deepcopy", "torch.optim.RMSprop", "torch.optim.RMSprop", "mac.parameters", "max_q_learner_ddpg.DDPGQLearner.mac.parameters", "list", "list", "copy.deepcopy", "max_q_learner_ddpg.DDPGQLearner.central_mac.parameters", "max_q_learner_ddpg.DDPGQLearner.central_mixer.parameters", "modules.mixers.vdn.VDNMixer", "max_q_learner_ddpg.DDPGQLearner.mixer.parameters", "max_q_learner_ddpg.DDPGQLearner.mixer.parameters", "modules.mixers.qmix.QMixer", "ValueError"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.parameters", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.parameters", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.parameters", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.parameters", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.parameters", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.parameters"], ["    ", "def", "__init__", "(", "self", ",", "mac", ",", "scheme", ",", "logger", ",", "args", ")", ":", "\n", "        ", "self", ".", "args", "=", "args", "\n", "self", ".", "mac", "=", "mac", "\n", "self", ".", "logger", "=", "logger", "\n", "\n", "self", ".", "mac_params", "=", "list", "(", "mac", ".", "parameters", "(", ")", ")", "\n", "self", ".", "params", "=", "list", "(", "self", ".", "mac", ".", "parameters", "(", ")", ")", "\n", "\n", "self", ".", "last_target_update_episode", "=", "0", "\n", "\n", "self", ".", "mixer", "=", "None", "\n", "assert", "args", ".", "mixer", "is", "not", "None", "\n", "if", "args", ".", "mixer", "is", "not", "None", ":", "\n", "            ", "if", "args", ".", "mixer", "==", "\"vdn\"", ":", "\n", "                ", "self", ".", "mixer", "=", "VDNMixer", "(", ")", "\n", "", "elif", "args", ".", "mixer", "==", "\"qmix\"", ":", "\n", "                ", "self", ".", "mixer", "=", "QMixer", "(", "args", ")", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"Mixer {} not recognised.\"", ".", "format", "(", "args", ".", "mixer", ")", ")", "\n", "", "self", ".", "mixer_params", "=", "list", "(", "self", ".", "mixer", ".", "parameters", "(", ")", ")", "\n", "self", ".", "params", "+=", "list", "(", "self", ".", "mixer", ".", "parameters", "(", ")", ")", "\n", "self", ".", "target_mixer", "=", "copy", ".", "deepcopy", "(", "self", ".", "mixer", ")", "\n", "\n", "# a little wasteful to deepcopy (e.g. duplicates action selector), but should work for any MAC", "\n", "", "self", ".", "target_mac", "=", "copy", ".", "deepcopy", "(", "mac", ")", "\n", "\n", "# Central Q", "\n", "# TODO: Clean this mess up!", "\n", "self", ".", "central_mac", "=", "None", "\n", "assert", "self", ".", "args", ".", "central_mixer", "==", "\"ff\"", "\n", "self", ".", "central_mixer", "=", "QMixerCentralFF", "(", "args", ")", "\n", "assert", "args", ".", "central_mac", "==", "\"basic_central_mac\"", "\n", "self", ".", "central_mac", "=", "mac_REGISTRY", "[", "args", ".", "central_mac", "]", "(", "scheme", ",", "args", ")", "# Groups aren't used in the CentralBasicController. Little hacky", "\n", "self", ".", "target_central_mac", "=", "copy", ".", "deepcopy", "(", "self", ".", "central_mac", ")", "\n", "self", ".", "params", "+=", "list", "(", "self", ".", "central_mac", ".", "parameters", "(", ")", ")", "\n", "self", ".", "params", "+=", "list", "(", "self", ".", "central_mixer", ".", "parameters", "(", ")", ")", "\n", "self", ".", "target_central_mixer", "=", "copy", ".", "deepcopy", "(", "self", ".", "central_mixer", ")", "\n", "\n", "self", ".", "optimiser", "=", "RMSprop", "(", "params", "=", "self", ".", "params", ",", "lr", "=", "args", ".", "lr", ",", "alpha", "=", "args", ".", "optim_alpha", ",", "eps", "=", "args", ".", "optim_eps", ")", "\n", "\n", "self", ".", "log_stats_t", "=", "-", "self", ".", "args", ".", "learner_log_interval", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.max_q_learner_ddpg.DDPGQLearner.train": [[55, 183], ["[].float", "[].float", "max_q_learner_ddpg.DDPGQLearner.mac.init_hidden", "range", "torch.stack", "torch.stack", "torch.gather().squeeze", "torch.gather().squeeze", "max_q_learner_ddpg.DDPGQLearner.target_mac.init_hidden", "range", "torch.stack", "torch.stack", "max_q_learner_ddpg.DDPGQLearner.central_mac.init_hidden", "range", "torch.stack", "torch.stack", "torch.gather().squeeze", "torch.gather().squeeze", "max_q_learner_ddpg.DDPGQLearner.target_central_mac.init_hidden", "range", "torch.stack", "torch.stack", "torch.gather().squeeze", "torch.gather().squeeze", "max_q_learner_ddpg.DDPGQLearner.target_central_mixer", "range", "max_q_learner_ddpg.DDPGQLearner.central_mixer", "[].float.expand_as", "max_q_learner_ddpg.DDPGQLearner.optimiser.zero_grad", "loss.backward", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "max_q_learner_ddpg.DDPGQLearner.optimiser.step", "max_q_learner_ddpg.DDPGQLearner.mac.forward", "torch.stack.append", "max_q_learner_ddpg.DDPGQLearner.target_mac.forward", "torch.stack.append", "Exception", "torch.stack.clone().detach", "mac_out_detach[].max", "torch.stack.clone().detach", "target_mac_out_detach[].max", "max_q_learner_ddpg.DDPGQLearner.central_mac.forward", "torch.stack.append", "max_q_learner_ddpg.DDPGQLearner.target_central_mac.forward", "torch.stack.append", "target_chosen_qvals.detach().clone", "range", "torch.cat", "torch.cat", "torch.gumbel_softmax", "torch.gumbel_softmax", "torch.softmax", "torch.softmax", "torch.log_softmax", "torch.log_softmax", "targets.detach", "[].float.sum", "max_q_learner_ddpg.DDPGQLearner._update_targets", "max_q_learner_ddpg.DDPGQLearner.logger.log_stat", "max_q_learner_ddpg.DDPGQLearner.logger.log_stat", "max_q_learner_ddpg.DDPGQLearner.logger.log_stat", "[].float.sum().item", "max_q_learner_ddpg.DDPGQLearner.logger.log_stat", "max_q_learner_ddpg.DDPGQLearner.logger.log_stat", "max_q_learner_ddpg.DDPGQLearner.logger.log_stat", "max_q_learner_ddpg.DDPGQLearner.logger.log_stat", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "max_q_learner_ddpg.DDPGQLearner.target_central_mixer", "qtots.append", "[].float.sum", "[].float.sum", "loss.item", "qmix_actor_loss.item", "central_loss.item", "logit_entropy.item", "torch.stack.clone", "torch.stack.clone", "cur_max_actions[].unsqueeze().repeat", "target_chosen_qvals.detach", "[].float.sum", "actions.unsqueeze().repeat", "cur_max_actions[].unsqueeze", "actions.unsqueeze", "avail_actions[].float", "qs_to_use.detach"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.agents.central_rnn_agent.CentralRNNAgent.init_hidden", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.agents.central_rnn_agent.CentralRNNAgent.init_hidden", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.agents.central_rnn_agent.CentralRNNAgent.init_hidden", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.agents.central_rnn_agent.CentralRNNAgent.init_hidden", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.step", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.sqddpg.SQDDPGMixer.forward", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.sqddpg.SQDDPGMixer.forward", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.sqddpg.SQDDPGMixer.forward", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.sqddpg.SQDDPGMixer.forward", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.q_learner.QLearner._update_targets", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat"], ["", "def", "train", "(", "self", ",", "batch", ":", "EpisodeBatch", ",", "t_env", ":", "int", ",", "episode_num", ":", "int", ")", ":", "\n", "# Get the relevant quantities", "\n", "        ", "rewards", "=", "batch", "[", "\"reward\"", "]", "[", ":", ",", ":", "-", "1", "]", "\n", "actions", "=", "batch", "[", "\"actions\"", "]", "[", ":", ",", ":", "-", "1", "]", "\n", "terminated", "=", "batch", "[", "\"terminated\"", "]", "[", ":", ",", ":", "-", "1", "]", ".", "float", "(", ")", "\n", "mask", "=", "batch", "[", "\"filled\"", "]", "[", ":", ",", ":", "-", "1", "]", ".", "float", "(", ")", "\n", "mask", "[", ":", ",", "1", ":", "]", "=", "mask", "[", ":", ",", "1", ":", "]", "*", "(", "1", "-", "terminated", "[", ":", ",", ":", "-", "1", "]", ")", "\n", "avail_actions", "=", "batch", "[", "\"avail_actions\"", "]", "\n", "\n", "# Calculate estimated Q-Values", "\n", "mac_out", "=", "[", "]", "\n", "self", ".", "mac", ".", "init_hidden", "(", "batch", ".", "batch_size", ")", "\n", "for", "t", "in", "range", "(", "batch", ".", "max_seq_length", ")", ":", "\n", "            ", "agent_outs", "=", "self", ".", "mac", ".", "forward", "(", "batch", ",", "t", "=", "t", ")", "\n", "mac_out", ".", "append", "(", "agent_outs", ")", "\n", "", "mac_out", "=", "th", ".", "stack", "(", "mac_out", ",", "dim", "=", "1", ")", "# Concat over time", "\n", "\n", "# Pick the Q-Values for the actions taken by each agent", "\n", "chosen_action_qvals_agents", "=", "th", ".", "gather", "(", "mac_out", "[", ":", ",", ":", "-", "1", "]", ",", "dim", "=", "3", ",", "index", "=", "actions", ")", ".", "squeeze", "(", "3", ")", "# Remove the last dim", "\n", "chosen_action_qvals", "=", "chosen_action_qvals_agents", "\n", "\n", "# Calculate the Q-Values necessary for the target", "\n", "target_mac_out", "=", "[", "]", "\n", "self", ".", "target_mac", ".", "init_hidden", "(", "batch", ".", "batch_size", ")", "\n", "for", "t", "in", "range", "(", "batch", ".", "max_seq_length", ")", ":", "\n", "            ", "target_agent_outs", "=", "self", ".", "target_mac", ".", "forward", "(", "batch", ",", "t", "=", "t", ")", "\n", "target_mac_out", ".", "append", "(", "target_agent_outs", ")", "\n", "\n", "# We don't need the first timesteps Q-Value estimate for calculating targets", "\n", "", "target_mac_out", "=", "th", ".", "stack", "(", "target_mac_out", "[", ":", "]", ",", "dim", "=", "1", ")", "# Concat across time", "\n", "\n", "# Max over target Q-Values", "\n", "if", "self", ".", "args", ".", "double_q", ":", "\n", "            ", "raise", "Exception", "(", "\"No double q for DDPG\"", ")", "\n", "", "else", ":", "\n", "            ", "mac_out_detach", "=", "mac_out", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "mac_out_detach", "[", "avail_actions", "==", "0", "]", "=", "-", "9999999", "\n", "_", ",", "cur_max_actions", "=", "mac_out_detach", "[", ":", ",", ":", "]", ".", "max", "(", "dim", "=", "3", ",", "keepdim", "=", "True", ")", "\n", "target_mac_out_detach", "=", "target_mac_out", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "target_mac_out_detach", "[", "avail_actions", "==", "0", "]", "=", "-", "9999999", "\n", "_", ",", "tar_max_actions", "=", "target_mac_out_detach", "[", ":", ",", ":", "]", ".", "max", "(", "dim", "=", "3", ",", "keepdim", "=", "True", ")", "\n", "\n", "# Central MAC stuff", "\n", "", "central_mac_out", "=", "[", "]", "\n", "self", ".", "central_mac", ".", "init_hidden", "(", "batch", ".", "batch_size", ")", "\n", "for", "t", "in", "range", "(", "batch", ".", "max_seq_length", ")", ":", "\n", "            ", "agent_outs", "=", "self", ".", "central_mac", ".", "forward", "(", "batch", ",", "t", "=", "t", ")", "\n", "central_mac_out", ".", "append", "(", "agent_outs", ")", "\n", "", "central_mac_out", "=", "th", ".", "stack", "(", "central_mac_out", ",", "dim", "=", "1", ")", "# Concat over time", "\n", "central_chosen_action_qvals_agents", "=", "th", ".", "gather", "(", "central_mac_out", "[", ":", ",", ":", "-", "1", "]", ",", "dim", "=", "3", ",", "index", "=", "actions", ".", "unsqueeze", "(", "4", ")", ".", "repeat", "(", "1", ",", "1", ",", "1", ",", "1", ",", "self", ".", "args", ".", "central_action_embed", ")", ")", ".", "squeeze", "(", "3", ")", "# Remove the last dim", "\n", "\n", "central_target_mac_out", "=", "[", "]", "\n", "self", ".", "target_central_mac", ".", "init_hidden", "(", "batch", ".", "batch_size", ")", "\n", "for", "t", "in", "range", "(", "batch", ".", "max_seq_length", ")", ":", "\n", "            ", "target_agent_outs", "=", "self", ".", "target_central_mac", ".", "forward", "(", "batch", ",", "t", "=", "t", ")", "\n", "central_target_mac_out", ".", "append", "(", "target_agent_outs", ")", "\n", "", "central_target_mac_out", "=", "th", ".", "stack", "(", "central_target_mac_out", "[", ":", "]", ",", "dim", "=", "1", ")", "# Concat across time", "\n", "# Use the Qmix max actions", "\n", "central_target_max_agent_qvals", "=", "th", ".", "gather", "(", "central_target_mac_out", "[", ":", ",", ":", "]", ",", "3", ",", "cur_max_actions", "[", ":", ",", ":", "]", ".", "unsqueeze", "(", "4", ")", ".", "repeat", "(", "1", ",", "1", ",", "1", ",", "1", ",", "self", ".", "args", ".", "central_action_embed", ")", ")", ".", "squeeze", "(", "3", ")", "\n", "# central_target_max_agent_qvals = th.gather(central_target_mac_out[:,:], 3, tar_max_actions[:,:].unsqueeze(4).repeat(1,1,1,1,self.args.central_action_embed)).squeeze(3)", "\n", "# ---", "\n", "\n", "# Mix", "\n", "target_max_qvals", "=", "self", ".", "target_central_mixer", "(", "central_target_max_agent_qvals", "[", ":", ",", "1", ":", "]", ",", "batch", "[", "\"state\"", "]", "[", ":", ",", "1", ":", "]", ")", "\n", "\n", "# Calculate 1-step Q-Learning targets", "\n", "targets", "=", "rewards", "+", "self", ".", "args", ".", "gamma", "*", "(", "1", "-", "terminated", ")", "*", "target_max_qvals", "\n", "\n", "# Bad naming, its not a qmix_actor", "\n", "qmix_actor_loss", "=", "0", "\n", "for", "agent", "in", "range", "(", "self", ".", "args", ".", "n_agents", ")", ":", "\n", "            ", "target_chosen_qvals", "=", "central_target_max_agent_qvals", "[", ":", ",", ":", "-", "1", "]", "\n", "chosen_utils", "=", "target_chosen_qvals", ".", "detach", "(", ")", ".", "clone", "(", ")", "\n", "# For each agent compute Q(u_i, u_{-i}) for each u_i, keeping u_{-i} fixed", "\n", "qtots", "=", "[", "]", "\n", "for", "action", "in", "range", "(", "self", ".", "args", ".", "n_actions", ")", ":", "\n", "                ", "chosen_utils", "[", ":", ",", ":", ",", "agent", "]", "=", "central_target_mac_out", "[", ":", ",", ":", "-", "1", ",", "agent", ",", "action", "]", "\n", "new_q_tot", "=", "self", ".", "target_central_mixer", "(", "chosen_utils", ",", "batch", "[", "\"state\"", "]", "[", ":", ",", ":", "-", "1", "]", ")", "\n", "qtots", ".", "append", "(", "new_q_tot", ")", "\n", "", "agent_q_tots", "=", "th", ".", "cat", "(", "qtots", ",", "dim", "=", "2", ")", "\n", "\n", "qs_to_use", "=", "agent_q_tots", "\n", "\n", "# Train via ST Gumbel Softmax", "\n", "log_agent_policy", "=", "F", ".", "gumbel_softmax", "(", "mac_out", "[", ":", ",", ":", "-", "1", ",", "agent", "]", ",", "hard", "=", "True", ",", "dim", "=", "2", ",", "tau", "=", "self", ".", "args", ".", "policy_temp", ")", "\n", "# Train via expected policy gradient", "\n", "# log_agent_policy = F.softmax(mac_out[:, :-1, agent] / self.args.policy_temp, dim=2)", "\n", "agent_policy_loss", "=", "(", "(", "log_agent_policy", "*", "qs_to_use", ".", "detach", "(", ")", "*", "avail_actions", "[", ":", ",", ":", "-", "1", ",", "agent", "]", ".", "float", "(", ")", ")", ".", "sum", "(", "dim", "=", "2", ",", "keepdim", "=", "True", ")", "*", "mask", ")", ".", "sum", "(", ")", "/", "mask", ".", "sum", "(", ")", "\n", "\n", "qmix_actor_loss", "=", "qmix_actor_loss", "+", "agent_policy_loss", "\n", "\n", "# Logit entropy", "\n", "", "ps", "=", "F", ".", "softmax", "(", "mac_out", "[", ":", ",", ":", "-", "1", "]", ",", "dim", "=", "3", ")", "*", "avail_actions", "[", ":", ",", ":", "-", "1", "]", "\n", "log_ps", "=", "F", ".", "log_softmax", "(", "mac_out", "[", ":", ",", ":", "-", "1", "]", ",", "dim", "=", "3", ")", "*", "avail_actions", "[", ":", ",", ":", "-", "1", "]", "\n", "logit_entropy", "=", "-", "(", "(", "(", "ps", "*", "log_ps", ")", ".", "sum", "(", "dim", "=", "3", ")", "*", "mask", ")", ".", "sum", "(", ")", "/", "mask", ".", "sum", "(", ")", ")", "\n", "\n", "# Training central Q", "\n", "central_chosen_action_qvals", "=", "self", ".", "central_mixer", "(", "central_chosen_action_qvals_agents", ",", "batch", "[", "\"state\"", "]", "[", ":", ",", ":", "-", "1", "]", ")", "\n", "central_td_error", "=", "(", "central_chosen_action_qvals", "-", "targets", ".", "detach", "(", ")", ")", "\n", "central_mask", "=", "mask", ".", "expand_as", "(", "central_td_error", ")", "\n", "central_masked_td_error", "=", "central_td_error", "*", "central_mask", "\n", "central_loss", "=", "(", "central_masked_td_error", "**", "2", ")", ".", "sum", "(", ")", "/", "mask", ".", "sum", "(", ")", "\n", "\n", "loss", "=", "-", "self", ".", "args", ".", "qmix_loss", "*", "qmix_actor_loss", "+", "self", ".", "args", ".", "central_loss", "*", "central_loss", "+", "-", "self", ".", "args", ".", "logit_entropy", "*", "logit_entropy", "\n", "\n", "# Optimise", "\n", "self", ".", "optimiser", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "\n", "grad_norm", "=", "th", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "params", ",", "self", ".", "args", ".", "grad_norm_clip", ")", "\n", "self", ".", "grad_norm", "=", "grad_norm", "\n", "\n", "self", ".", "optimiser", ".", "step", "(", ")", "\n", "\n", "if", "(", "episode_num", "-", "self", ".", "last_target_update_episode", ")", "/", "self", ".", "args", ".", "target_update_interval", ">=", "1.0", ":", "\n", "            ", "self", ".", "_update_targets", "(", ")", "\n", "self", ".", "last_target_update_episode", "=", "episode_num", "\n", "\n", "", "if", "t_env", "-", "self", ".", "log_stats_t", ">=", "self", ".", "args", ".", "learner_log_interval", ":", "\n", "            ", "self", ".", "logger", ".", "log_stat", "(", "\"loss\"", ",", "loss", ".", "item", "(", ")", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"actor_loss\"", ",", "qmix_actor_loss", ".", "item", "(", ")", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"grad_norm\"", ",", "grad_norm", ",", "t_env", ")", "\n", "mask_elems", "=", "mask", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"q_taken_mean\"", ",", "(", "chosen_action_qvals", "*", "mask", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "/", "(", "mask_elems", "*", "self", ".", "args", ".", "n_agents", ")", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"target_mean\"", ",", "(", "targets", "*", "mask", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "/", "(", "mask_elems", "*", "self", ".", "args", ".", "n_agents", ")", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"central_loss\"", ",", "central_loss", ".", "item", "(", ")", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"logit_entropy\"", ",", "logit_entropy", ".", "item", "(", ")", ",", "t_env", ")", "\n", "self", ".", "log_stats_t", "=", "t_env", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.max_q_learner_ddpg.DDPGQLearner._update_targets": [[184, 192], ["max_q_learner_ddpg.DDPGQLearner.target_mac.load_state", "max_q_learner_ddpg.DDPGQLearner.target_central_mixer.load_state_dict", "max_q_learner_ddpg.DDPGQLearner.logger.console_logger.info", "max_q_learner_ddpg.DDPGQLearner.target_mixer.load_state_dict", "max_q_learner_ddpg.DDPGQLearner.target_central_mac.load_state", "max_q_learner_ddpg.DDPGQLearner.central_mixer.state_dict", "max_q_learner_ddpg.DDPGQLearner.mixer.state_dict"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.load_state", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.load_state"], ["", "", "def", "_update_targets", "(", "self", ")", ":", "\n", "        ", "self", ".", "target_mac", ".", "load_state", "(", "self", ".", "mac", ")", "\n", "if", "self", ".", "mixer", "is", "not", "None", ":", "\n", "            ", "self", ".", "target_mixer", ".", "load_state_dict", "(", "self", ".", "mixer", ".", "state_dict", "(", ")", ")", "\n", "", "if", "self", ".", "central_mac", "is", "not", "None", ":", "\n", "            ", "self", ".", "target_central_mac", ".", "load_state", "(", "self", ".", "central_mac", ")", "\n", "", "self", ".", "target_central_mixer", ".", "load_state_dict", "(", "self", ".", "central_mixer", ".", "state_dict", "(", ")", ")", "\n", "self", ".", "logger", ".", "console_logger", ".", "info", "(", "\"Updated target network\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.max_q_learner_ddpg.DDPGQLearner.cuda": [[193, 204], ["max_q_learner_ddpg.DDPGQLearner.mac.cuda", "max_q_learner_ddpg.DDPGQLearner.target_mac.cuda", "max_q_learner_ddpg.DDPGQLearner.central_mixer.cuda", "max_q_learner_ddpg.DDPGQLearner.target_central_mixer.cuda", "max_q_learner_ddpg.DDPGQLearner.mixer.cuda", "max_q_learner_ddpg.DDPGQLearner.target_mixer.cuda", "max_q_learner_ddpg.DDPGQLearner.central_mac.cuda", "max_q_learner_ddpg.DDPGQLearner.target_central_mac.cuda"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda"], ["", "def", "cuda", "(", "self", ")", ":", "\n", "        ", "self", ".", "mac", ".", "cuda", "(", ")", "\n", "self", ".", "target_mac", ".", "cuda", "(", ")", "\n", "if", "self", ".", "mixer", "is", "not", "None", ":", "\n", "            ", "self", ".", "mixer", ".", "cuda", "(", ")", "\n", "self", ".", "target_mixer", ".", "cuda", "(", ")", "\n", "", "if", "self", ".", "central_mac", "is", "not", "None", ":", "\n", "            ", "self", ".", "central_mac", ".", "cuda", "(", ")", "\n", "self", ".", "target_central_mac", ".", "cuda", "(", ")", "\n", "", "self", ".", "central_mixer", ".", "cuda", "(", ")", "\n", "self", ".", "target_central_mixer", ".", "cuda", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.max_q_learner_ddpg.DDPGQLearner.save_models": [[205, 210], ["max_q_learner_ddpg.DDPGQLearner.mac.save_models", "torch.save", "torch.save", "torch.save", "torch.save", "max_q_learner_ddpg.DDPGQLearner.optimiser.state_dict", "max_q_learner_ddpg.DDPGQLearner.mixer.state_dict"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.save_models"], ["", "def", "save_models", "(", "self", ",", "path", ")", ":", "\n", "        ", "self", ".", "mac", ".", "save_models", "(", "path", ")", "\n", "if", "self", ".", "mixer", "is", "not", "None", ":", "\n", "            ", "th", ".", "save", "(", "self", ".", "mixer", ".", "state_dict", "(", ")", ",", "\"{}/mixer.th\"", ".", "format", "(", "path", ")", ")", "\n", "", "th", ".", "save", "(", "self", ".", "optimiser", ".", "state_dict", "(", ")", ",", "\"{}/opt.th\"", ".", "format", "(", "path", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.max_q_learner_ddpg.DDPGQLearner.load_models": [[211, 218], ["max_q_learner_ddpg.DDPGQLearner.mac.load_models", "max_q_learner_ddpg.DDPGQLearner.target_mac.load_models", "max_q_learner_ddpg.DDPGQLearner.optimiser.load_state_dict", "max_q_learner_ddpg.DDPGQLearner.mixer.load_state_dict", "torch.load", "torch.load", "torch.load", "torch.load"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.load_models", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.load_models"], ["", "def", "load_models", "(", "self", ",", "path", ")", ":", "\n", "        ", "self", ".", "mac", ".", "load_models", "(", "path", ")", "\n", "# Not quite right but I don't want to save target networks", "\n", "self", ".", "target_mac", ".", "load_models", "(", "path", ")", "\n", "if", "self", ".", "mixer", "is", "not", "None", ":", "\n", "            ", "self", ".", "mixer", ".", "load_state_dict", "(", "th", ".", "load", "(", "\"{}/mixer.th\"", ".", "format", "(", "path", ")", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", ")", "\n", "", "self", ".", "optimiser", ".", "load_state_dict", "(", "th", ".", "load", "(", "\"{}/opt.th\"", ".", "format", "(", "path", ")", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.shaq_learner.SHAQLearner.__init__": [[9, 37], ["list", "torch.optim.RMSprop", "torch.optim.RMSprop", "copy.deepcopy", "Exception", "mac.parameters", "SHAQMixer", "list", "shaq_learner.SHAQLearner.mixer.parameters"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.parameters", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.parameters"], ["    ", "def", "__init__", "(", "self", ",", "mac", ",", "scheme", ",", "logger", ",", "args", ")", ":", "\n", "        ", "self", ".", "args", "=", "args", "\n", "if", "args", ".", "name", "==", "\"shaq\"", ":", "\n", "            ", "from", "modules", ".", "mixers", ".", "shaq", "import", "SHAQMixer", "\n", "", "else", ":", "\n", "            ", "raise", "Exception", "(", "\"Please give the correct mixer name!\"", ")", "\n", "\n", "", "self", ".", "mac", "=", "mac", "\n", "self", ".", "logger", "=", "logger", "\n", "\n", "self", ".", "params", "=", "list", "(", "mac", ".", "parameters", "(", ")", ")", "\n", "\n", "self", ".", "last_target_update_episode", "=", "0", "\n", "self", ".", "last_mixer_update_episode", "=", "0", "\n", "self", ".", "last_sample_coalition_episode", "=", "0", "\n", "\n", "self", ".", "mixer", "=", "None", "\n", "if", "args", ".", "mixer", "is", "not", "None", ":", "\n", "            ", "self", ".", "mixer", "=", "SHAQMixer", "(", "args", ")", "\n", "self", ".", "params_mixer", "=", "list", "(", "self", ".", "mixer", ".", "parameters", "(", ")", ")", "\n", "\n", "", "self", ".", "optimiser", "=", "RMSprop", "(", "params", "=", "self", ".", "params", ",", "lr", "=", "args", ".", "lr", ",", "alpha", "=", "args", ".", "optim_alpha", ",", "eps", "=", "args", ".", "optim_eps", ")", "\n", "self", ".", "optimiser_mixer", "=", "RMSprop", "(", "params", "=", "self", ".", "params_mixer", ",", "lr", "=", "args", ".", "alpha_lr", ",", "alpha", "=", "args", ".", "optim_alpha", ",", "eps", "=", "args", ".", "optim_eps", ")", "\n", "\n", "# a little wasteful to deepcopy (e.g. duplicates action selector), but should work for any MAC", "\n", "self", ".", "target_mac", "=", "copy", ".", "deepcopy", "(", "mac", ")", "\n", "\n", "self", ".", "log_stats_t", "=", "-", "self", ".", "args", ".", "learner_log_interval", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.shaq_learner.SHAQLearner.train": [[39, 147], ["torch.nn.functional.one_hot", "[].float", "[].float", "shaq_learner.SHAQLearner.mac.init_hidden", "range", "torch.stack", "torch.gather().squeeze", "torch.stack.clone().detach", "[].squeeze", "shaq_learner.SHAQLearner.target_mac.init_hidden", "range", "torch.stack", "getattr", "mask.expand_as.expand_as.expand_as", "shaq_learner.SHAQLearner.optimiser.zero_grad", "shaq_learner.SHAQLearner.optimiser_mixer.zero_grad", "loss.backward", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "shaq_learner.SHAQLearner.optimiser.step", "shaq_learner.SHAQLearner.optimiser_mixer.step", "shaq_learner.SHAQLearner.mac.forward", "torch.stack.append", "shaq_learner.SHAQLearner.target_mac.forward", "torch.stack.append", "torch.stack.clone().detach", "torch.gather().squeeze", "shaq_learner.SHAQLearner.mixer", "shaq_learner.SHAQLearner.mixer", "torch.zeros_like", "torch.tensor", "mask.expand_as.expand_as.flip().cumsum().flip().clamp_max().long", "range", "torch.linspace().unsqueeze().long", "torch.gather", "targets.detach", "mask.expand_as.expand_as.sum", "shaq_learner.SHAQLearner._update_targets", "shaq_learner.SHAQLearner.logger.log_stat", "shaq_learner.SHAQLearner.logger.log_stat", "shaq_learner.SHAQLearner.logger.log_stat", "mask.expand_as.expand_as.sum().item", "shaq_learner.SHAQLearner.logger.log_stat", "shaq_learner.SHAQLearner.logger.log_stat", "shaq_learner.SHAQLearner.logger.log_stat", "shaq_learner.SHAQLearner.logger.log_stat", "shaq_learner.SHAQLearner.logger.log_stat", "torch.gather", "torch.stack.clone", "mac_out_detach[].max", "torch.stack.max", "loss.item", "_mac_out_detach[].max", "actions.detach().squeeze", "torch.stack.clone", "torch.gather", "mask.expand_as.expand_as.flip().cumsum().flip().clamp_max", "torch.linspace().unsqueeze", "torch.pow", "mask.expand_as.expand_as.sum", "masked_td_error.abs().sum().item", "range", "mask.expand_as.flip().cumsum().flip().clamp_max().long.float", "actions.detach", "mask.expand_as.expand_as.flip().cumsum().flip", "torch.linspace", "mask.expand_as.flip().cumsum().flip().clamp_max().long.long", "masked_td_error.abs().sum", "mask.expand_as.expand_as.flip().cumsum", "masked_td_error.abs", "torch.gather().squeeze", "mask.expand_as.expand_as.expand_as", "mask.expand_as.expand_as.expand_as", "mask.expand_as.expand_as.flip", "torch.gather"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.agents.central_rnn_agent.CentralRNNAgent.init_hidden", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.agents.central_rnn_agent.CentralRNNAgent.init_hidden", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.step", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.step", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.sqddpg.SQDDPGMixer.forward", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.sqddpg.SQDDPGMixer.forward", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.q_learner.QLearner._update_targets", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat"], ["", "def", "train", "(", "self", ",", "batch", ":", "EpisodeBatch", ",", "t_env", ":", "int", ",", "episode_num", ":", "int", ")", ":", "\n", "# Get the relevant quantities", "\n", "        ", "rewards", "=", "batch", "[", "\"reward\"", "]", "[", ":", ",", ":", "-", "1", "]", "\n", "actions", "=", "batch", "[", "\"actions\"", "]", "[", ":", ",", ":", "-", "1", "]", "\n", "one_hot_actions", "=", "th", ".", "nn", ".", "functional", ".", "one_hot", "(", "actions", ",", "num_classes", "=", "self", ".", "args", ".", "n_actions", ")", "\n", "terminated", "=", "batch", "[", "\"terminated\"", "]", "[", ":", ",", ":", "-", "1", "]", ".", "float", "(", ")", "\n", "mask", "=", "batch", "[", "\"filled\"", "]", "[", ":", ",", ":", "-", "1", "]", ".", "float", "(", ")", "\n", "mask", "[", ":", ",", "1", ":", "]", "=", "mask", "[", ":", ",", "1", ":", "]", "*", "(", "1", "-", "terminated", "[", ":", ",", ":", "-", "1", "]", ")", "\n", "avail_actions", "=", "batch", "[", "\"avail_actions\"", "]", "\n", "\n", "# Calculate estimated Q-Values", "\n", "mac_out", "=", "[", "]", "\n", "self", ".", "mac", ".", "init_hidden", "(", "batch", ".", "batch_size", ")", "\n", "for", "t", "in", "range", "(", "batch", ".", "max_seq_length", ")", ":", "\n", "            ", "agent_outs", "=", "self", ".", "mac", ".", "forward", "(", "batch", ",", "t", "=", "t", ")", "\n", "mac_out", ".", "append", "(", "agent_outs", ")", "\n", "", "mac_out", "=", "th", ".", "stack", "(", "mac_out", ",", "dim", "=", "1", ")", "# Concat over time", "\n", "\n", "# Pick the Q-Values for the actions taken by each agent", "\n", "chosen_action_qvals", "=", "th", ".", "gather", "(", "mac_out", "[", ":", ",", ":", "-", "1", "]", ",", "dim", "=", "3", ",", "index", "=", "actions", ")", ".", "squeeze", "(", "3", ")", "# Remove the last dim", "\n", "\n", "# generate a filter for selecting the agents with the max-action", "\n", "_mac_out_detach", "=", "mac_out", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "_mac_out_detach", "[", "avail_actions", "==", "0", "]", "=", "-", "9999999", "\n", "_cur_max_actions", "=", "_mac_out_detach", "[", ":", ",", ":", "-", "1", "]", ".", "max", "(", "dim", "=", "3", ",", "keepdim", "=", "True", ")", "[", "1", "]", ".", "squeeze", "(", "3", ")", "\n", "max_filter", "=", "(", "actions", ".", "detach", "(", ")", ".", "squeeze", "(", "3", ")", "==", "_cur_max_actions", ")", ".", "float", "(", ")", "\n", "\n", "# Calculate the Q-Values necessary for the target", "\n", "target_mac_out", "=", "[", "]", "\n", "self", ".", "target_mac", ".", "init_hidden", "(", "batch", ".", "batch_size", ")", "\n", "for", "t", "in", "range", "(", "batch", ".", "max_seq_length", ")", ":", "\n", "            ", "target_agent_outs", "=", "self", ".", "target_mac", ".", "forward", "(", "batch", ",", "t", "=", "t", ")", "\n", "target_mac_out", ".", "append", "(", "target_agent_outs", ")", "\n", "\n", "# We don't need the first timesteps Q-Value estimate for calculating targets", "\n", "", "target_mac_out", "=", "th", ".", "stack", "(", "target_mac_out", "[", "1", ":", "]", ",", "dim", "=", "1", ")", "# Concat across time", "\n", "\n", "# Mask out unavailable actions", "\n", "target_mac_out", "[", "avail_actions", "[", ":", ",", "1", ":", "]", "==", "0", "]", "=", "-", "9999999", "# From OG deepmarl", "\n", "\n", "# Max over target Q-Values", "\n", "if", "self", ".", "args", ".", "double_q", ":", "\n", "# Get actions that maximise live Q (for double q-learning)", "\n", "            ", "mac_out_detach", "=", "mac_out", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "mac_out_detach", "[", "avail_actions", "==", "0", "]", "=", "-", "9999999", "\n", "cur_max_actions", "=", "mac_out_detach", "[", ":", ",", "1", ":", "]", ".", "max", "(", "dim", "=", "3", ",", "keepdim", "=", "True", ")", "[", "1", "]", "\n", "target_max_qvals", "=", "th", ".", "gather", "(", "target_mac_out", ",", "3", ",", "cur_max_actions", ")", ".", "squeeze", "(", "3", ")", "\n", "", "else", ":", "\n", "            ", "target_max_qvals", "=", "target_mac_out", ".", "max", "(", "dim", "=", "3", ")", "[", "0", "]", "\n", "\n", "# Mix", "\n", "", "if", "self", ".", "mixer", "is", "not", "None", ":", "\n", "            ", "chosen_action_qvals", ",", "w_est", "=", "self", ".", "mixer", "(", "batch", "[", "\"state\"", "]", "[", ":", ",", ":", "-", "1", "]", ",", "one_hot_actions", ",", "chosen_action_qvals", ",", "max_filter", ",", "target", "=", "False", ",", "manual_alpha_estimates", "=", "self", ".", "args", ".", "manual_alpha_estimates", ")", "\n", "target_max_qvals", "=", "self", ".", "mixer", "(", "batch", "[", "\"state\"", "]", "[", ":", ",", "1", ":", "]", ",", "one_hot_actions", ",", "target_max_qvals", ",", "max_filter", ",", "target", "=", "True", ",", "manual_alpha_estimates", "=", "self", ".", "args", ".", "manual_alpha_estimates", ")", "\n", "\n", "", "N", "=", "getattr", "(", "self", ".", "args", ",", "\"n_step\"", ",", "1", ")", "\n", "if", "N", "==", "1", ":", "\n", "# Calculate 1-step Q-Learning targets", "\n", "            ", "targets", "=", "rewards", "+", "self", ".", "args", ".", "gamma", "*", "(", "1", "-", "terminated", ")", "*", "target_max_qvals", "\n", "", "else", ":", "\n", "# N step Q-Learning targets", "\n", "            ", "n_rewards", "=", "th", ".", "zeros_like", "(", "rewards", ")", "\n", "gamma_tensor", "=", "th", ".", "tensor", "(", "[", "self", ".", "args", ".", "gamma", "**", "i", "for", "i", "in", "range", "(", "N", ")", "]", ",", "dtype", "=", "th", ".", "float", ",", "device", "=", "n_rewards", ".", "device", ")", "\n", "steps", "=", "mask", ".", "flip", "(", "1", ")", ".", "cumsum", "(", "dim", "=", "1", ")", ".", "flip", "(", "1", ")", ".", "clamp_max", "(", "N", ")", ".", "long", "(", ")", "\n", "for", "i", "in", "range", "(", "batch", ".", "max_seq_length", "-", "1", ")", ":", "\n", "                ", "n_rewards", "[", ":", ",", "i", ",", "0", "]", "=", "(", "(", "rewards", "*", "mask", ")", "[", ":", ",", "i", ":", "i", "+", "N", ",", "0", "]", "*", "gamma_tensor", "[", ":", "(", "batch", ".", "max_seq_length", "-", "1", "-", "i", ")", "]", ")", ".", "sum", "(", "dim", "=", "1", ")", "\n", "", "indices", "=", "th", ".", "linspace", "(", "0", ",", "batch", ".", "max_seq_length", "-", "2", ",", "steps", "=", "batch", ".", "max_seq_length", "-", "1", ",", "device", "=", "steps", ".", "device", ")", ".", "unsqueeze", "(", "1", ")", ".", "long", "(", ")", "\n", "n_targets_terminated", "=", "th", ".", "gather", "(", "target_max_qvals", "*", "(", "1", "-", "terminated", ")", ",", "dim", "=", "1", ",", "index", "=", "steps", ".", "long", "(", ")", "+", "indices", "-", "1", ")", "\n", "targets", "=", "n_rewards", "+", "th", ".", "pow", "(", "self", ".", "args", ".", "gamma", ",", "steps", ".", "float", "(", ")", ")", "*", "n_targets_terminated", "\n", "\n", "# Td-error", "\n", "", "td_error", "=", "(", "chosen_action_qvals", "-", "targets", ".", "detach", "(", ")", ")", "\n", "\n", "mask", "=", "mask", ".", "expand_as", "(", "td_error", ")", "\n", "\n", "# 0-out the targets that came from padded data", "\n", "masked_td_error", "=", "td_error", "*", "mask", "\n", "\n", "# Normal L2 loss, take mean over actual data", "\n", "loss", "=", "(", "masked_td_error", "**", "2", ")", ".", "sum", "(", ")", "/", "mask", ".", "sum", "(", ")", "\n", "\n", "# Optimise", "\n", "self", ".", "optimiser", ".", "zero_grad", "(", ")", "\n", "self", ".", "optimiser_mixer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "grad_norm", "=", "th", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "params", ",", "self", ".", "args", ".", "grad_norm_clip", ")", "\n", "grad_norm_mixer", "=", "th", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "params_mixer", ",", "self", ".", "args", ".", "grad_norm_clip", ")", "\n", "self", ".", "optimiser", ".", "step", "(", ")", "\n", "self", ".", "optimiser_mixer", ".", "step", "(", ")", "\n", "\n", "# Periodically update target Q-values", "\n", "if", "(", "episode_num", "-", "self", ".", "last_target_update_episode", ")", "/", "self", ".", "args", ".", "target_update_interval", ">=", "1.0", ":", "\n", "            ", "self", ".", "_update_targets", "(", ")", "\n", "self", ".", "last_target_update_episode", "=", "episode_num", "\n", "\n", "# Logging", "\n", "", "if", "t_env", "-", "self", ".", "log_stats_t", ">=", "self", ".", "args", ".", "learner_log_interval", ":", "\n", "            ", "self", ".", "logger", ".", "log_stat", "(", "\"loss\"", ",", "loss", ".", "item", "(", ")", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"grad_norm\"", ",", "grad_norm", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"grad_norm_mixer\"", ",", "grad_norm_mixer", ",", "t_env", ")", "\n", "mask_elems", "=", "mask", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"td_error_abs\"", ",", "(", "masked_td_error", ".", "abs", "(", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "/", "mask_elems", ")", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"q_taken_mean\"", ",", "(", "chosen_action_qvals", "*", "mask", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "/", "(", "mask_elems", "*", "self", ".", "args", ".", "n_agents", ")", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"target_mean\"", ",", "(", "targets", "*", "mask", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "/", "(", "mask_elems", "*", "self", ".", "args", ".", "n_agents", ")", ",", "t_env", ")", "\n", "agent_utils", "=", "(", "th", ".", "gather", "(", "mac_out", "[", ":", ",", ":", "-", "1", "]", ",", "dim", "=", "3", ",", "index", "=", "actions", ")", ".", "squeeze", "(", "3", ")", "*", "mask", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "/", "(", "mask_elems", "*", "self", ".", "args", ".", "n_agents", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"agent_utils\"", ",", "agent_utils", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"w_est\"", ",", "(", "w_est", "*", "(", "1", "-", "max_filter", ")", "*", "mask", ".", "expand_as", "(", "w_est", ")", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "/", "(", "(", "(", "1", "-", "max_filter", ")", "*", "mask", ".", "expand_as", "(", "w_est", ")", ")", ".", "sum", "(", ")", ".", "item", "(", ")", ")", ",", "t_env", ")", "\n", "self", ".", "log_stats_t", "=", "t_env", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.shaq_learner.SHAQLearner._update_targets": [[149, 152], ["shaq_learner.SHAQLearner.target_mac.load_state", "shaq_learner.SHAQLearner.logger.console_logger.info"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.load_state"], ["", "", "def", "_update_targets", "(", "self", ")", ":", "\n", "        ", "self", ".", "target_mac", ".", "load_state", "(", "self", ".", "mac", ")", "\n", "self", ".", "logger", ".", "console_logger", ".", "info", "(", "\"Updated target network\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.shaq_learner.SHAQLearner.cuda": [[154, 159], ["shaq_learner.SHAQLearner.mac.cuda", "shaq_learner.SHAQLearner.target_mac.cuda", "shaq_learner.SHAQLearner.mixer.cuda"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda"], ["", "def", "cuda", "(", "self", ")", ":", "\n", "        ", "self", ".", "mac", ".", "cuda", "(", ")", "\n", "self", ".", "target_mac", ".", "cuda", "(", ")", "\n", "if", "self", ".", "mixer", "is", "not", "None", ":", "\n", "            ", "self", ".", "mixer", ".", "cuda", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.shaq_learner.SHAQLearner.save_models": [[161, 166], ["shaq_learner.SHAQLearner.mac.save_models", "torch.save", "torch.save", "shaq_learner.SHAQLearner.optimiser.state_dict", "shaq_learner.SHAQLearner.mixer.state_dict"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.save_models"], ["", "", "def", "save_models", "(", "self", ",", "path", ")", ":", "\n", "        ", "self", ".", "mac", ".", "save_models", "(", "path", ")", "\n", "if", "self", ".", "mixer", "is", "not", "None", ":", "\n", "            ", "th", ".", "save", "(", "self", ".", "mixer", ".", "state_dict", "(", ")", ",", "\"{}/mixer.th\"", ".", "format", "(", "path", ")", ")", "\n", "", "th", ".", "save", "(", "self", ".", "optimiser", ".", "state_dict", "(", ")", ",", "\"{}/opt.th\"", ".", "format", "(", "path", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.shaq_learner.SHAQLearner.load_models": [[168, 175], ["shaq_learner.SHAQLearner.mac.load_models", "shaq_learner.SHAQLearner.target_mac.load_models", "shaq_learner.SHAQLearner.optimiser.load_state_dict", "shaq_learner.SHAQLearner.mixer.load_state_dict", "torch.load", "torch.load"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.load_models", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.load_models"], ["", "def", "load_models", "(", "self", ",", "path", ")", ":", "\n", "        ", "self", ".", "mac", ".", "load_models", "(", "path", ")", "\n", "# Not quite right but I don't want to save target networks", "\n", "self", ".", "target_mac", ".", "load_models", "(", "path", ")", "\n", "if", "self", ".", "mixer", "is", "not", "None", ":", "\n", "            ", "self", ".", "mixer", ".", "load_state_dict", "(", "th", ".", "load", "(", "\"{}/mixer.th\"", ".", "format", "(", "path", ")", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", ")", "\n", "", "self", ".", "optimiser", ".", "load_state_dict", "(", "th", ".", "load", "(", "\"{}/opt.th\"", ".", "format", "(", "path", ")", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.sqddpg_learner.SQDDPGLearner.__init__": [[15, 45], ["list", "copy.deepcopy", "torch.optim.RMSprop", "torch.optim.RMSprop", "torch.optim.RMSprop", "torch.optim.RMSprop", "sqddpg_learner.SQDDPGLearner.mac.parameters", "list", "copy.deepcopy", "modules.mixers.sqddpg.SQDDPGMixer", "ValueError", "sqddpg_learner.SQDDPGLearner.mixer.parameters"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.parameters", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.parameters"], ["    ", "def", "__init__", "(", "self", ",", "mac", ",", "scheme", ",", "logger", ",", "args", ")", ":", "\n", "        ", "self", ".", "args", "=", "args", "\n", "self", ".", "mac", "=", "mac", "\n", "self", ".", "logger", "=", "logger", "\n", "\n", "self", ".", "mac_params", "=", "list", "(", "self", ".", "mac", ".", "parameters", "(", ")", ")", "\n", "\n", "self", ".", "last_target_update_episode", "=", "0", "\n", "self", ".", "last_actor_update_episode", "=", "0", "\n", "self", ".", "last_critic_update_episode", "=", "0", "\n", "\n", "self", ".", "mixer", "=", "None", "\n", "assert", "args", ".", "mixer", "is", "not", "None", "\n", "if", "args", ".", "mixer", "is", "not", "None", ":", "\n", "            ", "if", "args", ".", "mixer", "==", "\"sqddpg\"", ":", "\n", "                ", "self", ".", "mixer", "=", "SQDDPGMixer", "(", "args", ")", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"Mixer {} not recognised.\"", ".", "format", "(", "args", ".", "mixer", ")", ")", "\n", "", "self", ".", "mixer_params", "=", "list", "(", "self", ".", "mixer", ".", "parameters", "(", ")", ")", "\n", "# self.params += list(self.mixer.parameters())", "\n", "self", ".", "target_mixer", "=", "copy", ".", "deepcopy", "(", "self", ".", "mixer", ")", "\n", "\n", "# a little wasteful to deepcopy (e.g. duplicates action selector), but should work for any MAC", "\n", "", "self", ".", "target_mac", "=", "copy", ".", "deepcopy", "(", "mac", ")", "\n", "\n", "self", ".", "optimiser_mac", "=", "RMSprop", "(", "params", "=", "self", ".", "mac_params", ",", "lr", "=", "args", ".", "lr", ",", "alpha", "=", "args", ".", "optim_alpha", ",", "eps", "=", "args", ".", "optim_eps", ")", "\n", "\n", "self", ".", "optimiser_mixer", "=", "RMSprop", "(", "params", "=", "self", ".", "mixer_params", ",", "lr", "=", "args", ".", "critic_lr", ",", "alpha", "=", "args", ".", "optim_alpha", ",", "eps", "=", "args", ".", "optim_eps", ")", "\n", "\n", "self", ".", "log_stats_t", "=", "-", "self", ".", "args", ".", "learner_log_interval", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.sqddpg_learner.SQDDPGLearner.train": [[46, 186], ["[].float", "[].float", "sqddpg_learner.SQDDPGLearner.mac.init_hidden", "range", "torch.stack", "torch.stack", "torch.softmax", "torch.softmax", "torch.softmax.detach", "torch.stack.clone", "sqddpg_learner.SQDDPGLearner.target_mac.init_hidden", "range", "torch.stack", "torch.stack", "sqddpg_learner.SQDDPGLearner.mixer().sum", "sqddpg_learner.SQDDPGLearner.target_mixer().sum", "sqddpg_learner.SQDDPGLearner.mixer", "[].float.expand_as", "sqddpg_learner.SQDDPGLearner.mac.forward", "torch.stack.append", "torch.gumbel_softmax", "torch.gumbel_softmax", "torch.softmax", "torch.softmax", "sqddpg_learner.SQDDPGLearner.target_mac.forward", "torch.stack.append", "Exception", "torch.stack.clone().detach", "torch.softmax", "torch.softmax", "torch.log_softmax", "torch.log_softmax", "targets.detach", "[].float.sum", "[].float.sum", "sqddpg_learner.SQDDPGLearner.optimiser_mixer.zero_grad", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "sqddpg_learner.SQDDPGLearner.optimiser_mixer.step", "sqddpg_learner.SQDDPGLearner.optimiser_mixer.zero_grad", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "sqddpg_learner.SQDDPGLearner.optimiser_mac.zero_grad", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "sqddpg_learner.SQDDPGLearner.optimiser_mac.step", "sqddpg_learner.SQDDPGLearner.optimiser_mac.zero_grad", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "sqddpg_learner.SQDDPGLearner._update_targets", "sqddpg_learner.SQDDPGLearner.logger.log_stat", "sqddpg_learner.SQDDPGLearner.logger.log_stat", "sqddpg_learner.SQDDPGLearner.logger.log_stat", "sqddpg_learner.SQDDPGLearner.logger.log_stat", "[].float.sum().item", "sqddpg_learner.SQDDPGLearner.logger.log_stat", "sqddpg_learner.SQDDPGLearner.logger.log_stat", "sqddpg_learner.SQDDPGLearner.logger.log_stat", "torch.gumbel_softmax", "torch.gumbel_softmax", "torch.softmax.detach", "torch.softmax", "torch.softmax", "torch.softmax.detach", "sqddpg_learner.SQDDPGLearner.mixer", "sqddpg_learner.SQDDPGLearner.target_mixer", "[].float.sum", "shapley_masked_td_error.pow().sum", "loss.item", "actor_loss.item", "shapley_loss.item", "logit_entropy.item", "torch.stack.clone", "F.softmax.detach.detach", "[].float.sum", "shapley_masked_td_error.pow"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.agents.central_rnn_agent.CentralRNNAgent.init_hidden", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.agents.central_rnn_agent.CentralRNNAgent.init_hidden", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.sqddpg.SQDDPGMixer.forward", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.sqddpg.SQDDPGMixer.forward", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.step", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.step", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.q_learner.QLearner._update_targets", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat"], ["", "def", "train", "(", "self", ",", "batch", ":", "EpisodeBatch", ",", "t_env", ":", "int", ",", "episode_num", ":", "int", ")", ":", "\n", "# Get the relevant quantities", "\n", "        ", "rewards", "=", "batch", "[", "\"reward\"", "]", "[", ":", ",", ":", "-", "1", "]", "\n", "actions", "=", "batch", "[", "\"actions\"", "]", "[", ":", ",", ":", "-", "1", "]", "\n", "terminated", "=", "batch", "[", "\"terminated\"", "]", "[", ":", ",", ":", "-", "1", "]", ".", "float", "(", ")", "\n", "mask", "=", "batch", "[", "\"filled\"", "]", "[", ":", ",", ":", "-", "1", "]", ".", "float", "(", ")", "\n", "mask", "[", ":", ",", "1", ":", "]", "=", "mask", "[", ":", ",", "1", ":", "]", "*", "(", "1", "-", "terminated", "[", ":", ",", ":", "-", "1", "]", ")", "\n", "avail_actions", "=", "batch", "[", "\"avail_actions\"", "]", "\n", "\n", "# data structure = (b, t, agent, a)", "\n", "# Calculate estimated Q-Values", "\n", "mac_out", "=", "[", "]", "\n", "self", ".", "mac", ".", "init_hidden", "(", "batch", ".", "batch_size", ")", "\n", "for", "t", "in", "range", "(", "batch", ".", "max_seq_length", ")", ":", "\n", "            ", "agent_outs", "=", "self", ".", "mac", ".", "forward", "(", "batch", ",", "t", "=", "t", ")", "\n", "mac_out", ".", "append", "(", "agent_outs", ")", "\n", "", "mac_out", "=", "th", ".", "stack", "(", "mac_out", ",", "dim", "=", "1", ")", "# Concat over time", "\n", "\n", "# actions for training critics", "\n", "policy_outs", "=", "F", ".", "softmax", "(", "mac_out", "[", ":", ",", ":", "-", "1", "]", ",", "dim", "=", "-", "1", ")", "\n", "# chosen_action_qvals_agents = th.gather(policy_outs, dim=3, index=actions) ", "\n", "# chosen_action_qvals = chosen_action_qvals_agents # attention to the detach()", "\n", "chosen_action_qvals", "=", "policy_outs", ".", "detach", "(", ")", "\n", "\n", "# for ddpg style policy training", "\n", "mac_out_clone", "=", "mac_out", ".", "clone", "(", ")", "\n", "mac_out_clone", "[", "avail_actions", "==", "0", "]", "=", "-", "9999999", "\n", "\n", "if", "self", ".", "args", ".", "gumbel_softmax", ":", "\n", "# gumbel softmax", "\n", "            ", "gumbel_actions_distr", "=", "F", ".", "gumbel_softmax", "(", "mac_out_clone", "[", ":", ",", ":", "-", "1", "]", ",", "hard", "=", "False", ",", "dim", "=", "-", "1", ",", "tau", "=", "self", ".", "args", ".", "policy_temp", ")", "\n", "# gumbel_actions_label = gumbel_actions_distr.clone().detach().max(dim=-1, keepdim=True)[1]", "\n", "# actor_actions = th.gather(gumbel_actions_distr, 3, gumbel_actions_label)", "\n", "actor_actions", "=", "gumbel_actions_distr", "\n", "", "else", ":", "\n", "# greedy", "\n", "            ", "greedy_actions_distr", "=", "F", ".", "softmax", "(", "mac_out_clone", "[", ":", ",", ":", "-", "1", "]", ",", "dim", "=", "-", "1", ")", "\n", "# greedy_actions_label = greedy_actions_distr.clone().detach().max(dim=-1, keepdim=True)[1]", "\n", "# actor_actions = th.gather(greedy_actions_distr, 3, greedy_actions_label)", "\n", "actor_actions", "=", "greedy_actions_distr", "\n", "\n", "# Calculate the Q-Values necessary for the target", "\n", "", "target_mac_out", "=", "[", "]", "\n", "self", ".", "target_mac", ".", "init_hidden", "(", "batch", ".", "batch_size", ")", "\n", "for", "t", "in", "range", "(", "batch", ".", "max_seq_length", ")", ":", "\n", "            ", "target_agent_outs", "=", "self", ".", "target_mac", ".", "forward", "(", "batch", ",", "t", "=", "t", ")", "\n", "target_mac_out", ".", "append", "(", "target_agent_outs", ")", "\n", "\n", "# We don't need the first timesteps Q-Value estimate for calculating targets", "\n", "", "target_mac_out", "=", "th", ".", "stack", "(", "target_mac_out", "[", ":", "]", ",", "dim", "=", "1", ")", "# Concat across time", "\n", "\n", "# Max over target Q-Values", "\n", "if", "self", ".", "args", ".", "double_q", ":", "\n", "            ", "raise", "Exception", "(", "\"No double q for DDPG\"", ")", "\n", "", "else", ":", "\n", "# target_mac_out_detach = mac_out.clone().detach()", "\n", "            ", "target_mac_out_detach", "=", "target_mac_out", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "target_mac_out_detach", "[", "avail_actions", "==", "0", "]", "=", "-", "9999999", "\n", "# target_mac_out_detach = target_mac_out.clone().detach()", "\n", "# target_mac_out_detach[avail_actions == 0] = -9999999", "\n", "\n", "if", "self", ".", "args", ".", "gumbel_softmax", ":", "\n", "# gumbel softmax", "\n", "                ", "target_actions_distr", "=", "F", ".", "gumbel_softmax", "(", "target_mac_out_detach", "[", ":", ",", "1", ":", "]", ",", "hard", "=", "False", ",", "dim", "=", "-", "1", ",", "tau", "=", "self", ".", "args", ".", "policy_temp", ")", "\n", "# target_max_actions_label = target_actions_distr.clone().detach().max(dim=-1, keepdim=True)[1]", "\n", "# target_max_qvals = th.gather(target_actions_distr, 3, target_max_actions_label)", "\n", "target_max_qvals", "=", "target_actions_distr", ".", "detach", "(", ")", "\n", "", "else", ":", "\n", "# greedy", "\n", "                ", "target_actions_distr", "=", "F", ".", "softmax", "(", "target_mac_out_detach", "[", ":", ",", "1", ":", "]", ",", "dim", "=", "-", "1", ")", "\n", "# target_max_actions_label = target_actions_distr.clone().detach().max(dim=-1, keepdim=True)[1]", "\n", "# target_max_qvals = th.gather(target_actions_distr, 3, target_max_actions_label)", "\n", "target_max_qvals", "=", "target_actions_distr", ".", "detach", "(", ")", "\n", "\n", "# get shapley values", "\n", "", "", "shapley_values_sum", "=", "self", ".", "mixer", "(", "batch", "[", "\"state\"", "]", "[", ":", ",", ":", "-", "1", "]", ",", "chosen_action_qvals", ".", "detach", "(", ")", ")", ".", "sum", "(", "dim", "=", "2", ",", "keepdim", "=", "True", ")", "\n", "# shapley_values_sum = self.mixer(batch[\"state\"][:, :-1], chosen_action_qvals).sum(dim=2, keepdim=True)", "\n", "target_shapley_values_sum", "=", "self", ".", "target_mixer", "(", "batch", "[", "\"state\"", "]", "[", ":", ",", "1", ":", "]", ",", "target_max_qvals", ")", ".", "sum", "(", "dim", "=", "2", ",", "keepdim", "=", "True", ")", "\n", "shapley_value_actor", "=", "self", ".", "mixer", "(", "batch", "[", "\"state\"", "]", "[", ":", ",", ":", "-", "1", "]", ",", "actor_actions", ")", "\n", "\n", "# Calculate 1-step Q-Learning targets", "\n", "targets", "=", "rewards", "+", "self", ".", "args", ".", "gamma", "*", "(", "1", "-", "terminated", ")", "*", "target_shapley_values_sum", "\n", "\n", "# Logit entropy (correct)", "\n", "ps", "=", "F", ".", "softmax", "(", "mac_out", "[", ":", ",", ":", "-", "1", "]", ",", "dim", "=", "3", ")", "*", "avail_actions", "[", ":", ",", ":", "-", "1", "]", "\n", "log_ps", "=", "F", ".", "log_softmax", "(", "mac_out", "[", ":", ",", ":", "-", "1", "]", ",", "dim", "=", "3", ")", "*", "avail_actions", "[", ":", ",", ":", "-", "1", "]", "\n", "logit_entropy", "=", "-", "(", "(", "(", "ps", "*", "log_ps", ")", ".", "sum", "(", "dim", "=", "3", ")", "*", "mask", ")", ".", "sum", "(", ")", "/", "mask", ".", "sum", "(", ")", ")", "\n", "\n", "shapley_td_error", "=", "(", "shapley_values_sum", "-", "targets", ".", "detach", "(", ")", ")", "\n", "shapley_mask", "=", "mask", ".", "expand_as", "(", "shapley_td_error", ")", "\n", "shapley_masked_td_error", "=", "shapley_td_error", "*", "shapley_mask", "\n", "shapley_loss", "=", "0.5", "*", "shapley_masked_td_error", ".", "pow", "(", "2", ")", ".", "sum", "(", ")", "/", "mask", ".", "sum", "(", ")", "\n", "\n", "actor_loss", "=", "-", "(", "shapley_value_actor", "*", "shapley_mask", ")", ".", "sum", "(", ")", "/", "mask", ".", "sum", "(", ")", "\n", "\n", "loss", "=", "self", ".", "args", ".", "actor_loss", "*", "actor_loss", "+", "self", ".", "args", ".", "shapley_loss", "*", "shapley_loss", "+", "-", "self", ".", "args", ".", "logit_entropy", "*", "logit_entropy", "\n", "\n", "# self.optimiser_mac.zero_grad()", "\n", "# self.optimiser_mixer.zero_grad()", "\n", "# loss.backward()", "\n", "\n", "if", "(", "episode_num", "-", "self", ".", "last_critic_update_episode", ")", "/", "self", ".", "args", ".", "critic_update_interval", ">=", "1.0", ":", "\n", "            ", "self", ".", "optimiser_mixer", ".", "zero_grad", "(", ")", "\n", "(", "self", ".", "args", ".", "shapley_loss", "*", "shapley_loss", ")", ".", "backward", "(", ")", "\n", "mixer_grad_norm", "=", "th", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "mixer_params", ",", "self", ".", "args", ".", "grad_norm_clip", ")", "\n", "self", ".", "optimiser_mixer", ".", "step", "(", ")", "\n", "self", ".", "last_critic_update_episode", "=", "episode_num", "\n", "", "else", ":", "\n", "            ", "self", ".", "optimiser_mixer", ".", "zero_grad", "(", ")", "\n", "(", "self", ".", "args", ".", "shapley_loss", "*", "shapley_loss", ")", ".", "backward", "(", ")", "\n", "mixer_grad_norm", "=", "th", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "mixer_params", ",", "self", ".", "args", ".", "grad_norm_clip", ")", "\n", "\n", "", "if", "(", "episode_num", "-", "self", ".", "last_actor_update_episode", ")", "/", "self", ".", "args", ".", "actor_update_interval", ">=", "1.0", ":", "\n", "            ", "self", ".", "optimiser_mac", ".", "zero_grad", "(", ")", "\n", "(", "self", ".", "args", ".", "actor_loss", "*", "actor_loss", "-", "self", ".", "args", ".", "logit_entropy", "*", "logit_entropy", ")", ".", "backward", "(", ")", "\n", "mac_grad_norm", "=", "th", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "mac_params", ",", "self", ".", "args", ".", "grad_norm_clip", ")", "\n", "self", ".", "optimiser_mac", ".", "step", "(", ")", "\n", "self", ".", "last_actor_update_episode", "=", "episode_num", "\n", "", "else", ":", "\n", "            ", "self", ".", "optimiser_mac", ".", "zero_grad", "(", ")", "\n", "(", "self", ".", "args", ".", "actor_loss", "*", "actor_loss", "-", "self", ".", "args", ".", "logit_entropy", "*", "logit_entropy", ")", ".", "backward", "(", ")", "\n", "mac_grad_norm", "=", "th", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "mac_params", ",", "self", ".", "args", ".", "grad_norm_clip", ")", "\n", "\n", "\n", "", "if", "(", "episode_num", "-", "self", ".", "last_target_update_episode", ")", "/", "self", ".", "args", ".", "target_update_interval", ">=", "1.0", ":", "\n", "            ", "self", ".", "_update_targets", "(", ")", "\n", "self", ".", "last_target_update_episode", "=", "episode_num", "\n", "\n", "", "if", "t_env", "-", "self", ".", "log_stats_t", ">=", "self", ".", "args", ".", "learner_log_interval", ":", "\n", "            ", "self", ".", "logger", ".", "log_stat", "(", "\"loss\"", ",", "loss", ".", "item", "(", ")", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"actor_loss\"", ",", "actor_loss", ".", "item", "(", ")", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"mac_grad_norm\"", ",", "mac_grad_norm", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"mixer_grad_norm\"", ",", "mixer_grad_norm", ",", "t_env", ")", "\n", "mask_elems", "=", "mask", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "# self.logger.log_stat(\"q_taken_mean\", (chosen_action_qvals.squeeze(3) * mask).sum().item()/(mask_elems * self.args.n_agents), t_env)", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"target_mean\"", ",", "(", "targets", "*", "mask", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "/", "(", "mask_elems", "*", "self", ".", "args", ".", "n_agents", ")", ",", "t_env", ")", "\n", "# self.logger.log_stat(\"central_loss\", central_loss.item(), t_env)", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"shapley_loss\"", ",", "shapley_loss", ".", "item", "(", ")", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"logit_entropy\"", ",", "logit_entropy", ".", "item", "(", ")", ",", "t_env", ")", "\n", "self", ".", "log_stats_t", "=", "t_env", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.sqddpg_learner.SQDDPGLearner._update_targets": [[187, 198], ["sqddpg_learner.SQDDPGLearner.target_mac.agent.state_dict().items", "sqddpg_learner.SQDDPGLearner.logger.console_logger.info", "[].copy_", "sqddpg_learner.SQDDPGLearner.target_mixer.state_dict().items", "sqddpg_learner.SQDDPGLearner.target_mac.agent.state_dict", "[].copy_", "sqddpg_learner.SQDDPGLearner.target_mixer.state_dict", "sqddpg_learner.SQDDPGLearner.mac.agent.state_dict", "sqddpg_learner.SQDDPGLearner.target_mac.agent.state_dict", "sqddpg_learner.SQDDPGLearner.mixer.state_dict", "sqddpg_learner.SQDDPGLearner.target_mixer.state_dict"], "methods", ["None"], ["", "", "def", "_update_targets", "(", "self", ")", ":", "\n", "# self.target_mac.load_state(self.mac)", "\n", "        ", "for", "name", ",", "param", "in", "self", ".", "target_mac", ".", "agent", ".", "state_dict", "(", ")", ".", "items", "(", ")", ":", "\n", "            ", "update_params", "=", "(", "1", "-", "self", ".", "args", ".", "target_lr", ")", "*", "param", "+", "self", ".", "args", ".", "target_lr", "*", "self", ".", "mac", ".", "agent", ".", "state_dict", "(", ")", "[", "name", "]", "\n", "self", ".", "target_mac", ".", "agent", ".", "state_dict", "(", ")", "[", "name", "]", ".", "copy_", "(", "update_params", ")", "\n", "", "if", "self", ".", "mixer", "is", "not", "None", ":", "\n", "# self.target_mixer.load_state_dict(self.mixer.state_dict())", "\n", "            ", "for", "name", ",", "param", "in", "self", ".", "target_mixer", ".", "state_dict", "(", ")", ".", "items", "(", ")", ":", "\n", "                ", "update_params", "=", "(", "1", "-", "self", ".", "args", ".", "target_lr", ")", "*", "param", "+", "self", ".", "args", ".", "target_lr", "*", "self", ".", "mixer", ".", "state_dict", "(", ")", "[", "name", "]", "\n", "self", ".", "target_mixer", ".", "state_dict", "(", ")", "[", "name", "]", ".", "copy_", "(", "update_params", ")", "\n", "", "", "self", ".", "logger", ".", "console_logger", ".", "info", "(", "\"Updated target network\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.sqddpg_learner.SQDDPGLearner.cuda": [[199, 205], ["sqddpg_learner.SQDDPGLearner.mac.cuda", "sqddpg_learner.SQDDPGLearner.target_mac.cuda", "sqddpg_learner.SQDDPGLearner.mixer.cuda", "sqddpg_learner.SQDDPGLearner.target_mixer.cuda"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda"], ["", "def", "cuda", "(", "self", ")", ":", "\n", "        ", "self", ".", "mac", ".", "cuda", "(", ")", "\n", "self", ".", "target_mac", ".", "cuda", "(", ")", "\n", "if", "self", ".", "mixer", "is", "not", "None", ":", "\n", "            ", "self", ".", "mixer", ".", "cuda", "(", ")", "\n", "self", ".", "target_mixer", ".", "cuda", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.sqddpg_learner.SQDDPGLearner.save_models": [[206, 213], ["sqddpg_learner.SQDDPGLearner.mac.save_models", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "sqddpg_learner.SQDDPGLearner.optimiser_mac.state_dict", "sqddpg_learner.SQDDPGLearner.optimiser_mixer.state_dict", "sqddpg_learner.SQDDPGLearner.mixer.state_dict", "sqddpg_learner.SQDDPGLearner.target_mixer.state_dict"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.save_models"], ["", "", "def", "save_models", "(", "self", ",", "path", ")", ":", "\n", "        ", "self", ".", "mac", ".", "save_models", "(", "path", ")", "\n", "if", "self", ".", "mixer", "is", "not", "None", ":", "\n", "            ", "th", ".", "save", "(", "self", ".", "mixer", ".", "state_dict", "(", ")", ",", "\"{}/mixer.th\"", ".", "format", "(", "path", ")", ")", "\n", "th", ".", "save", "(", "self", ".", "target_mixer", ".", "state_dict", "(", ")", ",", "\"{}/target_mixer.th\"", ".", "format", "(", "path", ")", ")", "\n", "", "th", ".", "save", "(", "self", ".", "optimiser_mac", ".", "state_dict", "(", ")", ",", "\"{}/opt_mac.th\"", ".", "format", "(", "path", ")", ")", "\n", "th", ".", "save", "(", "self", ".", "optimiser_mixer", ".", "state_dict", "(", ")", ",", "\"{}/opt_mixer.th\"", ".", "format", "(", "path", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.sqddpg_learner.SQDDPGLearner.load_models": [[214, 223], ["sqddpg_learner.SQDDPGLearner.mac.load_models", "sqddpg_learner.SQDDPGLearner.target_mac.load_models", "sqddpg_learner.SQDDPGLearner.optimiser_mac.load_state_dict", "sqddpg_learner.SQDDPGLearner.optimiser_mixer.load_state_dict", "sqddpg_learner.SQDDPGLearner.mixer.load_state_dict", "sqddpg_learner.SQDDPGLearner.target_mixer.load_state_dict", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.load_models", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.load_models"], ["", "def", "load_models", "(", "self", ",", "path", ")", ":", "\n", "        ", "self", ".", "mac", ".", "load_models", "(", "path", ")", "\n", "# Not quite right but I don't want to save target networks", "\n", "self", ".", "target_mac", ".", "load_models", "(", "path", ")", "\n", "if", "self", ".", "mixer", "is", "not", "None", ":", "\n", "            ", "self", ".", "mixer", ".", "load_state_dict", "(", "th", ".", "load", "(", "\"{}/mixer.th\"", ".", "format", "(", "path", ")", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", ")", "\n", "self", ".", "target_mixer", ".", "load_state_dict", "(", "th", ".", "load", "(", "\"{}/target_mixer.th\"", ".", "format", "(", "path", ")", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", ")", "\n", "", "self", ".", "optimiser_mac", ".", "load_state_dict", "(", "th", ".", "load", "(", "\"{}/opt_mac.th\"", ".", "format", "(", "path", ")", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", ")", "\n", "self", ".", "optimiser_mixer", ".", "load_state_dict", "(", "th", ".", "load", "(", "\"{}/opt_mixer.th\"", ".", "format", "(", "path", ")", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.dmaq_qatten_learner.DMAQ_qattenLearner.__init__": [[13, 41], ["list", "torch.optim.RMSprop", "torch.optim.RMSprop", "copy.deepcopy", "mac.parameters", "list", "copy.deepcopy", "modules.mixers.dmaq_general.DMAQer", "dmaq_qatten_learner.DMAQ_qattenLearner.mixer.parameters", "modules.mixers.dmaq_qatten.DMAQ_QattenMixer", "ValueError"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.parameters", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.parameters"], ["    ", "def", "__init__", "(", "self", ",", "mac", ",", "scheme", ",", "logger", ",", "args", ")", ":", "\n", "        ", "self", ".", "args", "=", "args", "\n", "self", ".", "mac", "=", "mac", "\n", "self", ".", "logger", "=", "logger", "\n", "\n", "self", ".", "params", "=", "list", "(", "mac", ".", "parameters", "(", ")", ")", "\n", "\n", "self", ".", "last_target_update_episode", "=", "0", "\n", "\n", "self", ".", "mixer", "=", "None", "\n", "if", "args", ".", "mixer", "is", "not", "None", ":", "\n", "            ", "if", "args", ".", "mixer", "==", "\"dmaq\"", ":", "\n", "                ", "self", ".", "mixer", "=", "DMAQer", "(", "args", ")", "\n", "", "elif", "args", ".", "mixer", "==", "'dmaq_qatten'", ":", "\n", "                ", "self", ".", "mixer", "=", "DMAQ_QattenMixer", "(", "args", ")", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"Mixer {} not recognised.\"", ".", "format", "(", "args", ".", "mixer", ")", ")", "\n", "", "self", ".", "params", "+=", "list", "(", "self", ".", "mixer", ".", "parameters", "(", ")", ")", "\n", "self", ".", "target_mixer", "=", "copy", ".", "deepcopy", "(", "self", ".", "mixer", ")", "\n", "\n", "", "self", ".", "optimiser", "=", "RMSprop", "(", "params", "=", "self", ".", "params", ",", "lr", "=", "args", ".", "lr", ",", "alpha", "=", "args", ".", "optim_alpha", ",", "eps", "=", "args", ".", "optim_eps", ")", "\n", "\n", "# a little wasteful to deepcopy (e.g. duplicates action selector), but should work for any MAC", "\n", "self", ".", "target_mac", "=", "copy", ".", "deepcopy", "(", "mac", ")", "\n", "\n", "self", ".", "log_stats_t", "=", "-", "self", ".", "args", ".", "learner_log_interval", "-", "1", "\n", "\n", "self", ".", "n_actions", "=", "self", ".", "args", ".", "n_actions", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.dmaq_qatten_learner.DMAQ_qattenLearner.sub_train": [[42, 189], ["[].float", "[].float", "mac.init_hidden", "range", "torch.stack", "torch.stack", "torch.gather().squeeze", "torch.gather().squeeze", "torch.stack.clone().detach", "x_mac_out[].max", "max_action_index.detach().unsqueeze.detach().unsqueeze.detach().unsqueeze", "dmaq_qatten_learner.DMAQ_qattenLearner.target_mac.init_hidden", "range", "torch.stack", "torch.stack", "mask.expand_as.expand_as.expand_as", "optimiser.zero_grad", "loss.backward", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "optimiser.step", "mac.forward", "torch.stack.append", "torch.gather().squeeze.detach().cpu().numpy", "dmaq_qatten_learner.DMAQ_qattenLearner.target_mac.forward", "torch.stack.append", "torch.stack.clone().detach", "torch.gather().squeeze", "torch.gather().squeeze", "cur_max_actions.detach", "torch.zeros().cuda", "torch.zeros().cuda", "cur_max_actions_onehot.scatter_.scatter_.scatter_", "dmaq_qatten_learner.DMAQ_qattenLearner.target_mac.init_hidden", "range", "torch.stack", "torch.stack", "torch.gather().squeeze.detach().cpu().numpy", "targets.detach().cpu().numpy", "print", "dmaq_qatten_learner.DMAQ_qattenLearner.logger.log_stat", "targets.detach", "torch.mean", "torch.mean", "masked_hit_prob.sum", "mask.expand_as.expand_as.sum", "dmaq_qatten_learner.DMAQ_qattenLearner.logger.log_stat", "dmaq_qatten_learner.DMAQ_qattenLearner.logger.log_stat", "dmaq_qatten_learner.DMAQ_qattenLearner.logger.log_stat", "mask.expand_as.expand_as.sum().item", "dmaq_qatten_learner.DMAQ_qattenLearner.logger.log_stat", "dmaq_qatten_learner.DMAQ_qattenLearner.logger.log_stat", "dmaq_qatten_learner.DMAQ_qattenLearner.logger.log_stat", "torch.gather", "torch.gather", "torch.stack.clone", "max_action_index.detach().unsqueeze.detach().unsqueeze.detach", "mac_out_detach[].max", "torch.stack.max", "dmaq_qatten_learner.DMAQ_qattenLearner.target_mac.forward", "torch.stack.append", "torch.stack.max", "mixer", "mixer", "mixer", "mixer", "dmaq_qatten_learner.DMAQ_qattenLearner.target_mixer", "numpy.squeeze", "numpy.squeeze", "numpy.squeeze", "numpy.squeeze", "numpy.squeeze", "mask.expand_as.expand_as.sum", "loss.item", "hit_prob.item", "torch.gather().squeeze.detach().cpu", "torch.stack.clone", "torch.gather", "torch.gather", "torch.zeros", "torch.zeros", "dmaq_qatten_learner.DMAQ_qattenLearner.target_mixer", "dmaq_qatten_learner.DMAQ_qattenLearner.target_mixer", "dmaq_qatten_learner.DMAQ_qattenLearner.target_mixer", "dmaq_qatten_learner.DMAQ_qattenLearner.target_mixer", "torch.gather().squeeze.detach().cpu", "targets.detach().cpu", "mask.expand_as.expand_as.sum", "mask.expand_as.expand_as.sum", "masked_td_error.abs().sum().item", "torch.gather().squeeze.detach", "torch.gather().squeeze.detach", "targets.detach", "masked_td_error.abs().sum", "cur_max_actions.squeeze", "masked_td_error.abs"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.agents.central_rnn_agent.CentralRNNAgent.init_hidden", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.agents.central_rnn_agent.CentralRNNAgent.init_hidden", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.step", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.sqddpg.SQDDPGMixer.forward", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.sqddpg.SQDDPGMixer.forward", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.agents.central_rnn_agent.CentralRNNAgent.init_hidden", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.sqddpg.SQDDPGMixer.forward"], ["", "def", "sub_train", "(", "self", ",", "batch", ":", "EpisodeBatch", ",", "t_env", ":", "int", ",", "episode_num", ":", "int", ",", "mac", ",", "mixer", ",", "optimiser", ",", "params", ",", "\n", "show_demo", "=", "False", ",", "save_data", "=", "None", ")", ":", "\n", "# Get the relevant quantities", "\n", "        ", "rewards", "=", "batch", "[", "\"reward\"", "]", "[", ":", ",", ":", "-", "1", "]", "\n", "actions", "=", "batch", "[", "\"actions\"", "]", "[", ":", ",", ":", "-", "1", "]", "\n", "terminated", "=", "batch", "[", "\"terminated\"", "]", "[", ":", ",", ":", "-", "1", "]", ".", "float", "(", ")", "\n", "mask", "=", "batch", "[", "\"filled\"", "]", "[", ":", ",", ":", "-", "1", "]", ".", "float", "(", ")", "\n", "mask", "[", ":", ",", "1", ":", "]", "=", "mask", "[", ":", ",", "1", ":", "]", "*", "(", "1", "-", "terminated", "[", ":", ",", ":", "-", "1", "]", ")", "\n", "avail_actions", "=", "batch", "[", "\"avail_actions\"", "]", "\n", "actions_onehot", "=", "batch", "[", "\"actions_onehot\"", "]", "[", ":", ",", ":", "-", "1", "]", "\n", "\n", "# Calculate estimated Q-Values", "\n", "mac_out", "=", "[", "]", "\n", "mac", ".", "init_hidden", "(", "batch", ".", "batch_size", ")", "\n", "for", "t", "in", "range", "(", "batch", ".", "max_seq_length", ")", ":", "\n", "            ", "agent_outs", "=", "mac", ".", "forward", "(", "batch", ",", "t", "=", "t", ")", "\n", "mac_out", ".", "append", "(", "agent_outs", ")", "\n", "", "mac_out", "=", "th", ".", "stack", "(", "mac_out", ",", "dim", "=", "1", ")", "# Concat over time", "\n", "\n", "# Pick the Q-Values for the actions taken by each agent", "\n", "chosen_action_qvals", "=", "th", ".", "gather", "(", "mac_out", "[", ":", ",", ":", "-", "1", "]", ",", "dim", "=", "3", ",", "index", "=", "actions", ")", ".", "squeeze", "(", "3", ")", "# Remove the last dim", "\n", "\n", "x_mac_out", "=", "mac_out", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "x_mac_out", "[", "avail_actions", "==", "0", "]", "=", "-", "9999999", "\n", "max_action_qvals", ",", "max_action_index", "=", "x_mac_out", "[", ":", ",", ":", "-", "1", "]", ".", "max", "(", "dim", "=", "3", ")", "\n", "\n", "max_action_index", "=", "max_action_index", ".", "detach", "(", ")", ".", "unsqueeze", "(", "3", ")", "\n", "is_max_action", "=", "(", "max_action_index", "==", "actions", ")", ".", "int", "(", ")", ".", "float", "(", ")", "\n", "\n", "if", "show_demo", ":", "\n", "            ", "q_i_data", "=", "chosen_action_qvals", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "q_data", "=", "(", "max_action_qvals", "-", "chosen_action_qvals", ")", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "# self.logger.log_stat('agent_1_%d_q_1' % save_data[0], np.squeeze(q_data)[0], t_env)", "\n", "# self.logger.log_stat('agent_2_%d_q_2' % save_data[1], np.squeeze(q_data)[1], t_env)", "\n", "\n", "# Calculate the Q-Values necessary for the target", "\n", "", "target_mac_out", "=", "[", "]", "\n", "self", ".", "target_mac", ".", "init_hidden", "(", "batch", ".", "batch_size", ")", "\n", "for", "t", "in", "range", "(", "batch", ".", "max_seq_length", ")", ":", "\n", "            ", "target_agent_outs", "=", "self", ".", "target_mac", ".", "forward", "(", "batch", ",", "t", "=", "t", ")", "\n", "target_mac_out", ".", "append", "(", "target_agent_outs", ")", "\n", "\n", "# We don't need the first timesteps Q-Value estimate for calculating targets", "\n", "", "target_mac_out", "=", "th", ".", "stack", "(", "target_mac_out", "[", "1", ":", "]", ",", "dim", "=", "1", ")", "# Concat across time", "\n", "\n", "# Mask out unavailable actions", "\n", "target_mac_out", "[", "avail_actions", "[", ":", ",", "1", ":", "]", "==", "0", "]", "=", "-", "9999999", "\n", "\n", "# Max over target Q-Values", "\n", "if", "self", ".", "args", ".", "double_q", ":", "\n", "# Get actions that maximise live Q (for double q-learning)", "\n", "            ", "mac_out_detach", "=", "mac_out", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "mac_out_detach", "[", "avail_actions", "==", "0", "]", "=", "-", "9999999", "\n", "cur_max_actions", "=", "mac_out_detach", "[", ":", ",", "1", ":", "]", ".", "max", "(", "dim", "=", "3", ",", "keepdim", "=", "True", ")", "[", "1", "]", "\n", "target_chosen_qvals", "=", "th", ".", "gather", "(", "target_mac_out", ",", "3", ",", "cur_max_actions", ")", ".", "squeeze", "(", "3", ")", "\n", "target_max_qvals", "=", "target_mac_out", ".", "max", "(", "dim", "=", "3", ")", "[", "0", "]", "\n", "target_next_actions", "=", "cur_max_actions", ".", "detach", "(", ")", "\n", "\n", "cur_max_actions_onehot", "=", "th", ".", "zeros", "(", "cur_max_actions", ".", "squeeze", "(", "3", ")", ".", "shape", "+", "(", "self", ".", "n_actions", ",", ")", ")", ".", "cuda", "(", ")", "\n", "cur_max_actions_onehot", "=", "cur_max_actions_onehot", ".", "scatter_", "(", "3", ",", "cur_max_actions", ",", "1", ")", "\n", "", "else", ":", "\n", "# Calculate the Q-Values necessary for the target", "\n", "            ", "target_mac_out", "=", "[", "]", "\n", "self", ".", "target_mac", ".", "init_hidden", "(", "batch", ".", "batch_size", ")", "\n", "for", "t", "in", "range", "(", "batch", ".", "max_seq_length", ")", ":", "\n", "                ", "target_agent_outs", "=", "self", ".", "target_mac", ".", "forward", "(", "batch", ",", "t", "=", "t", ")", "\n", "target_mac_out", ".", "append", "(", "target_agent_outs", ")", "\n", "# We don't need the first timesteps Q-Value estimate for calculating targets", "\n", "", "target_mac_out", "=", "th", ".", "stack", "(", "target_mac_out", "[", "1", ":", "]", ",", "dim", "=", "1", ")", "# Concat across time", "\n", "target_max_qvals", "=", "target_mac_out", ".", "max", "(", "dim", "=", "3", ")", "[", "0", "]", "\n", "\n", "# Mix", "\n", "", "if", "mixer", "is", "not", "None", ":", "\n", "            ", "if", "self", ".", "args", ".", "mixer", "==", "\"dmaq_qatten\"", ":", "\n", "                ", "ans_chosen", ",", "q_attend_regs", ",", "head_entropies", "=", "mixer", "(", "chosen_action_qvals", ",", "batch", "[", "\"state\"", "]", "[", ":", ",", ":", "-", "1", "]", ",", "is_v", "=", "True", ")", "\n", "ans_adv", ",", "_", ",", "_", "=", "mixer", "(", "chosen_action_qvals", ",", "batch", "[", "\"state\"", "]", "[", ":", ",", ":", "-", "1", "]", ",", "actions", "=", "actions_onehot", ",", "\n", "max_q_i", "=", "max_action_qvals", ",", "is_v", "=", "False", ")", "\n", "chosen_action_qvals", "=", "ans_chosen", "+", "ans_adv", "\n", "", "else", ":", "\n", "                ", "ans_chosen", "=", "mixer", "(", "chosen_action_qvals", ",", "batch", "[", "\"state\"", "]", "[", ":", ",", ":", "-", "1", "]", ",", "is_v", "=", "True", ")", "\n", "ans_adv", "=", "mixer", "(", "chosen_action_qvals", ",", "batch", "[", "\"state\"", "]", "[", ":", ",", ":", "-", "1", "]", ",", "actions", "=", "actions_onehot", ",", "\n", "max_q_i", "=", "max_action_qvals", ",", "is_v", "=", "False", ")", "\n", "chosen_action_qvals", "=", "ans_chosen", "+", "ans_adv", "\n", "\n", "", "if", "self", ".", "args", ".", "double_q", ":", "\n", "                ", "if", "self", ".", "args", ".", "mixer", "==", "\"dmaq_qatten\"", ":", "\n", "                    ", "target_chosen", ",", "_", ",", "_", "=", "self", ".", "target_mixer", "(", "target_chosen_qvals", ",", "batch", "[", "\"state\"", "]", "[", ":", ",", "1", ":", "]", ",", "is_v", "=", "True", ")", "\n", "target_adv", ",", "_", ",", "_", "=", "self", ".", "target_mixer", "(", "target_chosen_qvals", ",", "batch", "[", "\"state\"", "]", "[", ":", ",", "1", ":", "]", ",", "\n", "actions", "=", "cur_max_actions_onehot", ",", "\n", "max_q_i", "=", "target_max_qvals", ",", "is_v", "=", "False", ")", "\n", "target_max_qvals", "=", "target_chosen", "+", "target_adv", "\n", "", "else", ":", "\n", "                    ", "target_chosen", "=", "self", ".", "target_mixer", "(", "target_chosen_qvals", ",", "batch", "[", "\"state\"", "]", "[", ":", ",", "1", ":", "]", ",", "is_v", "=", "True", ")", "\n", "target_adv", "=", "self", ".", "target_mixer", "(", "target_chosen_qvals", ",", "batch", "[", "\"state\"", "]", "[", ":", ",", "1", ":", "]", ",", "\n", "actions", "=", "cur_max_actions_onehot", ",", "\n", "max_q_i", "=", "target_max_qvals", ",", "is_v", "=", "False", ")", "\n", "target_max_qvals", "=", "target_chosen", "+", "target_adv", "\n", "", "", "else", ":", "\n", "                ", "target_max_qvals", "=", "self", ".", "target_mixer", "(", "target_max_qvals", ",", "batch", "[", "\"state\"", "]", "[", ":", ",", "1", ":", "]", ",", "is_v", "=", "True", ")", "\n", "\n", "# Calculate 1-step Q-Learning targets", "\n", "", "", "targets", "=", "rewards", "+", "self", ".", "args", ".", "gamma", "*", "(", "1", "-", "terminated", ")", "*", "target_max_qvals", "\n", "\n", "if", "show_demo", ":", "\n", "            ", "tot_q_data", "=", "chosen_action_qvals", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "tot_target", "=", "targets", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "print", "(", "'action_pair_%d_%d'", "%", "(", "save_data", "[", "0", "]", ",", "save_data", "[", "1", "]", ")", ",", "np", ".", "squeeze", "(", "q_data", "[", ":", ",", "0", "]", ")", ",", "\n", "np", ".", "squeeze", "(", "q_i_data", "[", ":", ",", "0", "]", ")", ",", "np", ".", "squeeze", "(", "tot_q_data", "[", ":", ",", "0", "]", ")", ",", "np", ".", "squeeze", "(", "tot_target", "[", ":", ",", "0", "]", ")", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "'action_pair_%d_%d'", "%", "(", "save_data", "[", "0", "]", ",", "save_data", "[", "1", "]", ")", ",", "\n", "np", ".", "squeeze", "(", "tot_q_data", "[", ":", ",", "0", "]", ")", ",", "t_env", ")", "\n", "return", "\n", "\n", "# Td-error", "\n", "", "td_error", "=", "(", "chosen_action_qvals", "-", "targets", ".", "detach", "(", ")", ")", "\n", "\n", "mask", "=", "mask", ".", "expand_as", "(", "td_error", ")", "\n", "\n", "# 0-out the targets that came from padded data", "\n", "masked_td_error", "=", "td_error", "*", "mask", "\n", "\n", "# Normal L2 loss, take mean over actual data", "\n", "if", "self", ".", "args", ".", "mixer", "==", "\"dmaq_qatten\"", ":", "\n", "            ", "loss", "=", "(", "masked_td_error", "**", "2", ")", ".", "sum", "(", ")", "/", "mask", ".", "sum", "(", ")", "+", "q_attend_regs", "\n", "", "else", ":", "\n", "            ", "loss", "=", "(", "masked_td_error", "**", "2", ")", ".", "sum", "(", ")", "/", "mask", ".", "sum", "(", ")", "\n", "\n", "", "masked_hit_prob", "=", "th", ".", "mean", "(", "is_max_action", ",", "dim", "=", "2", ")", "*", "mask", "\n", "hit_prob", "=", "masked_hit_prob", ".", "sum", "(", ")", "/", "mask", ".", "sum", "(", ")", "\n", "\n", "# Optimise", "\n", "optimiser", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "grad_norm", "=", "th", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "params", ",", "self", ".", "args", ".", "grad_norm_clip", ")", "\n", "optimiser", ".", "step", "(", ")", "\n", "\n", "if", "t_env", "-", "self", ".", "log_stats_t", ">=", "self", ".", "args", ".", "learner_log_interval", ":", "\n", "            ", "self", ".", "logger", ".", "log_stat", "(", "\"loss\"", ",", "loss", ".", "item", "(", ")", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"hit_prob\"", ",", "hit_prob", ".", "item", "(", ")", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"grad_norm\"", ",", "grad_norm", ",", "t_env", ")", "\n", "mask_elems", "=", "mask", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"td_error_abs\"", ",", "(", "masked_td_error", ".", "abs", "(", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "/", "mask_elems", ")", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"q_taken_mean\"", ",", "\n", "(", "chosen_action_qvals", "*", "mask", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "/", "(", "mask_elems", "*", "self", ".", "args", ".", "n_agents", ")", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"target_mean\"", ",", "(", "targets", "*", "mask", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "/", "(", "mask_elems", "*", "self", ".", "args", ".", "n_agents", ")", ",", "\n", "t_env", ")", "\n", "self", ".", "log_stats_t", "=", "t_env", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.dmaq_qatten_learner.DMAQ_qattenLearner.train": [[190, 196], ["dmaq_qatten_learner.DMAQ_qattenLearner.sub_train", "dmaq_qatten_learner.DMAQ_qattenLearner._update_targets"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.dmaq_qatten_learner.DMAQ_qattenLearner.sub_train", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.q_learner.QLearner._update_targets"], ["", "", "def", "train", "(", "self", ",", "batch", ":", "EpisodeBatch", ",", "t_env", ":", "int", ",", "episode_num", ":", "int", ",", "show_demo", "=", "False", ",", "save_data", "=", "None", ")", ":", "\n", "        ", "self", ".", "sub_train", "(", "batch", ",", "t_env", ",", "episode_num", ",", "self", ".", "mac", ",", "self", ".", "mixer", ",", "self", ".", "optimiser", ",", "self", ".", "params", ",", "\n", "show_demo", "=", "show_demo", ",", "save_data", "=", "save_data", ")", "\n", "if", "(", "episode_num", "-", "self", ".", "last_target_update_episode", ")", "/", "self", ".", "args", ".", "target_update_interval", ">=", "1.0", ":", "\n", "            ", "self", ".", "_update_targets", "(", ")", "\n", "self", ".", "last_target_update_episode", "=", "episode_num", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.dmaq_qatten_learner.DMAQ_qattenLearner._update_targets": [[197, 202], ["dmaq_qatten_learner.DMAQ_qattenLearner.target_mac.load_state", "dmaq_qatten_learner.DMAQ_qattenLearner.logger.console_logger.info", "dmaq_qatten_learner.DMAQ_qattenLearner.target_mixer.load_state_dict", "dmaq_qatten_learner.DMAQ_qattenLearner.mixer.state_dict"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.load_state"], ["", "", "def", "_update_targets", "(", "self", ")", ":", "\n", "        ", "self", ".", "target_mac", ".", "load_state", "(", "self", ".", "mac", ")", "\n", "if", "self", ".", "mixer", "is", "not", "None", ":", "\n", "            ", "self", ".", "target_mixer", ".", "load_state_dict", "(", "self", ".", "mixer", ".", "state_dict", "(", ")", ")", "\n", "", "self", ".", "logger", ".", "console_logger", ".", "info", "(", "\"Updated target network\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.dmaq_qatten_learner.DMAQ_qattenLearner.cuda": [[203, 209], ["dmaq_qatten_learner.DMAQ_qattenLearner.mac.cuda", "dmaq_qatten_learner.DMAQ_qattenLearner.target_mac.cuda", "dmaq_qatten_learner.DMAQ_qattenLearner.mixer.cuda", "dmaq_qatten_learner.DMAQ_qattenLearner.target_mixer.cuda"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda"], ["", "def", "cuda", "(", "self", ")", ":", "\n", "        ", "self", ".", "mac", ".", "cuda", "(", ")", "\n", "self", ".", "target_mac", ".", "cuda", "(", ")", "\n", "if", "self", ".", "mixer", "is", "not", "None", ":", "\n", "            ", "self", ".", "mixer", ".", "cuda", "(", ")", "\n", "self", ".", "target_mixer", ".", "cuda", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.dmaq_qatten_learner.DMAQ_qattenLearner.save_models": [[210, 215], ["dmaq_qatten_learner.DMAQ_qattenLearner.mac.save_models", "torch.save", "torch.save", "torch.save", "torch.save", "dmaq_qatten_learner.DMAQ_qattenLearner.optimiser.state_dict", "dmaq_qatten_learner.DMAQ_qattenLearner.mixer.state_dict"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.save_models"], ["", "", "def", "save_models", "(", "self", ",", "path", ")", ":", "\n", "        ", "self", ".", "mac", ".", "save_models", "(", "path", ")", "\n", "if", "self", ".", "mixer", "is", "not", "None", ":", "\n", "            ", "th", ".", "save", "(", "self", ".", "mixer", ".", "state_dict", "(", ")", ",", "\"{}/mixer.th\"", ".", "format", "(", "path", ")", ")", "\n", "", "th", ".", "save", "(", "self", ".", "optimiser", ".", "state_dict", "(", ")", ",", "\"{}/opt.th\"", ".", "format", "(", "path", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.dmaq_qatten_learner.DMAQ_qattenLearner.load_models": [[216, 225], ["dmaq_qatten_learner.DMAQ_qattenLearner.mac.load_models", "dmaq_qatten_learner.DMAQ_qattenLearner.target_mac.load_models", "dmaq_qatten_learner.DMAQ_qattenLearner.optimiser.load_state_dict", "dmaq_qatten_learner.DMAQ_qattenLearner.mixer.load_state_dict", "dmaq_qatten_learner.DMAQ_qattenLearner.target_mixer.load_state_dict", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.load_models", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.load_models"], ["", "def", "load_models", "(", "self", ",", "path", ")", ":", "\n", "        ", "self", ".", "mac", ".", "load_models", "(", "path", ")", "\n", "# Not quite right but I don't want to save target networks", "\n", "self", ".", "target_mac", ".", "load_models", "(", "path", ")", "\n", "if", "self", ".", "mixer", "is", "not", "None", ":", "\n", "            ", "self", ".", "mixer", ".", "load_state_dict", "(", "th", ".", "load", "(", "\"{}/mixer.th\"", ".", "format", "(", "path", ")", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", ")", "\n", "self", ".", "target_mixer", ".", "load_state_dict", "(", "th", ".", "load", "(", "\"{}/mixer.th\"", ".", "format", "(", "path", ")", ",", "\n", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", ")", "\n", "", "self", ".", "optimiser", ".", "load_state_dict", "(", "th", ".", "load", "(", "\"{}/opt.th\"", ".", "format", "(", "path", ")", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.coma_learner.COMALearner.__init__": [[10, 31], ["modules.critics.coma.COMACritic", "copy.deepcopy", "list", "list", "torch.optim.RMSprop", "torch.optim.RMSprop", "mac.parameters", "coma_learner.COMALearner.critic.parameters"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.parameters", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.parameters"], ["    ", "def", "__init__", "(", "self", ",", "mac", ",", "scheme", ",", "logger", ",", "args", ")", ":", "\n", "        ", "self", ".", "args", "=", "args", "\n", "self", ".", "n_agents", "=", "args", ".", "n_agents", "\n", "self", ".", "n_actions", "=", "args", ".", "n_actions", "\n", "self", ".", "mac", "=", "mac", "\n", "self", ".", "logger", "=", "logger", "\n", "\n", "self", ".", "last_target_update_step", "=", "0", "\n", "self", ".", "critic_training_steps", "=", "0", "\n", "\n", "self", ".", "log_stats_t", "=", "-", "self", ".", "args", ".", "learner_log_interval", "-", "1", "\n", "\n", "self", ".", "critic", "=", "COMACritic", "(", "scheme", ",", "args", ")", "\n", "self", ".", "target_critic", "=", "copy", ".", "deepcopy", "(", "self", ".", "critic", ")", "\n", "\n", "self", ".", "agent_params", "=", "list", "(", "mac", ".", "parameters", "(", ")", ")", "\n", "self", ".", "critic_params", "=", "list", "(", "self", ".", "critic", ".", "parameters", "(", ")", ")", "\n", "self", ".", "params", "=", "self", ".", "agent_params", "+", "self", ".", "critic_params", "\n", "\n", "self", ".", "agent_optimiser", "=", "RMSprop", "(", "params", "=", "self", ".", "agent_params", ",", "lr", "=", "args", ".", "lr", ",", "alpha", "=", "args", ".", "optim_alpha", ",", "eps", "=", "args", ".", "optim_eps", ")", "\n", "self", ".", "critic_optimiser", "=", "RMSprop", "(", "params", "=", "self", ".", "critic_params", ",", "lr", "=", "args", ".", "critic_lr", ",", "alpha", "=", "args", ".", "optim_alpha", ",", "eps", "=", "args", ".", "optim_eps", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.coma_learner.COMALearner.train": [[32, 101], ["[].float", "[].float", "mask.repeat().view.repeat().view.clone", "mask.repeat().view.repeat().view.repeat().view", "coma_learner.COMALearner._train_critic", "coma_learner.COMALearner.mac.init_hidden", "range", "torch.stack", "q_vals.reshape.reshape.reshape", "torch.stack.view", "torch.gather().squeeze", "torch.gather().squeeze", "torch.log", "coma_learner.COMALearner.agent_optimiser.zero_grad", "coma_loss.backward", "torch.nn.utils.clip_grad_norm_", "coma_learner.COMALearner.agent_optimiser.step", "coma_learner.COMALearner.mac.forward", "torch.stack.append", "torch.stack.sum", "mask.repeat().view.repeat().view.sum", "coma_learner.COMALearner._update_targets", "len", "coma_learner.COMALearner.logger.log_stat", "coma_learner.COMALearner.logger.log_stat", "coma_learner.COMALearner.logger.log_stat", "coma_learner.COMALearner.logger.log_stat", "mask.repeat().view.repeat().view.repeat", "torch.gather", "torch.gather", "coma_learner.COMALearner.logger.log_stat", "coma_loss.item", "mask.repeat().view.repeat().view.sum().item", "mask.repeat().view.repeat().view.sum().item", "actions.reshape", "actions.reshape", "sum", "mask.repeat().view.repeat().view.sum", "mask.repeat().view.repeat().view.sum", "th.stack.view.max"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.coma_learner.COMALearner._train_critic", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.agents.central_rnn_agent.CentralRNNAgent.init_hidden", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.step", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.sqddpg.SQDDPGMixer.forward", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.q_learner.QLearner._update_targets", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat"], ["", "def", "train", "(", "self", ",", "batch", ":", "EpisodeBatch", ",", "t_env", ":", "int", ",", "episode_num", ":", "int", ")", ":", "\n", "# Get the relevant quantities", "\n", "        ", "bs", "=", "batch", ".", "batch_size", "\n", "max_t", "=", "batch", ".", "max_seq_length", "\n", "rewards", "=", "batch", "[", "\"reward\"", "]", "[", ":", ",", ":", "-", "1", "]", "\n", "actions", "=", "batch", "[", "\"actions\"", "]", "[", ":", ",", ":", "]", "\n", "terminated", "=", "batch", "[", "\"terminated\"", "]", "[", ":", ",", ":", "-", "1", "]", ".", "float", "(", ")", "\n", "mask", "=", "batch", "[", "\"filled\"", "]", "[", ":", ",", ":", "-", "1", "]", ".", "float", "(", ")", "\n", "mask", "[", ":", ",", "1", ":", "]", "=", "mask", "[", ":", ",", "1", ":", "]", "*", "(", "1", "-", "terminated", "[", ":", ",", ":", "-", "1", "]", ")", "\n", "avail_actions", "=", "batch", "[", "\"avail_actions\"", "]", "[", ":", ",", ":", "-", "1", "]", "\n", "\n", "critic_mask", "=", "mask", ".", "clone", "(", ")", "\n", "# For SARSA: can't bootstrap off 2nd-to-last state as last state has no action computed", "\n", "# critic_mask[:, -1].zero_()", "\n", "\n", "mask", "=", "mask", ".", "repeat", "(", "1", ",", "1", ",", "self", ".", "n_agents", ")", ".", "view", "(", "-", "1", ")", "\n", "\n", "q_vals", ",", "critic_train_stats", "=", "self", ".", "_train_critic", "(", "batch", ",", "rewards", ",", "terminated", ",", "actions", ",", "avail_actions", ",", "\n", "critic_mask", ",", "bs", ",", "max_t", ")", "\n", "\n", "actions", "=", "actions", "[", ":", ",", ":", "-", "1", "]", "\n", "\n", "mac_out", "=", "[", "]", "\n", "self", ".", "mac", ".", "init_hidden", "(", "batch", ".", "batch_size", ")", "\n", "for", "t", "in", "range", "(", "batch", ".", "max_seq_length", "-", "1", ")", ":", "\n", "            ", "agent_outs", "=", "self", ".", "mac", ".", "forward", "(", "batch", ",", "t", "=", "t", ")", "\n", "mac_out", ".", "append", "(", "agent_outs", ")", "\n", "", "mac_out", "=", "th", ".", "stack", "(", "mac_out", ",", "dim", "=", "1", ")", "# Concat over time", "\n", "\n", "# Mask out unavailable actions, renormalise (as in action selection)", "\n", "mac_out", "[", "avail_actions", "==", "0", "]", "=", "0", "\n", "mac_out", "=", "mac_out", "/", "mac_out", ".", "sum", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "mac_out", "[", "avail_actions", "==", "0", "]", "=", "0", "\n", "\n", "# Calculated baseline", "\n", "q_vals", "=", "q_vals", ".", "reshape", "(", "-", "1", ",", "self", ".", "n_actions", ")", "\n", "pi", "=", "mac_out", ".", "view", "(", "-", "1", ",", "self", ".", "n_actions", ")", "\n", "baseline", "=", "(", "pi", "*", "q_vals", ")", ".", "sum", "(", "-", "1", ")", ".", "detach", "(", ")", "\n", "\n", "# Calculate policy grad with mask", "\n", "q_taken", "=", "th", ".", "gather", "(", "q_vals", ",", "dim", "=", "1", ",", "index", "=", "actions", ".", "reshape", "(", "-", "1", ",", "1", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "pi_taken", "=", "th", ".", "gather", "(", "pi", ",", "dim", "=", "1", ",", "index", "=", "actions", ".", "reshape", "(", "-", "1", ",", "1", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "pi_taken", "[", "mask", "==", "0", "]", "=", "1.0", "\n", "log_pi_taken", "=", "th", ".", "log", "(", "pi_taken", ")", "\n", "\n", "advantages", "=", "(", "q_taken", "-", "baseline", ")", ".", "detach", "(", ")", "\n", "\n", "coma_loss", "=", "-", "(", "(", "advantages", "*", "log_pi_taken", ")", "*", "mask", ")", ".", "sum", "(", ")", "/", "mask", ".", "sum", "(", ")", "\n", "\n", "# Optimise agents", "\n", "self", ".", "agent_optimiser", ".", "zero_grad", "(", ")", "\n", "coma_loss", ".", "backward", "(", ")", "\n", "grad_norm", "=", "th", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "agent_params", ",", "self", ".", "args", ".", "grad_norm_clip", ")", "\n", "self", ".", "agent_optimiser", ".", "step", "(", ")", "\n", "\n", "if", "(", "self", ".", "critic_training_steps", "-", "self", ".", "last_target_update_step", ")", "/", "self", ".", "args", ".", "target_update_interval", ">=", "1.0", ":", "\n", "            ", "self", ".", "_update_targets", "(", ")", "\n", "self", ".", "last_target_update_step", "=", "self", ".", "critic_training_steps", "\n", "\n", "", "if", "t_env", "-", "self", ".", "log_stats_t", ">=", "self", ".", "args", ".", "learner_log_interval", ":", "\n", "            ", "ts_logged", "=", "len", "(", "critic_train_stats", "[", "\"critic_loss\"", "]", ")", "\n", "for", "key", "in", "[", "\"critic_loss\"", ",", "\"critic_grad_norm\"", ",", "\"td_error_abs\"", ",", "\"q_taken_mean\"", ",", "\"target_mean\"", "]", ":", "\n", "                ", "self", ".", "logger", ".", "log_stat", "(", "key", ",", "sum", "(", "critic_train_stats", "[", "key", "]", ")", "/", "ts_logged", ",", "t_env", ")", "\n", "\n", "", "self", ".", "logger", ".", "log_stat", "(", "\"advantage_mean\"", ",", "(", "advantages", "*", "mask", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "/", "mask", ".", "sum", "(", ")", ".", "item", "(", ")", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"coma_loss\"", ",", "coma_loss", ".", "item", "(", ")", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"agent_grad_norm\"", ",", "grad_norm", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"pi_max\"", ",", "(", "pi", ".", "max", "(", "dim", "=", "1", ")", "[", "0", "]", "*", "mask", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "/", "mask", ".", "sum", "(", ")", ".", "item", "(", ")", ",", "t_env", ")", "\n", "self", ".", "log_stats_t", "=", "t_env", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.coma_learner.COMALearner._train_critic": [[102, 150], ["torch.gather().squeeze", "utils.rl_utils.build_td_lambda_targets", "reversed", "coma_learner.COMALearner.target_critic", "torch.zeros_like", "range", "mask[].expand", "coma_learner.COMALearner.critic", "coma_learner.COMALearner.view", "torch.gather().squeeze().squeeze", "coma_learner.COMALearner.critic_optimiser.zero_grad", "loss.backward", "torch.nn.utils.clip_grad_norm_", "coma_learner.COMALearner.critic_optimiser.step", "running_log[].append", "running_log[].append", "mask[].expand.sum().item", "running_log[].append", "running_log[].append", "running_log[].append", "torch.gather", "rewards.size", "mask[].expand.sum", "targets_t.detach", "mask[].expand.sum", "loss.item", "torch.gather().squeeze", "mask[].expand.sum", "masked_td_error.abs().sum().item", "torch.gather", "masked_td_error.abs().sum", "masked_td_error.abs"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.rl_utils.build_td_lambda_targets", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.step"], ["", "", "def", "_train_critic", "(", "self", ",", "batch", ",", "rewards", ",", "terminated", ",", "actions", ",", "avail_actions", ",", "mask", ",", "bs", ",", "max_t", ")", ":", "\n", "# Optimise critic", "\n", "        ", "target_q_vals", "=", "self", ".", "target_critic", "(", "batch", ")", "[", ":", ",", ":", "]", "\n", "targets_taken", "=", "th", ".", "gather", "(", "target_q_vals", ",", "dim", "=", "3", ",", "index", "=", "actions", ")", ".", "squeeze", "(", "3", ")", "\n", "\n", "# Calculate td-lambda targets", "\n", "targets", "=", "build_td_lambda_targets", "(", "rewards", ",", "terminated", ",", "mask", ",", "targets_taken", ",", "self", ".", "n_agents", ",", "self", ".", "args", ".", "gamma", ",", "self", ".", "args", ".", "td_lambda", ")", "\n", "q_vals", "=", "th", ".", "zeros_like", "(", "target_q_vals", ")", "[", ":", ",", ":", "-", "1", "]", "\n", "\n", "running_log", "=", "{", "\n", "\"critic_loss\"", ":", "[", "]", ",", "\n", "\"critic_grad_norm\"", ":", "[", "]", ",", "\n", "\"td_error_abs\"", ":", "[", "]", ",", "\n", "\"target_mean\"", ":", "[", "]", ",", "\n", "\"q_taken_mean\"", ":", "[", "]", ",", "\n", "}", "\n", "\n", "for", "t", "in", "reversed", "(", "range", "(", "rewards", ".", "size", "(", "1", ")", ")", ")", ":", "\n", "            ", "mask_t", "=", "mask", "[", ":", ",", "t", "]", ".", "expand", "(", "-", "1", ",", "self", ".", "n_agents", ")", "\n", "if", "mask_t", ".", "sum", "(", ")", "==", "0", ":", "\n", "                ", "continue", "\n", "\n", "", "q_t", "=", "self", ".", "critic", "(", "batch", ",", "t", ")", "\n", "q_vals", "[", ":", ",", "t", "]", "=", "q_t", ".", "view", "(", "bs", ",", "self", ".", "n_agents", ",", "self", ".", "n_actions", ")", "\n", "q_taken", "=", "th", ".", "gather", "(", "q_t", ",", "dim", "=", "3", ",", "index", "=", "actions", "[", ":", ",", "t", ":", "t", "+", "1", "]", ")", ".", "squeeze", "(", "3", ")", ".", "squeeze", "(", "1", ")", "\n", "targets_t", "=", "targets", "[", ":", ",", "t", "]", "\n", "\n", "td_error", "=", "(", "q_taken", "-", "targets_t", ".", "detach", "(", ")", ")", "\n", "\n", "# 0-out the targets that came from padded data", "\n", "masked_td_error", "=", "td_error", "*", "mask_t", "\n", "\n", "# Normal L2 loss, take mean over actual data", "\n", "loss", "=", "(", "masked_td_error", "**", "2", ")", ".", "sum", "(", ")", "/", "mask_t", ".", "sum", "(", ")", "\n", "self", ".", "critic_optimiser", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "grad_norm", "=", "th", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "critic_params", ",", "self", ".", "args", ".", "grad_norm_clip", ")", "\n", "self", ".", "critic_optimiser", ".", "step", "(", ")", "\n", "self", ".", "critic_training_steps", "+=", "1", "\n", "\n", "running_log", "[", "\"critic_loss\"", "]", ".", "append", "(", "loss", ".", "item", "(", ")", ")", "\n", "running_log", "[", "\"critic_grad_norm\"", "]", ".", "append", "(", "grad_norm", ")", "\n", "mask_elems", "=", "mask_t", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "running_log", "[", "\"td_error_abs\"", "]", ".", "append", "(", "(", "masked_td_error", ".", "abs", "(", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "/", "mask_elems", ")", ")", "\n", "running_log", "[", "\"q_taken_mean\"", "]", ".", "append", "(", "(", "q_taken", "*", "mask_t", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "/", "mask_elems", ")", "\n", "running_log", "[", "\"target_mean\"", "]", ".", "append", "(", "(", "targets_t", "*", "mask_t", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "/", "mask_elems", ")", "\n", "\n", "", "return", "q_vals", ",", "running_log", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.coma_learner.COMALearner._update_targets": [[151, 154], ["coma_learner.COMALearner.target_critic.load_state_dict", "coma_learner.COMALearner.logger.console_logger.info", "coma_learner.COMALearner.critic.state_dict"], "methods", ["None"], ["", "def", "_update_targets", "(", "self", ")", ":", "\n", "        ", "self", ".", "target_critic", ".", "load_state_dict", "(", "self", ".", "critic", ".", "state_dict", "(", ")", ")", "\n", "self", ".", "logger", ".", "console_logger", ".", "info", "(", "\"Updated target network\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.coma_learner.COMALearner.cuda": [[155, 159], ["coma_learner.COMALearner.mac.cuda", "coma_learner.COMALearner.critic.cuda", "coma_learner.COMALearner.target_critic.cuda"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda"], ["", "def", "cuda", "(", "self", ")", ":", "\n", "        ", "self", ".", "mac", ".", "cuda", "(", ")", "\n", "self", ".", "critic", ".", "cuda", "(", ")", "\n", "self", ".", "target_critic", ".", "cuda", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.coma_learner.COMALearner.save_models": [[160, 165], ["coma_learner.COMALearner.mac.save_models", "torch.save", "torch.save", "torch.save", "coma_learner.COMALearner.critic.state_dict", "coma_learner.COMALearner.agent_optimiser.state_dict", "coma_learner.COMALearner.critic_optimiser.state_dict"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.save_models"], ["", "def", "save_models", "(", "self", ",", "path", ")", ":", "\n", "        ", "self", ".", "mac", ".", "save_models", "(", "path", ")", "\n", "th", ".", "save", "(", "self", ".", "critic", ".", "state_dict", "(", ")", ",", "\"{}/critic.th\"", ".", "format", "(", "path", ")", ")", "\n", "th", ".", "save", "(", "self", ".", "agent_optimiser", ".", "state_dict", "(", ")", ",", "\"{}/agent_opt.th\"", ".", "format", "(", "path", ")", ")", "\n", "th", ".", "save", "(", "self", ".", "critic_optimiser", ".", "state_dict", "(", ")", ",", "\"{}/critic_opt.th\"", ".", "format", "(", "path", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.coma_learner.COMALearner.load_models": [[166, 173], ["coma_learner.COMALearner.mac.load_models", "coma_learner.COMALearner.critic.load_state_dict", "coma_learner.COMALearner.target_critic.load_state_dict", "coma_learner.COMALearner.agent_optimiser.load_state_dict", "coma_learner.COMALearner.critic_optimiser.load_state_dict", "torch.load", "coma_learner.COMALearner.critic.state_dict", "torch.load", "torch.load"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.load_models"], ["", "def", "load_models", "(", "self", ",", "path", ")", ":", "\n", "        ", "self", ".", "mac", ".", "load_models", "(", "path", ")", "\n", "self", ".", "critic", ".", "load_state_dict", "(", "th", ".", "load", "(", "\"{}/critic.th\"", ".", "format", "(", "path", ")", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", ")", "\n", "# Not quite right but I don't want to save target networks", "\n", "self", ".", "target_critic", ".", "load_state_dict", "(", "self", ".", "critic", ".", "state_dict", "(", ")", ")", "\n", "self", ".", "agent_optimiser", ".", "load_state_dict", "(", "th", ".", "load", "(", "\"{}/agent_opt.th\"", ".", "format", "(", "path", ")", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", ")", "\n", "self", ".", "critic_optimiser", ".", "load_state_dict", "(", "th", ".", "load", "(", "\"{}/critic_opt.th\"", ".", "format", "(", "path", ")", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.q_learner_w.QLearner.__init__": [[11, 37], ["list", "torch.optim.RMSprop", "copy.deepcopy", "mac.parameters", "list", "copy.deepcopy", "modules.mixers.vdn.VDNMixer", "q_learner_w.QLearner.mixer.parameters", "modules.mixers.qmix.QMixer", "ValueError"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.parameters", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.parameters"], ["    ", "def", "__init__", "(", "self", ",", "mac", ",", "scheme", ",", "logger", ",", "args", ")", ":", "\n", "        ", "self", ".", "args", "=", "args", "\n", "self", ".", "mac", "=", "mac", "\n", "self", ".", "logger", "=", "logger", "\n", "\n", "self", ".", "params", "=", "list", "(", "mac", ".", "parameters", "(", ")", ")", "\n", "\n", "self", ".", "last_target_update_episode", "=", "0", "\n", "\n", "self", ".", "mixer", "=", "None", "\n", "if", "args", ".", "mixer", "is", "not", "None", ":", "\n", "            ", "if", "args", ".", "mixer", "==", "\"vdn\"", ":", "\n", "                ", "self", ".", "mixer", "=", "VDNMixer", "(", ")", "\n", "", "elif", "args", ".", "mixer", "==", "\"qmix\"", ":", "\n", "                ", "self", ".", "mixer", "=", "QMixer", "(", "args", ")", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"Mixer {} not recognised.\"", ".", "format", "(", "args", ".", "mixer", ")", ")", "\n", "", "self", ".", "params", "+=", "list", "(", "self", ".", "mixer", ".", "parameters", "(", ")", ")", "\n", "self", ".", "target_mixer", "=", "copy", ".", "deepcopy", "(", "self", ".", "mixer", ")", "\n", "\n", "", "self", ".", "optimiser", "=", "RMSprop", "(", "params", "=", "self", ".", "params", ",", "lr", "=", "args", ".", "lr", ",", "alpha", "=", "args", ".", "optim_alpha", ",", "eps", "=", "args", ".", "optim_eps", ")", "\n", "\n", "# a little wasteful to deepcopy (e.g. duplicates action selector), but should work for any MAC", "\n", "self", ".", "target_mac", "=", "copy", ".", "deepcopy", "(", "mac", ")", "\n", "\n", "self", ".", "log_stats_t", "=", "-", "self", ".", "args", ".", "learner_log_interval", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.q_learner_w.QLearner.train": [[38, 136], ["[].float", "[].float", "q_learner_w.QLearner.mac.init_hidden", "range", "torch.stack", "torch.gather().squeeze", "q_learner_w.QLearner.target_mac.init_hidden", "range", "torch.stack", "mask.expand_as.expand_as.expand_as", "q_learner_w.QLearner.optimiser.zero_grad", "loss.backward", "torch.nn.utils.clip_grad_norm_", "q_learner_w.QLearner.optimiser.step", "q_learner_w.QLearner.mac.forward", "torch.stack.append", "q_learner_w.QLearner.target_mac.forward", "torch.stack.append", "torch.stack.clone().detach", "torch.gather().squeeze", "q_learner_w.QLearner.mixer", "q_learner_w.QLearner.target_mixer", "targets.detach", "torch.where", "torch.where.mean().item", "torch.gather().squeeze", "q_learner_w.QLearner.target_mixer", "torch.where", "torch.where.mean().item", "mask.expand_as.expand_as.sum", "q_learner_w.QLearner._update_targets", "q_learner_w.QLearner.logger.log_stat", "q_learner_w.QLearner.logger.log_stat", "mask.expand_as.expand_as.sum().item", "q_learner_w.QLearner.logger.log_stat", "q_learner_w.QLearner.logger.log_stat", "q_learner_w.QLearner.logger.log_stat", "q_learner_w.QLearner.logger.log_stat", "torch.gather", "mac_out_detach[].max", "torch.stack.max", "torch.ones_like", "torch.ones_like", "loss.item", "torch.stack.clone", "torch.gather", "torch.ones_like", "torch.where.mean", "torch.gather", "torch.ones_like", "torch.where.mean", "mask.expand_as.expand_as.sum", "masked_td_error.abs().sum().item", "torch.where.detach", "masked_td_error.abs().sum", "masked_td_error.abs"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.agents.central_rnn_agent.CentralRNNAgent.init_hidden", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.agents.central_rnn_agent.CentralRNNAgent.init_hidden", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.step", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.sqddpg.SQDDPGMixer.forward", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.sqddpg.SQDDPGMixer.forward", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.q_learner.QLearner._update_targets", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat"], ["", "def", "train", "(", "self", ",", "batch", ":", "EpisodeBatch", ",", "t_env", ":", "int", ",", "episode_num", ":", "int", ")", ":", "\n", "# Get the relevant quantities", "\n", "        ", "rewards", "=", "batch", "[", "\"reward\"", "]", "[", ":", ",", ":", "-", "1", "]", "\n", "actions", "=", "batch", "[", "\"actions\"", "]", "[", ":", ",", ":", "-", "1", "]", "\n", "terminated", "=", "batch", "[", "\"terminated\"", "]", "[", ":", ",", ":", "-", "1", "]", ".", "float", "(", ")", "\n", "mask", "=", "batch", "[", "\"filled\"", "]", "[", ":", ",", ":", "-", "1", "]", ".", "float", "(", ")", "\n", "mask", "[", ":", ",", "1", ":", "]", "=", "mask", "[", ":", ",", "1", ":", "]", "*", "(", "1", "-", "terminated", "[", ":", ",", ":", "-", "1", "]", ")", "\n", "avail_actions", "=", "batch", "[", "\"avail_actions\"", "]", "\n", "\n", "# Calculate estimated Q-Values", "\n", "mac_out", "=", "[", "]", "\n", "self", ".", "mac", ".", "init_hidden", "(", "batch", ".", "batch_size", ")", "\n", "for", "t", "in", "range", "(", "batch", ".", "max_seq_length", ")", ":", "\n", "            ", "agent_outs", "=", "self", ".", "mac", ".", "forward", "(", "batch", ",", "t", "=", "t", ")", "\n", "mac_out", ".", "append", "(", "agent_outs", ")", "\n", "", "mac_out", "=", "th", ".", "stack", "(", "mac_out", ",", "dim", "=", "1", ")", "# Concat over time", "\n", "\n", "# Pick the Q-Values for the actions taken by each agent", "\n", "chosen_action_qvals", "=", "th", ".", "gather", "(", "mac_out", "[", ":", ",", ":", "-", "1", "]", ",", "dim", "=", "3", ",", "index", "=", "actions", ")", ".", "squeeze", "(", "3", ")", "# Remove the last dim", "\n", "\n", "# Calculate the Q-Values necessary for the target", "\n", "target_mac_out", "=", "[", "]", "\n", "self", ".", "target_mac", ".", "init_hidden", "(", "batch", ".", "batch_size", ")", "\n", "for", "t", "in", "range", "(", "batch", ".", "max_seq_length", ")", ":", "\n", "            ", "target_agent_outs", "=", "self", ".", "target_mac", ".", "forward", "(", "batch", ",", "t", "=", "t", ")", "\n", "target_mac_out", ".", "append", "(", "target_agent_outs", ")", "\n", "\n", "# We don't need the first timesteps Q-Value estimate for calculating targets", "\n", "", "target_mac_out", "=", "th", ".", "stack", "(", "target_mac_out", "[", ":", "]", ",", "dim", "=", "1", ")", "# Concat across time", "\n", "\n", "# Mask out unavailable actions", "\n", "target_mac_out", "[", ":", ",", "1", ":", "]", "[", "avail_actions", "[", ":", ",", "1", ":", "]", "==", "0", "]", "=", "-", "9999999", "# From OG deepmarl", "\n", "\n", "# Max over target Q-Values", "\n", "if", "self", ".", "args", ".", "double_q", ":", "\n", "# Get actions that maximise live Q (for double q-learning)", "\n", "            ", "mac_out_detach", "=", "mac_out", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "mac_out_detach", "[", "avail_actions", "==", "0", "]", "=", "-", "9999999", "\n", "cur_max_actions", "=", "mac_out_detach", "[", ":", ",", ":", "]", ".", "max", "(", "dim", "=", "3", ",", "keepdim", "=", "True", ")", "[", "1", "]", "\n", "target_max_qvals", "=", "th", ".", "gather", "(", "target_mac_out", "[", ":", ",", "1", ":", "]", ",", "3", ",", "cur_max_actions", "[", ":", ",", "1", ":", "]", ")", ".", "squeeze", "(", "3", ")", "\n", "", "else", ":", "\n", "            ", "assert", "False", "\n", "target_max_qvals", "=", "target_mac_out", ".", "max", "(", "dim", "=", "3", ")", "[", "0", "]", "\n", "\n", "# Mix", "\n", "", "if", "self", ".", "mixer", "is", "not", "None", ":", "\n", "            ", "chosen_action_qvals", "=", "self", ".", "mixer", "(", "chosen_action_qvals", ",", "batch", "[", "\"state\"", "]", "[", ":", ",", ":", "-", "1", "]", ")", "\n", "target_max_qvals", "=", "self", ".", "target_mixer", "(", "target_max_qvals", ",", "batch", "[", "\"state\"", "]", "[", ":", ",", "1", ":", "]", ")", "\n", "\n", "# Calculate 1-step Q-Learning targets", "\n", "", "targets", "=", "rewards", "+", "self", ".", "args", ".", "gamma", "*", "(", "1", "-", "terminated", ")", "*", "target_max_qvals", "\n", "\n", "# Td-error", "\n", "td_error", "=", "(", "chosen_action_qvals", "-", "targets", ".", "detach", "(", ")", ")", "\n", "\n", "mask", "=", "mask", ".", "expand_as", "(", "td_error", ")", "\n", "\n", "# 0-out the targets that came from padded data", "\n", "masked_td_error", "=", "td_error", "*", "mask", "\n", "\n", "# Weighting", "\n", "w_to_use", "=", "self", ".", "args", ".", "w", "\n", "if", "self", ".", "args", ".", "hysteretic_qmix", ":", "\n", "            ", "ws", "=", "th", ".", "ones_like", "(", "td_error", ")", "*", "w_to_use", "\n", "ws", "=", "th", ".", "where", "(", "td_error", "<", "0", ",", "th", ".", "ones_like", "(", "td_error", ")", "*", "1", ",", "ws", ")", "# Target is greater than current max", "\n", "w_to_use", "=", "ws", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "", "else", ":", "\n", "            ", "is_max_action", "=", "(", "actions", "==", "cur_max_actions", "[", ":", ",", ":", "-", "1", "]", ")", ".", "min", "(", "dim", "=", "2", ")", "[", "0", "]", "\n", "target_max_agent_qvals", "=", "th", ".", "gather", "(", "target_mac_out", "[", ":", ",", ":", "-", "1", "]", ",", "3", ",", "cur_max_actions", "[", ":", ",", ":", "-", "1", "]", ")", ".", "squeeze", "(", "3", ")", "\n", "max_action_qtot", "=", "self", ".", "target_mixer", "(", "target_max_agent_qvals", "[", ":", ",", ":", "]", ",", "batch", "[", "\"state\"", "]", "[", ":", ",", ":", "-", "1", "]", ")", "\n", "qtot_larger", "=", "targets", ">", "max_action_qtot", "\n", "ws", "=", "th", ".", "ones_like", "(", "td_error", ")", "*", "w_to_use", "\n", "ws", "=", "th", ".", "where", "(", "is_max_action", "|", "qtot_larger", ",", "th", ".", "ones_like", "(", "td_error", ")", "*", "1", ",", "\n", "ws", ")", "# Target is greater than current max", "\n", "w_to_use", "=", "ws", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "\n", "# Weighted L2 loss, take mean over actual data", "\n", "", "loss", "=", "(", "ws", ".", "detach", "(", ")", "*", "(", "masked_td_error", "**", "2", ")", ")", ".", "sum", "(", ")", "/", "mask", ".", "sum", "(", ")", "\n", "\n", "# Optimise", "\n", "self", ".", "optimiser", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "grad_norm", "=", "th", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "params", ",", "self", ".", "args", ".", "grad_norm_clip", ")", "\n", "self", ".", "optimiser", ".", "step", "(", ")", "\n", "\n", "if", "(", "episode_num", "-", "self", ".", "last_target_update_episode", ")", "/", "self", ".", "args", ".", "target_update_interval", ">=", "1.0", ":", "\n", "            ", "self", ".", "_update_targets", "(", ")", "\n", "self", ".", "last_target_update_episode", "=", "episode_num", "\n", "\n", "", "if", "t_env", "-", "self", ".", "log_stats_t", ">=", "self", ".", "args", ".", "learner_log_interval", ":", "\n", "            ", "self", ".", "logger", ".", "log_stat", "(", "\"loss\"", ",", "loss", ".", "item", "(", ")", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"grad_norm\"", ",", "grad_norm", ",", "t_env", ")", "\n", "mask_elems", "=", "mask", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"td_error_abs\"", ",", "(", "masked_td_error", ".", "abs", "(", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "/", "mask_elems", ")", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"q_taken_mean\"", ",", "(", "chosen_action_qvals", "*", "mask", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "/", "(", "mask_elems", "*", "self", ".", "args", ".", "n_agents", ")", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"target_mean\"", ",", "(", "targets", "*", "mask", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "/", "(", "mask_elems", "*", "self", ".", "args", ".", "n_agents", ")", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"w_to_use\"", ",", "w_to_use", ",", "t_env", ")", "\n", "self", ".", "log_stats_t", "=", "t_env", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.q_learner_w.QLearner._update_targets": [[137, 142], ["q_learner_w.QLearner.target_mac.load_state", "q_learner_w.QLearner.logger.console_logger.info", "q_learner_w.QLearner.target_mixer.load_state_dict", "q_learner_w.QLearner.mixer.state_dict"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.load_state"], ["", "", "def", "_update_targets", "(", "self", ")", ":", "\n", "        ", "self", ".", "target_mac", ".", "load_state", "(", "self", ".", "mac", ")", "\n", "if", "self", ".", "mixer", "is", "not", "None", ":", "\n", "            ", "self", ".", "target_mixer", ".", "load_state_dict", "(", "self", ".", "mixer", ".", "state_dict", "(", ")", ")", "\n", "", "self", ".", "logger", ".", "console_logger", ".", "info", "(", "\"Updated target network\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.q_learner_w.QLearner.cuda": [[143, 149], ["q_learner_w.QLearner.mac.cuda", "q_learner_w.QLearner.target_mac.cuda", "q_learner_w.QLearner.mixer.cuda", "q_learner_w.QLearner.target_mixer.cuda"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda"], ["", "def", "cuda", "(", "self", ")", ":", "\n", "        ", "self", ".", "mac", ".", "cuda", "(", ")", "\n", "self", ".", "target_mac", ".", "cuda", "(", ")", "\n", "if", "self", ".", "mixer", "is", "not", "None", ":", "\n", "            ", "self", ".", "mixer", ".", "cuda", "(", ")", "\n", "self", ".", "target_mixer", ".", "cuda", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.q_learner_w.QLearner.save_models": [[150, 155], ["q_learner_w.QLearner.mac.save_models", "torch.save", "torch.save", "q_learner_w.QLearner.optimiser.state_dict", "q_learner_w.QLearner.mixer.state_dict"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.save_models"], ["", "", "def", "save_models", "(", "self", ",", "path", ")", ":", "\n", "        ", "self", ".", "mac", ".", "save_models", "(", "path", ")", "\n", "if", "self", ".", "mixer", "is", "not", "None", ":", "\n", "            ", "th", ".", "save", "(", "self", ".", "mixer", ".", "state_dict", "(", ")", ",", "\"{}/mixer.th\"", ".", "format", "(", "path", ")", ")", "\n", "", "th", ".", "save", "(", "self", ".", "optimiser", ".", "state_dict", "(", ")", ",", "\"{}/opt.th\"", ".", "format", "(", "path", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.q_learner_w.QLearner.load_models": [[156, 163], ["q_learner_w.QLearner.mac.load_models", "q_learner_w.QLearner.target_mac.load_models", "q_learner_w.QLearner.optimiser.load_state_dict", "q_learner_w.QLearner.mixer.load_state_dict", "torch.load", "torch.load"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.load_models", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.load_models"], ["", "def", "load_models", "(", "self", ",", "path", ")", ":", "\n", "        ", "self", ".", "mac", ".", "load_models", "(", "path", ")", "\n", "# Not quite right but I don't want to save target networks", "\n", "self", ".", "target_mac", ".", "load_models", "(", "path", ")", "\n", "if", "self", ".", "mixer", "is", "not", "None", ":", "\n", "            ", "self", ".", "mixer", ".", "load_state_dict", "(", "th", ".", "load", "(", "\"{}/mixer.th\"", ".", "format", "(", "path", ")", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", ")", "\n", "", "self", ".", "optimiser", ".", "load_state_dict", "(", "th", ".", "load", "(", "\"{}/opt.th\"", ".", "format", "(", "path", ")", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.max_q_learner_sac.SACQLearner.__init__": [[14, 55], ["list", "list", "copy.deepcopy", "modules.mixers.qmix_central_no_hyper.QMixerCentralFF", "copy.deepcopy", "list", "list", "copy.deepcopy", "torch.optim.RMSprop", "torch.optim.RMSprop", "mac.parameters", "max_q_learner_sac.SACQLearner.mac.parameters", "list", "list", "copy.deepcopy", "max_q_learner_sac.SACQLearner.central_mac.parameters", "max_q_learner_sac.SACQLearner.central_mixer.parameters", "modules.mixers.vdn.VDNMixer", "max_q_learner_sac.SACQLearner.mixer.parameters", "max_q_learner_sac.SACQLearner.mixer.parameters", "modules.mixers.qmix.QMixer", "ValueError"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.parameters", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.parameters", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.parameters", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.parameters", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.parameters", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.parameters"], ["    ", "def", "__init__", "(", "self", ",", "mac", ",", "scheme", ",", "logger", ",", "args", ")", ":", "\n", "        ", "self", ".", "args", "=", "args", "\n", "self", ".", "mac", "=", "mac", "\n", "self", ".", "logger", "=", "logger", "\n", "\n", "self", ".", "mac_params", "=", "list", "(", "mac", ".", "parameters", "(", ")", ")", "\n", "self", ".", "params", "=", "list", "(", "self", ".", "mac", ".", "parameters", "(", ")", ")", "\n", "\n", "self", ".", "last_target_update_episode", "=", "0", "\n", "\n", "self", ".", "mixer", "=", "None", "\n", "assert", "args", ".", "mixer", "is", "not", "None", "\n", "if", "args", ".", "mixer", "is", "not", "None", ":", "\n", "            ", "if", "args", ".", "mixer", "==", "\"vdn\"", ":", "\n", "                ", "self", ".", "mixer", "=", "VDNMixer", "(", ")", "\n", "", "elif", "args", ".", "mixer", "==", "\"qmix\"", ":", "\n", "                ", "self", ".", "mixer", "=", "QMixer", "(", "args", ")", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"Mixer {} not recognised.\"", ".", "format", "(", "args", ".", "mixer", ")", ")", "\n", "", "self", ".", "mixer_params", "=", "list", "(", "self", ".", "mixer", ".", "parameters", "(", ")", ")", "\n", "self", ".", "params", "+=", "list", "(", "self", ".", "mixer", ".", "parameters", "(", ")", ")", "\n", "self", ".", "target_mixer", "=", "copy", ".", "deepcopy", "(", "self", ".", "mixer", ")", "\n", "\n", "# a little wasteful to deepcopy (e.g. duplicates action selector), but should work for any MAC", "\n", "", "self", ".", "target_mac", "=", "copy", ".", "deepcopy", "(", "mac", ")", "\n", "\n", "# Central Q", "\n", "# TODO: Clean this mess up!", "\n", "self", ".", "central_mac", "=", "None", "\n", "assert", "self", ".", "args", ".", "central_mixer", "==", "\"ff\"", "\n", "self", ".", "central_mixer", "=", "QMixerCentralFF", "(", "args", ")", "\n", "assert", "args", ".", "central_mac", "==", "\"basic_central_mac\"", "\n", "self", ".", "central_mac", "=", "mac_REGISTRY", "[", "args", ".", "central_mac", "]", "(", "scheme", ",", "args", ")", "# Groups aren't used in the CentralBasicController. Little hacky", "\n", "self", ".", "target_central_mac", "=", "copy", ".", "deepcopy", "(", "self", ".", "central_mac", ")", "\n", "self", ".", "params", "+=", "list", "(", "self", ".", "central_mac", ".", "parameters", "(", ")", ")", "\n", "self", ".", "params", "+=", "list", "(", "self", ".", "central_mixer", ".", "parameters", "(", ")", ")", "\n", "self", ".", "target_central_mixer", "=", "copy", ".", "deepcopy", "(", "self", ".", "central_mixer", ")", "\n", "\n", "self", ".", "optimiser", "=", "RMSprop", "(", "params", "=", "self", ".", "params", ",", "lr", "=", "args", ".", "lr", ",", "alpha", "=", "args", ".", "optim_alpha", ",", "eps", "=", "args", ".", "optim_eps", ")", "\n", "\n", "self", ".", "log_stats_t", "=", "-", "self", ".", "args", ".", "learner_log_interval", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.max_q_learner_sac.SACQLearner.train": [[56, 170], ["[].float", "[].float", "max_q_learner_sac.SACQLearner.mac.init_hidden", "range", "torch.stack", "torch.stack", "max_q_learner_sac.SACQLearner.target_mac.init_hidden", "range", "torch.stack", "torch.stack", "torch.distributions.Categorical().sample().long", "torch.distributions.Categorical().sample().long", "torch.distributions.Categorical().sample().long", "torch.distributions.Categorical().sample().long", "max_q_learner_sac.SACQLearner.central_mac.init_hidden", "range", "torch.stack", "torch.stack", "torch.gather().squeeze", "torch.gather().squeeze", "max_q_learner_sac.SACQLearner.target_central_mac.init_hidden", "range", "torch.stack", "torch.stack", "torch.gather().squeeze", "torch.gather().squeeze", "max_q_learner_sac.SACQLearner.target_central_mixer", "torch.gather().squeeze", "torch.gather().squeeze", "torch.log().sum", "torch.log().sum", "max_q_learner_sac.SACQLearner.central_mixer", "[].float.expand_as", "torch.gather().squeeze", "torch.gather().squeeze", "max_q_learner_sac.SACQLearner.central_mixer().repeat", "torch.gather().squeeze", "torch.gather().squeeze", "[].float.expand_as", "max_q_learner_sac.SACQLearner.optimiser.zero_grad", "loss.backward", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "max_q_learner_sac.SACQLearner.optimiser.step", "max_q_learner_sac.SACQLearner.mac.forward", "torch.stack.append", "torch.stack.sum", "max_q_learner_sac.SACQLearner.target_mac.forward", "torch.stack.append", "torch.stack.sum", "max_q_learner_sac.SACQLearner.central_mac.forward", "torch.stack.append", "max_q_learner_sac.SACQLearner.target_central_mac.forward", "torch.stack.append", "targets.detach", "[].float.sum", "torch.log", "torch.log", "[].float.expand_as.sum", "max_q_learner_sac.SACQLearner._update_targets", "max_q_learner_sac.SACQLearner.logger.log_stat", "max_q_learner_sac.SACQLearner.logger.log_stat", "max_q_learner_sac.SACQLearner.logger.log_stat", "[].float.sum().item", "max_q_learner_sac.SACQLearner.logger.log_stat", "max_q_learner_sac.SACQLearner.logger.log_stat", "max_q_learner_sac.SACQLearner.logger.log_stat", "torch.distributions.Categorical().sample", "torch.distributions.Categorical().sample", "torch.distributions.Categorical().sample", "torch.distributions.Categorical().sample", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.log", "torch.log", "torch.gather", "torch.gather", "max_q_learner_sac.SACQLearner.central_mixer", "torch.gather", "torch.gather", "loss.item", "actor_loss.item", "central_loss.item", "torch.log", "torch.log", "actor_entropy.item", "sampled_target_actions[].unsqueeze().unsqueeze().repeat", "sampled_actions[].unsqueeze().unsqueeze().repeat", "[].float.sum", "[].float.sum", "torch.distributions.Categorical", "torch.distributions.Categorical", "torch.distributions.Categorical", "torch.distributions.Categorical", "actions.unsqueeze().repeat", "torch.distributions.Categorical().sample().long.unsqueeze", "torch.distributions.Categorical().sample().long.unsqueeze", "torch.stack.sum", "torch.stack.sum", "sampled_target_actions[].unsqueeze().unsqueeze", "sampled_actions[].unsqueeze().unsqueeze", "actions.unsqueeze", "sampled_target_actions[].unsqueeze", "sampled_actions[].unsqueeze"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.agents.central_rnn_agent.CentralRNNAgent.init_hidden", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.agents.central_rnn_agent.CentralRNNAgent.init_hidden", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.agents.central_rnn_agent.CentralRNNAgent.init_hidden", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.agents.central_rnn_agent.CentralRNNAgent.init_hidden", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.step", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.sqddpg.SQDDPGMixer.forward", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.sqddpg.SQDDPGMixer.forward", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.sqddpg.SQDDPGMixer.forward", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.sqddpg.SQDDPGMixer.forward", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.q_learner.QLearner._update_targets", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.ReplayBuffer.sample", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.ReplayBuffer.sample", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.ReplayBuffer.sample", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.ReplayBuffer.sample"], ["", "def", "train", "(", "self", ",", "batch", ":", "EpisodeBatch", ",", "t_env", ":", "int", ",", "episode_num", ":", "int", ")", ":", "\n", "# Get the relevant quantities", "\n", "        ", "rewards", "=", "batch", "[", "\"reward\"", "]", "[", ":", ",", ":", "-", "1", "]", "\n", "actions", "=", "batch", "[", "\"actions\"", "]", "[", ":", ",", ":", "-", "1", "]", "\n", "terminated", "=", "batch", "[", "\"terminated\"", "]", "[", ":", ",", ":", "-", "1", "]", ".", "float", "(", ")", "\n", "mask", "=", "batch", "[", "\"filled\"", "]", "[", ":", ",", ":", "-", "1", "]", ".", "float", "(", ")", "\n", "mask", "[", ":", ",", "1", ":", "]", "=", "mask", "[", ":", ",", "1", ":", "]", "*", "(", "1", "-", "terminated", "[", ":", ",", ":", "-", "1", "]", ")", "\n", "avail_actions", "=", "batch", "[", "\"avail_actions\"", "]", "\n", "\n", "# Current policies", "\n", "mac_out", "=", "[", "]", "\n", "self", ".", "mac", ".", "init_hidden", "(", "batch", ".", "batch_size", ")", "\n", "for", "t", "in", "range", "(", "batch", ".", "max_seq_length", ")", ":", "\n", "            ", "agent_outs", "=", "self", ".", "mac", ".", "forward", "(", "batch", ",", "t", "=", "t", ")", "\n", "mac_out", ".", "append", "(", "agent_outs", ")", "\n", "", "mac_out", "=", "th", ".", "stack", "(", "mac_out", ",", "dim", "=", "1", ")", "# Concat over time", "\n", "\n", "# Mask out unavailable actions, renormalise (as in action selection)", "\n", "mac_out", "[", "avail_actions", "==", "0", "]", "=", "0", "\n", "mac_out", "=", "mac_out", "/", "mac_out", ".", "sum", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "mac_out", "[", "avail_actions", "==", "0", "]", "=", "0", "\n", "mac_out", "[", "(", "mac_out", ".", "sum", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "==", "0", ")", ".", "expand_as", "(", "mac_out", ")", "]", "=", "1", "# Set any all 0 probability vectors to all 1s. They will be masked out later, but still need to be sampled.", "\n", "\n", "# Target policies", "\n", "target_mac_out", "=", "[", "]", "\n", "self", ".", "target_mac", ".", "init_hidden", "(", "batch", ".", "batch_size", ")", "\n", "for", "t", "in", "range", "(", "batch", ".", "max_seq_length", ")", ":", "\n", "            ", "target_agent_outs", "=", "self", ".", "target_mac", ".", "forward", "(", "batch", ",", "t", "=", "t", ")", "\n", "target_mac_out", ".", "append", "(", "target_agent_outs", ")", "\n", "", "target_mac_out", "=", "th", ".", "stack", "(", "target_mac_out", ",", "dim", "=", "1", ")", "# Concat across time", "\n", "\n", "# Mask out unavailable actions, renormalise (as in action selection)", "\n", "target_mac_out", "[", "avail_actions", "==", "0", "]", "=", "0", "\n", "target_mac_out", "=", "target_mac_out", "/", "target_mac_out", ".", "sum", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "target_mac_out", "[", "avail_actions", "==", "0", "]", "=", "0", "\n", "target_mac_out", "[", "(", "target_mac_out", ".", "sum", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "==", "0", ")", ".", "expand_as", "(", "target_mac_out", ")", "]", "=", "1", "# Set any all 0 probability vectors to all 1s. They will be masked out later, but still need to be sampled.", "\n", "\n", "# Sample actions", "\n", "sampled_actions", "=", "Categorical", "(", "mac_out", ")", ".", "sample", "(", ")", ".", "long", "(", ")", "\n", "sampled_target_actions", "=", "Categorical", "(", "target_mac_out", ")", ".", "sample", "(", ")", ".", "long", "(", ")", "\n", "\n", "# Central MAC stuff", "\n", "central_mac_out", "=", "[", "]", "\n", "self", ".", "central_mac", ".", "init_hidden", "(", "batch", ".", "batch_size", ")", "\n", "for", "t", "in", "range", "(", "batch", ".", "max_seq_length", ")", ":", "\n", "            ", "agent_outs", "=", "self", ".", "central_mac", ".", "forward", "(", "batch", ",", "t", "=", "t", ")", "\n", "central_mac_out", ".", "append", "(", "agent_outs", ")", "\n", "", "central_mac_out", "=", "th", ".", "stack", "(", "central_mac_out", ",", "dim", "=", "1", ")", "# Concat over time", "\n", "# Actions chosen from replay buffer", "\n", "central_chosen_action_qvals_agents", "=", "th", ".", "gather", "(", "central_mac_out", "[", ":", ",", ":", "-", "1", "]", ",", "dim", "=", "3", ",", "index", "=", "actions", ".", "unsqueeze", "(", "4", ")", ".", "repeat", "(", "1", ",", "1", ",", "1", ",", "1", ",", "self", ".", "args", ".", "central_action_embed", ")", ")", ".", "squeeze", "(", "3", ")", "# Remove the last dim", "\n", "\n", "central_target_mac_out", "=", "[", "]", "\n", "self", ".", "target_central_mac", ".", "init_hidden", "(", "batch", ".", "batch_size", ")", "\n", "for", "t", "in", "range", "(", "batch", ".", "max_seq_length", ")", ":", "\n", "            ", "target_agent_outs", "=", "self", ".", "target_central_mac", ".", "forward", "(", "batch", ",", "t", "=", "t", ")", "\n", "central_target_mac_out", ".", "append", "(", "target_agent_outs", ")", "\n", "", "central_target_mac_out", "=", "th", ".", "stack", "(", "central_target_mac_out", "[", ":", "]", ",", "dim", "=", "1", ")", "# Concat across time", "\n", "central_target_action_qvals_agents", "=", "th", ".", "gather", "(", "central_target_mac_out", "[", ":", ",", ":", "]", ",", "3", ",", "sampled_target_actions", "[", ":", ",", ":", "]", ".", "unsqueeze", "(", "3", ")", ".", "unsqueeze", "(", "4", ")", ".", "repeat", "(", "1", ",", "1", ",", "1", ",", "1", ",", "self", ".", "args", ".", "central_action_embed", ")", ")", ".", "squeeze", "(", "3", ")", "\n", "# ---", "\n", "\n", "critic_bootstrap_qvals", "=", "self", ".", "target_central_mixer", "(", "central_target_action_qvals_agents", "[", ":", ",", "1", ":", "]", ",", "batch", "[", "\"state\"", "]", "[", ":", ",", "1", ":", "]", ")", "\n", "\n", "target_chosen_action_probs", "=", "th", ".", "gather", "(", "target_mac_out", ",", "dim", "=", "3", ",", "index", "=", "sampled_target_actions", ".", "unsqueeze", "(", "3", ")", ")", ".", "squeeze", "(", "dim", "=", "3", ")", "\n", "target_policy_logs", "=", "th", ".", "log", "(", "target_chosen_action_probs", ")", ".", "sum", "(", "dim", "=", "2", ",", "keepdim", "=", "True", ")", "# Sum across agents", "\n", "# Calculate 1-step Q-Learning targets", "\n", "targets", "=", "rewards", "+", "self", ".", "args", ".", "gamma", "*", "(", "1", "-", "terminated", ")", "*", "(", "critic_bootstrap_qvals", "-", "self", ".", "args", ".", "entropy_temp", "*", "target_policy_logs", "[", ":", ",", "1", ":", "]", ")", "\n", "\n", "# Training Critic", "\n", "central_chosen_action_qvals", "=", "self", ".", "central_mixer", "(", "central_chosen_action_qvals_agents", ",", "batch", "[", "\"state\"", "]", "[", ":", ",", ":", "-", "1", "]", ")", "\n", "central_td_error", "=", "(", "central_chosen_action_qvals", "-", "targets", ".", "detach", "(", ")", ")", "\n", "central_mask", "=", "mask", ".", "expand_as", "(", "central_td_error", ")", "\n", "central_masked_td_error", "=", "central_td_error", "*", "central_mask", "\n", "central_loss", "=", "(", "central_masked_td_error", "**", "2", ")", ".", "sum", "(", ")", "/", "mask", ".", "sum", "(", ")", "\n", "\n", "# Actor Loss", "\n", "central_sampled_action_qvals_agents", "=", "th", ".", "gather", "(", "central_mac_out", "[", ":", ",", ":", "-", "1", "]", ",", "3", ",", "sampled_actions", "[", ":", ",", ":", "-", "1", "]", ".", "unsqueeze", "(", "3", ")", ".", "unsqueeze", "(", "4", ")", ".", "repeat", "(", "1", ",", "1", ",", "1", ",", "1", ",", "self", ".", "args", ".", "central_action_embed", ")", ")", ".", "squeeze", "(", "3", ")", "\n", "central_sampled_action_qvals", "=", "self", ".", "central_mixer", "(", "central_sampled_action_qvals_agents", ",", "batch", "[", "\"state\"", "]", "[", ":", ",", ":", "-", "1", "]", ")", ".", "repeat", "(", "1", ",", "1", ",", "self", ".", "args", ".", "n_agents", ")", "\n", "sampled_action_probs", "=", "th", ".", "gather", "(", "mac_out", ",", "dim", "=", "3", ",", "index", "=", "sampled_actions", ".", "unsqueeze", "(", "3", ")", ")", ".", "squeeze", "(", "3", ")", "\n", "policy_logs", "=", "th", ".", "log", "(", "sampled_action_probs", ")", "[", ":", ",", ":", "-", "1", "]", "\n", "actor_mask", "=", "mask", ".", "expand_as", "(", "policy_logs", ")", "\n", "actor_loss", "=", "(", "(", "policy_logs", "*", "(", "self", ".", "args", ".", "entropy_temp", "*", "(", "policy_logs", "+", "1", ")", "-", "central_sampled_action_qvals", ")", ".", "detach", "(", ")", ")", "*", "actor_mask", ")", ".", "sum", "(", ")", "/", "actor_mask", ".", "sum", "(", ")", "\n", "\n", "loss", "=", "self", ".", "args", ".", "actor_loss", "*", "actor_loss", "+", "self", ".", "args", ".", "central_loss", "*", "central_loss", "\n", "\n", "# Optimise", "\n", "self", ".", "optimiser", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "\n", "grad_norm", "=", "th", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "params", ",", "self", ".", "args", ".", "grad_norm_clip", ")", "\n", "self", ".", "grad_norm", "=", "grad_norm", "\n", "\n", "self", ".", "optimiser", ".", "step", "(", ")", "\n", "\n", "if", "(", "episode_num", "-", "self", ".", "last_target_update_episode", ")", "/", "self", ".", "args", ".", "target_update_interval", ">=", "1.0", ":", "\n", "            ", "self", ".", "_update_targets", "(", ")", "\n", "self", ".", "last_target_update_episode", "=", "episode_num", "\n", "\n", "", "if", "t_env", "-", "self", ".", "log_stats_t", ">=", "self", ".", "args", ".", "learner_log_interval", ":", "\n", "            ", "self", ".", "logger", ".", "log_stat", "(", "\"loss\"", ",", "loss", ".", "item", "(", ")", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"actor_loss\"", ",", "actor_loss", ".", "item", "(", ")", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"grad_norm\"", ",", "grad_norm", ",", "t_env", ")", "\n", "mask_elems", "=", "mask", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"target_mean\"", ",", "(", "targets", "*", "mask", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "/", "(", "mask_elems", "*", "self", ".", "args", ".", "n_agents", ")", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"central_loss\"", ",", "central_loss", ".", "item", "(", ")", ",", "t_env", ")", "\n", "ps", "=", "mac_out", "[", ":", ",", ":", "-", "1", "]", "*", "avail_actions", "[", ":", ",", ":", "-", "1", "]", "\n", "log_ps", "=", "th", ".", "log", "(", "mac_out", "[", ":", ",", ":", "-", "1", "]", "+", "0.00001", ")", "*", "avail_actions", "[", ":", ",", ":", "-", "1", "]", "\n", "actor_entropy", "=", "-", "(", "(", "(", "ps", "*", "log_ps", ")", ".", "sum", "(", "dim", "=", "3", ")", "*", "mask", ")", ".", "sum", "(", ")", "/", "mask", ".", "sum", "(", ")", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"actor_entropy\"", ",", "actor_entropy", ".", "item", "(", ")", ",", "t_env", ")", "\n", "self", ".", "log_stats_t", "=", "t_env", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.max_q_learner_sac.SACQLearner._update_targets": [[171, 179], ["max_q_learner_sac.SACQLearner.target_mac.load_state", "max_q_learner_sac.SACQLearner.target_central_mixer.load_state_dict", "max_q_learner_sac.SACQLearner.logger.console_logger.info", "max_q_learner_sac.SACQLearner.target_mixer.load_state_dict", "max_q_learner_sac.SACQLearner.target_central_mac.load_state", "max_q_learner_sac.SACQLearner.central_mixer.state_dict", "max_q_learner_sac.SACQLearner.mixer.state_dict"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.load_state", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.load_state"], ["", "", "def", "_update_targets", "(", "self", ")", ":", "\n", "        ", "self", ".", "target_mac", ".", "load_state", "(", "self", ".", "mac", ")", "\n", "if", "self", ".", "mixer", "is", "not", "None", ":", "\n", "            ", "self", ".", "target_mixer", ".", "load_state_dict", "(", "self", ".", "mixer", ".", "state_dict", "(", ")", ")", "\n", "", "if", "self", ".", "central_mac", "is", "not", "None", ":", "\n", "            ", "self", ".", "target_central_mac", ".", "load_state", "(", "self", ".", "central_mac", ")", "\n", "", "self", ".", "target_central_mixer", ".", "load_state_dict", "(", "self", ".", "central_mixer", ".", "state_dict", "(", ")", ")", "\n", "self", ".", "logger", ".", "console_logger", ".", "info", "(", "\"Updated target network\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.max_q_learner_sac.SACQLearner.cuda": [[180, 191], ["max_q_learner_sac.SACQLearner.mac.cuda", "max_q_learner_sac.SACQLearner.target_mac.cuda", "max_q_learner_sac.SACQLearner.central_mixer.cuda", "max_q_learner_sac.SACQLearner.target_central_mixer.cuda", "max_q_learner_sac.SACQLearner.mixer.cuda", "max_q_learner_sac.SACQLearner.target_mixer.cuda", "max_q_learner_sac.SACQLearner.central_mac.cuda", "max_q_learner_sac.SACQLearner.target_central_mac.cuda"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda"], ["", "def", "cuda", "(", "self", ")", ":", "\n", "        ", "self", ".", "mac", ".", "cuda", "(", ")", "\n", "self", ".", "target_mac", ".", "cuda", "(", ")", "\n", "if", "self", ".", "mixer", "is", "not", "None", ":", "\n", "            ", "self", ".", "mixer", ".", "cuda", "(", ")", "\n", "self", ".", "target_mixer", ".", "cuda", "(", ")", "\n", "", "if", "self", ".", "central_mac", "is", "not", "None", ":", "\n", "            ", "self", ".", "central_mac", ".", "cuda", "(", ")", "\n", "self", ".", "target_central_mac", ".", "cuda", "(", ")", "\n", "", "self", ".", "central_mixer", ".", "cuda", "(", ")", "\n", "self", ".", "target_central_mixer", ".", "cuda", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.max_q_learner_sac.SACQLearner.save_models": [[192, 197], ["max_q_learner_sac.SACQLearner.mac.save_models", "torch.save", "torch.save", "torch.save", "torch.save", "max_q_learner_sac.SACQLearner.optimiser.state_dict", "max_q_learner_sac.SACQLearner.mixer.state_dict"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.save_models"], ["", "def", "save_models", "(", "self", ",", "path", ")", ":", "\n", "        ", "self", ".", "mac", ".", "save_models", "(", "path", ")", "\n", "if", "self", ".", "mixer", "is", "not", "None", ":", "\n", "            ", "th", ".", "save", "(", "self", ".", "mixer", ".", "state_dict", "(", ")", ",", "\"{}/mixer.th\"", ".", "format", "(", "path", ")", ")", "\n", "", "th", ".", "save", "(", "self", ".", "optimiser", ".", "state_dict", "(", ")", ",", "\"{}/opt.th\"", ".", "format", "(", "path", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.max_q_learner_sac.SACQLearner.load_models": [[198, 205], ["max_q_learner_sac.SACQLearner.mac.load_models", "max_q_learner_sac.SACQLearner.target_mac.load_models", "max_q_learner_sac.SACQLearner.optimiser.load_state_dict", "max_q_learner_sac.SACQLearner.mixer.load_state_dict", "torch.load", "torch.load", "torch.load", "torch.load"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.load_models", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.load_models"], ["", "def", "load_models", "(", "self", ",", "path", ")", ":", "\n", "        ", "self", ".", "mac", ".", "load_models", "(", "path", ")", "\n", "# Not quite right but I don't want to save target networks", "\n", "self", ".", "target_mac", ".", "load_models", "(", "path", ")", "\n", "if", "self", ".", "mixer", "is", "not", "None", ":", "\n", "            ", "self", ".", "mixer", ".", "load_state_dict", "(", "th", ".", "load", "(", "\"{}/mixer.th\"", ".", "format", "(", "path", ")", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", ")", "\n", "", "self", ".", "optimiser", ".", "load_state_dict", "(", "th", ".", "load", "(", "\"{}/opt.th\"", ".", "format", "(", "path", ")", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.qtran_learner.QLearner.__init__": [[12, 36], ["list", "list", "copy.deepcopy", "torch.optim.RMSprop", "copy.deepcopy", "mac.parameters", "modules.mixers.qtran.QTranBase", "qtran_learner.QLearner.mixer.parameters", "modules.mixers.qtran.QTranAlt"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.parameters", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.parameters"], ["    ", "def", "__init__", "(", "self", ",", "mac", ",", "scheme", ",", "logger", ",", "args", ")", ":", "\n", "        ", "self", ".", "args", "=", "args", "\n", "self", ".", "mac", "=", "mac", "\n", "self", ".", "logger", "=", "logger", "\n", "\n", "self", ".", "params", "=", "list", "(", "mac", ".", "parameters", "(", ")", ")", "\n", "\n", "self", ".", "last_target_update_episode", "=", "0", "\n", "\n", "self", ".", "mixer", "=", "None", "\n", "if", "args", ".", "mixer", "==", "\"qtran_base\"", ":", "\n", "            ", "self", ".", "mixer", "=", "QTranBase", "(", "args", ")", "\n", "", "elif", "args", ".", "mixer", "==", "\"qtran_alt\"", ":", "\n", "            ", "self", ".", "mixer", "=", "QTranAlt", "(", "args", ")", "\n", "\n", "", "self", ".", "params", "+=", "list", "(", "self", ".", "mixer", ".", "parameters", "(", ")", ")", "\n", "self", ".", "target_mixer", "=", "copy", ".", "deepcopy", "(", "self", ".", "mixer", ")", "\n", "\n", "self", ".", "optimiser", "=", "RMSprop", "(", "params", "=", "self", ".", "params", ",", "lr", "=", "args", ".", "lr", ",", "alpha", "=", "args", ".", "optim_alpha", ",", "eps", "=", "args", ".", "optim_eps", ")", "\n", "\n", "# a little wasteful to deepcopy (e.g. duplicates action selector), but should work for any MAC", "\n", "self", ".", "target_mac", "=", "copy", ".", "deepcopy", "(", "mac", ")", "\n", "\n", "self", ".", "log_stats_t", "=", "-", "self", ".", "args", ".", "learner_log_interval", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.qtran_learner.QLearner.train": [[37, 211], ["[].float", "[].float", "qtran_learner.QLearner.mac.init_hidden", "range", "torch.stack", "torch.stack", "mac_hidden_states.reshape().transpose.reshape().transpose.reshape().transpose", "torch.gather().squeeze", "qtran_learner.QLearner.target_mac.init_hidden", "range", "torch.stack", "torch.stack", "target_mac_hidden_states.reshape().transpose.reshape().transpose.reshape().transpose", "torch.stack.clone", "mac_out_maxs[].max", "qtran_learner.QLearner.optimiser.zero_grad", "loss.backward", "torch.nn.utils.clip_grad_norm_", "qtran_learner.QLearner.optimiser.step", "qtran_learner.QLearner.mac.forward", "torch.stack.append", "mac_hidden_states.reshape().transpose.reshape().transpose.append", "qtran_learner.QLearner.target_mac.forward", "torch.stack.append", "target_mac_hidden_states.reshape().transpose.reshape().transpose.append", "torch.stack.max", "qtran_learner.QLearner.mixer", "qtran_learner.QLearner.target_mixer", "qtran_learner.QLearner.mixer", "nopt_values.clamp", "qtran_learner.QLearner._update_targets", "qtran_learner.QLearner.logger.log_stat", "qtran_learner.QLearner.logger.log_stat", "qtran_learner.QLearner.logger.log_stat", "qtran_learner.QLearner.logger.log_stat", "qtran_learner.QLearner.logger.log_stat", "mac_hidden_states.reshape().transpose.reshape().transpose.reshape", "torch.gather", "target_mac_hidden_states.reshape().transpose.reshape().transpose.reshape", "torch.zeros", "torch.zeros.scatter", "torch.zeros", "torch.zeros.scatter", "rewards.reshape", "td_targets.detach", "[].float.reshape", "[].float.sum", "torch.zeros", "torch.zeros.scatter", "[].float.reshape", "[].float.sum", "[].float.reshape", "[].float.sum", "Exception", "qtran_learner.QLearner.mixer", "torch.zeros", "torch.zeros.scatter", "th.zeros.scatter.repeat", "agent_mask.view().repeat.view().repeat.view().repeat", "masked_actions.view.view.view", "qtran_learner.QLearner.target_mixer", "target_counter_qs.gather", "counter_qs.gather", "[].float.repeat().view", "torch.zeros", "torch.zeros.scatter", "th.zeros.scatter.repeat", "agent_mask.view().repeat.view().repeat.view().repeat", "opt_masked_actions.view.view.view", "qtran_learner.QLearner.mixer", "torch.gather().squeeze.clone().unsqueeze().repeat().view", "torch.tensor().repeat", "th.gather().squeeze.clone().unsqueeze().repeat().view.scatter", "loss.item", "td_loss.item", "opt_loss.item", "nopt_loss.item", "[].float.sum().item", "qtran_learner.QLearner.logger.log_stat", "qtran_learner.QLearner.logger.log_stat", "qtran_learner.QLearner.logger.log_stat", "qtran_learner.QLearner.logger.log_stat", "qtran_learner.QLearner.logger.log_stat", "max_actions_qvals[].sum().reshape", "max_joint_qs.detach", "torch.gather().squeeze.sum().reshape", "joint_qs.detach", "torch.eye", "agent_mask.view().repeat.view().repeat.unsqueeze().unsqueeze", "target_max_actions.view", "actions.contiguous().view", "rewards.repeat().view", "td_targets.detach", "[].float.repeat().view.sum", "torch.eye", "agent_mask.view().repeat.view().repeat.unsqueeze().unsqueeze", "[].float.repeat().view.sum", "torch.tensor().repeat.unsqueeze", "torch.min", "[].float.repeat().view.sum", "[].float.sum().item", "[].float.repeat().view.sum().item", "qtran_learner.QLearner.logger.log_stat", "qtran_learner.QLearner.logger.log_stat", "qtran_learner.QLearner.logger.log_stat", "qtran_learner.QLearner.logger.log_stat", "qtran_learner.QLearner.logger.log_stat", "agent_mask.view().repeat.view().repeat.view", "[].float.repeat", "agent_mask.view().repeat.view().repeat.view", "max_actions_qvals.squeeze().sum().repeat().view", "opt_target_qs.gather().detach", "torch.gather().squeeze.clone().unsqueeze().repeat", "torch.tensor", "counter_qs.detach", "[].float.sum", "masked_td_error.abs().sum().item", "masked_td_error.sum().item", "joint_qs.sum().item", "vs.sum().item", "[].float.reshape", "max_actions_qvals[].sum", "torch.gather().squeeze.sum", "agent_mask.view().repeat.view().repeat.unsqueeze", "actions.contiguous", "rewards.repeat", "agent_mask.view().repeat.view().repeat.unsqueeze", "mac_out[].contiguous().view", "th.gather().squeeze.clone().unsqueeze().repeat().view.sum", "[].float.sum", "[].float.repeat().view.sum", "masked_td_error.abs().sum().item", "[].float.repeat().view", "max_actions_qvals.squeeze().sum().repeat", "opt_target_qs.gather", "torch.gather().squeeze.clone().unsqueeze", "masked_td_error.abs().sum", "masked_td_error.sum", "joint_qs.sum", "vs.sum", "max_actions_current.view", "range", "mac_out[].contiguous", "masked_td_error.abs().sum", "[].float.repeat", "max_actions_qvals.squeeze().sum", "torch.gather().squeeze.clone", "masked_td_error.abs", "masked_td_error.abs", "max_actions_qvals.squeeze"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.agents.central_rnn_agent.CentralRNNAgent.init_hidden", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.agents.central_rnn_agent.CentralRNNAgent.init_hidden", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.step", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.sqddpg.SQDDPGMixer.forward", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.sqddpg.SQDDPGMixer.forward", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.q_learner.QLearner._update_targets", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat"], ["", "def", "train", "(", "self", ",", "batch", ":", "EpisodeBatch", ",", "t_env", ":", "int", ",", "episode_num", ":", "int", ")", ":", "\n", "# Get the relevant quantities", "\n", "        ", "rewards", "=", "batch", "[", "\"reward\"", "]", "[", ":", ",", ":", "-", "1", "]", "\n", "actions", "=", "batch", "[", "\"actions\"", "]", "[", ":", ",", ":", "-", "1", "]", "\n", "terminated", "=", "batch", "[", "\"terminated\"", "]", "[", ":", ",", ":", "-", "1", "]", ".", "float", "(", ")", "\n", "mask", "=", "batch", "[", "\"filled\"", "]", "[", ":", ",", ":", "-", "1", "]", ".", "float", "(", ")", "\n", "mask", "[", ":", ",", "1", ":", "]", "=", "mask", "[", ":", ",", "1", ":", "]", "*", "(", "1", "-", "terminated", "[", ":", ",", ":", "-", "1", "]", ")", "\n", "avail_actions", "=", "batch", "[", "\"avail_actions\"", "]", "\n", "\n", "# Calculate estimated Q-Values", "\n", "mac_out", "=", "[", "]", "\n", "mac_hidden_states", "=", "[", "]", "\n", "self", ".", "mac", ".", "init_hidden", "(", "batch", ".", "batch_size", ")", "\n", "for", "t", "in", "range", "(", "batch", ".", "max_seq_length", ")", ":", "\n", "            ", "agent_outs", "=", "self", ".", "mac", ".", "forward", "(", "batch", ",", "t", "=", "t", ")", "\n", "mac_out", ".", "append", "(", "agent_outs", ")", "\n", "mac_hidden_states", ".", "append", "(", "self", ".", "mac", ".", "hidden_states", ")", "\n", "", "mac_out", "=", "th", ".", "stack", "(", "mac_out", ",", "dim", "=", "1", ")", "# Concat over time", "\n", "mac_hidden_states", "=", "th", ".", "stack", "(", "mac_hidden_states", ",", "dim", "=", "1", ")", "\n", "mac_hidden_states", "=", "mac_hidden_states", ".", "reshape", "(", "batch", ".", "batch_size", ",", "self", ".", "args", ".", "n_agents", ",", "batch", ".", "max_seq_length", ",", "-", "1", ")", ".", "transpose", "(", "1", ",", "2", ")", "#btav", "\n", "\n", "# Pick the Q-Values for the actions taken by each agent", "\n", "chosen_action_qvals", "=", "th", ".", "gather", "(", "mac_out", "[", ":", ",", ":", "-", "1", "]", ",", "dim", "=", "3", ",", "index", "=", "actions", ")", ".", "squeeze", "(", "3", ")", "# Remove the last dim", "\n", "\n", "# Calculate the Q-Values necessary for the target", "\n", "target_mac_out", "=", "[", "]", "\n", "target_mac_hidden_states", "=", "[", "]", "\n", "self", ".", "target_mac", ".", "init_hidden", "(", "batch", ".", "batch_size", ")", "\n", "for", "t", "in", "range", "(", "batch", ".", "max_seq_length", ")", ":", "\n", "            ", "target_agent_outs", "=", "self", ".", "target_mac", ".", "forward", "(", "batch", ",", "t", "=", "t", ")", "\n", "target_mac_out", ".", "append", "(", "target_agent_outs", ")", "\n", "target_mac_hidden_states", ".", "append", "(", "self", ".", "target_mac", ".", "hidden_states", ")", "\n", "\n", "# We don't need the first timesteps Q-Value estimate for calculating targets", "\n", "", "target_mac_out", "=", "th", ".", "stack", "(", "target_mac_out", "[", ":", "]", ",", "dim", "=", "1", ")", "# Concat across time", "\n", "target_mac_hidden_states", "=", "th", ".", "stack", "(", "target_mac_hidden_states", ",", "dim", "=", "1", ")", "\n", "target_mac_hidden_states", "=", "target_mac_hidden_states", ".", "reshape", "(", "batch", ".", "batch_size", ",", "self", ".", "args", ".", "n_agents", ",", "batch", ".", "max_seq_length", ",", "-", "1", ")", ".", "transpose", "(", "1", ",", "2", ")", "#btav", "\n", "\n", "# Mask out unavailable actions", "\n", "target_mac_out", "[", "avail_actions", "[", ":", ",", ":", "]", "==", "0", "]", "=", "-", "9999999", "# From OG deepmarl", "\n", "mac_out_maxs", "=", "mac_out", ".", "clone", "(", ")", "\n", "mac_out_maxs", "[", "avail_actions", "==", "0", "]", "=", "-", "9999999", "\n", "\n", "# Best joint action computed by target agents", "\n", "target_max_actions", "=", "target_mac_out", ".", "max", "(", "dim", "=", "3", ",", "keepdim", "=", "True", ")", "[", "1", "]", "\n", "# Best joint-action computed by regular agents", "\n", "max_actions_qvals", ",", "max_actions_current", "=", "mac_out_maxs", "[", ":", ",", ":", "]", ".", "max", "(", "dim", "=", "3", ",", "keepdim", "=", "True", ")", "\n", "\n", "if", "self", ".", "args", ".", "mixer", "==", "\"qtran_base\"", ":", "\n", "# -- TD Loss --", "\n", "# Joint-action Q-Value estimates", "\n", "            ", "joint_qs", ",", "vs", "=", "self", ".", "mixer", "(", "batch", "[", ":", ",", ":", "-", "1", "]", ",", "mac_hidden_states", "[", ":", ",", ":", "-", "1", "]", ")", "\n", "\n", "# Need to argmax across the target agents' actions to compute target joint-action Q-Values", "\n", "if", "self", ".", "args", ".", "double_q", ":", "\n", "                ", "max_actions_current_", "=", "th", ".", "zeros", "(", "size", "=", "(", "batch", ".", "batch_size", ",", "batch", ".", "max_seq_length", ",", "self", ".", "args", ".", "n_agents", ",", "self", ".", "args", ".", "n_actions", ")", ",", "device", "=", "batch", ".", "device", ")", "\n", "max_actions_current_onehot", "=", "max_actions_current_", ".", "scatter", "(", "3", ",", "max_actions_current", "[", ":", ",", ":", "]", ",", "1", ")", "\n", "max_actions_onehot", "=", "max_actions_current_onehot", "\n", "", "else", ":", "\n", "                ", "max_actions", "=", "th", ".", "zeros", "(", "size", "=", "(", "batch", ".", "batch_size", ",", "batch", ".", "max_seq_length", ",", "self", ".", "args", ".", "n_agents", ",", "self", ".", "args", ".", "n_actions", ")", ",", "device", "=", "batch", ".", "device", ")", "\n", "max_actions_onehot", "=", "max_actions", ".", "scatter", "(", "3", ",", "target_max_actions", "[", ":", ",", ":", "]", ",", "1", ")", "\n", "", "target_joint_qs", ",", "target_vs", "=", "self", ".", "target_mixer", "(", "batch", "[", ":", ",", "1", ":", "]", ",", "hidden_states", "=", "target_mac_hidden_states", "[", ":", ",", "1", ":", "]", ",", "actions", "=", "max_actions_onehot", "[", ":", ",", "1", ":", "]", ")", "\n", "\n", "# Td loss targets", "\n", "td_targets", "=", "rewards", ".", "reshape", "(", "-", "1", ",", "1", ")", "+", "self", ".", "args", ".", "gamma", "*", "(", "1", "-", "terminated", ".", "reshape", "(", "-", "1", ",", "1", ")", ")", "*", "target_joint_qs", "\n", "td_error", "=", "(", "joint_qs", "-", "td_targets", ".", "detach", "(", ")", ")", "\n", "masked_td_error", "=", "td_error", "*", "mask", ".", "reshape", "(", "-", "1", ",", "1", ")", "\n", "td_loss", "=", "(", "masked_td_error", "**", "2", ")", ".", "sum", "(", ")", "/", "mask", ".", "sum", "(", ")", "\n", "# -- TD Loss --", "\n", "\n", "# -- Opt Loss --", "\n", "# Argmax across the current agents' actions", "\n", "if", "not", "self", ".", "args", ".", "double_q", ":", "# Already computed if we're doing double Q-Learning", "\n", "                ", "max_actions_current_", "=", "th", ".", "zeros", "(", "size", "=", "(", "batch", ".", "batch_size", ",", "batch", ".", "max_seq_length", ",", "self", ".", "args", ".", "n_agents", ",", "self", ".", "args", ".", "n_actions", ")", ",", "device", "=", "batch", ".", "device", ")", "\n", "max_actions_current_onehot", "=", "max_actions_current_", ".", "scatter", "(", "3", ",", "max_actions_current", "[", ":", ",", ":", "]", ",", "1", ")", "\n", "", "max_joint_qs", ",", "_", "=", "self", ".", "mixer", "(", "batch", "[", ":", ",", ":", "-", "1", "]", ",", "mac_hidden_states", "[", ":", ",", ":", "-", "1", "]", ",", "actions", "=", "max_actions_current_onehot", "[", ":", ",", ":", "-", "1", "]", ")", "# Don't use the target network and target agent max actions as per author's email", "\n", "\n", "# max_actions_qvals = th.gather(mac_out[:, :-1], dim=3, index=max_actions_current[:,:-1])", "\n", "opt_error", "=", "max_actions_qvals", "[", ":", ",", ":", "-", "1", "]", ".", "sum", "(", "dim", "=", "2", ")", ".", "reshape", "(", "-", "1", ",", "1", ")", "-", "max_joint_qs", ".", "detach", "(", ")", "+", "vs", "\n", "masked_opt_error", "=", "opt_error", "*", "mask", ".", "reshape", "(", "-", "1", ",", "1", ")", "\n", "opt_loss", "=", "(", "masked_opt_error", "**", "2", ")", ".", "sum", "(", ")", "/", "mask", ".", "sum", "(", ")", "\n", "# -- Opt Loss --", "\n", "\n", "# -- Nopt Loss --", "\n", "# target_joint_qs, _ = self.target_mixer(batch[:, :-1])", "\n", "nopt_values", "=", "chosen_action_qvals", ".", "sum", "(", "dim", "=", "2", ")", ".", "reshape", "(", "-", "1", ",", "1", ")", "-", "joint_qs", ".", "detach", "(", ")", "+", "vs", "# Don't use target networks here either", "\n", "nopt_error", "=", "nopt_values", ".", "clamp", "(", "max", "=", "0", ")", "\n", "masked_nopt_error", "=", "nopt_error", "*", "mask", ".", "reshape", "(", "-", "1", ",", "1", ")", "\n", "nopt_loss", "=", "(", "masked_nopt_error", "**", "2", ")", ".", "sum", "(", ")", "/", "mask", ".", "sum", "(", ")", "\n", "# -- Nopt loss --", "\n", "\n", "", "elif", "self", ".", "args", ".", "mixer", "==", "\"qtran_alt\"", ":", "\n", "            ", "raise", "Exception", "(", "\"Not supported yet.\"", ")", "\n", "counter_qs", ",", "vs", "=", "self", ".", "mixer", "(", "batch", "[", ":", ",", ":", "-", "1", "]", ")", "\n", "\n", "# Need to argmax across the target agents' actions", "\n", "# Convert cur_max_actions to one hot", "\n", "max_actions", "=", "th", ".", "zeros", "(", "size", "=", "(", "batch", ".", "batch_size", ",", "batch", ".", "max_seq_length", "-", "1", ",", "self", ".", "args", ".", "n_agents", ",", "self", ".", "args", ".", "n_actions", ")", ",", "device", "=", "batch", ".", "device", ")", "\n", "max_actions_onehot", "=", "max_actions", ".", "scatter", "(", "3", ",", "target_max_actions", ",", "1", ")", "\n", "max_actions_onehot_repeat", "=", "max_actions_onehot", ".", "repeat", "(", "1", ",", "1", ",", "self", ".", "args", ".", "n_agents", ",", "1", ")", "\n", "agent_mask", "=", "(", "1", "-", "th", ".", "eye", "(", "self", ".", "args", ".", "n_agents", ",", "device", "=", "batch", ".", "device", ")", ")", "\n", "agent_mask", "=", "agent_mask", ".", "view", "(", "-", "1", ",", "1", ")", ".", "repeat", "(", "1", ",", "self", ".", "args", ".", "n_actions", ")", "#.view(self.n_agents, -1)", "\n", "masked_actions", "=", "max_actions_onehot_repeat", "*", "agent_mask", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "0", ")", "\n", "masked_actions", "=", "masked_actions", ".", "view", "(", "-", "1", ",", "self", ".", "args", ".", "n_agents", "*", "self", ".", "args", ".", "n_actions", ")", "\n", "target_counter_qs", ",", "target_vs", "=", "self", ".", "target_mixer", "(", "batch", "[", ":", ",", "1", ":", "]", ",", "masked_actions", ")", "\n", "\n", "# Td loss", "\n", "td_target_qs", "=", "target_counter_qs", ".", "gather", "(", "1", ",", "target_max_actions", ".", "view", "(", "-", "1", ",", "1", ")", ")", "\n", "td_chosen_qs", "=", "counter_qs", ".", "gather", "(", "1", ",", "actions", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "1", ")", ")", "\n", "td_targets", "=", "rewards", ".", "repeat", "(", "1", ",", "1", ",", "self", ".", "args", ".", "n_agents", ")", ".", "view", "(", "-", "1", ",", "1", ")", "+", "self", ".", "args", ".", "gamma", "*", "(", "1", "-", "terminated", ".", "repeat", "(", "1", ",", "1", ",", "self", ".", "args", ".", "n_agents", ")", ".", "view", "(", "-", "1", ",", "1", ")", ")", "*", "td_target_qs", "\n", "td_error", "=", "(", "td_chosen_qs", "-", "td_targets", ".", "detach", "(", ")", ")", "\n", "\n", "td_mask", "=", "mask", ".", "repeat", "(", "1", ",", "1", ",", "self", ".", "args", ".", "n_agents", ")", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "masked_td_error", "=", "td_error", "*", "td_mask", "\n", "\n", "td_loss", "=", "(", "masked_td_error", "**", "2", ")", ".", "sum", "(", ")", "/", "td_mask", ".", "sum", "(", ")", "\n", "\n", "# Opt loss", "\n", "# Computing the targets", "\n", "opt_max_actions", "=", "th", ".", "zeros", "(", "size", "=", "(", "batch", ".", "batch_size", ",", "batch", ".", "max_seq_length", "-", "1", ",", "self", ".", "args", ".", "n_agents", ",", "self", ".", "args", ".", "n_actions", ")", ",", "device", "=", "batch", ".", "device", ")", "\n", "opt_max_actions_onehot", "=", "opt_max_actions", ".", "scatter", "(", "3", ",", "max_actions_current", ",", "1", ")", "\n", "opt_max_actions_onehot_repeat", "=", "opt_max_actions_onehot", ".", "repeat", "(", "1", ",", "1", ",", "self", ".", "args", ".", "n_agents", ",", "1", ")", "\n", "agent_mask", "=", "(", "1", "-", "th", ".", "eye", "(", "self", ".", "args", ".", "n_agents", ",", "device", "=", "batch", ".", "device", ")", ")", "\n", "agent_mask", "=", "agent_mask", ".", "view", "(", "-", "1", ",", "1", ")", ".", "repeat", "(", "1", ",", "self", ".", "args", ".", "n_actions", ")", "\n", "opt_masked_actions", "=", "opt_max_actions_onehot_repeat", "*", "agent_mask", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "0", ")", "\n", "opt_masked_actions", "=", "opt_masked_actions", ".", "view", "(", "-", "1", ",", "self", ".", "args", ".", "n_agents", "*", "self", ".", "args", ".", "n_actions", ")", "\n", "\n", "opt_target_qs", ",", "opt_vs", "=", "self", ".", "mixer", "(", "batch", "[", ":", ",", ":", "-", "1", "]", ",", "opt_masked_actions", ")", "\n", "\n", "opt_error", "=", "max_actions_qvals", ".", "squeeze", "(", "3", ")", ".", "sum", "(", "dim", "=", "2", ",", "keepdim", "=", "True", ")", ".", "repeat", "(", "1", ",", "1", ",", "self", ".", "args", ".", "n_agents", ")", ".", "view", "(", "-", "1", ",", "1", ")", "-", "opt_target_qs", ".", "gather", "(", "1", ",", "max_actions_current", ".", "view", "(", "-", "1", ",", "1", ")", ")", ".", "detach", "(", ")", "+", "opt_vs", "\n", "opt_loss", "=", "(", "(", "opt_error", "*", "td_mask", ")", "**", "2", ")", ".", "sum", "(", ")", "/", "td_mask", ".", "sum", "(", ")", "\n", "\n", "# NOpt loss", "\n", "qsums", "=", "chosen_action_qvals", ".", "clone", "(", ")", ".", "unsqueeze", "(", "2", ")", ".", "repeat", "(", "1", ",", "1", ",", "self", ".", "args", ".", "n_agents", ",", "1", ")", ".", "view", "(", "-", "1", ",", "self", ".", "args", ".", "n_agents", ")", "\n", "ids_to_zero", "=", "th", ".", "tensor", "(", "[", "i", "for", "i", "in", "range", "(", "self", ".", "args", ".", "n_agents", ")", "]", ",", "device", "=", "batch", ".", "device", ")", ".", "repeat", "(", "batch", ".", "batch_size", "*", "(", "batch", ".", "max_seq_length", "-", "1", ")", ")", "\n", "qsums", ".", "scatter", "(", "1", ",", "ids_to_zero", ".", "unsqueeze", "(", "1", ")", ",", "0", ")", "\n", "nopt_error", "=", "mac_out", "[", ":", ",", ":", "-", "1", "]", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "self", ".", "args", ".", "n_actions", ")", "+", "qsums", ".", "sum", "(", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "-", "counter_qs", ".", "detach", "(", ")", "+", "opt_vs", "\n", "min_nopt_error", "=", "th", ".", "min", "(", "nopt_error", ",", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "[", "0", "]", "\n", "nopt_loss", "=", "(", "(", "min_nopt_error", "*", "td_mask", ")", "**", "2", ")", ".", "sum", "(", ")", "/", "td_mask", ".", "sum", "(", ")", "\n", "\n", "", "loss", "=", "td_loss", "+", "self", ".", "args", ".", "opt_loss", "*", "opt_loss", "+", "self", ".", "args", ".", "nopt_min_loss", "*", "nopt_loss", "\n", "\n", "# Optimise", "\n", "self", ".", "optimiser", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "grad_norm", "=", "th", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "params", ",", "self", ".", "args", ".", "grad_norm_clip", ")", "\n", "self", ".", "optimiser", ".", "step", "(", ")", "\n", "\n", "if", "(", "episode_num", "-", "self", ".", "last_target_update_episode", ")", "/", "self", ".", "args", ".", "target_update_interval", ">=", "1.0", ":", "\n", "            ", "self", ".", "_update_targets", "(", ")", "\n", "self", ".", "last_target_update_episode", "=", "episode_num", "\n", "\n", "", "if", "t_env", "-", "self", ".", "log_stats_t", ">=", "self", ".", "args", ".", "learner_log_interval", ":", "\n", "            ", "self", ".", "logger", ".", "log_stat", "(", "\"loss\"", ",", "loss", ".", "item", "(", ")", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"td_loss\"", ",", "td_loss", ".", "item", "(", ")", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"opt_loss\"", ",", "opt_loss", ".", "item", "(", ")", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"nopt_loss\"", ",", "nopt_loss", ".", "item", "(", ")", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"grad_norm\"", ",", "grad_norm", ",", "t_env", ")", "\n", "if", "self", ".", "args", ".", "mixer", "==", "\"qtran_base\"", ":", "\n", "                ", "mask_elems", "=", "mask", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"td_error_abs\"", ",", "(", "masked_td_error", ".", "abs", "(", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "/", "mask_elems", ")", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"td_targets\"", ",", "(", "(", "masked_td_error", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "/", "mask_elems", ")", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"td_chosen_qs\"", ",", "(", "joint_qs", ".", "sum", "(", ")", ".", "item", "(", ")", "/", "mask_elems", ")", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"v_mean\"", ",", "(", "vs", ".", "sum", "(", ")", ".", "item", "(", ")", "/", "mask_elems", ")", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"agent_indiv_qs\"", ",", "(", "(", "chosen_action_qvals", "*", "mask", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "/", "(", "mask_elems", "*", "self", ".", "args", ".", "n_agents", ")", ")", ",", "t_env", ")", "\n", "", "elif", "self", ".", "args", ".", "mixer", "==", "\"qtran_alt\"", ":", "\n", "                ", "mask_elems", "=", "mask", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "mask_td_elems", "=", "td_mask", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"td_error_abs\"", ",", "(", "masked_td_error", ".", "abs", "(", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "/", "mask_td_elems", ")", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"q_taken_mean\"", ",", "(", "td_chosen_qs", "*", "td_mask", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "/", "(", "mask_td_elems", ")", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"target_mean\"", ",", "(", "td_targets", "*", "td_mask", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "/", "mask_td_elems", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"agent_qs_mean\"", ",", "(", "chosen_action_qvals", "*", "mask", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "/", "(", "mask_elems", "*", "self", ".", "args", ".", "n_agents", ")", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"v_mean\"", ",", "(", "vs", "*", "td_mask", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "/", "(", "mask_td_elems", ")", ",", "t_env", ")", "\n", "", "self", ".", "log_stats_t", "=", "t_env", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.qtran_learner.QLearner._update_targets": [[212, 217], ["qtran_learner.QLearner.target_mac.load_state", "qtran_learner.QLearner.logger.console_logger.info", "qtran_learner.QLearner.target_mixer.load_state_dict", "qtran_learner.QLearner.mixer.state_dict"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.load_state"], ["", "", "def", "_update_targets", "(", "self", ")", ":", "\n", "        ", "self", ".", "target_mac", ".", "load_state", "(", "self", ".", "mac", ")", "\n", "if", "self", ".", "mixer", "is", "not", "None", ":", "\n", "            ", "self", ".", "target_mixer", ".", "load_state_dict", "(", "self", ".", "mixer", ".", "state_dict", "(", ")", ")", "\n", "", "self", ".", "logger", ".", "console_logger", ".", "info", "(", "\"Updated target network\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.qtran_learner.QLearner.cuda": [[218, 224], ["qtran_learner.QLearner.mac.cuda", "qtran_learner.QLearner.target_mac.cuda", "qtran_learner.QLearner.mixer.cuda", "qtran_learner.QLearner.target_mixer.cuda"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda"], ["", "def", "cuda", "(", "self", ")", ":", "\n", "        ", "self", ".", "mac", ".", "cuda", "(", ")", "\n", "self", ".", "target_mac", ".", "cuda", "(", ")", "\n", "if", "self", ".", "mixer", "is", "not", "None", ":", "\n", "            ", "self", ".", "mixer", ".", "cuda", "(", ")", "\n", "self", ".", "target_mixer", ".", "cuda", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.qtran_learner.QLearner.save_models": [[225, 230], ["qtran_learner.QLearner.mac.save_models", "torch.save", "torch.save", "qtran_learner.QLearner.optimiser.state_dict", "qtran_learner.QLearner.mixer.state_dict"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.save_models"], ["", "", "def", "save_models", "(", "self", ",", "path", ")", ":", "\n", "        ", "self", ".", "mac", ".", "save_models", "(", "path", ")", "\n", "if", "self", ".", "mixer", "is", "not", "None", ":", "\n", "            ", "th", ".", "save", "(", "self", ".", "mixer", ".", "state_dict", "(", ")", ",", "\"{}/mixer.th\"", ".", "format", "(", "path", ")", ")", "\n", "", "th", ".", "save", "(", "self", ".", "optimiser", ".", "state_dict", "(", ")", ",", "\"{}/opt.th\"", ".", "format", "(", "path", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.qtran_learner.QLearner.load_models": [[231, 238], ["qtran_learner.QLearner.mac.load_models", "qtran_learner.QLearner.target_mac.load_models", "qtran_learner.QLearner.optimiser.load_state_dict", "qtran_learner.QLearner.mixer.load_state_dict", "torch.load", "torch.load"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.load_models", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.load_models"], ["", "def", "load_models", "(", "self", ",", "path", ")", ":", "\n", "        ", "self", ".", "mac", ".", "load_models", "(", "path", ")", "\n", "# Not quite right but I don't want to save target networks", "\n", "self", ".", "target_mac", ".", "load_models", "(", "path", ")", "\n", "if", "self", ".", "mixer", "is", "not", "None", ":", "\n", "            ", "self", ".", "mixer", ".", "load_state_dict", "(", "th", ".", "load", "(", "\"{}/mixer.th\"", ".", "format", "(", "path", ")", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", ")", "\n", "", "self", ".", "optimiser", ".", "load_state_dict", "(", "th", ".", "load", "(", "\"{}/opt.th\"", ".", "format", "(", "path", ")", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.qatten_learner.QattenLearner.__init__": [[13, 41], ["list", "torch.optim.RMSprop", "copy.deepcopy", "mac.parameters", "list", "copy.deepcopy", "modules.mixers.vdn.VDNMixer", "qatten_learner.QattenLearner.mixer.parameters", "modules.mixers.qmix.QMixer", "modules.mixers.qatten.QattenMixer", "ValueError"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.parameters", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.parameters"], ["    ", "def", "__init__", "(", "self", ",", "mac", ",", "scheme", ",", "logger", ",", "args", ")", ":", "\n", "        ", "self", ".", "args", "=", "args", "\n", "self", ".", "mac", "=", "mac", "\n", "self", ".", "logger", "=", "logger", "\n", "\n", "self", ".", "params", "=", "list", "(", "mac", ".", "parameters", "(", ")", ")", "\n", "\n", "self", ".", "last_target_update_episode", "=", "0", "\n", "\n", "self", ".", "mixer", "=", "None", "\n", "if", "args", ".", "mixer", "is", "not", "None", ":", "\n", "            ", "if", "args", ".", "mixer", "==", "\"vdn\"", ":", "\n", "                ", "self", ".", "mixer", "=", "VDNMixer", "(", ")", "\n", "", "elif", "args", ".", "mixer", "==", "\"qmix\"", ":", "\n", "                ", "self", ".", "mixer", "=", "QMixer", "(", "args", ")", "\n", "", "elif", "args", ".", "mixer", "==", "\"qatten\"", ":", "\n", "                ", "self", ".", "mixer", "=", "QattenMixer", "(", "args", ")", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"Mixer {} not recognised.\"", ".", "format", "(", "args", ".", "mixer", ")", ")", "\n", "", "self", ".", "params", "+=", "list", "(", "self", ".", "mixer", ".", "parameters", "(", ")", ")", "\n", "self", ".", "target_mixer", "=", "copy", ".", "deepcopy", "(", "self", ".", "mixer", ")", "\n", "\n", "", "self", ".", "optimiser", "=", "RMSprop", "(", "params", "=", "self", ".", "params", ",", "lr", "=", "args", ".", "lr", ",", "alpha", "=", "args", ".", "optim_alpha", ",", "eps", "=", "args", ".", "optim_eps", ")", "\n", "\n", "# a little wasteful to deepcopy (e.g. duplicates action selector), but should work for any MAC", "\n", "self", ".", "target_mac", "=", "copy", ".", "deepcopy", "(", "mac", ")", "\n", "\n", "self", ".", "log_stats_t", "=", "-", "self", ".", "args", ".", "learner_log_interval", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.qatten_learner.QattenLearner.train": [[42, 158], ["[].float", "[].float", "qatten_learner.QattenLearner.mac.init_hidden", "range", "torch.stack", "torch.gather().squeeze", "torch.stack.clone().detach", "x_mac_out[].max", "max_action_index.detach().unsqueeze.detach().unsqueeze.detach().unsqueeze", "qatten_learner.QattenLearner.target_mac.init_hidden", "range", "torch.stack", "cur_max_actions.detach", "mask.expand_as.expand_as.expand_as", "qatten_learner.QattenLearner.optimiser.zero_grad", "loss.backward", "torch.nn.utils.clip_grad_norm_", "qatten_learner.QattenLearner.optimiser.step", "qatten_learner.QattenLearner.mac.forward", "torch.stack.append", "qatten_learner.QattenLearner.detach().cpu().numpy", "qatten_learner.QattenLearner.target_mac.forward", "torch.stack.append", "torch.stack.clone().detach", "torch.gather().squeeze", "qatten_learner.QattenLearner.detach().cpu().numpy", "targets.detach().cpu().numpy", "print", "qatten_learner.QattenLearner.logger.log_stat", "targets.detach", "torch.mean", "masked_hit_prob.sum", "mask.expand_as.expand_as.sum", "qatten_learner.QattenLearner._update_targets", "qatten_learner.QattenLearner.logger.log_stat", "qatten_learner.QattenLearner.logger.log_stat", "qatten_learner.QattenLearner.logger.log_stat", "mask.expand_as.expand_as.sum().item", "qatten_learner.QattenLearner.logger.log_stat", "qatten_learner.QattenLearner.logger.log_stat", "qatten_learner.QattenLearner.logger.log_stat", "torch.gather", "torch.stack.clone", "max_action_index.detach().unsqueeze.detach().unsqueeze.detach", "mac_out_detach[].max", "torch.stack.max", "torch.stack.max", "qatten_learner.QattenLearner.mixer", "qatten_learner.QattenLearner.target_mixer", "qatten_learner.QattenLearner.mixer", "qatten_learner.QattenLearner.target_mixer", "numpy.squeeze", "numpy.squeeze", "numpy.squeeze", "numpy.squeeze", "numpy.squeeze", "mask.expand_as.expand_as.sum", "loss.item", "hit_prob.item", "qatten_learner.QattenLearner.detach().cpu", "torch.stack.clone", "torch.gather", "qatten_learner.QattenLearner.detach().cpu", "targets.detach().cpu", "mask.expand_as.expand_as.sum", "mask.expand_as.expand_as.sum", "masked_td_error.abs().sum().item", "qatten_learner.QattenLearner.logger.log_stat", "ent.item", "enumerate", "qatten_learner.QattenLearner.detach", "qatten_learner.QattenLearner.detach", "targets.detach", "masked_td_error.abs().sum", "masked_td_error.abs"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.agents.central_rnn_agent.CentralRNNAgent.init_hidden", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.agents.central_rnn_agent.CentralRNNAgent.init_hidden", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.step", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.sqddpg.SQDDPGMixer.forward", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.sqddpg.SQDDPGMixer.forward", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.q_learner.QLearner._update_targets", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat"], ["", "def", "train", "(", "self", ",", "batch", ":", "EpisodeBatch", ",", "t_env", ":", "int", ",", "episode_num", ":", "int", ",", "show_demo", "=", "False", ",", "save_data", "=", "None", ")", ":", "\n", "# Get the relevant quantities", "\n", "        ", "rewards", "=", "batch", "[", "\"reward\"", "]", "[", ":", ",", ":", "-", "1", "]", "\n", "actions", "=", "batch", "[", "\"actions\"", "]", "[", ":", ",", ":", "-", "1", "]", "\n", "terminated", "=", "batch", "[", "\"terminated\"", "]", "[", ":", ",", ":", "-", "1", "]", ".", "float", "(", ")", "\n", "mask", "=", "batch", "[", "\"filled\"", "]", "[", ":", ",", ":", "-", "1", "]", ".", "float", "(", ")", "\n", "mask", "[", ":", ",", "1", ":", "]", "=", "mask", "[", ":", ",", "1", ":", "]", "*", "(", "1", "-", "terminated", "[", ":", ",", ":", "-", "1", "]", ")", "\n", "avail_actions", "=", "batch", "[", "\"avail_actions\"", "]", "\n", "\n", "# Calculate estimated Q-Values", "\n", "mac_out", "=", "[", "]", "\n", "self", ".", "mac", ".", "init_hidden", "(", "batch", ".", "batch_size", ")", "\n", "for", "t", "in", "range", "(", "batch", ".", "max_seq_length", ")", ":", "\n", "            ", "agent_outs", "=", "self", ".", "mac", ".", "forward", "(", "batch", ",", "t", "=", "t", ")", "\n", "mac_out", ".", "append", "(", "agent_outs", ")", "\n", "", "mac_out", "=", "th", ".", "stack", "(", "mac_out", ",", "dim", "=", "1", ")", "# Concat over time", "\n", "\n", "# Pick the Q-Values for the actions taken by each agent", "\n", "chosen_action_qvals", "=", "th", ".", "gather", "(", "mac_out", "[", ":", ",", ":", "-", "1", "]", ",", "dim", "=", "3", ",", "index", "=", "actions", ")", ".", "squeeze", "(", "3", ")", "# Remove the last dim", "\n", "\n", "x_mac_out", "=", "mac_out", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "x_mac_out", "[", "avail_actions", "==", "0", "]", "=", "-", "9999999", "\n", "max_action_qvals", ",", "max_action_index", "=", "x_mac_out", "[", ":", ",", ":", "-", "1", "]", ".", "max", "(", "dim", "=", "3", ")", "\n", "\n", "max_action_index", "=", "max_action_index", ".", "detach", "(", ")", ".", "unsqueeze", "(", "3", ")", "\n", "is_max_action", "=", "(", "max_action_index", "==", "actions", ")", ".", "int", "(", ")", ".", "float", "(", ")", "\n", "\n", "if", "show_demo", ":", "\n", "            ", "q_i_data", "=", "chosen_action_qvals", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "q_data", "=", "(", "max_action_qvals", "-", "chosen_action_qvals", ")", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "# Calculate the Q-Values necessary for the target", "\n", "", "target_mac_out", "=", "[", "]", "\n", "self", ".", "target_mac", ".", "init_hidden", "(", "batch", ".", "batch_size", ")", "\n", "for", "t", "in", "range", "(", "batch", ".", "max_seq_length", ")", ":", "\n", "            ", "target_agent_outs", "=", "self", ".", "target_mac", ".", "forward", "(", "batch", ",", "t", "=", "t", ")", "\n", "target_mac_out", ".", "append", "(", "target_agent_outs", ")", "\n", "\n", "# We don't need the first timesteps Q-Value estimate for calculating targets", "\n", "", "target_mac_out", "=", "th", ".", "stack", "(", "target_mac_out", "[", "1", ":", "]", ",", "dim", "=", "1", ")", "# Concat across time", "\n", "\n", "# Mask out unavailable actions", "\n", "target_mac_out", "[", "avail_actions", "[", ":", ",", "1", ":", "]", "==", "0", "]", "=", "-", "9999999", "\n", "\n", "# Max over target Q-Values", "\n", "if", "self", ".", "args", ".", "double_q", ":", "\n", "# Get actions that maximise live Q (for double q-learning)", "\n", "            ", "mac_out_detach", "=", "mac_out", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "mac_out_detach", "[", "avail_actions", "==", "0", "]", "=", "-", "9999999", "\n", "cur_max_actions", "=", "mac_out_detach", "[", ":", ",", "1", ":", "]", ".", "max", "(", "dim", "=", "3", ",", "keepdim", "=", "True", ")", "[", "1", "]", "\n", "target_max_qvals", "=", "th", ".", "gather", "(", "target_mac_out", ",", "3", ",", "cur_max_actions", ")", ".", "squeeze", "(", "3", ")", "\n", "", "else", ":", "\n", "            ", "target_max_qvals", "=", "target_mac_out", ".", "max", "(", "dim", "=", "3", ")", "[", "0", "]", "\n", "cur_max_actions", "=", "target_mac_out", ".", "max", "(", "dim", "=", "3", ",", "keepdim", "=", "True", ")", "[", "1", "]", "# get the indices", "\n", "# cur_max_actions: (episode_batch, episode_length - 1, agent_num, 1)", "\n", "", "target_next_actions", "=", "cur_max_actions", ".", "detach", "(", ")", "# actions are also inputs for mixer network", "\n", "\n", "# Mix", "\n", "if", "self", ".", "mixer", "is", "not", "None", ":", "\n", "            ", "if", "self", ".", "mixer", ".", "name", "==", "'qatten'", ":", "\n", "                ", "chosen_action_qvals", ",", "q_attend_regs", ",", "head_entropies", "=", "self", ".", "mixer", "(", "chosen_action_qvals", ",", "batch", "[", "\"state\"", "]", "[", ":", ",", ":", "-", "1", "]", ",", "actions", ")", "\n", "target_max_qvals", ",", "_", ",", "_", "=", "self", ".", "target_mixer", "(", "target_max_qvals", ",", "batch", "[", "\"state\"", "]", "[", ":", ",", "1", ":", "]", ",", "target_next_actions", ")", "\n", "", "else", ":", "\n", "                ", "chosen_action_qvals", "=", "self", ".", "mixer", "(", "chosen_action_qvals", ",", "batch", "[", "\"state\"", "]", "[", ":", ",", ":", "-", "1", "]", ")", "\n", "target_max_qvals", "=", "self", ".", "target_mixer", "(", "target_max_qvals", ",", "batch", "[", "\"state\"", "]", "[", ":", ",", "1", ":", "]", ")", "\n", "\n", "# Calculate 1-step Q-Learning targets", "\n", "", "", "targets", "=", "rewards", "+", "self", ".", "args", ".", "gamma", "*", "(", "1", "-", "terminated", ")", "*", "target_max_qvals", "\n", "\n", "if", "show_demo", ":", "\n", "            ", "tot_q_data", "=", "chosen_action_qvals", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "tot_target", "=", "targets", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "print", "(", "'action_pair_%d_%d'", "%", "(", "save_data", "[", "0", "]", ",", "save_data", "[", "1", "]", ")", ",", "np", ".", "squeeze", "(", "q_data", "[", ":", ",", "0", "]", ")", ",", "\n", "np", ".", "squeeze", "(", "q_i_data", "[", ":", ",", "0", "]", ")", ",", "np", ".", "squeeze", "(", "tot_q_data", "[", ":", ",", "0", "]", ")", ",", "np", ".", "squeeze", "(", "tot_target", "[", ":", ",", "0", "]", ")", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "'action_pair_%d_%d'", "%", "(", "save_data", "[", "0", "]", ",", "save_data", "[", "1", "]", ")", ",", "\n", "np", ".", "squeeze", "(", "tot_q_data", "[", ":", ",", "0", "]", ")", ",", "t_env", ")", "\n", "return", "\n", "\n", "# Td-error", "\n", "", "td_error", "=", "(", "chosen_action_qvals", "-", "targets", ".", "detach", "(", ")", ")", "\n", "\n", "mask", "=", "mask", ".", "expand_as", "(", "td_error", ")", "\n", "\n", "# 0-out the targets that came from padded data", "\n", "masked_td_error", "=", "td_error", "*", "mask", "\n", "\n", "# Normal L2 loss, take mean over actual data", "\n", "if", "self", ".", "mixer", ".", "name", "==", "'qatten'", ":", "\n", "            ", "loss", "=", "(", "masked_td_error", "**", "2", ")", ".", "sum", "(", ")", "/", "mask", ".", "sum", "(", ")", "+", "q_attend_regs", "\n", "", "else", ":", "\n", "            ", "loss", "=", "(", "masked_td_error", "**", "2", ")", ".", "sum", "(", ")", "/", "mask", ".", "sum", "(", ")", "\n", "\n", "", "masked_hit_prob", "=", "th", ".", "mean", "(", "is_max_action", ",", "dim", "=", "2", ")", "*", "mask", "\n", "hit_prob", "=", "masked_hit_prob", ".", "sum", "(", ")", "/", "mask", ".", "sum", "(", ")", "\n", "\n", "# Optimise", "\n", "self", ".", "optimiser", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "grad_norm", "=", "th", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "params", ",", "self", ".", "args", ".", "grad_norm_clip", ")", "\n", "self", ".", "optimiser", ".", "step", "(", ")", "\n", "\n", "if", "(", "episode_num", "-", "self", ".", "last_target_update_episode", ")", "/", "self", ".", "args", ".", "target_update_interval", ">=", "1.0", ":", "\n", "            ", "self", ".", "_update_targets", "(", ")", "\n", "self", ".", "last_target_update_episode", "=", "episode_num", "\n", "\n", "", "if", "t_env", "-", "self", ".", "log_stats_t", ">=", "self", ".", "args", ".", "learner_log_interval", ":", "\n", "            ", "self", ".", "logger", ".", "log_stat", "(", "\"loss\"", ",", "loss", ".", "item", "(", ")", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"hit_prob\"", ",", "hit_prob", ".", "item", "(", ")", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"grad_norm\"", ",", "grad_norm", ",", "t_env", ")", "\n", "mask_elems", "=", "mask", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"td_error_abs\"", ",", "(", "masked_td_error", ".", "abs", "(", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "/", "mask_elems", ")", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"q_taken_mean\"", ",", "(", "chosen_action_qvals", "*", "mask", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "/", "(", "mask_elems", "*", "self", ".", "args", ".", "n_agents", ")", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"target_mean\"", ",", "(", "targets", "*", "mask", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "/", "(", "mask_elems", "*", "self", ".", "args", ".", "n_agents", ")", ",", "t_env", ")", "\n", "if", "self", ".", "mixer", ".", "name", "==", "'qatten'", ":", "\n", "                ", "[", "self", ".", "logger", ".", "log_stat", "(", "'head_{}_entropy'", ".", "format", "(", "h_i", ")", ",", "ent", ".", "item", "(", ")", ",", "t_env", ")", "for", "h_i", ",", "ent", "in", "enumerate", "(", "head_entropies", ")", "]", "\n", "", "self", ".", "log_stats_t", "=", "t_env", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.qatten_learner.QattenLearner._update_targets": [[159, 164], ["qatten_learner.QattenLearner.target_mac.load_state", "qatten_learner.QattenLearner.logger.console_logger.info", "qatten_learner.QattenLearner.target_mixer.load_state_dict", "qatten_learner.QattenLearner.mixer.state_dict"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.load_state"], ["", "", "def", "_update_targets", "(", "self", ")", ":", "\n", "        ", "self", ".", "target_mac", ".", "load_state", "(", "self", ".", "mac", ")", "\n", "if", "self", ".", "mixer", "is", "not", "None", ":", "\n", "            ", "self", ".", "target_mixer", ".", "load_state_dict", "(", "self", ".", "mixer", ".", "state_dict", "(", ")", ")", "\n", "", "self", ".", "logger", ".", "console_logger", ".", "info", "(", "\"Updated target network\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.qatten_learner.QattenLearner.cuda": [[165, 171], ["qatten_learner.QattenLearner.mac.cuda", "qatten_learner.QattenLearner.target_mac.cuda", "qatten_learner.QattenLearner.mixer.cuda", "qatten_learner.QattenLearner.target_mixer.cuda"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda"], ["", "def", "cuda", "(", "self", ")", ":", "\n", "        ", "self", ".", "mac", ".", "cuda", "(", ")", "\n", "self", ".", "target_mac", ".", "cuda", "(", ")", "\n", "if", "self", ".", "mixer", "is", "not", "None", ":", "\n", "            ", "self", ".", "mixer", ".", "cuda", "(", ")", "\n", "self", ".", "target_mixer", ".", "cuda", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.qatten_learner.QattenLearner.save_models": [[172, 177], ["qatten_learner.QattenLearner.mac.save_models", "torch.save", "torch.save", "qatten_learner.QattenLearner.optimiser.state_dict", "qatten_learner.QattenLearner.mixer.state_dict"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.save_models"], ["", "", "def", "save_models", "(", "self", ",", "path", ")", ":", "\n", "        ", "self", ".", "mac", ".", "save_models", "(", "path", ")", "\n", "if", "self", ".", "mixer", "is", "not", "None", ":", "\n", "            ", "th", ".", "save", "(", "self", ".", "mixer", ".", "state_dict", "(", ")", ",", "\"{}/mixer.th\"", ".", "format", "(", "path", ")", ")", "\n", "", "th", ".", "save", "(", "self", ".", "optimiser", ".", "state_dict", "(", ")", ",", "\"{}/opt.th\"", ".", "format", "(", "path", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.qatten_learner.QattenLearner.load_models": [[178, 185], ["qatten_learner.QattenLearner.mac.load_models", "qatten_learner.QattenLearner.target_mac.load_models", "qatten_learner.QattenLearner.optimiser.load_state_dict", "qatten_learner.QattenLearner.mixer.load_state_dict", "torch.load", "torch.load"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.load_models", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.load_models"], ["", "def", "load_models", "(", "self", ",", "path", ")", ":", "\n", "        ", "self", ".", "mac", ".", "load_models", "(", "path", ")", "\n", "# Not quite right but I don't want to save target networks", "\n", "self", ".", "target_mac", ".", "load_models", "(", "path", ")", "\n", "if", "self", ".", "mixer", "is", "not", "None", ":", "\n", "            ", "self", ".", "mixer", ".", "load_state_dict", "(", "th", ".", "load", "(", "\"{}/mixer.th\"", ".", "format", "(", "path", ")", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", ")", "\n", "", "self", ".", "optimiser", ".", "load_state_dict", "(", "th", ".", "load", "(", "\"{}/opt.th\"", ".", "format", "(", "path", ")", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.q_learner.QLearner.__init__": [[10, 36], ["list", "torch.optim.RMSprop", "copy.deepcopy", "mac.parameters", "list", "copy.deepcopy", "modules.mixers.vdn.VDNMixer", "q_learner.QLearner.mixer.parameters", "modules.mixers.qmix.QMixer", "ValueError"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.parameters", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.parameters"], ["    ", "def", "__init__", "(", "self", ",", "mac", ",", "scheme", ",", "logger", ",", "args", ")", ":", "\n", "        ", "self", ".", "args", "=", "args", "\n", "self", ".", "mac", "=", "mac", "\n", "self", ".", "logger", "=", "logger", "\n", "\n", "self", ".", "params", "=", "list", "(", "mac", ".", "parameters", "(", ")", ")", "\n", "\n", "self", ".", "last_target_update_episode", "=", "0", "\n", "\n", "self", ".", "mixer", "=", "None", "\n", "if", "args", ".", "mixer", "is", "not", "None", ":", "\n", "            ", "if", "args", ".", "mixer", "==", "\"vdn\"", ":", "\n", "                ", "self", ".", "mixer", "=", "VDNMixer", "(", ")", "\n", "", "elif", "args", ".", "mixer", "==", "\"qmix\"", ":", "\n", "                ", "self", ".", "mixer", "=", "QMixer", "(", "args", ")", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"Mixer {} not recognised.\"", ".", "format", "(", "args", ".", "mixer", ")", ")", "\n", "", "self", ".", "params", "+=", "list", "(", "self", ".", "mixer", ".", "parameters", "(", ")", ")", "\n", "self", ".", "target_mixer", "=", "copy", ".", "deepcopy", "(", "self", ".", "mixer", ")", "\n", "\n", "", "self", ".", "optimiser", "=", "RMSprop", "(", "params", "=", "self", ".", "params", ",", "lr", "=", "args", ".", "lr", ",", "alpha", "=", "args", ".", "optim_alpha", ",", "eps", "=", "args", ".", "optim_eps", ")", "\n", "\n", "# a little wasteful to deepcopy (e.g. duplicates action selector), but should work for any MAC", "\n", "self", ".", "target_mac", "=", "copy", ".", "deepcopy", "(", "mac", ")", "\n", "\n", "self", ".", "log_stats_t", "=", "-", "self", ".", "args", ".", "learner_log_interval", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.q_learner.QLearner.train": [[37, 131], ["[].float", "[].float", "q_learner.QLearner.mac.init_hidden", "range", "torch.stack", "torch.gather().squeeze", "q_learner.QLearner.target_mac.init_hidden", "range", "torch.stack", "getattr", "mask.expand_as.expand_as.expand_as", "q_learner.QLearner.optimiser.zero_grad", "loss.backward", "torch.nn.utils.clip_grad_norm_", "q_learner.QLearner.optimiser.step", "q_learner.QLearner.mac.forward", "torch.stack.append", "q_learner.QLearner.target_mac.forward", "torch.stack.append", "torch.stack.clone().detach", "torch.gather().squeeze", "q_learner.QLearner.mixer", "q_learner.QLearner.target_mixer", "torch.zeros_like", "torch.tensor", "mask.expand_as.expand_as.flip().cumsum().flip().clamp_max().long", "range", "torch.linspace().unsqueeze().long", "torch.gather", "targets.detach", "mask.expand_as.expand_as.sum", "q_learner.QLearner._update_targets", "q_learner.QLearner.logger.log_stat", "q_learner.QLearner.logger.log_stat", "mask.expand_as.expand_as.sum().item", "q_learner.QLearner.logger.log_stat", "q_learner.QLearner.logger.log_stat", "q_learner.QLearner.logger.log_stat", "q_learner.QLearner.logger.log_stat", "torch.gather", "mac_out_detach[].max", "torch.stack.max", "loss.item", "torch.stack.clone", "torch.gather", "mask.expand_as.expand_as.flip().cumsum().flip().clamp_max", "torch.linspace().unsqueeze", "torch.pow", "mask.expand_as.expand_as.sum", "masked_td_error.abs().sum().item", "range", "mask.expand_as.flip().cumsum().flip().clamp_max().long.float", "mask.expand_as.expand_as.flip().cumsum().flip", "torch.linspace", "mask.expand_as.flip().cumsum().flip().clamp_max().long.long", "masked_td_error.abs().sum", "mask.expand_as.expand_as.flip().cumsum", "masked_td_error.abs", "torch.gather().squeeze", "mask.expand_as.expand_as.flip", "torch.gather"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.agents.central_rnn_agent.CentralRNNAgent.init_hidden", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.agents.central_rnn_agent.CentralRNNAgent.init_hidden", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.step", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.sqddpg.SQDDPGMixer.forward", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.sqddpg.SQDDPGMixer.forward", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.q_learner.QLearner._update_targets", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat"], ["", "def", "train", "(", "self", ",", "batch", ":", "EpisodeBatch", ",", "t_env", ":", "int", ",", "episode_num", ":", "int", ")", ":", "\n", "# Get the relevant quantities", "\n", "        ", "rewards", "=", "batch", "[", "\"reward\"", "]", "[", ":", ",", ":", "-", "1", "]", "\n", "actions", "=", "batch", "[", "\"actions\"", "]", "[", ":", ",", ":", "-", "1", "]", "\n", "terminated", "=", "batch", "[", "\"terminated\"", "]", "[", ":", ",", ":", "-", "1", "]", ".", "float", "(", ")", "\n", "mask", "=", "batch", "[", "\"filled\"", "]", "[", ":", ",", ":", "-", "1", "]", ".", "float", "(", ")", "\n", "mask", "[", ":", ",", "1", ":", "]", "=", "mask", "[", ":", ",", "1", ":", "]", "*", "(", "1", "-", "terminated", "[", ":", ",", ":", "-", "1", "]", ")", "\n", "avail_actions", "=", "batch", "[", "\"avail_actions\"", "]", "\n", "\n", "# Calculate estimated Q-Values", "\n", "mac_out", "=", "[", "]", "\n", "self", ".", "mac", ".", "init_hidden", "(", "batch", ".", "batch_size", ")", "\n", "for", "t", "in", "range", "(", "batch", ".", "max_seq_length", ")", ":", "\n", "            ", "agent_outs", "=", "self", ".", "mac", ".", "forward", "(", "batch", ",", "t", "=", "t", ")", "\n", "mac_out", ".", "append", "(", "agent_outs", ")", "\n", "", "mac_out", "=", "th", ".", "stack", "(", "mac_out", ",", "dim", "=", "1", ")", "# Concat over time", "\n", "\n", "# Pick the Q-Values for the actions taken by each agent", "\n", "chosen_action_qvals", "=", "th", ".", "gather", "(", "mac_out", "[", ":", ",", ":", "-", "1", "]", ",", "dim", "=", "3", ",", "index", "=", "actions", ")", ".", "squeeze", "(", "3", ")", "# Remove the last dim", "\n", "\n", "# Calculate the Q-Values necessary for the target", "\n", "target_mac_out", "=", "[", "]", "\n", "self", ".", "target_mac", ".", "init_hidden", "(", "batch", ".", "batch_size", ")", "\n", "for", "t", "in", "range", "(", "batch", ".", "max_seq_length", ")", ":", "\n", "            ", "target_agent_outs", "=", "self", ".", "target_mac", ".", "forward", "(", "batch", ",", "t", "=", "t", ")", "\n", "target_mac_out", ".", "append", "(", "target_agent_outs", ")", "\n", "\n", "# We don't need the first timesteps Q-Value estimate for calculating targets", "\n", "", "target_mac_out", "=", "th", ".", "stack", "(", "target_mac_out", "[", "1", ":", "]", ",", "dim", "=", "1", ")", "# Concat across time", "\n", "\n", "# Mask out unavailable actions", "\n", "target_mac_out", "[", "avail_actions", "[", ":", ",", "1", ":", "]", "==", "0", "]", "=", "-", "9999999", "# From OG deepmarl", "\n", "\n", "# Max over target Q-Values", "\n", "if", "self", ".", "args", ".", "double_q", ":", "\n", "# Get actions that maximise live Q (for double q-learning)", "\n", "            ", "mac_out_detach", "=", "mac_out", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "mac_out_detach", "[", "avail_actions", "==", "0", "]", "=", "-", "9999999", "\n", "cur_max_actions", "=", "mac_out_detach", "[", ":", ",", "1", ":", "]", ".", "max", "(", "dim", "=", "3", ",", "keepdim", "=", "True", ")", "[", "1", "]", "\n", "target_max_qvals", "=", "th", ".", "gather", "(", "target_mac_out", ",", "3", ",", "cur_max_actions", ")", ".", "squeeze", "(", "3", ")", "\n", "", "else", ":", "\n", "            ", "target_max_qvals", "=", "target_mac_out", ".", "max", "(", "dim", "=", "3", ")", "[", "0", "]", "\n", "\n", "# Mix", "\n", "", "if", "self", ".", "mixer", "is", "not", "None", ":", "\n", "            ", "chosen_action_qvals", "=", "self", ".", "mixer", "(", "chosen_action_qvals", ",", "batch", "[", "\"state\"", "]", "[", ":", ",", ":", "-", "1", "]", ")", "\n", "target_max_qvals", "=", "self", ".", "target_mixer", "(", "target_max_qvals", ",", "batch", "[", "\"state\"", "]", "[", ":", ",", "1", ":", "]", ")", "\n", "\n", "", "N", "=", "getattr", "(", "self", ".", "args", ",", "\"n_step\"", ",", "1", ")", "\n", "if", "N", "==", "1", ":", "\n", "# Calculate 1-step Q-Learning targets", "\n", "            ", "targets", "=", "rewards", "+", "self", ".", "args", ".", "gamma", "*", "(", "1", "-", "terminated", ")", "*", "target_max_qvals", "\n", "", "else", ":", "\n", "# N step Q-Learning targets", "\n", "            ", "n_rewards", "=", "th", ".", "zeros_like", "(", "rewards", ")", "\n", "gamma_tensor", "=", "th", ".", "tensor", "(", "[", "self", ".", "args", ".", "gamma", "**", "i", "for", "i", "in", "range", "(", "N", ")", "]", ",", "dtype", "=", "th", ".", "float", ",", "device", "=", "n_rewards", ".", "device", ")", "\n", "steps", "=", "mask", ".", "flip", "(", "1", ")", ".", "cumsum", "(", "dim", "=", "1", ")", ".", "flip", "(", "1", ")", ".", "clamp_max", "(", "N", ")", ".", "long", "(", ")", "\n", "for", "i", "in", "range", "(", "batch", ".", "max_seq_length", "-", "1", ")", ":", "\n", "                ", "n_rewards", "[", ":", ",", "i", ",", "0", "]", "=", "(", "(", "rewards", "*", "mask", ")", "[", ":", ",", "i", ":", "i", "+", "N", ",", "0", "]", "*", "gamma_tensor", "[", ":", "(", "batch", ".", "max_seq_length", "-", "1", "-", "i", ")", "]", ")", ".", "sum", "(", "dim", "=", "1", ")", "\n", "", "indices", "=", "th", ".", "linspace", "(", "0", ",", "batch", ".", "max_seq_length", "-", "2", ",", "steps", "=", "batch", ".", "max_seq_length", "-", "1", ",", "device", "=", "steps", ".", "device", ")", ".", "unsqueeze", "(", "1", ")", ".", "long", "(", ")", "\n", "n_targets_terminated", "=", "th", ".", "gather", "(", "target_max_qvals", "*", "(", "1", "-", "terminated", ")", ",", "dim", "=", "1", ",", "index", "=", "steps", ".", "long", "(", ")", "+", "indices", "-", "1", ")", "\n", "targets", "=", "n_rewards", "+", "th", ".", "pow", "(", "self", ".", "args", ".", "gamma", ",", "steps", ".", "float", "(", ")", ")", "*", "n_targets_terminated", "\n", "\n", "# Td-error", "\n", "", "td_error", "=", "(", "chosen_action_qvals", "-", "targets", ".", "detach", "(", ")", ")", "\n", "\n", "mask", "=", "mask", ".", "expand_as", "(", "td_error", ")", "\n", "\n", "# 0-out the targets that came from padded data", "\n", "masked_td_error", "=", "td_error", "*", "mask", "\n", "\n", "# Normal L2 loss, take mean over actual data", "\n", "loss", "=", "(", "masked_td_error", "**", "2", ")", ".", "sum", "(", ")", "/", "mask", ".", "sum", "(", ")", "\n", "\n", "# Optimise", "\n", "self", ".", "optimiser", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "grad_norm", "=", "th", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "params", ",", "self", ".", "args", ".", "grad_norm_clip", ")", "\n", "self", ".", "optimiser", ".", "step", "(", ")", "\n", "\n", "if", "(", "episode_num", "-", "self", ".", "last_target_update_episode", ")", "/", "self", ".", "args", ".", "target_update_interval", ">=", "1.0", ":", "\n", "            ", "self", ".", "_update_targets", "(", ")", "\n", "self", ".", "last_target_update_episode", "=", "episode_num", "\n", "\n", "", "if", "t_env", "-", "self", ".", "log_stats_t", ">=", "self", ".", "args", ".", "learner_log_interval", ":", "\n", "            ", "self", ".", "logger", ".", "log_stat", "(", "\"loss\"", ",", "loss", ".", "item", "(", ")", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"grad_norm\"", ",", "grad_norm", ",", "t_env", ")", "\n", "mask_elems", "=", "mask", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"td_error_abs\"", ",", "(", "masked_td_error", ".", "abs", "(", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "/", "mask_elems", ")", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"q_taken_mean\"", ",", "(", "chosen_action_qvals", "*", "mask", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "/", "(", "mask_elems", "*", "self", ".", "args", ".", "n_agents", ")", ",", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"target_mean\"", ",", "(", "targets", "*", "mask", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "/", "(", "mask_elems", "*", "self", ".", "args", ".", "n_agents", ")", ",", "t_env", ")", "\n", "agent_utils", "=", "(", "th", ".", "gather", "(", "mac_out", "[", ":", ",", ":", "-", "1", "]", ",", "dim", "=", "3", ",", "index", "=", "actions", ")", ".", "squeeze", "(", "3", ")", "*", "mask", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "/", "(", "mask_elems", "*", "self", ".", "args", ".", "n_agents", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "\"agent_utils\"", ",", "agent_utils", ",", "t_env", ")", "\n", "self", ".", "log_stats_t", "=", "t_env", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.q_learner.QLearner._update_targets": [[132, 137], ["q_learner.QLearner.target_mac.load_state", "q_learner.QLearner.logger.console_logger.info", "q_learner.QLearner.target_mixer.load_state_dict", "q_learner.QLearner.mixer.state_dict"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.load_state"], ["", "", "def", "_update_targets", "(", "self", ")", ":", "\n", "        ", "self", ".", "target_mac", ".", "load_state", "(", "self", ".", "mac", ")", "\n", "if", "self", ".", "mixer", "is", "not", "None", ":", "\n", "            ", "self", ".", "target_mixer", ".", "load_state_dict", "(", "self", ".", "mixer", ".", "state_dict", "(", ")", ")", "\n", "", "self", ".", "logger", ".", "console_logger", ".", "info", "(", "\"Updated target network\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.q_learner.QLearner.cuda": [[138, 144], ["q_learner.QLearner.mac.cuda", "q_learner.QLearner.target_mac.cuda", "q_learner.QLearner.mixer.cuda", "q_learner.QLearner.target_mixer.cuda"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda"], ["", "def", "cuda", "(", "self", ")", ":", "\n", "        ", "self", ".", "mac", ".", "cuda", "(", ")", "\n", "self", ".", "target_mac", ".", "cuda", "(", ")", "\n", "if", "self", ".", "mixer", "is", "not", "None", ":", "\n", "            ", "self", ".", "mixer", ".", "cuda", "(", ")", "\n", "self", ".", "target_mixer", ".", "cuda", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.q_learner.QLearner.save_models": [[145, 150], ["q_learner.QLearner.mac.save_models", "torch.save", "torch.save", "q_learner.QLearner.optimiser.state_dict", "q_learner.QLearner.mixer.state_dict"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.save_models"], ["", "", "def", "save_models", "(", "self", ",", "path", ")", ":", "\n", "        ", "self", ".", "mac", ".", "save_models", "(", "path", ")", "\n", "if", "self", ".", "mixer", "is", "not", "None", ":", "\n", "            ", "th", ".", "save", "(", "self", ".", "mixer", ".", "state_dict", "(", ")", ",", "\"{}/mixer.th\"", ".", "format", "(", "path", ")", ")", "\n", "", "th", ".", "save", "(", "self", ".", "optimiser", ".", "state_dict", "(", ")", ",", "\"{}/opt.th\"", ".", "format", "(", "path", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.learners.q_learner.QLearner.load_models": [[151, 158], ["q_learner.QLearner.mac.load_models", "q_learner.QLearner.target_mac.load_models", "q_learner.QLearner.optimiser.load_state_dict", "q_learner.QLearner.mixer.load_state_dict", "torch.load", "torch.load"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.load_models", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.load_models"], ["", "def", "load_models", "(", "self", ",", "path", ")", ":", "\n", "        ", "self", ".", "mac", ".", "load_models", "(", "path", ")", "\n", "# Not quite right but I don't want to save target networks", "\n", "self", ".", "target_mac", ".", "load_models", "(", "path", ")", "\n", "if", "self", ".", "mixer", "is", "not", "None", ":", "\n", "            ", "self", ".", "mixer", ".", "load_state_dict", "(", "th", ".", "load", "(", "\"{}/mixer.th\"", ".", "format", "(", "path", ")", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", ")", "\n", "", "self", ".", "optimiser", ".", "load_state_dict", "(", "th", ".", "load", "(", "\"{}/opt.th\"", ".", "format", "(", "path", ")", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller.BasicMAC.__init__": [[8, 18], ["basic_controller.BasicMAC._get_input_shape", "basic_controller.BasicMAC._build_agents"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.critics.coma.COMACritic._get_input_shape", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC._build_agents"], ["    ", "def", "__init__", "(", "self", ",", "scheme", ",", "groups", ",", "args", ")", ":", "\n", "        ", "self", ".", "n_agents", "=", "args", ".", "n_agents", "\n", "self", ".", "args", "=", "args", "\n", "input_shape", "=", "self", ".", "_get_input_shape", "(", "scheme", ")", "\n", "self", ".", "_build_agents", "(", "input_shape", ")", "\n", "self", ".", "agent_output_type", "=", "args", ".", "agent_output_type", "\n", "\n", "self", ".", "action_selector", "=", "action_REGISTRY", "[", "args", ".", "action_selector", "]", "(", "args", ")", "\n", "\n", "self", ".", "hidden_states", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller.BasicMAC.select_actions": [[19, 25], ["slice", "basic_controller.BasicMAC.forward", "basic_controller.BasicMAC.action_selector.select_action"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.sqddpg.SQDDPGMixer.forward", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.action_selectors.PolicyEpsilonGreedyActionSelector.select_action"], ["", "def", "select_actions", "(", "self", ",", "ep_batch", ",", "t_ep", ",", "t_env", ",", "bs", "=", "slice", "(", "None", ")", ",", "test_mode", "=", "False", ")", ":", "\n", "# Only select actions for the selected batch elements in bs", "\n", "        ", "avail_actions", "=", "ep_batch", "[", "\"avail_actions\"", "]", "[", ":", ",", "t_ep", "]", "\n", "agent_outputs", "=", "self", ".", "forward", "(", "ep_batch", ",", "t_ep", ",", "test_mode", "=", "test_mode", ")", "\n", "chosen_actions", "=", "self", ".", "action_selector", ".", "select_action", "(", "agent_outputs", "[", "bs", "]", ",", "avail_actions", "[", "bs", "]", ",", "t_env", ",", "test_mode", "=", "test_mode", ")", "\n", "return", "chosen_actions", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller.BasicMAC.forward": [[26, 54], ["basic_controller.BasicMAC._build_inputs", "basic_controller.BasicMAC.agent", "torch.nn.functional.softmax.view", "getattr", "torch.nn.functional.softmax", "avail_actions.reshape", "torch.nn.functional.softmax.size", "getattr", "getattr", "avail_actions.reshape.sum().float", "avail_actions.reshape.sum", "torch.ones_like"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.critics.coma.COMACritic._build_inputs"], ["", "def", "forward", "(", "self", ",", "ep_batch", ",", "t", ",", "test_mode", "=", "False", ")", ":", "\n", "        ", "agent_inputs", "=", "self", ".", "_build_inputs", "(", "ep_batch", ",", "t", ")", "\n", "avail_actions", "=", "ep_batch", "[", "\"avail_actions\"", "]", "[", ":", ",", "t", "]", "\n", "agent_outs", ",", "self", ".", "hidden_states", "=", "self", ".", "agent", "(", "agent_inputs", ",", "self", ".", "hidden_states", ")", "\n", "\n", "if", "self", ".", "agent_output_type", "==", "\"pi_logits\"", ":", "\n", "\n", "            ", "if", "getattr", "(", "self", ".", "args", ",", "\"mask_before_softmax\"", ",", "True", ")", ":", "\n", "# Make the logits for unavailable actions very negative to minimise their affect on the softmax", "\n", "                ", "reshaped_avail_actions", "=", "avail_actions", ".", "reshape", "(", "ep_batch", ".", "batch_size", "*", "self", ".", "n_agents", ",", "-", "1", ")", "\n", "agent_outs", "[", "reshaped_avail_actions", "==", "0", "]", "=", "-", "1e10", "\n", "\n", "", "agent_outs", "=", "th", ".", "nn", ".", "functional", ".", "softmax", "(", "agent_outs", ",", "dim", "=", "-", "1", ")", "\n", "if", "not", "test_mode", ":", "\n", "# Epsilon floor", "\n", "                ", "epsilon_action_num", "=", "agent_outs", ".", "size", "(", "-", "1", ")", "\n", "if", "getattr", "(", "self", ".", "args", ",", "\"mask_before_softmax\"", ",", "True", ")", ":", "\n", "# With probability epsilon, we will pick an available action uniformly", "\n", "                    ", "epsilon_action_num", "=", "reshaped_avail_actions", ".", "sum", "(", "dim", "=", "1", ",", "keepdim", "=", "True", ")", ".", "float", "(", ")", "\n", "\n", "", "agent_outs", "=", "(", "(", "1", "-", "self", ".", "action_selector", ".", "epsilon", ")", "*", "agent_outs", "\n", "+", "th", ".", "ones_like", "(", "agent_outs", ")", "*", "self", ".", "action_selector", ".", "epsilon", "/", "epsilon_action_num", ")", "\n", "\n", "if", "getattr", "(", "self", ".", "args", ",", "\"mask_before_softmax\"", ",", "True", ")", ":", "\n", "# Zero out the unavailable actions", "\n", "                    ", "agent_outs", "[", "reshaped_avail_actions", "==", "0", "]", "=", "0.0", "\n", "\n", "", "", "", "return", "agent_outs", ".", "view", "(", "ep_batch", ".", "batch_size", ",", "self", ".", "n_agents", ",", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller.BasicMAC.init_hidden": [[55, 57], ["basic_controller.BasicMAC.agent.init_hidden().unsqueeze().expand", "basic_controller.BasicMAC.agent.init_hidden().unsqueeze", "basic_controller.BasicMAC.agent.init_hidden"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.agents.central_rnn_agent.CentralRNNAgent.init_hidden"], ["", "def", "init_hidden", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "self", ".", "hidden_states", "=", "self", ".", "agent", ".", "init_hidden", "(", ")", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "batch_size", ",", "self", ".", "n_agents", ",", "-", "1", ")", "# bav", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller.BasicMAC.parameters": [[58, 60], ["basic_controller.BasicMAC.agent.parameters"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.parameters"], ["", "def", "parameters", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "agent", ".", "parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller.BasicMAC.load_state": [[61, 63], ["basic_controller.BasicMAC.agent.load_state_dict", "other_mac.agent.state_dict"], "methods", ["None"], ["", "def", "load_state", "(", "self", ",", "other_mac", ")", ":", "\n", "        ", "self", ".", "agent", ".", "load_state_dict", "(", "other_mac", ".", "agent", ".", "state_dict", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller.BasicMAC.cuda": [[64, 66], ["basic_controller.BasicMAC.agent.cuda"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda"], ["", "def", "cuda", "(", "self", ")", ":", "\n", "        ", "self", ".", "agent", ".", "cuda", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller.BasicMAC.save_models": [[67, 69], ["torch.save", "basic_controller.BasicMAC.agent.state_dict"], "methods", ["None"], ["", "def", "save_models", "(", "self", ",", "path", ")", ":", "\n", "        ", "th", ".", "save", "(", "self", ".", "agent", ".", "state_dict", "(", ")", ",", "\"{}/agent.th\"", ".", "format", "(", "path", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller.BasicMAC.load_models": [[70, 72], ["basic_controller.BasicMAC.agent.load_state_dict", "torch.load"], "methods", ["None"], ["", "def", "load_models", "(", "self", ",", "path", ")", ":", "\n", "        ", "self", ".", "agent", ".", "load_state_dict", "(", "th", ".", "load", "(", "\"{}/agent.th\"", ".", "format", "(", "path", ")", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller.BasicMAC._build_agents": [[73, 75], ["None"], "methods", ["None"], ["", "def", "_build_agents", "(", "self", ",", "input_shape", ")", ":", "\n", "        ", "self", ".", "agent", "=", "agent_REGISTRY", "[", "self", ".", "args", ".", "agent", "]", "(", "input_shape", ",", "self", ".", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller.BasicMAC._build_inputs": [[76, 92], ["torch.cat.append", "torch.cat", "torch.cat.append", "torch.cat.append", "torch.cat.append", "torch.eye().unsqueeze().expand", "x.reshape", "torch.zeros_like", "torch.eye().unsqueeze", "torch.eye"], "methods", ["None"], ["", "def", "_build_inputs", "(", "self", ",", "batch", ",", "t", ")", ":", "\n", "# Assumes homogenous agents with flat observations.", "\n", "# Other MACs might want to e.g. delegate building inputs to each agent", "\n", "        ", "bs", "=", "batch", ".", "batch_size", "\n", "inputs", "=", "[", "]", "\n", "inputs", ".", "append", "(", "batch", "[", "\"obs\"", "]", "[", ":", ",", "t", "]", ")", "# b1av", "\n", "if", "self", ".", "args", ".", "obs_last_action", ":", "\n", "            ", "if", "t", "==", "0", ":", "\n", "                ", "inputs", ".", "append", "(", "th", ".", "zeros_like", "(", "batch", "[", "\"actions_onehot\"", "]", "[", ":", ",", "t", "]", ")", ")", "\n", "", "else", ":", "\n", "                ", "inputs", ".", "append", "(", "batch", "[", "\"actions_onehot\"", "]", "[", ":", ",", "t", "-", "1", "]", ")", "\n", "", "", "if", "self", ".", "args", ".", "obs_agent_id", ":", "\n", "            ", "inputs", ".", "append", "(", "th", ".", "eye", "(", "self", ".", "n_agents", ",", "device", "=", "batch", ".", "device", ")", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "bs", ",", "-", "1", ",", "-", "1", ")", ")", "\n", "\n", "", "inputs", "=", "th", ".", "cat", "(", "[", "x", ".", "reshape", "(", "bs", "*", "self", ".", "n_agents", ",", "-", "1", ")", "for", "x", "in", "inputs", "]", ",", "dim", "=", "1", ")", "\n", "return", "inputs", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller.BasicMAC._get_input_shape": [[93, 101], ["None"], "methods", ["None"], ["", "def", "_get_input_shape", "(", "self", ",", "scheme", ")", ":", "\n", "        ", "input_shape", "=", "scheme", "[", "\"obs\"", "]", "[", "\"vshape\"", "]", "\n", "if", "self", ".", "args", ".", "obs_last_action", ":", "\n", "            ", "input_shape", "+=", "scheme", "[", "\"actions_onehot\"", "]", "[", "\"vshape\"", "]", "[", "0", "]", "\n", "", "if", "self", ".", "args", ".", "obs_agent_id", ":", "\n", "            ", "input_shape", "+=", "self", ".", "n_agents", "\n", "\n", "", "return", "input_shape", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.central_basic_controller.CentralBasicMAC.__init__": [[8, 16], ["central_basic_controller.CentralBasicMAC._get_input_shape", "central_basic_controller.CentralBasicMAC._build_agents"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.critics.coma.COMACritic._get_input_shape", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC._build_agents"], ["    ", "def", "__init__", "(", "self", ",", "scheme", ",", "args", ")", ":", "\n", "        ", "self", ".", "n_agents", "=", "args", ".", "n_agents", "\n", "self", ".", "args", "=", "args", "\n", "input_shape", "=", "self", ".", "_get_input_shape", "(", "scheme", ")", "\n", "self", ".", "_build_agents", "(", "input_shape", ")", "\n", "self", ".", "agent_output_type", "=", "args", ".", "agent_output_type", "\n", "\n", "self", ".", "hidden_states", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.central_basic_controller.CentralBasicMAC.forward": [[17, 22], ["central_basic_controller.CentralBasicMAC._build_inputs", "central_basic_controller.CentralBasicMAC.agent", "agent_outs.view"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.critics.coma.COMACritic._build_inputs"], ["", "def", "forward", "(", "self", ",", "ep_batch", ",", "t", ",", "test_mode", "=", "False", ")", ":", "\n", "        ", "agent_inputs", "=", "self", ".", "_build_inputs", "(", "ep_batch", ",", "t", ")", "\n", "agent_outs", ",", "self", ".", "hidden_states", "=", "self", ".", "agent", "(", "agent_inputs", ",", "self", ".", "hidden_states", ")", "\n", "\n", "return", "agent_outs", ".", "view", "(", "ep_batch", ".", "batch_size", ",", "self", ".", "n_agents", ",", "self", ".", "args", ".", "n_actions", ",", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.central_basic_controller.CentralBasicMAC.init_hidden": [[23, 25], ["central_basic_controller.CentralBasicMAC.agent.init_hidden().unsqueeze().expand", "central_basic_controller.CentralBasicMAC.agent.init_hidden().unsqueeze", "central_basic_controller.CentralBasicMAC.agent.init_hidden"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.agents.central_rnn_agent.CentralRNNAgent.init_hidden"], ["", "def", "init_hidden", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "self", ".", "hidden_states", "=", "self", ".", "agent", ".", "init_hidden", "(", ")", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "batch_size", ",", "self", ".", "n_agents", ",", "-", "1", ")", "# bav", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.central_basic_controller.CentralBasicMAC.parameters": [[26, 28], ["central_basic_controller.CentralBasicMAC.agent.parameters"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.parameters"], ["", "def", "parameters", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "agent", ".", "parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.central_basic_controller.CentralBasicMAC.load_state": [[29, 31], ["central_basic_controller.CentralBasicMAC.agent.load_state_dict", "other_mac.agent.state_dict"], "methods", ["None"], ["", "def", "load_state", "(", "self", ",", "other_mac", ")", ":", "\n", "        ", "self", ".", "agent", ".", "load_state_dict", "(", "other_mac", ".", "agent", ".", "state_dict", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.central_basic_controller.CentralBasicMAC.cuda": [[32, 34], ["central_basic_controller.CentralBasicMAC.agent.cuda"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda"], ["", "def", "cuda", "(", "self", ")", ":", "\n", "        ", "self", ".", "agent", ".", "cuda", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.central_basic_controller.CentralBasicMAC.save_models": [[35, 37], ["torch.save", "central_basic_controller.CentralBasicMAC.agent.state_dict"], "methods", ["None"], ["", "def", "save_models", "(", "self", ",", "path", ")", ":", "\n", "        ", "th", ".", "save", "(", "self", ".", "agent", ".", "state_dict", "(", ")", ",", "\"{}/central_agent.th\"", ".", "format", "(", "path", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.central_basic_controller.CentralBasicMAC.load_models": [[38, 40], ["central_basic_controller.CentralBasicMAC.agent.load_state_dict", "torch.load"], "methods", ["None"], ["", "def", "load_models", "(", "self", ",", "path", ")", ":", "\n", "        ", "self", ".", "agent", ".", "load_state_dict", "(", "th", ".", "load", "(", "\"{}/central_agent.th\"", ".", "format", "(", "path", ")", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.central_basic_controller.CentralBasicMAC._build_agents": [[41, 43], ["None"], "methods", ["None"], ["", "def", "_build_agents", "(", "self", ",", "input_shape", ")", ":", "\n", "        ", "self", ".", "agent", "=", "agent_REGISTRY", "[", "self", ".", "args", ".", "central_agent", "]", "(", "input_shape", ",", "self", ".", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.central_basic_controller.CentralBasicMAC._build_inputs": [[44, 63], ["torch.cat.append", "torch.cat", "[].unsqueeze().repeat", "torch.cat.append", "torch.cat.append", "torch.cat.append", "torch.eye().unsqueeze().expand", "x.reshape", "[].unsqueeze", "torch.zeros_like", "torch.eye().unsqueeze", "torch.eye"], "methods", ["None"], ["", "def", "_build_inputs", "(", "self", ",", "batch", ",", "t", ")", ":", "\n", "# Assumes homogenous agents with flat observations.", "\n", "# Other MACs might want to e.g. delegate building inputs to each agent", "\n", "        ", "bs", "=", "batch", ".", "batch_size", "\n", "inputs", "=", "[", "]", "\n", "inputs", ".", "append", "(", "batch", "[", "\"obs\"", "]", "[", ":", ",", "t", "]", ")", "# b1av", "\n", "if", "self", ".", "args", ".", "central_agent", "==", "\"central_rnn_big\"", ":", "\n", "            ", "inputs", "[", "0", "]", "=", "(", "batch", "[", "\"state\"", "]", "[", ":", ",", "t", "]", ".", "unsqueeze", "(", "1", ")", ".", "repeat", "(", "1", ",", "self", ".", "args", ".", "n_agents", ",", "1", ")", ")", "# b1av", "\n", "# inputs.append(batch[\"state\"][:,t].unsqueeze(1).repeat(1,self.args.n_agents,1))  # b1av", "\n", "", "if", "self", ".", "args", ".", "obs_last_action", ":", "\n", "            ", "if", "t", "==", "0", ":", "\n", "                ", "inputs", ".", "append", "(", "th", ".", "zeros_like", "(", "batch", "[", "\"actions_onehot\"", "]", "[", ":", ",", "t", "]", ")", ")", "\n", "", "else", ":", "\n", "                ", "inputs", ".", "append", "(", "batch", "[", "\"actions_onehot\"", "]", "[", ":", ",", "t", "-", "1", "]", ")", "\n", "", "", "if", "self", ".", "args", ".", "obs_agent_id", ":", "\n", "            ", "inputs", ".", "append", "(", "th", ".", "eye", "(", "self", ".", "n_agents", ",", "device", "=", "batch", ".", "device", ")", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "bs", ",", "-", "1", ",", "-", "1", ")", ")", "\n", "\n", "", "inputs", "=", "th", ".", "cat", "(", "[", "x", ".", "reshape", "(", "bs", "*", "self", ".", "n_agents", ",", "-", "1", ")", "for", "x", "in", "inputs", "]", ",", "dim", "=", "1", ")", "\n", "return", "inputs", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.central_basic_controller.CentralBasicMAC._get_input_shape": [[64, 75], ["None"], "methods", ["None"], ["", "def", "_get_input_shape", "(", "self", ",", "scheme", ")", ":", "\n", "        ", "input_shape", "=", "scheme", "[", "\"obs\"", "]", "[", "\"vshape\"", "]", "\n", "if", "self", ".", "args", ".", "central_agent", "==", "\"central_rnn_big\"", ":", "\n", "            ", "input_shape", "+=", "scheme", "[", "\"state\"", "]", "[", "\"vshape\"", "]", "\n", "input_shape", "-=", "scheme", "[", "\"obs\"", "]", "[", "\"vshape\"", "]", "\n", "", "if", "self", ".", "args", ".", "obs_last_action", ":", "\n", "            ", "input_shape", "+=", "scheme", "[", "\"actions_onehot\"", "]", "[", "\"vshape\"", "]", "[", "0", "]", "\n", "", "if", "self", ".", "args", ".", "obs_agent_id", ":", "\n", "            ", "input_shape", "+=", "self", ".", "n_agents", "\n", "\n", "", "return", "input_shape", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.__init__": [[8, 19], ["basic_controller_policy.BasicMAC._get_input_shape", "basic_controller_policy.BasicMAC._build_agents"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.critics.coma.COMACritic._get_input_shape", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC._build_agents"], ["    ", "def", "__init__", "(", "self", ",", "scheme", ",", "groups", ",", "args", ")", ":", "\n", "        ", "self", ".", "n_agents", "=", "args", ".", "n_agents", "\n", "self", ".", "args", "=", "args", "\n", "input_shape", "=", "self", ".", "_get_input_shape", "(", "scheme", ")", "\n", "self", ".", "_build_agents", "(", "input_shape", ")", "\n", "self", ".", "agent_output_type", "=", "args", ".", "agent_output_type", "\n", "\n", "assert", "args", ".", "action_selector", "==", "\"policy_epsilon_greedy\"", "\n", "self", ".", "action_selector", "=", "action_REGISTRY", "[", "args", ".", "action_selector", "]", "(", "args", ")", "\n", "\n", "self", ".", "hidden_states", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.select_actions": [[20, 26], ["slice", "basic_controller_policy.BasicMAC.forward", "basic_controller_policy.BasicMAC.action_selector.select_action"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.sqddpg.SQDDPGMixer.forward", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.action_selectors.PolicyEpsilonGreedyActionSelector.select_action"], ["", "def", "select_actions", "(", "self", ",", "ep_batch", ",", "t_ep", ",", "t_env", ",", "bs", "=", "slice", "(", "None", ")", ",", "test_mode", "=", "False", ")", ":", "\n", "# Only select actions for the selected batch elements in bs", "\n", "        ", "avail_actions", "=", "ep_batch", "[", "\"avail_actions\"", "]", "[", ":", ",", "t_ep", "]", "\n", "agent_qs", ",", "agent_pis", "=", "self", ".", "forward", "(", "ep_batch", ",", "t_ep", ",", "test_mode", "=", "test_mode", ")", "\n", "chosen_actions", "=", "self", ".", "action_selector", ".", "select_action", "(", "agent_qs", "[", "bs", "]", ",", "agent_pis", "[", "bs", "]", ",", "avail_actions", "[", "bs", "]", ",", "t_env", ",", "test_mode", "=", "test_mode", ")", "\n", "return", "chosen_actions", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.forward": [[27, 32], ["basic_controller_policy.BasicMAC._build_inputs", "basic_controller_policy.BasicMAC.agent", "agent_q.view", "agent_policy.view"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.critics.coma.COMACritic._build_inputs"], ["", "def", "forward", "(", "self", ",", "ep_batch", ",", "t", ",", "test_mode", "=", "False", ")", ":", "\n", "        ", "agent_inputs", "=", "self", ".", "_build_inputs", "(", "ep_batch", ",", "t", ")", "\n", "agent_q", ",", "agent_policy", ",", "self", ".", "hidden_states", "=", "self", ".", "agent", "(", "agent_inputs", ",", "self", ".", "hidden_states", ")", "\n", "\n", "return", "agent_q", ".", "view", "(", "ep_batch", ".", "batch_size", ",", "self", ".", "n_agents", ",", "-", "1", ")", ",", "agent_policy", ".", "view", "(", "ep_batch", ".", "batch_size", ",", "self", ".", "n_agents", ",", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.init_hidden": [[33, 35], ["basic_controller_policy.BasicMAC.agent.init_hidden().unsqueeze().expand", "basic_controller_policy.BasicMAC.agent.init_hidden().unsqueeze", "basic_controller_policy.BasicMAC.agent.init_hidden"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.agents.central_rnn_agent.CentralRNNAgent.init_hidden"], ["", "def", "init_hidden", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "self", ".", "hidden_states", "=", "self", ".", "agent", ".", "init_hidden", "(", ")", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "batch_size", ",", "self", ".", "n_agents", ",", "-", "1", ")", "# bav", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.parameters": [[36, 38], ["basic_controller_policy.BasicMAC.agent.parameters"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.parameters"], ["", "def", "parameters", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "agent", ".", "parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.load_state": [[39, 41], ["basic_controller_policy.BasicMAC.agent.load_state_dict", "other_mac.agent.state_dict"], "methods", ["None"], ["", "def", "load_state", "(", "self", ",", "other_mac", ")", ":", "\n", "        ", "self", ".", "agent", ".", "load_state_dict", "(", "other_mac", ".", "agent", ".", "state_dict", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda": [[42, 44], ["basic_controller_policy.BasicMAC.agent.cuda"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda"], ["", "def", "cuda", "(", "self", ")", ":", "\n", "        ", "self", ".", "agent", ".", "cuda", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.save_models": [[45, 47], ["torch.save", "basic_controller_policy.BasicMAC.agent.state_dict"], "methods", ["None"], ["", "def", "save_models", "(", "self", ",", "path", ")", ":", "\n", "        ", "th", ".", "save", "(", "self", ".", "agent", ".", "state_dict", "(", ")", ",", "\"{}/agent.th\"", ".", "format", "(", "path", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.load_models": [[48, 50], ["basic_controller_policy.BasicMAC.agent.load_state_dict", "torch.load"], "methods", ["None"], ["", "def", "load_models", "(", "self", ",", "path", ")", ":", "\n", "        ", "self", ".", "agent", ".", "load_state_dict", "(", "th", ".", "load", "(", "\"{}/agent.th\"", ".", "format", "(", "path", ")", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC._build_agents": [[51, 53], ["None"], "methods", ["None"], ["", "def", "_build_agents", "(", "self", ",", "input_shape", ")", ":", "\n", "        ", "self", ".", "agent", "=", "agent_REGISTRY", "[", "self", ".", "args", ".", "agent", "]", "(", "input_shape", ",", "self", ".", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC._build_inputs": [[54, 70], ["torch.cat.append", "torch.cat", "torch.cat.append", "torch.cat.append", "torch.cat.append", "torch.eye().unsqueeze().expand", "x.reshape", "torch.zeros_like", "torch.eye().unsqueeze", "torch.eye"], "methods", ["None"], ["", "def", "_build_inputs", "(", "self", ",", "batch", ",", "t", ")", ":", "\n", "# Assumes homogenous agents with flat observations.", "\n", "# Other MACs might want to e.g. delegate building inputs to each agent", "\n", "        ", "bs", "=", "batch", ".", "batch_size", "\n", "inputs", "=", "[", "]", "\n", "inputs", ".", "append", "(", "batch", "[", "\"obs\"", "]", "[", ":", ",", "t", "]", ")", "# b1av", "\n", "if", "self", ".", "args", ".", "obs_last_action", ":", "\n", "            ", "if", "t", "==", "0", ":", "\n", "                ", "inputs", ".", "append", "(", "th", ".", "zeros_like", "(", "batch", "[", "\"actions_onehot\"", "]", "[", ":", ",", "t", "]", ")", ")", "\n", "", "else", ":", "\n", "                ", "inputs", ".", "append", "(", "batch", "[", "\"actions_onehot\"", "]", "[", ":", ",", "t", "-", "1", "]", ")", "\n", "", "", "if", "self", ".", "args", ".", "obs_agent_id", ":", "\n", "            ", "inputs", ".", "append", "(", "th", ".", "eye", "(", "self", ".", "n_agents", ",", "device", "=", "batch", ".", "device", ")", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "bs", ",", "-", "1", ",", "-", "1", ")", ")", "\n", "\n", "", "inputs", "=", "th", ".", "cat", "(", "[", "x", ".", "reshape", "(", "bs", "*", "self", ".", "n_agents", ",", "-", "1", ")", "for", "x", "in", "inputs", "]", ",", "dim", "=", "1", ")", "\n", "return", "inputs", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC._get_input_shape": [[71, 79], ["None"], "methods", ["None"], ["", "def", "_get_input_shape", "(", "self", ",", "scheme", ")", ":", "\n", "        ", "input_shape", "=", "scheme", "[", "\"obs\"", "]", "[", "\"vshape\"", "]", "\n", "if", "self", ".", "args", ".", "obs_last_action", ":", "\n", "            ", "input_shape", "+=", "scheme", "[", "\"actions_onehot\"", "]", "[", "\"vshape\"", "]", "[", "0", "]", "\n", "", "if", "self", ".", "args", ".", "obs_agent_id", ":", "\n", "            ", "input_shape", "+=", "self", ".", "n_agents", "\n", "\n", "", "return", "input_shape", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.agents.rnn_agent.RNNAgent.__init__": [[6, 13], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.GRUCell", "torch.GRUCell", "torch.GRUCell", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input_shape", ",", "args", ")", ":", "\n", "        ", "super", "(", "RNNAgent", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "args", "=", "args", "\n", "\n", "self", ".", "fc1", "=", "nn", ".", "Linear", "(", "input_shape", ",", "args", ".", "rnn_hidden_dim", ")", "\n", "self", ".", "rnn", "=", "nn", ".", "GRUCell", "(", "args", ".", "rnn_hidden_dim", ",", "args", ".", "rnn_hidden_dim", ")", "\n", "self", ".", "fc2", "=", "nn", ".", "Linear", "(", "args", ".", "rnn_hidden_dim", ",", "args", ".", "n_actions", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.agents.rnn_agent.RNNAgent.init_hidden": [[14, 17], ["rnn_agent.RNNAgent.fc1.weight.new().zero_", "rnn_agent.RNNAgent.fc1.weight.new"], "methods", ["None"], ["", "def", "init_hidden", "(", "self", ")", ":", "\n", "# make hidden states on same device as model", "\n", "        ", "return", "self", ".", "fc1", ".", "weight", ".", "new", "(", "1", ",", "self", ".", "args", ".", "rnn_hidden_dim", ")", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.agents.rnn_agent.RNNAgent.forward": [[18, 24], ["torch.relu", "torch.relu", "torch.relu", "hidden_state.reshape", "rnn_agent.RNNAgent.rnn", "rnn_agent.RNNAgent.fc2", "rnn_agent.RNNAgent.fc1"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ",", "hidden_state", ")", ":", "\n", "        ", "x", "=", "F", ".", "relu", "(", "self", ".", "fc1", "(", "inputs", ")", ")", "\n", "h_in", "=", "hidden_state", ".", "reshape", "(", "-", "1", ",", "self", ".", "args", ".", "rnn_hidden_dim", ")", "\n", "h", "=", "self", ".", "rnn", "(", "x", ",", "h_in", ")", "\n", "q", "=", "self", ".", "fc2", "(", "h", ")", "\n", "return", "q", ",", "h", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.agents.ff_agent.FFAgent.__init__": [[6, 14], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input_shape", ",", "args", ")", ":", "\n", "        ", "super", "(", "FFAgent", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "args", "=", "args", "\n", "\n", "# Easiest to reuse rnn_hidden_dim variable", "\n", "self", ".", "fc1", "=", "nn", ".", "Linear", "(", "input_shape", ",", "args", ".", "rnn_hidden_dim", ")", "\n", "self", ".", "fc2", "=", "nn", ".", "Linear", "(", "args", ".", "rnn_hidden_dim", ",", "args", ".", "rnn_hidden_dim", ")", "\n", "self", ".", "fc3", "=", "nn", ".", "Linear", "(", "args", ".", "rnn_hidden_dim", ",", "args", ".", "n_actions", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.agents.ff_agent.FFAgent.init_hidden": [[15, 18], ["ff_agent.FFAgent.fc1.weight.new().zero_", "ff_agent.FFAgent.fc1.weight.new"], "methods", ["None"], ["", "def", "init_hidden", "(", "self", ")", ":", "\n", "# make hidden states on same device as model", "\n", "        ", "return", "self", ".", "fc1", ".", "weight", ".", "new", "(", "1", ",", "self", ".", "args", ".", "rnn_hidden_dim", ")", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.agents.ff_agent.FFAgent.forward": [[19, 25], ["torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "ff_agent.FFAgent.fc3", "ff_agent.FFAgent.fc1", "ff_agent.FFAgent.fc2"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ",", "hidden_state", ")", ":", "\n", "        ", "x", "=", "F", ".", "relu", "(", "self", ".", "fc1", "(", "inputs", ")", ")", "\n", "# h_in = hidden_state.reshape(-1, self.args.rnn_hidden_dim)", "\n", "h", "=", "F", ".", "relu", "(", "self", ".", "fc2", "(", "x", ")", ")", "\n", "q", "=", "self", ".", "fc3", "(", "h", ")", "\n", "return", "q", ",", "h", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.agents.central_rnn_agent.CentralRNNAgent.__init__": [[6, 13], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.GRUCell", "torch.GRUCell", "torch.GRUCell", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input_shape", ",", "args", ")", ":", "\n", "        ", "super", "(", "CentralRNNAgent", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "args", "=", "args", "\n", "\n", "self", ".", "fc1", "=", "nn", ".", "Linear", "(", "input_shape", ",", "args", ".", "rnn_hidden_dim", ")", "\n", "self", ".", "rnn", "=", "nn", ".", "GRUCell", "(", "args", ".", "rnn_hidden_dim", ",", "args", ".", "rnn_hidden_dim", ")", "\n", "self", ".", "fc2", "=", "nn", ".", "Linear", "(", "args", ".", "rnn_hidden_dim", ",", "args", ".", "n_actions", "*", "args", ".", "central_action_embed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.agents.central_rnn_agent.CentralRNNAgent.init_hidden": [[14, 17], ["central_rnn_agent.CentralRNNAgent.fc1.weight.new().zero_", "central_rnn_agent.CentralRNNAgent.fc1.weight.new"], "methods", ["None"], ["", "def", "init_hidden", "(", "self", ")", ":", "\n", "# make hidden states on same device as model", "\n", "        ", "return", "self", ".", "fc1", ".", "weight", ".", "new", "(", "1", ",", "self", ".", "args", ".", "rnn_hidden_dim", ")", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.agents.central_rnn_agent.CentralRNNAgent.forward": [[18, 25], ["torch.relu", "torch.relu", "torch.relu", "hidden_state.reshape", "central_rnn_agent.CentralRNNAgent.rnn", "central_rnn_agent.CentralRNNAgent.fc2", "q.reshape.reshape.reshape", "central_rnn_agent.CentralRNNAgent.fc1"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ",", "hidden_state", ")", ":", "\n", "        ", "x", "=", "F", ".", "relu", "(", "self", ".", "fc1", "(", "inputs", ")", ")", "\n", "h_in", "=", "hidden_state", ".", "reshape", "(", "-", "1", ",", "self", ".", "args", ".", "rnn_hidden_dim", ")", "\n", "h", "=", "self", ".", "rnn", "(", "x", ",", "h_in", ")", "\n", "q", "=", "self", ".", "fc2", "(", "h", ")", "\n", "q", "=", "q", ".", "reshape", "(", "-", "1", ",", "self", ".", "args", ".", "n_actions", ",", "self", ".", "args", ".", "central_action_embed", ")", "\n", "return", "q", ",", "h", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.critics.centralV.CentralVCritic.__init__": [[7, 21], ["torch.Module.__init__", "centralV.CentralVCritic._get_input_shape", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner.__init__", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.critics.coma.COMACritic._get_input_shape"], ["    ", "def", "__init__", "(", "self", ",", "scheme", ",", "args", ")", ":", "\n", "        ", "super", "(", "CentralVCritic", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "args", "=", "args", "\n", "self", ".", "n_actions", "=", "args", ".", "n_actions", "\n", "self", ".", "n_agents", "=", "args", ".", "n_agents", "\n", "\n", "input_shape", "=", "self", ".", "_get_input_shape", "(", "scheme", ")", "\n", "self", ".", "output_type", "=", "\"v\"", "\n", "\n", "# Set up network layers", "\n", "self", ".", "fc1", "=", "nn", ".", "Linear", "(", "input_shape", ",", "128", ")", "\n", "self", ".", "fc2", "=", "nn", ".", "Linear", "(", "128", ",", "128", ")", "\n", "self", ".", "fc3", "=", "nn", ".", "Linear", "(", "128", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.critics.centralV.CentralVCritic.forward": [[22, 28], ["centralV.CentralVCritic._build_inputs", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "centralV.CentralVCritic.fc3", "centralV.CentralVCritic.view().repeat", "centralV.CentralVCritic.fc1", "centralV.CentralVCritic.fc2", "centralV.CentralVCritic.view"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.critics.coma.COMACritic._build_inputs"], ["", "def", "forward", "(", "self", ",", "batch", ",", "t", "=", "None", ")", ":", "\n", "        ", "inputs", ",", "bs", ",", "max_t", "=", "self", ".", "_build_inputs", "(", "batch", ",", "t", "=", "t", ")", "\n", "x", "=", "F", ".", "relu", "(", "self", ".", "fc1", "(", "inputs", ")", ")", "\n", "x", "=", "F", ".", "relu", "(", "self", ".", "fc2", "(", "x", ")", ")", "\n", "q", "=", "self", ".", "fc3", "(", "x", ")", "\n", "return", "q", ".", "view", "(", "bs", ",", "max_t", ",", "1", ",", "-", "1", ")", ".", "repeat", "(", "1", ",", "1", ",", "self", ".", "n_agents", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.critics.centralV.CentralVCritic._build_inputs": [[29, 52], ["torch.cat.append", "torch.cat.append", "torch.cat", "torch.cat", "torch.cat", "slice", "slice", "[].view", "torch.cat.append", "isinstance", "torch.zeros_like().view", "torch.zeros_like().view", "torch.zeros_like().view", "torch.cat.append", "torch.cat", "torch.cat", "torch.cat", "last_actions.view.view.view", "torch.cat.append", "x.reshape", "[].view", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "slice"], "methods", ["None"], ["", "def", "_build_inputs", "(", "self", ",", "batch", ",", "t", "=", "None", ")", ":", "\n", "        ", "bs", "=", "batch", ".", "batch_size", "\n", "max_t", "=", "batch", ".", "max_seq_length", "if", "t", "is", "None", "else", "1", "\n", "ts", "=", "slice", "(", "None", ")", "if", "t", "is", "None", "else", "slice", "(", "t", ",", "t", "+", "1", ")", "\n", "inputs", "=", "[", "]", "\n", "# state", "\n", "inputs", ".", "append", "(", "batch", "[", "\"state\"", "]", "[", ":", ",", "ts", "]", ")", "\n", "\n", "# observations", "\n", "inputs", ".", "append", "(", "batch", "[", "\"obs\"", "]", "[", ":", ",", "ts", "]", ".", "view", "(", "bs", ",", "max_t", ",", "-", "1", ")", ")", "\n", "\n", "# last actions", "\n", "if", "t", "==", "0", ":", "\n", "            ", "inputs", ".", "append", "(", "th", ".", "zeros_like", "(", "batch", "[", "\"actions_onehot\"", "]", "[", ":", ",", "0", ":", "1", "]", ")", ".", "view", "(", "bs", ",", "max_t", ",", "1", ",", "-", "1", ")", ")", "\n", "", "elif", "isinstance", "(", "t", ",", "int", ")", ":", "\n", "            ", "inputs", ".", "append", "(", "batch", "[", "\"actions_onehot\"", "]", "[", ":", ",", "slice", "(", "t", "-", "1", ",", "t", ")", "]", ".", "view", "(", "bs", ",", "max_t", ",", "1", ",", "-", "1", ")", ")", "\n", "", "else", ":", "\n", "            ", "last_actions", "=", "th", ".", "cat", "(", "[", "th", ".", "zeros_like", "(", "batch", "[", "\"actions_onehot\"", "]", "[", ":", ",", "0", ":", "1", "]", ")", ",", "batch", "[", "\"actions_onehot\"", "]", "[", ":", ",", ":", "-", "1", "]", "]", ",", "dim", "=", "1", ")", "\n", "last_actions", "=", "last_actions", ".", "view", "(", "bs", ",", "max_t", ",", "1", ",", "-", "1", ")", "\n", "inputs", ".", "append", "(", "last_actions", ")", "\n", "\n", "", "inputs", "=", "th", ".", "cat", "(", "[", "x", ".", "reshape", "(", "bs", "*", "max_t", ",", "-", "1", ")", "for", "x", "in", "inputs", "]", ",", "dim", "=", "1", ")", "\n", "return", "inputs", ",", "bs", ",", "max_t", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.critics.centralV.CentralVCritic._get_input_shape": [[53, 61], ["None"], "methods", ["None"], ["", "def", "_get_input_shape", "(", "self", ",", "scheme", ")", ":", "\n", "# state", "\n", "        ", "input_shape", "=", "scheme", "[", "\"state\"", "]", "[", "\"vshape\"", "]", "\n", "# observations", "\n", "input_shape", "+=", "scheme", "[", "\"obs\"", "]", "[", "\"vshape\"", "]", "*", "self", ".", "n_agents", "\n", "# last actions", "\n", "input_shape", "+=", "scheme", "[", "\"actions_onehot\"", "]", "[", "\"vshape\"", "]", "[", "0", "]", "*", "self", ".", "n_agents", "\n", "return", "input_shape", "", "", "", ""]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.critics.coma.COMACritic.__init__": [[7, 28], ["torch.Module.__init__", "coma.COMACritic._get_input_shape", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.GRU", "torch.GRU", "torch.GRU"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner.__init__", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.critics.coma.COMACritic._get_input_shape"], ["    ", "def", "__init__", "(", "self", ",", "scheme", ",", "args", ")", ":", "\n", "        ", "super", "(", "COMACritic", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "args", "=", "args", "\n", "self", ".", "n_actions", "=", "args", ".", "n_actions", "\n", "self", ".", "n_agents", "=", "args", ".", "n_agents", "\n", "\n", "input_shape", "=", "self", ".", "_get_input_shape", "(", "scheme", ")", "\n", "self", ".", "output_type", "=", "\"q\"", "\n", "\n", "# Set up network layers", "\n", "self", ".", "fc1", "=", "nn", ".", "Linear", "(", "input_shape", ",", "128", ")", "\n", "\n", "if", "args", ".", "recurrent_critic", ":", "\n", "            ", "self", ".", "rnn", "=", "nn", ".", "GRU", "(", "128", ",", "128", ",", "batch_first", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "rnn", "=", "None", "\n", "\n", "", "self", ".", "fc2", "=", "nn", ".", "Linear", "(", "128", ",", "128", ")", "\n", "# self.v_head = nn.Linear(128, 1)", "\n", "self", ".", "fc3", "=", "nn", ".", "Linear", "(", "128", ",", "self", ".", "n_actions", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.critics.coma.COMACritic.forward": [[29, 44], ["coma.COMACritic._build_inputs", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "coma.COMACritic.fc3", "coma.COMACritic.fc1", "coma.COMACritic.fc2", "x.reshape().permute.reshape().permute.permute().reshape", "coma.COMACritic.rnn", "x.reshape().permute.reshape().permute.reshape().permute", "x.reshape().permute.reshape().permute.permute", "x.reshape().permute.reshape().permute.reshape"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.critics.coma.COMACritic._build_inputs"], ["", "def", "forward", "(", "self", ",", "batch", ",", "t", "=", "None", ")", ":", "\n", "        ", "inputs", "=", "self", ".", "_build_inputs", "(", "batch", ",", "t", "=", "t", ")", "\n", "bs", ",", "max_t", ",", "n_agents", ",", "vdim", "=", "inputs", ".", "shape", "\n", "x", "=", "F", ".", "relu", "(", "self", ".", "fc1", "(", "inputs", ")", ")", "\n", "x", "=", "F", ".", "relu", "(", "self", ".", "fc2", "(", "x", ")", ")", "\n", "\n", "if", "self", ".", "rnn", "is", "not", "None", ":", "\n", "            ", "x", "=", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", ".", "reshape", "(", "bs", "*", "n_agents", ",", "max_t", ",", "-", "1", ")", "\n", "x", ",", "h_out", "=", "self", ".", "rnn", "(", "x", ")", "# h0 defaults to 0 if not provided, TODO: make explicit", "\n", "x", "=", "x", ".", "reshape", "(", "bs", ",", "n_agents", ",", "max_t", ",", "-", "1", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "\n", "", "q", "=", "self", ".", "fc3", "(", "x", ")", "\n", "# v = self.v_head(x)", "\n", "# q = adv - adv.mean(-1, keepdim=True).expand_as(adv) + v.expand_as(adv)", "\n", "return", "q", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.critics.coma.COMACritic._build_inputs": [[45, 76], ["torch.cat.append", "torch.cat.append", "[].view().repeat", "agent_mask.view().repeat().view.view().repeat().view.view().repeat().view", "torch.cat.append", "torch.cat.append", "torch.cat", "torch.cat", "torch.cat", "slice", "slice", "[].unsqueeze().repeat", "torch.eye", "torch.eye", "torch.eye", "torch.cat.append", "isinstance", "torch.eye().unsqueeze().unsqueeze().expand", "torch.eye().unsqueeze().unsqueeze().expand", "torch.eye().unsqueeze().unsqueeze().expand", "[].view", "agent_mask.view().repeat().view.view().repeat().view.view().repeat", "agent_mask.view().repeat().view.view().repeat().view.unsqueeze().unsqueeze", "torch.zeros_like().view().repeat", "torch.zeros_like().view().repeat", "torch.zeros_like().view().repeat", "torch.cat.append", "torch.cat", "torch.cat", "torch.cat", "last_actions.view().repeat.view().repeat.view().repeat", "torch.cat.append", "x.reshape", "[].unsqueeze", "[].view().repeat", "torch.eye().unsqueeze().unsqueeze", "torch.eye().unsqueeze().unsqueeze", "torch.eye().unsqueeze().unsqueeze", "agent_mask.view().repeat().view.view().repeat().view.view", "agent_mask.view().repeat().view.view().repeat().view.unsqueeze", "torch.zeros_like().view", "torch.zeros_like().view", "torch.zeros_like().view", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "last_actions.view().repeat.view().repeat.view", "[].view", "torch.eye().unsqueeze", "torch.eye().unsqueeze", "torch.eye().unsqueeze", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.eye", "torch.eye", "torch.eye", "slice"], "methods", ["None"], ["", "def", "_build_inputs", "(", "self", ",", "batch", ",", "t", "=", "None", ")", ":", "\n", "        ", "bs", "=", "batch", ".", "batch_size", "\n", "max_t", "=", "batch", ".", "max_seq_length", "if", "t", "is", "None", "else", "1", "\n", "ts", "=", "slice", "(", "None", ")", "if", "t", "is", "None", "else", "slice", "(", "t", ",", "t", "+", "1", ")", "\n", "inputs", "=", "[", "]", "\n", "# state", "\n", "inputs", ".", "append", "(", "batch", "[", "\"state\"", "]", "[", ":", ",", "ts", "]", ".", "unsqueeze", "(", "2", ")", ".", "repeat", "(", "1", ",", "1", ",", "self", ".", "n_agents", ",", "1", ")", ")", "\n", "\n", "# observation", "\n", "inputs", ".", "append", "(", "batch", "[", "\"obs\"", "]", "[", ":", ",", "ts", "]", ")", "\n", "\n", "# actions (masked out by agent)", "\n", "actions", "=", "batch", "[", "\"actions_onehot\"", "]", "[", ":", ",", "ts", "]", ".", "view", "(", "bs", ",", "max_t", ",", "1", ",", "-", "1", ")", ".", "repeat", "(", "1", ",", "1", ",", "self", ".", "n_agents", ",", "1", ")", "\n", "agent_mask", "=", "(", "1", "-", "th", ".", "eye", "(", "self", ".", "n_agents", ",", "device", "=", "batch", ".", "device", ")", ")", "\n", "agent_mask", "=", "agent_mask", ".", "view", "(", "-", "1", ",", "1", ")", ".", "repeat", "(", "1", ",", "self", ".", "n_actions", ")", ".", "view", "(", "self", ".", "n_agents", ",", "-", "1", ")", "\n", "inputs", ".", "append", "(", "actions", "*", "agent_mask", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "0", ")", ")", "\n", "\n", "# last actions", "\n", "if", "t", "==", "0", ":", "\n", "            ", "inputs", ".", "append", "(", "th", ".", "zeros_like", "(", "batch", "[", "\"actions_onehot\"", "]", "[", ":", ",", "0", ":", "1", "]", ")", ".", "view", "(", "bs", ",", "max_t", ",", "1", ",", "-", "1", ")", ".", "repeat", "(", "1", ",", "1", ",", "self", ".", "n_agents", ",", "1", ")", ")", "\n", "", "elif", "isinstance", "(", "t", ",", "int", ")", ":", "\n", "            ", "inputs", ".", "append", "(", "batch", "[", "\"actions_onehot\"", "]", "[", ":", ",", "slice", "(", "t", "-", "1", ",", "t", ")", "]", ".", "view", "(", "bs", ",", "max_t", ",", "1", ",", "-", "1", ")", ".", "repeat", "(", "1", ",", "1", ",", "self", ".", "n_agents", ",", "1", ")", ")", "\n", "", "else", ":", "\n", "            ", "last_actions", "=", "th", ".", "cat", "(", "[", "th", ".", "zeros_like", "(", "batch", "[", "\"actions_onehot\"", "]", "[", ":", ",", "0", ":", "1", "]", ")", ",", "batch", "[", "\"actions_onehot\"", "]", "[", ":", ",", ":", "-", "1", "]", "]", ",", "dim", "=", "1", ")", "\n", "last_actions", "=", "last_actions", ".", "view", "(", "bs", ",", "max_t", ",", "1", ",", "-", "1", ")", ".", "repeat", "(", "1", ",", "1", ",", "self", ".", "n_agents", ",", "1", ")", "\n", "inputs", ".", "append", "(", "last_actions", ")", "\n", "\n", "", "inputs", ".", "append", "(", "th", ".", "eye", "(", "self", ".", "n_agents", ",", "device", "=", "batch", ".", "device", ")", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "bs", ",", "max_t", ",", "-", "1", ",", "-", "1", ")", ")", "\n", "\n", "inputs", "=", "th", ".", "cat", "(", "[", "x", ".", "reshape", "(", "bs", ",", "max_t", ",", "self", ".", "n_agents", ",", "-", "1", ")", "for", "x", "in", "inputs", "]", ",", "dim", "=", "-", "1", ")", "\n", "return", "inputs", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.critics.coma.COMACritic._get_input_shape": [[77, 87], ["None"], "methods", ["None"], ["", "def", "_get_input_shape", "(", "self", ",", "scheme", ")", ":", "\n", "# state", "\n", "        ", "input_shape", "=", "scheme", "[", "\"state\"", "]", "[", "\"vshape\"", "]", "\n", "# observation", "\n", "input_shape", "+=", "scheme", "[", "\"obs\"", "]", "[", "\"vshape\"", "]", "\n", "# actions and last actions", "\n", "input_shape", "+=", "scheme", "[", "\"actions_onehot\"", "]", "[", "\"vshape\"", "]", "[", "0", "]", "*", "self", ".", "n_agents", "*", "2", "\n", "# agent id", "\n", "input_shape", "+=", "self", ".", "n_agents", "\n", "return", "input_shape", "", "", "", ""]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.dmaq_general.DMAQer.__init__": [[10, 31], ["torch.Module.__init__", "int", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "dmaq_si_weight.DMAQ_SI_Weight", "numpy.prod", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "DMAQer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "args", "=", "args", "\n", "self", ".", "n_agents", "=", "args", ".", "n_agents", "\n", "self", ".", "n_actions", "=", "args", ".", "n_actions", "\n", "self", ".", "state_dim", "=", "int", "(", "np", ".", "prod", "(", "args", ".", "state_shape", ")", ")", "\n", "self", ".", "action_dim", "=", "args", ".", "n_agents", "*", "self", ".", "n_actions", "\n", "self", ".", "state_action_dim", "=", "self", ".", "state_dim", "+", "self", ".", "action_dim", "+", "1", "\n", "\n", "self", ".", "embed_dim", "=", "args", ".", "mixing_embed_dim", "\n", "\n", "hypernet_embed", "=", "self", ".", "args", ".", "hypernet_embed", "\n", "self", ".", "hyper_w_final", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "self", ".", "state_dim", ",", "hypernet_embed", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "hypernet_embed", ",", "self", ".", "n_agents", ")", ")", "\n", "self", ".", "V", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "self", ".", "state_dim", ",", "hypernet_embed", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "hypernet_embed", ",", "self", ".", "n_agents", ")", ")", "\n", "\n", "self", ".", "si_weight", "=", "DMAQ_SI_Weight", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.dmaq_general.DMAQer.calc_v": [[32, 36], ["agent_qs.view.view.view", "torch.sum", "torch.sum", "torch.sum"], "methods", ["None"], ["", "def", "calc_v", "(", "self", ",", "agent_qs", ")", ":", "\n", "        ", "agent_qs", "=", "agent_qs", ".", "view", "(", "-", "1", ",", "self", ".", "n_agents", ")", "\n", "v_tot", "=", "th", ".", "sum", "(", "agent_qs", ",", "dim", "=", "-", "1", ")", "\n", "return", "v_tot", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.dmaq_general.DMAQer.calc_adv": [[37, 53], ["states.reshape.reshape.reshape", "actions.reshape.reshape.reshape", "agent_qs.view.view.view", "max_q_i.view.view.view", "dmaq_general.DMAQer.si_weight", "adv_w_final.view.view.view", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum"], "methods", ["None"], ["", "def", "calc_adv", "(", "self", ",", "agent_qs", ",", "states", ",", "actions", ",", "max_q_i", ")", ":", "\n", "        ", "states", "=", "states", ".", "reshape", "(", "-", "1", ",", "self", ".", "state_dim", ")", "\n", "actions", "=", "actions", ".", "reshape", "(", "-", "1", ",", "self", ".", "action_dim", ")", "\n", "agent_qs", "=", "agent_qs", ".", "view", "(", "-", "1", ",", "self", ".", "n_agents", ")", "\n", "max_q_i", "=", "max_q_i", ".", "view", "(", "-", "1", ",", "self", ".", "n_agents", ")", "\n", "\n", "adv_q", "=", "(", "agent_qs", "-", "max_q_i", ")", ".", "view", "(", "-", "1", ",", "self", ".", "n_agents", ")", ".", "detach", "(", ")", "\n", "\n", "adv_w_final", "=", "self", ".", "si_weight", "(", "states", ",", "actions", ")", "\n", "adv_w_final", "=", "adv_w_final", ".", "view", "(", "-", "1", ",", "self", ".", "n_agents", ")", "\n", "\n", "if", "self", ".", "args", ".", "is_minus_one", ":", "\n", "            ", "adv_tot", "=", "th", ".", "sum", "(", "adv_q", "*", "(", "adv_w_final", "-", "1.", ")", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "            ", "adv_tot", "=", "th", ".", "sum", "(", "adv_q", "*", "adv_w_final", ",", "dim", "=", "1", ")", "\n", "", "return", "adv_tot", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.dmaq_general.DMAQer.calc": [[54, 61], ["dmaq_general.DMAQer.calc_v", "dmaq_general.DMAQer.calc_adv"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.dmaq_qatten.DMAQ_QattenMixer.calc_v", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.dmaq_qatten.DMAQ_QattenMixer.calc_adv"], ["", "def", "calc", "(", "self", ",", "agent_qs", ",", "states", ",", "actions", "=", "None", ",", "max_q_i", "=", "None", ",", "is_v", "=", "False", ")", ":", "\n", "        ", "if", "is_v", ":", "\n", "            ", "v_tot", "=", "self", ".", "calc_v", "(", "agent_qs", ")", "\n", "return", "v_tot", "\n", "", "else", ":", "\n", "            ", "adv_tot", "=", "self", ".", "calc_adv", "(", "agent_qs", ",", "states", ",", "actions", ",", "max_q_i", ")", "\n", "return", "adv_tot", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.dmaq_general.DMAQer.forward": [[62, 84], ["agent_qs.view.view.size", "states.reshape.reshape.reshape", "agent_qs.view.view.view", "dmaq_general.DMAQer.hyper_w_final", "torch.abs", "torch.abs", "torch.abs", "dmaq_general.DMAQer.V", "v.view.view.view", "dmaq_general.DMAQer.calc", "dmaq_general.DMAQer.view", "torch.abs.view", "max_q_i.view.view.view"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.dmaq_qatten.DMAQ_QattenMixer.calc"], ["", "", "def", "forward", "(", "self", ",", "agent_qs", ",", "states", ",", "actions", "=", "None", ",", "max_q_i", "=", "None", ",", "is_v", "=", "False", ")", ":", "\n", "        ", "bs", "=", "agent_qs", ".", "size", "(", "0", ")", "\n", "states", "=", "states", ".", "reshape", "(", "-", "1", ",", "self", ".", "state_dim", ")", "\n", "agent_qs", "=", "agent_qs", ".", "view", "(", "-", "1", ",", "self", ".", "n_agents", ")", "\n", "\n", "w_final", "=", "self", ".", "hyper_w_final", "(", "states", ")", "\n", "w_final", "=", "th", ".", "abs", "(", "w_final", ")", "\n", "w_final", "=", "w_final", ".", "view", "(", "-", "1", ",", "self", ".", "n_agents", ")", "+", "1e-10", "\n", "v", "=", "self", ".", "V", "(", "states", ")", "\n", "v", "=", "v", ".", "view", "(", "-", "1", ",", "self", ".", "n_agents", ")", "\n", "\n", "if", "self", ".", "args", ".", "weighted_head", ":", "\n", "            ", "agent_qs", "=", "w_final", "*", "agent_qs", "+", "v", "\n", "", "if", "not", "is_v", ":", "\n", "            ", "max_q_i", "=", "max_q_i", ".", "view", "(", "-", "1", ",", "self", ".", "n_agents", ")", "\n", "if", "self", ".", "args", ".", "weighted_head", ":", "\n", "                ", "max_q_i", "=", "w_final", "*", "max_q_i", "+", "v", "\n", "\n", "", "", "y", "=", "self", ".", "calc", "(", "agent_qs", ",", "states", ",", "actions", "=", "actions", ",", "max_q_i", "=", "max_q_i", ",", "is_v", "=", "is_v", ")", "\n", "v_tot", "=", "y", ".", "view", "(", "bs", ",", "-", "1", ",", "1", ")", "\n", "\n", "return", "v_tot", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.dmaq_qatten_weight.Qatten_Weight.__init__": [[10, 47], ["torch.Module.__init__", "int", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "range", "torch.Sequential", "torch.Sequential", "torch.Sequential", "numpy.prod", "torch.Sequential", "torch.Sequential", "torch.Sequential", "dmaq_qatten_weight.Qatten_Weight.selector_extractors.append", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "dmaq_qatten_weight.Qatten_Weight.key_extractors.append", "dmaq_qatten_weight.Qatten_Weight.key_extractors.append", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "Qatten_Weight", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "name", "=", "'qatten_weight'", "\n", "self", ".", "args", "=", "args", "\n", "self", ".", "n_agents", "=", "args", ".", "n_agents", "\n", "self", ".", "state_dim", "=", "int", "(", "np", ".", "prod", "(", "args", ".", "state_shape", ")", ")", "\n", "self", ".", "unit_dim", "=", "args", ".", "unit_dim", "\n", "self", ".", "n_actions", "=", "args", ".", "n_actions", "\n", "self", ".", "sa_dim", "=", "self", ".", "state_dim", "+", "self", ".", "n_agents", "*", "self", ".", "n_actions", "\n", "self", ".", "n_head", "=", "args", ".", "n_head", "# attention head num", "\n", "\n", "self", ".", "embed_dim", "=", "args", ".", "mixing_embed_dim", "\n", "self", ".", "attend_reg_coef", "=", "args", ".", "attend_reg_coef", "\n", "\n", "self", ".", "key_extractors", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "selector_extractors", "=", "nn", ".", "ModuleList", "(", ")", "\n", "\n", "hypernet_embed", "=", "self", ".", "args", ".", "hypernet_embed", "\n", "for", "i", "in", "range", "(", "self", ".", "n_head", ")", ":", "# multi-head attention", "\n", "            ", "selector_nn", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "self", ".", "state_dim", ",", "hypernet_embed", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "hypernet_embed", ",", "self", ".", "embed_dim", ",", "bias", "=", "False", ")", ")", "\n", "self", ".", "selector_extractors", ".", "append", "(", "selector_nn", ")", "# query", "\n", "if", "self", ".", "args", ".", "nonlinear", ":", "# add qs", "\n", "                ", "self", ".", "key_extractors", ".", "append", "(", "nn", ".", "Linear", "(", "self", ".", "unit_dim", "+", "1", ",", "self", ".", "embed_dim", ",", "bias", "=", "False", ")", ")", "# key", "\n", "", "else", ":", "\n", "                ", "self", ".", "key_extractors", ".", "append", "(", "nn", ".", "Linear", "(", "self", ".", "unit_dim", ",", "self", ".", "embed_dim", ",", "bias", "=", "False", ")", ")", "# key", "\n", "", "", "if", "self", ".", "args", ".", "weighted_head", ":", "\n", "            ", "self", ".", "hyper_w_head", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "self", ".", "state_dim", ",", "hypernet_embed", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "hypernet_embed", ",", "self", ".", "n_head", ")", ")", "\n", "\n", "# V(s) instead of a bias for the last layers", "\n", "", "self", ".", "V", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "self", ".", "state_dim", ",", "self", ".", "embed_dim", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.dmaq_qatten_weight.Qatten_Weight.forward": [[48, 108], ["states.reshape.reshape.reshape", "torch.cat.reshape", "torch.cat.permute", "agent_qs.view.view.view", "zip", "torch.stack", "torch.stack", "torch.stack", "torch.sum.view", "dmaq_qatten_weight.Qatten_Weight.V().view", "torch.sum", "torch.sum", "torch.sum", "torch.cat", "torch.cat", "torch.cat", "sel_ext", "torch.matmul", "torch.matmul", "torch.matmul", "torch.softmax", "torch.softmax", "torch.softmax", "head_attend_logits.append", "head_attend_weights.append", "torch.abs", "torch.abs", "torch.abs", "w_head.view().repeat.view().repeat.view().repeat", "sum", "k_ext", "curr_head_selector.view", "torch.stack().permute", "torch.stack().permute", "torch.stack().permute", "numpy.sqrt", "actions.reshape.reshape.reshape", "dmaq_qatten_weight.Qatten_Weight.V", "dmaq_qatten_weight.Qatten_Weight.hyper_w_head", "agent_qs.view.view.permute", "w_head.view().repeat.view().repeat.view", "torch.stack", "torch.stack", "torch.stack"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "agent_qs", ",", "states", ",", "actions", ")", ":", "\n", "        ", "states", "=", "states", ".", "reshape", "(", "-", "1", ",", "self", ".", "state_dim", ")", "\n", "unit_states", "=", "states", "[", ":", ",", ":", "self", ".", "unit_dim", "*", "self", ".", "n_agents", "]", "# get agent own features from state", "\n", "unit_states", "=", "unit_states", ".", "reshape", "(", "-", "1", ",", "self", ".", "n_agents", ",", "self", ".", "unit_dim", ")", "\n", "unit_states", "=", "unit_states", ".", "permute", "(", "1", ",", "0", ",", "2", ")", "\n", "\n", "agent_qs", "=", "agent_qs", ".", "view", "(", "-", "1", ",", "1", ",", "self", ".", "n_agents", ")", "# agent_qs: (batch_size, 1, agent_num)", "\n", "\n", "if", "self", ".", "args", ".", "nonlinear", ":", "\n", "            ", "unit_states", "=", "th", ".", "cat", "(", "(", "unit_states", ",", "agent_qs", ".", "permute", "(", "2", ",", "0", ",", "1", ")", ")", ",", "dim", "=", "2", ")", "\n", "# states: (batch_size, state_dim)", "\n", "", "all_head_selectors", "=", "[", "sel_ext", "(", "states", ")", "for", "sel_ext", "in", "self", ".", "selector_extractors", "]", "\n", "# all_head_selectors: (head_num, batch_size, embed_dim)", "\n", "# unit_states: (agent_num, batch_size, unit_dim)", "\n", "all_head_keys", "=", "[", "[", "k_ext", "(", "enc", ")", "for", "enc", "in", "unit_states", "]", "for", "k_ext", "in", "self", ".", "key_extractors", "]", "\n", "# all_head_keys: (head_num, agent_num, batch_size, embed_dim)", "\n", "\n", "# calculate attention per head", "\n", "head_attend_logits", "=", "[", "]", "\n", "head_attend_weights", "=", "[", "]", "\n", "for", "curr_head_keys", ",", "curr_head_selector", "in", "zip", "(", "all_head_keys", ",", "all_head_selectors", ")", ":", "\n", "# curr_head_keys: (agent_num, batch_size, embed_dim)", "\n", "# curr_head_selector: (batch_size, embed_dim)", "\n", "\n", "# (batch_size, 1, embed_dim) * (batch_size, embed_dim, agent_num)", "\n", "            ", "attend_logits", "=", "th", ".", "matmul", "(", "curr_head_selector", ".", "view", "(", "-", "1", ",", "1", ",", "self", ".", "embed_dim", ")", ",", "\n", "th", ".", "stack", "(", "curr_head_keys", ")", ".", "permute", "(", "1", ",", "2", ",", "0", ")", ")", "\n", "# attend_logits: (batch_size, 1, agent_num)", "\n", "# scale dot-products by size of key (from Attention is All You Need)", "\n", "scaled_attend_logits", "=", "attend_logits", "/", "np", ".", "sqrt", "(", "self", ".", "embed_dim", ")", "\n", "if", "self", ".", "args", ".", "mask_dead", ":", "\n", "# actions: (episode_batch, episode_length - 1, agent_num, 1)", "\n", "                ", "actions", "=", "actions", ".", "reshape", "(", "-", "1", ",", "1", ",", "self", ".", "n_agents", ")", "\n", "# actions: (batch_size, 1, agent_num)", "\n", "scaled_attend_logits", "[", "actions", "==", "0", "]", "=", "-", "99999999", "# action == 0 means the unit is dead", "\n", "", "attend_weights", "=", "F", ".", "softmax", "(", "scaled_attend_logits", ",", "dim", "=", "2", ")", "# (batch_size, 1, agent_num)", "\n", "\n", "head_attend_logits", ".", "append", "(", "attend_logits", ")", "\n", "head_attend_weights", ".", "append", "(", "attend_weights", ")", "\n", "\n", "", "head_attend", "=", "th", ".", "stack", "(", "head_attend_weights", ",", "dim", "=", "1", ")", "# (batch_size, self.n_head, self.n_agents)", "\n", "head_attend", "=", "head_attend", ".", "view", "(", "-", "1", ",", "self", ".", "n_head", ",", "self", ".", "n_agents", ")", "\n", "\n", "v", "=", "self", ".", "V", "(", "states", ")", ".", "view", "(", "-", "1", ",", "1", ")", "# v: (bs, 1)", "\n", "# head_qs: [head_num, bs, 1]", "\n", "if", "self", ".", "args", ".", "weighted_head", ":", "\n", "            ", "w_head", "=", "th", ".", "abs", "(", "self", ".", "hyper_w_head", "(", "states", ")", ")", "# w_head: (bs, head_num)", "\n", "w_head", "=", "w_head", ".", "view", "(", "-", "1", ",", "self", ".", "n_head", ",", "1", ")", ".", "repeat", "(", "1", ",", "1", ",", "self", ".", "n_agents", ")", "# w_head: (bs, head_num, self.n_agents)", "\n", "head_attend", "*=", "w_head", "\n", "\n", "", "head_attend", "=", "th", ".", "sum", "(", "head_attend", ",", "dim", "=", "1", ")", "\n", "\n", "if", "not", "self", ".", "args", ".", "state_bias", ":", "\n", "            ", "v", "*=", "0.", "\n", "\n", "# regularize magnitude of attention logits", "\n", "", "attend_mag_regs", "=", "self", ".", "attend_reg_coef", "*", "sum", "(", "(", "logit", "**", "2", ")", ".", "mean", "(", ")", "for", "logit", "in", "head_attend_logits", ")", "\n", "head_entropies", "=", "[", "(", "-", "(", "(", "probs", "+", "1e-8", ")", ".", "log", "(", ")", "*", "probs", ")", ".", "squeeze", "(", ")", ".", "sum", "(", "1", ")", ".", "mean", "(", ")", ")", "for", "probs", "in", "head_attend_weights", "]", "\n", "\n", "return", "head_attend", ",", "v", ",", "attend_mag_regs", ",", "head_entropies", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.qtran.QTranBase.__init__": [[8, 69], ["torch.Module.__init__", "int", "numpy.prod", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "Exception", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "QTranBase", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "args", "=", "args", "\n", "\n", "self", ".", "n_agents", "=", "args", ".", "n_agents", "\n", "self", ".", "n_actions", "=", "args", ".", "n_actions", "\n", "self", ".", "state_dim", "=", "int", "(", "np", ".", "prod", "(", "args", ".", "state_shape", ")", ")", "\n", "self", ".", "arch", "=", "self", ".", "args", ".", "qtran_arch", "# QTran architecture", "\n", "\n", "self", ".", "embed_dim", "=", "args", ".", "mixing_embed_dim", "\n", "\n", "# Q(s,u)", "\n", "if", "self", ".", "arch", "==", "\"coma_critic\"", ":", "\n", "# Q takes [state, u] as input", "\n", "            ", "q_input_size", "=", "self", ".", "state_dim", "+", "(", "self", ".", "n_agents", "*", "self", ".", "n_actions", ")", "\n", "", "elif", "self", ".", "arch", "==", "\"qtran_paper\"", ":", "\n", "# Q takes [state, agent_action_observation_encodings]", "\n", "            ", "q_input_size", "=", "self", ".", "state_dim", "+", "self", ".", "args", ".", "rnn_hidden_dim", "+", "self", ".", "n_actions", "\n", "", "else", ":", "\n", "            ", "raise", "Exception", "(", "\"{} is not a valid QTran architecture\"", ".", "format", "(", "self", ".", "arch", ")", ")", "\n", "\n", "", "if", "self", ".", "args", ".", "network_size", "==", "\"small\"", ":", "\n", "            ", "self", ".", "Q", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "q_input_size", ",", "self", ".", "embed_dim", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "self", ".", "embed_dim", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "1", ")", ")", "\n", "\n", "# V(s)", "\n", "self", ".", "V", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "self", ".", "state_dim", ",", "self", ".", "embed_dim", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "self", ".", "embed_dim", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "1", ")", ")", "\n", "ae_input", "=", "self", ".", "args", ".", "rnn_hidden_dim", "+", "self", ".", "n_actions", "\n", "self", ".", "action_encoding", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "ae_input", ",", "ae_input", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "ae_input", ",", "ae_input", ")", ")", "\n", "", "elif", "self", ".", "args", ".", "network_size", "==", "\"big\"", ":", "\n", "            ", "self", ".", "Q", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "q_input_size", ",", "self", ".", "embed_dim", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "self", ".", "embed_dim", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "self", ".", "embed_dim", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "1", ")", ")", "\n", "# V(s)", "\n", "self", ".", "V", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "self", ".", "state_dim", ",", "self", ".", "embed_dim", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "self", ".", "embed_dim", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "self", ".", "embed_dim", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "1", ")", ")", "\n", "ae_input", "=", "self", ".", "args", ".", "rnn_hidden_dim", "+", "self", ".", "n_actions", "\n", "self", ".", "action_encoding", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "ae_input", ",", "ae_input", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "ae_input", ",", "ae_input", ")", ")", "\n", "", "else", ":", "\n", "            ", "assert", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.qtran.QTranBase.forward": [[70, 105], ["batch[].reshape", "qtran.QTranBase.Q", "batch[].reshape", "qtran.QTranBase.V", "torch.cat", "torch.cat", "torch.cat", "batch[].reshape", "actions.reshape.reshape.reshape", "hidden_states.reshape.reshape.reshape", "torch.cat", "torch.cat", "torch.cat", "qtran.QTranBase.action_encoding().reshape", "agent_state_action_encoding.sum.sum.sum", "torch.cat", "torch.cat", "torch.cat", "batch[].reshape", "actions.reshape.reshape.reshape", "qtran.QTranBase.action_encoding", "torch.cat.reshape"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "batch", ",", "hidden_states", ",", "actions", "=", "None", ")", ":", "\n", "        ", "bs", "=", "batch", ".", "batch_size", "\n", "ts", "=", "batch", ".", "max_seq_length", "\n", "\n", "states", "=", "batch", "[", "\"state\"", "]", ".", "reshape", "(", "bs", "*", "ts", ",", "self", ".", "state_dim", ")", "\n", "\n", "if", "self", ".", "arch", "==", "\"coma_critic\"", ":", "\n", "            ", "if", "actions", "is", "None", ":", "\n", "# Use the actions taken by the agents", "\n", "                ", "actions", "=", "batch", "[", "\"actions_onehot\"", "]", ".", "reshape", "(", "bs", "*", "ts", ",", "self", ".", "n_agents", "*", "self", ".", "n_actions", ")", "\n", "", "else", ":", "\n", "# It will arrive as (bs, ts, agents, actions), we need to reshape it", "\n", "                ", "actions", "=", "actions", ".", "reshape", "(", "bs", "*", "ts", ",", "self", ".", "n_agents", "*", "self", ".", "n_actions", ")", "\n", "", "inputs", "=", "th", ".", "cat", "(", "[", "states", ",", "actions", "]", ",", "dim", "=", "1", ")", "\n", "", "elif", "self", ".", "arch", "==", "\"qtran_paper\"", ":", "\n", "            ", "if", "actions", "is", "None", ":", "\n", "# Use the actions taken by the agents", "\n", "                ", "actions", "=", "batch", "[", "\"actions_onehot\"", "]", ".", "reshape", "(", "bs", "*", "ts", ",", "self", ".", "n_agents", ",", "self", ".", "n_actions", ")", "\n", "", "else", ":", "\n", "# It will arrive as (bs, ts, agents, actions), we need to reshape it", "\n", "                ", "actions", "=", "actions", ".", "reshape", "(", "bs", "*", "ts", ",", "self", ".", "n_agents", ",", "self", ".", "n_actions", ")", "\n", "\n", "", "hidden_states", "=", "hidden_states", ".", "reshape", "(", "bs", "*", "ts", ",", "self", ".", "n_agents", ",", "-", "1", ")", "\n", "agent_state_action_input", "=", "th", ".", "cat", "(", "[", "hidden_states", ",", "actions", "]", ",", "dim", "=", "2", ")", "\n", "agent_state_action_encoding", "=", "self", ".", "action_encoding", "(", "agent_state_action_input", ".", "reshape", "(", "bs", "*", "ts", "*", "self", ".", "n_agents", ",", "-", "1", ")", ")", ".", "reshape", "(", "bs", "*", "ts", ",", "self", ".", "n_agents", ",", "-", "1", ")", "\n", "agent_state_action_encoding", "=", "agent_state_action_encoding", ".", "sum", "(", "dim", "=", "1", ")", "# Sum across agents", "\n", "\n", "inputs", "=", "th", ".", "cat", "(", "[", "states", ",", "agent_state_action_encoding", "]", ",", "dim", "=", "1", ")", "\n", "\n", "", "q_outputs", "=", "self", ".", "Q", "(", "inputs", ")", "\n", "\n", "states", "=", "batch", "[", "\"state\"", "]", ".", "reshape", "(", "bs", "*", "ts", ",", "self", ".", "state_dim", ")", "\n", "v_outputs", "=", "self", ".", "V", "(", "states", ")", "\n", "\n", "return", "q_outputs", ",", "v_outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.qtran.QTranAlt.__init__": [[108, 151], ["torch.Module.__init__", "int", "numpy.prod", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "QTranAlt", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "args", "=", "args", "\n", "\n", "self", ".", "n_agents", "=", "args", ".", "n_agents", "\n", "self", ".", "n_actions", "=", "args", ".", "n_actions", "\n", "self", ".", "state_dim", "=", "int", "(", "np", ".", "prod", "(", "args", ".", "state_shape", ")", ")", "\n", "\n", "self", ".", "embed_dim", "=", "args", ".", "mixing_embed_dim", "\n", "\n", "# Q(s,-,u-i)", "\n", "# Q takes [state, u-i, i] as input", "\n", "q_input_size", "=", "self", ".", "state_dim", "+", "(", "self", ".", "n_agents", "*", "self", ".", "n_actions", ")", "+", "self", ".", "n_agents", "\n", "\n", "if", "self", ".", "args", ".", "network_size", "==", "\"small\"", ":", "\n", "            ", "self", ".", "Q", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "q_input_size", ",", "self", ".", "embed_dim", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "self", ".", "embed_dim", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "self", ".", "n_actions", ")", ")", "\n", "\n", "# V(s)", "\n", "self", ".", "V", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "self", ".", "state_dim", ",", "self", ".", "embed_dim", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "1", ")", ")", "\n", "", "elif", "self", ".", "args", ".", "network_size", "==", "\"big\"", ":", "\n", "# Adding another layer", "\n", "             ", "self", ".", "Q", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "q_input_size", ",", "self", ".", "embed_dim", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "self", ".", "embed_dim", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "self", ".", "embed_dim", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "self", ".", "n_actions", ")", ")", "\n", "# V(s)", "\n", "self", ".", "V", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "self", ".", "state_dim", ",", "self", ".", "embed_dim", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "self", ".", "embed_dim", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "1", ")", ")", "\n", "", "else", ":", "\n", "            ", "assert", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.qtran.QTranAlt.forward": [[152, 175], ["batch[].repeat().view", "torch.eye().unsqueeze().unsqueeze().repeat().view", "torch.eye().unsqueeze().unsqueeze().repeat().view", "torch.eye().unsqueeze().unsqueeze().repeat().view", "torch.cat", "torch.cat", "torch.cat", "qtran.QTranAlt.Q", "batch[].repeat().view", "qtran.QTranAlt.V", "batch[].repeat", "agent_mask.view().repeat.view().repeat.view().repeat", "masked_actions.view.view.view", "batch[].repeat", "torch.eye", "torch.eye", "torch.eye", "agent_mask.view().repeat.view().repeat.unsqueeze().unsqueeze", "torch.eye().unsqueeze().unsqueeze().repeat", "torch.eye().unsqueeze().unsqueeze().repeat", "torch.eye().unsqueeze().unsqueeze().repeat", "batch[].repeat", "agent_mask.view().repeat.view().repeat.view", "agent_mask.view().repeat.view().repeat.unsqueeze", "torch.eye().unsqueeze().unsqueeze", "torch.eye().unsqueeze().unsqueeze", "torch.eye().unsqueeze().unsqueeze", "torch.eye().unsqueeze", "torch.eye().unsqueeze", "torch.eye().unsqueeze", "torch.eye", "torch.eye", "torch.eye"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "batch", ",", "masked_actions", "=", "None", ")", ":", "\n", "        ", "bs", "=", "batch", ".", "batch_size", "\n", "ts", "=", "batch", ".", "max_seq_length", "\n", "# Repeat each state n_agents times", "\n", "repeated_states", "=", "batch", "[", "\"state\"", "]", ".", "repeat", "(", "1", ",", "1", ",", "self", ".", "n_agents", ")", ".", "view", "(", "-", "1", ",", "self", ".", "state_dim", ")", "\n", "\n", "if", "masked_actions", "is", "None", ":", "\n", "            ", "actions", "=", "batch", "[", "\"actions_onehot\"", "]", ".", "repeat", "(", "1", ",", "1", ",", "self", ".", "n_agents", ",", "1", ")", "\n", "agent_mask", "=", "(", "1", "-", "th", ".", "eye", "(", "self", ".", "n_agents", ",", "device", "=", "batch", ".", "device", ")", ")", "\n", "agent_mask", "=", "agent_mask", ".", "view", "(", "-", "1", ",", "1", ")", ".", "repeat", "(", "1", ",", "self", ".", "n_actions", ")", "#.view(self.n_agents, -1)", "\n", "masked_actions", "=", "actions", "*", "agent_mask", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "0", ")", "\n", "masked_actions", "=", "masked_actions", ".", "view", "(", "-", "1", ",", "self", ".", "n_agents", "*", "self", ".", "n_actions", ")", "\n", "\n", "", "agent_ids", "=", "th", ".", "eye", "(", "self", ".", "n_agents", ",", "device", "=", "batch", ".", "device", ")", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "0", ")", ".", "repeat", "(", "bs", ",", "ts", ",", "1", ",", "1", ")", ".", "view", "(", "-", "1", ",", "self", ".", "n_agents", ")", "\n", "\n", "inputs", "=", "th", ".", "cat", "(", "[", "repeated_states", ",", "masked_actions", ",", "agent_ids", "]", ",", "dim", "=", "1", ")", "\n", "\n", "q_outputs", "=", "self", ".", "Q", "(", "inputs", ")", "\n", "\n", "states", "=", "batch", "[", "\"state\"", "]", ".", "repeat", "(", "1", ",", "1", ",", "self", ".", "n_agents", ")", ".", "view", "(", "-", "1", ",", "self", ".", "state_dim", ")", "\n", "v_outputs", "=", "self", ".", "V", "(", "states", ")", "\n", "\n", "return", "q_outputs", ",", "v_outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.dmaq_qatten.DMAQ_QattenMixer.__init__": [[12, 24], ["torch.Module.__init__", "int", "dmaq_qatten_weight.Qatten_Weight", "dmaq_si_weight.DMAQ_SI_Weight", "numpy.prod"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "DMAQ_QattenMixer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "args", "=", "args", "\n", "self", ".", "n_agents", "=", "args", ".", "n_agents", "\n", "self", ".", "n_actions", "=", "args", ".", "n_actions", "\n", "self", ".", "state_dim", "=", "int", "(", "np", ".", "prod", "(", "args", ".", "state_shape", ")", ")", "\n", "self", ".", "action_dim", "=", "args", ".", "n_agents", "*", "self", ".", "n_actions", "\n", "self", ".", "state_action_dim", "=", "self", ".", "state_dim", "+", "self", ".", "action_dim", "+", "1", "\n", "\n", "self", ".", "attention_weight", "=", "Qatten_Weight", "(", "args", ")", "\n", "self", ".", "si_weight", "=", "DMAQ_SI_Weight", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.dmaq_qatten.DMAQ_QattenMixer.calc_v": [[25, 29], ["agent_qs.view.view.view", "torch.sum", "torch.sum", "torch.sum"], "methods", ["None"], ["", "def", "calc_v", "(", "self", ",", "agent_qs", ")", ":", "\n", "        ", "agent_qs", "=", "agent_qs", ".", "view", "(", "-", "1", ",", "self", ".", "n_agents", ")", "\n", "v_tot", "=", "th", ".", "sum", "(", "agent_qs", ",", "dim", "=", "-", "1", ")", "\n", "return", "v_tot", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.dmaq_qatten.DMAQ_QattenMixer.calc_adv": [[30, 46], ["states.reshape.reshape.reshape", "actions.reshape.reshape.reshape", "agent_qs.view.view.view", "max_q_i.view.view.view", "dmaq_qatten.DMAQ_QattenMixer.si_weight", "adv_w_final.view.view.view", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum"], "methods", ["None"], ["", "def", "calc_adv", "(", "self", ",", "agent_qs", ",", "states", ",", "actions", ",", "max_q_i", ")", ":", "\n", "        ", "states", "=", "states", ".", "reshape", "(", "-", "1", ",", "self", ".", "state_dim", ")", "\n", "actions", "=", "actions", ".", "reshape", "(", "-", "1", ",", "self", ".", "action_dim", ")", "\n", "agent_qs", "=", "agent_qs", ".", "view", "(", "-", "1", ",", "self", ".", "n_agents", ")", "\n", "max_q_i", "=", "max_q_i", ".", "view", "(", "-", "1", ",", "self", ".", "n_agents", ")", "\n", "\n", "adv_q", "=", "(", "agent_qs", "-", "max_q_i", ")", ".", "view", "(", "-", "1", ",", "self", ".", "n_agents", ")", ".", "detach", "(", ")", "\n", "\n", "adv_w_final", "=", "self", ".", "si_weight", "(", "states", ",", "actions", ")", "\n", "adv_w_final", "=", "adv_w_final", ".", "view", "(", "-", "1", ",", "self", ".", "n_agents", ")", "\n", "\n", "if", "self", ".", "args", ".", "is_minus_one", ":", "\n", "            ", "adv_tot", "=", "th", ".", "sum", "(", "adv_q", "*", "(", "adv_w_final", "-", "1.", ")", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "            ", "adv_tot", "=", "th", ".", "sum", "(", "adv_q", "*", "adv_w_final", ",", "dim", "=", "1", ")", "\n", "", "return", "adv_tot", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.dmaq_qatten.DMAQ_QattenMixer.calc": [[47, 54], ["dmaq_qatten.DMAQ_QattenMixer.calc_v", "dmaq_qatten.DMAQ_QattenMixer.calc_adv"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.dmaq_qatten.DMAQ_QattenMixer.calc_v", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.dmaq_qatten.DMAQ_QattenMixer.calc_adv"], ["", "def", "calc", "(", "self", ",", "agent_qs", ",", "states", ",", "actions", "=", "None", ",", "max_q_i", "=", "None", ",", "is_v", "=", "False", ")", ":", "\n", "        ", "if", "is_v", ":", "\n", "            ", "v_tot", "=", "self", ".", "calc_v", "(", "agent_qs", ")", "\n", "return", "v_tot", "\n", "", "else", ":", "\n", "            ", "adv_tot", "=", "self", ".", "calc_adv", "(", "agent_qs", ",", "states", ",", "actions", ",", "max_q_i", ")", "\n", "return", "adv_tot", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.dmaq_qatten.DMAQ_QattenMixer.forward": [[55, 73], ["agent_qs.view.view.size", "dmaq_qatten.DMAQ_QattenMixer.attention_weight", "v.view().repeat.view().repeat.view().repeat", "agent_qs.view.view.view", "dmaq_qatten.DMAQ_QattenMixer.calc", "dmaq_qatten.DMAQ_QattenMixer.view", "w_final.view", "max_q_i.view.view.view", "v.view().repeat.view().repeat.view"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.dmaq_qatten.DMAQ_QattenMixer.calc"], ["", "", "def", "forward", "(", "self", ",", "agent_qs", ",", "states", ",", "actions", "=", "None", ",", "max_q_i", "=", "None", ",", "is_v", "=", "False", ")", ":", "\n", "        ", "bs", "=", "agent_qs", ".", "size", "(", "0", ")", "\n", "\n", "w_final", ",", "v", ",", "attend_mag_regs", ",", "head_entropies", "=", "self", ".", "attention_weight", "(", "agent_qs", ",", "states", ",", "actions", ")", "\n", "w_final", "=", "w_final", ".", "view", "(", "-", "1", ",", "self", ".", "n_agents", ")", "+", "1e-10", "\n", "v", "=", "v", ".", "view", "(", "-", "1", ",", "1", ")", ".", "repeat", "(", "1", ",", "self", ".", "n_agents", ")", "\n", "v", "/=", "self", ".", "n_agents", "\n", "\n", "agent_qs", "=", "agent_qs", ".", "view", "(", "-", "1", ",", "self", ".", "n_agents", ")", "\n", "agent_qs", "=", "w_final", "*", "agent_qs", "+", "v", "\n", "if", "not", "is_v", ":", "\n", "            ", "max_q_i", "=", "max_q_i", ".", "view", "(", "-", "1", ",", "self", ".", "n_agents", ")", "\n", "max_q_i", "=", "w_final", "*", "max_q_i", "+", "v", "\n", "\n", "", "y", "=", "self", ".", "calc", "(", "agent_qs", ",", "states", ",", "actions", "=", "actions", ",", "max_q_i", "=", "max_q_i", ",", "is_v", "=", "is_v", ")", "\n", "v_tot", "=", "y", ".", "view", "(", "bs", ",", "-", "1", ",", "1", ")", "\n", "\n", "return", "v_tot", ",", "attend_mag_regs", ",", "head_entropies", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.qmix.QMixer.__init__": [[8, 40], ["torch.Module.__init__", "int", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Sequential", "torch.Sequential", "torch.Sequential", "numpy.prod", "getattr", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "getattr", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "getattr", "Exception", "Exception"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "QMixer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "args", "=", "args", "\n", "self", ".", "n_agents", "=", "args", ".", "n_agents", "\n", "self", ".", "state_dim", "=", "int", "(", "np", ".", "prod", "(", "args", ".", "state_shape", ")", ")", "\n", "\n", "self", ".", "embed_dim", "=", "args", ".", "mixing_embed_dim", "\n", "\n", "if", "getattr", "(", "args", ",", "\"hypernet_layers\"", ",", "1", ")", "==", "1", ":", "\n", "            ", "self", ".", "hyper_w_1", "=", "nn", ".", "Linear", "(", "self", ".", "state_dim", ",", "self", ".", "embed_dim", "*", "self", ".", "n_agents", ")", "\n", "self", ".", "hyper_w_final", "=", "nn", ".", "Linear", "(", "self", ".", "state_dim", ",", "self", ".", "embed_dim", ")", "\n", "", "elif", "getattr", "(", "args", ",", "\"hypernet_layers\"", ",", "1", ")", "==", "2", ":", "\n", "            ", "hypernet_embed", "=", "self", ".", "args", ".", "hypernet_embed", "\n", "self", ".", "hyper_w_1", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "self", ".", "state_dim", ",", "hypernet_embed", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "hypernet_embed", ",", "self", ".", "embed_dim", "*", "self", ".", "n_agents", ")", ")", "\n", "self", ".", "hyper_w_final", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "self", ".", "state_dim", ",", "hypernet_embed", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "hypernet_embed", ",", "self", ".", "embed_dim", ")", ")", "\n", "", "elif", "getattr", "(", "args", ",", "\"hypernet_layers\"", ",", "1", ")", ">", "2", ":", "\n", "            ", "raise", "Exception", "(", "\"Sorry >2 hypernet layers is not implemented!\"", ")", "\n", "", "else", ":", "\n", "            ", "raise", "Exception", "(", "\"Error setting number of hypernet layers.\"", ")", "\n", "\n", "# State dependent bias for hidden layer", "\n", "", "self", ".", "hyper_b_1", "=", "nn", ".", "Linear", "(", "self", ".", "state_dim", ",", "self", ".", "embed_dim", ")", "\n", "\n", "# V(s) instead of a bias for the last layers", "\n", "self", ".", "V", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "self", ".", "state_dim", ",", "self", ".", "embed_dim", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.qmix.QMixer.forward": [[41, 61], ["agent_qs.view.view.size", "states.reshape.reshape.reshape", "agent_qs.view.view.view", "torch.abs", "torch.abs", "torch.abs", "qmix.QMixer.hyper_b_1", "w1.view.view.view", "b1.view.view.view", "torch.elu", "torch.elu", "torch.elu", "torch.abs", "torch.abs", "torch.abs", "w_final.view.view.view", "qmix.QMixer.V().view", "y.view", "qmix.QMixer.hyper_w_1", "qmix.QMixer.hyper_w_final", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "qmix.QMixer.V"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "agent_qs", ",", "states", ")", ":", "\n", "        ", "bs", "=", "agent_qs", ".", "size", "(", "0", ")", "\n", "states", "=", "states", ".", "reshape", "(", "-", "1", ",", "self", ".", "state_dim", ")", "\n", "agent_qs", "=", "agent_qs", ".", "view", "(", "-", "1", ",", "1", ",", "self", ".", "n_agents", ")", "\n", "# First layer", "\n", "w1", "=", "th", ".", "abs", "(", "self", ".", "hyper_w_1", "(", "states", ")", ")", "\n", "b1", "=", "self", ".", "hyper_b_1", "(", "states", ")", "\n", "w1", "=", "w1", ".", "view", "(", "-", "1", ",", "self", ".", "n_agents", ",", "self", ".", "embed_dim", ")", "\n", "b1", "=", "b1", ".", "view", "(", "-", "1", ",", "1", ",", "self", ".", "embed_dim", ")", "\n", "hidden", "=", "F", ".", "elu", "(", "th", ".", "bmm", "(", "agent_qs", ",", "w1", ")", "+", "b1", ")", "\n", "# Second layer", "\n", "w_final", "=", "th", ".", "abs", "(", "self", ".", "hyper_w_final", "(", "states", ")", ")", "\n", "w_final", "=", "w_final", ".", "view", "(", "-", "1", ",", "self", ".", "embed_dim", ",", "1", ")", "\n", "# State-dependent bias", "\n", "v", "=", "self", ".", "V", "(", "states", ")", ".", "view", "(", "-", "1", ",", "1", ",", "1", ")", "\n", "# Compute final output", "\n", "y", "=", "th", ".", "bmm", "(", "hidden", ",", "w_final", ")", "+", "v", "\n", "# Reshape and return", "\n", "q_tot", "=", "y", ".", "view", "(", "bs", ",", "-", "1", ",", "1", ")", "\n", "return", "q_tot", "", "", "", ""]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.qatten.QattenMixer.__init__": [[10, 63], ["torch.Module.__init__", "int", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "numpy.prod", "getattr", "range", "torch.Sequential", "torch.Sequential", "torch.Sequential", "qatten.QattenMixer.selector_extractors.append", "torch.Linear", "torch.Linear", "torch.Linear", "getattr", "range", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "qatten.QattenMixer.key_extractors.append", "qatten.QattenMixer.key_extractors.append", "torch.Sequential", "torch.Sequential", "torch.Sequential", "qatten.QattenMixer.selector_extractors.append", "torch.Sequential", "torch.Sequential", "torch.Sequential", "getattr", "Exception", "Exception", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "qatten.QattenMixer.key_extractors.append", "qatten.QattenMixer.key_extractors.append", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "QattenMixer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "name", "=", "'qatten'", "\n", "self", ".", "args", "=", "args", "\n", "self", ".", "n_agents", "=", "args", ".", "n_agents", "\n", "self", ".", "state_dim", "=", "int", "(", "np", ".", "prod", "(", "args", ".", "state_shape", ")", ")", "\n", "self", ".", "unit_dim", "=", "args", ".", "unit_dim", "\n", "self", ".", "n_actions", "=", "args", ".", "n_actions", "\n", "self", ".", "sa_dim", "=", "self", ".", "state_dim", "+", "self", ".", "n_agents", "*", "self", ".", "n_actions", "\n", "self", ".", "n_head", "=", "args", ".", "n_head", "# attention head num", "\n", "\n", "self", ".", "embed_dim", "=", "args", ".", "mixing_embed_dim", "\n", "self", ".", "attend_reg_coef", "=", "args", ".", "attend_reg_coef", "\n", "\n", "self", ".", "key_extractors", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "selector_extractors", "=", "nn", ".", "ModuleList", "(", ")", "\n", "\n", "if", "getattr", "(", "args", ",", "\"hypernet_layers\"", ",", "1", ")", "==", "1", ":", "\n", "            ", "for", "i", "in", "range", "(", "self", ".", "n_head", ")", ":", "# multi-head attention", "\n", "                ", "self", ".", "selector_extractors", ".", "append", "(", "nn", ".", "Linear", "(", "self", ".", "state_dim", ",", "self", ".", "embed_dim", ",", "bias", "=", "False", ")", ")", "# query", "\n", "if", "self", ".", "args", ".", "nonlinear", ":", "# add qs", "\n", "                    ", "self", ".", "key_extractors", ".", "append", "(", "nn", ".", "Linear", "(", "self", ".", "unit_dim", "+", "1", ",", "self", ".", "embed_dim", ",", "bias", "=", "False", ")", ")", "# key", "\n", "", "else", ":", "\n", "                    ", "self", ".", "key_extractors", ".", "append", "(", "nn", ".", "Linear", "(", "self", ".", "unit_dim", ",", "self", ".", "embed_dim", ",", "bias", "=", "False", ")", ")", "# key", "\n", "", "", "if", "self", ".", "args", ".", "weighted_head", ":", "\n", "                ", "self", ".", "hyper_w_head", "=", "nn", ".", "Linear", "(", "self", ".", "state_dim", ",", "self", ".", "n_head", ")", "\n", "# self.hyper_w_head = nn.Linear(self.state_dim, self.embed_dim * self.n_head)", "\n", "", "", "elif", "getattr", "(", "args", ",", "\"hypernet_layers\"", ",", "1", ")", "==", "2", ":", "\n", "            ", "hypernet_embed", "=", "self", ".", "args", ".", "hypernet_embed", "\n", "for", "i", "in", "range", "(", "self", ".", "n_head", ")", ":", "# multi-head attention", "\n", "                ", "selector_nn", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "self", ".", "state_dim", ",", "hypernet_embed", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "hypernet_embed", ",", "self", ".", "embed_dim", ",", "bias", "=", "False", ")", ")", "\n", "self", ".", "selector_extractors", ".", "append", "(", "selector_nn", ")", "# query", "\n", "if", "self", ".", "args", ".", "nonlinear", ":", "# add qs", "\n", "                    ", "self", ".", "key_extractors", ".", "append", "(", "nn", ".", "Linear", "(", "self", ".", "unit_dim", "+", "1", ",", "self", ".", "embed_dim", ",", "bias", "=", "False", ")", ")", "# key", "\n", "", "else", ":", "\n", "                    ", "self", ".", "key_extractors", ".", "append", "(", "nn", ".", "Linear", "(", "self", ".", "unit_dim", ",", "self", ".", "embed_dim", ",", "bias", "=", "False", ")", ")", "# key", "\n", "", "", "if", "self", ".", "args", ".", "weighted_head", ":", "\n", "                ", "self", ".", "hyper_w_head", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "self", ".", "state_dim", ",", "hypernet_embed", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "hypernet_embed", ",", "self", ".", "n_head", ")", ")", "\n", "", "", "elif", "getattr", "(", "args", ",", "\"hypernet_layers\"", ",", "1", ")", ">", "2", ":", "\n", "            ", "raise", "Exception", "(", "\"Sorry >2 embednet layers is not implemented!\"", ")", "\n", "", "else", ":", "\n", "            ", "raise", "Exception", "(", "\"Error setting number of embednet layers.\"", ")", "\n", "\n", "", "if", "self", ".", "args", ".", "state_bias", ":", "\n", "# V(s) instead of a bias for the last layers", "\n", "            ", "self", ".", "V", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "self", ".", "state_dim", ",", "self", ".", "embed_dim", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.qatten.QattenMixer.forward": [[64, 132], ["agent_qs.view.view.size", "states.reshape.reshape.reshape", "torch.cat.reshape", "torch.cat.permute", "agent_qs.view.view.view", "zip", "torch.stack().sum.view", "torch.cat", "torch.cat", "torch.cat", "sel_ext", "torch.matmul", "torch.matmul", "torch.matmul", "torch.softmax", "torch.softmax", "torch.softmax", "head_qs.append", "head_attend_logits.append", "head_attend_weights.append", "qatten.QattenMixer.V().view", "sum", "k_ext", "curr_head_selector.view", "torch.stack().permute", "torch.stack().permute", "torch.stack().permute", "numpy.sqrt", "actions.reshape.reshape.reshape", "torch.abs", "torch.abs", "torch.abs", "w_head.view.view.view", "torch.stack().permute", "torch.stack().permute", "torch.stack().permute", "torch.abs", "torch.abs", "torch.abs", "w_head.view.view.view", "torch.stack().permute", "torch.stack().permute", "torch.stack().permute", "torch.stack().sum", "torch.stack().sum", "torch.stack().sum", "agent_qs.view.view.permute", "qatten.QattenMixer.V", "qatten.QattenMixer.hyper_w_head", "torch.stack().sum", "torch.stack().sum", "torch.stack().sum", "qatten.QattenMixer.hyper_w_head", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "agent_qs", ",", "states", ",", "actions", ")", ":", "\n", "        ", "bs", "=", "agent_qs", ".", "size", "(", "0", ")", "\n", "states", "=", "states", ".", "reshape", "(", "-", "1", ",", "self", ".", "state_dim", ")", "\n", "unit_states", "=", "states", "[", ":", ",", ":", "self", ".", "unit_dim", "*", "self", ".", "n_agents", "]", "# get agent own features from state", "\n", "unit_states", "=", "unit_states", ".", "reshape", "(", "-", "1", ",", "self", ".", "n_agents", ",", "self", ".", "unit_dim", ")", "\n", "unit_states", "=", "unit_states", ".", "permute", "(", "1", ",", "0", ",", "2", ")", "\n", "\n", "agent_qs", "=", "agent_qs", ".", "view", "(", "-", "1", ",", "1", ",", "self", ".", "n_agents", ")", "# agent_qs: (batch_size, 1, agent_num)", "\n", "\n", "if", "self", ".", "args", ".", "nonlinear", ":", "\n", "            ", "unit_states", "=", "th", ".", "cat", "(", "(", "unit_states", ",", "agent_qs", ".", "permute", "(", "2", ",", "0", ",", "1", ")", ")", ",", "dim", "=", "2", ")", "\n", "# states: (batch_size, state_dim)", "\n", "", "all_head_selectors", "=", "[", "sel_ext", "(", "states", ")", "for", "sel_ext", "in", "self", ".", "selector_extractors", "]", "\n", "# all_head_selectors: (head_num, batch_size, embed_dim)", "\n", "# unit_states: (agent_num, batch_size, unit_dim)", "\n", "all_head_keys", "=", "[", "[", "k_ext", "(", "enc", ")", "for", "enc", "in", "unit_states", "]", "for", "k_ext", "in", "self", ".", "key_extractors", "]", "\n", "# all_head_keys: (head_num, agent_num, batch_size, embed_dim)", "\n", "\n", "# calculate attention per head", "\n", "head_qs", "=", "[", "]", "\n", "head_attend_logits", "=", "[", "]", "\n", "head_attend_weights", "=", "[", "]", "\n", "for", "curr_head_keys", ",", "curr_head_selector", "in", "zip", "(", "all_head_keys", ",", "all_head_selectors", ")", ":", "\n", "# curr_head_keys: (agent_num, batch_size, embed_dim)", "\n", "# curr_head_selector: (batch_size, embed_dim)", "\n", "\n", "# (batch_size, 1, embed_dim) * (batch_size, embed_dim, agent_num)", "\n", "            ", "attend_logits", "=", "th", ".", "matmul", "(", "curr_head_selector", ".", "view", "(", "-", "1", ",", "1", ",", "self", ".", "embed_dim", ")", ",", "\n", "th", ".", "stack", "(", "curr_head_keys", ")", ".", "permute", "(", "1", ",", "2", ",", "0", ")", ")", "\n", "# attend_logits: (batch_size, 1, agent_num)", "\n", "# scale dot-products by size of key (from Attention is All You Need)", "\n", "scaled_attend_logits", "=", "attend_logits", "/", "np", ".", "sqrt", "(", "self", ".", "embed_dim", ")", "\n", "if", "self", ".", "args", ".", "mask_dead", ":", "\n", "# actions: (episode_batch, episode_length - 1, agent_num, 1)", "\n", "                ", "actions", "=", "actions", ".", "reshape", "(", "-", "1", ",", "1", ",", "self", ".", "n_agents", ")", "\n", "# actions: (batch_size, 1, agent_num)", "\n", "scaled_attend_logits", "[", "actions", "==", "0", "]", "=", "-", "99999999", "# action == 0 means the unit is dead", "\n", "", "attend_weights", "=", "F", ".", "softmax", "(", "scaled_attend_logits", ",", "dim", "=", "2", ")", "# (batch_size, 1, agent_num)", "\n", "# (batch_size, 1, agent_num) * (batch_size, 1, agent_num)", "\n", "head_q", "=", "(", "agent_qs", "*", "attend_weights", ")", ".", "sum", "(", "dim", "=", "2", ")", "\n", "head_qs", ".", "append", "(", "head_q", ")", "\n", "head_attend_logits", ".", "append", "(", "attend_logits", ")", "\n", "head_attend_weights", ".", "append", "(", "attend_weights", ")", "\n", "", "if", "self", ".", "args", ".", "state_bias", ":", "\n", "# State-dependent bias", "\n", "            ", "v", "=", "self", ".", "V", "(", "states", ")", ".", "view", "(", "-", "1", ",", "1", ")", "# v: (bs, 1)", "\n", "# head_qs: [head_num, bs, 1]", "\n", "if", "self", ".", "args", ".", "weighted_head", ":", "\n", "                ", "w_head", "=", "th", ".", "abs", "(", "self", ".", "hyper_w_head", "(", "states", ")", ")", "# w_head: (bs, head_num)", "\n", "w_head", "=", "w_head", ".", "view", "(", "-", "1", ",", "self", ".", "n_head", ",", "1", ")", "# w_head: (bs, head_num, 1)", "\n", "y", "=", "th", ".", "stack", "(", "head_qs", ")", ".", "permute", "(", "1", ",", "0", ",", "2", ")", "# head_qs: (head_num, bs, 1); y: (bs, head_num, 1)", "\n", "y", "=", "(", "w_head", "*", "y", ")", ".", "sum", "(", "dim", "=", "1", ")", "+", "v", "# y: (bs, 1)", "\n", "", "else", ":", "\n", "                ", "y", "=", "th", ".", "stack", "(", "head_qs", ")", ".", "sum", "(", "dim", "=", "0", ")", "+", "v", "# y: (bs, 1)", "\n", "", "", "else", ":", "\n", "            ", "if", "self", ".", "args", ".", "weighted_head", ":", "\n", "                ", "w_head", "=", "th", ".", "abs", "(", "self", ".", "hyper_w_head", "(", "states", ")", ")", "# w_head: (bs, head_num)", "\n", "w_head", "=", "w_head", ".", "view", "(", "-", "1", ",", "self", ".", "n_head", ",", "1", ")", "# w_head: (bs, head_num, 1)", "\n", "y", "=", "th", ".", "stack", "(", "head_qs", ")", ".", "permute", "(", "1", ",", "0", ",", "2", ")", "# head_qs: (head_num, bs, 1); y: (bs, head_num, 1)", "\n", "y", "=", "(", "w_head", "*", "y", ")", ".", "sum", "(", "dim", "=", "1", ")", "# y: (bs, 1)", "\n", "", "else", ":", "\n", "                ", "y", "=", "th", ".", "stack", "(", "head_qs", ")", ".", "sum", "(", "dim", "=", "0", ")", "# y: (bs, 1)", "\n", "# Reshape and return", "\n", "", "", "q_tot", "=", "y", ".", "view", "(", "bs", ",", "-", "1", ",", "1", ")", "\n", "# regularize magnitude of attention logits", "\n", "attend_mag_regs", "=", "self", ".", "attend_reg_coef", "*", "sum", "(", "(", "logit", "**", "2", ")", ".", "mean", "(", ")", "for", "logit", "in", "head_attend_logits", ")", "\n", "head_entropies", "=", "[", "(", "-", "(", "(", "probs", "+", "1e-8", ")", ".", "log", "(", ")", "*", "probs", ")", ".", "squeeze", "(", "dim", "=", "1", ")", ".", "sum", "(", "1", ")", ".", "mean", "(", ")", ")", "for", "probs", "in", "head_attend_weights", "]", "\n", "return", "q_tot", ",", "attend_mag_regs", ",", "head_entropies", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.qmix_central_no_hyper.QMixerCentralFF.__init__": [[8, 33], ["torch.Module.__init__", "int", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "numpy.prod", "torch.Linear", "torch.Linear", "torch.Linear", "non_lin", "torch.Linear", "torch.Linear", "torch.Linear", "non_lin", "torch.Linear", "torch.Linear", "torch.Linear", "non_lin", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "non_lin", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "QMixerCentralFF", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "args", "=", "args", "\n", "\n", "self", ".", "n_agents", "=", "args", ".", "n_agents", "\n", "self", ".", "state_dim", "=", "int", "(", "np", ".", "prod", "(", "args", ".", "state_shape", ")", ")", "\n", "\n", "self", ".", "input_dim", "=", "self", ".", "n_agents", "*", "self", ".", "args", ".", "central_action_embed", "+", "self", ".", "state_dim", "\n", "self", ".", "embed_dim", "=", "args", ".", "central_mixing_embed_dim", "\n", "\n", "non_lin", "=", "nn", ".", "ReLU", "\n", "\n", "self", ".", "net", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "self", ".", "input_dim", ",", "self", ".", "embed_dim", ")", ",", "\n", "non_lin", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "self", ".", "embed_dim", ")", ",", "\n", "non_lin", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "self", ".", "embed_dim", ")", ",", "\n", "non_lin", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "1", ")", ")", "\n", "\n", "# V(s) instead of a bias for the last layers", "\n", "self", ".", "V", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "self", ".", "state_dim", ",", "self", ".", "embed_dim", ")", ",", "\n", "non_lin", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.qmix_central_no_hyper.QMixerCentralFF.forward": [[34, 48], ["agent_qs.reshape.reshape.size", "states.reshape.reshape.reshape", "agent_qs.reshape.reshape.reshape", "torch.cat", "torch.cat", "torch.cat", "qmix_central_no_hyper.QMixerCentralFF.net", "qmix_central_no_hyper.QMixerCentralFF.V", "y.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "agent_qs", ",", "states", ")", ":", "\n", "        ", "bs", "=", "agent_qs", ".", "size", "(", "0", ")", "\n", "states", "=", "states", ".", "reshape", "(", "-", "1", ",", "self", ".", "state_dim", ")", "\n", "agent_qs", "=", "agent_qs", ".", "reshape", "(", "-", "1", ",", "self", ".", "n_agents", "*", "self", ".", "args", ".", "central_action_embed", ")", "\n", "\n", "inputs", "=", "th", ".", "cat", "(", "[", "states", ",", "agent_qs", "]", ",", "dim", "=", "1", ")", "\n", "\n", "advs", "=", "self", ".", "net", "(", "inputs", ")", "\n", "vs", "=", "self", ".", "V", "(", "states", ")", "\n", "\n", "y", "=", "advs", "+", "vs", "\n", "\n", "q_tot", "=", "y", ".", "view", "(", "bs", ",", "-", "1", ",", "1", ")", "\n", "return", "q_tot", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.qmix_central_attention.QMixerCentralAtten.__init__": [[7, 35], ["torch.Module.__init__", "int", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "numpy.prod", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "QMixerCentralAtten", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "args", "=", "args", "\n", "\n", "self", ".", "n_agents", "=", "args", ".", "n_agents", "\n", "self", ".", "state_dim", "=", "int", "(", "np", ".", "prod", "(", "args", ".", "state_shape", ")", ")", "\n", "\n", "self", ".", "input_dim", "=", "self", ".", "n_agents", "*", "self", ".", "args", ".", "central_action_embed", "\n", "self", ".", "embed_dim", "=", "args", ".", "central_mixing_embed_dim", "\n", "\n", "# assert self.embed_dim % self.n_agents == 0", "\n", "self", ".", "heads", "=", "self", ".", "embed_dim", "\n", "\n", "self", ".", "atten_layer", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "self", ".", "state_dim", ",", "self", ".", "args", ".", "hypernet_embed", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "args", ".", "hypernet_embed", ",", "self", ".", "heads", "*", "self", ".", "args", ".", "n_agents", ")", ")", "\n", "\n", "self", ".", "net", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "self", ".", "embed_dim", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "self", ".", "embed_dim", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "1", ")", ")", "\n", "\n", "# V(s) instead of a bias for the last layers", "\n", "self", ".", "V", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "self", ".", "state_dim", ",", "self", ".", "embed_dim", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.qmix_central_attention.QMixerCentralAtten.forward": [[36, 53], ["agent_qs.reshape.reshape.size", "states.reshape.reshape.reshape", "agent_qs.reshape.reshape.reshape", "qmix_central_attention.QMixerCentralAtten.atten_layer", "atten_output.softmax.softmax.reshape", "atten_output.softmax.softmax.softmax", "torch.bmm().reshape", "torch.bmm().reshape", "qmix_central_attention.QMixerCentralAtten.net", "qmix_central_attention.QMixerCentralAtten.V", "y.view", "torch.bmm", "torch.bmm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "agent_qs", ",", "states", ")", ":", "\n", "        ", "bs", "=", "agent_qs", ".", "size", "(", "0", ")", "\n", "states", "=", "states", ".", "reshape", "(", "-", "1", ",", "self", ".", "state_dim", ")", "\n", "agent_qs", "=", "agent_qs", ".", "reshape", "(", "-", "1", ",", "self", ".", "args", ".", "central_action_embed", ",", "self", ".", "n_agents", ")", "\n", "\n", "atten_output", "=", "self", ".", "atten_layer", "(", "states", ")", "\n", "atten_output", "=", "atten_output", ".", "reshape", "(", "-", "1", ",", "self", ".", "n_agents", ",", "self", ".", "heads", ")", "\n", "atten_output", "=", "atten_output", ".", "softmax", "(", "dim", "=", "1", ")", "\n", "\n", "inputs", "=", "th", ".", "bmm", "(", "agent_qs", ",", "atten_output", ")", ".", "reshape", "(", "-", "1", ",", "self", ".", "heads", ")", "\n", "\n", "advs", "=", "self", ".", "net", "(", "inputs", ")", "\n", "vs", "=", "self", ".", "V", "(", "states", ")", "\n", "\n", "y", "=", "advs", "+", "vs", "\n", "q_tot", "=", "y", ".", "view", "(", "bs", ",", "-", "1", ",", "1", ")", "\n", "return", "q_tot", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.shaq.SHAQMixer.__init__": [[8, 41], ["torch.Module.__init__", "int", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Sequential", "torch.Sequential", "torch.Sequential", "numpy.prod", "getattr", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "getattr", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "getattr", "Exception", "Exception"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "SHAQMixer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "args", "=", "args", "\n", "self", ".", "n_agents", "=", "args", ".", "n_agents", "\n", "self", ".", "state_dim", "=", "int", "(", "np", ".", "prod", "(", "args", ".", "state_shape", ")", ")", "\n", "self", ".", "embed_dim", "=", "args", ".", "mixing_embed_dim", "\n", "self", ".", "n_actions", "=", "args", ".", "n_actions", "\n", "\n", "self", ".", "sample_size", "=", "args", ".", "sample_size", "\n", "\n", "if", "getattr", "(", "args", ",", "\"hypernet_layers\"", ",", "1", ")", "==", "1", ":", "\n", "            ", "self", ".", "hyper_w_1", "=", "nn", ".", "Linear", "(", "self", ".", "state_dim", ",", "self", ".", "embed_dim", "*", "2", ")", "\n", "self", ".", "hyper_w_final", "=", "nn", ".", "Linear", "(", "self", ".", "state_dim", ",", "self", ".", "embed_dim", ")", "\n", "", "elif", "getattr", "(", "args", ",", "\"hypernet_layers\"", ",", "1", ")", "==", "2", ":", "\n", "            ", "hypernet_embed", "=", "self", ".", "args", ".", "hypernet_embed", "\n", "self", ".", "hyper_w_1", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "self", ".", "state_dim", ",", "hypernet_embed", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "hypernet_embed", ",", "self", ".", "embed_dim", "*", "2", ")", ")", "\n", "self", ".", "hyper_w_final", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "self", ".", "state_dim", ",", "hypernet_embed", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "hypernet_embed", ",", "self", ".", "embed_dim", ")", ")", "\n", "", "elif", "getattr", "(", "args", ",", "\"hypernet_layers\"", ",", "1", ")", ">", "2", ":", "\n", "            ", "raise", "Exception", "(", "\"Sorry >2 hypernet layers is not implemented!\"", ")", "\n", "", "else", ":", "\n", "            ", "raise", "Exception", "(", "\"Error setting number of hypernet layers.\"", ")", "\n", "\n", "# State dependent bias for hidden layer", "\n", "", "self", ".", "hyper_b_1", "=", "nn", ".", "Linear", "(", "self", ".", "state_dim", ",", "self", ".", "embed_dim", ")", "\n", "# State dependent bias for the last layers", "\n", "self", ".", "hyper_b_2", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "self", ".", "state_dim", ",", "self", ".", "embed_dim", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.shaq.SHAQMixer.sample_grandcoalitions": [[43, 101], ["torch.tril", "torch.tril", "torch.tril", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "individual_map.contiguous().view.contiguous().view.scatter_", "individual_map.contiguous().view.contiguous().view.contiguous().view", "torch.matmul", "torch.matmul", "torch.matmul", "torch.zeros_like().cuda", "torch.zeros_like().cuda", "torch.zeros_like().cuda", "torch.arange().cuda", "torch.arange().cuda", "torch.arange().cuda", "grand_coalitions.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view", "torch.ones().cuda", "torch.ones().cuda", "torch.ones().cuda", "torch.multinomial.contiguous().view", "grand_coalitions.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.reshape", "torch.ones().cuda", "torch.ones().cuda", "torch.ones().cuda", "torch.zeros", "torch.zeros", "torch.zeros", "individual_map.contiguous().view.contiguous().view.contiguous", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "grand_coalitions_pos_alter.flatten", "torch.arange", "torch.arange", "torch.arange", "grand_coalitions.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous", "torch.ones", "torch.ones", "torch.ones", "torch.multinomial.contiguous", "grand_coalitions_pos_alter.flatten", "torch.ones", "torch.ones", "torch.ones", "grand_coalitions.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze().expand", "torch.arange", "torch.arange", "torch.arange", "grand_coalitions.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda"], ["", "def", "sample_grandcoalitions", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "\"\"\"\n        E.g. batch_size = 2, n_agents = 3:\n\n        >>> grand_coalitions_pos\n        tensor([[2, 0, 1],\n                [1, 2, 0]])\n\n        >>> subcoalition_map\n        tensor([[[[1., 1., 1.],\n                [1., 0., 0.],\n                [1., 1., 0.]]],\n\n                [[[1., 1., 0.],\n                [1., 1., 1.],\n                [1., 0., 0.]]]])\n\n        >>> individual_map\n        tensor([[[[0., 0., 1.],\n                [1., 0., 0.],\n                [0., 1., 0.]]],\n\n                [[[0., 1., 0.],\n                [0., 0., 1.],\n                [1., 0., 0.]]]])\n        \"\"\"", "\n", "seq_set", "=", "th", ".", "tril", "(", "th", ".", "ones", "(", "self", ".", "n_agents", ",", "self", ".", "n_agents", ")", ".", "cuda", "(", ")", ",", "diagonal", "=", "0", ",", "out", "=", "None", ")", "\n", "grand_coalitions_pos", "=", "th", ".", "multinomial", "(", "th", ".", "ones", "(", "batch_size", "*", "self", ".", "sample_size", ",", "\n", "self", ".", "n_agents", ")", ".", "cuda", "(", ")", "/", "self", ".", "n_agents", ",", "\n", "self", ".", "n_agents", ",", "\n", "replacement", "=", "False", ")", "\n", "individual_map", "=", "th", ".", "zeros", "(", "batch_size", "*", "self", ".", "sample_size", "*", "self", ".", "n_agents", ",", "self", ".", "n_agents", ")", ".", "cuda", "(", ")", "\n", "individual_map", ".", "scatter_", "(", "1", ",", "grand_coalitions_pos", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "1", ")", ",", "1", ")", "\n", "individual_map", "=", "individual_map", ".", "contiguous", "(", ")", ".", "view", "(", "batch_size", ",", "self", ".", "sample_size", ",", "self", ".", "n_agents", ",", "self", ".", "n_agents", ")", "\n", "subcoalition_map", "=", "th", ".", "matmul", "(", "individual_map", ",", "seq_set", ")", "\n", "\n", "\n", "# FIX: construct the grand coalition (in sequence by agent_idx) from the grand_coalitions_pos (e.g., pos_idx <- grand_coalitions_pos[agent_idx])", "\n", "# grand_coalitions = []", "\n", "# for grand_coalition_pos in grand_coalitions_pos:", "\n", "#     grand_coalition = th.zeros_like(grand_coalition_pos)", "\n", "#     for agent, pos in enumerate(grand_coalition_pos):", "\n", "#         grand_coalition[pos] = agent", "\n", "#     grand_coalitions.append(grand_coalition)", "\n", "# grand_coalitions = th.stack(grand_coalitions, dim=0).to(self.device)", "\n", "offset", "=", "(", "th", ".", "arange", "(", "batch_size", "*", "self", ".", "sample_size", ")", "*", "self", ".", "n_agents", ")", ".", "reshape", "(", "-", "1", ",", "1", ")", ".", "cuda", "(", ")", "\n", "grand_coalitions_pos_alter", "=", "grand_coalitions_pos", "+", "offset", "\n", "grand_coalitions", "=", "th", ".", "zeros_like", "(", "grand_coalitions_pos_alter", ".", "flatten", "(", ")", ")", ".", "cuda", "(", ")", "\n", "grand_coalitions", "[", "grand_coalitions_pos_alter", ".", "flatten", "(", ")", "]", "=", "th", ".", "arange", "(", "batch_size", "*", "self", ".", "sample_size", "*", "self", ".", "n_agents", ")", ".", "cuda", "(", ")", "\n", "grand_coalitions", "=", "grand_coalitions", ".", "reshape", "(", "batch_size", "*", "self", ".", "sample_size", ",", "self", ".", "n_agents", ")", "-", "offset", "\n", "\n", "grand_coalitions", "=", "grand_coalitions", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "batch_size", "*", "self", ".", "sample_size", ",", "\n", "self", ".", "n_agents", ",", "\n", "self", ".", "n_agents", ")", ".", "contiguous", "(", ")", ".", "view", "(", "batch_size", ",", "\n", "self", ".", "sample_size", ",", "\n", "self", ".", "n_agents", ",", "\n", "self", ".", "n_agents", ")", "# shape = (b, n_s, n, n)", "\n", "return", "subcoalition_map", ",", "individual_map", ",", "grand_coalitions", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.shaq.SHAQMixer.get_alpha_estimate": [[103, 166], ["states.size", "shaq.SHAQMixer.sample_grandcoalitions", "grand_coalitions.unsqueeze().expand.unsqueeze().expand.unsqueeze().expand", "subcoalition_map_no_i.unsqueeze().expand.unsqueeze().expand.unsqueeze().expand", "agent_qs.unsqueeze().unsqueeze().expand().gather", "subcoalition_map_no_i.unsqueeze().expand.unsqueeze().expand.sum().clone", "agent_qs.unsqueeze().expand_as", "agent_qs_coalition_norm_vec.contiguous().view", "agent_qs.unsqueeze().expand_as.contiguous().view", "states.unsqueeze().unsqueeze().expand().contiguous().view", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.abs", "torch.abs", "torch.abs", "shaq.SHAQMixer.hyper_b_1", "w1.view.view.view", "b1.view.view.view", "torch.elu", "torch.elu", "torch.elu", "torch.abs", "torch.abs", "torch.abs", "w_final.view.view.view", "shaq.SHAQMixer.hyper_b_2().view", "torch.abs().view", "torch.abs().view", "torch.abs().view", "alpha_estimates.mean.mean.mean", "agent_qs_coalition.sum", "shaq.SHAQMixer.hyper_w_1", "shaq.SHAQMixer.hyper_w_final", "torch.bmm", "torch.bmm", "torch.bmm", "grand_coalitions.unsqueeze().expand.unsqueeze().expand.unsqueeze", "subcoalition_map_no_i.unsqueeze().expand.unsqueeze().expand.unsqueeze", "agent_qs.unsqueeze().unsqueeze().expand", "subcoalition_map_no_i.unsqueeze().expand.unsqueeze().expand.sum", "agent_qs.unsqueeze", "agent_qs_coalition_norm_vec.contiguous", "agent_qs.unsqueeze().expand_as.contiguous", "states.unsqueeze().unsqueeze().expand().contiguous", "torch.cat", "torch.cat", "torch.cat", "torch.bmm", "torch.bmm", "torch.bmm", "shaq.SHAQMixer.hyper_b_2", "torch.abs", "torch.abs", "torch.abs", "subcoalition_map_no_i.unsqueeze().expand.unsqueeze().expand.sum", "agent_qs.unsqueeze().unsqueeze", "states.unsqueeze().unsqueeze().expand", "agent_qs.unsqueeze", "states.unsqueeze().unsqueeze", "states.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.sqddpg.SQDDPGMixer.sample_grandcoalitions"], ["", "def", "get_alpha_estimate", "(", "self", ",", "states", ",", "agent_qs", ")", ":", "\n", "        ", "batch_size", "=", "states", ".", "size", "(", "0", ")", "\n", "\n", "# get subcoalition map including agent i", "\n", "subcoalition_map", ",", "individual_map", ",", "grand_coalitions", "=", "self", ".", "sample_grandcoalitions", "(", "batch_size", ")", "# shape = (b, n_s, n, n)", "\n", "\n", "# reshape the grand coalition map for rearranging the sequence of actions of agents", "\n", "grand_coalitions", "=", "grand_coalitions", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "batch_size", ",", "\n", "self", ".", "sample_size", ",", "\n", "self", ".", "n_agents", ",", "\n", "self", ".", "n_agents", ",", "\n", "1", ")", "# shape = (b, n_s, n, n, 1)", "\n", "\n", "# remove agent i from the subcloation map", "\n", "subcoalition_map_no_i", "=", "subcoalition_map", "-", "individual_map", "\n", "subcoalition_map_no_i", "=", "subcoalition_map_no_i", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "batch_size", ",", "\n", "self", ".", "sample_size", ",", "\n", "self", ".", "n_agents", ",", "\n", "self", ".", "n_agents", ",", "\n", "1", ")", "# shape = (b, n_s, n, n, 1)", "\n", "\n", "# reshape actions for further process on coalitions", "\n", "reshape_agent_qs", "=", "agent_qs", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "batch_size", ",", "\n", "self", ".", "sample_size", ",", "\n", "self", ".", "n_agents", ",", "\n", "self", ".", "n_agents", ",", "\n", "1", ")", ".", "gather", "(", "3", ",", "grand_coalitions", ")", "# shape = (b, n, 1) -> (b, 1, 1, n, 1) -> (b, n_s, n, n, 1)", "\n", "\n", "# get actions of its coalition memebers for each agent", "\n", "agent_qs_coalition", "=", "reshape_agent_qs", "*", "subcoalition_map_no_i", "# shape = (b, n_s, n, n, 1)", "\n", "\n", "# get actions vector of its coalition members for each agent", "\n", "subcoalition_map_no_i_", "=", "subcoalition_map_no_i", ".", "sum", "(", "dim", "=", "-", "2", ")", ".", "clone", "(", ")", "\n", "subcoalition_map_no_i_", "[", "subcoalition_map_no_i", ".", "sum", "(", "dim", "=", "-", "2", ")", "==", "0", "]", "=", "1", "\n", "agent_qs_coalition_norm_vec", "=", "agent_qs_coalition", ".", "sum", "(", "dim", "=", "-", "2", ")", "/", "subcoalition_map_no_i_", "# shape = (b, n_s, n, 1)", "\n", "\n", "# get action vector of each agent", "\n", "agent_qs_individual", "=", "agent_qs", ".", "unsqueeze", "(", "1", ")", ".", "expand_as", "(", "agent_qs_coalition_norm_vec", ")", "# shape = (b, n_s, n, 1)", "\n", "\n", "reshape_agent_qs_coalition_norm_vec", "=", "agent_qs_coalition_norm_vec", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "1", ")", "# shape = (b*n_s*n, 1)", "\n", "reshape_agent_qs_individual", "=", "agent_qs_individual", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "1", ")", "# shape = (b*n_s*n, 1)", "\n", "reshape_states", "=", "states", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "batch_size", ",", "self", ".", "sample_size", ",", "self", ".", "n_agents", ",", "self", ".", "state_dim", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "self", ".", "state_dim", ")", "# shape = (b*n_s*n, s)", "\n", "\n", "inputs", "=", "th", ".", "cat", "(", "[", "reshape_agent_qs_coalition_norm_vec", ",", "reshape_agent_qs_individual", "]", ",", "dim", "=", "-", "1", ")", ".", "unsqueeze", "(", "1", ")", "# shape = (b*n_s*n, 1, 2*1)", "\n", "\n", "# First layer", "\n", "w1", "=", "th", ".", "abs", "(", "self", ".", "hyper_w_1", "(", "reshape_states", ")", ")", "\n", "b1", "=", "self", ".", "hyper_b_1", "(", "reshape_states", ")", "\n", "w1", "=", "w1", ".", "view", "(", "-", "1", ",", "2", ",", "self", ".", "embed_dim", ")", "\n", "b1", "=", "b1", ".", "view", "(", "-", "1", ",", "1", ",", "self", ".", "embed_dim", ")", "\n", "hidden", "=", "F", ".", "elu", "(", "th", ".", "bmm", "(", "inputs", ",", "w1", ")", "+", "b1", ")", "\n", "# Second layer", "\n", "w_final", "=", "th", ".", "abs", "(", "self", ".", "hyper_w_final", "(", "reshape_states", ")", ")", "\n", "w_final", "=", "w_final", ".", "view", "(", "-", "1", ",", "self", ".", "embed_dim", ",", "1", ")", "\n", "b2", "=", "self", ".", "hyper_b_2", "(", "reshape_states", ")", ".", "view", "(", "-", "1", ",", "1", ",", "1", ")", "\n", "# Compute final output", "\n", "y", "=", "th", ".", "bmm", "(", "hidden", ",", "w_final", ")", "+", "b2", "\n", "# Reshape and return", "\n", "alpha_estimates", "=", "th", ".", "abs", "(", "y", ")", ".", "view", "(", "batch_size", ",", "self", ".", "sample_size", ",", "self", ".", "n_agents", ")", "# shape = (b, n_s, n)", "\n", "# normalise among the sample_size", "\n", "alpha_estimates", "=", "alpha_estimates", ".", "mean", "(", "dim", "=", "1", ")", "# shape = (b, n)", "\n", "\n", "return", "alpha_estimates", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.shaq.SHAQMixer.forward": [[168, 186], ["states.contiguous().view", "agent_qs.unsqueeze().contiguous().view", "torch.sum", "torch.sum", "torch.sum", "states.contiguous", "agent_qs.unsqueeze().contiguous", "shaq.SHAQMixer.get_alpha_estimate", "alpha_estimates.contiguous().view.contiguous().view.contiguous().view", "states.size", "states.size", "torch.ones_like", "torch.ones_like", "torch.ones_like", "agent_qs.unsqueeze", "alpha_estimates.contiguous().view.contiguous().view.contiguous"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.shaq.SHAQMixer.get_alpha_estimate"], ["", "def", "forward", "(", "self", ",", "states", ",", "actions", ",", "agent_qs", ",", "max_filter", ",", "target", ",", "manual_alpha_estimates", "=", "None", ")", ":", "\n", "# agent_qs, max_filter = (b, t, n)", "\n", "        ", "reshape_states", "=", "states", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "self", ".", "state_dim", ")", "\n", "reshape_agent_qs", "=", "agent_qs", ".", "unsqueeze", "(", "-", "1", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "self", ".", "n_agents", ",", "1", ")", "\n", "if", "target", ":", "\n", "            ", "return", "th", ".", "sum", "(", "agent_qs", ",", "dim", "=", "2", ",", "keepdim", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "if", "manual_alpha_estimates", "==", "None", ":", "\n", "                ", "alpha_estimates", "=", "self", ".", "get_alpha_estimate", "(", "reshape_states", ",", "reshape_agent_qs", ")", "\n", "# restrict the range of alpha to [1, \\infty)", "\n", "alpha_estimates", "=", "alpha_estimates", "+", "1", "\n", "alpha_estimates", "=", "alpha_estimates", ".", "contiguous", "(", ")", ".", "view", "(", "states", ".", "size", "(", "0", ")", ",", "states", ".", "size", "(", "1", ")", ",", "self", ".", "n_agents", ")", "\n", "", "else", ":", "\n", "                ", "alpha_estimates", "=", "manual_alpha_estimates", "*", "th", ".", "ones_like", "(", "max_filter", ")", "\n", "# agent with non-max action will be given 1", "\n", "", "non_max_filter", "=", "1", "-", "max_filter", "\n", "# if the agent with the max-action then alpha = 1. Otherwise, the agent will use the learned alpha", "\n", "return", "(", "(", "alpha_estimates", "*", "non_max_filter", "+", "max_filter", ")", "*", "agent_qs", ")", ".", "sum", "(", "dim", "=", "2", ",", "keepdim", "=", "True", ")", ",", "alpha_estimates", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.dmaq_si_weight.DMAQ_SI_Weight.__init__": [[9, 59], ["torch.Module.__init__", "int", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "range", "numpy.prod", "getattr", "dmaq_si_weight.DMAQ_SI_Weight.key_extractors.append", "dmaq_si_weight.DMAQ_SI_Weight.agents_extractors.append", "dmaq_si_weight.DMAQ_SI_Weight.action_extractors.append", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "getattr", "dmaq_si_weight.DMAQ_SI_Weight.key_extractors.append", "dmaq_si_weight.DMAQ_SI_Weight.agents_extractors.append", "dmaq_si_weight.DMAQ_SI_Weight.action_extractors.append", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "getattr", "dmaq_si_weight.DMAQ_SI_Weight.key_extractors.append", "dmaq_si_weight.DMAQ_SI_Weight.agents_extractors.append", "dmaq_si_weight.DMAQ_SI_Weight.action_extractors.append", "Exception", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "DMAQ_SI_Weight", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "args", "=", "args", "\n", "self", ".", "n_agents", "=", "args", ".", "n_agents", "\n", "self", ".", "n_actions", "=", "args", ".", "n_actions", "\n", "self", ".", "state_dim", "=", "int", "(", "np", ".", "prod", "(", "args", ".", "state_shape", ")", ")", "\n", "self", ".", "action_dim", "=", "args", ".", "n_agents", "*", "self", ".", "n_actions", "\n", "self", ".", "state_action_dim", "=", "self", ".", "state_dim", "+", "self", ".", "action_dim", "\n", "\n", "self", ".", "num_kernel", "=", "args", ".", "num_kernel", "\n", "\n", "self", ".", "key_extractors", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "agents_extractors", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "action_extractors", "=", "nn", ".", "ModuleList", "(", ")", "\n", "\n", "adv_hypernet_embed", "=", "self", ".", "args", ".", "adv_hypernet_embed", "\n", "for", "i", "in", "range", "(", "self", ".", "num_kernel", ")", ":", "# multi-head attention", "\n", "            ", "if", "getattr", "(", "args", ",", "\"adv_hypernet_layers\"", ",", "1", ")", "==", "1", ":", "\n", "                ", "self", ".", "key_extractors", ".", "append", "(", "nn", ".", "Linear", "(", "self", ".", "state_dim", ",", "1", ")", ")", "# key", "\n", "self", ".", "agents_extractors", ".", "append", "(", "nn", ".", "Linear", "(", "self", ".", "state_dim", ",", "self", ".", "n_agents", ")", ")", "# agent", "\n", "self", ".", "action_extractors", ".", "append", "(", "nn", ".", "Linear", "(", "self", ".", "state_action_dim", ",", "self", ".", "n_agents", ")", ")", "# action", "\n", "", "elif", "getattr", "(", "args", ",", "\"adv_hypernet_layers\"", ",", "1", ")", "==", "2", ":", "\n", "                ", "self", ".", "key_extractors", ".", "append", "(", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "self", ".", "state_dim", ",", "adv_hypernet_embed", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "adv_hypernet_embed", ",", "1", ")", ")", ")", "# key", "\n", "self", ".", "agents_extractors", ".", "append", "(", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "self", ".", "state_dim", ",", "adv_hypernet_embed", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "adv_hypernet_embed", ",", "self", ".", "n_agents", ")", ")", ")", "# agent", "\n", "self", ".", "action_extractors", ".", "append", "(", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "self", ".", "state_action_dim", ",", "adv_hypernet_embed", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "adv_hypernet_embed", ",", "self", ".", "n_agents", ")", ")", ")", "# action", "\n", "", "elif", "getattr", "(", "args", ",", "\"adv_hypernet_layers\"", ",", "1", ")", "==", "3", ":", "\n", "                ", "self", ".", "key_extractors", ".", "append", "(", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "self", ".", "state_dim", ",", "adv_hypernet_embed", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "adv_hypernet_embed", ",", "adv_hypernet_embed", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "adv_hypernet_embed", ",", "1", ")", ")", ")", "# key", "\n", "self", ".", "agents_extractors", ".", "append", "(", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "self", ".", "state_dim", ",", "adv_hypernet_embed", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "adv_hypernet_embed", ",", "adv_hypernet_embed", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "adv_hypernet_embed", ",", "self", ".", "n_agents", ")", ")", ")", "# agent", "\n", "self", ".", "action_extractors", ".", "append", "(", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "self", ".", "state_action_dim", ",", "adv_hypernet_embed", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "adv_hypernet_embed", ",", "adv_hypernet_embed", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "adv_hypernet_embed", ",", "self", ".", "n_agents", ")", ")", ")", "# action", "\n", "", "else", ":", "\n", "                ", "raise", "Exception", "(", "\"Error setting number of adv hypernet layers.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.dmaq_si_weight.DMAQ_SI_Weight.forward": [[60, 82], ["states.reshape.reshape.reshape", "actions.reshape.reshape.reshape", "torch.cat", "torch.cat", "torch.cat", "zip", "torch.stack", "torch.stack", "torch.stack", "torch.sum.view", "torch.sum", "torch.sum", "torch.sum", "k_ext", "k_ext", "sel_ext", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "head_attend_weights.append", "torch.abs().repeat", "torch.abs().repeat", "torch.abs().repeat", "torch.abs", "torch.abs", "torch.abs"], "methods", ["None"], ["", "", "", "def", "forward", "(", "self", ",", "states", ",", "actions", ")", ":", "\n", "        ", "states", "=", "states", ".", "reshape", "(", "-", "1", ",", "self", ".", "state_dim", ")", "\n", "actions", "=", "actions", ".", "reshape", "(", "-", "1", ",", "self", ".", "action_dim", ")", "\n", "data", "=", "th", ".", "cat", "(", "[", "states", ",", "actions", "]", ",", "dim", "=", "1", ")", "\n", "\n", "all_head_key", "=", "[", "k_ext", "(", "states", ")", "for", "k_ext", "in", "self", ".", "key_extractors", "]", "\n", "all_head_agents", "=", "[", "k_ext", "(", "states", ")", "for", "k_ext", "in", "self", ".", "agents_extractors", "]", "\n", "all_head_action", "=", "[", "sel_ext", "(", "data", ")", "for", "sel_ext", "in", "self", ".", "action_extractors", "]", "\n", "\n", "head_attend_weights", "=", "[", "]", "\n", "for", "curr_head_key", ",", "curr_head_agents", ",", "curr_head_action", "in", "zip", "(", "all_head_key", ",", "all_head_agents", ",", "all_head_action", ")", ":", "\n", "            ", "x_key", "=", "th", ".", "abs", "(", "curr_head_key", ")", ".", "repeat", "(", "1", ",", "self", ".", "n_agents", ")", "+", "1e-10", "\n", "x_agents", "=", "F", ".", "sigmoid", "(", "curr_head_agents", ")", "\n", "x_action", "=", "F", ".", "sigmoid", "(", "curr_head_action", ")", "\n", "weights", "=", "x_key", "*", "x_agents", "*", "x_action", "\n", "head_attend_weights", ".", "append", "(", "weights", ")", "\n", "\n", "", "head_attend", "=", "th", ".", "stack", "(", "head_attend_weights", ",", "dim", "=", "1", ")", "\n", "head_attend", "=", "head_attend", ".", "view", "(", "-", "1", ",", "self", ".", "num_kernel", ",", "self", ".", "n_agents", ")", "\n", "head_attend", "=", "th", ".", "sum", "(", "head_attend", ",", "dim", "=", "1", ")", "\n", "\n", "return", "head_attend", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.vdn.VDNMixer.__init__": [[6, 8], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "VDNMixer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.vdn.VDNMixer.forward": [[9, 11], ["torch.sum", "torch.sum"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "agent_qs", ",", "batch", ")", ":", "\n", "        ", "return", "th", ".", "sum", "(", "agent_qs", ",", "dim", "=", "2", ",", "keepdim", "=", "True", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.sqddpg.QMixerCentralFF.__init__": [[11, 35], ["torch.Module.__init__", "int", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "numpy.prod", "torch.Linear", "torch.Linear", "torch.Linear", "non_lin", "torch.Linear", "torch.Linear", "torch.Linear", "non_lin", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "non_lin", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "QMixerCentralFF", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "args", "=", "args", "\n", "\n", "self", ".", "n_agents", "=", "args", ".", "n_agents", "\n", "self", ".", "state_dim", "=", "int", "(", "np", ".", "prod", "(", "args", ".", "state_shape", ")", ")", "\n", "\n", "self", ".", "input_dim", "=", "self", ".", "n_agents", "*", "self", ".", "args", ".", "n_actions", "+", "self", ".", "state_dim", "\n", "self", ".", "embed_dim", "=", "args", ".", "central_mixing_embed_dim", "\n", "self", ".", "n_actions", "=", "args", ".", "n_actions", "\n", "\n", "non_lin", "=", "nn", ".", "ReLU", "\n", "\n", "self", ".", "net", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "self", ".", "input_dim", ",", "self", ".", "embed_dim", ")", ",", "\n", "non_lin", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "self", ".", "embed_dim", ")", ",", "\n", "non_lin", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "1", ")", ")", "\n", "\n", "# V(s) instead of a bias for the last layers", "\n", "self", ".", "V", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "self", ".", "state_dim", ",", "self", ".", "embed_dim", ")", ",", "\n", "non_lin", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.sqddpg.QMixerCentralFF.forward": [[36, 50], ["agent_qs.reshape.reshape.size", "states.reshape.reshape.reshape", "agent_qs.reshape.reshape.reshape", "torch.cat", "torch.cat", "torch.cat", "sqddpg.QMixerCentralFF.net", "sqddpg.QMixerCentralFF.V", "y.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "agent_qs", ",", "states", ")", ":", "\n", "        ", "bs", "=", "agent_qs", ".", "size", "(", "0", ")", "\n", "states", "=", "states", ".", "reshape", "(", "-", "1", ",", "self", ".", "state_dim", ")", "\n", "agent_qs", "=", "agent_qs", ".", "reshape", "(", "-", "1", ",", "self", ".", "n_agents", "*", "self", ".", "n_actions", ")", "\n", "\n", "inputs", "=", "th", ".", "cat", "(", "[", "states", ",", "agent_qs", "]", ",", "dim", "=", "1", ")", "\n", "\n", "advs", "=", "self", ".", "net", "(", "inputs", ")", "\n", "vs", "=", "self", ".", "V", "(", "states", ")", "\n", "\n", "y", "=", "advs", "+", "vs", "\n", "\n", "q_tot", "=", "y", ".", "view", "(", "bs", ",", "-", "1", ",", "1", ")", "\n", "return", "q_tot", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.sqddpg.SQDDPGMixer.__init__": [[54, 71], ["torch.Module.__init__", "int", "numpy.prod", "sqddpg.QMixerCentralFF", "qmix.QMixer", "ValueError"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "SQDDPGMixer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "args", "=", "args", "\n", "self", ".", "n_agents", "=", "args", ".", "n_agents", "\n", "self", ".", "state_dim", "=", "int", "(", "np", ".", "prod", "(", "args", ".", "state_shape", ")", ")", "\n", "# self.embed_dim = args.mixing_embed_dim", "\n", "self", ".", "n_actions", "=", "args", ".", "n_actions", "\n", "\n", "self", ".", "sample_size", "=", "args", ".", "sample_size", "\n", "\n", "if", "args", ".", "marginal_contribution_type", "==", "'ff'", ":", "\n", "            ", "self", ".", "marginal_contribution", "=", "QMixerCentralFF", "(", "args", ")", "\n", "", "elif", "args", ".", "marginal_contribution_type", "==", "'qmix'", ":", "\n", "            ", "self", ".", "marginal_contribution", "=", "QMixer", "(", "args", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Marginal contribution type {} not recognised.\"", ".", "format", "(", "args", ".", "marginal_contribution_type", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.sqddpg.SQDDPGMixer.sample_grandcoalitions": [[73, 131], ["torch.tril", "torch.tril", "torch.tril", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "individual_map.contiguous().view.contiguous().view.scatter_", "individual_map.contiguous().view.contiguous().view.contiguous().view", "torch.matmul", "torch.matmul", "torch.matmul", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.arange", "torch.arange", "torch.arange", "grand_coalitions.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view", "torch.ones().cuda", "torch.ones().cuda", "torch.ones().cuda", "torch.multinomial.contiguous().view", "grand_coalitions_pos_alter.flatten", "grand_coalitions.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.reshape", "torch.ones().cuda", "torch.ones().cuda", "torch.ones().cuda", "torch.zeros", "torch.zeros", "torch.zeros", "individual_map.contiguous().view.contiguous().view.contiguous", "grand_coalitions_pos_alter.flatten", "grand_coalitions.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous", "torch.ones", "torch.ones", "torch.ones", "torch.multinomial.contiguous", "torch.arange", "torch.arange", "torch.arange", "torch.ones", "torch.ones", "torch.ones", "grand_coalitions.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze().expand", "grand_coalitions.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.cuda"], ["", "", "def", "sample_grandcoalitions", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "\"\"\"\n        E.g. batch_size = 2, n_agents = 3:\n\n        >>> grand_coalitions_pos\n        tensor([[2, 0, 1],\n                [1, 2, 0]])\n\n        >>> subcoalition_map\n        tensor([[[[1., 1., 1.],\n                [1., 0., 0.],\n                [1., 1., 0.]]],\n\n                [[[1., 1., 0.],\n                [1., 1., 1.],\n                [1., 0., 0.]]]])\n\n        >>> individual_map\n        tensor([[[[0., 0., 1.],\n                [1., 0., 0.],\n                [0., 1., 0.]]],\n\n                [[[0., 1., 0.],\n                [0., 0., 1.],\n                [1., 0., 0.]]]])\n        \"\"\"", "\n", "seq_set", "=", "th", ".", "tril", "(", "th", ".", "ones", "(", "self", ".", "n_agents", ",", "self", ".", "n_agents", ")", ".", "cuda", "(", ")", ",", "diagonal", "=", "0", ",", "out", "=", "None", ")", "\n", "grand_coalitions_pos", "=", "th", ".", "multinomial", "(", "th", ".", "ones", "(", "batch_size", "*", "self", ".", "sample_size", ",", "\n", "self", ".", "n_agents", ")", ".", "cuda", "(", ")", "/", "self", ".", "n_agents", ",", "\n", "self", ".", "n_agents", ",", "\n", "replacement", "=", "False", ")", "\n", "individual_map", "=", "th", ".", "zeros", "(", "batch_size", "*", "self", ".", "sample_size", "*", "self", ".", "n_agents", ",", "self", ".", "n_agents", ")", ".", "cuda", "(", ")", "\n", "individual_map", ".", "scatter_", "(", "1", ",", "grand_coalitions_pos", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "1", ")", ",", "1", ")", "\n", "individual_map", "=", "individual_map", ".", "contiguous", "(", ")", ".", "view", "(", "batch_size", ",", "self", ".", "sample_size", ",", "self", ".", "n_agents", ",", "self", ".", "n_agents", ")", "\n", "subcoalition_map", "=", "th", ".", "matmul", "(", "individual_map", ",", "seq_set", ")", "\n", "\n", "\n", "# FIX: construct the grand coalition (in sequence by agent_idx) from the grand_coalitions_pos (e.g., pos_idx <- grand_coalitions_pos[agent_idx])", "\n", "# grand_coalitions = []", "\n", "# for grand_coalition_pos in grand_coalitions_pos:", "\n", "#     grand_coalition = th.zeros_like(grand_coalition_pos)", "\n", "#     for agent, pos in enumerate(grand_coalition_pos):", "\n", "#         grand_coalition[pos] = agent", "\n", "#     grand_coalitions.append(grand_coalition)", "\n", "# grand_coalitions = th.stack(grand_coalitions, dim=0).to(self.device)", "\n", "offset", "=", "(", "th", ".", "arange", "(", "batch_size", "*", "self", ".", "sample_size", ")", "*", "self", ".", "n_", ")", ".", "reshape", "(", "-", "1", ",", "1", ")", "\n", "grand_coalitions_pos_alter", "=", "grand_coalitions_pos", "+", "offset", "\n", "grand_coalitions", "=", "th", ".", "zeros_like", "(", "grand_coalitions_pos_alter", ".", "flatten", "(", ")", ")", "\n", "grand_coalitions", "[", "grand_coalitions_pos_alter", ".", "flatten", "(", ")", "]", "=", "th", ".", "arange", "(", "batch_size", "*", "self", ".", "sample_size", "*", "self", ".", "n_", ")", "\n", "grand_coalitions", "=", "grand_coalitions", ".", "reshape", "(", "batch_size", "*", "self", ".", "sample_size", ",", "self", ".", "n_", ")", "-", "offset", "\n", "\n", "grand_coalitions", "=", "grand_coalitions", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "batch_size", "*", "self", ".", "sample_size", ",", "\n", "self", ".", "n_agents", ",", "\n", "self", ".", "n_agents", ")", ".", "contiguous", "(", ")", ".", "view", "(", "batch_size", ",", "\n", "self", ".", "sample_size", ",", "\n", "self", ".", "n_agents", ",", "\n", "self", ".", "n_agents", ")", "# shape = (b, n_s, n, n)", "\n", "return", "subcoalition_map", ",", "individual_map", ",", "grand_coalitions", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.sqddpg.SQDDPGMixer.get_shapley_values": [[133, 184], ["states.size", "sqddpg.SQDDPGMixer.sample_grandcoalitions", "grand_coalitions.unsqueeze().expand.unsqueeze().expand.unsqueeze().expand", "subcoalition_map_no_i.unsqueeze().expand.unsqueeze().expand.unsqueeze().expand", "individual_map.unsqueeze().expand.unsqueeze().expand.unsqueeze().expand", "agent_qs.unsqueeze().unsqueeze().expand().gather", "agent_qs_coalition.contiguous().view", "states.unsqueeze().unsqueeze().expand().contiguous().view", "sqddpg.SQDDPGMixer.marginal_contribution", "marginal_contributions.contiguous().view.contiguous().view.contiguous().view", "marginal_contributions.contiguous().view.contiguous().view.mean", "agent_qs_coalition_no_i.detach", "grand_coalitions.unsqueeze().expand.unsqueeze().expand.unsqueeze", "subcoalition_map_no_i.unsqueeze().expand.unsqueeze().expand.unsqueeze", "individual_map.unsqueeze().expand.unsqueeze().expand.unsqueeze", "agent_qs.unsqueeze().unsqueeze().expand", "agent_qs_coalition.contiguous", "states.unsqueeze().unsqueeze().expand().contiguous", "marginal_contributions.contiguous().view.contiguous().view.contiguous", "agent_qs.unsqueeze().unsqueeze", "states.unsqueeze().unsqueeze().expand", "agent_qs.unsqueeze", "states.unsqueeze().unsqueeze", "states.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.sqddpg.SQDDPGMixer.sample_grandcoalitions"], ["", "def", "get_shapley_values", "(", "self", ",", "states", ",", "agent_qs", ")", ":", "\n", "        ", "batch_size", "=", "states", ".", "size", "(", "0", ")", "\n", "\n", "# get subcoalition map including agent i", "\n", "subcoalition_map", ",", "individual_map", ",", "grand_coalitions", "=", "self", ".", "sample_grandcoalitions", "(", "batch_size", ")", "# shape = (b, n_s, n, n)", "\n", "\n", "# reshape the grand coalition map for rearranging the sequence of actions of agents", "\n", "grand_coalitions", "=", "grand_coalitions", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "batch_size", ",", "\n", "self", ".", "sample_size", ",", "\n", "self", ".", "n_agents", ",", "\n", "self", ".", "n_agents", ",", "\n", "self", ".", "n_actions", ")", "# shape = (b, n_s, n, n, 1)", "\n", "\n", "# remove agent i from the subcoalition map", "\n", "subcoalition_map_no_i", "=", "subcoalition_map", "-", "individual_map", "\n", "subcoalition_map_no_i", "=", "subcoalition_map_no_i", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "batch_size", ",", "\n", "self", ".", "sample_size", ",", "\n", "self", ".", "n_agents", ",", "\n", "self", ".", "n_agents", ",", "\n", "self", ".", "n_actions", ")", "# shape = (b, n_s, n, n, 1)", "\n", "individual_map", "=", "individual_map", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "batch_size", ",", "\n", "self", ".", "sample_size", ",", "\n", "self", ".", "n_agents", ",", "\n", "self", ".", "n_agents", ",", "\n", "self", ".", "n_actions", ")", "# shape = (b, n_s, n, n, 1)", "\n", "\n", "# reshape actions for further process on coalitions", "\n", "reshape_agent_qs", "=", "agent_qs", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "batch_size", ",", "\n", "self", ".", "sample_size", ",", "\n", "self", ".", "n_agents", ",", "\n", "self", ".", "n_agents", ",", "\n", "self", ".", "n_actions", ")", ".", "gather", "(", "3", ",", "grand_coalitions", ")", "# shape = (b, n, 1) -> (b, 1, 1, n, 1) -> (b, n_s, n, n, 1)", "\n", "\n", "# get actions of its coalition memebers for each agent", "\n", "agent_qs_coalition_no_i", "=", "reshape_agent_qs", "*", "subcoalition_map_no_i", "# shape = (b, n_s, n, n, 1)", "\n", "agent_qs_coalition_i", "=", "reshape_agent_qs", "*", "individual_map", "# shape = (b, n_s, n, n, 1)", "\n", "\n", "# keep u_{-i} no gradient backprop", "\n", "agent_qs_coalition", "=", "agent_qs_coalition_no_i", ".", "detach", "(", ")", "+", "agent_qs_coalition_i", "# shape = (b, n_s, n, n, 1)", "\n", "# agent_qs_coalition = agent_qs_coalition_no_i + agent_qs_coalition_i # shape = (b, n_s, n, n, 1)", "\n", "\n", "reshape_agent_qs_coalition", "=", "agent_qs_coalition", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "self", ".", "n_agents", "*", "self", ".", "n_actions", ")", "# shape = (b*n_s*n, n)", "\n", "reshape_states", "=", "states", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "batch_size", ",", "self", ".", "sample_size", ",", "self", ".", "n_agents", ",", "self", ".", "state_dim", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "self", ".", "state_dim", ")", "# shape = (b*n_s*n, s)", "\n", "\n", "# inputs = th.cat([reshape_agent_qs_coalition, reshape_states], dim=-1) # shape = (b*n_s*n, n+s)", "\n", "\n", "marginal_contributions", "=", "self", ".", "marginal_contribution", "(", "reshape_agent_qs_coalition", ",", "reshape_states", ")", "# shape = (b*n_s*n, 1)", "\n", "marginal_contributions", "=", "marginal_contributions", ".", "contiguous", "(", ")", ".", "view", "(", "batch_size", ",", "self", ".", "sample_size", ",", "self", ".", "n_agents", ")", "# shape = (b, n_s, n)", "\n", "shapley_values", "=", "marginal_contributions", ".", "mean", "(", "dim", "=", "1", ")", "# shape = (b, n)", "\n", "\n", "return", "shapley_values", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.sqddpg.SQDDPGMixer.forward": [[186, 195], ["states.contiguous().view", "agent_qs.unsqueeze().contiguous().view", "sqddpg.SQDDPGMixer.get_shapley_values", "shapley_values.contiguous().view.contiguous().view.contiguous().view", "states.size", "states.size", "states.contiguous", "agent_qs.unsqueeze().contiguous", "shapley_values.contiguous().view.contiguous().view.contiguous", "agent_qs.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.sqddpg.SQDDPGMixer.get_shapley_values"], ["", "def", "forward", "(", "self", ",", "states", ",", "agent_qs", ")", ":", "\n", "# agent_qs = (b, t, n)", "\n", "        ", "reshape_states", "=", "states", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "self", ".", "state_dim", ")", "\n", "reshape_agent_qs", "=", "agent_qs", ".", "unsqueeze", "(", "-", "1", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "self", ".", "n_agents", ",", "self", ".", "n_actions", ")", "\n", "\n", "shapley_values", "=", "self", ".", "get_shapley_values", "(", "reshape_states", ",", "reshape_agent_qs", ")", "# shape = (b*t, n)", "\n", "shapley_values", "=", "shapley_values", ".", "contiguous", "(", ")", ".", "view", "(", "states", ".", "size", "(", "0", ")", ",", "states", ".", "size", "(", "1", ")", ",", "self", ".", "n_agents", ")", "# shape = (b, t, n)", "\n", "\n", "return", "shapley_values", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.envs.multiagentenv.MultiAgentEnv.step": [[3, 6], ["None"], "methods", ["None"], ["    ", "def", "step", "(", "self", ",", "actions", ")", ":", "\n", "        ", "\"\"\" Returns reward, terminated, info \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.envs.multiagentenv.MultiAgentEnv.get_obs": [[7, 10], ["None"], "methods", ["None"], ["", "def", "get_obs", "(", "self", ")", ":", "\n", "        ", "\"\"\" Returns all agent observations in a list \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.envs.multiagentenv.MultiAgentEnv.get_obs_agent": [[11, 14], ["None"], "methods", ["None"], ["", "def", "get_obs_agent", "(", "self", ",", "agent_id", ")", ":", "\n", "        ", "\"\"\" Returns observation for agent_id \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.envs.multiagentenv.MultiAgentEnv.get_obs_size": [[15, 18], ["None"], "methods", ["None"], ["", "def", "get_obs_size", "(", "self", ")", ":", "\n", "        ", "\"\"\" Returns the shape of the observation \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.envs.multiagentenv.MultiAgentEnv.get_state": [[19, 21], ["None"], "methods", ["None"], ["", "def", "get_state", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.envs.multiagentenv.MultiAgentEnv.get_state_size": [[22, 25], ["None"], "methods", ["None"], ["", "def", "get_state_size", "(", "self", ")", ":", "\n", "        ", "\"\"\" Returns the shape of the state\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.envs.multiagentenv.MultiAgentEnv.get_avail_actions": [[26, 28], ["None"], "methods", ["None"], ["", "def", "get_avail_actions", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.envs.multiagentenv.MultiAgentEnv.get_avail_agent_actions": [[29, 32], ["None"], "methods", ["None"], ["", "def", "get_avail_agent_actions", "(", "self", ",", "agent_id", ")", ":", "\n", "        ", "\"\"\" Returns the available actions for agent_id \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.envs.multiagentenv.MultiAgentEnv.get_total_actions": [[33, 37], ["None"], "methods", ["None"], ["", "def", "get_total_actions", "(", "self", ")", ":", "\n", "        ", "\"\"\" Returns the total number of actions an agent could ever take \"\"\"", "\n", "# TODO: This is only suitable for a discrete 1 dimensional action space for each agent", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.envs.multiagentenv.MultiAgentEnv.get_stats": [[38, 40], ["None"], "methods", ["None"], ["", "def", "get_stats", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.envs.multiagentenv.MultiAgentEnv.get_agg_stats": [[42, 44], ["None"], "methods", ["None"], ["", "def", "get_agg_stats", "(", "self", ",", "stats", ")", ":", "\n", "        ", "return", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.envs.multiagentenv.MultiAgentEnv.reset": [[45, 48], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "\"\"\" Returns initial observations and states\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.envs.multiagentenv.MultiAgentEnv.render": [[49, 51], ["None"], "methods", ["None"], ["", "def", "render", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.envs.multiagentenv.MultiAgentEnv.close": [[52, 54], ["None"], "methods", ["None"], ["", "def", "close", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.envs.multiagentenv.MultiAgentEnv.seed": [[55, 57], ["None"], "methods", ["None"], ["", "def", "seed", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.envs.multiagentenv.MultiAgentEnv.save_replay": [[58, 60], ["None"], "methods", ["None"], ["", "def", "save_replay", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.envs.multiagentenv.MultiAgentEnv.get_env_info": [[61, 68], ["multiagentenv.MultiAgentEnv.get_state_size", "multiagentenv.MultiAgentEnv.get_obs_size", "multiagentenv.MultiAgentEnv.get_total_actions"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.get_state_size", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.get_obs_size", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.get_total_actions"], ["", "def", "get_env_info", "(", "self", ")", ":", "\n", "        ", "env_info", "=", "{", "\"state_shape\"", ":", "self", ".", "get_state_size", "(", ")", ",", "\n", "\"obs_shape\"", ":", "self", ".", "get_obs_size", "(", ")", ",", "\n", "\"n_actions\"", ":", "self", ".", "get_total_actions", "(", ")", ",", "\n", "\"n_agents\"", ":", "self", ".", "n_agents", ",", "\n", "\"episode_limit\"", ":", "self", ".", "episode_limit", "}", "\n", "return", "env_info", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.envs.__init__.env_fn": [[9, 12], ["env"], "function", ["None"], []], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt.__init__": [[61, 173], ["isinstance", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "numpy.asarray", "numpy.asarray", "numpy.zeros", "getattr", "getattr", "numpy.asarray", "numpy.asarray", "numpy.asarray", "getattr", "getattr", "getattr", "getattr", "float", "getattr", "getattr", "getattr", "numpy.zeros", "numpy.ones", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.ones", "stag_hunt.StagHunt.reset", "utils.dict2namedtuple.convert", "getattr", "getattr", "getattr", "getattr", "numpy.asarray", "getattr", "int", "getattr"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner.reset", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.dict2namedtuple.convert"], ["def", "__init__", "(", "self", ",", "batch_size", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "# Unpack arguments from sacred", "\n", "        ", "args", "=", "kwargs", "[", "\"env_args\"", "]", "\n", "if", "isinstance", "(", "args", ",", "dict", ")", ":", "\n", "            ", "args", "=", "convert", "(", "args", ")", "\n", "", "self", ".", "args", "=", "args", "\n", "self", ".", "print_caught_prey", "=", "getattr", "(", "args", ",", "\"print_caught_prey\"", ",", "False", ")", "\n", "self", ".", "print_frozen_agents", "=", "getattr", "(", "args", ",", "\"print_frozen_agents\"", ",", "False", ")", "\n", "\n", "# Add-on for graph interface", "\n", "self", ".", "state_as_graph", "=", "args", ".", "state_as_graph", "\n", "if", "self", ".", "state_as_graph", ":", "\n", "            ", "self", ".", "absolute_distance", "=", "getattr", "(", "args", ",", "\"absolute_distance\"", ",", "False", ")", "\n", "self", ".", "normalise_distance", "=", "getattr", "(", "args", ",", "\"normalise_distance\"", ",", "False", ")", "\n", "self", ".", "add_walls", "=", "getattr", "(", "args", ",", "\"add_walls\"", ",", "False", ")", "\n", "self", ".", "prey_relational", "=", "getattr", "(", "args", ",", "\"prey_relational\"", ",", "True", ")", "\n", "\n", "# Add-on for goat-hunts (which like to climb mountains)", "\n", "", "self", ".", "mountain_slope", "=", "getattr", "(", "args", ",", "\"mountain_slope\"", ",", "0.0", ")", "\n", "self", ".", "capture_conditions", "=", "getattr", "(", "args", ",", "\"capture_conditions\"", ",", "[", "0", ",", "1", "]", ")", "\n", "self", ".", "mountain_spawn", "=", "getattr", "(", "args", ",", "\"mountain_spawn\"", ",", "False", ")", "\n", "self", ".", "mountain_agent_row", "=", "getattr", "(", "args", ",", "\"mountain_agent_row\"", ",", "-", "1", ")", "\n", "\n", "# Downwards compatibility of batch_mode", "\n", "self", ".", "batch_mode", "=", "batch_size", "is", "not", "None", "\n", "self", ".", "batch_size", "=", "batch_size", "if", "self", ".", "batch_mode", "else", "1", "\n", "\n", "# Define the environment grid", "\n", "self", ".", "truncate_episodes", "=", "getattr", "(", "args", ",", "\"truncate_episodes\"", ",", "True", ")", "\n", "self", ".", "observe_ids", "=", "getattr", "(", "args", ",", "\"observe_ids\"", ",", "False", ")", "\n", "self", ".", "intersection_global_view", "=", "getattr", "(", "args", ",", "\"intersection_global_view\"", ",", "False", ")", "\n", "self", ".", "intersection_unknown", "=", "getattr", "(", "args", ",", "\"intersection_unknown\"", ",", "False", ")", "\n", "self", ".", "directed_observations", "=", "getattr", "(", "args", ",", "\"directed_observations\"", ",", "False", ")", "\n", "self", ".", "directed_cone_narrow", "=", "getattr", "(", "args", ",", "\"directed_cone_narrow\"", ",", "True", ")", "\n", "self", ".", "directed_exta_actions", "=", "getattr", "(", "args", ",", "\"directed_exta_actions\"", ",", "True", ")", "\n", "self", ".", "random_ghosts", "=", "getattr", "(", "args", ",", "\"random_ghosts\"", ",", "False", ")", "\n", "self", ".", "random_ghosts_prob", "=", "getattr", "(", "args", ",", "\"random_ghosts_prob\"", ",", "0.5", ")", "\n", "self", ".", "random_ghosts_mul", "=", "getattr", "(", "args", ",", "\"random_ghosts_mul\"", ",", "-", "1.0", ")", "\n", "self", ".", "random_ghosts_random_indicator", "=", "getattr", "(", "args", ",", "\"random_ghosts_indicator\"", ",", "False", ")", "\n", "self", ".", "observe_state", "=", "getattr", "(", "args", ",", "\"observe_state\"", ",", "False", ")", "\n", "self", ".", "observe_walls", "=", "getattr", "(", "args", ",", "\"observe_walls\"", ",", "True", ")", "\n", "self", ".", "observe_one_hot", "=", "getattr", "(", "args", ",", "\"observe_one_hot\"", ",", "False", ")", "\n", "self", ".", "n_feats", "=", "(", "5", "if", "self", ".", "observe_one_hot", "else", "3", ")", "+", "(", "1", "if", "self", ".", "random_ghosts", "else", "0", ")", "\n", "self", ".", "toroidal", "=", "args", ".", "toroidal", "\n", "shape", "=", "args", ".", "world_shape", "\n", "self", ".", "x_max", ",", "self", ".", "y_max", "=", "shape", "\n", "self", ".", "state_size", "=", "self", ".", "x_max", "*", "self", ".", "y_max", "*", "self", ".", "n_feats", "\n", "self", ".", "env_max", "=", "np", ".", "asarray", "(", "shape", ",", "dtype", "=", "int_type", ")", "\n", "self", ".", "grid_shape", "=", "np", ".", "asarray", "(", "shape", ",", "dtype", "=", "int_type", ")", "\n", "self", ".", "grid", "=", "np", ".", "zeros", "(", "(", "self", ".", "batch_size", ",", "self", ".", "x_max", ",", "self", ".", "y_max", ",", "self", ".", "n_feats", ")", ",", "dtype", "=", "float_type", ")", "\n", "# 0=agents, 1=stag, 2=hare, [3=wall, 4=unknown], [-1=ghost-indicator]", "\n", "\n", "if", "self", ".", "random_ghosts", ":", "\n", "            ", "self", ".", "ghost_indicator", "=", "False", "# indicator whether whether prey is a ghost (True) or not (False)", "\n", "self", ".", "ghost_indicator_potential_positions", "=", "np", ".", "asarray", "(", "[", "[", "0", ",", "0", "]", ",", "[", "0", ",", "self", ".", "x_max", "-", "1", "]", ",", "[", "self", ".", "y_max", "-", "1", ",", "0", "]", ",", "\n", "[", "self", ".", "y_max", "-", "1", ",", "self", ".", "x_max", "-", "1", "]", "]", ",", "dtype", "=", "int_type", ")", "\n", "self", ".", "ghost_indicator_pos", "=", "[", "0", ",", "0", "]", "# position of the indicator whether prey is a ghost (-1) or not (+1)", "\n", "\n", "# Define the agents and their action space", "\n", "", "self", ".", "capture_action", "=", "getattr", "(", "args", ",", "\"capture_action\"", ",", "False", ")", "\n", "self", ".", "capture_action_conditions", "=", "getattr", "(", "args", ",", "\"capture_action_conditions\"", ",", "(", "2", ",", "1", ")", ")", "\n", "self", ".", "actions", "=", "np", ".", "asarray", "(", "[", "[", "0", ",", "1", "]", ",", "[", "1", ",", "0", "]", ",", "[", "0", ",", "-", "1", "]", ",", "[", "-", "1", ",", "0", "]", ",", "[", "0", ",", "0", "]", ",", "[", "0", ",", "0", "]", ",", "\n", "[", "0", ",", "0", "]", ",", "[", "0", ",", "0", "]", ",", "[", "0", ",", "0", "]", ",", "[", "0", ",", "0", "]", "]", ",", "dtype", "=", "int_type", ")", "\n", "self", ".", "action_names", "=", "[", "\"right\"", ",", "\"down\"", ",", "\"left\"", ",", "\"up\"", ",", "\"stay\"", ",", "\"catch\"", ",", "\n", "'look-right'", ",", "'look-down'", ",", "'look-left'", ",", "'look-up'", "]", "\n", "self", ".", "agent_move_block", "=", "np", ".", "asarray", "(", "getattr", "(", "args", ",", "\"agent_move_block\"", ",", "[", "0", "]", ")", ",", "dtype", "=", "int_type", ")", "\n", "self", ".", "n_actions", "=", "10", "if", "self", ".", "directed_observations", "and", "self", ".", "directed_exta_actions", "else", "(", "6", "if", "self", ".", "capture_action", "else", "5", ")", "\n", "self", ".", "n_agents", "=", "args", ".", "n_agents", "\n", "self", ".", "n_stags", "=", "args", ".", "n_stags", "\n", "self", ".", "p_stags_rest", "=", "args", ".", "p_stags_rest", "\n", "self", ".", "n_hare", "=", "args", ".", "n_hare", "\n", "self", ".", "p_hare_rest", "=", "args", ".", "p_hare_rest", "\n", "self", ".", "n_prey", "=", "self", ".", "n_stags", "+", "self", ".", "n_hare", "\n", "self", ".", "agent_obs", "=", "args", ".", "agent_obs", "\n", "self", ".", "agent_obs_dim", "=", "np", ".", "asarray", "(", "self", ".", "agent_obs", ",", "dtype", "=", "int_type", ")", "\n", "\n", "if", "self", ".", "observe_state", ":", "\n", "# The size of the global state as observation (with one additional position feature)", "\n", "            ", "self", ".", "obs_size", "=", "int", "(", "self", ".", "state_size", "+", "self", ".", "grid_shape", "[", "0", "]", "*", "self", ".", "grid_shape", "[", "1", "]", ")", "\n", "", "elif", "self", ".", "directed_observations", "and", "self", ".", "directed_cone_narrow", ":", "\n", "# The size of the visible observation cones for this option", "\n", "            ", "self", ".", "obs_size", "=", "self", ".", "n_feats", "*", "(", "2", "*", "args", ".", "agent_obs", "[", "0", "]", "-", "1", ")", "*", "(", "2", "*", "args", ".", "agent_obs", "[", "1", "]", "-", "1", ")", "\n", "", "else", ":", "\n", "# The agent-centric observation size", "\n", "            ", "self", ".", "obs_size", "=", "self", ".", "n_feats", "*", "(", "2", "*", "args", ".", "agent_obs", "[", "0", "]", "+", "1", ")", "*", "(", "2", "*", "args", ".", "agent_obs", "[", "1", "]", "+", "1", ")", "\n", "\n", "# Define the episode and rewards", "\n", "", "self", ".", "episode_limit", "=", "args", ".", "episode_limit", "\n", "self", ".", "time_reward", "=", "getattr", "(", "args", ",", "\"reward_time\"", ",", "-", "0.1", ")", "\n", "self", ".", "collision_reward", "=", "getattr", "(", "args", ",", "\"reward_collision\"", ",", "0.0", ")", "\n", "self", ".", "capture_hare_reward", "=", "getattr", "(", "args", ",", "\"reward_hare\"", ",", "1.0", ")", "\n", "self", ".", "capture_stag_reward", "=", "getattr", "(", "args", ",", "\"reward_stag\"", ",", "2.0", ")", "\n", "self", ".", "miscapture_punishment", "=", "float", "(", "getattr", "(", "args", ",", "\"miscapture_punishment\"", ",", "-", "self", ".", "capture_stag_reward", ")", ")", "\n", "self", ".", "capture_terminal", "=", "getattr", "(", "args", ",", "\"capture_terminal\"", ",", "True", ")", "\n", "self", ".", "capture_freezes", "=", "getattr", "(", "args", ",", "\"capture_freezes\"", ",", "True", ")", "\n", "self", ".", "remove_frozen", "=", "getattr", "(", "args", ",", "\"remove_frozen\"", ",", "False", ")", "\n", "\n", "# Define the internal state", "\n", "self", ".", "agents", "=", "np", ".", "zeros", "(", "(", "self", ".", "n_agents", ",", "self", ".", "batch_size", ",", "2", ")", ",", "dtype", "=", "int_type", ")", "\n", "self", ".", "agents_not_frozen", "=", "np", ".", "ones", "(", "(", "self", ".", "n_agents", ",", "self", ".", "batch_size", ")", ",", "dtype", "=", "int_type", ")", "\n", "self", ".", "agents_orientation", "=", "np", ".", "zeros", "(", "(", "self", ".", "n_agents", ",", "self", ".", "batch_size", ")", ",", "dtype", "=", "int_type", ")", "# use action_labels 0..3", "\n", "self", ".", "prey", "=", "np", ".", "zeros", "(", "(", "self", ".", "n_prey", ",", "self", ".", "batch_size", ",", "2", ")", ",", "dtype", "=", "int_type", ")", "\n", "self", ".", "prey_alive", "=", "np", ".", "zeros", "(", "(", "self", ".", "n_prey", ",", "self", ".", "batch_size", ")", ",", "dtype", "=", "int_type", ")", "\n", "self", ".", "prey_type", "=", "np", ".", "ones", "(", "(", "self", ".", "n_prey", ",", "self", ".", "batch_size", ")", ",", "dtype", "=", "int_type", ")", "# fill with stag (1)", "\n", "self", ".", "prey_type", "[", "self", ".", "n_stags", ":", ",", ":", "]", "=", "2", "# set hares to 2", "\n", "self", ".", "steps", "=", "0", "\n", "self", ".", "sum_rewards", "=", "0", "\n", "self", ".", "reset", "(", ")", "\n", "\n", "self", ".", "made_screen", "=", "False", "\n", "self", ".", "scaling", "=", "5", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt.reset": [[175, 201], ["stag_hunt.StagHunt.prey_alive.fill", "stag_hunt.StagHunt.agents_not_frozen.fill", "stag_hunt.StagHunt.grid.fill", "stag_hunt.StagHunt._place_actors", "stag_hunt.StagHunt._place_actors", "stag_hunt.StagHunt._place_actors", "numpy.random.random_integers", "stag_hunt.StagHunt.ghost_indicator_potential_positions[].tolist", "stag_hunt.StagHunt.get_obs", "stag_hunt.StagHunt.get_state", "random.randint", "len"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt._place_actors", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt._place_actors", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt._place_actors", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.get_obs", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.get_state"], ["", "def", "reset", "(", "self", ")", ":", "\n", "# Reset old episode", "\n", "        ", "self", ".", "prey_alive", ".", "fill", "(", "1", ")", "\n", "self", ".", "agents_not_frozen", ".", "fill", "(", "1", ")", "\n", "self", ".", "steps", "=", "0", "\n", "self", ".", "sum_rewards", "=", "0", "\n", "\n", "# Clear the grid", "\n", "self", ".", "grid", ".", "fill", "(", "0.0", ")", "\n", "\n", "# Place n_agents and n_preys on the grid", "\n", "self", ".", "_place_actors", "(", "self", ".", "agents", ",", "0", ",", "row", "=", "self", ".", "mountain_agent_row", "if", "self", ".", "mountain_agent_row", ">=", "0", "else", "None", ")", "\n", "# Place the stags/goats", "\n", "self", ".", "_place_actors", "(", "self", ".", "prey", "[", ":", "self", ".", "n_stags", ",", ":", ",", ":", "]", ",", "1", ",", "row", "=", "0", "if", "self", ".", "mountain_spawn", "else", "None", ")", "\n", "# Place the hares/sheep", "\n", "self", ".", "_place_actors", "(", "self", ".", "prey", "[", "self", ".", "n_stags", ":", ",", ":", ",", ":", "]", ",", "2", ",", "row", "=", "self", ".", "env_max", "[", "1", "]", "-", "1", "if", "self", ".", "mountain_spawn", "else", "None", ")", "\n", "\n", "# Agent orientations are initialized randomly", "\n", "self", ".", "agents_orientation", "=", "np", ".", "random", ".", "random_integers", "(", "low", "=", "0", ",", "high", "=", "3", ",", "size", "=", "(", "self", ".", "n_agents", ",", "self", ".", "batch_size", ")", ")", "\n", "\n", "if", "self", ".", "random_ghosts", "and", "self", ".", "random_ghosts_random_indicator", ":", "\n", "            ", "self", ".", "ghost_indicator_pos", "=", "self", ".", "ghost_indicator_potential_positions", "[", "\n", "random", ".", "randint", "(", "0", ",", "len", "(", "self", ".", "ghost_indicator_potential_positions", ")", "-", "1", ")", "]", ".", "tolist", "(", ")", "\n", "\n", "# self.step(th.zeros(self.n_agents).fill_(self.action_labels['stay']))", "\n", "", "return", "self", ".", "get_obs", "(", ")", ",", "self", ".", "get_state", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt.step": [[202, 356], ["numpy.expand_dims.astype", "range", "range", "numpy.expand_dims", "numpy.ones", "numpy.random.permutation", "numpy.random.permutation", "print", "numpy.asarray", "len", "range", "reward[].item", "int", "range", "sum", "sum", "random.random", "range", "stag_hunt.StagHunt._move_actor", "range", "possible.append", "stag_hunt.StagHunt._move_actor", "len", "range", "print", "numpy.random.rand", "stag_hunt.StagHunt._move_actor", "numpy.asarray", "range", "len", "stag_hunt.StagHunt._move_actor", "numpy.asarray", "range", "numpy.random.rand", "numpy.random.randint", "numpy.asarray", "len", "numpy.random.rand", "print"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt._move_actor", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt._move_actor", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt._move_actor", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt._move_actor"], ["", "def", "step", "(", "self", ",", "actions", ")", ":", "\n", "        ", "\"\"\" Execute a*bs actions in the environment. \"\"\"", "\n", "if", "not", "self", ".", "batch_mode", ":", "\n", "            ", "actions", "=", "np", ".", "expand_dims", "(", "np", ".", "asarray", "(", "actions", ",", "dtype", "=", "int_type", ")", ",", "axis", "=", "1", ")", "\n", "", "assert", "len", "(", "actions", ".", "shape", ")", "==", "2", "and", "actions", ".", "shape", "[", "0", "]", "==", "self", ".", "n_agents", "and", "actions", ".", "shape", "[", "1", "]", "==", "self", ".", "batch_size", ",", "\"improper number of agents and/or parallel environments!\"", "\n", "actions", "=", "actions", ".", "astype", "(", "dtype", "=", "int_type", ")", "\n", "\n", "# Initialise returned values and grid", "\n", "reward", "=", "np", ".", "ones", "(", "self", ".", "batch_size", ",", "dtype", "=", "float_type", ")", "*", "self", ".", "time_reward", "\n", "terminated", "=", "[", "False", "for", "_", "in", "range", "(", "self", ".", "batch_size", ")", "]", "\n", "\n", "# Move the agents sequentially in random order", "\n", "for", "b", "in", "range", "(", "self", ".", "batch_size", ")", ":", "\n", "            ", "for", "a", "in", "np", ".", "random", ".", "permutation", "(", "self", ".", "n_agents", ")", ":", "\n", "# Only move if not frozen", "\n", "                ", "if", "self", ".", "agents_not_frozen", "[", "a", ",", "b", "]", ">", "0", ":", "\n", "# Only moves \"up\" if the mountain permits it (as defined by mountain_slope)", "\n", "                    ", "if", "not", "(", "np", ".", "random", ".", "rand", "(", ")", "<", "self", ".", "mountain_slope", "and", "actions", "[", "a", ",", "b", "]", "==", "3", ")", ":", "\n", "                        ", "self", ".", "agents", "[", "a", ",", "b", ",", ":", "]", ",", "collide", "=", "self", ".", "_move_actor", "(", "self", ".", "agents", "[", "a", ",", "b", ",", ":", "]", ",", "actions", "[", "a", ",", "b", "]", ",", "b", ",", "\n", "self", ".", "agent_move_block", ",", "0", ")", "\n", "if", "collide", ":", "\n", "                            ", "reward", "[", "b", "]", "=", "reward", "[", "b", "]", "+", "self", ".", "collision_reward", "\n", "# Set the agent's orientation (if the observation depends on it)", "\n", "", "", "if", "self", ".", "directed_observations", ":", "\n", "                        ", "if", "self", ".", "directed_exta_actions", ":", "\n", "                            ", "if", "actions", "[", "a", ",", "b", "]", ">=", "self", ".", "action_look_to_act", ":", "\n", "                                ", "self", ".", "agents_orientation", "[", "a", ",", "b", "]", "=", "actions", "[", "a", ",", "b", "]", "-", "self", ".", "action_look_to_act", "\n", "", "", "else", ":", "\n", "                            ", "if", "actions", "[", "a", ",", "b", "]", "<", "4", ":", "\n", "                                ", "self", ".", "agents_orientation", "[", "a", ",", "b", "]", "=", "actions", "[", "a", ",", "b", "]", "\n", "\n", "# Move the prey", "\n", "", "", "", "", "", "", "for", "b", "in", "range", "(", "self", ".", "batch_size", ")", ":", "\n", "            ", "for", "p", "in", "np", ".", "random", ".", "permutation", "(", "self", ".", "n_prey", ")", ":", "\n", "                ", "if", "self", ".", "prey_alive", "[", "p", ",", "b", "]", ">", "0", ":", "\n", "# Collect all allowed actions for the prey", "\n", "                    ", "possible", "=", "[", "]", "\n", "next_to_agent", "=", "False", "\n", "# Run through all potential movement actions (without actually moving)", "\n", "for", "u", "in", "range", "(", "4", ")", ":", "\n", "                        ", "if", "not", "self", ".", "_move_actor", "(", "self", ".", "prey", "[", "p", ",", "b", ",", ":", "]", ",", "u", ",", "b", ",", "np", ".", "asarray", "(", "[", "0", ",", "1", ",", "2", "]", ",", "dtype", "=", "int_type", ")", ")", "[", "1", "]", ":", "\n", "                            ", "possible", ".", "append", "(", "u", ")", "\n", "", "if", "self", ".", "_move_actor", "(", "self", ".", "prey", "[", "p", ",", "b", ",", ":", "]", ",", "u", ",", "b", ",", "np", ".", "asarray", "(", "[", "0", "]", ",", "dtype", "=", "int_type", ")", ")", "[", "1", "]", ":", "\n", "                            ", "next_to_agent", "=", "True", "\n", "# Capturing prey works differently when the agents have a 'catch' action", "\n", "", "", "if", "self", ".", "capture_action", ":", "\n", "                        ", "n_catching_agents", "=", "0", "\n", "# Prey does not move (rest) if they have capture_conditions or less available moves", "\n", "# if len(possible) <= self.capture_conditions[self.prey_type[p, b] - 1]:", "\n", "#     possible = []", "\n", "# Capturing happens if capture_action_conditions many agents execute 'catch'", "\n", "for", "a", "in", "range", "(", "self", ".", "n_agents", ")", ":", "\n", "                            ", "if", "actions", "[", "a", ",", "b", "]", "==", "self", ".", "action_labels", "[", "'catch'", "]", "and", "self", ".", "agents_not_frozen", "[", "a", ",", "b", "]", ">", "0", ":", "\n", "# If any movement action in [0, 4) would end up on agent a, that agent can 'catch' p", "\n", "                                ", "for", "u", "in", "range", "(", "4", ")", ":", "\n", "                                    ", "pos", "=", "self", ".", "prey", "[", "p", ",", "b", "]", "+", "self", ".", "actions", "[", "u", "]", "\n", "if", "pos", "[", "0", "]", "==", "self", ".", "agents", "[", "a", ",", "b", ",", "0", "]", "and", "pos", "[", "1", "]", "==", "self", ".", "agents", "[", "a", ",", "b", ",", "1", "]", ":", "\n", "                                        ", "n_catching_agents", "+=", "1", "\n", "break", "\n", "# If the number of neighboring agents that execute 'catch' >= condition, prey is captured", "\n", "", "", "", "", "captured", "=", "n_catching_agents", ">=", "self", ".", "capture_action_conditions", "[", "self", ".", "prey_type", "[", "p", ",", "b", "]", "-", "1", "]", "\n", "#          and len(possible) == 0", "\n", "if", "n_catching_agents", ">", "0", "and", "not", "captured", ":", "\n", "                            ", "reward", "[", "b", "]", "+=", "self", ".", "miscapture_punishment", "\n", "", "", "else", ":", "\n", "# Prey is caught when the number of possible moves is less or equal to their capture_condition", "\n", "                        ", "captured", "=", "len", "(", "possible", ")", "<=", "self", ".", "capture_conditions", "[", "self", ".", "prey_type", "[", "p", ",", "b", "]", "-", "1", "]", "\n", "", "captured", "=", "captured", "and", "next_to_agent", "if", "self", ".", "args", ".", "prevent_cannibalism", "else", "captured", "\n", "# If the prey is captured, remove it from the grid and terminate episode if specified", "\n", "if", "captured", ":", "\n", "# kill prey", "\n", "                        ", "self", ".", "prey_alive", "[", "p", ",", "b", "]", "=", "0", "\n", "self", ".", "grid", "[", "b", ",", "self", ".", "prey", "[", "p", ",", "b", ",", "0", "]", ",", "self", ".", "prey", "[", "p", ",", "b", ",", "1", "]", ",", "self", ".", "prey_type", "[", "p", ",", "b", "]", "]", "=", "0", "\n", "# terminate if capture_terminal=True", "\n", "terminated", "[", "b", "]", "=", "terminated", "[", "b", "]", "or", "self", ".", "capture_terminal", "\n", "# determine reward for capture", "\n", "rew", "=", "0", "\n", "rew", "+=", "self", ".", "capture_stag_reward", "if", "self", ".", "prey_type", "[", "p", ",", "b", "]", "==", "1", "else", "0", "\n", "rew", "+=", "self", ".", "capture_hare_reward", "if", "self", ".", "prey_type", "[", "p", ",", "b", "]", "==", "2", "else", "0", "\n", "if", "self", ".", "random_ghosts", "and", "self", ".", "ghost_indicator", ":", "\n", "                            ", "rew", "*=", "self", ".", "random_ghosts_mul", "\n", "", "reward", "[", "b", "]", "+=", "rew", "\n", "# freeze all surrounding agents if capture_freezes=True", "\n", "if", "self", ".", "capture_freezes", ":", "\n", "# each agent a...", "\n", "                            ", "for", "a", "in", "range", "(", "self", ".", "n_agents", ")", ":", "\n", "# ... which is not frozen...", "\n", "                                ", "if", "self", ".", "agents_not_frozen", "[", "a", ",", "b", "]", ">", "0", "and", "(", "not", "self", ".", "capture_action", "or", "actions", "[", "a", ",", "b", "]", "==", "self", ".", "action_labels", "[", "'catch'", "]", ")", ":", "\n", "# ... checks all possible movement actions ...", "\n", "                                    ", "for", "u", "in", "range", "(", "self", ".", "n_actions", "-", "1", ")", ":", "\n", "                                        ", "x", "=", "self", ".", "agents", "[", "a", ",", "b", ",", ":", "]", "+", "self", ".", "actions", "[", "u", "]", "\n", "# ... to see if it would have moved onto prey p's position ...", "\n", "if", "x", "[", "0", "]", "==", "self", ".", "prey", "[", "p", ",", "b", ",", "0", "]", "and", "x", "[", "1", "]", "==", "self", ".", "prey", "[", "p", ",", "b", ",", "1", "]", ":", "\n", "# ... which freezes the agent!", "\n", "                                            ", "self", ".", "agents_not_frozen", "[", "a", ",", "b", "]", "=", "0", "\n", "# remove frozen agents from the grid if specified", "\n", "if", "self", ".", "remove_frozen", ":", "\n", "                                                ", "self", ".", "grid", "[", "b", ",", "self", ".", "agents", "[", "a", ",", "b", ",", "0", "]", ",", "self", ".", "agents", "[", "a", ",", "b", ",", "1", "]", ",", "0", "]", "=", "0", "\n", "# debug message if requested", "\n", "", "if", "self", ".", "print_frozen_agents", ":", "\n", "                                                ", "print", "(", "\"Freeze agent %u at height %u and pos %u.\"", "%", "\n", "(", "a", ",", "self", ".", "env_max", "[", "0", "]", "-", "1", "-", "self", ".", "agents", "[", "a", ",", "b", ",", "0", "]", ",", "self", ".", "agents", "[", "a", ",", "b", ",", "1", "]", ")", ",", "\n", "\"    Agents active:\"", ",", "self", ".", "agents_not_frozen", "[", ":", ",", "b", "]", ")", "\n", "# print debug messages", "\n", "", "", "", "", "", "", "if", "self", ".", "print_caught_prey", ":", "\n", "                            ", "print", "(", "\"Captured %s at time %u, height %d and pos %u.\"", "%", "\n", "(", "(", "\"stag\"", "if", "self", ".", "prey_type", "[", "p", ",", "b", "]", "==", "1", "else", "\"hare\"", ")", ",", "self", ".", "steps", ",", "\n", "self", ".", "env_max", "[", "0", "]", "-", "1", "-", "self", ".", "prey", "[", "p", ",", "b", ",", "0", "]", ",", "self", ".", "prey", "[", "p", ",", "b", ",", "1", "]", ")", ",", "\n", "\"   Agents: \"", ",", "self", ".", "agents_not_frozen", "[", ":", ",", "b", "]", ",", "\n", "\"   reward %g\"", "%", "reward", "[", "b", "]", "\n", ")", "\n", "", "", "else", ":", "\n", "# If not, check if the prey can rest and if so determine randomly whether it wants to", "\n", "                        ", "rest", "=", "(", "self", ".", "grid", "[", "b", ",", "self", ".", "prey", "[", "p", ",", "b", ",", "0", "]", ",", "self", ".", "prey", "[", "p", ",", "b", ",", "1", "]", ",", "0", "]", "==", "0", ")", "and", "(", "np", ".", "random", ".", "rand", "(", ")", "<", "(", "self", ".", "p_stags_rest", "if", "self", ".", "prey_type", "[", "p", ",", "b", "]", "==", "1", "\n", "else", "self", ".", "p_hare_rest", ")", ")", "or", "len", "(", "possible", ")", "==", "0", "\n", "# If the prey decides not to rest, choose a movement action randomly", "\n", "if", "not", "rest", ":", "\n", "                            ", "u", "=", "possible", "[", "np", ".", "random", ".", "randint", "(", "len", "(", "possible", ")", ")", "]", "\n", "# Only moves up/down if the mountain permits it (as defined by mountain_slope)", "\n", "if", "not", "(", "np", ".", "random", ".", "rand", "(", ")", "<", "self", ".", "mountain_slope", "\n", "and", "self", ".", "grid", "[", "b", ",", "self", ".", "prey", "[", "p", ",", "b", ",", "0", "]", ",", "self", ".", "prey", "[", "p", ",", "b", ",", "1", "]", ",", "0", "]", "==", "0", "\n", "and", "(", "self", ".", "prey_type", "[", "p", ",", "b", "]", "==", "2", "and", "u", "==", "3", "or", "self", ".", "prey_type", "[", "p", ",", "b", "]", "==", "1", "and", "u", "==", "1", ")", ")", ":", "\n", "# Execute movement", "\n", "                                ", "self", ".", "prey", "[", "p", ",", "b", ",", ":", "]", ",", "_", "=", "self", ".", "_move_actor", "(", "self", ".", "prey", "[", "p", ",", "b", ",", ":", "]", ",", "u", ",", "b", ",", "\n", "np", ".", "asarray", "(", "[", "0", ",", "1", ",", "2", "]", ",", "dtype", "=", "int_type", ")", ",", "\n", "self", ".", "prey_type", "[", "p", ",", "b", "]", ")", "\n", "# Terminate batch if all prey are caught or all agents are frozen", "\n", "", "", "", "", "", "terminated", "[", "b", "]", "=", "terminated", "[", "b", "]", "or", "sum", "(", "self", ".", "prey_alive", "[", ":", ",", "b", "]", ")", "==", "0", "or", "sum", "(", "self", ".", "agents_not_frozen", "[", ":", ",", "b", "]", ")", "==", "0", "\n", "\n", "", "if", "self", ".", "random_ghosts", ":", "\n", "            ", "self", ".", "ghost_indicator", "=", "not", "(", "random", ".", "random", "(", ")", "<", "self", ".", "random_ghosts_prob", ")", "\n", "\n", "# Terminate if episode_limit is reached", "\n", "", "info", "=", "{", "}", "\n", "self", ".", "sum_rewards", "+=", "reward", "[", "0", "]", "\n", "self", ".", "steps", "+=", "1", "\n", "if", "self", ".", "steps", ">=", "self", ".", "episode_limit", ":", "\n", "            ", "terminated", "=", "[", "True", "for", "_", "in", "range", "(", "self", ".", "batch_size", ")", "]", "\n", "info", "[", "\"episode_limit\"", "]", "=", "self", ".", "truncate_episodes", "\n", "", "else", ":", "\n", "            ", "info", "[", "\"episode_limit\"", "]", "=", "False", "\n", "\n", "", "if", "terminated", "[", "0", "]", "and", "self", ".", "print_caught_prey", ":", "\n", "            ", "print", "(", "\"Episode terminated at time %u with return %g\"", "%", "(", "self", ".", "steps", ",", "self", ".", "sum_rewards", ")", ")", "\n", "\n", "", "if", "self", ".", "batch_mode", ":", "\n", "            ", "return", "reward", ",", "terminated", ",", "info", "\n", "", "else", ":", "\n", "            ", "return", "reward", "[", "0", "]", ".", "item", "(", ")", ",", "int", "(", "terminated", "[", "0", "]", ")", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt.get_obs_agent": [[358, 377], ["list", "numpy.zeros", "obs.flatten.flatten.flatten", "stag_hunt.StagHunt._observe", "numpy.reshape", "stag_hunt.StagHunt.get_state"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt._observe", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.get_state"], ["", "", "def", "get_obs_agent", "(", "self", ",", "agent_id", ",", "batch", "=", "0", ")", ":", "\n", "        ", "if", "self", ".", "observe_state", ":", "\n", "# Get the state as observation (in the right format)", "\n", "            ", "dim", "=", "list", "(", "self", ".", "grid", ".", "shape", ")", "\n", "state", "=", "np", ".", "reshape", "(", "self", ".", "get_state", "(", ")", ",", "dim", ")", "[", "batch", ",", ":", "]", "\n", "# Reshape and add a blank feature (last dimension) for the agent's position", "\n", "dim", "=", "dim", "[", "1", ":", "]", "# only one batch", "\n", "dim", "[", "-", "1", "]", "+=", "1", "# one more feature", "\n", "obs", "=", "np", ".", "zeros", "(", "dim", ")", "\n", "obs", "[", ":", ",", ":", ",", ":", "-", "1", "]", "=", "state", "\n", "# Mark the position of agent_id in the new feature", "\n", "obs", "[", "self", ".", "agents", "[", "agent_id", ",", "batch", ",", "0", "]", ",", "self", ".", "agents", "[", "agent_id", ",", "batch", ",", "1", "]", ",", "-", "1", "]", "=", "1.0", "\n", "obs", "=", "obs", ".", "flatten", "(", ")", "\n", "", "else", ":", "\n", "            ", "obs", ",", "_", "=", "self", ".", "_observe", "(", "[", "agent_id", "]", ")", "\n", "# If the frozen agents are removed, their observation is blank", "\n", "", "if", "self", ".", "capture_freezes", "and", "self", ".", "remove_frozen", "and", "self", ".", "agents_not_frozen", "[", "agent_id", ",", "batch", "]", "==", "0", ":", "\n", "            ", "obs", "*=", "0", "\n", "", "return", "obs", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt.get_obs": [[378, 381], ["stag_hunt.StagHunt.get_obs_agent", "range"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.get_obs_agent"], ["", "def", "get_obs", "(", "self", ")", ":", "\n", "        ", "agents_obs", "=", "[", "self", ".", "get_obs_agent", "(", "i", ")", "for", "i", "in", "range", "(", "self", ".", "n_agents", ")", "]", "\n", "return", "agents_obs", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt.get_state_as_graph": [[382, 401], ["range", "range", "range", "state.append", "range", "[].append", "state.append", "state.append", "stag_hunt.StagHunt._move_actor", "[].append", "numpy.asarray"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt._move_actor"], ["", "def", "get_state_as_graph", "(", "self", ")", ":", "\n", "        ", "state", "=", "[", "]", "\n", "# Enqueue all agents", "\n", "for", "a", "in", "range", "(", "self", ".", "n_agents", ")", ":", "\n", "            ", "state", ".", "append", "(", "{", "'type'", ":", "'agent'", ",", "'pos'", ":", "self", ".", "agents", "[", "a", ",", "0", "]", ",", "'avail_actions'", ":", "[", "]", ",", "'id'", ":", "a", "}", ")", "\n", "# check the 4 movement actions for availability", "\n", "for", "u", "in", "range", "(", "self", ".", "n_actions", "-", "1", ")", ":", "\n", "                ", "_", ",", "c", "=", "self", ".", "_move_actor", "(", "self", ".", "agents", "[", "a", ",", "0", ",", ":", "]", ",", "u", ",", "0", ",", "np", ".", "asarray", "(", "[", "0", "]", ",", "dtype", "=", "int_type", ")", ")", "\n", "state", "[", "-", "1", "]", "[", "'avail_actions'", "]", ".", "append", "(", "0", "if", "c", "else", "1", ")", "\n", "# 'pass' action (4) is always available", "\n", "", "state", "[", "-", "1", "]", "[", "'avail_actions'", "]", ".", "append", "(", "1", ")", "\n", "# Enqueue all stags", "\n", "", "for", "s", "in", "range", "(", "self", ".", "n_stags", ")", ":", "\n", "            ", "state", ".", "append", "(", "{", "'type'", ":", "'stag'", ",", "'pos'", ":", "self", ".", "prey", "[", "s", ",", "0", "]", "}", ")", "\n", "# Enqueue all hares", "\n", "", "for", "h", "in", "range", "(", "self", ".", "n_hare", ")", ":", "\n", "            ", "state", ".", "append", "(", "{", "'type'", ":", "'hare'", ",", "'pos'", ":", "self", ".", "prey", "[", "self", ".", "n_stags", "+", "h", ",", "0", "]", "}", ")", "\n", "# Return list f entities", "\n", "", "return", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt.get_state": [[402, 411], ["stag_hunt.StagHunt.state_to_graph", "stag_hunt.StagHunt.grid.copy().reshape", "stag_hunt.StagHunt.grid[].reshape", "stag_hunt.StagHunt.get_state_as_graph", "stag_hunt.StagHunt.grid.copy"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt.state_to_graph", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt.get_state_as_graph"], ["", "def", "get_state", "(", "self", ")", ":", "\n", "# Either return the state as a list of entities...", "\n", "        ", "if", "self", ".", "state_as_graph", ":", "\n", "            ", "return", "self", ".", "state_to_graph", "(", "self", ".", "get_state_as_graph", "(", ")", ")", "\n", "# ... or return the entire grid", "\n", "", "if", "self", ".", "batch_mode", ":", "\n", "            ", "return", "self", ".", "grid", ".", "copy", "(", ")", ".", "reshape", "(", "self", ".", "state_size", ")", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "grid", "[", "0", ",", ":", ",", ":", ",", ":", "]", ".", "reshape", "(", "self", ".", "state_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt.get_obs_intersect_pair_size": [[412, 414], ["stag_hunt.StagHunt.get_obs_size"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.get_obs_size"], ["", "", "def", "get_obs_intersect_pair_size", "(", "self", ")", ":", "\n", "        ", "return", "2", "*", "self", ".", "get_obs_size", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt.get_obs_intersect_all_size": [[415, 417], ["stag_hunt.StagHunt.get_obs_size"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.get_obs_size"], ["", "def", "get_obs_intersect_all_size", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "n_agents", "*", "self", ".", "get_obs_size", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt.get_obs_intersection": [[418, 420], ["stag_hunt.StagHunt._observe"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt._observe"], ["", "def", "get_obs_intersection", "(", "self", ",", "agent_ids", ")", ":", "\n", "        ", "return", "self", ".", "_observe", "(", "agent_ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt.get_total_actions": [[422, 424], ["None"], "methods", ["None"], ["", "def", "get_total_actions", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "n_actions", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt.get_avail_agent_actions": [[425, 451], ["numpy.logical_and().all", "numpy.any", "range", "range", "range", "int", "stag_hunt.StagHunt._move_actor", "range", "numpy.logical_and", "range", "numpy.asarray"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt._move_actor"], ["", "def", "get_avail_agent_actions", "(", "self", ",", "agent_id", ")", ":", "\n", "        ", "\"\"\" Currently runs only with batch_size==1. \"\"\"", "\n", "if", "self", ".", "agents_not_frozen", "[", "agent_id", "]", "==", "0", ":", "\n", "# All agents that are frozen can only perform the \"stay\" action", "\n", "            ", "avail_actions", "=", "[", "0", "for", "_", "in", "range", "(", "self", ".", "n_actions", ")", "]", "\n", "avail_actions", "[", "self", ".", "action_labels", "[", "'stay'", "]", "]", "=", "1", "\n", "", "elif", "self", ".", "toroidal", ":", "\n", "# In a toroidal environment, all actions are allowed", "\n", "            ", "avail_actions", "=", "[", "1", "for", "_", "in", "range", "(", "self", ".", "n_actions", ")", "]", "\n", "", "else", ":", "\n", "# In a bounded environment, you cannot move into walls", "\n", "            ", "new_pos", "=", "self", ".", "agents", "[", "agent_id", ",", "0", ",", ":", "]", "+", "self", ".", "actions", "[", ":", "self", ".", "n_actions", "]", "\n", "allowed", "=", "np", ".", "logical_and", "(", "new_pos", ">=", "0", ",", "new_pos", "<", "self", ".", "grid_shape", ")", ".", "all", "(", "axis", "=", "1", ")", "\n", "assert", "np", ".", "any", "(", "allowed", ")", ",", "\"No available action in the environment: this should never happen!\"", "\n", "avail_actions", "=", "[", "int", "(", "allowed", "[", "a", "]", ")", "for", "a", "in", "range", "(", "self", ".", "n_actions", ")", "]", "\n", "# If the agent is not frozen, the 'catch' action is only available next to a prey", "\n", "", "if", "self", ".", "capture_action", "and", "self", ".", "agents_not_frozen", "[", "agent_id", "]", ">", "0", ":", "\n", "            ", "avail_actions", "[", "self", ".", "action_labels", "[", "'catch'", "]", "]", "=", "0", "\n", "# Check with virtual move actions if there is a prey next to the agent", "\n", "possible_catches", "=", "range", "(", "4", ")", "if", "not", "self", ".", "directed_observations", "else", "range", "(", "self", ".", "agents_orientation", "[", "agent_id", ",", "0", "]", ",", "self", ".", "agents_orientation", "[", "agent_id", ",", "0", "]", "+", "1", ")", "\n", "for", "u", "in", "possible_catches", ":", "\n", "                ", "if", "self", ".", "_move_actor", "(", "self", ".", "agents", "[", "agent_id", ",", "0", ",", ":", "]", ",", "u", ",", "0", ",", "np", ".", "asarray", "(", "[", "1", ",", "2", "]", ",", "dtype", "=", "int_type", ")", ")", "[", "1", "]", ":", "\n", "                    ", "avail_actions", "[", "self", ".", "action_labels", "[", "'catch'", "]", "]", "=", "1", "\n", "break", "\n", "", "", "", "return", "avail_actions", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt.get_avail_actions": [[452, 457], ["range", "avail_actions.append", "stag_hunt.StagHunt.get_avail_agent_actions"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.get_avail_agent_actions"], ["", "def", "get_avail_actions", "(", "self", ")", ":", "\n", "        ", "avail_actions", "=", "[", "]", "\n", "for", "agent_id", "in", "range", "(", "self", ".", "n_agents", ")", ":", "\n", "            ", "avail_actions", ".", "append", "(", "self", ".", "get_avail_agent_actions", "(", "agent_id", ")", ")", "\n", "", "return", "avail_actions", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt.get_obs_size": [[458, 460], ["None"], "methods", ["None"], ["", "def", "get_obs_size", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "obs_size", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt.get_state_size": [[461, 463], ["None"], "methods", ["None"], ["", "def", "get_state_size", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "state_size", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt.get_stats": [[464, 466], ["None"], "methods", ["None"], ["", "def", "get_stats", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt.get_env_info": [[467, 472], ["envs.multiagentenv.MultiAgentEnv.get_env_info"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner.get_env_info"], ["", "def", "get_env_info", "(", "self", ")", ":", "\n", "        ", "info", "=", "MultiAgentEnv", ".", "get_env_info", "(", "self", ")", "\n", "if", "self", ".", "state_as_graph", ":", "\n", "            ", "raise", "Exception", "\n", "", "return", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt.close": [[474, 478], ["print", "pygame.quit"], "methods", ["None"], ["", "def", "close", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "made_screen", ":", "\n", "            ", "pygame", ".", "quit", "(", ")", "\n", "", "print", "(", "\"Closing Multi-Agent Navigation\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt.render_array": [[479, 482], ["None"], "methods", ["None"], ["", "def", "render_array", "(", "self", ")", ":", "\n", "# Return an rgb array of the frame", "\n", "        ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt.render": [[483, 486], ["None"], "methods", ["None"], ["", "def", "render", "(", "self", ")", ":", "\n", "# TODO!", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt.seed": [[487, 489], ["None"], "methods", ["None"], ["", "def", "seed", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt._place_actors": [[491, 502], ["range", "range", "numpy.random.randint", "numpy.random.randint", "numpy.sum"], "methods", ["None"], ["", "def", "_place_actors", "(", "self", ",", "actors", ":", "np", ".", "ndarray", ",", "type_id", ":", "int", ",", "row", "=", "None", ",", "col", "=", "None", ")", ":", "\n", "        ", "for", "b", "in", "range", "(", "self", ".", "batch_size", ")", ":", "\n", "            ", "for", "a", "in", "range", "(", "actors", ".", "shape", "[", "0", "]", ")", ":", "\n", "                ", "is_free", "=", "False", "\n", "while", "not", "is_free", ":", "\n", "# Draw actors's position randomly", "\n", "                    ", "actors", "[", "a", ",", "b", ",", "0", "]", "=", "np", ".", "random", ".", "randint", "(", "self", ".", "env_max", "[", "0", "]", ")", "if", "row", "is", "None", "else", "row", "\n", "actors", "[", "a", ",", "b", ",", "1", "]", "=", "np", ".", "random", ".", "randint", "(", "self", ".", "env_max", "[", "1", "]", ")", "if", "col", "is", "None", "else", "col", "\n", "# Check if position is valid", "\n", "is_free", "=", "np", ".", "sum", "(", "self", ".", "grid", "[", "b", ",", "actors", "[", "a", ",", "b", ",", "0", "]", ",", "actors", "[", "a", ",", "b", ",", "1", "]", ",", ":", "]", ")", "==", "0", "\n", "", "self", ".", "grid", "[", "b", ",", "actors", "[", "a", ",", "b", ",", "0", "]", ",", "actors", "[", "a", ",", "b", ",", "1", "]", ",", "type_id", "]", "=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt.print_grid": [[503, 511], ["grid[].squeeze().copy", "range", "numpy.sum", "print", "grid[].squeeze"], "methods", ["None"], ["", "", "", "def", "print_grid", "(", "self", ",", "batch", "=", "0", ",", "grid", "=", "None", ")", ":", "\n", "        ", "if", "grid", "is", "None", ":", "\n", "            ", "grid", "=", "self", ".", "grid", "\n", "", "grid", "=", "grid", "[", "batch", ",", ":", ",", ":", ",", ":", "]", ".", "squeeze", "(", ")", ".", "copy", "(", ")", "\n", "for", "i", "in", "range", "(", "grid", ".", "shape", "[", "2", "]", ")", ":", "\n", "            ", "grid", "[", ":", ",", ":", ",", "i", "]", "*=", "i", "+", "1", "\n", "", "grid", "=", "np", ".", "sum", "(", "grid", ",", "axis", "=", "2", ")", "\n", "print", "(", "grid", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt.print_agents": [[512, 520], ["numpy.zeros", "range", "range", "print"], "methods", ["None"], ["", "def", "print_agents", "(", "self", ",", "batch", "=", "0", ")", ":", "\n", "        ", "obs", "=", "np", ".", "zeros", "(", "(", "self", ".", "grid_shape", "[", "0", "]", ",", "self", ".", "grid_shape", "[", "1", "]", ")", ")", "\n", "for", "a", "in", "range", "(", "self", ".", "n_agents", ")", ":", "\n", "            ", "obs", "[", "self", ".", "agents", "[", "a", ",", "batch", ",", "0", "]", ",", "self", ".", "agents", "[", "a", ",", "batch", ",", "1", "]", "]", "=", "a", "+", "1", "\n", "", "for", "p", "in", "range", "(", "self", ".", "n_prey", ")", ":", "\n", "            ", "if", "self", ".", "prey_alive", "[", "p", "]", ":", "\n", "                ", "obs", "[", "self", ".", "prey", "[", "p", ",", "batch", ",", "0", "]", ",", "self", ".", "prey", "[", "p", ",", "batch", ",", "1", "]", "]", "=", "-", "p", "-", "1", "\n", "", "", "print", "(", "obs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt._env_bounds": [[521, 529], ["numpy.minimum", "numpy.maximum"], "methods", ["None"], ["", "def", "_env_bounds", "(", "self", ",", "positions", ":", "np", ".", "ndarray", ")", ":", "\n", "# positions is 2 dimensional", "\n", "        ", "if", "self", ".", "toroidal", ":", "\n", "            ", "positions", "=", "positions", "%", "self", ".", "env_max", "\n", "", "else", ":", "\n", "            ", "positions", "=", "np", ".", "minimum", "(", "positions", ",", "self", ".", "env_max", "-", "1", ")", "\n", "positions", "=", "np", ".", "maximum", "(", "positions", ",", "0", ")", "\n", "", "return", "positions", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt._move_actor": [[530, 544], ["stag_hunt.StagHunt._env_bounds", "numpy.sum"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt._env_bounds"], ["", "def", "_move_actor", "(", "self", ",", "pos", ":", "np", ".", "ndarray", ",", "action", ":", "int", ",", "batch", ":", "int", ",", "collision_mask", ":", "np", ".", "ndarray", ",", "move_type", "=", "None", ")", ":", "\n", "# compute hypothetical next position", "\n", "        ", "new_pos", "=", "self", ".", "_env_bounds", "(", "pos", "+", "self", ".", "actions", "[", "action", "]", ")", "\n", "# check for a collision with anything in the collision_mask", "\n", "found_at_new_pos", "=", "self", ".", "grid", "[", "batch", ",", "new_pos", "[", "0", "]", ",", "new_pos", "[", "1", "]", ",", ":", "]", "\n", "collision", "=", "np", ".", "sum", "(", "found_at_new_pos", "[", "collision_mask", "]", ")", ">", "0", "\n", "if", "collision", ":", "\n", "# No change in position", "\n", "            ", "new_pos", "=", "pos", "\n", "", "elif", "move_type", "is", "not", "None", ":", "\n", "# change the agent's state and position on the grid", "\n", "            ", "self", ".", "grid", "[", "batch", ",", "pos", "[", "0", "]", ",", "pos", "[", "1", "]", ",", "move_type", "]", "=", "0", "\n", "self", ".", "grid", "[", "batch", ",", "new_pos", "[", "0", "]", ",", "new_pos", "[", "1", "]", ",", "move_type", "]", "=", "1", "\n", "", "return", "new_pos", ",", "collision", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt._is_visible": [[545, 559], ["target.reshape().repeat.reshape().repeat.reshape().repeat", "numpy.all", "numpy.minimum", "numpy.maximum", "numpy.abs", "numpy.abs", "target.reshape().repeat.reshape().repeat.reshape", "numpy.minimum"], "methods", ["None"], ["", "def", "_is_visible", "(", "self", ",", "agents", ",", "target", ")", ":", "\n", "        ", "\"\"\" agents are plural and target is singular. \"\"\"", "\n", "target", "=", "target", ".", "reshape", "(", "1", ",", "2", ")", ".", "repeat", "(", "agents", ".", "shape", "[", "0", "]", ",", "0", ")", "\n", "# Determine the Manhattan distance of all agents to the target", "\n", "if", "self", ".", "toroidal", ":", "\n", "# Account for the environment wrapping around in a toroidal fashion", "\n", "            ", "lower", "=", "np", ".", "minimum", "(", "agents", ",", "target", ")", "\n", "higher", "=", "np", ".", "maximum", "(", "agents", ",", "target", ")", "\n", "d", "=", "np", ".", "abs", "(", "np", ".", "minimum", "(", "higher", "-", "lower", ",", "lower", "-", "higher", "+", "self", ".", "grid_shape", ")", ")", "\n", "", "else", ":", "\n", "# Account for the environment being bounded", "\n", "            ", "d", "=", "np", ".", "abs", "(", "agents", "-", "target", ")", "\n", "# Return true if all targets are visible by all agents", "\n", "", "return", "np", ".", "all", "(", "d", "<=", "self", ".", "agent_obs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt._intersect_targets": [[560, 570], ["range", "stag_hunt.StagHunt._is_visible", "range", "len"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt._is_visible"], ["", "def", "_intersect_targets", "(", "self", ",", "grid", ",", "agent_ids", ",", "targets", ",", "batch", "=", "0", ",", "target_id", "=", "0", ",", "targets_alive", "=", "None", ",", "offset", "=", "0", ")", ":", "\n", "        ", "\"\"\"\" Helper for get_obs_intersection(). \"\"\"", "\n", "for", "a", "in", "range", "(", "targets", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "marker", "=", "a", "+", "1", "if", "self", ".", "observe_ids", "else", "1", "\n", "if", "targets_alive", "is", "None", "or", "targets_alive", "[", "a", ",", "batch", "]", ":", "\n", "# If the target is visible to all agents", "\n", "                ", "if", "self", ".", "_is_visible", "(", "self", ".", "agents", "[", "agent_ids", ",", "batch", ",", ":", "]", ",", "targets", "[", "a", ",", "batch", ",", ":", "]", ")", ":", "\n", "# include the target in all observations (in relative positions)", "\n", "                    ", "for", "o", "in", "range", "(", "len", "(", "agent_ids", ")", ")", ":", "\n", "                        ", "grid", "[", "batch", ",", "targets", "[", "a", ",", "batch", ",", "0", "]", "+", "offset", ",", "targets", "[", "a", ",", "batch", ",", "1", "]", "+", "offset", ",", "target_id", "]", "=", "marker", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt._observe": [[571, 655], ["numpy.array", "numpy.zeros", "range", "len", "stag_hunt.StagHunt.get_avail_agent_actions", "all", "range", "obs.reshape.reshape.reshape", "numpy.zeros", "range", "obs.reshape.reshape.reshape", "stag_hunt.StagHunt._mask_invisible", "len", "numpy.reshape", "numpy.reshape", "numpy.reshape.dot", "stag_hunt.StagHunt._intersect_targets", "stag_hunt.StagHunt._intersect_targets", "stag_hunt.StagHunt._intersect_targets", "enumerate", "len", "obs.reshape.reshape.reshape", "obs.reshape.reshape.fill", "obs.reshape.reshape.reshape", "obs[].squeeze", "numpy.array", "numpy.array", "range", "stag_hunt.StagHunt._is_visible", "stag_hunt.StagHunt._mask_agent", "len", "stag_hunt.StagHunt.get_avail_agent_actions", "stag_hunt.StagHunt.get_avail_agent_actions", "range", "len"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.get_avail_agent_actions", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt._mask_invisible", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt._intersect_targets", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt._intersect_targets", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt._intersect_targets", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt._is_visible", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt._mask_agent", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.get_avail_agent_actions", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.get_avail_agent_actions"], ["", "", "", "", "", "def", "_observe", "(", "self", ",", "agent_ids", ")", ":", "\n", "# Compute available actions", "\n", "        ", "if", "len", "(", "agent_ids", ")", "==", "1", ":", "\n", "            ", "avail_all", "=", "self", ".", "get_avail_agent_actions", "(", "agent_ids", "[", "0", "]", ")", "\n", "", "elif", "len", "(", "agent_ids", ")", "==", "2", ":", "\n", "            ", "a_a1", "=", "np", ".", "reshape", "(", "np", ".", "array", "(", "self", ".", "get_avail_agent_actions", "(", "agent_ids", "[", "0", "]", ")", ")", ",", "[", "-", "1", ",", "1", "]", ")", "\n", "a_a2", "=", "np", ".", "reshape", "(", "np", ".", "array", "(", "self", ".", "get_avail_agent_actions", "(", "agent_ids", "[", "1", "]", ")", ")", ",", "[", "1", ",", "-", "1", "]", ")", "\n", "avail_actions", "=", "a_a1", ".", "dot", "(", "a_a2", ")", "\n", "avail_all", "=", "avail_actions", "*", "0", "+", "1", "\n", "", "else", ":", "\n", "            ", "avail_all", "=", "[", "]", "\n", "# Create over-sized grid", "\n", "", "ashape", "=", "np", ".", "array", "(", "self", ".", "agent_obs", ")", "\n", "ushape", "=", "self", ".", "grid_shape", "+", "2", "*", "ashape", "\n", "grid", "=", "np", ".", "zeros", "(", "(", "self", ".", "batch_size", ",", "ushape", "[", "0", "]", ",", "ushape", "[", "1", "]", ",", "self", ".", "n_feats", ")", ",", "dtype", "=", "float_type", ")", "\n", "# Make walls", "\n", "if", "self", ".", "observe_walls", ":", "\n", "            ", "wall_dim", "=", "3", "if", "self", ".", "observe_one_hot", "else", "0", "\n", "wall_id", "=", "1", "if", "self", ".", "observe_one_hot", "else", "-", "1", "\n", "grid", "[", ":", ",", ":", "ashape", "[", "0", "]", ",", ":", ",", "wall_dim", "]", "=", "wall_id", "\n", "grid", "[", ":", ",", "(", "self", ".", "grid_shape", "[", "0", "]", "+", "ashape", "[", "0", "]", ")", ":", ",", ":", ",", "wall_dim", "]", "=", "wall_id", "\n", "grid", "[", ":", ",", ":", ",", ":", "ashape", "[", "1", "]", ",", "wall_dim", "]", "=", "wall_id", "\n", "grid", "[", ":", ",", ":", ",", "(", "self", ".", "grid_shape", "[", "1", "]", "+", "ashape", "[", "1", "]", ")", ":", ",", "wall_dim", "]", "=", "wall_id", "\n", "# Mark the ghost-indicator, if specified", "\n", "", "if", "self", ".", "random_ghosts", ":", "\n", "            ", "pos", "=", "[", "ashape", "[", "i", "]", "+", "self", ".", "ghost_indicator_pos", "[", "i", "]", "for", "i", "in", "range", "(", "2", ")", "]", "\n", "grid", "[", "0", ",", "pos", "[", "0", "]", ",", "pos", "[", "1", "]", ",", "-", "1", "]", "=", "-", "1", "if", "self", ".", "ghost_indicator", "else", "1", "\n", "# Mark the grid with all intersected entities", "\n", "", "noinformation", "=", "False", "\n", "for", "b", "in", "range", "(", "self", ".", "batch_size", ")", ":", "\n", "            ", "if", "all", "(", "[", "self", ".", "_is_visible", "(", "self", ".", "agents", "[", "agent_ids", ",", "b", ",", ":", "]", ",", "self", ".", "agents", "[", "agent_ids", "[", "a", "]", ",", "b", ",", ":", "]", ")", "\n", "for", "a", "in", "range", "(", "len", "(", "agent_ids", ")", ")", "]", ")", ":", "\n", "# Every agent sees other intersected agents", "\n", "                ", "self", ".", "_intersect_targets", "(", "grid", ",", "agent_ids", ",", "targets", "=", "self", ".", "agents", ",", "batch", "=", "b", ",", "target_id", "=", "0", ",", "\n", "targets_alive", "=", "self", ".", "agents_not_frozen", ",", "offset", "=", "ashape", ")", "\n", "# Every agent sees intersected stags", "\n", "self", ".", "_intersect_targets", "(", "grid", ",", "agent_ids", ",", "targets", "=", "self", ".", "prey", "[", ":", "self", ".", "n_stags", ",", ":", ",", ":", "]", ",", "batch", "=", "b", ",", "target_id", "=", "1", ",", "\n", "targets_alive", "=", "self", ".", "prey_alive", "[", ":", "self", ".", "n_stags", ",", ":", "]", ",", "offset", "=", "ashape", ")", "\n", "# Every agent sees intersected hares", "\n", "self", ".", "_intersect_targets", "(", "grid", ",", "agent_ids", ",", "targets", "=", "self", ".", "prey", "[", "self", ".", "n_stags", ":", ",", ":", ",", ":", "]", ",", "batch", "=", "b", ",", "target_id", "=", "2", ",", "\n", "targets_alive", "=", "self", ".", "prey_alive", "[", "self", ".", "n_stags", ":", ",", ":", "]", ",", "offset", "=", "ashape", ")", "\n", "", "else", ":", "\n", "                ", "noinformation", "=", "True", "\n", "# Mask out all unknown if specified", "\n", "", "", "if", "self", ".", "intersection_unknown", ":", "\n", "            ", "for", "b", "in", "range", "(", "self", ".", "batch_size", ")", ":", "\n", "                ", "for", "a", "in", "agent_ids", ":", "\n", "                    ", "self", ".", "_mask_agent", "(", "grid", ",", "self", ".", "agents", "[", "a", ",", "b", ",", ":", "]", "+", "ashape", ",", "ashape", ")", "\n", "\n", "", "", "", "if", "self", ".", "intersection_global_view", ":", "\n", "# In case of the global view", "\n", "            ", "obs", "=", "grid", "[", ":", ",", "ashape", "[", "0", "]", ":", "(", "ashape", "[", "0", "]", "+", "self", ".", "grid_shape", "[", "0", "]", ")", ",", "ashape", "[", "1", "]", ":", "(", "ashape", "[", "1", "]", "+", "self", ".", "grid_shape", "[", "1", "]", ")", ",", ":", "]", "\n", "obs", "=", "obs", ".", "reshape", "(", "(", "1", ",", "self", ".", "batch_size", ",", "self", ".", "state_size", ")", ")", "\n", "", "else", ":", "\n", "# otherwise local view", "\n", "            ", "obs", "=", "np", ".", "zeros", "(", "(", "len", "(", "agent_ids", ")", ",", "self", ".", "batch_size", ",", "2", "*", "ashape", "[", "0", "]", "+", "1", ",", "2", "*", "ashape", "[", "1", "]", "+", "1", ",", "self", ".", "n_feats", ")", ",", "\n", "dtype", "=", "float_type", ")", "\n", "for", "b", "in", "range", "(", "self", ".", "batch_size", ")", ":", "\n", "                ", "for", "i", ",", "a", "in", "enumerate", "(", "agent_ids", ")", ":", "\n", "                    ", "obs", "[", "i", ",", "b", ",", ":", ",", ":", ",", ":", "]", "=", "grid", "[", "b", ",", "(", "self", ".", "agents", "[", "a", ",", "b", ",", "0", "]", ")", ":", "(", "self", ".", "agents", "[", "a", ",", "b", ",", "0", "]", "+", "2", "*", "ashape", "[", "0", "]", "+", "1", ")", ",", "\n", "(", "self", ".", "agents", "[", "a", ",", "b", ",", "1", "]", ")", ":", "(", "self", ".", "agents", "[", "a", ",", "b", ",", "1", "]", "+", "2", "*", "ashape", "[", "1", "]", "+", "1", ")", ",", ":", "]", "\n", "", "", "obs", "=", "obs", ".", "reshape", "(", "len", "(", "agent_ids", ")", ",", "self", ".", "batch_size", ",", "-", "1", ")", "\n", "\n", "# Final check: if not all agents can see each other, the mutual knowledge is empty", "\n", "", "if", "noinformation", ":", "\n", "            ", "if", "self", ".", "intersection_unknown", ":", "\n", "                ", "obs", "=", "obs", ".", "reshape", "(", "obs", ".", "shape", "[", "0", "]", ",", "obs", ".", "shape", "[", "1", "]", ",", "obs", ".", "shape", "[", "2", "]", "//", "self", ".", "n_feats", ",", "self", ".", "n_feats", ")", "\n", "unknown_dim", "=", "4", "if", "self", ".", "observe_one_hot", "else", "1", "\n", "unknown_id", "=", "1", "if", "self", ".", "observe_one_hot", "else", "-", "1", "\n", "obs", ".", "fill", "(", "0.0", ")", "\n", "obs", "[", ":", ",", ":", ",", ":", ",", "unknown_dim", "]", "=", "unknown_id", "\n", "obs", "=", "obs", ".", "reshape", "(", "obs", ".", "shape", "[", "0", "]", ",", "obs", ".", "shape", "[", "1", "]", ",", "self", ".", "n_feats", "*", "obs", ".", "shape", "[", "2", "]", ")", "\n", "", "else", ":", "\n", "                ", "obs", "=", "0", "*", "obs", "\n", "\n", "# Mask out everything that is not in the cone, if directed_observations=True", "\n", "", "", "if", "self", ".", "directed_observations", ":", "\n", "            ", "obs", "=", "self", ".", "_mask_invisible", "(", "obs", ",", "agent_ids", ")", "\n", "\n", "# Return considering batch-mode", "\n", "", "if", "self", ".", "batch_mode", ":", "\n", "            ", "return", "obs", ",", "avail_all", "\n", "", "else", ":", "\n", "            ", "return", "obs", "[", ":", ",", "0", ",", ":", "]", ".", "squeeze", "(", ")", ",", "avail_all", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt._mask_agent": [[656, 667], ["grid[].fill", "grid[].fill", "grid[].fill", "grid[].fill"], "methods", ["None"], ["", "", "def", "_mask_agent", "(", "self", ",", "grid", ",", "pos", ",", "ashape", ")", ":", "\n", "        ", "unknown_dim", "=", "4", "if", "self", ".", "observe_one_hot", "else", "1", "\n", "unknown_id", "=", "1", "if", "self", ".", "observe_one_hot", "else", "-", "1", "\n", "grid", "[", ":", ",", ":", "(", "pos", "[", "0", "]", "-", "ashape", "[", "0", "]", ")", ",", ":", ",", ":", "]", ".", "fill", "(", "0.0", ")", "\n", "grid", "[", ":", ",", ":", "(", "pos", "[", "0", "]", "-", "ashape", "[", "0", "]", ")", ",", ":", ",", "unknown_dim", "]", "=", "unknown_id", "\n", "grid", "[", ":", ",", "(", "pos", "[", "0", "]", "+", "ashape", "[", "0", "]", "+", "1", ")", ":", ",", ":", ",", ":", "]", ".", "fill", "(", "0.0", ")", "\n", "grid", "[", ":", ",", "(", "pos", "[", "0", "]", "+", "ashape", "[", "0", "]", "+", "1", ")", ":", ",", ":", ",", "unknown_dim", "]", "=", "unknown_id", "\n", "grid", "[", ":", ",", ":", ",", ":", "(", "pos", "[", "1", "]", "-", "ashape", "[", "1", "]", ")", ",", ":", "]", ".", "fill", "(", "0", ")", "\n", "grid", "[", ":", ",", ":", ",", ":", "(", "pos", "[", "1", "]", "-", "ashape", "[", "1", "]", ")", ",", "unknown_dim", "]", "=", "unknown_id", "\n", "grid", "[", ":", ",", ":", ",", "(", "pos", "[", "1", "]", "+", "ashape", "[", "1", "]", "+", "1", ")", ":", ",", ":", "]", ".", "fill", "(", "0.0", ")", "\n", "grid", "[", ":", ",", ":", ",", "(", "pos", "[", "1", "]", "+", "ashape", "[", "1", "]", "+", "1", ")", ":", ",", "unknown_dim", "]", "=", "unknown_id", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt._mask_invisible": [[668, 695], ["numpy.reshape", "range", "vis.reshape", "tuple", "numpy.ones", "enumerate", "list", "range", "range", "range", "range"], "methods", ["None"], ["", "def", "_mask_invisible", "(", "self", ",", "obs", ",", "agent_ids", ")", ":", "\n", "        ", "\"\"\" Generates new observations from obs that only contain the visible cone. \"\"\"", "\n", "narrow", "=", "1", "if", "self", ".", "directed_cone_narrow", "else", "0", "\n", "dim", "=", "list", "(", "obs", ".", "shape", "[", ":", "2", "]", ")", "+", "[", "2", "*", "i", "+", "1", "for", "i", "in", "self", ".", "agent_obs", "]", "+", "[", "self", ".", "n_feats", "]", "\n", "obs", "=", "np", ".", "reshape", "(", "obs", ",", "tuple", "(", "dim", ")", ")", "\n", "vis", "=", "-", "np", ".", "ones", "(", "(", "dim", "[", "0", "]", ",", "dim", "[", "1", "]", ",", "2", "*", "self", ".", "agent_obs", "[", "0", "]", "+", "1", "-", "2", "*", "narrow", ",", "2", "*", "self", ".", "agent_obs", "[", "1", "]", "+", "1", "-", "2", "*", "narrow", ",", "self", ".", "n_feats", ")", ")", "\n", "for", "b", "in", "range", "(", "dim", "[", "1", "]", ")", ":", "\n", "            ", "for", "i", ",", "a", "in", "enumerate", "(", "agent_ids", ")", ":", "\n", "                ", "if", "self", ".", "agents_orientation", "[", "a", ",", "b", "]", "==", "self", ".", "action_labels", "[", "'up'", "]", ":", "\n", "                    ", "for", "j", "in", "range", "(", "self", ".", "agent_obs", "[", "0", "]", "+", "1", "-", "narrow", ")", ":", "\n", "                        ", "vis", "[", "i", ",", "b", ",", "j", ",", "j", ":", "(", "vis", ".", "shape", "[", "3", "]", "-", "j", ")", ",", ":", "]", "=", "obs", "[", "i", ",", "b", ",", "j", ",", "(", "j", "+", "narrow", ")", ":", "(", "obs", ".", "shape", "[", "3", "]", "-", "j", "-", "narrow", ")", ",", ":", "]", "\n", "", "", "elif", "self", ".", "agents_orientation", "[", "a", ",", "b", "]", "==", "self", ".", "action_labels", "[", "'down'", "]", ":", "\n", "                    ", "for", "j", "in", "range", "(", "self", ".", "agent_obs", "[", "0", "]", "+", "1", "-", "narrow", ")", ":", "\n", "                        ", "vis", "[", "i", ",", "b", ",", "-", "j", "-", "1", ",", "j", ":", "(", "vis", ".", "shape", "[", "3", "]", "-", "j", ")", ",", ":", "]", "=", "obs", "[", "i", ",", "b", ",", "-", "j", "-", "1", ",", "(", "j", "+", "narrow", ")", ":", "(", "obs", ".", "shape", "[", "3", "]", "-", "j", "-", "narrow", ")", ",", ":", "]", "\n", "", "", "elif", "self", ".", "agents_orientation", "[", "a", ",", "b", "]", "==", "self", ".", "action_labels", "[", "'left'", "]", ":", "\n", "                    ", "for", "j", "in", "range", "(", "self", ".", "agent_obs", "[", "0", "]", "+", "1", "-", "narrow", ")", ":", "\n", "                        ", "vis", "[", "i", ",", "b", ",", "j", ":", "(", "vis", ".", "shape", "[", "2", "]", "-", "j", ")", ",", "j", ",", ":", "]", "=", "obs", "[", "i", ",", "b", ",", "(", "j", "+", "narrow", ")", ":", "(", "obs", ".", "shape", "[", "2", "]", "-", "j", "-", "narrow", ")", ",", "j", ",", ":", "]", "\n", "", "", "elif", "self", ".", "agents_orientation", "[", "a", ",", "b", "]", "==", "self", ".", "action_labels", "[", "'right'", "]", ":", "\n", "                    ", "for", "j", "in", "range", "(", "self", ".", "agent_obs", "[", "0", "]", "+", "1", "-", "narrow", ")", ":", "\n", "                        ", "vis", "[", "i", ",", "b", ",", "j", ":", "(", "vis", ".", "shape", "[", "2", "]", "-", "j", ")", ",", "-", "j", "-", "1", ",", ":", "]", "=", "obs", "[", "i", ",", "b", ",", "(", "j", "+", "narrow", ")", ":", "(", "obs", ".", "shape", "[", "2", "]", "-", "j", "-", "narrow", ")", ",", "-", "j", "-", "1", ",", ":", "]", "\n", "", "", "else", ":", "\n", "                    ", "assert", "True", ",", "\"Agent directions need to be 0..3!\"", "\n", "", "", "", "return", "vis", ".", "reshape", "(", "dim", "[", ":", "2", "]", "+", "[", "-", "1", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt._get_obs_from_grid": [[696, 702], ["stag_hunt.StagHunt._get_obs_from_grid_troidal", "stag_hunt.StagHunt._get_obs_from_grid_bounded"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt._get_obs_from_grid_troidal", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt._get_obs_from_grid_bounded"], ["", "def", "_get_obs_from_grid", "(", "self", ",", "grid", ",", "agent_id", ",", "batch", "=", "0", ")", ":", "\n", "        ", "\"\"\" OBSOLETE! \"\"\"", "\n", "if", "self", ".", "toroidal", ":", "\n", "            ", "return", "self", ".", "_get_obs_from_grid_troidal", "(", "grid", ",", "agent_id", ",", "batch", ")", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "_get_obs_from_grid_bounded", "(", "grid", ",", "agent_id", ",", "batch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt._get_obs_from_grid_bounded": [[703, 722], ["numpy.zeros", "numpy.maximum", "numpy.minimum", "numpy.reshape"], "methods", ["None"], ["", "", "def", "_get_obs_from_grid_bounded", "(", "self", ",", "grid", ",", "agent_id", ",", "batch", "=", "0", ")", ":", "\n", "        ", "\"\"\" Return a bounded observation for other agents' locations and targets, the size specified by observation\n            shape, centered on the agent. Values outside the bounds of the grid are set to 0.\n            OBSOLETE! \"\"\"", "\n", "# Create the empty observation grid", "\n", "agent_obs", "=", "np", ".", "zeros", "(", "(", "2", "*", "self", ".", "agent_obs", "[", "0", "]", "+", "1", ",", "2", "*", "self", ".", "agent_obs", "[", "1", "]", "+", "1", ",", "3", ")", ",", "dtype", "=", "float_type", ")", "\n", "# Determine the unbounded limits of the agent's observation", "\n", "ul", "=", "self", ".", "agents", "[", "agent_id", ",", "batch", ",", ":", "]", "-", "self", ".", "agent_obs", "\n", "lr", "=", "self", ".", "agents", "[", "agent_id", ",", "batch", ",", ":", "]", "+", "self", ".", "agent_obs", "\n", "# Bound the limits to the grid", "\n", "bul", "=", "np", ".", "maximum", "(", "ul", ",", "[", "0", ",", "0", "]", ")", "\n", "blr", "=", "np", ".", "minimum", "(", "lr", ",", "self", ".", "grid_shape", "-", "1", ")", "\n", "# Compute the ranges in x/y direction in the agent's observation", "\n", "bias", "=", "bul", "-", "ul", "\n", "aoy", "=", "[", "bias", "[", "0", "]", ",", "blr", "[", "0", "]", "-", "bul", "[", "0", "]", "+", "bias", "[", "0", "]", "]", "\n", "aox", "=", "[", "bias", "[", "1", "]", ",", "blr", "[", "1", "]", "-", "bul", "[", "1", "]", "+", "bias", "[", "1", "]", "]", "\n", "# Copy the bounded observations", "\n", "agent_obs", "[", "aoy", "[", "0", "]", ":", "(", "aoy", "[", "1", "]", "+", "1", ")", ",", "aox", "[", "0", "]", ":", "(", "aox", "[", "1", "]", "+", "1", ")", ",", ":", "]", "=", "grid", "[", "batch", ",", "bul", "[", "0", "]", ":", "(", "blr", "[", "0", "]", "+", "1", ")", ",", "bul", "[", "1", "]", ":", "(", "blr", "[", "1", "]", "+", "1", ")", ",", ":", "]", "\n", "return", "np", ".", "reshape", "(", "agent_obs", ",", "self", ".", "obs_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt._get_obs_from_grid_troidal": [[723, 734], ["range", "range", "grid[].astype", "grid[].astype.take().take", "numpy.reshape", "grid[].astype.take"], "methods", ["None"], ["", "def", "_get_obs_from_grid_troidal", "(", "self", ",", "grid", ",", "agent_id", ",", "batch", "=", "0", ")", ":", "\n", "        ", "\"\"\" Return a wrapped observation for other agents' locations and targets, the size specified by observation\n            shape, centered on the agent.\n            OBSOLETE! \"\"\"", "\n", "a_x", ",", "a_y", "=", "self", ".", "agents", "[", "agent_id", ",", "batch", ",", ":", "]", "\n", "o_x", ",", "o_y", "=", "self", ".", "agent_obs", "\n", "x_range", "=", "range", "(", "(", "a_x", "-", "o_x", ")", ",", "(", "a_x", "+", "o_x", "+", "1", ")", ")", "\n", "y_range", "=", "range", "(", "(", "a_y", "-", "o_y", ")", ",", "(", "a_y", "+", "o_y", "+", "1", ")", ")", "\n", "ex_grid", "=", "grid", "[", "batch", ",", ":", ",", ":", ",", ":", "]", ".", "astype", "(", "dtype", "=", "float_type", ")", "\n", "agent_obs", "=", "ex_grid", ".", "take", "(", "x_range", ",", "0", ",", "mode", "=", "'wrap'", ")", ".", "take", "(", "y_range", ",", "1", ",", "mode", "=", "'wrap'", ")", "\n", "return", "np", ".", "reshape", "(", "agent_obs", ",", "self", ".", "obs_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt._get_obs_intersection_old": [[735, 776], ["numpy.zeros", "numpy.reshape", "numpy.reshape", "numpy.reshape.dot", "range", "numpy.array", "numpy.array", "all", "numpy.zeros", "range", "stag_hunt.StagHunt.get_avail_agent_actions", "stag_hunt.StagHunt.get_avail_agent_actions", "stag_hunt.StagHunt._intersect_targets", "stag_hunt.StagHunt._intersect_targets", "range", "stag_hunt.StagHunt._is_visible", "numpy.zeros.reshape", "grid[].reshape", "len", "len", "stag_hunt.StagHunt._get_obs_from_grid", "range", "len"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.get_avail_agent_actions", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.get_avail_agent_actions", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt._intersect_targets", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt._intersect_targets", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt._is_visible", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt._get_obs_from_grid"], ["", "def", "_get_obs_intersection_old", "(", "self", ",", "agent_ids", ")", ":", "\n", "        ", "\"\"\" Returns the intersection of the all of agent_ids agents' observations.\n            OBSOLETE, only maintained for legacy issues! \"\"\"", "\n", "# Create grid", "\n", "grid", "=", "np", ".", "zeros", "(", "(", "self", ".", "batch_size", ",", "self", ".", "grid_shape", "[", "0", "]", ",", "self", ".", "grid_shape", "[", "0", "]", ",", "3", ")", ",", "dtype", "=", "float_type", ")", "\n", "\n", "a_a1", "=", "np", ".", "reshape", "(", "np", ".", "array", "(", "self", ".", "get_avail_agent_actions", "(", "agent_ids", "[", "0", "]", ")", ")", ",", "[", "-", "1", ",", "1", "]", ")", "\n", "a_a2", "=", "np", ".", "reshape", "(", "np", ".", "array", "(", "self", ".", "get_avail_agent_actions", "(", "agent_ids", "[", "1", "]", ")", ")", ",", "[", "1", ",", "-", "1", "]", ")", "\n", "avail_actions", "=", "a_a1", ".", "dot", "(", "a_a2", ")", "\n", "avail_all", "=", "avail_actions", "*", "0", "+", "1", "\n", "# If all agent_ids can see each other (otherwise the observation is empty)", "\n", "for", "b", "in", "range", "(", "self", ".", "batch_size", ")", ":", "\n", "            ", "if", "all", "(", "[", "self", ".", "_is_visible", "(", "self", ".", "agents", "[", "agent_ids", ",", "b", ",", ":", "]", ",", "self", ".", "agents", "[", "agent_ids", "[", "a", "]", ",", "b", ",", ":", "]", ")", "\n", "for", "a", "in", "range", "(", "len", "(", "agent_ids", ")", ")", "]", ")", ":", "\n", "# Every agent sees other intersected agents", "\n", "                ", "self", ".", "_intersect_targets", "(", "grid", ",", "agent_ids", ",", "targets", "=", "self", ".", "agents", ",", "batch", "=", "b", ",", "target_id", "=", "0", ")", "\n", "# Every agent sees intersected prey", "\n", "self", ".", "_intersect_targets", "(", "grid", ",", "agent_ids", ",", "targets", "=", "self", ".", "prey", ",", "batch", "=", "b", ",", "target_id", "=", "1", ",", "\n", "targets_alive", "=", "self", ".", "prey_alive", ")", "\n", "avail_all", "=", "avail_actions", "\n", "# Return 0-1 encoded intersection if necessary", "\n", "", "", "if", "not", "self", ".", "observe_ids", ":", "\n", "            ", "grid", "=", "(", "grid", "!=", "0.0", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "# The intersection grid is constructed, now we have to generate the observations from it", "\n", "", "if", "self", ".", "intersection_global_view", ":", "\n", "# Return the intersection as a state", "\n", "            ", "if", "self", ".", "batch_mode", ":", "\n", "                ", "return", "grid", ".", "reshape", "(", "(", "self", ".", "batch_size", ",", "self", ".", "state_size", ")", ")", ",", "avail_all", "\n", "", "else", ":", "\n", "                ", "return", "grid", "[", "0", ",", ":", ",", ":", ",", ":", "]", ".", "reshape", "(", "self", ".", "state_size", ")", ",", "avail_all", "\n", "", "", "else", ":", "\n", "# Return the intersection as individual observations", "\n", "            ", "obs", "=", "np", ".", "zeros", "(", "(", "len", "(", "agent_ids", ")", ",", "self", ".", "batch_size", ",", "self", ".", "obs_size", ")", ",", "\n", "dtype", "=", "float_type", ")", "\n", "for", "b", "in", "range", "(", "self", ".", "batch_size", ")", ":", "\n", "                ", "for", "a", "in", "range", "(", "len", "(", "agent_ids", ")", ")", ":", "\n", "                    ", "obs", "[", "a", ",", "b", ",", ":", "]", "=", "self", ".", "_get_obs_from_grid", "(", "grid", ",", "a", ",", "b", ")", "\n", "", "", "if", "self", ".", "batch_mode", ":", "\n", "                ", "return", "obs", ",", "avail_all", "\n", "", "else", ":", "\n", "                ", "return", "obs", "[", ":", ",", "0", ",", ":", "]", ",", "avail_all", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt.state_to_graph": [[777, 779], ["None"], "methods", ["None"], ["", "", "", "def", "state_to_graph", "(", "self", ",", "state_list", ")", ":", "\n", "        ", "raise", "Exception", "\n", "#     return build_stag_hunt_env_graph(state_list, self.args)", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt.get_action_id": [[782, 785], ["None"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "get_action_id", "(", "cls", ",", "label", ")", ":", "\n", "        ", "return", "cls", ".", "action_labels", "[", "label", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.__init__": [[7, 25], ["isinstance", "numpy.array", "numpy.ones", "utils.dict2namedtuple.convert"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.dict2namedtuple.convert"], ["    ", "def", "__init__", "(", "self", ",", "batch_size", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "# Unpack arguments from sacred", "\n", "        ", "args", "=", "kwargs", "[", "\"env_args\"", "]", "\n", "if", "isinstance", "(", "args", ",", "dict", ")", ":", "\n", "            ", "args", "=", "convert", "(", "args", ")", "\n", "\n", "# Define the agents and actions", "\n", "", "self", ".", "n_agents", "=", "2", "\n", "self", ".", "n_actions", "=", "3", "\n", "self", ".", "episode_limit", "=", "1", "\n", "\n", "self", ".", "payoff_matrix", "=", "np", ".", "array", "(", "[", "\n", "[", "8", ",", "-", "12", ",", "-", "12", "]", ",", "\n", "[", "-", "12", ",", "0", ",", "0", "]", ",", "\n", "[", "-", "12", ",", "0", ",", "0", "]", "\n", "]", ")", "\n", "\n", "self", ".", "state", "=", "np", ".", "ones", "(", "5", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.reset": [[26, 29], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "\"\"\" Returns initial observations and states\"\"\"", "\n", "return", "self", ".", "state", ",", "self", ".", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.step": [[30, 39], ["None"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "actions", ")", ":", "\n", "        ", "\"\"\" Returns reward, terminated, info \"\"\"", "\n", "reward", "=", "self", ".", "payoff_matrix", "[", "actions", "[", "0", "]", ",", "actions", "[", "1", "]", "]", "\n", "\n", "info", "=", "{", "}", "\n", "terminated", "=", "True", "\n", "info", "[", "\"episode_limit\"", "]", "=", "False", "\n", "\n", "return", "reward", ",", "terminated", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.get_obs": [[40, 42], ["range"], "methods", ["None"], ["", "def", "get_obs", "(", "self", ")", ":", "\n", "        ", "return", "[", "self", ".", "state", "for", "_", "in", "range", "(", "self", ".", "n_agents", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.get_obs_agent": [[43, 46], ["None"], "methods", ["None"], ["", "def", "get_obs_agent", "(", "self", ",", "agent_id", ")", ":", "\n", "        ", "\"\"\" Returns observation for agent_id \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.get_obs_size": [[47, 50], ["matrix_game_simple.Matrixgame.get_state_size"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.get_state_size"], ["", "def", "get_obs_size", "(", "self", ")", ":", "\n", "        ", "\"\"\" Returns the shape of the observation \"\"\"", "\n", "return", "self", ".", "get_state_size", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.get_state": [[51, 53], ["None"], "methods", ["None"], ["", "def", "get_state", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.get_state_size": [[54, 57], ["len"], "methods", ["None"], ["", "def", "get_state_size", "(", "self", ")", ":", "\n", "        ", "\"\"\" Returns the shape of the state\"\"\"", "\n", "return", "len", "(", "self", ".", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.get_avail_actions": [[58, 64], ["range", "matrix_game_simple.Matrixgame.get_avail_agent_actions", "avail_actions.append"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.get_avail_agent_actions"], ["", "def", "get_avail_actions", "(", "self", ")", ":", "\n", "        ", "avail_actions", "=", "[", "]", "\n", "for", "agent_id", "in", "range", "(", "self", ".", "n_agents", ")", ":", "\n", "            ", "avail_agent", "=", "self", ".", "get_avail_agent_actions", "(", "agent_id", ")", "\n", "avail_actions", ".", "append", "(", "avail_agent", ")", "\n", "", "return", "avail_actions", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.get_avail_agent_actions": [[65, 68], ["numpy.ones"], "methods", ["None"], ["", "def", "get_avail_agent_actions", "(", "self", ",", "agent_id", ")", ":", "\n", "        ", "\"\"\" Returns the available actions for agent_id \"\"\"", "\n", "return", "np", ".", "ones", "(", "self", ".", "n_actions", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.get_total_actions": [[69, 73], ["None"], "methods", ["None"], ["", "def", "get_total_actions", "(", "self", ")", ":", "\n", "        ", "\"\"\" Returns the total number of actions an agent could ever take \"\"\"", "\n", "# TODO: This is only suitable for a discrete 1 dimensional action space for each agent", "\n", "return", "self", ".", "n_actions", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.get_stats": [[74, 76], ["None"], "methods", ["None"], ["", "def", "get_stats", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.parallel_runner.ParallelRunner.__init__": [[13, 50], ["zip", "parallel_runner.ParallelRunner.parent_conns[].send", "parallel_runner.ParallelRunner.parent_conns[].recv", "p.start", "multiprocessing.Process", "multiprocessing.Process", "multiprocessing.Pipe", "range", "parallel_runner.CloudpickleWrapper", "parallel_runner.CloudpickleWrapper", "functools.partial", "functools.partial"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "logger", ")", ":", "\n", "        ", "self", ".", "args", "=", "args", "\n", "self", ".", "logger", "=", "logger", "\n", "self", ".", "batch_size", "=", "self", ".", "args", ".", "batch_size_run", "\n", "\n", "# Make subprocesses for the envs", "\n", "# TODO: Add a delay when making sc2 envs", "\n", "self", ".", "parent_conns", ",", "self", ".", "worker_conns", "=", "zip", "(", "*", "[", "Pipe", "(", ")", "for", "_", "in", "range", "(", "self", ".", "batch_size", ")", "]", ")", "\n", "env_fn", "=", "env_REGISTRY", "[", "self", ".", "args", ".", "env", "]", "\n", "if", "'sc2'", "in", "self", ".", "args", ".", "env", ":", "\n", "            ", "self", ".", "ps", "=", "[", "Process", "(", "target", "=", "env_worker", ",", "args", "=", "(", "worker_conn", ",", "CloudpickleWrapper", "(", "partial", "(", "env_fn", ",", "**", "self", ".", "args", ".", "env_args", ")", ")", ")", ")", "\n", "for", "worker_conn", "in", "self", ".", "worker_conns", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "ps", "=", "[", "Process", "(", "target", "=", "env_worker", ",", "args", "=", "(", "worker_conn", ",", "CloudpickleWrapper", "(", "partial", "(", "env_fn", ",", "env_args", "=", "self", ".", "args", ".", "env_args", ",", "args", "=", "self", ".", "args", ")", ")", ")", ")", "\n", "for", "worker_conn", "in", "self", ".", "worker_conns", "]", "\n", "\n", "", "for", "p", "in", "self", ".", "ps", ":", "\n", "            ", "p", ".", "daemon", "=", "True", "\n", "p", ".", "start", "(", ")", "\n", "\n", "# TODO: Close stuff if appropriate", "\n", "\n", "", "self", ".", "parent_conns", "[", "0", "]", ".", "send", "(", "(", "\"get_env_info\"", ",", "None", ")", ")", "\n", "self", ".", "env_info", "=", "self", ".", "parent_conns", "[", "0", "]", ".", "recv", "(", ")", "\n", "self", ".", "episode_limit", "=", "self", ".", "env_info", "[", "\"episode_limit\"", "]", "\n", "\n", "# TODO: Will have to add stuff to episode batch for envs that terminate at different times to ensure filled is correct", "\n", "self", ".", "t", "=", "0", "\n", "\n", "self", ".", "t_env", "=", "0", "\n", "\n", "self", ".", "train_returns", "=", "[", "]", "\n", "self", ".", "test_returns", "=", "[", "]", "\n", "self", ".", "train_stats", "=", "{", "}", "\n", "self", ".", "test_stats", "=", "{", "}", "\n", "\n", "self", ".", "log_train_stats_t", "=", "-", "100000", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.parallel_runner.ParallelRunner.setup": [[51, 59], ["functools.partial"], "methods", ["None"], ["", "def", "setup", "(", "self", ",", "scheme", ",", "groups", ",", "preprocess", ",", "mac", ")", ":", "\n", "        ", "self", ".", "new_batch", "=", "partial", "(", "EpisodeBatch", ",", "scheme", ",", "groups", ",", "self", ".", "batch_size", ",", "self", ".", "episode_limit", "+", "1", ",", "\n", "preprocess", "=", "preprocess", ",", "device", "=", "self", ".", "args", ".", "device", ")", "\n", "self", ".", "mac", "=", "mac", "\n", "# TODO: Remove these if the runner doesn't need them", "\n", "self", ".", "scheme", "=", "scheme", "\n", "self", ".", "groups", "=", "groups", "\n", "self", ".", "preprocess", "=", "preprocess", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.parallel_runner.ParallelRunner.get_env_info": [[60, 62], ["None"], "methods", ["None"], ["", "def", "get_env_info", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "env_info", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.parallel_runner.ParallelRunner.save_replay": [[63, 65], ["None"], "methods", ["None"], ["", "def", "save_replay", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.parallel_runner.ParallelRunner.close_env": [[66, 69], ["parent_conn.send"], "methods", ["None"], ["", "def", "close_env", "(", "self", ")", ":", "\n", "        ", "for", "parent_conn", "in", "self", ".", "parent_conns", ":", "\n", "            ", "parent_conn", ".", "send", "(", "(", "\"close\"", ",", "None", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.parallel_runner.ParallelRunner.reset": [[70, 93], ["parallel_runner.ParallelRunner.new_batch", "parallel_runner.ParallelRunner.batch.update", "parent_conn.send", "parent_conn.recv", "pre_transition_data[].append", "pre_transition_data[].append", "pre_transition_data[].append"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.EpisodeBatch.update"], ["", "", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "batch", "=", "self", ".", "new_batch", "(", ")", "\n", "\n", "# Reset the envs", "\n", "for", "parent_conn", "in", "self", ".", "parent_conns", ":", "\n", "            ", "parent_conn", ".", "send", "(", "(", "\"reset\"", ",", "None", ")", ")", "\n", "\n", "", "pre_transition_data", "=", "{", "\n", "\"state\"", ":", "[", "]", ",", "\n", "\"avail_actions\"", ":", "[", "]", ",", "\n", "\"obs\"", ":", "[", "]", "\n", "}", "\n", "# Get the obs, state and avail_actions back", "\n", "for", "parent_conn", "in", "self", ".", "parent_conns", ":", "\n", "            ", "data", "=", "parent_conn", ".", "recv", "(", ")", "\n", "pre_transition_data", "[", "\"state\"", "]", ".", "append", "(", "data", "[", "\"state\"", "]", ")", "\n", "pre_transition_data", "[", "\"avail_actions\"", "]", ".", "append", "(", "data", "[", "\"avail_actions\"", "]", ")", "\n", "pre_transition_data", "[", "\"obs\"", "]", ".", "append", "(", "data", "[", "\"obs\"", "]", ")", "\n", "\n", "", "self", ".", "batch", ".", "update", "(", "pre_transition_data", ",", "ts", "=", "0", ")", "\n", "\n", "self", ".", "t", "=", "0", "\n", "self", ".", "env_steps_this_run", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.parallel_runner.ParallelRunner.run": [[94, 213], ["parallel_runner.ParallelRunner.reset", "parallel_runner.ParallelRunner.mac.init_hidden", "cur_stats.update", "cur_returns.extend", "parallel_runner.ParallelRunner.mac.select_actions", "parallel_runner.ParallelRunner.to().numpy", "parallel_runner.ParallelRunner.batch.update", "enumerate", "all", "enumerate", "parallel_runner.ParallelRunner.batch.update", "parallel_runner.ParallelRunner.batch.update", "parent_conn.send", "parent_conn.recv", "env_stats.append", "cur_stats.get", "sum", "cur_stats.get", "max", "parallel_runner.ParallelRunner._log", "range", "range", "range", "enumerate", "parallel_runner.ParallelRunner.unsqueeze", "sum", "len", "parallel_runner.ParallelRunner._log", "hasattr", "parallel_runner.ParallelRunner.to", "enumerate", "parent_conn.recv", "post_transition_data[].append", "post_transition_data[].append", "pre_transition_data[].append", "pre_transition_data[].append", "pre_transition_data[].append", "set.union", "parallel_runner.ParallelRunner.logger.log_stat", "parent_conn.send", "final_env_infos.append", "d.get", "data[].get", "set"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner.reset", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.agents.central_rnn_agent.CentralRNNAgent.init_hidden", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.EpisodeBatch.update", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.EpisodeBatch.extend", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.select_actions", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.EpisodeBatch.update", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.EpisodeBatch.update", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.EpisodeBatch.update", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner._log", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner._log", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.EpisodeBatch.to", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat"], ["", "def", "run", "(", "self", ",", "test_mode", "=", "False", ")", ":", "\n", "        ", "self", ".", "reset", "(", ")", "\n", "\n", "all_terminated", "=", "False", "\n", "episode_returns", "=", "[", "0", "for", "_", "in", "range", "(", "self", ".", "batch_size", ")", "]", "\n", "episode_lengths", "=", "[", "0", "for", "_", "in", "range", "(", "self", ".", "batch_size", ")", "]", "\n", "self", ".", "mac", ".", "init_hidden", "(", "batch_size", "=", "self", ".", "batch_size", ")", "\n", "terminated", "=", "[", "False", "for", "_", "in", "range", "(", "self", ".", "batch_size", ")", "]", "\n", "envs_not_terminated", "=", "[", "b_idx", "for", "b_idx", ",", "termed", "in", "enumerate", "(", "terminated", ")", "if", "not", "termed", "]", "\n", "final_env_infos", "=", "[", "]", "# may store extra stats like battle won. this is filled in ORDER OF TERMINATION", "\n", "\n", "while", "True", ":", "\n", "\n", "# Pass the entire batch of experiences up till now to the agents", "\n", "# Receive the actions for each agent at this timestep in a batch for each un-terminated env", "\n", "            ", "actions", "=", "self", ".", "mac", ".", "select_actions", "(", "self", ".", "batch", ",", "t_ep", "=", "self", ".", "t", ",", "t_env", "=", "self", ".", "t_env", ",", "bs", "=", "envs_not_terminated", ",", "test_mode", "=", "test_mode", ")", "\n", "cpu_actions", "=", "actions", ".", "to", "(", "\"cpu\"", ")", ".", "numpy", "(", ")", "\n", "\n", "# Update the actions taken", "\n", "actions_chosen", "=", "{", "\n", "\"actions\"", ":", "actions", ".", "unsqueeze", "(", "1", ")", "\n", "}", "\n", "self", ".", "batch", ".", "update", "(", "actions_chosen", ",", "bs", "=", "envs_not_terminated", ",", "ts", "=", "self", ".", "t", ",", "mark_filled", "=", "False", ")", "\n", "\n", "# Send actions to each env", "\n", "action_idx", "=", "0", "\n", "for", "idx", ",", "parent_conn", "in", "enumerate", "(", "self", ".", "parent_conns", ")", ":", "\n", "                ", "if", "idx", "in", "envs_not_terminated", ":", "# We produced actions for this env", "\n", "                    ", "if", "not", "terminated", "[", "idx", "]", ":", "# Only send the actions to the env if it hasn't terminated", "\n", "                        ", "parent_conn", ".", "send", "(", "(", "\"step\"", ",", "cpu_actions", "[", "action_idx", "]", ")", ")", "\n", "", "action_idx", "+=", "1", "# actions is not a list over every env", "\n", "\n", "# Post step data we will insert for the current timestep", "\n", "", "", "post_transition_data", "=", "{", "\n", "# \"actions\": actions.unsqueeze(1),", "\n", "\"reward\"", ":", "[", "]", ",", "\n", "\"terminated\"", ":", "[", "]", "\n", "}", "\n", "# Data for the next step we will insert in order to select an action", "\n", "pre_transition_data", "=", "{", "\n", "\"state\"", ":", "[", "]", ",", "\n", "\"avail_actions\"", ":", "[", "]", ",", "\n", "\"obs\"", ":", "[", "]", "\n", "}", "\n", "\n", "# Update terminated envs after adding post_transition_data", "\n", "envs_not_terminated", "=", "[", "b_idx", "for", "b_idx", ",", "termed", "in", "enumerate", "(", "terminated", ")", "if", "not", "termed", "]", "\n", "all_terminated", "=", "all", "(", "terminated", ")", "\n", "if", "all_terminated", ":", "\n", "                ", "break", "\n", "\n", "# Receive data back for each unterminated env", "\n", "", "for", "idx", ",", "parent_conn", "in", "enumerate", "(", "self", ".", "parent_conns", ")", ":", "\n", "                ", "if", "not", "terminated", "[", "idx", "]", ":", "\n", "                    ", "data", "=", "parent_conn", ".", "recv", "(", ")", "\n", "# Remaining data for this current timestep", "\n", "post_transition_data", "[", "\"reward\"", "]", ".", "append", "(", "(", "data", "[", "\"reward\"", "]", ",", ")", ")", "\n", "\n", "episode_returns", "[", "idx", "]", "+=", "data", "[", "\"reward\"", "]", "\n", "episode_lengths", "[", "idx", "]", "+=", "1", "\n", "if", "not", "test_mode", ":", "\n", "                        ", "self", ".", "env_steps_this_run", "+=", "1", "\n", "\n", "", "env_terminated", "=", "False", "\n", "if", "data", "[", "\"terminated\"", "]", ":", "\n", "                        ", "final_env_infos", ".", "append", "(", "data", "[", "\"info\"", "]", ")", "\n", "", "if", "data", "[", "\"terminated\"", "]", "and", "not", "data", "[", "\"info\"", "]", ".", "get", "(", "\"episode_limit\"", ",", "False", ")", ":", "\n", "                        ", "env_terminated", "=", "True", "\n", "", "terminated", "[", "idx", "]", "=", "data", "[", "\"terminated\"", "]", "\n", "post_transition_data", "[", "\"terminated\"", "]", ".", "append", "(", "(", "env_terminated", ",", ")", ")", "\n", "\n", "# Data for the next timestep needed to select an action", "\n", "pre_transition_data", "[", "\"state\"", "]", ".", "append", "(", "data", "[", "\"state\"", "]", ")", "\n", "pre_transition_data", "[", "\"avail_actions\"", "]", ".", "append", "(", "data", "[", "\"avail_actions\"", "]", ")", "\n", "pre_transition_data", "[", "\"obs\"", "]", ".", "append", "(", "data", "[", "\"obs\"", "]", ")", "\n", "\n", "# Add post_transiton data into the batch", "\n", "", "", "self", ".", "batch", ".", "update", "(", "post_transition_data", ",", "bs", "=", "envs_not_terminated", ",", "ts", "=", "self", ".", "t", ",", "mark_filled", "=", "False", ")", "\n", "\n", "# Move onto the next timestep", "\n", "self", ".", "t", "+=", "1", "\n", "\n", "# Add the pre-transition data", "\n", "\n", "self", ".", "batch", ".", "update", "(", "pre_transition_data", ",", "bs", "=", "envs_not_terminated", ",", "ts", "=", "self", ".", "t", ",", "mark_filled", "=", "True", ")", "\n", "\n", "\n", "", "if", "not", "test_mode", ":", "\n", "            ", "self", ".", "t_env", "+=", "self", ".", "env_steps_this_run", "\n", "\n", "# Get stats back for each env", "\n", "", "for", "parent_conn", "in", "self", ".", "parent_conns", ":", "\n", "            ", "parent_conn", ".", "send", "(", "(", "\"get_stats\"", ",", "None", ")", ")", "\n", "\n", "", "env_stats", "=", "[", "]", "\n", "for", "parent_conn", "in", "self", ".", "parent_conns", ":", "\n", "            ", "env_stat", "=", "parent_conn", ".", "recv", "(", ")", "\n", "env_stats", ".", "append", "(", "env_stat", ")", "\n", "\n", "", "cur_stats", "=", "self", ".", "test_stats", "if", "test_mode", "else", "self", ".", "train_stats", "\n", "cur_returns", "=", "self", ".", "test_returns", "if", "test_mode", "else", "self", ".", "train_returns", "\n", "log_prefix", "=", "\"test_\"", "if", "test_mode", "else", "\"\"", "\n", "infos", "=", "[", "cur_stats", "]", "+", "final_env_infos", "\n", "cur_stats", ".", "update", "(", "{", "k", ":", "sum", "(", "d", ".", "get", "(", "k", ",", "0", ")", "for", "d", "in", "infos", ")", "for", "k", "in", "set", ".", "union", "(", "*", "[", "set", "(", "d", ")", "for", "d", "in", "infos", "]", ")", "}", ")", "\n", "cur_stats", "[", "\"n_episodes\"", "]", "=", "self", ".", "batch_size", "+", "cur_stats", ".", "get", "(", "\"n_episodes\"", ",", "0", ")", "\n", "cur_stats", "[", "\"ep_length\"", "]", "=", "sum", "(", "episode_lengths", ")", "+", "cur_stats", ".", "get", "(", "\"ep_length\"", ",", "0", ")", "\n", "\n", "cur_returns", ".", "extend", "(", "episode_returns", ")", "\n", "\n", "n_test_runs", "=", "max", "(", "1", ",", "self", ".", "args", ".", "test_nepisode", "//", "self", ".", "batch_size", ")", "*", "self", ".", "batch_size", "\n", "if", "test_mode", "and", "(", "len", "(", "self", ".", "test_returns", ")", "==", "n_test_runs", ")", ":", "\n", "            ", "self", ".", "_log", "(", "cur_returns", ",", "cur_stats", ",", "log_prefix", ")", "\n", "", "elif", "self", ".", "t_env", "-", "self", ".", "log_train_stats_t", ">=", "self", ".", "args", ".", "runner_log_interval", ":", "\n", "            ", "self", ".", "_log", "(", "cur_returns", ",", "cur_stats", ",", "log_prefix", ")", "\n", "if", "hasattr", "(", "self", ".", "mac", ".", "action_selector", ",", "\"epsilon\"", ")", ":", "\n", "                ", "self", ".", "logger", ".", "log_stat", "(", "\"epsilon\"", ",", "self", ".", "mac", ".", "action_selector", ".", "epsilon", ",", "self", ".", "t_env", ")", "\n", "", "self", ".", "log_train_stats_t", "=", "self", ".", "t_env", "\n", "\n", "", "return", "self", ".", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.parallel_runner.ParallelRunner._log": [[214, 223], ["parallel_runner.ParallelRunner.logger.log_stat", "parallel_runner.ParallelRunner.logger.log_stat", "returns.clear", "stats.items", "stats.clear", "numpy.mean", "numpy.std", "parallel_runner.ParallelRunner.logger.log_stat"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat"], ["", "def", "_log", "(", "self", ",", "returns", ",", "stats", ",", "prefix", ")", ":", "\n", "        ", "self", ".", "logger", ".", "log_stat", "(", "prefix", "+", "\"return_mean\"", ",", "np", ".", "mean", "(", "returns", ")", ",", "self", ".", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "prefix", "+", "\"return_std\"", ",", "np", ".", "std", "(", "returns", ")", ",", "self", ".", "t_env", ")", "\n", "returns", ".", "clear", "(", ")", "\n", "\n", "for", "k", ",", "v", "in", "stats", ".", "items", "(", ")", ":", "\n", "            ", "if", "k", "!=", "\"n_episodes\"", ":", "\n", "                ", "self", ".", "logger", ".", "log_stat", "(", "prefix", "+", "k", "+", "\"_mean\"", ",", "v", "/", "stats", "[", "\"n_episodes\"", "]", ",", "self", ".", "t_env", ")", "\n", "", "", "stats", ".", "clear", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.parallel_runner.CloudpickleWrapper.__init__": [[275, 277], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "x", ")", ":", "\n", "        ", "self", ".", "x", "=", "x", "\n", "", "def", "__getstate__", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.parallel_runner.CloudpickleWrapper.__getstate__": [[277, 280], ["cloudpickle.dumps"], "methods", ["None"], ["", "def", "__getstate__", "(", "self", ")", ":", "\n", "        ", "import", "cloudpickle", "\n", "return", "cloudpickle", ".", "dumps", "(", "self", ".", "x", ")", "\n", "", "def", "__setstate__", "(", "self", ",", "ob", ")", ":", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.parallel_runner.CloudpickleWrapper.__setstate__": [[280, 283], ["pickle.loads"], "methods", ["None"], ["", "def", "__setstate__", "(", "self", ",", "ob", ")", ":", "\n", "        ", "import", "pickle", "\n", "self", ".", "x", "=", "pickle", ".", "loads", "(", "ob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.parallel_runner.env_worker": [[225, 269], ["env_fn.x", "remote.recv", "env_fn.x.step", "env_fn.x.get_state", "env_fn.x.get_avail_actions", "env_fn.x.get_obs", "remote.send", "env_fn.x.reset", "remote.send", "env_fn.x.close", "remote.close", "env_fn.x.get_state", "env_fn.x.get_avail_actions", "env_fn.x.get_obs", "remote.send", "env_fn.x.get_env_info", "remote.send", "env_fn.x.get_stats"], "function", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.step", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.get_state", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.get_avail_actions", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.get_obs", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner.reset", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt.close", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt.close", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.get_state", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.get_avail_actions", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.get_obs", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner.get_env_info", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.get_stats"], ["", "", "def", "env_worker", "(", "remote", ",", "env_fn", ")", ":", "\n", "# Make environment", "\n", "    ", "env", "=", "env_fn", ".", "x", "(", ")", "\n", "while", "True", ":", "\n", "        ", "cmd", ",", "data", "=", "remote", ".", "recv", "(", ")", "\n", "if", "cmd", "==", "\"step\"", ":", "\n", "            ", "actions", "=", "data", "\n", "# Take a step in the environment", "\n", "reward", ",", "terminated", ",", "env_info", "=", "env", ".", "step", "(", "actions", ")", "\n", "# Return the observations, avail_actions and state to make the next action", "\n", "state", "=", "env", ".", "get_state", "(", ")", "\n", "avail_actions", "=", "env", ".", "get_avail_actions", "(", ")", "\n", "obs", "=", "env", ".", "get_obs", "(", ")", "\n", "remote", ".", "send", "(", "{", "\n", "# Data for the next timestep needed to pick an action", "\n", "\"state\"", ":", "state", ",", "\n", "\"avail_actions\"", ":", "avail_actions", ",", "\n", "\"obs\"", ":", "obs", ",", "\n", "# Rest of the data for the current timestep", "\n", "\"reward\"", ":", "reward", ",", "\n", "\"terminated\"", ":", "terminated", ",", "\n", "\"info\"", ":", "env_info", "\n", "}", ")", "\n", "", "elif", "cmd", "==", "\"reset\"", ":", "\n", "            ", "env", ".", "reset", "(", ")", "\n", "remote", ".", "send", "(", "{", "\n", "\"state\"", ":", "env", ".", "get_state", "(", ")", ",", "\n", "\"avail_actions\"", ":", "env", ".", "get_avail_actions", "(", ")", ",", "\n", "\"obs\"", ":", "env", ".", "get_obs", "(", ")", "\n", "}", ")", "\n", "", "elif", "cmd", "==", "\"close\"", ":", "\n", "            ", "env", ".", "close", "(", ")", "\n", "remote", ".", "close", "(", ")", "\n", "break", "\n", "", "elif", "cmd", "==", "\"get_env_info\"", ":", "\n", "            ", "remote", ".", "send", "(", "env", ".", "get_env_info", "(", ")", ")", "\n", "", "elif", "cmd", "==", "\"get_stats\"", ":", "\n", "            ", "remote", ".", "send", "(", "env", ".", "get_stats", "(", ")", ")", "\n", "# TODO: unused now?", "\n", "# elif cmd == \"agg_stats\":", "\n", "#     agg_stats = env.get_agg_stats(data)", "\n", "#     remote.send(agg_stats)", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner.__init__": [[10, 33], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "logger", ")", ":", "\n", "        ", "self", ".", "args", "=", "args", "\n", "self", ".", "logger", "=", "logger", "\n", "self", ".", "batch_size", "=", "self", ".", "args", ".", "batch_size_run", "\n", "assert", "self", ".", "batch_size", "==", "1", "\n", "\n", "if", "'sc2'", "in", "self", ".", "args", ".", "env", ":", "\n", "            ", "self", ".", "env", "=", "env_REGISTRY", "[", "self", ".", "args", ".", "env", "]", "(", "**", "self", ".", "args", ".", "env_args", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "env", "=", "env_REGISTRY", "[", "self", ".", "args", ".", "env", "]", "(", "env_args", "=", "self", ".", "args", ".", "env_args", ",", "args", "=", "args", ")", "\n", "\n", "", "self", ".", "episode_limit", "=", "self", ".", "env", ".", "episode_limit", "\n", "self", ".", "t", "=", "0", "\n", "\n", "self", ".", "t_env", "=", "0", "\n", "\n", "self", ".", "train_returns", "=", "[", "]", "\n", "self", ".", "test_returns", "=", "[", "]", "\n", "self", ".", "train_stats", "=", "{", "}", "\n", "self", ".", "test_stats", "=", "{", "}", "\n", "\n", "# Log the first run", "\n", "self", ".", "log_train_stats_t", "=", "-", "1000000", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner.setup": [[34, 38], ["functools.partial"], "methods", ["None"], ["", "def", "setup", "(", "self", ",", "scheme", ",", "groups", ",", "preprocess", ",", "mac", ")", ":", "\n", "        ", "self", ".", "new_batch", "=", "partial", "(", "EpisodeBatch", ",", "scheme", ",", "groups", ",", "self", ".", "batch_size", ",", "self", ".", "episode_limit", "+", "1", ",", "\n", "preprocess", "=", "preprocess", ",", "device", "=", "self", ".", "args", ".", "device", ")", "\n", "self", ".", "mac", "=", "mac", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner.get_env_info": [[39, 41], ["episode_runner.EpisodeRunner.env.get_env_info"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner.get_env_info"], ["", "def", "get_env_info", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "env", ".", "get_env_info", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner.save_replay": [[42, 44], ["episode_runner.EpisodeRunner.env.save_replay"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner.save_replay"], ["", "def", "save_replay", "(", "self", ")", ":", "\n", "        ", "self", ".", "env", ".", "save_replay", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner.close_env": [[45, 47], ["episode_runner.EpisodeRunner.env.close"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.stag_hunt.stag_hunt.StagHunt.close"], ["", "def", "close_env", "(", "self", ")", ":", "\n", "        ", "self", ".", "env", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner.reset": [[48, 52], ["episode_runner.EpisodeRunner.new_batch", "episode_runner.EpisodeRunner.env.reset"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner.reset"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "batch", "=", "self", ".", "new_batch", "(", ")", "\n", "self", ".", "env", ".", "reset", "(", ")", "\n", "self", ".", "t", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner.run": [[53, 119], ["episode_runner.EpisodeRunner.reset", "episode_runner.EpisodeRunner.mac.init_hidden", "episode_runner.EpisodeRunner.batch.update", "episode_runner.EpisodeRunner.mac.select_actions", "episode_runner.EpisodeRunner.batch.update", "cur_stats.update", "cur_returns.append", "episode_runner.EpisodeRunner.batch.update", "episode_runner.EpisodeRunner.mac.select_actions", "episode_runner.EpisodeRunner.env.step", "episode_runner.EpisodeRunner.batch.update", "cur_stats.get", "cur_stats.get", "episode_runner.EpisodeRunner._log", "actions[].cpu", "episode_runner.EpisodeRunner.env.get_state", "episode_runner.EpisodeRunner.env.get_avail_actions", "episode_runner.EpisodeRunner.env.get_obs", "len", "episode_runner.EpisodeRunner._log", "hasattr", "episode_runner.EpisodeRunner.env.get_state", "episode_runner.EpisodeRunner.env.get_avail_actions", "episode_runner.EpisodeRunner.env.get_obs", "cur_stats.get", "env_info.get", "episode_runner.EpisodeRunner.logger.log_stat", "set", "set", "env_info.get"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner.reset", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.agents.central_rnn_agent.CentralRNNAgent.init_hidden", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.EpisodeBatch.update", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.select_actions", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.EpisodeBatch.update", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.EpisodeBatch.update", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.EpisodeBatch.update", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.controllers.basic_controller_policy.BasicMAC.select_actions", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.step", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.components.episode_buffer.EpisodeBatch.update", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner._log", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.get_state", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.get_avail_actions", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.get_obs", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner._log", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.get_state", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.get_avail_actions", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.matrix_game.matrix_game_simple.Matrixgame.get_obs", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat"], ["", "def", "run", "(", "self", ",", "test_mode", "=", "False", ")", ":", "\n", "        ", "self", ".", "reset", "(", ")", "\n", "\n", "terminated", "=", "False", "\n", "episode_return", "=", "0", "\n", "self", ".", "mac", ".", "init_hidden", "(", "batch_size", "=", "self", ".", "batch_size", ")", "\n", "\n", "while", "not", "terminated", ":", "\n", "\n", "            ", "pre_transition_data", "=", "{", "\n", "\"state\"", ":", "[", "self", ".", "env", ".", "get_state", "(", ")", "]", ",", "\n", "\"avail_actions\"", ":", "[", "self", ".", "env", ".", "get_avail_actions", "(", ")", "]", ",", "\n", "\"obs\"", ":", "[", "self", ".", "env", ".", "get_obs", "(", ")", "]", "\n", "}", "\n", "\n", "self", ".", "batch", ".", "update", "(", "pre_transition_data", ",", "ts", "=", "self", ".", "t", ")", "\n", "\n", "# Pass the entire batch of experiences up till now to the agents", "\n", "# Receive the actions for each agent at this timestep in a batch of size 1", "\n", "actions", "=", "self", ".", "mac", ".", "select_actions", "(", "self", ".", "batch", ",", "t_ep", "=", "self", ".", "t", ",", "t_env", "=", "self", ".", "t_env", ",", "test_mode", "=", "test_mode", ")", "\n", "\n", "reward", ",", "terminated", ",", "env_info", "=", "self", ".", "env", ".", "step", "(", "actions", "[", "0", "]", ".", "cpu", "(", ")", ")", "\n", "episode_return", "+=", "reward", "\n", "\n", "post_transition_data", "=", "{", "\n", "\"actions\"", ":", "actions", ",", "\n", "\"reward\"", ":", "[", "(", "reward", ",", ")", "]", ",", "\n", "\"terminated\"", ":", "[", "(", "terminated", "!=", "env_info", ".", "get", "(", "\"episode_limit\"", ",", "False", ")", ",", ")", "]", ",", "\n", "}", "\n", "\n", "self", ".", "batch", ".", "update", "(", "post_transition_data", ",", "ts", "=", "self", ".", "t", ")", "\n", "\n", "self", ".", "t", "+=", "1", "\n", "\n", "", "last_data", "=", "{", "\n", "\"state\"", ":", "[", "self", ".", "env", ".", "get_state", "(", ")", "]", ",", "\n", "\"avail_actions\"", ":", "[", "self", ".", "env", ".", "get_avail_actions", "(", ")", "]", ",", "\n", "\"obs\"", ":", "[", "self", ".", "env", ".", "get_obs", "(", ")", "]", "\n", "}", "\n", "self", ".", "batch", ".", "update", "(", "last_data", ",", "ts", "=", "self", ".", "t", ")", "\n", "\n", "# Select actions in the last stored state", "\n", "actions", "=", "self", ".", "mac", ".", "select_actions", "(", "self", ".", "batch", ",", "t_ep", "=", "self", ".", "t", ",", "t_env", "=", "self", ".", "t_env", ",", "test_mode", "=", "test_mode", ")", "\n", "self", ".", "batch", ".", "update", "(", "{", "\"actions\"", ":", "actions", "}", ",", "ts", "=", "self", ".", "t", ")", "\n", "\n", "cur_stats", "=", "self", ".", "test_stats", "if", "test_mode", "else", "self", ".", "train_stats", "\n", "cur_returns", "=", "self", ".", "test_returns", "if", "test_mode", "else", "self", ".", "train_returns", "\n", "log_prefix", "=", "\"test_\"", "if", "test_mode", "else", "\"\"", "\n", "cur_stats", ".", "update", "(", "{", "k", ":", "cur_stats", ".", "get", "(", "k", ",", "0", ")", "+", "env_info", ".", "get", "(", "k", ",", "0", ")", "for", "k", "in", "set", "(", "cur_stats", ")", "|", "set", "(", "env_info", ")", "}", ")", "\n", "cur_stats", "[", "\"n_episodes\"", "]", "=", "1", "+", "cur_stats", ".", "get", "(", "\"n_episodes\"", ",", "0", ")", "\n", "cur_stats", "[", "\"ep_length\"", "]", "=", "self", ".", "t", "+", "cur_stats", ".", "get", "(", "\"ep_length\"", ",", "0", ")", "\n", "\n", "if", "not", "test_mode", ":", "\n", "            ", "self", ".", "t_env", "+=", "self", ".", "t", "\n", "\n", "", "cur_returns", ".", "append", "(", "episode_return", ")", "\n", "\n", "if", "test_mode", "and", "(", "len", "(", "self", ".", "test_returns", ")", "==", "self", ".", "args", ".", "test_nepisode", ")", ":", "\n", "            ", "self", ".", "_log", "(", "cur_returns", ",", "cur_stats", ",", "log_prefix", ")", "\n", "", "elif", "self", ".", "t_env", "-", "self", ".", "log_train_stats_t", ">=", "self", ".", "args", ".", "runner_log_interval", ":", "\n", "            ", "self", ".", "_log", "(", "cur_returns", ",", "cur_stats", ",", "log_prefix", ")", "\n", "if", "hasattr", "(", "self", ".", "mac", ".", "action_selector", ",", "\"epsilon\"", ")", ":", "\n", "                ", "self", ".", "logger", ".", "log_stat", "(", "\"epsilon\"", ",", "self", ".", "mac", ".", "action_selector", ".", "epsilon", ",", "self", ".", "t_env", ")", "\n", "", "self", ".", "log_train_stats_t", "=", "self", ".", "t_env", "\n", "\n", "", "return", "self", ".", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner.cal_values": [[120, 145], ["episode_runner.EpisodeRunner.mac.init_hidden", "torch.stack", "episode_runner.EpisodeRunner.mac.forward", "torch.stack.append", "float"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.agents.central_rnn_agent.CentralRNNAgent.init_hidden", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.mixers.sqddpg.SQDDPGMixer.forward"], ["", "def", "cal_values", "(", "self", ",", "batch", ")", ":", "\n", "\n", "        ", "test_mode", "=", "True", "\n", "\n", "terminated", "=", "False", "\n", "episode_return", "=", "0", "\n", "self", ".", "mac", ".", "init_hidden", "(", "batch_size", "=", "self", ".", "batch_size", ")", "\n", "\n", "t", ",", "t_env", "=", "0", ",", "0", "\n", "values", "=", "[", "]", "\n", "\n", "while", "batch", "[", "\"terminated\"", "]", "[", "0", "]", "[", "t", "]", "==", "0", ":", "\n", "# Pass the entire batch of experiences up till now to the agents", "\n", "# Receive the actions for each agent at this timestep in a batch of size 1", "\n", "\n", "# Calculate values", "\n", "            ", "agent_outs", "=", "self", ".", "mac", ".", "forward", "(", "batch", ",", "t", ",", "test_mode", "=", "test_mode", ")", "\n", "avail_actions", "=", "batch", "[", "\"avail_actions\"", "]", "[", ":", ",", "t", "]", "\n", "agent_outs", "[", "avail_actions", "==", "0.0", "]", "=", "-", "float", "(", "\"inf\"", ")", "\n", "values", ".", "append", "(", "agent_outs", ")", "\n", "\n", "t", "+=", "1", "\n", "\n", "", "values", "=", "th", ".", "stack", "(", "values", ",", "1", ")", "\n", "return", "values", "\n", "\n"]], "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.runners.episode_runner.EpisodeRunner._log": [[146, 155], ["episode_runner.EpisodeRunner.logger.log_stat", "episode_runner.EpisodeRunner.logger.log_stat", "returns.clear", "stats.items", "stats.clear", "numpy.mean", "numpy.std", "episode_runner.EpisodeRunner.logger.log_stat"], "methods", ["home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat", "home.repos.pwc.inspect_result.hsvgbkhgbv_shapley-q-learning.utils.logging.Logger.log_stat"], ["", "def", "_log", "(", "self", ",", "returns", ",", "stats", ",", "prefix", ")", ":", "\n", "        ", "self", ".", "logger", ".", "log_stat", "(", "prefix", "+", "\"return_mean\"", ",", "np", ".", "mean", "(", "returns", ")", ",", "self", ".", "t_env", ")", "\n", "self", ".", "logger", ".", "log_stat", "(", "prefix", "+", "\"return_std\"", ",", "np", ".", "std", "(", "returns", ")", ",", "self", ".", "t_env", ")", "\n", "returns", ".", "clear", "(", ")", "\n", "\n", "for", "k", ",", "v", "in", "stats", ".", "items", "(", ")", ":", "\n", "            ", "if", "k", "!=", "\"n_episodes\"", ":", "\n", "                ", "self", ".", "logger", ".", "log_stat", "(", "prefix", "+", "k", "+", "\"_mean\"", ",", "v", "/", "stats", "[", "\"n_episodes\"", "]", ",", "self", ".", "t_env", ")", "\n", "", "", "stats", ".", "clear", "(", ")", "\n", "", "", ""]]}