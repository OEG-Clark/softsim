{"home.repos.pwc.inspect_result.THU-BPM_MetaSRE.src.bert_base.BertForSequenceClassificationUserDefined.__init__": [[19, 28], ["transformers.BertPreTrainedModel.__init__", "transformers.BertModel", "torch.nn.Dropout", "torch.nn.Linear", "bert_base.BertForSequenceClassificationUserDefined.init_weights"], "methods", ["home.repos.pwc.inspect_result.THU-BPM_MetaSRE.src.networks.LabelGeneration.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "config", ".", "num_labels", "\n", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "2", "*", "config", ".", "hidden_size", ",", "self", ".", "config", ".", "num_labels", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.src.bert_base.BertForSequenceClassificationUserDefined.forward": [[29, 76], ["bert_base.BertForSequenceClassificationUserDefined.bert", "range", "torch.stack", "bert_base.BertForSequenceClassificationUserDefined.dropout", "bert_base.BertForSequenceClassificationUserDefined.classifier", "len", "torch.cat", "e_pos_outputs.append", "torch.nn.MSELoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "bert_base.BertForSequenceClassificationUserDefined.view", "labels.view", "bert_base.BertForSequenceClassificationUserDefined.view", "labels.view", "e1_pos[].item", "e2_pos[].item"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "labels", "=", "None", ",", "\n", "e1_pos", "=", "None", ",", "\n", "e2_pos", "=", "None", "\n", ")", ":", "\n", "\n", "        ", "outputs", "=", "self", ".", "bert", "(", "\n", "input_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", ")", "\n", "\n", "e_pos_outputs", "=", "[", "]", "\n", "sequence_output", "=", "outputs", "[", "0", "]", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "e1_pos", ")", ")", ":", "\n", "            ", "e1_pos_output_i", "=", "sequence_output", "[", "i", ",", "e1_pos", "[", "i", "]", ".", "item", "(", ")", ",", ":", "]", "\n", "e2_pos_output_i", "=", "sequence_output", "[", "i", ",", "e2_pos", "[", "i", "]", ".", "item", "(", ")", ",", ":", "]", "\n", "e_pos_output_i", "=", "torch", ".", "cat", "(", "(", "e1_pos_output_i", ",", "e2_pos_output_i", ")", ",", "dim", "=", "0", ")", "\n", "e_pos_outputs", ".", "append", "(", "e_pos_output_i", ")", "\n", "", "e_pos_output", "=", "torch", ".", "stack", "(", "e_pos_outputs", ")", "\n", "\n", "e_pos_output", "=", "self", ".", "dropout", "(", "e_pos_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "e_pos_output", ")", "\n", "\n", "outputs", "=", "(", "logits", ",", ")", "+", "outputs", "[", "2", ":", "]", "# add hidden states and attention if they are here", "\n", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "if", "self", ".", "num_labels", "==", "1", ":", "\n", "#  We are doing regression", "\n", "                ", "loss_fct", "=", "MSELoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "else", ":", "\n", "                ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "outputs", "=", "(", "loss", ",", ")", "+", "outputs", "\n", "\n", "", "return", "outputs", "# (loss), logits, (hidden_states), (attentions)", "\n", "\n"]], "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.src.bert_base.flat_accuracy": [[196, 200], ["numpy.argmax().flatten", "labels.flatten", "numpy.sum", "len", "numpy.argmax"], "function", ["None"], ["def", "flat_accuracy", "(", "preds", ",", "labels", ")", ":", "\n", "    ", "pred_flat", "=", "np", ".", "argmax", "(", "preds", ",", "axis", "=", "1", ")", ".", "flatten", "(", ")", "\n", "labels_flat", "=", "labels", ".", "flatten", "(", ")", "\n", "return", "np", ".", "sum", "(", "pred_flat", "==", "labels_flat", ")", "/", "len", "(", "labels_flat", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.src.bert_base.format_time": [[202, 211], ["int", "str", "round", "datetime.timedelta"], "function", ["None"], ["", "def", "format_time", "(", "elapsed", ")", ":", "\n", "    ", "'''\n    Takes a time in seconds and returns a string hh:mm:ss\n    '''", "\n", "# Round to the nearest second.", "\n", "elapsed_rounded", "=", "int", "(", "round", "(", "(", "elapsed", ")", ")", ")", "\n", "\n", "# Format as hh:mm:ss", "\n", "return", "str", "(", "datetime", ".", "timedelta", "(", "seconds", "=", "elapsed_rounded", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.src.networks.BertForSequenceClassificationUserDefined.__init__": [[8, 17], ["transformers.BertPreTrainedModel.__init__", "transformers.BertModel", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "networks.BertForSequenceClassificationUserDefined.init_weights"], "methods", ["home.repos.pwc.inspect_result.THU-BPM_MetaSRE.src.networks.LabelGeneration.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "config", ".", "num_labels", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "2", "*", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "classifier_2", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "config", ".", "num_labels", ")", "\n", "self", ".", "init_weights", "(", ")", "\n", "self", ".", "output_emebedding", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.src.networks.BertForSequenceClassificationUserDefined.forward": [[18, 61], ["networks.BertForSequenceClassificationUserDefined.bert", "range", "torch.stack", "networks.BertForSequenceClassificationUserDefined.dropout", "networks.BertForSequenceClassificationUserDefined.classifier", "networks.BertForSequenceClassificationUserDefined.classifier_2", "len", "torch.cat", "e_pos_outputs.append", "torch.nn.MSELoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss", "range", "networks.BertForSequenceClassificationUserDefined.view", "labels.view", "len", "len", "e1_pos[].item", "e2_pos[].item", "math.exp", "torch.nn.CrossEntropyLoss.", "logits[].view", "labels[].view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "token_type_ids", "=", "None", ",", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "inputs_embeds", "=", "None", ",", "labels", "=", "None", ",", "e1_pos", "=", "None", ",", "e2_pos", "=", "None", ",", "w", "=", "None", ")", ":", "\n", "\n", "        ", "outputs", "=", "self", ".", "bert", "(", "\n", "input_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", ")", "# sequence_output, pooled_output, (hidden_states), (attentions)", "\n", "\n", "e_pos_outputs", "=", "[", "]", "\n", "sequence_output", "=", "outputs", "[", "0", "]", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "e1_pos", ")", ")", ":", "\n", "            ", "e1_pos_output_i", "=", "sequence_output", "[", "i", ",", "e1_pos", "[", "i", "]", ".", "item", "(", ")", ",", ":", "]", "\n", "e2_pos_output_i", "=", "sequence_output", "[", "i", ",", "e2_pos", "[", "i", "]", ".", "item", "(", ")", ",", ":", "]", "\n", "e_pos_output_i", "=", "torch", ".", "cat", "(", "(", "e1_pos_output_i", ",", "e2_pos_output_i", ")", ",", "dim", "=", "0", ")", "\n", "e_pos_outputs", ".", "append", "(", "e_pos_output_i", ")", "\n", "", "e_pos_output", "=", "torch", ".", "stack", "(", "e_pos_outputs", ")", "\n", "self", ".", "output_emebedding", "=", "e_pos_output", "#e1&e2 cancat output", "\n", "\n", "e_pos_output", "=", "self", ".", "dropout", "(", "e_pos_output", ")", "\n", "hidden", "=", "self", ".", "classifier", "(", "e_pos_output", ")", "\n", "logits", "=", "self", ".", "classifier_2", "(", "hidden", ")", "\n", "\n", "outputs", "=", "(", "logits", ",", ")", "+", "outputs", "[", "2", ":", "]", "# add hidden states and attention if they are here", "\n", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "if", "self", ".", "num_labels", "==", "1", ":", "\n", "#  We are doing regression", "\n", "                ", "loss_fct", "=", "MSELoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "else", ":", "\n", "                ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "0", "\n", "for", "i", "in", "range", "(", "len", "(", "w", ")", ")", ":", "\n", "                    ", "loss", "+=", "math", ".", "exp", "(", "w", "[", "i", "]", "-", "1", ")", "*", "loss_fct", "(", "logits", "[", "i", "]", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", "[", "i", "]", ".", "view", "(", "-", "1", ")", ")", "\n", "#loss += w[i] * loss_fct(logits[i].view(-1, self.num_labels), labels[i].view(-1))", "\n", "", "loss", "=", "loss", "/", "len", "(", "w", ")", "\n", "", "outputs", "=", "(", "loss", ",", ")", "+", "outputs", "+", "(", "self", ".", "output_emebedding", ",", ")", "\n", "\n", "", "return", "outputs", "# (loss), logits, (hidden_states), (attentions), (self.output_emebedding)", "\n", "\n"]], "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.src.networks.RelationClassification.__init__": [[65, 67], ["networks.BertForSequenceClassificationUserDefined.__init__"], "methods", ["home.repos.pwc.inspect_result.THU-BPM_MetaSRE.src.networks.LabelGeneration.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.src.networks.LabelGeneration.__init__": [[71, 73], ["networks.BertForSequenceClassificationUserDefined.__init__"], "methods", ["home.repos.pwc.inspect_result.THU-BPM_MetaSRE.src.networks.LabelGeneration.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.src.train.flat_accuracy": [[46, 51], ["numpy.argmax().flatten", "labels.flatten", "numpy.sum", "len", "numpy.argmax"], "function", ["None"], ["def", "flat_accuracy", "(", "preds", ",", "labels", ")", ":", "\n", "    ", "pred_flat", "=", "np", ".", "argmax", "(", "preds", ",", "axis", "=", "1", ")", ".", "flatten", "(", ")", "\n", "labels_flat", "=", "labels", ".", "flatten", "(", ")", "\n", "non_zero_idx", "=", "(", "labels_flat", "!=", "0", ")", "\n", "return", "np", ".", "sum", "(", "pred_flat", "[", "non_zero_idx", "]", "==", "labels_flat", "[", "non_zero_idx", "]", ")", "/", "len", "(", "labels_flat", "[", "non_zero_idx", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.src.train.format_time": [[54, 59], ["int", "str", "round", "datetime.timedelta"], "function", ["None"], ["", "def", "format_time", "(", "elapsed", ")", ":", "\n", "# Round to the nearest second.", "\n", "    ", "elapsed_rounded", "=", "int", "(", "round", "(", "(", "elapsed", ")", ")", ")", "\n", "# Format as hh:mm:ss", "\n", "return", "str", "(", "datetime", ".", "timedelta", "(", "seconds", "=", "elapsed_rounded", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.src.train.score": [[62, 103], ["collections.Counter", "collections.Counter", "collections.Counter", "range", "print", "print", "print", "print", "len", "print", "sum", "sum", "collections.Counter.values", "float", "float", "collections.Counter.values", "float", "float", "sum", "sum", "sum", "sum", "collections.Counter.values", "collections.Counter.values", "collections.Counter.values", "collections.Counter.values"], "function", ["None"], ["", "def", "score", "(", "key", ",", "prediction", ",", "verbose", "=", "True", ",", "NO_RELATION", "=", "0", ")", ":", "\n", "    ", "correct_by_relation", "=", "Counter", "(", ")", "\n", "guessed_by_relation", "=", "Counter", "(", ")", "\n", "gold_by_relation", "=", "Counter", "(", ")", "\n", "\n", "# Loop over the data to compute a score", "\n", "for", "row", "in", "range", "(", "len", "(", "key", ")", ")", ":", "\n", "        ", "gold", "=", "key", "[", "row", "]", "\n", "guess", "=", "prediction", "[", "row", "]", "\n", "\n", "if", "gold", "==", "NO_RELATION", "and", "guess", "==", "NO_RELATION", ":", "\n", "            ", "pass", "\n", "", "elif", "gold", "==", "NO_RELATION", "and", "guess", "!=", "NO_RELATION", ":", "\n", "            ", "guessed_by_relation", "[", "guess", "]", "+=", "1", "\n", "", "elif", "gold", "!=", "NO_RELATION", "and", "guess", "==", "NO_RELATION", ":", "\n", "            ", "gold_by_relation", "[", "gold", "]", "+=", "1", "\n", "", "elif", "gold", "!=", "NO_RELATION", "and", "guess", "!=", "NO_RELATION", ":", "\n", "            ", "guessed_by_relation", "[", "guess", "]", "+=", "1", "\n", "gold_by_relation", "[", "gold", "]", "+=", "1", "\n", "if", "gold", "==", "guess", ":", "\n", "                ", "correct_by_relation", "[", "guess", "]", "+=", "1", "\n", "\n", "# Print the aggregate score", "\n", "", "", "", "if", "verbose", ":", "\n", "        ", "print", "(", "\"Final Score:\"", ")", "\n", "", "prec_micro", "=", "1.0", "\n", "if", "sum", "(", "guessed_by_relation", ".", "values", "(", ")", ")", ">", "0", ":", "\n", "        ", "prec_micro", "=", "float", "(", "sum", "(", "correct_by_relation", ".", "values", "(", ")", ")", ")", "/", "float", "(", "\n", "sum", "(", "guessed_by_relation", ".", "values", "(", ")", ")", ")", "\n", "", "recall_micro", "=", "0.0", "\n", "if", "sum", "(", "gold_by_relation", ".", "values", "(", ")", ")", ">", "0", ":", "\n", "        ", "recall_micro", "=", "float", "(", "sum", "(", "correct_by_relation", ".", "values", "(", ")", ")", ")", "/", "float", "(", "\n", "sum", "(", "gold_by_relation", ".", "values", "(", ")", ")", ")", "\n", "", "f1_micro", "=", "0.0", "\n", "if", "prec_micro", "+", "recall_micro", ">", "0.0", ":", "\n", "        ", "f1_micro", "=", "2.0", "*", "prec_micro", "*", "recall_micro", "/", "(", "prec_micro", "+", "recall_micro", ")", "\n", "", "print", "(", "\"SET NO_RELATION ID: \"", ",", "NO_RELATION", ")", "\n", "print", "(", "\"Precision (micro): {:.3%}\"", ".", "format", "(", "prec_micro", ")", ")", "\n", "print", "(", "\"   Recall (micro): {:.3%}\"", ".", "format", "(", "recall_micro", ")", ")", "\n", "print", "(", "\"       F1 (micro): {:.3%}\"", ".", "format", "(", "f1_micro", ")", ")", "\n", "return", "prec_micro", ",", "recall_micro", ",", "f1_micro", "\n", "\n"]], "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.src.train.pre_processing": [[107, 155], ["print", "transformers.BertTokenizer.from_pretrained", "range", "torch.cat().to", "torch.cat().to", "torch.cat().to", "torch.cat().to", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.ones", "torch.ones", "torch.utils.data.TensorDataset", "len", "BertTokenizer.from_pretrained.encode_plus", "len", "[].item", "[].item", "torch.tensor.append", "torch.tensor.append", "torch.cat().to.append", "torch.cat().to.append", "torch.tensor.append", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "function", ["None"], ["", "def", "pre_processing", "(", "sentence_train", ",", "sentence_train_label", ")", ":", "\n", "    ", "input_ids", "=", "[", "]", "\n", "attention_masks", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "e1_pos", "=", "[", "]", "\n", "e2_pos", "=", "[", "]", "\n", "\n", "# Load tokenizer.", "\n", "print", "(", "'Loading BERT tokenizer...'", ")", "\n", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "'bert-base-uncased'", ",", "do_lower_case", "=", "True", ")", "\n", "\n", "# pre-processing sentenses to BERT pattern", "\n", "for", "i", "in", "range", "(", "len", "(", "sentence_train", ")", ")", ":", "\n", "        ", "encoded_dict", "=", "tokenizer", ".", "encode_plus", "(", "\n", "sentence_train", "[", "i", "]", ",", "# Sentence to encode.", "\n", "add_special_tokens", "=", "False", ",", "# Add '[CLS]' and '[SEP]'", "\n", "max_length", "=", "MAX_LENGTH", ",", "# Pad & truncate all sentences.", "\n", "pad_to_max_length", "=", "True", ",", "\n", "return_attention_mask", "=", "True", ",", "# Construct attn. masks.", "\n", "return_tensors", "=", "'pt'", ",", "# Return pytorch tensors.", "\n", ")", "\n", "try", ":", "\n", "# Find e1(id:2487) and e2(id:2475) position", "\n", "            ", "pos1", "=", "(", "encoded_dict", "[", "'input_ids'", "]", "==", "2487", ")", ".", "nonzero", "(", ")", "[", "0", "]", "[", "1", "]", ".", "item", "(", ")", "\n", "pos2", "=", "(", "encoded_dict", "[", "'input_ids'", "]", "==", "2475", ")", ".", "nonzero", "(", ")", "[", "0", "]", "[", "1", "]", ".", "item", "(", ")", "\n", "e1_pos", ".", "append", "(", "pos1", ")", "\n", "e2_pos", ".", "append", "(", "pos2", ")", "\n", "# Add the encoded sentence to the list.", "\n", "input_ids", ".", "append", "(", "encoded_dict", "[", "'input_ids'", "]", ")", "\n", "# And its attention mask (simply differentiates padding from non-padding).", "\n", "attention_masks", ".", "append", "(", "encoded_dict", "[", "'attention_mask'", "]", ")", "\n", "labels", ".", "append", "(", "sentence_train_label", "[", "i", "]", ")", "\n", "", "except", ":", "\n", "            ", "pass", "\n", "#print(sent)", "\n", "\n", "# Convert the lists into tensors.", "\n", "", "", "input_ids", "=", "torch", ".", "cat", "(", "input_ids", ",", "dim", "=", "0", ")", ".", "to", "(", "device", ")", "\n", "attention_masks", "=", "torch", ".", "cat", "(", "attention_masks", ",", "dim", "=", "0", ")", ".", "to", "(", "device", ")", "\n", "labels", "=", "torch", ".", "tensor", "(", "labels", ",", "device", "=", "'cuda'", ")", "\n", "e1_pos", "=", "torch", ".", "tensor", "(", "e1_pos", ",", "device", "=", "'cuda'", ")", "\n", "e2_pos", "=", "torch", ".", "tensor", "(", "e2_pos", ",", "device", "=", "'cuda'", ")", "\n", "w", "=", "torch", ".", "ones", "(", "len", "(", "e1_pos", ")", ",", "device", "=", "'cuda'", ")", "\n", "\n", "# Combine the training inputs into a TensorDataset.", "\n", "train_dataset", "=", "TensorDataset", "(", "input_ids", ",", "attention_masks", ",", "labels", ",", "e1_pos", ",", "e2_pos", ",", "w", ")", "\n", "\n", "return", "train_dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.src.train.stratified_sample": [[157, 170], ["range", "data_dict.values", "len", "data_dict[].append", "random.shuffle", "torch.utils.data.Subset", "torch.utils.data.Subset", "data_dict.get", "[].item", "int", "int", "len", "[].item", "[].item", "len", "len"], "function", ["None"], ["", "def", "stratified_sample", "(", "dataset", ",", "ratio", ")", ":", "\n", "    ", "data_dict", "=", "{", "}", "\n", "for", "i", "in", "range", "(", "len", "(", "dataset", ")", ")", ":", "\n", "        ", "if", "not", "data_dict", ".", "get", "(", "dataset", "[", "i", "]", "[", "2", "]", ".", "item", "(", ")", ")", ":", "\n", "            ", "data_dict", "[", "dataset", "[", "i", "]", "[", "2", "]", ".", "item", "(", ")", "]", "=", "[", "]", "\n", "", "data_dict", "[", "dataset", "[", "i", "]", "[", "2", "]", ".", "item", "(", ")", "]", ".", "append", "(", "i", ")", "\n", "", "sampled_indices", "=", "[", "]", "\n", "rest_indices", "=", "[", "]", "\n", "for", "indices", "in", "data_dict", ".", "values", "(", ")", ":", "\n", "        ", "random", ".", "shuffle", "(", "indices", ")", "\n", "sampled_indices", "+=", "indices", "[", "0", ":", "int", "(", "len", "(", "indices", ")", "*", "ratio", ")", "]", "\n", "rest_indices", "+=", "indices", "[", "int", "(", "len", "(", "indices", ")", "*", "ratio", ")", ":", "len", "(", "indices", ")", "]", "\n", "", "return", "[", "Subset", "(", "dataset", ",", "sampled_indices", ")", ",", "Subset", "(", "dataset", ",", "rest_indices", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.src.train.main": [[174, 705], ["json.load", "json.load", "train.pre_processing", "train.stratified_sample", "range", "torch.utils.data.DataLoader", "range", "json.load", "json.load", "train.pre_processing", "torch.utils.data.DataLoader", "networks.RelationClassification.from_pretrained", "torch.nn.DataParallel", "nn.DataParallel.to", "networks.LabelGeneration.from_pretrained", "torch.nn.DataParallel", "nn.DataParallel.to", "transformers.AdamW", "transformers.AdamW", "range", "transformers.get_linear_schedule_with_warmup", "transformers.get_linear_schedule_with_warmup", "random.seed", "numpy.random.seed", "torch.manual_seed", "torch.manual_seed", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all", "time.time", "print", "range", "print", "print", "open", "open", "train.stratified_sample", "unlabeled_dataset.append", "torch.utils.data.DataLoader", "unlabeled_dataloader.append", "open", "open", "nn.DataParallel.parameters", "nn.DataParallel.parameters", "len", "range", "range", "range", "print", "time.time", "nn.DataParallel.eval", "numpy.array", "numpy.array", "train.format_time", "sklearn.metrics.f1_score", "train.score", "torch.utils.data.RandomSampler", "torch.utils.data.SequentialSampler", "len", "range", "print", "range", "print", "nn.DataParallel.eval", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "torch.cat", "torch.cat", "torch.softmax", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.max", "torch.max", "logits.detach().cpu().numpy.detach().cpu().numpy", "torch.cat.to().numpy", "label_weights.to().numpy", "numpy.argmax().flatten", "b_labels.to().numpy.flatten", "pred_weights.flatten.flatten", "numpy.concatenate", "numpy.concatenate", "numpy.concatenate", "torch.argsort", "torch.argsort", "pseudo_labels.to().numpy", "pseudo_gold_labels.to().numpy", "w.to().numpy", "pseudo_labels.to().numpy.flatten", "b_labels.to().numpy.flatten", "w.to().numpy.flatten", "numpy.concatenate", "numpy.concatenate", "numpy.concatenate", "numpy.argsort", "print", "train.score", "numpy.save", "numpy.save", "numpy.save", "print", "train.score", "numpy.save", "numpy.save", "numpy.save", "torch.utils.data.DataLoader", "print", "time.time", "nn.DataParallel.train", "enumerate", "train.format_time", "print", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "loss.sum().item", "logits.detach().cpu().numpy.detach().cpu().numpy", "batch[].to.to().numpy", "numpy.argmax().flatten", "b_labels.to().numpy.flatten", "numpy.concatenate", "numpy.concatenate", "len", "numpy.sum", "len", "train.format_time", "torch.utils.data.RandomSampler", "len", "print", "time.time", "nn.DataParallel.train", "enumerate", "train.format_time", "print", "print", "time.time", "nn.DataParallel.eval", "numpy.array", "numpy.array", "train.format_time", "train.score", "print", "time.time", "nn.DataParallel.train", "nn.DataParallel.eval", "enumerate", "train.format_time", "print", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "logits.detach().cpu().numpy.detach", "all_logits.append", "torch.cat.append", "torch.cat.append", "torch.cat.append", "torch.cat.append", "torch.cat.append", "w.append", "torch.utils.data.TensorDataset", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "nn.DataParallel.zero_grad", "nn.DataParallel.", "loss.sum().item", "loss.sum().backward", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "transformers.AdamW.step", "transformers.get_linear_schedule_with_warmup.step", "len", "torch.no_grad", "torch.no_grad", "nn.DataParallel.", "time.time", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "nn.DataParallel.zero_grad", "nn.DataParallel.", "loss.sum().item", "loss.sum().backward", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "transformers.AdamW.step", "transformers.get_linear_schedule_with_warmup.step", "len", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "loss.sum().item", "logits.detach().cpu().numpy.detach().cpu().numpy", "batch[].to.to().numpy", "numpy.argmax().flatten", "b_labels.to().numpy.flatten", "numpy.concatenate", "numpy.concatenate", "len", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "nn.DataParallel.zero_grad", "nn.DataParallel.", "nn.DataParallel.zero_grad", "nn.DataParallel.", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "transformers.AdamW.step", "transformers.get_linear_schedule_with_warmup.step", "loss2.sum().item", "len", "torch.no_grad", "torch.no_grad", "nn.DataParallel.", "logits.detach().cpu().numpy.detach().cpu", "torch.cat.to", "label_weights.to", "numpy.argmax", "int", "pseudo_labels.to", "pseudo_gold_labels.to", "w.to", "torch.utils.data.RandomSampler", "train.format_time", "nn.DataParallel.parameters", "time.time", "loss.sum", "logits.detach().cpu().numpy.detach().cpu", "batch[].to.to", "numpy.argmax", "time.time", "train.format_time", "nn.DataParallel.parameters", "time.time", "torch.no_grad", "torch.no_grad", "nn.DataParallel.", "time.time", "train.format_time", "nn.DataParallel.parameters", "time.time", "loss.sum", "loss.sum", "loss.sum", "loss.sum", "loss.sum", "logits.detach().cpu().numpy.detach().cpu", "batch[].to.to", "numpy.argmax", "loss2.sum", "logits.detach().cpu().numpy.detach", "len", "time.time", "logits.detach().cpu().numpy.detach", "time.time", "time.time", "loss1.sum", "logits.detach().cpu().numpy.detach", "loss2.sum"], "function", ["home.repos.pwc.inspect_result.THU-BPM_MetaSRE.src.train_self_training.pre_processing", "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.src.train_self_training.stratified_sample", "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.src.train_self_training.pre_processing", "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.src.train_self_training.stratified_sample", "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.src.train_self_training.format_time", "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.data.data_analysis.score", "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.data.data_analysis.score", "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.data.data_analysis.score", "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.src.train_self_training.format_time", "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.src.train_self_training.format_time", "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.src.train_self_training.format_time", "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.src.train_self_training.format_time", "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.data.data_analysis.score", "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.src.train_self_training.format_time", "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.src.train_self_training.format_time", "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.src.train_self_training.format_time", "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.src.train_self_training.format_time"], ["", "def", "main", "(", "argv", "=", "None", ")", ":", "\n", "# Load the dataset.", "\n", "    ", "sentence_train", "=", "json", ".", "load", "(", "open", "(", "'data/'", "+", "DATASET", "+", "'/train_sentence.json'", ",", "'r'", ")", ")", "\n", "sentence_train_label", "=", "json", ".", "load", "(", "open", "(", "'data/'", "+", "DATASET", "+", "'/train_label_id.json'", ",", "'r'", ")", ")", "\n", "train_dataset", "=", "pre_processing", "(", "sentence_train", ",", "sentence_train_label", ")", "\n", "\n", "# split training data to labeled set and unlabeled set", "\n", "labeled_dataset", ",", "unlabeled_dataset_total", "=", "stratified_sample", "(", "train_dataset", ",", "LABEL_OF_TRAIN", ")", "\n", "\n", "unlabeled_dataset", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "MATE_EPOCHS", ")", ":", "\n", "        ", "unlabeled_dataset_now", ",", "unlabeled_dataset_total", "=", "stratified_sample", "(", "unlabeled_dataset_total", ",", "\n", "UNLABEL_OF_TRAIN", "/", "MATE_EPOCHS", ")", "\n", "unlabeled_dataset", ".", "append", "(", "unlabeled_dataset_now", ")", "\n", "\n", "# Create the DataLoaders for our label and unlabel sets.", "\n", "", "labeled_dataloader", "=", "DataLoader", "(", "\n", "labeled_dataset", ",", "# The training samples.", "\n", "sampler", "=", "RandomSampler", "(", "labeled_dataset", ")", ",", "# Select batches randomly", "\n", "batch_size", "=", "BATCH_SIZE", "# Trains with this batch size.", "\n", ")", "\n", "unlabeled_dataloader", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "MATE_EPOCHS", ")", ":", "\n", "        ", "unlabeled_dataloader_now", "=", "DataLoader", "(", "\n", "unlabeled_dataset", "[", "i", "]", ",", "# The training samples.", "\n", "sampler", "=", "RandomSampler", "(", "unlabeled_dataset", "[", "i", "]", ")", ",", "# Select batches randomly", "\n", "batch_size", "=", "BATCH_SIZE", "# Trains with this batch size.", "\n", ")", "\n", "unlabeled_dataloader", ".", "append", "(", "unlabeled_dataloader_now", ")", "\n", "\n", "", "sentence_val", "=", "json", ".", "load", "(", "open", "(", "'data/'", "+", "DATASET", "+", "'/test_sentence.json'", ",", "'r'", ")", ")", "\n", "sentence_val_label", "=", "json", ".", "load", "(", "open", "(", "'data/'", "+", "DATASET", "+", "'/test_label_id.json'", ",", "'r'", ")", ")", "\n", "val_dataset", "=", "pre_processing", "(", "sentence_val", ",", "sentence_val_label", ")", "\n", "\n", "validation_dataloader", "=", "DataLoader", "(", "\n", "val_dataset", ",", "# The validation samples.", "\n", "sampler", "=", "SequentialSampler", "(", "val_dataset", ")", ",", "# Pull out batches sequentially.", "\n", "batch_size", "=", "BATCH_SIZE", "# Evaluate with this batch size.", "\n", ")", "\n", "\n", "# Load models", "\n", "modelf1", "=", "RelationClassification", ".", "from_pretrained", "(", "\n", "\"bert-base-uncased\"", ",", "# Use the 12-layer BERT model, with an uncased vocab.", "\n", "num_labels", "=", "NUM_LABELS", ",", "# The number of output labels--2 for binary classification.", "\n", "# You can increase this for multi-class tasks.", "\n", "output_attentions", "=", "False", ",", "# Whether the model returns attentions weights.", "\n", "output_hidden_states", "=", "False", ",", "# Whether the model returns all hidden-states.", "\n", ")", "\n", "\n", "modelf1", "=", "nn", ".", "DataParallel", "(", "modelf1", ")", "\n", "modelf1", ".", "to", "(", "device", ")", "\n", "\n", "modelg2", "=", "LabelGeneration", ".", "from_pretrained", "(", "\n", "\"bert-base-uncased\"", ",", "# Use the 12-layer BERT model, with an uncased vocab.", "\n", "num_labels", "=", "NUM_LABELS", ",", "# The number of output labels--2 for binary classification.", "\n", "# You can increase this for multi-class tasks.", "\n", "output_attentions", "=", "False", ",", "# Whether the model returns attentions weights.", "\n", "output_hidden_states", "=", "False", ",", "# Whether the model returns all hidden-states.", "\n", ")", "\n", "\n", "modelg2", "=", "nn", ".", "DataParallel", "(", "modelg2", ")", "\n", "modelg2", ".", "to", "(", "device", ")", "\n", "\n", "optimizer1", "=", "AdamW", "(", "modelf1", ".", "parameters", "(", ")", ",", "\n", "lr", "=", "LR", ",", "# args.learning_rate - default is 5e-5, our notebook had 2e-5", "\n", "eps", "=", "EPS", "# args.adam_epsilon  - default is 1e-8.", "\n", ")", "\n", "optimizer2", "=", "AdamW", "(", "modelg2", ".", "parameters", "(", ")", ",", "\n", "lr", "=", "LR", ",", "# args.learning_rate - default is 5e-5, our notebook had 2e-5", "\n", "eps", "=", "EPS", "# args.adam_epsilon  - default is 1e-8.", "\n", ")", "\n", "\n", "total_steps1", "=", "0", "\n", "for", "i", "in", "range", "(", "MATE_EPOCHS", "+", "1", ")", ":", "\n", "        ", "total_steps1", "+=", "len", "(", "labeled_dataloader", ")", "\n", "for", "j", "in", "range", "(", "i", ")", ":", "\n", "            ", "total_steps1", "+=", "len", "(", "unlabeled_dataloader", "[", "j", "]", ")", "\n", "", "", "total_steps1", "=", "total_steps1", "*", "EPOCHS", "*", "TOTAL_EPOCHS", "\n", "total_steps2", "=", "len", "(", "labeled_dataloader", ")", "*", "EPOCHS", "*", "MATE_EPOCHS", "*", "TOTAL_EPOCHS", "\n", "\n", "# Create the learning rate scheduler.", "\n", "scheduler1", "=", "get_linear_schedule_with_warmup", "(", "optimizer1", ",", "\n", "num_warmup_steps", "=", "0", ",", "\n", "num_training_steps", "=", "total_steps1", ")", "\n", "scheduler2", "=", "get_linear_schedule_with_warmup", "(", "optimizer2", ",", "\n", "num_warmup_steps", "=", "0", ",", "\n", "num_training_steps", "=", "total_steps2", ")", "\n", "\n", "# Set the seed value all over the place to make this reproducible.", "\n", "random", ".", "seed", "(", "seed_val", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed_val", ")", "\n", "torch", ".", "manual_seed", "(", "seed_val", ")", "\n", "torch", ".", "cuda", ".", "manual_seed_all", "(", "seed_val", ")", "\n", "\n", "# validation accuracy, and timings.", "\n", "training_stats", "=", "[", "]", "\n", "# Measure the total training time for the whole run.", "\n", "total_t0", "=", "time", ".", "time", "(", ")", "\n", "\n", "# ========================================", "\n", "#               train", "\n", "# ========================================", "\n", "\n", "print", "(", "\"training start...\"", ")", "\n", "cnt", "=", "0", "\n", "\n", "# count how many rounds the whole big model has to train", "\n", "# a round means all unlabel data are labeled", "\n", "for", "total_epoch", "in", "range", "(", "TOTAL_EPOCHS", ")", ":", "\n", "\n", "# Add an inner loop to judge the entire unlabeled data set finished", "\n", "        ", "train_dataloader", "=", "labeled_dataloader", "\n", "\n", "for", "mate_epoch", "in", "range", "(", "MATE_EPOCHS", ")", ":", "\n", "\n", "# -------train f1---------", "\n", "\n", "            ", "for", "epoch_i", "in", "range", "(", "0", ",", "EPOCHS", ")", ":", "\n", "                ", "print", "(", "'--- Epoch {:} / {:} ---'", ".", "format", "(", "epoch_i", "+", "1", ",", "EPOCHS", ")", ")", "\n", "# Measure how long the training epoch takes.", "\n", "t0", "=", "time", ".", "time", "(", ")", "\n", "\n", "# Reset the total loss for this epoch.", "\n", "total_train_loss", "=", "0", "\n", "# Put the model into training mode", "\n", "modelf1", ".", "train", "(", ")", "\n", "\n", "for", "step", ",", "batch", "in", "enumerate", "(", "train_dataloader", ")", ":", "\n", "# Progress update every 40 batches.", "\n", "                    ", "if", "step", "%", "40", "==", "0", "and", "not", "step", "==", "0", ":", "\n", "# Calculate elapsed time in minutes.", "\n", "                        ", "elapsed", "=", "format_time", "(", "time", ".", "time", "(", ")", "-", "t0", ")", "\n", "\n", "# Unpack this training batch from our dataloader.", "\n", "", "b_input_ids", "=", "batch", "[", "0", "]", ".", "to", "(", "device", ")", "\n", "b_input_mask", "=", "batch", "[", "1", "]", ".", "to", "(", "device", ")", "\n", "b_labels", "=", "batch", "[", "2", "]", ".", "to", "(", "device", ")", "\n", "b_e1_pos", "=", "batch", "[", "3", "]", ".", "to", "(", "device", ")", "\n", "b_e2_pos", "=", "batch", "[", "4", "]", ".", "to", "(", "device", ")", "\n", "b_w", "=", "batch", "[", "5", "]", ".", "to", "(", "device", ")", "\n", "\n", "modelf1", ".", "zero_grad", "(", ")", "\n", "\n", "# Perform a forward pass (evaluate the model on this training batch)", "\n", "loss", ",", "logits", ",", "_", "=", "modelf1", "(", "b_input_ids", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "attention_mask", "=", "b_input_mask", ",", "\n", "labels", "=", "b_labels", ",", "\n", "e1_pos", "=", "b_e1_pos", ",", "\n", "e2_pos", "=", "b_e2_pos", ",", "\n", "w", "=", "b_w", ")", "\n", "\n", "# Accumulate the training loss over all of the batches", "\n", "total_train_loss", "+=", "loss", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "# Perform a backward pass to calculate the gradients.", "\n", "loss", ".", "sum", "(", ")", ".", "backward", "(", ")", "\n", "# Clip the norm of the gradients to 1.0 to prevent exploding gradients problem.", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "modelf1", ".", "parameters", "(", ")", ",", "1.0", ")", "\n", "# Update parameters and take a step using the computed gradient", "\n", "optimizer1", ".", "step", "(", ")", "\n", "# Update the learning rate.", "\n", "scheduler1", ".", "step", "(", ")", "\n", "\n", "# Calculate the average loss over all of the batches.", "\n", "", "avg_train_loss", "=", "total_train_loss", "/", "len", "(", "train_dataloader", ")", "\n", "# Measure how long this epoch took.", "\n", "training_time", "=", "format_time", "(", "time", ".", "time", "(", ")", "-", "t0", ")", "\n", "\n", "print", "(", "\" f1 Average training loss: {0:.4f}\"", ".", "format", "(", "avg_train_loss", ")", ")", "\n", "# print(\" f1 Training epcoh took: {:}\".format(training_time))", "\n", "\n", "# --------f1 validation-----------", "\n", "\n", "print", "(", "\"validation start...\"", ")", "\n", "t0", "=", "time", ".", "time", "(", ")", "\n", "\n", "# Put the model in evaluation mode--the dropout layers behave differently during evaluation.", "\n", "modelf1", ".", "eval", "(", ")", "\n", "\n", "# Tracking variables", "\n", "total_eval_accuracy", "=", "0", "\n", "total_eval_loss", "=", "0", "\n", "nb_eval_steps", "=", "0", "\n", "all_prediction", "=", "np", ".", "array", "(", "[", "]", ")", "\n", "all_ground_truth", "=", "np", ".", "array", "(", "[", "]", ")", "\n", "\n", "# Evaluate data for one epoch", "\n", "for", "batch", "in", "validation_dataloader", ":", "\n", "# Unpack this training batch from our dataloader.", "\n", "                    ", "b_input_ids", "=", "batch", "[", "0", "]", ".", "to", "(", "device", ")", "\n", "b_input_mask", "=", "batch", "[", "1", "]", ".", "to", "(", "device", ")", "\n", "b_labels", "=", "batch", "[", "2", "]", ".", "to", "(", "device", ")", "\n", "b_e1_pos", "=", "batch", "[", "3", "]", ".", "to", "(", "device", ")", "\n", "b_e2_pos", "=", "batch", "[", "4", "]", ".", "to", "(", "device", ")", "\n", "b_w", "=", "batch", "[", "5", "]", ".", "to", "(", "device", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# Forward pass, calculate logit predictions.", "\n", "                        ", "(", "loss", ",", "logits", ",", "_", ")", "=", "modelf1", "(", "b_input_ids", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "attention_mask", "=", "b_input_mask", ",", "\n", "labels", "=", "b_labels", ",", "\n", "e1_pos", "=", "b_e1_pos", ",", "\n", "e2_pos", "=", "b_e2_pos", ",", "\n", "w", "=", "b_w", ")", "\n", "\n", "# Accumulate the validation loss.", "\n", "", "total_eval_loss", "+=", "loss", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "# Move logits and labels to CPU", "\n", "logits", "=", "logits", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "label_ids", "=", "b_labels", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "\n", "pred_flat", "=", "np", ".", "argmax", "(", "logits", ",", "axis", "=", "1", ")", ".", "flatten", "(", ")", "\n", "labels_flat", "=", "label_ids", ".", "flatten", "(", ")", "\n", "all_prediction", "=", "np", ".", "concatenate", "(", "(", "all_prediction", ",", "pred_flat", ")", ",", "axis", "=", "None", ")", "\n", "all_ground_truth", "=", "np", ".", "concatenate", "(", "(", "all_ground_truth", ",", "labels_flat", ")", ",", "axis", "=", "None", ")", "\n", "\n", "\n", "# Calculate the average loss over all of the batches.", "\n", "", "avg_val_loss", "=", "total_eval_loss", "/", "len", "(", "validation_dataloader", ")", "\n", "\n", "# Measure how long the validation run took.", "\n", "validation_time", "=", "format_time", "(", "time", ".", "time", "(", ")", "-", "t0", ")", "\n", "\n", "score", "(", "all_ground_truth", ",", "all_prediction", ")", "\n", "\n", "# -------f1 predict and g2 update---------", "\n", "\n", "", "print", "(", "\"f1 predict and g2 update start...\"", ")", "\n", "\n", "for", "epoch_i", "in", "range", "(", "0", ",", "EPOCHS", ")", ":", "\n", "                ", "print", "(", "'--- Epoch {:} / {:} ---'", ".", "format", "(", "epoch_i", "+", "1", ",", "EPOCHS", ")", ")", "\n", "# Measure how long the training epoch takes.", "\n", "t0", "=", "time", ".", "time", "(", ")", "\n", "\n", "# Reset the total loss for this epoch.", "\n", "total_train_loss", "=", "0", "\n", "\n", "modelg2", ".", "train", "(", ")", "\n", "modelf1", ".", "eval", "(", ")", "\n", "\n", "for", "step", ",", "batch", "in", "enumerate", "(", "labeled_dataloader", ")", ":", "\n", "# Progress update every 40 batches.", "\n", "                    ", "if", "step", "%", "40", "==", "0", "and", "not", "step", "==", "0", ":", "\n", "# Calculate elapsed time in minutes.", "\n", "                        ", "elapsed", "=", "format_time", "(", "time", ".", "time", "(", ")", "-", "t0", ")", "\n", "# Report progress.", "\n", "\n", "# Unpack this training batch from our dataloader.", "\n", "", "b_input_ids", "=", "batch", "[", "0", "]", ".", "to", "(", "device", ")", "\n", "b_input_mask", "=", "batch", "[", "1", "]", ".", "to", "(", "device", ")", "\n", "b_labels", "=", "batch", "[", "2", "]", ".", "to", "(", "device", ")", "\n", "b_e1_pos", "=", "batch", "[", "3", "]", ".", "to", "(", "device", ")", "\n", "b_e2_pos", "=", "batch", "[", "4", "]", ".", "to", "(", "device", ")", "\n", "b_w", "=", "batch", "[", "5", "]", ".", "to", "(", "device", ")", "\n", "\n", "# f1 predict", "\n", "modelf1", ".", "zero_grad", "(", ")", "\n", "#with torch.no_grad():", "\n", "# Forward pass, calculate logit predictions.", "\n", "loss1", ",", "logits1", ",", "_", "=", "modelf1", "(", "b_input_ids", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "attention_mask", "=", "b_input_mask", ",", "\n", "labels", "=", "b_labels", ",", "\n", "e1_pos", "=", "b_e1_pos", ",", "\n", "e2_pos", "=", "b_e2_pos", ",", "\n", "w", "=", "b_w", ")", "\n", "\n", "# g2 train", "\n", "modelg2", ".", "zero_grad", "(", ")", "\n", "# Perform a forward pass", "\n", "loss2", ",", "logits2", ",", "_", "=", "modelg2", "(", "b_input_ids", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "attention_mask", "=", "b_input_mask", ",", "\n", "labels", "=", "b_labels", ",", "\n", "e1_pos", "=", "b_e1_pos", ",", "\n", "e2_pos", "=", "b_e2_pos", ",", "\n", "w", "=", "b_w", ")", "\n", "\n", "# Perform a backward pass to calculate the gradients.", "\n", "#loss1.sum().backward()", "\n", "(", "loss1", ".", "sum", "(", ")", "+", "0.4", "*", "loss2", ".", "sum", "(", ")", ")", ".", "backward", "(", ")", "\n", "# Clip the norm of the gradients to 1.0 to prevent exploding gradients problem.", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "modelg2", ".", "parameters", "(", ")", ",", "1.0", ")", "\n", "# Update parameters and take a step using the computed gradient", "\n", "optimizer2", ".", "step", "(", ")", "\n", "# Update the learning rate.", "\n", "scheduler2", ".", "step", "(", ")", "\n", "\n", "# Accumulate the training loss over all of the batches", "\n", "total_train_loss", "+=", "loss2", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "\n", "# Calculate the average loss over all of the batches.", "\n", "", "avg_train_loss", "=", "total_train_loss", "/", "len", "(", "labeled_dataloader", ")", "\n", "# Measure how long this epoch took.", "\n", "training_time", "=", "format_time", "(", "time", ".", "time", "(", ")", "-", "t0", ")", "\n", "\n", "print", "(", "\" g2 Average training loss by g2: {0:.4f}\"", ".", "format", "(", "avg_train_loss", ")", ")", "\n", "# print(\" g2 Training epcoh took: {:}\".format(training_time))", "\n", "\n", "# -------g2 generate pseudo label---------", "\n", "\n", "", "print", "(", "\"g2 generate pseudo label\"", ")", "\n", "\n", "modelg2", ".", "eval", "(", ")", "\n", "input_ids", "=", "[", "]", "\n", "input_mask", "=", "[", "]", "\n", "# labels = []", "\n", "gold_labels", "=", "[", "]", "\n", "e1_pos", "=", "[", "]", "\n", "e2_pos", "=", "[", "]", "\n", "w", "=", "[", "]", "\n", "all_logits", "=", "[", "]", "\n", "all_prediction", "=", "np", ".", "array", "(", "[", "]", ")", "\n", "all_ground_truth", "=", "np", ".", "array", "(", "[", "]", ")", "\n", "all_weights", "=", "np", ".", "array", "(", "[", "]", ")", "\n", "all_pseudo_prediction", "=", "np", ".", "array", "(", "[", "]", ")", "\n", "all_pseudo_ground_truth", "=", "np", ".", "array", "(", "[", "]", ")", "\n", "all_pseudo_weights", "=", "np", ".", "array", "(", "[", "]", ")", "\n", "# Evaluate data for one epoch", "\n", "for", "batch", "in", "unlabeled_dataloader", "[", "mate_epoch", "]", ":", "# unlabeled_dataloader", "\n", "# Unpack this training batch from our dataloader.", "\n", "                ", "b_input_ids", "=", "batch", "[", "0", "]", ".", "to", "(", "device", ")", "\n", "b_input_mask", "=", "batch", "[", "1", "]", ".", "to", "(", "device", ")", "\n", "b_labels", "=", "batch", "[", "2", "]", ".", "to", "(", "device", ")", "\n", "b_e1_pos", "=", "batch", "[", "3", "]", ".", "to", "(", "device", ")", "\n", "b_e2_pos", "=", "batch", "[", "4", "]", ".", "to", "(", "device", ")", "\n", "b_w", "=", "batch", "[", "5", "]", ".", "to", "(", "device", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# Forward pass, calculate logit predictions.", "\n", "                    ", "(", "loss", ",", "logits", ",", "_", ")", "=", "modelg2", "(", "b_input_ids", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "attention_mask", "=", "b_input_mask", ",", "\n", "labels", "=", "b_labels", ",", "\n", "e1_pos", "=", "b_e1_pos", ",", "\n", "e2_pos", "=", "b_e2_pos", ",", "\n", "w", "=", "b_w", ")", "\n", "\n", "", "logits", "=", "logits", ".", "detach", "(", ")", "\n", "all_logits", ".", "append", "(", "logits", ")", "\n", "input_ids", ".", "append", "(", "b_input_ids", ")", "\n", "input_mask", ".", "append", "(", "b_input_mask", ")", "\n", "gold_labels", ".", "append", "(", "b_labels", ")", "\n", "e1_pos", ".", "append", "(", "b_e1_pos", ")", "\n", "e2_pos", ".", "append", "(", "b_e2_pos", ")", "\n", "w", ".", "append", "(", "b_w", ")", "\n", "\n", "", "logits", "=", "torch", ".", "cat", "(", "all_logits", ",", "dim", "=", "0", ")", "\n", "probs", "=", "F", ".", "softmax", "(", "logits", ",", "dim", "=", "1", ")", "\n", "input_ids", "=", "torch", ".", "cat", "(", "input_ids", ",", "dim", "=", "0", ")", "\n", "input_mask", "=", "torch", ".", "cat", "(", "input_mask", ",", "dim", "=", "0", ")", "\n", "gold_labels", "=", "torch", ".", "cat", "(", "gold_labels", ",", "dim", "=", "0", ")", "\n", "e1_pos", "=", "torch", ".", "cat", "(", "e1_pos", ",", "dim", "=", "0", ")", "\n", "e2_pos", "=", "torch", ".", "cat", "(", "e2_pos", ",", "dim", "=", "0", ")", "\n", "label_weights", ",", "labels", "=", "torch", ".", "max", "(", "probs", ",", "dim", "=", "1", ")", "\n", "\n", "# evaluate generate label quality", "\n", "logits", "=", "logits", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "label_ids", "=", "gold_labels", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "pred_weights", "=", "label_weights", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "pred_flat", "=", "np", ".", "argmax", "(", "logits", ",", "axis", "=", "1", ")", ".", "flatten", "(", ")", "\n", "labels_flat", "=", "label_ids", ".", "flatten", "(", ")", "\n", "pred_weights", "=", "pred_weights", ".", "flatten", "(", ")", "\n", "all_prediction", "=", "np", ".", "concatenate", "(", "(", "all_prediction", ",", "pred_flat", ")", ",", "axis", "=", "None", ")", "\n", "all_ground_truth", "=", "np", ".", "concatenate", "(", "(", "all_ground_truth", ",", "labels_flat", ")", ",", "axis", "=", "None", ")", "\n", "all_weights", "=", "np", ".", "concatenate", "(", "(", "all_weights", ",", "pred_weights", ")", ",", "axis", "=", "None", ")", "\n", "\n", "\n", "# Pseudo Label Selection, top Z%", "\n", "sort", "=", "torch", ".", "argsort", "(", "label_weights", ",", "descending", "=", "True", ")", "\n", "sort", "=", "sort", "[", "0", ":", "int", "(", "len", "(", "sort", ")", "*", "Z_RATIO", ")", "]", "\n", "input_ids", "=", "input_ids", "[", "sort", "]", "\n", "input_mask", "=", "input_mask", "[", "sort", "]", "\n", "pseudo_labels", "=", "labels", "[", "sort", "]", "\n", "e1_pos", "=", "e1_pos", "[", "sort", "]", "\n", "e2_pos", "=", "e2_pos", "[", "sort", "]", "\n", "w", "=", "label_weights", "[", "sort", "]", "\n", "pseudo_gold_labels", "=", "gold_labels", "[", "sort", "]", "\n", "\n", "\n", "# evaluate pseudo label quality", "\n", "pred", "=", "pseudo_labels", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "label_ids", "=", "pseudo_gold_labels", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "weights", "=", "w", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "pred_flat", "=", "pred", ".", "flatten", "(", ")", "\n", "labels_flat", "=", "label_ids", ".", "flatten", "(", ")", "\n", "weights_flat", "=", "weights", ".", "flatten", "(", ")", "\n", "all_pseudo_prediction", "=", "np", ".", "concatenate", "(", "(", "all_pseudo_prediction", ",", "pred_flat", ")", ",", "axis", "=", "None", ")", "\n", "all_pseudo_ground_truth", "=", "np", ".", "concatenate", "(", "(", "all_pseudo_ground_truth", ",", "labels_flat", ")", ",", "axis", "=", "None", ")", "\n", "all_pseudo_weights", "=", "np", ".", "concatenate", "(", "(", "all_pseudo_weights", ",", "weights_flat", ")", ",", "axis", "=", "None", ")", "\n", "\n", "#  evaluate generate label quality", "\n", "np_sort", "=", "np", ".", "argsort", "(", "all_weights", ")", "\n", "print", "(", "\"  Generate F1 score\"", ")", "\n", "score", "(", "all_ground_truth", ",", "all_prediction", ")", "\n", "np", ".", "save", "(", "LOG_DIR", "+", "'/'", "+", "LOG_DIR", "+", "'_all_prediction{}'", ".", "format", "(", "cnt", ")", ",", "all_prediction", "[", "np_sort", "]", ")", "\n", "np", ".", "save", "(", "LOG_DIR", "+", "'/'", "+", "LOG_DIR", "+", "'_all_label{}'", ".", "format", "(", "cnt", ")", ",", "all_ground_truth", "[", "np_sort", "]", ")", "\n", "np", ".", "save", "(", "LOG_DIR", "+", "'/'", "+", "LOG_DIR", "+", "'_all_weights{}'", ".", "format", "(", "cnt", ")", ",", "all_weights", "[", "np_sort", "]", ")", "\n", "#  evaluate pseudo label quality", "\n", "print", "(", "\"  Pseudo F1 score\"", ")", "\n", "score", "(", "all_pseudo_ground_truth", ",", "all_pseudo_prediction", ")", "\n", "np", ".", "save", "(", "LOG_DIR", "+", "'/'", "+", "LOG_DIR", "+", "'_all_pseudo_prediction{}'", ".", "format", "(", "cnt", ")", ",", "all_pseudo_prediction", ")", "\n", "np", ".", "save", "(", "LOG_DIR", "+", "'/'", "+", "LOG_DIR", "+", "'_all_pseudo_label{}'", ".", "format", "(", "cnt", ")", ",", "all_pseudo_ground_truth", ")", "\n", "np", ".", "save", "(", "LOG_DIR", "+", "'/'", "+", "LOG_DIR", "+", "'_all_pseudo_weights{}'", ".", "format", "(", "cnt", ")", ",", "all_pseudo_weights", ")", "\n", "cnt", "+=", "1", "\n", "\n", "\n", "# update training data", "\n", "train_add_dataset", "=", "train_dataloader", ".", "dataset", "+", "TensorDataset", "(", "input_ids", ",", "input_mask", ",", "pseudo_labels", ",", "e1_pos", ",", "e2_pos", ",", "w", ")", "\n", "train_dataloader", "=", "DataLoader", "(", "\n", "train_add_dataset", ",", "# The training samples.", "\n", "sampler", "=", "RandomSampler", "(", "train_add_dataset", ")", ",", "# Select batches randomly", "\n", "batch_size", "=", "BATCH_SIZE", "# Trains with this batch size.", "\n", ")", "\n", "\n", "# train f1 with all data", "\n", "", "for", "epoch_i", "in", "range", "(", "0", ",", "EPOCHS", ")", ":", "\n", "            ", "print", "(", "'--- Epoch {:} / {:} ---'", ".", "format", "(", "epoch_i", "+", "1", ",", "EPOCHS", ")", ")", "\n", "# Measure how long the training epoch takes.", "\n", "t0", "=", "time", ".", "time", "(", ")", "\n", "\n", "# Reset the total loss for this epoch.", "\n", "total_train_loss", "=", "0", "\n", "# Put the model into training mode", "\n", "modelf1", ".", "train", "(", ")", "\n", "\n", "for", "step", ",", "batch", "in", "enumerate", "(", "train_dataloader", ")", ":", "\n", "# Progress update every 40 batches.", "\n", "                ", "if", "step", "%", "40", "==", "0", "and", "not", "step", "==", "0", ":", "\n", "# Calculate elapsed time in minutes.", "\n", "                    ", "elapsed", "=", "format_time", "(", "time", ".", "time", "(", ")", "-", "t0", ")", "\n", "\n", "# Unpack this training batch from our dataloader.", "\n", "", "b_input_ids", "=", "batch", "[", "0", "]", ".", "to", "(", "device", ")", "\n", "b_input_mask", "=", "batch", "[", "1", "]", ".", "to", "(", "device", ")", "\n", "b_labels", "=", "batch", "[", "2", "]", ".", "to", "(", "device", ")", "\n", "b_e1_pos", "=", "batch", "[", "3", "]", ".", "to", "(", "device", ")", "\n", "b_e2_pos", "=", "batch", "[", "4", "]", ".", "to", "(", "device", ")", "\n", "b_w", "=", "batch", "[", "5", "]", ".", "to", "(", "device", ")", "\n", "\n", "modelf1", ".", "zero_grad", "(", ")", "\n", "\n", "# Perform a forward pass (evaluate the model on this training batch)", "\n", "loss", ",", "logits", ",", "_", "=", "modelf1", "(", "b_input_ids", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "attention_mask", "=", "b_input_mask", ",", "\n", "labels", "=", "b_labels", ",", "\n", "e1_pos", "=", "b_e1_pos", ",", "\n", "e2_pos", "=", "b_e2_pos", ",", "\n", "w", "=", "b_w", ")", "\n", "\n", "# Accumulate the training loss over all of the batches", "\n", "total_train_loss", "+=", "loss", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "# Perform a backward pass to calculate the gradients.", "\n", "loss", ".", "sum", "(", ")", ".", "backward", "(", ")", "\n", "# Clip the norm of the gradients to 1.0 to prevent exploding gradients problem.", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "modelf1", ".", "parameters", "(", ")", ",", "1.0", ")", "\n", "# Update parameters and take a step using the computed gradient", "\n", "optimizer1", ".", "step", "(", ")", "\n", "# Update the learning rate.", "\n", "scheduler1", ".", "step", "(", ")", "\n", "\n", "# Calculate the average loss over all of the batches.", "\n", "", "avg_train_loss", "=", "total_train_loss", "/", "len", "(", "train_dataloader", ")", "\n", "# Measure how long this epoch took.", "\n", "training_time", "=", "format_time", "(", "time", ".", "time", "(", ")", "-", "t0", ")", "\n", "\n", "print", "(", "\" f1 Average training loss: {0:.4f}\"", ".", "format", "(", "avg_train_loss", ")", ")", "\n", "# print(\" f1 Training epcoh took: {:}\".format(training_time))", "\n", "\n", "# final f1 validation", "\n", "", "print", "(", "\"final validation start...\"", ")", "\n", "t0", "=", "time", ".", "time", "(", ")", "\n", "\n", "# Put the model in evaluation mode--the dropout layers behave differently during evaluation.", "\n", "modelf1", ".", "eval", "(", ")", "\n", "\n", "# Tracking variables", "\n", "total_eval_accuracy", "=", "0", "\n", "total_eval_loss", "=", "0", "\n", "nb_eval_steps", "=", "0", "\n", "\n", "all_prediction", "=", "np", ".", "array", "(", "[", "]", ")", "\n", "all_ground_truth", "=", "np", ".", "array", "(", "[", "]", ")", "\n", "\n", "# Evaluate data for one epoch", "\n", "for", "batch", "in", "validation_dataloader", ":", "\n", "# Unpack this training batch from our dataloader.", "\n", "            ", "b_input_ids", "=", "batch", "[", "0", "]", ".", "to", "(", "device", ")", "\n", "b_input_mask", "=", "batch", "[", "1", "]", ".", "to", "(", "device", ")", "\n", "b_labels", "=", "batch", "[", "2", "]", ".", "to", "(", "device", ")", "\n", "b_e1_pos", "=", "batch", "[", "3", "]", ".", "to", "(", "device", ")", "\n", "b_e2_pos", "=", "batch", "[", "4", "]", ".", "to", "(", "device", ")", "\n", "b_w", "=", "batch", "[", "5", "]", ".", "to", "(", "device", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# Forward pass, calculate logit predictions.", "\n", "                ", "(", "loss", ",", "logits", ",", "_", ")", "=", "modelf1", "(", "b_input_ids", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "attention_mask", "=", "b_input_mask", ",", "\n", "labels", "=", "b_labels", ",", "\n", "e1_pos", "=", "b_e1_pos", ",", "\n", "e2_pos", "=", "b_e2_pos", ",", "\n", "w", "=", "b_w", ")", "\n", "\n", "# Accumulate the validation loss.", "\n", "", "total_eval_loss", "+=", "loss", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "# Move logits and labels to CPU", "\n", "logits", "=", "logits", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "label_ids", "=", "b_labels", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "pred_flat", "=", "np", ".", "argmax", "(", "logits", ",", "axis", "=", "1", ")", ".", "flatten", "(", ")", "\n", "labels_flat", "=", "label_ids", ".", "flatten", "(", ")", "\n", "all_prediction", "=", "np", ".", "concatenate", "(", "(", "all_prediction", ",", "pred_flat", ")", ",", "axis", "=", "None", ")", "\n", "all_ground_truth", "=", "np", ".", "concatenate", "(", "(", "all_ground_truth", ",", "labels_flat", ")", ",", "axis", "=", "None", ")", "\n", "\n", "\n", "# Calculate the average loss over all of the batches.", "\n", "", "avg_val_loss", "=", "total_eval_loss", "/", "len", "(", "validation_dataloader", ")", "\n", "\n", "validation_time", "=", "format_time", "(", "time", ".", "time", "(", ")", "-", "t0", ")", "\n", "non_zero_idx", "=", "(", "all_ground_truth", "!=", "0", ")", "\n", "validation_acc", "=", "np", ".", "sum", "(", "all_prediction", "[", "non_zero_idx", "]", "==", "all_ground_truth", "[", "non_zero_idx", "]", ")", "/", "len", "(", "all_ground_truth", "[", "non_zero_idx", "]", ")", "\n", "validation_f1_score", "=", "f1_score", "(", "all_ground_truth", "[", "non_zero_idx", "]", ",", "all_prediction", "[", "non_zero_idx", "]", ",", "average", "=", "\"micro\"", ")", "\n", "\n", "score", "(", "all_ground_truth", ",", "all_prediction", ")", "\n", "\n", "# ----------------------training complete-----------------------", "\n", "\n", "", "print", "(", "\"Training complete!\"", ")", "\n", "print", "(", "\"Total training took {:} (h:mm:ss)\"", ".", "format", "(", "format_time", "(", "time", ".", "time", "(", ")", "-", "total_t0", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.src.train_self_training.flat_accuracy": [[56, 64], ["numpy.argmax().flatten", "labels.flatten", "numpy.sum", "len", "numpy.argmax"], "function", ["None"], ["def", "flat_accuracy", "(", "preds", ",", "labels", ")", ":", "\n", "    ", "pred_flat", "=", "np", ".", "argmax", "(", "preds", ",", "axis", "=", "1", ")", ".", "flatten", "(", ")", "\n", "labels_flat", "=", "labels", ".", "flatten", "(", ")", "\n", "non_zero_idx", "=", "(", "labels_flat", "!=", "0", ")", "\n", "# if len(labels_flat[non_zero_idx])==0:", "\n", "#     print(\"error occur: \", labels_flat)", "\n", "#     return 0", "\n", "return", "np", ".", "sum", "(", "pred_flat", "[", "non_zero_idx", "]", "==", "labels_flat", "[", "non_zero_idx", "]", ")", "/", "len", "(", "labels_flat", "[", "non_zero_idx", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.src.train_self_training.format_time": [[67, 72], ["int", "str", "round", "datetime.timedelta"], "function", ["None"], ["", "def", "format_time", "(", "elapsed", ")", ":", "\n", "# Round to the nearest second.", "\n", "    ", "elapsed_rounded", "=", "int", "(", "round", "(", "(", "elapsed", ")", ")", ")", "\n", "# Format as hh:mm:ss", "\n", "return", "str", "(", "datetime", ".", "timedelta", "(", "seconds", "=", "elapsed_rounded", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.src.train_self_training.score": [[75, 116], ["collections.Counter", "collections.Counter", "collections.Counter", "range", "print", "print", "print", "print", "len", "print", "sum", "sum", "collections.Counter.values", "float", "float", "collections.Counter.values", "float", "float", "sum", "sum", "sum", "sum", "collections.Counter.values", "collections.Counter.values", "collections.Counter.values", "collections.Counter.values"], "function", ["None"], ["", "def", "score", "(", "key", ",", "prediction", ",", "verbose", "=", "True", ",", "NO_RELATION", "=", "0", ")", ":", "\n", "    ", "correct_by_relation", "=", "Counter", "(", ")", "\n", "guessed_by_relation", "=", "Counter", "(", ")", "\n", "gold_by_relation", "=", "Counter", "(", ")", "\n", "\n", "# Loop over the data to compute a score", "\n", "for", "row", "in", "range", "(", "len", "(", "key", ")", ")", ":", "\n", "        ", "gold", "=", "key", "[", "row", "]", "\n", "guess", "=", "prediction", "[", "row", "]", "\n", "\n", "if", "gold", "==", "NO_RELATION", "and", "guess", "==", "NO_RELATION", ":", "\n", "            ", "pass", "\n", "", "elif", "gold", "==", "NO_RELATION", "and", "guess", "!=", "NO_RELATION", ":", "\n", "            ", "guessed_by_relation", "[", "guess", "]", "+=", "1", "\n", "", "elif", "gold", "!=", "NO_RELATION", "and", "guess", "==", "NO_RELATION", ":", "\n", "            ", "gold_by_relation", "[", "gold", "]", "+=", "1", "\n", "", "elif", "gold", "!=", "NO_RELATION", "and", "guess", "!=", "NO_RELATION", ":", "\n", "            ", "guessed_by_relation", "[", "guess", "]", "+=", "1", "\n", "gold_by_relation", "[", "gold", "]", "+=", "1", "\n", "if", "gold", "==", "guess", ":", "\n", "                ", "correct_by_relation", "[", "guess", "]", "+=", "1", "\n", "\n", "# Print the aggregate score", "\n", "", "", "", "if", "verbose", ":", "\n", "        ", "print", "(", "\"Final Score:\"", ")", "\n", "", "prec_micro", "=", "1.0", "\n", "if", "sum", "(", "guessed_by_relation", ".", "values", "(", ")", ")", ">", "0", ":", "\n", "        ", "prec_micro", "=", "float", "(", "sum", "(", "correct_by_relation", ".", "values", "(", ")", ")", ")", "/", "float", "(", "\n", "sum", "(", "guessed_by_relation", ".", "values", "(", ")", ")", ")", "\n", "", "recall_micro", "=", "0.0", "\n", "if", "sum", "(", "gold_by_relation", ".", "values", "(", ")", ")", ">", "0", ":", "\n", "        ", "recall_micro", "=", "float", "(", "sum", "(", "correct_by_relation", ".", "values", "(", ")", ")", ")", "/", "float", "(", "\n", "sum", "(", "gold_by_relation", ".", "values", "(", ")", ")", ")", "\n", "", "f1_micro", "=", "0.0", "\n", "if", "prec_micro", "+", "recall_micro", ">", "0.0", ":", "\n", "        ", "f1_micro", "=", "2.0", "*", "prec_micro", "*", "recall_micro", "/", "(", "prec_micro", "+", "recall_micro", ")", "\n", "", "print", "(", "\"SET NO_RELATION ID: \"", ",", "NO_RELATION", ")", "\n", "print", "(", "\"Precision (micro): {:.3%}\"", ".", "format", "(", "prec_micro", ")", ")", "\n", "print", "(", "\"   Recall (micro): {:.3%}\"", ".", "format", "(", "recall_micro", ")", ")", "\n", "print", "(", "\"       F1 (micro): {:.3%}\"", ".", "format", "(", "f1_micro", ")", ")", "\n", "return", "prec_micro", ",", "recall_micro", ",", "f1_micro", "\n", "\n"]], "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.src.train_self_training.pre_processing": [[120, 168], ["print", "transformers.BertTokenizer.from_pretrained", "range", "torch.cat().to", "torch.cat().to", "torch.cat().to", "torch.cat().to", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.ones", "torch.ones", "torch.utils.data.TensorDataset", "len", "BertTokenizer.from_pretrained.encode_plus", "len", "[].item", "[].item", "torch.tensor.append", "torch.tensor.append", "torch.cat().to.append", "torch.cat().to.append", "torch.tensor.append", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "function", ["None"], ["", "def", "pre_processing", "(", "sentence_train", ",", "sentence_train_label", ")", ":", "\n", "    ", "input_ids", "=", "[", "]", "\n", "attention_masks", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "e1_pos", "=", "[", "]", "\n", "e2_pos", "=", "[", "]", "\n", "\n", "# Load tokenizer.", "\n", "print", "(", "'Loading BERT tokenizer...'", ")", "\n", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "'bert-base-uncased'", ",", "do_lower_case", "=", "True", ")", "\n", "\n", "# pre-processing sentenses to BERT pattern", "\n", "for", "i", "in", "range", "(", "len", "(", "sentence_train", ")", ")", ":", "\n", "        ", "encoded_dict", "=", "tokenizer", ".", "encode_plus", "(", "\n", "sentence_train", "[", "i", "]", ",", "# Sentence to encode.", "\n", "add_special_tokens", "=", "False", ",", "# Add '[CLS]' and '[SEP]'", "\n", "max_length", "=", "MAX_LENGTH", ",", "# Pad & truncate all sentences.", "\n", "pad_to_max_length", "=", "True", ",", "\n", "return_attention_mask", "=", "True", ",", "# Construct attn. masks.", "\n", "return_tensors", "=", "'pt'", ",", "# Return pytorch tensors.", "\n", ")", "\n", "try", ":", "\n", "# Find e1(id:2487) and e2(id:2475) position", "\n", "            ", "pos1", "=", "(", "encoded_dict", "[", "'input_ids'", "]", "==", "2487", ")", ".", "nonzero", "(", ")", "[", "0", "]", "[", "1", "]", ".", "item", "(", ")", "\n", "pos2", "=", "(", "encoded_dict", "[", "'input_ids'", "]", "==", "2475", ")", ".", "nonzero", "(", ")", "[", "0", "]", "[", "1", "]", ".", "item", "(", ")", "\n", "e1_pos", ".", "append", "(", "pos1", ")", "\n", "e2_pos", ".", "append", "(", "pos2", ")", "\n", "# Add the encoded sentence to the list.", "\n", "input_ids", ".", "append", "(", "encoded_dict", "[", "'input_ids'", "]", ")", "\n", "# And its attention mask (simply differentiates padding from non-padding).", "\n", "attention_masks", ".", "append", "(", "encoded_dict", "[", "'attention_mask'", "]", ")", "\n", "labels", ".", "append", "(", "sentence_train_label", "[", "i", "]", ")", "\n", "", "except", ":", "\n", "            ", "pass", "\n", "#print(sent)", "\n", "\n", "# Convert the lists into tensors.", "\n", "", "", "input_ids", "=", "torch", ".", "cat", "(", "input_ids", ",", "dim", "=", "0", ")", ".", "to", "(", "device", ")", "\n", "attention_masks", "=", "torch", ".", "cat", "(", "attention_masks", ",", "dim", "=", "0", ")", ".", "to", "(", "device", ")", "\n", "labels", "=", "torch", ".", "tensor", "(", "labels", ",", "device", "=", "'cuda'", ")", "\n", "e1_pos", "=", "torch", ".", "tensor", "(", "e1_pos", ",", "device", "=", "'cuda'", ")", "\n", "e2_pos", "=", "torch", ".", "tensor", "(", "e2_pos", ",", "device", "=", "'cuda'", ")", "\n", "w", "=", "torch", ".", "ones", "(", "len", "(", "e1_pos", ")", ",", "device", "=", "'cuda'", ")", "\n", "\n", "# Combine the training inputs into a TensorDataset.", "\n", "train_dataset", "=", "TensorDataset", "(", "input_ids", ",", "attention_masks", ",", "labels", ",", "e1_pos", ",", "e2_pos", ",", "w", ")", "\n", "\n", "return", "train_dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.src.train_self_training.stratified_sample": [[170, 183], ["range", "data_dict.values", "len", "data_dict[].append", "random.shuffle", "torch.utils.data.Subset", "torch.utils.data.Subset", "data_dict.get", "[].item", "int", "int", "len", "[].item", "[].item", "len", "len"], "function", ["None"], ["", "def", "stratified_sample", "(", "dataset", ",", "ratio", ")", ":", "\n", "    ", "data_dict", "=", "{", "}", "\n", "for", "i", "in", "range", "(", "len", "(", "dataset", ")", ")", ":", "\n", "        ", "if", "not", "data_dict", ".", "get", "(", "dataset", "[", "i", "]", "[", "2", "]", ".", "item", "(", ")", ")", ":", "\n", "            ", "data_dict", "[", "dataset", "[", "i", "]", "[", "2", "]", ".", "item", "(", ")", "]", "=", "[", "]", "\n", "", "data_dict", "[", "dataset", "[", "i", "]", "[", "2", "]", ".", "item", "(", ")", "]", ".", "append", "(", "i", ")", "\n", "", "sampled_indices", "=", "[", "]", "\n", "rest_indices", "=", "[", "]", "\n", "for", "indices", "in", "data_dict", ".", "values", "(", ")", ":", "\n", "        ", "random", ".", "shuffle", "(", "indices", ")", "\n", "sampled_indices", "+=", "indices", "[", "0", ":", "int", "(", "len", "(", "indices", ")", "*", "ratio", ")", "]", "\n", "rest_indices", "+=", "indices", "[", "int", "(", "len", "(", "indices", ")", "*", "ratio", ")", ":", "len", "(", "indices", ")", "]", "\n", "", "return", "[", "Subset", "(", "dataset", ",", "sampled_indices", ")", ",", "Subset", "(", "dataset", ",", "rest_indices", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.src.train_self_training.main": [[187, 775], ["json.load", "json.load", "train_self_training.pre_processing", "train_self_training.stratified_sample", "range", "torch.utils.data.DataLoader", "range", "json.load", "json.load", "train_self_training.pre_processing", "torch.utils.data.DataLoader", "networks.RelationClassification.from_pretrained", "torch.nn.DataParallel", "nn.DataParallel.to", "transformers.AdamW", "range", "transformers.get_linear_schedule_with_warmup", "random.seed", "numpy.random.seed", "torch.manual_seed", "torch.manual_seed", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all", "time.time", "print", "range", "print", "print", "open", "open", "train_self_training.stratified_sample", "unlabeled_dataset.append", "torch.utils.data.DataLoader", "unlabeled_dataloader.append", "open", "open", "nn.DataParallel.parameters", "len", "range", "range", "range", "print", "time.time", "nn.DataParallel.eval", "numpy.array", "numpy.array", "train_self_training.format_time", "sklearn.metrics.f1_score", "train_self_training.score", "torch.utils.data.RandomSampler", "torch.utils.data.SequentialSampler", "len", "range", "print", "nn.DataParallel.eval", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "torch.cat", "torch.cat", "torch.softmax", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.max", "torch.max", "logits.detach().cpu().numpy.detach().cpu().numpy", "torch.cat.to().numpy", "label_weights.to().numpy", "numpy.argmax().flatten", "b_labels.to().numpy.flatten", "pred_weights.flatten.flatten", "numpy.concatenate", "numpy.concatenate", "numpy.concatenate", "torch.argsort", "torch.argsort", "pseudo_labels.to().numpy", "pseudo_gold_labels.to().numpy", "w.to().numpy", "pseudo_labels.to().numpy.flatten", "b_labels.to().numpy.flatten", "w.to().numpy.flatten", "numpy.concatenate", "numpy.concatenate", "numpy.concatenate", "numpy.argsort", "print", "train_self_training.score", "numpy.save", "numpy.save", "numpy.save", "print", "train_self_training.score", "numpy.save", "numpy.save", "numpy.save", "torch.utils.data.DataLoader", "print", "time.time", "nn.DataParallel.train", "enumerate", "train_self_training.format_time", "print", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "loss.sum().item", "logits.detach().cpu().numpy.detach().cpu().numpy", "batch[].to.to().numpy", "numpy.argmax().flatten", "b_labels.to().numpy.flatten", "numpy.concatenate", "numpy.concatenate", "len", "numpy.sum", "len", "train_self_training.format_time", "torch.utils.data.RandomSampler", "len", "print", "time.time", "nn.DataParallel.train", "enumerate", "train_self_training.format_time", "print", "print", "time.time", "nn.DataParallel.eval", "numpy.array", "numpy.array", "train_self_training.format_time", "train_self_training.score", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "logits.detach().cpu().numpy.detach", "all_logits.append", "torch.cat.append", "torch.cat.append", "torch.cat.append", "torch.cat.append", "torch.cat.append", "w.append", "torch.utils.data.TensorDataset", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "nn.DataParallel.zero_grad", "nn.DataParallel.", "loss.sum().item", "loss.sum().backward", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "transformers.AdamW.step", "transformers.get_linear_schedule_with_warmup.step", "len", "torch.no_grad", "torch.no_grad", "nn.DataParallel.", "time.time", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "nn.DataParallel.zero_grad", "nn.DataParallel.", "loss.sum().item", "loss.sum().backward", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "transformers.AdamW.step", "transformers.get_linear_schedule_with_warmup.step", "len", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "loss.sum().item", "logits.detach().cpu().numpy.detach().cpu().numpy", "batch[].to.to().numpy", "numpy.argmax().flatten", "b_labels.to().numpy.flatten", "numpy.concatenate", "numpy.concatenate", "len", "torch.no_grad", "torch.no_grad", "nn.DataParallel.", "logits.detach().cpu().numpy.detach().cpu", "torch.cat.to", "label_weights.to", "numpy.argmax", "int", "pseudo_labels.to", "pseudo_gold_labels.to", "w.to", "torch.utils.data.RandomSampler", "train_self_training.format_time", "nn.DataParallel.parameters", "time.time", "loss.sum", "logits.detach().cpu().numpy.detach().cpu", "batch[].to.to", "numpy.argmax", "time.time", "train_self_training.format_time", "nn.DataParallel.parameters", "time.time", "torch.no_grad", "torch.no_grad", "nn.DataParallel.", "time.time", "loss.sum", "loss.sum", "loss.sum", "loss.sum", "loss.sum", "logits.detach().cpu().numpy.detach().cpu", "batch[].to.to", "numpy.argmax", "logits.detach().cpu().numpy.detach", "len", "time.time", "logits.detach().cpu().numpy.detach", "time.time", "logits.detach().cpu().numpy.detach"], "function", ["home.repos.pwc.inspect_result.THU-BPM_MetaSRE.src.train_self_training.pre_processing", "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.src.train_self_training.stratified_sample", "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.src.train_self_training.pre_processing", "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.src.train_self_training.stratified_sample", "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.src.train_self_training.format_time", "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.data.data_analysis.score", "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.data.data_analysis.score", "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.data.data_analysis.score", "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.src.train_self_training.format_time", "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.src.train_self_training.format_time", "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.src.train_self_training.format_time", "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.src.train_self_training.format_time", "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.data.data_analysis.score", "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.src.train_self_training.format_time", "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.src.train_self_training.format_time"], ["", "def", "main", "(", "argv", "=", "None", ")", ":", "\n", "# Load the dataset.", "\n", "    ", "sentence_train", "=", "json", ".", "load", "(", "open", "(", "'data/'", "+", "DATASET", "+", "'/train_sentence.json'", ",", "'r'", ")", ")", "\n", "sentence_train_label", "=", "json", ".", "load", "(", "open", "(", "'data/'", "+", "DATASET", "+", "'/train_label_id.json'", ",", "'r'", ")", ")", "\n", "train_dataset", "=", "pre_processing", "(", "sentence_train", ",", "sentence_train_label", ")", "\n", "\n", "# split training data to labeled set and unlabeled set", "\n", "# labeled_dataset, unlabeled_dataset_total = random_split(train_dataset, [int(LABEL_OF_TRAIN * len(train_dataset)),", "\n", "#                                                                         len(train_dataset) -", "\n", "#                                                                         int(LABEL_OF_TRAIN * len(train_dataset))])", "\n", "labeled_dataset", ",", "unlabeled_dataset_total", "=", "stratified_sample", "(", "train_dataset", ",", "LABEL_OF_TRAIN", ")", "\n", "\n", "unlabeled_dataset", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "MATE_EPOCHS", ")", ":", "\n", "# unlabeled_dataset_now, unlabeled_dataset_total = random_split(unlabeled_dataset_total,", "\n", "#                                                               [int(UNLABEL_OF_TRAIN / MATE_EPOCHS * len(", "\n", "#                                                                   train_dataset)),", "\n", "#                                                                len(unlabeled_dataset_total) - int(", "\n", "#                                                                    UNLABEL_OF_TRAIN / MATE_EPOCHS * len(", "\n", "#                                                                        train_dataset))])", "\n", "        ", "unlabeled_dataset_now", ",", "unlabeled_dataset_total", "=", "stratified_sample", "(", "unlabeled_dataset_total", ",", "\n", "UNLABEL_OF_TRAIN", "/", "MATE_EPOCHS", ")", "\n", "unlabeled_dataset", ".", "append", "(", "unlabeled_dataset_now", ")", "\n", "\n", "# Create the DataLoaders for our label and unlabel sets.", "\n", "", "labeled_dataloader", "=", "DataLoader", "(", "\n", "labeled_dataset", ",", "# The training samples.", "\n", "sampler", "=", "RandomSampler", "(", "labeled_dataset", ")", ",", "# Select batches randomly", "\n", "batch_size", "=", "BATCH_SIZE", "# Trains with this batch size.", "\n", ")", "\n", "unlabeled_dataloader", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "MATE_EPOCHS", ")", ":", "\n", "        ", "unlabeled_dataloader_now", "=", "DataLoader", "(", "\n", "unlabeled_dataset", "[", "i", "]", ",", "# The training samples.", "\n", "sampler", "=", "RandomSampler", "(", "unlabeled_dataset", "[", "i", "]", ")", ",", "# Select batches randomly", "\n", "batch_size", "=", "BATCH_SIZE", "# Trains with this batch size.", "\n", ")", "\n", "unlabeled_dataloader", ".", "append", "(", "unlabeled_dataloader_now", ")", "\n", "\n", "", "sentence_val", "=", "json", ".", "load", "(", "open", "(", "'data/'", "+", "DATASET", "+", "'/test_sentence.json'", ",", "'r'", ")", ")", "\n", "sentence_val_label", "=", "json", ".", "load", "(", "open", "(", "'data/'", "+", "DATASET", "+", "'/test_label_id.json'", ",", "'r'", ")", ")", "\n", "val_dataset", "=", "pre_processing", "(", "sentence_val", ",", "sentence_val_label", ")", "\n", "\n", "validation_dataloader", "=", "DataLoader", "(", "\n", "val_dataset", ",", "# The validation samples.", "\n", "sampler", "=", "SequentialSampler", "(", "val_dataset", ")", ",", "# Pull out batches sequentially.", "\n", "batch_size", "=", "BATCH_SIZE", "# Evaluate with this batch size.", "\n", ")", "\n", "\n", "# Load models", "\n", "modelf1", "=", "RelationClassification", ".", "from_pretrained", "(", "\n", "\"bert-base-uncased\"", ",", "# Use the 12-layer BERT model, with an uncased vocab.", "\n", "num_labels", "=", "NUM_LABELS", ",", "# The number of output labels--2 for binary classification.", "\n", "# You can increase this for multi-class tasks.", "\n", "output_attentions", "=", "False", ",", "# Whether the model returns attentions weights.", "\n", "output_hidden_states", "=", "False", ",", "# Whether the model returns all hidden-states.", "\n", ")", "\n", "\n", "modelf1", "=", "nn", ".", "DataParallel", "(", "modelf1", ")", "\n", "# modelf1.cuda()", "\n", "modelf1", ".", "to", "(", "device", ")", "\n", "\n", "# modelg2 = LabelGeneration.from_pretrained(", "\n", "#     \"bert-base-uncased\",  # Use the 12-layer BERT model, with an uncased vocab.", "\n", "#     num_labels=NUM_LABELS,  # The number of output labels--2 for binary classification.", "\n", "#     # You can increase this for multi-class tasks.", "\n", "#     output_attentions=False,  # Whether the model returns attentions weights.", "\n", "#     output_hidden_states=False,  # Whether the model returns all hidden-states.", "\n", "# )", "\n", "\n", "# modelg2 = nn.DataParallel(modelg2)", "\n", "# modelg2.cuda()", "\n", "# modelg2.to(device)", "\n", "\n", "optimizer1", "=", "AdamW", "(", "modelf1", ".", "parameters", "(", ")", ",", "\n", "lr", "=", "LR", ",", "# args.learning_rate - default is 5e-5, our notebook had 2e-5", "\n", "eps", "=", "EPS", "# args.adam_epsilon  - default is 1e-8.", "\n", ")", "\n", "# optimizer2 = AdamW(modelg2.parameters(),", "\n", "#                    lr=LR,  # args.learning_rate - default is 5e-5, our notebook had 2e-5", "\n", "#                    eps=EPS  # args.adam_epsilon  - default is 1e-8.", "\n", "#                    )", "\n", "\n", "total_steps1", "=", "0", "\n", "for", "i", "in", "range", "(", "MATE_EPOCHS", "+", "1", ")", ":", "\n", "        ", "total_steps1", "+=", "len", "(", "labeled_dataloader", ")", "\n", "for", "j", "in", "range", "(", "i", ")", ":", "\n", "            ", "total_steps1", "+=", "len", "(", "unlabeled_dataloader", "[", "j", "]", ")", "\n", "", "", "total_steps1", "=", "total_steps1", "*", "EPOCHS", "*", "TOTAL_EPOCHS", "\n", "# total_steps2 = 0", "\n", "# for i in range(MATE_EPOCHS):", "\n", "#     total_steps2 += len(unlabeled_dataloader[i])", "\n", "total_steps2", "=", "len", "(", "labeled_dataloader", ")", "*", "EPOCHS", "*", "MATE_EPOCHS", "*", "TOTAL_EPOCHS", "\n", "\n", "# Create the learning rate scheduler.", "\n", "scheduler1", "=", "get_linear_schedule_with_warmup", "(", "optimizer1", ",", "\n", "num_warmup_steps", "=", "0", ",", "\n", "num_training_steps", "=", "total_steps1", ")", "\n", "# scheduler2 = get_linear_schedule_with_warmup(optimizer2,", "\n", "#                                              num_warmup_steps=0,", "\n", "#                                              num_training_steps=total_steps2)", "\n", "\n", "# Set the seed value all over the place to make this reproducible.", "\n", "random", ".", "seed", "(", "seed_val", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed_val", ")", "\n", "torch", ".", "manual_seed", "(", "seed_val", ")", "\n", "torch", ".", "cuda", ".", "manual_seed_all", "(", "seed_val", ")", "\n", "\n", "# validation accuracy, and timings.", "\n", "training_stats", "=", "[", "]", "\n", "# Measure the total training time for the whole run.", "\n", "total_t0", "=", "time", ".", "time", "(", ")", "\n", "\n", "# ========================================", "\n", "#               train", "\n", "# ========================================", "\n", "\n", "print", "(", "\"training start...\"", ")", "\n", "cnt", "=", "0", "\n", "\n", "# count how many rounds the whole big model has to train", "\n", "# a round means all unlabel data are labeled", "\n", "for", "total_epoch", "in", "range", "(", "TOTAL_EPOCHS", ")", ":", "\n", "\n", "# Add an inner loop to judge the entire unlabeled data set finished", "\n", "        ", "train_dataloader", "=", "labeled_dataloader", "\n", "\n", "for", "mate_epoch", "in", "range", "(", "MATE_EPOCHS", ")", ":", "\n", "\n", "# -------train f1---------", "\n", "\n", "            ", "for", "epoch_i", "in", "range", "(", "0", ",", "EPOCHS", ")", ":", "\n", "                ", "print", "(", "'--- Epoch {:} / {:} ---'", ".", "format", "(", "epoch_i", "+", "1", ",", "EPOCHS", ")", ")", "\n", "# Measure how long the training epoch takes.", "\n", "t0", "=", "time", ".", "time", "(", ")", "\n", "\n", "# Reset the total loss for this epoch.", "\n", "total_train_loss", "=", "0", "\n", "# Put the model into training mode", "\n", "modelf1", ".", "train", "(", ")", "\n", "\n", "for", "step", ",", "batch", "in", "enumerate", "(", "train_dataloader", ")", ":", "\n", "# Progress update every 40 batches.", "\n", "                    ", "if", "step", "%", "40", "==", "0", "and", "not", "step", "==", "0", ":", "\n", "# Calculate elapsed time in minutes.", "\n", "                        ", "elapsed", "=", "format_time", "(", "time", ".", "time", "(", ")", "-", "t0", ")", "\n", "# Report progress.", "\n", "# print(", "\n", "#     '  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))", "\n", "\n", "# Unpack this training batch from our dataloader.", "\n", "", "b_input_ids", "=", "batch", "[", "0", "]", ".", "to", "(", "device", ")", "\n", "b_input_mask", "=", "batch", "[", "1", "]", ".", "to", "(", "device", ")", "\n", "b_labels", "=", "batch", "[", "2", "]", ".", "to", "(", "device", ")", "\n", "b_e1_pos", "=", "batch", "[", "3", "]", ".", "to", "(", "device", ")", "\n", "b_e2_pos", "=", "batch", "[", "4", "]", ".", "to", "(", "device", ")", "\n", "b_w", "=", "batch", "[", "5", "]", ".", "to", "(", "device", ")", "\n", "\n", "modelf1", ".", "zero_grad", "(", ")", "\n", "\n", "# Perform a forward pass (evaluate the model on this training batch)", "\n", "loss", ",", "logits", ",", "_", "=", "modelf1", "(", "b_input_ids", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "attention_mask", "=", "b_input_mask", ",", "\n", "labels", "=", "b_labels", ",", "\n", "e1_pos", "=", "b_e1_pos", ",", "\n", "e2_pos", "=", "b_e2_pos", ",", "\n", "w", "=", "b_w", ")", "\n", "\n", "# Accumulate the training loss over all of the batches", "\n", "total_train_loss", "+=", "loss", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "# Perform a backward pass to calculate the gradients.", "\n", "loss", ".", "sum", "(", ")", ".", "backward", "(", ")", "\n", "# Clip the norm of the gradients to 1.0 to prevent exploding gradients problem.", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "modelf1", ".", "parameters", "(", ")", ",", "1.0", ")", "\n", "# Update parameters and take a step using the computed gradient", "\n", "optimizer1", ".", "step", "(", ")", "\n", "# Update the learning rate.", "\n", "scheduler1", ".", "step", "(", ")", "\n", "\n", "# Calculate the average loss over all of the batches.", "\n", "", "avg_train_loss", "=", "total_train_loss", "/", "len", "(", "train_dataloader", ")", "\n", "# Measure how long this epoch took.", "\n", "training_time", "=", "format_time", "(", "time", ".", "time", "(", ")", "-", "t0", ")", "\n", "\n", "print", "(", "\" f1 Average training loss: {0:.4f}\"", ".", "format", "(", "avg_train_loss", ")", ")", "\n", "# print(\" f1 Training epcoh took: {:}\".format(training_time))", "\n", "\n", "# --------f1 validation-----------", "\n", "\n", "print", "(", "\"validation start...\"", ")", "\n", "t0", "=", "time", ".", "time", "(", ")", "\n", "\n", "# Put the model in evaluation mode--the dropout layers behave differently during evaluation.", "\n", "modelf1", ".", "eval", "(", ")", "\n", "\n", "# Tracking variables", "\n", "total_eval_accuracy", "=", "0", "\n", "total_eval_loss", "=", "0", "\n", "nb_eval_steps", "=", "0", "\n", "all_prediction", "=", "np", ".", "array", "(", "[", "]", ")", "\n", "all_ground_truth", "=", "np", ".", "array", "(", "[", "]", ")", "\n", "\n", "# Evaluate data for one epoch", "\n", "for", "batch", "in", "validation_dataloader", ":", "\n", "# Unpack this training batch from our dataloader.", "\n", "                    ", "b_input_ids", "=", "batch", "[", "0", "]", ".", "to", "(", "device", ")", "\n", "b_input_mask", "=", "batch", "[", "1", "]", ".", "to", "(", "device", ")", "\n", "b_labels", "=", "batch", "[", "2", "]", ".", "to", "(", "device", ")", "\n", "b_e1_pos", "=", "batch", "[", "3", "]", ".", "to", "(", "device", ")", "\n", "b_e2_pos", "=", "batch", "[", "4", "]", ".", "to", "(", "device", ")", "\n", "b_w", "=", "batch", "[", "5", "]", ".", "to", "(", "device", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# Forward pass, calculate logit predictions.", "\n", "                        ", "(", "loss", ",", "logits", ",", "_", ")", "=", "modelf1", "(", "b_input_ids", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "attention_mask", "=", "b_input_mask", ",", "\n", "labels", "=", "b_labels", ",", "\n", "e1_pos", "=", "b_e1_pos", ",", "\n", "e2_pos", "=", "b_e2_pos", ",", "\n", "w", "=", "b_w", ")", "\n", "\n", "# Accumulate the validation loss.", "\n", "# print(loss.shape)", "\n", "# print(loss)", "\n", "", "total_eval_loss", "+=", "loss", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "# Move logits and labels to CPU", "\n", "logits", "=", "logits", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "label_ids", "=", "b_labels", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "\n", "pred_flat", "=", "np", ".", "argmax", "(", "logits", ",", "axis", "=", "1", ")", ".", "flatten", "(", ")", "\n", "labels_flat", "=", "label_ids", ".", "flatten", "(", ")", "\n", "all_prediction", "=", "np", ".", "concatenate", "(", "(", "all_prediction", ",", "pred_flat", ")", ",", "axis", "=", "None", ")", "\n", "all_ground_truth", "=", "np", ".", "concatenate", "(", "(", "all_ground_truth", ",", "labels_flat", ")", ",", "axis", "=", "None", ")", "\n", "\n", "# Report the final accuracy for this validation run.", "\n", "# avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)", "\n", "# print(\"  Accuracy: {0:.4f}\".format(avg_val_accuracy))", "\n", "\n", "# Calculate the average loss over all of the batches.", "\n", "", "avg_val_loss", "=", "total_eval_loss", "/", "len", "(", "validation_dataloader", ")", "\n", "\n", "# Measure how long the validation run took.", "\n", "validation_time", "=", "format_time", "(", "time", ".", "time", "(", ")", "-", "t0", ")", "\n", "\n", "score", "(", "all_ground_truth", ",", "all_prediction", ")", "\n", "\n", "# -------f1 predict and g2 update---------", "\n", "\n", "# print(\"f1 predict and g2 update start...\")", "\n", "#", "\n", "# for epoch_i in range(0, EPOCHS):", "\n", "#     print('--- Epoch {:} / {:} ---'.format(epoch_i + 1, EPOCHS))", "\n", "#     # Measure how long the training epoch takes.", "\n", "#     t0 = time.time()", "\n", "#", "\n", "#     # Reset the total loss for this epoch.", "\n", "#     total_train_loss = 0", "\n", "#", "\n", "#     modelg2.train()", "\n", "#     modelf1.eval()", "\n", "#", "\n", "#     for step, batch in enumerate(labeled_dataloader):", "\n", "#         # Progress update every 40 batches.", "\n", "#         if step % 40 == 0 and not step == 0:", "\n", "#             # Calculate elapsed time in minutes.", "\n", "#             elapsed = format_time(time.time() - t0)", "\n", "#             # Report progress.", "\n", "#", "\n", "#         # Unpack this training batch from our dataloader.", "\n", "#         b_input_ids = batch[0].to(device)", "\n", "#         b_input_mask = batch[1].to(device)", "\n", "#         b_labels = batch[2].to(device)", "\n", "#         b_e1_pos = batch[3].to(device)", "\n", "#         b_e2_pos = batch[4].to(device)", "\n", "#         b_w = batch[5].to(device)", "\n", "#         # b1_input_ids = batch[0].to(device)", "\n", "#         # b1_input_mask = batch[1].to(device)", "\n", "#         # b1_labels = batch[2].to(device)", "\n", "#         # b1_e1_pos = batch[3].to(device)", "\n", "#         # b1_e2_pos = batch[4].to(device)", "\n", "#         # b1_w = batch[5].to(device)", "\n", "#", "\n", "#         # f1 predict", "\n", "#         modelf1.zero_grad()", "\n", "#         #with torch.no_grad():", "\n", "#             # Forward pass, calculate logit predictions.", "\n", "#         loss1, logits1, _ = modelf1(b_input_ids,", "\n", "#                                    token_type_ids=None,", "\n", "#                                    attention_mask=b_input_mask,", "\n", "#                                    labels=b_labels,", "\n", "#                                    e1_pos=b_e1_pos,", "\n", "#                                    e2_pos=b_e2_pos,", "\n", "#                                    w=b_w)", "\n", "#", "\n", "#         # g2 train", "\n", "#         modelg2.zero_grad()", "\n", "#         # Perform a forward pass", "\n", "#         loss2, logits2, _ = modelg2(b_input_ids,", "\n", "#                                  token_type_ids=None,", "\n", "#                                  attention_mask=b_input_mask,", "\n", "#                                  labels=b_labels,", "\n", "#                                  e1_pos=b_e1_pos,", "\n", "#                                  e2_pos=b_e2_pos,", "\n", "#                                  w=b_w)", "\n", "#", "\n", "#         # Perform a backward pass to calculate the gradients.", "\n", "#         #loss2.sum().backward()", "\n", "#         (loss1.sum() + 0.4 * loss2.sum()).backward()", "\n", "#         # Clip the norm of the gradients to 1.0 to prevent exploding gradients problem.", "\n", "#         torch.nn.utils.clip_grad_norm_(modelg2.parameters(), 1.0)", "\n", "#         # Update parameters and take a step using the computed gradient", "\n", "#         optimizer2.step()", "\n", "#         # Update the learning rate.", "\n", "#         scheduler2.step()", "\n", "#", "\n", "#         # Accumulate the training loss over all of the batches", "\n", "#         total_train_loss += loss2.sum().item()", "\n", "#", "\n", "#     # Calculate the average loss over all of the batches.", "\n", "#     avg_train_loss = total_train_loss / len(labeled_dataloader)", "\n", "#     # Measure how long this epoch took.", "\n", "#     training_time = format_time(time.time() - t0)", "\n", "#", "\n", "#     print(\" g2 Average training loss by g2: {0:.4f}\".format(avg_train_loss))", "\n", "#     # print(\" g2 Training epcoh took: {:}\".format(training_time))", "\n", "\n", "# -------g2 generate pseudo label---------", "\n", "\n", "", "print", "(", "\"f1 generate pseudo label\"", ")", "\n", "\n", "modelf1", ".", "eval", "(", ")", "\n", "input_ids", "=", "[", "]", "\n", "input_mask", "=", "[", "]", "\n", "# labels = []", "\n", "gold_labels", "=", "[", "]", "\n", "e1_pos", "=", "[", "]", "\n", "e2_pos", "=", "[", "]", "\n", "w", "=", "[", "]", "\n", "all_logits", "=", "[", "]", "\n", "all_prediction", "=", "np", ".", "array", "(", "[", "]", ")", "\n", "all_ground_truth", "=", "np", ".", "array", "(", "[", "]", ")", "\n", "all_weights", "=", "np", ".", "array", "(", "[", "]", ")", "\n", "all_pseudo_prediction", "=", "np", ".", "array", "(", "[", "]", ")", "\n", "all_pseudo_ground_truth", "=", "np", ".", "array", "(", "[", "]", ")", "\n", "all_pseudo_weights", "=", "np", ".", "array", "(", "[", "]", ")", "\n", "# Evaluate data for one epoch", "\n", "for", "batch", "in", "unlabeled_dataloader", "[", "mate_epoch", "]", ":", "# unlabeled_dataloader", "\n", "# Unpack this training batch from our dataloader.", "\n", "                ", "b_input_ids", "=", "batch", "[", "0", "]", ".", "to", "(", "device", ")", "\n", "b_input_mask", "=", "batch", "[", "1", "]", ".", "to", "(", "device", ")", "\n", "b_labels", "=", "batch", "[", "2", "]", ".", "to", "(", "device", ")", "\n", "b_e1_pos", "=", "batch", "[", "3", "]", ".", "to", "(", "device", ")", "\n", "b_e2_pos", "=", "batch", "[", "4", "]", ".", "to", "(", "device", ")", "\n", "b_w", "=", "batch", "[", "5", "]", ".", "to", "(", "device", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# Forward pass, calculate logit predictions.", "\n", "                    ", "(", "loss", ",", "logits", ",", "_", ")", "=", "modelf1", "(", "b_input_ids", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "attention_mask", "=", "b_input_mask", ",", "\n", "labels", "=", "b_labels", ",", "\n", "e1_pos", "=", "b_e1_pos", ",", "\n", "e2_pos", "=", "b_e2_pos", ",", "\n", "w", "=", "b_w", ")", "\n", "\n", "", "logits", "=", "logits", ".", "detach", "(", ")", "\n", "all_logits", ".", "append", "(", "logits", ")", "\n", "input_ids", ".", "append", "(", "b_input_ids", ")", "\n", "input_mask", ".", "append", "(", "b_input_mask", ")", "\n", "gold_labels", ".", "append", "(", "b_labels", ")", "\n", "e1_pos", ".", "append", "(", "b_e1_pos", ")", "\n", "e2_pos", ".", "append", "(", "b_e2_pos", ")", "\n", "w", ".", "append", "(", "b_w", ")", "\n", "\n", "", "logits", "=", "torch", ".", "cat", "(", "all_logits", ",", "dim", "=", "0", ")", "\n", "probs", "=", "F", ".", "softmax", "(", "logits", ",", "dim", "=", "1", ")", "\n", "input_ids", "=", "torch", ".", "cat", "(", "input_ids", ",", "dim", "=", "0", ")", "\n", "input_mask", "=", "torch", ".", "cat", "(", "input_mask", ",", "dim", "=", "0", ")", "\n", "gold_labels", "=", "torch", ".", "cat", "(", "gold_labels", ",", "dim", "=", "0", ")", "\n", "e1_pos", "=", "torch", ".", "cat", "(", "e1_pos", ",", "dim", "=", "0", ")", "\n", "e2_pos", "=", "torch", ".", "cat", "(", "e2_pos", ",", "dim", "=", "0", ")", "\n", "# w = torch.cat(w, dim=0)", "\n", "label_weights", ",", "labels", "=", "torch", ".", "max", "(", "probs", ",", "dim", "=", "1", ")", "\n", "# labels = torch.argmax(logits, dim=1)", "\n", "# label_weights = torch.max(probs, dim=1)[0]", "\n", "\n", "# evaluate generate label quality", "\n", "logits", "=", "logits", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "label_ids", "=", "gold_labels", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "pred_weights", "=", "label_weights", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "pred_flat", "=", "np", ".", "argmax", "(", "logits", ",", "axis", "=", "1", ")", ".", "flatten", "(", ")", "\n", "labels_flat", "=", "label_ids", ".", "flatten", "(", ")", "\n", "pred_weights", "=", "pred_weights", ".", "flatten", "(", ")", "\n", "all_prediction", "=", "np", ".", "concatenate", "(", "(", "all_prediction", ",", "pred_flat", ")", ",", "axis", "=", "None", ")", "\n", "all_ground_truth", "=", "np", ".", "concatenate", "(", "(", "all_ground_truth", ",", "labels_flat", ")", ",", "axis", "=", "None", ")", "\n", "all_weights", "=", "np", ".", "concatenate", "(", "(", "all_weights", ",", "pred_weights", ")", ",", "axis", "=", "None", ")", "\n", "\n", "\n", "# top Z", "\n", "sort", "=", "torch", ".", "argsort", "(", "label_weights", ",", "descending", "=", "True", ")", "\n", "sort", "=", "sort", "[", "0", ":", "int", "(", "len", "(", "sort", ")", "*", "Z_RATIO", ")", "]", "\n", "input_ids", "=", "input_ids", "[", "sort", "]", "\n", "input_mask", "=", "input_mask", "[", "sort", "]", "\n", "pseudo_labels", "=", "labels", "[", "sort", "]", "\n", "e1_pos", "=", "e1_pos", "[", "sort", "]", "\n", "e2_pos", "=", "e2_pos", "[", "sort", "]", "\n", "w", "=", "label_weights", "[", "sort", "]", "\n", "pseudo_gold_labels", "=", "gold_labels", "[", "sort", "]", "\n", "\n", "\n", "# evaluate pseudo label quality", "\n", "pred", "=", "pseudo_labels", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "label_ids", "=", "pseudo_gold_labels", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "weights", "=", "w", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "pred_flat", "=", "pred", ".", "flatten", "(", ")", "\n", "labels_flat", "=", "label_ids", ".", "flatten", "(", ")", "\n", "weights_flat", "=", "weights", ".", "flatten", "(", ")", "\n", "all_pseudo_prediction", "=", "np", ".", "concatenate", "(", "(", "all_pseudo_prediction", ",", "pred_flat", ")", ",", "axis", "=", "None", ")", "\n", "all_pseudo_ground_truth", "=", "np", ".", "concatenate", "(", "(", "all_pseudo_ground_truth", ",", "labels_flat", ")", ",", "axis", "=", "None", ")", "\n", "all_pseudo_weights", "=", "np", ".", "concatenate", "(", "(", "all_pseudo_weights", ",", "weights_flat", ")", ",", "axis", "=", "None", ")", "\n", "\n", "#  evaluate generate label quality", "\n", "np_sort", "=", "np", ".", "argsort", "(", "all_weights", ")", "\n", "print", "(", "\"  Generate F1 score\"", ")", "\n", "score", "(", "all_ground_truth", ",", "all_prediction", ")", "\n", "np", ".", "save", "(", "LOG_DIR", "+", "'/'", "+", "LOG_DIR", "+", "'_all_prediction{}'", ".", "format", "(", "cnt", ")", ",", "all_prediction", "[", "np_sort", "]", ")", "\n", "np", ".", "save", "(", "LOG_DIR", "+", "'/'", "+", "LOG_DIR", "+", "'_all_label{}'", ".", "format", "(", "cnt", ")", ",", "all_ground_truth", "[", "np_sort", "]", ")", "\n", "np", ".", "save", "(", "LOG_DIR", "+", "'/'", "+", "LOG_DIR", "+", "'_all_weights{}'", ".", "format", "(", "cnt", ")", ",", "all_weights", "[", "np_sort", "]", ")", "\n", "#  evaluate pseudo label quality", "\n", "print", "(", "\"  Pseudo F1 score\"", ")", "\n", "score", "(", "all_pseudo_ground_truth", ",", "all_pseudo_prediction", ")", "\n", "np", ".", "save", "(", "LOG_DIR", "+", "'/'", "+", "LOG_DIR", "+", "'_all_pseudo_prediction{}'", ".", "format", "(", "cnt", ")", ",", "all_pseudo_prediction", ")", "\n", "np", ".", "save", "(", "LOG_DIR", "+", "'/'", "+", "LOG_DIR", "+", "'_all_pseudo_label{}'", ".", "format", "(", "cnt", ")", ",", "all_pseudo_ground_truth", ")", "\n", "np", ".", "save", "(", "LOG_DIR", "+", "'/'", "+", "LOG_DIR", "+", "'_all_pseudo_weights{}'", ".", "format", "(", "cnt", ")", ",", "all_pseudo_weights", ")", "\n", "cnt", "+=", "1", "\n", "\n", "\n", "# update training data", "\n", "# input_ids = torch.stack(input_ids).to(device)", "\n", "# input_mask = torch.stack(input_mask).to(device)", "\n", "# labels = torch.tensor(labels, device='cuda')", "\n", "# e1_pos = torch.tensor(e1_pos, device='cuda')", "\n", "# e2_pos = torch.tensor(e2_pos, device='cuda')", "\n", "# w = torch.tensor(w, device='cuda')", "\n", "train_add_dataset", "=", "train_dataloader", ".", "dataset", "+", "TensorDataset", "(", "input_ids", ",", "input_mask", ",", "pseudo_labels", ",", "e1_pos", ",", "e2_pos", ",", "w", ")", "\n", "train_dataloader", "=", "DataLoader", "(", "\n", "train_add_dataset", ",", "# The training samples.", "\n", "sampler", "=", "RandomSampler", "(", "train_add_dataset", ")", ",", "# Select batches randomly", "\n", "batch_size", "=", "BATCH_SIZE", "# Trains with this batch size.", "\n", ")", "\n", "\n", "# train f1 with all data", "\n", "", "for", "epoch_i", "in", "range", "(", "0", ",", "EPOCHS", ")", ":", "\n", "            ", "print", "(", "'--- Epoch {:} / {:} ---'", ".", "format", "(", "epoch_i", "+", "1", ",", "EPOCHS", ")", ")", "\n", "# Measure how long the training epoch takes.", "\n", "t0", "=", "time", ".", "time", "(", ")", "\n", "\n", "# Reset the total loss for this epoch.", "\n", "total_train_loss", "=", "0", "\n", "# Put the model into training mode", "\n", "modelf1", ".", "train", "(", ")", "\n", "\n", "for", "step", ",", "batch", "in", "enumerate", "(", "train_dataloader", ")", ":", "\n", "# Progress update every 40 batches.", "\n", "                ", "if", "step", "%", "40", "==", "0", "and", "not", "step", "==", "0", ":", "\n", "# Calculate elapsed time in minutes.", "\n", "                    ", "elapsed", "=", "format_time", "(", "time", ".", "time", "(", ")", "-", "t0", ")", "\n", "# Report progress.", "\n", "# print(", "\n", "#     '  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader),", "\n", "#                                                           elapsed))", "\n", "\n", "# Unpack this training batch from our dataloader.", "\n", "", "b_input_ids", "=", "batch", "[", "0", "]", ".", "to", "(", "device", ")", "\n", "b_input_mask", "=", "batch", "[", "1", "]", ".", "to", "(", "device", ")", "\n", "b_labels", "=", "batch", "[", "2", "]", ".", "to", "(", "device", ")", "\n", "b_e1_pos", "=", "batch", "[", "3", "]", ".", "to", "(", "device", ")", "\n", "b_e2_pos", "=", "batch", "[", "4", "]", ".", "to", "(", "device", ")", "\n", "b_w", "=", "batch", "[", "5", "]", ".", "to", "(", "device", ")", "\n", "\n", "modelf1", ".", "zero_grad", "(", ")", "\n", "\n", "# Perform a forward pass (evaluate the model on this training batch)", "\n", "loss", ",", "logits", ",", "_", "=", "modelf1", "(", "b_input_ids", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "attention_mask", "=", "b_input_mask", ",", "\n", "labels", "=", "b_labels", ",", "\n", "e1_pos", "=", "b_e1_pos", ",", "\n", "e2_pos", "=", "b_e2_pos", ",", "\n", "w", "=", "b_w", ")", "\n", "\n", "# Accumulate the training loss over all of the batches", "\n", "total_train_loss", "+=", "loss", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "# Perform a backward pass to calculate the gradients.", "\n", "loss", ".", "sum", "(", ")", ".", "backward", "(", ")", "\n", "# Clip the norm of the gradients to 1.0 to prevent exploding gradients problem.", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "modelf1", ".", "parameters", "(", ")", ",", "1.0", ")", "\n", "# Update parameters and take a step using the computed gradient", "\n", "optimizer1", ".", "step", "(", ")", "\n", "# Update the learning rate.", "\n", "scheduler1", ".", "step", "(", ")", "\n", "\n", "# Calculate the average loss over all of the batches.", "\n", "", "avg_train_loss", "=", "total_train_loss", "/", "len", "(", "train_dataloader", ")", "\n", "# Measure how long this epoch took.", "\n", "training_time", "=", "format_time", "(", "time", ".", "time", "(", ")", "-", "t0", ")", "\n", "\n", "print", "(", "\" f1 Average training loss: {0:.4f}\"", ".", "format", "(", "avg_train_loss", ")", ")", "\n", "# print(\" f1 Training epcoh took: {:}\".format(training_time))", "\n", "\n", "# final f1 validation", "\n", "", "print", "(", "\"final validation start...\"", ")", "\n", "t0", "=", "time", ".", "time", "(", ")", "\n", "\n", "# Put the model in evaluation mode--the dropout layers behave differently during evaluation.", "\n", "modelf1", ".", "eval", "(", ")", "\n", "\n", "# Tracking variables", "\n", "total_eval_accuracy", "=", "0", "\n", "total_eval_loss", "=", "0", "\n", "nb_eval_steps", "=", "0", "\n", "\n", "all_prediction", "=", "np", ".", "array", "(", "[", "]", ")", "\n", "all_ground_truth", "=", "np", ".", "array", "(", "[", "]", ")", "\n", "\n", "# Evaluate data for one epoch", "\n", "for", "batch", "in", "validation_dataloader", ":", "\n", "# Unpack this training batch from our dataloader.", "\n", "            ", "b_input_ids", "=", "batch", "[", "0", "]", ".", "to", "(", "device", ")", "\n", "b_input_mask", "=", "batch", "[", "1", "]", ".", "to", "(", "device", ")", "\n", "b_labels", "=", "batch", "[", "2", "]", ".", "to", "(", "device", ")", "\n", "b_e1_pos", "=", "batch", "[", "3", "]", ".", "to", "(", "device", ")", "\n", "b_e2_pos", "=", "batch", "[", "4", "]", ".", "to", "(", "device", ")", "\n", "b_w", "=", "batch", "[", "5", "]", ".", "to", "(", "device", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# Forward pass, calculate logit predictions.", "\n", "                ", "(", "loss", ",", "logits", ",", "_", ")", "=", "modelf1", "(", "b_input_ids", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "attention_mask", "=", "b_input_mask", ",", "\n", "labels", "=", "b_labels", ",", "\n", "e1_pos", "=", "b_e1_pos", ",", "\n", "e2_pos", "=", "b_e2_pos", ",", "\n", "w", "=", "b_w", ")", "\n", "\n", "# Accumulate the validation loss.", "\n", "", "total_eval_loss", "+=", "loss", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "# Move logits and labels to CPU", "\n", "logits", "=", "logits", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "label_ids", "=", "b_labels", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "pred_flat", "=", "np", ".", "argmax", "(", "logits", ",", "axis", "=", "1", ")", ".", "flatten", "(", ")", "\n", "labels_flat", "=", "label_ids", ".", "flatten", "(", ")", "\n", "all_prediction", "=", "np", ".", "concatenate", "(", "(", "all_prediction", ",", "pred_flat", ")", ",", "axis", "=", "None", ")", "\n", "all_ground_truth", "=", "np", ".", "concatenate", "(", "(", "all_ground_truth", ",", "labels_flat", ")", ",", "axis", "=", "None", ")", "\n", "\n", "# Calculate the accuracy for this batch of test sentences, and accumulate it over all batches.", "\n", "# acc = flat_accuracy(logits, label_ids)", "\n", "# total_eval_accuracy += acc", "\n", "\n", "# Report the final accuracy for this validation run.", "\n", "# avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)", "\n", "# print(\"  Accuracy: {0:.4f}\".format(avg_val_accuracy))", "\n", "\n", "# Calculate the average loss over all of the batches.", "\n", "", "avg_val_loss", "=", "total_eval_loss", "/", "len", "(", "validation_dataloader", ")", "\n", "\n", "validation_time", "=", "format_time", "(", "time", ".", "time", "(", ")", "-", "t0", ")", "\n", "non_zero_idx", "=", "(", "all_ground_truth", "!=", "0", ")", "\n", "validation_acc", "=", "np", ".", "sum", "(", "all_prediction", "[", "non_zero_idx", "]", "==", "all_ground_truth", "[", "non_zero_idx", "]", ")", "/", "len", "(", "all_ground_truth", "[", "non_zero_idx", "]", ")", "\n", "validation_f1_score", "=", "f1_score", "(", "all_ground_truth", "[", "non_zero_idx", "]", ",", "all_prediction", "[", "non_zero_idx", "]", ",", "average", "=", "\"micro\"", ")", "\n", "\n", "#print(\"  Validation Loss: {0:.4f}\".format(avg_val_loss))", "\n", "# print(\"  Validation took: {:}\".format(validation_time))", "\n", "#print(\"  Validation Accuracy: {0:.4f}\".format(validation_acc))", "\n", "#print(\"  Validation F1 score: {0:.4f}\".format(validation_f1_score))", "\n", "\n", "# np.save('prediction{}'.format(cnt), all_prediction)", "\n", "# np.save('label{}'.format(cnt), all_ground_truth)", "\n", "# np.save('nonzero_prediction{}'.format(cnt), all_prediction[non_zero_idx])", "\n", "# np.save('nonzero_label{}'.format(cnt), all_ground_truth[non_zero_idx])", "\n", "score", "(", "all_ground_truth", ",", "all_prediction", ")", "\n", "\n", "# ----------------------training complete-----------------------", "\n", "\n", "", "print", "(", "\"Training complete!\"", ")", "\n", "print", "(", "\"Total training took {:} (h:mm:ss)\"", ".", "format", "(", "format_time", "(", "time", ".", "time", "(", ")", "-", "total_t0", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.data.data_prepare.Read_SemEval_data": [[3, 21], ["open", "open", "line.split.split", "line[].strip", "interval.join.insert", "interval.join.insert", "interval.join.insert", "interval.join", "sentence.append", "line.split.split", "line[].strip", "label_name.append"], "function", ["None"], ["def", "Read_SemEval_data", "(", "CATEGORY", ")", ":", "\n", "    ", "sentence", "=", "[", "]", "\n", "label_name", "=", "[", "]", "\n", "interval", "=", "' '", "\n", "for", "line", "in", "open", "(", "'data/SemEval/%s/%s.txt'", "%", "(", "CATEGORY", ",", "CATEGORY", ")", ")", ":", "\n", "        ", "token", "=", "[", "]", "\n", "line", "=", "line", ".", "split", "(", "\"\t\"", ")", "\n", "line", "[", "1", "]", "=", "line", "[", "1", "]", ".", "strip", "(", "'\\n'", ")", "\n", "token", ".", "insert", "(", "0", ",", "'[CLS]'", ")", "\n", "token", ".", "insert", "(", "1", ",", "line", "[", "1", "]", ")", "\n", "token", ".", "insert", "(", "2", ",", "'[SEP]'", ")", "\n", "token", "=", "interval", ".", "join", "(", "token", ")", "\n", "sentence", ".", "append", "(", "token", ")", "\n", "", "for", "line", "in", "open", "(", "'data/SemEval/%s/%s_result_full.txt'", "%", "(", "CATEGORY", ",", "CATEGORY", ")", ")", ":", "\n", "        ", "line", "=", "line", ".", "split", "(", "\"\t\"", ")", "\n", "line", "[", "1", "]", "=", "line", "[", "1", "]", ".", "strip", "(", "'\\n'", ")", "\n", "label_name", ".", "append", "(", "line", "[", "1", "]", ")", "\n", "", "return", "sentence", ",", "label_name", "\n", "\n"]], "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.data.data_prepare.realtion2id": [[30, 38], ["range", "open", "json.load", "len", "label_id.append"], "function", ["None"], ["", "def", "realtion2id", "(", "dataset_name", ",", "label_name", ")", ":", "\n", "    ", "label_id", "=", "[", "]", "\n", "with", "open", "(", "'data/%s/relation2id.json'", "%", "dataset_name", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "data", "=", "json", ".", "load", "(", "f", ")", "\n", "", "for", "i", "in", "range", "(", "len", "(", "label_name", ")", ")", ":", "\n", "        ", "id", "=", "data", "[", "label_name", "[", "i", "]", "]", "\n", "label_id", ".", "append", "(", "id", ")", "\n", "", "return", "label_id", "\n", "\n"]], "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.data.data_prepare.Read_TACRED_data": [[49, 73], ["range", "open", "f.readline", "json.loads", "len", "interval.join.insert", "interval.join.insert", "interval.join.insert", "interval.join.insert", "interval.join.insert", "interval.join.insert", "interval.join", "sentence.append", "label_name.append"], "function", ["None"], ["", "def", "Read_TACRED_data", "(", "CATEGORY", ")", ":", "\n", "    ", "sentence", "=", "[", "]", "\n", "label_name", "=", "[", "]", "\n", "interval", "=", "' '", "\n", "with", "open", "(", "'data/tacred/data/json/%s.json'", "%", "CATEGORY", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "line", "=", "f", ".", "readline", "(", ")", "\n", "data", "=", "json", ".", "loads", "(", "line", ")", "\n", "", "for", "i", "in", "range", "(", "len", "(", "data", ")", ")", ":", "\n", "        ", "tokens", "=", "data", "[", "i", "]", "[", "\"token\"", "]", "\n", "subj_start", "=", "data", "[", "i", "]", "[", "\"subj_start\"", "]", "\n", "subj_end", "=", "data", "[", "i", "]", "[", "\"subj_end\"", "]", "\n", "obj_start", "=", "data", "[", "i", "]", "[", "\"obj_start\"", "]", "\n", "obj_end", "=", "data", "[", "i", "]", "[", "\"obj_end\"", "]", "\n", "relation", "=", "data", "[", "i", "]", "[", "\"relation\"", "]", "\n", "tokens", ".", "insert", "(", "subj_start", ",", "'<e1>'", ")", "\n", "tokens", ".", "insert", "(", "subj_end", "+", "2", ",", "'</e1>'", ")", "\n", "tokens", ".", "insert", "(", "obj_start", "+", "2", ",", "'<e2>'", ")", "\n", "tokens", ".", "insert", "(", "obj_end", "+", "4", ",", "'</e2>'", ")", "\n", "tokens", ".", "insert", "(", "0", ",", "'[CLS]'", ")", "\n", "tokens", ".", "insert", "(", "-", "1", ",", "'[SEP]'", ")", "\n", "tokens", "=", "interval", ".", "join", "(", "tokens", ")", "\n", "sentence", ".", "append", "(", "tokens", ")", "\n", "label_name", ".", "append", "(", "relation", ")", "\n", "", "return", "sentence", ",", "label_name", "\n", "\n"]], "home.repos.pwc.inspect_result.THU-BPM_MetaSRE.data.data_analysis.score": [[14, 101], ["collections.Counter", "collections.Counter", "collections.Counter", "range", "print", "print", "print", "print", "len", "print", "collections.Counter.keys", "sorted", "print", "print", "sum", "sum", "sys.stdout.write", "sys.stdout.write", "sys.stdout.write", "sys.stdout.write", "sys.stdout.write", "sys.stdout.write", "sys.stdout.write", "sys.stdout.write", "sys.stdout.write", "collections.Counter.values", "float", "float", "collections.Counter.values", "float", "float", "sys.stdout.write", "sys.stdout.write", "sys.stdout.write", "sys.stdout.write", "sys.stdout.write", "sys.stdout.write", "sum", "sum", "sum", "sum", "float", "float", "float", "float", "collections.Counter.values", "collections.Counter.values", "collections.Counter.values", "collections.Counter.values", "str"], "function", ["None"], ["def", "score", "(", "key", ",", "prediction", ",", "verbose", "=", "True", ",", "NO_RELATION", "=", "NO_RELATION", ")", ":", "\n", "\n", "    ", "correct_by_relation", "=", "Counter", "(", ")", "\n", "guessed_by_relation", "=", "Counter", "(", ")", "\n", "gold_by_relation", "=", "Counter", "(", ")", "\n", "\n", "# Loop over the data to compute a score", "\n", "for", "row", "in", "range", "(", "len", "(", "key", ")", ")", ":", "\n", "        ", "gold", "=", "key", "[", "row", "]", "\n", "guess", "=", "prediction", "[", "row", "]", "\n", "\n", "if", "gold", "==", "NO_RELATION", "and", "guess", "==", "NO_RELATION", ":", "\n", "            ", "pass", "\n", "", "elif", "gold", "==", "NO_RELATION", "and", "guess", "!=", "NO_RELATION", ":", "\n", "            ", "guessed_by_relation", "[", "guess", "]", "+=", "1", "\n", "", "elif", "gold", "!=", "NO_RELATION", "and", "guess", "==", "NO_RELATION", ":", "\n", "            ", "gold_by_relation", "[", "gold", "]", "+=", "1", "\n", "", "elif", "gold", "!=", "NO_RELATION", "and", "guess", "!=", "NO_RELATION", ":", "\n", "            ", "guessed_by_relation", "[", "guess", "]", "+=", "1", "\n", "gold_by_relation", "[", "gold", "]", "+=", "1", "\n", "if", "gold", "==", "guess", ":", "\n", "                ", "correct_by_relation", "[", "guess", "]", "+=", "1", "\n", "\n", "# Print verbose information", "\n", "", "", "", "if", "verbose", ":", "\n", "        ", "print", "(", "\"Per-relation statistics:\"", ")", "\n", "relations", "=", "gold_by_relation", ".", "keys", "(", ")", "\n", "longest_relation", "=", "0", "\n", "\n", "# for relation in sorted(relations):", "\n", "#     longest_relation = max(len(relation), longest_relation)", "\n", "for", "relation", "in", "sorted", "(", "relations", ")", ":", "\n", "# (compute the score)", "\n", "            ", "correct", "=", "correct_by_relation", "[", "relation", "]", "\n", "guessed", "=", "guessed_by_relation", "[", "relation", "]", "\n", "gold", "=", "gold_by_relation", "[", "relation", "]", "\n", "prec", "=", "1.0", "\n", "if", "guessed", ">", "0", ":", "\n", "                ", "prec", "=", "float", "(", "correct", ")", "/", "float", "(", "guessed", ")", "\n", "", "recall", "=", "0.0", "\n", "if", "gold", ">", "0", ":", "\n", "                ", "recall", "=", "float", "(", "correct", ")", "/", "float", "(", "gold", ")", "\n", "", "f1", "=", "0.0", "\n", "if", "prec", "+", "recall", ">", "0", ":", "\n", "                ", "f1", "=", "2.0", "*", "prec", "*", "recall", "/", "(", "prec", "+", "recall", ")", "\n", "# (print the score)", "\n", "", "sys", ".", "stdout", ".", "write", "(", "(", "\"{:<\"", "+", "str", "(", "longest_relation", ")", "+", "\"}\"", ")", ".", "format", "(", "relation", ")", ")", "\n", "sys", ".", "stdout", ".", "write", "(", "\"  P: \"", ")", "\n", "if", "prec", "<", "0.1", ":", "\n", "                ", "sys", ".", "stdout", ".", "write", "(", "' '", ")", "\n", "", "if", "prec", "<", "1.0", ":", "\n", "                ", "sys", ".", "stdout", ".", "write", "(", "' '", ")", "\n", "", "sys", ".", "stdout", ".", "write", "(", "\"{:.2%}\"", ".", "format", "(", "prec", ")", ")", "\n", "sys", ".", "stdout", ".", "write", "(", "\"  R: \"", ")", "\n", "if", "recall", "<", "0.1", ":", "\n", "                ", "sys", ".", "stdout", ".", "write", "(", "' '", ")", "\n", "", "if", "recall", "<", "1.0", ":", "\n", "                ", "sys", ".", "stdout", ".", "write", "(", "' '", ")", "\n", "", "sys", ".", "stdout", ".", "write", "(", "\"{:.2%}\"", ".", "format", "(", "recall", ")", ")", "\n", "sys", ".", "stdout", ".", "write", "(", "\"  F1: \"", ")", "\n", "if", "f1", "<", "0.1", ":", "\n", "                ", "sys", ".", "stdout", ".", "write", "(", "' '", ")", "\n", "", "if", "f1", "<", "1.0", ":", "\n", "                ", "sys", ".", "stdout", ".", "write", "(", "' '", ")", "\n", "", "sys", ".", "stdout", ".", "write", "(", "\"{:.2%}\"", ".", "format", "(", "f1", ")", ")", "\n", "sys", ".", "stdout", ".", "write", "(", "\"  #: %d\"", "%", "gold", ")", "\n", "sys", ".", "stdout", ".", "write", "(", "\"\\n\"", ")", "\n", "", "print", "(", "\"\"", ")", "\n", "\n", "# Print the aggregate score", "\n", "", "if", "verbose", ":", "\n", "        ", "print", "(", "\"Final Score:\"", ")", "\n", "", "prec_micro", "=", "1.0", "\n", "if", "sum", "(", "guessed_by_relation", ".", "values", "(", ")", ")", ">", "0", ":", "\n", "        ", "prec_micro", "=", "float", "(", "sum", "(", "correct_by_relation", ".", "values", "(", ")", ")", ")", "/", "float", "(", "\n", "sum", "(", "guessed_by_relation", ".", "values", "(", ")", ")", ")", "\n", "", "recall_micro", "=", "0.0", "\n", "if", "sum", "(", "gold_by_relation", ".", "values", "(", ")", ")", ">", "0", ":", "\n", "        ", "recall_micro", "=", "float", "(", "sum", "(", "correct_by_relation", ".", "values", "(", ")", ")", ")", "/", "float", "(", "\n", "sum", "(", "gold_by_relation", ".", "values", "(", ")", ")", ")", "\n", "", "f1_micro", "=", "0.0", "\n", "if", "prec_micro", "+", "recall_micro", ">", "0.0", ":", "\n", "        ", "f1_micro", "=", "2.0", "*", "prec_micro", "*", "recall_micro", "/", "(", "prec_micro", "+", "recall_micro", ")", "\n", "", "print", "(", "\"SET NO_RELATION ID: \"", ",", "NO_RELATION", ")", "\n", "print", "(", "\"Precision (micro): {:.3%}\"", ".", "format", "(", "prec_micro", ")", ")", "\n", "print", "(", "\"   Recall (micro): {:.3%}\"", ".", "format", "(", "recall_micro", ")", ")", "\n", "print", "(", "\"       F1 (micro): {:.3%}\"", ".", "format", "(", "f1_micro", ")", ")", "\n", "\n"]]}