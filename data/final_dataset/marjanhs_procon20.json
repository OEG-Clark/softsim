{"home.repos.pwc.inspect_result.marjanhs_procon20.utils.emotion.Emotion.__init__": [[9, 14], ["Emotion.load_nrc_emotion", "len"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.utils.emotion.Emotion.load_nrc_emotion"], ["    ", "def", "__init__", "(", "self", ",", "nrc_path", ",", "max_em_len", ",", "emotion_filters", ")", ":", "\n", "        ", "self", ".", "nrc_path", "=", "nrc_path", "\n", "self", ".", "word_to_em", ",", "self", ".", "em_to_id", "=", "Emotion", ".", "load_nrc_emotion", "(", "self", ".", "nrc_path", ",", "emotion_filters", ")", "\n", "self", ".", "max_len", "=", "len", "(", "self", ".", "word_to_em", ")", "+", "1", "\n", "self", ".", "max_em_len", "=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.utils.emotion.Emotion.load_nrc_emotion": [[15, 52], ["collections.defaultdict", "dict", "open", "em.strip", "line.strip.strip.strip", "emotion_filters.split", "len", "print", "line.strip.strip.split", "line.strip.strip.split", "int", "words_to_em[].append"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "load_nrc_emotion", "(", "input_file", ",", "emotion_filters", "=", "None", ")", ":", "\n", "        ", "'''\n        load NRC file and load it to a dictionary!\n        :param input_file:\n        :return:\n        '''", "\n", "ss", "=", "0", "\n", "if", "emotion_filters", ":", "\n", "            ", "emotion_filters", "=", "[", "em", ".", "strip", "(", ")", "for", "em", "in", "emotion_filters", ".", "split", "(", "','", ")", "]", "\n", "#print('Filtering emotions', emotion_filters)", "\n", "", "words_to_em", "=", "collections", ".", "defaultdict", "(", "lambda", ":", "[", "\"UNKE\"", "]", ")", "\n", "emotion_to_id", "=", "dict", "(", ")", "\n", "emotion_to_id", "[", "\"UNKE\"", "]", "=", "0", "\n", "idx", "=", "1", "\n", "with", "open", "(", "input_file", ",", "'r'", ",", "encoding", "=", "\"utf-8\"", ")", "as", "fr", ":", "\n", "            ", "for", "line", "in", "fr", ":", "\n", "                ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "line", "!=", "\"\"", ":", "\n", "                    ", "if", "len", "(", "line", ".", "split", "(", "'\\t'", ")", ")", "!=", "3", ":", "\n", "                        ", "print", "(", "'Skipping line in lemotion lexicon!'", ",", "line", ")", "\n", "", "else", ":", "\n", "                        ", "word", ",", "emotion", ",", "val", "=", "line", ".", "split", "(", "'\\t'", ")", "\n", "if", "emotion_filters", "is", "None", "or", "(", "emotion_filters", "is", "not", "None", "and", "emotion", "not", "in", "emotion_filters", ")", ":", "\n", "                            ", "if", "emotion", "not", "in", "emotion_to_id", ":", "\n", "                                ", "emotion_to_id", "[", "emotion", "]", "=", "idx", "\n", "idx", "+=", "1", "\n", "", "if", "int", "(", "val", ")", "==", "1", ":", "\n", "                                ", "if", "word", "not", "in", "words_to_em", ":", "\n", "                                    ", "words_to_em", "[", "word", "]", "=", "[", "emotion", "]", "\n", "", "else", ":", "\n", "                                    ", "words_to_em", "[", "word", "]", ".", "append", "(", "emotion", ")", "\n", "", "", "", "if", "emotion_filters", "is", "not", "None", "and", "emotion", "not", "in", "emotion_filters", ":", "\n", "                            ", "ss", "+=", "1", "\n", "#print('total number of emotion-filtered words', ss)", "\n", "#print('tottal number of emotion-unfiltered words', len(words_to_em))", "\n", "", "", "", "", "", "return", "words_to_em", ",", "emotion_to_id", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.utils.emotion.Emotion.convert_tokens_to_ids": [[53, 65], ["ids.append", "len", "logger.warning", "len"], "methods", ["None"], ["", "def", "convert_tokens_to_ids", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "ids", "=", "[", "]", "\n", "for", "token", "in", "tokens", ":", "\n", "            ", "ids", ".", "append", "(", "[", "self", ".", "em_to_id", "[", "em", "]", "for", "em", "in", "self", ".", "word_to_em", "[", "token", "]", "]", ")", "\n", "\n", "", "if", "len", "(", "ids", ")", ">", "self", ".", "max_len", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "\"Token indices sequence length is longer than the specified maximum \"", "\n", "\" sequence length for this BERT model ({} > {}). Running this\"", "\n", "\" sequence through BERT will result in indexing errors\"", ".", "format", "(", "len", "(", "ids", ")", ",", "self", ".", "max_len", ")", "\n", ")", "\n", "", "return", "ids", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.utils.emotion.Emotion.get_padded_ids": [[66, 80], ["emotion.Emotion.convert_tokens_to_ids", "list", "sequence.split", "list.append", "len"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.BertTokenizer.convert_tokens_to_ids"], ["", "def", "get_padded_ids", "(", "self", ",", "sequence", ")", ":", "\n", "        ", "'''\n        return emotion ids of the tokens in given sequence\n        :param sequence:\n        :param max_emotion: keep the total emotions for all tokens equal! padding 0 will be added if necessary\n        :return:\n        '''", "\n", "ids", "=", "self", ".", "convert_tokens_to_ids", "(", "sequence", ".", "split", "(", "\" \"", ")", ")", "\n", "padded_ids", "=", "list", "(", ")", "\n", "for", "gid", "in", "ids", ":", "\n", "            ", "gid", "=", "gid", "[", ":", "self", ".", "max_em_len", "]", "\n", "gid", "=", "[", "0", "]", "*", "(", "self", ".", "max_em_len", "-", "len", "(", "gid", ")", ")", "+", "gid", "\n", "padded_ids", ".", "append", "(", "gid", ")", "\n", "", "return", "padded_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.utils.optimization.BertAdam.__init__": [[66, 85], ["dict", "torch.optim.Optimizer.__init__", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.classification_evaluator.ClassificationEvaluator.__init__"], ["def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "required", ",", "warmup", "=", "-", "1", ",", "t_total", "=", "-", "1", ",", "schedule", "=", "'warmup_linear'", ",", "\n", "b1", "=", "0.9", ",", "b2", "=", "0.999", ",", "e", "=", "1e-6", ",", "weight_decay", "=", "0.01", ",", "\n", "max_grad_norm", "=", "1.0", ")", ":", "\n", "        ", "if", "lr", "is", "not", "required", "and", "lr", "<", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid learning rate: {} - should be >= 0.0\"", ".", "format", "(", "lr", ")", ")", "\n", "", "if", "schedule", "not", "in", "SCHEDULES", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid schedule parameter: {}\"", ".", "format", "(", "schedule", ")", ")", "\n", "", "if", "not", "0.0", "<=", "warmup", "<", "1.0", "and", "not", "warmup", "==", "-", "1", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid warmup: {} - should be in [0.0, 1.0[ or -1\"", ".", "format", "(", "warmup", ")", ")", "\n", "", "if", "not", "0.0", "<=", "b1", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid b1 parameter: {} - should be in [0.0, 1.0[\"", ".", "format", "(", "b1", ")", ")", "\n", "", "if", "not", "0.0", "<=", "b2", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid b2 parameter: {} - should be in [0.0, 1.0[\"", ".", "format", "(", "b2", ")", ")", "\n", "", "if", "not", "e", ">=", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid epsilon value: {} - should be >= 0.0\"", ".", "format", "(", "e", ")", ")", "\n", "", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "schedule", "=", "schedule", ",", "warmup", "=", "warmup", ",", "t_total", "=", "t_total", ",", "\n", "b1", "=", "b1", ",", "b2", "=", "b2", ",", "e", "=", "e", ",", "weight_decay", "=", "weight_decay", ",", "\n", "max_grad_norm", "=", "max_grad_norm", ")", "\n", "super", "(", "BertAdam", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.utils.optimization.BertAdam.get_lr": [[86, 100], ["lr.append", "len", "schedule_fct"], "methods", ["None"], ["", "def", "get_lr", "(", "self", ")", ":", "\n", "        ", "lr", "=", "[", "]", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "return", "[", "0", "]", "\n", "", "if", "group", "[", "'t_total'", "]", "!=", "-", "1", ":", "\n", "                    ", "schedule_fct", "=", "SCHEDULES", "[", "group", "[", "'schedule'", "]", "]", "\n", "lr_scheduled", "=", "group", "[", "'lr'", "]", "*", "schedule_fct", "(", "state", "[", "'step'", "]", "/", "group", "[", "'t_total'", "]", ",", "group", "[", "'warmup'", "]", ")", "\n", "", "else", ":", "\n", "                    ", "lr_scheduled", "=", "group", "[", "'lr'", "]", "\n", "", "lr", ".", "append", "(", "lr_scheduled", ")", "\n", "", "", "return", "lr", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.utils.optimization.BertAdam.step": [[101, 180], ["closure", "next_m.mul_().add_", "next_v.mul_().addcmul_", "p.data.add_", "RuntimeError", "len", "torch.zeros_like", "torch.zeros_like", "torch.nn.utils.clip_grad_norm_", "next_m.mul_", "next_v.mul_", "next_v.sqrt", "schedule_fct", "logger.warning"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "warned_for_t_total", "=", "False", "\n", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", ".", "data", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "'Adam does not support sparse gradients, please consider SparseAdam instead'", ")", "\n", "\n", "", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "# Exponential moving average of gradient values", "\n", "state", "[", "'next_m'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "# Exponential moving average of squared gradient values", "\n", "state", "[", "'next_v'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "\n", "", "next_m", ",", "next_v", "=", "state", "[", "'next_m'", "]", ",", "state", "[", "'next_v'", "]", "\n", "beta1", ",", "beta2", "=", "group", "[", "'b1'", "]", ",", "group", "[", "'b2'", "]", "\n", "\n", "# Add grad clipping", "\n", "if", "group", "[", "'max_grad_norm'", "]", ">", "0", ":", "\n", "                    ", "clip_grad_norm_", "(", "p", ",", "group", "[", "'max_grad_norm'", "]", ")", "\n", "\n", "# Decay the first and second moment running average coefficient", "\n", "# In-place operations to update the averages at the same time", "\n", "", "next_m", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "1", "-", "beta1", ",", "grad", ")", "\n", "next_v", ".", "mul_", "(", "beta2", ")", ".", "addcmul_", "(", "1", "-", "beta2", ",", "grad", ",", "grad", ")", "\n", "update", "=", "next_m", "/", "(", "next_v", ".", "sqrt", "(", ")", "+", "group", "[", "'e'", "]", ")", "\n", "\n", "# Just adding the square of the weights to the loss function is *not*", "\n", "# the correct way of using L2 regularization/weight decay with Adam,", "\n", "# since that will interact with the m and v parameters in strange ways.", "\n", "#", "\n", "# Instead we want to decay the weights in a manner that doesn't interact", "\n", "# with the m/v parameters. This is equivalent to adding the square", "\n", "# of the weights to the loss with plain (non-momentum) SGD.", "\n", "if", "group", "[", "'weight_decay'", "]", ">", "0.0", ":", "\n", "                    ", "update", "+=", "group", "[", "'weight_decay'", "]", "*", "p", ".", "data", "\n", "\n", "", "if", "group", "[", "'t_total'", "]", "!=", "-", "1", ":", "\n", "                    ", "schedule_fct", "=", "SCHEDULES", "[", "group", "[", "'schedule'", "]", "]", "\n", "progress", "=", "state", "[", "'step'", "]", "/", "group", "[", "'t_total'", "]", "\n", "lr_scheduled", "=", "group", "[", "'lr'", "]", "*", "schedule_fct", "(", "progress", ",", "group", "[", "'warmup'", "]", ")", "\n", "# warning for exceeding t_total (only active with warmup_linear", "\n", "if", "group", "[", "'schedule'", "]", "==", "\"warmup_linear\"", "and", "progress", ">", "1.", "and", "not", "warned_for_t_total", ":", "\n", "                        ", "logger", ".", "warning", "(", "\n", "\"Training beyond specified 't_total' steps with schedule '{}'. Learning rate set to {}. \"", "\n", "\"Please set 't_total' of {} correctly.\"", ".", "format", "(", "group", "[", "'schedule'", "]", ",", "lr_scheduled", ",", "self", ".", "__class__", ".", "__name__", ")", ")", "\n", "warned_for_t_total", "=", "True", "\n", "# end warning", "\n", "", "", "else", ":", "\n", "                    ", "lr_scheduled", "=", "group", "[", "'lr'", "]", "\n", "\n", "", "update_with_lr", "=", "lr_scheduled", "*", "update", "\n", "p", ".", "data", ".", "add_", "(", "-", "update_with_lr", ")", "\n", "\n", "state", "[", "'step'", "]", "+=", "1", "\n", "\n", "# step_size = lr_scheduled * math.sqrt(bias_correction2) / bias_correction1", "\n", "# No bias correction", "\n", "# bias_correction1 = 1 - beta1 ** state['step']", "\n", "# bias_correction2 = 1 - beta2 ** state['step']", "\n", "\n", "", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.marjanhs_procon20.utils.optimization.warmup_cosine": [[26, 30], ["torch.cos"], "function", ["None"], ["def", "warmup_cosine", "(", "x", ",", "warmup", "=", "0.002", ")", ":", "\n", "    ", "if", "x", "<", "warmup", ":", "\n", "        ", "return", "x", "/", "warmup", "\n", "", "return", "0.5", "*", "(", "1.0", "+", "torch", ".", "cos", "(", "math", ".", "pi", "*", "x", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.utils.optimization.warmup_constant": [[31, 37], ["None"], "function", ["None"], ["", "def", "warmup_constant", "(", "x", ",", "warmup", "=", "0.002", ")", ":", "\n", "    ", "\"\"\" Linearly increases learning rate over `warmup`*`t_total` (as provided to BertAdam) training steps.\n        Learning rate is 1. afterwards. \"\"\"", "\n", "if", "x", "<", "warmup", ":", "\n", "        ", "return", "x", "/", "warmup", "\n", "", "return", "1.0", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.utils.optimization.warmup_linear": [[38, 44], ["max"], "function", ["None"], ["", "def", "warmup_linear", "(", "x", ",", "warmup", "=", "0.002", ")", ":", "\n", "    ", "\"\"\" Specifies a triangular learning rate schedule where peak is reached at `warmup`*`t_total`-th (as provided to BertAdam) training step.\n        After `t_total`-th training step, learning rate is zero. \"\"\"", "\n", "if", "x", "<", "warmup", ":", "\n", "        ", "return", "x", "/", "warmup", "\n", "", "return", "max", "(", "(", "x", "-", "1.", ")", "/", "(", "warmup", "-", "1.", ")", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.utils.io.url_to_filename": [[46, 62], ["url.encode", "hashlib.sha256", "hashlib.sha256.hexdigest", "etag.encode", "hashlib.sha256", "hashlib.sha256.hexdigest"], "function", ["None"], ["def", "url_to_filename", "(", "url", ",", "etag", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Convert `url` into a hashed filename in a repeatable way.\n    If `etag` is specified, append its hash to the url's, delimited\n    by a period.\n    \"\"\"", "\n", "url_bytes", "=", "url", ".", "encode", "(", "'utf-8'", ")", "\n", "url_hash", "=", "sha256", "(", "url_bytes", ")", "\n", "filename", "=", "url_hash", ".", "hexdigest", "(", ")", "\n", "\n", "if", "etag", ":", "\n", "        ", "etag_bytes", "=", "etag", ".", "encode", "(", "'utf-8'", ")", "\n", "etag_hash", "=", "sha256", "(", "etag_bytes", ")", "\n", "filename", "+=", "'.'", "+", "etag_hash", ".", "hexdigest", "(", ")", "\n", "\n", "", "return", "filename", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.utils.io.filename_to_url": [[64, 88], ["os.path.join", "isinstance", "str", "os.path.exists", "EnvironmentError", "os.path.exists", "EnvironmentError", "io.open", "json.load"], "function", ["None"], ["", "def", "filename_to_url", "(", "filename", ",", "cache_dir", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Return the url and etag (which may be ``None``) stored for `filename`.\n    Raise ``EnvironmentError`` if `filename` or its stored metadata do not exist.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "PYTORCH_PRETRAINED_BERT_CACHE", "\n", "", "if", "sys", ".", "version_info", "[", "0", "]", "==", "3", "and", "isinstance", "(", "cache_dir", ",", "Path", ")", ":", "\n", "        ", "cache_dir", "=", "str", "(", "cache_dir", ")", "\n", "\n", "", "cache_path", "=", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "filename", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "cache_path", ")", ":", "\n", "        ", "raise", "EnvironmentError", "(", "\"file {} not found\"", ".", "format", "(", "cache_path", ")", ")", "\n", "\n", "", "meta_path", "=", "cache_path", "+", "'.json'", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "meta_path", ")", ":", "\n", "        ", "raise", "EnvironmentError", "(", "\"file {} not found\"", ".", "format", "(", "meta_path", ")", ")", "\n", "\n", "", "with", "open", "(", "meta_path", ",", "encoding", "=", "\"utf-8\"", ")", "as", "meta_file", ":", "\n", "        ", "metadata", "=", "json", ".", "load", "(", "meta_file", ")", "\n", "", "url", "=", "metadata", "[", "'url'", "]", "\n", "etag", "=", "metadata", "[", "'etag'", "]", "\n", "\n", "return", "url", ",", "etag", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.utils.io.cached_path": [[90, 118], ["urllib.parse.urlparse", "isinstance", "str", "isinstance", "str", "io.get_from_cache", "os.path.exists", "EnvironmentError", "ValueError"], "function", ["home.repos.pwc.inspect_result.marjanhs_procon20.utils.io.get_from_cache"], ["", "def", "cached_path", "(", "url_or_filename", ",", "cache_dir", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Given something that might be a URL (or might be a local path),\n    determine which. If it's a URL, download the file and cache it, and\n    return the path to the cached file. If it's already a local path,\n    make sure the file exists and then return the path.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "PYTORCH_PRETRAINED_BERT_CACHE", "\n", "", "if", "sys", ".", "version_info", "[", "0", "]", "==", "3", "and", "isinstance", "(", "url_or_filename", ",", "Path", ")", ":", "\n", "        ", "url_or_filename", "=", "str", "(", "url_or_filename", ")", "\n", "", "if", "sys", ".", "version_info", "[", "0", "]", "==", "3", "and", "isinstance", "(", "cache_dir", ",", "Path", ")", ":", "\n", "        ", "cache_dir", "=", "str", "(", "cache_dir", ")", "\n", "\n", "", "parsed", "=", "urlparse", "(", "url_or_filename", ")", "\n", "\n", "if", "parsed", ".", "scheme", "in", "(", "'http'", ",", "'https'", ",", "'s3'", ")", ":", "\n", "# URL, so get it from the cache (downloading if necessary)", "\n", "        ", "return", "get_from_cache", "(", "url_or_filename", ",", "cache_dir", ")", "\n", "", "elif", "os", ".", "path", ".", "exists", "(", "url_or_filename", ")", ":", "\n", "# File, and it exists.", "\n", "        ", "return", "url_or_filename", "\n", "", "elif", "parsed", ".", "scheme", "==", "''", ":", "\n", "# File, but it doesn't exist.", "\n", "        ", "raise", "EnvironmentError", "(", "\"file {} not found\"", ".", "format", "(", "url_or_filename", ")", ")", "\n", "", "else", ":", "\n", "# Something unknown", "\n", "        ", "raise", "ValueError", "(", "\"unable to parse {} as a URL or as a local path\"", ".", "format", "(", "url_or_filename", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.utils.io.split_s3_path": [[120, 131], ["urllib.parse.urlparse", "s3_path.startswith", "ValueError"], "function", ["None"], ["", "", "def", "split_s3_path", "(", "url", ")", ":", "\n", "    ", "\"\"\"Split a full s3 path into the bucket name and path.\"\"\"", "\n", "parsed", "=", "urlparse", "(", "url", ")", "\n", "if", "not", "parsed", ".", "netloc", "or", "not", "parsed", ".", "path", ":", "\n", "        ", "raise", "ValueError", "(", "\"bad s3 path {}\"", ".", "format", "(", "url", ")", ")", "\n", "", "bucket_name", "=", "parsed", ".", "netloc", "\n", "s3_path", "=", "parsed", ".", "path", "\n", "# Remove '/' at beginning of path.", "\n", "if", "s3_path", ".", "startswith", "(", "\"/\"", ")", ":", "\n", "        ", "s3_path", "=", "s3_path", "[", "1", ":", "]", "\n", "", "return", "bucket_name", ",", "s3_path", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.utils.io.s3_request": [[133, 150], ["functools.wraps", "func", "int", "EnvironmentError"], "function", ["None"], ["", "def", "s3_request", "(", "func", ")", ":", "\n", "    ", "\"\"\"\n    Wrapper function for s3 requests in order to create more helpful error\n    messages.\n    \"\"\"", "\n", "\n", "@", "wraps", "(", "func", ")", "\n", "def", "wrapper", "(", "url", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "return", "func", "(", "url", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "", "except", "ClientError", "as", "exc", ":", "\n", "            ", "if", "int", "(", "exc", ".", "response", "[", "\"Error\"", "]", "[", "\"Code\"", "]", ")", "==", "404", ":", "\n", "                ", "raise", "EnvironmentError", "(", "\"file {} not found\"", ".", "format", "(", "url", ")", ")", "\n", "", "else", ":", "\n", "                ", "raise", "\n", "\n", "", "", "", "return", "wrapper", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.utils.io.s3_etag": [[152, 159], ["boto3.resource", "io.split_s3_path", "boto3.resource.Object"], "function", ["home.repos.pwc.inspect_result.marjanhs_procon20.utils.io.split_s3_path"], ["", "@", "s3_request", "\n", "def", "s3_etag", "(", "url", ")", ":", "\n", "    ", "\"\"\"Check ETag on S3 object.\"\"\"", "\n", "s3_resource", "=", "boto3", ".", "resource", "(", "\"s3\"", ")", "\n", "bucket_name", ",", "s3_path", "=", "split_s3_path", "(", "url", ")", "\n", "s3_object", "=", "s3_resource", ".", "Object", "(", "bucket_name", ",", "s3_path", ")", "\n", "return", "s3_object", ".", "e_tag", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.utils.io.s3_get": [[161, 167], ["boto3.resource", "io.split_s3_path", "boto3.resource.Bucket().download_fileobj", "boto3.resource.Bucket"], "function", ["home.repos.pwc.inspect_result.marjanhs_procon20.utils.io.split_s3_path"], ["", "@", "s3_request", "\n", "def", "s3_get", "(", "url", ",", "temp_file", ")", ":", "\n", "    ", "\"\"\"Pull a file directly from S3.\"\"\"", "\n", "s3_resource", "=", "boto3", ".", "resource", "(", "\"s3\"", ")", "\n", "bucket_name", ",", "s3_path", "=", "split_s3_path", "(", "url", ")", "\n", "s3_resource", ".", "Bucket", "(", "bucket_name", ")", ".", "download_fileobj", "(", "s3_path", ",", "temp_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.utils.io.http_get": [[169, 179], ["requests.get", "requests.get.headers.get", "tqdm.tqdm", "requests.get.iter_content", "tqdm.tqdm.close", "int", "tqdm.tqdm.update", "temp_file.write", "len"], "function", ["None"], ["", "def", "http_get", "(", "url", ",", "temp_file", ")", ":", "\n", "    ", "req", "=", "requests", ".", "get", "(", "url", ",", "stream", "=", "True", ")", "\n", "content_length", "=", "req", ".", "headers", ".", "get", "(", "'Content-Length'", ")", "\n", "total", "=", "int", "(", "content_length", ")", "if", "content_length", "is", "not", "None", "else", "None", "\n", "progress", "=", "tqdm", "(", "unit", "=", "\"B\"", ",", "total", "=", "total", ")", "\n", "for", "chunk", "in", "req", ".", "iter_content", "(", "chunk_size", "=", "1024", ")", ":", "\n", "        ", "if", "chunk", ":", "# filter out keep-alive new chunks", "\n", "            ", "progress", ".", "update", "(", "len", "(", "chunk", ")", ")", "\n", "temp_file", ".", "write", "(", "chunk", ")", "\n", "", "", "progress", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.utils.io.get_from_cache": [[181, 239], ["url.startswith", "io.url_to_filename", "os.path.join", "isinstance", "str", "os.path.exists", "os.makedirs", "io.s3_etag", "requests.head", "requests.head.headers.get", "os.path.exists", "IOError", "tempfile.NamedTemporaryFile", "logger.info", "url.startswith", "temp_file.flush", "temp_file.seek", "logger.info", "logger.info", "logger.info", "io.s3_get", "io.http_get", "io.open", "shutil.copyfileobj", "io.open", "json.dump"], "function", ["home.repos.pwc.inspect_result.marjanhs_procon20.utils.io.url_to_filename", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.io.s3_etag", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.io.s3_get", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.io.http_get"], ["", "def", "get_from_cache", "(", "url", ",", "cache_dir", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Given a URL, look for the corresponding dataset in the local cache.\n    If it's not there, download it. Then return the path to the cached file.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "PYTORCH_PRETRAINED_BERT_CACHE", "\n", "", "if", "sys", ".", "version_info", "[", "0", "]", "==", "3", "and", "isinstance", "(", "cache_dir", ",", "Path", ")", ":", "\n", "        ", "cache_dir", "=", "str", "(", "cache_dir", ")", "\n", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "cache_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "cache_dir", ")", "\n", "\n", "# Get eTag to add to filename, if it exists.", "\n", "", "if", "url", ".", "startswith", "(", "\"s3://\"", ")", ":", "\n", "        ", "etag", "=", "s3_etag", "(", "url", ")", "\n", "", "else", ":", "\n", "        ", "response", "=", "requests", ".", "head", "(", "url", ",", "allow_redirects", "=", "True", ")", "\n", "if", "response", ".", "status_code", "!=", "200", ":", "\n", "            ", "raise", "IOError", "(", "\"HEAD request failed for url {} with status code {}\"", "\n", ".", "format", "(", "url", ",", "response", ".", "status_code", ")", ")", "\n", "", "etag", "=", "response", ".", "headers", ".", "get", "(", "\"ETag\"", ")", "\n", "\n", "", "filename", "=", "url_to_filename", "(", "url", ",", "etag", ")", "\n", "\n", "# get cache path to put the file", "\n", "cache_path", "=", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "filename", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "cache_path", ")", ":", "\n", "# Download to temporary file, then copy to cache dir once finished.", "\n", "# Otherwise you get corrupt cache entries if the download gets interrupted.", "\n", "        ", "with", "tempfile", ".", "NamedTemporaryFile", "(", ")", "as", "temp_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"%s not found in cache, downloading to %s\"", ",", "url", ",", "temp_file", ".", "name", ")", "\n", "\n", "# GET file object", "\n", "if", "url", ".", "startswith", "(", "\"s3://\"", ")", ":", "\n", "                ", "s3_get", "(", "url", ",", "temp_file", ")", "\n", "", "else", ":", "\n", "                ", "http_get", "(", "url", ",", "temp_file", ")", "\n", "\n", "# we are copying the file before closing it, so flush to avoid truncation", "\n", "", "temp_file", ".", "flush", "(", ")", "\n", "# shutil.copyfileobj() starts at the current position, so go to the start", "\n", "temp_file", ".", "seek", "(", "0", ")", "\n", "\n", "logger", ".", "info", "(", "\"copying %s to cache at %s\"", ",", "temp_file", ".", "name", ",", "cache_path", ")", "\n", "with", "open", "(", "cache_path", ",", "'wb'", ")", "as", "cache_file", ":", "\n", "                ", "shutil", ".", "copyfileobj", "(", "temp_file", ",", "cache_file", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"creating metadata file for %s\"", ",", "cache_path", ")", "\n", "meta", "=", "{", "'url'", ":", "url", ",", "'etag'", ":", "etag", "}", "\n", "meta_path", "=", "cache_path", "+", "'.json'", "\n", "with", "open", "(", "meta_path", ",", "'w'", ",", "encoding", "=", "\"utf-8\"", ")", "as", "meta_file", ":", "\n", "                ", "json", ".", "dump", "(", "meta", ",", "meta_file", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"removing temp file %s\"", ",", "temp_file", ".", "name", ")", "\n", "\n", "", "", "return", "cache_path", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.utils.io.read_set_from_file": [[241, 252], ["set", "io.open", "set.add", "line.rstrip"], "function", ["None"], ["", "def", "read_set_from_file", "(", "filename", ")", ":", "\n", "    ", "\"\"\"\n    Extract a de-duped collection (set) of text from a file.\n    Expected file format is one item per line.\n    \"\"\"", "\n", "\n", "collection", "=", "set", "(", ")", "\n", "with", "open", "(", "filename", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "file_", ":", "\n", "        ", "for", "line", "in", "file_", ":", "\n", "            ", "collection", ".", "add", "(", "line", ".", "rstrip", "(", ")", ")", "\n", "", "", "return", "collection", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.utils.io.get_file_extension": [[254, 258], ["os.path.splitext", "ext.lower"], "function", ["None"], ["", "def", "get_file_extension", "(", "path", ",", "dot", "=", "True", ",", "lower", "=", "True", ")", ":", "\n", "    ", "ext", "=", "os", ".", "path", ".", "splitext", "(", "path", ")", "[", "1", "]", "\n", "ext", "=", "ext", "if", "dot", "else", "ext", "[", "1", ":", "]", "\n", "return", "ext", ".", "lower", "(", ")", "if", "lower", "else", "ext", "", "", ""]], "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.BertTokenizer.__init__": [[74, 103], ["tokenization.load_vocab", "collections.OrderedDict", "tokenization.WordpieceTokenizer", "os.path.isfile", "ValueError", "tokenization.BasicTokenizer", "int", "tokenization.BertTokenizer.vocab.items"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.load_vocab"], ["def", "__init__", "(", "self", ",", "vocab_file", ",", "is_lowercase", "=", "True", ",", "max_len", "=", "None", ",", "do_basic_tokenize", "=", "True", ",", "\n", "never_split", "=", "(", "\"[UNK]\"", ",", "\"[SEP]\"", ",", "\"[PAD]\"", ",", "\"[CLS]\"", ",", "\"[MASK]\"", ")", ")", ":", "\n", "        ", "\"\"\"Constructs a BertTokenizer.\n\n        Args:\n          vocab_file: Path to a one-wordpiece-per-line vocabulary file\n          is_lowercase: Whether to lower case the input\n                         Only has an effect when do_wordpiece_only=False\n          do_basic_tokenize: Whether to do basic tokenization before wordpiece.\n          max_len: An artificial maximum length to truncate tokenized sequences to;\n                         Effective maximum length is always the minimum of this\n                         value (if specified) and the underlying BERT model's\n                         sequence length.\n          never_split: List of tokens which will never be split during tokenization.\n                         Only has an effect when do_wordpiece_only=False\n        \"\"\"", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "vocab_file", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Can't find a vocabulary file at path '{}'. To load the vocabulary from a Google pretrained \"", "\n", "\"model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\"", ".", "format", "(", "vocab_file", ")", ")", "\n", "", "self", ".", "vocab", "=", "load_vocab", "(", "vocab_file", ")", "\n", "self", ".", "ids_to_tokens", "=", "collections", ".", "OrderedDict", "(", "\n", "[", "(", "ids", ",", "tok", ")", "for", "tok", ",", "ids", "in", "self", ".", "vocab", ".", "items", "(", ")", "]", ")", "\n", "self", ".", "do_basic_tokenize", "=", "do_basic_tokenize", "\n", "if", "do_basic_tokenize", ":", "\n", "          ", "self", ".", "basic_tokenizer", "=", "BasicTokenizer", "(", "is_lowercase", "=", "is_lowercase", ",", "\n", "never_split", "=", "never_split", ")", "\n", "", "self", ".", "wordpiece_tokenizer", "=", "WordpieceTokenizer", "(", "vocab", "=", "self", ".", "vocab", ")", "\n", "self", ".", "max_len", "=", "max_len", "if", "max_len", "is", "not", "None", "else", "int", "(", "1e12", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.BertTokenizer.tokenize": [[104, 113], ["tokenization.BertTokenizer.basic_tokenizer.tokenize", "tokenization.BertTokenizer.wordpiece_tokenizer.tokenize", "tokenization.BertTokenizer.wordpiece_tokenizer.tokenize", "tokenization.BertTokenizer.append"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.WordpieceTokenizer.tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "if", "self", ".", "do_basic_tokenize", ":", "\n", "          ", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "self", ".", "basic_tokenizer", ".", "tokenize", "(", "text", ")", ":", "\n", "              ", "for", "sub_token", "in", "self", ".", "wordpiece_tokenizer", ".", "tokenize", "(", "token", ")", ":", "\n", "                  ", "split_tokens", ".", "append", "(", "sub_token", ")", "\n", "", "", "", "else", ":", "\n", "          ", "split_tokens", "=", "self", ".", "wordpiece_tokenizer", ".", "tokenize", "(", "text", ")", "\n", "", "return", "split_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.BertTokenizer.convert_tokens_to_ids": [[114, 126], ["ids.append", "len", "logger.warning", "len"], "methods", ["None"], ["", "def", "convert_tokens_to_ids", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "\"\"\"Converts a sequence of tokens into ids using the vocab.\"\"\"", "\n", "ids", "=", "[", "]", "\n", "for", "token", "in", "tokens", ":", "\n", "            ", "ids", ".", "append", "(", "self", ".", "vocab", "[", "token", "]", ")", "\n", "", "if", "len", "(", "ids", ")", ">", "self", ".", "max_len", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "\"Token indices sequence length is longer than the specified maximum \"", "\n", "\" sequence length for this BERT model ({} > {}). Running this\"", "\n", "\" sequence through BERT will result in indexing errors\"", ".", "format", "(", "len", "(", "ids", ")", ",", "self", ".", "max_len", ")", "\n", ")", "\n", "", "return", "ids", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.BertTokenizer.convert_ids_to_tokens": [[127, 133], ["tokens.append"], "methods", ["None"], ["", "def", "convert_ids_to_tokens", "(", "self", ",", "ids", ")", ":", "\n", "        ", "\"\"\"Converts a sequence of ids in wordpiece tokens using the vocab.\"\"\"", "\n", "tokens", "=", "[", "]", "\n", "for", "i", "in", "ids", ":", "\n", "            ", "tokens", ".", "append", "(", "self", ".", "ids_to_tokens", "[", "i", "]", ")", "\n", "", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.BertTokenizer.from_pretrained": [[134, 171], ["os.path.isdir", "cls", "os.path.join", "utils.io.cached_path", "logger.info", "logger.info", "min", "logger.error", "kwargs.get", "int", "PRETRAINED_VOCAB_ARCHIVE_MAP.keys"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.utils.io.cached_path"], ["", "@", "classmethod", "\n", "def", "from_pretrained", "(", "cls", ",", "pretrained_model_name_or_path", ",", "cache_dir", "=", "None", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Instantiate a PreTrainedBertModel from a pre-trained model file.\n        Download and cache the pre-trained model file if needed.\n        \"\"\"", "\n", "if", "pretrained_model_name_or_path", "in", "PRETRAINED_VOCAB_ARCHIVE_MAP", ":", "\n", "            ", "vocab_file", "=", "PRETRAINED_VOCAB_ARCHIVE_MAP", "[", "pretrained_model_name_or_path", "]", "\n", "", "else", ":", "\n", "            ", "vocab_file", "=", "pretrained_model_name_or_path", "\n", "", "if", "os", ".", "path", ".", "isdir", "(", "vocab_file", ")", ":", "\n", "            ", "vocab_file", "=", "os", ".", "path", ".", "join", "(", "vocab_file", ",", "VOCAB_NAME", ")", "\n", "# redirect to the cache, if necessary", "\n", "", "try", ":", "\n", "            ", "resolved_vocab_file", "=", "cached_path", "(", "vocab_file", ",", "cache_dir", "=", "cache_dir", ")", "\n", "", "except", "EnvironmentError", ":", "\n", "            ", "logger", ".", "error", "(", "\n", "\"Model name '{}' was not found in model name list ({}). \"", "\n", "\"We assumed '{}' was a path or url but couldn't find any file \"", "\n", "\"associated to this path or url.\"", ".", "format", "(", "\n", "pretrained_model_name_or_path", ",", "\n", "', '", ".", "join", "(", "PRETRAINED_VOCAB_ARCHIVE_MAP", ".", "keys", "(", ")", ")", ",", "\n", "vocab_file", ")", ")", "\n", "return", "None", "\n", "", "if", "resolved_vocab_file", "==", "vocab_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading vocabulary file {}\"", ".", "format", "(", "vocab_file", ")", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading vocabulary file {} from cache at {}\"", ".", "format", "(", "\n", "vocab_file", ",", "resolved_vocab_file", ")", ")", "\n", "", "if", "pretrained_model_name_or_path", "in", "PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP", ":", "\n", "# if we're using a pretrained model, ensure the tokenizer wont index sequences longer", "\n", "# than the number of positional embeddings", "\n", "            ", "max_len", "=", "PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP", "[", "pretrained_model_name_or_path", "]", "\n", "kwargs", "[", "'max_len'", "]", "=", "min", "(", "kwargs", ".", "get", "(", "'max_len'", ",", "int", "(", "1e12", ")", ")", ",", "max_len", ")", "\n", "# Instantiate tokenizer.", "\n", "", "tokenizer", "=", "cls", "(", "resolved_vocab_file", ",", "*", "inputs", ",", "**", "kwargs", ")", "\n", "return", "tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.BasicTokenizer.__init__": [[176, 186], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "is_lowercase", "=", "True", ",", "\n", "never_split", "=", "(", "\"[UNK]\"", ",", "\"[SEP]\"", ",", "\"[PAD]\"", ",", "\"[CLS]\"", ",", "\"[MASK]\"", ")", ")", ":", "\n", "        ", "\"\"\"Constructs a BasicTokenizer.\n\n        Args:\n          is_lowercase: Whether to lower case the input.\n        \"\"\"", "\n", "self", ".", "is_lowercase", "=", "is_lowercase", "\n", "self", ".", "never_split", "=", "never_split", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.BasicTokenizer.tokenize": [[187, 207], ["tokenization.BasicTokenizer._clean_text", "tokenization.BasicTokenizer._tokenize_chinese_chars", "tokenization.whitespace_tokenize", "tokenization.whitespace_tokenize", "split_tokens.extend", "tokenization.BasicTokenizer.lower", "tokenization.BasicTokenizer._run_strip_accents", "tokenization.BasicTokenizer._run_split_on_punc"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.BasicTokenizer._clean_text", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.BasicTokenizer._tokenize_chinese_chars", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.whitespace_tokenize", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.whitespace_tokenize", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.BasicTokenizer._run_strip_accents", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.BasicTokenizer._run_split_on_punc"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Tokenizes a piece of text.\"\"\"", "\n", "text", "=", "self", ".", "_clean_text", "(", "text", ")", "\n", "# This was added on November 1st, 2018 for the multilingual and Chinese", "\n", "# models. This is also applied to the English models now, but it doesn't", "\n", "# matter since the English models were not trained on any Chinese data", "\n", "# and generally don't have any Chinese data in them (there are Chinese", "\n", "# characters in the vocabulary because Wikipedia does have some Chinese", "\n", "# words in the English Wikipedia.).", "\n", "text", "=", "self", ".", "_tokenize_chinese_chars", "(", "text", ")", "\n", "orig_tokens", "=", "whitespace_tokenize", "(", "text", ")", "\n", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "orig_tokens", ":", "\n", "            ", "if", "self", ".", "is_lowercase", "and", "token", "not", "in", "self", ".", "never_split", ":", "\n", "                ", "token", "=", "token", ".", "lower", "(", ")", "\n", "token", "=", "self", ".", "_run_strip_accents", "(", "token", ")", "\n", "", "split_tokens", ".", "extend", "(", "self", ".", "_run_split_on_punc", "(", "token", ")", ")", "\n", "\n", "", "output_tokens", "=", "whitespace_tokenize", "(", "\" \"", ".", "join", "(", "split_tokens", ")", ")", "\n", "return", "output_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.BasicTokenizer._run_strip_accents": [[208, 218], ["unicodedata.normalize", "unicodedata.category", "output.append"], "methods", ["None"], ["", "def", "_run_strip_accents", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Strips accents from a piece of text.\"\"\"", "\n", "text", "=", "unicodedata", ".", "normalize", "(", "\"NFD\"", ",", "text", ")", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Mn\"", ":", "\n", "                ", "continue", "\n", "", "output", ".", "append", "(", "char", ")", "\n", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.BasicTokenizer._run_split_on_punc": [[219, 240], ["list", "len", "tokenization._is_punctuation", "output.append", "output[].append", "output.append"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization._is_punctuation"], ["", "def", "_run_split_on_punc", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Splits punctuation on a piece of text.\"\"\"", "\n", "if", "text", "in", "self", ".", "never_split", ":", "\n", "            ", "return", "[", "text", "]", "\n", "", "chars", "=", "list", "(", "text", ")", "\n", "i", "=", "0", "\n", "start_new_word", "=", "True", "\n", "output", "=", "[", "]", "\n", "while", "i", "<", "len", "(", "chars", ")", ":", "\n", "            ", "char", "=", "chars", "[", "i", "]", "\n", "if", "_is_punctuation", "(", "char", ")", ":", "\n", "                ", "output", ".", "append", "(", "[", "char", "]", ")", "\n", "start_new_word", "=", "True", "\n", "", "else", ":", "\n", "                ", "if", "start_new_word", ":", "\n", "                    ", "output", ".", "append", "(", "[", "]", ")", "\n", "", "start_new_word", "=", "False", "\n", "output", "[", "-", "1", "]", ".", "append", "(", "char", ")", "\n", "", "i", "+=", "1", "\n", "\n", "", "return", "[", "\"\"", ".", "join", "(", "x", ")", "for", "x", "in", "output", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.BasicTokenizer._tokenize_chinese_chars": [[241, 253], ["ord", "tokenization.BasicTokenizer._is_chinese_char", "output.append", "output.append", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.BasicTokenizer._is_chinese_char"], ["", "def", "_tokenize_chinese_chars", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Adds whitespace around any CJK character.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "self", ".", "_is_chinese_char", "(", "cp", ")", ":", "\n", "                ", "output", ".", "append", "(", "\" \"", ")", "\n", "output", ".", "append", "(", "char", ")", "\n", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n", "                ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.BasicTokenizer._is_chinese_char": [[254, 275], ["None"], "methods", ["None"], ["", "def", "_is_chinese_char", "(", "self", ",", "cp", ")", ":", "\n", "        ", "\"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"", "\n", "# This defines a \"chinese character\" as anything in the CJK Unicode block:", "\n", "#   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)", "\n", "#", "\n", "# Note that the CJK Unicode block is NOT all Japanese and Korean characters,", "\n", "# despite its name. The modern Korean Hangul alphabet is a different block,", "\n", "# as is Japanese Hiragana and Katakana. Those alphabets are used to write", "\n", "# space-separated words, so they are not treated specially and handled", "\n", "# like the all of the other languages.", "\n", "if", "(", "(", "cp", ">=", "0x4E00", "and", "cp", "<=", "0x9FFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x3400", "and", "cp", "<=", "0x4DBF", ")", "or", "#", "\n", "(", "cp", ">=", "0x20000", "and", "cp", "<=", "0x2A6DF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2A700", "and", "cp", "<=", "0x2B73F", ")", "or", "#", "\n", "(", "cp", ">=", "0x2B740", "and", "cp", "<=", "0x2B81F", ")", "or", "#", "\n", "(", "cp", ">=", "0x2B820", "and", "cp", "<=", "0x2CEAF", ")", "or", "\n", "(", "cp", ">=", "0xF900", "and", "cp", "<=", "0xFAFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2F800", "and", "cp", "<=", "0x2FA1F", ")", ")", ":", "#", "\n", "            ", "return", "True", "\n", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.BasicTokenizer._clean_text": [[276, 288], ["ord", "tokenization._is_whitespace", "tokenization._is_control", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization._is_whitespace", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization._is_control"], ["", "def", "_clean_text", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "cp", "==", "0", "or", "cp", "==", "0xfffd", "or", "_is_control", "(", "char", ")", ":", "\n", "                ", "continue", "\n", "", "if", "_is_whitespace", "(", "char", ")", ":", "\n", "                ", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n", "                ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.WordpieceTokenizer.__init__": [[293, 297], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "vocab", ",", "unk_token", "=", "\"[UNK]\"", ",", "max_input_chars_per_word", "=", "100", ")", ":", "\n", "        ", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "unk_token", "=", "unk_token", "\n", "self", ".", "max_input_chars_per_word", "=", "max_input_chars_per_word", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.WordpieceTokenizer.tokenize": [[298, 348], ["tokenization.whitespace_tokenize", "list", "len", "output_tokens.append", "len", "len", "sub_tokens.append", "output_tokens.append", "output_tokens.extend"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.whitespace_tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Tokenizes a piece of text into its word pieces.\n\n        This uses a greedy longest-match-first algorithm to perform tokenization\n        using the given vocabulary.\n\n        For example:\n          input = \"unaffable\"\n          output = [\"un\", \"##aff\", \"##able\"]\n\n        Args:\n          text: A single token or whitespace separated tokens. This should have\n            already been passed through `BasicTokenizer`.\n\n        Returns:\n          A list of wordpiece tokens.\n        \"\"\"", "\n", "\n", "output_tokens", "=", "[", "]", "\n", "for", "token", "in", "whitespace_tokenize", "(", "text", ")", ":", "\n", "            ", "chars", "=", "list", "(", "token", ")", "\n", "if", "len", "(", "chars", ")", ">", "self", ".", "max_input_chars_per_word", ":", "\n", "                ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "continue", "\n", "\n", "", "is_bad", "=", "False", "\n", "start", "=", "0", "\n", "sub_tokens", "=", "[", "]", "\n", "while", "start", "<", "len", "(", "chars", ")", ":", "\n", "                ", "end", "=", "len", "(", "chars", ")", "\n", "cur_substr", "=", "None", "\n", "while", "start", "<", "end", ":", "\n", "                    ", "substr", "=", "\"\"", ".", "join", "(", "chars", "[", "start", ":", "end", "]", ")", "\n", "if", "start", ">", "0", ":", "\n", "                        ", "substr", "=", "\"##\"", "+", "substr", "\n", "", "if", "substr", "in", "self", ".", "vocab", ":", "\n", "                        ", "cur_substr", "=", "substr", "\n", "break", "\n", "", "end", "-=", "1", "\n", "", "if", "cur_substr", "is", "None", ":", "\n", "                    ", "is_bad", "=", "True", "\n", "break", "\n", "", "sub_tokens", ".", "append", "(", "cur_substr", ")", "\n", "start", "=", "end", "\n", "\n", "", "if", "is_bad", ":", "\n", "                ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "", "else", ":", "\n", "                ", "output_tokens", ".", "extend", "(", "sub_tokens", ")", "\n", "", "", "return", "output_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.load_vocab": [[48, 61], ["collections.OrderedDict", "io.open", "reader.readline", "token.strip.strip"], "function", ["None"], ["def", "load_vocab", "(", "vocab_file", ")", ":", "\n", "    ", "\"\"\"Loads a vocabulary file into a dictionary.\"\"\"", "\n", "vocab", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "index", "=", "0", "\n", "with", "open", "(", "vocab_file", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "reader", ":", "\n", "        ", "while", "True", ":", "\n", "            ", "token", "=", "reader", ".", "readline", "(", ")", "\n", "if", "not", "token", ":", "\n", "                ", "break", "\n", "", "token", "=", "token", ".", "strip", "(", ")", "\n", "vocab", "[", "token", "]", "=", "index", "\n", "index", "+=", "1", "\n", "", "", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.whitespace_tokenize": [[63, 69], ["text.split"], "function", ["None"], ["", "def", "whitespace_tokenize", "(", "text", ")", ":", "\n", "    ", "\"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"", "\n", "if", "not", "text", ":", "\n", "        ", "return", "[", "]", "\n", "", "tokens", "=", "text", ".", "split", "(", ")", "\n", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization._is_whitespace": [[350, 360], ["unicodedata.category"], "function", ["None"], ["", "", "def", "_is_whitespace", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a whitespace character.\"\"\"", "\n", "# \\t, \\n, and \\r are technically contorl characters but we treat them", "\n", "# as whitespace since they are generally considered as such.", "\n", "if", "char", "==", "\" \"", "or", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "        ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Zs\"", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization._is_control": [[362, 372], ["unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "def", "_is_control", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a control character.\"\"\"", "\n", "# These are technically control characters but we count them as whitespace", "\n", "# characters.", "\n", "if", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "        ", "return", "False", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "\"C\"", ")", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization._is_punctuation": [[374, 388], ["ord", "unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "def", "_is_punctuation", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a punctuation character.\"\"\"", "\n", "cp", "=", "ord", "(", "char", ")", "\n", "# We treat all non-letter/number ASCII as punctuation.", "\n", "# Characters such as \"^\", \"$\", and \"`\" are not in the Unicode", "\n", "# Punctuation class but we treat them as punctuation anyways, for", "\n", "# consistency.", "\n", "if", "(", "(", "cp", ">=", "33", "and", "cp", "<=", "47", ")", "or", "(", "cp", ">=", "58", "and", "cp", "<=", "64", ")", "or", "\n", "(", "cp", ">=", "91", "and", "cp", "<=", "96", ")", "or", "(", "cp", ">=", "123", "and", "cp", "<=", "126", ")", ")", ":", "\n", "        ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "\"P\"", ")", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "", ""]], "home.repos.pwc.inspect_result.marjanhs_procon20.utils.preprocessing.pad_input_matrix": [[4, 18], ["min", "range", "max", "len", "range", "len", "len", "len", "len", "range", "len"], "function", ["None"], ["def", "pad_input_matrix", "(", "unpadded_matrix", ",", "max_doc_length", ")", ":", "\n", "    ", "\"\"\"\n    Returns a zero-padded matrix for a given jagged list\n    :param unpadded_matrix: jagged list to be padded\n    :return: zero-padded matrix\n    \"\"\"", "\n", "max_doc_length", "=", "min", "(", "max_doc_length", ",", "max", "(", "len", "(", "x", ")", "for", "x", "in", "unpadded_matrix", ")", ")", "\n", "zero_padding_array", "=", "[", "0", "for", "i0", "in", "range", "(", "len", "(", "unpadded_matrix", "[", "0", "]", "[", "0", "]", ")", ")", "]", "\n", "\n", "for", "i0", "in", "range", "(", "len", "(", "unpadded_matrix", ")", ")", ":", "\n", "        ", "if", "len", "(", "unpadded_matrix", "[", "i0", "]", ")", "<", "max_doc_length", ":", "\n", "            ", "unpadded_matrix", "[", "i0", "]", "+=", "[", "zero_padding_array", "for", "i1", "in", "range", "(", "max_doc_length", "-", "len", "(", "unpadded_matrix", "[", "i0", "]", ")", ")", "]", "\n", "", "elif", "len", "(", "unpadded_matrix", "[", "i0", "]", ")", ">", "max_doc_length", ":", "\n", "            ", "unpadded_matrix", "[", "i0", "]", "=", "unpadded_matrix", "[", "i0", "]", "[", ":", "max_doc_length", "]", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.marjanhs_procon20.utils.serialization.save_checkpoint": [[7, 19], ["state_dict.items", "torch.save", "tensor.cpu"], "function", ["None"], ["def", "save_checkpoint", "(", "epoch", ",", "arch", ",", "state_dict", ",", "optimizer_state", ",", "eval_metric", ",", "filename", ")", ":", "\n", "    ", "for", "k", ",", "tensor", "in", "state_dict", ".", "items", "(", ")", ":", "\n", "        ", "state_dict", "[", "k", "]", "=", "tensor", ".", "cpu", "(", ")", "\n", "\n", "", "state", "=", "{", "\n", "'epoch'", ":", "epoch", ",", "\n", "'arch'", ":", "arch", ",", "\n", "'state_dict'", ":", "state_dict", ",", "\n", "'optimizer_state'", ":", "None", ",", "# currently do not save optimizer state", "\n", "'eval_metric'", ":", "eval_metric", "\n", "}", "\n", "torch", ".", "save", "(", "state", ",", "filename", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.utils.serialization.load_checkpoint": [[21, 24], ["torch.load"], "function", ["None"], ["", "def", "load_checkpoint", "(", "filename", ")", ":", "\n", "    ", "state", "=", "torch", ".", "load", "(", "filename", ")", "\n", "return", "state", "[", "'epoch'", "]", ",", "state", "[", "'arch'", "]", ",", "state", "[", "'state_dict'", "]", ",", "state", "[", "'optimizer_state'", "]", ",", "state", "[", "'eval_metric'", "]", "\n", "", ""]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.robust04.Robust04.sort_key": [[26, 29], ["len"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "sort_key", "(", "ex", ")", ":", "\n", "        ", "return", "len", "(", "ex", ".", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.robust04.Robust04.splits": [[30, 35], ["super().splits"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.datasets.reuters.Reuters.splits"], ["", "@", "classmethod", "\n", "def", "splits", "(", "cls", ",", "path", ",", "train", ",", "validation", ",", "test", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "super", "(", "Robust04", ",", "cls", ")", ".", "splits", "(", "\n", "path", ",", "train", "=", "train", ",", "validation", "=", "validation", ",", "test", "=", "test", ",", "\n", "format", "=", "'tsv'", ",", "fields", "=", "[", "(", "'label'", ",", "cls", ".", "LABEL_FIELD", ")", ",", "(", "'docid'", ",", "cls", ".", "DOCID_FIELD", ")", ",", "(", "'text'", ",", "cls", ".", "TEXT_FIELD", ")", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.robust04.Robust04.iters": [[37, 61], ["os.path.join", "os.path.join", "os.path.join", "cls.splits", "cls.TEXT_FIELD.build_vocab", "torchtext.data.iterator.BucketIterator.splits", "torchtext.vocab.Vectors"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.datasets.reuters.Reuters.splits", "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.reuters.Reuters.splits"], ["", "@", "classmethod", "\n", "def", "iters", "(", "cls", ",", "path", ",", "vectors_name", ",", "vectors_cache", ",", "topic", ",", "batch_size", "=", "64", ",", "shuffle", "=", "True", ",", "device", "=", "0", ",", "\n", "vectors", "=", "None", ",", "unk_init", "=", "torch", ".", "Tensor", ".", "zero_", ")", ":", "\n", "        ", "\"\"\"\n        :param path: directory containing train, test, dev files\n        :param vectors_name: name of word vectors file\n        :param vectors_cache: path to directory containing word vectors file\n        :param topic: topic from which articles should be fetched\n        :param batch_size: batch size\n        :param device: GPU device\n        :param vectors: custom vectors - either predefined torchtext vectors or your own custom Vector classes\n        :param unk_init: function used to generate vector for OOV words\n        :return:\n        \"\"\"", "\n", "if", "vectors", "is", "None", ":", "\n", "            ", "vectors", "=", "Vectors", "(", "name", "=", "vectors_name", ",", "cache", "=", "vectors_cache", ",", "unk_init", "=", "unk_init", ")", "\n", "\n", "", "train_path", "=", "os", ".", "path", ".", "join", "(", "'TREC'", ",", "'robust04_train_%s.tsv'", "%", "topic", ")", "\n", "dev_path", "=", "os", ".", "path", ".", "join", "(", "'TREC'", ",", "'robust04_dev_%s.tsv'", "%", "topic", ")", "\n", "test_path", "=", "os", ".", "path", ".", "join", "(", "'TREC'", ",", "'core17_10k_%s.tsv'", "%", "topic", ")", "\n", "train", ",", "val", ",", "test", "=", "cls", ".", "splits", "(", "path", ",", "train", "=", "train_path", ",", "validation", "=", "dev_path", ",", "test", "=", "test_path", ")", "\n", "cls", ".", "TEXT_FIELD", ".", "build_vocab", "(", "train", ",", "val", ",", "test", ",", "vectors", "=", "vectors", ")", "\n", "return", "BucketIterator", ".", "splits", "(", "(", "train", ",", "val", ",", "test", ")", ",", "batch_size", "=", "batch_size", ",", "repeat", "=", "False", ",", "shuffle", "=", "shuffle", ",", "\n", "sort_within_batch", "=", "True", ",", "device", "=", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.sst.SST.sort_key": [[38, 41], ["len"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "sort_key", "(", "ex", ")", ":", "\n", "        ", "return", "len", "(", "ex", ".", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.sst.SST.splits": [[42, 49], ["os.path.join", "os.path.join", "os.path.join", "super().splits"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.datasets.reuters.Reuters.splits"], ["", "@", "classmethod", "\n", "def", "splits", "(", "cls", ",", "path", ",", "train", "=", "os", ".", "path", ".", "join", "(", "'SST-2'", ",", "'train.tsv'", ")", ",", "\n", "validation", "=", "os", ".", "path", ".", "join", "(", "'SST-2'", ",", "'dev.tsv'", ")", ",", "\n", "test", "=", "os", ".", "path", ".", "join", "(", "'SST-2'", ",", "'test.tsv'", ")", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "super", "(", "SST", ",", "cls", ")", ".", "splits", "(", "\n", "path", ",", "train", "=", "train", ",", "validation", "=", "validation", ",", "test", "=", "test", ",", "\n", "format", "=", "'tsv'", ",", "fields", "=", "[", "(", "'label'", ",", "cls", ".", "LABEL_FIELD", ")", ",", "(", "'text'", ",", "cls", ".", "TEXT_FIELD", ")", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.sst.SST.iters": [[51, 71], ["cls.splits", "cls.TEXT_FIELD.build_vocab", "torchtext.data.iterator.BucketIterator.splits", "torchtext.vocab.Vectors"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.datasets.reuters.Reuters.splits", "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.reuters.Reuters.splits"], ["", "@", "classmethod", "\n", "def", "iters", "(", "cls", ",", "path", ",", "vectors_name", ",", "vectors_cache", ",", "batch_size", "=", "64", ",", "shuffle", "=", "True", ",", "device", "=", "0", ",", "vectors", "=", "None", ",", "\n", "unk_init", "=", "torch", ".", "Tensor", ".", "zero_", ")", ":", "\n", "        ", "\"\"\"\n        :param path: directory containing train, test, dev files\n        :param vectors_name: name of word vectors file\n        :param vectors_cache: path to directory containing word vectors file\n        :param batch_size: batch size\n        :param device: GPU device\n        :param vectors: custom vectors - either predefined torchtext vectors or your own custom Vector classes\n        :param unk_init: function used to generate vector for OOV words\n        :return:\n        \"\"\"", "\n", "if", "vectors", "is", "None", ":", "\n", "            ", "vectors", "=", "Vectors", "(", "name", "=", "vectors_name", ",", "cache", "=", "vectors_cache", ",", "unk_init", "=", "unk_init", ")", "\n", "\n", "", "train", ",", "val", ",", "test", "=", "cls", ".", "splits", "(", "path", ")", "\n", "cls", ".", "TEXT_FIELD", ".", "build_vocab", "(", "train", ",", "val", ",", "test", ",", "vectors", "=", "vectors", ")", "\n", "return", "BucketIterator", ".", "splits", "(", "(", "train", ",", "val", ",", "test", ")", ",", "batch_size", "=", "batch_size", ",", "repeat", "=", "False", ",", "shuffle", "=", "shuffle", ",", "\n", "sort_within_batch", "=", "True", ",", "device", "=", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.sst.SSTCharQuantized.iters": [[77, 88], ["cls.splits", "torchtext.data.iterator.BucketIterator.splits"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.datasets.reuters.Reuters.splits", "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.reuters.Reuters.splits"], ["@", "classmethod", "\n", "def", "iters", "(", "cls", ",", "path", ",", "vectors_name", ",", "vectors_cache", ",", "batch_size", "=", "64", ",", "shuffle", "=", "True", ",", "device", "=", "0", ",", "vectors", "=", "None", ",", "\n", "unk_init", "=", "torch", ".", "Tensor", ".", "zero_", ")", ":", "\n", "        ", "\"\"\"\n        :param path: directory containing train, test, dev files\n        :param batch_size: batch size\n        :param device: GPU device\n        :return:\n        \"\"\"", "\n", "train", ",", "val", ",", "test", "=", "cls", ".", "splits", "(", "path", ")", "\n", "return", "BucketIterator", ".", "splits", "(", "(", "train", ",", "val", ",", "test", ")", ",", "batch_size", "=", "batch_size", ",", "repeat", "=", "False", ",", "shuffle", "=", "shuffle", ",", "device", "=", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.sst.char_quantize": [[12, 19], ["numpy.identity", "numpy.array", "len", "len", "numpy.concatenate", "list", "numpy.zeros", "string.lower", "len", "len"], "function", ["None"], ["def", "char_quantize", "(", "string", ",", "max_length", "=", "500", ")", ":", "\n", "    ", "identity", "=", "np", ".", "identity", "(", "len", "(", "SSTCharQuantized", ".", "ALPHABET", ")", ")", "\n", "quantized_string", "=", "np", ".", "array", "(", "[", "identity", "[", "SSTCharQuantized", ".", "ALPHABET", "[", "char", "]", "]", "for", "char", "in", "list", "(", "string", ".", "lower", "(", ")", ")", "if", "char", "in", "SSTCharQuantized", ".", "ALPHABET", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "if", "len", "(", "quantized_string", ")", ">", "max_length", ":", "\n", "        ", "return", "quantized_string", "[", ":", "max_length", "]", "\n", "", "else", ":", "\n", "        ", "return", "np", ".", "concatenate", "(", "(", "quantized_string", ",", "np", ".", "zeros", "(", "(", "max_length", "-", "len", "(", "quantized_string", ")", ",", "len", "(", "SSTCharQuantized", ".", "ALPHABET", ")", ")", ",", "dtype", "=", "np", ".", "float32", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.sst.process_labels": [[21, 28], ["float"], "function", ["None"], ["", "", "def", "process_labels", "(", "string", ")", ":", "\n", "    ", "\"\"\"\n    Returns the label string as a list of integers\n    :param string:\n    :return:\n    \"\"\"", "\n", "return", "[", "float", "(", "x", ")", "for", "x", "in", "string", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.aapd.AAPD.sort_key": [[38, 41], ["len"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "sort_key", "(", "ex", ")", ":", "\n", "        ", "return", "len", "(", "ex", ".", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.aapd.AAPD.splits": [[42, 49], ["os.path.join", "os.path.join", "os.path.join", "super().splits"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.datasets.reuters.Reuters.splits"], ["", "@", "classmethod", "\n", "def", "splits", "(", "cls", ",", "path", ",", "train", "=", "os", ".", "path", ".", "join", "(", "'AAPD'", ",", "'train.tsv'", ")", ",", "\n", "validation", "=", "os", ".", "path", ".", "join", "(", "'AAPD'", ",", "'dev.tsv'", ")", ",", "\n", "test", "=", "os", ".", "path", ".", "join", "(", "'AAPD'", ",", "'test.tsv'", ")", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "super", "(", "AAPD", ",", "cls", ")", ".", "splits", "(", "\n", "path", ",", "train", "=", "train", ",", "validation", "=", "validation", ",", "test", "=", "test", ",", "\n", "format", "=", "'tsv'", ",", "fields", "=", "[", "(", "'label'", ",", "cls", ".", "LABEL_FIELD", ")", ",", "(", "'text'", ",", "cls", ".", "TEXT_FIELD", ")", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.aapd.AAPD.iters": [[51, 71], ["cls.splits", "cls.TEXT_FIELD.build_vocab", "torchtext.data.iterator.BucketIterator.splits", "torchtext.vocab.Vectors"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.datasets.reuters.Reuters.splits", "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.reuters.Reuters.splits"], ["", "@", "classmethod", "\n", "def", "iters", "(", "cls", ",", "path", ",", "vectors_name", ",", "vectors_cache", ",", "batch_size", "=", "64", ",", "shuffle", "=", "True", ",", "device", "=", "0", ",", "vectors", "=", "None", ",", "\n", "unk_init", "=", "torch", ".", "Tensor", ".", "zero_", ")", ":", "\n", "        ", "\"\"\"\n        :param path: directory containing train, test, dev files\n        :param vectors_name: name of word vectors file\n        :param vectors_cache: path to directory containing word vectors file\n        :param batch_size: batch size\n        :param device: GPU device\n        :param vectors: custom vectors - either predefined torchtext vectors or your own custom Vector classes\n        :param unk_init: function used to generate vector for OOV words\n        :return:\n        \"\"\"", "\n", "if", "vectors", "is", "None", ":", "\n", "            ", "vectors", "=", "Vectors", "(", "name", "=", "vectors_name", ",", "cache", "=", "vectors_cache", ",", "unk_init", "=", "unk_init", ")", "\n", "\n", "", "train", ",", "val", ",", "test", "=", "cls", ".", "splits", "(", "path", ")", "\n", "cls", ".", "TEXT_FIELD", ".", "build_vocab", "(", "train", ",", "val", ",", "test", ",", "vectors", "=", "vectors", ")", "\n", "return", "BucketIterator", ".", "splits", "(", "(", "train", ",", "val", ",", "test", ")", ",", "batch_size", "=", "batch_size", ",", "repeat", "=", "False", ",", "shuffle", "=", "shuffle", ",", "\n", "sort_within_batch", "=", "True", ",", "device", "=", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.aapd.AAPDCharQuantized.iters": [[77, 88], ["cls.splits", "torchtext.data.iterator.BucketIterator.splits"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.datasets.reuters.Reuters.splits", "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.reuters.Reuters.splits"], ["@", "classmethod", "\n", "def", "iters", "(", "cls", ",", "path", ",", "vectors_name", ",", "vectors_cache", ",", "batch_size", "=", "64", ",", "shuffle", "=", "True", ",", "device", "=", "0", ",", "vectors", "=", "None", ",", "\n", "unk_init", "=", "torch", ".", "Tensor", ".", "zero_", ")", ":", "\n", "        ", "\"\"\"\n        :param path: directory containing train, test, dev files\n        :param batch_size: batch size\n        :param device: GPU device\n        :return:\n        \"\"\"", "\n", "train", ",", "val", ",", "test", "=", "cls", ".", "splits", "(", "path", ")", "\n", "return", "BucketIterator", ".", "splits", "(", "(", "train", ",", "val", ",", "test", ")", ",", "batch_size", "=", "batch_size", ",", "repeat", "=", "False", ",", "shuffle", "=", "shuffle", ",", "device", "=", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.aapd.char_quantize": [[12, 19], ["numpy.identity", "numpy.array", "len", "len", "numpy.concatenate", "list", "numpy.zeros", "string.lower", "len", "len"], "function", ["None"], ["def", "char_quantize", "(", "string", ",", "max_length", "=", "1000", ")", ":", "\n", "    ", "identity", "=", "np", ".", "identity", "(", "len", "(", "AAPDCharQuantized", ".", "ALPHABET", ")", ")", "\n", "quantized_string", "=", "np", ".", "array", "(", "[", "identity", "[", "AAPDCharQuantized", ".", "ALPHABET", "[", "char", "]", "]", "for", "char", "in", "list", "(", "string", ".", "lower", "(", ")", ")", "if", "char", "in", "AAPDCharQuantized", ".", "ALPHABET", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "if", "len", "(", "quantized_string", ")", ">", "max_length", ":", "\n", "        ", "return", "quantized_string", "[", ":", "max_length", "]", "\n", "", "else", ":", "\n", "        ", "return", "np", ".", "concatenate", "(", "(", "quantized_string", ",", "np", ".", "zeros", "(", "(", "max_length", "-", "len", "(", "quantized_string", ")", ",", "len", "(", "AAPDCharQuantized", ".", "ALPHABET", ")", ")", ",", "dtype", "=", "np", ".", "float32", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.aapd.process_labels": [[21, 28], ["float"], "function", ["None"], ["", "", "def", "process_labels", "(", "string", ")", ":", "\n", "    ", "\"\"\"\n    Returns the label string as a list of integers\n    :param string:\n    :return:\n    \"\"\"", "\n", "return", "[", "float", "(", "x", ")", "for", "x", "in", "string", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.imdb.IMDB.sort_key": [[38, 41], ["len"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "sort_key", "(", "ex", ")", ":", "\n", "        ", "return", "len", "(", "ex", ".", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.imdb.IMDB.splits": [[42, 49], ["os.path.join", "os.path.join", "os.path.join", "super().splits"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.datasets.reuters.Reuters.splits"], ["", "@", "classmethod", "\n", "def", "splits", "(", "cls", ",", "path", ",", "train", "=", "os", ".", "path", ".", "join", "(", "'IMDB'", ",", "'train.tsv'", ")", ",", "\n", "validation", "=", "os", ".", "path", ".", "join", "(", "'IMDB'", ",", "'dev.tsv'", ")", ",", "\n", "test", "=", "os", ".", "path", ".", "join", "(", "'IMDB'", ",", "'test.tsv'", ")", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "super", "(", "IMDB", ",", "cls", ")", ".", "splits", "(", "\n", "path", ",", "train", "=", "train", ",", "validation", "=", "validation", ",", "test", "=", "test", ",", "\n", "format", "=", "'tsv'", ",", "fields", "=", "[", "(", "'label'", ",", "cls", ".", "LABEL_FIELD", ")", ",", "(", "'text'", ",", "cls", ".", "TEXT_FIELD", ")", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.imdb.IMDB.iters": [[51, 71], ["cls.splits", "cls.TEXT_FIELD.build_vocab", "torchtext.data.iterator.BucketIterator.splits", "torchtext.vocab.Vectors"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.datasets.reuters.Reuters.splits", "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.reuters.Reuters.splits"], ["", "@", "classmethod", "\n", "def", "iters", "(", "cls", ",", "path", ",", "vectors_name", ",", "vectors_cache", ",", "batch_size", "=", "64", ",", "shuffle", "=", "True", ",", "device", "=", "0", ",", "vectors", "=", "None", ",", "\n", "unk_init", "=", "torch", ".", "Tensor", ".", "zero_", ")", ":", "\n", "        ", "\"\"\"\n        :param path: directory containing train, test, dev files\n        :param vectors_name: name of word vectors file\n        :param vectors_cache: path to directory containing word vectors file\n        :param batch_size: batch size\n        :param device: GPU device\n        :param vectors: custom vectors - either predefined torchtext vectors or your own custom Vector classes\n        :param unk_init: function used to generate vector for OOV words\n        :return:\n        \"\"\"", "\n", "if", "vectors", "is", "None", ":", "\n", "            ", "vectors", "=", "Vectors", "(", "name", "=", "vectors_name", ",", "cache", "=", "vectors_cache", ",", "unk_init", "=", "unk_init", ")", "\n", "\n", "", "train", ",", "val", ",", "test", "=", "cls", ".", "splits", "(", "path", ")", "\n", "cls", ".", "TEXT_FIELD", ".", "build_vocab", "(", "train", ",", "val", ",", "test", ",", "vectors", "=", "vectors", ")", "\n", "return", "BucketIterator", ".", "splits", "(", "(", "train", ",", "val", ",", "test", ")", ",", "batch_size", "=", "batch_size", ",", "repeat", "=", "False", ",", "shuffle", "=", "shuffle", ",", "\n", "sort_within_batch", "=", "True", ",", "device", "=", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.imdb.IMDBCharQuantized.iters": [[77, 88], ["cls.splits", "torchtext.data.iterator.BucketIterator.splits"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.datasets.reuters.Reuters.splits", "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.reuters.Reuters.splits"], ["@", "classmethod", "\n", "def", "iters", "(", "cls", ",", "path", ",", "vectors_name", ",", "vectors_cache", ",", "batch_size", "=", "64", ",", "shuffle", "=", "True", ",", "device", "=", "0", ",", "vectors", "=", "None", ",", "\n", "unk_init", "=", "torch", ".", "Tensor", ".", "zero_", ")", ":", "\n", "        ", "\"\"\"\n        :param path: directory containing train, test, dev files\n        :param batch_size: batch size\n        :param device: GPU device\n        :return:\n        \"\"\"", "\n", "train", ",", "val", ",", "test", "=", "cls", ".", "splits", "(", "path", ")", "\n", "return", "BucketIterator", ".", "splits", "(", "(", "train", ",", "val", ",", "test", ")", ",", "batch_size", "=", "batch_size", ",", "repeat", "=", "False", ",", "shuffle", "=", "shuffle", ",", "device", "=", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.imdb.char_quantize": [[12, 19], ["numpy.identity", "numpy.array", "len", "len", "numpy.concatenate", "list", "numpy.zeros", "string.lower", "len", "len"], "function", ["None"], ["def", "char_quantize", "(", "string", ",", "max_length", "=", "500", ")", ":", "\n", "    ", "identity", "=", "np", ".", "identity", "(", "len", "(", "IMDBCharQuantized", ".", "ALPHABET", ")", ")", "\n", "quantized_string", "=", "np", ".", "array", "(", "[", "identity", "[", "IMDBCharQuantized", ".", "ALPHABET", "[", "char", "]", "]", "for", "char", "in", "list", "(", "string", ".", "lower", "(", ")", ")", "if", "char", "in", "IMDBCharQuantized", ".", "ALPHABET", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "if", "len", "(", "quantized_string", ")", ">", "max_length", ":", "\n", "        ", "return", "quantized_string", "[", ":", "max_length", "]", "\n", "", "else", ":", "\n", "        ", "return", "np", ".", "concatenate", "(", "(", "quantized_string", ",", "np", ".", "zeros", "(", "(", "max_length", "-", "len", "(", "quantized_string", ")", ",", "len", "(", "IMDBCharQuantized", ".", "ALPHABET", ")", ")", ",", "dtype", "=", "np", ".", "float32", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.imdb.process_labels": [[21, 28], ["float"], "function", ["None"], ["", "", "def", "process_labels", "(", "string", ")", ":", "\n", "    ", "\"\"\"\n    Returns the label string as a list of integers\n    :param string:\n    :return:\n    \"\"\"", "\n", "return", "[", "float", "(", "x", ")", "for", "x", "in", "string", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.yelp2014.Yelp2014.sort_key": [[39, 42], ["len"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "sort_key", "(", "ex", ")", ":", "\n", "        ", "return", "len", "(", "ex", ".", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.yelp2014.Yelp2014.splits": [[43, 50], ["os.path.join", "os.path.join", "os.path.join", "super().splits"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.datasets.reuters.Reuters.splits"], ["", "@", "classmethod", "\n", "def", "splits", "(", "cls", ",", "path", ",", "train", "=", "os", ".", "path", ".", "join", "(", "'Yelp2014'", ",", "'train.tsv'", ")", ",", "\n", "validation", "=", "os", ".", "path", ".", "join", "(", "'Yelp2014'", ",", "'dev.tsv'", ")", ",", "\n", "test", "=", "os", ".", "path", ".", "join", "(", "'Yelp2014'", ",", "'test.tsv'", ")", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "super", "(", "Yelp2014", ",", "cls", ")", ".", "splits", "(", "\n", "path", ",", "train", "=", "train", ",", "validation", "=", "validation", ",", "test", "=", "test", ",", "\n", "format", "=", "'tsv'", ",", "fields", "=", "[", "(", "'label'", ",", "cls", ".", "LABEL_FIELD", ")", ",", "(", "'text'", ",", "cls", ".", "TEXT_FIELD", ")", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.yelp2014.Yelp2014.iters": [[52, 72], ["cls.splits", "cls.TEXT_FIELD.build_vocab", "torchtext.data.iterator.BucketIterator.splits", "torchtext.vocab.Vectors"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.datasets.reuters.Reuters.splits", "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.reuters.Reuters.splits"], ["", "@", "classmethod", "\n", "def", "iters", "(", "cls", ",", "path", ",", "vectors_name", ",", "vectors_cache", ",", "batch_size", "=", "64", ",", "shuffle", "=", "True", ",", "device", "=", "0", ",", "vectors", "=", "None", ",", "\n", "unk_init", "=", "torch", ".", "Tensor", ".", "zero_", ")", ":", "\n", "        ", "\"\"\"\n        :param path: directory containing train, test, dev files\n        :param vectors_name: name of word vectors file\n        :param vectors_cache: path to directory containing word vectors file\n        :param batch_size: batch size\n        :param device: GPU device\n        :param vectors: custom vectors - either predefined torchtext vectors or your own custom Vector classes\n        :param unk_init: function used to generate vector for OOV words\n        :return:\n        \"\"\"", "\n", "if", "vectors", "is", "None", ":", "\n", "            ", "vectors", "=", "Vectors", "(", "name", "=", "vectors_name", ",", "cache", "=", "vectors_cache", ",", "unk_init", "=", "unk_init", ")", "\n", "\n", "", "train", ",", "val", ",", "test", "=", "cls", ".", "splits", "(", "path", ")", "\n", "cls", ".", "TEXT_FIELD", ".", "build_vocab", "(", "train", ",", "val", ",", "test", ",", "vectors", "=", "vectors", ")", "\n", "return", "BucketIterator", ".", "splits", "(", "(", "train", ",", "val", ",", "test", ")", ",", "batch_size", "=", "batch_size", ",", "repeat", "=", "False", ",", "shuffle", "=", "shuffle", ",", "\n", "sort_within_batch", "=", "True", ",", "device", "=", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.yelp2014.Yelp2014CharQuantized.iters": [[78, 89], ["cls.splits", "torchtext.data.iterator.BucketIterator.splits"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.datasets.reuters.Reuters.splits", "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.reuters.Reuters.splits"], ["@", "classmethod", "\n", "def", "iters", "(", "cls", ",", "path", ",", "vectors_name", ",", "vectors_cache", ",", "batch_size", "=", "64", ",", "shuffle", "=", "True", ",", "device", "=", "0", ",", "vectors", "=", "None", ",", "\n", "unk_init", "=", "torch", ".", "Tensor", ".", "zero_", ")", ":", "\n", "        ", "\"\"\"\n        :param path: directory containing train, test, dev files\n        :param batch_size: batch size\n        :param device: GPU device\n        :return:\n        \"\"\"", "\n", "train", ",", "val", ",", "test", "=", "cls", ".", "splits", "(", "path", ")", "\n", "return", "BucketIterator", ".", "splits", "(", "(", "train", ",", "val", ",", "test", ")", ",", "batch_size", "=", "batch_size", ",", "repeat", "=", "False", ",", "shuffle", "=", "shuffle", ",", "device", "=", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.yelp2014.char_quantize": [[13, 20], ["numpy.identity", "numpy.array", "len", "len", "numpy.concatenate", "list", "numpy.zeros", "string.lower", "len", "len"], "function", ["None"], ["def", "char_quantize", "(", "string", ",", "max_length", "=", "1000", ")", ":", "\n", "    ", "identity", "=", "np", ".", "identity", "(", "len", "(", "Yelp2014CharQuantized", ".", "ALPHABET", ")", ")", "\n", "quantized_string", "=", "np", ".", "array", "(", "[", "identity", "[", "Yelp2014CharQuantized", ".", "ALPHABET", "[", "char", "]", "]", "for", "char", "in", "list", "(", "string", ".", "lower", "(", ")", ")", "if", "char", "in", "Yelp2014CharQuantized", ".", "ALPHABET", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "if", "len", "(", "quantized_string", ")", ">", "max_length", ":", "\n", "        ", "return", "quantized_string", "[", ":", "max_length", "]", "\n", "", "else", ":", "\n", "        ", "return", "np", ".", "concatenate", "(", "(", "quantized_string", ",", "np", ".", "zeros", "(", "(", "max_length", "-", "len", "(", "quantized_string", ")", ",", "len", "(", "Yelp2014CharQuantized", ".", "ALPHABET", ")", ")", ",", "dtype", "=", "np", ".", "float32", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.yelp2014.process_labels": [[22, 29], ["float"], "function", ["None"], ["", "", "def", "process_labels", "(", "string", ")", ":", "\n", "    ", "\"\"\"\n    Returns the label string as a list of integers\n    :param string:\n    :return:\n    \"\"\"", "\n", "return", "[", "float", "(", "x", ")", "for", "x", "in", "string", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.procon.Procon.sort_key": [[38, 41], ["len"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "sort_key", "(", "ex", ")", ":", "\n", "        ", "return", "len", "(", "ex", ".", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.procon.Procon.splits": [[42, 49], ["os.path.join", "os.path.join", "os.path.join", "super().splits"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.datasets.reuters.Reuters.splits"], ["", "@", "classmethod", "\n", "def", "splits", "(", "cls", ",", "path", ",", "train", "=", "os", ".", "path", ".", "join", "(", "'Procon'", ",", "'train.tsv'", ")", ",", "\n", "validation", "=", "os", ".", "path", ".", "join", "(", "'Procon'", ",", "'dev.tsv'", ")", ",", "\n", "test", "=", "os", ".", "path", ".", "join", "(", "'Procon'", ",", "'test.tsv'", ")", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "super", "(", "Procon", ",", "cls", ")", ".", "splits", "(", "\n", "path", ",", "train", "=", "train", ",", "validation", "=", "validation", ",", "test", "=", "test", ",", "\n", "format", "=", "'tsv'", ",", "fields", "=", "[", "(", "'label'", ",", "cls", ".", "LABEL_FIELD", ")", ",", "(", "'text'", ",", "cls", ".", "TEXT_FIELD", ")", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.procon.Procon.iters": [[51, 71], ["cls.splits", "cls.TEXT_FIELD.build_vocab", "torchtext.data.iterator.BucketIterator.splits", "torchtext.vocab.Vectors"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.datasets.reuters.Reuters.splits", "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.reuters.Reuters.splits"], ["", "@", "classmethod", "\n", "def", "iters", "(", "cls", ",", "path", ",", "vectors_name", ",", "vectors_cache", ",", "batch_size", "=", "64", ",", "shuffle", "=", "True", ",", "device", "=", "0", ",", "vectors", "=", "None", ",", "\n", "unk_init", "=", "torch", ".", "Tensor", ".", "zero_", ")", ":", "\n", "        ", "\"\"\"\n        :param path: directory containing train, test, dev files\n        :param vectors_name: name of word vectors file\n        :param vectors_cache: path to directory containing word vectors file\n        :param batch_size: batch size\n        :param device: GPU device\n        :param vectors: custom vectors - either predefined torchtext vectors or your own custom Vector classes\n        :param unk_init: function used to generate vector for OOV words\n        :return:\n        \"\"\"", "\n", "if", "vectors", "is", "None", ":", "\n", "            ", "vectors", "=", "Vectors", "(", "name", "=", "vectors_name", ",", "cache", "=", "vectors_cache", ",", "unk_init", "=", "unk_init", ")", "\n", "\n", "", "train", ",", "val", ",", "test", "=", "cls", ".", "splits", "(", "path", ")", "\n", "cls", ".", "TEXT_FIELD", ".", "build_vocab", "(", "train", ",", "val", ",", "test", ",", "vectors", "=", "vectors", ")", "\n", "return", "BucketIterator", ".", "splits", "(", "(", "train", ",", "val", ",", "test", ")", ",", "batch_size", "=", "batch_size", ",", "repeat", "=", "False", ",", "shuffle", "=", "shuffle", ",", "\n", "sort_within_batch", "=", "True", ",", "device", "=", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.procon.ProconCharQuantized.iters": [[77, 88], ["cls.splits", "torchtext.data.iterator.BucketIterator.splits"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.datasets.reuters.Reuters.splits", "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.reuters.Reuters.splits"], ["@", "classmethod", "\n", "def", "iters", "(", "cls", ",", "path", ",", "vectors_name", ",", "vectors_cache", ",", "batch_size", "=", "64", ",", "shuffle", "=", "True", ",", "device", "=", "0", ",", "vectors", "=", "None", ",", "\n", "unk_init", "=", "torch", ".", "Tensor", ".", "zero_", ")", ":", "\n", "        ", "\"\"\"\n        :param path: directory containing train, test, dev files\n        :param batch_size: batch size\n        :param device: GPU device\n        :return:\n        \"\"\"", "\n", "train", ",", "val", ",", "test", "=", "cls", ".", "splits", "(", "path", ")", "\n", "return", "BucketIterator", ".", "splits", "(", "(", "train", ",", "val", ",", "test", ")", ",", "batch_size", "=", "batch_size", ",", "repeat", "=", "False", ",", "shuffle", "=", "shuffle", ",", "device", "=", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.procon.char_quantize": [[12, 19], ["numpy.identity", "numpy.array", "len", "len", "numpy.concatenate", "list", "numpy.zeros", "string.lower", "len", "len"], "function", ["None"], ["def", "char_quantize", "(", "string", ",", "max_length", "=", "500", ")", ":", "\n", "    ", "identity", "=", "np", ".", "identity", "(", "len", "(", "ProconCharQuantized", ".", "ALPHABET", ")", ")", "\n", "quantized_string", "=", "np", ".", "array", "(", "[", "identity", "[", "ProconCharQuantized", ".", "ALPHABET", "[", "char", "]", "]", "for", "char", "in", "list", "(", "string", ".", "lower", "(", ")", ")", "if", "char", "in", "ProconCharQuantized", ".", "ALPHABET", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "if", "len", "(", "quantized_string", ")", ">", "max_length", ":", "\n", "        ", "return", "quantized_string", "[", ":", "max_length", "]", "\n", "", "else", ":", "\n", "        ", "return", "np", ".", "concatenate", "(", "(", "quantized_string", ",", "np", ".", "zeros", "(", "(", "max_length", "-", "len", "(", "quantized_string", ")", ",", "len", "(", "ProconCharQuantized", ".", "ALPHABET", ")", ")", ",", "dtype", "=", "np", ".", "float32", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.procon.process_labels": [[21, 28], ["float"], "function", ["None"], ["", "", "def", "process_labels", "(", "string", ")", ":", "\n", "    ", "\"\"\"\n    Returns the label string as a list of integers\n    :param string:\n    :return:\n    \"\"\"", "\n", "return", "[", "float", "(", "x", ")", "for", "x", "in", "string", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.robust45.Robust45.sort_key": [[70, 73], ["len"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "sort_key", "(", "ex", ")", ":", "\n", "        ", "return", "len", "(", "ex", ".", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.robust45.Robust45.splits": [[74, 79], ["super().splits"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.datasets.reuters.Reuters.splits"], ["", "@", "classmethod", "\n", "def", "splits", "(", "cls", ",", "path", ",", "train", ",", "validation", ",", "test", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "super", "(", "Robust45", ",", "cls", ")", ".", "splits", "(", "\n", "path", ",", "train", "=", "train", ",", "validation", "=", "validation", ",", "test", "=", "test", ",", "\n", "format", "=", "'tsv'", ",", "fields", "=", "[", "(", "'label'", ",", "cls", ".", "LABEL_FIELD", ")", ",", "(", "'docid'", ",", "cls", ".", "DOCID_FIELD", ")", ",", "(", "'text'", ",", "cls", ".", "TEXT_FIELD", ")", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.robust45.Robust45.iters": [[81, 105], ["os.path.join", "os.path.join", "os.path.join", "cls.splits", "cls.TEXT_FIELD.build_vocab", "torchtext.data.iterator.BucketIterator.splits", "torchtext.vocab.Vectors"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.datasets.reuters.Reuters.splits", "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.reuters.Reuters.splits"], ["", "@", "classmethod", "\n", "def", "iters", "(", "cls", ",", "path", ",", "vectors_name", ",", "vectors_cache", ",", "topic", ",", "batch_size", "=", "64", ",", "shuffle", "=", "True", ",", "device", "=", "0", ",", "\n", "vectors", "=", "None", ",", "unk_init", "=", "torch", ".", "Tensor", ".", "zero_", ")", ":", "\n", "        ", "\"\"\"\n        :param path: directory containing train, test, dev files\n        :param vectors_name: name of word vectors file\n        :param vectors_cache: path to directory containing word vectors file\n        :param topic: topic from which articles should be fetched\n        :param batch_size: batch size\n        :param device: GPU device\n        :param vectors: custom vectors - either predefined torchtext vectors or your own custom Vector classes\n        :param unk_init: function used to generate vector for OOV words\n        :return:\n        \"\"\"", "\n", "if", "vectors", "is", "None", ":", "\n", "            ", "vectors", "=", "Vectors", "(", "name", "=", "vectors_name", ",", "cache", "=", "vectors_cache", ",", "unk_init", "=", "unk_init", ")", "\n", "\n", "", "train_path", "=", "os", ".", "path", ".", "join", "(", "'TREC'", ",", "'robust45_aug_train_%s.tsv'", "%", "topic", ")", "\n", "dev_path", "=", "os", ".", "path", ".", "join", "(", "'TREC'", ",", "'robust45_dev_%s.tsv'", "%", "topic", ")", "\n", "test_path", "=", "os", ".", "path", ".", "join", "(", "'TREC'", ",", "'core17_10k_%s.tsv'", "%", "topic", ")", "\n", "train", ",", "val", ",", "test", "=", "cls", ".", "splits", "(", "path", ",", "train", "=", "train_path", ",", "validation", "=", "dev_path", ",", "test", "=", "test_path", ")", "\n", "cls", ".", "TEXT_FIELD", ".", "build_vocab", "(", "train", ",", "val", ",", "test", ",", "vectors", "=", "vectors", ")", "\n", "return", "BucketIterator", ".", "splits", "(", "(", "train", ",", "val", ",", "test", ")", ",", "batch_size", "=", "batch_size", ",", "repeat", "=", "False", ",", "shuffle", "=", "shuffle", ",", "\n", "sort_within_batch", "=", "True", ",", "device", "=", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.robust45.Robust45Hierarchical.clean_sentence": [[108, 111], ["robust45.clean_string"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.datasets.reuters.clean_string"], ["    ", "@", "staticmethod", "\n", "def", "clean_sentence", "(", "string", ")", ":", "\n", "        ", "return", "clean_string", "(", "string", ",", "sentence_droprate", "=", "0", ",", "max_length", "=", "100", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.robust45.clean_string": [[16, 29], ["re.sub", "re.sub", "re.sub.lower().strip().split", "re.sub.lower().strip", "min", "nltk.tokenize.sent_tokenize", "len", "len", "random.randint", "len", "re.sub.lower"], "function", ["None"], ["def", "clean_string", "(", "string", ",", "sentence_droprate", "=", "0", ",", "max_length", "=", "5000", ")", ":", "\n", "    ", "\"\"\"\n    Performs tokenization and string cleaning\n    \"\"\"", "\n", "if", "sentence_droprate", ">", "0", ":", "\n", "        ", "lines", "=", "[", "x", "for", "x", "in", "tokenize", ".", "sent_tokenize", "(", "string", ")", "if", "len", "(", "x", ")", ">", "1", "]", "\n", "lines_drop", "=", "[", "x", "for", "x", "in", "lines", "if", "random", ".", "randint", "(", "0", ",", "100", ")", ">", "100", "*", "sentence_droprate", "]", "\n", "string", "=", "' '", ".", "join", "(", "lines_drop", "if", "len", "(", "lines_drop", ")", ">", "0", "else", "lines", ")", "\n", "\n", "", "string", "=", "re", ".", "sub", "(", "r'[^A-Za-z0-9]'", ",", "' '", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r'\\s{2,}'", ",", "' '", ",", "string", ")", "\n", "tokenized_string", "=", "string", ".", "lower", "(", ")", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "return", "tokenized_string", "[", ":", "min", "(", "max_length", ",", "len", "(", "tokenized_string", ")", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.robust45.split_sents": [[31, 34], ["nltk.tokenize.sent_tokenize", "min", "len", "len"], "function", ["None"], ["", "def", "split_sents", "(", "string", ",", "max_length", "=", "50", ")", ":", "\n", "    ", "tokenized_string", "=", "[", "x", "for", "x", "in", "tokenize", ".", "sent_tokenize", "(", "string", ")", "if", "len", "(", "x", ")", ">", "1", "]", "\n", "return", "tokenized_string", "[", ":", "min", "(", "max_length", ",", "len", "(", "tokenized_string", ")", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.robust45.process_labels": [[36, 43], ["None"], "function", ["None"], ["", "def", "process_labels", "(", "string", ")", ":", "\n", "    ", "\"\"\"\n    Returns the label string as a list of integers\n    :param string:\n    :return:\n    \"\"\"", "\n", "return", "0", "if", "string", "==", "'01'", "else", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.robust45.process_docids": [[45, 57], ["int"], "function", ["None"], ["", "def", "process_docids", "(", "string", ")", ":", "\n", "    ", "\"\"\"\n    Returns the docid as an integer\n    :param string:\n    :return:\n    \"\"\"", "\n", "try", ":", "\n", "        ", "docid", "=", "int", "(", "string", ")", "\n", "", "except", "ValueError", ":", "\n", "# print(\"Error converting docid to integer:\", string)", "\n", "        ", "docid", "=", "0", "\n", "", "return", "docid", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.robust05.Robust05.sort_key": [[25, 28], ["len"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "sort_key", "(", "ex", ")", ":", "\n", "        ", "return", "len", "(", "ex", ".", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.robust05.Robust05.splits": [[29, 34], ["super().splits"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.datasets.reuters.Reuters.splits"], ["", "@", "classmethod", "\n", "def", "splits", "(", "cls", ",", "path", ",", "train", ",", "validation", ",", "test", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "super", "(", "Robust05", ",", "cls", ")", ".", "splits", "(", "\n", "path", ",", "train", "=", "train", ",", "validation", "=", "validation", ",", "test", "=", "test", ",", "\n", "format", "=", "'tsv'", ",", "fields", "=", "[", "(", "'label'", ",", "cls", ".", "LABEL_FIELD", ")", ",", "(", "'docid'", ",", "cls", ".", "DOCID_FIELD", ")", ",", "(", "'text'", ",", "cls", ".", "TEXT_FIELD", ")", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.robust05.Robust05.iters": [[36, 60], ["os.path.join", "os.path.join", "os.path.join", "cls.splits", "cls.TEXT_FIELD.build_vocab", "torchtext.data.iterator.BucketIterator.splits", "torchtext.vocab.Vectors"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.datasets.reuters.Reuters.splits", "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.reuters.Reuters.splits"], ["", "@", "classmethod", "\n", "def", "iters", "(", "cls", ",", "path", ",", "vectors_name", ",", "vectors_cache", ",", "topic", ",", "batch_size", "=", "64", ",", "shuffle", "=", "True", ",", "device", "=", "0", ",", "\n", "vectors", "=", "None", ",", "unk_init", "=", "torch", ".", "Tensor", ".", "zero_", ")", ":", "\n", "        ", "\"\"\"\n        :param path: directory containing train, test, dev files\n        :param vectors_name: name of word vectors file\n        :param vectors_cache: path to directory containing word vectors file\n        :param topic: topic from which articles should be fetched\n        :param batch_size: batch size\n        :param device: GPU device\n        :param vectors: custom vectors - either predefined torchtext vectors or your own custom Vector classes\n        :param unk_init: function used to generate vector for OOV words\n        :return:\n        \"\"\"", "\n", "if", "vectors", "is", "None", ":", "\n", "            ", "vectors", "=", "Vectors", "(", "name", "=", "vectors_name", ",", "cache", "=", "vectors_cache", ",", "unk_init", "=", "unk_init", ")", "\n", "\n", "", "train_path", "=", "os", ".", "path", ".", "join", "(", "'TREC'", ",", "'robust05_train_%s.tsv'", "%", "topic", ")", "\n", "dev_path", "=", "os", ".", "path", ".", "join", "(", "'TREC'", ",", "'robust05_dev_%s.tsv'", "%", "topic", ")", "\n", "test_path", "=", "os", ".", "path", ".", "join", "(", "'TREC'", ",", "'core17_%s.tsv'", "%", "topic", ")", "\n", "train", ",", "val", ",", "test", "=", "cls", ".", "splits", "(", "path", ",", "train", "=", "train_path", ",", "validation", "=", "dev_path", ",", "test", "=", "test_path", ")", "\n", "cls", ".", "TEXT_FIELD", ".", "build_vocab", "(", "train", ",", "val", ",", "test", ",", "vectors", "=", "vectors", ")", "\n", "return", "BucketIterator", ".", "splits", "(", "(", "train", ",", "val", ",", "test", ")", ",", "batch_size", "=", "batch_size", ",", "repeat", "=", "False", ",", "shuffle", "=", "shuffle", ",", "\n", "sort_within_batch", "=", "True", ",", "device", "=", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.reuters.Reuters.sort_key": [[51, 54], ["len"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "sort_key", "(", "ex", ")", ":", "\n", "        ", "return", "len", "(", "ex", ".", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.reuters.Reuters.splits": [[55, 62], ["os.path.join", "os.path.join", "os.path.join", "super().splits"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.datasets.reuters.Reuters.splits"], ["", "@", "classmethod", "\n", "def", "splits", "(", "cls", ",", "path", ",", "train", "=", "os", ".", "path", ".", "join", "(", "'Reuters'", ",", "'train.tsv'", ")", ",", "\n", "validation", "=", "os", ".", "path", ".", "join", "(", "'Reuters'", ",", "'dev.tsv'", ")", ",", "\n", "test", "=", "os", ".", "path", ".", "join", "(", "'Reuters'", ",", "'test.tsv'", ")", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "super", "(", "Reuters", ",", "cls", ")", ".", "splits", "(", "\n", "path", ",", "train", "=", "train", ",", "validation", "=", "validation", ",", "test", "=", "test", ",", "\n", "format", "=", "'tsv'", ",", "fields", "=", "[", "(", "'label'", ",", "cls", ".", "LABEL_FIELD", ")", ",", "(", "'text'", ",", "cls", ".", "TEXT_FIELD", ")", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.reuters.Reuters.iters": [[64, 84], ["cls.splits", "cls.TEXT_FIELD.build_vocab", "torchtext.data.iterator.BucketIterator.splits", "torchtext.vocab.Vectors"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.datasets.reuters.Reuters.splits", "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.reuters.Reuters.splits"], ["", "@", "classmethod", "\n", "def", "iters", "(", "cls", ",", "path", ",", "vectors_name", ",", "vectors_cache", ",", "batch_size", "=", "64", ",", "shuffle", "=", "True", ",", "device", "=", "0", ",", "vectors", "=", "None", ",", "\n", "unk_init", "=", "torch", ".", "Tensor", ".", "zero_", ")", ":", "\n", "        ", "\"\"\"\n        :param path: directory containing train, test, dev files\n        :param vectors_name: name of word vectors file\n        :param vectors_cache: path to directory containing word vectors file\n        :param batch_size: batch size\n        :param device: GPU device\n        :param vectors: custom vectors - either predefined torchtext vectors or your own custom Vector classes\n        :param unk_init: function used to generate vector for OOV words\n        :return:\n        \"\"\"", "\n", "if", "vectors", "is", "None", ":", "\n", "            ", "vectors", "=", "Vectors", "(", "name", "=", "vectors_name", ",", "cache", "=", "vectors_cache", ",", "unk_init", "=", "unk_init", ")", "\n", "\n", "", "train", ",", "val", ",", "test", "=", "cls", ".", "splits", "(", "path", ")", "\n", "cls", ".", "TEXT_FIELD", ".", "build_vocab", "(", "train", ",", "val", ",", "test", ",", "vectors", "=", "vectors", ")", "\n", "return", "BucketIterator", ".", "splits", "(", "(", "train", ",", "val", ",", "test", ")", ",", "batch_size", "=", "batch_size", ",", "repeat", "=", "False", ",", "shuffle", "=", "shuffle", ",", "\n", "sort_within_batch", "=", "True", ",", "device", "=", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.reuters.ReutersCharQuantized.iters": [[90, 101], ["cls.splits", "torchtext.data.iterator.BucketIterator.splits"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.datasets.reuters.Reuters.splits", "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.reuters.Reuters.splits"], ["@", "classmethod", "\n", "def", "iters", "(", "cls", ",", "path", ",", "vectors_name", ",", "vectors_cache", ",", "batch_size", "=", "64", ",", "shuffle", "=", "True", ",", "device", "=", "0", ",", "vectors", "=", "None", ",", "\n", "unk_init", "=", "torch", ".", "Tensor", ".", "zero_", ")", ":", "\n", "        ", "\"\"\"\n        :param path: directory containing train, test, dev files\n        :param batch_size: batch size\n        :param device: GPU device\n        :return:\n        \"\"\"", "\n", "train", ",", "val", ",", "test", "=", "cls", ".", "splits", "(", "path", ")", "\n", "return", "BucketIterator", ".", "splits", "(", "(", "train", ",", "val", ",", "test", ")", ",", "batch_size", "=", "batch_size", ",", "repeat", "=", "False", ",", "shuffle", "=", "shuffle", ",", "device", "=", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.reuters.clean_string": [[11, 18], ["re.sub", "re.sub", "re.sub.lower().strip().split", "re.sub.lower().strip", "re.sub.lower"], "function", ["None"], ["def", "clean_string", "(", "string", ")", ":", "\n", "    ", "\"\"\"\n    Performs tokenization and string cleaning for the Reuters dataset\n    \"\"\"", "\n", "string", "=", "re", ".", "sub", "(", "r\"[^A-Za-z0-9(),!?\\'`]\"", ",", "\" \"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\s{2,}\"", ",", "\" \"", ",", "string", ")", "\n", "return", "string", ".", "lower", "(", ")", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.reuters.split_sents": [[20, 23], ["re.sub", "re.sub.strip().split", "re.sub.strip"], "function", ["None"], ["", "def", "split_sents", "(", "string", ")", ":", "\n", "    ", "string", "=", "re", ".", "sub", "(", "r\"[!?]\"", ",", "\" \"", ",", "string", ")", "\n", "return", "string", ".", "strip", "(", ")", ".", "split", "(", "'.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.reuters.char_quantize": [[25, 32], ["numpy.identity", "numpy.array", "len", "len", "numpy.concatenate", "list", "numpy.zeros", "string.lower", "len", "len"], "function", ["None"], ["", "def", "char_quantize", "(", "string", ",", "max_length", "=", "1000", ")", ":", "\n", "    ", "identity", "=", "np", ".", "identity", "(", "len", "(", "ReutersCharQuantized", ".", "ALPHABET", ")", ")", "\n", "quantized_string", "=", "np", ".", "array", "(", "[", "identity", "[", "ReutersCharQuantized", ".", "ALPHABET", "[", "char", "]", "]", "for", "char", "in", "list", "(", "string", ".", "lower", "(", ")", ")", "if", "char", "in", "ReutersCharQuantized", ".", "ALPHABET", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "if", "len", "(", "quantized_string", ")", ">", "max_length", ":", "\n", "        ", "return", "quantized_string", "[", ":", "max_length", "]", "\n", "", "else", ":", "\n", "        ", "return", "np", ".", "concatenate", "(", "(", "quantized_string", ",", "np", ".", "zeros", "(", "(", "max_length", "-", "len", "(", "quantized_string", ")", ",", "len", "(", "ReutersCharQuantized", ".", "ALPHABET", ")", ")", ",", "dtype", "=", "np", ".", "float32", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.datasets.reuters.process_labels": [[34, 41], ["float"], "function", ["None"], ["", "", "def", "process_labels", "(", "string", ")", ":", "\n", "    ", "\"\"\"\n    Returns the label string as a list of integers\n    :param string:\n    :return:\n    \"\"\"", "\n", "return", "[", "float", "(", "x", ")", "for", "x", "in", "string", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.sogou_processor.SogouProcessor.get_train_examples": [[11, 14], ["sogou_processor.SogouProcessor._create_examples", "sogou_processor.SogouProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.agnews_processor.AGNewsProcessor._create_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor._read_tsv"], ["def", "get_train_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'Sogou'", ",", "'train.tsv'", ")", ")", ",", "'train'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.sogou_processor.SogouProcessor.get_dev_examples": [[15, 18], ["sogou_processor.SogouProcessor._create_examples", "sogou_processor.SogouProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.agnews_processor.AGNewsProcessor._create_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor._read_tsv"], ["", "def", "get_dev_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'Sogou'", ",", "'dev.tsv'", ")", ")", ",", "'dev'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.sogou_processor.SogouProcessor.get_test_examples": [[19, 22], ["sogou_processor.SogouProcessor._create_examples", "sogou_processor.SogouProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.agnews_processor.AGNewsProcessor._create_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor._read_tsv"], ["", "def", "get_test_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'Sogou'", ",", "'test.tsv'", ")", ")", ",", "'test'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.sogou_processor.SogouProcessor._create_examples": [[23, 35], ["enumerate", "examples.append", "datasets.bert_processors.abstract_processor.InputExample"], "methods", ["None"], ["", "def", "_create_examples", "(", "self", ",", "lines", ",", "set_type", ")", ":", "\n", "        ", "\"\"\"Creates examples for the training and dev sets.\"\"\"", "\n", "examples", "=", "[", "]", "\n", "for", "(", "i", ",", "line", ")", "in", "enumerate", "(", "lines", ")", ":", "\n", "            ", "if", "i", "==", "0", ":", "\n", "                ", "continue", "\n", "", "guid", "=", "\"%s-%s\"", "%", "(", "set_type", ",", "i", ")", "\n", "text_a", "=", "line", "[", "1", "]", "\n", "label", "=", "line", "[", "0", "]", "\n", "examples", ".", "append", "(", "\n", "InputExample", "(", "guid", "=", "guid", ",", "text_a", "=", "text_a", ",", "text_b", "=", "None", ",", "label", "=", "label", ")", ")", "\n", "", "return", "examples", "\n", "", "", ""]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.perspectrum_processor.PerspectrumProcessor.get_train_examples": [[11, 15], ["perspectrum_processor.PerspectrumProcessor._create_examples", "perspectrum_processor.PerspectrumProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.agnews_processor.AGNewsProcessor._create_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor._read_tsv"], ["def", "get_train_examples", "(", "self", ",", "data_dir", ",", "name", "=", "None", ")", ":", "\n", "        ", "name", "=", "'train.tsv'", "if", "not", "name", "else", "name", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'Perspectrum'", ",", "name", ")", ")", ",", "'train'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.perspectrum_processor.PerspectrumProcessor.get_dev_examples": [[16, 20], ["perspectrum_processor.PerspectrumProcessor._create_examples", "perspectrum_processor.PerspectrumProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.agnews_processor.AGNewsProcessor._create_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor._read_tsv"], ["", "def", "get_dev_examples", "(", "self", ",", "data_dir", ",", "name", "=", "None", ")", ":", "\n", "        ", "name", "=", "'dev.tsv'", "if", "not", "name", "else", "name", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'Perspectrum'", ",", "name", ")", ")", ",", "'dev'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.perspectrum_processor.PerspectrumProcessor.get_test_examples": [[21, 25], ["perspectrum_processor.PerspectrumProcessor._create_examples", "perspectrum_processor.PerspectrumProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.agnews_processor.AGNewsProcessor._create_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor._read_tsv"], ["", "def", "get_test_examples", "(", "self", ",", "data_dir", ",", "name", "=", "None", ")", ":", "\n", "        ", "name", "=", "'test.tsv'", "if", "not", "name", "else", "name", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'Perspectrum'", ",", "name", ")", ")", ",", "'test'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.perspectrum_processor.PerspectrumProcessor.get_any_examples": [[26, 29], ["perspectrum_processor.PerspectrumProcessor._create_examples", "perspectrum_processor.PerspectrumProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.agnews_processor.AGNewsProcessor._create_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor._read_tsv"], ["", "def", "get_any_examples", "(", "self", ",", "data_dir", ",", "split", ")", ":", "\n", "        ", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'Perspectrum'", ",", "split", ")", ")", ",", "split", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.perspectrum_processor.PerspectrumProcessor._create_examples": [[30, 43], ["enumerate", "examples.append", "datasets.bert_processors.abstract_processor.InputExample"], "methods", ["None"], ["", "def", "_create_examples", "(", "self", ",", "lines", ",", "set_type", ")", ":", "\n", "        ", "\"\"\"Creates examples for the training and dev sets.\"\"\"", "\n", "examples", "=", "[", "]", "\n", "for", "(", "i", ",", "line", ")", "in", "enumerate", "(", "lines", ")", ":", "\n", "            ", "if", "i", "==", "0", ":", "\n", "                ", "continue", "\n", "", "guid", "=", "\"%s-%s\"", "%", "(", "set_type", ",", "i", ")", "\n", "text_a", "=", "line", "[", "1", "]", "#question/perspective", "\n", "label", "=", "line", "[", "0", "]", "\n", "text_b", "=", "line", "[", "2", "]", "#opinion/claim", "\n", "examples", ".", "append", "(", "\n", "InputExample", "(", "guid", "=", "guid", ",", "text_a", "=", "text_a", ",", "text_b", "=", "text_b", ",", "label", "=", "label", ")", ")", "\n", "", "return", "examples", "\n", "", "", ""]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.sst_processor.SST2Processor.get_train_examples": [[11, 14], ["sst_processor.SST2Processor._create_examples", "sst_processor.SST2Processor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.agnews_processor.AGNewsProcessor._create_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor._read_tsv"], ["def", "get_train_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'SST-2'", ",", "'train.tsv'", ")", ")", ",", "'train'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.sst_processor.SST2Processor.get_dev_examples": [[15, 18], ["sst_processor.SST2Processor._create_examples", "sst_processor.SST2Processor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.agnews_processor.AGNewsProcessor._create_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor._read_tsv"], ["", "def", "get_dev_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'SST-2'", ",", "'dev.tsv'", ")", ")", ",", "'dev'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.sst_processor.SST2Processor.get_test_examples": [[19, 22], ["sst_processor.SST2Processor._create_examples", "sst_processor.SST2Processor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.agnews_processor.AGNewsProcessor._create_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor._read_tsv"], ["", "def", "get_test_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'SST-2'", ",", "'test.tsv'", ")", ")", ",", "'test'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.sst_processor.SST2Processor._create_examples": [[23, 40], ["enumerate", "examples.append", "datasets.bert_processors.abstract_processor.InputExample"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_create_examples", "(", "lines", ",", "set_type", ")", ":", "\n", "        ", "\"\"\"\n        Creates examples for the training and dev sets\n        :param lines:\n        :param set_type:\n        :return:\n        \"\"\"", "\n", "examples", "=", "[", "]", "\n", "for", "(", "i", ",", "line", ")", "in", "enumerate", "(", "lines", ")", ":", "\n", "            ", "if", "i", "==", "0", ":", "\n", "                ", "continue", "\n", "", "guid", "=", "'%s-%s'", "%", "(", "set_type", ",", "i", ")", "\n", "label", "=", "line", "[", "0", "]", "\n", "text", "=", "line", "[", "1", "]", "\n", "examples", ".", "append", "(", "InputExample", "(", "guid", "=", "guid", ",", "text_a", "=", "text", ",", "text_b", "=", "None", ",", "label", "=", "label", ")", ")", "\n", "", "return", "examples", "\n", "", "", ""]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.aapd_processor.AAPDProcessor.get_train_examples": [[11, 14], ["aapd_processor.AAPDProcessor._create_examples", "aapd_processor.AAPDProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.agnews_processor.AGNewsProcessor._create_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor._read_tsv"], ["def", "get_train_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'AAPD'", ",", "'train.tsv'", ")", ")", ",", "'train'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.aapd_processor.AAPDProcessor.get_dev_examples": [[15, 18], ["aapd_processor.AAPDProcessor._create_examples", "aapd_processor.AAPDProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.agnews_processor.AGNewsProcessor._create_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor._read_tsv"], ["", "def", "get_dev_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'AAPD'", ",", "'dev.tsv'", ")", ")", ",", "'dev'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.aapd_processor.AAPDProcessor.get_test_examples": [[19, 22], ["aapd_processor.AAPDProcessor._create_examples", "aapd_processor.AAPDProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.agnews_processor.AGNewsProcessor._create_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor._read_tsv"], ["", "def", "get_test_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'AAPD'", ",", "'test.tsv'", ")", ")", ",", "'test'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.aapd_processor.AAPDProcessor._create_examples": [[23, 34], ["enumerate", "examples.append", "datasets.bert_processors.abstract_processor.InputExample"], "methods", ["None"], ["", "def", "_create_examples", "(", "self", ",", "lines", ",", "set_type", ")", ":", "\n", "        ", "examples", "=", "[", "]", "\n", "for", "(", "i", ",", "line", ")", "in", "enumerate", "(", "lines", ")", ":", "\n", "            ", "if", "i", "==", "0", ":", "\n", "                ", "continue", "\n", "", "guid", "=", "\"%s-%s\"", "%", "(", "set_type", ",", "i", ")", "\n", "text_a", "=", "line", "[", "1", "]", "\n", "label", "=", "line", "[", "0", "]", "\n", "examples", ".", "append", "(", "\n", "InputExample", "(", "guid", "=", "guid", ",", "text_a", "=", "text_a", ",", "text_b", "=", "None", ",", "label", "=", "label", ")", ")", "\n", "", "return", "examples", "\n", "", "", ""]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.reuters_processor.ReutersProcessor.get_train_examples": [[11, 14], ["reuters_processor.ReutersProcessor._create_examples", "reuters_processor.ReutersProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.agnews_processor.AGNewsProcessor._create_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor._read_tsv"], ["def", "get_train_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'Reuters'", ",", "'train.tsv'", ")", ")", ",", "'train'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.reuters_processor.ReutersProcessor.get_dev_examples": [[15, 18], ["reuters_processor.ReutersProcessor._create_examples", "reuters_processor.ReutersProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.agnews_processor.AGNewsProcessor._create_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor._read_tsv"], ["", "def", "get_dev_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'Reuters'", ",", "'dev.tsv'", ")", ")", ",", "'dev'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.reuters_processor.ReutersProcessor.get_test_examples": [[19, 22], ["reuters_processor.ReutersProcessor._create_examples", "reuters_processor.ReutersProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.agnews_processor.AGNewsProcessor._create_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor._read_tsv"], ["", "def", "get_test_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'Reuters'", ",", "'test.tsv'", ")", ")", ",", "'test'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.reuters_processor.ReutersProcessor._create_examples": [[23, 34], ["enumerate", "examples.append", "datasets.bert_processors.abstract_processor.InputExample"], "methods", ["None"], ["", "def", "_create_examples", "(", "self", ",", "lines", ",", "set_type", ")", ":", "\n", "        ", "examples", "=", "[", "]", "\n", "for", "(", "i", ",", "line", ")", "in", "enumerate", "(", "lines", ")", ":", "\n", "            ", "if", "i", "==", "0", ":", "\n", "                ", "continue", "\n", "", "guid", "=", "'%s-%s'", "%", "(", "set_type", ",", "i", ")", "\n", "text_a", "=", "line", "[", "1", "]", "\n", "label", "=", "line", "[", "0", "]", "\n", "examples", ".", "append", "(", "\n", "InputExample", "(", "guid", "=", "guid", ",", "text_a", "=", "text_a", ",", "text_b", "=", "None", ",", "label", "=", "label", ")", ")", "\n", "", "return", "examples", "", "", "", ""]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.robust45_processor.RelevanceFeatures.__init__": [[11, 14], ["datasets.bert_processors.abstract_processor.InputFeatures.__init__"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.classification_evaluator.ClassificationEvaluator.__init__"], ["def", "__init__", "(", "self", ",", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "label_id", ",", "guid", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "label_id", ")", "\n", "self", ".", "guid", "=", "guid", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.robust45_processor.Robust45Processor.get_train_examples": [[24, 27], ["robust45_processor.Robust45Processor._create_examples", "robust45_processor.Robust45Processor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.agnews_processor.AGNewsProcessor._create_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor._read_tsv"], ["def", "get_train_examples", "(", "self", ",", "data_dir", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'TREC'", ",", "'robust45_aug_train_%s.tsv'", "%", "kwargs", "[", "'topic'", "]", ")", ")", ",", "'train'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.robust45_processor.Robust45Processor.get_dev_examples": [[28, 31], ["robust45_processor.Robust45Processor._create_examples", "robust45_processor.Robust45Processor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.agnews_processor.AGNewsProcessor._create_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor._read_tsv"], ["", "def", "get_dev_examples", "(", "self", ",", "data_dir", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'TREC'", ",", "'robust45_dev_%s.tsv'", "%", "kwargs", "[", "'topic'", "]", ")", ")", ",", "'dev'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.robust45_processor.Robust45Processor.get_test_examples": [[32, 35], ["robust45_processor.Robust45Processor._create_examples", "robust45_processor.Robust45Processor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.agnews_processor.AGNewsProcessor._create_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor._read_tsv"], ["", "def", "get_test_examples", "(", "self", ",", "data_dir", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'TREC'", ",", "'core17_10k_%s.tsv'", "%", "kwargs", "[", "'topic'", "]", ")", ")", ",", "'test'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.robust45_processor.Robust45Processor._create_examples": [[36, 49], ["enumerate", "examples.append", "datasets.bert_processors.abstract_processor.InputExample"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_create_examples", "(", "lines", ",", "split", ")", ":", "\n", "        ", "\"\"\"Creates examples for the training and dev sets.\"\"\"", "\n", "examples", "=", "[", "]", "\n", "for", "(", "i", ",", "line", ")", "in", "enumerate", "(", "lines", ")", ":", "\n", "            ", "if", "i", "==", "0", ":", "\n", "                ", "continue", "\n", "", "text_a", "=", "line", "[", "2", "]", "\n", "guid", "=", "line", "[", "1", "]", "\n", "label", "=", "line", "[", "0", "]", "\n", "examples", ".", "append", "(", "\n", "InputExample", "(", "guid", "=", "guid", ",", "text_a", "=", "text_a", ",", "text_b", "=", "None", ",", "label", "=", "label", ")", ")", "\n", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.robust45_processor.convert_examples_to_features": [[51, 121], ["enumerate", "features.append", "range", "list", "range", "tokenizer.tokenize", "tokenizer.convert_tokens_to_ids", "int", "robust45_processor.RelevanceFeatures", "tokenizer.tokenize", "len", "tokenizer.convert_tokens_to_ids.append", "len", "len", "len", "len", "nltk.sent_tokenize", "len", "len", "tokenizer.convert_tokens_to_ids", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.BertTokenizer.convert_tokens_to_ids", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.BertTokenizer.convert_tokens_to_ids"], ["", "", "def", "convert_examples_to_features", "(", "examples", ",", "max_seq_length", ",", "tokenizer", ",", "is_hierarchical", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Loads a data file into a list of InputBatch objects\n    :param is_hierarchical:\n    :param examples:\n    :param max_seq_length:\n    :param tokenizer:\n    :return: a list of InputBatch objects\n    \"\"\"", "\n", "\n", "features", "=", "[", "]", "\n", "for", "(", "ex_index", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "        ", "if", "is_hierarchical", ":", "\n", "            ", "tokens_a", "=", "[", "tokenizer", ".", "tokenize", "(", "line", ")", "for", "line", "in", "sent_tokenize", "(", "example", ".", "text_a", ")", "]", "\n", "\n", "# Account for [CLS] and [SEP]", "\n", "for", "i0", "in", "range", "(", "len", "(", "tokens_a", ")", ")", ":", "\n", "                ", "if", "len", "(", "tokens_a", "[", "i0", "]", ")", ">", "max_seq_length", "-", "2", ":", "\n", "                    ", "tokens_a", "[", "i0", "]", "=", "tokens_a", "[", "i0", "]", "[", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "\n", "", "", "tokens", "=", "[", "[", "\"[CLS]\"", "]", "+", "line", "+", "[", "\"[SEP]\"", "]", "for", "line", "in", "tokens_a", "]", "\n", "segment_ids", "=", "[", "[", "0", "]", "*", "len", "(", "line", ")", "for", "line", "in", "tokens", "]", "\n", "\n", "input_ids", "=", "list", "(", ")", "\n", "for", "line", "in", "tokens", ":", "\n", "                ", "input_ids", ".", "append", "(", "tokenizer", ".", "convert_tokens_to_ids", "(", "line", ")", ")", "\n", "\n", "# Input mask has 1 for real tokens and 0 for padding tokens", "\n", "", "input_mask", "=", "[", "[", "1", "]", "*", "len", "(", "line_ids", ")", "for", "line_ids", "in", "input_ids", "]", "\n", "\n", "# Zero-pad up to the sequence length.", "\n", "padding", "=", "[", "[", "0", "]", "*", "(", "max_seq_length", "-", "len", "(", "line_ids", ")", ")", "for", "line_ids", "in", "input_ids", "]", "\n", "for", "i0", "in", "range", "(", "len", "(", "input_ids", ")", ")", ":", "\n", "                ", "input_ids", "[", "i0", "]", "+=", "padding", "[", "i0", "]", "\n", "input_mask", "[", "i0", "]", "+=", "padding", "[", "i0", "]", "\n", "segment_ids", "[", "i0", "]", "+=", "padding", "[", "i0", "]", "\n", "\n", "", "", "else", ":", "\n", "            ", "tokens_a", "=", "tokenizer", ".", "tokenize", "(", "example", ".", "text_a", ")", "\n", "\n", "# Account for [CLS] and [SEP] with \"- 2\"", "\n", "if", "len", "(", "tokens_a", ")", ">", "max_seq_length", "-", "2", ":", "\n", "                ", "tokens_a", "=", "tokens_a", "[", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "\n", "", "tokens", "=", "[", "\"[CLS]\"", "]", "+", "tokens_a", "+", "[", "\"[SEP]\"", "]", "\n", "segment_ids", "=", "[", "0", "]", "*", "len", "(", "tokens", ")", "\n", "\n", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "\n", "# The mask has 1 for real tokens and 0 for padding tokens", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "# Zero-pad up to the sequence length", "\n", "padding", "=", "[", "0", "]", "*", "(", "max_seq_length", "-", "len", "(", "input_ids", ")", ")", "\n", "input_ids", "+=", "padding", "\n", "input_mask", "+=", "padding", "\n", "segment_ids", "+=", "padding", "\n", "\n", "", "try", ":", "\n", "            ", "docid", "=", "int", "(", "example", ".", "guid", ")", "\n", "", "except", "ValueError", ":", "\n", "# print(\"Error converting docid to integer:\", string)", "\n", "            ", "docid", "=", "0", "\n", "\n", "", "features", ".", "append", "(", "RelevanceFeatures", "(", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "segment_ids", "=", "segment_ids", ",", "\n", "label_id", "=", "0", "if", "example", ".", "label", "==", "'01'", "else", "1", ",", "\n", "guid", "=", "docid", ")", ")", "\n", "", "return", "features", "\n", "", ""]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.procondual_processor.ProconDualProcessor.get_train_examples": [[11, 15], ["procondual_processor.ProconDualProcessor._create_examples", "procondual_processor.ProconDualProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.agnews_processor.AGNewsProcessor._create_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor._read_tsv"], ["def", "get_train_examples", "(", "self", ",", "data_dir", ",", "name", "=", "None", ")", ":", "\n", "        ", "name", "=", "'train.tsv'", "if", "not", "name", "else", "name", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'ProconDual'", ",", "name", ")", ")", ",", "'train'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.procondual_processor.ProconDualProcessor.get_dev_examples": [[16, 20], ["procondual_processor.ProconDualProcessor._create_examples", "procondual_processor.ProconDualProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.agnews_processor.AGNewsProcessor._create_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor._read_tsv"], ["", "def", "get_dev_examples", "(", "self", ",", "data_dir", ",", "name", "=", "None", ")", ":", "\n", "        ", "name", "=", "'dev.tsv'", "if", "not", "name", "else", "name", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'ProconDual'", ",", "name", ")", ")", ",", "'dev'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.procondual_processor.ProconDualProcessor.get_test_examples": [[21, 25], ["procondual_processor.ProconDualProcessor._create_examples", "procondual_processor.ProconDualProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.agnews_processor.AGNewsProcessor._create_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor._read_tsv"], ["", "def", "get_test_examples", "(", "self", ",", "data_dir", ",", "name", "=", "None", ")", ":", "\n", "        ", "name", "=", "'test.tsv'", "if", "not", "name", "else", "name", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'ProconDual'", ",", "name", ")", ")", ",", "'test'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.procondual_processor.ProconDualProcessor.get_any_examples": [[26, 29], ["procondual_processor.ProconDualProcessor._create_examples", "procondual_processor.ProconDualProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.agnews_processor.AGNewsProcessor._create_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor._read_tsv"], ["", "def", "get_any_examples", "(", "self", ",", "data_dir", ",", "split", ")", ":", "\n", "        ", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'ProconDual'", ",", "split", ")", ")", ",", "split", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.procondual_processor.ProconDualProcessor._create_examples": [[30, 43], ["enumerate", "examples.append", "datasets.bert_processors.abstract_processor.InputExample"], "methods", ["None"], ["", "def", "_create_examples", "(", "self", ",", "lines", ",", "set_type", ")", ":", "\n", "        ", "\"\"\"Creates examples for the training and dev sets.\"\"\"", "\n", "examples", "=", "[", "]", "\n", "for", "(", "i", ",", "line", ")", "in", "enumerate", "(", "lines", ")", ":", "\n", "            ", "if", "i", "==", "0", ":", "\n", "                ", "continue", "\n", "", "guid", "=", "\"%s-%s\"", "%", "(", "set_type", ",", "i", ")", "\n", "text_a", "=", "line", "[", "1", "]", "#question", "\n", "label", "=", "line", "[", "0", "]", "\n", "text_b", "=", "line", "[", "2", "]", "#opinion", "\n", "examples", ".", "append", "(", "\n", "InputExample", "(", "guid", "=", "guid", ",", "text_a", "=", "text_a", ",", "text_b", "=", "text_b", ",", "label", "=", "label", ")", ")", "\n", "", "return", "examples", "\n", "", "", ""]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.procon_processor.ProconProcessor.get_train_examples": [[11, 15], ["procon_processor.ProconProcessor._create_examples", "procon_processor.ProconProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.agnews_processor.AGNewsProcessor._create_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor._read_tsv"], ["def", "get_train_examples", "(", "self", ",", "data_dir", ",", "name", "=", "None", ")", ":", "\n", "        ", "name", "=", "'train.tsv'", "if", "not", "name", "else", "name", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'Procon'", ",", "name", ")", ")", ",", "'train'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.procon_processor.ProconProcessor.get_dev_examples": [[16, 20], ["procon_processor.ProconProcessor._create_examples", "procon_processor.ProconProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.agnews_processor.AGNewsProcessor._create_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor._read_tsv"], ["", "def", "get_dev_examples", "(", "self", ",", "data_dir", ",", "name", "=", "None", ")", ":", "\n", "        ", "name", "=", "'dev.tsv'", "if", "not", "name", "else", "name", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'Procon'", ",", "name", ")", ")", ",", "'dev'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.procon_processor.ProconProcessor.get_test_examples": [[21, 25], ["procon_processor.ProconProcessor._create_examples", "procon_processor.ProconProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.agnews_processor.AGNewsProcessor._create_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor._read_tsv"], ["", "def", "get_test_examples", "(", "self", ",", "data_dir", ",", "name", "=", "None", ")", ":", "\n", "        ", "name", "=", "'test.tsv'", "if", "not", "name", "else", "name", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'Procon'", ",", "name", ")", ")", ",", "'test'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.procon_processor.ProconProcessor.get_any_examples": [[26, 29], ["procon_processor.ProconProcessor._create_examples", "procon_processor.ProconProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.agnews_processor.AGNewsProcessor._create_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor._read_tsv"], ["", "def", "get_any_examples", "(", "self", ",", "data_dir", ",", "split", ")", ":", "\n", "        ", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'Procon'", ",", "split", ")", ")", ",", "split", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.procon_processor.ProconProcessor._create_examples": [[30, 42], ["enumerate", "examples.append", "datasets.bert_processors.abstract_processor.InputExample"], "methods", ["None"], ["", "def", "_create_examples", "(", "self", ",", "lines", ",", "set_type", ")", ":", "\n", "        ", "\"\"\"Creates examples for the training and dev sets.\"\"\"", "\n", "examples", "=", "[", "]", "\n", "for", "(", "i", ",", "line", ")", "in", "enumerate", "(", "lines", ")", ":", "\n", "            ", "if", "i", "==", "0", ":", "\n", "                ", "continue", "\n", "", "guid", "=", "\"%s-%s\"", "%", "(", "set_type", ",", "i", ")", "\n", "text_a", "=", "line", "[", "1", "]", "\n", "label", "=", "line", "[", "0", "]", "\n", "examples", ".", "append", "(", "\n", "InputExample", "(", "guid", "=", "guid", ",", "text_a", "=", "text_a", ",", "text_b", "=", "None", ",", "label", "=", "label", ")", ")", "\n", "", "return", "examples", "\n", "", "", ""]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.procondual_mt_processor.ProconDual_mtProcessor.get_train_examples": [[11, 15], ["procondual_mt_processor.ProconDual_mtProcessor._create_examples", "procondual_mt_processor.ProconDual_mtProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.agnews_processor.AGNewsProcessor._create_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor._read_tsv"], ["def", "get_train_examples", "(", "self", ",", "data_dir", ",", "name", "=", "None", ")", ":", "\n", "        ", "name", "=", "'train.tsv'", "if", "not", "name", "else", "name", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'ProconDual_mt'", ",", "name", ")", ")", ",", "'train'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.procondual_mt_processor.ProconDual_mtProcessor.get_dev_examples": [[16, 20], ["procondual_mt_processor.ProconDual_mtProcessor._create_examples", "procondual_mt_processor.ProconDual_mtProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.agnews_processor.AGNewsProcessor._create_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor._read_tsv"], ["", "def", "get_dev_examples", "(", "self", ",", "data_dir", ",", "name", "=", "None", ")", ":", "\n", "        ", "name", "=", "'dev.tsv'", "if", "not", "name", "else", "name", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'ProconDual_mt'", ",", "name", ")", ")", ",", "'dev'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.procondual_mt_processor.ProconDual_mtProcessor.get_test_examples": [[21, 25], ["procondual_mt_processor.ProconDual_mtProcessor._create_examples", "procondual_mt_processor.ProconDual_mtProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.agnews_processor.AGNewsProcessor._create_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor._read_tsv"], ["", "def", "get_test_examples", "(", "self", ",", "data_dir", ",", "name", "=", "None", ")", ":", "\n", "        ", "name", "=", "'test.tsv'", "if", "not", "name", "else", "name", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'ProconDual_mt'", ",", "name", ")", ")", ",", "'test'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.procondual_mt_processor.ProconDual_mtProcessor.get_any_examples": [[26, 29], ["procondual_mt_processor.ProconDual_mtProcessor._create_examples", "procondual_mt_processor.ProconDual_mtProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.agnews_processor.AGNewsProcessor._create_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor._read_tsv"], ["", "def", "get_any_examples", "(", "self", ",", "data_dir", ",", "split", ")", ":", "\n", "        ", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'ProconDual_mt'", ",", "split", ")", ")", ",", "split", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.procondual_mt_processor.ProconDual_mtProcessor._create_examples": [[30, 43], ["enumerate", "examples.append", "datasets.bert_processors.abstract_processor.InputExample"], "methods", ["None"], ["", "def", "_create_examples", "(", "self", ",", "lines", ",", "set_type", ")", ":", "\n", "        ", "\"\"\"Creates examples for the training and dev sets.\"\"\"", "\n", "examples", "=", "[", "]", "\n", "for", "(", "i", ",", "line", ")", "in", "enumerate", "(", "lines", ")", ":", "\n", "            ", "if", "i", "==", "0", ":", "\n", "                ", "continue", "\n", "", "guid", "=", "\"%s-%s\"", "%", "(", "set_type", ",", "i", ")", "\n", "text_a", "=", "line", "[", "1", "]", "#question", "\n", "label", "=", "line", "[", "0", "]", "\n", "text_b", "=", "line", "[", "2", "]", "#opinion", "\n", "examples", ".", "append", "(", "\n", "InputExample", "(", "guid", "=", "guid", ",", "text_a", "=", "text_a", ",", "text_b", "=", "text_b", ",", "label", "=", "label", ")", ")", "\n", "", "return", "examples", "\n", "", "", ""]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.imdb_processor.IMDBProcessor.get_train_examples": [[11, 15], ["imdb_processor.IMDBProcessor._create_examples", "imdb_processor.IMDBProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.agnews_processor.AGNewsProcessor._create_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor._read_tsv"], ["def", "get_train_examples", "(", "self", ",", "data_dir", ",", "name", "=", "None", ")", ":", "\n", "        ", "name", "=", "'train.tsv'", "if", "not", "name", "else", "name", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'IMDB'", ",", "name", ")", ")", ",", "'train'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.imdb_processor.IMDBProcessor.get_dev_examples": [[16, 20], ["imdb_processor.IMDBProcessor._create_examples", "imdb_processor.IMDBProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.agnews_processor.AGNewsProcessor._create_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor._read_tsv"], ["", "def", "get_dev_examples", "(", "self", ",", "data_dir", ",", "name", "=", "None", ")", ":", "\n", "        ", "name", "=", "'dev.tsv'", "if", "not", "name", "else", "name", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'IMDB'", ",", "name", ")", ")", ",", "'dev'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.imdb_processor.IMDBProcessor.get_test_examples": [[21, 25], ["imdb_processor.IMDBProcessor._create_examples", "imdb_processor.IMDBProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.agnews_processor.AGNewsProcessor._create_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor._read_tsv"], ["", "def", "get_test_examples", "(", "self", ",", "data_dir", ",", "name", "=", "None", ")", ":", "\n", "        ", "name", "=", "'test.tsv'", "if", "not", "name", "else", "name", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'IMDB'", ",", "name", ")", ")", ",", "'test'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.imdb_processor.IMDBProcessor.get_any_examples": [[26, 29], ["imdb_processor.IMDBProcessor._create_examples", "imdb_processor.IMDBProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.agnews_processor.AGNewsProcessor._create_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor._read_tsv"], ["", "def", "get_any_examples", "(", "self", ",", "data_dir", ",", "split", ")", ":", "\n", "        ", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'IMDB'", ",", "split", ")", ")", ",", "split", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.imdb_processor.IMDBProcessor._create_examples": [[30, 42], ["enumerate", "examples.append", "datasets.bert_processors.abstract_processor.InputExample"], "methods", ["None"], ["", "def", "_create_examples", "(", "self", ",", "lines", ",", "set_type", ")", ":", "\n", "        ", "\"\"\"Creates examples for the training and dev sets.\"\"\"", "\n", "examples", "=", "[", "]", "\n", "for", "(", "i", ",", "line", ")", "in", "enumerate", "(", "lines", ")", ":", "\n", "            ", "if", "i", "==", "0", ":", "\n", "                ", "continue", "\n", "", "guid", "=", "\"%s-%s\"", "%", "(", "set_type", ",", "i", ")", "\n", "text_a", "=", "line", "[", "1", "]", "\n", "label", "=", "line", "[", "0", "]", "\n", "examples", ".", "append", "(", "\n", "InputExample", "(", "guid", "=", "guid", ",", "text_a", "=", "text_a", ",", "text_b", "=", "None", ",", "label", "=", "label", ")", ")", "\n", "", "return", "examples", "", "", "", ""]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.agnews_processor.AGNewsProcessor.get_train_examples": [[11, 14], ["agnews_processor.AGNewsProcessor._create_examples", "agnews_processor.AGNewsProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.agnews_processor.AGNewsProcessor._create_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor._read_tsv"], ["def", "get_train_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'AGNews'", ",", "'train.tsv'", ")", ")", ",", "'train'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.agnews_processor.AGNewsProcessor.get_dev_examples": [[15, 18], ["agnews_processor.AGNewsProcessor._create_examples", "agnews_processor.AGNewsProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.agnews_processor.AGNewsProcessor._create_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor._read_tsv"], ["", "def", "get_dev_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'AGNews'", ",", "'dev.tsv'", ")", ")", ",", "'dev'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.agnews_processor.AGNewsProcessor.get_test_examples": [[19, 22], ["agnews_processor.AGNewsProcessor._create_examples", "agnews_processor.AGNewsProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.agnews_processor.AGNewsProcessor._create_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor._read_tsv"], ["", "def", "get_test_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'AGNews'", ",", "'test.tsv'", ")", ")", ",", "'test'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.agnews_processor.AGNewsProcessor._create_examples": [[23, 35], ["enumerate", "examples.append", "datasets.bert_processors.abstract_processor.InputExample"], "methods", ["None"], ["", "def", "_create_examples", "(", "self", ",", "lines", ",", "set_type", ")", ":", "\n", "        ", "\"\"\"Creates examples for the training and dev sets.\"\"\"", "\n", "examples", "=", "[", "]", "\n", "for", "(", "i", ",", "line", ")", "in", "enumerate", "(", "lines", ")", ":", "\n", "            ", "if", "i", "==", "0", ":", "\n", "                ", "continue", "\n", "", "guid", "=", "\"%s-%s\"", "%", "(", "set_type", ",", "i", ")", "\n", "text_a", "=", "line", "[", "1", "]", "\n", "label", "=", "line", "[", "0", "]", "\n", "examples", ".", "append", "(", "\n", "InputExample", "(", "guid", "=", "guid", ",", "text_a", "=", "text_a", ",", "text_b", "=", "None", ",", "label", "=", "label", ")", ")", "\n", "", "return", "examples", "\n", "", "", ""]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.InputExample.__init__": [[64, 80], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "guid", ",", "text_a", ",", "text_b", "=", "None", ",", "label", "=", "None", ")", ":", "\n", "        ", "\"\"\"Constructs a InputExample.\n\n        Args:\n            guid: Unique id for the example.\n            text_a: string. The untokenized text of the first sequence. For single\n            sequence tasks, only this sequence must be specified.\n            text_b: (Optional) string. The untokenized text of the second sequence.\n            Only must be specified for sequence pair tasks.\n            label: (Optional) string. The label of the example. This should be\n            specified for train and dev examples, but not for test examples.\n        \"\"\"", "\n", "self", ".", "guid", "=", "guid", "\n", "self", ".", "text_a", "=", "text_a", "\n", "self", ".", "text_b", "=", "text_b", "\n", "self", ".", "label", "=", "label", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.InputFeatures.__init__": [[85, 91], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "label_id", ",", "sentiment_scores", "=", "None", ")", ":", "\n", "        ", "self", ".", "input_ids", "=", "input_ids", "\n", "self", ".", "input_mask", "=", "input_mask", "\n", "self", ".", "segment_ids", "=", "segment_ids", "\n", "self", ".", "label_id", "=", "label_id", "\n", "self", ".", "sentiment_scores", "=", "sentiment_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor.get_train_examples": [[98, 105], ["NotImplementedError"], "methods", ["None"], ["def", "get_train_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"\n        Gets a collection of `InputExample`s for the train set\n        :param data_dir:\n        :return:\n        \"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor.get_dev_examples": [[106, 113], ["NotImplementedError"], "methods", ["None"], ["", "def", "get_dev_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"\n        Gets a collection of `InputExample`s for the dev set\n        :param data_dir:\n        :return:\n        \"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor.get_test_examples": [[114, 121], ["NotImplementedError"], "methods", ["None"], ["", "def", "get_test_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"\n        Gets a collection of `InputExample`s for the test set\n        :param data_dir:\n        :return:\n        \"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor.get_labels": [[122, 128], ["NotImplementedError"], "methods", ["None"], ["", "def", "get_labels", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Gets a list of possible labels in the dataset\n        :return:\n        \"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor._read_tsv": [[129, 145], ["open", "csv.reader", "lines.append", "list", "str"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "_read_tsv", "(", "cls", ",", "input_file", ",", "quotechar", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Reads a Tab Separated Values (TSV) file\n        :param input_file:\n        :param quotechar:\n        :return:\n        \"\"\"", "\n", "with", "open", "(", "input_file", ",", "\"r\"", ")", "as", "f", ":", "\n", "            ", "reader", "=", "csv", ".", "reader", "(", "f", ",", "delimiter", "=", "\"\\t\"", ",", "quotechar", "=", "quotechar", ")", "\n", "lines", "=", "[", "]", "\n", "for", "line", "in", "reader", ":", "\n", "                ", "if", "sys", ".", "version_info", "[", "0", "]", "==", "2", ":", "\n", "                    ", "line", "=", "list", "(", "str", "(", "cell", ",", "'utf-8'", ")", "for", "cell", "in", "line", ")", "\n", "", "lines", ".", "append", "(", "line", ")", "\n", "", "return", "lines", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.split_sents_Overal_Score": [[10, 38], ["nltk.sent_tokenize", "sum", "scores.append", "analyzer.polarity_scores", "len", "len", "len"], "function", ["None"], ["def", "split_sents_Overal_Score", "(", "string", ")", ":", "\n", "    ", "r'''\n    split sentences and assign the overall score to all tokens\n    :param string:\n    :return:\n    '''", "\n", "\n", "sentences", "=", "nltk_tokenize", ".", "sent_tokenize", "(", "string", ")", "\n", "scores", "=", "[", "]", "\n", "for", "st", "in", "sentences", ":", "\n", "        ", "score", "=", "analyzer", ".", "polarity_scores", "(", "st", ")", "[", "'compound'", "]", "\n", "# neutral: 0, positive: 1, negative : 2", "\n", "if", "score", ">=", "0.05", ":", "\n", "            ", "score", "=", "1", "\n", "", "elif", "score", "<=", "-", "0.05", ":", "\n", "            ", "score", "=", "-", "1", "\n", "", "else", ":", "\n", "            ", "score", "=", "0", "\n", "", "scores", ".", "append", "(", "score", ")", "\n", "", "t_score", "=", "sum", "(", "scores", ")", "\n", "if", "t_score", ">", "0", ":", "\n", "        ", "scores", "=", "[", "1", "]", "*", "len", "(", "sentences", ")", "\n", "", "elif", "t_score", "<", "0", ":", "\n", "        ", "scores", "=", "[", "2", "]", "*", "len", "(", "sentences", ")", "\n", "", "else", ":", "\n", "        ", "scores", "=", "[", "0", "]", "*", "len", "(", "sentences", ")", "\n", "\n", "", "return", "sentences", ",", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.split_sents_Sent_Score": [[40, 47], ["nltk.sent_tokenize", "abstract_processor.get_sent_score", "scores.append"], "function", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.get_sent_score"], ["", "def", "split_sents_Sent_Score", "(", "string", ")", ":", "\n", "    ", "sentences", "=", "nltk_tokenize", ".", "sent_tokenize", "(", "string", ")", "\n", "scores", "=", "[", "]", "\n", "for", "st", "in", "sentences", ":", "\n", "        ", "score", "=", "get_sent_score", "(", "st", ")", "\n", "scores", ".", "append", "(", "score", ")", "\n", "", "return", "sentences", ",", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.get_sent_score": [[49, 59], ["analyzer.polarity_scores"], "function", ["None"], ["", "def", "get_sent_score", "(", "sentence", ")", ":", "\n", "    ", "score", "=", "analyzer", ".", "polarity_scores", "(", "sentence", ")", "[", "'compound'", "]", "\n", "# neutral: 0, positive: 1, negative : 2", "\n", "if", "score", ">=", "0.05", ":", "\n", "        ", "score", "=", "1", "\n", "", "elif", "score", "<=", "-", "0.05", ":", "\n", "        ", "score", "=", "2", "\n", "", "else", ":", "\n", "        ", "score", "=", "0", "\n", "", "return", "score", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.convert_examples_to_features": [[147, 228], ["enumerate", "tokenizer.tokenize", "tokenizer.convert_tokens_to_ids", "features.append", "tokenizer.tokenize", "abstract_processor._truncate_seq_pair", "len", "len", "len", "len", "len", "float", "print", "print", "print", "print", "print", "abstract_processor.InputFeatures", "len", "len", "len", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.BertTokenizer.convert_tokens_to_ids", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor._truncate_seq_pair"], ["", "", "", "def", "convert_examples_to_features", "(", "examples", ",", "max_seq_length", ",", "tokenizer", ",", "print_examples", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Loads a data file into a list of InputBatch objects\n    :param examples:\n    :param max_seq_length:\n    :param tokenizer:\n    :param print_examples:\n    :return: a list of InputBatch objects\n    \"\"\"", "\n", "\n", "features", "=", "[", "]", "\n", "for", "(", "ex_index", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "        ", "tokens_a", "=", "tokenizer", ".", "tokenize", "(", "example", ".", "text_a", ")", "\n", "\n", "tokens_b", "=", "None", "\n", "if", "example", ".", "text_b", ":", "\n", "            ", "tokens_b", "=", "tokenizer", ".", "tokenize", "(", "example", ".", "text_b", ")", "\n", "# Modifies `tokens_a` and `tokens_b` in place so that the total", "\n", "# length is less than the specified length.", "\n", "# Account for [CLS], [SEP], [SEP] with \"- 3\"", "\n", "_truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "max_seq_length", "-", "3", ")", "\n", "", "else", ":", "\n", "# Account for [CLS] and [SEP] with \"- 2\"", "\n", "            ", "if", "len", "(", "tokens_a", ")", ">", "max_seq_length", "-", "2", ":", "\n", "                ", "tokens_a", "=", "tokens_a", "[", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "\n", "# The convention in BERT is:", "\n", "# (a) For sequence pairs:", "\n", "#  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]", "\n", "#  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1", "\n", "# (b) For single sequences:", "\n", "#  tokens:   [CLS] the dog is hairy . [SEP]", "\n", "#  type_ids: 0   0   0   0  0     0 0", "\n", "#", "\n", "# Where \"type_ids\" are used to indicate whether this is the first", "\n", "# sequence or the second sequence. The embedding vectors for `type=0` and", "\n", "# `type=1` were learned during pre-training and are added to the wordpiece", "\n", "# embedding vector (and position vector). This is not *strictly* necessary", "\n", "# since the [SEP] token unambigiously separates the sequences, but it makes", "\n", "# it easier for the model to learn the concept of sequences.", "\n", "#", "\n", "# For classification tasks, the first vector (corresponding to [CLS]) is", "\n", "# used as as the \"sentence vector\". Note that this only makes sense because", "\n", "# the entire model is fine-tuned.", "\n", "", "", "tokens", "=", "[", "\"[CLS]\"", "]", "+", "tokens_a", "+", "[", "\"[SEP]\"", "]", "\n", "segment_ids", "=", "[", "0", "]", "*", "len", "(", "tokens", ")", "\n", "\n", "if", "tokens_b", ":", "\n", "            ", "tokens", "+=", "tokens_b", "+", "[", "\"[SEP]\"", "]", "\n", "segment_ids", "+=", "[", "1", "]", "*", "(", "len", "(", "tokens_b", ")", "+", "1", ")", "\n", "\n", "", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "\n", "# The mask has 1 for real tokens and 0 for padding tokens. Only real", "\n", "# tokens are attended to.", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "# Zero-pad up to the sequence length.", "\n", "padding", "=", "[", "0", "]", "*", "(", "max_seq_length", "-", "len", "(", "input_ids", ")", ")", "\n", "input_ids", "+=", "padding", "\n", "input_mask", "+=", "padding", "\n", "segment_ids", "+=", "padding", "\n", "\n", "assert", "len", "(", "input_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "input_mask", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "segment_ids", ")", "==", "max_seq_length", "\n", "\n", "label_id", "=", "[", "float", "(", "x", ")", "for", "x", "in", "example", ".", "label", "]", "\n", "\n", "if", "print_examples", "and", "ex_index", "<", "5", ":", "\n", "            ", "print", "(", "\"tokens: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "tokens", "]", ")", ")", "\n", "print", "(", "\"input_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_ids", "]", ")", ")", "\n", "print", "(", "\"input_mask: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_mask", "]", ")", ")", "\n", "print", "(", "\"segment_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "segment_ids", "]", ")", ")", "\n", "print", "(", "\"label: %s\"", "%", "example", ".", "label", ")", "\n", "\n", "", "features", ".", "append", "(", "InputFeatures", "(", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "segment_ids", "=", "segment_ids", ",", "\n", "label_id", "=", "label_id", ")", ")", "\n", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.convert_examples_to_stancy_features": [[230, 320], ["enumerate", "tokenizer.tokenize", "tokenizer.tokenize", "abstract_processor._truncate_seq_pair", "tokenizer.convert_tokens_to_ids", "tokenizer.convert_tokens_to_ids", "features.append", "len", "len", "len", "len", "len", "len", "len", "len", "len", "float", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "abstract_processor.InputFeatures", "len", "len", "len", "str", "str", "str", "str", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor._truncate_seq_pair", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.BertTokenizer.convert_tokens_to_ids", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.BertTokenizer.convert_tokens_to_ids"], ["", "def", "convert_examples_to_stancy_features", "(", "examples", ",", "max_seq_length", ",", "tokenizer", ",", "print_examples", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Loads a data file into a list of InputBatch objects\n    :param examples:\n    :param max_seq_length:\n    :param tokenizer:\n    :param print_examples:\n    :return: a list of InputBatch objects\n    \"\"\"", "\n", "\n", "features", "=", "[", "]", "\n", "for", "(", "ex_index", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "        ", "tokens_a", "=", "tokenizer", ".", "tokenize", "(", "example", ".", "text_a", ")", "\n", "tokens_b", "=", "tokenizer", ".", "tokenize", "(", "example", ".", "text_b", ")", "\n", "_truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "max_seq_length", "-", "3", ")", "\n", "# The convention in BERT is:", "\n", "# (a) For sequence pairs:", "\n", "#  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]", "\n", "#  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1", "\n", "# (b) For single sequences:", "\n", "#  tokens:   [CLS] the dog is hairy . [SEP]", "\n", "#  type_ids: 0   0   0   0  0     0 0", "\n", "#", "\n", "# Where \"type_ids\" are used to indicate whether this is the first", "\n", "# sequence or the second sequence. The embedding vectors for `type=0` and", "\n", "# `type=1` were learned during pre-training and are added to the wordpiece", "\n", "# embedding vector (and position vector). This is not *strictly* necessary", "\n", "# since the [SEP] token unambigiously separates the sequences, but it makes", "\n", "# it easier for the model to learn the concept of sequences.", "\n", "#", "\n", "# For classification tasks, the first vector (corresponding to [CLS]) is", "\n", "# used as as the \"sentence vector\". Note that this only makes sense because", "\n", "# the entire model is fine-tuned.", "\n", "tokens_single", "=", "[", "\"[CLS]\"", "]", "+", "tokens_a", "+", "[", "\"[SEP]\"", "]", "\n", "segment_ids_single", "=", "[", "0", "]", "*", "len", "(", "tokens_single", ")", "\n", "#tokens_double = tokens_single + tokens_b + [\"[SEP]\"]", "\n", "tokens_double", "=", "[", "\"[CLS]\"", "]", "+", "tokens_b", "+", "[", "\"[SEP]\"", "]", "+", "tokens_a", "+", "[", "\"[SEP]\"", "]", "\n", "segment_ids_double", "=", "[", "1", "]", "*", "(", "len", "(", "tokens_b", ")", "+", "1", ")", "+", "segment_ids_single", "\n", "\n", "input_ids_single", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens_single", ")", "\n", "input_ids_double", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens_double", ")", "\n", "\n", "# The mask has 1 for real tokens and 0 for padding tokens. Only real", "\n", "# tokens are attended to.", "\n", "input_mask_single", "=", "[", "1", "]", "*", "len", "(", "input_ids_single", ")", "\n", "input_mask_double", "=", "[", "1", "]", "*", "len", "(", "input_ids_double", ")", "\n", "\n", "max_seq_length_single", "=", "max_seq_length", "\n", "\n", "# Zero-pad up to the sequence length.", "\n", "padding_single", "=", "[", "0", "]", "*", "(", "max_seq_length_single", "-", "len", "(", "input_ids_single", ")", ")", "\n", "input_ids_single", "+=", "padding_single", "\n", "input_mask_single", "+=", "padding_single", "\n", "segment_ids_single", "+=", "padding_single", "\n", "\n", "assert", "len", "(", "input_ids_single", ")", "==", "max_seq_length_single", "\n", "assert", "len", "(", "input_mask_single", ")", "==", "max_seq_length_single", "\n", "assert", "len", "(", "segment_ids_single", ")", "==", "max_seq_length_single", "\n", "\n", "padding_double", "=", "[", "0", "]", "*", "(", "max_seq_length", "-", "len", "(", "input_ids_double", ")", ")", "\n", "input_ids_double", "+=", "padding_double", "\n", "input_mask_double", "+=", "padding_double", "\n", "segment_ids_double", "+=", "padding_double", "\n", "\n", "assert", "len", "(", "input_ids_double", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "input_mask_double", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "segment_ids_double", ")", "==", "max_seq_length", "\n", "\n", "label_id", "=", "[", "float", "(", "x", ")", "for", "x", "in", "example", ".", "label", "]", "\n", "\n", "if", "print_examples", "and", "ex_index", "<", "5", ":", "\n", "            ", "print", "(", "\"single:\"", ")", "\n", "print", "(", "\"tokens: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "tokens_single", "]", ")", ")", "\n", "print", "(", "\"input_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_ids_single", "]", ")", ")", "\n", "print", "(", "\"input_mask: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_mask_single", "]", ")", ")", "\n", "print", "(", "\"segment_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "segment_ids_single", "]", ")", ")", "\n", "\n", "print", "(", "'double:'", ")", "\n", "print", "(", "\"tokens: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "tokens_double", "]", ")", ")", "\n", "print", "(", "\"input_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_ids_double", "]", ")", ")", "\n", "print", "(", "\"input_mask: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_mask_double", "]", ")", ")", "\n", "print", "(", "\"segment_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "segment_ids_double", "]", ")", ")", "\n", "print", "(", "\"label: %s\"", "%", "example", ".", "label", ")", "\n", "\n", "", "features", ".", "append", "(", "InputFeatures", "(", "input_ids", "=", "{", "\"single\"", ":", "input_ids_single", ",", "\"double\"", ":", "input_ids_double", "}", ",", "\n", "input_mask", "=", "{", "\"single\"", ":", "input_mask_single", ",", "\"double\"", ":", "input_mask_double", "}", ",", "\n", "segment_ids", "=", "{", "\"single\"", ":", "segment_ids_single", ",", "\"double\"", ":", "segment_ids_double", "}", ",", "\n", "label_id", "=", "label_id", ")", ")", "\n", "\n", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.convert_examples_to_features_with_sentiment": [[322, 434], ["enumerate", "zip", "tokenizer.convert_tokens_to_ids", "features.append", "abstract_processor.split_sents_Sent_Score", "tokenizer.tokenize", "zip", "abstract_processor._truncate_seq_pair", "abstract_processor._truncate_seq_pair", "len", "len", "len", "len", "len", "float", "print", "print", "print", "print", "print", "print", "abstract_processor.InputFeatures", "abstract_processor.get_sent_score", "len", "abstract_processor.split_sents_Sent_Score", "tokenizer.tokenize", "len", "len", "len", "abstract_processor.get_sent_score", "len", "len", "len", "str", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.BertTokenizer.convert_tokens_to_ids", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.split_sents_Sent_Score", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor._truncate_seq_pair", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor._truncate_seq_pair", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.get_sent_score", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.split_sents_Sent_Score", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.get_sent_score"], ["", "def", "convert_examples_to_features_with_sentiment", "(", "examples", ",", "max_seq_length", ",", "tokenizer", ",", "print_examples", "=", "False", ",", "overal_sent", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Loads a data file into a list of InputBatch objects\n    :param examples:\n    :param max_seq_length:\n    :param tokenizer:\n    :param print_examples:\n    :param overal_sent: whether choose overall sentiment of a sentence for tokens or not\n    :return: a list of InputBatch objects\n    \"\"\"", "\n", "\n", "features", "=", "[", "]", "\n", "for", "(", "ex_index", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "        ", "if", "overal_sent", ":", "\n", "            ", "tmp_scores_a", "=", "[", "get_sent_score", "(", "example", ".", "text_a", ")", "]", "\n", "tmp_sntcs_a", "=", "[", "example", ".", "text_a", "]", "\n", "", "else", ":", "\n", "            ", "tmp_sntcs_a", ",", "tmp_scores_a", "=", "split_sents_Sent_Score", "(", "example", ".", "text_a", ")", "\n", "", "tokens_a", ",", "scores_a", "=", "[", "]", ",", "[", "]", "\n", "for", "st", ",", "score", "in", "zip", "(", "tmp_sntcs_a", ",", "tmp_scores_a", ")", ":", "\n", "            ", "tok_st", "=", "tokenizer", ".", "tokenize", "(", "st", ")", "\n", "score_st", "=", "[", "score", "]", "*", "len", "(", "tok_st", ")", "\n", "tokens_a", "+=", "tok_st", "\n", "scores_a", "+=", "score_st", "\n", "\n", "", "if", "example", ".", "text_b", ":", "\n", "#tokens_b = tokenizer.tokenize(example.text_b)", "\n", "# Modifies `tokens_a` and `tokens_b` in place so that the total", "\n", "# length is less than the specified length.", "\n", "# Account for [CLS], [SEP], [SEP] with \"- 3\"", "\n", "#_truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)", "\n", "\n", "            ", "if", "overal_sent", ":", "\n", "                ", "tmp_scores_b", "=", "[", "get_sent_score", "(", "example", ".", "text_b", ")", "]", "\n", "tmp_sntcs_b", "=", "[", "example", ".", "text_b", "]", "\n", "", "else", ":", "\n", "                ", "tmp_sntcs_b", ",", "tmp_scores_b", "=", "split_sents_Sent_Score", "(", "example", ".", "text_b", ")", "\n", "", "tokens_b", ",", "scores_b", "=", "[", "]", ",", "[", "]", "\n", "for", "st", ",", "score", "in", "zip", "(", "tmp_sntcs_b", ",", "tmp_scores_b", ")", ":", "\n", "                ", "tok_st", "=", "tokenizer", ".", "tokenize", "(", "st", ")", "\n", "score_st", "=", "[", "score", "]", "*", "len", "(", "tok_st", ")", "\n", "tokens_b", "+=", "tok_st", "\n", "scores_b", "+=", "score_st", "\n", "", "_truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "max_seq_length", "-", "3", ")", "\n", "_truncate_seq_pair", "(", "scores_a", ",", "scores_b", ",", "max_seq_length", "-", "3", ")", "\n", "\n", "tokens", "=", "[", "\"[CLS]\"", "]", "+", "tokens_a", "+", "[", "\"[SEP]\"", "]", "+", "tokens_b", "+", "[", "\"[SEP]\"", "]", "\n", "segment_ids", "=", "[", "0", "]", "*", "(", "len", "(", "tokens_a", ")", "+", "2", ")", "+", "[", "1", "]", "*", "(", "len", "(", "tokens_b", ")", "+", "1", ")", "\n", "scores", "=", "[", "0", "]", "+", "scores_a", "+", "[", "0", "]", "+", "scores_b", "+", "[", "0", "]", "\n", "", "else", ":", "\n", "# Account for [CLS] and [SEP] with \"- 2\"", "\n", "            ", "if", "len", "(", "tokens_a", ")", ">", "max_seq_length", "-", "2", ":", "\n", "                ", "tokens_a", "=", "tokens_a", "[", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "scores_a", "=", "scores_a", "[", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "\n", "# The convention in BERT is:", "\n", "# (a) For sequence pairs:", "\n", "#  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]", "\n", "#  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1", "\n", "# (b) For single sequences:", "\n", "#  tokens:   [CLS] the dog is hairy . [SEP]", "\n", "#  type_ids: 0   0   0   0  0     0 0", "\n", "#", "\n", "# Where \"type_ids\" are used to indicate whether this is the first", "\n", "# sequence or the second sequence. The embedding vectors for `type=0` and", "\n", "# `type=1` were learned during pre-training and are added to the wordpiece", "\n", "# embedding vector (and position vector). This is not *strictly* necessary", "\n", "# since the [SEP] token unambigiously separates the sequences, but it makes", "\n", "# it easier for the model to learn the concept of sequences.", "\n", "#", "\n", "# For classification tasks, the first vector (corresponding to [CLS]) is", "\n", "# used as as the \"sentence vector\". Note that this only makes sense because", "\n", "# the entire model is fine-tuned.", "\n", "", "tokens", "=", "[", "\"[CLS]\"", "]", "+", "tokens_a", "+", "[", "\"[SEP]\"", "]", "\n", "segment_ids", "=", "[", "0", "]", "*", "len", "(", "tokens", ")", "\n", "scores", "=", "[", "0", "]", "+", "scores_a", "+", "[", "0", "]", "\n", "\n", "", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "\n", "# The mask has 1 for real tokens and 0 for padding tokens. Only real", "\n", "# tokens are attended to.", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "# Zero-pad up to the sequence length.", "\n", "padding", "=", "[", "0", "]", "*", "(", "max_seq_length", "-", "len", "(", "input_ids", ")", ")", "\n", "input_ids", "+=", "padding", "\n", "input_mask", "+=", "padding", "\n", "segment_ids", "+=", "padding", "\n", "scores", "+=", "padding", "\n", "\n", "assert", "len", "(", "input_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "input_mask", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "segment_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "scores", ")", "==", "max_seq_length", "\n", "\n", "label_id", "=", "[", "float", "(", "x", ")", "for", "x", "in", "example", ".", "label", "]", "\n", "\n", "if", "print_examples", "and", "ex_index", "<", "5", ":", "\n", "            ", "print", "(", "\"tokens: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "tokens", "]", ")", ")", "\n", "print", "(", "\"input_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_ids", "]", ")", ")", "\n", "print", "(", "\"input_mask: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_mask", "]", ")", ")", "\n", "print", "(", "\"segment_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "segment_ids", "]", ")", ")", "\n", "print", "(", "\"sentiment_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "scores", "]", ")", ")", "\n", "print", "(", "\"label: %s\"", "%", "example", ".", "label", ")", "\n", "\n", "", "features", ".", "append", "(", "InputFeatures", "(", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "segment_ids", "=", "segment_ids", ",", "\n", "sentiment_scores", "=", "scores", ",", "\n", "label_id", "=", "label_id", ",", "\n", ")", ")", "\n", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.convert_examples_to_features_with_emotion": [[436, 546], ["enumerate", "emotioner.get_padded_ids", "zip", "tokenizer.convert_tokens_to_ids", "features.append", "example.text_a.split", "tokenizer.tokenize", "emotioner.get_padded_ids", "zip", "abstract_processor._truncate_seq_pair", "abstract_processor._truncate_seq_pair", "len", "len", "len", "len", "len", "float", "print", "print", "print", "print", "print", "print", "abstract_processor.InputFeatures", "len", "example.text_b.split", "tokenizer.tokenize", "len", "len", "len", "len", "len", "len", "len", "str", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.marjanhs_procon20.utils.emotion.Emotion.get_padded_ids", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.BertTokenizer.convert_tokens_to_ids", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.emotion.Emotion.get_padded_ids", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor._truncate_seq_pair", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor._truncate_seq_pair", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.WordpieceTokenizer.tokenize"], ["", "def", "convert_examples_to_features_with_emotion", "(", "examples", ",", "max_seq_length", ",", "tokenizer", ",", "emotioner", ",", "print_examples", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Loads a data file into a list of InputBatch objects\n    :param examples:\n    :param max_seq_length:\n    :param tokenizer:\n    :param emotioner: Emotion object to convert emotions to ids\n    :param print_examples:\n    :return: a list of InputBatch objects\n    \"\"\"", "\n", "\n", "features", "=", "[", "]", "\n", "\n", "for", "(", "ex_index", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "\n", "        ", "emotion_ids_a", "=", "emotioner", ".", "get_padded_ids", "(", "example", ".", "text_a", ")", "\n", "tokens_a", ",", "scores_a", "=", "[", "]", ",", "[", "]", "\n", "for", "st", ",", "score", "in", "zip", "(", "example", ".", "text_a", ".", "split", "(", "\" \"", ")", ",", "emotion_ids_a", ")", ":", "\n", "            ", "tok_st", "=", "tokenizer", ".", "tokenize", "(", "st", ")", "\n", "score_st", "=", "[", "score", "]", "*", "len", "(", "tok_st", ")", "\n", "tokens_a", "+=", "tok_st", "\n", "scores_a", "+=", "score_st", "\n", "\n", "", "if", "example", ".", "text_b", ":", "\n", "#tokens_b = tokenizer.tokenize(example.text_b)", "\n", "# Modifies `tokens_a` and `tokens_b` in place so that the total", "\n", "# length is less than the specified length.", "\n", "# Account for [CLS], [SEP], [SEP] with \"- 3\"", "\n", "#_truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)", "\n", "            ", "emotion_ids_b", "=", "emotioner", ".", "get_padded_ids", "(", "example", ".", "text_b", ")", "\n", "tokens_b", ",", "scores_b", "=", "[", "]", ",", "[", "]", "\n", "for", "st", ",", "score", "in", "zip", "(", "example", ".", "text_b", ".", "split", "(", "\" \"", ")", ",", "emotion_ids_b", ")", ":", "\n", "                ", "tok_st", "=", "tokenizer", ".", "tokenize", "(", "st", ")", "\n", "score_st", "=", "[", "score", "]", "*", "len", "(", "tok_st", ")", "\n", "tokens_b", "+=", "tok_st", "\n", "scores_b", "+=", "score_st", "\n", "", "_truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "max_seq_length", "-", "3", ")", "\n", "_truncate_seq_pair", "(", "scores_a", ",", "scores_b", ",", "max_seq_length", "-", "3", ")", "\n", "\n", "tokens", "=", "[", "\"[CLS]\"", "]", "+", "tokens_a", "+", "[", "\"[SEP]\"", "]", "+", "tokens_b", "+", "[", "\"[SEP]\"", "]", "\n", "segment_ids", "=", "[", "0", "]", "*", "(", "len", "(", "tokens_a", ")", "+", "2", ")", "+", "[", "1", "]", "*", "(", "len", "(", "tokens_b", ")", "+", "1", ")", "\n", "pad", "=", "[", "[", "0", "]", "*", "emotioner", ".", "max_em_len", "]", "\n", "scores", "=", "pad", "+", "scores_a", "+", "pad", "+", "scores_b", "+", "pad", "\n", "", "else", ":", "\n", "# Account for [CLS] and [SEP] with \"- 2\"", "\n", "            ", "if", "len", "(", "tokens_a", ")", ">", "max_seq_length", "-", "2", ":", "\n", "                ", "tokens_a", "=", "tokens_a", "[", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "scores_a", "=", "scores_a", "[", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "\n", "# The convention in BERT is:", "\n", "# (a) For sequence pairs:", "\n", "#  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]", "\n", "#  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1", "\n", "# (b) For single sequences:", "\n", "#  tokens:   [CLS] the dog is hairy . [SEP]", "\n", "#  type_ids: 0   0   0   0  0     0 0", "\n", "#", "\n", "# Where \"type_ids\" are used to indicate whether this is the first", "\n", "# sequence or the second sequence. The embedding vectors for `type=0` and", "\n", "# `type=1` were learned during pre-training and are added to the wordpiece", "\n", "# embedding vector (and position vector). This is not *strictly* necessary", "\n", "# since the [SEP] token unambigiously separates the sequences, but it makes", "\n", "# it easier for the model to learn the concept of sequences.", "\n", "#", "\n", "# For classification tasks, the first vector (corresponding to [CLS]) is", "\n", "# used as as the \"sentence vector\". Note that this only makes sense because", "\n", "# the entire model is fine-tuned.", "\n", "", "tokens", "=", "[", "\"[CLS]\"", "]", "+", "tokens_a", "+", "[", "\"[SEP]\"", "]", "\n", "segment_ids", "=", "[", "0", "]", "*", "len", "(", "tokens", ")", "\n", "pad", "=", "[", "[", "0", "]", "*", "emotioner", ".", "max_em_len", "]", "\n", "scores", "=", "pad", "+", "scores_a", "+", "pad", "\n", "#print('len tokens a', len(tokens), 'len scores a', len(scores))", "\n", "\n", "", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "\n", "# The mask has 1 for real tokens and 0 for padding tokens. Only real", "\n", "# tokens are attended to.", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "# Zero-pad up to the sequence length.", "\n", "padding", "=", "[", "0", "]", "*", "(", "max_seq_length", "-", "len", "(", "input_ids", ")", ")", "\n", "padding_scores", "=", "[", "[", "0", "]", "*", "emotioner", ".", "max_em_len", "]", "*", "(", "max_seq_length", "-", "len", "(", "input_ids", ")", ")", "\n", "\n", "input_ids", "+=", "padding", "\n", "input_mask", "+=", "padding", "\n", "segment_ids", "+=", "padding", "\n", "scores", "+=", "padding_scores", "\n", "\n", "assert", "len", "(", "input_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "input_mask", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "segment_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "scores", ")", "==", "max_seq_length", "\n", "\n", "label_id", "=", "[", "float", "(", "x", ")", "for", "x", "in", "example", ".", "label", "]", "\n", "\n", "if", "print_examples", "and", "ex_index", "<", "5", ":", "\n", "            ", "print", "(", "\"tokens: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "tokens", "]", ")", ")", "\n", "print", "(", "\"input_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_ids", "]", ")", ")", "\n", "print", "(", "\"input_mask: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_mask", "]", ")", ")", "\n", "print", "(", "\"segment_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "segment_ids", "]", ")", ")", "\n", "print", "(", "\"sentiment_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "scores", "]", ")", ")", "\n", "print", "(", "\"label: %s\"", "%", "example", ".", "label", ")", "\n", "\n", "", "features", ".", "append", "(", "InputFeatures", "(", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "segment_ids", "=", "segment_ids", ",", "\n", "sentiment_scores", "=", "scores", ",", "\n", "label_id", "=", "label_id", ",", "\n", ")", ")", "\n", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.convert_examples_to_stancy_features_with_emotion": [[548, 659], ["enumerate", "emotioner.get_padded_ids", "zip", "tokenizer.convert_tokens_to_ids", "features.append", "example.text_a.split", "tokenizer.tokenize", "emotioner.get_padded_ids", "zip", "abstract_processor._truncate_seq_pair", "abstract_processor._truncate_seq_pair", "len", "len", "len", "len", "len", "float", "print", "print", "print", "print", "print", "print", "abstract_processor.InputFeatures", "len", "example.text_b.split", "tokenizer.tokenize", "len", "len", "len", "len", "len", "len", "len", "str", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.marjanhs_procon20.utils.emotion.Emotion.get_padded_ids", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.BertTokenizer.convert_tokens_to_ids", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.emotion.Emotion.get_padded_ids", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor._truncate_seq_pair", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor._truncate_seq_pair", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.WordpieceTokenizer.tokenize"], ["", "def", "convert_examples_to_stancy_features_with_emotion", "(", "examples", ",", "max_seq_length", ",", "tokenizer", ",", "emotioner", ",", "print_examples", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    TODO: !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!1COMPLETE THIS FUNCTION!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n    Loads a data file into a list of InputBatch objects\n    :param examples:\n    :param max_seq_length:\n    :param tokenizer:\n    :param emotioner: Emotion object to convert emotions to ids\n    :param print_examples:\n    :return: a list of InputBatch objects\n    \"\"\"", "\n", "\n", "features", "=", "[", "]", "\n", "\n", "for", "(", "ex_index", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "\n", "        ", "emotion_ids_a", "=", "emotioner", ".", "get_padded_ids", "(", "example", ".", "text_a", ")", "\n", "tokens_a", ",", "scores_a", "=", "[", "]", ",", "[", "]", "\n", "for", "st", ",", "score", "in", "zip", "(", "example", ".", "text_a", ".", "split", "(", "\" \"", ")", ",", "emotion_ids_a", ")", ":", "\n", "            ", "tok_st", "=", "tokenizer", ".", "tokenize", "(", "st", ")", "\n", "score_st", "=", "[", "score", "]", "*", "len", "(", "tok_st", ")", "\n", "tokens_a", "+=", "tok_st", "\n", "scores_a", "+=", "score_st", "\n", "\n", "", "if", "example", ".", "text_b", ":", "\n", "#tokens_b = tokenizer.tokenize(example.text_b)", "\n", "# Modifies `tokens_a` and `tokens_b` in place so that the total", "\n", "# length is less than the specified length.", "\n", "# Account for [CLS], [SEP], [SEP] with \"- 3\"", "\n", "#_truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)", "\n", "            ", "emotion_ids_b", "=", "emotioner", ".", "get_padded_ids", "(", "example", ".", "text_b", ")", "\n", "tokens_b", ",", "scores_b", "=", "[", "]", ",", "[", "]", "\n", "for", "st", ",", "score", "in", "zip", "(", "example", ".", "text_b", ".", "split", "(", "\" \"", ")", ",", "emotion_ids_b", ")", ":", "\n", "                ", "tok_st", "=", "tokenizer", ".", "tokenize", "(", "st", ")", "\n", "score_st", "=", "[", "score", "]", "*", "len", "(", "tok_st", ")", "\n", "tokens_b", "+=", "tok_st", "\n", "scores_b", "+=", "score_st", "\n", "", "_truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "max_seq_length", "-", "3", ")", "\n", "_truncate_seq_pair", "(", "scores_a", ",", "scores_b", ",", "max_seq_length", "-", "3", ")", "\n", "\n", "tokens", "=", "[", "\"[CLS]\"", "]", "+", "tokens_a", "+", "[", "\"[SEP]\"", "]", "+", "tokens_b", "+", "[", "\"[SEP]\"", "]", "\n", "segment_ids", "=", "[", "0", "]", "*", "(", "len", "(", "tokens_a", ")", "+", "2", ")", "+", "[", "1", "]", "*", "(", "len", "(", "tokens_b", ")", "+", "1", ")", "\n", "pad", "=", "[", "[", "0", "]", "*", "emotioner", ".", "max_em_len", "]", "\n", "scores", "=", "pad", "+", "scores_a", "+", "pad", "+", "scores_b", "+", "pad", "\n", "", "else", ":", "\n", "# Account for [CLS] and [SEP] with \"- 2\"", "\n", "            ", "if", "len", "(", "tokens_a", ")", ">", "max_seq_length", "-", "2", ":", "\n", "                ", "tokens_a", "=", "tokens_a", "[", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "scores_a", "=", "scores_a", "[", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "\n", "# The convention in BERT is:", "\n", "# (a) For sequence pairs:", "\n", "#  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]", "\n", "#  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1", "\n", "# (b) For single sequences:", "\n", "#  tokens:   [CLS] the dog is hairy . [SEP]", "\n", "#  type_ids: 0   0   0   0  0     0 0", "\n", "#", "\n", "# Where \"type_ids\" are used to indicate whether this is the first", "\n", "# sequence or the second sequence. The embedding vectors for `type=0` and", "\n", "# `type=1` were learned during pre-training and are added to the wordpiece", "\n", "# embedding vector (and position vector). This is not *strictly* necessary", "\n", "# since the [SEP] token unambigiously separates the sequences, but it makes", "\n", "# it easier for the model to learn the concept of sequences.", "\n", "#", "\n", "# For classification tasks, the first vector (corresponding to [CLS]) is", "\n", "# used as as the \"sentence vector\". Note that this only makes sense because", "\n", "# the entire model is fine-tuned.", "\n", "", "tokens", "=", "[", "\"[CLS]\"", "]", "+", "tokens_a", "+", "[", "\"[SEP]\"", "]", "\n", "segment_ids", "=", "[", "0", "]", "*", "len", "(", "tokens", ")", "\n", "pad", "=", "[", "[", "0", "]", "*", "emotioner", ".", "max_em_len", "]", "\n", "scores", "=", "pad", "+", "scores_a", "+", "pad", "\n", "#print('len tokens a', len(tokens), 'len scores a', len(scores))", "\n", "\n", "", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "\n", "# The mask has 1 for real tokens and 0 for padding tokens. Only real", "\n", "# tokens are attended to.", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "# Zero-pad up to the sequence length.", "\n", "padding", "=", "[", "0", "]", "*", "(", "max_seq_length", "-", "len", "(", "input_ids", ")", ")", "\n", "padding_scores", "=", "[", "[", "0", "]", "*", "emotioner", ".", "max_em_len", "]", "*", "(", "max_seq_length", "-", "len", "(", "input_ids", ")", ")", "\n", "\n", "input_ids", "+=", "padding", "\n", "input_mask", "+=", "padding", "\n", "segment_ids", "+=", "padding", "\n", "scores", "+=", "padding_scores", "\n", "\n", "assert", "len", "(", "input_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "input_mask", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "segment_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "scores", ")", "==", "max_seq_length", "\n", "\n", "label_id", "=", "[", "float", "(", "x", ")", "for", "x", "in", "example", ".", "label", "]", "\n", "\n", "if", "print_examples", "and", "ex_index", "<", "5", ":", "\n", "            ", "print", "(", "\"tokens: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "tokens", "]", ")", ")", "\n", "print", "(", "\"input_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_ids", "]", ")", ")", "\n", "print", "(", "\"input_mask: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_mask", "]", ")", ")", "\n", "print", "(", "\"segment_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "segment_ids", "]", ")", ")", "\n", "print", "(", "\"sentiment_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "scores", "]", ")", ")", "\n", "print", "(", "\"label: %s\"", "%", "example", ".", "label", ")", "\n", "\n", "", "features", ".", "append", "(", "InputFeatures", "(", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "segment_ids", "=", "segment_ids", ",", "\n", "sentiment_scores", "=", "scores", ",", "\n", "label_id", "=", "label_id", ",", "\n", ")", ")", "\n", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.convert_examples_to_hierarchical_features": [[661, 722], ["enumerate", "list", "range", "features.append", "tokenizer.tokenize", "abstract_processor._truncate_seq_pair", "range", "list.append", "len", "float", "print", "print", "print", "print", "print", "abstract_processor.InputFeatures", "nltk.tokenize.sent_tokenize", "tokenizer.tokenize", "len", "len", "tokenizer.convert_tokens_to_ids", "len", "nltk.tokenize.sent_tokenize", "len", "len", "len", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor._truncate_seq_pair", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.BertTokenizer.convert_tokens_to_ids"], ["", "def", "convert_examples_to_hierarchical_features", "(", "examples", ",", "max_seq_length", ",", "tokenizer", ",", "print_examples", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Loads a data file into a list of InputBatch objects\n    :param examples:\n    :param max_seq_length:\n    :param tokenizer:\n    :param print_examples:\n    :return: a list of InputBatch objects\n    \"\"\"", "\n", "\n", "features", "=", "[", "]", "\n", "for", "(", "ex_index", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "        ", "tokens_a", "=", "[", "tokenizer", ".", "tokenize", "(", "line", ")", "for", "line", "in", "sent_tokenize", "(", "example", ".", "text_a", ")", "]", "\n", "tokens_b", "=", "None", "\n", "\n", "if", "example", ".", "text_b", ":", "\n", "            ", "tokens_b", "=", "[", "tokenizer", ".", "tokenize", "(", "line", ")", "for", "line", "in", "sent_tokenize", "(", "example", ".", "text_b", ")", "]", "\n", "# Modifies `tokens_a` and `tokens_b` in place so that the total length is less than the specified length", "\n", "# Account for [CLS], [SEP], [SEP]", "\n", "_truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "max_seq_length", "-", "3", ")", "\n", "", "else", ":", "\n", "# Account for [CLS] and [SEP]", "\n", "            ", "for", "i0", "in", "range", "(", "len", "(", "tokens_a", ")", ")", ":", "\n", "                ", "if", "len", "(", "tokens_a", "[", "i0", "]", ")", ">", "max_seq_length", "-", "2", ":", "\n", "                    ", "tokens_a", "[", "i0", "]", "=", "tokens_a", "[", "i0", "]", "[", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "\n", "", "", "", "tokens", "=", "[", "[", "\"[CLS]\"", "]", "+", "line", "+", "[", "\"[SEP]\"", "]", "for", "line", "in", "tokens_a", "]", "\n", "segment_ids", "=", "[", "[", "0", "]", "*", "len", "(", "line", ")", "for", "line", "in", "tokens", "]", "\n", "\n", "if", "tokens_b", ":", "\n", "            ", "tokens", "+=", "tokens_b", "+", "[", "\"[SEP]\"", "]", "\n", "segment_ids", "+=", "[", "1", "]", "*", "(", "len", "(", "tokens_b", ")", "+", "1", ")", "\n", "\n", "", "input_ids", "=", "list", "(", ")", "\n", "for", "line", "in", "tokens", ":", "\n", "            ", "input_ids", ".", "append", "(", "tokenizer", ".", "convert_tokens_to_ids", "(", "line", ")", ")", "\n", "\n", "# Input mask has 1 for real tokens and 0 for padding tokens", "\n", "", "input_mask", "=", "[", "[", "1", "]", "*", "len", "(", "line_ids", ")", "for", "line_ids", "in", "input_ids", "]", "\n", "\n", "# Zero-pad up to the sequence length.", "\n", "padding", "=", "[", "[", "0", "]", "*", "(", "max_seq_length", "-", "len", "(", "line_ids", ")", ")", "for", "line_ids", "in", "input_ids", "]", "\n", "for", "i0", "in", "range", "(", "len", "(", "input_ids", ")", ")", ":", "\n", "            ", "input_ids", "[", "i0", "]", "+=", "padding", "[", "i0", "]", "\n", "input_mask", "[", "i0", "]", "+=", "padding", "[", "i0", "]", "\n", "segment_ids", "[", "i0", "]", "+=", "padding", "[", "i0", "]", "\n", "\n", "", "label_id", "=", "[", "float", "(", "x", ")", "for", "x", "in", "example", ".", "label", "]", "\n", "\n", "if", "print_examples", "and", "ex_index", "<", "5", ":", "\n", "            ", "print", "(", "\"tokens: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "tokens", "]", ")", ")", "\n", "print", "(", "\"input_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_ids", "]", ")", ")", "\n", "print", "(", "\"input_mask: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_mask", "]", ")", ")", "\n", "print", "(", "\"segment_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "segment_ids", "]", ")", ")", "\n", "print", "(", "\"label: %s\"", "%", "example", ".", "label", ")", "\n", "\n", "", "features", ".", "append", "(", "InputFeatures", "(", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "segment_ids", "=", "segment_ids", ",", "\n", "label_id", "=", "label_id", ")", ")", "\n", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor._truncate_seq_pair": [[724, 745], ["len", "len", "len", "len", "tokens_a.pop", "tokens_b.pop"], "function", ["None"], ["", "def", "_truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "max_length", ")", ":", "\n", "    ", "\"\"\"\n    Truncates a sequence pair in place to the maximum length\n    :param tokens_a:\n    :param tokens_b:\n    :param max_length:\n    :return:\n    \"\"\"", "\n", "\n", "# This is a simple heuristic which will always truncate the longer sequence", "\n", "# one token at a time. This makes more sense than truncating an equal percent", "\n", "# of tokens from each, since if one sequence is very short then each token", "\n", "# that's truncated likely contains more information than a longer sequence.", "\n", "while", "True", ":", "\n", "        ", "total_length", "=", "len", "(", "tokens_a", ")", "+", "len", "(", "tokens_b", ")", "\n", "if", "total_length", "<=", "max_length", ":", "\n", "            ", "break", "\n", "", "if", "len", "(", "tokens_a", ")", ">", "len", "(", "tokens_b", ")", ":", "\n", "            ", "tokens_a", ".", "pop", "(", ")", "\n", "", "else", ":", "\n", "            ", "tokens_b", ".", "pop", "(", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.marjanhs_procon20.models.args.get_args": [[5, 20], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "os.path.join"], "function", ["None"], ["def", "get_args", "(", ")", ":", "\n", "    ", "parser", "=", "ArgumentParser", "(", "description", "=", "\"PyTorch deep learning models for document classification\"", ")", "\n", "par", "=", "'/home/projects/'", "\n", "parser", ".", "add_argument", "(", "'--no-cuda'", ",", "action", "=", "'store_false'", ",", "dest", "=", "'cuda'", ")", "\n", "parser", ".", "add_argument", "(", "'--gpu'", ",", "type", "=", "int", ",", "default", "=", "0", ")", "\n", "parser", ".", "add_argument", "(", "'--epochs'", ",", "type", "=", "int", ",", "default", "=", "50", ")", "\n", "parser", ".", "add_argument", "(", "'--batch-size'", ",", "type", "=", "int", ",", "default", "=", "1024", ")", "\n", "parser", ".", "add_argument", "(", "'--lr'", ",", "type", "=", "float", ",", "default", "=", "0.001", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "type", "=", "int", ",", "default", "=", "3435", ")", "# default value: 3435", "\n", "parser", ".", "add_argument", "(", "'--patience'", ",", "type", "=", "int", ",", "default", "=", "5", ")", "\n", "parser", ".", "add_argument", "(", "'--log-every'", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "'--data-dir'", ",", "default", "=", "os", ".", "path", ".", "join", "(", "par", ",", "'hedwig-data'", ",", "'datasets'", ")", ")", "#os.pardir", "\n", "parser", ".", "add_argument", "(", "'--early_on_f1'", ",", "action", "=", "'store_true'", ")", "\n", "\n", "return", "parser", "\n", "", ""]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_lstm.__main__.plot_heatmap": [[37, 50], ["pandas.DataFrame", "matplotlib.subplots", "seaborn.heatmap", "sns.heatmap.set_xticklabels", "fig.savefig", "numpy.array().reshape", "numpy.max", "numpy.array"], "function", ["None"], ["def", "plot_heatmap", "(", "x", ",", "values", ")", ":", "\n", "\n", "\n", "#x = x[:10]", "\n", "#values = values[:10]", "\n", "# sphinx_gallery_thumbnail_number = 2", "\n", "    ", "lbl_plot", "=", "x", "\n", "d", "=", "pd", ".", "DataFrame", "(", "np", ".", "array", "(", "values", ")", ".", "reshape", "(", "1", ",", "-", "1", ")", ")", "\n", "fig", ",", "axes", "=", "plt", ".", "subplots", "(", "nrows", "=", "10", ",", "figsize", "=", "(", "10", ",", "20", ")", ")", "\n", "ax", "=", "sns", ".", "heatmap", "(", "d", ",", "cmap", "=", "\"Blues\"", ",", "square", "=", "True", ",", "annot", "=", "True", ",", "xticklabels", "=", "x", ",", "yticklabels", "=", "False", ",", "\n", "vmax", "=", "np", ".", "max", "(", "values", ")", ",", "cbar", "=", "False", ")", "\n", "ax", ".", "set_xticklabels", "(", "rotation", "=", "40", ",", "labels", "=", "lbl_plot", ")", "\n", "fig", ".", "savefig", "(", "'example.png'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_lstm.__main__.analyze": [[52, 76], ["datasets.bert_processors.abstract_processor.convert_examples_to_features_with_sentiment", "enumerate", "numpy.array", "evaluator.tokenizer.convert_ids_to_tokens", "evaluator.tokenizer.convert_ids_to_tokens", "print", "print", "np.array.append", "numpy.where", "numpy.where", "collections.Counter", "collections.Counter", "numpy.array", "numpy.where", "numpy.where", "all_indices[].flatten().tolist", "all_indices[].flatten().tolist", "cp.most_common", "cn.most_common", "all_indices[].flatten", "all_indices[].flatten"], "function", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.convert_examples_to_features_with_sentiment", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.BertTokenizer.convert_ids_to_tokens", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.BertTokenizer.convert_ids_to_tokens"], ["", "def", "analyze", "(", "evaluator", ",", "input_indices", ",", "predicted_labels", ",", "target_labels", ")", ":", "\n", "    ", "eval_features", "=", "convert_examples_to_features_with_sentiment", "(", "\n", "evaluator", ".", "eval_examples", ",", "evaluator", ".", "args", ".", "max_seq_length", ",", "evaluator", ".", "tokenizer", ",", "overal_sent", "=", "evaluator", ".", "args", ".", "overal_sent", ")", "\n", "\n", "all_indices", "=", "[", "]", "\n", "for", "i", ",", "f", "in", "enumerate", "(", "eval_features", ")", ":", "\n", "        ", "indices", "=", "np", ".", "array", "(", "f", ".", "input_ids", ")", "[", "input_indices", "[", "i", "]", "]", "\n", "all_indices", ".", "append", "(", "indices", ")", "\n", "\n", "", "all_indices", "=", "np", ".", "array", "(", "all_indices", ")", "\n", "\n", "tp", "=", "np", ".", "where", "(", "predicted_labels", "==", "target_labels", ")", "and", "np", ".", "where", "(", "target_labels", ")", "[", "0", "]", "\n", "tn", "=", "np", ".", "where", "(", "predicted_labels", "==", "target_labels", ")", "and", "np", ".", "where", "(", "target_labels", "==", "0", ")", "[", "0", "]", "\n", "\n", "cp", ",", "cn", "=", "Counter", "(", "all_indices", "[", "tp", "]", ".", "flatten", "(", ")", ".", "tolist", "(", ")", ")", ",", "Counter", "(", "all_indices", "[", "tn", "]", ".", "flatten", "(", ")", ".", "tolist", "(", ")", ")", "\n", "\n", "p_most_commons_ids", "=", "[", "k", "for", "(", "k", ",", "v", ")", "in", "cp", ".", "most_common", "(", "20", ")", "]", "\n", "n_most_commons_ids", "=", "[", "k", "for", "(", "k", ",", "v", ")", "in", "cn", ".", "most_common", "(", "20", ")", "]", "\n", "\n", "p_most_commons_tokens", "=", "evaluator", ".", "tokenizer", ".", "convert_ids_to_tokens", "(", "p_most_commons_ids", ")", "\n", "n_most_commons_tokens", "=", "evaluator", ".", "tokenizer", ".", "convert_ids_to_tokens", "(", "n_most_commons_ids", ")", "\n", "print", "(", "'top 10 most common tokens in TP: '", ",", "p_most_commons_tokens", ")", "\n", "\n", "print", "(", "'top 10 most common tokens in TN: '", ",", "n_most_commons_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_lstm.__main__.analyze_example": [[78, 120], ["datasets.bert_processors.abstract_processor.convert_examples_to_features_with_sentiment", "numpy.array", "range", "pandas.DataFrame", "pd.DataFrame.to_csv", "numpy.where", "len", "all_indices[].flatten().tolist", "collections.Counter", "numpy.array", "evaluator.tokenizer.convert_ids_to_tokens", "range", "ls.append", "numpy.where", "len", "freqs.append", "len", "len", "all_indices[].flatten"], "function", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.convert_examples_to_features_with_sentiment", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.BertTokenizer.convert_ids_to_tokens"], ["", "def", "analyze_example", "(", "evaluator", ",", "input_indices", ",", "predicted_labels", ",", "target_labels", ",", "path", ")", ":", "\n", "    ", "eval_features", "=", "convert_examples_to_features_with_sentiment", "(", "\n", "evaluator", ".", "eval_examples", ",", "evaluator", ".", "args", ".", "max_seq_length", ",", "evaluator", ".", "tokenizer", ",", "overal_sent", "=", "evaluator", ".", "args", ".", "overal_sent", ")", "\n", "\n", "all_indices", "=", "input_indices", "\n", "'''for i, f in enumerate(eval_features):\n        locs = np.array(input_indices[i])\n        all_indices.append(indices)'''", "\n", "\n", "all_indices", "=", "np", ".", "array", "(", "all_indices", ")", "\n", "tp", "=", "np", ".", "where", "(", "predicted_labels", "==", "target_labels", ")", "and", "np", ".", "where", "(", "target_labels", "==", "0", ")", "[", "0", "]", "\n", "#tn = np.where(predicted_labels==target_labels) and np.where(target_labels==0)[0]", "\n", "\n", "ls", "=", "[", "]", "\n", "\n", "for", "j", "in", "range", "(", "len", "(", "tp", ")", ")", ":", "\n", "\n", "        ", "id_p_example", "=", "tp", "[", "j", "]", "\n", "#id_n_example = tn[0]", "\n", "\n", "\n", "example_pos", "=", "all_indices", "[", "id_p_example", "]", ".", "flatten", "(", ")", ".", "tolist", "(", ")", "\n", "#example_neg = all_indices[id_n_example].flatten().tolist()", "\n", "\n", "\n", "cp", "=", "Counter", "(", "example_pos", ")", "\n", "\n", "p_ids", "=", "np", ".", "array", "(", "eval_features", "[", "id_p_example", "]", ".", "input_ids", ")", "\n", "\n", "\n", "p_tokens", "=", "evaluator", ".", "tokenizer", ".", "convert_ids_to_tokens", "(", "p_ids", ")", "\n", "\n", "\n", "freqs", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "eval_features", "[", "id_p_example", "]", ".", "input_ids", ")", ")", ":", "\n", "            ", "freqs", ".", "append", "(", "cp", "[", "i", "]", ")", "\n", "\n", "", "assert", "len", "(", "freqs", ")", "==", "len", "(", "p_tokens", ")", "\n", "l", "=", "[", "p_tokens", ",", "freqs", "]", "\n", "ls", ".", "append", "(", "l", ")", "\n", "", "df", "=", "pd", ".", "DataFrame", "(", "ls", ",", "columns", "=", "[", "'tokens'", ",", "'freqs'", "]", ")", "\n", "df", ".", "to_csv", "(", "path", "/", "'neg_examples.tsv'", ",", "sep", "=", "'\\t'", ",", "index", "=", "None", ")", "\n", "#plot_heatmap(p_tokens, freqs)", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_lstm.__main__.evaluate_split": [[124, 148], ["common.evaluators.bert_sent_evaluator.BertEvaluator", "time.time", "print", "print", "print", "args.save_path.replace", "pathlib.Path", "pathlib.Path.mkdir", "print", "numpy.save", "numpy.save", "common.evaluators.bert_sent_evaluator.BertEvaluator.get_scores", "__main__.analyze_example", "LOG_TEMPLATE.format", "args.save_path.replace", "pathlib.Path", "time.time", "split.upper"], "function", ["home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.classification_evaluator.ClassificationEvaluator.get_scores", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_lstm_emotion.__main__.analyze_example"], ["", "def", "evaluate_split", "(", "model", ",", "processor", ",", "args", ",", "split", "=", "'dev'", ",", "return_indices", "=", "False", ")", ":", "\n", "    ", "evaluator", "=", "BertEvaluator", "(", "model", ",", "processor", ",", "args", ",", "split", ")", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "\n", "outs", "=", "evaluator", ".", "get_scores", "(", "silent", "=", "True", ",", "return_indices", "=", "return_indices", ")", "[", "0", "]", "\n", "accuracy", ",", "precision", ",", "recall", ",", "f1", ",", "avg_loss", ",", "f1_mac", ",", "hamming_loss", ",", "jacc", "=", "outs", "[", ":", "8", "]", "\n", "predicted_values", ",", "target_values", "=", "outs", "[", "-", "2", ":", "]", "\n", "\n", "if", "return_indices", ":", "\n", "        ", "indices", "=", "outs", "[", "-", "1", "]", "\n", "accuracy", ",", "precision", ",", "recall", ",", "f1", ",", "avg_loss", ",", "f1_mac", ",", "hamming_loss", ",", "jacc", ",", "predicted_labels", ",", "target_labels", ",", "indice", "=", "outs", "\n", "analyze_example", "(", "evaluator", ",", "indices", ",", "predicted_labels", ",", "target_labels", ",", "Path", "(", "'out'", ")", ")", "\n", "\n", "", "print", "(", "\"Inference time\"", ",", "time", ".", "time", "(", ")", "-", "start_time", ")", "\n", "print", "(", "'\\n'", "+", "LOG_HEADER", ")", "\n", "print", "(", "LOG_TEMPLATE", ".", "format", "(", "split", ".", "upper", "(", ")", ",", "accuracy", ",", "precision", ",", "recall", ",", "f1", ",", "avg_loss", ",", "f1_mac", ",", "hamming_loss", ",", "jacc", ")", ")", "\n", "\n", "model_name", "=", "args", ".", "save_path", ".", "replace", "(", "'model_checkpoints/'", ",", "''", ")", "\n", "path", "=", "Path", "(", "args", ".", "save_path", ".", "replace", "(", "'model_checkpoints'", ",", "'out'", ")", ")", "\n", "path", "=", "path", "/", "args", ".", "dataset", "\n", "path", ".", "mkdir", "(", "parents", "=", "True", ",", "exist_ok", "=", "True", ")", "\n", "print", "(", "'Saving prediction files in '", ",", "path", ")", "\n", "np", ".", "save", "(", "path", "/", "f'predicted_{model_name}_{split}.npy'", ",", "predicted_values", ")", "\n", "np", ".", "save", "(", "path", "/", "f'target_{model_name}_{split}.npy'", ",", "target_values", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_lstm.__main__.do_main": [[150, 289], ["models.bert_lstm.args.get_args", "print", "print", "print", "print", "random.seed", "numpy.random.seed", "torch.manual_seed", "utils.tokenization.BertTokenizer.from_pretrained", "models.bert_lstm.model.BertForSequenceClassification.from_pretrained", "model.to.to", "list", "common.trainers.bert_sent_trainer.BertTrainer", "print", "__main__.evaluate_split", "__main__.evaluate_split", "torch.device", "torch.cuda.device_count", "torch.cuda.set_device", "torch.cuda.set_device", "torch.device", "torch.distributed.init_process_group", "str().upper", "bool", "torch.cuda.manual_seed_all", "ValueError", "ValueError", "os.path.join", "os.makedirs", "processor.get_train_examples", "os.path.join", "model.to.half", "DDP", "model.to.named_parameters", "FusedAdam", "utils.optimization.BertAdam", "common.trainers.bert_sent_trainer.BertTrainer.train", "torch.load", "models.bert_lstm.model.BertForSequenceClassification.from_pretrained", "torch.load", "torch.load.state_dict().keys", "model.to.load_state_dict", "model.to.to", "int", "str", "FP16_Optimizer", "FP16_Optimizer", "key.replace", "str", "torch.distributed.get_world_size", "ImportError", "ImportError", "torch.load.state_dict", "torch.load.state_dict", "torch.cuda.is_available", "any", "len", "any"], "function", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_lstm_emotion.args.get_args", "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertPreTrainedModel.from_pretrained", "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertPreTrainedModel.from_pretrained", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_lstm_emotion.__main__.evaluate_split", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_lstm_emotion.__main__.evaluate_split", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor.get_train_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.trainers.bert_sent_trainer.BertTrainer.train", "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertPreTrainedModel.from_pretrained"], ["", "def", "do_main", "(", ")", ":", "\n", "# Set default configuration in args.py", "\n", "    ", "args", "=", "get_args", "(", ")", "\n", "args", ".", "overal_sent", "=", "False", "### change it ?!!!", "\n", "\n", "if", "args", ".", "local_rank", "==", "-", "1", "or", "not", "args", ".", "cuda", ":", "\n", "        ", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "args", ".", "cuda", "else", "\"cpu\"", ")", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "gpu", ")", "\n", "", "else", ":", "\n", "        ", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "local_rank", ")", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ",", "args", ".", "local_rank", ")", "\n", "n_gpu", "=", "1", "\n", "# Initializes the distributed backend which will take care of sychronizing nodes/GPUs", "\n", "torch", ".", "distributed", ".", "init_process_group", "(", "backend", "=", "'nccl'", ")", "\n", "\n", "", "print", "(", "'Device:'", ",", "str", "(", "device", ")", ".", "upper", "(", ")", ")", "\n", "print", "(", "'Number of GPUs:'", ",", "n_gpu", ")", "\n", "print", "(", "'Distributed training:'", ",", "bool", "(", "args", ".", "local_rank", "!=", "-", "1", ")", ")", "\n", "print", "(", "'FP16:'", ",", "args", ".", "fp16", ")", "\n", "\n", "# Set random seed for reproducibility", "\n", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "backends", ".", "cudnn", ".", "deterministic", "=", "True", "# new!", "\n", "if", "n_gpu", ">", "0", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n", "", "dataset_map", "=", "{", "\n", "'SST-2'", ":", "SST2Processor", ",", "\n", "'Reuters'", ":", "ReutersProcessor", ",", "\n", "'IMDB'", ":", "IMDBProcessor", ",", "\n", "'AAPD'", ":", "AAPDProcessor", ",", "\n", "'AGNews'", ":", "AGNewsProcessor", ",", "\n", "'Sogou'", ":", "SogouProcessor", ",", "\n", "'Procon'", ":", "ProconProcessor", ",", "\n", "'ProconDual'", ":", "ProconDualProcessor", "\n", "}", "\n", "\n", "if", "args", ".", "gradient_accumulation_steps", "<", "1", ":", "\n", "        ", "raise", "ValueError", "(", "\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\"", ".", "format", "(", "\n", "args", ".", "gradient_accumulation_steps", ")", ")", "\n", "\n", "", "if", "args", ".", "dataset", "not", "in", "dataset_map", ":", "\n", "        ", "raise", "ValueError", "(", "'Unrecognized dataset'", ")", "\n", "\n", "", "args", ".", "batch_size", "=", "args", ".", "batch_size", "//", "args", ".", "gradient_accumulation_steps", "\n", "args", ".", "device", "=", "device", "\n", "args", ".", "n_gpu", "=", "n_gpu", "\n", "args", ".", "num_labels", "=", "dataset_map", "[", "args", ".", "dataset", "]", ".", "NUM_CLASSES", "\n", "args", ".", "is_multilabel", "=", "dataset_map", "[", "args", ".", "dataset", "]", ".", "IS_MULTILABEL", "\n", "\n", "if", "not", "args", ".", "trained_model", ":", "\n", "        ", "save_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "save_path", ",", "dataset_map", "[", "args", ".", "dataset", "]", ".", "NAME", ")", "\n", "os", ".", "makedirs", "(", "save_path", ",", "exist_ok", "=", "True", ")", "\n", "\n", "", "processor", "=", "dataset_map", "[", "args", ".", "dataset", "]", "(", ")", "\n", "args", ".", "is_lowercase", "=", "'uncased'", "in", "args", ".", "model", "\n", "args", ".", "is_hierarchical", "=", "False", "\n", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "args", ".", "model", ",", "is_lowercase", "=", "args", ".", "is_lowercase", ")", "\n", "\n", "train_examples", "=", "None", "\n", "num_train_optimization_steps", "=", "None", "\n", "if", "not", "args", ".", "trained_model", ":", "\n", "        ", "train_examples", "=", "processor", ".", "get_train_examples", "(", "args", ".", "data_dir", ",", "args", ".", "train_name", ")", "\n", "num_train_optimization_steps", "=", "int", "(", "\n", "len", "(", "train_examples", ")", "/", "args", ".", "batch_size", "/", "args", ".", "gradient_accumulation_steps", ")", "*", "args", ".", "epochs", "\n", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "            ", "num_train_optimization_steps", "=", "num_train_optimization_steps", "//", "torch", ".", "distributed", ".", "get_world_size", "(", ")", "\n", "\n", "", "", "cache_dir", "=", "args", ".", "cache_dir", "if", "args", ".", "cache_dir", "else", "os", ".", "path", ".", "join", "(", "str", "(", "PYTORCH_PRETRAINED_BERT_CACHE", ")", ",", "'distributed_{}'", ".", "format", "(", "args", ".", "local_rank", ")", ")", "\n", "model", "=", "BertForSequenceClassification", ".", "from_pretrained", "(", "args", ".", "model", ",", "cache_dir", "=", "cache_dir", ",", "num_labels", "=", "args", ".", "num_labels", ",", "\n", "pooling", "=", "args", ".", "pooling", ")", "\n", "\n", "\n", "if", "args", ".", "fp16", ":", "\n", "        ", "model", ".", "half", "(", ")", "\n", "", "model", ".", "to", "(", "device", ")", "\n", "\n", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "        ", "try", ":", "\n", "            ", "from", "apex", ".", "parallel", "import", "DistributedDataParallel", "as", "DDP", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "\"Install NVIDIA Apex to use distributed and FP16 training.\"", ")", "\n", "", "model", "=", "DDP", "(", "model", ")", "\n", "", "'''elif n_gpu > 1: changed by marjan\n\n        model = torch.nn.DataParallel(model)'''", "\n", "\n", "# Prepare optimizer", "\n", "param_optimizer", "=", "list", "(", "model", ".", "named_parameters", "(", ")", ")", "\n", "no_decay", "=", "[", "'bias'", ",", "'LayerNorm.bias'", ",", "'LayerNorm.weight'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.0", "}", "]", "\n", "\n", "if", "args", ".", "fp16", ":", "\n", "        ", "try", ":", "\n", "            ", "from", "apex", ".", "optimizers", "import", "FP16_Optimizer", "\n", "from", "apex", ".", "optimizers", "import", "FusedAdam", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "\"Please install NVIDIA Apex for distributed and FP16 training\"", ")", "\n", "\n", "", "optimizer", "=", "FusedAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "args", ".", "lr", ",", "\n", "bias_correction", "=", "False", ",", "\n", "max_grad_norm", "=", "1.0", ")", "\n", "if", "args", ".", "loss_scale", "==", "0", ":", "\n", "            ", "optimizer", "=", "FP16_Optimizer", "(", "optimizer", ",", "dynamic_loss_scale", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "optimizer", "=", "FP16_Optimizer", "(", "optimizer", ",", "static_loss_scale", "=", "args", ".", "loss_scale", ")", "\n", "\n", "", "", "else", ":", "\n", "        ", "optimizer", "=", "BertAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "args", ".", "lr", ",", "\n", "warmup", "=", "args", ".", "warmup_proportion", ",", "\n", "t_total", "=", "num_train_optimization_steps", ")", "\n", "\n", "", "trainer", "=", "BertTrainer", "(", "model", ",", "optimizer", ",", "processor", ",", "args", ")", "\n", "\n", "if", "not", "args", ".", "trained_model", ":", "\n", "        ", "trainer", ".", "train", "(", ")", "\n", "#print('last epoch')", "\n", "#evaluate_split(model, processor, args, split=args.test_name, return_indices=False)", "\n", "model", "=", "torch", ".", "load", "(", "trainer", ".", "snapshot_path", ")", "\n", "\n", "", "else", ":", "\n", "        ", "model", "=", "BertForSequenceClassification", ".", "from_pretrained", "(", "args", ".", "model", ",", "num_labels", "=", "args", ".", "num_labels", ",", "pooling", "=", "args", ".", "pooling", ")", "\n", "model_", "=", "torch", ".", "load", "(", "args", ".", "trained_model", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", "\n", "state", "=", "{", "}", "\n", "for", "key", "in", "model_", ".", "state_dict", "(", ")", ".", "keys", "(", ")", ":", "\n", "            ", "new_key", "=", "key", ".", "replace", "(", "\"module.\"", ",", "\"\"", ")", "\n", "state", "[", "new_key", "]", "=", "model_", ".", "state_dict", "(", ")", "[", "key", "]", "\n", "", "model", ".", "load_state_dict", "(", "state", ")", "\n", "model", "=", "model", ".", "to", "(", "device", ")", "\n", "", "print", "(", "'best epoch'", ")", "\n", "evaluate_split", "(", "model", ",", "processor", ",", "args", ",", "split", "=", "args", ".", "dev_name", ",", "return_indices", "=", "False", ")", "\n", "evaluate_split", "(", "model", ",", "processor", ",", "args", ",", "split", "=", "args", ".", "test_name", ",", "return_indices", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_lstm.model.BertForSequenceClassification.__init__": [[84, 107], ["models.bert.model.BertPreTrainedModel.__init__", "models.bert.model.BertModel", "torch.nn.Embedding", "torch.nn.Dropout", "model.BertForSequenceClassification.apply", "torch.nn.GRU", "torch.device", "torch.nn.Linear", "torch.nn.Linear", "torch.cuda.is_available", "abs"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.classification_evaluator.ClassificationEvaluator.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "num_labels", ",", "pooling", "=", "False", ")", ":", "\n", "        ", "super", "(", "BertForSequenceClassification", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "num_labels", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "sent_dim", "=", "config", ".", "hidden_size", "\n", "self", ".", "sent_embeddings", "=", "nn", ".", "Embedding", "(", "3", ",", "sent_dim", ")", "\n", "self", ".", "no_bert_layers", "=", "1", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "pooling", "=", "pooling", "\n", "self", ".", "hidden_size", "=", "config", ".", "hidden_size", "\n", "\n", "if", "self", ".", "pooling", ":", "\n", "            ", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "2", "*", "3", "*", "config", ".", "hidden_size", ",", "num_labels", ")", "# avd_pool max_pool, last hidden state", "\n", "", "else", ":", "\n", "            ", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "1", "*", "config", ".", "hidden_size", ",", "num_labels", ")", "\n", "", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n", "# wo att", "\n", "self", ".", "lstm_final", "=", "nn", ".", "GRU", "(", "config", ".", "hidden_size", "*", "(", "abs", "(", "self", ".", "no_bert_layers", ")", "+", "1", ")", ",", "config", ".", "hidden_size", ",", "bidirectional", "=", "True", ")", "\n", "'''self.lstm_final = nn.LSTM(config.hidden_size * (abs(self.no_bert_layers) + 1), config.hidden_size,\n                                 bidirectional=True)'''", "\n", "\n", "self", ".", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_lstm.model.BertForSequenceClassification.init_hidden": [[108, 114], ["torch.zeros().to", "torch.zeros"], "methods", ["None"], ["", "def", "init_hidden", "(", "self", ",", "bs", ")", ":", "\n", "# Before we've done anything, we dont have any hidden state.", "\n", "# Refer to the Pytorch documentation to see exactly", "\n", "# why they have this dimensionality.", "\n", "# The axes semantics are (num_layers, minibatch_size, hidden_dim)", "\n", "        ", "return", "torch", ".", "zeros", "(", "2", ",", "bs", ",", "self", ".", "hidden_size", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "#return (torch.zeros(2, bs, self.hidden_size).to(self.device), torch.zeros(2, bs, self.hidden_size).to(self.device))", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_lstm.model.BertForSequenceClassification.forward": [[116, 151], ["model.BertForSequenceClassification.bert", "model.BertForSequenceClassification.dropout", "model.BertForSequenceClassification.sent_embeddings", "torch.cat().permute", "model.BertForSequenceClassification.init_hidden", "model.BertForSequenceClassification.lstm_final", "model.BertForSequenceClassification.dropout", "model.BertForSequenceClassification.classifier", "encoded_layers[].clone().detach", "torch.nn.functional.adaptive_avg_pool1d", "torch.cat", "model.BertForSequenceClassification.dropout", "torch.cat", "torch.nn.functional.adaptive_max_pool1d_with_indices", "torch.nn.functional.adaptive_max_pool1d", "model.BertForSequenceClassification.permute", "encoded_layers[].clone", "model.BertForSequenceClassification.permute", "model.BertForSequenceClassification.permute", "torch.nn.functional.adaptive_max_pool1d.view", "torch.nn.functional.adaptive_avg_pool1d.view"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_lstm_emotion.model.BertForSequenceClassification.init_hidden"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "sent_scores", "=", "None", ",", "return_indices", "=", "False", ")", ":", "\n", "\n", "        ", "encoded_layers", ",", "_", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ")", "# 12 layers each (batch_size, seg_lengh, hidden_size)", "\n", "bert_encoded_layers", "=", "self", ".", "dropout", "(", "encoded_layers", "[", "-", "1", "]", ".", "clone", "(", ")", ".", "detach", "(", ")", ")", "# (batch, sent_len, embed_dim)", "\n", "bs", ",", "_", ",", "_", "=", "bert_encoded_layers", ".", "shape", "\n", "sent_embeddings", "=", "self", ".", "sent_embeddings", "(", "sent_scores", ")", "\n", "\n", "concat", "=", "torch", ".", "cat", "(", "[", "bert_encoded_layers", ",", "sent_embeddings", "]", ",", "-", "1", ")", ".", "permute", "(", "1", ",", "0", ",", "2", ")", "\n", "\n", "\n", "\n", "hidden", "=", "self", ".", "init_hidden", "(", "bs", ")", "\n", "\n", "out_score", ",", "_", "=", "self", ".", "lstm_final", "(", "concat", ",", "hidden", ")", "# hidden added", "\n", "out_score", "=", "self", ".", "dropout", "(", "out_score", ")", "# added", "\n", "\n", "if", "self", ".", "pooling", ":", "\n", "            ", "_", ",", "bs", ",", "_", "=", "out_score", ".", "shape", "\n", "if", "return_indices", ":", "\n", "\n", "                ", "mx_pool", ",", "mx_pool_idx", "=", "F", ".", "adaptive_max_pool1d_with_indices", "(", "out_score", ".", "permute", "(", "1", ",", "2", ",", "0", ")", ",", "(", "1", ",", ")", ")", "\n", "", "else", ":", "\n", "                ", "mx_pool", "=", "F", ".", "adaptive_max_pool1d", "(", "out_score", ".", "permute", "(", "1", ",", "2", ",", "0", ")", ",", "(", "1", ",", ")", ")", "\n", "", "avg_pool", "=", "F", ".", "adaptive_avg_pool1d", "(", "out_score", ".", "permute", "(", "1", ",", "2", ",", "0", ")", ",", "(", "1", ",", ")", ")", "# (bs, input_size, seq_len)", "\n", "out_score", "=", "torch", ".", "cat", "(", "[", "mx_pool", ".", "view", "(", "bs", ",", "-", "1", ")", ",", "avg_pool", ".", "view", "(", "bs", ",", "-", "1", ")", ",", "out_score", "[", "-", "1", "]", "]", ",", "-", "1", ")", "\n", "out_score", "=", "self", ".", "dropout", "(", "out_score", ")", "\n", "\n", "", "else", ":", "\n", "            ", "out_score", "=", "out_score", "[", "-", "1", "]", "\n", "\n", "", "logits", "=", "self", ".", "classifier", "(", "out_score", ")", "\n", "if", "return_indices", "and", "self", ".", "pooling", ":", "\n", "            ", "return", "logits", ",", "mx_pool_idx", "\n", "\n", "", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_lstm.args.get_args": [[6, 51], ["models.args.get_args", "models.args.get_args.add_argument", "models.args.get_args.add_argument", "models.args.get_args.add_argument", "models.args.get_args.add_argument", "models.args.get_args.add_argument", "models.args.get_args.add_argument", "models.args.get_args.add_argument", "models.args.get_args.add_argument", "models.args.get_args.add_argument", "models.args.get_args.add_argument", "models.args.get_args.add_argument", "models.args.get_args.add_argument", "models.args.get_args.add_argument", "models.args.get_args.add_argument", "models.args.get_args.add_argument", "models.args.get_args.parse_args", "os.path.join"], "function", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_lstm_emotion.args.get_args"], ["    ", "parser", "=", "ArgumentParser", "(", "description", "=", "\"PyTorch deep learning models for document classification\"", ")", "\n", "par", "=", "'/home/projects/'", "\n", "parser", ".", "add_argument", "(", "'--no-cuda'", ",", "action", "=", "'store_false'", ",", "dest", "=", "'cuda'", ")", "\n", "parser", ".", "add_argument", "(", "'--gpu'", ",", "type", "=", "int", ",", "default", "=", "0", ")", "\n", "parser", ".", "add_argument", "(", "'--epochs'", ",", "type", "=", "int", ",", "default", "=", "50", ")", "\n", "parser", ".", "add_argument", "(", "'--batch-size'", ",", "type", "=", "int", ",", "default", "=", "1024", ")", "\n", "parser", ".", "add_argument", "(", "'--lr'", ",", "type", "=", "float", ",", "default", "=", "0.001", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "type", "=", "int", ",", "default", "=", "3435", ")", "# default value: 3435", "\n", "parser", ".", "add_argument", "(", "'--patience'", ",", "type", "=", "int", ",", "default", "=", "5", ")", "\n", "parser", ".", "add_argument", "(", "'--log-every'", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "'--data-dir'", ",", "default", "=", "os", ".", "path", ".", "join", "(", "par", ",", "'hedwig-data'", ",", "'datasets'", ")", ")", "#os.pardir", "\n", "parser", ".", "add_argument", "(", "'--early_on_f1'", ",", "action", "=", "'store_true'", ")", "\n", "\n", "return", "parser", "\n", "", ""]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.classify.get_feature_vector": [[36, 65], ["enumerate", "enumerate", "enumerate", "sklearn.feature_extraction.text.TfidfVectorizer", "sklearn.feature_extraction.text.TfidfVectorizer.fit_transform", "sklearn.feature_extraction.text.TfidfVectorizer.transform", "sklearn.feature_extraction.text.TfidfVectorizer.transform", "train_text.append", "test_text.append", "dev_text.append", "x.text_a.strip().split", "x.text_a.strip().split", "x.text_a.strip().split", "x.text_a.strip", "x.text_a.strip", "x.text_a.strip"], "function", ["None"], ["def", "get_feature_vector", "(", "evaluators", ",", "use_idf", "=", "False", ",", "ngram_range", "=", "(", "1", ",", "1", ")", ",", "max_seq_len", "=", "256", ")", ":", "\n", "\n", "    ", "train_ev", ",", "dev_ev", ",", "test_ev", "=", "evaluators", "\n", "train_text", ",", "test_text", ",", "dev_text", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "for", "i", ",", "x", "in", "enumerate", "(", "train_ev", ".", "eval_examples", ")", ":", "\n", "#tokens_a = x.text_a.strip().split()", "\n", "        ", "tokens_a", "=", "[", "t", "for", "t", "in", "x", ".", "text_a", ".", "strip", "(", ")", ".", "split", "(", ")", "if", "t", "not", "in", "[", "''", ",", "' '", "]", "]", "\n", "#tokens_a = bert_tokenizer.tokenize(x.text_a)", "\n", "tokens_a", "=", "tokens_a", "[", ":", "max_seq_len", "]", "\n", "train_text", ".", "append", "(", "' '", ".", "join", "(", "tokens_a", ")", ")", "\n", "\n", "", "for", "i", ",", "x", "in", "enumerate", "(", "test_ev", ".", "eval_examples", ")", ":", "\n", "        ", "tokens_a", "=", "[", "t", "for", "t", "in", "x", ".", "text_a", ".", "strip", "(", ")", ".", "split", "(", ")", "if", "t", "not", "in", "[", "''", ",", "' '", "]", "]", "\n", "#tokens_a = bert_tokenizer.tokenize(x.text_a)", "\n", "tokens_a", "=", "tokens_a", "[", ":", "max_seq_len", "]", "\n", "test_text", ".", "append", "(", "' '", ".", "join", "(", "tokens_a", ")", ")", "\n", "\n", "", "for", "i", ",", "x", "in", "enumerate", "(", "dev_ev", ".", "eval_examples", ")", ":", "\n", "        ", "tokens_a", "=", "[", "t", "for", "t", "in", "x", ".", "text_a", ".", "strip", "(", ")", ".", "split", "(", ")", "if", "t", "not", "in", "[", "''", ",", "' '", "]", "]", "\n", "#tokens_a = bert_tokenizer.tokenize(x.text_a)", "\n", "tokens_a", "=", "tokens_a", "[", ":", "max_seq_len", "]", "\n", "dev_text", ".", "append", "(", "' '", ".", "join", "(", "tokens_a", ")", ")", "\n", "\n", "\n", "", "tf_vect", "=", "TfidfVectorizer", "(", "use_idf", "=", "use_idf", ",", "ngram_range", "=", "ngram_range", ",", "binary", "=", "True", ")", "#change ! max_features=300 min_df=5", "\n", "train_xf", "=", "tf_vect", ".", "fit_transform", "(", "train_text", ")", "\n", "test_xf", "=", "tf_vect", ".", "transform", "(", "test_text", ")", "\n", "dev_xf", "=", "tf_vect", ".", "transform", "(", "dev_text", ")", "\n", "return", "train_xf", ",", "dev_xf", ",", "test_xf", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.classify.classification": [[66, 92], ["print", "clf.fit.predict", "clf.fit.predict", "classify.get_score", "print", "classify.get_score", "sklearn.naive_bayes.MultinomialNB().fit", "sklearn.svm.SVC().fit", "sklearn.naive_bayes.MultinomialNB", "sklearn.linear_model.LogisticRegression().fit", "sklearn.svm.SVC", "sklearn.pipeline.Pipeline", "clf.fit.set_params", "clf.fit.fit", "Exception", "sklearn.linear_model.LogisticRegression", "sklearn.feature_selection.SelectPercentile", "sklearn.preprocessing.StandardScaler", "sklearn.svm.SVC"], "function", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert.classify.get_score", "home.repos.pwc.inspect_result.marjanhs_procon20.bert.classify.get_score"], ["", "def", "classification", "(", "train", ",", "test", ",", "cls", "=", "'svc'", ")", ":", "\n", "    ", "train_x", ",", "train_y", "=", "train", "\n", "test_x", ",", "test_y", "=", "test", "\n", "\n", "\n", "print", "(", "'classifying ...'", ")", "\n", "if", "cls", "==", "'nb'", ":", "\n", "        ", "clf", "=", "MultinomialNB", "(", ")", ".", "fit", "(", "train_x", ",", "train_y", ")", "\n", "", "elif", "cls", "==", "'svc'", ":", "\n", "        ", "clf", "=", "SVC", "(", "kernel", "=", "'linear'", ")", ".", "fit", "(", "train_x", ",", "train_y", ")", "\n", "", "elif", "cls", "==", "'lr'", ":", "\n", "        ", "clf", "=", "LogisticRegression", "(", ")", ".", "fit", "(", "train_x", ",", "train_y", ")", "\n", "", "elif", "cls", "==", "'psvc'", ":", "\n", "        ", "clf", "=", "Pipeline", "(", "[", "(", "'anova'", ",", "SelectPercentile", "(", "chi2", ")", ")", ",", "\n", "(", "'scaler'", ",", "StandardScaler", "(", ")", ")", ",", "\n", "(", "'svc'", ",", "SVC", "(", "kernel", "=", "'linear'", ")", ")", "]", ")", "\n", "clf", ".", "set_params", "(", "anova__percentile", "=", "80", ")", "\n", "clf", "=", "clf", ".", "fit", "(", "train_x", ",", "train_y", ")", "\n", "", "else", ":", "\n", "        ", "raise", "Exception", "(", "'Error in classifier name!'", ")", "\n", "\n", "", "test_predicted", "=", "clf", ".", "predict", "(", "test_x", ")", "\n", "train_predicted", "=", "clf", ".", "predict", "(", "train_x", ")", "\n", "get_score", "(", "train_y", ",", "train_predicted", ")", "\n", "print", "(", "cls", ")", "\n", "return", "get_score", "(", "test_y", ",", "test_predicted", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.classify.get_score": [[93, 102], ["sklearn.metrics.accuracy_score", "sklearn.metrics.precision_score", "sklearn.metrics.recall_score", "sklearn.metrics.f1_score"], "function", ["None"], ["", "def", "get_score", "(", "true_y", ",", "predicted_y", ",", "is_multilabel", "=", "False", ")", ":", "\n", "    ", "avg", "=", "'micro'", "if", "is_multilabel", "else", "'binary'", "\n", "target_labels", ",", "predicted_labels", "=", "true_y", ",", "predicted_y", "\n", "accuracy", "=", "metrics", ".", "accuracy_score", "(", "target_labels", ",", "predicted_labels", ")", "\n", "precision", "=", "metrics", ".", "precision_score", "(", "target_labels", ",", "predicted_labels", ",", "average", "=", "avg", ")", "\n", "recall", "=", "metrics", ".", "recall_score", "(", "target_labels", ",", "predicted_labels", ",", "average", "=", "avg", ")", "\n", "f1", "=", "metrics", ".", "f1_score", "(", "target_labels", ",", "predicted_labels", ",", "average", "=", "avg", ")", "\n", "d", "=", "{", "'acc'", ":", "accuracy", ",", "'pr'", ":", "precision", ",", "'rc'", ":", "recall", ",", "'f1'", ":", "f1", "}", "\n", "return", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.classify.evaluate": [[104, 152], ["common.evaluators.bert_evaluator.BertEvaluator", "common.evaluators.bert_evaluator.BertEvaluator", "common.evaluators.bert_evaluator.BertEvaluator", "time.time", "common.evaluators.bert_evaluator.BertEvaluator.get_bert_layers", "common.evaluators.bert_evaluator.BertEvaluator.get_bert_layers", "common.evaluators.bert_evaluator.BertEvaluator.get_bert_layers", "classify.get_feature_vector", "train_xf.toarray.toarray", "train_layers.cpu().data.numpy.cpu().data.numpy", "numpy.concatenate", "dev_xf.toarray.toarray", "dev_layers.cpu().data.numpy.cpu().data.numpy", "numpy.concatenate", "test_xf.toarray.toarray", "tst_layers.cpu().data.numpy.cpu().data.numpy", "numpy.concatenate", "classify.scatter_plot", "print", "print", "classify.classification", "classify.classification", "print", "print", "print", "LOG_TEMPLATE.format", "LOG_TEMPLATE.format", "time.time", "train_layers.cpu().data.numpy.cpu", "dev_layers.cpu().data.numpy.cpu", "tst_layers.cpu().data.numpy.cpu"], "function", ["home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.bert_emotion_evaluator.BertEvaluator.get_bert_layers", "home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.bert_emotion_evaluator.BertEvaluator.get_bert_layers", "home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.bert_emotion_evaluator.BertEvaluator.get_bert_layers", "home.repos.pwc.inspect_result.marjanhs_procon20.bert.classify.get_feature_vector", "home.repos.pwc.inspect_result.marjanhs_procon20.bert.classify.scatter_plot", "home.repos.pwc.inspect_result.marjanhs_procon20.bert.classify.classification", "home.repos.pwc.inspect_result.marjanhs_procon20.bert.classify.classification"], ["", "def", "evaluate", "(", "model", ",", "processor", ",", "args", ",", "last_bert_layers", "=", "-", "1", ",", "ngram_range", "=", "(", "1", ",", "1", ")", ")", ":", "\n", "\n", "\n", "\n", "\n", "    ", "train_evaluator", "=", "BertEvaluator", "(", "model", ",", "processor", ",", "args", ",", "args", ".", "train_name", ")", "\n", "dev_evaluator", "=", "BertEvaluator", "(", "model", ",", "processor", ",", "args", ",", "args", ".", "dev_name", ")", "\n", "tst_evaluator", "=", "BertEvaluator", "(", "model", ",", "processor", ",", "args", ",", "args", ".", "test_name", ")", "\n", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "train_layers", ",", "train_labels", "=", "train_evaluator", ".", "get_bert_layers", "(", "silent", "=", "True", ",", "last_bert_layers", "=", "last_bert_layers", ")", "\n", "dev_layers", ",", "dev_labels", "=", "dev_evaluator", ".", "get_bert_layers", "(", "silent", "=", "True", ",", "last_bert_layers", "=", "last_bert_layers", ")", "\n", "tst_layers", ",", "tst_labels", "=", "tst_evaluator", ".", "get_bert_layers", "(", "silent", "=", "True", ",", "last_bert_layers", "=", "last_bert_layers", ")", "\n", "\n", "train_xf", ",", "dev_xf", ",", "test_xf", "=", "get_feature_vector", "(", "(", "train_evaluator", ",", "dev_evaluator", ",", "tst_evaluator", ")", ",", "ngram_range", "=", "ngram_range", ",", "max_seq_len", "=", "args", ".", "max_seq_length", ")", "\n", "\n", "\n", "# train", "\n", "train_xf", "=", "train_xf", ".", "toarray", "(", ")", "\n", "train_layers", "=", "train_layers", ".", "cpu", "(", ")", ".", "data", ".", "numpy", "(", ")", "\n", "train_x", "=", "np", ".", "concatenate", "(", "(", "train_layers", ",", "train_xf", ")", ",", "axis", "=", "1", ")", "\n", "\n", "#dev", "\n", "dev_xf", "=", "dev_xf", ".", "toarray", "(", ")", "\n", "dev_layers", "=", "dev_layers", ".", "cpu", "(", ")", ".", "data", ".", "numpy", "(", ")", "\n", "dev_x", "=", "np", ".", "concatenate", "(", "(", "dev_layers", ",", "dev_xf", ")", ",", "axis", "=", "1", ")", "\n", "\n", "#test", "\n", "test_xf", "=", "test_xf", ".", "toarray", "(", ")", "\n", "tst_layers", "=", "tst_layers", ".", "cpu", "(", ")", ".", "data", ".", "numpy", "(", ")", "\n", "test_x", "=", "np", ".", "concatenate", "(", "(", "tst_layers", ",", "test_xf", ")", ",", "axis", "=", "1", ")", "\n", "\n", "\n", "#train, tst, dev = (train_x, train_labels), (test_x, tst_labels), (dev_x, dev_labels)", "\n", "train", ",", "tst", ",", "dev", "=", "(", "train_xf", ",", "train_labels", ")", ",", "(", "test_xf", ",", "tst_labels", ")", ",", "(", "dev_xf", ",", "dev_labels", ")", "\n", "#print('train labels length', len(train_labels), train_labels[0])", "\n", "#train, tst, dev = (train_layers, train_labels), (tst_layers, tst_labels), (dev_layers, dev_labels)", "\n", "scatter_plot", "(", "train", ",", "dev", ",", "tst", ")", "\n", "#train, tst, dev = (train_both_models, train_labels), (tst_both_models, tst_labels), (dev_both_models, dev_labels)", "\n", "\n", "\n", "print", "(", "'train, test shape : '", ",", "train", "[", "0", "]", ".", "shape", ",", "tst", "[", "0", "]", ".", "shape", ")", "\n", "print", "(", "\"Inference time\"", ",", "time", ".", "time", "(", ")", "-", "start_time", ")", "\n", "r_test", "=", "classification", "(", "train", ",", "tst", ")", "\n", "r_dev", "=", "classification", "(", "train", ",", "dev", ")", "\n", "print", "(", "'\\n'", "+", "LOG_HEADER", ")", "\n", "print", "(", "LOG_TEMPLATE", ".", "format", "(", "\"DEV\"", ",", "r_dev", "[", "'acc'", "]", ",", "r_dev", "[", "'pr'", "]", ",", "r_dev", "[", "'rc'", "]", ",", "r_dev", "[", "'f1'", "]", ")", ")", "\n", "print", "(", "LOG_TEMPLATE", ".", "format", "(", "\"TEST\"", ",", "r_test", "[", "'acc'", "]", ",", "r_test", "[", "'pr'", "]", ",", "r_test", "[", "'rc'", "]", ",", "r_test", "[", "'f1'", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.classify.scatter_plot": [[154, 182], ["numpy.concatenate", "sklearn.decomposition.PCA().fit_transform", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "matplotlib.scatter", "matplotlib.scatter", "matplotlib.scatter", "matplotlib.scatter", "matplotlib.savefig", "sklearn.decomposition.PCA", "zip", "zip", "zip", "zip", "len", "len", "len", "len"], "function", ["None"], ["", "def", "scatter_plot", "(", "train", ",", "dev", ",", "tst", ")", ":", "\n", "\n", "    ", "'''print(train[0].shape)\n    x = np.concatenate((train[0] , dev[0] , tst[0]))\n    y = train[1] + dev[1] + tst[1]'''", "\n", "\n", "x_train", ",", "labels_train", "=", "train", "\n", "x_test", ",", "labels_test", "=", "tst", "\n", "\n", "x", "=", "np", ".", "concatenate", "(", "(", "x_train", ",", "x_test", ")", ")", "\n", "\n", "\n", "\n", "x", "=", "PCA", "(", "n_components", "=", "2", ")", ".", "fit_transform", "(", "x", ")", "\n", "#x = TSNE(n_components=2).fit_transform(x)", "\n", "pos_train", "=", "np", ".", "array", "(", "[", "t", "[", "0", "]", "for", "t", "in", "zip", "(", "x", "[", ":", "len", "(", "x_train", ")", ",", "]", ",", "labels_train", ")", "if", "t", "[", "1", "]", "==", "1", "]", ")", "\n", "neg_train", "=", "np", ".", "array", "(", "[", "t", "[", "0", "]", "for", "t", "in", "zip", "(", "x", "[", ":", "len", "(", "x_train", ")", ",", "]", ",", "labels_train", ")", "if", "t", "[", "1", "]", "==", "0", "]", ")", "\n", "\n", "pos_test", "=", "np", ".", "array", "(", "[", "t", "[", "0", "]", "for", "t", "in", "zip", "(", "x", "[", "len", "(", "x_train", ")", ":", ",", "]", ",", "labels_test", ")", "if", "t", "[", "1", "]", "==", "1", "]", ")", "\n", "neg_test", "=", "np", ".", "array", "(", "[", "t", "[", "0", "]", "for", "t", "in", "zip", "(", "x", "[", "len", "(", "x_train", ")", ":", ",", "]", ",", "labels_test", ")", "if", "t", "[", "1", "]", "==", "0", "]", ")", "\n", "\n", "#plt.scatter(x[:, 0],x[:, 1], c=colors, alpha=0.5)", "\n", "plt", ".", "scatter", "(", "pos_train", "[", ":", ",", "0", "]", ",", "pos_train", "[", ":", ",", "1", "]", ",", "c", "=", "'k'", ",", "marker", "=", "'+'", ",", "alpha", "=", "1", ")", "\n", "plt", ".", "scatter", "(", "neg_train", "[", ":", ",", "0", "]", ",", "neg_train", "[", ":", ",", "1", "]", ",", "c", "=", "'b'", ",", "marker", "=", "'o'", ",", "alpha", "=", "0.5", ")", "\n", "\n", "plt", ".", "scatter", "(", "pos_test", "[", ":", ",", "0", "]", ",", "pos_test", "[", ":", ",", "1", "]", ",", "c", "=", "'r'", ",", "marker", "=", "'+'", ",", "alpha", "=", "1", ")", "\n", "plt", ".", "scatter", "(", "neg_test", "[", ":", ",", "0", "]", ",", "neg_test", "[", ":", ",", "1", "]", ",", "c", "=", "'c'", ",", "marker", "=", "'o'", ",", "alpha", "=", "0.5", ")", "\n", "plt", ".", "savefig", "(", "'news_art_pers.png'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.classify.do_main": [[185, 266], ["models.bert.args.get_args", "print", "print", "print", "print", "random.seed", "numpy.random.seed", "torch.manual_seed", "utils.tokenization.BertTokenizer.from_pretrained", "models.bert.model.BertForSequenceClassification.from_pretrained", "model_bert.to.to", "classify.evaluate", "torch.device", "torch.cuda.device_count", "torch.cuda.set_device", "torch.cuda.set_device", "torch.device", "torch.distributed.init_process_group", "str().upper", "bool", "torch.cuda.manual_seed_all", "ValueError", "ValueError", "os.path.join", "os.makedirs", "processor.get_train_examples", "torch.load", "torch.load.state_dict().keys", "model_bert.to.load_state_dict", "model_bert.to.to", "int", "key.replace", "str", "torch.distributed.get_world_size", "torch.load.state_dict", "torch.load.state_dict", "torch.cuda.is_available", "math.ceil", "math.ceil", "len"], "function", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_lstm_emotion.args.get_args", "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertPreTrainedModel.from_pretrained", "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertPreTrainedModel.from_pretrained", "home.repos.pwc.inspect_result.marjanhs_procon20.trainers.trainer.Trainer.evaluate", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor.get_train_examples"], ["", "def", "do_main", "(", ")", ":", "\n", "# Set default configuration in args.py", "\n", "    ", "args", "=", "get_args", "(", ")", "\n", "\n", "if", "args", ".", "local_rank", "==", "-", "1", "or", "not", "args", ".", "cuda", ":", "\n", "        ", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "args", ".", "cuda", "else", "\"cpu\"", ")", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "gpu", ")", "\n", "", "else", ":", "\n", "        ", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "local_rank", ")", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ",", "args", ".", "local_rank", ")", "\n", "n_gpu", "=", "1", "\n", "# Initializes the distributed backend which will take care of sychronizing nodes/GPUs", "\n", "torch", ".", "distributed", ".", "init_process_group", "(", "backend", "=", "'nccl'", ")", "\n", "\n", "", "print", "(", "'Device:'", ",", "str", "(", "device", ")", ".", "upper", "(", ")", ")", "\n", "print", "(", "'Number of GPUs:'", ",", "n_gpu", ")", "\n", "print", "(", "'Distributed training:'", ",", "bool", "(", "args", ".", "local_rank", "!=", "-", "1", ")", ")", "\n", "print", "(", "'FP16:'", ",", "args", ".", "fp16", ")", "\n", "\n", "# Set random seed for reproducibility", "\n", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "if", "n_gpu", ">", "0", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n", "", "dataset_map", "=", "{", "\n", "\n", "'News_art'", ":", "News_artProcessor", ",", "\n", "'News'", ":", "News_Processor", "\n", "}", "\n", "\n", "if", "args", ".", "gradient_accumulation_steps", "<", "1", ":", "\n", "        ", "raise", "ValueError", "(", "\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\"", ".", "format", "(", "\n", "args", ".", "gradient_accumulation_steps", ")", ")", "\n", "\n", "", "if", "args", ".", "dataset", "not", "in", "dataset_map", ":", "\n", "        ", "raise", "ValueError", "(", "'Unrecognized dataset'", ")", "\n", "\n", "", "args", ".", "batch_size", "=", "args", ".", "batch_size", "//", "args", ".", "gradient_accumulation_steps", "\n", "args", ".", "device", "=", "device", "\n", "args", ".", "n_gpu", "=", "n_gpu", "\n", "args", ".", "num_labels", "=", "dataset_map", "[", "args", ".", "dataset", "]", ".", "NUM_CLASSES", "\n", "args", ".", "is_multilabel", "=", "dataset_map", "[", "args", ".", "dataset", "]", ".", "IS_MULTILABEL", "\n", "\n", "if", "not", "args", ".", "trained_model", ":", "\n", "        ", "save_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "save_path", ",", "dataset_map", "[", "args", ".", "dataset", "]", ".", "NAME", ")", "\n", "os", ".", "makedirs", "(", "save_path", ",", "exist_ok", "=", "True", ")", "\n", "\n", "", "processor", "=", "dataset_map", "[", "args", ".", "dataset", "]", "(", ")", "\n", "args", ".", "is_lowercase", "=", "'uncased'", "in", "args", ".", "model", "\n", "args", ".", "is_hierarchical", "=", "False", "\n", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "args", ".", "model", ",", "is_lowercase", "=", "args", ".", "is_lowercase", ")", "\n", "\n", "num_train_optimization_steps", "=", "None", "\n", "if", "args", ".", "trained_model", ":", "\n", "        ", "train_examples", "=", "processor", ".", "get_train_examples", "(", "args", ".", "data_dir", ",", "args", ".", "train_name", ")", "\n", "num_train_optimization_steps", "=", "int", "(", "\n", "math", ".", "ceil", "(", "len", "(", "train_examples", ")", "/", "args", ".", "batch_size", ")", "/", "args", ".", "gradient_accumulation_steps", ")", "*", "args", ".", "epochs", "\n", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "            ", "num_train_optimization_steps", "=", "num_train_optimization_steps", "//", "torch", ".", "distributed", ".", "get_world_size", "(", ")", "\n", "\n", "", "", "model_bert", "=", "BertForSequenceClassification", ".", "from_pretrained", "(", "args", ".", "model", ",", "num_labels", "=", "4", ")", "\n", "\n", "\n", "model_bert", ".", "to", "(", "device", ")", "\n", "\n", "if", "args", ".", "trained_model", ":", "\n", "        ", "model_", "=", "torch", ".", "load", "(", "args", ".", "trained_model", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", "# load personality model", "\n", "state", "=", "{", "}", "\n", "for", "key", "in", "model_", ".", "state_dict", "(", ")", ".", "keys", "(", ")", ":", "\n", "            ", "new_key", "=", "key", ".", "replace", "(", "\"module.\"", ",", "\"\"", ")", "\n", "state", "[", "new_key", "]", "=", "model_", ".", "state_dict", "(", ")", "[", "key", "]", "\n", "\n", "", "del", "state", "[", "'classifier.weight'", "]", "# removing  personality classifier!", "\n", "del", "state", "[", "'classifier.bias'", "]", "\n", "model_bert", ".", "load_state_dict", "(", "state", ",", "strict", "=", "False", ")", "\n", "model_bert", "=", "model_bert", ".", "to", "(", "device", ")", "\n", "", "args", ".", "freez_bert", "=", "False", "\n", "evaluate", "(", "model_bert", ",", "processor", ",", "args", ",", "last_bert_layers", "=", "-", "1", ",", "ngram_range", "=", "(", "1", ",", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.personality_analyze.do_personality_analysis": [[30, 35], ["print", "print", "numpy.save", "numpy.save"], "function", ["None"], ["def", "do_personality_analysis", "(", "labels", ",", "set_name", ",", "path", ")", ":", "\n", "    ", "print", "(", "labels", "[", "0", "]", ".", "shape", ",", "labels", "[", "1", "]", ".", "shape", ")", "\n", "print", "(", "'files saved at '", ",", "path", ")", "\n", "np", ".", "save", "(", "path", "/", "f'predicted_{set_name}.npy'", ",", "labels", "[", "0", "]", ",", ")", "\n", "np", ".", "save", "(", "path", "/", "f'target_{set_name}.npy'", ",", "labels", "[", "1", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.personality_analyze.evaluate_split": [[37, 47], ["pathlib.Path", "pathlib.Path.mkdir", "common.evaluators.bert_evaluator.BertEvaluator", "time.time", "personality_analyze.do_personality_analysis", "common.evaluators.bert_evaluator.BertEvaluator.get_scores"], "function", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert.personality_analyze.do_personality_analysis", "home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.classification_evaluator.ClassificationEvaluator.get_scores"], ["", "def", "evaluate_split", "(", "model", ",", "processor", ",", "args", ",", "split", "=", "'dev'", ")", ":", "\n", "    ", "root", "=", "Path", "(", "'out/bert'", ")", "\n", "root", ".", "mkdir", "(", "exist_ok", "=", "True", ")", "\n", "evaluator", "=", "BertEvaluator", "(", "model", ",", "processor", ",", "args", ",", "split", ")", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "accuracy", ",", "precision", ",", "recall", ",", "f1", ",", "avg_loss", ",", "f1_mac", ",", "hamming_loss", ",", "jacc", ",", "predicted_labels", ",", "target_labels", "=", "evaluator", ".", "get_scores", "(", "silent", "=", "True", ")", "[", "0", "]", "\n", "'''print(\"Inference time\", time.time() - start_time)\n    print('\\n' + LOG_HEADER)\n    print(LOG_TEMPLATE.format(split.upper(), accuracy, precision, recall, f1, avg_loss, f1_mac, hamming_loss, jacc))'''", "\n", "do_personality_analysis", "(", "[", "predicted_labels", ",", "target_labels", "]", ",", "split", ",", "root", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.personality_analyze.do_main": [[49, 114], ["models.bert.args.get_args", "print", "print", "print", "print", "random.seed", "numpy.random.seed", "torch.manual_seed", "torch.device", "torch.cuda.device_count", "torch.cuda.set_device", "torch.cuda.set_device", "torch.device", "torch.distributed.init_process_group", "str().upper", "bool", "torch.cuda.manual_seed_all", "ValueError", "ValueError", "Exception", "models.bert.model.BertForSequenceClassification.from_pretrained", "torch.load", "torch.load.state_dict().keys", "model.to.load_state_dict", "model.to.to", "personality_analyze.evaluate_split", "key.replace", "str", "torch.load.state_dict", "torch.load.state_dict", "torch.cuda.is_available"], "function", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_lstm_emotion.args.get_args", "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertPreTrainedModel.from_pretrained", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_lstm_emotion.__main__.evaluate_split"], ["", "def", "do_main", "(", ")", ":", "\n", "# Set default configuration in args.py", "\n", "    ", "args", "=", "get_args", "(", ")", "\n", "\n", "if", "args", ".", "local_rank", "==", "-", "1", "or", "not", "args", ".", "cuda", ":", "\n", "        ", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "args", ".", "cuda", "else", "\"cpu\"", ")", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "gpu", ")", "\n", "", "else", ":", "\n", "        ", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "local_rank", ")", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ",", "args", ".", "local_rank", ")", "\n", "n_gpu", "=", "1", "\n", "# Initializes the distributed backend which will take care of sychronizing nodes/GPUs", "\n", "torch", ".", "distributed", ".", "init_process_group", "(", "backend", "=", "'nccl'", ")", "\n", "\n", "", "print", "(", "'Device:'", ",", "str", "(", "device", ")", ".", "upper", "(", ")", ")", "\n", "print", "(", "'Number of GPUs:'", ",", "n_gpu", ")", "\n", "print", "(", "'Distributed training:'", ",", "bool", "(", "args", ".", "local_rank", "!=", "-", "1", ")", ")", "\n", "print", "(", "'FP16:'", ",", "args", ".", "fp16", ")", "\n", "\n", "# Set random seed for reproducibility", "\n", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "if", "n_gpu", ">", "0", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n", "", "dataset_map", "=", "{", "\n", "\n", "'Personality'", ":", "PersonalityProcessor", "\n", "}", "\n", "\n", "if", "args", ".", "gradient_accumulation_steps", "<", "1", ":", "\n", "        ", "raise", "ValueError", "(", "\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\"", ".", "format", "(", "\n", "args", ".", "gradient_accumulation_steps", ")", ")", "\n", "\n", "", "if", "args", ".", "dataset", "not", "in", "dataset_map", ":", "\n", "        ", "raise", "ValueError", "(", "'Unrecognized dataset'", ")", "\n", "\n", "", "args", ".", "batch_size", "=", "args", ".", "batch_size", "//", "args", ".", "gradient_accumulation_steps", "\n", "args", ".", "device", "=", "device", "\n", "args", ".", "n_gpu", "=", "n_gpu", "\n", "args", ".", "num_labels", "=", "dataset_map", "[", "args", ".", "dataset", "]", ".", "NUM_CLASSES", "\n", "args", ".", "is_multilabel", "=", "dataset_map", "[", "args", ".", "dataset", "]", ".", "IS_MULTILABEL", "\n", "\n", "if", "not", "args", ".", "trained_model", ":", "\n", "        ", "raise", "Exception", "(", "'This method only works wit pre-trained models!'", ")", "\n", "\n", "", "processor", "=", "dataset_map", "[", "args", ".", "dataset", "]", "(", ")", "\n", "args", ".", "is_lowercase", "=", "'uncased'", "in", "args", ".", "model", "\n", "args", ".", "is_hierarchical", "=", "False", "\n", "\n", "if", "args", ".", "trained_model", ":", "\n", "        ", "model", "=", "BertForSequenceClassification", ".", "from_pretrained", "(", "args", ".", "model", ",", "num_labels", "=", "args", ".", "num_labels", ")", "\n", "model_", "=", "torch", ".", "load", "(", "args", ".", "trained_model", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", "\n", "state", "=", "{", "}", "\n", "for", "key", "in", "model_", ".", "state_dict", "(", ")", ".", "keys", "(", ")", ":", "\n", "            ", "new_key", "=", "key", ".", "replace", "(", "\"module.\"", ",", "\"\"", ")", "\n", "state", "[", "new_key", "]", "=", "model_", ".", "state_dict", "(", ")", "[", "key", "]", "\n", "", "model", ".", "load_state_dict", "(", "state", ")", "\n", "model", "=", "model", ".", "to", "(", "device", ")", "\n", "\n", "#evaluate_split(model, processor, args, split='dev')", "\n", "#evaluate_split(model, processor, args, split='test')", "\n", "evaluate_split", "(", "model", ",", "processor", ",", "args", ",", "split", "=", "args", ".", "analyze_split", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.transfer_learning.evaluate_split": [[23, 30], ["common.evaluators.bert_evaluator.BertEvaluator", "time.time", "print", "print", "print", "common.evaluators.bert_evaluator.BertEvaluator.get_scores", "LOG_TEMPLATE.format", "time.time", "split.upper"], "function", ["home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.classification_evaluator.ClassificationEvaluator.get_scores"], ["def", "evaluate_split", "(", "model", ",", "processor", ",", "args", ",", "split", "=", "'dev'", ")", ":", "\n", "    ", "evaluator", "=", "BertEvaluator", "(", "model", ",", "processor", ",", "args", ",", "split", ")", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "accuracy", ",", "precision", ",", "recall", ",", "f1", ",", "avg_loss", ",", "f1_mac", ",", "hamming_loss", ",", "jacc", ",", "_", ",", "_", "=", "evaluator", ".", "get_scores", "(", "silent", "=", "True", ")", "[", "0", "]", "\n", "print", "(", "\"Inference time\"", ",", "time", ".", "time", "(", ")", "-", "start_time", ")", "\n", "print", "(", "'\\n'", "+", "LOG_HEADER", ")", "\n", "print", "(", "LOG_TEMPLATE", ".", "format", "(", "split", ".", "upper", "(", ")", ",", "accuracy", ",", "precision", ",", "recall", ",", "f1", ",", "avg_loss", ",", "f1_mac", ",", "hamming_loss", ",", "jacc", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.transfer_learning.do_main": [[32, 136], ["models.bert.args.get_args", "print", "print", "print", "print", "random.seed", "numpy.random.seed", "torch.manual_seed", "utils.tokenization.BertTokenizer.from_pretrained", "models.bert.model.BertForSequenceClassification.from_pretrained", "torch.load.to", "torch.load", "torch.load.state_dict().keys", "torch.load.load_state_dict", "torch.load.to", "list", "print", "utils.optimization.BertAdam", "common.trainers.bert_trainer.BertTrainer", "common.trainers.bert_trainer.BertTrainer.train", "torch.load", "transfer_learning.evaluate_split", "transfer_learning.evaluate_split", "torch.device", "torch.cuda.device_count", "torch.cuda.set_device", "torch.cuda.set_device", "torch.device", "torch.distributed.init_process_group", "str().upper", "bool", "torch.cuda.manual_seed_all", "ValueError", "ValueError", "os.path.join", "os.makedirs", "processor.get_train_examples", "os.path.join", "torch.load.half", "key.replace", "torch.load.named_parameters", "int", "str", "torch.load.state_dict", "torch.load.state_dict", "str", "torch.distributed.get_world_size", "torch.cuda.is_available", "math.ceil", "any", "any", "len"], "function", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_lstm_emotion.args.get_args", "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertPreTrainedModel.from_pretrained", "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertPreTrainedModel.from_pretrained", "home.repos.pwc.inspect_result.marjanhs_procon20.trainers.bert_sent_trainer.BertTrainer.train", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_lstm_emotion.__main__.evaluate_split", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_lstm_emotion.__main__.evaluate_split", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor.get_train_examples"], ["", "def", "do_main", "(", ")", ":", "\n", "# Set default configuration in args.py", "\n", "    ", "args", "=", "get_args", "(", ")", "\n", "\n", "if", "args", ".", "local_rank", "==", "-", "1", "or", "not", "args", ".", "cuda", ":", "\n", "        ", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "args", ".", "cuda", "else", "\"cpu\"", ")", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "gpu", ")", "\n", "", "else", ":", "\n", "        ", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "local_rank", ")", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ",", "args", ".", "local_rank", ")", "\n", "n_gpu", "=", "1", "\n", "# Initializes the distributed backend which will take care of sychronizing nodes/GPUs", "\n", "torch", ".", "distributed", ".", "init_process_group", "(", "backend", "=", "'nccl'", ")", "\n", "\n", "", "print", "(", "'Device:'", ",", "str", "(", "device", ")", ".", "upper", "(", ")", ")", "\n", "print", "(", "'Number of GPUs:'", ",", "n_gpu", ")", "\n", "print", "(", "'Distributed training:'", ",", "bool", "(", "args", ".", "local_rank", "!=", "-", "1", ")", ")", "\n", "print", "(", "'FP16:'", ",", "args", ".", "fp16", ")", "\n", "\n", "# Set random seed for reproducibility", "\n", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "if", "n_gpu", ">", "0", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n", "", "dataset_map", "=", "{", "\n", "\n", "'News_art'", ":", "News_artProcessor", ",", "\n", "'News'", ":", "News_Processor", "\n", "}", "\n", "\n", "if", "args", ".", "gradient_accumulation_steps", "<", "1", ":", "\n", "        ", "raise", "ValueError", "(", "\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\"", ".", "format", "(", "\n", "args", ".", "gradient_accumulation_steps", ")", ")", "\n", "\n", "", "if", "args", ".", "dataset", "not", "in", "dataset_map", ":", "\n", "        ", "raise", "ValueError", "(", "'Unrecognized dataset'", ")", "\n", "\n", "", "args", ".", "batch_size", "=", "args", ".", "batch_size", "//", "args", ".", "gradient_accumulation_steps", "\n", "args", ".", "device", "=", "device", "\n", "args", ".", "n_gpu", "=", "n_gpu", "\n", "args", ".", "num_labels", "=", "dataset_map", "[", "args", ".", "dataset", "]", ".", "NUM_CLASSES", "\n", "args", ".", "is_multilabel", "=", "dataset_map", "[", "args", ".", "dataset", "]", ".", "IS_MULTILABEL", "\n", "\n", "if", "not", "args", ".", "trained_model", ":", "\n", "        ", "save_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "save_path", ",", "dataset_map", "[", "args", ".", "dataset", "]", ".", "NAME", ")", "\n", "os", ".", "makedirs", "(", "save_path", ",", "exist_ok", "=", "True", ")", "\n", "\n", "", "processor", "=", "dataset_map", "[", "args", ".", "dataset", "]", "(", ")", "\n", "args", ".", "is_lowercase", "=", "'uncased'", "in", "args", ".", "model", "\n", "args", ".", "is_hierarchical", "=", "False", "\n", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "args", ".", "model", ",", "is_lowercase", "=", "args", ".", "is_lowercase", ")", "\n", "\n", "train_examples", "=", "None", "\n", "num_train_optimization_steps", "=", "None", "\n", "if", "args", ".", "trained_model", ":", "\n", "        ", "train_examples", "=", "processor", ".", "get_train_examples", "(", "args", ".", "data_dir", ",", "args", ".", "train_name", ")", "\n", "num_train_optimization_steps", "=", "int", "(", "\n", "math", ".", "ceil", "(", "len", "(", "train_examples", ")", "/", "args", ".", "batch_size", ")", "/", "args", ".", "gradient_accumulation_steps", ")", "*", "args", ".", "epochs", "\n", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "            ", "num_train_optimization_steps", "=", "num_train_optimization_steps", "//", "torch", ".", "distributed", ".", "get_world_size", "(", ")", "\n", "\n", "", "", "cache_dir", "=", "args", ".", "cache_dir", "if", "args", ".", "cache_dir", "else", "os", ".", "path", ".", "join", "(", "str", "(", "PYTORCH_PRETRAINED_BERT_CACHE", ")", ",", "'distributed_{}'", ".", "format", "(", "args", ".", "local_rank", ")", ")", "\n", "model", "=", "BertForSequenceClassification", ".", "from_pretrained", "(", "args", ".", "model", ",", "num_labels", "=", "2", ")", "# creating news model!", "\n", "#model = BertForSequenceClassification.from_pretrained(args.model, cache_dir=cache_dir, num_labels=args.num_labels)", "\n", "\n", "if", "args", ".", "fp16", ":", "\n", "        ", "model", ".", "half", "(", ")", "\n", "", "model", ".", "to", "(", "device", ")", "\n", "\n", "#model = BertForSequenceClassification.from_pretrained(args.model, num_labels=args.num_labels)", "\n", "model_", "=", "torch", ".", "load", "(", "args", ".", "trained_model", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", "# load personality model", "\n", "state", "=", "{", "}", "\n", "for", "key", "in", "model_", ".", "state_dict", "(", ")", ".", "keys", "(", ")", ":", "\n", "        ", "new_key", "=", "key", ".", "replace", "(", "\"module.\"", ",", "\"\"", ")", "\n", "state", "[", "new_key", "]", "=", "model_", ".", "state_dict", "(", ")", "[", "key", "]", "\n", "\n", "", "del", "state", "[", "'classifier.weight'", "]", "# removing  personality classifier!", "\n", "del", "state", "[", "'classifier.bias'", "]", "\n", "model", ".", "load_state_dict", "(", "state", ",", "strict", "=", "False", ")", "\n", "model", "=", "model", ".", "to", "(", "device", ")", "\n", "\n", "# Prepare optimizer", "\n", "param_optimizer", "=", "list", "(", "model", ".", "named_parameters", "(", ")", ")", "\n", "no_decay", "=", "[", "'bias'", ",", "'LayerNorm.bias'", ",", "'LayerNorm.weight'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.0", "}", "]", "\n", "\n", "print", "(", "'t_total :'", ",", "num_train_optimization_steps", ")", "\n", "optimizer", "=", "BertAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "args", ".", "lr", ",", "\n", "warmup", "=", "args", ".", "warmup_proportion", ",", "\n", "t_total", "=", "num_train_optimization_steps", ")", "\n", "args", ".", "freez_bert", "=", "False", "\n", "trainer", "=", "BertTrainer", "(", "model", ",", "optimizer", ",", "processor", ",", "args", ")", "\n", "\n", "trainer", ".", "train", "(", ")", "\n", "model", "=", "torch", ".", "load", "(", "trainer", ".", "snapshot_path", ")", "\n", "\n", "evaluate_split", "(", "model", ",", "processor", ",", "args", ",", "split", "=", "args", ".", "dev_name", ")", "\n", "evaluate_split", "(", "model", ",", "processor", ",", "args", ",", "split", "=", "args", ".", "test_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.__main__main.evaluate_split": [[41, 56], ["common.evaluators.bert_evaluator.BertEvaluator", "time.time", "print", "print", "print", "args.save_path.replace", "pathlib.Path", "pathlib.Path.mkdir", "print", "numpy.save", "numpy.save", "common.evaluators.bert_evaluator.BertEvaluator.get_scores", "LOG_TEMPLATE.format", "args.save_path.replace", "time.time", "split.upper"], "function", ["home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.classification_evaluator.ClassificationEvaluator.get_scores"], ["def", "evaluate_split", "(", "model", ",", "processor", ",", "args", ",", "split", "=", "'dev'", ")", ":", "\n", "    ", "evaluator", "=", "BertEvaluator", "(", "model", ",", "processor", ",", "args", ",", "split", ")", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "accuracy", ",", "precision", ",", "recall", ",", "f1", ",", "avg_loss", ",", "f1_mac", ",", "hamming_loss", ",", "jacc", ",", "predicted_values", ",", "target_values", "=", "evaluator", ".", "get_scores", "(", "silent", "=", "True", ")", "[", "0", "]", "\n", "print", "(", "\"Inference time\"", ",", "time", ".", "time", "(", ")", "-", "start_time", ")", "\n", "print", "(", "'\\n'", "+", "LOG_HEADER", ")", "\n", "print", "(", "LOG_TEMPLATE", ".", "format", "(", "split", ".", "upper", "(", ")", ",", "accuracy", ",", "precision", ",", "recall", ",", "f1", ",", "avg_loss", ",", "f1_mac", ",", "hamming_loss", ",", "jacc", ")", ")", "\n", "\n", "model_name", "=", "args", ".", "save_path", ".", "replace", "(", "'model_checkpoints/'", ",", "''", ")", "\n", "path", "=", "Path", "(", "args", ".", "save_path", ".", "replace", "(", "'model_checkpoints'", ",", "'out'", ")", ")", "\n", "path", "=", "path", "/", "args", ".", "dataset", "\n", "path", ".", "mkdir", "(", "exist_ok", "=", "True", ")", "\n", "print", "(", "'Saving prediction files in '", ",", "path", ")", "\n", "np", ".", "save", "(", "path", "/", "f'predicted_{model_name}_{split}.npy'", ",", "predicted_values", ")", "\n", "np", ".", "save", "(", "path", "/", "f'target_{model_name}_{split}.npy'", ",", "target_values", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.__main__main.do_main": [[58, 200], ["models.bert.args.get_args", "print", "print", "print", "print", "random.seed", "numpy.random.seed", "torch.manual_seed", "utils.tokenization.BertTokenizer.from_pretrained", "models.bert.model.BertForSequenceClassification.from_pretrained", "model.to.to", "list", "common.trainers.bert_trainer.BertTrainer", "__main__main.evaluate_split", "__main__main.evaluate_split", "torch.device", "torch.cuda.device_count", "torch.cuda.set_device", "torch.cuda.set_device", "torch.device", "torch.distributed.init_process_group", "str().upper", "bool", "torch.cuda.manual_seed_all", "ValueError", "ValueError", "os.path.join", "os.makedirs", "processor.get_train_examples", "os.path.join", "model.to.half", "DDP", "model.to.named_parameters", "FusedAdam", "utils.optimization.BertAdam", "common.trainers.bert_trainer.BertTrainer.train", "torch.load", "models.bert.model.BertForSequenceClassification.from_pretrained", "torch.load", "torch.load.state_dict().keys", "model.to.load_state_dict", "model.to.to", "int", "str", "FP16_Optimizer", "FP16_Optimizer", "key.replace", "str", "torch.distributed.get_world_size", "ImportError", "ImportError", "torch.load.state_dict", "torch.load.state_dict", "torch.cuda.is_available", "any", "len", "any"], "function", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_lstm_emotion.args.get_args", "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertPreTrainedModel.from_pretrained", "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertPreTrainedModel.from_pretrained", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_lstm_emotion.__main__.evaluate_split", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_lstm_emotion.__main__.evaluate_split", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor.get_train_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.trainers.bert_sent_trainer.BertTrainer.train", "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertPreTrainedModel.from_pretrained"], ["", "def", "do_main", "(", ")", ":", "\n", "# Set default configuration in args.py", "\n", "    ", "args", "=", "get_args", "(", ")", "\n", "\n", "if", "args", ".", "local_rank", "==", "-", "1", "or", "not", "args", ".", "cuda", ":", "\n", "        ", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "args", ".", "cuda", "else", "\"cpu\"", ")", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "gpu", ")", "\n", "", "else", ":", "\n", "        ", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "local_rank", ")", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ",", "args", ".", "local_rank", ")", "\n", "n_gpu", "=", "1", "\n", "# Initializes the distributed backend which will take care of sychronizing nodes/GPUs", "\n", "torch", ".", "distributed", ".", "init_process_group", "(", "backend", "=", "'nccl'", ")", "\n", "\n", "", "print", "(", "'Device:'", ",", "str", "(", "device", ")", ".", "upper", "(", ")", ")", "\n", "print", "(", "'Number of GPUs:'", ",", "n_gpu", ")", "\n", "print", "(", "'Distributed training:'", ",", "bool", "(", "args", ".", "local_rank", "!=", "-", "1", ")", ")", "\n", "print", "(", "'FP16:'", ",", "args", ".", "fp16", ")", "\n", "\n", "# Set random seed for reproducibility", "\n", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "if", "n_gpu", ">", "0", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n", "", "dataset_map", "=", "{", "\n", "'SST-2'", ":", "SST2Processor", ",", "\n", "'Reuters'", ":", "ReutersProcessor", ",", "\n", "'IMDB'", ":", "IMDBProcessor", ",", "\n", "'AAPD'", ":", "AAPDProcessor", ",", "\n", "'AGNews'", ":", "AGNewsProcessor", ",", "\n", "'Yelp2014'", ":", "Yelp2014Processor", ",", "\n", "'Sogou'", ":", "SogouProcessor", ",", "\n", "'Personality'", ":", "PersonalityProcessor", ",", "\n", "'News_art'", ":", "News_artProcessor", ",", "\n", "'News'", ":", "News_Processor", ",", "\n", "'UCI_yelp'", ":", "UCI_yelpProcessor", ",", "\n", "'Procon'", ":", "ProconProcessor", ",", "\n", "'Style'", ":", "StyleProcessor", ",", "\n", "'ProconDual'", ":", "ProconDualProcessor", ",", "\n", "'Pan15'", ":", "Pan15_Processor", ",", "\n", "'Pan14E'", ":", "Pan14E_Processor", ",", "\n", "'Pan14N'", ":", "Pan14N_Processor", ",", "\n", "'Perspectrum'", ":", "PerspectrumProcessor", "\n", "}", "\n", "\n", "if", "args", ".", "gradient_accumulation_steps", "<", "1", ":", "\n", "        ", "raise", "ValueError", "(", "\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\"", ".", "format", "(", "\n", "args", ".", "gradient_accumulation_steps", ")", ")", "\n", "\n", "", "if", "args", ".", "dataset", "not", "in", "dataset_map", ":", "\n", "        ", "raise", "ValueError", "(", "'Unrecognized dataset'", ")", "\n", "\n", "", "args", ".", "batch_size", "=", "args", ".", "batch_size", "//", "args", ".", "gradient_accumulation_steps", "\n", "args", ".", "device", "=", "device", "\n", "args", ".", "n_gpu", "=", "n_gpu", "\n", "args", ".", "num_labels", "=", "dataset_map", "[", "args", ".", "dataset", "]", ".", "NUM_CLASSES", "\n", "args", ".", "is_multilabel", "=", "dataset_map", "[", "args", ".", "dataset", "]", ".", "IS_MULTILABEL", "\n", "\n", "if", "not", "args", ".", "trained_model", ":", "\n", "        ", "save_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "save_path", ",", "dataset_map", "[", "args", ".", "dataset", "]", ".", "NAME", ")", "\n", "os", ".", "makedirs", "(", "save_path", ",", "exist_ok", "=", "True", ")", "\n", "\n", "", "processor", "=", "dataset_map", "[", "args", ".", "dataset", "]", "(", ")", "\n", "args", ".", "is_lowercase", "=", "'uncased'", "in", "args", ".", "model", "\n", "args", ".", "is_hierarchical", "=", "False", "\n", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "args", ".", "model", ",", "is_lowercase", "=", "args", ".", "is_lowercase", ")", "\n", "\n", "train_examples", "=", "None", "\n", "num_train_optimization_steps", "=", "None", "\n", "if", "not", "args", ".", "trained_model", ":", "\n", "        ", "train_examples", "=", "processor", ".", "get_train_examples", "(", "args", ".", "data_dir", ",", "args", ".", "train_name", ")", "\n", "num_train_optimization_steps", "=", "int", "(", "\n", "len", "(", "train_examples", ")", "/", "args", ".", "batch_size", "/", "args", ".", "gradient_accumulation_steps", ")", "*", "args", ".", "epochs", "\n", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "            ", "num_train_optimization_steps", "=", "num_train_optimization_steps", "//", "torch", ".", "distributed", ".", "get_world_size", "(", ")", "\n", "\n", "", "", "cache_dir", "=", "args", ".", "cache_dir", "if", "args", ".", "cache_dir", "else", "os", ".", "path", ".", "join", "(", "str", "(", "PYTORCH_PRETRAINED_BERT_CACHE", ")", ",", "'distributed_{}'", ".", "format", "(", "args", ".", "local_rank", ")", ")", "\n", "model", "=", "BertForSequenceClassification", ".", "from_pretrained", "(", "args", ".", "model", ",", "cache_dir", "=", "cache_dir", ",", "num_labels", "=", "args", ".", "num_labels", ")", "\n", "\n", "if", "args", ".", "fp16", ":", "\n", "        ", "model", ".", "half", "(", ")", "\n", "", "model", ".", "to", "(", "device", ")", "\n", "\n", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "        ", "try", ":", "\n", "            ", "from", "apex", ".", "parallel", "import", "DistributedDataParallel", "as", "DDP", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "\"Install NVIDIA Apex to use distributed and FP16 training.\"", ")", "\n", "", "model", "=", "DDP", "(", "model", ")", "\n", "", "'''elif n_gpu > 1: changed by marjan\n\n        model = torch.nn.DataParallel(model)'''", "\n", "\n", "# Prepare optimizer", "\n", "param_optimizer", "=", "list", "(", "model", ".", "named_parameters", "(", ")", ")", "\n", "no_decay", "=", "[", "'bias'", ",", "'LayerNorm.bias'", ",", "'LayerNorm.weight'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.0", "}", "]", "\n", "\n", "if", "args", ".", "fp16", ":", "\n", "        ", "try", ":", "\n", "            ", "from", "apex", ".", "optimizers", "import", "FP16_Optimizer", "\n", "from", "apex", ".", "optimizers", "import", "FusedAdam", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "\"Please install NVIDIA Apex for distributed and FP16 training\"", ")", "\n", "\n", "", "optimizer", "=", "FusedAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "args", ".", "lr", ",", "\n", "bias_correction", "=", "False", ",", "\n", "max_grad_norm", "=", "1.0", ")", "\n", "if", "args", ".", "loss_scale", "==", "0", ":", "\n", "            ", "optimizer", "=", "FP16_Optimizer", "(", "optimizer", ",", "dynamic_loss_scale", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "optimizer", "=", "FP16_Optimizer", "(", "optimizer", ",", "static_loss_scale", "=", "args", ".", "loss_scale", ")", "\n", "\n", "", "", "else", ":", "\n", "        ", "optimizer", "=", "BertAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "args", ".", "lr", ",", "\n", "warmup", "=", "args", ".", "warmup_proportion", ",", "\n", "t_total", "=", "num_train_optimization_steps", ")", "\n", "\n", "", "trainer", "=", "BertTrainer", "(", "model", ",", "optimizer", ",", "processor", ",", "args", ")", "\n", "\n", "if", "not", "args", ".", "trained_model", ":", "\n", "        ", "trainer", ".", "train", "(", ")", "\n", "model", "=", "torch", ".", "load", "(", "trainer", ".", "snapshot_path", ")", "\n", "", "else", ":", "\n", "        ", "model", "=", "BertForSequenceClassification", ".", "from_pretrained", "(", "args", ".", "model", ",", "num_labels", "=", "args", ".", "num_labels", ")", "\n", "model_", "=", "torch", ".", "load", "(", "args", ".", "trained_model", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", "\n", "state", "=", "{", "}", "\n", "for", "key", "in", "model_", ".", "state_dict", "(", ")", ".", "keys", "(", ")", ":", "\n", "            ", "new_key", "=", "key", ".", "replace", "(", "\"module.\"", ",", "\"\"", ")", "\n", "state", "[", "new_key", "]", "=", "model_", ".", "state_dict", "(", ")", "[", "key", "]", "\n", "", "model", ".", "load_state_dict", "(", "state", ")", "\n", "model", "=", "model", ".", "to", "(", "device", ")", "\n", "\n", "", "evaluate_split", "(", "model", ",", "processor", ",", "args", ",", "split", "=", "args", ".", "dev_name", ")", "\n", "evaluate_split", "(", "model", ",", "processor", ",", "args", ",", "split", "=", "args", ".", "test_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertConfig.__init__": [[130, 186], ["isinstance", "json.loads.items", "isinstance", "isinstance", "io.open", "json.loads", "ValueError", "reader.read"], "methods", ["None"], ["out_score", "=", "self", ".", "dropout", "(", "out_score", ")", "# added", "\n", "\n", "if", "self", ".", "pooling", ":", "\n", "            ", "_", ",", "bs", ",", "_", "=", "out_score", ".", "shape", "\n", "if", "return_indices", ":", "\n", "\n", "                ", "mx_pool", ",", "mx_pool_idx", "=", "F", ".", "adaptive_max_pool1d_with_indices", "(", "out_score", ".", "permute", "(", "1", ",", "2", ",", "0", ")", ",", "(", "1", ",", ")", ")", "\n", "", "else", ":", "\n", "                ", "mx_pool", "=", "F", ".", "adaptive_max_pool1d", "(", "out_score", ".", "permute", "(", "1", ",", "2", ",", "0", ")", ",", "(", "1", ",", ")", ")", "\n", "", "avg_pool", "=", "F", ".", "adaptive_avg_pool1d", "(", "out_score", ".", "permute", "(", "1", ",", "2", ",", "0", ")", ",", "(", "1", ",", ")", ")", "# (bs, input_size, seq_len)", "\n", "out_score", "=", "torch", ".", "cat", "(", "[", "mx_pool", ".", "view", "(", "bs", ",", "-", "1", ")", ",", "avg_pool", ".", "view", "(", "bs", ",", "-", "1", ")", ",", "out_score", "[", "-", "1", "]", "]", ",", "-", "1", ")", "\n", "out_score", "=", "self", ".", "dropout", "(", "out_score", ")", "\n", "\n", "", "else", ":", "\n", "            ", "out_score", "=", "out_score", "[", "-", "1", "]", "\n", "\n", "", "logits", "=", "self", ".", "classifier", "(", "out_score", ")", "\n", "if", "return_indices", "and", "self", ".", "pooling", ":", "\n", "            ", "return", "logits", ",", "mx_pool_idx", "\n", "\n", "", "return", "logits", "\n", "\n", "\n", "\n", "\n", "", "", ""]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertConfig.from_dict": [[188, 195], ["model.BertConfig", "json_object.items"], "methods", ["None"], []], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertConfig.from_json_file": [[196, 202], ["cls.from_dict", "io.open", "reader.read", "json.loads"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertConfig.from_dict"], []], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertConfig.__repr__": [[203, 205], ["str", "model.BertConfig.to_json_string"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertConfig.to_json_string"], []], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertConfig.to_dict": [[206, 210], ["copy.deepcopy"], "methods", ["None"], []], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertConfig.to_json_string": [[211, 214], ["json.dumps", "model.BertConfig.to_dict"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertConfig.to_dict"], []], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertEmbeddings.__init__": [[241, 251], ["torch.nn.Module.__init__", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "BertLayerNorm", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.classification_evaluator.ClassificationEvaluator.__init__"], []], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertEmbeddings.forward": [[252, 267], ["input_ids.size", "torch.arange", "position_ids.unsqueeze().expand_as.unsqueeze().expand_as.unsqueeze().expand_as", "model.BertEmbeddings.word_embeddings", "model.BertEmbeddings.position_embeddings", "model.BertEmbeddings.token_type_embeddings", "model.BertEmbeddings.LayerNorm", "model.BertEmbeddings.dropout", "torch.zeros_like", "position_ids.unsqueeze().expand_as.unsqueeze().expand_as.unsqueeze"], "methods", ["None"], []], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertSelfAttention.__init__": [[270, 285], ["torch.nn.Module.__init__", "int", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "ValueError"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.classification_evaluator.ClassificationEvaluator.__init__"], []], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertSelfAttention.transpose_for_scores": [[286, 290], ["x.view.view.view", "x.view.view.permute", "x.view.view.size"], "methods", ["None"], []], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertSelfAttention.forward": [[291, 318], ["model.BertSelfAttention.query", "model.BertSelfAttention.key", "model.BertSelfAttention.value", "model.BertSelfAttention.transpose_for_scores", "model.BertSelfAttention.transpose_for_scores", "model.BertSelfAttention.transpose_for_scores", "torch.matmul", "model.BertSelfAttention.dropout", "torch.matmul", "context_layer.view.view.permute().contiguous", "context_layer.view.view.view", "model.BertSelfAttention.transpose", "math.sqrt", "torch.nn.Softmax", "context_layer.view.view.permute", "context_layer.view.view.size"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertSelfAttention.transpose_for_scores"], []], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertSelfOutput.__init__": [[321, 326], ["torch.nn.Module.__init__", "torch.nn.Linear", "BertLayerNorm", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.classification_evaluator.ClassificationEvaluator.__init__"], []], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertSelfOutput.forward": [[327, 332], ["model.BertSelfOutput.dense", "model.BertSelfOutput.dropout", "model.BertSelfOutput.LayerNorm"], "methods", ["None"], []], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertAttention.__init__": [[335, 339], ["torch.nn.Module.__init__", "model.BertSelfAttention", "model.BertSelfOutput"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.classification_evaluator.ClassificationEvaluator.__init__"], []], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertAttention.forward": [[340, 344], ["model.BertAttention.self", "model.BertAttention.output"], "methods", ["None"], []], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertIntermediate.__init__": [[347, 354], ["torch.nn.Module.__init__", "torch.nn.Linear", "isinstance", "isinstance"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.classification_evaluator.ClassificationEvaluator.__init__"], []], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertIntermediate.forward": [[355, 359], ["model.BertIntermediate.dense", "model.BertIntermediate.intermediate_act_fn"], "methods", ["None"], []], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertOutput.__init__": [[362, 367], ["torch.nn.Module.__init__", "torch.nn.Linear", "BertLayerNorm", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.classification_evaluator.ClassificationEvaluator.__init__"], []], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertOutput.forward": [[368, 373], ["model.BertOutput.dense", "model.BertOutput.dropout", "model.BertOutput.LayerNorm"], "methods", ["None"], []], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertLayer.__init__": [[376, 381], ["torch.nn.Module.__init__", "model.BertAttention", "model.BertIntermediate", "model.BertOutput"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.classification_evaluator.ClassificationEvaluator.__init__"], []], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertLayer.forward": [[382, 387], ["model.BertLayer.attention", "model.BertLayer.intermediate", "model.BertLayer.output"], "methods", ["None"], []], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertEncoder.__init__": [[390, 394], ["torch.nn.Module.__init__", "model.BertLayer", "torch.nn.ModuleList", "copy.deepcopy", "range"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.classification_evaluator.ClassificationEvaluator.__init__"], []], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertEncoder.forward": [[395, 404], ["layer_module", "all_encoder_layers.append", "all_encoder_layers.append"], "methods", ["None"], []], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertPooler.__init__": [[407, 411], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Tanh"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.classification_evaluator.ClassificationEvaluator.__init__"], []], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertPooler.forward": [[412, 419], ["model.BertPooler.dense", "model.BertPooler.activation"], "methods", ["None"], []], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertPredictionHeadTransform.__init__": [[422, 430], ["torch.nn.Module.__init__", "torch.nn.Linear", "BertLayerNorm", "isinstance", "isinstance"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.classification_evaluator.ClassificationEvaluator.__init__"], []], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertPredictionHeadTransform.forward": [[431, 436], ["model.BertPredictionHeadTransform.dense", "model.BertPredictionHeadTransform.transform_act_fn", "model.BertPredictionHeadTransform.LayerNorm"], "methods", ["None"], []], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertLMPredictionHead.__init__": [[439, 450], ["torch.nn.Module.__init__", "model.BertPredictionHeadTransform", "torch.nn.Linear", "torch.nn.Parameter", "bert_model_embedding_weights.size", "bert_model_embedding_weights.size", "torch.zeros", "bert_model_embedding_weights.size"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.classification_evaluator.ClassificationEvaluator.__init__"], []], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertLMPredictionHead.forward": [[451, 455], ["model.BertLMPredictionHead.transform", "model.BertLMPredictionHead.decoder"], "methods", ["None"], []], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertOnlyMLMHead.__init__": [[458, 461], ["torch.nn.Module.__init__", "model.BertLMPredictionHead"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.classification_evaluator.ClassificationEvaluator.__init__"], []], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertOnlyMLMHead.forward": [[462, 465], ["model.BertOnlyMLMHead.predictions"], "methods", ["None"], []], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertOnlyNSPHead.__init__": [[468, 471], ["torch.nn.Module.__init__", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.classification_evaluator.ClassificationEvaluator.__init__"], []], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertOnlyNSPHead.forward": [[472, 475], ["model.BertOnlyNSPHead.seq_relationship"], "methods", ["None"], []], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertPreTrainingHeads.__init__": [[478, 482], ["torch.nn.Module.__init__", "model.BertLMPredictionHead", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.classification_evaluator.ClassificationEvaluator.__init__"], []], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertPreTrainingHeads.forward": [[483, 487], ["model.BertPreTrainingHeads.predictions", "model.BertPreTrainingHeads.seq_relationship"], "methods", ["None"], []], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertPreTrainedModel.__init__": [[493, 503], ["torch.nn.Module.__init__", "isinstance", "ValueError"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.classification_evaluator.ClassificationEvaluator.__init__"], []], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertPreTrainedModel.init_bert_weights": [[504, 516], ["isinstance", "module.weight.data.normal_", "isinstance", "isinstance", "module.bias.data.zero_", "module.bias.data.zero_", "module.weight.data.fill_"], "methods", ["None"], []], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertPreTrainedModel.from_pretrained": [[517, 639], ["os.path.join", "cls.BertConfig.from_json_file", "logger.info", "cls", "torch.load.keys", "zip", "getattr", "torch.load.copy", "cls.BertPreTrainedModel.from_pretrained.load"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertConfig.from_json_file"], []], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertModel.__init__": [[685, 691], ["model.BertPreTrainedModel.__init__", "model.BertEmbeddings", "model.BertEncoder", "model.BertPooler", "model.BertModel.apply"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.classification_evaluator.ClassificationEvaluator.__init__"], []], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertModel.forward": [[692, 722], ["torch.ones_like.unsqueeze().unsqueeze", "extended_attention_mask.to.to.to", "model.BertModel.embeddings", "model.BertModel.encoder", "model.BertModel.pooler", "torch.ones_like", "torch.zeros_like", "torch.ones_like.unsqueeze", "next", "model.BertModel.parameters"], "methods", ["None"], []], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertForPreTraining.__init__": [[774, 779], ["model.BertPreTrainedModel.__init__", "model.BertModel", "model.BertPreTrainingHeads", "model.BertForPreTraining.apply"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.classification_evaluator.ClassificationEvaluator.__init__"], []], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertForPreTraining.forward": [[780, 793], ["model.BertForPreTraining.bert", "model.BertForPreTraining.cls", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "prediction_scores.view", "masked_lm_labels.view", "seq_relationship_score.view", "next_sentence_label.view"], "methods", ["None"], []], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertForSequenceClassification.__init__": [[840, 852], ["model.BertPreTrainedModel.__init__", "model.BertModel", "torch.nn.Dropout", "torch.nn.Linear", "model.BertForSequenceClassification.apply"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.classification_evaluator.ClassificationEvaluator.__init__"], []], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertForSequenceClassification.forward": [[854, 859], ["model.BertForSequenceClassification.bert", "model.BertForSequenceClassification.dropout", "model.BertForSequenceClassification.classifier"], "methods", ["None"], []], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertForSequenceClassification.get_bert_embedding": [[908, 914], ["model.BertForSequenceClassification.bert"], "methods", ["None"], []], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertForSequenceClassification.get_bert_sep_cls_embedding": [[915, 922], ["BertTokenizer.from_pretrained", "model.BertForSequenceClassification.bert", "BertTokenizer.from_pretrained.convert_ids_to_tokens", "locations.append", "BertTokenizer.from_pretrained.convert_ids_to_tokens.find"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertPreTrainedModel.from_pretrained", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.BertTokenizer.convert_ids_to_tokens"], []], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.load_tf_weights_in_bert": [[50, 109], ["os.path.abspath", "print", "tf.train.list_variables", "zip", "print", "tf.train.load_variable", "names.append", "arrays.append", "name.split.split", "any", "print", "torch.from_numpy", "print", "print", "re.fullmatch", "getattr", "re.split", "getattr", "len", "int", "np.transpose", "getattr", "getattr", "getattr"], "function", ["None"], ["\n", "def", "__init__", "(", "self", ",", "config", ",", "num_labels", ",", "pooling", "=", "False", ")", ":", "\n", "        ", "super", "(", "BertForSequenceClassification", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "num_labels", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "sent_dim", "=", "config", ".", "hidden_size", "\n", "self", ".", "sent_embeddings", "=", "nn", ".", "Embedding", "(", "3", ",", "sent_dim", ")", "\n", "self", ".", "no_bert_layers", "=", "1", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "pooling", "=", "pooling", "\n", "self", ".", "hidden_size", "=", "config", ".", "hidden_size", "\n", "\n", "if", "self", ".", "pooling", ":", "\n", "            ", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "2", "*", "3", "*", "config", ".", "hidden_size", ",", "num_labels", ")", "# avd_pool max_pool, last hidden state", "\n", "", "else", ":", "\n", "            ", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "1", "*", "config", ".", "hidden_size", ",", "num_labels", ")", "\n", "", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n", "# wo att", "\n", "self", ".", "lstm_final", "=", "nn", ".", "GRU", "(", "config", ".", "hidden_size", "*", "(", "abs", "(", "self", ".", "no_bert_layers", ")", "+", "1", ")", ",", "config", ".", "hidden_size", ",", "bidirectional", "=", "True", ")", "\n", "'''self.lstm_final = nn.LSTM(config.hidden_size * (abs(self.no_bert_layers) + 1), config.hidden_size,\n                                 bidirectional=True)'''", "\n", "\n", "self", ".", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "\n", "", "def", "init_hidden", "(", "self", ",", "bs", ")", ":", "\n", "# Before we've done anything, we dont have any hidden state.", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.gelu": [[111, 118], ["torch.erf", "math.sqrt"], "function", ["None"], ["# why they have this dimensionality.", "\n", "# The axes semantics are (num_layers, minibatch_size, hidden_dim)", "\n", "        ", "return", "torch", ".", "zeros", "(", "2", ",", "bs", ",", "self", ".", "hidden_size", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "#return (torch.zeros(2, bs, self.hidden_size).to(self.device), torch.zeros(2, bs, self.hidden_size).to(self.device))", "\n", "\n", "", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "sent_scores", "=", "None", ",", "return_indices", "=", "False", ")", ":", "\n", "\n", "        ", "encoded_layers", ",", "_", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ")", "# 12 layers each (batch_size, seg_lengh, hidden_size)", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.swish": [[120, 122], ["torch.sigmoid"], "function", ["None"], ["bs", ",", "_", ",", "_", "=", "bert_encoded_layers", ".", "shape", "\n", "sent_embeddings", "=", "self", ".", "sent_embeddings", "(", "sent_scores", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert.args.get_args": [[6, 52], ["models.args.get_args", "models.args.get_args.add_argument", "models.args.get_args.add_argument", "models.args.get_args.add_argument", "models.args.get_args.add_argument", "models.args.get_args.add_argument", "models.args.get_args.add_argument", "models.args.get_args.add_argument", "models.args.get_args.add_argument", "models.args.get_args.add_argument", "models.args.get_args.add_argument", "models.args.get_args.add_argument", "models.args.get_args.add_argument", "models.args.get_args.add_argument", "models.args.get_args.add_argument", "models.args.get_args.add_argument", "models.args.get_args.add_argument", "models.args.get_args.parse_args", "os.path.join"], "function", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_lstm_emotion.args.get_args"], ["    ", "parser", "=", "ArgumentParser", "(", "description", "=", "\"PyTorch deep learning models for document classification\"", ")", "\n", "par", "=", "'/home/projects/'", "\n", "parser", ".", "add_argument", "(", "'--no-cuda'", ",", "action", "=", "'store_false'", ",", "dest", "=", "'cuda'", ")", "\n", "parser", ".", "add_argument", "(", "'--gpu'", ",", "type", "=", "int", ",", "default", "=", "0", ")", "\n", "parser", ".", "add_argument", "(", "'--epochs'", ",", "type", "=", "int", ",", "default", "=", "50", ")", "\n", "parser", ".", "add_argument", "(", "'--batch-size'", ",", "type", "=", "int", ",", "default", "=", "1024", ")", "\n", "parser", ".", "add_argument", "(", "'--lr'", ",", "type", "=", "float", ",", "default", "=", "0.001", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "type", "=", "int", ",", "default", "=", "3435", ")", "# default value: 3435", "\n", "parser", ".", "add_argument", "(", "'--patience'", ",", "type", "=", "int", ",", "default", "=", "5", ")", "\n", "parser", ".", "add_argument", "(", "'--log-every'", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "'--data-dir'", ",", "default", "=", "os", ".", "path", ".", "join", "(", "par", ",", "'hedwig-data'", ",", "'datasets'", ")", ")", "#os.pardir", "\n", "parser", ".", "add_argument", "(", "'--early_on_f1'", ",", "action", "=", "'store_true'", ")", "\n", "\n", "return", "parser", "\n", "", ""]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_lstm_emotion.__main__.plot_heatmap": [[38, 51], ["pandas.DataFrame", "matplotlib.subplots", "seaborn.heatmap", "sns.heatmap.set_xticklabels", "fig.savefig", "numpy.array().reshape", "numpy.max", "numpy.array"], "function", ["None"], ["\n", "\n", "#x = x[:10]", "\n", "#values = values[:10]", "\n", "# sphinx_gallery_thumbnail_number = 2", "\n", "    ", "lbl_plot", "=", "x", "\n", "d", "=", "pd", ".", "DataFrame", "(", "np", ".", "array", "(", "values", ")", ".", "reshape", "(", "1", ",", "-", "1", ")", ")", "\n", "fig", ",", "axes", "=", "plt", ".", "subplots", "(", "nrows", "=", "10", ",", "figsize", "=", "(", "10", ",", "20", ")", ")", "\n", "ax", "=", "sns", ".", "heatmap", "(", "d", ",", "cmap", "=", "\"Blues\"", ",", "square", "=", "True", ",", "annot", "=", "True", ",", "xticklabels", "=", "x", ",", "yticklabels", "=", "False", ",", "\n", "vmax", "=", "np", ".", "max", "(", "values", ")", ",", "cbar", "=", "False", ")", "\n", "ax", ".", "set_xticklabels", "(", "rotation", "=", "40", ",", "labels", "=", "lbl_plot", ")", "\n", "fig", ".", "savefig", "(", "'example.png'", ")", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_lstm_emotion.__main__.analyze": [[53, 77], ["datasets.bert_processors.abstract_processor.convert_examples_to_features_with_emotion", "enumerate", "numpy.array", "evaluator.tokenizer.convert_ids_to_tokens", "evaluator.tokenizer.convert_ids_to_tokens", "print", "print", "np.array.append", "numpy.where", "numpy.where", "collections.Counter", "collections.Counter", "numpy.array", "numpy.where", "numpy.where", "all_indices[].flatten().tolist", "all_indices[].flatten().tolist", "cp.most_common", "cn.most_common", "all_indices[].flatten", "all_indices[].flatten"], "function", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.convert_examples_to_features_with_emotion", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.BertTokenizer.convert_ids_to_tokens", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.BertTokenizer.convert_ids_to_tokens"], ["    ", "eval_features", "=", "convert_examples_to_features_with_sentiment", "(", "\n", "evaluator", ".", "eval_examples", ",", "evaluator", ".", "args", ".", "max_seq_length", ",", "evaluator", ".", "tokenizer", ",", "overal_sent", "=", "evaluator", ".", "args", ".", "overal_sent", ")", "\n", "\n", "all_indices", "=", "[", "]", "\n", "for", "i", ",", "f", "in", "enumerate", "(", "eval_features", ")", ":", "\n", "        ", "indices", "=", "np", ".", "array", "(", "f", ".", "input_ids", ")", "[", "input_indices", "[", "i", "]", "]", "\n", "all_indices", ".", "append", "(", "indices", ")", "\n", "\n", "", "all_indices", "=", "np", ".", "array", "(", "all_indices", ")", "\n", "\n", "tp", "=", "np", ".", "where", "(", "predicted_labels", "==", "target_labels", ")", "and", "np", ".", "where", "(", "target_labels", ")", "[", "0", "]", "\n", "tn", "=", "np", ".", "where", "(", "predicted_labels", "==", "target_labels", ")", "and", "np", ".", "where", "(", "target_labels", "==", "0", ")", "[", "0", "]", "\n", "\n", "cp", ",", "cn", "=", "Counter", "(", "all_indices", "[", "tp", "]", ".", "flatten", "(", ")", ".", "tolist", "(", ")", ")", ",", "Counter", "(", "all_indices", "[", "tn", "]", ".", "flatten", "(", ")", ".", "tolist", "(", ")", ")", "\n", "\n", "p_most_commons_ids", "=", "[", "k", "for", "(", "k", ",", "v", ")", "in", "cp", ".", "most_common", "(", "20", ")", "]", "\n", "n_most_commons_ids", "=", "[", "k", "for", "(", "k", ",", "v", ")", "in", "cn", ".", "most_common", "(", "20", ")", "]", "\n", "\n", "p_most_commons_tokens", "=", "evaluator", ".", "tokenizer", ".", "convert_ids_to_tokens", "(", "p_most_commons_ids", ")", "\n", "n_most_commons_tokens", "=", "evaluator", ".", "tokenizer", ".", "convert_ids_to_tokens", "(", "n_most_commons_ids", ")", "\n", "print", "(", "'top 10 most common tokens in TP: '", ",", "p_most_commons_tokens", ")", "\n", "\n", "print", "(", "'top 10 most common tokens in TN: '", ",", "n_most_commons_tokens", ")", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_lstm_emotion.__main__.analyze_example": [[79, 121], ["datasets.bert_processors.abstract_processor.convert_examples_to_features_with_emotion", "numpy.array", "range", "pandas.DataFrame", "pd.DataFrame.to_csv", "numpy.where", "len", "all_indices[].flatten().tolist", "collections.Counter", "numpy.array", "evaluator.tokenizer.convert_ids_to_tokens", "range", "ls.append", "numpy.where", "len", "freqs.append", "len", "len", "all_indices[].flatten"], "function", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.convert_examples_to_features_with_emotion", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.tokenization.BertTokenizer.convert_ids_to_tokens"], ["    ", "eval_features", "=", "convert_examples_to_features_with_sentiment", "(", "\n", "evaluator", ".", "eval_examples", ",", "evaluator", ".", "args", ".", "max_seq_length", ",", "evaluator", ".", "tokenizer", ",", "overal_sent", "=", "evaluator", ".", "args", ".", "overal_sent", ")", "\n", "\n", "all_indices", "=", "input_indices", "\n", "'''for i, f in enumerate(eval_features):\n        locs = np.array(input_indices[i])\n        all_indices.append(indices)'''", "\n", "\n", "all_indices", "=", "np", ".", "array", "(", "all_indices", ")", "\n", "tp", "=", "np", ".", "where", "(", "predicted_labels", "==", "target_labels", ")", "and", "np", ".", "where", "(", "target_labels", "==", "0", ")", "[", "0", "]", "\n", "#tn = np.where(predicted_labels==target_labels) and np.where(target_labels==0)[0]", "\n", "\n", "ls", "=", "[", "]", "\n", "\n", "for", "j", "in", "range", "(", "len", "(", "tp", ")", ")", ":", "\n", "\n", "        ", "id_p_example", "=", "tp", "[", "j", "]", "\n", "#id_n_example = tn[0]", "\n", "\n", "\n", "example_pos", "=", "all_indices", "[", "id_p_example", "]", ".", "flatten", "(", ")", ".", "tolist", "(", ")", "\n", "#example_neg = all_indices[id_n_example].flatten().tolist()", "\n", "\n", "\n", "cp", "=", "Counter", "(", "example_pos", ")", "\n", "\n", "p_ids", "=", "np", ".", "array", "(", "eval_features", "[", "id_p_example", "]", ".", "input_ids", ")", "\n", "\n", "\n", "p_tokens", "=", "evaluator", ".", "tokenizer", ".", "convert_ids_to_tokens", "(", "p_ids", ")", "\n", "\n", "\n", "freqs", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "eval_features", "[", "id_p_example", "]", ".", "input_ids", ")", ")", ":", "\n", "            ", "freqs", ".", "append", "(", "cp", "[", "i", "]", ")", "\n", "\n", "", "assert", "len", "(", "freqs", ")", "==", "len", "(", "p_tokens", ")", "\n", "l", "=", "[", "p_tokens", ",", "freqs", "]", "\n", "ls", ".", "append", "(", "l", ")", "\n", "", "df", "=", "pd", ".", "DataFrame", "(", "ls", ",", "columns", "=", "[", "'tokens'", ",", "'freqs'", "]", ")", "\n", "df", ".", "to_csv", "(", "path", "/", "'neg_examples.tsv'", ",", "sep", "=", "'\\t'", ",", "index", "=", "None", ")", "\n", "#plot_heatmap(p_tokens, freqs)", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_lstm_emotion.__main__.evaluate_split": [[141, 156], ["common.evaluators.bert_emotion_evaluator.BertEvaluator", "time.time", "print", "print", "print", "args.save_path.replace", "pathlib.Path", "pathlib.Path.mkdir", "print", "numpy.save", "numpy.save", "common.evaluators.bert_emotion_evaluator.BertEvaluator.get_scores", "LOG_TEMPLATE.format", "args.save_path.replace", "time.time", "split.upper"], "function", ["home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.classification_evaluator.ClassificationEvaluator.get_scores"], ["model_name", "=", "args", ".", "save_path", ".", "replace", "(", "'model_checkpoints/'", ",", "''", ")", "\n", "path", "=", "Path", "(", "args", ".", "save_path", ".", "replace", "(", "'model_checkpoints'", ",", "'out'", ")", ")", "\n", "path", "=", "path", "/", "args", ".", "dataset", "\n", "path", ".", "mkdir", "(", "parents", "=", "True", ",", "exist_ok", "=", "True", ")", "\n", "print", "(", "'Saving prediction files in '", ",", "path", ")", "\n", "np", ".", "save", "(", "path", "/", "f'predicted_{model_name}_{split}.npy'", ",", "predicted_values", ")", "\n", "np", ".", "save", "(", "path", "/", "f'target_{model_name}_{split}.npy'", ",", "target_values", ")", "\n", "\n", "\n", "", "def", "do_main", "(", ")", ":", "\n", "# Set default configuration in args.py", "\n", "    ", "args", "=", "get_args", "(", ")", "\n", "args", ".", "overal_sent", "=", "False", "### change it ?!!!", "\n", "\n", "if", "args", ".", "local_rank", "==", "-", "1", "or", "not", "args", ".", "cuda", ":", "\n", "        ", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "args", ".", "cuda", "else", "\"cpu\"", ")", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_lstm_emotion.__main__.do_main": [[158, 300], ["models.bert_lstm_emotion.args.get_args", "print", "print", "print", "print", "random.seed", "numpy.random.seed", "torch.manual_seed", "utils.tokenization.BertTokenizer.from_pretrained", "models.bert_lstm_emotion.model.BertForSequenceClassification.from_pretrained", "model.to.to", "list", "common.trainers.bert_emotion_trainer.BertTrainer", "print", "__main__.evaluate_split", "__main__.evaluate_split", "torch.device", "torch.cuda.device_count", "torch.cuda.set_device", "torch.cuda.set_device", "torch.device", "torch.distributed.init_process_group", "str().upper", "bool", "torch.cuda.manual_seed_all", "ValueError", "ValueError", "os.path.join", "os.makedirs", "processor.get_train_examples", "os.path.join", "model.to.half", "DDP", "model.to.named_parameters", "FusedAdam", "utils.optimization.BertAdam", "common.trainers.bert_emotion_trainer.BertTrainer.train", "print", "torch.load", "models.bert_lstm_emotion.model.BertForSequenceClassification.from_pretrained", "torch.load", "torch.load.state_dict().keys", "model.to.load_state_dict", "model.to.to", "int", "str", "FP16_Optimizer", "FP16_Optimizer", "key.replace", "str", "torch.distributed.get_world_size", "ImportError", "ImportError", "torch.load.state_dict", "torch.load.state_dict", "torch.cuda.is_available", "any", "len", "any"], "function", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_lstm_emotion.args.get_args", "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertPreTrainedModel.from_pretrained", "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertPreTrainedModel.from_pretrained", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_lstm_emotion.__main__.evaluate_split", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_lstm_emotion.__main__.evaluate_split", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor.get_train_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.trainers.bert_sent_trainer.BertTrainer.train", "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertPreTrainedModel.from_pretrained"], ["torch", ".", "cuda", ".", "set_device", "(", "args", ".", "gpu", ")", "\n", "", "else", ":", "\n", "        ", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "local_rank", ")", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ",", "args", ".", "local_rank", ")", "\n", "n_gpu", "=", "1", "\n", "# Initializes the distributed backend which will take care of sychronizing nodes/GPUs", "\n", "torch", ".", "distributed", ".", "init_process_group", "(", "backend", "=", "'nccl'", ")", "\n", "\n", "", "print", "(", "'Device:'", ",", "str", "(", "device", ")", ".", "upper", "(", ")", ")", "\n", "print", "(", "'Number of GPUs:'", ",", "n_gpu", ")", "\n", "print", "(", "'Distributed training:'", ",", "bool", "(", "args", ".", "local_rank", "!=", "-", "1", ")", ")", "\n", "print", "(", "'FP16:'", ",", "args", ".", "fp16", ")", "\n", "\n", "# Set random seed for reproducibility", "\n", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "backends", ".", "cudnn", ".", "deterministic", "=", "True", "# new!", "\n", "if", "n_gpu", ">", "0", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n", "", "dataset_map", "=", "{", "\n", "'SST-2'", ":", "SST2Processor", ",", "\n", "'Reuters'", ":", "ReutersProcessor", ",", "\n", "'IMDB'", ":", "IMDBProcessor", ",", "\n", "'AAPD'", ":", "AAPDProcessor", ",", "\n", "'AGNews'", ":", "AGNewsProcessor", ",", "\n", "'Sogou'", ":", "SogouProcessor", ",", "\n", "'Procon'", ":", "ProconProcessor", ",", "\n", "'ProconDual'", ":", "ProconDualProcessor", "\n", "}", "\n", "\n", "if", "args", ".", "gradient_accumulation_steps", "<", "1", ":", "\n", "        ", "raise", "ValueError", "(", "\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\"", ".", "format", "(", "\n", "args", ".", "gradient_accumulation_steps", ")", ")", "\n", "\n", "", "if", "args", ".", "dataset", "not", "in", "dataset_map", ":", "\n", "        ", "raise", "ValueError", "(", "'Unrecognized dataset'", ")", "\n", "\n", "", "args", ".", "batch_size", "=", "args", ".", "batch_size", "//", "args", ".", "gradient_accumulation_steps", "\n", "args", ".", "device", "=", "device", "\n", "args", ".", "n_gpu", "=", "n_gpu", "\n", "args", ".", "num_labels", "=", "dataset_map", "[", "args", ".", "dataset", "]", ".", "NUM_CLASSES", "\n", "args", ".", "is_multilabel", "=", "dataset_map", "[", "args", ".", "dataset", "]", ".", "IS_MULTILABEL", "\n", "\n", "if", "not", "args", ".", "trained_model", ":", "\n", "        ", "save_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "save_path", ",", "dataset_map", "[", "args", ".", "dataset", "]", ".", "NAME", ")", "\n", "os", ".", "makedirs", "(", "save_path", ",", "exist_ok", "=", "True", ")", "\n", "\n", "", "processor", "=", "dataset_map", "[", "args", ".", "dataset", "]", "(", ")", "\n", "args", ".", "is_lowercase", "=", "'uncased'", "in", "args", ".", "model", "\n", "args", ".", "is_hierarchical", "=", "False", "\n", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "args", ".", "model", ",", "is_lowercase", "=", "args", ".", "is_lowercase", ")", "\n", "\n", "train_examples", "=", "None", "\n", "num_train_optimization_steps", "=", "None", "\n", "if", "not", "args", ".", "trained_model", ":", "\n", "        ", "train_examples", "=", "processor", ".", "get_train_examples", "(", "args", ".", "data_dir", ",", "args", ".", "train_name", ")", "\n", "num_train_optimization_steps", "=", "int", "(", "\n", "len", "(", "train_examples", ")", "/", "args", ".", "batch_size", "/", "args", ".", "gradient_accumulation_steps", ")", "*", "args", ".", "epochs", "\n", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "            ", "num_train_optimization_steps", "=", "num_train_optimization_steps", "//", "torch", ".", "distributed", ".", "get_world_size", "(", ")", "\n", "\n", "", "", "cache_dir", "=", "args", ".", "cache_dir", "if", "args", ".", "cache_dir", "else", "os", ".", "path", ".", "join", "(", "str", "(", "PYTORCH_PRETRAINED_BERT_CACHE", ")", ",", "'distributed_{}'", ".", "format", "(", "args", ".", "local_rank", ")", ")", "\n", "model", "=", "BertForSequenceClassification", ".", "from_pretrained", "(", "args", ".", "model", ",", "cache_dir", "=", "cache_dir", ",", "num_labels", "=", "args", ".", "num_labels", ",", "\n", "pooling", "=", "args", ".", "pooling", ")", "\n", "\n", "\n", "if", "args", ".", "fp16", ":", "\n", "        ", "model", ".", "half", "(", ")", "\n", "", "model", ".", "to", "(", "device", ")", "\n", "\n", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "        ", "try", ":", "\n", "            ", "from", "apex", ".", "parallel", "import", "DistributedDataParallel", "as", "DDP", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "\"Install NVIDIA Apex to use distributed and FP16 training.\"", ")", "\n", "", "model", "=", "DDP", "(", "model", ")", "\n", "", "'''elif n_gpu > 1: changed by marjan\n\n        model = torch.nn.DataParallel(model)'''", "\n", "\n", "# Prepare optimizer", "\n", "param_optimizer", "=", "list", "(", "model", ".", "named_parameters", "(", ")", ")", "\n", "no_decay", "=", "[", "'bias'", ",", "'LayerNorm.bias'", ",", "'LayerNorm.weight'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.0", "}", "]", "\n", "\n", "if", "args", ".", "fp16", ":", "\n", "        ", "try", ":", "\n", "            ", "from", "apex", ".", "optimizers", "import", "FP16_Optimizer", "\n", "from", "apex", ".", "optimizers", "import", "FusedAdam", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "\"Please install NVIDIA Apex for distributed and FP16 training\"", ")", "\n", "\n", "", "optimizer", "=", "FusedAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "args", ".", "lr", ",", "\n", "bias_correction", "=", "False", ",", "\n", "max_grad_norm", "=", "1.0", ")", "\n", "if", "args", ".", "loss_scale", "==", "0", ":", "\n", "            ", "optimizer", "=", "FP16_Optimizer", "(", "optimizer", ",", "dynamic_loss_scale", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "optimizer", "=", "FP16_Optimizer", "(", "optimizer", ",", "static_loss_scale", "=", "args", ".", "loss_scale", ")", "\n", "\n", "", "", "else", ":", "\n", "        ", "optimizer", "=", "BertAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "args", ".", "lr", ",", "\n", "warmup", "=", "args", ".", "warmup_proportion", ",", "\n", "t_total", "=", "num_train_optimization_steps", ")", "\n", "\n", "", "trainer", "=", "BertTrainer", "(", "model", ",", "optimizer", ",", "processor", ",", "args", ")", "\n", "\n", "if", "not", "args", ".", "trained_model", ":", "\n", "        ", "trainer", ".", "train", "(", ")", "\n", "#print('last epoch')", "\n", "#evaluate_split(model, processor, args, split=args.test_name, return_indices=False)", "\n", "model", "=", "torch", ".", "load", "(", "trainer", ".", "snapshot_path", ")", "\n", "\n", "", "else", ":", "\n", "        ", "model", "=", "BertForSequenceClassification", ".", "from_pretrained", "(", "args", ".", "model", ",", "num_labels", "=", "args", ".", "num_labels", ",", "pooling", "=", "args", ".", "pooling", ")", "\n", "model_", "=", "torch", ".", "load", "(", "args", ".", "trained_model", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", "\n", "state", "=", "{", "}", "\n", "for", "key", "in", "model_", ".", "state_dict", "(", ")", ".", "keys", "(", ")", ":", "\n", "            ", "new_key", "=", "key", ".", "replace", "(", "\"module.\"", ",", "\"\"", ")", "\n", "state", "[", "new_key", "]", "=", "model_", ".", "state_dict", "(", ")", "[", "key", "]", "\n", "", "model", ".", "load_state_dict", "(", "state", ")", "\n", "model", "=", "model", ".", "to", "(", "device", ")", "\n", "", "print", "(", "'best epoch'", ")", "\n", "evaluate_split", "(", "model", ",", "processor", ",", "args", ",", "split", "=", "args", ".", "dev_name", ",", "return_indices", "=", "False", ")", "\n", "evaluate_split", "(", "model", ",", "processor", ",", "args", ",", "split", "=", "args", ".", "test_name", ",", "return_indices", "=", "False", ")", "\n", "\n", "\n", "", "if", "__name__", "==", "\"__main__\"", ":", "\n", "    ", "do_main", "(", ")", "", "", ""]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_lstm_emotion.model.BertForSequenceClassification.__init__": [[84, 113], ["models.bert.model.BertPreTrainedModel.__init__", "models.bert.model.BertModel", "torch.nn.Embedding", "torch.nn.Dropout", "torch.nn.Dropout", "model.BertForSequenceClassification.apply", "torch.nn.GRU", "torch.nn.Linear", "torch.device", "torch.nn.Linear", "torch.nn.Linear", "abs", "torch.cuda.is_available", "abs", "abs"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.classification_evaluator.ClassificationEvaluator.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "num_labels", ",", "pooling", "=", "False", ")", ":", "\n", "        ", "super", "(", "BertForSequenceClassification", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "num_labels", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "sent_dim", "=", "config", ".", "hidden_size", "\n", "self", ".", "sent_embeddings", "=", "nn", ".", "Embedding", "(", "3", ",", "sent_dim", ")", "\n", "self", ".", "no_bert_layers", "=", "1", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "pooling", "=", "pooling", "\n", "self", ".", "hidden_size", "=", "config", ".", "hidden_size", "\n", "\n", "if", "self", ".", "pooling", ":", "\n", "            ", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "2", "*", "3", "*", "config", ".", "hidden_size", ",", "num_labels", ")", "# avd_pool max_pool, last hidden state", "\n", "", "else", ":", "\n", "            ", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "1", "*", "config", ".", "hidden_size", ",", "num_labels", ")", "\n", "", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n", "# wo att", "\n", "self", ".", "lstm_final", "=", "nn", ".", "GRU", "(", "config", ".", "hidden_size", "*", "(", "abs", "(", "self", ".", "no_bert_layers", ")", "+", "1", ")", ",", "config", ".", "hidden_size", ",", "bidirectional", "=", "True", ")", "\n", "'''self.lstm_final = nn.LSTM(config.hidden_size * (abs(self.no_bert_layers) + 1), config.hidden_size,\n                                 bidirectional=True)'''", "\n", "\n", "self", ".", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "\n", "", "def", "init_hidden", "(", "self", ",", "bs", ")", ":", "\n", "# Before we've done anything, we dont have any hidden state.", "\n", "# Refer to the Pytorch documentation to see exactly", "\n", "# why they have this dimensionality.", "\n", "# The axes semantics are (num_layers, minibatch_size, hidden_dim)", "\n", "        ", "return", "torch", ".", "zeros", "(", "2", ",", "bs", ",", "self", ".", "hidden_size", ")", ".", "to", "(", "self", ".", "device", ")", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_lstm_emotion.model.BertForSequenceClassification.init_hidden": [[114, 120], ["torch.zeros().to", "torch.zeros"], "methods", ["None"], ["#return (torch.zeros(2, bs, self.hidden_size).to(self.device), torch.zeros(2, bs, self.hidden_size).to(self.device))", "\n", "\n", "", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "sent_scores", "=", "None", ",", "return_indices", "=", "False", ")", ":", "\n", "\n", "        ", "encoded_layers", ",", "_", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ")", "# 12 layers each (batch_size, seg_lengh, hidden_size)", "\n", "bert_encoded_layers", "=", "self", ".", "dropout", "(", "encoded_layers", "[", "-", "1", "]", ".", "clone", "(", ")", ".", "detach", "(", ")", ")", "# (batch, sent_len, embed_dim)", "\n", "bs", ",", "_", ",", "_", "=", "bert_encoded_layers", ".", "shape", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_lstm_emotion.model.BertForSequenceClassification.get_att": [[121, 128], ["torch.cat", "model.BertForSequenceClassification.att_lin", "torch.nn.functional.softmax"], "methods", ["None"], ["sent_embeddings", "=", "self", ".", "sent_embeddings", "(", "sent_scores", ")", "\n", "\n", "concat", "=", "torch", ".", "cat", "(", "[", "bert_encoded_layers", ",", "sent_embeddings", "]", ",", "-", "1", ")", ".", "permute", "(", "1", ",", "0", ",", "2", ")", "\n", "\n", "\n", "\n", "hidden", "=", "self", ".", "init_hidden", "(", "bs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_lstm_emotion.model.BertForSequenceClassification.forward": [[129, 182], ["model.BertForSequenceClassification.bert", "model.BertForSequenceClassification.dropout", "model.BertForSequenceClassification.emotion_embeddings().mean", "torch.cat().permute", "model.BertForSequenceClassification.init_hidden", "model.BertForSequenceClassification.lstm_final", "model.BertForSequenceClassification.dropout", "model.BertForSequenceClassification.classifier", "l.clone().detach", "torch.cat", "torch.nn.functional.adaptive_avg_pool1d", "torch.cat", "model.BertForSequenceClassification.dropout_pool", "model.BertForSequenceClassification.emotion_embeddings", "torch.cat", "torch.nn.functional.adaptive_max_pool1d_with_indices", "torch.nn.functional.adaptive_max_pool1d", "model.BertForSequenceClassification.permute", "l.clone", "model.BertForSequenceClassification.permute", "model.BertForSequenceClassification.permute", "torch.nn.functional.adaptive_max_pool1d.view", "torch.nn.functional.adaptive_avg_pool1d.view"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_lstm_emotion.model.BertForSequenceClassification.init_hidden"], ["out_score", ",", "_", "=", "self", ".", "lstm_final", "(", "concat", ",", "hidden", ")", "# hidden added", "\n", "out_score", "=", "self", ".", "dropout", "(", "out_score", ")", "# added", "\n", "\n", "if", "self", ".", "pooling", ":", "\n", "            ", "_", ",", "bs", ",", "_", "=", "out_score", ".", "shape", "\n", "if", "return_indices", ":", "\n", "\n", "                ", "mx_pool", ",", "mx_pool_idx", "=", "F", ".", "adaptive_max_pool1d_with_indices", "(", "out_score", ".", "permute", "(", "1", ",", "2", ",", "0", ")", ",", "(", "1", ",", ")", ")", "\n", "", "else", ":", "\n", "                ", "mx_pool", "=", "F", ".", "adaptive_max_pool1d", "(", "out_score", ".", "permute", "(", "1", ",", "2", ",", "0", ")", ",", "(", "1", ",", ")", ")", "\n", "", "avg_pool", "=", "F", ".", "adaptive_avg_pool1d", "(", "out_score", ".", "permute", "(", "1", ",", "2", ",", "0", ")", ",", "(", "1", ",", ")", ")", "# (bs, input_size, seq_len)", "\n", "out_score", "=", "torch", ".", "cat", "(", "[", "mx_pool", ".", "view", "(", "bs", ",", "-", "1", ")", ",", "avg_pool", ".", "view", "(", "bs", ",", "-", "1", ")", ",", "out_score", "[", "-", "1", "]", "]", ",", "-", "1", ")", "\n", "out_score", "=", "self", ".", "dropout", "(", "out_score", ")", "\n", "\n", "", "else", ":", "\n", "            ", "out_score", "=", "out_score", "[", "-", "1", "]", "\n", "\n", "", "logits", "=", "self", ".", "classifier", "(", "out_score", ")", "\n", "if", "return_indices", "and", "self", ".", "pooling", ":", "\n", "            ", "return", "logits", ",", "mx_pool_idx", "\n", "\n", "", "return", "logits", "\n", "\n", "\n", "\n", "\n", "", "", ""]], "home.repos.pwc.inspect_result.marjanhs_procon20.bert_lstm_emotion.args.get_args": [[6, 57], ["models.args.get_args", "models.args.get_args.add_argument", "models.args.get_args.add_argument", "models.args.get_args.add_argument", "models.args.get_args.add_argument", "models.args.get_args.add_argument", "models.args.get_args.add_argument", "models.args.get_args.add_argument", "models.args.get_args.add_argument", "models.args.get_args.add_argument", "models.args.get_args.add_argument", "models.args.get_args.add_argument", "models.args.get_args.add_argument", "models.args.get_args.add_argument", "models.args.get_args.add_argument", "models.args.get_args.add_argument", "models.args.get_args.add_argument", "models.args.get_args.add_argument", "models.args.get_args.add_argument", "models.args.get_args.add_argument", "models.args.get_args.parse_args", "os.path.join", "os.path.join"], "function", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_lstm_emotion.args.get_args"], ["    ", "parser", "=", "ArgumentParser", "(", "description", "=", "\"PyTorch deep learning models for document classification\"", ")", "\n", "par", "=", "'/home/projects/'", "\n", "parser", ".", "add_argument", "(", "'--no-cuda'", ",", "action", "=", "'store_false'", ",", "dest", "=", "'cuda'", ")", "\n", "parser", ".", "add_argument", "(", "'--gpu'", ",", "type", "=", "int", ",", "default", "=", "0", ")", "\n", "parser", ".", "add_argument", "(", "'--epochs'", ",", "type", "=", "int", ",", "default", "=", "50", ")", "\n", "parser", ".", "add_argument", "(", "'--batch-size'", ",", "type", "=", "int", ",", "default", "=", "1024", ")", "\n", "parser", ".", "add_argument", "(", "'--lr'", ",", "type", "=", "float", ",", "default", "=", "0.001", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "type", "=", "int", ",", "default", "=", "3435", ")", "# default value: 3435", "\n", "parser", ".", "add_argument", "(", "'--patience'", ",", "type", "=", "int", ",", "default", "=", "5", ")", "\n", "parser", ".", "add_argument", "(", "'--log-every'", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "'--data-dir'", ",", "default", "=", "os", ".", "path", ".", "join", "(", "par", ",", "'hedwig-data'", ",", "'datasets'", ")", ")", "#os.pardir", "\n", "parser", ".", "add_argument", "(", "'--early_on_f1'", ",", "action", "=", "'store_true'", ")", "\n", "\n", "return", "parser", "\n", "", ""]], "home.repos.pwc.inspect_result.marjanhs_procon20.common.evaluate.EvaluatorFactory.get_evaluator": [[23, 36], ["hasattr", "ValueError", "ValueError"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "get_evaluator", "(", "dataset_cls", ",", "model", ",", "embedding", ",", "data_loader", ",", "batch_size", ",", "device", ",", "keep_results", "=", "False", ")", ":", "\n", "        ", "if", "data_loader", "is", "None", ":", "\n", "            ", "return", "None", "\n", "\n", "", "if", "not", "hasattr", "(", "dataset_cls", ",", "'NAME'", ")", ":", "\n", "            ", "raise", "ValueError", "(", "'Invalid dataset. Dataset should have NAME attribute.'", ")", "\n", "\n", "", "if", "dataset_cls", ".", "NAME", "not", "in", "EvaluatorFactory", ".", "evaluator_map", ":", "\n", "            ", "raise", "ValueError", "(", "'{} is not implemented.'", ".", "format", "(", "dataset_cls", ")", ")", "\n", "\n", "", "return", "EvaluatorFactory", ".", "evaluator_map", "[", "dataset_cls", ".", "NAME", "]", "(", "\n", "dataset_cls", ",", "model", ",", "embedding", ",", "data_loader", ",", "batch_size", ",", "device", ",", "keep_results", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.common.train.TrainerFactory.get_trainer": [[23, 31], ["ValueError"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "get_trainer", "(", "dataset_name", ",", "model", ",", "embedding", ",", "train_loader", ",", "trainer_config", ",", "train_evaluator", ",", "test_evaluator", ",", "dev_evaluator", "=", "None", ")", ":", "\n", "\n", "        ", "if", "dataset_name", "not", "in", "TrainerFactory", ".", "trainer_map", ":", "\n", "            ", "raise", "ValueError", "(", "'{} is not implemented.'", ".", "format", "(", "dataset_name", ")", ")", "\n", "\n", "", "return", "TrainerFactory", ".", "trainer_map", "[", "dataset_name", "]", "(", "\n", "model", ",", "embedding", ",", "train_loader", ",", "trainer_config", ",", "train_evaluator", ",", "test_evaluator", ",", "dev_evaluator", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.trainers.bert_trainer.BertTrainer.__init__": [[57, 79], ["bert_trainer.BertTrainer.processor.get_train_examples", "utils.tokenization.BertTokenizer.from_pretrained", "datetime.datetime.now().strftime", "os.path.join", "int", "datetime.datetime.now", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "len"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor.get_train_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertPreTrainedModel.from_pretrained"], ["    ", "def", "__init__", "(", "self", ",", "model", ",", "optimizer", ",", "processor", ",", "args", ")", ":", "\n", "        ", "self", ".", "args", "=", "args", "\n", "self", ".", "model", "=", "model", "\n", "self", ".", "optimizer", "=", "optimizer", "\n", "self", ".", "processor", "=", "processor", "\n", "self", ".", "train_examples", "=", "self", ".", "processor", ".", "get_train_examples", "(", "args", ".", "data_dir", ",", "args", ".", "train_name", ")", "\n", "self", ".", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "args", ".", "model", ",", "is_lowercase", "=", "args", ".", "is_lowercase", ")", "\n", "\n", "timestamp", "=", "datetime", ".", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "\"%Y-%m-%d_%H-%M-%S\"", ")", "\n", "self", ".", "snapshot_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "args", ".", "save_path", ",", "self", ".", "processor", ".", "NAME", ",", "'%s.pt'", "%", "timestamp", ")", "\n", "\n", "self", ".", "num_train_optimization_steps", "=", "int", "(", "\n", "len", "(", "self", ".", "train_examples", ")", "/", "args", ".", "batch_size", "/", "args", ".", "gradient_accumulation_steps", ")", "*", "args", ".", "epochs", "\n", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "            ", "self", ".", "num_train_optimization_steps", "=", "args", ".", "num_train_optimization_steps", "//", "torch", ".", "distributed", ".", "get_world_size", "(", ")", "\n", "\n", "", "self", ".", "log_header", "=", "'Epoch Iteration Progress   Dev/Acc.  Dev/Pr.  Dev/Re.   Dev/F1   Dev/Loss   Dev/F1ma, Dev/HLoss, Dev/Jacc, Train/Loss'", "\n", "self", ".", "log_template", "=", "' '", ".", "join", "(", "'{:>5.0f},{:>9.0f},{:>6.0f}/{:<5.0f} {:>6.4f},{:>8.4f},{:8.4f},{:8.4f},{:10.4f},{:10.4f},{:10.4f},{:10.4f},{:10.4f}'", ".", "split", "(", "','", ")", ")", "\n", "\n", "self", ".", "iterations", ",", "self", ".", "nb_tr_steps", ",", "self", ".", "tr_loss", "=", "0", ",", "0", ",", "0", "\n", "self", ".", "best_dev_measure", ",", "self", ".", "unimproved_iters", "=", "0", ",", "0", "\n", "self", ".", "early_stop", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.trainers.bert_trainer.BertTrainer.get_order": [[80, 87], ["range", "groups.items", "name.startswith", "str"], "methods", ["None"], ["", "def", "get_order", "(", "self", ",", "name", ")", ":", "\n", "\n", "        ", "groups", "=", "{", "'bert.embeddings'", ":", "0", ",", "'bert.pooler'", ":", "12", ",", "'classifier'", ":", "13", "}", "#classifier", "\n", "for", "i", "in", "range", "(", "12", ")", ":", "\n", "            ", "groups", "[", "'bert.encoder.layer.'", "+", "str", "(", "i", ")", "]", "=", "i", "+", "1", "\n", "", "x", "=", "[", "v", "for", "k", ",", "v", "in", "groups", ".", "items", "(", ")", "if", "name", ".", "startswith", "(", "k", ")", "]", "[", "0", "]", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.trainers.bert_trainer.BertTrainer.train_layer_qroup": [[88, 105], ["bert_trainer.BertTrainer.train_epoch", "common.evaluators.bert_evaluator.BertEvaluator", "tqdm.tqdm.tqdm.write", "tqdm.tqdm.tqdm.write", "torch.save", "torch.save", "torch.save", "torch.save", "common.evaluators.bert_evaluator.BertEvaluator.get_scores", "bert_trainer.BertTrainer.log_template.format"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.trainers.bert_sent_trainer.BertTrainer.train_epoch", "home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.classification_evaluator.ClassificationEvaluator.get_scores"], ["", "def", "train_layer_qroup", "(", "self", ",", "dataloader", ",", "to_freeze_layer", ",", "model_path", ")", ":", "\n", "        ", "self", ".", "train_epoch", "(", "dataloader", ",", "freez_layer", "=", "to_freeze_layer", ")", "\n", "dev_evaluator", "=", "BertEvaluator", "(", "self", ".", "model", ",", "self", ".", "processor", ",", "self", ".", "args", ",", "split", "=", "'dev'", ")", "\n", "dev_acc", ",", "dev_precision", ",", "dev_recall", ",", "dev_f1", ",", "dev_loss", ",", "dev_f1_macro", ",", "dev_hamming_loss", ",", "dev_jaccard_score", ",", "dev_predicted_labels", ",", "dev_target_labels", "=", "dev_evaluator", ".", "get_scores", "(", ")", "[", "0", "]", "\n", "\n", "# Print validation results", "\n", "tqdm", ".", "write", "(", "self", ".", "log_header", ")", "\n", "tqdm", ".", "write", "(", "self", ".", "log_template", ".", "format", "(", "1", ",", "self", ".", "iterations", ",", "1", ",", "self", ".", "args", ".", "epochs", ",", "\n", "dev_acc", ",", "dev_precision", ",", "dev_recall", ",", "dev_f1", ",", "dev_loss", ",", "dev_f1_macro", ",", "\n", "dev_hamming_loss", ",", "dev_jaccard_score", ")", ")", "\n", "\n", "torch", ".", "save", "(", "self", ".", "model", ",", "model_path", "/", "f'{to_freeze_layer}.pt'", ")", "\n", "# update learning rate", "\n", "for", "groups", "in", "self", ".", "optimizer", ".", "param_groups", ":", "\n", "            ", "lr", "=", "groups", "[", "'lr'", "]", "if", "'lr'", "in", "groups", "else", "self", ".", "args", ".", "lr", "\n", "groups", "[", "'lr'", "]", "=", "2e-5", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.trainers.bert_trainer.BertTrainer.freez": [[106, 119], ["bert_trainer.BertTrainer.get_order", "bert_trainer.BertTrainer.model.named_parameters", "bert_trainer.BertTrainer.get_order"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.trainers.bert_trainer.BertTrainer.get_order", "home.repos.pwc.inspect_result.marjanhs_procon20.trainers.bert_trainer.BertTrainer.get_order"], ["", "", "def", "freez", "(", "self", ",", "layer", ")", ":", "\n", "        ", "'''\n        layer and its subsequent layers will be unfreezd, the layers befor 'layer' will be freezed!\n        :param layer:\n        :return:\n        '''", "\n", "if", "layer", ":", "\n", "            ", "order", "=", "self", ".", "get_order", "(", "layer", ")", "\n", "for", "n", ",", "p", "in", "self", ".", "model", ".", "named_parameters", "(", ")", ":", "\n", "                ", "if", "self", ".", "get_order", "(", "n", ")", "<", "order", ":", "\n", "                    ", "p", ".", "requires_grad", "=", "False", "\n", "", "else", ":", "\n", "                    ", "p", ".", "requires_grad", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.trainers.bert_trainer.BertTrainer.unfreez_all": [[120, 123], ["bert_trainer.BertTrainer.model.named_parameters"], "methods", ["None"], ["", "", "", "", "def", "unfreez_all", "(", "self", ")", ":", "\n", "        ", "for", "n", ",", "p", "in", "self", ".", "model", ".", "named_parameters", "(", ")", ":", "\n", "            ", "p", ".", "requires_grad", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.trainers.bert_trainer.BertTrainer.train_epoch": [[124, 178], ["enumerate", "tqdm.tqdm.tqdm", "bert_trainer.BertTrainer.model.train", "tuple", "bert_trainer.BertTrainer.model", "isinstance", "loss.mean.mean.item", "loss.mean.mean.item", "bert_trainer.BertTrainer.freez", "torch.cosine_similarity", "torch.cosine_similarity", "range", "torch.binary_cross_entropy_with_logits", "torch.binary_cross_entropy_with_logits", "torch.cross_entropy", "torch.cross_entropy", "loss.mean.mean.mean", "bert_trainer.BertTrainer.optimizer.backward", "loss.mean.mean.backward", "bert_trainer.BertTrainer.optimizer.step", "bert_trainer.BertTrainer.optimizer.zero_grad", "t.to", "len", "torch.eq().all", "torch.eq().all", "torch.eq().all", "torch.eq().all", "label_ids.float", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.eq().all", "torch.eq().all", "torch.eq().all", "torch.eq().all", "utils.optimization.warmup_linear", "torch.eq", "torch.eq", "torch.eq", "torch.eq", "max", "print", "exit", "torch.Tensor().long().cuda", "torch.Tensor().long().cuda", "torch.Tensor().long().cuda", "torch.Tensor().long().cuda", "torch.eq", "torch.eq", "torch.eq", "torch.eq", "torch.Tensor().long().cuda", "torch.Tensor().long().cuda", "torch.Tensor().long().cuda", "torch.Tensor().long().cuda", "torch.Tensor().long", "torch.Tensor().long", "torch.Tensor().long", "torch.Tensor().long", "torch.Tensor().long", "torch.Tensor().long", "torch.Tensor().long", "torch.Tensor().long", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.trainers.bert_sent_trainer.BertTrainer.train", "home.repos.pwc.inspect_result.marjanhs_procon20.trainers.bert_trainer.BertTrainer.freez", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.optimization.BertAdam.step", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.optimization.warmup_linear"], ["", "", "def", "train_epoch", "(", "self", ",", "train_dataloader", ",", "freez_layer", "=", "None", ")", ":", "\n", "        ", "loss_epoch", "=", "0", "\n", "for", "step", ",", "batch", "in", "enumerate", "(", "tqdm", "(", "train_dataloader", ",", "desc", "=", "\"Training\"", ")", ")", ":", "\n", "            ", "self", ".", "model", ".", "train", "(", ")", "\n", "if", "freez_layer", ":", "self", ".", "freez", "(", "freez_layer", ")", "\n", "\n", "batch", "=", "tuple", "(", "t", ".", "to", "(", "self", ".", "args", ".", "device", ")", "for", "t", "in", "batch", ")", "\n", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "label_ids", "=", "batch", "\n", "logits", "=", "self", ".", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ")", "\n", "loss_extra", "=", "0", "\n", "if", "isinstance", "(", "logits", ",", "tuple", ")", ":", "\n", "                ", "logits", ",", "(", "first_SEP", ",", "second_SEP", ")", "=", "logits", "\n", "cos_simi", "=", "F", ".", "cosine_similarity", "(", "first_SEP", ",", "second_SEP", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "label_ids", ")", ")", ":", "\n", "                    ", "if", "torch", ".", "eq", "(", "label_ids", "[", "i", "]", ",", "torch", ".", "Tensor", "(", "[", "0", ",", "1", "]", ")", ".", "long", "(", ")", ".", "cuda", "(", ")", ")", ".", "all", "(", ")", ":", "\n", "                        ", "loss_extra", "+=", "1", "-", "cos_simi", "[", "i", "]", "\n", "", "elif", "torch", ".", "eq", "(", "label_ids", "[", "i", "]", ",", "torch", ".", "Tensor", "(", "[", "1", ",", "0", "]", ")", ".", "long", "(", ")", ".", "cuda", "(", ")", ")", ".", "all", "(", ")", ":", "\n", "                        ", "loss_extra", "+=", "max", "(", "0", ",", "cos_simi", "[", "i", "]", ")", "\n", "", "else", ":", "\n", "                        ", "print", "(", "'Invalid label value ERROR'", ",", "label_ids", "[", "i", "]", ")", "\n", "exit", "(", "1", ")", "\n", "\n", "", "", "", "if", "self", ".", "args", ".", "is_multilabel", ":", "\n", "                ", "loss", "=", "F", ".", "binary_cross_entropy_with_logits", "(", "logits", ",", "label_ids", ".", "float", "(", ")", ")", "\n", "", "else", ":", "\n", "                ", "loss", "=", "F", ".", "cross_entropy", "(", "logits", ",", "torch", ".", "argmax", "(", "label_ids", ",", "dim", "=", "1", ")", ")", "\n", "#print( 'loss extra: ', loss_extra)", "\n", "loss", "+=", "loss_extra", "\n", "\n", "", "if", "self", ".", "args", ".", "n_gpu", ">", "1", ":", "\n", "                ", "loss", "=", "loss", ".", "mean", "(", ")", "\n", "", "if", "self", ".", "args", ".", "gradient_accumulation_steps", ">", "1", ":", "\n", "                ", "loss", "=", "loss", "/", "self", ".", "args", ".", "gradient_accumulation_steps", "\n", "\n", "", "if", "self", ".", "args", ".", "fp16", ":", "\n", "                ", "self", ".", "optimizer", ".", "backward", "(", "loss", ")", "\n", "", "else", ":", "\n", "                ", "loss", ".", "backward", "(", ")", "\n", "\n", "", "self", ".", "tr_loss", "+=", "loss", ".", "item", "(", ")", "\n", "loss_epoch", "+=", "loss", ".", "item", "(", ")", "\n", "self", ".", "nb_tr_steps", "+=", "1", "\n", "if", "(", "step", "+", "1", ")", "%", "self", ".", "args", ".", "gradient_accumulation_steps", "==", "0", ":", "\n", "                ", "if", "self", ".", "args", ".", "fp16", ":", "\n", "                    ", "lr_this_step", "=", "self", ".", "args", ".", "learning_rate", "*", "warmup_linear", "(", "self", ".", "iterations", "/", "self", ".", "num_train_optimization_steps", ",", "self", ".", "args", ".", "warmup_proportion", ")", "\n", "for", "param_group", "in", "self", ".", "optimizer", ".", "param_groups", ":", "\n", "                        ", "param_group", "[", "'lr'", "]", "=", "lr_this_step", "\n", "", "", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "self", ".", "iterations", "+=", "1", "\n", "\n", "#print('train loss', np.mean(tr_loss))", "\n", "#print('avg grads', np.mean(grads))", "\n", "", "", "return", "loss_epoch", "/", "(", "step", "+", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.trainers.bert_trainer.BertTrainer.train": [[179, 246], ["print", "print", "print", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "torch.utils.data.TensorDataset", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "tqdm.tqdm.trange", "datasets.bert_processors.abstract_processor.convert_examples_to_hierarchical_features", "datasets.bert_processors.abstract_processor.convert_examples_to_features", "utils.preprocessing.pad_input_matrix", "utils.preprocessing.pad_input_matrix", "utils.preprocessing.pad_input_matrix", "len", "torch.utils.data.RandomSampler", "torch.utils.data.RandomSampler", "torch.utils.data.distributed.DistributedSampler", "torch.utils.data.distributed.DistributedSampler", "int", "bert_trainer.BertTrainer.train_epoch", "common.evaluators.bert_evaluator.BertEvaluator", "tqdm.tqdm.tqdm.write", "tqdm.tqdm.tqdm.write", "common.evaluators.bert_evaluator.BertEvaluator.get_scores", "bert_trainer.BertTrainer.log_template.format", "torch.save", "torch.save", "torch.save", "torch.save", "print"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.convert_examples_to_hierarchical_features", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.convert_examples_to_features", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.preprocessing.pad_input_matrix", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.preprocessing.pad_input_matrix", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.preprocessing.pad_input_matrix", "home.repos.pwc.inspect_result.marjanhs_procon20.trainers.bert_sent_trainer.BertTrainer.train_epoch", "home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.classification_evaluator.ClassificationEvaluator.get_scores"], ["", "def", "train", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "args", ".", "is_hierarchical", ":", "\n", "            ", "train_features", "=", "convert_examples_to_hierarchical_features", "(", "\n", "self", ".", "train_examples", ",", "self", ".", "args", ".", "max_seq_length", ",", "self", ".", "tokenizer", ")", "\n", "", "else", ":", "\n", "            ", "train_features", "=", "convert_examples_to_features", "(", "\n", "self", ".", "train_examples", ",", "self", ".", "args", ".", "max_seq_length", ",", "self", ".", "tokenizer", ")", "\n", "\n", "", "unpadded_input_ids", "=", "[", "f", ".", "input_ids", "for", "f", "in", "train_features", "]", "\n", "unpadded_input_mask", "=", "[", "f", ".", "input_mask", "for", "f", "in", "train_features", "]", "\n", "unpadded_segment_ids", "=", "[", "f", ".", "segment_ids", "for", "f", "in", "train_features", "]", "\n", "\n", "if", "self", ".", "args", ".", "is_hierarchical", ":", "\n", "            ", "pad_input_matrix", "(", "unpadded_input_ids", ",", "self", ".", "args", ".", "max_doc_length", ")", "\n", "pad_input_matrix", "(", "unpadded_input_mask", ",", "self", ".", "args", ".", "max_doc_length", ")", "\n", "pad_input_matrix", "(", "unpadded_segment_ids", ",", "self", ".", "args", ".", "max_doc_length", ")", "\n", "\n", "", "print", "(", "\"Number of examples: \"", ",", "len", "(", "self", ".", "train_examples", ")", ")", "\n", "print", "(", "\"Batch size:\"", ",", "self", ".", "args", ".", "batch_size", ")", "\n", "print", "(", "\"Num of steps:\"", ",", "self", ".", "num_train_optimization_steps", ")", "\n", "\n", "padded_input_ids", "=", "torch", ".", "tensor", "(", "unpadded_input_ids", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "padded_input_mask", "=", "torch", ".", "tensor", "(", "unpadded_input_mask", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "padded_segment_ids", "=", "torch", ".", "tensor", "(", "unpadded_segment_ids", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "label_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "label_id", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "train_data", "=", "TensorDataset", "(", "padded_input_ids", ",", "padded_input_mask", ",", "padded_segment_ids", ",", "label_ids", ")", "\n", "\n", "if", "self", ".", "args", ".", "local_rank", "==", "-", "1", ":", "\n", "            ", "train_sampler", "=", "RandomSampler", "(", "train_data", ")", "\n", "", "else", ":", "\n", "            ", "train_sampler", "=", "DistributedSampler", "(", "train_data", ")", "\n", "\n", "", "train_dataloader", "=", "DataLoader", "(", "train_data", ",", "sampler", "=", "train_sampler", ",", "batch_size", "=", "self", ".", "args", ".", "batch_size", ")", "\n", "\n", "for", "epoch", "in", "trange", "(", "int", "(", "self", ".", "args", ".", "epochs", ")", ",", "desc", "=", "\"Epoch\"", ")", ":", "\n", "            ", "loss_epoch", "=", "self", ".", "train_epoch", "(", "train_dataloader", ")", "\n", "dev_evaluator", "=", "BertEvaluator", "(", "self", ".", "model", ",", "self", ".", "processor", ",", "self", ".", "args", ",", "split", "=", "'dev'", ")", "\n", "dev_acc", ",", "dev_precision", ",", "dev_recall", ",", "dev_f1", ",", "dev_loss", ",", "dev_f1_macro", ",", "dev_hamming_loss", ",", "dev_jaccard_score", ",", "dev_predicted_labels", ",", "dev_target_labels", "=", "dev_evaluator", ".", "get_scores", "(", ")", "[", "0", "]", "\n", "\n", "# Print validation results", "\n", "tqdm", ".", "write", "(", "self", ".", "log_header", ")", "\n", "tqdm", ".", "write", "(", "self", ".", "log_template", ".", "format", "(", "epoch", "+", "1", ",", "self", ".", "iterations", ",", "epoch", "+", "1", ",", "self", ".", "args", ".", "epochs", ",", "\n", "dev_acc", ",", "dev_precision", ",", "dev_recall", ",", "dev_f1", ",", "dev_loss", ",", "dev_f1_macro", ",", "dev_hamming_loss", ",", "dev_jaccard_score", ",", "loss_epoch", ")", ")", "\n", "\n", "if", "self", ".", "args", ".", "early_on_f1", ":", "\n", "                ", "if", "dev_recall", "!=", "1", ":", "\n", "                    ", "dev_measure", "=", "dev_f1", "\n", "", "else", ":", "\n", "                    ", "dev_measure", "=", "0", "\n", "", "measure_name", "=", "'F1'", "\n", "", "else", ":", "\n", "                ", "dev_measure", "=", "dev_acc", "\n", "measure_name", "=", "'Balanced Acc'", "\n", "\n", "# Update validation results", "\n", "", "if", "dev_measure", ">", "self", ".", "best_dev_measure", ":", "\n", "                ", "self", ".", "unimproved_iters", "=", "0", "\n", "self", ".", "best_dev_measure", "=", "dev_measure", "\n", "torch", ".", "save", "(", "self", ".", "model", ",", "self", ".", "snapshot_path", ")", "\n", "\n", "", "else", ":", "\n", "                ", "self", ".", "unimproved_iters", "+=", "1", "\n", "if", "self", ".", "unimproved_iters", ">=", "self", ".", "args", ".", "patience", ":", "\n", "                    ", "self", ".", "early_stop", "=", "True", "\n", "print", "(", "\"Early Stopping. Epoch: {}, Best {}: {}\"", ".", "format", "(", "epoch", ",", "measure_name", ",", "self", ".", "best_dev_measure", ")", ")", "\n", "break", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.trainers.bert_trainer.BertTrainer.train_gradually": [[247, 319], ["print", "print", "print", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "torch.utils.data.TensorDataset", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "pathlib.Path", "bert_trainer.BertTrainer.train_layer_qroup", "bert_trainer.BertTrainer.unfreez_all", "tqdm.tqdm.trange", "datasets.bert_processors.abstract_processor.convert_examples_to_hierarchical_features", "datasets.bert_processors.abstract_processor.convert_examples_to_features", "utils.preprocessing.pad_input_matrix", "utils.preprocessing.pad_input_matrix", "utils.preprocessing.pad_input_matrix", "len", "torch.utils.data.RandomSampler", "torch.utils.data.RandomSampler", "torch.utils.data.distributed.DistributedSampler", "torch.utils.data.distributed.DistributedSampler", "bert_trainer.BertTrainer.snapshot_path.split", "int", "bert_trainer.BertTrainer.train_epoch", "common.evaluators.bert_evaluator.BertEvaluator", "tqdm.tqdm.tqdm.write", "tqdm.tqdm.tqdm.write", "common.evaluators.bert_evaluator.BertEvaluator.get_scores", "bert_trainer.BertTrainer.log_template.format", "torch.save", "torch.save", "torch.save", "torch.save", "tqdm.tqdm.tqdm.write"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.trainers.bert_trainer.BertTrainer.train_layer_qroup", "home.repos.pwc.inspect_result.marjanhs_procon20.trainers.bert_trainer.BertTrainer.unfreez_all", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.convert_examples_to_hierarchical_features", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.convert_examples_to_features", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.preprocessing.pad_input_matrix", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.preprocessing.pad_input_matrix", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.preprocessing.pad_input_matrix", "home.repos.pwc.inspect_result.marjanhs_procon20.trainers.bert_sent_trainer.BertTrainer.train_epoch", "home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.classification_evaluator.ClassificationEvaluator.get_scores"], ["", "", "", "", "def", "train_gradually", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "args", ".", "is_hierarchical", ":", "\n", "            ", "train_features", "=", "convert_examples_to_hierarchical_features", "(", "\n", "self", ".", "train_examples", ",", "self", ".", "args", ".", "max_seq_length", ",", "self", ".", "tokenizer", ")", "\n", "", "else", ":", "\n", "            ", "train_features", "=", "convert_examples_to_features", "(", "\n", "self", ".", "train_examples", ",", "self", ".", "args", ".", "max_seq_length", ",", "self", ".", "tokenizer", ")", "\n", "\n", "", "unpadded_input_ids", "=", "[", "f", ".", "input_ids", "for", "f", "in", "train_features", "]", "\n", "unpadded_input_mask", "=", "[", "f", ".", "input_mask", "for", "f", "in", "train_features", "]", "\n", "unpadded_segment_ids", "=", "[", "f", ".", "segment_ids", "for", "f", "in", "train_features", "]", "\n", "\n", "if", "self", ".", "args", ".", "is_hierarchical", ":", "\n", "            ", "pad_input_matrix", "(", "unpadded_input_ids", ",", "self", ".", "args", ".", "max_doc_length", ")", "\n", "pad_input_matrix", "(", "unpadded_input_mask", ",", "self", ".", "args", ".", "max_doc_length", ")", "\n", "pad_input_matrix", "(", "unpadded_segment_ids", ",", "self", ".", "args", ".", "max_doc_length", ")", "\n", "\n", "", "print", "(", "\"Number of examples: \"", ",", "len", "(", "self", ".", "train_examples", ")", ")", "\n", "print", "(", "\"Batch size:\"", ",", "self", ".", "args", ".", "batch_size", ")", "\n", "print", "(", "\"Num of steps:\"", ",", "self", ".", "num_train_optimization_steps", ")", "\n", "\n", "padded_input_ids", "=", "torch", ".", "tensor", "(", "unpadded_input_ids", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "padded_input_mask", "=", "torch", ".", "tensor", "(", "unpadded_input_mask", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "padded_segment_ids", "=", "torch", ".", "tensor", "(", "unpadded_segment_ids", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "label_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "label_id", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "train_data", "=", "TensorDataset", "(", "padded_input_ids", ",", "padded_input_mask", ",", "padded_segment_ids", ",", "label_ids", ")", "\n", "\n", "if", "self", ".", "args", ".", "local_rank", "==", "-", "1", ":", "\n", "            ", "train_sampler", "=", "RandomSampler", "(", "train_data", ")", "\n", "", "else", ":", "\n", "            ", "train_sampler", "=", "DistributedSampler", "(", "train_data", ")", "\n", "\n", "", "train_dataloader", "=", "DataLoader", "(", "train_data", ",", "sampler", "=", "train_sampler", ",", "batch_size", "=", "self", ".", "args", ".", "batch_size", ")", "\n", "\n", "# train gradually", "\n", "model_path", "=", "self", ".", "snapshot_path", ".", "split", "(", "'/'", ")", "[", "0", ":", "-", "1", "]", "\n", "model_path", "=", "Path", "(", "'/'", ".", "join", "(", "model_path", ")", ")", "\n", "# freeze all layers except classifier", "\n", "\n", "self", ".", "train_layer_qroup", "(", "train_dataloader", ",", "to_freeze_layer", "=", "'classifier'", ",", "model_path", "=", "model_path", ")", "\n", "\n", "# freeze all layers expect pooler and its subsequents", "\n", "\n", "'''self.train_layer_qroup(train_dataloader, to_freeze_layer='bert.pooler', model_path=model_path)\n        for i in range(11,-1, -1):\n            self.train_layer_qroup(train_dataloader, to_freeze_layer='bert.encoder.layer.'+str(i), model_path=model_path)'''", "\n", "\n", "self", ".", "unfreez_all", "(", ")", "\n", "\n", "for", "epoch", "in", "trange", "(", "int", "(", "self", ".", "args", ".", "epochs", ")", ",", "desc", "=", "\"Epoch\"", ")", ":", "\n", "            ", "self", ".", "train_epoch", "(", "train_dataloader", ")", "\n", "dev_evaluator", "=", "BertEvaluator", "(", "self", ".", "model", ",", "self", ".", "processor", ",", "self", ".", "args", ",", "split", "=", "'dev'", ")", "\n", "dev_acc", ",", "dev_precision", ",", "dev_recall", ",", "dev_f1", ",", "dev_loss", ",", "dev_f1_macro", ",", "dev_hamming_loss", ",", "dev_jaccard_score", ",", "dev_predicted_labels", ",", "dev_target_labels", "=", "dev_evaluator", ".", "get_scores", "(", ")", "[", "0", "]", "\n", "\n", "# Print validation results", "\n", "tqdm", ".", "write", "(", "self", ".", "log_header", ")", "\n", "tqdm", ".", "write", "(", "self", ".", "log_template", ".", "format", "(", "epoch", "+", "1", ",", "self", ".", "iterations", ",", "epoch", "+", "1", ",", "self", ".", "args", ".", "epochs", ",", "\n", "dev_acc", ",", "dev_precision", ",", "dev_recall", ",", "dev_f1", ",", "dev_loss", ",", "dev_f1_macro", ",", "dev_hamming_loss", ",", "dev_jaccard_score", ")", ")", "\n", "\n", "# Update validation results", "\n", "if", "dev_f1", ">", "self", ".", "best_dev_f1", ":", "\n", "                ", "self", ".", "unimproved_iters", "=", "0", "\n", "self", ".", "best_dev_f1", "=", "dev_f1", "\n", "torch", ".", "save", "(", "self", ".", "model", ",", "self", ".", "snapshot_path", ")", "\n", "\n", "", "else", ":", "\n", "                ", "self", ".", "unimproved_iters", "+=", "1", "\n", "if", "self", ".", "unimproved_iters", ">=", "self", ".", "args", ".", "patience", ":", "\n", "                    ", "self", ".", "early_stop", "=", "True", "\n", "tqdm", ".", "write", "(", "\"Early Stopping. Epoch: {}, Best Dev F1: {}\"", ".", "format", "(", "epoch", ",", "self", ".", "best_dev_f1", ")", ")", "\n", "break", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.trainers.bert_trainer.plot_grad_flow": [[25, 53], ["matplotlib.bar", "matplotlib.bar", "matplotlib.hlines", "matplotlib.xticks", "matplotlib.xlim", "matplotlib.ylim", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.title", "matplotlib.grid", "matplotlib.legend", "matplotlib.savefig", "numpy.arange", "numpy.arange", "range", "layers.append", "ave_grads.append", "max_grads.append", "len", "len", "len", "len", "len", "matplotlib.lines.Line2D", "matplotlib.lines.Line2D", "matplotlib.lines.Line2D", "p.grad.abs().mean", "p.grad.abs().max", "p.grad.abs", "p.grad.abs"], "function", ["None"], ["def", "plot_grad_flow", "(", "named_parameters", ")", ":", "\n", "    ", "'''Plots the gradients flowing through different layers in the net during training.\n    Can be used for checking for possible gradient vanishing / exploding problems.\n\n    Usage: Plug this function in Trainer class after loss.backwards() as\n    \"plot_grad_flow(self.model.named_parameters())\" to visualize the gradient flow'''", "\n", "ave_grads", "=", "[", "]", "\n", "max_grads", "=", "[", "]", "\n", "layers", "=", "[", "]", "\n", "for", "n", ",", "p", "in", "named_parameters", ":", "\n", "        ", "if", "(", "p", ".", "requires_grad", ")", "and", "(", "\"bias\"", "not", "in", "n", ")", ":", "\n", "            ", "layers", ".", "append", "(", "n", ")", "\n", "ave_grads", ".", "append", "(", "p", ".", "grad", ".", "abs", "(", ")", ".", "mean", "(", ")", ")", "\n", "max_grads", ".", "append", "(", "p", ".", "grad", ".", "abs", "(", ")", ".", "max", "(", ")", ")", "\n", "", "", "plt", ".", "bar", "(", "np", ".", "arange", "(", "len", "(", "max_grads", ")", ")", ",", "max_grads", ",", "alpha", "=", "0.1", ",", "lw", "=", "1", ",", "color", "=", "\"c\"", ")", "\n", "plt", ".", "bar", "(", "np", ".", "arange", "(", "len", "(", "max_grads", ")", ")", ",", "ave_grads", ",", "alpha", "=", "0.1", ",", "lw", "=", "1", ",", "color", "=", "\"b\"", ")", "\n", "plt", ".", "hlines", "(", "0", ",", "0", ",", "len", "(", "ave_grads", ")", "+", "1", ",", "lw", "=", "2", ",", "color", "=", "\"k\"", ")", "\n", "plt", ".", "xticks", "(", "range", "(", "0", ",", "len", "(", "ave_grads", ")", ",", "1", ")", ",", "layers", ",", "rotation", "=", "\"vertical\"", ")", "\n", "plt", ".", "xlim", "(", "left", "=", "0", ",", "right", "=", "len", "(", "ave_grads", ")", ")", "\n", "plt", ".", "ylim", "(", "bottom", "=", "-", "0.001", ",", "top", "=", "0.02", ")", "# zoom in on the lower gradient regions", "\n", "plt", ".", "xlabel", "(", "\"Layers\"", ")", "\n", "plt", ".", "ylabel", "(", "\"average gradient\"", ")", "\n", "plt", ".", "title", "(", "\"Gradient flow\"", ")", "\n", "plt", ".", "grid", "(", "True", ")", "\n", "plt", ".", "legend", "(", "[", "Line2D", "(", "[", "0", "]", ",", "[", "0", "]", ",", "color", "=", "\"c\"", ",", "lw", "=", "4", ")", ",", "\n", "Line2D", "(", "[", "0", "]", ",", "[", "0", "]", ",", "color", "=", "\"b\"", ",", "lw", "=", "4", ")", ",", "\n", "Line2D", "(", "[", "0", "]", ",", "[", "0", "]", ",", "color", "=", "\"k\"", ",", "lw", "=", "4", ")", "]", ",", "[", "'max-gradient'", ",", "'mean-gradient'", ",", "'zero-gradient'", "]", ")", "\n", "plt", ".", "savefig", "(", "'grads.png'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.trainers.classification_trainer_marjan.ClassificationTrainer.__init__": [[14, 29], ["common.trainers.trainer.Trainer.__init__", "datetime.datetime.now().strftime", "os.path.join", "datetime.datetime.now"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.classification_evaluator.ClassificationEvaluator.__init__"], ["    ", "def", "__init__", "(", "self", ",", "model", ",", "embedding", ",", "train_loader", ",", "trainer_config", ",", "train_evaluator", ",", "test_evaluator", ",", "dev_evaluator", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "model", ",", "embedding", ",", "train_loader", ",", "trainer_config", ",", "train_evaluator", ",", "test_evaluator", ",", "dev_evaluator", ")", "\n", "self", ".", "config", "=", "trainer_config", "\n", "self", ".", "early_stop", "=", "False", "\n", "self", ".", "best_dev_f1", "=", "0", "\n", "self", ".", "iterations", "=", "0", "\n", "self", ".", "iters_not_improved", "=", "0", "\n", "self", ".", "start", "=", "None", "\n", "self", ".", "log_template", "=", "' '", ".", "join", "(", "\n", "'{:>6.0f},{:>5.0f},{:>9.0f},{:>5.0f}/{:<5.0f} {:>7.0f}%,{:>8.6f},{:12.4f}'", ".", "split", "(", "','", ")", ")", "\n", "self", ".", "dev_log_template", "=", "' '", ".", "join", "(", "\n", "'{:>6.0f},{:>5.0f},{:>9.0f},{:>5.0f}/{:<5.0f} {:>7.4f},{:>8.4f},{:8.4f},{:12.4f},{:12.4f}'", ".", "split", "(", "','", ")", ")", "\n", "\n", "timestamp", "=", "datetime", ".", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "\"%Y-%m-%d_%H-%M-%S\"", ")", "\n", "self", ".", "snapshot_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "model_outfile", ",", "self", ".", "train_loader", ".", "dataset", ".", "NAME", ",", "'%s.pt'", "%", "timestamp", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.trainers.classification_trainer_marjan.ClassificationTrainer.train_epoch": [[30, 79], ["classification_trainer_marjan.ClassificationTrainer.train_loader.init_epoch", "enumerate", "classification_trainer_marjan.ClassificationTrainer.model.train", "classification_trainer_marjan.ClassificationTrainer.optimizer.zero_grad", "torch.cross_entropy.backward", "classification_trainer_marjan.ClassificationTrainer.optimizer.step", "hasattr", "torch.sigmoid().round().long", "torch.sigmoid().round().long", "zip", "torch.binary_cross_entropy_with_logits", "torch.binary_cross_entropy_with_logits", "zip", "torch.cross_entropy", "torch.cross_entropy", "hasattr", "hasattr", "hasattr", "classification_trainer_marjan.ClassificationTrainer.model.update_ema", "print", "classification_trainer_marjan.ClassificationTrainer.model", "classification_trainer_marjan.ClassificationTrainer.model", "classification_trainer_marjan.ClassificationTrainer.model", "classification_trainer_marjan.ClassificationTrainer.model", "numpy.array_equal", "batch.label.float", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "numpy.array_equal", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "classification_trainer_marjan.ClassificationTrainer.log_template.format", "torch.sigmoid().round", "torch.sigmoid().round", "rnn_outs[].pow().mean", "len", "len", "torch.cross_entropy.item", "time.time", "len", "torch.sigmoid", "torch.sigmoid", "rnn_outs[].pow"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.trainers.bert_sent_trainer.BertTrainer.train", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.optimization.BertAdam.step"], ["", "def", "train_epoch", "(", "self", ",", "epoch", ")", ":", "\n", "        ", "self", ".", "train_loader", ".", "init_epoch", "(", ")", "\n", "n_correct", ",", "n_total", "=", "0", ",", "0", "\n", "for", "batch_idx", ",", "batch", "in", "enumerate", "(", "self", ".", "train_loader", ")", ":", "\n", "            ", "self", ".", "iterations", "+=", "1", "\n", "self", ".", "model", ".", "train", "(", ")", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "if", "hasattr", "(", "self", ".", "model", ",", "'tar'", ")", "and", "self", ".", "model", ".", "tar", ":", "\n", "                ", "if", "'ignore_lengths'", "in", "self", ".", "config", "and", "self", ".", "config", "[", "'ignore_lengths'", "]", ":", "\n", "                    ", "scores", ",", "rnn_outs", "=", "self", ".", "model", "(", "batch", ".", "text", ")", "\n", "", "else", ":", "\n", "                    ", "scores", ",", "rnn_outs", "=", "self", ".", "model", "(", "batch", ".", "text", "[", "0", "]", ",", "lengths", "=", "batch", ".", "text", "[", "1", "]", ")", "\n", "", "", "else", ":", "\n", "                ", "if", "'ignore_lengths'", "in", "self", ".", "config", "and", "self", ".", "config", "[", "'ignore_lengths'", "]", ":", "\n", "                    ", "scores", "=", "self", ".", "model", "(", "batch", ".", "text", ")", "\n", "", "else", ":", "\n", "                    ", "scores", "=", "self", ".", "model", "(", "batch", ".", "text", "[", "0", "]", ",", "lengths", "=", "batch", ".", "text", "[", "1", "]", ")", "\n", "\n", "", "", "if", "'is_multilabel'", "in", "self", ".", "config", "and", "self", ".", "config", "[", "'is_multilabel'", "]", ":", "\n", "                ", "predictions", "=", "F", ".", "sigmoid", "(", "scores", ")", ".", "round", "(", ")", ".", "long", "(", ")", "\n", "for", "tensor1", ",", "tensor2", "in", "zip", "(", "predictions", ",", "batch", ".", "label", ")", ":", "\n", "                    ", "if", "np", ".", "array_equal", "(", "tensor1", ",", "tensor2", ")", ":", "\n", "                        ", "n_correct", "+=", "1", "\n", "", "", "loss", "=", "F", ".", "binary_cross_entropy_with_logits", "(", "scores", ",", "batch", ".", "label", ".", "float", "(", ")", ")", "\n", "", "else", ":", "\n", "                ", "for", "tensor1", ",", "tensor2", "in", "zip", "(", "torch", ".", "argmax", "(", "scores", ",", "dim", "=", "1", ")", ",", "torch", ".", "argmax", "(", "batch", ".", "label", ".", "data", ",", "dim", "=", "1", ")", ")", ":", "\n", "                    ", "if", "np", ".", "array_equal", "(", "tensor1", ",", "tensor2", ")", ":", "\n", "                        ", "n_correct", "+=", "1", "\n", "", "", "loss", "=", "F", ".", "cross_entropy", "(", "scores", ",", "torch", ".", "argmax", "(", "batch", ".", "label", ".", "data", ",", "dim", "=", "1", ")", ")", "\n", "\n", "", "if", "hasattr", "(", "self", ".", "model", ",", "'tar'", ")", "and", "self", ".", "model", ".", "tar", ":", "\n", "                ", "loss", "=", "loss", "+", "self", ".", "model", ".", "tar", "*", "(", "rnn_outs", "[", "1", ":", "]", "-", "rnn_outs", "[", ":", "-", "1", "]", ")", ".", "pow", "(", "2", ")", ".", "mean", "(", ")", "\n", "", "if", "hasattr", "(", "self", ".", "model", ",", "'ar'", ")", "and", "self", ".", "model", ".", "ar", ":", "\n", "                ", "loss", "=", "loss", "+", "self", ".", "model", ".", "ar", "*", "(", "rnn_outs", "[", ":", "]", ")", ".", "pow", "(", "2", ")", ".", "mean", "(", ")", "\n", "\n", "", "n_total", "+=", "batch", ".", "batch_size", "\n", "train_acc", "=", "100.", "*", "n_correct", "/", "n_total", "\n", "loss", ".", "backward", "(", ")", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "if", "hasattr", "(", "self", ".", "model", ",", "'beta_ema'", ")", "and", "self", ".", "model", ".", "beta_ema", ">", "0", ":", "\n", "# Temporal averaging", "\n", "                ", "self", ".", "model", ".", "update_ema", "(", ")", "\n", "\n", "", "if", "self", ".", "iterations", "%", "self", ".", "log_interval", "==", "1", ":", "\n", "                ", "niter", "=", "epoch", "*", "len", "(", "self", ".", "train_loader", ")", "+", "batch_idx", "\n", "print", "(", "self", ".", "log_template", ".", "format", "(", "time", ".", "time", "(", ")", "-", "self", ".", "start", ",", "epoch", ",", "self", ".", "iterations", ",", "1", "+", "batch_idx", ",", "\n", "len", "(", "self", ".", "train_loader", ")", ",", "100.0", "*", "(", "1", "+", "batch_idx", ")", "/", "len", "(", "self", ".", "train_loader", ")", ",", "\n", "loss", ".", "item", "(", ")", ",", "train_acc", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.trainers.classification_trainer_marjan.ClassificationTrainer.train": [[80, 113], ["time.time", "os.makedirs", "os.makedirs", "range", "os.path.join", "print", "classification_trainer_marjan.ClassificationTrainer.train_epoch", "print", "print", "classification_trainer_marjan.ClassificationTrainer.dev_evaluator.get_scores", "classification_trainer_marjan.ClassificationTrainer.dev_log_template.format", "torch.save", "torch.save", "torch.save", "torch.save", "print", "time.time"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.trainers.bert_sent_trainer.BertTrainer.train_epoch", "home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.classification_evaluator.ClassificationEvaluator.get_scores"], ["", "", "", "def", "train", "(", "self", ",", "epochs", ")", ":", "\n", "        ", "self", ".", "start", "=", "time", ".", "time", "(", ")", "\n", "header", "=", "'  Time Epoch Iteration Progress    (%Epoch)   Loss     Accuracy'", "\n", "dev_header", "=", "'  Time Epoch Iteration Progress     Dev/Acc. Dev/Pr.  Dev/Recall   Dev/F1       Dev/Loss'", "\n", "os", ".", "makedirs", "(", "self", ".", "model_outfile", ",", "exist_ok", "=", "True", ")", "\n", "os", ".", "makedirs", "(", "os", ".", "path", ".", "join", "(", "self", ".", "model_outfile", ",", "self", ".", "train_loader", ".", "dataset", ".", "NAME", ")", ",", "exist_ok", "=", "True", ")", "\n", "\n", "for", "epoch", "in", "range", "(", "1", ",", "epochs", "+", "1", ")", ":", "\n", "            ", "print", "(", "'\\n'", "+", "header", ")", "\n", "self", ".", "train_epoch", "(", "epoch", ")", "\n", "\n", "# Evaluate performance on validation set", "\n", "dev_acc", ",", "dev_precision", ",", "dev_recall", ",", "dev_f1_micro", ",", "dev_loss", ",", "dev_hamming_loss", ",", "dev_jaccard_score", ",", "dev_f1_macro", ",", "dev_auc_micro", "=", "self", ".", "dev_evaluator", ".", "get_scores", "(", ")", "[", "0", "]", "\n", "\n", "# Print validation results", "\n", "print", "(", "'\\n'", "+", "dev_header", ")", "\n", "print", "(", "self", ".", "dev_log_template", ".", "format", "(", "time", ".", "time", "(", ")", "-", "self", ".", "start", ",", "epoch", ",", "self", ".", "iterations", ",", "epoch", ",", "epochs", ",", "\n", "dev_acc", ",", "dev_precision", ",", "dev_recall", ",", "dev_f1_micro", ",", "dev_loss", ",", "\n", "dev_hamming_loss", ",", "dev_jaccard_score", ",", "dev_f1_macro", ",", "dev_auc_micro", ")", ")", "\n", "\n", "# Update validation results", "\n", "if", "dev_f1_micro", ">", "self", ".", "best_dev_f1", ":", "\n", "                ", "self", ".", "iters_not_improved", "=", "0", "\n", "self", ".", "best_dev_f1", "=", "dev_f1_micro", "\n", "torch", ".", "save", "(", "self", ".", "model", ",", "self", ".", "snapshot_path", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "iters_not_improved", "+=", "1", "\n", "if", "self", ".", "iters_not_improved", ">=", "self", ".", "patience", ":", "\n", "                    ", "self", ".", "early_stop", "=", "True", "\n", "print", "(", "\"Early Stopping. Epoch: {}, Best Dev F1: {}\"", ".", "format", "(", "epoch", ",", "self", ".", "best_dev_f1", ")", ")", "\n", "break", "\n", "", "", "", "", "", ""]], "home.repos.pwc.inspect_result.marjanhs_procon20.trainers.trainer.Trainer.__init__": [[7, 24], ["trainer_config.get", "trainer_config.get", "trainer_config.get", "trainer_config.get", "trainer_config.get", "trainer_config.get", "trainer_config.get", "trainer_config.get"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "model", ",", "embedding", ",", "train_loader", ",", "trainer_config", ",", "train_evaluator", ",", "test_evaluator", ",", "dev_evaluator", "=", "None", ")", ":", "\n", "        ", "self", ".", "model", "=", "model", "\n", "self", ".", "embedding", "=", "embedding", "\n", "self", ".", "optimizer", "=", "trainer_config", ".", "get", "(", "'optimizer'", ")", "\n", "self", ".", "train_loader", "=", "train_loader", "\n", "self", ".", "batch_size", "=", "trainer_config", ".", "get", "(", "'batch_size'", ")", "\n", "self", ".", "log_interval", "=", "trainer_config", ".", "get", "(", "'log_interval'", ")", "\n", "self", ".", "model_outfile", "=", "trainer_config", ".", "get", "(", "'model_outfile'", ")", "\n", "self", ".", "lr_reduce_factor", "=", "trainer_config", ".", "get", "(", "'lr_reduce_factor'", ")", "\n", "self", ".", "patience", "=", "trainer_config", ".", "get", "(", "'patience'", ")", "\n", "self", ".", "clip_norm", "=", "trainer_config", ".", "get", "(", "'clip_norm'", ")", "\n", "\n", "self", ".", "logger", "=", "trainer_config", ".", "get", "(", "'logger'", ")", "\n", "\n", "self", ".", "train_evaluator", "=", "train_evaluator", "\n", "self", ".", "test_evaluator", "=", "test_evaluator", "\n", "self", ".", "dev_evaluator", "=", "dev_evaluator", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.trainers.trainer.Trainer.evaluate": [[25, 32], ["evaluator.get_scores", "trainer.Trainer.logger.info", "trainer.Trainer.logger.info", "trainer.Trainer.logger.info", "list", "map"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.classification_evaluator.ClassificationEvaluator.get_scores"], ["", "def", "evaluate", "(", "self", ",", "evaluator", ",", "dataset_name", ")", ":", "\n", "        ", "scores", ",", "metric_names", "=", "evaluator", ".", "get_scores", "(", ")", "\n", "if", "self", ".", "logger", "is", "not", "None", ":", "\n", "            ", "self", ".", "logger", ".", "info", "(", "'Evaluation metrics for {}:'", ".", "format", "(", "dataset_name", ")", ")", "\n", "self", ".", "logger", ".", "info", "(", "'\\t'", ".", "join", "(", "[", "' '", "]", "+", "metric_names", ")", ")", "\n", "self", ".", "logger", ".", "info", "(", "'\\t'", ".", "join", "(", "[", "dataset_name", "]", "+", "list", "(", "map", "(", "str", ",", "scores", ")", ")", ")", ")", "\n", "", "return", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.trainers.trainer.Trainer.get_sentence_embeddings": [[33, 37], ["trainer.Trainer.embedding().transpose", "trainer.Trainer.embedding().transpose", "trainer.Trainer.embedding", "trainer.Trainer.embedding"], "methods", ["None"], ["", "def", "get_sentence_embeddings", "(", "self", ",", "batch", ")", ":", "\n", "        ", "sent1", "=", "self", ".", "embedding", "(", "batch", ".", "sentence_1", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "sent2", "=", "self", ".", "embedding", "(", "batch", ".", "sentence_2", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "return", "sent1", ",", "sent2", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.trainers.trainer.Trainer.train_epoch": [[38, 40], ["NotImplementedError"], "methods", ["None"], ["", "def", "train_epoch", "(", "self", ",", "epoch", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.trainers.trainer.Trainer.train": [[41, 43], ["NotImplementedError"], "methods", ["None"], ["", "def", "train", "(", "self", ",", "epochs", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.marjanhs_procon20.trainers.classification_trainer.ClassificationTrainer.__init__": [[14, 29], ["common.trainers.trainer.Trainer.__init__", "datetime.datetime.now().strftime", "os.path.join", "datetime.datetime.now"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.classification_evaluator.ClassificationEvaluator.__init__"], ["    ", "def", "__init__", "(", "self", ",", "model", ",", "embedding", ",", "train_loader", ",", "trainer_config", ",", "train_evaluator", ",", "test_evaluator", ",", "dev_evaluator", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "model", ",", "embedding", ",", "train_loader", ",", "trainer_config", ",", "train_evaluator", ",", "test_evaluator", ",", "dev_evaluator", ")", "\n", "self", ".", "config", "=", "trainer_config", "\n", "self", ".", "early_stop", "=", "False", "\n", "self", ".", "best_dev_f1", "=", "0", "\n", "self", ".", "iterations", "=", "0", "\n", "self", ".", "iters_not_improved", "=", "0", "\n", "self", ".", "start", "=", "None", "\n", "self", ".", "log_template", "=", "' '", ".", "join", "(", "\n", "'{:>6.0f},{:>5.0f},{:>9.0f},{:>5.0f}/{:<5.0f} {:>7.0f}%,{:>8.6f},{:12.4f}'", ".", "split", "(", "','", ")", ")", "\n", "self", ".", "dev_log_template", "=", "' '", ".", "join", "(", "\n", "'{:>6.0f},{:>5.0f},{:>9.0f},{:>5.0f}/{:<5.0f} {:>7.4f},{:>8.4f},{:8.4f},{:12.4f},{:12.4f}'", ".", "split", "(", "','", ")", ")", "\n", "\n", "timestamp", "=", "datetime", ".", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "\"%Y-%m-%d_%H-%M-%S\"", ")", "\n", "self", ".", "snapshot_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "model_outfile", ",", "self", ".", "train_loader", ".", "dataset", ".", "NAME", ",", "'%s.pt'", "%", "timestamp", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.trainers.classification_trainer.ClassificationTrainer.train_epoch": [[30, 97], ["classification_trainer.ClassificationTrainer.train_loader.init_epoch", "enumerate", "classification_trainer.ClassificationTrainer.model.train", "classification_trainer.ClassificationTrainer.optimizer.zero_grad", "torch.cross_entropy.backward", "classification_trainer.ClassificationTrainer.optimizer.step", "hasattr", "torch.sigmoid().round().long", "torch.sigmoid().round().long", "torch.sigmoid().round().long", "zip", "torch.binary_cross_entropy_with_logits", "torch.binary_cross_entropy_with_logits", "torch.binary_cross_entropy_with_logits", "zip", "torch.cross_entropy", "torch.cross_entropy", "torch.cross_entropy", "hasattr", "hasattr", "hasattr", "classification_trainer.ClassificationTrainer.model.update_ema", "print", "classification_trainer.ClassificationTrainer.model", "classification_trainer.ClassificationTrainer.model", "classification_trainer.ClassificationTrainer.model", "classification_trainer.ClassificationTrainer.model", "numpy.array_equal", "batch.label.float", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "numpy.array_equal", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "classification_trainer.ClassificationTrainer.log_template.format", "torch.sigmoid().round", "torch.sigmoid().round", "torch.sigmoid().round", "rnn_outs[].pow().mean", "len", "len", "torch.cross_entropy.item", "time.time", "len", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "rnn_outs[].pow"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.trainers.bert_sent_trainer.BertTrainer.train", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.optimization.BertAdam.step"], ["", "def", "train_epoch", "(", "self", ",", "epoch", ")", ":", "\n", "        ", "self", ".", "train_loader", ".", "init_epoch", "(", ")", "\n", "n_correct", ",", "n_total", "=", "0", ",", "0", "\n", "for", "batch_idx", ",", "batch", "in", "enumerate", "(", "self", ".", "train_loader", ")", ":", "\n", "            ", "self", ".", "iterations", "+=", "1", "\n", "self", ".", "model", ".", "train", "(", ")", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "if", "hasattr", "(", "self", ".", "model", ",", "'tar'", ")", "and", "self", ".", "model", ".", "tar", ":", "\n", "                ", "if", "'ignore_lengths'", "in", "self", ".", "config", "and", "self", ".", "config", "[", "'ignore_lengths'", "]", ":", "\n", "                    ", "scores", ",", "rnn_outs", "=", "self", ".", "model", "(", "batch", ".", "text", ")", "\n", "", "else", ":", "\n", "                    ", "scores", ",", "rnn_outs", "=", "self", ".", "model", "(", "batch", ".", "text", "[", "0", "]", ",", "lengths", "=", "batch", ".", "text", "[", "1", "]", ")", "\n", "", "", "else", ":", "\n", "                ", "if", "'ignore_lengths'", "in", "self", ".", "config", "and", "self", ".", "config", "[", "'ignore_lengths'", "]", ":", "\n", "                    ", "scores", "=", "self", ".", "model", "(", "batch", ".", "text", ")", "\n", "", "else", ":", "\n", "                    ", "scores", "=", "self", ".", "model", "(", "batch", ".", "text", "[", "0", "]", ",", "lengths", "=", "batch", ".", "text", "[", "1", "]", ")", "\n", "\n", "", "", "if", "'is_multilabel'", "in", "self", ".", "config", "and", "self", ".", "config", "[", "'is_multilabel'", "]", ":", "\n", "                ", "predictions", "=", "F", ".", "sigmoid", "(", "scores", ")", ".", "round", "(", ")", ".", "long", "(", ")", "\n", "for", "tensor1", ",", "tensor2", "in", "zip", "(", "predictions", ",", "batch", ".", "label", ")", ":", "\n", "                    ", "if", "np", ".", "array_equal", "(", "tensor1", ",", "tensor2", ")", ":", "\n", "                        ", "n_correct", "+=", "1", "\n", "#loss = F.binary_cross_entropy_with_logits(scores, batch.label.float()) changed!!!", "\n", "", "", "''' Weighted LOSS for MULTICLASS'''", "\n", "#weights = torch.Tensor([0.12228265, 0.0913913, 0.44239141, 0.34393464]).cuda()", "\n", "# class with smaller size gets more weights", "\n", "# class size: array([118588,  88630, 429025, 333543])", "\n", "'''                \n                train = pd.read_csv('~/projects/hedwig-data/datasets/Personality/train.tsv', sep='\\t', dtype=str)\n                labels = train.iloc[:,0].apply(lambda x: [int(e) for e in x]).to_list()\n                labels_np = np.array(labels)\n                '''", "\n", "#pos_weights = torch.Tensor([8.17777515, 10.94196096,  2.2604417,  2.90752916]).cuda()", "\n", "loss", "=", "F", ".", "binary_cross_entropy_with_logits", "(", "scores", ",", "batch", ".", "label", ".", "float", "(", ")", ",", "reduction", "=", "'mean'", ")", "\n", "#weights = torch.Tensor([0.12228265, 0.0913913, 0.44239141, 0.34393464]).cuda()", "\n", "'''weights = torch.Tensor([1, 1, 4, 4]).cuda()\n                loss = loss * weights\n                loss = loss.mean()'''", "\n", "''' END OF WEIGHTED LOSS!'''", "\n", "", "else", ":", "\n", "                ", "for", "tensor1", ",", "tensor2", "in", "zip", "(", "torch", ".", "argmax", "(", "scores", ",", "dim", "=", "1", ")", ",", "torch", ".", "argmax", "(", "batch", ".", "label", ".", "data", ",", "dim", "=", "1", ")", ")", ":", "\n", "                    ", "if", "np", ".", "array_equal", "(", "tensor1", ",", "tensor2", ")", ":", "\n", "                        ", "n_correct", "+=", "1", "\n", "", "", "loss", "=", "F", ".", "cross_entropy", "(", "scores", ",", "torch", ".", "argmax", "(", "batch", ".", "label", ".", "data", ",", "dim", "=", "1", ")", ")", "\n", "\n", "", "if", "hasattr", "(", "self", ".", "model", ",", "'tar'", ")", "and", "self", ".", "model", ".", "tar", ":", "\n", "                ", "loss", "=", "loss", "+", "self", ".", "model", ".", "tar", "*", "(", "rnn_outs", "[", "1", ":", "]", "-", "rnn_outs", "[", ":", "-", "1", "]", ")", ".", "pow", "(", "2", ")", ".", "mean", "(", ")", "\n", "", "if", "hasattr", "(", "self", ".", "model", ",", "'ar'", ")", "and", "self", ".", "model", ".", "ar", ":", "\n", "                ", "loss", "=", "loss", "+", "self", ".", "model", ".", "ar", "*", "(", "rnn_outs", "[", ":", "]", ")", ".", "pow", "(", "2", ")", ".", "mean", "(", ")", "\n", "\n", "", "n_total", "+=", "batch", ".", "batch_size", "\n", "train_acc", "=", "100.", "*", "n_correct", "/", "n_total", "\n", "loss", ".", "backward", "(", ")", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "# dist.barrier()  # added", "\n", "\n", "\n", "if", "hasattr", "(", "self", ".", "model", ",", "'beta_ema'", ")", "and", "self", ".", "model", ".", "beta_ema", ">", "0", ":", "\n", "# Temporal averaging", "\n", "                ", "self", ".", "model", ".", "update_ema", "(", ")", "\n", "\n", "", "if", "self", ".", "iterations", "%", "self", ".", "log_interval", "==", "1", ":", "\n", "                ", "niter", "=", "epoch", "*", "len", "(", "self", ".", "train_loader", ")", "+", "batch_idx", "\n", "print", "(", "self", ".", "log_template", ".", "format", "(", "time", ".", "time", "(", ")", "-", "self", ".", "start", ",", "epoch", ",", "self", ".", "iterations", ",", "1", "+", "batch_idx", ",", "\n", "len", "(", "self", ".", "train_loader", ")", ",", "100.0", "*", "(", "1", "+", "batch_idx", ")", "/", "len", "(", "self", ".", "train_loader", ")", ",", "\n", "loss", ".", "item", "(", ")", ",", "train_acc", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.trainers.classification_trainer.ClassificationTrainer.train": [[98, 132], ["time.time", "os.makedirs", "os.makedirs", "range", "os.path.join", "print", "classification_trainer.ClassificationTrainer.train_epoch", "print", "print", "classification_trainer.ClassificationTrainer.dev_evaluator.get_scores", "classification_trainer.ClassificationTrainer.dev_log_template.format", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "print", "time.time"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.trainers.bert_sent_trainer.BertTrainer.train_epoch", "home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.classification_evaluator.ClassificationEvaluator.get_scores"], ["", "", "", "def", "train", "(", "self", ",", "epochs", ")", ":", "\n", "        ", "self", ".", "start", "=", "time", ".", "time", "(", ")", "\n", "header", "=", "'  Time Epoch Iteration Progress    (%Epoch)   Loss     Accuracy'", "\n", "dev_header", "=", "'  Time Epoch Iteration Progress     Dev/Acc. Dev/Pr.  Dev/Recall   Dev/F1       Dev/Loss'", "\n", "os", ".", "makedirs", "(", "self", ".", "model_outfile", ",", "exist_ok", "=", "True", ")", "\n", "os", ".", "makedirs", "(", "os", ".", "path", ".", "join", "(", "self", ".", "model_outfile", ",", "self", ".", "train_loader", ".", "dataset", ".", "NAME", ")", ",", "exist_ok", "=", "True", ")", "\n", "\n", "for", "epoch", "in", "range", "(", "1", ",", "epochs", "+", "1", ")", ":", "\n", "            ", "print", "(", "'\\n'", "+", "header", ")", "\n", "self", ".", "train_epoch", "(", "epoch", ")", "\n", "# dist.destroy_process_group()  ## added", "\n", "# Evaluate performance on validation set", "\n", "dev_acc", ",", "dev_precision", ",", "dev_recall", ",", "dev_f1_micro", ",", "dev_loss", ",", "dev_hamming_loss", ",", "dev_jaccard_score", ",", "dev_f1_macro", ",", "dev_auc_micro", "=", "self", ".", "dev_evaluator", ".", "get_scores", "(", ")", "[", "0", "]", "\n", "\n", "# Print validation results", "\n", "print", "(", "'\\n'", "+", "dev_header", ")", "\n", "print", "(", "self", ".", "dev_log_template", ".", "format", "(", "time", ".", "time", "(", ")", "-", "self", ".", "start", ",", "epoch", ",", "self", ".", "iterations", ",", "epoch", ",", "epochs", ",", "\n", "dev_acc", ",", "dev_precision", ",", "dev_recall", ",", "dev_f1_micro", ",", "dev_loss", ",", "\n", "dev_hamming_loss", ",", "dev_jaccard_score", ",", "dev_f1_macro", ",", "dev_auc_micro", ")", ")", "\n", "\n", "# Update validation results", "\n", "if", "dev_f1_micro", ">", "self", ".", "best_dev_f1", ":", "\n", "                ", "self", ".", "iters_not_improved", "=", "0", "\n", "self", ".", "best_dev_f1", "=", "dev_f1_micro", "\n", "torch", ".", "save", "(", "self", ".", "model", ",", "self", ".", "snapshot_path", ")", "\n", "\n", "", "else", ":", "\n", "                ", "self", ".", "iters_not_improved", "+=", "1", "\n", "if", "self", ".", "iters_not_improved", ">=", "self", ".", "patience", ":", "\n", "                    ", "self", ".", "early_stop", "=", "True", "\n", "print", "(", "\"Early Stopping. Epoch: {}, Best Dev F1: {}\"", ".", "format", "(", "epoch", ",", "self", ".", "best_dev_f1", ")", ")", "\n", "break", "\n", "", "", "", "", "", ""]], "home.repos.pwc.inspect_result.marjanhs_procon20.trainers.bert_emotion_trainer.BertTrainer.__init__": [[19, 43], ["bert_emotion_trainer.BertTrainer.processor.get_train_examples", "utils.tokenization.BertTokenizer.from_pretrained", "utils.emotion.Emotion", "datetime.datetime.now().strftime", "os.path.join", "print", "int", "datetime.datetime.now", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "len"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor.get_train_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertPreTrainedModel.from_pretrained"], ["    ", "def", "__init__", "(", "self", ",", "model", ",", "optimizer", ",", "processor", ",", "args", ")", ":", "\n", "        ", "self", ".", "args", "=", "args", "\n", "self", ".", "model", "=", "model", "\n", "self", ".", "optimizer", "=", "optimizer", "\n", "self", ".", "processor", "=", "processor", "\n", "self", ".", "train_examples", "=", "self", ".", "processor", ".", "get_train_examples", "(", "args", ".", "data_dir", ",", "args", ".", "train_name", ")", "\n", "self", ".", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "args", ".", "model", ",", "is_lowercase", "=", "args", ".", "is_lowercase", ")", "\n", "self", ".", "emotioner", "=", "Emotion", "(", "args", ".", "nrc_path", ",", "args", ".", "max_em_len", ",", "args", ".", "emotion_filters", ")", "\n", "\n", "timestamp", "=", "datetime", ".", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "\"%Y-%m-%d_%H-%M-%S\"", ")", "\n", "self", ".", "snapshot_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "args", ".", "save_path", ",", "self", ".", "processor", ".", "NAME", ",", "'%s.pt'", "%", "timestamp", ")", "\n", "print", "(", "'initial snapshot path'", ",", "self", ".", "snapshot_path", ")", "\n", "\n", "self", ".", "num_train_optimization_steps", "=", "int", "(", "\n", "len", "(", "self", ".", "train_examples", ")", "/", "args", ".", "batch_size", "/", "args", ".", "gradient_accumulation_steps", ")", "*", "args", ".", "epochs", "\n", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "            ", "self", ".", "num_train_optimization_steps", "=", "args", ".", "num_train_optimization_steps", "//", "torch", ".", "distributed", ".", "get_world_size", "(", ")", "\n", "\n", "", "self", ".", "log_header", "=", "'Epoch Iteration Progress   Dev/Acc.  Dev/Pr.  Dev/Re.   Dev/F1   Dev/Loss   Dev/F1ma, Dev/HLoss, Dev/Jacc, Train/Loss'", "\n", "self", ".", "log_template", "=", "' '", ".", "join", "(", "'{:>5.0f},{:>9.0f},{:>6.0f}/{:<5.0f} {:>6.4f},{:>8.4f},{:8.4f},{:8.4f},{:10.4f},{:10.4f},{:10.4f},{:10.4f},{:10.4f}'", ".", "split", "(", "','", ")", ")", "\n", "\n", "self", ".", "iterations", ",", "self", ".", "nb_tr_steps", ",", "self", ".", "tr_loss", "=", "0", ",", "0", ",", "0", "\n", "self", ".", "best_dev_measure", ",", "self", ".", "unimproved_iters", "=", "0", ",", "0", "\n", "self", ".", "early_stop", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.trainers.bert_emotion_trainer.BertTrainer.train_epoch": [[44, 93], ["enumerate", "tqdm.tqdm.tqdm", "bert_emotion_trainer.BertTrainer.model.train", "tuple", "bert_emotion_trainer.BertTrainer.model", "isinstance", "loss.mean.mean.item", "loss.mean.mean.item", "torch.cosine_similarity", "torch.cosine_similarity", "range", "torch.binary_cross_entropy_with_logits", "torch.binary_cross_entropy_with_logits", "torch.cross_entropy", "torch.cross_entropy", "loss.mean.mean.mean", "bert_emotion_trainer.BertTrainer.optimizer.backward", "loss.mean.mean.backward", "bert_emotion_trainer.BertTrainer.optimizer.step", "bert_emotion_trainer.BertTrainer.optimizer.zero_grad", "t.to", "len", "torch.eq().all", "torch.eq().all", "torch.eq().all", "torch.eq().all", "label_ids.float", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.eq().all", "torch.eq().all", "torch.eq().all", "torch.eq().all", "utils.optimization.warmup_linear", "torch.eq", "torch.eq", "torch.eq", "torch.eq", "max", "print", "exit", "torch.Tensor().long().cuda", "torch.Tensor().long().cuda", "torch.Tensor().long().cuda", "torch.Tensor().long().cuda", "torch.eq", "torch.eq", "torch.eq", "torch.eq", "torch.Tensor().long().cuda", "torch.Tensor().long().cuda", "torch.Tensor().long().cuda", "torch.Tensor().long().cuda", "torch.Tensor().long", "torch.Tensor().long", "torch.Tensor().long", "torch.Tensor().long", "torch.Tensor().long", "torch.Tensor().long", "torch.Tensor().long", "torch.Tensor().long", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.trainers.bert_sent_trainer.BertTrainer.train", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.optimization.BertAdam.step", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.optimization.warmup_linear"], ["", "def", "train_epoch", "(", "self", ",", "train_dataloader", ")", ":", "\n", "        ", "loss_epoch", "=", "0", "\n", "for", "step", ",", "batch", "in", "enumerate", "(", "tqdm", "(", "train_dataloader", ",", "desc", "=", "\"Training\"", ")", ")", ":", "\n", "            ", "self", ".", "model", ".", "train", "(", ")", "\n", "batch", "=", "tuple", "(", "t", ".", "to", "(", "self", ".", "args", ".", "device", ")", "for", "t", "in", "batch", ")", "\n", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "emotion_ids", ",", "label_ids", "=", "batch", "\n", "\n", "logits", "=", "self", ".", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "emotion_ids", "=", "emotion_ids", ")", "\n", "loss_extra", "=", "0", "\n", "if", "isinstance", "(", "logits", ",", "tuple", ")", ":", "\n", "                ", "logits", ",", "(", "first_SEP", ",", "second_SEP", ")", "=", "logits", "\n", "cos_simi", "=", "F", ".", "cosine_similarity", "(", "first_SEP", ",", "second_SEP", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "label_ids", ")", ")", ":", "\n", "                    ", "if", "torch", ".", "eq", "(", "label_ids", "[", "i", "]", ",", "torch", ".", "Tensor", "(", "[", "0", ",", "1", "]", ")", ".", "long", "(", ")", ".", "cuda", "(", ")", ")", ".", "all", "(", ")", ":", "\n", "                        ", "loss_extra", "+=", "1", "-", "cos_simi", "[", "i", "]", "\n", "", "elif", "torch", ".", "eq", "(", "label_ids", "[", "i", "]", ",", "torch", ".", "Tensor", "(", "[", "1", ",", "0", "]", ")", ".", "long", "(", ")", ".", "cuda", "(", ")", ")", ".", "all", "(", ")", ":", "\n", "                        ", "loss_extra", "+=", "max", "(", "0", ",", "cos_simi", "[", "i", "]", ")", "\n", "", "else", ":", "\n", "                        ", "print", "(", "'Invalid label value ERROR'", ",", "label_ids", "[", "i", "]", ")", "\n", "exit", "(", "1", ")", "\n", "\n", "", "", "", "if", "self", ".", "args", ".", "is_multilabel", ":", "\n", "                ", "loss", "=", "F", ".", "binary_cross_entropy_with_logits", "(", "logits", ",", "label_ids", ".", "float", "(", ")", ")", "\n", "", "else", ":", "\n", "                ", "loss", "=", "F", ".", "cross_entropy", "(", "logits", ",", "torch", ".", "argmax", "(", "label_ids", ",", "dim", "=", "1", ")", ")", "\n", "loss", "+=", "loss_extra", "\n", "\n", "", "if", "self", ".", "args", ".", "n_gpu", ">", "1", ":", "\n", "                ", "loss", "=", "loss", ".", "mean", "(", ")", "\n", "", "if", "self", ".", "args", ".", "gradient_accumulation_steps", ">", "1", ":", "\n", "                ", "loss", "=", "loss", "/", "self", ".", "args", ".", "gradient_accumulation_steps", "\n", "\n", "", "if", "self", ".", "args", ".", "fp16", ":", "\n", "                ", "self", ".", "optimizer", ".", "backward", "(", "loss", ")", "\n", "", "else", ":", "\n", "                ", "loss", ".", "backward", "(", ")", "\n", "\n", "", "self", ".", "tr_loss", "+=", "loss", ".", "item", "(", ")", "\n", "loss_epoch", "+=", "loss", ".", "item", "(", ")", "\n", "self", ".", "nb_tr_steps", "+=", "1", "\n", "if", "(", "step", "+", "1", ")", "%", "self", ".", "args", ".", "gradient_accumulation_steps", "==", "0", ":", "\n", "                ", "if", "self", ".", "args", ".", "fp16", ":", "\n", "                    ", "lr_this_step", "=", "self", ".", "args", ".", "learning_rate", "*", "warmup_linear", "(", "self", ".", "iterations", "/", "self", ".", "num_train_optimization_steps", ",", "self", ".", "args", ".", "warmup_proportion", ")", "\n", "for", "param_group", "in", "self", ".", "optimizer", ".", "param_groups", ":", "\n", "                        ", "param_group", "[", "'lr'", "]", "=", "lr_this_step", "\n", "", "", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "self", ".", "iterations", "+=", "1", "\n", "", "", "return", "loss_epoch", "/", "(", "step", "+", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.trainers.bert_emotion_trainer.BertTrainer.train": [[94, 160], ["print", "print", "print", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "torch.utils.data.TensorDataset", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "tqdm.tqdm.trange", "datasets.bert_processors.abstract_processor.convert_examples_to_hierarchical_features", "datasets.bert_processors.abstract_processor.convert_examples_to_features_with_emotion", "utils.preprocessing.pad_input_matrix", "utils.preprocessing.pad_input_matrix", "utils.preprocessing.pad_input_matrix", "len", "torch.utils.data.RandomSampler", "torch.utils.data.RandomSampler", "torch.utils.data.distributed.DistributedSampler", "torch.utils.data.distributed.DistributedSampler", "int", "bert_emotion_trainer.BertTrainer.train_epoch", "common.evaluators.bert_emotion_evaluator.BertEvaluator", "tqdm.tqdm.tqdm.write", "tqdm.tqdm.tqdm.write", "common.evaluators.bert_emotion_evaluator.BertEvaluator.get_scores", "bert_emotion_trainer.BertTrainer.log_template.format", "torch.save", "torch.save", "torch.save", "torch.save", "print"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.convert_examples_to_hierarchical_features", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.convert_examples_to_features_with_emotion", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.preprocessing.pad_input_matrix", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.preprocessing.pad_input_matrix", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.preprocessing.pad_input_matrix", "home.repos.pwc.inspect_result.marjanhs_procon20.trainers.bert_sent_trainer.BertTrainer.train_epoch", "home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.classification_evaluator.ClassificationEvaluator.get_scores"], ["", "def", "train", "(", "self", ")", ":", "\n", "\n", "        ", "if", "self", ".", "args", ".", "is_hierarchical", ":", "\n", "            ", "train_features", "=", "convert_examples_to_hierarchical_features", "(", "\n", "self", ".", "train_examples", ",", "self", ".", "args", ".", "max_seq_length", ",", "self", ".", "tokenizer", ")", "\n", "", "else", ":", "\n", "            ", "train_features", "=", "convert_examples_to_features_with_emotion", "(", "\n", "self", ".", "train_examples", ",", "self", ".", "args", ".", "max_seq_length", ",", "self", ".", "tokenizer", ",", "self", ".", "emotioner", ")", "\n", "\n", "", "unpadded_input_ids", "=", "[", "f", ".", "input_ids", "for", "f", "in", "train_features", "]", "\n", "unpadded_input_mask", "=", "[", "f", ".", "input_mask", "for", "f", "in", "train_features", "]", "\n", "unpadded_segment_ids", "=", "[", "f", ".", "segment_ids", "for", "f", "in", "train_features", "]", "\n", "unpadded_emotion_scores", "=", "[", "f", ".", "sentiment_scores", "for", "f", "in", "train_features", "]", "# sentiment_scores are emotion_ids!", "\n", "\n", "if", "self", ".", "args", ".", "is_hierarchical", ":", "\n", "            ", "pad_input_matrix", "(", "unpadded_input_ids", ",", "self", ".", "args", ".", "max_doc_length", ")", "\n", "pad_input_matrix", "(", "unpadded_input_mask", ",", "self", ".", "args", ".", "max_doc_length", ")", "\n", "pad_input_matrix", "(", "unpadded_segment_ids", ",", "self", ".", "args", ".", "max_doc_length", ")", "\n", "\n", "", "print", "(", "\"Number of examples: \"", ",", "len", "(", "self", ".", "train_examples", ")", ")", "\n", "print", "(", "\"Batch size:\"", ",", "self", ".", "args", ".", "batch_size", ")", "\n", "print", "(", "\"Num of steps:\"", ",", "self", ".", "num_train_optimization_steps", ")", "\n", "\n", "padded_input_ids", "=", "torch", ".", "tensor", "(", "unpadded_input_ids", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "padded_input_mask", "=", "torch", ".", "tensor", "(", "unpadded_input_mask", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "padded_segment_ids", "=", "torch", ".", "tensor", "(", "unpadded_segment_ids", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "label_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "label_id", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "padded_emotion_ids", "=", "torch", ".", "tensor", "(", "unpadded_emotion_scores", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "train_data", "=", "TensorDataset", "(", "padded_input_ids", ",", "padded_input_mask", ",", "padded_segment_ids", ",", "padded_emotion_ids", ",", "label_ids", ")", "\n", "\n", "if", "self", ".", "args", ".", "local_rank", "==", "-", "1", ":", "\n", "            ", "train_sampler", "=", "RandomSampler", "(", "train_data", ")", "\n", "", "else", ":", "\n", "            ", "train_sampler", "=", "DistributedSampler", "(", "train_data", ")", "\n", "\n", "", "train_dataloader", "=", "DataLoader", "(", "train_data", ",", "sampler", "=", "train_sampler", ",", "batch_size", "=", "self", ".", "args", ".", "batch_size", ")", "\n", "for", "epoch", "in", "trange", "(", "int", "(", "self", ".", "args", ".", "epochs", ")", ",", "desc", "=", "\"Epoch\"", ")", ":", "\n", "            ", "loss_epoch", "=", "self", ".", "train_epoch", "(", "train_dataloader", ")", "\n", "dev_evaluator", "=", "BertEvaluator", "(", "self", ".", "model", ",", "self", ".", "processor", ",", "self", ".", "args", ",", "split", "=", "'dev'", ")", "\n", "dev_acc", ",", "dev_precision", ",", "dev_recall", ",", "dev_f1", ",", "dev_loss", ",", "dev_f1_macro", ",", "dev_hamming_loss", ",", "dev_jaccard_score", ",", "dev_predicted_labels", ",", "dev_target_labels", "=", "dev_evaluator", ".", "get_scores", "(", ")", "[", "0", "]", "\n", "\n", "# Print validation results", "\n", "tqdm", ".", "write", "(", "self", ".", "log_header", ")", "\n", "tqdm", ".", "write", "(", "self", ".", "log_template", ".", "format", "(", "epoch", "+", "1", ",", "self", ".", "iterations", ",", "epoch", "+", "1", ",", "self", ".", "args", ".", "epochs", ",", "\n", "dev_acc", ",", "dev_precision", ",", "dev_recall", ",", "dev_f1", ",", "dev_loss", ",", "dev_f1_macro", ",", "dev_hamming_loss", ",", "dev_jaccard_score", ",", "loss_epoch", ")", ")", "\n", "\n", "if", "self", ".", "args", ".", "early_on_f1", ":", "\n", "                ", "dev_measure", "=", "dev_f1", "\n", "measure_name", "=", "'F1'", "\n", "", "else", ":", "\n", "                ", "dev_measure", "=", "dev_acc", "\n", "measure_name", "=", "'Balanced Acc'", "\n", "\n", "# Update validation results", "\n", "", "if", "dev_measure", ">", "self", ".", "best_dev_measure", ":", "\n", "                ", "self", ".", "unimproved_iters", "=", "0", "\n", "self", ".", "best_dev_measure", "=", "dev_measure", "\n", "torch", ".", "save", "(", "self", ".", "model", ",", "self", ".", "snapshot_path", ")", "\n", "\n", "", "else", ":", "\n", "                ", "self", ".", "unimproved_iters", "+=", "1", "\n", "if", "self", ".", "unimproved_iters", ">=", "self", ".", "args", ".", "patience", ":", "\n", "                    ", "self", ".", "early_stop", "=", "True", "\n", "print", "(", "\"Early Stopping. Epoch: {}, Best {}: {}\"", ".", "format", "(", "epoch", ",", "measure_name", ",", "self", ".", "best_dev_measure", ")", ")", "\n", "break", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.trainers.relevance_transfer_trainer.RelevanceTransferTrainer.__init__": [[17, 41], ["common.trainers.trainer.Trainer.__init__", "datetime.datetime.now().strftime", "os.path.join", "utils.tokenization.BertTokenizer.from_pretrained", "relevance_transfer_trainer.RelevanceTransferTrainer.processor.get_train_examples", "int", "datetime.datetime.now", "len"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.classification_evaluator.ClassificationEvaluator.__init__", "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertPreTrainedModel.from_pretrained", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor.get_train_examples"], ["    ", "def", "__init__", "(", "self", ",", "model", ",", "config", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "model", ",", "kwargs", "[", "'embedding'", "]", ",", "kwargs", "[", "'train_loader'", "]", ",", "config", ",", "None", ",", "kwargs", "[", "'test_evaluator'", "]", ",", "kwargs", "[", "'dev_evaluator'", "]", ")", "\n", "\n", "if", "config", "[", "'model'", "]", "in", "{", "'BERT-Base'", ",", "'BERT-Large'", ",", "'HBERT-Base'", ",", "'HBERT-Large'", "}", ":", "\n", "            ", "variant", "=", "'bert-large-uncased'", "if", "config", "[", "'model'", "]", "==", "'BERT-Large'", "else", "'bert-base-uncased'", "\n", "self", ".", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "variant", ",", "is_lowercase", "=", "config", "[", "'is_lowercase'", "]", ")", "\n", "self", ".", "processor", "=", "kwargs", "[", "'processor'", "]", "\n", "self", ".", "optimizer", "=", "config", "[", "'optimizer'", "]", "\n", "self", ".", "train_examples", "=", "self", ".", "processor", ".", "get_train_examples", "(", "config", "[", "'data_dir'", "]", ",", "topic", "=", "config", "[", "'topic'", "]", ")", "\n", "self", ".", "num_train_optimization_steps", "=", "int", "(", "len", "(", "self", ".", "train_examples", ")", "/", "\n", "config", "[", "'batch_size'", "]", "/", "\n", "config", "[", "'gradient_accumulation_steps'", "]", "\n", ")", "*", "config", "[", "'epochs'", "]", "\n", "", "self", ".", "config", "=", "config", "\n", "self", ".", "early_stop", "=", "False", "\n", "self", ".", "best_dev_ap", "=", "0", "\n", "self", ".", "iterations", "=", "0", "\n", "self", ".", "unimproved_iters", "=", "0", "\n", "\n", "self", ".", "log_header", "=", "'Epoch Iteration Progress   Dev/Acc.  Dev/Pr.  Dev/AP.   Dev/F1   Dev/Loss'", "\n", "self", ".", "log_template", "=", "' '", ".", "join", "(", "'{:>5.0f},{:>9.0f},{:>6.0f}/{:<5.0f} {:>6.4f},{:>8.4f},{:8.4f},{:8.4f},{:10.4f}'", ".", "split", "(", "','", ")", ")", "\n", "\n", "timestamp", "=", "datetime", ".", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "\"%Y-%m-%d_%H-%M-%S\"", ")", "\n", "self", ".", "snapshot_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "model_outfile", ",", "config", "[", "'dataset'", "]", ".", "NAME", ",", "'%s.pt'", "%", "timestamp", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.trainers.relevance_transfer_trainer.RelevanceTransferTrainer.train_epoch": [[42, 112], ["enumerate", "tqdm.tqdm.tqdm", "relevance_transfer_trainer.RelevanceTransferTrainer.model.train", "tuple", "torch.sigmoid().squeeze", "torch.sigmoid().squeeze", "torch.sigmoid().squeeze", "torch.sigmoid().squeeze", "torch.binary_cross_entropy", "torch.binary_cross_entropy", "loss.mean.mean.backward", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "relevance_transfer_trainer.RelevanceTransferTrainer.train_loader.init_epoch", "torch.binary_cross_entropy", "torch.binary_cross_entropy", "loss.mean.mean.backward", "relevance_transfer_trainer.RelevanceTransferTrainer.optimizer.step", "relevance_transfer_trainer.RelevanceTransferTrainer.optimizer.zero_grad", "label_ids.float", "loss.mean.mean.mean", "relevance_transfer_trainer.RelevanceTransferTrainer.optimizer.step", "relevance_transfer_trainer.RelevanceTransferTrainer.optimizer.zero_grad", "relevance_transfer_trainer.RelevanceTransferTrainer.model.parameters", "hasattr", "batch_label.float", "hasattr", "hasattr", "relevance_transfer_trainer.RelevanceTransferTrainer.model.update_ema", "t.to", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "tasks.relevance_transfer.resample.ImbalancedDatasetSampler().get_indices", "tasks.relevance_transfer.resample.ImbalancedDatasetSampler().get_indices", "torch.sigmoid().squeeze", "torch.sigmoid().squeeze", "torch.sigmoid().squeeze", "torch.sigmoid().squeeze", "torch.sigmoid().squeeze", "torch.sigmoid().squeeze", "torch.sigmoid().squeeze", "torch.sigmoid().squeeze", "torch.sigmoid().squeeze", "torch.sigmoid().squeeze", "torch.sigmoid().squeeze", "torch.sigmoid().squeeze", "torch.sigmoid().squeeze", "torch.sigmoid().squeeze", "torch.sigmoid().squeeze", "torch.sigmoid().squeeze", "relevance_transfer_trainer.RelevanceTransferTrainer.model", "tasks.relevance_transfer.resample.ImbalancedDatasetSampler", "tasks.relevance_transfer.resample.ImbalancedDatasetSampler", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "relevance_transfer_trainer.RelevanceTransferTrainer.model", "relevance_transfer_trainer.RelevanceTransferTrainer.model", "relevance_transfer_trainer.RelevanceTransferTrainer.model", "relevance_transfer_trainer.RelevanceTransferTrainer.model"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.trainers.bert_sent_trainer.BertTrainer.train", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.optimization.BertAdam.step", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.optimization.BertAdam.step"], ["", "def", "train_epoch", "(", "self", ")", ":", "\n", "        ", "for", "step", ",", "batch", "in", "enumerate", "(", "tqdm", "(", "self", ".", "train_loader", ",", "desc", "=", "\"Training\"", ")", ")", ":", "\n", "            ", "self", ".", "model", ".", "train", "(", ")", "\n", "\n", "if", "self", ".", "config", "[", "'model'", "]", "in", "{", "'BERT-Base'", ",", "'BERT-Large'", ",", "'HBERT-Base'", ",", "'HBERT-Large'", "}", ":", "\n", "                ", "batch", "=", "tuple", "(", "t", ".", "to", "(", "self", ".", "config", "[", "'device'", "]", ")", "for", "t", "in", "batch", ")", "\n", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "label_ids", "=", "batch", "\n", "logits", "=", "torch", ".", "sigmoid", "(", "self", ".", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ")", ")", ".", "squeeze", "(", "dim", "=", "1", ")", "\n", "loss", "=", "F", ".", "binary_cross_entropy", "(", "logits", ",", "label_ids", ".", "float", "(", ")", ")", "\n", "\n", "if", "self", ".", "config", "[", "'n_gpu'", "]", ">", "1", ":", "\n", "                    ", "loss", "=", "loss", ".", "mean", "(", ")", "\n", "", "if", "self", ".", "config", "[", "'gradient_accumulation_steps'", "]", ">", "1", ":", "\n", "                    ", "loss", "=", "loss", "/", "self", ".", "config", "[", "'gradient_accumulation_steps'", "]", "\n", "\n", "", "loss", ".", "backward", "(", ")", "\n", "\n", "if", "(", "step", "+", "1", ")", "%", "self", ".", "config", "[", "'gradient_accumulation_steps'", "]", "==", "0", ":", "\n", "                    ", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "self", ".", "iterations", "+=", "1", "\n", "\n", "", "", "else", ":", "\n", "# Clip gradients to address exploding gradients in LSTM", "\n", "                ", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "model", ".", "parameters", "(", ")", ",", "25.0", ")", "\n", "\n", "# Randomly sample equal number of positive and negative documents", "\n", "self", ".", "train_loader", ".", "init_epoch", "(", ")", "\n", "if", "'ignore_lengths'", "in", "self", ".", "config", "and", "self", ".", "config", "[", "'ignore_lengths'", "]", ":", "\n", "                    ", "if", "'resample'", "in", "self", ".", "config", "and", "self", ".", "config", "[", "'resample'", "]", ":", "\n", "                        ", "indices", "=", "ImbalancedDatasetSampler", "(", "batch", ".", "text", ",", "batch", ".", "label", ")", ".", "get_indices", "(", ")", "\n", "batch_text", "=", "batch", ".", "text", "[", "indices", "]", "\n", "batch_label", "=", "batch", ".", "label", "[", "indices", "]", "\n", "", "else", ":", "\n", "                        ", "batch_text", "=", "batch", ".", "text", "\n", "batch_label", "=", "batch", ".", "label", "\n", "", "", "else", ":", "\n", "                    ", "if", "'resample'", "in", "self", ".", "config", "and", "self", ".", "config", "[", "'resample'", "]", ":", "\n", "                        ", "indices", "=", "ImbalancedDatasetSampler", "(", "batch", ".", "text", "[", "0", "]", ",", "batch", ".", "label", ")", ".", "get_indices", "(", ")", "\n", "batch_text", "=", "batch", ".", "text", "[", "0", "]", "[", "indices", "]", "\n", "batch_lengths", "=", "batch", ".", "text", "[", "1", "]", "[", "indices", "]", "\n", "batch_label", "=", "batch", ".", "label", "\n", "", "else", ":", "\n", "                        ", "batch_text", "=", "batch", ".", "text", "[", "0", "]", "\n", "batch_lengths", "=", "batch", ".", "text", "[", "1", "]", "\n", "batch_label", "=", "batch", ".", "label", "\n", "\n", "", "", "if", "hasattr", "(", "self", ".", "model", ",", "'tar'", ")", "and", "self", ".", "model", ".", "tar", ":", "\n", "                    ", "if", "'ignore_lengths'", "in", "self", ".", "config", "and", "self", ".", "config", "[", "'ignore_lengths'", "]", ":", "\n", "                        ", "logits", ",", "rnn_outs", "=", "torch", ".", "sigmoid", "(", "self", ".", "model", "(", "batch_text", ")", ")", ".", "squeeze", "(", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "                        ", "logits", ",", "rnn_outs", "=", "torch", ".", "sigmoid", "(", "self", ".", "model", "(", "batch_text", ",", "lengths", "=", "batch_lengths", ")", ")", ".", "squeeze", "(", "dim", "=", "1", ")", "\n", "", "", "else", ":", "\n", "                    ", "if", "'ignore_lengths'", "in", "self", ".", "config", "and", "self", ".", "config", "[", "'ignore_lengths'", "]", ":", "\n", "                        ", "logits", "=", "torch", ".", "sigmoid", "(", "self", ".", "model", "(", "batch_text", ")", ")", ".", "squeeze", "(", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "                        ", "logits", "=", "torch", ".", "sigmoid", "(", "self", ".", "model", "(", "batch_text", ",", "lengths", "=", "batch_lengths", ")", ")", ".", "squeeze", "(", "dim", "=", "1", ")", "\n", "\n", "", "", "loss", "=", "F", ".", "binary_cross_entropy", "(", "logits", ",", "batch_label", ".", "float", "(", ")", ")", "\n", "if", "hasattr", "(", "self", ".", "model", ",", "'tar'", ")", "and", "self", ".", "model", ".", "tar", ":", "\n", "                    ", "loss", "=", "loss", "+", "(", "rnn_outs", "[", "1", ":", "]", "-", "rnn_outs", "[", ":", "-", "1", "]", ")", ".", "pow", "(", "2", ")", ".", "mean", "(", ")", "\n", "\n", "", "loss", ".", "backward", "(", ")", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "self", ".", "iterations", "+=", "1", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "if", "hasattr", "(", "self", ".", "model", ",", "'beta_ema'", ")", "and", "self", ".", "model", ".", "beta_ema", ">", "0", ":", "\n", "# Temporal averaging", "\n", "                    ", "self", ".", "model", ".", "update_ema", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.trainers.relevance_transfer_trainer.RelevanceTransferTrainer.train": [[113, 165], ["os.makedirs", "os.makedirs", "os.path.join", "datasets.bert_processors.robust45_processor.convert_examples_to_features", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "torch.utils.data.TensorDataset", "torch.utils.data.RandomSampler", "torch.utils.data.RandomSampler", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "tqdm.tqdm.trange", "utils.preprocessing.pad_input_matrix", "utils.preprocessing.pad_input_matrix", "utils.preprocessing.pad_input_matrix", "relevance_transfer_trainer.RelevanceTransferTrainer.train_epoch", "tqdm.tqdm.tqdm.write", "tqdm.tqdm.tqdm.write", "relevance_transfer_trainer.RelevanceTransferTrainer.dev_evaluator.get_scores", "relevance_transfer_trainer.RelevanceTransferTrainer.log_template.format", "torch.save", "torch.save", "torch.save", "torch.save", "tqdm.tqdm.tqdm.write", "t_epochs.close"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.convert_examples_to_features", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.preprocessing.pad_input_matrix", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.preprocessing.pad_input_matrix", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.preprocessing.pad_input_matrix", "home.repos.pwc.inspect_result.marjanhs_procon20.trainers.bert_sent_trainer.BertTrainer.train_epoch", "home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.classification_evaluator.ClassificationEvaluator.get_scores"], ["", "", "", "", "def", "train", "(", "self", ",", "epochs", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "self", ".", "model_outfile", ",", "exist_ok", "=", "True", ")", "\n", "os", ".", "makedirs", "(", "os", ".", "path", ".", "join", "(", "self", ".", "model_outfile", ",", "self", ".", "config", "[", "'dataset'", "]", ".", "NAME", ")", ",", "exist_ok", "=", "True", ")", "\n", "\n", "if", "self", ".", "config", "[", "'model'", "]", "in", "{", "'BERT-Base'", ",", "'BERT-Large'", ",", "'HBERT-Base'", ",", "'HBERT-Large'", "}", ":", "\n", "            ", "train_features", "=", "convert_examples_to_features", "(", "\n", "self", ".", "train_examples", ",", "\n", "self", ".", "config", "[", "'max_seq_length'", "]", ",", "\n", "self", ".", "tokenizer", ",", "\n", "self", ".", "config", "[", "'is_hierarchical'", "]", "\n", ")", "\n", "\n", "unpadded_input_ids", "=", "[", "f", ".", "input_ids", "for", "f", "in", "train_features", "]", "\n", "unpadded_input_mask", "=", "[", "f", ".", "input_mask", "for", "f", "in", "train_features", "]", "\n", "unpadded_segment_ids", "=", "[", "f", ".", "segment_ids", "for", "f", "in", "train_features", "]", "\n", "\n", "if", "self", ".", "config", "[", "'is_hierarchical'", "]", ":", "\n", "                ", "pad_input_matrix", "(", "unpadded_input_ids", ",", "self", ".", "config", "[", "'max_doc_length'", "]", ")", "\n", "pad_input_matrix", "(", "unpadded_input_mask", ",", "self", ".", "config", "[", "'max_doc_length'", "]", ")", "\n", "pad_input_matrix", "(", "unpadded_segment_ids", ",", "self", ".", "config", "[", "'max_doc_length'", "]", ")", "\n", "\n", "", "padded_input_ids", "=", "torch", ".", "tensor", "(", "unpadded_input_ids", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "padded_input_mask", "=", "torch", ".", "tensor", "(", "unpadded_input_mask", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "padded_segment_ids", "=", "torch", ".", "tensor", "(", "unpadded_segment_ids", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "label_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "label_id", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "train_data", "=", "TensorDataset", "(", "padded_input_ids", ",", "padded_input_mask", ",", "padded_segment_ids", ",", "label_ids", ")", "\n", "train_sampler", "=", "RandomSampler", "(", "train_data", ")", "\n", "self", ".", "train_loader", "=", "DataLoader", "(", "train_data", ",", "sampler", "=", "train_sampler", ",", "batch_size", "=", "self", ".", "config", "[", "'batch_size'", "]", ")", "\n", "\n", "", "with", "trange", "(", "1", ",", "epochs", "+", "1", ",", "desc", "=", "\"Epoch\"", ")", "as", "t_epochs", ":", "\n", "            ", "for", "epoch", "in", "t_epochs", ":", "\n", "                ", "self", ".", "train_epoch", "(", ")", "\n", "\n", "# Evaluate performance on validation set", "\n", "dev_acc", ",", "dev_precision", ",", "dev_ap", ",", "dev_f1", ",", "dev_loss", "=", "self", ".", "dev_evaluator", ".", "get_scores", "(", ")", "[", "0", "]", "\n", "tqdm", ".", "write", "(", "self", ".", "log_header", ")", "\n", "tqdm", ".", "write", "(", "self", ".", "log_template", ".", "format", "(", "epoch", ",", "self", ".", "iterations", ",", "epoch", ",", "epochs", ",", "\n", "dev_acc", ",", "dev_precision", ",", "dev_ap", ",", "dev_f1", ",", "dev_loss", ")", ")", "\n", "\n", "# Update validation results", "\n", "if", "dev_f1", ">", "self", ".", "best_dev_ap", ":", "\n", "                    ", "self", ".", "unimproved_iters", "=", "0", "\n", "self", ".", "best_dev_ap", "=", "dev_f1", "\n", "torch", ".", "save", "(", "self", ".", "model", ",", "self", ".", "snapshot_path", ")", "\n", "", "else", ":", "\n", "                    ", "self", ".", "unimproved_iters", "+=", "1", "\n", "if", "self", ".", "unimproved_iters", ">=", "self", ".", "patience", ":", "\n", "                        ", "self", ".", "early_stop", "=", "True", "\n", "tqdm", ".", "write", "(", "\"Early Stopping. Epoch: {}, Best Dev F1: {}\"", ".", "format", "(", "epoch", ",", "self", ".", "best_dev_ap", ")", ")", "\n", "t_epochs", ".", "close", "(", ")", "\n", "break", "\n", "", "", "", "", "", "", ""]], "home.repos.pwc.inspect_result.marjanhs_procon20.trainers.bert_sent_trainer.BertTrainer.__init__": [[57, 79], ["bert_sent_trainer.BertTrainer.processor.get_train_examples", "utils.tokenization.BertTokenizer.from_pretrained", "datetime.datetime.now().strftime", "os.path.join", "int", "datetime.datetime.now", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "len"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor.get_train_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertPreTrainedModel.from_pretrained"], ["    ", "def", "__init__", "(", "self", ",", "model", ",", "optimizer", ",", "processor", ",", "args", ")", ":", "\n", "        ", "self", ".", "args", "=", "args", "\n", "self", ".", "model", "=", "model", "\n", "self", ".", "optimizer", "=", "optimizer", "\n", "self", ".", "processor", "=", "processor", "\n", "self", ".", "train_examples", "=", "self", ".", "processor", ".", "get_train_examples", "(", "args", ".", "data_dir", ",", "args", ".", "train_name", ")", "\n", "self", ".", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "args", ".", "model", ",", "is_lowercase", "=", "args", ".", "is_lowercase", ")", "\n", "\n", "timestamp", "=", "datetime", ".", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "\"%Y-%m-%d_%H-%M-%S\"", ")", "\n", "self", ".", "snapshot_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "args", ".", "save_path", ",", "self", ".", "processor", ".", "NAME", ",", "'%s.pt'", "%", "timestamp", ")", "\n", "\n", "self", ".", "num_train_optimization_steps", "=", "int", "(", "\n", "len", "(", "self", ".", "train_examples", ")", "/", "args", ".", "batch_size", "/", "args", ".", "gradient_accumulation_steps", ")", "*", "args", ".", "epochs", "\n", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "            ", "self", ".", "num_train_optimization_steps", "=", "args", ".", "num_train_optimization_steps", "//", "torch", ".", "distributed", ".", "get_world_size", "(", ")", "\n", "\n", "", "self", ".", "log_header", "=", "'Epoch Iteration Progress   Dev/Acc.  Dev/Pr.  Dev/Re.   Dev/F1   Dev/Loss   Dev/F1ma, Dev/HLoss, Dev/Jacc, Train/Loss'", "\n", "self", ".", "log_template", "=", "' '", ".", "join", "(", "'{:>5.0f},{:>9.0f},{:>6.0f}/{:<5.0f} {:>6.4f},{:>8.4f},{:8.4f},{:8.4f},{:10.4f},{:10.4f},{:10.4f},{:10.4f},{:10.4f}'", ".", "split", "(", "','", ")", ")", "\n", "\n", "self", ".", "iterations", ",", "self", ".", "nb_tr_steps", ",", "self", ".", "tr_loss", "=", "0", ",", "0", ",", "0", "\n", "self", ".", "best_dev_measure", ",", "self", ".", "unimproved_iters", "=", "0", ",", "0", "\n", "self", ".", "early_stop", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.trainers.bert_sent_trainer.BertTrainer.train_epoch": [[80, 142], ["enumerate", "tqdm.tqdm.tqdm", "bert_sent_trainer.BertTrainer.model.train", "tuple", "bert_sent_trainer.BertTrainer.model", "isinstance", "loss.mean.mean.item", "loss.mean.mean.item", "torch.cosine_similarity", "torch.cosine_similarity", "range", "torch.binary_cross_entropy_with_logits", "torch.binary_cross_entropy_with_logits", "torch.cross_entropy", "torch.cross_entropy", "loss.mean.mean.mean", "bert_sent_trainer.BertTrainer.optimizer.backward", "loss.mean.mean.backward", "bert_sent_trainer.BertTrainer.optimizer.step", "bert_sent_trainer.BertTrainer.optimizer.zero_grad", "t.to", "len", "torch.eq().all", "torch.eq().all", "torch.eq().all", "torch.eq().all", "label_ids.float", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.eq().all", "torch.eq().all", "torch.eq().all", "torch.eq().all", "utils.optimization.warmup_linear", "torch.eq", "torch.eq", "torch.eq", "torch.eq", "max", "print", "exit", "torch.Tensor().long().cuda", "torch.Tensor().long().cuda", "torch.Tensor().long().cuda", "torch.Tensor().long().cuda", "torch.eq", "torch.eq", "torch.eq", "torch.eq", "torch.Tensor().long().cuda", "torch.Tensor().long().cuda", "torch.Tensor().long().cuda", "torch.Tensor().long().cuda", "torch.Tensor().long", "torch.Tensor().long", "torch.Tensor().long", "torch.Tensor().long", "torch.Tensor().long", "torch.Tensor().long", "torch.Tensor().long", "torch.Tensor().long", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.trainers.bert_sent_trainer.BertTrainer.train", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.optimization.BertAdam.step", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.optimization.warmup_linear"], ["", "def", "train_epoch", "(", "self", ",", "train_dataloader", ")", ":", "\n", "        ", "loss_epoch", "=", "0", "\n", "for", "step", ",", "batch", "in", "enumerate", "(", "tqdm", "(", "train_dataloader", ",", "desc", "=", "\"Training\"", ")", ")", ":", "\n", "            ", "self", ".", "model", ".", "train", "(", ")", "\n", "batch", "=", "tuple", "(", "t", ".", "to", "(", "self", ".", "args", ".", "device", ")", "for", "t", "in", "batch", ")", "\n", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "sent_scores", ",", "label_ids", "=", "batch", "\n", "#print('sent scores in bert_sent_trainer',sent_scores.shape, sent_scores[:,-10:])", "\n", "'''print('label ids in bert_sent_trainer', label_ids[:-10])\n            print('input ids in bert_sent_trainer', input_ids[:-10])\n            print('input mask in bert_sent_trainer', input_mask[:-10])\n            print('segment ids in bert_sent_trainer', segment_ids[:-10])'''", "\n", "\n", "logits", "=", "self", ".", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "sent_scores", "=", "sent_scores", ")", "\n", "loss_extra", "=", "0", "\n", "if", "isinstance", "(", "logits", ",", "tuple", ")", ":", "\n", "                ", "logits", ",", "(", "first_SEP", ",", "second_SEP", ")", "=", "logits", "\n", "cos_simi", "=", "F", ".", "cosine_similarity", "(", "first_SEP", ",", "second_SEP", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "label_ids", ")", ")", ":", "\n", "                    ", "if", "torch", ".", "eq", "(", "label_ids", "[", "i", "]", ",", "torch", ".", "Tensor", "(", "[", "0", ",", "1", "]", ")", ".", "long", "(", ")", ".", "cuda", "(", ")", ")", ".", "all", "(", ")", ":", "\n", "                        ", "loss_extra", "+=", "1", "-", "cos_simi", "[", "i", "]", "\n", "", "elif", "torch", ".", "eq", "(", "label_ids", "[", "i", "]", ",", "torch", ".", "Tensor", "(", "[", "1", ",", "0", "]", ")", ".", "long", "(", ")", ".", "cuda", "(", ")", ")", ".", "all", "(", ")", ":", "\n", "                        ", "loss_extra", "+=", "max", "(", "0", ",", "cos_simi", "[", "i", "]", ")", "\n", "", "else", ":", "\n", "                        ", "print", "(", "'Invalid label value ERROR'", ",", "label_ids", "[", "i", "]", ")", "\n", "exit", "(", "1", ")", "\n", "\n", "", "", "", "'''import copy\n            model_copied = copy.deepcopy(self.model)\n            for p in model_copied.parameters():\n                if p.requires_grad:\n                    p.detach_()'''", "\n", "\n", "if", "self", ".", "args", ".", "is_multilabel", ":", "\n", "                ", "loss", "=", "F", ".", "binary_cross_entropy_with_logits", "(", "logits", ",", "label_ids", ".", "float", "(", ")", ")", "\n", "", "else", ":", "\n", "                ", "loss", "=", "F", ".", "cross_entropy", "(", "logits", ",", "torch", ".", "argmax", "(", "label_ids", ",", "dim", "=", "1", ")", ")", "\n", "loss", "+=", "loss_extra", "\n", "\n", "", "if", "self", ".", "args", ".", "n_gpu", ">", "1", ":", "\n", "                ", "loss", "=", "loss", ".", "mean", "(", ")", "\n", "", "if", "self", ".", "args", ".", "gradient_accumulation_steps", ">", "1", ":", "\n", "                ", "loss", "=", "loss", "/", "self", ".", "args", ".", "gradient_accumulation_steps", "\n", "\n", "", "if", "self", ".", "args", ".", "fp16", ":", "\n", "                ", "self", ".", "optimizer", ".", "backward", "(", "loss", ")", "\n", "", "else", ":", "\n", "                ", "loss", ".", "backward", "(", ")", "\n", "\n", "", "self", ".", "tr_loss", "+=", "loss", ".", "item", "(", ")", "\n", "loss_epoch", "+=", "loss", ".", "item", "(", ")", "\n", "self", ".", "nb_tr_steps", "+=", "1", "\n", "\n", "if", "(", "step", "+", "1", ")", "%", "self", ".", "args", ".", "gradient_accumulation_steps", "==", "0", ":", "\n", "                ", "if", "self", ".", "args", ".", "fp16", ":", "\n", "                    ", "lr_this_step", "=", "self", ".", "args", ".", "learning_rate", "*", "warmup_linear", "(", "self", ".", "iterations", "/", "self", ".", "num_train_optimization_steps", ",", "self", ".", "args", ".", "warmup_proportion", ")", "\n", "for", "param_group", "in", "self", ".", "optimizer", ".", "param_groups", ":", "\n", "                        ", "param_group", "[", "'lr'", "]", "=", "lr_this_step", "\n", "\n", "", "", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "self", ".", "iterations", "+=", "1", "\n", "", "", "return", "loss_epoch", "/", "(", "step", "+", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.trainers.bert_sent_trainer.BertTrainer.train": [[143, 211], ["print", "print", "print", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "torch.utils.data.TensorDataset", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "tqdm.tqdm.trange", "datasets.bert_processors.abstract_processor.convert_examples_to_hierarchical_features", "datasets.bert_processors.abstract_processor.convert_examples_to_features_with_sentiment", "utils.preprocessing.pad_input_matrix", "utils.preprocessing.pad_input_matrix", "utils.preprocessing.pad_input_matrix", "len", "torch.utils.data.RandomSampler", "torch.utils.data.RandomSampler", "torch.utils.data.distributed.DistributedSampler", "torch.utils.data.distributed.DistributedSampler", "int", "bert_sent_trainer.BertTrainer.train_epoch", "common.evaluators.bert_sent_evaluator.BertEvaluator", "tqdm.tqdm.tqdm.write", "tqdm.tqdm.tqdm.write", "common.evaluators.bert_sent_evaluator.BertEvaluator.get_scores", "bert_sent_trainer.BertTrainer.log_template.format", "torch.save", "torch.save", "torch.save", "torch.save", "print"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.convert_examples_to_hierarchical_features", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.convert_examples_to_features_with_sentiment", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.preprocessing.pad_input_matrix", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.preprocessing.pad_input_matrix", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.preprocessing.pad_input_matrix", "home.repos.pwc.inspect_result.marjanhs_procon20.trainers.bert_sent_trainer.BertTrainer.train_epoch", "home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.classification_evaluator.ClassificationEvaluator.get_scores"], ["", "def", "train", "(", "self", ")", ":", "\n", "\n", "        ", "if", "self", ".", "args", ".", "is_hierarchical", ":", "\n", "            ", "train_features", "=", "convert_examples_to_hierarchical_features", "(", "\n", "self", ".", "train_examples", ",", "self", ".", "args", ".", "max_seq_length", ",", "self", ".", "tokenizer", ")", "\n", "", "else", ":", "\n", "            ", "train_features", "=", "convert_examples_to_features_with_sentiment", "(", "\n", "self", ".", "train_examples", ",", "self", ".", "args", ".", "max_seq_length", ",", "self", ".", "tokenizer", ",", "overal_sent", "=", "self", ".", "args", ".", "overal_sent", ")", "\n", "\n", "", "unpadded_input_ids", "=", "[", "f", ".", "input_ids", "for", "f", "in", "train_features", "]", "\n", "unpadded_input_mask", "=", "[", "f", ".", "input_mask", "for", "f", "in", "train_features", "]", "\n", "unpadded_segment_ids", "=", "[", "f", ".", "segment_ids", "for", "f", "in", "train_features", "]", "\n", "unpadded_sent_scores", "=", "[", "f", ".", "sentiment_scores", "for", "f", "in", "train_features", "]", "\n", "\n", "if", "self", ".", "args", ".", "is_hierarchical", ":", "\n", "            ", "pad_input_matrix", "(", "unpadded_input_ids", ",", "self", ".", "args", ".", "max_doc_length", ")", "\n", "pad_input_matrix", "(", "unpadded_input_mask", ",", "self", ".", "args", ".", "max_doc_length", ")", "\n", "pad_input_matrix", "(", "unpadded_segment_ids", ",", "self", ".", "args", ".", "max_doc_length", ")", "\n", "\n", "", "print", "(", "\"Number of examples: \"", ",", "len", "(", "self", ".", "train_examples", ")", ")", "\n", "print", "(", "\"Batch size:\"", ",", "self", ".", "args", ".", "batch_size", ")", "\n", "print", "(", "\"Num of steps:\"", ",", "self", ".", "num_train_optimization_steps", ")", "\n", "\n", "padded_input_ids", "=", "torch", ".", "tensor", "(", "unpadded_input_ids", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "padded_input_mask", "=", "torch", ".", "tensor", "(", "unpadded_input_mask", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "padded_segment_ids", "=", "torch", ".", "tensor", "(", "unpadded_segment_ids", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "label_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "label_id", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "sent_scores", "=", "torch", ".", "tensor", "(", "unpadded_sent_scores", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "train_data", "=", "TensorDataset", "(", "padded_input_ids", ",", "padded_input_mask", ",", "padded_segment_ids", ",", "sent_scores", ",", "label_ids", ")", "\n", "\n", "if", "self", ".", "args", ".", "local_rank", "==", "-", "1", ":", "\n", "            ", "train_sampler", "=", "RandomSampler", "(", "train_data", ")", "\n", "", "else", ":", "\n", "            ", "train_sampler", "=", "DistributedSampler", "(", "train_data", ")", "\n", "\n", "", "train_dataloader", "=", "DataLoader", "(", "train_data", ",", "sampler", "=", "train_sampler", ",", "batch_size", "=", "self", ".", "args", ".", "batch_size", ")", "\n", "k", "=", "0", "\n", "for", "epoch", "in", "trange", "(", "int", "(", "self", ".", "args", ".", "epochs", ")", ",", "desc", "=", "\"Epoch\"", ")", ":", "\n", "            ", "loss_epoch", "=", "self", ".", "train_epoch", "(", "train_dataloader", ")", "\n", "dev_evaluator", "=", "BertEvaluator", "(", "self", ".", "model", ",", "self", ".", "processor", ",", "self", ".", "args", ",", "split", "=", "'dev'", ")", "\n", "dev_acc", ",", "dev_precision", ",", "dev_recall", ",", "dev_f1", ",", "dev_loss", ",", "dev_f1_macro", ",", "dev_hamming_loss", ",", "dev_jaccard_score", ",", "dev_predicted_labels", ",", "dev_target_labels", "=", "dev_evaluator", ".", "get_scores", "(", ")", "[", "0", "]", "\n", "\n", "# Print validation results", "\n", "tqdm", ".", "write", "(", "self", ".", "log_header", ")", "\n", "tqdm", ".", "write", "(", "self", ".", "log_template", ".", "format", "(", "epoch", "+", "1", ",", "self", ".", "iterations", ",", "epoch", "+", "1", ",", "self", ".", "args", ".", "epochs", ",", "\n", "dev_acc", ",", "dev_precision", ",", "dev_recall", ",", "dev_f1", ",", "dev_loss", ",", "dev_f1_macro", ",", "dev_hamming_loss", ",", "dev_jaccard_score", ",", "loss_epoch", ")", ")", "\n", "\n", "if", "self", ".", "args", ".", "early_on_f1", ":", "\n", "                ", "dev_measure", "=", "dev_f1", "\n", "measure_name", "=", "'F1'", "\n", "", "else", ":", "\n", "                ", "dev_measure", "=", "dev_acc", "\n", "measure_name", "=", "'Balanced Acc'", "\n", "\n", "# Update validation results", "\n", "", "if", "dev_measure", ">", "self", ".", "best_dev_measure", ":", "\n", "                ", "self", ".", "unimproved_iters", "=", "0", "\n", "self", ".", "best_dev_measure", "=", "dev_measure", "\n", "torch", ".", "save", "(", "self", ".", "model", ",", "self", ".", "snapshot_path", ")", "\n", "\n", "\n", "", "else", ":", "\n", "                ", "self", ".", "unimproved_iters", "+=", "1", "\n", "if", "self", ".", "unimproved_iters", ">=", "self", ".", "args", ".", "patience", ":", "\n", "                    ", "self", ".", "early_stop", "=", "True", "\n", "print", "(", "\"Early Stopping. Epoch: {}, Best {}: {}\"", ".", "format", "(", "epoch", ",", "measure_name", ",", "self", ".", "best_dev_measure", ")", ")", "\n", "break", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.trainers.bert_sent_trainer.plot_grad_flow": [[25, 53], ["matplotlib.bar", "matplotlib.bar", "matplotlib.hlines", "matplotlib.xticks", "matplotlib.xlim", "matplotlib.ylim", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.title", "matplotlib.grid", "matplotlib.legend", "matplotlib.savefig", "numpy.arange", "numpy.arange", "range", "layers.append", "ave_grads.append", "max_grads.append", "len", "len", "len", "len", "len", "matplotlib.lines.Line2D", "matplotlib.lines.Line2D", "matplotlib.lines.Line2D", "p.grad.abs().mean", "p.grad.abs().max", "p.grad.abs", "p.grad.abs"], "function", ["None"], ["def", "plot_grad_flow", "(", "named_parameters", ")", ":", "\n", "    ", "'''Plots the gradients flowing through different layers in the net during training.\n    Can be used for checking for possible gradient vanishing / exploding problems.\n\n    Usage: Plug this function in Trainer class after loss.backwards() as\n    \"plot_grad_flow(self.model.named_parameters())\" to visualize the gradient flow'''", "\n", "ave_grads", "=", "[", "]", "\n", "max_grads", "=", "[", "]", "\n", "layers", "=", "[", "]", "\n", "for", "n", ",", "p", "in", "named_parameters", ":", "\n", "        ", "if", "(", "p", ".", "requires_grad", ")", "and", "(", "\"bias\"", "not", "in", "n", ")", ":", "\n", "            ", "layers", ".", "append", "(", "n", ")", "\n", "ave_grads", ".", "append", "(", "p", ".", "grad", ".", "abs", "(", ")", ".", "mean", "(", ")", ")", "\n", "max_grads", ".", "append", "(", "p", ".", "grad", ".", "abs", "(", ")", ".", "max", "(", ")", ")", "\n", "", "", "plt", ".", "bar", "(", "np", ".", "arange", "(", "len", "(", "max_grads", ")", ")", ",", "max_grads", ",", "alpha", "=", "0.1", ",", "lw", "=", "1", ",", "color", "=", "\"c\"", ")", "\n", "plt", ".", "bar", "(", "np", ".", "arange", "(", "len", "(", "max_grads", ")", ")", ",", "ave_grads", ",", "alpha", "=", "0.1", ",", "lw", "=", "1", ",", "color", "=", "\"b\"", ")", "\n", "plt", ".", "hlines", "(", "0", ",", "0", ",", "len", "(", "ave_grads", ")", "+", "1", ",", "lw", "=", "2", ",", "color", "=", "\"k\"", ")", "\n", "plt", ".", "xticks", "(", "range", "(", "0", ",", "len", "(", "ave_grads", ")", ",", "1", ")", ",", "layers", ",", "rotation", "=", "\"vertical\"", ")", "\n", "plt", ".", "xlim", "(", "left", "=", "0", ",", "right", "=", "len", "(", "ave_grads", ")", ")", "\n", "plt", ".", "ylim", "(", "bottom", "=", "-", "0.001", ",", "top", "=", "0.02", ")", "# zoom in on the lower gradient regions", "\n", "plt", ".", "xlabel", "(", "\"Layers\"", ")", "\n", "plt", ".", "ylabel", "(", "\"average gradient\"", ")", "\n", "plt", ".", "title", "(", "\"Gradient flow\"", ")", "\n", "plt", ".", "grid", "(", "True", ")", "\n", "plt", ".", "legend", "(", "[", "Line2D", "(", "[", "0", "]", ",", "[", "0", "]", ",", "color", "=", "\"c\"", ",", "lw", "=", "4", ")", ",", "\n", "Line2D", "(", "[", "0", "]", ",", "[", "0", "]", ",", "color", "=", "\"b\"", ",", "lw", "=", "4", ")", ",", "\n", "Line2D", "(", "[", "0", "]", ",", "[", "0", "]", ",", "color", "=", "\"k\"", ",", "lw", "=", "4", ")", "]", ",", "[", "'max-gradient'", ",", "'mean-gradient'", ",", "'zero-gradient'", "]", ")", "\n", "plt", ".", "savefig", "(", "'grads.png'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.bert_sent_mt_evaluator.BertEvaluator.__init__": [[20, 31], ["utils.tokenization.BertTokenizer.from_pretrained", "bert_sent_mt_evaluator.BertEvaluator.processor.get_test_examples", "bert_sent_mt_evaluator.BertEvaluator.processor.get_dev_examples", "bert_sent_mt_evaluator.BertEvaluator.processor.get_any_examples"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertPreTrainedModel.from_pretrained", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor.get_test_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor.get_dev_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.imdb_processor.IMDBProcessor.get_any_examples"], ["    ", "def", "__init__", "(", "self", ",", "model", ",", "processor", ",", "args", ",", "split", "=", "'dev'", ")", ":", "\n", "        ", "self", ".", "args", "=", "args", "\n", "self", ".", "model", "=", "model", "\n", "self", ".", "processor", "=", "processor", "\n", "self", ".", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "args", ".", "model", ",", "is_lowercase", "=", "args", ".", "is_lowercase", ")", "\n", "if", "split", "==", "'test'", ":", "\n", "            ", "self", ".", "eval_examples", "=", "self", ".", "processor", ".", "get_test_examples", "(", "args", ".", "data_dir", ",", "args", ".", "test_name", ")", "\n", "", "elif", "split", "==", "'dev'", ":", "\n", "            ", "self", ".", "eval_examples", "=", "self", ".", "processor", ".", "get_dev_examples", "(", "args", ".", "data_dir", ",", "args", ".", "dev_name", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "eval_examples", "=", "self", ".", "processor", ".", "get_any_examples", "(", "args", ".", "data_dir", ",", "split", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.bert_sent_mt_evaluator.BertEvaluator.get_scores": [[32, 130], ["torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "torch.utils.data.TensorDataset", "torch.utils.data.SequentialSampler", "torch.utils.data.SequentialSampler", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "bert_sent_mt_evaluator.BertEvaluator.model.eval", "tqdm.tqdm.tqdm", "sklearn.metrics.accuracy_score", "sklearn.metrics.precision_score", "sklearn.metrics.recall_score", "sklearn.metrics.hamming_loss", "sklearn.metrics.jaccard_score", "sklearn.metrics.f1_score", "sklearn.metrics.f1_score", "datasets.bert_processors.abstract_processor.convert_examples_to_hierarchical_features", "datasets.bert_processors.abstract_processor.convert_examples_to_features_with_sentiment", "utils.preprocessing.pad_input_matrix", "utils.preprocessing.pad_input_matrix", "utils.preprocessing.pad_input_matrix", "list", "list", "input_ids.to.to.to", "input_mask.to.to.to", "segment_ids.to.to.to", "sent_scores.to.to.to", "label_ids_personality.to.to.to", "label_ids_task.to.to.to", "predicted_labels.extend", "target_labels.extend", "torch.cross_entropy", "torch.cross_entropy", "loss.mean.mean.item", "input_ids.to.to.size", "numpy.array", "numpy.array", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "all_indices.extend", "torch.argmax().cpu().detach().numpy", "torch.argmax().cpu().detach().numpy", "torch.argmax().cpu().detach().numpy", "torch.argmax().cpu().detach().numpy", "torch.argmax().cpu().detach().numpy", "torch.argmax().cpu().detach().numpy", "torch.argmax().cpu().detach().numpy", "torch.argmax().cpu().detach().numpy", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "loss.mean.mean.mean", "bert_sent_mt_evaluator.BertEvaluator.model", "bert_sent_mt_evaluator.BertEvaluator.model", "isinstance", "indices.cpu().detach().numpy", "torch.argmax().cpu().detach", "torch.argmax().cpu().detach", "torch.argmax().cpu().detach", "torch.argmax().cpu().detach", "torch.argmax().cpu().detach", "torch.argmax().cpu().detach", "torch.argmax().cpu().detach", "torch.argmax().cpu().detach", "indices.cpu().detach", "torch.argmax().cpu", "torch.argmax().cpu", "torch.argmax().cpu", "torch.argmax().cpu", "torch.argmax().cpu", "torch.argmax().cpu", "torch.argmax().cpu", "torch.argmax().cpu", "indices.cpu", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.convert_examples_to_hierarchical_features", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.convert_examples_to_features_with_sentiment", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.preprocessing.pad_input_matrix", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.preprocessing.pad_input_matrix", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.preprocessing.pad_input_matrix"], ["", "", "def", "get_scores", "(", "self", ",", "silent", "=", "False", ",", "return_indices", "=", "False", ")", ":", "\n", "        ", "all_indices", "=", "[", "]", "\n", "if", "self", ".", "args", ".", "is_hierarchical", ":", "\n", "            ", "eval_features", "=", "convert_examples_to_hierarchical_features", "(", "\n", "self", ".", "eval_examples", ",", "self", ".", "args", ".", "max_seq_length", ",", "self", ".", "tokenizer", ")", "\n", "", "else", ":", "\n", "            ", "eval_features", "=", "convert_examples_to_features_with_sentiment", "(", "\n", "self", ".", "eval_examples", ",", "self", ".", "args", ".", "max_seq_length", ",", "self", ".", "tokenizer", ",", "overal_sent", "=", "self", ".", "args", ".", "overal_sent", ")", "\n", "\n", "", "unpadded_input_ids", "=", "[", "f", ".", "input_ids", "for", "f", "in", "eval_features", "]", "\n", "unpadded_input_mask", "=", "[", "f", ".", "input_mask", "for", "f", "in", "eval_features", "]", "\n", "unpadded_segment_ids", "=", "[", "f", ".", "segment_ids", "for", "f", "in", "eval_features", "]", "\n", "unpadded_sent_scores", "=", "[", "f", ".", "sentiment_scores", "for", "f", "in", "eval_features", "]", "\n", "\n", "if", "self", ".", "args", ".", "is_hierarchical", ":", "\n", "            ", "pad_input_matrix", "(", "unpadded_input_ids", ",", "self", ".", "args", ".", "max_doc_length", ")", "\n", "pad_input_matrix", "(", "unpadded_input_mask", ",", "self", ".", "args", ".", "max_doc_length", ")", "\n", "pad_input_matrix", "(", "unpadded_segment_ids", ",", "self", ".", "args", ".", "max_doc_length", ")", "\n", "\n", "", "padded_input_ids", "=", "torch", ".", "tensor", "(", "unpadded_input_ids", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "padded_input_mask", "=", "torch", ".", "tensor", "(", "unpadded_input_mask", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "padded_segment_ids", "=", "torch", ".", "tensor", "(", "unpadded_segment_ids", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "sent_scores", "=", "torch", ".", "tensor", "(", "unpadded_sent_scores", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "label_ids_personality", "=", "torch", ".", "tensor", "(", "[", "f", ".", "label_id", "[", ":", "4", "]", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "label_ids_task", "=", "torch", ".", "tensor", "(", "[", "f", ".", "label_id", "[", "4", ":", "]", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "eval_data", "=", "TensorDataset", "(", "padded_input_ids", ",", "padded_input_mask", ",", "padded_segment_ids", ",", "sent_scores", ",", "\n", "label_ids_personality", ",", "label_ids_task", ")", "\n", "eval_sampler", "=", "SequentialSampler", "(", "eval_data", ")", "\n", "eval_dataloader", "=", "DataLoader", "(", "eval_data", ",", "sampler", "=", "eval_sampler", ",", "batch_size", "=", "self", ".", "args", ".", "batch_size", ")", "\n", "\n", "self", ".", "model", ".", "eval", "(", ")", "\n", "\n", "total_loss", "=", "0", "\n", "nb_eval_steps", ",", "nb_eval_examples", "=", "0", ",", "0", "\n", "predicted_labels", ",", "target_labels", "=", "list", "(", ")", ",", "list", "(", ")", "\n", "\n", "for", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "sent_scores", ",", "label_ids_personality", ",", "label_ids_task", "in", "tqdm", "(", "eval_dataloader", ",", "desc", "=", "\"Evaluating\"", ",", "disable", "=", "silent", ")", ":", "\n", "            ", "input_ids", "=", "input_ids", ".", "to", "(", "self", ".", "args", ".", "device", ")", "\n", "input_mask", "=", "input_mask", ".", "to", "(", "self", ".", "args", ".", "device", ")", "\n", "segment_ids", "=", "segment_ids", ".", "to", "(", "self", ".", "args", ".", "device", ")", "\n", "sent_scores", "=", "sent_scores", ".", "to", "(", "self", ".", "args", ".", "device", ")", "\n", "label_ids_personality", "=", "label_ids_personality", ".", "to", "(", "self", ".", "args", ".", "device", ")", "\n", "label_ids_task", "=", "label_ids_task", ".", "to", "(", "self", ".", "args", ".", "device", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "if", "return_indices", ":", "\n", "                    ", "outs", "=", "self", ".", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "sent_scores", "=", "sent_scores", ",", "return_indices", "=", "return_indices", ")", "\n", "", "else", ":", "\n", "                    ", "outs", "=", "self", ".", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "sent_scores", "=", "sent_scores", ")", "\n", "if", "isinstance", "(", "outs", ",", "tuple", ")", ":", "\n", "                        ", "outs", ",", "_", "=", "outs", "\n", "\n", "", "", "", "if", "return_indices", ":", "\n", "                ", "logits", ",", "indices", "=", "outs", "\n", "all_indices", ".", "extend", "(", "indices", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "", "else", ":", "\n", "                ", "logits", "=", "outs", "\n", "\n", "", "'''if self.args.is_multilabel:\n                predicted_labels.extend(F.sigmoid(logits).round().long().cpu().detach().numpy())\n                target_labels.extend(label_ids.cpu().detach().numpy())\n                loss = F.binary_cross_entropy_with_logits(logits, label_ids.float(), size_average=False)\n                average, average_mac = 'micro', 'macro'''", "\n", "\n", "#print('EVALUATION ONLY ON ONE TASK (NO PERSONALITY!')", "\n", "predicted_labels", ".", "extend", "(", "torch", ".", "argmax", "(", "logits", "[", "'logits_task'", "]", ",", "dim", "=", "1", ")", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "target_labels", ".", "extend", "(", "torch", ".", "argmax", "(", "label_ids_task", ",", "dim", "=", "1", ")", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "loss", "=", "F", ".", "cross_entropy", "(", "logits", "[", "'logits_task'", "]", ",", "torch", ".", "argmax", "(", "label_ids_task", ",", "dim", "=", "1", ")", ")", "\n", "average", ",", "average_mac", "=", "'binary'", ",", "'binary'", "\n", "\n", "if", "self", ".", "args", ".", "n_gpu", ">", "1", ":", "\n", "                ", "loss", "=", "loss", ".", "mean", "(", ")", "\n", "", "if", "self", ".", "args", ".", "gradient_accumulation_steps", ">", "1", ":", "\n", "                ", "loss", "=", "loss", "/", "self", ".", "args", ".", "gradient_accumulation_steps", "\n", "", "total_loss", "+=", "loss", ".", "item", "(", ")", "\n", "\n", "nb_eval_examples", "+=", "input_ids", ".", "size", "(", "0", ")", "\n", "nb_eval_steps", "+=", "1", "\n", "\n", "", "predicted_labels", ",", "target_labels", "=", "np", ".", "array", "(", "predicted_labels", ")", ",", "np", ".", "array", "(", "target_labels", ")", "\n", "accuracy", "=", "metrics", ".", "accuracy_score", "(", "target_labels", ",", "predicted_labels", ")", "\n", "precision", "=", "metrics", ".", "precision_score", "(", "target_labels", ",", "predicted_labels", ",", "average", "=", "average", ")", "\n", "recall", "=", "metrics", ".", "recall_score", "(", "target_labels", ",", "predicted_labels", ",", "average", "=", "average", ")", "\n", "avg_loss", "=", "total_loss", "/", "nb_eval_steps", "\n", "\n", "hamming_loss", "=", "metrics", ".", "hamming_loss", "(", "target_labels", ",", "predicted_labels", ")", "\n", "jaccard_score", "=", "metrics", ".", "jaccard_score", "(", "target_labels", ",", "predicted_labels", ",", "average", "=", "average", ")", "\n", "f1_micro", "=", "metrics", ".", "f1_score", "(", "target_labels", ",", "predicted_labels", ",", "average", "=", "average", ")", "\n", "f1_macro", "=", "metrics", ".", "f1_score", "(", "target_labels", ",", "predicted_labels", ",", "average", "=", "average_mac", ")", "\n", "\n", "if", "return_indices", ":", "\n", "            ", "return", "[", "accuracy", ",", "precision", ",", "recall", ",", "f1_micro", ",", "avg_loss", ",", "f1_macro", ",", "hamming_loss", ",", "jaccard_score", ",", "predicted_labels", ",", "target_labels", ",", "all_indices", "]", ",", "[", "'accuracy'", ",", "'precision'", ",", "'recall'", ",", "'f1_micro'", ",", "'avg_loss'", ",", "'f1_macro'", ",", "'hamming_loss'", ",", "'jaccard'", ",", "'predicted_labels'", ",", "'target_labels'", ",", "'all_indices'", "]", "\n", "", "else", ":", "\n", "\n", "            ", "return", "[", "accuracy", ",", "precision", ",", "recall", ",", "f1_micro", ",", "avg_loss", ",", "f1_macro", ",", "hamming_loss", ",", "jaccard_score", ",", "predicted_labels", ",", "target_labels", "]", ",", "[", "'accuracy'", ",", "'precision'", ",", "'recall'", ",", "'f1_micro'", ",", "'avg_loss'", ",", "'f1_macro'", ",", "'hamming_loss'", ",", "'jaccard'", ",", "'predicted_labels'", ",", "'target_labels'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.bert_evaluator.BertEvaluator.__init__": [[20, 31], ["utils.tokenization.BertTokenizer.from_pretrained", "bert_evaluator.BertEvaluator.processor.get_test_examples", "bert_evaluator.BertEvaluator.processor.get_dev_examples", "bert_evaluator.BertEvaluator.processor.get_any_examples"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertPreTrainedModel.from_pretrained", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor.get_test_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor.get_dev_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.imdb_processor.IMDBProcessor.get_any_examples"], ["    ", "def", "__init__", "(", "self", ",", "model", ",", "processor", ",", "args", ",", "split", "=", "'dev'", ")", ":", "\n", "        ", "self", ".", "args", "=", "args", "\n", "self", ".", "model", "=", "model", "\n", "self", ".", "processor", "=", "processor", "\n", "self", ".", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "args", ".", "model", ",", "is_lowercase", "=", "args", ".", "is_lowercase", ")", "\n", "if", "split", "==", "'test'", ":", "\n", "            ", "self", ".", "eval_examples", "=", "self", ".", "processor", ".", "get_test_examples", "(", "args", ".", "data_dir", ",", "args", ".", "test_name", ")", "\n", "", "elif", "split", "==", "'dev'", ":", "\n", "            ", "self", ".", "eval_examples", "=", "self", ".", "processor", ".", "get_dev_examples", "(", "args", ".", "data_dir", ",", "args", ".", "dev_name", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "eval_examples", "=", "self", ".", "processor", ".", "get_any_examples", "(", "args", ".", "data_dir", ",", "split", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.bert_evaluator.BertEvaluator.get_scores": [[32, 110], ["torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "torch.utils.data.TensorDataset", "torch.utils.data.SequentialSampler", "torch.utils.data.SequentialSampler", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "bert_evaluator.BertEvaluator.model.eval", "tqdm.tqdm.tqdm", "sklearn.metrics.balanced_accuracy_score", "sklearn.metrics.precision_score", "sklearn.metrics.recall_score", "sklearn.metrics.hamming_loss", "sklearn.metrics.jaccard_score", "sklearn.metrics.f1_score", "sklearn.metrics.f1_score", "datasets.bert_processors.abstract_processor.convert_examples_to_hierarchical_features", "datasets.bert_processors.abstract_processor.convert_examples_to_features", "utils.preprocessing.pad_input_matrix", "utils.preprocessing.pad_input_matrix", "utils.preprocessing.pad_input_matrix", "list", "list", "input_ids.to.to.to", "input_mask.to.to.to", "segment_ids.to.to.to", "label_ids.to.to.to", "loss.mean.mean.item", "input_ids.to.to.size", "numpy.array", "numpy.array", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "bert_evaluator.BertEvaluator.model", "isinstance", "predicted_labels.extend", "target_labels.extend", "torch.binary_cross_entropy_with_logits", "torch.binary_cross_entropy_with_logits", "predicted_labels.extend", "target_labels.extend", "torch.cross_entropy", "torch.cross_entropy", "loss.mean.mean.mean", "torch.sigmoid().round().long().cpu().detach().numpy", "torch.sigmoid().round().long().cpu().detach().numpy", "label_ids.to.to.cpu().detach().numpy", "label_ids.to.to.float", "torch.argmax().cpu().detach().numpy", "torch.argmax().cpu().detach().numpy", "torch.argmax().cpu().detach().numpy", "torch.argmax().cpu().detach().numpy", "torch.argmax().cpu().detach().numpy", "torch.argmax().cpu().detach().numpy", "torch.argmax().cpu().detach().numpy", "torch.argmax().cpu().detach().numpy", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.sigmoid().round().long().cpu().detach", "torch.sigmoid().round().long().cpu().detach", "label_ids.to.to.cpu().detach", "torch.argmax().cpu().detach", "torch.argmax().cpu().detach", "torch.argmax().cpu().detach", "torch.argmax().cpu().detach", "torch.argmax().cpu().detach", "torch.argmax().cpu().detach", "torch.argmax().cpu().detach", "torch.argmax().cpu().detach", "torch.sigmoid().round().long().cpu", "torch.sigmoid().round().long().cpu", "label_ids.to.to.cpu", "torch.argmax().cpu", "torch.argmax().cpu", "torch.argmax().cpu", "torch.argmax().cpu", "torch.argmax().cpu", "torch.argmax().cpu", "torch.argmax().cpu", "torch.argmax().cpu", "torch.sigmoid().round().long", "torch.sigmoid().round().long", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.sigmoid().round", "torch.sigmoid().round", "torch.sigmoid", "torch.sigmoid"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.convert_examples_to_hierarchical_features", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.convert_examples_to_features", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.preprocessing.pad_input_matrix", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.preprocessing.pad_input_matrix", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.preprocessing.pad_input_matrix"], ["", "", "def", "get_scores", "(", "self", ",", "silent", "=", "False", ")", ":", "\n", "        ", "if", "self", ".", "args", ".", "is_hierarchical", ":", "\n", "            ", "eval_features", "=", "convert_examples_to_hierarchical_features", "(", "\n", "self", ".", "eval_examples", ",", "self", ".", "args", ".", "max_seq_length", ",", "self", ".", "tokenizer", ")", "\n", "", "else", ":", "\n", "            ", "eval_features", "=", "convert_examples_to_features", "(", "\n", "self", ".", "eval_examples", ",", "self", ".", "args", ".", "max_seq_length", ",", "self", ".", "tokenizer", ")", "\n", "\n", "", "unpadded_input_ids", "=", "[", "f", ".", "input_ids", "for", "f", "in", "eval_features", "]", "\n", "unpadded_input_mask", "=", "[", "f", ".", "input_mask", "for", "f", "in", "eval_features", "]", "\n", "unpadded_segment_ids", "=", "[", "f", ".", "segment_ids", "for", "f", "in", "eval_features", "]", "\n", "\n", "if", "self", ".", "args", ".", "is_hierarchical", ":", "\n", "            ", "pad_input_matrix", "(", "unpadded_input_ids", ",", "self", ".", "args", ".", "max_doc_length", ")", "\n", "pad_input_matrix", "(", "unpadded_input_mask", ",", "self", ".", "args", ".", "max_doc_length", ")", "\n", "pad_input_matrix", "(", "unpadded_segment_ids", ",", "self", ".", "args", ".", "max_doc_length", ")", "\n", "\n", "", "padded_input_ids", "=", "torch", ".", "tensor", "(", "unpadded_input_ids", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "padded_input_mask", "=", "torch", ".", "tensor", "(", "unpadded_input_mask", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "padded_segment_ids", "=", "torch", ".", "tensor", "(", "unpadded_segment_ids", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "label_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "label_id", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "eval_data", "=", "TensorDataset", "(", "padded_input_ids", ",", "padded_input_mask", ",", "padded_segment_ids", ",", "label_ids", ")", "\n", "eval_sampler", "=", "SequentialSampler", "(", "eval_data", ")", "\n", "eval_dataloader", "=", "DataLoader", "(", "eval_data", ",", "sampler", "=", "eval_sampler", ",", "batch_size", "=", "self", ".", "args", ".", "batch_size", ")", "\n", "\n", "self", ".", "model", ".", "eval", "(", ")", "\n", "\n", "total_loss", "=", "0", "\n", "nb_eval_steps", ",", "nb_eval_examples", "=", "0", ",", "0", "\n", "predicted_labels", ",", "target_labels", "=", "list", "(", ")", ",", "list", "(", ")", "\n", "\n", "for", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "label_ids", "in", "tqdm", "(", "eval_dataloader", ",", "desc", "=", "\"Evaluating\"", ",", "disable", "=", "silent", ")", ":", "\n", "            ", "input_ids", "=", "input_ids", ".", "to", "(", "self", ".", "args", ".", "device", ")", "\n", "input_mask", "=", "input_mask", ".", "to", "(", "self", ".", "args", ".", "device", ")", "\n", "segment_ids", "=", "segment_ids", ".", "to", "(", "self", ".", "args", ".", "device", ")", "\n", "label_ids", "=", "label_ids", ".", "to", "(", "self", ".", "args", ".", "device", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "logits", "=", "self", ".", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ")", "\n", "if", "isinstance", "(", "logits", ",", "tuple", ")", ":", "\n", "                    ", "logits", ",", "_", "=", "logits", "\n", "\n", "", "", "if", "self", ".", "args", ".", "is_multilabel", ":", "\n", "                ", "predicted_labels", ".", "extend", "(", "F", ".", "sigmoid", "(", "logits", ")", ".", "round", "(", ")", ".", "long", "(", ")", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "target_labels", ".", "extend", "(", "label_ids", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "loss", "=", "F", ".", "binary_cross_entropy_with_logits", "(", "logits", ",", "label_ids", ".", "float", "(", ")", ",", "size_average", "=", "False", ")", "\n", "average", ",", "average_mac", "=", "'micro'", ",", "'macro'", "\n", "\n", "", "else", ":", "\n", "                ", "predicted_labels", ".", "extend", "(", "torch", ".", "argmax", "(", "logits", ",", "dim", "=", "1", ")", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "target_labels", ".", "extend", "(", "torch", ".", "argmax", "(", "label_ids", ",", "dim", "=", "1", ")", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "loss", "=", "F", ".", "cross_entropy", "(", "logits", ",", "torch", ".", "argmax", "(", "label_ids", ",", "dim", "=", "1", ")", ")", "\n", "average", ",", "average_mac", "=", "'binary'", ",", "'binary'", "\n", "\n", "", "if", "self", ".", "args", ".", "n_gpu", ">", "1", ":", "\n", "                ", "loss", "=", "loss", ".", "mean", "(", ")", "\n", "", "if", "self", ".", "args", ".", "gradient_accumulation_steps", ">", "1", ":", "\n", "                ", "loss", "=", "loss", "/", "self", ".", "args", ".", "gradient_accumulation_steps", "\n", "", "total_loss", "+=", "loss", ".", "item", "(", ")", "\n", "\n", "nb_eval_examples", "+=", "input_ids", ".", "size", "(", "0", ")", "\n", "nb_eval_steps", "+=", "1", "\n", "\n", "", "predicted_labels", ",", "target_labels", "=", "np", ".", "array", "(", "predicted_labels", ")", ",", "np", ".", "array", "(", "target_labels", ")", "\n", "accuracy", "=", "metrics", ".", "balanced_accuracy_score", "(", "target_labels", ",", "predicted_labels", ")", "\n", "#accuracy = metrics.accuracy_score(target_labels, predicted_labels)", "\n", "precision", "=", "metrics", ".", "precision_score", "(", "target_labels", ",", "predicted_labels", ",", "average", "=", "average", ")", "\n", "recall", "=", "metrics", ".", "recall_score", "(", "target_labels", ",", "predicted_labels", ",", "average", "=", "average", ")", "\n", "avg_loss", "=", "total_loss", "/", "nb_eval_steps", "\n", "\n", "hamming_loss", "=", "metrics", ".", "hamming_loss", "(", "target_labels", ",", "predicted_labels", ")", "\n", "jaccard_score", "=", "metrics", ".", "jaccard_score", "(", "target_labels", ",", "predicted_labels", ",", "average", "=", "average", ")", "\n", "f1_micro", "=", "metrics", ".", "f1_score", "(", "target_labels", ",", "predicted_labels", ",", "average", "=", "average", ")", "\n", "f1_macro", "=", "metrics", ".", "f1_score", "(", "target_labels", ",", "predicted_labels", ",", "average", "=", "average_mac", ")", "\n", "\n", "return", "[", "accuracy", ",", "precision", ",", "recall", ",", "f1_micro", ",", "avg_loss", ",", "f1_macro", ",", "hamming_loss", ",", "jaccard_score", ",", "predicted_labels", ",", "target_labels", "]", ",", "[", "'accuracy'", ",", "'precision'", ",", "'recall'", ",", "'f1_micro'", ",", "'avg_loss'", ",", "'f1_macro'", ",", "'hamming_loss'", ",", "'jaccard'", ",", "'predicted_labels'", ",", "'target_labels'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.bert_evaluator.BertEvaluator.get_bert_layers": [[111, 156], ["torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "torch.utils.data.TensorDataset", "torch.utils.data.SequentialSampler", "torch.utils.data.SequentialSampler", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "bert_evaluator.BertEvaluator.model.eval", "tqdm.tqdm.tqdm", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "datasets.bert_processors.abstract_processor.convert_examples_to_hierarchical_features", "datasets.bert_processors.abstract_processor.convert_examples_to_features", "utils.preprocessing.pad_input_matrix", "utils.preprocessing.pad_input_matrix", "utils.preprocessing.pad_input_matrix", "input_ids.to.to.to", "input_mask.to.to.to", "segment_ids.to.to.to", "torch.argmax().cpu().detach().numpy.to", "torch.argmax().cpu().detach().numpy.to", "torch.argmax().cpu().detach().numpy", "torch.argmax().cpu().detach().numpy", "torch.argmax().cpu().detach().numpy", "torch.argmax().cpu().detach().numpy", "torch.stack.extend", "torch.stack.extend", "label_ids_l.extend", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "bert_evaluator.BertEvaluator.model.get_bert_embedding", "torch.argmax().cpu().detach", "torch.argmax().cpu().detach", "torch.argmax().cpu().detach", "torch.argmax().cpu().detach", "torch.argmax().cpu", "torch.argmax().cpu", "torch.argmax().cpu", "torch.argmax().cpu", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.convert_examples_to_hierarchical_features", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.convert_examples_to_features", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.preprocessing.pad_input_matrix", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.preprocessing.pad_input_matrix", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.preprocessing.pad_input_matrix", "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertForSequenceClassification.get_bert_embedding"], ["", "def", "get_bert_layers", "(", "self", ",", "silent", "=", "False", ",", "last_bert_layers", "=", "-", "1", ")", ":", "\n", "        ", "if", "self", ".", "args", ".", "is_hierarchical", ":", "\n", "            ", "eval_features", "=", "convert_examples_to_hierarchical_features", "(", "\n", "self", ".", "eval_examples", ",", "self", ".", "args", ".", "max_seq_length", ",", "self", ".", "tokenizer", ")", "\n", "", "else", ":", "\n", "            ", "eval_features", "=", "convert_examples_to_features", "(", "\n", "self", ".", "eval_examples", ",", "self", ".", "args", ".", "max_seq_length", ",", "self", ".", "tokenizer", ")", "\n", "\n", "", "unpadded_input_ids", "=", "[", "f", ".", "input_ids", "for", "f", "in", "eval_features", "]", "\n", "unpadded_input_mask", "=", "[", "f", ".", "input_mask", "for", "f", "in", "eval_features", "]", "\n", "unpadded_segment_ids", "=", "[", "f", ".", "segment_ids", "for", "f", "in", "eval_features", "]", "\n", "\n", "if", "self", ".", "args", ".", "is_hierarchical", ":", "\n", "            ", "pad_input_matrix", "(", "unpadded_input_ids", ",", "self", ".", "args", ".", "max_doc_length", ")", "\n", "pad_input_matrix", "(", "unpadded_input_mask", ",", "self", ".", "args", ".", "max_doc_length", ")", "\n", "pad_input_matrix", "(", "unpadded_segment_ids", ",", "self", ".", "args", ".", "max_doc_length", ")", "\n", "\n", "", "padded_input_ids", "=", "torch", ".", "tensor", "(", "unpadded_input_ids", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "padded_input_mask", "=", "torch", ".", "tensor", "(", "unpadded_input_mask", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "padded_segment_ids", "=", "torch", ".", "tensor", "(", "unpadded_segment_ids", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "label_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "label_id", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "eval_data", "=", "TensorDataset", "(", "padded_input_ids", ",", "padded_input_mask", ",", "padded_segment_ids", ",", "label_ids", ")", "\n", "eval_sampler", "=", "SequentialSampler", "(", "eval_data", ")", "\n", "eval_dataloader", "=", "DataLoader", "(", "eval_data", ",", "sampler", "=", "eval_sampler", ",", "batch_size", "=", "self", ".", "args", ".", "batch_size", ")", "\n", "\n", "self", ".", "model", ".", "eval", "(", ")", "\n", "\n", "bert_layers_l", ",", "label_ids_l", "=", "[", "]", ",", "[", "]", "\n", "\n", "for", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "label_ids", "in", "tqdm", "(", "eval_dataloader", ",", "desc", "=", "\"Evaluating\"", ",", "disable", "=", "silent", ")", ":", "\n", "            ", "input_ids", "=", "input_ids", ".", "to", "(", "self", ".", "args", ".", "device", ")", "\n", "input_mask", "=", "input_mask", ".", "to", "(", "self", ".", "args", ".", "device", ")", "\n", "segment_ids", "=", "segment_ids", ".", "to", "(", "self", ".", "args", ".", "device", ")", "\n", "label_ids", "=", "label_ids", ".", "to", "(", "self", ".", "args", ".", "device", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "bert_layers", "=", "self", ".", "model", ".", "get_bert_embedding", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "last_bert_layers", "=", "last_bert_layers", ")", "\n", "\n", "", "label_ids", "=", "torch", ".", "argmax", "(", "label_ids", ",", "dim", "=", "1", ")", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "bert_layers_l", ".", "extend", "(", "bert_layers", ")", "\n", "label_ids_l", ".", "extend", "(", "label_ids", ")", "\n", "", "bert_layers_l", "=", "torch", ".", "stack", "(", "bert_layers_l", ",", "dim", "=", "0", ")", "\n", "#label_ids_l = torch.stack(label_ids_l, dim=0)", "\n", "return", "bert_layers_l", ",", "label_ids_l", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.bert_emotion_evaluator.BertEvaluator.__init__": [[21, 33], ["utils.tokenization.BertTokenizer.from_pretrained", "utils.emotion.Emotion", "bert_emotion_evaluator.BertEvaluator.processor.get_test_examples", "bert_emotion_evaluator.BertEvaluator.processor.get_dev_examples", "bert_emotion_evaluator.BertEvaluator.processor.get_any_examples"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertPreTrainedModel.from_pretrained", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor.get_test_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor.get_dev_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.imdb_processor.IMDBProcessor.get_any_examples"], ["    ", "def", "__init__", "(", "self", ",", "model", ",", "processor", ",", "args", ",", "split", "=", "'dev'", ")", ":", "\n", "        ", "self", ".", "args", "=", "args", "\n", "self", ".", "model", "=", "model", "\n", "self", ".", "processor", "=", "processor", "\n", "self", ".", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "args", ".", "model", ",", "is_lowercase", "=", "args", ".", "is_lowercase", ")", "\n", "self", ".", "emotioner", "=", "Emotion", "(", "args", ".", "nrc_path", ",", "args", ".", "max_em_len", ",", "args", ".", "emotion_filters", ")", "\n", "if", "split", "==", "'test'", ":", "\n", "            ", "self", ".", "eval_examples", "=", "self", ".", "processor", ".", "get_test_examples", "(", "args", ".", "data_dir", ",", "args", ".", "test_name", ")", "\n", "", "elif", "split", "==", "'dev'", ":", "\n", "            ", "self", ".", "eval_examples", "=", "self", ".", "processor", ".", "get_dev_examples", "(", "args", ".", "data_dir", ",", "args", ".", "dev_name", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "eval_examples", "=", "self", ".", "processor", ".", "get_any_examples", "(", "args", ".", "data_dir", ",", "split", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.bert_emotion_evaluator.BertEvaluator.get_scores": [[34, 130], ["torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "torch.utils.data.TensorDataset", "torch.utils.data.SequentialSampler", "torch.utils.data.SequentialSampler", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "bert_emotion_evaluator.BertEvaluator.model.eval", "tqdm.tqdm.tqdm", "sklearn.metrics.accuracy_score", "sklearn.metrics.precision_score", "sklearn.metrics.recall_score", "sklearn.metrics.hamming_loss", "sklearn.metrics.jaccard_score", "sklearn.metrics.f1_score", "sklearn.metrics.f1_score", "datasets.bert_processors.abstract_processor.convert_examples_to_hierarchical_features", "datasets.bert_processors.abstract_processor.convert_examples_to_features_with_emotion", "utils.preprocessing.pad_input_matrix", "utils.preprocessing.pad_input_matrix", "utils.preprocessing.pad_input_matrix", "list", "list", "input_ids.to.to.to", "input_mask.to.to.to", "segment_ids.to.to.to", "emotion_ids.to.to.to", "label_ids.to.to.to", "loss.mean.mean.item", "input_ids.to.to.size", "numpy.array", "numpy.array", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "all_indices.extend", "predicted_labels.extend", "target_labels.extend", "torch.binary_cross_entropy_with_logits", "torch.binary_cross_entropy_with_logits", "predicted_labels.extend", "target_labels.extend", "torch.cross_entropy", "torch.cross_entropy", "loss.mean.mean.mean", "bert_emotion_evaluator.BertEvaluator.model", "bert_emotion_evaluator.BertEvaluator.model", "isinstance", "indices.cpu().detach().numpy", "torch.sigmoid().round().long().cpu().detach().numpy", "torch.sigmoid().round().long().cpu().detach().numpy", "label_ids.to.to.cpu().detach().numpy", "label_ids.to.to.float", "torch.argmax().cpu().detach().numpy", "torch.argmax().cpu().detach().numpy", "torch.argmax().cpu().detach().numpy", "torch.argmax().cpu().detach().numpy", "torch.argmax().cpu().detach().numpy", "torch.argmax().cpu().detach().numpy", "torch.argmax().cpu().detach().numpy", "torch.argmax().cpu().detach().numpy", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "indices.cpu().detach", "torch.sigmoid().round().long().cpu().detach", "torch.sigmoid().round().long().cpu().detach", "label_ids.to.to.cpu().detach", "torch.argmax().cpu().detach", "torch.argmax().cpu().detach", "torch.argmax().cpu().detach", "torch.argmax().cpu().detach", "torch.argmax().cpu().detach", "torch.argmax().cpu().detach", "torch.argmax().cpu().detach", "torch.argmax().cpu().detach", "indices.cpu", "torch.sigmoid().round().long().cpu", "torch.sigmoid().round().long().cpu", "label_ids.to.to.cpu", "torch.argmax().cpu", "torch.argmax().cpu", "torch.argmax().cpu", "torch.argmax().cpu", "torch.argmax().cpu", "torch.argmax().cpu", "torch.argmax().cpu", "torch.argmax().cpu", "torch.sigmoid().round().long", "torch.sigmoid().round().long", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.sigmoid().round", "torch.sigmoid().round", "torch.sigmoid", "torch.sigmoid"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.convert_examples_to_hierarchical_features", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.convert_examples_to_features_with_emotion", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.preprocessing.pad_input_matrix", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.preprocessing.pad_input_matrix", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.preprocessing.pad_input_matrix"], ["", "", "def", "get_scores", "(", "self", ",", "silent", "=", "False", ",", "return_indices", "=", "False", ")", ":", "\n", "        ", "all_indices", "=", "[", "]", "\n", "if", "self", ".", "args", ".", "is_hierarchical", ":", "\n", "            ", "eval_features", "=", "convert_examples_to_hierarchical_features", "(", "\n", "self", ".", "eval_examples", ",", "self", ".", "args", ".", "max_seq_length", ",", "self", ".", "tokenizer", ")", "\n", "", "else", ":", "\n", "            ", "eval_features", "=", "convert_examples_to_features_with_emotion", "(", "\n", "self", ".", "eval_examples", ",", "self", ".", "args", ".", "max_seq_length", ",", "self", ".", "tokenizer", ",", "self", ".", "emotioner", ")", "\n", "\n", "", "unpadded_input_ids", "=", "[", "f", ".", "input_ids", "for", "f", "in", "eval_features", "]", "\n", "unpadded_input_mask", "=", "[", "f", ".", "input_mask", "for", "f", "in", "eval_features", "]", "\n", "unpadded_segment_ids", "=", "[", "f", ".", "segment_ids", "for", "f", "in", "eval_features", "]", "\n", "unpadded_emotion_scores", "=", "[", "f", ".", "sentiment_scores", "for", "f", "in", "eval_features", "]", "\n", "\n", "\n", "if", "self", ".", "args", ".", "is_hierarchical", ":", "\n", "            ", "pad_input_matrix", "(", "unpadded_input_ids", ",", "self", ".", "args", ".", "max_doc_length", ")", "\n", "pad_input_matrix", "(", "unpadded_input_mask", ",", "self", ".", "args", ".", "max_doc_length", ")", "\n", "pad_input_matrix", "(", "unpadded_segment_ids", ",", "self", ".", "args", ".", "max_doc_length", ")", "\n", "\n", "", "padded_input_ids", "=", "torch", ".", "tensor", "(", "unpadded_input_ids", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "padded_input_mask", "=", "torch", ".", "tensor", "(", "unpadded_input_mask", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "padded_segment_ids", "=", "torch", ".", "tensor", "(", "unpadded_segment_ids", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "padded_emotion_ids", "=", "torch", ".", "tensor", "(", "unpadded_emotion_scores", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "label_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "label_id", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "eval_data", "=", "TensorDataset", "(", "padded_input_ids", ",", "padded_input_mask", ",", "padded_segment_ids", ",", "padded_emotion_ids", ",", "label_ids", ")", "\n", "eval_sampler", "=", "SequentialSampler", "(", "eval_data", ")", "\n", "eval_dataloader", "=", "DataLoader", "(", "eval_data", ",", "sampler", "=", "eval_sampler", ",", "batch_size", "=", "self", ".", "args", ".", "batch_size", ")", "\n", "\n", "self", ".", "model", ".", "eval", "(", ")", "\n", "\n", "total_loss", "=", "0", "\n", "nb_eval_steps", ",", "nb_eval_examples", "=", "0", ",", "0", "\n", "predicted_labels", ",", "target_labels", "=", "list", "(", ")", ",", "list", "(", ")", "\n", "\n", "for", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "emotion_ids", ",", "label_ids", "in", "tqdm", "(", "eval_dataloader", ",", "desc", "=", "\"Evaluating\"", ",", "disable", "=", "silent", ")", ":", "\n", "            ", "input_ids", "=", "input_ids", ".", "to", "(", "self", ".", "args", ".", "device", ")", "\n", "input_mask", "=", "input_mask", ".", "to", "(", "self", ".", "args", ".", "device", ")", "\n", "segment_ids", "=", "segment_ids", ".", "to", "(", "self", ".", "args", ".", "device", ")", "\n", "emotion_ids", "=", "emotion_ids", ".", "to", "(", "self", ".", "args", ".", "device", ")", "\n", "label_ids", "=", "label_ids", ".", "to", "(", "self", ".", "args", ".", "device", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "if", "return_indices", ":", "\n", "                    ", "outs", "=", "self", ".", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "emotion_ids", "=", "emotion_ids", ",", "return_indices", "=", "return_indices", ")", "\n", "", "else", ":", "\n", "                    ", "outs", "=", "self", ".", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "emotion_ids", "=", "emotion_ids", ")", "\n", "if", "isinstance", "(", "outs", ",", "tuple", ")", ":", "\n", "                        ", "outs", ",", "_", "=", "outs", "\n", "\n", "", "", "", "if", "return_indices", ":", "\n", "                ", "logits", ",", "indices", "=", "outs", "\n", "all_indices", ".", "extend", "(", "indices", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "", "else", ":", "\n", "                ", "logits", "=", "outs", "\n", "\n", "", "if", "self", ".", "args", ".", "is_multilabel", ":", "\n", "                ", "predicted_labels", ".", "extend", "(", "F", ".", "sigmoid", "(", "logits", ")", ".", "round", "(", ")", ".", "long", "(", ")", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "target_labels", ".", "extend", "(", "label_ids", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "loss", "=", "F", ".", "binary_cross_entropy_with_logits", "(", "logits", ",", "label_ids", ".", "float", "(", ")", ",", "size_average", "=", "False", ")", "\n", "average", ",", "average_mac", "=", "'micro'", ",", "'macro'", "\n", "\n", "", "else", ":", "\n", "                ", "predicted_labels", ".", "extend", "(", "torch", ".", "argmax", "(", "logits", ",", "dim", "=", "1", ")", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "target_labels", ".", "extend", "(", "torch", ".", "argmax", "(", "label_ids", ",", "dim", "=", "1", ")", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "loss", "=", "F", ".", "cross_entropy", "(", "logits", ",", "torch", ".", "argmax", "(", "label_ids", ",", "dim", "=", "1", ")", ")", "\n", "average", ",", "average_mac", "=", "'binary'", ",", "'binary'", "\n", "\n", "", "if", "self", ".", "args", ".", "n_gpu", ">", "1", ":", "\n", "                ", "loss", "=", "loss", ".", "mean", "(", ")", "\n", "", "if", "self", ".", "args", ".", "gradient_accumulation_steps", ">", "1", ":", "\n", "                ", "loss", "=", "loss", "/", "self", ".", "args", ".", "gradient_accumulation_steps", "\n", "", "total_loss", "+=", "loss", ".", "item", "(", ")", "\n", "\n", "nb_eval_examples", "+=", "input_ids", ".", "size", "(", "0", ")", "\n", "nb_eval_steps", "+=", "1", "\n", "\n", "", "predicted_labels", ",", "target_labels", "=", "np", ".", "array", "(", "predicted_labels", ")", ",", "np", ".", "array", "(", "target_labels", ")", "\n", "accuracy", "=", "metrics", ".", "accuracy_score", "(", "target_labels", ",", "predicted_labels", ")", "\n", "precision", "=", "metrics", ".", "precision_score", "(", "target_labels", ",", "predicted_labels", ",", "average", "=", "average", ")", "\n", "recall", "=", "metrics", ".", "recall_score", "(", "target_labels", ",", "predicted_labels", ",", "average", "=", "average", ")", "\n", "avg_loss", "=", "total_loss", "/", "nb_eval_steps", "\n", "\n", "hamming_loss", "=", "metrics", ".", "hamming_loss", "(", "target_labels", ",", "predicted_labels", ")", "\n", "jaccard_score", "=", "metrics", ".", "jaccard_score", "(", "target_labels", ",", "predicted_labels", ",", "average", "=", "average", ")", "\n", "f1_micro", "=", "metrics", ".", "f1_score", "(", "target_labels", ",", "predicted_labels", ",", "average", "=", "average", ")", "\n", "f1_macro", "=", "metrics", ".", "f1_score", "(", "target_labels", ",", "predicted_labels", ",", "average", "=", "average_mac", ")", "\n", "\n", "if", "return_indices", ":", "\n", "            ", "return", "[", "accuracy", ",", "precision", ",", "recall", ",", "f1_micro", ",", "avg_loss", ",", "f1_macro", ",", "hamming_loss", ",", "jaccard_score", ",", "predicted_labels", ",", "target_labels", ",", "all_indices", "]", ",", "[", "'accuracy'", ",", "'precision'", ",", "'recall'", ",", "'f1_micro'", ",", "'avg_loss'", ",", "'f1_macro'", ",", "'hamming_loss'", ",", "'jaccard'", ",", "'predicted_labels'", ",", "'target_labels'", ",", "'all_indices'", "]", "\n", "", "else", ":", "\n", "\n", "            ", "return", "[", "accuracy", ",", "precision", ",", "recall", ",", "f1_micro", ",", "avg_loss", ",", "f1_macro", ",", "hamming_loss", ",", "jaccard_score", ",", "predicted_labels", ",", "target_labels", "]", ",", "[", "'accuracy'", ",", "'precision'", ",", "'recall'", ",", "'f1_micro'", ",", "'avg_loss'", ",", "'f1_macro'", ",", "'hamming_loss'", ",", "'jaccard'", ",", "'predicted_labels'", ",", "'target_labels'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.bert_emotion_evaluator.BertEvaluator.get_bert_layers": [[132, 179], ["torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "torch.utils.data.TensorDataset", "torch.utils.data.SequentialSampler", "torch.utils.data.SequentialSampler", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "bert_emotion_evaluator.BertEvaluator.model.eval", "tqdm.tqdm.tqdm", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "datasets.bert_processors.abstract_processor.convert_examples_to_hierarchical_features", "datasets.bert_processors.abstract_processor.convert_examples_to_features_with_emotion", "utils.preprocessing.pad_input_matrix", "utils.preprocessing.pad_input_matrix", "utils.preprocessing.pad_input_matrix", "input_ids.to.to.to", "input_mask.to.to.to", "segment_ids.to.to.to", "emotion_ids.to.to.to", "torch.argmax().cpu().detach().numpy.to", "torch.argmax().cpu().detach().numpy.to", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "bert_emotion_evaluator.BertEvaluator.model.get_bert_embedding", "torch.argmax().cpu().detach().numpy", "torch.argmax().cpu().detach().numpy", "torch.argmax().cpu().detach().numpy", "torch.argmax().cpu().detach().numpy", "torch.stack.extend", "torch.stack.extend", "label_ids_l.extend", "torch.argmax().cpu().detach", "torch.argmax().cpu().detach", "torch.argmax().cpu().detach", "torch.argmax().cpu().detach", "torch.argmax().cpu", "torch.argmax().cpu", "torch.argmax().cpu", "torch.argmax().cpu", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.convert_examples_to_hierarchical_features", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.convert_examples_to_features_with_emotion", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.preprocessing.pad_input_matrix", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.preprocessing.pad_input_matrix", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.preprocessing.pad_input_matrix", "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertForSequenceClassification.get_bert_embedding"], ["", "", "def", "get_bert_layers", "(", "self", ",", "silent", "=", "False", ",", "last_bert_layers", "=", "-", "1", ")", ":", "\n", "        ", "if", "self", ".", "args", ".", "is_hierarchical", ":", "\n", "            ", "eval_features", "=", "convert_examples_to_hierarchical_features", "(", "\n", "self", ".", "eval_examples", ",", "self", ".", "args", ".", "max_seq_length", ",", "self", ".", "tokenizer", ")", "\n", "", "else", ":", "\n", "            ", "eval_features", "=", "convert_examples_to_features_with_emotion", "(", "\n", "self", ".", "eval_examples", ",", "self", ".", "args", ".", "max_seq_length", ",", "self", ".", "tokenizer", ",", "self", ".", "emotioner", ")", "\n", "\n", "", "unpadded_input_ids", "=", "[", "f", ".", "input_ids", "for", "f", "in", "eval_features", "]", "\n", "unpadded_input_mask", "=", "[", "f", ".", "input_mask", "for", "f", "in", "eval_features", "]", "\n", "unpadded_segment_ids", "=", "[", "f", ".", "segment_ids", "for", "f", "in", "eval_features", "]", "\n", "unpadded_emotion_ids", "=", "[", "f", ".", "emotioniment_scores", "for", "f", "in", "eval_features", "]", "\n", "\n", "\n", "if", "self", ".", "args", ".", "is_hierarchical", ":", "\n", "            ", "pad_input_matrix", "(", "unpadded_input_ids", ",", "self", ".", "args", ".", "max_doc_length", ")", "\n", "pad_input_matrix", "(", "unpadded_input_mask", ",", "self", ".", "args", ".", "max_doc_length", ")", "\n", "pad_input_matrix", "(", "unpadded_segment_ids", ",", "self", ".", "args", ".", "max_doc_length", ")", "\n", "\n", "", "padded_input_ids", "=", "torch", ".", "tensor", "(", "unpadded_input_ids", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "padded_input_mask", "=", "torch", ".", "tensor", "(", "unpadded_input_mask", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "padded_segment_ids", "=", "torch", ".", "tensor", "(", "unpadded_segment_ids", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "padded_emotion_ids", "=", "torch", ".", "tensor", "(", "unpadded_emotion_ids", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "label_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "label_id", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "eval_data", "=", "TensorDataset", "(", "padded_input_ids", ",", "padded_input_mask", ",", "padded_segment_ids", ",", "padded_emotion_ids", ",", "label_ids", ")", "\n", "eval_sampler", "=", "SequentialSampler", "(", "eval_data", ")", "\n", "eval_dataloader", "=", "DataLoader", "(", "eval_data", ",", "sampler", "=", "eval_sampler", ",", "batch_size", "=", "self", ".", "args", ".", "batch_size", ")", "\n", "\n", "self", ".", "model", ".", "eval", "(", ")", "\n", "\n", "bert_layers_l", ",", "label_ids_l", "=", "[", "]", ",", "[", "]", "\n", "\n", "for", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "emotion_ids", ",", "label_ids", "in", "tqdm", "(", "eval_dataloader", ",", "desc", "=", "\"Evaluating\"", ",", "disable", "=", "silent", ")", ":", "\n", "            ", "input_ids", "=", "input_ids", ".", "to", "(", "self", ".", "args", ".", "device", ")", "\n", "input_mask", "=", "input_mask", ".", "to", "(", "self", ".", "args", ".", "device", ")", "\n", "segment_ids", "=", "segment_ids", ".", "to", "(", "self", ".", "args", ".", "device", ")", "\n", "emotion_ids", "=", "emotion_ids", ".", "to", "(", "self", ".", "args", ".", "device", ")", "\n", "label_ids", "=", "label_ids", ".", "to", "(", "self", ".", "args", ".", "device", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "bert_layers", "=", "self", ".", "model", ".", "get_bert_embedding", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "emotion_ids", "=", "emotion_ids", ",", "last_bert_layers", "=", "last_bert_layers", ")", "\n", "label_ids", "=", "torch", ".", "argmax", "(", "label_ids", ",", "dim", "=", "1", ")", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "bert_layers_l", ".", "extend", "(", "bert_layers", ")", "\n", "label_ids_l", ".", "extend", "(", "label_ids", ")", "\n", "", "", "bert_layers_l", "=", "torch", ".", "stack", "(", "bert_layers_l", ",", "dim", "=", "0", ")", "\n", "return", "bert_layers_l", ",", "label_ids_l", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.bert_sent_evaluator.BertEvaluator.__init__": [[20, 31], ["utils.tokenization.BertTokenizer.from_pretrained", "bert_sent_evaluator.BertEvaluator.processor.get_test_examples", "bert_sent_evaluator.BertEvaluator.processor.get_dev_examples", "bert_sent_evaluator.BertEvaluator.processor.get_any_examples"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertPreTrainedModel.from_pretrained", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor.get_test_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor.get_dev_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.imdb_processor.IMDBProcessor.get_any_examples"], ["    ", "def", "__init__", "(", "self", ",", "model", ",", "processor", ",", "args", ",", "split", "=", "'dev'", ")", ":", "\n", "        ", "self", ".", "args", "=", "args", "\n", "self", ".", "model", "=", "model", "\n", "self", ".", "processor", "=", "processor", "\n", "self", ".", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "args", ".", "model", ",", "is_lowercase", "=", "args", ".", "is_lowercase", ")", "\n", "if", "split", "==", "'test'", ":", "\n", "            ", "self", ".", "eval_examples", "=", "self", ".", "processor", ".", "get_test_examples", "(", "args", ".", "data_dir", ",", "args", ".", "test_name", ")", "\n", "", "elif", "split", "==", "'dev'", ":", "\n", "            ", "self", ".", "eval_examples", "=", "self", ".", "processor", ".", "get_dev_examples", "(", "args", ".", "data_dir", ",", "args", ".", "dev_name", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "eval_examples", "=", "self", ".", "processor", ".", "get_any_examples", "(", "args", ".", "data_dir", ",", "split", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.bert_sent_evaluator.BertEvaluator.get_scores": [[32, 129], ["torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "torch.utils.data.TensorDataset", "torch.utils.data.SequentialSampler", "torch.utils.data.SequentialSampler", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "bert_sent_evaluator.BertEvaluator.model.eval", "tqdm.tqdm.tqdm", "sklearn.metrics.balanced_accuracy_score", "sklearn.metrics.precision_score", "sklearn.metrics.recall_score", "sklearn.metrics.hamming_loss", "sklearn.metrics.jaccard_score", "sklearn.metrics.f1_score", "sklearn.metrics.f1_score", "datasets.bert_processors.abstract_processor.convert_examples_to_hierarchical_features", "datasets.bert_processors.abstract_processor.convert_examples_to_features_with_sentiment", "utils.preprocessing.pad_input_matrix", "utils.preprocessing.pad_input_matrix", "utils.preprocessing.pad_input_matrix", "list", "list", "input_ids.to.to.to", "input_mask.to.to.to", "segment_ids.to.to.to", "sent_scores.to.to.to", "label_ids.to.to.to", "loss.mean.mean.item", "input_ids.to.to.size", "numpy.array", "numpy.array", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "all_indices.extend", "predicted_labels.extend", "target_labels.extend", "torch.binary_cross_entropy_with_logits", "torch.binary_cross_entropy_with_logits", "predicted_labels.extend", "target_labels.extend", "torch.cross_entropy", "torch.cross_entropy", "loss.mean.mean.mean", "bert_sent_evaluator.BertEvaluator.model", "bert_sent_evaluator.BertEvaluator.model", "isinstance", "indices.cpu().detach().numpy", "torch.sigmoid().round().long().cpu().detach().numpy", "torch.sigmoid().round().long().cpu().detach().numpy", "label_ids.to.to.cpu().detach().numpy", "label_ids.to.to.float", "torch.argmax().cpu().detach().numpy", "torch.argmax().cpu().detach().numpy", "torch.argmax().cpu().detach().numpy", "torch.argmax().cpu().detach().numpy", "torch.argmax().cpu().detach().numpy", "torch.argmax().cpu().detach().numpy", "torch.argmax().cpu().detach().numpy", "torch.argmax().cpu().detach().numpy", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "indices.cpu().detach", "torch.sigmoid().round().long().cpu().detach", "torch.sigmoid().round().long().cpu().detach", "label_ids.to.to.cpu().detach", "torch.argmax().cpu().detach", "torch.argmax().cpu().detach", "torch.argmax().cpu().detach", "torch.argmax().cpu().detach", "torch.argmax().cpu().detach", "torch.argmax().cpu().detach", "torch.argmax().cpu().detach", "torch.argmax().cpu().detach", "indices.cpu", "torch.sigmoid().round().long().cpu", "torch.sigmoid().round().long().cpu", "label_ids.to.to.cpu", "torch.argmax().cpu", "torch.argmax().cpu", "torch.argmax().cpu", "torch.argmax().cpu", "torch.argmax().cpu", "torch.argmax().cpu", "torch.argmax().cpu", "torch.argmax().cpu", "torch.sigmoid().round().long", "torch.sigmoid().round().long", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.sigmoid().round", "torch.sigmoid().round", "torch.sigmoid", "torch.sigmoid"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.convert_examples_to_hierarchical_features", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.convert_examples_to_features_with_sentiment", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.preprocessing.pad_input_matrix", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.preprocessing.pad_input_matrix", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.preprocessing.pad_input_matrix"], ["", "", "def", "get_scores", "(", "self", ",", "silent", "=", "False", ",", "return_indices", "=", "False", ")", ":", "\n", "        ", "all_indices", "=", "[", "]", "\n", "if", "self", ".", "args", ".", "is_hierarchical", ":", "\n", "            ", "eval_features", "=", "convert_examples_to_hierarchical_features", "(", "\n", "self", ".", "eval_examples", ",", "self", ".", "args", ".", "max_seq_length", ",", "self", ".", "tokenizer", ")", "\n", "", "else", ":", "\n", "            ", "eval_features", "=", "convert_examples_to_features_with_sentiment", "(", "\n", "self", ".", "eval_examples", ",", "self", ".", "args", ".", "max_seq_length", ",", "self", ".", "tokenizer", ",", "overal_sent", "=", "self", ".", "args", ".", "overal_sent", ")", "\n", "\n", "", "unpadded_input_ids", "=", "[", "f", ".", "input_ids", "for", "f", "in", "eval_features", "]", "\n", "unpadded_input_mask", "=", "[", "f", ".", "input_mask", "for", "f", "in", "eval_features", "]", "\n", "unpadded_segment_ids", "=", "[", "f", ".", "segment_ids", "for", "f", "in", "eval_features", "]", "\n", "unpadded_sent_scores", "=", "[", "f", ".", "sentiment_scores", "for", "f", "in", "eval_features", "]", "\n", "\n", "\n", "if", "self", ".", "args", ".", "is_hierarchical", ":", "\n", "            ", "pad_input_matrix", "(", "unpadded_input_ids", ",", "self", ".", "args", ".", "max_doc_length", ")", "\n", "pad_input_matrix", "(", "unpadded_input_mask", ",", "self", ".", "args", ".", "max_doc_length", ")", "\n", "pad_input_matrix", "(", "unpadded_segment_ids", ",", "self", ".", "args", ".", "max_doc_length", ")", "\n", "\n", "", "padded_input_ids", "=", "torch", ".", "tensor", "(", "unpadded_input_ids", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "padded_input_mask", "=", "torch", ".", "tensor", "(", "unpadded_input_mask", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "padded_segment_ids", "=", "torch", ".", "tensor", "(", "unpadded_segment_ids", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "sent_scores", "=", "torch", ".", "tensor", "(", "unpadded_sent_scores", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "label_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "label_id", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "eval_data", "=", "TensorDataset", "(", "padded_input_ids", ",", "padded_input_mask", ",", "padded_segment_ids", ",", "sent_scores", ",", "label_ids", ")", "\n", "eval_sampler", "=", "SequentialSampler", "(", "eval_data", ")", "\n", "eval_dataloader", "=", "DataLoader", "(", "eval_data", ",", "sampler", "=", "eval_sampler", ",", "batch_size", "=", "self", ".", "args", ".", "batch_size", ")", "\n", "\n", "self", ".", "model", ".", "eval", "(", ")", "\n", "\n", "total_loss", "=", "0", "\n", "nb_eval_steps", ",", "nb_eval_examples", "=", "0", ",", "0", "\n", "predicted_labels", ",", "target_labels", "=", "list", "(", ")", ",", "list", "(", ")", "\n", "\n", "for", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "sent_scores", ",", "label_ids", "in", "tqdm", "(", "eval_dataloader", ",", "desc", "=", "\"Evaluating\"", ",", "disable", "=", "silent", ")", ":", "\n", "            ", "input_ids", "=", "input_ids", ".", "to", "(", "self", ".", "args", ".", "device", ")", "\n", "input_mask", "=", "input_mask", ".", "to", "(", "self", ".", "args", ".", "device", ")", "\n", "segment_ids", "=", "segment_ids", ".", "to", "(", "self", ".", "args", ".", "device", ")", "\n", "sent_scores", "=", "sent_scores", ".", "to", "(", "self", ".", "args", ".", "device", ")", "\n", "label_ids", "=", "label_ids", ".", "to", "(", "self", ".", "args", ".", "device", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "if", "return_indices", ":", "\n", "                    ", "outs", "=", "self", ".", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "sent_scores", "=", "sent_scores", ",", "return_indices", "=", "return_indices", ")", "\n", "", "else", ":", "\n", "                    ", "outs", "=", "self", ".", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "sent_scores", "=", "sent_scores", ")", "\n", "if", "isinstance", "(", "outs", ",", "tuple", ")", ":", "\n", "                        ", "outs", ",", "_", "=", "outs", "\n", "\n", "", "", "", "if", "return_indices", ":", "\n", "                ", "logits", ",", "indices", "=", "outs", "\n", "all_indices", ".", "extend", "(", "indices", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "", "else", ":", "\n", "                ", "logits", "=", "outs", "\n", "\n", "", "if", "self", ".", "args", ".", "is_multilabel", ":", "\n", "                ", "predicted_labels", ".", "extend", "(", "F", ".", "sigmoid", "(", "logits", ")", ".", "round", "(", ")", ".", "long", "(", ")", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "target_labels", ".", "extend", "(", "label_ids", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "loss", "=", "F", ".", "binary_cross_entropy_with_logits", "(", "logits", ",", "label_ids", ".", "float", "(", ")", ",", "size_average", "=", "False", ")", "\n", "average", ",", "average_mac", "=", "'micro'", ",", "'macro'", "\n", "\n", "", "else", ":", "\n", "                ", "predicted_labels", ".", "extend", "(", "torch", ".", "argmax", "(", "logits", ",", "dim", "=", "1", ")", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "target_labels", ".", "extend", "(", "torch", ".", "argmax", "(", "label_ids", ",", "dim", "=", "1", ")", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "loss", "=", "F", ".", "cross_entropy", "(", "logits", ",", "torch", ".", "argmax", "(", "label_ids", ",", "dim", "=", "1", ")", ")", "\n", "average", ",", "average_mac", "=", "'binary'", ",", "'binary'", "\n", "\n", "", "if", "self", ".", "args", ".", "n_gpu", ">", "1", ":", "\n", "                ", "loss", "=", "loss", ".", "mean", "(", ")", "\n", "", "if", "self", ".", "args", ".", "gradient_accumulation_steps", ">", "1", ":", "\n", "                ", "loss", "=", "loss", "/", "self", ".", "args", ".", "gradient_accumulation_steps", "\n", "", "total_loss", "+=", "loss", ".", "item", "(", ")", "\n", "\n", "nb_eval_examples", "+=", "input_ids", ".", "size", "(", "0", ")", "\n", "nb_eval_steps", "+=", "1", "\n", "\n", "", "predicted_labels", ",", "target_labels", "=", "np", ".", "array", "(", "predicted_labels", ")", ",", "np", ".", "array", "(", "target_labels", ")", "\n", "#accuracy = metrics.accuracy_score(target_labels, predicted_labels)", "\n", "accuracy", "=", "metrics", ".", "balanced_accuracy_score", "(", "target_labels", ",", "predicted_labels", ")", "\n", "precision", "=", "metrics", ".", "precision_score", "(", "target_labels", ",", "predicted_labels", ",", "average", "=", "average", ")", "\n", "recall", "=", "metrics", ".", "recall_score", "(", "target_labels", ",", "predicted_labels", ",", "average", "=", "average", ")", "\n", "avg_loss", "=", "total_loss", "/", "nb_eval_steps", "\n", "\n", "hamming_loss", "=", "metrics", ".", "hamming_loss", "(", "target_labels", ",", "predicted_labels", ")", "\n", "jaccard_score", "=", "metrics", ".", "jaccard_score", "(", "target_labels", ",", "predicted_labels", ",", "average", "=", "average", ")", "\n", "f1_micro", "=", "metrics", ".", "f1_score", "(", "target_labels", ",", "predicted_labels", ",", "average", "=", "average", ")", "\n", "f1_macro", "=", "metrics", ".", "f1_score", "(", "target_labels", ",", "predicted_labels", ",", "average", "=", "average_mac", ")", "\n", "\n", "if", "return_indices", ":", "\n", "            ", "return", "[", "accuracy", ",", "precision", ",", "recall", ",", "f1_micro", ",", "avg_loss", ",", "f1_macro", ",", "hamming_loss", ",", "jaccard_score", ",", "predicted_labels", ",", "target_labels", ",", "all_indices", "]", ",", "[", "'accuracy'", ",", "'precision'", ",", "'recall'", ",", "'f1_micro'", ",", "'avg_loss'", ",", "'f1_macro'", ",", "'hamming_loss'", ",", "'jaccard'", ",", "'predicted_labels'", ",", "'target_labels'", ",", "'all_indices'", "]", "\n", "", "else", ":", "\n", "\n", "            ", "return", "[", "accuracy", ",", "precision", ",", "recall", ",", "f1_micro", ",", "avg_loss", ",", "f1_macro", ",", "hamming_loss", ",", "jaccard_score", ",", "predicted_labels", ",", "target_labels", "]", ",", "[", "'accuracy'", ",", "'precision'", ",", "'recall'", ",", "'f1_micro'", ",", "'avg_loss'", ",", "'f1_macro'", ",", "'hamming_loss'", ",", "'jaccard'", ",", "'predicted_labels'", ",", "'target_labels'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.relevance_transfer_evaluator.RelevanceTransferEvaluator.__init__": [[21, 39], ["common.evaluators.evaluator.Evaluator.__init__", "utils.tokenization.BertTokenizer.from_pretrained", "relevance_transfer_evaluator.RelevanceTransferEvaluator.processor.get_test_examples", "relevance_transfer_evaluator.RelevanceTransferEvaluator.processor.get_dev_examples"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.classification_evaluator.ClassificationEvaluator.__init__", "home.repos.pwc.inspect_result.marjanhs_procon20.bert.model.BertPreTrainedModel.from_pretrained", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor.get_test_examples", "home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.BertProcessor.get_dev_examples"], ["    ", "def", "__init__", "(", "self", ",", "model", ",", "config", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "kwargs", "[", "'dataset'", "]", ",", "model", ",", "kwargs", "[", "'embedding'", "]", ",", "kwargs", "[", "'data_loader'", "]", ",", "\n", "batch_size", "=", "config", "[", "'batch_size'", "]", ",", "device", "=", "config", "[", "'device'", "]", ")", "\n", "\n", "if", "config", "[", "'model'", "]", "in", "{", "'BERT-Base'", ",", "'BERT-Large'", ",", "'HBERT-Base'", ",", "'HBERT-Large'", "}", ":", "\n", "            ", "variant", "=", "'bert-large-uncased'", "if", "config", "[", "'model'", "]", "==", "'BERT-Large'", "else", "'bert-base-uncased'", "\n", "self", ".", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "variant", ",", "is_lowercase", "=", "config", "[", "'is_lowercase'", "]", ")", "\n", "self", ".", "processor", "=", "kwargs", "[", "'processor'", "]", "\n", "if", "config", "[", "'split'", "]", "==", "'test'", ":", "\n", "                ", "self", ".", "eval_examples", "=", "self", ".", "processor", ".", "get_test_examples", "(", "config", "[", "'data_dir'", "]", ",", "topic", "=", "config", "[", "'topic'", "]", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "eval_examples", "=", "self", ".", "processor", ".", "get_dev_examples", "(", "config", "[", "'data_dir'", "]", ",", "topic", "=", "config", "[", "'topic'", "]", ")", "\n", "\n", "", "", "self", ".", "config", "=", "config", "\n", "self", ".", "ignore_lengths", "=", "config", "[", "'ignore_lengths'", "]", "\n", "self", ".", "y_target", "=", "None", "\n", "self", ".", "y_pred", "=", "None", "\n", "self", ".", "docid", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.relevance_transfer_evaluator.RelevanceTransferEvaluator.get_scores": [[40, 143], ["relevance_transfer_evaluator.RelevanceTransferEvaluator.model.eval", "list", "list", "list", "numpy.around", "numpy.array", "sklearn.metrics.accuracy_score", "sklearn.metrics.average_precision_score", "sklearn.metrics.f1_score", "datasets.bert_processors.robust45_processor.convert_examples_to_features", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "torch.utils.data.TensorDataset", "torch.utils.data.SequentialSampler", "torch.utils.data.SequentialSampler", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "tqdm.tqdm.tqdm", "relevance_transfer_evaluator.RelevanceTransferEvaluator.data_loader.init_epoch", "tqdm.tqdm.tqdm", "numpy.array", "len", "utils.preprocessing.pad_input_matrix", "utils.preprocessing.pad_input_matrix", "utils.preprocessing.pad_input_matrix", "input_ids.to.to.to", "input_mask.to.to.to", "segment_ids.to.to.to", "label_ids.to.to.to", "relevance_transfer_evaluator.RelevanceTransferEvaluator.docid.extend", "relevance_transfer_evaluator.RelevanceTransferEvaluator.y_pred.extend", "relevance_transfer_evaluator.RelevanceTransferEvaluator.y_target.extend", "torch.binary_cross_entropy", "torch.binary_cross_entropy", "loss.mean.mean.item", "hasattr", "relevance_transfer_evaluator.RelevanceTransferEvaluator.model.get_params", "relevance_transfer_evaluator.RelevanceTransferEvaluator.model.load_ema_params", "torch.binary_cross_entropy().item", "torch.binary_cross_entropy().item", "relevance_transfer_evaluator.RelevanceTransferEvaluator.docid.extend", "relevance_transfer_evaluator.RelevanceTransferEvaluator.y_pred.extend", "relevance_transfer_evaluator.RelevanceTransferEvaluator.y_target.extend", "hasattr", "relevance_transfer_evaluator.RelevanceTransferEvaluator.model.load_params", "sklearn.metrics.precision_score", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.sigmoid().squeeze", "torch.sigmoid().squeeze", "torch.sigmoid().squeeze", "torch.sigmoid().squeeze", "torch.tensor.cpu().detach().numpy", "torch.tensor.cpu().detach().numpy", "torch.sigmoid().squeeze.cpu().detach().numpy", "torch.sigmoid().squeeze.cpu().detach().numpy", "label_ids.to.to.cpu().detach().numpy", "label_ids.to.to.float", "loss.mean.mean.mean", "hasattr", "hasattr", "batch.docid.cpu().detach().numpy", "torch.sigmoid().squeeze.cpu().detach().numpy", "torch.sigmoid().squeeze.cpu().detach().numpy", "batch.label.cpu().detach().numpy", "torch.sigmoid().squeeze", "torch.sigmoid().squeeze", "torch.sigmoid().squeeze", "torch.sigmoid().squeeze", "torch.sigmoid().squeeze", "torch.sigmoid().squeeze", "torch.sigmoid().squeeze", "torch.sigmoid().squeeze", "torch.sigmoid().squeeze", "torch.sigmoid().squeeze", "torch.sigmoid().squeeze", "torch.sigmoid().squeeze", "torch.sigmoid().squeeze", "torch.sigmoid().squeeze", "torch.sigmoid().squeeze", "torch.sigmoid().squeeze", "torch.binary_cross_entropy", "torch.binary_cross_entropy", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.tensor.cpu().detach", "torch.tensor.cpu().detach", "torch.sigmoid().squeeze.cpu().detach", "torch.sigmoid().squeeze.cpu().detach", "label_ids.to.to.cpu().detach", "batch.label.float", "batch.docid.cpu().detach", "torch.sigmoid().squeeze.cpu().detach", "torch.sigmoid().squeeze.cpu().detach", "batch.label.cpu().detach", "relevance_transfer_evaluator.RelevanceTransferEvaluator.model", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.tensor.cpu", "torch.tensor.cpu", "torch.sigmoid().squeeze.cpu", "torch.sigmoid().squeeze.cpu", "label_ids.to.to.cpu", "relevance_transfer_evaluator.RelevanceTransferEvaluator.model", "relevance_transfer_evaluator.RelevanceTransferEvaluator.model", "relevance_transfer_evaluator.RelevanceTransferEvaluator.model", "relevance_transfer_evaluator.RelevanceTransferEvaluator.model", "batch.docid.cpu", "torch.sigmoid().squeeze.cpu", "torch.sigmoid().squeeze.cpu", "batch.label.cpu"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.bert_processors.abstract_processor.convert_examples_to_features", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.preprocessing.pad_input_matrix", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.preprocessing.pad_input_matrix", "home.repos.pwc.inspect_result.marjanhs_procon20.utils.preprocessing.pad_input_matrix"], ["", "def", "get_scores", "(", "self", ",", "silent", "=", "False", ")", ":", "\n", "        ", "self", ".", "model", ".", "eval", "(", ")", "\n", "self", ".", "y_target", "=", "list", "(", ")", "\n", "self", ".", "y_pred", "=", "list", "(", ")", "\n", "self", ".", "docid", "=", "list", "(", ")", "\n", "total_loss", "=", "0", "\n", "\n", "if", "self", ".", "config", "[", "'model'", "]", "in", "{", "'BERT-Base'", ",", "'BERT-Large'", ",", "'HBERT-Base'", ",", "'HBERT-Large'", "}", ":", "\n", "            ", "eval_features", "=", "convert_examples_to_features", "(", "\n", "self", ".", "eval_examples", ",", "\n", "self", ".", "config", "[", "'max_seq_length'", "]", ",", "\n", "self", ".", "tokenizer", ",", "\n", "self", ".", "config", "[", "'is_hierarchical'", "]", "\n", ")", "\n", "\n", "unpadded_input_ids", "=", "[", "f", ".", "input_ids", "for", "f", "in", "eval_features", "]", "\n", "unpadded_input_mask", "=", "[", "f", ".", "input_mask", "for", "f", "in", "eval_features", "]", "\n", "unpadded_segment_ids", "=", "[", "f", ".", "segment_ids", "for", "f", "in", "eval_features", "]", "\n", "\n", "if", "self", ".", "config", "[", "'is_hierarchical'", "]", ":", "\n", "                ", "pad_input_matrix", "(", "unpadded_input_ids", ",", "self", ".", "config", "[", "'max_doc_length'", "]", ")", "\n", "pad_input_matrix", "(", "unpadded_input_mask", ",", "self", ".", "config", "[", "'max_doc_length'", "]", ")", "\n", "pad_input_matrix", "(", "unpadded_segment_ids", ",", "self", ".", "config", "[", "'max_doc_length'", "]", ")", "\n", "\n", "", "padded_input_ids", "=", "torch", ".", "tensor", "(", "unpadded_input_ids", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "padded_input_mask", "=", "torch", ".", "tensor", "(", "unpadded_input_mask", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "padded_segment_ids", "=", "torch", ".", "tensor", "(", "unpadded_segment_ids", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "label_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "label_id", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "document_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "guid", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "eval_data", "=", "TensorDataset", "(", "padded_input_ids", ",", "padded_input_mask", ",", "padded_segment_ids", ",", "label_ids", ",", "document_ids", ")", "\n", "eval_sampler", "=", "SequentialSampler", "(", "eval_data", ")", "\n", "eval_dataloader", "=", "DataLoader", "(", "eval_data", ",", "sampler", "=", "eval_sampler", ",", "batch_size", "=", "self", ".", "config", "[", "'batch_size'", "]", ")", "\n", "\n", "for", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "label_ids", ",", "document_ids", "in", "tqdm", "(", "eval_dataloader", ",", "desc", "=", "\"Evaluating\"", ",", "disable", "=", "silent", ")", ":", "\n", "                ", "input_ids", "=", "input_ids", ".", "to", "(", "self", ".", "config", "[", "'device'", "]", ")", "\n", "input_mask", "=", "input_mask", ".", "to", "(", "self", ".", "config", "[", "'device'", "]", ")", "\n", "segment_ids", "=", "segment_ids", ".", "to", "(", "self", ".", "config", "[", "'device'", "]", ")", "\n", "label_ids", "=", "label_ids", ".", "to", "(", "self", ".", "config", "[", "'device'", "]", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                    ", "logits", "=", "torch", ".", "sigmoid", "(", "self", ".", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ")", ")", ".", "squeeze", "(", "dim", "=", "1", ")", "\n", "\n", "# Computing loss and storing predictions", "\n", "", "self", ".", "docid", ".", "extend", "(", "document_ids", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "self", ".", "y_pred", ".", "extend", "(", "logits", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "self", ".", "y_target", ".", "extend", "(", "label_ids", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "loss", "=", "F", ".", "binary_cross_entropy", "(", "logits", ",", "label_ids", ".", "float", "(", ")", ")", "\n", "\n", "if", "self", ".", "config", "[", "'n_gpu'", "]", ">", "1", ":", "\n", "                    ", "loss", "=", "loss", ".", "mean", "(", ")", "\n", "", "if", "self", ".", "config", "[", "'gradient_accumulation_steps'", "]", ">", "1", ":", "\n", "                    ", "loss", "=", "loss", "/", "self", ".", "config", "[", "'gradient_accumulation_steps'", "]", "\n", "", "total_loss", "+=", "loss", ".", "item", "(", ")", "\n", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "data_loader", ".", "init_epoch", "(", ")", "\n", "\n", "if", "hasattr", "(", "self", ".", "model", ",", "'beta_ema'", ")", "and", "self", ".", "model", ".", "beta_ema", ">", "0", ":", "\n", "# Temporal averaging", "\n", "                ", "old_params", "=", "self", ".", "model", ".", "get_params", "(", ")", "\n", "self", ".", "model", ".", "load_ema_params", "(", ")", "\n", "\n", "", "for", "batch", "in", "tqdm", "(", "self", ".", "data_loader", ",", "desc", "=", "\"Evaluating\"", ",", "disable", "=", "silent", ")", ":", "\n", "                ", "if", "hasattr", "(", "self", ".", "model", ",", "'tar'", ")", "and", "self", ".", "model", ".", "tar", ":", "\n", "                    ", "if", "self", ".", "ignore_lengths", ":", "\n", "                        ", "logits", ",", "rnn_outs", "=", "torch", ".", "sigmoid", "(", "self", ".", "model", "(", "batch", ".", "text", ")", ")", ".", "squeeze", "(", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "                        ", "logits", ",", "rnn_outs", "=", "torch", ".", "sigmoid", "(", "self", ".", "model", "(", "batch", ".", "text", "[", "0", "]", ",", "lengths", "=", "batch", ".", "text", "[", "1", "]", ")", ")", ".", "squeeze", "(", "dim", "=", "1", ")", "\n", "", "", "else", ":", "\n", "                    ", "if", "self", ".", "ignore_lengths", ":", "\n", "                        ", "logits", "=", "torch", ".", "sigmoid", "(", "self", ".", "model", "(", "batch", ".", "text", ")", ")", ".", "squeeze", "(", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "                        ", "logits", "=", "torch", ".", "sigmoid", "(", "self", ".", "model", "(", "batch", ".", "text", "[", "0", "]", ",", "lengths", "=", "batch", ".", "text", "[", "1", "]", ")", ")", ".", "squeeze", "(", "dim", "=", "1", ")", "\n", "\n", "", "", "total_loss", "+=", "F", ".", "binary_cross_entropy", "(", "logits", ",", "batch", ".", "label", ".", "float", "(", ")", ")", ".", "item", "(", ")", "\n", "if", "hasattr", "(", "self", ".", "model", ",", "'tar'", ")", "and", "self", ".", "model", ".", "tar", ":", "\n", "# Temporal activation regularization", "\n", "                    ", "total_loss", "+=", "(", "rnn_outs", "[", "1", ":", "]", "-", "rnn_outs", "[", ":", "-", "1", "]", ")", ".", "pow", "(", "2", ")", ".", "mean", "(", ")", "\n", "\n", "", "self", ".", "docid", ".", "extend", "(", "batch", ".", "docid", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "self", ".", "y_pred", ".", "extend", "(", "logits", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "self", ".", "y_target", ".", "extend", "(", "batch", ".", "label", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "\n", "", "if", "hasattr", "(", "self", ".", "model", ",", "'beta_ema'", ")", "and", "self", ".", "model", ".", "beta_ema", ">", "0", ":", "\n", "# Temporal averaging", "\n", "                ", "self", ".", "model", ".", "load_params", "(", "old_params", ")", "\n", "\n", "", "", "predicted_labels", "=", "np", ".", "around", "(", "np", ".", "array", "(", "self", ".", "y_pred", ")", ")", "\n", "target_labels", "=", "np", ".", "array", "(", "self", ".", "y_target", ")", "\n", "accuracy", "=", "metrics", ".", "accuracy_score", "(", "target_labels", ",", "predicted_labels", ")", "\n", "average_precision", "=", "metrics", ".", "average_precision_score", "(", "target_labels", ",", "predicted_labels", ",", "average", "=", "None", ")", "\n", "f1", "=", "metrics", ".", "f1_score", "(", "target_labels", ",", "predicted_labels", ",", "average", "=", "'macro'", ")", "\n", "avg_loss", "=", "total_loss", "/", "len", "(", "predicted_labels", ")", "\n", "\n", "try", ":", "\n", "            ", "precision", "=", "metrics", ".", "precision_score", "(", "target_labels", ",", "predicted_labels", ",", "average", "=", "None", ")", "[", "1", "]", "\n", "", "except", "IndexError", ":", "\n", "# Handle cases without positive labels", "\n", "            ", "precision", "=", "0", "\n", "\n", "", "return", "[", "accuracy", ",", "precision", ",", "average_precision", ",", "f1", ",", "avg_loss", "]", ",", "[", "'accuracy'", ",", "'precision'", ",", "'average_precision'", ",", "'f1'", ",", "'cross_entropy_loss'", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.evaluator.Evaluator.__init__": [[6, 14], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "dataset_cls", ",", "model", ",", "embedding", ",", "data_loader", ",", "batch_size", ",", "device", ",", "keep_results", "=", "False", ")", ":", "\n", "        ", "self", ".", "dataset_cls", "=", "dataset_cls", "\n", "self", ".", "model", "=", "model", "\n", "self", ".", "embedding", "=", "embedding", "\n", "self", ".", "data_loader", "=", "data_loader", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "device", "=", "device", "\n", "self", ".", "keep_results", "=", "keep_results", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.evaluator.Evaluator.get_sentence_embeddings": [[15, 19], ["evaluator.Evaluator.embedding().transpose", "evaluator.Evaluator.embedding().transpose", "evaluator.Evaluator.embedding", "evaluator.Evaluator.embedding"], "methods", ["None"], ["", "def", "get_sentence_embeddings", "(", "self", ",", "batch", ")", ":", "\n", "        ", "sent1", "=", "self", ".", "embedding", "(", "batch", ".", "sentence_1", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "sent2", "=", "self", ".", "embedding", "(", "batch", ".", "sentence_2", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "return", "sent1", ",", "sent2", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.evaluator.Evaluator.get_scores": [[20, 27], ["NotImplementedError"], "methods", ["None"], ["", "def", "get_scores", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Get the scores used to evaluate the model.\n        Should return ([score1, score2, ..], [score1_name, score2_name, ...]).\n        The first score is the primary score used to determine if the model has improved.\n        \"\"\"", "\n", "raise", "NotImplementedError", "(", "'Evaluator subclass needs to implement get_score'", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.classification_evaluator.ClassificationEvaluator.__init__": [[11, 15], ["common.evaluators.evaluator.Evaluator.__init__"], "methods", ["home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.classification_evaluator.ClassificationEvaluator.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dataset_cls", ",", "model", ",", "embedding", ",", "data_loader", ",", "batch_size", ",", "device", ",", "keep_results", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dataset_cls", ",", "model", ",", "embedding", ",", "data_loader", ",", "batch_size", ",", "device", ",", "keep_results", ")", "\n", "self", ".", "ignore_lengths", "=", "False", "\n", "self", ".", "is_multilabel", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.marjanhs_procon20.evaluators.classification_evaluator.ClassificationEvaluator.get_scores": [[16, 79], ["classification_evaluator.ClassificationEvaluator.model.eval", "classification_evaluator.ClassificationEvaluator.data_loader.init_epoch", "numpy.array", "numpy.array", "sklearn.metrics.balanced_accuracy_score", "sklearn.metrics.precision_score", "sklearn.metrics.recall_score", "sklearn.metrics.hamming_loss", "sklearn.metrics.jaccard_score", "sklearn.metrics.f1_score", "sklearn.metrics.f1_score", "hasattr", "classification_evaluator.ClassificationEvaluator.model.get_params", "classification_evaluator.ClassificationEvaluator.model.load_ema_params", "list", "list", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "enumerate", "len", "hasattr", "classification_evaluator.ClassificationEvaluator.model.load_params", "hasattr", "torch.sigmoid().round().long", "torch.sigmoid().round().long", "numpy.array.extend", "numpy.array.extend", "torch.binary_cross_entropy_with_logits().item", "torch.binary_cross_entropy_with_logits().item", "numpy.array.extend", "numpy.array.extend", "torch.cross_entropy().item", "torch.cross_entropy().item", "hasattr", "classification_evaluator.ClassificationEvaluator.model", "classification_evaluator.ClassificationEvaluator.model", "classification_evaluator.ClassificationEvaluator.model", "classification_evaluator.ClassificationEvaluator.model", "torch.sigmoid().round().long.cpu().detach().numpy", "batch.label.cpu().detach().numpy", "torch.argmax().cpu().detach().numpy", "torch.argmax().cpu().detach().numpy", "torch.argmax().cpu().detach().numpy", "torch.argmax().cpu().detach().numpy", "torch.argmax().cpu().detach().numpy", "torch.argmax().cpu().detach().numpy", "torch.argmax().cpu().detach().numpy", "torch.argmax().cpu().detach().numpy", "torch.sigmoid().round", "torch.sigmoid().round", "torch.binary_cross_entropy_with_logits", "torch.binary_cross_entropy_with_logits", "torch.cross_entropy", "torch.cross_entropy", "torch.sigmoid().round().long.cpu().detach", "batch.label.cpu().detach", "batch.label.float", "torch.argmax().cpu().detach", "torch.argmax().cpu().detach", "torch.argmax().cpu().detach", "torch.argmax().cpu().detach", "torch.argmax().cpu().detach", "torch.argmax().cpu().detach", "torch.argmax().cpu().detach", "torch.argmax().cpu().detach", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid().round().long.cpu", "batch.label.cpu", "torch.argmax().cpu", "torch.argmax().cpu", "torch.argmax().cpu", "torch.argmax().cpu", "torch.argmax().cpu", "torch.argmax().cpu", "torch.argmax().cpu", "torch.argmax().cpu", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax"], "methods", ["None"], ["", "def", "get_scores", "(", "self", ")", ":", "\n", "        ", "self", ".", "model", ".", "eval", "(", ")", "\n", "self", ".", "data_loader", ".", "init_epoch", "(", ")", "\n", "total_loss", "=", "0", "\n", "\n", "if", "hasattr", "(", "self", ".", "model", ",", "'beta_ema'", ")", "and", "self", ".", "model", ".", "beta_ema", ">", "0", ":", "\n", "# Temporal averaging", "\n", "            ", "old_params", "=", "self", ".", "model", ".", "get_params", "(", ")", "\n", "self", ".", "model", ".", "load_ema_params", "(", ")", "\n", "\n", "", "predicted_labels", ",", "target_labels", "=", "list", "(", ")", ",", "list", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "batch_idx", ",", "batch", "in", "enumerate", "(", "self", ".", "data_loader", ")", ":", "\n", "                ", "if", "hasattr", "(", "self", ".", "model", ",", "'tar'", ")", "and", "self", ".", "model", ".", "tar", ":", "\n", "                    ", "if", "self", ".", "ignore_lengths", ":", "\n", "                        ", "scores", ",", "rnn_outs", "=", "self", ".", "model", "(", "batch", ".", "text", ")", "\n", "", "else", ":", "\n", "                        ", "scores", ",", "rnn_outs", "=", "self", ".", "model", "(", "batch", ".", "text", "[", "0", "]", ",", "lengths", "=", "batch", ".", "text", "[", "1", "]", ")", "\n", "", "", "else", ":", "\n", "                    ", "if", "self", ".", "ignore_lengths", ":", "\n", "                        ", "scores", "=", "self", ".", "model", "(", "batch", ".", "text", ")", "\n", "", "else", ":", "\n", "                        ", "scores", "=", "self", ".", "model", "(", "batch", ".", "text", "[", "0", "]", ",", "lengths", "=", "batch", ".", "text", "[", "1", "]", ")", "\n", "\n", "", "", "if", "self", ".", "is_multilabel", ":", "\n", "                    ", "scores_rounded", "=", "F", ".", "sigmoid", "(", "scores", ")", ".", "round", "(", ")", ".", "long", "(", ")", "\n", "predicted_labels", ".", "extend", "(", "scores_rounded", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "target_labels", ".", "extend", "(", "batch", ".", "label", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "total_loss", "+=", "F", ".", "binary_cross_entropy_with_logits", "(", "scores", ",", "batch", ".", "label", ".", "float", "(", ")", ",", "size_average", "=", "False", ")", ".", "item", "(", ")", "\n", "average", ",", "average_mac", "=", "'micro'", ",", "'macro'", "\n", "", "else", ":", "\n", "                    ", "predicted_labels", ".", "extend", "(", "torch", ".", "argmax", "(", "scores", ",", "dim", "=", "1", ")", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "target_labels", ".", "extend", "(", "torch", ".", "argmax", "(", "batch", ".", "label", ",", "dim", "=", "1", ")", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "total_loss", "+=", "F", ".", "cross_entropy", "(", "scores", ",", "torch", ".", "argmax", "(", "batch", ".", "label", ",", "dim", "=", "1", ")", ",", "size_average", "=", "False", ")", ".", "item", "(", ")", "\n", "average", ",", "average_mac", "=", "'binary'", ",", "'binary'", "\n", "\n", "", "if", "hasattr", "(", "self", ".", "model", ",", "'tar'", ")", "and", "self", ".", "model", ".", "tar", ":", "\n", "# Temporal activation regularization", "\n", "                    ", "total_loss", "+=", "(", "rnn_outs", "[", "1", ":", "]", "-", "rnn_outs", "[", ":", "-", "1", "]", ")", ".", "pow", "(", "2", ")", ".", "mean", "(", ")", "\n", "\n", "", "", "", "predicted_labels", "=", "np", ".", "array", "(", "predicted_labels", ")", "\n", "target_labels", "=", "np", ".", "array", "(", "target_labels", ")", "\n", "#accuracy = metrics.accuracy_score(target_labels, predicted_labels)", "\n", "accuracy", "=", "metrics", ".", "balanced_accuracy_score", "(", "target_labels", ",", "predicted_labels", ")", "\n", "precision", "=", "metrics", ".", "precision_score", "(", "target_labels", ",", "predicted_labels", ",", "average", "=", "average", ")", "\n", "recall", "=", "metrics", ".", "recall_score", "(", "target_labels", ",", "predicted_labels", ",", "average", "=", "average", ")", "\n", "\n", "hamming_loss", "=", "metrics", ".", "hamming_loss", "(", "target_labels", ",", "predicted_labels", ")", "\n", "jaccard_score", "=", "metrics", ".", "jaccard_score", "(", "target_labels", ",", "predicted_labels", ",", "average", "=", "average", ")", "\n", "f1_micro", "=", "metrics", ".", "f1_score", "(", "target_labels", ",", "predicted_labels", ",", "average", "=", "average", ")", "\n", "f1_macro", "=", "metrics", ".", "f1_score", "(", "target_labels", ",", "predicted_labels", ",", "average", "=", "average_mac", ")", "\n", "#auc_micro = metrics.roc_auc_score(target_labels, predicted_labels, average='micro')", "\n", "auc_micro", "=", "0", "\n", "\n", "avg_loss", "=", "total_loss", "/", "len", "(", "self", ".", "data_loader", ".", "dataset", ".", "examples", ")", "\n", "\n", "if", "hasattr", "(", "self", ".", "model", ",", "'beta_ema'", ")", "and", "self", ".", "model", ".", "beta_ema", ">", "0", ":", "\n", "# Temporal averaging", "\n", "            ", "self", ".", "model", ".", "load_params", "(", "old_params", ")", "\n", "\n", "", "return", "[", "accuracy", ",", "precision", ",", "recall", ",", "f1_micro", ",", "avg_loss", ",", "hamming_loss", ",", "jaccard_score", ",", "f1_macro", ",", "auc_micro", "]", ",", "[", "'accuracy'", ",", "'precision_micro'", ",", "'recall_micro'", ",", "'f1_micro'", ",", "'cross_entropy_loss'", ",", "'hamming_loss'", ",", "'jaccard_score'", ",", "\n", "'f1_macro'", ",", "'auc_micro'", "]", "\n", "", "", ""]]}