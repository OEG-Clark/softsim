{"home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.preproc_categorization_data.readCategoryData": [[19, 55], ["print", "open", "csv.reader", "next", "row[].strip", "int", "int", "int", "int", "int", "comment_list.append", "category_labels.append", "len", "len", "int", "int", "int", "int", "row[].strip().lower", "row[].strip().lower", "row[].strip().lower", "row[].strip().lower", "row[].strip().lower", "numpy.sum", "numpy.sum", "row[].strip.split", "row[].strip().lower", "row[].strip().lower", "row[].strip().lower", "row[].strip().lower", "row[].strip", "row[].strip", "row[].strip", "row[].strip", "row[].strip", "row[].strip", "row[].strip", "row[].strip", "row[].strip"], "function", ["None"], ["def", "readCategoryData", "(", "fn", ")", ":", "\n", "    ", "comment_list", "=", "[", "]", "\n", "category_labels", "=", "[", "]", "\n", "with", "open", "(", "fn", ",", "\"rt\"", ")", "as", "csvfile", ":", "\n", "        ", "reader", "=", "csv", ".", "reader", "(", "csvfile", ",", "delimiter", "=", "\",\"", ")", "\n", "# skip reader", "\n", "next", "(", "reader", ",", "None", ")", "\n", "for", "row", "in", "reader", ":", "\n", "            ", "comment", "=", "row", "[", "0", "]", ".", "strip", "(", ")", "\n", "if", "(", "len", "(", "comment", ")", "==", "0", ")", ":", "\n", "                ", "continue", "\n", "", "bullying_label", "=", "int", "(", "row", "[", "2", "]", ".", "strip", "(", ")", ".", "lower", "(", ")", "==", "\"yes\"", ")", "\n", "# explicit / implicit", "\n", "explicit_label", "=", "[", "int", "(", "row", "[", "3", "]", ".", "strip", "(", ")", ".", "lower", "(", ")", "==", "\"yes\"", ")", ",", "int", "(", "row", "[", "1", "]", ".", "strip", "(", ")", ".", "lower", "(", ")", "==", "\"yes\"", ")", "]", "\n", "# generic / targeted", "\n", "generic_label", "=", "[", "int", "(", "row", "[", "4", "]", ".", "strip", "(", ")", ".", "lower", "(", ")", "==", "\"yes\"", ")", ",", "int", "(", "row", "[", "5", "]", ".", "strip", "(", ")", ".", "lower", "(", ")", "==", "\"yes\"", ")", "]", "\n", "# gender", "\n", "val", "=", "int", "(", "row", "[", "8", "]", ".", "strip", "(", ")", ".", "lower", "(", ")", "==", "\"yes\"", ")", "\n", "gender_label", "=", "[", "val", ",", "1", "-", "val", "]", "\n", "# race", "\n", "val", "=", "int", "(", "row", "[", "9", "]", ".", "strip", "(", ")", ".", "lower", "(", ")", "==", "\"yes\"", ")", "\n", "race_label", "=", "[", "val", ",", "1", "-", "val", "]", "\n", "# appear", "\n", "val", "=", "int", "(", "row", "[", "7", "]", ".", "strip", "(", ")", ".", "lower", "(", ")", "==", "\"yes\"", ")", "\n", "appear_label", "=", "[", "val", ",", "1", "-", "val", "]", "\n", "# idea", "\n", "val", "=", "int", "(", "row", "[", "6", "]", ".", "strip", "(", ")", ".", "lower", "(", ")", "==", "\"yes\"", ")", "\n", "idea_label", "=", "[", "val", ",", "1", "-", "val", "]", "\n", "if", "(", "bullying_label", "<", "1", "or", "np", ".", "sum", "(", "explicit_label", ")", "<", "1", "or", "np", ".", "sum", "(", "generic_label", ")", "<", "1", ")", ":", "\n", "                ", "continue", "\n", "", "comment_list", ".", "append", "(", "\" \"", ".", "join", "(", "comment", ".", "split", "(", ")", ")", ")", "\n", "#comment_list.append(comment)", "\n", "category_labels", ".", "append", "(", "(", "explicit_label", "[", ":", "]", ",", "generic_label", "[", ":", "]", ",", "gender_label", "[", ":", "]", ",", "race_label", "[", ":", "]", ",", "appear_label", "[", ":", "]", ",", "idea_label", "[", ":", "]", ")", ")", "\n", "", "", "print", "(", "\"Comment # for categorization {}\"", ".", "format", "(", "len", "(", "comment_list", ")", ")", ")", "\n", "return", "comment_list", ",", "category_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.preproc_categorization_data.readCommentData": [[57, 79], ["dict", "enumerate", "dict", "enumerate", "open", "pickle.load", "open", "pickle.load", "open", "pickle.load", "open", "pickle.load", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "comm.strip().split", "comm.strip().split", "comm.strip", "comm.strip"], "function", ["None"], ["", "def", "readCommentData", "(", ")", ":", "\n", "    ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "param", ".", "dump_folder", ",", "\"raw_train_comm.data\"", ")", ",", "\"rb\"", ")", "as", "handle", ":", "\n", "        ", "train_comments", ",", "train_labels", "=", "pickle", ".", "load", "(", "handle", ")", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "param", ".", "dump_folder", ",", "\"raw_test_comm.data\"", ")", ",", "\"rb\"", ")", "as", "handle", ":", "\n", "        ", "test_comments", ",", "test_labels", "=", "pickle", ".", "load", "(", "handle", ")", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "param", ".", "dump_folder", ",", "\"train_map.data\"", ")", ",", "\"rb\"", ")", "as", "handle", ":", "\n", "        ", "train_map_dict", "=", "pickle", ".", "load", "(", "handle", ")", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "param", ".", "dump_folder", ",", "\"test_map.data\"", ")", ",", "\"rb\"", ")", "as", "handle", ":", "\n", "        ", "test_map_dict", "=", "pickle", ".", "load", "(", "handle", ")", "\n", "\n", "", "abusive_train_comments", "=", "dict", "(", ")", "\n", "for", "comm_ind", ",", "comm", "in", "enumerate", "(", "train_comments", ")", ":", "\n", "        ", "if", "train_labels", "[", "comm_ind", "]", "[", "0", "]", "==", "1", ":", "\n", "            ", "comm", "=", "\" \"", ".", "join", "(", "comm", ".", "strip", "(", ")", ".", "split", "(", ")", ")", "\n", "abusive_train_comments", "[", "comm", "]", "=", "comm_ind", "\n", "\n", "", "", "abusive_test_comments", "=", "dict", "(", ")", "\n", "for", "comm_ind", ",", "comm", "in", "enumerate", "(", "test_comments", ")", ":", "\n", "        ", "if", "test_labels", "[", "comm_ind", "]", "[", "0", "]", "==", "1", ":", "\n", "            ", "comm", "=", "\" \"", ".", "join", "(", "comm", ".", "strip", "(", ")", ".", "split", "(", ")", ")", "\n", "abusive_test_comments", "[", "comm", "]", "=", "comm_ind", "\n", "", "", "return", "abusive_train_comments", ",", "abusive_test_comments", ",", "train_map_dict", ",", "test_map_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.preproc_categorization_data.dumpCategorizationData": [[81, 129], ["preproc_categorization_data.readCategoryData", "preproc_categorization_data.readCommentData", "print", "dict", "dict", "dict", "range", "print", "print", "len", "open", "pickle.dump", "open", "pickle.dump", "len", "os.path.join", "os.path.join", "len", "len", "orig_match_dict[].append", "orig_match_dict[].append"], "function", ["home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.preproc_categorization_data.readCategoryData", "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.preproc_categorization_data.readCommentData"], ["", "def", "dumpCategorizationData", "(", "fn", "=", "\"Anonymized_Comments_Categorized.csv\"", ")", ":", "\n", "# category data", "\n", "    ", "category_comments", ",", "category_labels", "=", "readCategoryData", "(", "fn", ")", "\n", "# original data", "\n", "train_comments_dict", ",", "test_comments_dict", ",", "train_map_dict", ",", "test_map_dict", "=", "readCommentData", "(", ")", "\n", "print", "(", "\"Comment #: {}\"", ".", "format", "(", "len", "(", "train_comments_dict", ")", "+", "len", "(", "test_comments_dict", ")", ")", ")", "\n", "\n", "category_match_dict", "=", "dict", "(", ")", "\n", "orig_match_dict", "=", "dict", "(", ")", "\n", "comm_category_dict", "=", "dict", "(", ")", "\n", "match_count", "=", "0", "\n", "for", "cate_comm_ind", "in", "range", "(", "len", "(", "category_comments", ")", ")", ":", "\n", "# exact match", "\n", "        ", "cate_comm", "=", "category_comments", "[", "cate_comm_ind", "]", "\n", "is_match", "=", "False", "\n", "for", "train_comm", "in", "train_comments_dict", ":", "\n", "            ", "comm_ind", "=", "train_comments_dict", "[", "train_comm", "]", "\n", "if", "(", "cate_comm", "==", "train_comm", ")", ":", "\n", "                ", "category_match_dict", "[", "cate_comm_ind", "]", "=", "(", "\"train\"", ",", "comm_ind", ")", "\n", "if", "(", "(", "\"train\"", ",", "comm_ind", ")", "not", "in", "orig_match_dict", ")", ":", "\n", "                    ", "orig_match_dict", "[", "(", "\"train\"", ",", "comm_ind", ")", "]", "=", "[", "]", "\n", "", "orig_match_dict", "[", "(", "\"train\"", ",", "comm_ind", ")", "]", ".", "append", "(", "cate_comm_ind", ")", "\n", "comm_category_dict", "[", "(", "\"train\"", ",", "comm_ind", ")", "]", "=", "category_labels", "[", "cate_comm_ind", "]", "\n", "is_match", "=", "True", "\n", "match_count", "+=", "1", "\n", "break", "\n", "", "", "if", "(", "is_match", ")", ":", "\n", "            ", "continue", "\n", "", "for", "test_comm", "in", "test_comments_dict", ":", "\n", "            ", "comm_ind", "=", "test_comments_dict", "[", "test_comm", "]", "\n", "if", "(", "cate_comm", "==", "test_comm", ")", ":", "\n", "                ", "category_match_dict", "[", "cate_comm_ind", "]", "=", "(", "\"test\"", ",", "comm_ind", ")", "\n", "if", "(", "(", "\"test\"", ",", "comm_ind", ")", "not", "in", "orig_match_dict", ")", ":", "\n", "                    ", "orig_match_dict", "[", "(", "\"test\"", ",", "comm_ind", ")", "]", "=", "[", "]", "\n", "", "orig_match_dict", "[", "(", "\"test\"", ",", "comm_ind", ")", "]", ".", "append", "(", "cate_comm_ind", ")", "\n", "comm_category_dict", "[", "(", "\"test\"", ",", "comm_ind", ")", "]", "=", "category_labels", "[", "cate_comm_ind", "]", "\n", "is_match", "=", "True", "\n", "match_count", "+=", "1", "\n", "break", "\n", "", "", "if", "(", "is_match", ")", ":", "\n", "            ", "continue", "\n", "\n", "", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "param", ".", "dump_folder", ",", "\"category_map.data\"", ")", ",", "\"wb\"", ")", "as", "handle", ":", "\n", "        ", "pickle", ".", "dump", "(", "category_match_dict", ",", "handle", ")", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "param", ".", "dump_folder", ",", "\"comm_category.data\"", ")", ",", "\"wb\"", ")", "as", "handle", ":", "\n", "        ", "pickle", ".", "dump", "(", "comm_category_dict", ",", "handle", ")", "\n", "", "print", "(", "\"# of matched category comments:\"", ",", "len", "(", "category_match_dict", ")", ")", "\n", "print", "(", "\"match count:\"", ",", "match_count", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.preproc_classification_data.clean_str": [[26, 41], ["re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub.strip().lower", "re.sub.strip"], "function", ["None"], ["def", "clean_str", "(", "string", ")", ":", "\n", "  ", "string", "=", "re", ".", "sub", "(", "r\"[^A-Za-z0-9()$,!?\\'\\`]\"", ",", "\" \"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\'s\"", ",", "\" \\'s\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\'ve\"", ",", "\" \\'ve\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"n\\'t\"", ",", "\" n\\'t\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\'re\"", ",", "\" \\'re\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\'d\"", ",", "\" \\'d\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\'ll\"", ",", "\" \\'ll\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\",\"", ",", "\" , \"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"!\"", ",", "\" ! \"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\(\"", ",", "\" \\( \"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\)\"", ",", "\" \\) \"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\?\"", ",", "\" \\? \"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\s{2,}\"", ",", "\" \"", ",", "string", ")", "\n", "return", "string", ".", "strip", "(", ")", ".", "lower", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.preproc_classification_data.readRawCommSentData": [[43, 96], ["open", "csv.reader", "next", "comm_sent_list.append", "comm_label_list.append", "sent.strip.strip", "range", "print", "int", "sent_bullying_label_str.strip", "tmp_sent_list.append", "tmp_label_list.append", "comm_sent_list.append", "comm_label_list.append", "print", "print", "sent_bullying_label_str.strip", "print"], "function", ["None"], ["", "def", "readRawCommSentData", "(", "fn", ",", "verbose", "=", "True", ")", ":", "\n", "    ", "with", "open", "(", "fn", ",", "\"rt\"", ")", "as", "csvfile", ":", "\n", "        ", "reader", "=", "csv", ".", "reader", "(", "csvfile", ",", "delimiter", "=", "\",\"", ")", "\n", "next", "(", "reader", ",", "None", ")", "\n", "prev_comment_ind", "=", "1", "\n", "# [[\"this is true\", \"I have to say\"], [\"you hear that?\", \"alright\", \"forget it\"]]", "\n", "comm_sent_list", "=", "[", "]", "\n", "# [[0, 0], [0, 0, 0]]", "\n", "comm_label_list", "=", "[", "]", "\n", "tmp_sent_list", "=", "[", "]", "\n", "tmp_label_list", "=", "[", "]", "\n", "unlabeled_cnt", "=", "0", "\n", "for", "row", "in", "reader", ":", "\n", "            ", "comment_ind", ",", "_", ",", "sent", ",", "_", ",", "sent_bullying_label_str", "=", "row", "\n", "try", ":", "\n", "                ", "comment_ind", "=", "int", "(", "comment_ind", ")", "\n", "", "except", ":", "\n", "                ", "print", "(", "\"empty comment_ind\"", ",", "row", ")", "\n", "continue", "\n", "", "sent", "=", "sent", ".", "strip", "(", ")", "\n", "# skip empty sentence", "\n", "if", "(", "sent", "==", "\"\"", ")", ":", "\n", "                ", "continue", "\n", "# sentence label", "\n", "", "if", "(", "sent_bullying_label_str", ".", "strip", "(", ")", "==", "\"No\"", ")", ":", "\n", "                ", "sent_label", "=", "0", "\n", "", "elif", "(", "sent_bullying_label_str", ".", "strip", "(", ")", "==", "\"Yes\"", ")", ":", "\n", "                ", "sent_label", "=", "1", "\n", "", "else", ":", "\n", "                ", "print", "(", "\"Missing label in sent: {}\"", ".", "format", "(", "row", ")", ")", "\n", "unlabeled_cnt", "+=", "1", "\n", "continue", "\n", "", "if", "(", "comment_ind", "==", "prev_comment_ind", ")", ":", "\n", "                ", "tmp_sent_list", ".", "append", "(", "sent", ")", "\n", "tmp_label_list", ".", "append", "(", "sent_label", ")", "\n", "", "else", ":", "\n", "# store prev comment", "\n", "                ", "comm_sent_list", ".", "append", "(", "tmp_sent_list", "[", ":", "]", ")", "\n", "comm_label_list", ".", "append", "(", "tmp_label_list", "[", ":", "]", ")", "\n", "# update tmp", "\n", "tmp_sent_list", "=", "[", "sent", "]", "\n", "tmp_label_list", "=", "[", "sent_label", "]", "\n", "prev_comment_ind", "=", "comment_ind", "\n", "# store the last comment", "\n", "", "", "comm_sent_list", ".", "append", "(", "tmp_sent_list", "[", ":", "]", ")", "\n", "comm_label_list", ".", "append", "(", "tmp_label_list", "[", ":", "]", ")", "\n", "\n", "# sanity check: print last two comments and labels", "\n", "if", "verbose", ":", "\n", "            ", "for", "i", "in", "range", "(", "1", ",", "3", ")", ":", "\n", "                ", "print", "(", "\"example comment:\"", ",", "comm_sent_list", "[", "-", "i", "]", ",", "\"labels:\"", ",", "comm_label_list", "[", "-", "i", "]", ")", "\n", "", "print", "(", "\"# of unlabeld sents: {}\"", ".", "format", "(", "unlabeled_cnt", ")", ")", "\n", "", "return", "comm_sent_list", ",", "comm_label_list", "\n", "\n"]], "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.preproc_classification_data.shuffleCommData": [[98, 113], ["len", "numpy.array", "numpy.array", "numpy.random.permutation", "int", "numpy.arange"], "function", ["None"], ["", "", "def", "shuffleCommData", "(", "comm_sent_list", ",", "comm_label_list", ",", "train_test_ratio", ")", ":", "\n", "    ", "total_num", "=", "len", "(", "comm_sent_list", ")", "\n", "sent_arr", "=", "np", ".", "array", "(", "comm_sent_list", ")", "\n", "label_arr", "=", "np", ".", "array", "(", "comm_label_list", ")", "\n", "shuffle_indices", "=", "np", ".", "random", ".", "permutation", "(", "np", ".", "arange", "(", "total_num", ")", ")", "\n", "shuffle_sent", "=", "sent_arr", "[", "shuffle_indices", "]", "\n", "shuffle_label", "=", "label_arr", "[", "shuffle_indices", "]", "\n", "# train data", "\n", "train_num", "=", "int", "(", "total_num", "*", "train_test_ratio", ")", "\n", "train_comm_list", "=", "shuffle_sent", "[", ":", "train_num", "]", "\n", "train_label_list", "=", "shuffle_label", "[", ":", "train_num", "]", "\n", "# test data", "\n", "test_comm_list", "=", "shuffle_sent", "[", "train_num", ":", "]", "\n", "test_label_list", "=", "shuffle_label", "[", "train_num", ":", "]", "\n", "return", "train_comm_list", ",", "train_label_list", ",", "test_comm_list", ",", "test_label_list", "\n", "\n"]], "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.preproc_classification_data.mapCommSent": [[115, 124], ["dict", "range", "len", "len"], "function", ["None"], ["", "def", "mapCommSent", "(", "comm_sent_list", ")", ":", "\n", "# comm-sent mapping", "\n", "    ", "map_dict", "=", "dict", "(", ")", "\n", "start_ind", "=", "0", "\n", "for", "comm_ind", "in", "range", "(", "len", "(", "comm_sent_list", ")", ")", ":", "\n", "        ", "end_ind", "=", "start_ind", "+", "len", "(", "comm_sent_list", "[", "comm_ind", "]", ")", "# not including itself", "\n", "map_dict", "[", "comm_ind", "]", "=", "[", "start_ind", ",", "end_ind", "]", "\n", "start_ind", "=", "end_ind", "\n", "", "return", "map_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.preproc_classification_data.vectorizeLabel": [[126, 129], ["None"], "function", ["None"], ["", "def", "vectorizeLabel", "(", "label_list", ")", ":", "\n", "    ", "binary_label_list", "=", "[", "[", "label", ",", "1", "-", "label", "]", "for", "label", "in", "label_list", "]", "\n", "return", "binary_label_list", "\n", "\n"]], "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.preproc_classification_data.preprocessData": [[131, 145], ["print", "print", "fn.endswith", "print", "open", "pickle.load", "preprocessor.tokenize", "open", "pickle.dump", "corpus_file.write", "corpus_file.write", "os.path.join", "clean_str().split", "os.path.join", "preproc_classification_data.clean_str"], "function", ["home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.preproc_classification_data.clean_str"], ["", "def", "preprocessData", "(", "fn", ",", "save_fn", ",", "corpus_file", ")", ":", "\n", "    ", "print", "(", "\"Preprocessing {}...\"", ".", "format", "(", "fn", ")", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "param", ".", "dump_folder", ",", "fn", ")", ",", "\"rb\"", ")", "as", "handle", ":", "\n", "        ", "sent_list", ",", "label_list", "=", "pickle", ".", "load", "(", "handle", ")", "\n", "", "print", "(", "\"Tokenization...\"", ")", "\n", "sent_list", "=", "[", "p", ".", "tokenize", "(", "sent", ")", "for", "sent", "in", "sent_list", "]", "\n", "sent_list", "=", "[", "clean_str", "(", "sent", ")", ".", "split", "(", ")", "[", ":", "]", "for", "sent", "in", "sent_list", "]", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "param", ".", "dump_folder", ",", "save_fn", ")", ",", "\"wb\"", ")", "as", "handle", ":", "\n", "        ", "pickle", ".", "dump", "(", "(", "sent_list", ",", "label_list", ")", ",", "handle", ")", "\n", "# write to corpus with comments", "\n", "", "if", "fn", ".", "endswith", "(", "\"comm.data\"", ")", ":", "\n", "        ", "corpus_file", ".", "write", "(", "\"\\n\"", ".", "join", "(", "[", "\" \"", ".", "join", "(", "sent", ")", "for", "sent", "in", "sent_list", "]", ")", ")", "\n", "corpus_file", ".", "write", "(", "\"\\n\"", ")", "\n", "", "print", "(", "\"Done preprocessing, save to {}\"", ".", "format", "(", "save_fn", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.preproc_classification_data.getAnnotatedAttention": [[147, 164], ["open", "pickle.load", "open", "pickle.load", "range", "attention_list.append", "open", "pickle.dump", "os.path.join", "os.path.join", "os.path.join", "print", "len", "len", "len"], "function", ["None"], ["", "def", "getAnnotatedAttention", "(", "sent_fn", ",", "map_fn", ",", "attention_fn", ")", ":", "\n", "    ", "attention_list", "=", "[", "]", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "param", ".", "dump_folder", ",", "sent_fn", ")", ",", "\"rb\"", ")", "as", "handle", ":", "\n", "        ", "sent_list", ",", "sent_label_list", "=", "pickle", ".", "load", "(", "handle", ")", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "param", ".", "dump_folder", ",", "map_fn", ")", ",", "\"rb\"", ")", "as", "handle", ":", "\n", "        ", "map_dict", "=", "pickle", ".", "load", "(", "handle", ")", "\n", "", "for", "comm_ind", "in", "map_dict", ":", "\n", "        ", "sent_start_ind", ",", "sent_end_ind", "=", "map_dict", "[", "comm_ind", "]", "\n", "attention_vec", "=", "[", "]", "\n", "for", "sent_ind", "in", "range", "(", "sent_start_ind", ",", "sent_end_ind", ")", ":", "\n", "            ", "try", ":", "\n", "                ", "attention_vec", "=", "attention_vec", "+", "[", "sent_label_list", "[", "sent_ind", "]", "[", "0", "]", "]", "*", "len", "(", "sent_list", "[", "sent_ind", "]", ")", "\n", "", "except", ":", "\n", "                ", "print", "(", "\"sent_ind: {}, len sent_label_list: {}, len sent_list: {}\"", ".", "format", "(", "sent_ind", ",", "len", "(", "sent_label_list", ")", ",", "len", "(", "sent_list", ")", ")", ")", "\n", "", "", "attention_list", ".", "append", "(", "attention_vec", "[", ":", "]", ")", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "param", ".", "dump_folder", ",", "attention_fn", ")", ",", "\"wb\"", ")", "as", "handle", ":", "\n", "        ", "pickle", ".", "dump", "(", "attention_list", ",", "handle", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.preproc_classification_data.normalizeUser": [[170, 178], ["norm_sents.append", "tok.startswith", "sent.split"], "function", ["None"], ["def", "normalizeUser", "(", "sentences", ")", ":", "\n", "# replace @userXXX with @user", "\n", "    ", "user_tok", "=", "\"@user\"", "\n", "norm_sents", "=", "[", "]", "\n", "for", "sent", "in", "sentences", ":", "\n", "        ", "nsent", "=", "\" \"", ".", "join", "(", "[", "\"\"", "if", "tok", ".", "startswith", "(", "user_tok", ")", "else", "tok", "for", "tok", "in", "sent", ".", "split", "(", ")", "]", ")", "\n", "norm_sents", ".", "append", "(", "nsent", ")", "\n", "", "return", "norm_sents", "\n", "\n"]], "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.preproc_classification_data.genPOSTags": [[179, 211], ["text_path.endswith", "open", "pickle.load", "len", "CMUTweetTagger.runtagger_parse", "open", "pickle.dump", "pos_corpus_file.write", "pos_corpus_file.write", "print", "range", "print", "os.path.join", "sent.lower", "pos_list.append", "os.path.join", "len", "print", "CMUTweetTokenizer.runtokenizer_parse"], "function", ["None"], ["", "def", "genPOSTags", "(", "text_path", ",", "pos_path", ",", "pos_corpus_file", ",", "verbose", "=", "True", ")", ":", "\n", "    ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "param", ".", "dump_folder", ",", "text_path", ")", ",", "\"rb\"", ")", "as", "handle", ":", "\n", "        ", "comment_list", ",", "_", "=", "pickle", ".", "load", "(", "handle", ")", "\n", "# tokenize", "\n", "#comment_list = genTokens(comment_list)", "\n", "# POS tagging", "\n", "", "pos_comment_list", "=", "[", "]", "\n", "max_sent_num", "=", "5000", "\n", "ind", "=", "0", "\n", "while", "(", "ind", "<", "len", "(", "comment_list", ")", ")", ":", "\n", "        ", "sent_list", "=", "comment_list", "[", "ind", ":", "ind", "+", "max_sent_num", "]", "\n", "#[\" \".join(seq) for seq in comment_list[ind: ind+max_sent_num]]", "\n", "tok_sent_list", "=", "[", "sent", ".", "lower", "(", ")", "for", "sent", "in", "CMUTweetTokenizer", ".", "runtokenizer_parse", "(", "sent_list", ")", "]", "\n", "raw_pos_list", "=", "CMUTweetTagger", ".", "runtagger_parse", "(", "\n", "tok_sent_list", "\n", ")", "\n", "pos_list", "=", "[", "]", "\n", "for", "raw_seq", "in", "raw_pos_list", ":", "\n", "            ", "seq", "=", "[", "tup", "[", "1", "]", "for", "tup", "in", "raw_seq", "]", "\n", "pos_list", ".", "append", "(", "seq", "[", ":", "]", ")", "\n", "", "pos_comment_list", "=", "pos_comment_list", "+", "pos_list", "\n", "ind", "+=", "max_sent_num", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "param", ".", "dump_folder", ",", "pos_path", ")", ",", "\"wb\"", ")", "as", "handle", ":", "\n", "      ", "pickle", ".", "dump", "(", "pos_comment_list", ",", "handle", ")", "\n", "", "if", "text_path", ".", "endswith", "(", "\"comm.data\"", ")", ":", "\n", "      ", "pos_corpus_file", ".", "write", "(", "\"\\n\"", ".", "join", "(", "[", "\" \"", ".", "join", "(", "pos_comm", ")", "for", "pos_comm", "in", "pos_comment_list", "]", ")", ")", "\n", "pos_corpus_file", ".", "write", "(", "\"\\n\"", ")", "\n", "", "if", "verbose", ":", "\n", "      ", "print", "(", "\"# pos sequences:\"", ",", "len", "(", "pos_comment_list", ")", ")", "\n", "for", "i", "in", "range", "(", "3", ")", ":", "\n", "        ", "print", "(", "\"example of pos sequence:\"", ",", "pos_comment_list", "[", "i", "]", ")", "\n", "", "print", "(", "\"save pos to {}\"", ".", "format", "(", "pos_path", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.preproc_classification_data.dumpClassificationData": [[213, 280], ["open", "zip", "open.close", "preproc_classification_data.getAnnotatedAttention", "preproc_classification_data.getAnnotatedAttention", "open", "zip", "open.close", "preproc_classification_data.readRawCommSentData", "preproc_classification_data.shuffleCommData", "preproc_classification_data.mapCommSent", "preproc_classification_data.mapCommSent", "print", "preproc_classification_data.vectorizeLabel", "list", "list", "preproc_classification_data.vectorizeLabel", "preproc_classification_data.vectorizeLabel", "list", "list", "preproc_classification_data.vectorizeLabel", "print", "print", "os.path.join", "preproc_classification_data.preprocessData", "os.path.join", "preproc_classification_data.genPOSTags", "numpy.max", "open", "pickle.dump", "itertools.chain.from_iterable", "itertools.chain.from_iterable", "open", "pickle.dump", "open", "pickle.dump", "numpy.max", "open", "pickle.dump", "itertools.chain.from_iterable", "itertools.chain.from_iterable", "open", "pickle.dump", "open", "pickle.dump", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.preproc_classification_data.getAnnotatedAttention", "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.preproc_classification_data.getAnnotatedAttention", "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.preproc_classification_data.readRawCommSentData", "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.preproc_classification_data.shuffleCommData", "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.preproc_classification_data.mapCommSent", "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.preproc_classification_data.mapCommSent", "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.preproc_classification_data.vectorizeLabel", "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.preproc_classification_data.vectorizeLabel", "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.preproc_classification_data.vectorizeLabel", "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.preproc_classification_data.vectorizeLabel", "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.preproc_classification_data.preprocessData", "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.preproc_classification_data.genPOSTags"], ["", "", "def", "dumpClassificationData", "(", "fn", "=", "\"Anonymized_Sentences_Classified.csv\"", ",", "random_split", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    data type:\n    comm_list, sent_list, map_dict\n    \"\"\"", "\n", "if", "random_split", ":", "\n", "      ", "comm_sent_list", ",", "comm_label_list", "=", "readRawCommSentData", "(", "fn", ")", "\n", "train_comm_sent_list", ",", "train_label_list", ",", "test_comm_sent_list", ",", "test_label_list", "=", "shuffleCommData", "(", "comm_sent_list", ",", "comm_label_list", ",", "train_test_ratio", "=", "0.8", ")", "\n", "train_map_dict", "=", "mapCommSent", "(", "train_comm_sent_list", ")", "\n", "test_map_dict", "=", "mapCommSent", "(", "test_comm_sent_list", ")", "\n", "\n", "# raw_train_comm_data", "\n", "merged_train_comm_list", "=", "[", "\" \"", ".", "join", "(", "sent_list", ")", "for", "sent_list", "in", "train_comm_sent_list", "]", "\n", "print", "(", "\"debug: comm_label_list\"", ",", "comm_label_list", "[", ":", "5", "]", ")", "\n", "merged_train_label_list", "=", "[", "np", ".", "max", "(", "label_list", ")", "for", "label_list", "in", "train_label_list", "]", "\n", "merged_train_label_list", "=", "vectorizeLabel", "(", "merged_train_label_list", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "param", ".", "dump_folder", ",", "\"raw_train_comm.data\"", ")", ",", "\"wb\"", ")", "as", "handle", ":", "\n", "          ", "pickle", ".", "dump", "(", "(", "merged_train_comm_list", ",", "merged_train_label_list", ")", ",", "handle", ")", "\n", "# raw_train_sent_data", "\n", "", "flat_train_sent_list", "=", "list", "(", "itertools", ".", "chain", ".", "from_iterable", "(", "train_comm_sent_list", ")", ")", "\n", "flat_train_label_list", "=", "list", "(", "itertools", ".", "chain", ".", "from_iterable", "(", "train_label_list", ")", ")", "\n", "flat_train_label_list", "=", "vectorizeLabel", "(", "flat_train_label_list", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "param", ".", "dump_folder", ",", "\"raw_train_sent.data\"", ")", ",", "\"wb\"", ")", "as", "handle", ":", "\n", "          ", "pickle", ".", "dump", "(", "(", "flat_train_sent_list", ",", "flat_train_label_list", ")", ",", "handle", ")", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "param", ".", "dump_folder", ",", "\"train_map.data\"", ")", ",", "\"wb\"", ")", "as", "handle", ":", "\n", "          ", "pickle", ".", "dump", "(", "train_map_dict", ",", "handle", ")", "\n", "\n", "# raw_test_comm_data", "\n", "", "merged_test_comm_list", "=", "[", "\" \"", ".", "join", "(", "sent_list", ")", "for", "sent_list", "in", "test_comm_sent_list", "]", "\n", "merged_test_label_list", "=", "[", "np", ".", "max", "(", "label_list", ")", "for", "label_list", "in", "test_label_list", "]", "\n", "merged_test_label_list", "=", "vectorizeLabel", "(", "merged_test_label_list", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "param", ".", "dump_folder", ",", "\"raw_test_comm.data\"", ")", ",", "\"wb\"", ")", "as", "handle", ":", "\n", "          ", "pickle", ".", "dump", "(", "(", "merged_test_comm_list", ",", "merged_test_label_list", ")", ",", "handle", ")", "\n", "# raw_test_sent_data", "\n", "", "flat_test_sent_list", "=", "list", "(", "itertools", ".", "chain", ".", "from_iterable", "(", "test_comm_sent_list", ")", ")", "\n", "flat_test_label_list", "=", "list", "(", "itertools", ".", "chain", ".", "from_iterable", "(", "test_label_list", ")", ")", "\n", "flat_test_label_list", "=", "vectorizeLabel", "(", "flat_test_label_list", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "param", ".", "dump_folder", ",", "\"raw_test_sent.data\"", ")", ",", "\"wb\"", ")", "as", "handle", ":", "\n", "          ", "pickle", ".", "dump", "(", "(", "flat_test_sent_list", ",", "flat_test_label_list", ")", ",", "handle", ")", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "param", ".", "dump_folder", ",", "\"test_map.data\"", ")", ",", "\"wb\"", ")", "as", "handle", ":", "\n", "          ", "pickle", ".", "dump", "(", "test_map_dict", ",", "handle", ")", "\n", "\n", "", "print", "(", "\"# of train comm: {}, # of test comm: {}\"", ".", "format", "(", "len", "(", "merged_train_comm_list", ")", ",", "len", "(", "merged_test_comm_list", ")", ")", ")", "\n", "print", "(", "\"# of train sent: {}, # of test sent: {}\"", ".", "format", "(", "len", "(", "flat_train_sent_list", ")", ",", "len", "(", "flat_test_sent_list", ")", ")", ")", "\n", "\n", "# preprocessing word sequence", "\n", "", "fn_list", "=", "[", "\"raw_train_comm.data\"", ",", "\"raw_train_sent.data\"", ",", "\"raw_test_comm.data\"", ",", "\"raw_test_sent.data\"", "]", "\n", "save_fn_list", "=", "[", "\"train_comm.data\"", ",", "\"train_sent.data\"", ",", "\"test_comm.data\"", ",", "\"test_sent.data\"", "]", "\n", "corpus_file", "=", "open", "(", "os", ".", "path", ".", "join", "(", "param", ".", "dump_folder", ",", "\"corpus.txt\"", ")", ",", "\"w\"", ")", "\n", "for", "(", "fn", ",", "save_fn", ")", "in", "zip", "(", "fn_list", ",", "save_fn_list", ")", ":", "\n", "        ", "preprocessData", "(", "fn", ",", "save_fn", ",", "corpus_file", ")", "\n", "", "corpus_file", ".", "close", "(", ")", "\n", "\n", "# get attention", "\n", "getAnnotatedAttention", "(", "\"train_sent.data\"", ",", "\"train_map.data\"", ",", "\"train_attention.data\"", ")", "\n", "getAnnotatedAttention", "(", "\"test_sent.data\"", ",", "\"test_map.data\"", ",", "\"test_attention.data\"", ")", "\n", "\n", "# get POS tags", "\n", "fn_list", "=", "[", "\"raw_train_comm.data\"", ",", "\"raw_test_comm.data\"", "]", "\n", "save_fn_list", "=", "[", "\"train_comm_pos.data\"", ",", "\"test_comm_pos.data\"", "]", "\n", "pos_corpus_file", "=", "open", "(", "os", ".", "path", ".", "join", "(", "param", ".", "dump_folder", ",", "\"pos_corpus.txt\"", ")", ",", "\"w\"", ")", "\n", "for", "(", "fn", ",", "save_fn", ")", "in", "zip", "(", "fn_list", ",", "save_fn_list", ")", ":", "\n", "      ", "genPOSTags", "(", "fn", ",", "save_fn", ",", "pos_corpus_file", ")", "\n", "", "pos_corpus_file", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.data_helpers.loadVocabEmb": [[19, 28], ["open", "pickle.load", "open", "pickle.load", "open", "pickle.load", "os.path.join", "os.path.join", "os.path.join"], "function", ["None"], ["def", "loadVocabEmb", "(", ")", ":", "\n", "# vocabulary & embedding", "\n", "    ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "param", ".", "dump_folder", ",", "\"vocab.pkl\"", ")", ",", "\"rb\"", ")", "as", "handle", ":", "\n", "        ", "vocabulary", "=", "pickle", ".", "load", "(", "handle", ")", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "param", ".", "dump_folder", ",", "\"pos_vocab.pkl\"", ")", ",", "\"rb\"", ")", "as", "handle", ":", "\n", "        ", "pos_vocabulary", "=", "pickle", ".", "load", "(", "handle", ")", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "param", ".", "dump_folder", ",", "\"norm_init_embed.pkl\"", ")", ",", "\"rb\"", ")", "as", "handle", ":", "\n", "        ", "init_embed", "=", "pickle", ".", "load", "(", "handle", ")", "\n", "", "return", "vocabulary", ",", "pos_vocabulary", ",", "init_embed", "\n", "\n"]], "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.data_helpers.splitTrainData": [[30, 48], ["len", "len", "int", "set", "range", "numpy.random.choice", "print", "range", "range", "range", "range", "range", "train_data[].append", "dev_data[].append", "len", "len", "copy.deepcopy", "copy.deepcopy"], "function", ["None"], ["", "def", "splitTrainData", "(", "data", ",", "train_ratio", "=", "0.8", ",", "verbose", "=", "True", ")", ":", "\n", "# split train data into train & dev sets", "\n", "    ", "data_size", "=", "len", "(", "data", "[", "0", "]", ")", "\n", "groups", "=", "len", "(", "data", ")", "\n", "train_size", "=", "int", "(", "data_size", "*", "train_ratio", ")", "\n", "train_inds", "=", "set", "(", "np", ".", "random", ".", "choice", "(", "range", "(", "data_size", ")", ",", "size", "=", "train_size", ",", "replace", "=", "False", ")", ")", "\n", "train_data", "=", "[", "[", "]", "for", "t", "in", "range", "(", "groups", ")", "]", "\n", "dev_data", "=", "[", "[", "]", "for", "t", "in", "range", "(", "groups", ")", "]", "\n", "for", "ind", "in", "range", "(", "data_size", ")", ":", "\n", "        ", "if", "ind", "in", "train_inds", ":", "\n", "            ", "for", "t", "in", "range", "(", "groups", ")", ":", "\n", "                ", "train_data", "[", "t", "]", ".", "append", "(", "copy", ".", "deepcopy", "(", "data", "[", "t", "]", "[", "ind", "]", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "for", "t", "in", "range", "(", "groups", ")", ":", "\n", "                ", "dev_data", "[", "t", "]", ".", "append", "(", "copy", ".", "deepcopy", "(", "data", "[", "t", "]", "[", "ind", "]", ")", ")", "\n", "", "", "", "if", "verbose", ":", "\n", "        ", "print", "(", "\"split into train ({} examples) and dev sets ({} examples)\"", ".", "format", "(", "len", "(", "train_data", "[", "0", "]", ")", ",", "len", "(", "dev_data", "[", "0", "]", ")", ")", ")", "\n", "", "return", "train_data", "+", "dev_data", "\n", "\n"]], "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.data_helpers.loadData": [[50, 70], ["data_helpers.genFeatures", "data_util.tag_data_helpers.genPOSFeatures", "numpy.array", "open", "pickle.load", "open", "pickle.load", "open", "pickle.load", "open", "pickle.load", "open", "pickle.load", "print", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "numpy.array", "numpy.array", "numpy.array"], "function", ["home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.data_helpers.genFeatures", "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.tag_data_helpers.genPOSFeatures"], ["", "def", "loadData", "(", "data_type", ",", "verbose", "=", "True", ")", ":", "\n", "    ", "assert", "data_type", "in", "[", "\"train\"", ",", "\"test\"", "]", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "param", ".", "dump_folder", ",", "\"vocab.pkl\"", ")", ",", "\"rb\"", ")", "as", "handle", ":", "\n", "        ", "vocabulary", "=", "pickle", ".", "load", "(", "handle", ")", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "param", ".", "dump_folder", ",", "\"pos_vocab.pkl\"", ")", ",", "\"rb\"", ")", "as", "handle", ":", "\n", "        ", "pos_vocabulary", "=", "pickle", ".", "load", "(", "handle", ")", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "param", ".", "dump_folder", ",", "data_type", "+", "\"_comm.data\"", ")", ",", "\"rb\"", ")", "as", "handle", ":", "\n", "        ", "sentences", ",", "labels", "=", "pickle", ".", "load", "(", "handle", ")", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "param", ".", "dump_folder", ",", "data_type", "+", "\"_comm_pos.data\"", ")", ",", "\"rb\"", ")", "as", "handle", ":", "\n", "        ", "pos_sentences", "=", "pickle", ".", "load", "(", "handle", ")", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "param", ".", "dump_folder", ",", "data_type", "+", "\"_attention.data\"", ")", ",", "\"rb\"", ")", "as", "handle", ":", "\n", "        ", "attention", "=", "pickle", ".", "load", "(", "handle", ")", "\n", "# generate features & labels", "\n", "", "x", ",", "length", ",", "attention", "=", "genFeatures", "(", "sentences", ",", "attention", ",", "param", ".", "max_sent_len", ",", "vocabulary", ")", "\n", "pos", ",", "pos_length", "=", "tag_data_helpers", ".", "genPOSFeatures", "(", "pos_sentences", ",", "param", ".", "max_sent_len", ",", "pos_vocabulary", ")", "\n", "y", "=", "np", ".", "array", "(", "labels", ")", "\n", "if", "verbose", ":", "\n", "        ", "print", "(", "\"load {} data, input sent size: {}, input POS size: {}, label size: {}\"", ".", "format", "(", "\n", "data_type", ",", "np", ".", "array", "(", "x", ")", ".", "shape", ",", "np", ".", "array", "(", "pos", ")", ".", "shape", ",", "np", ".", "array", "(", "y", ")", ".", "shape", ")", ")", "\n", "", "return", "x", ",", "length", ",", "attention", ",", "pos", ",", "pos_length", ",", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.data_helpers.loadTrainData": [[72, 75], ["data_helpers.loadData", "data_helpers.splitTrainData"], "function", ["home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.data_helpers.loadData", "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.data_helpers.splitTrainData"], ["", "def", "loadTrainData", "(", ")", ":", "\n", "     ", "data", "=", "loadData", "(", "data_type", "=", "\"train\"", ",", "verbose", "=", "True", ")", "\n", "return", "splitTrainData", "(", "data", ",", "train_ratio", "=", "0.8", ",", "verbose", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.data_helpers.loadTestData": [[77, 79], ["data_helpers.loadData"], "function", ["home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.data_helpers.loadData"], ["", "def", "loadTestData", "(", ")", ":", "\n", "    ", "return", "loadData", "(", "data_type", "=", "\"test\"", ",", "verbose", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.data_helpers.padSents": [[81, 92], ["range", "len", "length_list.append", "padded_sentences.append", "numpy.array", "len", "len"], "function", ["None"], ["", "def", "padSents", "(", "sentences", ",", "max_len", ",", "padding_word", "=", "param", ".", "pad", ")", ":", "\n", "#length_list = np.array([len(sent) for sent in sentences])", "\n", "    ", "length_list", "=", "[", "]", "\n", "padded_sentences", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "sentences", ")", ")", ":", "\n", "        ", "sent", "=", "sentences", "[", "i", "]", "[", ":", "max_len", "]", "\n", "num_padding", "=", "max_len", "-", "len", "(", "sent", ")", "\n", "new_sentence", "=", "sent", "+", "[", "padding_word", "]", "*", "num_padding", "\n", "length_list", ".", "append", "(", "len", "(", "new_sentence", ")", ")", "\n", "padded_sentences", ".", "append", "(", "new_sentence", ")", "\n", "", "return", "padded_sentences", ",", "np", ".", "array", "(", "length_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.data_helpers.genFeatures": [[94, 115], ["data_helpers.padSents", "data_helpers.padSents", "print", "numpy.array", "numpy.array", "print", "np.array.append", "numpy.array", "numpy.array", "sent_x.append", "sent_x.append", "data_util.param.max_sent_len"], "function", ["home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.data_helpers.padSents", "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.data_helpers.padSents"], ["", "def", "genFeatures", "(", "sent_list", ",", "attention_list", ",", "max_sent_len", ",", "vocabulary", ")", ":", "\n", "# pad sentences", "\n", "    ", "padded_sent_list", ",", "length_list", "=", "padSents", "(", "sent_list", ",", "max_sent_len", ")", "\n", "padded_attention_list", ",", "_", "=", "padSents", "(", "attention_list", ",", "max_sent_len", ",", "0", ")", "\n", "print", "(", "\"padded sent:\"", ",", "np", ".", "array", "(", "padded_sent_list", ")", ".", "shape", ")", "\n", "# generate features", "\n", "x", "=", "[", "]", "\n", "for", "sent", "in", "padded_sent_list", ":", "\n", "        ", "sent_x", "=", "[", "]", "\n", "for", "word", "in", "sent", ":", "\n", "            ", "try", ":", "\n", "                ", "sent_x", ".", "append", "(", "vocabulary", "[", "word", "]", ")", "\n", "", "except", ":", "\n", "                ", "sent_x", ".", "append", "(", "vocabulary", "[", "param", ".", "unk", "]", ")", "\n", "continue", "\n", "", "", "x", ".", "append", "(", "sent_x", "[", ":", "]", ")", "\n", "", "x", "=", "np", ".", "array", "(", "x", ")", "\n", "#x = np.array([[vocabulary[word] for word in sent] for sent in padded_sent_list])", "\n", "padded_attention_list", "=", "np", ".", "array", "(", "padded_attention_list", ")", "\n", "print", "(", "\"feature shape:\"", ",", "np", ".", "array", "(", "x", ")", ".", "shape", ")", "\n", "return", "x", ",", "length_list", ",", "padded_attention_list", "\n", "\n"]], "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.data_helpers.batch_iter": [[117, 132], ["numpy.array", "len", "range", "int", "numpy.random.permutation", "range", "numpy.arange", "min"], "function", ["None"], ["", "def", "batch_iter", "(", "data", ",", "batch_size", ",", "num_epochs", ")", ":", "\n", "  ", "\"\"\"\n  Generates a batch iterator for a dataset.\n  \"\"\"", "\n", "data", "=", "np", ".", "array", "(", "data", ")", "\n", "data_size", "=", "len", "(", "data", ")", "\n", "num_batches_per_epoch", "=", "int", "(", "data_size", "/", "batch_size", ")", "+", "1", "\n", "for", "epoch", "in", "range", "(", "num_epochs", ")", ":", "\n", "# Shuffle the data at each epoch", "\n", "    ", "shuffle_indices", "=", "np", ".", "random", ".", "permutation", "(", "np", ".", "arange", "(", "data_size", ")", ")", "\n", "shuffled_data", "=", "data", "[", "shuffle_indices", "]", "\n", "for", "batch_num", "in", "range", "(", "num_batches_per_epoch", ")", ":", "\n", "      ", "start_index", "=", "batch_num", "*", "batch_size", "\n", "end_index", "=", "min", "(", "(", "batch_num", "+", "1", ")", "*", "batch_size", ",", "data_size", ")", "\n", "yield", "shuffled_data", "[", "start_index", ":", "end_index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.tag_data_helpers.padPOSSents": [[7, 17], ["range", "len", "length_list.append", "padded_pos_sentences.append", "numpy.array", "len", "len"], "function", ["None"], ["def", "padPOSSents", "(", "pos_sentences", ",", "max_len", ",", "padding_pos", "=", "\"<POS/>\"", ")", ":", "\n", "    ", "length_list", "=", "[", "]", "\n", "padded_pos_sentences", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "pos_sentences", ")", ")", ":", "\n", "        ", "sent", "=", "pos_sentences", "[", "i", "]", "[", ":", "max_len", "]", "\n", "num_padding", "=", "max_len", "-", "len", "(", "sent", ")", "\n", "new_sentence", "=", "sent", "+", "[", "padding_pos", "]", "*", "num_padding", "\n", "length_list", ".", "append", "(", "len", "(", "new_sentence", ")", ")", "\n", "padded_pos_sentences", ".", "append", "(", "new_sentence", "[", ":", "]", ")", "\n", "", "return", "padded_pos_sentences", ",", "np", ".", "array", "(", "length_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.tag_data_helpers.cleanPOSSents": [[19, 25], ["enumerate", "enumerate"], "function", ["None"], ["", "def", "cleanPOSSents", "(", "pos_sentences", ",", "pos_vocabulary", ",", "unk_pos", "=", "param", ".", "unk", ")", ":", "\n", "# replace pos tags not in pos_vocabulary with unk", "\n", "    ", "for", "(", "sent_ind", ",", "pos_sent", ")", "in", "enumerate", "(", "pos_sentences", ")", ":", "\n", "        ", "for", "(", "word_ind", ",", "word", ")", "in", "enumerate", "(", "pos_sent", ")", ":", "\n", "            ", "if", "word", "not", "in", "pos_vocabulary", ":", "\n", "                ", "pos_sentences", "[", "sent_ind", "]", "[", "word_ind", "]", "=", "unk_pos", "\n", "\n"]], "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.tag_data_helpers.genPOSFeatures": [[27, 36], ["tag_data_helpers.padPOSSents", "tag_data_helpers.cleanPOSSents", "numpy.array", "print", "print", "print", "numpy.array", "numpy.array"], "function", ["home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.tag_data_helpers.padPOSSents", "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.tag_data_helpers.cleanPOSSents"], ["", "", "", "", "def", "genPOSFeatures", "(", "pos_sentences", ",", "max_sent_len", ",", "pos_vocabulary", ",", "verbose", "=", "True", ")", ":", "\n", "    ", "padded_pos_sentences", ",", "length_list", "=", "padPOSSents", "(", "pos_sentences", ",", "max_sent_len", ")", "\n", "cleanPOSSents", "(", "padded_pos_sentences", ",", "pos_vocabulary", ")", "\n", "x", "=", "np", ".", "array", "(", "[", "[", "pos_vocabulary", "[", "word", "]", "for", "word", "in", "sent", "]", "for", "sent", "in", "padded_pos_sentences", "]", ")", "\n", "if", "verbose", ":", "\n", "        ", "print", "(", "\"padded pos sentences:\"", ",", "np", ".", "array", "(", "padded_pos_sentences", ")", ".", "shape", ")", "\n", "print", "(", "\"debug padded_pos_sentences:\"", ",", "padded_pos_sentences", "[", "0", "]", "[", ":", "10", "]", ")", "\n", "print", "(", "\"pos feature shape:\"", ",", "np", ".", "array", "(", "x", ")", ".", "shape", ")", "\n", "", "return", "x", ",", "length_list", "\n", "", ""]], "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.category_data_helpers.countLabelStats": [[20, 25], ["len", "numpy.sum", "print", "float"], "function", ["None"], ["def", "countLabelStats", "(", "bin_labels", ",", "bin_names", ")", ":", "\n", "    ", "num", "=", "len", "(", "bin_labels", ")", "\n", "pos_num", "=", "np", ".", "sum", "(", "[", "labels", "[", "0", "]", "for", "labels", "in", "bin_labels", "]", ")", "\n", "pos_ratio", "=", "float", "(", "pos_num", ")", "/", "num", "\n", "print", "(", "\"total num: {}, {} ratio: {}, {} ratio: {}\"", ".", "format", "(", "num", ",", "bin_names", "[", "0", "]", ",", "pos_ratio", ",", "bin_names", "[", "1", "]", ",", "1", "-", "pos_ratio", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.category_data_helpers.loadCategoryData": [[27, 61], ["data_util.data_helpers.loadData", "list", "open", "pickle.load", "range", "print", "category_data_helpers.countLabelStats", "category_data_helpers.countLabelStats", "category_data_helpers.countLabelStats", "category_data_helpers.countLabelStats", "os.path.join", "cate_x.append", "cate_length.append", "cate_attention.append", "cate_pos.append", "cate_pos_length.append", "gender_labels.append", "race_labels.append", "appear_labels.append", "idea_labels.append", "copy.deepcopy", "copy.deepcopy", "copy.deepcopy", "copy.deepcopy", "copy.deepcopy", "copy.deepcopy", "copy.deepcopy", "copy.deepcopy", "copy.deepcopy", "len"], "function", ["home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.data_helpers.loadData", "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.category_data_helpers.countLabelStats", "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.category_data_helpers.countLabelStats", "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.category_data_helpers.countLabelStats", "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.category_data_helpers.countLabelStats"], ["", "def", "loadCategoryData", "(", "data_type", ",", "verbose", "=", "True", ")", ":", "\n", "    ", "x", ",", "length", ",", "attention", ",", "pos", ",", "pos_length", ",", "_", "=", "data_helpers", ".", "loadData", "(", "data_type", ",", "verbose", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "param", ".", "dump_folder", ",", "\"comm_category.data\"", ")", ",", "\"rb\"", ")", "as", "handle", ":", "\n", "        ", "comm_category_map", "=", "pickle", ".", "load", "(", "handle", ")", "\n", "", "cate_x", ",", "cate_length", ",", "cate_attention", ",", "cate_pos", ",", "cate_pos_length", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "gender_labels", ",", "race_labels", ",", "appear_labels", ",", "idea_labels", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "# category idx", "\n", "explicit_idx", ",", "generic_idx", ",", "gender_idx", ",", "race_idx", ",", "appear_idx", ",", "idea_idx", "=", "list", "(", "range", "(", "6", ")", ")", "\n", "\n", "for", "key", "in", "comm_category_map", ":", "\n", "        ", "dt", ",", "comm_ind", "=", "key", "\n", "cate_labels", "=", "comm_category_map", "[", "key", "]", "\n", "if", "dt", "==", "data_type", ":", "\n", "            ", "cate_x", ".", "append", "(", "copy", ".", "deepcopy", "(", "x", "[", "comm_ind", "]", ")", ")", "\n", "cate_length", ".", "append", "(", "copy", ".", "deepcopy", "(", "length", "[", "comm_ind", "]", ")", ")", "\n", "cate_attention", ".", "append", "(", "copy", ".", "deepcopy", "(", "attention", "[", "comm_ind", "]", ")", ")", "\n", "cate_pos", ".", "append", "(", "copy", ".", "deepcopy", "(", "pos", "[", "comm_ind", "]", ")", ")", "\n", "cate_pos_length", ".", "append", "(", "copy", ".", "deepcopy", "(", "pos_length", "[", "comm_ind", "]", ")", ")", "\n", "# category label", "\n", "gender_labels", ".", "append", "(", "copy", ".", "deepcopy", "(", "cate_labels", "[", "gender_idx", "]", ")", ")", "\n", "race_labels", ".", "append", "(", "copy", ".", "deepcopy", "(", "cate_labels", "[", "race_idx", "]", ")", ")", "\n", "appear_labels", ".", "append", "(", "copy", ".", "deepcopy", "(", "cate_labels", "[", "appear_idx", "]", ")", ")", "\n", "idea_labels", ".", "append", "(", "copy", ".", "deepcopy", "(", "cate_labels", "[", "idea_idx", "]", ")", ")", "\n", "", "else", ":", "\n", "            ", "continue", "\n", "", "", "if", "verbose", ":", "\n", "        ", "print", "(", "\"{} example #: {} for categorization\"", ".", "format", "(", "data_type", ",", "len", "(", "cate_x", ")", ")", ")", "\n", "countLabelStats", "(", "gender_labels", ",", "[", "\"gender\"", ",", "\"non-gender\"", "]", ")", "\n", "countLabelStats", "(", "race_labels", ",", "[", "\"race\"", ",", "\"non-race\"", "]", ")", "\n", "countLabelStats", "(", "appear_labels", ",", "[", "\"appear\"", ",", "\"non-appear\"", "]", ")", "\n", "countLabelStats", "(", "idea_labels", ",", "[", "\"idea\"", ",", "\"non-idea\"", "]", ")", "\n", "\n", "", "return", "cate_x", ",", "cate_length", ",", "cate_attention", ",", "cate_pos", ",", "cate_pos_length", ",", "gender_labels", ",", "race_labels", ",", "appear_labels", ",", "idea_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.category_data_helpers.loadCategoryTrainData": [[63, 66], ["category_data_helpers.loadCategoryData", "data_util.data_helpers.splitTrainData"], "function", ["home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.abuse_categorization.category_data_helpers.loadCategoryData", "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.data_helpers.splitTrainData"], ["", "def", "loadCategoryTrainData", "(", ")", ":", "\n", "    ", "data", "=", "loadCategoryData", "(", "data_type", "=", "\"train\"", ",", "verbose", "=", "True", ")", "\n", "return", "data_helpers", ".", "splitTrainData", "(", "data", ",", "train_ratio", "=", "0.8", ",", "verbose", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.category_data_helpers.loadCategoryTestData": [[68, 70], ["category_data_helpers.loadCategoryData"], "function", ["home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.abuse_categorization.category_data_helpers.loadCategoryData"], ["", "def", "loadCategoryTestData", "(", ")", ":", "\n", "    ", "return", "loadCategoryData", "(", "data_type", "=", "\"test\"", ",", "verbose", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.eval_helpers.evalROC": [[7, 12], ["roc_curve", "auc", "print"], "function", ["None"], ["def", "evalROC", "(", "gold_scores", ",", "pred_scores", ")", ":", "\n", "    ", "from", "sklearn", ".", "metrics", "import", "roc_curve", ",", "auc", "\n", "fpr", ",", "tpr", ",", "_", "=", "roc_curve", "(", "gold_scores", ",", "pred_scores", ",", "pos_label", "=", "1", ")", "\n", "roc_auc", "=", "auc", "(", "fpr", ",", "tpr", ")", "\n", "print", "(", "\"roc_auc:\"", ",", "roc_auc", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.eval_helpers.evalPR": [[14, 19], ["precision_recall_curve", "auc", "print"], "function", ["None"], ["", "def", "evalPR", "(", "gold_scores", ",", "pred_scores", ")", ":", "\n", "    ", "from", "sklearn", ".", "metrics", "import", "precision_recall_curve", ",", "auc", "\n", "prec", ",", "recall", ",", "_", "=", "precision_recall_curve", "(", "gold_scores", ",", "pred_scores", ",", "pos_label", "=", "1", ")", "\n", "pr_auc", "=", "auc", "(", "recall", ",", "prec", ")", "\n", "print", "(", "\"pr_auc:\"", ",", "pr_auc", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.eval_helpers.tuneThreshold": [[21, 32], ["numpy.arange", "f1_score", "int"], "function", ["None"], ["", "def", "tuneThreshold", "(", "gold_scores", ",", "pred_scores", ")", ":", "\n", "    ", "from", "sklearn", ".", "metrics", "import", "f1_score", "\n", "best_t", "=", "0.0", "\n", "best_fscore", "=", "0.0", "\n", "for", "t", "in", "np", ".", "arange", "(", "0", ",", "1.1", ",", "0.1", ")", ":", "\n", "        ", "pred_labels", "=", "[", "int", "(", "s", ">", "t", ")", "for", "s", "in", "pred_scores", "]", "\n", "fscore", "=", "f1_score", "(", "gold_scores", ",", "pred_labels", ")", "\n", "if", "(", "best_fscore", "<", "fscore", ")", ":", "\n", "            ", "best_fscore", "=", "fscore", "\n", "best_t", "=", "t", "\n", "", "", "return", "best_t", ",", "best_fscore", "\n", "\n"]], "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.eval_helpers.evalFscore": [[34, 41], ["eval_helpers.tuneThreshold", "f1_score", "print", "int"], "function", ["home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.eval_helpers.tuneThreshold"], ["", "def", "evalFscore", "(", "train_gold_scores", ",", "train_pred_scores", ",", "test_gold_scores", ",", "test_pred_scores", ")", ":", "\n", "    ", "from", "sklearn", ".", "metrics", "import", "f1_score", "\n", "# threshold from train data", "\n", "threshold", ",", "_", "=", "tuneThreshold", "(", "train_gold_scores", ",", "train_pred_scores", ")", "\n", "test_pred_labels", "=", "[", "int", "(", "s", ">", "threshold", ")", "for", "s", "in", "test_pred_scores", "]", "\n", "fscore", "=", "f1_score", "(", "test_gold_scores", ",", "test_pred_labels", ")", "\n", "print", "(", "\"fscore: {}\"", ".", "format", "(", "fscore", ")", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.build_vocab_emb.buildVocab": [[23, 31], ["collections.Counter", "itertools.chain", "enumerate", "collections.Counter.most_common"], "function", ["None"], ["def", "buildVocab", "(", "sentences", ",", "unk", "=", "param", ".", "unk", ",", "pad", "=", "param", ".", "pad", ")", ":", "\n", "# Build vocabulary", "\n", "  ", "word_counts", "=", "Counter", "(", "itertools", ".", "chain", "(", "*", "sentences", ")", ")", "\n", "# Mapping from index to word", "\n", "vocabulary_inv", "=", "[", "unk", ",", "pad", "]", "+", "[", "x", "[", "0", "]", "for", "x", "in", "word_counts", ".", "most_common", "(", ")", "]", "\n", "# Mapping from word to index", "\n", "vocabulary", "=", "{", "x", ":", "i", "for", "i", ",", "x", "in", "enumerate", "(", "vocabulary_inv", ")", "}", "\n", "return", "[", "vocabulary", ",", "vocabulary_inv", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.build_vocab_emb.tuneEmbed": [[33, 54], ["gensim.models.word2vec.LineSentence", "print", "gensim.models.Word2Vec", "gensim.models.Word2Vec.intersect_word2vec_format", "time.time", "gensim.models.Word2Vec.train", "time.time", "print", "word_vectors.save_word2vec_format", "print"], "function", ["None"], ["", "def", "tuneEmbed", "(", "wiki_embed_fn", ",", "corpus_fn", ",", "tune_embed_fn", ")", ":", "\n", "    ", "sentences", "=", "LineSentence", "(", "corpus_fn", ")", "\n", "sent_cnt", "=", "0", "\n", "for", "sentence", "in", "sentences", ":", "\n", "        ", "sent_cnt", "+=", "1", "\n", "", "print", "(", "\"# of sents: {}\"", ".", "format", "(", "sent_cnt", ")", ")", "\n", "model", "=", "Word2Vec", "(", "\n", "sentences", ",", "min_count", "=", "1", ",", "size", "=", "param", ".", "emb_dim", ",", "\n", "window", "=", "5", ",", "iter", "=", "5", ",", "workers", "=", "10", "\n", ")", "\n", "model", ".", "intersect_word2vec_format", "(", "\n", "wiki_embed_fn", ",", "lockf", "=", "1.0", ",", "binary", "=", "False", "\n", ")", "\n", "# measure running time", "\n", "start", "=", "time", ".", "time", "(", ")", "\n", "model", ".", "train", "(", "sentences", ",", "total_examples", "=", "model", ".", "corpus_count", ",", "epochs", "=", "model", ".", "iter", ")", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "print", "(", "\"Done embedding tuning, time used: {}s\"", ".", "format", "(", "end", "-", "start", ")", ")", "\n", "word_vectors", "=", "model", ".", "wv", "\n", "word_vectors", ".", "save_word2vec_format", "(", "tune_embed_fn", ")", "\n", "print", "(", "\"Saving embedding to {}\"", ".", "format", "(", "tune_embed_fn", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.build_vocab_emb.loadTuneEmbed": [[56, 92], ["open", "open.readline", "numpy.array", "print", "open.close", "print", "numpy.array", "print", "f.readline.strip().split", "min", "open.readline", "f.readline.strip().split", "np.array.append", "len", "all_vocab.append", "numpy.array", "all_embed_arr.append", "all_vocab.index", "f.readline.strip", "f.readline.strip", "print", "sys.exit", "unknown_word.append", "numpy.random.rand"], "function", ["None"], ["", "def", "loadTuneEmbed", "(", "vocabulary", ",", "embed_fn", ",", "word_limit", "=", "50000", ")", ":", "\n", "  ", "f", "=", "open", "(", "embed_fn", ",", "\"r\"", ")", "\n", "# parameters of word vectors", "\n", "header", "=", "f", ".", "readline", "(", ")", "\n", "vocab_size", ",", "dim", "=", "np", ".", "array", "(", "header", ".", "strip", "(", ")", ".", "split", "(", ")", ",", "\"int\"", ")", "\n", "print", "(", "\"load from {}, vector dim: {}\"", ".", "format", "(", "embed_fn", ",", "dim", ")", ")", "\n", "\n", "all_vocab", "=", "[", "]", "\n", "all_embed_arr", "=", "[", "]", "\n", "i", "=", "0", "\n", "while", "(", "i", "<", "min", "(", "word_limit", ",", "vocab_size", ")", ")", ":", "\n", "    ", "line", "=", "f", ".", "readline", "(", ")", "\n", "i", "+=", "1", "\n", "seq", "=", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "try", ":", "\n", "        ", "all_vocab", ".", "append", "(", "seq", "[", "0", "]", ")", "\n", "vec", "=", "np", ".", "array", "(", "seq", "[", "1", ":", "]", ",", "\"float\"", ")", "\n", "all_embed_arr", ".", "append", "(", "vec", "[", ":", "]", ")", "\n", "", "except", ":", "\n", "        ", "print", "(", "\"wrong line:\"", ",", "i", ")", "\n", "sys", ".", "exit", "(", "0", ")", "\n", "", "", "f", ".", "close", "(", ")", "\n", "init_embed", "=", "[", "]", "\n", "unknown_word", "=", "[", "]", "\n", "for", "w", "in", "vocabulary", ":", "\n", "    ", "try", ":", "\n", "      ", "ind", "=", "all_vocab", ".", "index", "(", "w", ")", "\n", "vec", "=", "all_embed_arr", "[", "ind", "]", "\n", "", "except", ":", "\n", "      ", "vec", "=", "(", "np", ".", "random", ".", "rand", "(", "dim", ")", "-", "0.5", ")", "*", "2", "# random vec generation [-1, 1]", "\n", "unknown_word", ".", "append", "(", "w", ")", "\n", "", "init_embed", ".", "append", "(", "vec", "[", ":", "]", ")", "\n", "", "print", "(", "\"unknown word:\"", ",", "len", "(", "unknown_word", ")", ",", "unknown_word", "[", ":", "10", "]", ")", "\n", "init_embed", "=", "np", ".", "array", "(", "init_embed", ")", "\n", "print", "(", "\"init_embed shape\"", ",", "init_embed", ".", "shape", ")", "\n", "return", "init_embed", "\n", "\n"]], "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.build_vocab_emb.savePOSVocab": [[94, 104], ["print", "build_vocab_emb.buildVocab", "print", "open", "open", "pickle.dump", "sent_list.append", "len", "len", "line.strip().split", "line.strip"], "function", ["home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.build_vocab_emb.buildVocab"], ["", "def", "savePOSVocab", "(", "corpus_path", ",", "vocab_pkl", ")", ":", "\n", "    ", "sent_list", "=", "[", "]", "\n", "with", "open", "(", "corpus_path", ",", "\"r\"", ")", "as", "fin", ":", "\n", "        ", "for", "line", "in", "fin", ":", "\n", "            ", "sent_list", ".", "append", "(", "line", ".", "strip", "(", ")", ".", "split", "(", ")", ")", "\n", "", "", "print", "(", "\"# of sents for pos vocab: {}\"", ".", "format", "(", "len", "(", "sent_list", ")", ")", ")", "\n", "vocabulary", ",", "vocabulary_inv", "=", "buildVocab", "(", "sent_list", ")", "\n", "with", "open", "(", "vocab_pkl", ",", "\"wb\"", ")", "as", "handle", ":", "\n", "        ", "pickle", ".", "dump", "(", "vocabulary", ",", "handle", ")", "\n", "", "print", "(", "\"Vocab size: {}, save to {}\"", ".", "format", "(", "len", "(", "vocabulary", ")", ",", "vocab_pkl", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.build_vocab_emb.saveVocabEmbed": [[106, 120], ["print", "build_vocab_emb.buildVocab", "print", "build_vocab_emb.loadTuneEmbed", "print", "open", "open", "pickle.dump", "open", "pickle.dump", "sent_list.append", "len", "len", "line.split"], "function", ["home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.build_vocab_emb.buildVocab", "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.build_vocab_emb.loadTuneEmbed"], ["", "def", "saveVocabEmbed", "(", "corpus_path", ",", "vocab_pkl", ",", "embed_fn", ",", "embed_pkl", ")", ":", "\n", "    ", "sent_list", "=", "[", "]", "\n", "with", "open", "(", "corpus_path", ",", "\"r\"", ")", "as", "fin", ":", "\n", "        ", "for", "line", "in", "fin", ":", "\n", "            ", "sent_list", ".", "append", "(", "line", ".", "split", "(", ")", ")", "\n", "", "", "print", "(", "\"# of sents for vocab: {}\"", ".", "format", "(", "len", "(", "sent_list", ")", ")", ")", "\n", "vocabulary", ",", "vocabulary_inv", "=", "buildVocab", "(", "sent_list", ")", "\n", "with", "open", "(", "vocab_pkl", ",", "\"wb\"", ")", "as", "handle", ":", "\n", "        ", "pickle", ".", "dump", "(", "vocabulary", ",", "handle", ")", "\n", "", "print", "(", "\"Vocab size: {}, save to {}\"", ".", "format", "(", "len", "(", "vocabulary", ")", ",", "vocab_pkl", ")", ")", "\n", "init_embed", "=", "loadTuneEmbed", "(", "vocabulary", ",", "embed_fn", ")", "\n", "with", "open", "(", "embed_pkl", ",", "\"wb\"", ")", "as", "handle", ":", "\n", "        ", "pickle", ".", "dump", "(", "init_embed", ",", "handle", ")", "\n", "", "print", "(", "\"Save embedding to {}\"", ".", "format", "(", "embed_pkl", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.build_vocab_emb.normEmbed": [[122, 133], ["numpy.array", "print", "open", "pickle.load", "np.array.append", "open", "pickle.dump", "numpy.array", "numpy.linalg.norm", "list"], "function", ["None"], ["", "def", "normEmbed", "(", "embed_pkl", ",", "norm_embed_pkl", ")", ":", "\n", "    ", "with", "open", "(", "embed_pkl", ",", "\"rb\"", ")", "as", "handle", ":", "\n", "        ", "init_embed", "=", "pickle", ".", "load", "(", "handle", ")", "\n", "", "norm_init_embed", "=", "[", "]", "\n", "for", "vec", "in", "init_embed", ":", "\n", "        ", "norm_vec", "=", "np", ".", "array", "(", "vec", ")", "/", "np", ".", "linalg", ".", "norm", "(", "vec", ")", "\n", "norm_init_embed", ".", "append", "(", "list", "(", "norm_vec", ")", "[", ":", "]", ")", "\n", "", "norm_init_embed", "=", "np", ".", "array", "(", "norm_init_embed", ")", "\n", "with", "open", "(", "norm_embed_pkl", ",", "\"wb\"", ")", "as", "handle", ":", "\n", "        ", "pickle", ".", "dump", "(", "norm_init_embed", ",", "handle", ")", "\n", "", "print", "(", "\"done saving normalized vectors.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.model.attention.attention": [[4, 58], ["isinstance", "tensorflow.Variable", "tensorflow.Variable", "tensorflow.Variable", "tensorflow.tanh", "tensorflow.matmul", "tensorflow.reshape", "tensorflow.nn.softmax", "tensorflow.reduce_sum", "tensorflow.concat", "tensorflow.array_ops.transpose", "tensorflow.random_normal", "tensorflow.random_normal", "tensorflow.random_normal", "tensorflow.expand_dims", "tensorflow.matmul", "tensorflow.expand_dims", "tensorflow.shape", "tensorflow.expand_dims", "tensorflow.reshape"], "function", ["None"], ["def", "attention", "(", "inputs", ",", "attention_size", ",", "time_major", "=", "False", ",", "return_alphas", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Attention mechanism layer which reduces RNN/Bi-RNN outputs with Attention vector.\n   \n    Args:\n        inputs: The Attention inputs.\n        attention_size: Linear size of the Attention weights.\n        time_major: The shape format of the `inputs` Tensors.\n            If true, these `Tensors` must be shaped `[max_time, batch_size, depth]`.\n            If false, these `Tensors` must be shaped `[batch_size, max_time, depth]`.\n            Using `time_major = True` is a bit more efficient because it avoids\n            transposes at the beginning and end of the RNN calculation.  However,\n            most TensorFlow data is batch-major, so by default this function\n            accepts input and emits output in batch-major form.\n        return_alphas: Whether to return attention coefficients variable along with layer's output.\n            Used for visualization purpose.\n    Returns:\n        The Attention output `Tensor`.\n        In case of RNN, this will be a `Tensor` shaped:\n            `[batch_size, cell.output_size]`.\n        In case of Bidirectional RNN, this will be a `Tensor` shaped:\n            `[batch_size, cell_fw.output_size + cell_bw.output_size]`.\n    \"\"\"", "\n", "\n", "if", "isinstance", "(", "inputs", ",", "tuple", ")", ":", "\n", "# In case of Bi-RNN, concatenate the forward and the backward RNN outputs.", "\n", "        ", "inputs", "=", "tf", ".", "concat", "(", "inputs", ",", "2", ")", "\n", "\n", "", "if", "time_major", ":", "\n", "# (T,B,D) => (B,T,D)", "\n", "        ", "inputs", "=", "tf", ".", "array_ops", ".", "transpose", "(", "inputs", ",", "[", "1", ",", "0", ",", "2", "]", ")", "\n", "\n", "", "hidden_size", "=", "inputs", ".", "shape", "[", "2", "]", ".", "value", "# D value - hidden size of the RNN layer", "\n", "\n", "# Trainable parameters", "\n", "W_omega", "=", "tf", ".", "Variable", "(", "tf", ".", "random_normal", "(", "[", "hidden_size", ",", "attention_size", "]", ",", "stddev", "=", "0.1", ")", ")", "\n", "b_omega", "=", "tf", ".", "Variable", "(", "tf", ".", "random_normal", "(", "[", "attention_size", "]", ",", "stddev", "=", "0.1", ")", ")", "\n", "u_omega", "=", "tf", ".", "Variable", "(", "tf", ".", "random_normal", "(", "[", "attention_size", "]", ",", "stddev", "=", "0.1", ")", ")", "\n", "\n", "# One fully connected layer with non-linear activation for each of the hidden states;", "\n", "#  the shape of `v` is (B*T,A), where A=attention_size", "\n", "v", "=", "tf", ".", "tanh", "(", "tf", ".", "matmul", "(", "tf", ".", "reshape", "(", "inputs", ",", "[", "-", "1", ",", "hidden_size", "]", ")", ",", "W_omega", ")", "+", "tf", ".", "expand_dims", "(", "b_omega", ",", "0", ")", ")", "\n", "# For each of the B*T hidden states its vector of size A from `v` is reduced with `u` vector", "\n", "vu", "=", "tf", ".", "matmul", "(", "v", ",", "tf", ".", "expand_dims", "(", "u_omega", ",", "-", "1", ")", ")", "# (B*T, 1) shape", "\n", "vu", "=", "tf", ".", "reshape", "(", "vu", ",", "tf", ".", "shape", "(", "inputs", ")", "[", ":", "2", "]", ")", "# (B,T) shape", "\n", "alphas", "=", "tf", ".", "nn", ".", "softmax", "(", "vu", ")", "# (B,T) shape also", "\n", "\n", "# Output of (Bi-)RNN is reduced with attention vector; the result has (B,D) shape", "\n", "output", "=", "tf", ".", "reduce_sum", "(", "inputs", "*", "tf", ".", "expand_dims", "(", "alphas", ",", "-", "1", ")", ",", "1", ")", "\n", "\n", "if", "not", "return_alphas", ":", "\n", "        ", "return", "output", "\n", "", "else", ":", "\n", "        ", "return", "output", ",", "alphas", "\n", "", "", ""]], "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.model.abuse_classifier.AbuseClassifier.__init__": [[12, 99], ["tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.constant", "tensorflow.name_scope", "tensorflow.Variable", "tensorflow.nn.embedding_lookup", "tensorflow.variable_scope", "model.attention.attention", "tensorflow.nn.dropout", "tensorflow.name_scope", "tensorflow.Variable", "tensorflow.Variable", "tensorflow.nn.tanh", "tensorflow.nn.l2_loss", "tensorflow.nn.l2_loss", "tensorflow.name_scope", "tensorflow.Variable", "tensorflow.Variable", "tensorflow.nn.xw_plus_b", "tensorflow.nn.softmax", "tensorflow.nn.l2_loss", "tensorflow.nn.l2_loss", "tensorflow.name_scope", "tensorflow.reduce_mean", "tensorflow.name_scope", "tensorflow.Variable", "tensorflow.nn.embedding_lookup", "tensorflow.concat", "tensorflow.python.ops.rnn.bidirectional_dynamic_rnn", "tensorflow.truncated_normal", "tensorflow.constant", "tensorflow.nn.xw_plus_b", "tensorflow.truncated_normal", "tensorflow.constant", "tensorflow.nn.softmax_cross_entropy_with_logits", "print", "tensorflow.Variable", "tensorflow.Variable", "tensorflow.nn.tanh", "tensorflow.Variable", "tensorflow.Variable", "tensorflow.nn.tanh", "tensorflow.eye", "tensorflow.contrib.rnn.GRUCell", "tensorflow.contrib.rnn.GRUCell", "tensorflow.python.ops.rnn.bidirectional_dynamic_rnn", "Exception", "tensorflow.truncated_normal", "tensorflow.constant", "tensorflow.nn.xw_plus_b", "tensorflow.truncated_normal", "tensorflow.constant", "tensorflow.nn.xw_plus_b", "tensorflow.reduce_mean", "print", "tensorflow.reduce_mean", "tensorflow.contrib.rnn.LSTMCell", "tensorflow.contrib.rnn.LSTMCell", "tensorflow.multiply", "tensorflow.abs", "print", "tensorflow.reduce_mean", "print", "tensorflow.constant", "tensorflow.subtract", "tensorflow.square", "tensorflow.nn.dropout.get_shape", "tensorflow.nn.tanh.get_shape", "tensorflow.nn.softmax", "tensorflow.subtract", "abuse_classifier.AbuseClassifier.input_attention.get_shape", "abuse_classifier.AbuseClassifier.alphas.get_shape", "tensorflow.nn.softmax"], "methods", ["home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.model.attention.attention"], ["    ", "def", "__init__", "(", "self", ",", "max_sequence_length", ",", "num_classes", ",", "pos_vocab_size", ",", "init_embed", ",", "hidden_size", ",", "attention_size", ",", "keep_prob", ",", "attention_lambda", ",", "attention_loss_type", ",", "l2_reg_lambda", ",", "use_pos_flag", "=", "True", ",", "rnn_cell", "=", "\"lstm\"", ")", ":", "\n", "# word index", "\n", "        ", "self", ".", "input_word", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "None", ",", "max_sequence_length", "]", ",", "name", "=", "\"input_word\"", ")", "\n", "# pos index", "\n", "self", ".", "input_pos", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "None", ",", "max_sequence_length", "]", ",", "name", "=", "\"input_pos\"", ")", "\n", "# sequence length of words", "\n", "self", ".", "sequence_length", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "None", "]", ",", "name", "=", "\"length\"", ")", "\n", "# attention over x", "\n", "self", ".", "input_attention", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "None", ",", "max_sequence_length", "]", ",", "name", "=", "\"input_attention\"", ")", "\n", "# output probability", "\n", "self", ".", "input_y", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "None", ",", "num_classes", "]", ",", "name", "=", "\"input_y\"", ")", "\n", "self", ".", "dropout_keep_prob", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "name", "=", "\"dropout_keep_prob\"", ")", "\n", "l2_loss", "=", "tf", ".", "constant", "(", "0.0", ")", "\n", "\n", "# embedding layer with initialization of words and pos tags", "\n", "with", "tf", ".", "name_scope", "(", "\"embedding\"", ")", ":", "\n", "            ", "W", "=", "tf", ".", "Variable", "(", "init_embed", ",", "name", "=", "\"W\"", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "self", ".", "embedded_chars", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "W", ",", "self", ".", "input_word", ")", "\n", "self", ".", "embedded_input", "=", "self", ".", "embedded_chars", "\n", "\n", "", "if", "(", "use_pos_flag", ")", ":", "\n", "            ", "with", "tf", ".", "name_scope", "(", "\"pos_embedding\"", ")", ":", "\n", "                ", "W_pos", "=", "tf", ".", "Variable", "(", "tf", ".", "eye", "(", "pos_vocab_size", ")", ",", "name", "=", "\"W_pos\"", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "self", ".", "embedded_pos", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "W_pos", ",", "self", ".", "input_pos", ")", "\n", "self", ".", "embedded_input", "=", "tf", ".", "concat", "(", "[", "self", ".", "embedded_chars", ",", "self", ".", "embedded_pos", "]", ",", "axis", "=", "-", "1", ")", "\n", "\n", "# RNN layer + attention for words", "\n", "", "", "with", "tf", ".", "variable_scope", "(", "\"bi-rnn\"", ")", ":", "\n", "            ", "if", "rnn_cell", "==", "\"gru\"", ":", "\n", "                ", "rnn_outputs", ",", "_", "=", "bi_rnn", "(", "GRUCell", "(", "hidden_size", ")", ",", "GRUCell", "(", "hidden_size", ")", ",", "inputs", "=", "self", ".", "embedded_input", ",", "sequence_length", "=", "self", ".", "sequence_length", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "", "elif", "rnn_cell", "==", "\"lstm\"", ":", "\n", "                ", "rnn_outputs", ",", "_", "=", "bi_rnn", "(", "LSTMCell", "(", "hidden_size", ")", ",", "LSTMCell", "(", "hidden_size", ")", ",", "inputs", "=", "self", ".", "embedded_input", ",", "sequence_length", "=", "self", ".", "sequence_length", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "", "else", ":", "\n", "                ", "raise", "Exception", "(", "\"Cell type {} is not supported!\"", ".", "format", "(", "rnn_cell", ")", ")", "\n", "", "attention_outputs", ",", "self", ".", "alphas", "=", "attention", "(", "rnn_outputs", ",", "attention_size", ",", "return_alphas", "=", "True", ")", "\n", "drop_outputs", "=", "tf", ".", "nn", ".", "dropout", "(", "attention_outputs", ",", "keep_prob", ")", "\n", "\n", "# Fully connected layer by taking both rnn-words and rnn-pos as inputs", "\n", "", "with", "tf", ".", "name_scope", "(", "\"fc-layer-1\"", ")", ":", "\n", "            ", "fc_dim", "=", "10", "\n", "W", "=", "tf", ".", "Variable", "(", "tf", ".", "truncated_normal", "(", "[", "drop_outputs", ".", "get_shape", "(", ")", "[", "1", "]", ".", "value", ",", "fc_dim", "]", ",", "stddev", "=", "0.1", ")", ",", "name", "=", "\"W\"", ")", "\n", "b", "=", "tf", ".", "Variable", "(", "tf", ".", "constant", "(", "0.1", ",", "shape", "=", "[", "fc_dim", "]", ")", ",", "name", "=", "\"b\"", ")", "\n", "fc_outputs", "=", "tf", ".", "nn", ".", "tanh", "(", "tf", ".", "nn", ".", "xw_plus_b", "(", "drop_outputs", ",", "W", ",", "b", ")", ")", "\n", "l2_loss", "+=", "tf", ".", "nn", ".", "l2_loss", "(", "W", ")", "\n", "l2_loss", "+=", "tf", ".", "nn", ".", "l2_loss", "(", "b", ")", "\n", "\n", "", "with", "tf", ".", "name_scope", "(", "\"fc-layer-2\"", ")", ":", "\n", "            ", "W", "=", "tf", ".", "Variable", "(", "tf", ".", "truncated_normal", "(", "[", "fc_outputs", ".", "get_shape", "(", ")", "[", "1", "]", ".", "value", ",", "num_classes", "]", ",", "stddev", "=", "0.1", ")", ",", "name", "=", "\"W\"", ")", "\n", "b", "=", "tf", ".", "Variable", "(", "tf", ".", "constant", "(", "0.1", ",", "shape", "=", "[", "num_classes", "]", ")", ",", "name", "=", "\"b\"", ")", "\n", "self", ".", "logits", "=", "tf", ".", "nn", ".", "xw_plus_b", "(", "fc_outputs", ",", "W", ",", "b", ")", "\n", "self", ".", "prob", "=", "tf", ".", "nn", ".", "softmax", "(", "self", ".", "logits", ")", "\n", "l2_loss", "+=", "tf", ".", "nn", ".", "l2_loss", "(", "W", ")", "\n", "l2_loss", "+=", "tf", ".", "nn", ".", "l2_loss", "(", "b", ")", "\n", "\n", "", "with", "tf", ".", "name_scope", "(", "\"cross_entropy\"", ")", ":", "\n", "            ", "entropy_loss", "=", "tf", ".", "reduce_mean", "(", "tf", ".", "nn", ".", "softmax_cross_entropy_with_logits", "(", "labels", "=", "self", ".", "input_y", ",", "logits", "=", "self", ".", "logits", ")", ")", "\n", "if", "(", "attention_loss_type", "==", "\"encoded\"", ")", ":", "\n", "                ", "print", "(", "\"Supervised attention with encoded loss.\"", ")", "\n", "att_shared_dim", "=", "20", "\n", "# rationale input_attention: (batch_size, max_sent_len)", "\n", "# W: (max_sent_len, att_shared_dim)", "\n", "# b: (att_shared_dim,)", "\n", "# proj: (batch_size, att_shared_dim)", "\n", "ration_W", "=", "tf", ".", "Variable", "(", "tf", ".", "truncated_normal", "(", "[", "self", ".", "input_attention", ".", "get_shape", "(", ")", "[", "1", "]", ".", "value", ",", "att_shared_dim", "]", ",", "stddev", "=", "0.1", ")", ",", "name", "=", "\"ration_W\"", ")", "\n", "ration_b", "=", "tf", ".", "Variable", "(", "tf", ".", "constant", "(", "0.05", ",", "shape", "=", "[", "att_shared_dim", "]", ")", ",", "name", "=", "\"ration_b\"", ")", "\n", "proj_ration", "=", "tf", ".", "nn", ".", "tanh", "(", "tf", ".", "nn", ".", "xw_plus_b", "(", "self", ".", "input_attention", ",", "ration_W", ",", "ration_b", ")", ")", "\n", "alpha_W", "=", "tf", ".", "Variable", "(", "tf", ".", "truncated_normal", "(", "[", "self", ".", "alphas", ".", "get_shape", "(", ")", "[", "1", "]", ".", "value", ",", "att_shared_dim", "]", ",", "stddev", "=", "0.1", ")", ",", "name", "=", "\"alpha_W\"", ")", "\n", "alpha_b", "=", "tf", ".", "Variable", "(", "tf", ".", "constant", "(", "0.05", ",", "shape", "=", "[", "att_shared_dim", "]", ")", ",", "name", "=", "\"alpha_b\"", ")", "\n", "proj_alphas", "=", "tf", ".", "nn", ".", "tanh", "(", "tf", ".", "nn", ".", "xw_plus_b", "(", "self", ".", "alphas", ",", "alpha_W", ",", "alpha_b", ")", ")", "\n", "# negative of inner product", "\n", "attention_loss", "=", "-", "1", "*", "tf", ".", "reduce_mean", "(", "tf", ".", "multiply", "(", "proj_ration", ",", "proj_alphas", ")", ")", "\n", "", "elif", "(", "attention_loss_type", "==", "\"l1\"", ")", ":", "\n", "                ", "print", "(", "\"Supervised attention with L1 loss.\"", ")", "\n", "attention_loss", "=", "tf", ".", "reduce_mean", "(", "tf", ".", "abs", "(", "tf", ".", "subtract", "(", "tf", ".", "nn", ".", "softmax", "(", "self", ".", "input_attention", ")", ",", "self", ".", "alphas", ")", ")", ")", "\n", "", "elif", "(", "attention_loss_type", "==", "\"l2\"", ")", ":", "\n", "                ", "print", "(", "\"Supervised attention with L2 loss.\"", ")", "\n", "attention_loss", "=", "tf", ".", "reduce_mean", "(", "tf", ".", "square", "(", "tf", ".", "subtract", "(", "tf", ".", "nn", ".", "softmax", "(", "self", ".", "input_attention", ")", ",", "self", ".", "alphas", ")", ")", ")", "\n", "", "else", ":", "\n", "                ", "print", "(", "\"No supervised attention.\"", ")", "\n", "attention_loss", "=", "tf", ".", "constant", "(", "0.0", ")", "\n", "", "self", ".", "loss", "=", "entropy_loss", "+", "attention_lambda", "*", "attention_loss", "+", "l2_reg_lambda", "*", "l2_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.model.abuse_categorizer.AbuseCategorizer.__init__": [[12, 136], ["tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.constant", "tensorflow.name_scope", "tensorflow.Variable", "tensorflow.nn.embedding_lookup", "tensorflow.variable_scope", "model.attention.attention", "tensorflow.nn.dropout", "tensorflow.name_scope", "tensorflow.Variable", "tensorflow.Variable", "tensorflow.nn.tanh", "tensorflow.nn.l2_loss", "tensorflow.nn.l2_loss", "tensorflow.name_scope", "tensorflow.Variable", "tensorflow.Variable", "tensorflow.nn.xw_plus_b", "tensorflow.nn.softmax", "tensorflow.nn.l2_loss", "tensorflow.nn.l2_loss", "tensorflow.Variable", "tensorflow.Variable", "tensorflow.nn.xw_plus_b", "tensorflow.nn.softmax", "tensorflow.nn.l2_loss", "tensorflow.nn.l2_loss", "tensorflow.Variable", "tensorflow.Variable", "tensorflow.nn.xw_plus_b", "tensorflow.nn.softmax", "tensorflow.nn.l2_loss", "tensorflow.nn.l2_loss", "tensorflow.Variable", "tensorflow.Variable", "tensorflow.nn.xw_plus_b", "tensorflow.nn.softmax", "tensorflow.nn.l2_loss", "tensorflow.nn.l2_loss", "tensorflow.name_scope", "tensorflow.nn.softmax_cross_entropy_with_logits", "tensorflow.nn.softmax_cross_entropy_with_logits", "tensorflow.nn.softmax_cross_entropy_with_logits", "tensorflow.nn.softmax_cross_entropy_with_logits", "tensorflow.reduce_mean", "tensorflow.name_scope", "tensorflow.Variable", "tensorflow.nn.embedding_lookup", "tensorflow.concat", "tensorflow.python.ops.rnn.bidirectional_dynamic_rnn", "tensorflow.truncated_normal", "tensorflow.constant", "tensorflow.nn.xw_plus_b", "tensorflow.truncated_normal", "tensorflow.constant", "tensorflow.truncated_normal", "tensorflow.constant", "tensorflow.truncated_normal", "tensorflow.constant", "tensorflow.truncated_normal", "tensorflow.constant", "print", "tensorflow.Variable", "tensorflow.Variable", "tensorflow.nn.tanh", "tensorflow.Variable", "tensorflow.Variable", "tensorflow.nn.tanh", "tensorflow.eye", "tensorflow.contrib.rnn.GRUCell", "tensorflow.contrib.rnn.GRUCell", "tensorflow.python.ops.rnn.bidirectional_dynamic_rnn", "Exception", "tensorflow.truncated_normal", "tensorflow.constant", "tensorflow.nn.xw_plus_b", "tensorflow.truncated_normal", "tensorflow.constant", "tensorflow.nn.xw_plus_b", "tensorflow.reduce_mean", "print", "tensorflow.reduce_mean", "tensorflow.contrib.rnn.LSTMCell", "tensorflow.contrib.rnn.LSTMCell", "tensorflow.multiply", "tensorflow.abs", "print", "tensorflow.reduce_mean", "print", "tensorflow.constant", "tensorflow.subtract", "tensorflow.square", "tensorflow.nn.dropout.get_shape", "tensorflow.nn.tanh.get_shape", "tensorflow.nn.tanh.get_shape", "tensorflow.nn.tanh.get_shape", "tensorflow.nn.tanh.get_shape", "tensorflow.nn.softmax", "tensorflow.subtract", "abuse_categorizer.AbuseCategorizer.input_attention.get_shape", "abuse_categorizer.AbuseCategorizer.alphas.get_shape", "tensorflow.nn.softmax"], "methods", ["home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.model.attention.attention"], ["    ", "def", "__init__", "(", "self", ",", "max_sequence_length", ",", "num_classes", ",", "pos_vocab_size", ",", "init_embed", ",", "hidden_size", ",", "attention_size", ",", "keep_prob", ",", "attention_lambda", ",", "attention_loss_type", ",", "l2_reg_lambda", ",", "use_pos_flag", ",", "category_weights", ",", "rnn_cell", "=", "\"lstm\"", ")", ":", "\n", "# word index", "\n", "        ", "self", ".", "input_x", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "None", ",", "max_sequence_length", "]", ",", "name", "=", "\"input_x\"", ")", "\n", "# pos index", "\n", "self", ".", "input_pos", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "None", ",", "max_sequence_length", "]", ",", "name", "=", "\"input_pos\"", ")", "\n", "# sequence length of words", "\n", "self", ".", "sequence_length", "=", "tf", ".", "placeholder", "(", "tf", ".", "int32", ",", "[", "None", "]", ",", "name", "=", "\"length\"", ")", "\n", "# attention over x", "\n", "self", ".", "input_attention", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "None", ",", "max_sequence_length", "]", ",", "name", "=", "\"input_attention\"", ")", "\n", "self", ".", "gender_weight", ",", "self", ".", "race_weight", ",", "self", ".", "appear_weight", ",", "self", ".", "idea_weight", "=", "category_weights", "\n", "# gender", "\n", "self", ".", "input_y_gender", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "None", ",", "2", "]", ",", "name", "=", "\"input_y_gender\"", ")", "\n", "# race", "\n", "self", ".", "input_y_race", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "None", ",", "2", "]", ",", "name", "=", "\"input_y_race\"", ")", "\n", "# appearance", "\n", "self", ".", "input_y_appear", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "None", ",", "2", "]", ",", "name", "=", "\"input_y_appear\"", ")", "\n", "# ideology", "\n", "self", ".", "input_y_idea", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "None", ",", "2", "]", ",", "name", "=", "\"input_y_idea\"", ")", "\n", "\n", "self", ".", "dropout_keep_prob", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "name", "=", "\"dropout_keep_prob\"", ")", "\n", "l2_loss", "=", "tf", ".", "constant", "(", "0.0", ")", "\n", "\n", "# embedding layer with initialization of words and pos tags", "\n", "with", "tf", ".", "name_scope", "(", "\"embedding\"", ")", ":", "\n", "            ", "W", "=", "tf", ".", "Variable", "(", "init_embed", ",", "name", "=", "\"W\"", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "self", ".", "embedded_chars", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "W", ",", "self", ".", "input_x", ")", "\n", "self", ".", "embedded_input", "=", "self", ".", "embedded_chars", "\n", "\n", "", "if", "(", "use_pos_flag", ")", ":", "\n", "            ", "with", "tf", ".", "name_scope", "(", "\"pos_embedding\"", ")", ":", "\n", "                ", "W_pos", "=", "tf", ".", "Variable", "(", "tf", ".", "eye", "(", "pos_vocab_size", ")", ",", "name", "=", "\"W_pos\"", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "self", ".", "embedded_pos", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "W_pos", ",", "self", ".", "input_pos", ")", "\n", "self", ".", "embedded_input", "=", "tf", ".", "concat", "(", "[", "self", ".", "embedded_chars", ",", "self", ".", "embedded_pos", "]", ",", "axis", "=", "-", "1", ")", "\n", "\n", "# RNN layer + attention for words", "\n", "", "", "with", "tf", ".", "variable_scope", "(", "\"bi-rnn\"", ")", ":", "\n", "            ", "if", "rnn_cell", "==", "\"gru\"", ":", "\n", "                ", "rnn_outputs", ",", "_", "=", "bi_rnn", "(", "GRUCell", "(", "hidden_size", ")", ",", "GRUCell", "(", "hidden_size", ")", ",", "inputs", "=", "self", ".", "embedded_input", ",", "sequence_length", "=", "self", ".", "sequence_length", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "", "elif", "rnn_cell", "==", "\"lstm\"", ":", "\n", "                ", "rnn_outputs", ",", "_", "=", "bi_rnn", "(", "LSTMCell", "(", "hidden_size", ")", ",", "LSTMCell", "(", "hidden_size", ")", ",", "inputs", "=", "self", ".", "embedded_input", ",", "sequence_length", "=", "self", ".", "sequence_length", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "", "else", ":", "\n", "                ", "raise", "Exception", "(", "\"Cell type {} is not supported!\"", ".", "format", "(", "rnn_cell", ")", ")", "\n", "", "attention_outputs", ",", "self", ".", "alphas", "=", "attention", "(", "rnn_outputs", ",", "attention_size", ",", "return_alphas", "=", "True", ")", "\n", "drop_outputs", "=", "tf", ".", "nn", ".", "dropout", "(", "attention_outputs", ",", "keep_prob", ")", "\n", "\n", "# Fully connected layer", "\n", "", "with", "tf", ".", "name_scope", "(", "\"fc-layer-1\"", ")", ":", "\n", "            ", "fc_dim", "=", "10", "\n", "W", "=", "tf", ".", "Variable", "(", "tf", ".", "truncated_normal", "(", "[", "drop_outputs", ".", "get_shape", "(", ")", "[", "1", "]", ".", "value", ",", "fc_dim", "]", ",", "stddev", "=", "0.1", ")", ",", "name", "=", "\"W\"", ")", "\n", "b", "=", "tf", ".", "Variable", "(", "tf", ".", "constant", "(", "0.1", ",", "shape", "=", "[", "fc_dim", "]", ")", ",", "name", "=", "\"b\"", ")", "\n", "fc_outputs", "=", "tf", ".", "nn", ".", "tanh", "(", "tf", ".", "nn", ".", "xw_plus_b", "(", "drop_outputs", ",", "W", ",", "b", ")", ")", "\n", "l2_loss", "+=", "tf", ".", "nn", ".", "l2_loss", "(", "W", ")", "\n", "l2_loss", "+=", "tf", ".", "nn", ".", "l2_loss", "(", "b", ")", "\n", "\n", "", "with", "tf", ".", "name_scope", "(", "\"fc-layer-2\"", ")", ":", "\n", "# gender", "\n", "            ", "W_gender", "=", "tf", ".", "Variable", "(", "tf", ".", "truncated_normal", "(", "[", "fc_outputs", ".", "get_shape", "(", ")", "[", "1", "]", ".", "value", ",", "num_classes", "]", ",", "stddev", "=", "0.1", ")", ",", "name", "=", "\"W_gender\"", ")", "\n", "b_gender", "=", "tf", ".", "Variable", "(", "tf", ".", "constant", "(", "0.1", ",", "shape", "=", "[", "num_classes", "]", ")", ",", "name", "=", "\"b_gender\"", ")", "\n", "self", ".", "logits_gender", "=", "tf", ".", "nn", ".", "xw_plus_b", "(", "fc_outputs", ",", "W_gender", ",", "b_gender", ")", "\n", "self", ".", "prob_gender", "=", "tf", ".", "nn", ".", "softmax", "(", "self", ".", "logits_gender", ")", "\n", "l2_loss", "+=", "tf", ".", "nn", ".", "l2_loss", "(", "W_gender", ")", "\n", "l2_loss", "+=", "tf", ".", "nn", ".", "l2_loss", "(", "b_gender", ")", "\n", "# race", "\n", "W_race", "=", "tf", ".", "Variable", "(", "tf", ".", "truncated_normal", "(", "[", "fc_outputs", ".", "get_shape", "(", ")", "[", "1", "]", ".", "value", ",", "num_classes", "]", ",", "stddev", "=", "0.1", ")", ",", "name", "=", "\"W_race\"", ")", "\n", "b_race", "=", "tf", ".", "Variable", "(", "tf", ".", "constant", "(", "0.1", ",", "shape", "=", "[", "num_classes", "]", ")", ",", "name", "=", "\"b_race\"", ")", "\n", "self", ".", "logits_race", "=", "tf", ".", "nn", ".", "xw_plus_b", "(", "fc_outputs", ",", "W_race", ",", "b_race", ")", "\n", "self", ".", "prob_race", "=", "tf", ".", "nn", ".", "softmax", "(", "self", ".", "logits_race", ")", "\n", "l2_loss", "+=", "tf", ".", "nn", ".", "l2_loss", "(", "W_race", ")", "\n", "l2_loss", "+=", "tf", ".", "nn", ".", "l2_loss", "(", "b_race", ")", "\n", "# appearance", "\n", "W_appear", "=", "tf", ".", "Variable", "(", "tf", ".", "truncated_normal", "(", "[", "fc_outputs", ".", "get_shape", "(", ")", "[", "1", "]", ".", "value", ",", "num_classes", "]", ",", "stddev", "=", "0.1", ")", ",", "name", "=", "\"W_appear\"", ")", "\n", "b_appear", "=", "tf", ".", "Variable", "(", "tf", ".", "constant", "(", "0.1", ",", "shape", "=", "[", "num_classes", "]", ")", ",", "name", "=", "\"b_appear\"", ")", "\n", "self", ".", "logits_appear", "=", "tf", ".", "nn", ".", "xw_plus_b", "(", "fc_outputs", ",", "W_appear", ",", "b_appear", ")", "\n", "self", ".", "prob_appear", "=", "tf", ".", "nn", ".", "softmax", "(", "self", ".", "logits_appear", ")", "\n", "l2_loss", "+=", "tf", ".", "nn", ".", "l2_loss", "(", "W_appear", ")", "\n", "l2_loss", "+=", "tf", ".", "nn", ".", "l2_loss", "(", "b_appear", ")", "\n", "# ideology", "\n", "W_idea", "=", "tf", ".", "Variable", "(", "tf", ".", "truncated_normal", "(", "[", "fc_outputs", ".", "get_shape", "(", ")", "[", "1", "]", ".", "value", ",", "num_classes", "]", ",", "stddev", "=", "0.1", ")", ",", "name", "=", "\"W_idea\"", ")", "\n", "b_idea", "=", "tf", ".", "Variable", "(", "tf", ".", "constant", "(", "0.1", ",", "shape", "=", "[", "num_classes", "]", ")", ",", "name", "=", "\"b_idea\"", ")", "\n", "self", ".", "logits_idea", "=", "tf", ".", "nn", ".", "xw_plus_b", "(", "fc_outputs", ",", "W_idea", ",", "b_idea", ")", "\n", "self", ".", "prob_idea", "=", "tf", ".", "nn", ".", "softmax", "(", "self", ".", "logits_idea", ")", "\n", "l2_loss", "+=", "tf", ".", "nn", ".", "l2_loss", "(", "W_idea", ")", "\n", "l2_loss", "+=", "tf", ".", "nn", ".", "l2_loss", "(", "b_idea", ")", "\n", "\n", "", "with", "tf", ".", "name_scope", "(", "\"cross_entropy\"", ")", ":", "\n", "            ", "loss_gender", "=", "tf", ".", "nn", ".", "softmax_cross_entropy_with_logits", "(", "labels", "=", "self", ".", "input_y_gender", ",", "logits", "=", "self", ".", "logits_gender", ")", "\n", "loss_race", "=", "tf", ".", "nn", ".", "softmax_cross_entropy_with_logits", "(", "labels", "=", "self", ".", "input_y_race", ",", "logits", "=", "self", ".", "logits_race", ")", "\n", "loss_appear", "=", "tf", ".", "nn", ".", "softmax_cross_entropy_with_logits", "(", "labels", "=", "self", ".", "input_y_appear", ",", "logits", "=", "self", ".", "logits_appear", ")", "\n", "loss_idea", "=", "tf", ".", "nn", ".", "softmax_cross_entropy_with_logits", "(", "labels", "=", "self", ".", "input_y_idea", ",", "logits", "=", "self", ".", "logits_idea", ")", "\n", "# weighted entropy loss", "\n", "entropy_loss", "=", "tf", ".", "reduce_mean", "(", "self", ".", "gender_weight", "*", "loss_gender", "+", "self", ".", "race_weight", "*", "loss_race", "+", "self", ".", "appear_weight", "*", "loss_appear", "+", "self", ".", "idea_weight", "*", "loss_idea", ")", "\n", "\n", "if", "(", "attention_loss_type", "==", "\"encoded\"", ")", ":", "\n", "                ", "print", "(", "\"Supervised attention with encoded loss.\"", ")", "\n", "att_shared_dim", "=", "20", "\n", "# rationale input_attention: (batch_size, max_sequence_length)", "\n", "# W: (max_sequence_length, att_shared_dim)", "\n", "# b: (att_shared_dim,)", "\n", "# proj: (batch_size, att_shared_dim)", "\n", "ration_W", "=", "tf", ".", "Variable", "(", "tf", ".", "truncated_normal", "(", "[", "self", ".", "input_attention", ".", "get_shape", "(", ")", "[", "1", "]", ".", "value", ",", "att_shared_dim", "]", ",", "stddev", "=", "0.1", ")", ",", "name", "=", "\"ration_W\"", ")", "\n", "ration_b", "=", "tf", ".", "Variable", "(", "tf", ".", "constant", "(", "0.05", ",", "shape", "=", "[", "att_shared_dim", "]", ")", ",", "name", "=", "\"ration_b\"", ")", "\n", "proj_ration", "=", "tf", ".", "nn", ".", "tanh", "(", "tf", ".", "nn", ".", "xw_plus_b", "(", "self", ".", "input_attention", ",", "ration_W", ",", "ration_b", ")", ")", "\n", "alpha_W", "=", "tf", ".", "Variable", "(", "tf", ".", "truncated_normal", "(", "[", "self", ".", "alphas", ".", "get_shape", "(", ")", "[", "1", "]", ".", "value", ",", "att_shared_dim", "]", ",", "stddev", "=", "0.1", ")", ",", "name", "=", "\"alpha_W\"", ")", "\n", "alpha_b", "=", "tf", ".", "Variable", "(", "tf", ".", "constant", "(", "0.05", ",", "shape", "=", "[", "att_shared_dim", "]", ")", ",", "name", "=", "\"alpha_b\"", ")", "\n", "proj_alphas", "=", "tf", ".", "nn", ".", "tanh", "(", "tf", ".", "nn", ".", "xw_plus_b", "(", "self", ".", "alphas", ",", "alpha_W", ",", "alpha_b", ")", ")", "\n", "# negative of inner product", "\n", "attention_loss", "=", "-", "1", "*", "tf", ".", "reduce_mean", "(", "tf", ".", "multiply", "(", "proj_ration", ",", "proj_alphas", ")", ")", "\n", "", "elif", "(", "attention_loss_type", "==", "\"l1\"", ")", ":", "\n", "                ", "print", "(", "\"Supervised attention with L1 loss.\"", ")", "\n", "attention_loss", "=", "tf", ".", "reduce_mean", "(", "tf", ".", "abs", "(", "tf", ".", "subtract", "(", "tf", ".", "nn", ".", "softmax", "(", "self", ".", "input_attention", ")", ",", "self", ".", "alphas", ")", ")", ")", "\n", "", "elif", "(", "attention_loss_type", "==", "\"l2\"", ")", ":", "\n", "                ", "print", "(", "\"Supervised attention with L2 loss.\"", ")", "\n", "attention_loss", "=", "tf", ".", "reduce_mean", "(", "tf", ".", "square", "(", "tf", ".", "subtract", "(", "tf", ".", "nn", ".", "softmax", "(", "self", ".", "input_attention", ")", ",", "self", ".", "alphas", ")", ")", ")", "\n", "", "else", ":", "\n", "                ", "print", "(", "\"No supervised attention.\"", ")", "\n", "attention_loss", "=", "tf", ".", "constant", "(", "0.0", ")", "\n", "", "self", ".", "loss", "=", "entropy_loss", "+", "attention_lambda", "*", "attention_loss", "+", "l2_reg_lambda", "*", "l2_loss", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.abuse_classification.test.scoreUtil": [[49, 96], ["tensorflow.Graph().as_default", "tensorflow.ConfigProto", "tensorflow.Session", "tf.Session.as_default", "model.abuse_classifier.AbuseClassifier", "tensorflow.Variable", "tensorflow.train.Saver", "tf.Session.run", "tf.train.Saver.restore", "tensorflow.Graph", "tensorflow.all_variables", "tensorflow.initialize_all_variables", "len", "tf.Session.run", "list"], "function", ["None"], ["def", "scoreUtil", "(", "init_embed", ",", "x_dev", ",", "pos_dev", ",", "y_dev", ",", "length_dev", ",", "pos_length_dev", ",", "model_path", ")", ":", "\n", "    ", "with", "tf", ".", "Graph", "(", ")", ".", "as_default", "(", ")", ":", "\n", "      ", "session_conf", "=", "tf", ".", "ConfigProto", "(", "\n", "allow_soft_placement", "=", "FLAGS", ".", "allow_soft_placement", ",", "\n", "log_device_placement", "=", "FLAGS", ".", "log_device_placement", ")", "\n", "sess", "=", "tf", ".", "Session", "(", "config", "=", "session_conf", ")", "\n", "with", "sess", ".", "as_default", "(", ")", ":", "\n", "        ", "model", "=", "AbuseClassifier", "(", "\n", "max_sequence_length", "=", "param", ".", "max_sent_len", ",", "\n", "num_classes", "=", "2", ",", "\n", "pos_vocab_size", "=", "FLAGS", ".", "pos_vocab_size", ",", "\n", "init_embed", "=", "init_embed", ",", "\n", "hidden_size", "=", "FLAGS", ".", "hidden_size", ",", "\n", "attention_size", "=", "FLAGS", ".", "attention_size", ",", "\n", "keep_prob", "=", "FLAGS", ".", "dropout_keep_prob", ",", "\n", "attention_lambda", "=", "FLAGS", ".", "attention_lambda", ",", "\n", "attention_loss_type", "=", "FLAGS", ".", "attention_loss_type", ",", "\n", "l2_reg_lambda", "=", "0.1", ",", "\n", "use_pos_flag", "=", "FLAGS", ".", "use_pos_flag", ")", "\n", "\n", "global_step", "=", "tf", ".", "Variable", "(", "0", ",", "name", "=", "\"global_step\"", ",", "trainable", "=", "False", ")", "\n", "saver", "=", "tf", ".", "train", ".", "Saver", "(", "tf", ".", "all_variables", "(", ")", ")", "\n", "# Initialize all variables", "\n", "sess", ".", "run", "(", "tf", ".", "initialize_all_variables", "(", ")", ")", "\n", "saver", ".", "restore", "(", "sess", ",", "model_path", ")", "\n", "\n", "dev_scores", "=", "[", "]", "\n", "pos", "=", "0", "\n", "gap", "=", "50", "\n", "while", "(", "pos", "<", "len", "(", "x_dev", ")", ")", ":", "\n", "          ", "x_batch", "=", "x_dev", "[", "pos", ":", "pos", "+", "gap", "]", "\n", "pos_batch", "=", "pos_dev", "[", "pos", ":", "pos", "+", "gap", "]", "\n", "y_batch", "=", "y_dev", "[", "pos", ":", "pos", "+", "gap", "]", "\n", "length_batch", "=", "length_dev", "[", "pos", ":", "pos", "+", "gap", "]", "\n", "pos_length_batch", "=", "pos_length_dev", "[", "pos", ":", "pos", "+", "gap", "]", "\n", "pos", "+=", "gap", "\n", "# score sentences", "\n", "feed_dict", "=", "{", "\n", "model", ".", "input_word", ":", "x_batch", ",", "\n", "model", ".", "input_pos", ":", "pos_batch", ",", "\n", "model", ".", "input_y", ":", "y_batch", ",", "\n", "model", ".", "sequence_length", ":", "length_batch", ",", "\n", "model", ".", "dropout_keep_prob", ":", "1.0", "\n", "}", "\n", "step", ",", "scores", "=", "sess", ".", "run", "(", "[", "global_step", ",", "model", ".", "prob", "]", ",", "feed_dict", ")", "\n", "dev_scores", "=", "dev_scores", "+", "list", "(", "[", "s", "[", "0", "]", "for", "s", "in", "scores", "]", ")", "\n", "", "", "", "return", "dev_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.abuse_classification.test.scoreComments": [[98, 110], ["data_util.data_helpers.loadVocabEmb", "print", "data_util.data_helpers.loadData", "test.scoreUtil", "len"], "function", ["home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.data_helpers.loadVocabEmb", "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.data_helpers.loadData", "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.abuse_categorization.test.scoreUtil"], ["", "def", "scoreComments", "(", "model_path", ",", "data_type", "=", "\"test\"", ")", ":", "\n", "    ", "\"\"\"\n    Score comments with saved model\n    \"\"\"", "\n", "vocabulary", ",", "pos_vocabulary", ",", "init_embed", "=", "data_helpers", ".", "loadVocabEmb", "(", ")", "\n", "print", "(", "\"pos vocab size: {}\"", ".", "format", "(", "len", "(", "pos_vocabulary", ")", ")", ")", "\n", "x_test", ",", "length_test", ",", "attention_test", ",", "pos_test", ",", "pos_length_test", ",", "y_test", "=", "data_helpers", ".", "loadData", "(", "data_type", ")", "\n", "test_scores", "=", "scoreUtil", "(", "init_embed", ",", "x_test", ",", "pos_test", ",", "y_test", ",", "length_test", ",", "pos_length_test", ",", "model_path", ")", "\n", "gold_scores", "=", "[", "s", "[", "0", "]", "for", "s", "in", "y_test", "]", "\n", "return", "gold_scores", ",", "test_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.abuse_categorization.category_data_helpers.loadCategoryData": [[13, 15], ["None"], "function", ["None"], ["import", "sys", "\n", "import", "copy", "\n", "\n"]], "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.abuse_categorization.test.scoreUtil": [[62, 118], ["tensorflow.Graph().as_default", "tensorflow.ConfigProto", "tensorflow.Session", "tf.Session.as_default", "model.abuse_categorizer.AbuseCategorizer", "tensorflow.Variable", "tensorflow.train.Saver", "tf.Session.run", "tf.train.Saver.restore", "tensorflow.Graph", "tensorflow.all_variables", "tensorflow.initialize_all_variables", "len", "tf.Session.run"], "function", ["None"], ["attention_size", "=", "FLAGS", ".", "attention_size", ",", "\n", "keep_prob", "=", "FLAGS", ".", "dropout_keep_prob", ",", "\n", "attention_lambda", "=", "FLAGS", ".", "attention_lambda", ",", "\n", "attention_loss_type", "=", "FLAGS", ".", "attention_loss_type", ",", "\n", "l2_reg_lambda", "=", "0.1", ",", "\n", "use_pos_flag", "=", "FLAGS", ".", "use_pos_flag", ")", "\n", "\n", "global_step", "=", "tf", ".", "Variable", "(", "0", ",", "name", "=", "\"global_step\"", ",", "trainable", "=", "False", ")", "\n", "saver", "=", "tf", ".", "train", ".", "Saver", "(", "tf", ".", "all_variables", "(", ")", ")", "\n", "# Initialize all variables", "\n", "sess", ".", "run", "(", "tf", ".", "initialize_all_variables", "(", ")", ")", "\n", "saver", ".", "restore", "(", "sess", ",", "model_path", ")", "\n", "\n", "dev_scores", "=", "[", "]", "\n", "pos", "=", "0", "\n", "gap", "=", "50", "\n", "while", "(", "pos", "<", "len", "(", "x_dev", ")", ")", ":", "\n", "          ", "x_batch", "=", "x_dev", "[", "pos", ":", "pos", "+", "gap", "]", "\n", "pos_batch", "=", "pos_dev", "[", "pos", ":", "pos", "+", "gap", "]", "\n", "y_batch", "=", "y_dev", "[", "pos", ":", "pos", "+", "gap", "]", "\n", "length_batch", "=", "length_dev", "[", "pos", ":", "pos", "+", "gap", "]", "\n", "pos_length_batch", "=", "pos_length_dev", "[", "pos", ":", "pos", "+", "gap", "]", "\n", "pos", "+=", "gap", "\n", "# score sentences", "\n", "feed_dict", "=", "{", "\n", "model", ".", "input_word", ":", "x_batch", ",", "\n", "model", ".", "input_pos", ":", "pos_batch", ",", "\n", "model", ".", "input_y", ":", "y_batch", ",", "\n", "model", ".", "sequence_length", ":", "length_batch", ",", "\n", "model", ".", "dropout_keep_prob", ":", "1.0", "\n", "}", "\n", "step", ",", "scores", "=", "sess", ".", "run", "(", "[", "global_step", ",", "model", ".", "prob", "]", ",", "feed_dict", ")", "\n", "dev_scores", "=", "dev_scores", "+", "list", "(", "[", "s", "[", "0", "]", "for", "s", "in", "scores", "]", ")", "\n", "", "", "", "return", "dev_scores", "\n", "\n", "\n", "", "def", "scoreComments", "(", "model_path", ",", "data_type", "=", "\"test\"", ")", ":", "\n", "    ", "\"\"\"\n    Score comments with saved model\n    \"\"\"", "\n", "vocabulary", ",", "pos_vocabulary", ",", "init_embed", "=", "data_helpers", ".", "loadVocabEmb", "(", ")", "\n", "print", "(", "\"pos vocab size: {}\"", ".", "format", "(", "len", "(", "pos_vocabulary", ")", ")", ")", "\n", "x_test", ",", "length_test", ",", "attention_test", ",", "pos_test", ",", "pos_length_test", ",", "y_test", "=", "data_helpers", ".", "loadData", "(", "data_type", ")", "\n", "test_scores", "=", "scoreUtil", "(", "init_embed", ",", "x_test", ",", "pos_test", ",", "y_test", ",", "length_test", ",", "pos_length_test", ",", "model_path", ")", "\n", "gold_scores", "=", "[", "s", "[", "0", "]", "for", "s", "in", "y_test", "]", "\n", "return", "gold_scores", ",", "test_scores", "\n", "\n", "\n", "", "if", "__name__", "==", "\"__main__\"", ":", "\n", "# locate checkpoint", "\n", "    ", "if", "FLAGS", ".", "checkpoint", "==", "\"\"", ":", "\n", "        ", "out_dir", "=", "os", ".", "path", ".", "abspath", "(", "os", ".", "path", ".", "join", "(", "os", ".", "path", ".", "pardir", ",", "\"model\"", ")", ")", "\n", "print", "(", "\"Writing to {}\\n\"", ".", "format", "(", "out_dir", ")", ")", "\n", "", "else", ":", "\n", "        ", "out_dir", "=", "FLAGS", ".", "checkpoint", "\n"]], "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.abuse_categorization.test.scoreComments": [[120, 137], ["data_util.data_helpers.loadVocabEmb", "data_util.category_data_helpers.loadCategoryData", "test.scoreUtil"], "function", ["home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.data_util.data_helpers.loadVocabEmb", "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.abuse_categorization.category_data_helpers.loadCategoryData", "home.repos.pwc.inspect_result.HongyuGong_Abusive-Language-Detection-Categorization.abuse_categorization.test.scoreUtil"], ["        ", "checkpoint_dir", "=", "os", ".", "path", ".", "abspath", "(", "os", ".", "path", ".", "join", "(", "out_dir", ",", "\"model_noatt_checkpoints\"", ")", ")", "\n", "", "else", ":", "\n", "        ", "checkpoint_dir", "=", "os", ".", "path", ".", "abspath", "(", "os", ".", "path", ".", "join", "(", "out_dir", ",", "\"model_att=\"", "+", "FLAGS", ".", "attention_loss_type", "+", "\"_checkpoints\"", ")", ")", "\n", "", "model_path", "=", "os", ".", "path", ".", "join", "(", "checkpoint_dir", ",", "\"best_model\"", ")", "\n", "\n", "# evaluate on train data", "\n", "train_gold_scores", ",", "train_pred_scores", "=", "scoreComments", "(", "model_path", ",", "data_type", "=", "\"train\"", ")", "\n", "# evaluate on test data", "\n", "test_gold_scores", ",", "test_pred_scores", "=", "scoreComments", "(", "model_path", ",", "data_type", "=", "\"test\"", ")", "\n", "\n", "# roc auc", "\n", "eval_helpers", ".", "evalROC", "(", "test_gold_scores", ",", "test_pred_scores", ")", "\n", "# pr auc", "\n", "eval_helpers", ".", "evalPR", "(", "test_gold_scores", ",", "test_pred_scores", ")", "\n", "# f1 score", "\n", "eval_helpers", ".", "evalFscore", "(", "train_gold_scores", ",", "train_pred_scores", ",", "\n", "test_gold_scores", ",", "test_pred_scores", ")", "\n", "\n"]]}