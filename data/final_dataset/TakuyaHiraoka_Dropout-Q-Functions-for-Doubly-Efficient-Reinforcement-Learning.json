{"home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.main.run": [[20, 133], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "gym.make", "os.path.join", "agent.SacAgent.run", "print", "util.utilsTH.SparseRewardEnv", "str", "NotImplementedError", "datetime.datetime.now().isoformat", "agent4profile.SacAgent4Profile", "agent.SacAgent", "str", "datetime.datetime.now"], "function", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.play.run"], ["def", "run", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "# replaced wtih SAC-extention args 20210705", "\n", "parser", ".", "add_argument", "(", "\"-env\"", ",", "type", "=", "str", ",", "default", "=", "\"HalfCheetah-v2\"", ",", "\n", "help", "=", "\"Environment name, default = HalfCheetahBulletEnv-v0\"", ")", "\n", "parser", ".", "add_argument", "(", "'-seed'", ",", "type", "=", "int", ",", "default", "=", "0", ")", "\n", "#added byTH 20210705", "\n", "# common", "\n", "parser", ".", "add_argument", "(", "\"-info\"", ",", "type", "=", "str", ",", "help", "=", "\"Information or name of the run\"", ")", "\n", "parser", ".", "add_argument", "(", "\"-frames\"", ",", "type", "=", "int", ",", "default", "=", "1_000_000", ",", "\n", "help", "=", "\"The amount of training interactions with the environment, default is 1mio\"", ")", "\n", "parser", ".", "add_argument", "(", "\"-gpu_id\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"GPU device ID to be used in GPU experiment, default is 1e6\"", ")", "\n", "# evaluation", "\n", "parser", ".", "add_argument", "(", "\"-eval_every\"", ",", "type", "=", "int", ",", "default", "=", "1000", ",", "\n", "help", "=", "\"Number of interactions after which the evaluation runs are performed, default = 1000\"", ")", "\n", "parser", ".", "add_argument", "(", "\"-eval_runs\"", ",", "type", "=", "int", ",", "default", "=", "3", ",", "help", "=", "\"Number of evaluation runs performed, default = 1\"", ")", "\n", "# sparse env", "\n", "parser", ".", "add_argument", "(", "\"-sparsity_th\"", ",", "type", "=", "float", ",", "default", "=", "0.0", ",", "\n", "help", "=", "\"threshold for make reward sparse (i.e., lambda in PolyRL paper), default is 0.0\"", ")", "\n", "# stabilization", "\n", "parser", ".", "add_argument", "(", "\"-huber\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "choices", "=", "[", "0", ",", "1", "]", ",", "\n", "help", "=", "\"Using Huber loss for training critics if set to 1 (TH), default=0\"", ")", "# TODO remove", "\n", "parser", ".", "add_argument", "(", "\"-layer_norm\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "choices", "=", "[", "0", ",", "1", "]", ",", "\n", "help", "=", "\"Using layer normalization for training critics if set to 1 (TH), default=0\"", ")", "\n", "# multi-step and per", "\n", "parser", ".", "add_argument", "(", "\"-n_step\"", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "\"Using n-step bootstrapping, default=1\"", ")", "\n", "parser", ".", "add_argument", "(", "\"-per\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "choices", "=", "[", "0", ",", "1", "]", ",", "\n", "help", "=", "\"Adding Priorizied Experience Replay to the agent if set to 1, default = 0\"", ")", "\n", "# dist RL added @ 20210711", "\n", "parser", ".", "add_argument", "(", "\"-dist\"", ",", "\"--distributional\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "choices", "=", "[", "0", ",", "1", "]", ",", "\n", "help", "=", "\"Using a distributional IQN Critic if set to 1, default=0\"", ")", "# TODO remove", "\n", "# learning per steps", "\n", "parser", ".", "add_argument", "(", "\"-updates_per_step\"", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "\"Number of training updates per one environment step, default = 1\"", ")", "\n", "# th 20210724", "\n", "parser", ".", "add_argument", "(", "\"-target_entropy\"", ",", "type", "=", "float", ",", "default", "=", "None", ",", "help", "=", "\"target entropy , default=Num action\"", ")", "\n", "# for MBPO and redq setting, Hopper: -1, HC: -3, Walker: -3, Ant: -4, Humaoid: -2", "\n", "#", "\n", "parser", ".", "add_argument", "(", "\"-method\"", ",", "default", "=", "\"sac\"", ",", "choices", "=", "[", "\"sac\"", ",", "\"redq\"", ",", "\"duvn\"", ",", "\"monosac\"", "]", ",", "help", "=", "\"method, default=sac\"", ")", "\n", "# learning per steps", "\n", "parser", ".", "add_argument", "(", "\"-batch_size\"", ",", "type", "=", "int", ",", "default", "=", "256", ",", "\n", "help", "=", "\"Number of training batch, default = 256\"", ")", "\n", "#", "\n", "parser", ".", "add_argument", "(", "\"-target_drop_rate\"", ",", "type", "=", "float", ",", "default", "=", "0.0", ",", "help", "=", "\"drop out rate of target value function, default=0\"", ")", "\n", "#", "\n", "parser", ".", "add_argument", "(", "\"-critic_update_delay\"", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "\"number of critic learning delay (tau and UDP is rescaled), default=1 (no delay)\"", ")", "# TODO remove", "\n", "\n", "# 20210813", "\n", "# dist RL added @ 20210711", "\n", "parser", ".", "add_argument", "(", "\"-profile\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "choices", "=", "[", "0", ",", "1", "]", ",", "\n", "help", "=", "\"Using profile for cpu/gpu speed and memory usage if set to 1, default=0\"", ")", "\n", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "# You can define configs in the external json or yaml file.", "\n", "configs", "=", "{", "\n", "'num_steps'", ":", "args", ".", "frames", ",", "#3000000,", "\n", "'batch_size'", ":", "args", ".", "batch_size", ",", "#, 256,", "\n", "'lr'", ":", "0.0003", ",", "\n", "'hidden_units'", ":", "[", "256", ",", "256", "]", ",", "\n", "'memory_size'", ":", "1e6", ",", "\n", "'gamma'", ":", "0.99", ",", "\n", "'tau'", ":", "0.005", ",", "\n", "'entropy_tuning'", ":", "True", ",", "\n", "'ent_coef'", ":", "0.2", ",", "# It's ignored when entropy_tuning=True.", "\n", "'multi_step'", ":", "args", ".", "n_step", ",", "#1,", "\n", "'per'", ":", "args", ".", "per", ",", "#False,  # prioritized experience replay", "\n", "'alpha'", ":", "0.6", ",", "# It's ignored when per=False.", "\n", "'beta'", ":", "0.4", ",", "# It's ignored when per=False.", "\n", "'beta_annealing'", ":", "(", "1.0", "-", "0.4", ")", "/", "(", "1.0", "*", "args", ".", "frames", "*", "args", ".", "updates_per_step", ")", ",", "# 0.0001,  # It's ignored when per=False.", "\n", "'grad_clip'", ":", "None", ",", "\n", "'updates_per_step'", ":", "args", ".", "updates_per_step", "*", "args", ".", "critic_update_delay", ",", "# args.updates_per_step, #1,", "\n", "'start_steps'", ":", "5000", ",", "#10000,", "\n", "'log_interval'", ":", "10", ",", "\n", "'target_update_interval'", ":", "1", ",", "\n", "'eval_interval'", ":", "args", ".", "eval_every", ",", "# 10000,", "\n", "'cuda'", ":", "args", ".", "gpu_id", ",", "# args.cuda,", "\n", "'seed'", ":", "args", ".", "seed", ",", "\n", "\n", "# adde by TH", "\n", "'eval_runs'", ":", "args", ".", "eval_runs", ",", "\n", "'huber'", ":", "args", ".", "huber", ",", "# TODO remove", "\n", "'layer_norm'", ":", "args", ".", "layer_norm", ",", "\n", "#", "\n", "'target_entropy'", ":", "args", ".", "target_entropy", ",", "\n", "'method'", ":", "args", ".", "method", ",", "\n", "'target_drop_rate'", ":", "args", ".", "target_drop_rate", ",", "\n", "'critic_update_delay'", ":", "args", ".", "critic_update_delay", "\n", "}", "\n", "\n", "env", "=", "gym", ".", "make", "(", "args", ".", "env", ")", "\n", "\n", "# make sparse en: TH 20210705", "\n", "if", "args", ".", "sparsity_th", ">", "0.0", ":", "\n", "        ", "print", "(", "\"Evaluation in sparse reward setting with lambda = \"", "+", "str", "(", "args", ".", "sparsity_th", ")", ")", "\n", "env", "=", "SparseRewardEnv", "(", "env", ",", "rew_thresh", "=", "args", ".", "sparsity_th", ")", "\n", "env", ".", "_max_episode_steps", "=", "env", ".", "wrapped_env", ".", "_max_episode_steps", "\n", "\n", "", "label", "=", "args", ".", "env", "+", "\"_\"", "+", "str", "(", "datetime", ".", "datetime", ".", "now", "(", ")", ".", "isoformat", "(", ")", ")", "\n", "log_dir", "=", "os", ".", "path", ".", "join", "(", "'runs'", ",", "args", ".", "info", ",", "label", ")", "\n", "\n", "if", "args", ".", "distributional", ":", "# TODO remove", "\n", "        ", "raise", "NotImplementedError", "(", ")", "\n", "#print(\" Use IQN agent\")", "\n", "#agent = IQNSacAgent(env=env, log_dir=log_dir, **configs)", "\n", "", "else", ":", "\n", "        ", "if", "args", ".", "profile", ":", "\n", "            ", "agent", "=", "SacAgent4Profile", "(", "env", "=", "env", ",", "log_dir", "=", "log_dir", ",", "**", "configs", ")", "\n", "", "else", ":", "\n", "            ", "agent", "=", "SacAgent", "(", "env", "=", "env", ",", "log_dir", "=", "log_dir", ",", "**", "configs", ")", "\n", "", "", "agent", ".", "run", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent4profile.SacAgent4Profile.__init__": [[22, 139], ["torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "numpy.random.seed", "agent4profile.SacAgent4Profile.env.seed", "torch.device", "torch.device", "torch.device", "torch.device", "model.GaussianPolicy().to", "utils.hard_update", "utils.grad_false", "torch.optim.Adam", "torch.optim.Adam", "os.path.join", "os.path.join", "torch.utils.tensorboard.SummaryWriter", "torch.utils.tensorboard.SummaryWriter", "utils.RunningMeanStats", "model.RandomizedEnsembleNetwork().to", "model.RandomizedEnsembleNetwork().to", "model.TwinnedQNetwork().to", "model.TwinnedQNetwork().to", "agent4profile.SacAgent4Profile.critic_target.eval", "agent4profile.SacAgent4Profile.policy.parameters", "range", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "agent4profile.SacAgent4Profile.log_alpha.exp", "torch.optim.Adam", "torch.optim.Adam", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "rltorch.memory.PrioritizedMemory", "rltorch.memory.MultiStepMemory", "os.path.exists", "os.makedirs", "os.path.exists", "os.makedirs", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "model.GaussianPolicy", "setattr", "agent4profile.SacAgent4Profile.critic.Q1.parameters", "agent4profile.SacAgent4Profile.critic.Q2.parameters", "torch.prod().item", "torch.prod().item", "torch.prod().item", "torch.prod().item", "str", "model.RandomizedEnsembleNetwork", "model.RandomizedEnsembleNetwork", "model.TwinnedQNetwork", "model.TwinnedQNetwork", "torch.optim.Adam", "torch.optim.Adam", "torch.prod().item", "torch.prod().item", "torch.prod().item", "torch.prod().item", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "getattr().parameters", "torch.prod", "torch.prod", "torch.prod", "torch.prod", "str", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "torch.prod", "torch.prod", "torch.prod", "torch.prod", "getattr", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "str", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.hard_update", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.grad_false"], ["    ", "def", "__init__", "(", "self", ",", "env", ",", "log_dir", ",", "num_steps", "=", "3000000", ",", "batch_size", "=", "256", ",", "\n", "lr", "=", "0.0003", ",", "hidden_units", "=", "[", "256", ",", "256", "]", ",", "memory_size", "=", "1e6", ",", "\n", "gamma", "=", "0.99", ",", "tau", "=", "0.005", ",", "entropy_tuning", "=", "True", ",", "ent_coef", "=", "0.2", ",", "\n", "multi_step", "=", "1", ",", "per", "=", "False", ",", "alpha", "=", "0.6", ",", "beta", "=", "0.4", ",", "\n", "beta_annealing", "=", "0.0001", ",", "grad_clip", "=", "None", ",", "updates_per_step", "=", "1", ",", "\n", "start_steps", "=", "10000", ",", "log_interval", "=", "10", ",", "target_update_interval", "=", "1", ",", "\n", "eval_interval", "=", "1000", ",", "cuda", "=", "0", ",", "seed", "=", "0", ",", "\n", "# added by TH 20210707", "\n", "eval_runs", "=", "1", ",", "huber", "=", "0", ",", "layer_norm", "=", "0", ",", "\n", "method", "=", "None", ",", "target_entropy", "=", "None", ",", "target_drop_rate", "=", "0.0", ",", "critic_update_delay", "=", "1", ")", ":", "\n", "        ", "self", ".", "env", "=", "env", "\n", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "self", ".", "env", ".", "seed", "(", "seed", ")", "\n", "torch", ".", "backends", ".", "cudnn", ".", "deterministic", "=", "True", "# It harms a performance.", "\n", "torch", ".", "backends", ".", "cudnn", ".", "benchmark", "=", "False", "\n", "\n", "self", ".", "method", "=", "method", "\n", "self", ".", "critic_update_delay", "=", "critic_update_delay", "\n", "self", ".", "target_drop_rate", "=", "target_drop_rate", "\n", "\n", "self", ".", "device", "=", "torch", ".", "device", "(", "\"cuda:\"", "+", "str", "(", "cuda", ")", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "\n", "# policy", "\n", "self", ".", "policy", "=", "GaussianPolicy", "(", "\n", "self", ".", "env", ".", "observation_space", ".", "shape", "[", "0", "]", ",", "\n", "self", ".", "env", ".", "action_space", ".", "shape", "[", "0", "]", ",", "\n", "hidden_units", "=", "hidden_units", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "# Q functions", "\n", "kwargs_q", "=", "{", "\"num_inputs\"", ":", "self", ".", "env", ".", "observation_space", ".", "shape", "[", "0", "]", ",", "\n", "\"num_actions\"", ":", "self", ".", "env", ".", "action_space", ".", "shape", "[", "0", "]", ",", "\n", "\"hidden_units\"", ":", "hidden_units", ",", "\n", "\"layer_norm\"", ":", "layer_norm", ",", "\n", "\"drop_rate\"", ":", "self", ".", "target_drop_rate", "}", "\n", "if", "self", ".", "method", "==", "\"redq\"", ":", "\n", "            ", "self", ".", "critic", "=", "RandomizedEnsembleNetwork", "(", "**", "kwargs_q", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "critic_target", "=", "RandomizedEnsembleNetwork", "(", "**", "kwargs_q", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "critic", "=", "TwinnedQNetwork", "(", "**", "kwargs_q", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "critic_target", "=", "TwinnedQNetwork", "(", "**", "kwargs_q", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "", "if", "self", ".", "target_drop_rate", "<=", "0.0", ":", "\n", "            ", "self", ".", "critic_target", "=", "self", ".", "critic_target", ".", "eval", "(", ")", "\n", "# copy parameters of the learning network to the target network", "\n", "", "hard_update", "(", "self", ".", "critic_target", ",", "self", ".", "critic", ")", "\n", "# disable gradient calculations of the target network", "\n", "grad_false", "(", "self", ".", "critic_target", ")", "\n", "\n", "# optimizer", "\n", "self", ".", "policy_optim", "=", "Adam", "(", "self", ".", "policy", ".", "parameters", "(", ")", ",", "lr", "=", "lr", ")", "\n", "if", "self", ".", "method", "==", "\"redq\"", ":", "\n", "            ", "for", "i", "in", "range", "(", "self", ".", "critic", ".", "N", ")", ":", "\n", "                ", "setattr", "(", "self", ",", "\"q\"", "+", "str", "(", "i", ")", "+", "\"_optim\"", ",", "\n", "Adam", "(", "getattr", "(", "self", ".", "critic", ",", "\"Q\"", "+", "str", "(", "i", ")", ")", ".", "parameters", "(", ")", ",", "lr", "=", "lr", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "q1_optim", "=", "Adam", "(", "self", ".", "critic", ".", "Q1", ".", "parameters", "(", ")", ",", "lr", "=", "lr", ")", "\n", "self", ".", "q2_optim", "=", "Adam", "(", "self", ".", "critic", ".", "Q2", ".", "parameters", "(", ")", ",", "lr", "=", "lr", ")", "\n", "\n", "", "if", "entropy_tuning", ":", "\n", "            ", "if", "not", "(", "target_entropy", "is", "None", ")", ":", "\n", "                ", "self", ".", "target_entropy", "=", "torch", ".", "prod", "(", "torch", ".", "Tensor", "(", "[", "target_entropy", "]", ")", ".", "to", "(", "self", ".", "device", ")", ")", ".", "item", "(", ")", "\n", "", "else", ":", "\n", "# Target entropy is -|A|.", "\n", "                ", "self", ".", "target_entropy", "=", "-", "torch", ".", "prod", "(", "torch", ".", "Tensor", "(", "self", ".", "env", ".", "action_space", ".", "shape", ")", ".", "to", "(", "self", ".", "device", ")", ")", ".", "item", "(", ")", "\n", "# We optimize log(alpha), instead of alpha.", "\n", "", "self", ".", "log_alpha", "=", "torch", ".", "zeros", "(", "1", ",", "requires_grad", "=", "True", ",", "device", "=", "self", ".", "device", ")", "\n", "self", ".", "alpha", "=", "self", ".", "log_alpha", ".", "exp", "(", ")", "\n", "self", ".", "alpha_optim", "=", "Adam", "(", "[", "self", ".", "log_alpha", "]", ",", "lr", "=", "lr", ")", "\n", "", "else", ":", "\n", "# fixed alpha", "\n", "            ", "self", ".", "alpha", "=", "torch", ".", "tensor", "(", "ent_coef", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "", "if", "per", ":", "\n", "# replay memory with prioritied experience replay", "\n", "# See https://github.com/ku2482/rltorch/blob/master/rltorch/memory", "\n", "            ", "self", ".", "memory", "=", "PrioritizedMemory", "(", "\n", "memory_size", ",", "self", ".", "env", ".", "observation_space", ".", "shape", ",", "\n", "self", ".", "env", ".", "action_space", ".", "shape", ",", "self", ".", "device", ",", "gamma", ",", "multi_step", ",", "\n", "alpha", "=", "alpha", ",", "beta", "=", "beta", ",", "beta_annealing", "=", "beta_annealing", ")", "\n", "", "else", ":", "\n", "# replay memory without prioritied experience replay", "\n", "# See https://github.com/ku2482/rltorch/blob/master/rltorch/memory", "\n", "            ", "self", ".", "memory", "=", "MultiStepMemory", "(", "\n", "memory_size", ",", "self", ".", "env", ".", "observation_space", ".", "shape", ",", "\n", "self", ".", "env", ".", "action_space", ".", "shape", ",", "self", ".", "device", ",", "gamma", ",", "multi_step", ")", "\n", "\n", "", "self", ".", "log_dir", "=", "log_dir", "\n", "self", ".", "model_dir", "=", "os", ".", "path", ".", "join", "(", "log_dir", ",", "'model'", ")", "\n", "self", ".", "summary_dir", "=", "os", ".", "path", ".", "join", "(", "log_dir", ",", "'summary'", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "self", ".", "model_dir", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "self", ".", "model_dir", ")", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "self", ".", "summary_dir", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "self", ".", "summary_dir", ")", "\n", "\n", "", "self", ".", "writer", "=", "SummaryWriter", "(", "log_dir", "=", "self", ".", "summary_dir", ")", "\n", "self", ".", "train_rewards", "=", "RunningMeanStats", "(", "log_interval", ")", "\n", "\n", "self", ".", "steps", "=", "0", "\n", "self", ".", "learning_steps", "=", "0", "\n", "self", ".", "episodes", "=", "0", "\n", "self", ".", "num_steps", "=", "num_steps", "\n", "self", ".", "tau", "=", "tau", "\n", "self", ".", "per", "=", "per", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "start_steps", "=", "start_steps", "\n", "self", ".", "gamma_n", "=", "gamma", "**", "multi_step", "\n", "self", ".", "entropy_tuning", "=", "entropy_tuning", "\n", "self", ".", "grad_clip", "=", "grad_clip", "\n", "self", ".", "updates_per_step", "=", "updates_per_step", "\n", "self", ".", "log_interval", "=", "log_interval", "\n", "self", ".", "target_update_interval", "=", "target_update_interval", "\n", "self", ".", "eval_interval", "=", "eval_interval", "\n", "#", "\n", "self", ".", "eval_runs", "=", "eval_runs", "\n", "self", ".", "huber", "=", "huber", "\n", "self", ".", "multi_step", "=", "multi_step", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent4profile.SacAgent4Profile.run": [[140, 145], ["agent4profile.SacAgent4Profile.train_episode"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.train_episode"], ["", "def", "run", "(", "self", ")", ":", "\n", "        ", "while", "True", ":", "\n", "            ", "self", ".", "train_episode", "(", ")", "\n", "if", "self", ".", "steps", ">", "self", ".", "num_steps", ":", "\n", "                ", "break", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent4profile.SacAgent4Profile.is_update": [[146, 148], ["len"], "methods", ["None"], ["", "", "", "def", "is_update", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "memory", ")", ">", "self", ".", "batch_size", "and", "self", ".", "steps", ">=", "self", ".", "start_steps", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent4profile.SacAgent4Profile.act": [[149, 155], ["agent4profile.SacAgent4Profile.env.action_space.sample", "agent4profile.SacAgent4Profile.explore"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.TanhNormal.sample", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.explore"], ["", "def", "act", "(", "self", ",", "state", ")", ":", "\n", "        ", "if", "self", ".", "start_steps", ">", "self", ".", "steps", ":", "\n", "            ", "action", "=", "self", ".", "env", ".", "action_space", ".", "sample", "(", ")", "\n", "", "else", ":", "\n", "            ", "action", "=", "self", ".", "explore", "(", "state", ")", "\n", "", "return", "action", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent4profile.SacAgent4Profile.explore": [[156, 162], ["torch.FloatTensor().unsqueeze().to", "torch.FloatTensor().unsqueeze().to", "torch.FloatTensor().unsqueeze().to", "torch.FloatTensor().unsqueeze().to", "action.cpu().numpy().reshape", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "agent4profile.SacAgent4Profile.policy.sample", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "action.cpu().numpy", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "action.cpu"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.TanhNormal.sample"], ["", "def", "explore", "(", "self", ",", "state", ")", ":", "\n", "# act with randomness", "\n", "        ", "state", "=", "torch", ".", "FloatTensor", "(", "state", ")", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "action", ",", "_", ",", "_", "=", "self", ".", "policy", ".", "sample", "(", "state", ")", "\n", "", "return", "action", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "reshape", "(", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent4profile.SacAgent4Profile.exploit": [[163, 169], ["torch.FloatTensor().unsqueeze().to", "torch.FloatTensor().unsqueeze().to", "torch.FloatTensor().unsqueeze().to", "torch.FloatTensor().unsqueeze().to", "action.cpu().numpy().reshape", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "agent4profile.SacAgent4Profile.policy.sample", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "action.cpu().numpy", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "action.cpu"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.TanhNormal.sample"], ["", "def", "exploit", "(", "self", ",", "state", ")", ":", "\n", "# act without randomness", "\n", "        ", "state", "=", "torch", ".", "FloatTensor", "(", "state", ")", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "_", ",", "_", ",", "action", "=", "self", ".", "policy", ".", "sample", "(", "state", ")", "\n", "", "return", "action", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "reshape", "(", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent4profile.SacAgent4Profile.calc_current_q": [[170, 173], ["agent4profile.SacAgent4Profile.critic"], "methods", ["None"], ["", "def", "calc_current_q", "(", "self", ",", "states", ",", "actions", ",", "rewards", ",", "next_states", ",", "dones", ")", ":", "\n", "        ", "curr_q1", ",", "curr_q2", "=", "self", ".", "critic", "(", "states", ",", "actions", ")", "\n", "return", "curr_q1", ",", "curr_q2", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent4profile.SacAgent4Profile.calc_target_q": [[174, 187], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "agent4profile.SacAgent4Profile.policy.sample", "agent4profile.SacAgent4Profile.critic_target", "torch.min", "torch.min", "torch.min", "torch.min", "NotImplementedError"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.TanhNormal.sample"], ["", "def", "calc_target_q", "(", "self", ",", "states", ",", "actions", ",", "rewards", ",", "next_states", ",", "dones", ")", ":", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "next_actions", ",", "next_entropies", ",", "_", "=", "self", ".", "policy", ".", "sample", "(", "next_states", ")", "\n", "next_q1", ",", "next_q2", "=", "self", ".", "critic_target", "(", "next_states", ",", "next_actions", ")", "\n", "if", "self", ".", "method", "==", "\"sac\"", "or", "self", ".", "method", "==", "\"redq\"", ":", "\n", "                ", "next_q", "=", "torch", ".", "min", "(", "next_q1", ",", "next_q2", ")", "+", "self", ".", "alpha", "*", "next_entropies", "\n", "", "elif", "self", ".", "method", "==", "\"duvn\"", ":", "\n", "                ", "next_q", "=", "next_q1", "+", "self", ".", "alpha", "*", "next_entropies", "# discard q2", "\n", "", "else", ":", "\n", "                ", "raise", "NotImplementedError", "(", ")", "\n", "# rescale rewards by num step TH20210705", "\n", "", "", "target_q", "=", "(", "rewards", "/", "(", "self", ".", "multi_step", "*", "1.0", ")", ")", "+", "(", "1.0", "-", "dones", ")", "*", "self", ".", "gamma_n", "*", "next_q", "\n", "return", "target_q", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent4profile.SacAgent4Profile.train_episode": [[188, 286], ["agent4profile.SacAgent4Profile.env.reset", "agent4profile.SacAgent4Profile.train_rewards.append", "print", "agent4profile.SacAgent4Profile.is_update", "agent4profile.SacAgent4Profile.writer.add_scalar", "torch.profile", "torch.profile", "prof.key_averages().table", "print", "prof.key_averages", "prof.key_averages().table", "print", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "agent4profile.SacAgent4Profile.evaluate", "agent4profile.SacAgent4Profile.save_models", "agent4profile.SacAgent4Profile.train_rewards.get", "torch.record_function", "torch.record_function", "agent4profile.SacAgent4Profile.act", "agent4profile.SacAgent4Profile.env.step", "agent4profile.SacAgent4Profile.is_update", "open", "f.write", "f.flush", "open", "f.write", "f.flush", "prof.key_averages().table", "print", "prof.key_averages().table", "print", "hasattr", "sum", "sum", "sys.exit", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "utils.to_batch", "agent4profile.SacAgent4Profile.calc_target_q", "agent4profile.SacAgent4Profile.memory.append", "agent4profile.SacAgent4Profile.memory.append", "agent4profile.SacAgent4Profile.learn", "prof.key_averages", "prof.key_averages", "open", "f.write", "f.flush", "open", "f.write", "f.flush", "open", "f.write", "f.flush", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "agent4profile.SacAgent4Profile.calc_current_q", "prof.key_averages", "prof.key_averages", "p.numel", "p.numel", "str", "agent4profile.SacAgent4Profile.critic.parameters", "agent4profile.SacAgent4Profile.critic_target.parameters", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "torch.abs"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.util.utilsTH.SparseRewardEnv.reset", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.RunningMeanStats.append", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.is_update", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.evaluate", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.save_models", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.RunningMeanStats.get", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.act", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.customenvs.humanoid.HumanoidTruncatedObsEnv.step", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.is_update", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.to_batch", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.calc_target_q", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.RunningMeanStats.append", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.RunningMeanStats.append", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.learn", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.calc_current_q"], ["", "def", "train_episode", "(", "self", ")", ":", "\n", "        ", "self", ".", "episodes", "+=", "1", "\n", "episode_reward", "=", "0.", "\n", "episode_steps", "=", "0", "\n", "done", "=", "False", "\n", "state", "=", "self", ".", "env", ".", "reset", "(", ")", "\n", "\n", "while", "not", "done", ":", "\n", "            ", "with", "profiler", ".", "profile", "(", "record_shapes", "=", "False", ",", "use_cuda", "=", "torch", ".", "cuda", ".", "is_available", "(", ")", ",", "profile_memory", "=", "True", ",", "with_stack", "=", "True", ")", "as", "prof", ":", "\n", "                ", "with", "profiler", ".", "record_function", "(", "\"Whole_lines3-11\"", ")", ":", "\n", "                    ", "action", "=", "self", ".", "act", "(", "state", ")", "\n", "next_state", ",", "reward", ",", "done", ",", "_", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "self", ".", "steps", "+=", "1", "\n", "episode_steps", "+=", "1", "\n", "episode_reward", "+=", "reward", "\n", "\n", "# ignore done if the agent reach time horizons", "\n", "# (set done=True only when the agent fails)", "\n", "if", "episode_steps", ">=", "self", ".", "env", ".", "_max_episode_steps", ":", "\n", "                        ", "masked_done", "=", "False", "\n", "", "else", ":", "\n", "                        ", "masked_done", "=", "done", "\n", "\n", "", "if", "self", ".", "per", ":", "\n", "                        ", "batch", "=", "to_batch", "(", "\n", "state", ",", "action", ",", "reward", ",", "next_state", ",", "masked_done", ",", "\n", "self", ".", "device", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                            ", "curr_q1", ",", "curr_q2", "=", "self", ".", "calc_current_q", "(", "*", "batch", ")", "\n", "", "target_q", "=", "self", ".", "calc_target_q", "(", "*", "batch", ")", "\n", "# fixed by tH20210715", "\n", "error", "=", "(", "0.5", "*", "torch", ".", "abs", "(", "curr_q1", "-", "target_q", ")", "+", "0.5", "*", "torch", ".", "abs", "(", "curr_q2", "-", "target_q", ")", ")", ".", "item", "(", ")", "\n", "# We need to give true done signal with addition to masked done", "\n", "# signal to calculate multi-step rewards.", "\n", "self", ".", "memory", ".", "append", "(", "state", ",", "action", ",", "reward", ",", "next_state", ",", "masked_done", ",", "error", ",", "episode_done", "=", "done", ")", "\n", "", "else", ":", "\n", "# We need to give true done signal with addition to masked done", "\n", "# signal to calculate multi-step rewards.", "\n", "                        ", "self", ".", "memory", ".", "append", "(", "state", ",", "action", ",", "reward", ",", "next_state", ",", "masked_done", ",", "episode_done", "=", "done", ")", "\n", "\n", "", "if", "self", ".", "is_update", "(", ")", ":", "\n", "                        ", "self", ".", "learn", "(", ")", "\n", "", "", "", "if", "self", ".", "is_update", "(", ")", ":", "\n", "                ", "res", "=", "prof", ".", "key_averages", "(", "group_by_stack_n", "=", "1", ")", ".", "table", "(", "sort_by", "=", "\"cpu_time_total\"", ",", "row_limit", "=", "10", ")", "\n", "print", "(", "res", ")", "\n", "a", "=", "prof", ".", "key_averages", "(", "group_by_stack_n", "=", "1", ")", "\n", "#", "\n", "with", "open", "(", "self", ".", "log_dir", "+", "\"/\"", "+", "\"cpu_profile.csv\"", ",", "\"a\"", ")", "as", "f", ":", "\n", "                    ", "f", ".", "write", "(", "res", ")", "\n", "f", ".", "flush", "(", ")", "\n", "", "res", "=", "prof", ".", "key_averages", "(", "group_by_stack_n", "=", "1", ")", ".", "table", "(", "sort_by", "=", "\"cpu_memory_usage\"", ",", "row_limit", "=", "10", ")", "\n", "print", "(", "res", ")", "\n", "#", "\n", "with", "open", "(", "self", ".", "log_dir", "+", "\"/\"", "+", "\"memory_profile.csv\"", ",", "\"a\"", ")", "as", "f", ":", "\n", "                    ", "f", ".", "write", "(", "res", ")", "\n", "f", ".", "flush", "(", ")", "\n", "", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "                    ", "res", "=", "prof", ".", "key_averages", "(", "group_by_stack_n", "=", "1", ")", ".", "table", "(", "sort_by", "=", "\"cuda_time_total\"", ",", "row_limit", "=", "10", ")", "\n", "print", "(", "res", ")", "\n", "#", "\n", "with", "open", "(", "self", ".", "log_dir", "+", "\"/\"", "+", "\"cuda_profile.csv\"", ",", "\"a\"", ")", "as", "f", ":", "\n", "                        ", "f", ".", "write", "(", "res", ")", "\n", "f", ".", "flush", "(", ")", "\n", "", "res", "=", "prof", ".", "key_averages", "(", "group_by_stack_n", "=", "1", ")", ".", "table", "(", "sort_by", "=", "\"cuda_memory_usage\"", ",", "row_limit", "=", "10", ")", "\n", "print", "(", "res", ")", "\n", "#", "\n", "with", "open", "(", "self", ".", "log_dir", "+", "\"/\"", "+", "\"cuda_memory_profile.csv\"", ",", "\"a\"", ")", "as", "f", ":", "\n", "                        ", "f", ".", "write", "(", "res", ")", "\n", "f", ".", "flush", "(", ")", "\n", "", "", "if", "not", "hasattr", "(", "self", ",", "\"num_rec\"", ")", ":", "\n", "                    ", "self", ".", "num_rec", "=", "0", "\n", "", "self", ".", "num_rec", "+=", "1", "\n", "if", "self", ".", "num_rec", ">", "4", ":", "\n", "                    ", "num_param", "=", "sum", "(", "p", ".", "numel", "(", ")", "for", "p", "in", "self", ".", "critic", ".", "parameters", "(", ")", "if", "p", ".", "requires_grad", ")", "\n", "num_param", "+=", "sum", "(", "p", ".", "numel", "(", ")", "for", "p", "in", "self", ".", "critic_target", ".", "parameters", "(", ")", "if", "p", ".", "requires_grad", ")", "\n", "#", "\n", "with", "open", "(", "self", ".", "log_dir", "+", "\"/\"", "+", "\"total_critc_params.csv\"", ",", "\"a\"", ")", "as", "f", ":", "\n", "                        ", "f", ".", "write", "(", "str", "(", "num_param", ")", ")", "\n", "f", ".", "flush", "(", ")", "\n", "\n", "\n", "", "sys", ".", "exit", "(", ")", "\n", "\n", "\n", "\n", "", "", "if", "self", ".", "steps", "%", "self", ".", "eval_interval", "==", "0", ":", "\n", "                ", "self", ".", "evaluate", "(", ")", "\n", "self", ".", "save_models", "(", ")", "\n", "\n", "", "state", "=", "next_state", "\n", "\n", "# We log running mean of training rewards.", "\n", "", "self", ".", "train_rewards", ".", "append", "(", "episode_reward", ")", "\n", "\n", "if", "self", ".", "episodes", "%", "self", ".", "log_interval", "==", "0", ":", "\n", "            ", "self", ".", "writer", ".", "add_scalar", "(", "'reward/train'", ",", "self", ".", "train_rewards", ".", "get", "(", ")", ",", "self", ".", "steps", ")", "\n", "\n", "", "print", "(", "f'episode: {self.episodes:<4}  '", "\n", "f'episode steps: {episode_steps:<4}  '", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent4profile.SacAgent4Profile.learn": [[289, 338], ["torch.record_function", "torch.record_function", "torch.record_function", "torch.record_function", "agent4profile.SacAgent4Profile.calc_policy_loss", "utils.update_params", "range", "agent4profile.SacAgent4Profile.memory.sample", "agent4profile.SacAgent4Profile.memory.sample", "agent4profile.SacAgent4Profile.calc_entropy_loss", "utils.update_params", "agent4profile.SacAgent4Profile.log_alpha.exp", "agent4profile.SacAgent4Profile.memory.sample", "agent4profile.SacAgent4Profile.memory.sample", "agent4profile.SacAgent4Profile.calc_critic_4redq_loss", "range", "agent4profile.SacAgent4Profile.calc_critic_loss", "utils.update_params", "utils.update_params", "utils.soft_update", "agent4profile.SacAgent4Profile.memory.update_priority", "utils.update_params", "errors.cpu().numpy", "getattr", "getattr", "errors.cpu", "str", "str"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.calc_policy_loss", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.update_params", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.TanhNormal.sample", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.TanhNormal.sample", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.calc_entropy_loss", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.update_params", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.TanhNormal.sample", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.TanhNormal.sample", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.calc_critic_4redq_loss", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.calc_critic_loss", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.update_params", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.update_params", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.soft_update", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.update_params"], ["", "def", "learn", "(", "self", ")", ":", "\n", "        ", "self", ".", "learning_steps", "+=", "1", "\n", "\n", "# critic update", "\n", "with", "profiler", ".", "record_function", "(", "\"TrainingQs_lines4-8\"", ")", ":", "\n", "            ", "if", "(", "self", ".", "learning_steps", "-", "1", ")", "%", "self", ".", "critic_update_delay", "==", "0", ":", "\n", "                ", "for", "_", "in", "range", "(", "self", ".", "updates_per_step", ")", ":", "\n", "                    ", "if", "self", ".", "per", ":", "\n", "# batch with indices and priority weights", "\n", "                        ", "batch", ",", "indices", ",", "weights", "=", "self", ".", "memory", ".", "sample", "(", "self", ".", "batch_size", ")", "\n", "", "else", ":", "\n", "                        ", "batch", "=", "self", ".", "memory", ".", "sample", "(", "self", ".", "batch_size", ")", "\n", "# set priority weights to 1 when we don't use PER.", "\n", "weights", "=", "1.", "\n", "\n", "", "if", "self", ".", "method", "==", "\"redq\"", ":", "\n", "                        ", "losses", ",", "errors", ",", "mean_q1", ",", "mean_q2", "=", "self", ".", "calc_critic_4redq_loss", "(", "batch", ",", "weights", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "critic", ".", "N", ")", ":", "\n", "                            ", "update_params", "(", "getattr", "(", "self", ",", "\"q\"", "+", "str", "(", "i", ")", "+", "\"_optim\"", ")", ",", "\n", "getattr", "(", "self", ".", "critic", ",", "\"Q\"", "+", "str", "(", "i", ")", ")", ",", "\n", "losses", "[", "i", "]", ",", "self", ".", "grad_clip", ")", "\n", "", "", "else", ":", "\n", "                        ", "q1_loss", ",", "q2_loss", ",", "errors", ",", "mean_q1", ",", "mean_q2", "=", "self", ".", "calc_critic_loss", "(", "batch", ",", "weights", ")", "\n", "\n", "update_params", "(", "self", ".", "q1_optim", ",", "self", ".", "critic", ".", "Q1", ",", "q1_loss", ",", "self", ".", "grad_clip", ")", "\n", "update_params", "(", "self", ".", "q2_optim", ",", "self", ".", "critic", ".", "Q2", ",", "q2_loss", ",", "self", ".", "grad_clip", ")", "\n", "\n", "", "if", "self", ".", "learning_steps", "%", "self", ".", "target_update_interval", "==", "0", ":", "\n", "                        ", "soft_update", "(", "self", ".", "critic_target", ",", "self", ".", "critic", ",", "self", ".", "tau", ")", "\n", "\n", "", "if", "self", ".", "per", ":", "\n", "# update priority weights", "\n", "                        ", "self", ".", "memory", ".", "update_priority", "(", "indices", ",", "errors", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "\n", "# policy and alpha update", "\n", "", "", "", "", "with", "profiler", ".", "record_function", "(", "\"TrainingPolicy_line10\"", ")", ":", "\n", "            ", "if", "self", ".", "per", ":", "\n", "                ", "batch", ",", "indices", ",", "weights", "=", "self", ".", "memory", ".", "sample", "(", "self", ".", "batch_size", ")", "\n", "", "else", ":", "\n", "                ", "batch", "=", "self", ".", "memory", ".", "sample", "(", "self", ".", "batch_size", ")", "\n", "weights", "=", "1.", "\n", "\n", "", "policy_loss", ",", "entropies", "=", "self", ".", "calc_policy_loss", "(", "batch", ",", "weights", ")", "# added by tH 20210705", "\n", "update_params", "(", "self", ".", "policy_optim", ",", "self", ".", "policy", ",", "policy_loss", ",", "self", ".", "grad_clip", ")", "\n", "\n", "if", "self", ".", "entropy_tuning", ":", "\n", "                ", "entropy_loss", "=", "self", ".", "calc_entropy_loss", "(", "entropies", ",", "weights", ")", "\n", "update_params", "(", "self", ".", "alpha_optim", ",", "None", ",", "entropy_loss", ")", "\n", "self", ".", "alpha", "=", "self", ".", "log_alpha", ".", "exp", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent4profile.SacAgent4Profile.calc_critic_4redq_loss": [[339, 356], ["agent4profile.SacAgent4Profile.critic.allQs", "agent4profile.SacAgent4Profile.calc_target_q", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "curr_qs[].detach().mean().item", "curr_qs[].detach().mean().item", "losses.append", "curr_qs[].detach", "curr_qs[].detach().mean", "curr_qs[].detach().mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "curr_qs[].detach", "curr_qs[].detach"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.model.RandomizedEnsembleNetwork.allQs", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.calc_target_q", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.RunningMeanStats.append"], ["", "", "", "def", "calc_critic_4redq_loss", "(", "self", ",", "batch", ",", "weights", ")", ":", "\n", "        ", "states", ",", "actions", ",", "rewards", ",", "next_states", ",", "dones", "=", "batch", "\n", "curr_qs", "=", "self", ".", "critic", ".", "allQs", "(", "states", ",", "actions", ")", "\n", "\n", "target_q", "=", "self", ".", "calc_target_q", "(", "*", "batch", ")", "\n", "\n", "# TD errors for updating priority weights", "\n", "errors", "=", "torch", ".", "abs", "(", "curr_qs", "[", "0", "]", ".", "detach", "(", ")", "-", "target_q", ")", "# TODO better to use average of all errors", "\n", "# We log means of Q to monitor training.", "\n", "mean_q1", "=", "curr_qs", "[", "0", "]", ".", "detach", "(", ")", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "mean_q2", "=", "curr_qs", "[", "1", "]", ".", "detach", "(", ")", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "\n", "# Critic loss is mean squared TD errors with priority weights.", "\n", "losses", "=", "[", "]", "\n", "for", "curr_q", "in", "curr_qs", ":", "\n", "            ", "losses", ".", "append", "(", "torch", ".", "mean", "(", "(", "curr_q", "-", "target_q", ")", ".", "pow", "(", "2", ")", "*", "weights", ")", ")", "\n", "", "return", "losses", ",", "errors", ",", "mean_q1", ",", "mean_q2", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent4profile.SacAgent4Profile.calc_critic_loss": [[357, 372], ["agent4profile.SacAgent4Profile.calc_current_q", "agent4profile.SacAgent4Profile.calc_target_q", "torch.abs", "torch.abs", "torch.abs", "torch.abs", "curr_q1.detach().mean().item", "curr_q2.detach().mean().item", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "curr_q1.detach", "curr_q1.detach().mean", "curr_q2.detach().mean", "curr_q1.detach", "curr_q2.detach"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.calc_current_q", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.calc_target_q"], ["", "def", "calc_critic_loss", "(", "self", ",", "batch", ",", "weights", ")", ":", "\n", "        ", "assert", "self", ".", "method", "==", "\"sac\"", "or", "self", ".", "method", "==", "\"duvn\"", ",", "\"This method is only for sac or duvn method\"", "\n", "\n", "curr_q1", ",", "curr_q2", "=", "self", ".", "calc_current_q", "(", "*", "batch", ")", "\n", "target_q", "=", "self", ".", "calc_target_q", "(", "*", "batch", ")", "\n", "\n", "# TD errors for updating priority weights", "\n", "errors", "=", "torch", ".", "abs", "(", "curr_q1", ".", "detach", "(", ")", "-", "target_q", ")", "\n", "# We log means of Q to monitor training.", "\n", "mean_q1", "=", "curr_q1", ".", "detach", "(", ")", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "mean_q2", "=", "curr_q2", ".", "detach", "(", ")", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "\n", "q1_loss", "=", "torch", ".", "mean", "(", "(", "curr_q1", "-", "target_q", ")", ".", "pow", "(", "2", ")", "*", "weights", ")", "\n", "q2_loss", "=", "torch", ".", "mean", "(", "(", "curr_q2", "-", "target_q", ")", ".", "pow", "(", "2", ")", "*", "weights", ")", "\n", "return", "q1_loss", ",", "q2_loss", ",", "errors", ",", "mean_q1", ",", "mean_q2", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent4profile.SacAgent4Profile.calc_policy_loss": [[373, 394], ["agent4profile.SacAgent4Profile.policy.sample", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "agent4profile.SacAgent4Profile.critic.averageQ", "agent4profile.SacAgent4Profile.critic", "torch.min", "torch.min", "torch.min", "torch.min"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.TanhNormal.sample", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.model.RandomizedEnsembleNetwork.averageQ"], ["", "def", "calc_policy_loss", "(", "self", ",", "batch", ",", "weights", ")", ":", "\n", "        ", "states", ",", "actions", ",", "rewards", ",", "next_states", ",", "dones", "=", "batch", "\n", "\n", "# We re-sample actions to calculate expectations of Q.", "\n", "sampled_action", ",", "entropy", ",", "_", "=", "self", ".", "policy", ".", "sample", "(", "states", ")", "\n", "# expectations of Q with clipped double Q technique", "\n", "if", "self", ".", "method", "==", "\"redq\"", ":", "\n", "            ", "q", "=", "self", ".", "critic", ".", "averageQ", "(", "states", ",", "sampled_action", ")", "\n", "", "else", ":", "\n", "            ", "q1", ",", "q2", "=", "self", ".", "critic", "(", "states", ",", "sampled_action", ")", "\n", "if", "self", ".", "method", "==", "\"duvn\"", ":", "\n", "                ", "q2", "=", "q1", "# discard q2", "\n", "", "if", "self", ".", "target_drop_rate", ">", "0.0", ":", "\n", "                ", "q", "=", "0.5", "*", "(", "q1", "+", "q2", ")", "\n", "", "else", ":", "\n", "                ", "q", "=", "torch", ".", "min", "(", "q1", ",", "q2", ")", "\n", "# Policy objective is maximization of (Q + alpha * entropy) with", "\n", "# priority weights.", "\n", "", "", "policy_loss", "=", "torch", ".", "mean", "(", "(", "-", "q", "-", "self", ".", "alpha", "*", "entropy", ")", "*", "weights", ")", "\n", "\n", "return", "policy_loss", ",", "entropy", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent4profile.SacAgent4Profile.calc_entropy_loss": [[395, 398], ["torch.mean", "torch.mean", "torch.mean", "torch.mean"], "methods", ["None"], ["", "def", "calc_entropy_loss", "(", "self", ",", "entropy", ",", "weights", ")", ":", "\n", "        ", "entropy_loss", "=", "-", "torch", ".", "mean", "(", "self", ".", "log_alpha", "*", "(", "self", ".", "target_entropy", "-", "entropy", ")", ".", "detach", "(", ")", "*", "weights", ")", "\n", "return", "entropy_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent4profile.SacAgent4Profile.evaluate": [[399, 478], ["numpy.zeros", "range", "numpy.mean", "range", "numpy.mean", "range", "list", "numpy.mean", "numpy.std", "print", "print", "agent4profile.SacAgent4Profile.writer.add_scalar", "print", "print", "print", "agent4profile.SacAgent4Profile.env.reset", "collections.deque", "reversed", "list", "math.fabs", "[].tolist", "[].tolist", "range", "itertools.chain.from_iterable", "open", "f.write", "f.flush", "open", "f.write", "f.flush", "open", "f.write", "f.flush", "range", "agent4profile.SacAgent4Profile.exploit", "agent4profile.SacAgent4Profile.env.step", "sar_buf[].append", "range", "mc_discounted_return[].appendleft", "itertools.chain.from_iterable", "range", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "agent4profile.SacAgent4Profile.to().numpy", "len", "norm_scores[].append", "str", "str", "len", "agent4profile.SacAgent4Profile.critic.averageQ", "agent4profile.SacAgent4Profile.critic", "numpy.array", "numpy.array", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "agent4profile.SacAgent4Profile.to", "str", "str", "str", "str", "str", "str"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.util.utilsTH.SparseRewardEnv.reset", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.exploit", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.customenvs.humanoid.HumanoidTruncatedObsEnv.step", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.RunningMeanStats.append", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.RunningMeanStats.append", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.model.RandomizedEnsembleNetwork.averageQ"], ["", "def", "evaluate", "(", "self", ")", ":", "\n", "        ", "episodes", "=", "self", ".", "eval_runs", "\n", "returns", "=", "np", ".", "zeros", "(", "(", "episodes", ",", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "# for return bias estimation TH", "\n", "sar_buf", "=", "[", "[", "]", "for", "_", "in", "range", "(", "episodes", ")", "]", "# episodes x (satte, action , reward)", "\n", "\n", "for", "i", "in", "range", "(", "episodes", ")", ":", "\n", "            ", "state", "=", "self", ".", "env", ".", "reset", "(", ")", "\n", "episode_reward", "=", "0.", "\n", "done", "=", "False", "\n", "while", "not", "done", ":", "\n", "                ", "action", "=", "self", ".", "exploit", "(", "state", ")", "\n", "next_state", ",", "reward", ",", "done", ",", "_", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "episode_reward", "+=", "reward", "\n", "state", "=", "next_state", "\n", "# MCE store all (state, action, reward) TH 20210723", "\n", "sar_buf", "[", "i", "]", ".", "append", "(", "[", "state", ",", "action", ",", "reward", "]", ")", "\n", "\n", "", "returns", "[", "i", "]", "=", "episode_reward", "\n", "\n", "", "mean_return", "=", "np", ".", "mean", "(", "returns", ")", "\n", "\n", "# calculate mean / std return bias. TH 20210801", "\n", "# - calculate MCE future discounted return (in backward)", "\n", "mc_discounted_return", "=", "[", "deque", "(", ")", "for", "_", "in", "range", "(", "episodes", ")", "]", "\n", "for", "i", "in", "range", "(", "episodes", ")", ":", "\n", "            ", "for", "re_tran", "in", "reversed", "(", "sar_buf", "[", "i", "]", ")", ":", "\n", "                ", "if", "len", "(", "mc_discounted_return", "[", "i", "]", ")", ">", "0", ":", "\n", "                    ", "mcret", "=", "re_tran", "[", "2", "]", "+", "self", ".", "gamma_n", "*", "mc_discounted_return", "[", "i", "]", "[", "0", "]", "\n", "", "else", ":", "\n", "                    ", "mcret", "=", "re_tran", "[", "2", "]", "\n", "", "mc_discounted_return", "[", "i", "]", ".", "appendleft", "(", "mcret", ")", "\n", "# - calculate normalized MCE return by averaging all MCE returns", "\n", "", "", "norm_coef", "=", "np", ".", "mean", "(", "list", "(", "itertools", ".", "chain", ".", "from_iterable", "(", "mc_discounted_return", ")", ")", ")", "\n", "norm_coef", "=", "math", ".", "fabs", "(", "norm_coef", ")", "+", "0.000001", "\n", "# - estimate return for all state action, and normalized score", "\n", "norm_scores", "=", "[", "[", "]", "for", "_", "in", "range", "(", "episodes", ")", "]", "\n", "for", "i", "in", "range", "(", "episodes", ")", ":", "\n", "# calculate normalized score", "\n", "            ", "states", "=", "np", ".", "array", "(", "sar_buf", "[", "i", "]", ",", "dtype", "=", "\"object\"", ")", "[", ":", ",", "0", "]", ".", "tolist", "(", ")", "\n", "actions", "=", "np", ".", "array", "(", "sar_buf", "[", "i", "]", ",", "dtype", "=", "\"object\"", ")", "[", ":", ",", "1", "]", ".", "tolist", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "state", "=", "torch", ".", "FloatTensor", "(", "states", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "action", "=", "torch", ".", "FloatTensor", "(", "actions", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "if", "self", ".", "method", "==", "\"redq\"", ":", "\n", "                    ", "q", "=", "self", ".", "critic", ".", "averageQ", "(", "state", ",", "action", ")", "\n", "", "else", ":", "\n", "                    ", "q1", ",", "q2", "=", "self", ".", "critic", "(", "state", ",", "action", ")", "\n", "q", "=", "0.5", "*", "(", "q1", "+", "q2", ")", "\n", "", "qs", "=", "q", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "", "for", "j", "in", "range", "(", "len", "(", "sar_buf", "[", "i", "]", ")", ")", ":", "\n", "                ", "score", "=", "(", "qs", "[", "j", "]", "[", "0", "]", "-", "mc_discounted_return", "[", "i", "]", "[", "j", "]", ")", "/", "norm_coef", "\n", "norm_scores", "[", "i", "]", ".", "append", "(", "score", ")", "\n", "# calculate std", "\n", "", "", "flatten_norm_score", "=", "list", "(", "itertools", ".", "chain", ".", "from_iterable", "(", "norm_scores", ")", ")", "\n", "mean_norm_score", "=", "np", ".", "mean", "(", "flatten_norm_score", ")", "\n", "std_norm_score", "=", "np", ".", "std", "(", "flatten_norm_score", ")", "\n", "print", "(", "\"mean norm score \"", "+", "str", "(", "mean_norm_score", ")", ")", "\n", "print", "(", "\"std norm score \"", "+", "str", "(", "std_norm_score", ")", ")", "\n", "\n", "self", ".", "writer", ".", "add_scalar", "(", "\n", "'reward/test'", ",", "mean_return", ",", "self", ".", "steps", ")", "\n", "print", "(", "'-'", "*", "60", ")", "\n", "print", "(", "f'Num steps: {self.steps:<5}  '", "\n", "f'reward: {mean_return:<5.1f}'", ")", "\n", "print", "(", "'-'", "*", "60", ")", "\n", "#", "\n", "with", "open", "(", "self", ".", "log_dir", "+", "\"/\"", "+", "\"reward.csv\"", ",", "\"a\"", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "str", "(", "self", ".", "steps", ")", "+", "\",\"", "+", "str", "(", "mean_return", ")", "+", "\",\\n\"", ")", "\n", "f", ".", "flush", "(", ")", "\n", "#", "\n", "", "with", "open", "(", "self", ".", "log_dir", "+", "\"/\"", "+", "\"avrbias.csv\"", ",", "\"a\"", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "str", "(", "self", ".", "steps", ")", "+", "\",\"", "+", "str", "(", "mean_norm_score", ")", "+", "\",\\n\"", ")", "\n", "f", ".", "flush", "(", ")", "\n", "#", "\n", "", "with", "open", "(", "self", ".", "log_dir", "+", "\"/\"", "+", "\"stdbias.csv\"", ",", "\"a\"", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "str", "(", "self", ".", "steps", ")", "+", "\",\"", "+", "str", "(", "std_norm_score", ")", "+", "\",\\n\"", ")", "\n", "f", ".", "flush", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent4profile.SacAgent4Profile.save_models": [[480, 485], ["agent4profile.SacAgent4Profile.policy.save", "agent4profile.SacAgent4Profile.critic.save", "agent4profile.SacAgent4Profile.critic_target.save", "os.path.join", "os.path.join", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.model.BaseNetwork.save", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.model.BaseNetwork.save", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.model.BaseNetwork.save"], ["", "", "def", "save_models", "(", "self", ")", ":", "\n", "        ", "self", ".", "policy", ".", "save", "(", "os", ".", "path", ".", "join", "(", "self", ".", "model_dir", ",", "'policy.pth'", ")", ")", "\n", "self", ".", "critic", ".", "save", "(", "os", ".", "path", ".", "join", "(", "self", ".", "model_dir", ",", "'critic.pth'", ")", ")", "\n", "self", ".", "critic_target", ".", "save", "(", "\n", "os", ".", "path", ".", "join", "(", "self", ".", "model_dir", ",", "'critic_target.pth'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent4profile.SacAgent4Profile.__del__": [[486, 489], ["agent4profile.SacAgent4Profile.writer.close", "agent4profile.SacAgent4Profile.env.close"], "methods", ["None"], ["", "def", "__del__", "(", "self", ")", ":", "\n", "        ", "self", ".", "writer", ".", "close", "(", ")", "\n", "self", ".", "env", ".", "close", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.__init__": [[17, 134], ["torch.manual_seed", "numpy.random.seed", "agent.SacAgent.env.seed", "torch.device", "model.GaussianPolicy().to", "utils.hard_update", "utils.grad_false", "torch.optim.Adam", "os.path.join", "os.path.join", "torch.utils.tensorboard.SummaryWriter", "utils.RunningMeanStats", "model.RandomizedEnsembleNetwork().to", "model.RandomizedEnsembleNetwork().to", "model.TwinnedQNetwork().to", "model.TwinnedQNetwork().to", "agent.SacAgent.critic_target.eval", "agent.SacAgent.policy.parameters", "range", "torch.optim.Adam", "torch.optim.Adam", "torch.zeros", "agent.SacAgent.log_alpha.exp", "torch.optim.Adam", "torch.tensor().to", "rltorch.memory.PrioritizedMemory", "rltorch.memory.MultiStepMemory", "os.path.exists", "os.makedirs", "os.path.exists", "os.makedirs", "torch.cuda.is_available", "model.GaussianPolicy", "setattr", "agent.SacAgent.critic.Q1.parameters", "agent.SacAgent.critic.Q2.parameters", "torch.prod().item", "str", "model.RandomizedEnsembleNetwork", "model.RandomizedEnsembleNetwork", "model.TwinnedQNetwork", "model.TwinnedQNetwork", "torch.optim.Adam", "torch.prod().item", "torch.tensor", "getattr().parameters", "torch.prod", "str", "torch.Tensor().to", "torch.prod", "getattr", "torch.Tensor().to", "torch.Tensor", "str", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.hard_update", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.grad_false"], ["    ", "def", "__init__", "(", "self", ",", "env", ",", "log_dir", ",", "num_steps", "=", "3000000", ",", "batch_size", "=", "256", ",", "\n", "lr", "=", "0.0003", ",", "hidden_units", "=", "[", "256", ",", "256", "]", ",", "memory_size", "=", "1e6", ",", "\n", "gamma", "=", "0.99", ",", "tau", "=", "0.005", ",", "entropy_tuning", "=", "True", ",", "ent_coef", "=", "0.2", ",", "\n", "multi_step", "=", "1", ",", "per", "=", "False", ",", "alpha", "=", "0.6", ",", "beta", "=", "0.4", ",", "\n", "beta_annealing", "=", "0.0001", ",", "grad_clip", "=", "None", ",", "updates_per_step", "=", "1", ",", "\n", "start_steps", "=", "10000", ",", "log_interval", "=", "10", ",", "target_update_interval", "=", "1", ",", "\n", "eval_interval", "=", "1000", ",", "cuda", "=", "0", ",", "seed", "=", "0", ",", "\n", "# added by TH 20210707", "\n", "eval_runs", "=", "1", ",", "huber", "=", "0", ",", "layer_norm", "=", "0", ",", "\n", "method", "=", "None", ",", "target_entropy", "=", "None", ",", "target_drop_rate", "=", "0.0", ",", "critic_update_delay", "=", "1", ")", ":", "\n", "        ", "self", ".", "env", "=", "env", "\n", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "self", ".", "env", ".", "seed", "(", "seed", ")", "\n", "torch", ".", "backends", ".", "cudnn", ".", "deterministic", "=", "True", "# It harms a performance.", "\n", "torch", ".", "backends", ".", "cudnn", ".", "benchmark", "=", "False", "\n", "\n", "self", ".", "method", "=", "method", "\n", "self", ".", "critic_update_delay", "=", "critic_update_delay", "\n", "self", ".", "target_drop_rate", "=", "target_drop_rate", "\n", "\n", "self", ".", "device", "=", "torch", ".", "device", "(", "\"cuda:\"", "+", "str", "(", "cuda", ")", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "\n", "# policy", "\n", "self", ".", "policy", "=", "GaussianPolicy", "(", "\n", "self", ".", "env", ".", "observation_space", ".", "shape", "[", "0", "]", ",", "\n", "self", ".", "env", ".", "action_space", ".", "shape", "[", "0", "]", ",", "\n", "hidden_units", "=", "hidden_units", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "# Q functions", "\n", "kwargs_q", "=", "{", "\"num_inputs\"", ":", "self", ".", "env", ".", "observation_space", ".", "shape", "[", "0", "]", ",", "\n", "\"num_actions\"", ":", "self", ".", "env", ".", "action_space", ".", "shape", "[", "0", "]", ",", "\n", "\"hidden_units\"", ":", "hidden_units", ",", "\n", "\"layer_norm\"", ":", "layer_norm", ",", "\n", "\"drop_rate\"", ":", "self", ".", "target_drop_rate", "}", "\n", "if", "self", ".", "method", "==", "\"redq\"", ":", "\n", "            ", "self", ".", "critic", "=", "RandomizedEnsembleNetwork", "(", "**", "kwargs_q", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "critic_target", "=", "RandomizedEnsembleNetwork", "(", "**", "kwargs_q", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "critic", "=", "TwinnedQNetwork", "(", "**", "kwargs_q", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "critic_target", "=", "TwinnedQNetwork", "(", "**", "kwargs_q", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "", "if", "self", ".", "target_drop_rate", "<=", "0.0", ":", "\n", "            ", "self", ".", "critic_target", "=", "self", ".", "critic_target", ".", "eval", "(", ")", "\n", "# copy parameters of the learning network to the target network", "\n", "", "hard_update", "(", "self", ".", "critic_target", ",", "self", ".", "critic", ")", "\n", "# disable gradient calculations of the target network", "\n", "grad_false", "(", "self", ".", "critic_target", ")", "\n", "\n", "# optimizer", "\n", "self", ".", "policy_optim", "=", "Adam", "(", "self", ".", "policy", ".", "parameters", "(", ")", ",", "lr", "=", "lr", ")", "\n", "if", "self", ".", "method", "==", "\"redq\"", ":", "\n", "            ", "for", "i", "in", "range", "(", "self", ".", "critic", ".", "N", ")", ":", "\n", "                ", "setattr", "(", "self", ",", "\"q\"", "+", "str", "(", "i", ")", "+", "\"_optim\"", ",", "\n", "Adam", "(", "getattr", "(", "self", ".", "critic", ",", "\"Q\"", "+", "str", "(", "i", ")", ")", ".", "parameters", "(", ")", ",", "lr", "=", "lr", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "q1_optim", "=", "Adam", "(", "self", ".", "critic", ".", "Q1", ".", "parameters", "(", ")", ",", "lr", "=", "lr", ")", "\n", "self", ".", "q2_optim", "=", "Adam", "(", "self", ".", "critic", ".", "Q2", ".", "parameters", "(", ")", ",", "lr", "=", "lr", ")", "\n", "\n", "", "if", "entropy_tuning", ":", "\n", "            ", "if", "not", "(", "target_entropy", "is", "None", ")", ":", "\n", "                ", "self", ".", "target_entropy", "=", "torch", ".", "prod", "(", "torch", ".", "Tensor", "(", "[", "target_entropy", "]", ")", ".", "to", "(", "self", ".", "device", ")", ")", ".", "item", "(", ")", "\n", "", "else", ":", "\n", "# Target entropy is -|A|.", "\n", "                ", "self", ".", "target_entropy", "=", "-", "torch", ".", "prod", "(", "torch", ".", "Tensor", "(", "self", ".", "env", ".", "action_space", ".", "shape", ")", ".", "to", "(", "self", ".", "device", ")", ")", ".", "item", "(", ")", "\n", "# We optimize log(alpha), instead of alpha.", "\n", "", "self", ".", "log_alpha", "=", "torch", ".", "zeros", "(", "1", ",", "requires_grad", "=", "True", ",", "device", "=", "self", ".", "device", ")", "\n", "self", ".", "alpha", "=", "self", ".", "log_alpha", ".", "exp", "(", ")", "\n", "self", ".", "alpha_optim", "=", "Adam", "(", "[", "self", ".", "log_alpha", "]", ",", "lr", "=", "lr", ")", "\n", "", "else", ":", "\n", "# fixed alpha", "\n", "            ", "self", ".", "alpha", "=", "torch", ".", "tensor", "(", "ent_coef", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "", "if", "per", ":", "\n", "# replay memory with prioritied experience replay", "\n", "# See https://github.com/ku2482/rltorch/blob/master/rltorch/memory", "\n", "            ", "self", ".", "memory", "=", "PrioritizedMemory", "(", "\n", "memory_size", ",", "self", ".", "env", ".", "observation_space", ".", "shape", ",", "\n", "self", ".", "env", ".", "action_space", ".", "shape", ",", "self", ".", "device", ",", "gamma", ",", "multi_step", ",", "\n", "alpha", "=", "alpha", ",", "beta", "=", "beta", ",", "beta_annealing", "=", "beta_annealing", ")", "\n", "", "else", ":", "\n", "# replay memory without prioritied experience replay", "\n", "# See https://github.com/ku2482/rltorch/blob/master/rltorch/memory", "\n", "            ", "self", ".", "memory", "=", "MultiStepMemory", "(", "\n", "memory_size", ",", "self", ".", "env", ".", "observation_space", ".", "shape", ",", "\n", "self", ".", "env", ".", "action_space", ".", "shape", ",", "self", ".", "device", ",", "gamma", ",", "multi_step", ")", "\n", "\n", "", "self", ".", "log_dir", "=", "log_dir", "\n", "self", ".", "model_dir", "=", "os", ".", "path", ".", "join", "(", "log_dir", ",", "'model'", ")", "\n", "self", ".", "summary_dir", "=", "os", ".", "path", ".", "join", "(", "log_dir", ",", "'summary'", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "self", ".", "model_dir", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "self", ".", "model_dir", ")", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "self", ".", "summary_dir", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "self", ".", "summary_dir", ")", "\n", "\n", "", "self", ".", "writer", "=", "SummaryWriter", "(", "log_dir", "=", "self", ".", "summary_dir", ")", "\n", "self", ".", "train_rewards", "=", "RunningMeanStats", "(", "log_interval", ")", "\n", "\n", "self", ".", "steps", "=", "0", "\n", "self", ".", "learning_steps", "=", "0", "\n", "self", ".", "episodes", "=", "0", "\n", "self", ".", "num_steps", "=", "num_steps", "\n", "self", ".", "tau", "=", "tau", "\n", "self", ".", "per", "=", "per", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "start_steps", "=", "start_steps", "\n", "self", ".", "gamma_n", "=", "gamma", "**", "multi_step", "\n", "self", ".", "entropy_tuning", "=", "entropy_tuning", "\n", "self", ".", "grad_clip", "=", "grad_clip", "\n", "self", ".", "updates_per_step", "=", "updates_per_step", "\n", "self", ".", "log_interval", "=", "log_interval", "\n", "self", ".", "target_update_interval", "=", "target_update_interval", "\n", "self", ".", "eval_interval", "=", "eval_interval", "\n", "#", "\n", "self", ".", "eval_runs", "=", "eval_runs", "\n", "self", ".", "huber", "=", "huber", "\n", "self", ".", "multi_step", "=", "multi_step", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.run": [[135, 140], ["agent.SacAgent.train_episode"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.train_episode"], ["", "def", "run", "(", "self", ")", ":", "\n", "        ", "while", "True", ":", "\n", "            ", "self", ".", "train_episode", "(", ")", "\n", "if", "self", ".", "steps", ">", "self", ".", "num_steps", ":", "\n", "                ", "break", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.is_update": [[141, 143], ["len"], "methods", ["None"], ["", "", "", "def", "is_update", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "memory", ")", ">", "self", ".", "batch_size", "and", "self", ".", "steps", ">=", "self", ".", "start_steps", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.act": [[144, 150], ["agent.SacAgent.env.action_space.sample", "agent.SacAgent.explore"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.TanhNormal.sample", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.explore"], ["", "def", "act", "(", "self", ",", "state", ")", ":", "\n", "        ", "if", "self", ".", "start_steps", ">", "self", ".", "steps", ":", "\n", "            ", "action", "=", "self", ".", "env", ".", "action_space", ".", "sample", "(", ")", "\n", "", "else", ":", "\n", "            ", "action", "=", "self", ".", "explore", "(", "state", ")", "\n", "", "return", "action", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.explore": [[151, 157], ["torch.FloatTensor().unsqueeze().to", "action.cpu().numpy().reshape", "torch.no_grad", "agent.SacAgent.policy.sample", "torch.FloatTensor().unsqueeze", "action.cpu().numpy", "torch.FloatTensor", "action.cpu"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.TanhNormal.sample"], ["", "def", "explore", "(", "self", ",", "state", ")", ":", "\n", "# act with randomness", "\n", "        ", "state", "=", "torch", ".", "FloatTensor", "(", "state", ")", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "action", ",", "_", ",", "_", "=", "self", ".", "policy", ".", "sample", "(", "state", ")", "\n", "", "return", "action", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "reshape", "(", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.exploit": [[158, 164], ["torch.FloatTensor().unsqueeze().to", "action.cpu().numpy().reshape", "torch.no_grad", "agent.SacAgent.policy.sample", "torch.FloatTensor().unsqueeze", "action.cpu().numpy", "torch.FloatTensor", "action.cpu"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.TanhNormal.sample"], ["", "def", "exploit", "(", "self", ",", "state", ")", ":", "\n", "# act without randomness", "\n", "        ", "state", "=", "torch", ".", "FloatTensor", "(", "state", ")", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "_", ",", "_", ",", "action", "=", "self", ".", "policy", ".", "sample", "(", "state", ")", "\n", "", "return", "action", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "reshape", "(", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.calc_current_q": [[165, 168], ["agent.SacAgent.critic"], "methods", ["None"], ["", "def", "calc_current_q", "(", "self", ",", "states", ",", "actions", ",", "rewards", ",", "next_states", ",", "dones", ")", ":", "\n", "        ", "curr_q1", ",", "curr_q2", "=", "self", ".", "critic", "(", "states", ",", "actions", ")", "\n", "return", "curr_q1", ",", "curr_q2", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.calc_target_q": [[169, 185], ["torch.no_grad", "agent.SacAgent.policy.sample", "agent.SacAgent.critic_target", "torch.min", "agent.SacAgent.critic_target", "NotImplementedError", "torch.min"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.TanhNormal.sample"], ["", "def", "calc_target_q", "(", "self", ",", "states", ",", "actions", ",", "rewards", ",", "next_states", ",", "dones", ")", ":", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "next_actions", ",", "next_entropies", ",", "_", "=", "self", ".", "policy", ".", "sample", "(", "next_states", ")", "\n", "next_q1", ",", "next_q2", "=", "self", ".", "critic_target", "(", "next_states", ",", "next_actions", ")", "\n", "if", "self", ".", "method", "==", "\"sac\"", "or", "self", ".", "method", "==", "\"redq\"", ":", "\n", "                ", "next_q", "=", "torch", ".", "min", "(", "next_q1", ",", "next_q2", ")", "+", "self", ".", "alpha", "*", "next_entropies", "\n", "", "elif", "self", ".", "method", "==", "\"duvn\"", ":", "\n", "                ", "next_q", "=", "next_q1", "+", "self", ".", "alpha", "*", "next_entropies", "# discard q2", "\n", "", "elif", "self", ".", "method", "==", "\"monosac\"", ":", "\n", "                ", "next_q2", ",", "_", "=", "self", ".", "critic_target", "(", "next_states", ",", "next_actions", ")", "\n", "next_q", "=", "torch", ".", "min", "(", "next_q1", ",", "next_q2", ")", "+", "self", ".", "alpha", "*", "next_entropies", "\n", "", "else", ":", "\n", "                ", "raise", "NotImplementedError", "(", ")", "\n", "# rescale rewards by num step TH20210705", "\n", "", "", "target_q", "=", "(", "rewards", "/", "(", "self", ".", "multi_step", "*", "1.0", ")", ")", "+", "(", "1.0", "-", "dones", ")", "*", "self", ".", "gamma_n", "*", "next_q", "\n", "return", "target_q", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.train_episode": [[186, 240], ["agent.SacAgent.env.reset", "agent.SacAgent.train_rewards.append", "print", "agent.SacAgent.act", "agent.SacAgent.env.step", "agent.SacAgent.is_update", "agent.SacAgent.writer.add_scalar", "utils.to_batch", "agent.SacAgent.calc_target_q", "agent.SacAgent.memory.append", "agent.SacAgent.memory.append", "agent.SacAgent.learn", "agent.SacAgent.evaluate", "agent.SacAgent.save_models", "agent.SacAgent.train_rewards.get", "torch.no_grad", "agent.SacAgent.calc_current_q", "torch.abs", "torch.abs"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.util.utilsTH.SparseRewardEnv.reset", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.RunningMeanStats.append", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.act", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.customenvs.humanoid.HumanoidTruncatedObsEnv.step", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.is_update", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.to_batch", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.calc_target_q", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.RunningMeanStats.append", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.RunningMeanStats.append", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.learn", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.evaluate", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.save_models", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.RunningMeanStats.get", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.calc_current_q"], ["", "def", "train_episode", "(", "self", ")", ":", "\n", "        ", "self", ".", "episodes", "+=", "1", "\n", "episode_reward", "=", "0.", "\n", "episode_steps", "=", "0", "\n", "done", "=", "False", "\n", "state", "=", "self", ".", "env", ".", "reset", "(", ")", "\n", "\n", "while", "not", "done", ":", "\n", "            ", "action", "=", "self", ".", "act", "(", "state", ")", "\n", "next_state", ",", "reward", ",", "done", ",", "_", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "self", ".", "steps", "+=", "1", "\n", "episode_steps", "+=", "1", "\n", "episode_reward", "+=", "reward", "\n", "\n", "# ignore done if the agent reach time horizons", "\n", "# (set done=True only when the agent fails)", "\n", "if", "episode_steps", ">=", "self", ".", "env", ".", "_max_episode_steps", ":", "\n", "                ", "masked_done", "=", "False", "\n", "", "else", ":", "\n", "                ", "masked_done", "=", "done", "\n", "\n", "", "if", "self", ".", "per", ":", "\n", "                ", "batch", "=", "to_batch", "(", "\n", "state", ",", "action", ",", "reward", ",", "next_state", ",", "masked_done", ",", "\n", "self", ".", "device", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                    ", "curr_q1", ",", "curr_q2", "=", "self", ".", "calc_current_q", "(", "*", "batch", ")", "\n", "", "target_q", "=", "self", ".", "calc_target_q", "(", "*", "batch", ")", "\n", "# fixed by tH20210715", "\n", "error", "=", "(", "0.5", "*", "torch", ".", "abs", "(", "curr_q1", "-", "target_q", ")", "+", "0.5", "*", "torch", ".", "abs", "(", "curr_q2", "-", "target_q", ")", ")", ".", "item", "(", ")", "\n", "# We need to give true done signal with addition to masked done", "\n", "# signal to calculate multi-step rewards.", "\n", "self", ".", "memory", ".", "append", "(", "state", ",", "action", ",", "reward", ",", "next_state", ",", "masked_done", ",", "error", ",", "episode_done", "=", "done", ")", "\n", "", "else", ":", "\n", "# We need to give true done signal with addition to masked done", "\n", "# signal to calculate multi-step rewards.", "\n", "                ", "self", ".", "memory", ".", "append", "(", "state", ",", "action", ",", "reward", ",", "next_state", ",", "masked_done", ",", "episode_done", "=", "done", ")", "\n", "\n", "", "if", "self", ".", "is_update", "(", ")", ":", "\n", "                ", "self", ".", "learn", "(", ")", "\n", "\n", "", "if", "self", ".", "steps", "%", "self", ".", "eval_interval", "==", "0", ":", "\n", "                ", "self", ".", "evaluate", "(", ")", "\n", "self", ".", "save_models", "(", ")", "\n", "\n", "", "state", "=", "next_state", "\n", "\n", "# We log running mean of training rewards.", "\n", "", "self", ".", "train_rewards", ".", "append", "(", "episode_reward", ")", "\n", "\n", "if", "self", ".", "episodes", "%", "self", ".", "log_interval", "==", "0", ":", "\n", "            ", "self", ".", "writer", ".", "add_scalar", "(", "'reward/train'", ",", "self", ".", "train_rewards", ".", "get", "(", ")", ",", "self", ".", "steps", ")", "\n", "\n", "", "print", "(", "f'episode: {self.episodes:<4}  '", "\n", "f'episode steps: {episode_steps:<4}  '", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.learn": [[243, 290], ["agent.SacAgent.calc_policy_loss", "utils.update_params", "range", "agent.SacAgent.memory.sample", "agent.SacAgent.memory.sample", "agent.SacAgent.calc_entropy_loss", "utils.update_params", "agent.SacAgent.log_alpha.exp", "agent.SacAgent.memory.sample", "agent.SacAgent.memory.sample", "agent.SacAgent.calc_critic_4redq_loss", "range", "agent.SacAgent.calc_critic_loss", "utils.update_params", "utils.update_params", "utils.soft_update", "agent.SacAgent.memory.update_priority", "utils.update_params", "errors.cpu().numpy", "getattr", "getattr", "errors.cpu", "str", "str"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.calc_policy_loss", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.update_params", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.TanhNormal.sample", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.TanhNormal.sample", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.calc_entropy_loss", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.update_params", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.TanhNormal.sample", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.TanhNormal.sample", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.calc_critic_4redq_loss", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.calc_critic_loss", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.update_params", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.update_params", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.soft_update", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.update_params"], ["", "def", "learn", "(", "self", ")", ":", "\n", "        ", "self", ".", "learning_steps", "+=", "1", "\n", "\n", "# critic update", "\n", "if", "(", "self", ".", "learning_steps", "-", "1", ")", "%", "self", ".", "critic_update_delay", "==", "0", ":", "\n", "            ", "for", "_", "in", "range", "(", "self", ".", "updates_per_step", ")", ":", "\n", "                ", "if", "self", ".", "per", ":", "\n", "# batch with indices and priority weights", "\n", "                    ", "batch", ",", "indices", ",", "weights", "=", "self", ".", "memory", ".", "sample", "(", "self", ".", "batch_size", ")", "\n", "", "else", ":", "\n", "                    ", "batch", "=", "self", ".", "memory", ".", "sample", "(", "self", ".", "batch_size", ")", "\n", "# set priority weights to 1 when we don't use PER.", "\n", "weights", "=", "1.", "\n", "\n", "", "if", "self", ".", "method", "==", "\"redq\"", ":", "\n", "                    ", "losses", ",", "errors", ",", "mean_q1", ",", "mean_q2", "=", "self", ".", "calc_critic_4redq_loss", "(", "batch", ",", "weights", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "critic", ".", "N", ")", ":", "\n", "                        ", "update_params", "(", "getattr", "(", "self", ",", "\"q\"", "+", "str", "(", "i", ")", "+", "\"_optim\"", ")", ",", "\n", "getattr", "(", "self", ".", "critic", ",", "\"Q\"", "+", "str", "(", "i", ")", ")", ",", "\n", "losses", "[", "i", "]", ",", "self", ".", "grad_clip", ")", "\n", "", "", "else", ":", "\n", "                    ", "q1_loss", ",", "q2_loss", ",", "errors", ",", "mean_q1", ",", "mean_q2", "=", "self", ".", "calc_critic_loss", "(", "batch", ",", "weights", ")", "\n", "\n", "update_params", "(", "self", ".", "q1_optim", ",", "self", ".", "critic", ".", "Q1", ",", "q1_loss", ",", "self", ".", "grad_clip", ")", "\n", "update_params", "(", "self", ".", "q2_optim", ",", "self", ".", "critic", ".", "Q2", ",", "q2_loss", ",", "self", ".", "grad_clip", ")", "\n", "\n", "", "if", "self", ".", "learning_steps", "%", "self", ".", "target_update_interval", "==", "0", ":", "\n", "                    ", "soft_update", "(", "self", ".", "critic_target", ",", "self", ".", "critic", ",", "self", ".", "tau", ")", "\n", "\n", "", "if", "self", ".", "per", ":", "\n", "# update priority weights", "\n", "                    ", "self", ".", "memory", ".", "update_priority", "(", "indices", ",", "errors", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "\n", "# policy and alpha update", "\n", "", "", "", "if", "self", ".", "per", ":", "\n", "            ", "batch", ",", "indices", ",", "weights", "=", "self", ".", "memory", ".", "sample", "(", "self", ".", "batch_size", ")", "\n", "", "else", ":", "\n", "            ", "batch", "=", "self", ".", "memory", ".", "sample", "(", "self", ".", "batch_size", ")", "\n", "weights", "=", "1.", "\n", "\n", "", "policy_loss", ",", "entropies", "=", "self", ".", "calc_policy_loss", "(", "batch", ",", "weights", ")", "# added by tH 20210705", "\n", "update_params", "(", "self", ".", "policy_optim", ",", "self", ".", "policy", ",", "policy_loss", ",", "self", ".", "grad_clip", ")", "\n", "\n", "if", "self", ".", "entropy_tuning", ":", "\n", "            ", "entropy_loss", "=", "self", ".", "calc_entropy_loss", "(", "entropies", ",", "weights", ")", "\n", "update_params", "(", "self", ".", "alpha_optim", ",", "None", ",", "entropy_loss", ")", "\n", "self", ".", "alpha", "=", "self", ".", "log_alpha", ".", "exp", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.calc_critic_4redq_loss": [[291, 308], ["agent.SacAgent.critic.allQs", "agent.SacAgent.calc_target_q", "torch.abs", "curr_qs[].detach().mean().item", "curr_qs[].detach().mean().item", "losses.append", "curr_qs[].detach", "curr_qs[].detach().mean", "curr_qs[].detach().mean", "torch.mean", "curr_qs[].detach", "curr_qs[].detach"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.model.RandomizedEnsembleNetwork.allQs", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.calc_target_q", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.RunningMeanStats.append"], ["", "", "def", "calc_critic_4redq_loss", "(", "self", ",", "batch", ",", "weights", ")", ":", "\n", "        ", "states", ",", "actions", ",", "rewards", ",", "next_states", ",", "dones", "=", "batch", "\n", "curr_qs", "=", "self", ".", "critic", ".", "allQs", "(", "states", ",", "actions", ")", "\n", "\n", "target_q", "=", "self", ".", "calc_target_q", "(", "*", "batch", ")", "\n", "\n", "# TD errors for updating priority weights", "\n", "errors", "=", "torch", ".", "abs", "(", "curr_qs", "[", "0", "]", ".", "detach", "(", ")", "-", "target_q", ")", "# TODO better to use average of all errors?", "\n", "# We log means of Q to monitor training.", "\n", "mean_q1", "=", "curr_qs", "[", "0", "]", ".", "detach", "(", ")", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "mean_q2", "=", "curr_qs", "[", "1", "]", ".", "detach", "(", ")", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "\n", "# Critic loss is mean squared TD errors with priority weights.", "\n", "losses", "=", "[", "]", "\n", "for", "curr_q", "in", "curr_qs", ":", "\n", "            ", "losses", ".", "append", "(", "torch", ".", "mean", "(", "(", "curr_q", "-", "target_q", ")", ".", "pow", "(", "2", ")", "*", "weights", ")", ")", "\n", "", "return", "losses", ",", "errors", ",", "mean_q1", ",", "mean_q2", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.calc_critic_loss": [[309, 324], ["agent.SacAgent.calc_current_q", "agent.SacAgent.calc_target_q", "torch.abs", "curr_q1.detach().mean().item", "curr_q2.detach().mean().item", "torch.mean", "torch.mean", "curr_q1.detach", "curr_q1.detach().mean", "curr_q2.detach().mean", "curr_q1.detach", "curr_q2.detach"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.calc_current_q", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.calc_target_q"], ["", "def", "calc_critic_loss", "(", "self", ",", "batch", ",", "weights", ")", ":", "\n", "        ", "assert", "self", ".", "method", "==", "\"sac\"", "or", "self", ".", "method", "==", "\"duvn\"", "or", "self", ".", "method", "==", "\"monosac\"", ",", "\"This method is only for sac or duvn or monosac method\"", "\n", "\n", "curr_q1", ",", "curr_q2", "=", "self", ".", "calc_current_q", "(", "*", "batch", ")", "\n", "target_q", "=", "self", ".", "calc_target_q", "(", "*", "batch", ")", "\n", "\n", "# TD errors for updating priority weights", "\n", "errors", "=", "torch", ".", "abs", "(", "curr_q1", ".", "detach", "(", ")", "-", "target_q", ")", "\n", "# We log means of Q to monitor training.", "\n", "mean_q1", "=", "curr_q1", ".", "detach", "(", ")", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "mean_q2", "=", "curr_q2", ".", "detach", "(", ")", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "\n", "q1_loss", "=", "torch", ".", "mean", "(", "(", "curr_q1", "-", "target_q", ")", ".", "pow", "(", "2", ")", "*", "weights", ")", "\n", "q2_loss", "=", "torch", ".", "mean", "(", "(", "curr_q2", "-", "target_q", ")", ".", "pow", "(", "2", ")", "*", "weights", ")", "\n", "return", "q1_loss", ",", "q2_loss", ",", "errors", ",", "mean_q1", ",", "mean_q2", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.calc_policy_loss": [[325, 346], ["agent.SacAgent.policy.sample", "torch.mean", "agent.SacAgent.critic.averageQ", "agent.SacAgent.critic", "torch.min"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.TanhNormal.sample", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.model.RandomizedEnsembleNetwork.averageQ"], ["", "def", "calc_policy_loss", "(", "self", ",", "batch", ",", "weights", ")", ":", "\n", "        ", "states", ",", "actions", ",", "rewards", ",", "next_states", ",", "dones", "=", "batch", "\n", "\n", "# We re-sample actions to calculate expectations of Q.", "\n", "sampled_action", ",", "entropy", ",", "_", "=", "self", ".", "policy", ".", "sample", "(", "states", ")", "\n", "# expectations of Q with clipped double Q technique", "\n", "if", "self", ".", "method", "==", "\"redq\"", ":", "\n", "            ", "q", "=", "self", ".", "critic", ".", "averageQ", "(", "states", ",", "sampled_action", ")", "\n", "", "else", ":", "\n", "            ", "q1", ",", "q2", "=", "self", ".", "critic", "(", "states", ",", "sampled_action", ")", "\n", "if", "(", "self", ".", "method", "==", "\"duvn\"", ")", "or", "(", "self", ".", "method", "==", "\"monosac\"", ")", ":", "\n", "                ", "q2", "=", "q1", "# discard q2", "\n", "", "if", "self", ".", "target_drop_rate", ">", "0.0", ":", "\n", "                ", "q", "=", "0.5", "*", "(", "q1", "+", "q2", ")", "\n", "", "else", ":", "\n", "                ", "q", "=", "torch", ".", "min", "(", "q1", ",", "q2", ")", "\n", "# Policy objective is maximization of (Q + alpha * entropy) with", "\n", "# priority weights.", "\n", "", "", "policy_loss", "=", "torch", ".", "mean", "(", "(", "-", "q", "-", "self", ".", "alpha", "*", "entropy", ")", "*", "weights", ")", "\n", "\n", "return", "policy_loss", ",", "entropy", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.calc_entropy_loss": [[347, 350], ["torch.mean"], "methods", ["None"], ["", "def", "calc_entropy_loss", "(", "self", ",", "entropy", ",", "weights", ")", ":", "\n", "        ", "entropy_loss", "=", "-", "torch", ".", "mean", "(", "self", ".", "log_alpha", "*", "(", "self", ".", "target_entropy", "-", "entropy", ")", ".", "detach", "(", ")", "*", "weights", ")", "\n", "return", "entropy_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.evaluate": [[351, 430], ["numpy.zeros", "range", "numpy.mean", "range", "numpy.mean", "range", "list", "numpy.mean", "numpy.std", "print", "print", "agent.SacAgent.writer.add_scalar", "print", "print", "print", "agent.SacAgent.env.reset", "collections.deque", "reversed", "list", "math.fabs", "[].tolist", "[].tolist", "range", "itertools.chain.from_iterable", "open", "f.write", "f.flush", "open", "f.write", "f.flush", "open", "f.write", "f.flush", "range", "agent.SacAgent.exploit", "agent.SacAgent.env.step", "sar_buf[].append", "range", "mc_discounted_return[].appendleft", "itertools.chain.from_iterable", "range", "torch.no_grad", "torch.FloatTensor().to", "torch.FloatTensor().to", "agent.SacAgent.to().numpy", "len", "norm_scores[].append", "str", "str", "len", "agent.SacAgent.critic.averageQ", "agent.SacAgent.critic", "numpy.array", "numpy.array", "torch.FloatTensor", "torch.FloatTensor", "agent.SacAgent.to", "str", "str", "str", "str", "str", "str"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.util.utilsTH.SparseRewardEnv.reset", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.exploit", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.customenvs.humanoid.HumanoidTruncatedObsEnv.step", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.RunningMeanStats.append", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.RunningMeanStats.append", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.model.RandomizedEnsembleNetwork.averageQ"], ["", "def", "evaluate", "(", "self", ")", ":", "\n", "        ", "episodes", "=", "self", ".", "eval_runs", "\n", "returns", "=", "np", ".", "zeros", "(", "(", "episodes", ",", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "# for return bias estimation TH", "\n", "sar_buf", "=", "[", "[", "]", "for", "_", "in", "range", "(", "episodes", ")", "]", "# episodes x (satte, action , reward)", "\n", "\n", "for", "i", "in", "range", "(", "episodes", ")", ":", "\n", "            ", "state", "=", "self", ".", "env", ".", "reset", "(", ")", "\n", "episode_reward", "=", "0.", "\n", "done", "=", "False", "\n", "while", "not", "done", ":", "\n", "                ", "action", "=", "self", ".", "exploit", "(", "state", ")", "\n", "next_state", ",", "reward", ",", "done", ",", "_", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "episode_reward", "+=", "reward", "\n", "state", "=", "next_state", "\n", "# MCE store all (state, action, reward) TH 20210723", "\n", "sar_buf", "[", "i", "]", ".", "append", "(", "[", "state", ",", "action", ",", "reward", "]", ")", "\n", "\n", "", "returns", "[", "i", "]", "=", "episode_reward", "\n", "\n", "", "mean_return", "=", "np", ".", "mean", "(", "returns", ")", "\n", "\n", "# calculate mean / std return bias. TH 20210801", "\n", "# - calculate MCE future discounted return (in backward)", "\n", "mc_discounted_return", "=", "[", "deque", "(", ")", "for", "_", "in", "range", "(", "episodes", ")", "]", "\n", "for", "i", "in", "range", "(", "episodes", ")", ":", "\n", "            ", "for", "re_tran", "in", "reversed", "(", "sar_buf", "[", "i", "]", ")", ":", "\n", "                ", "if", "len", "(", "mc_discounted_return", "[", "i", "]", ")", ">", "0", ":", "\n", "                    ", "mcret", "=", "re_tran", "[", "2", "]", "+", "self", ".", "gamma_n", "*", "mc_discounted_return", "[", "i", "]", "[", "0", "]", "\n", "", "else", ":", "\n", "                    ", "mcret", "=", "re_tran", "[", "2", "]", "\n", "", "mc_discounted_return", "[", "i", "]", ".", "appendleft", "(", "mcret", ")", "\n", "# - calculate normalized MCE return by averaging all MCE returns", "\n", "", "", "norm_coef", "=", "np", ".", "mean", "(", "list", "(", "itertools", ".", "chain", ".", "from_iterable", "(", "mc_discounted_return", ")", ")", ")", "\n", "norm_coef", "=", "math", ".", "fabs", "(", "norm_coef", ")", "+", "0.000001", "\n", "# - estimate return for all state action, and normalized score", "\n", "norm_scores", "=", "[", "[", "]", "for", "_", "in", "range", "(", "episodes", ")", "]", "\n", "for", "i", "in", "range", "(", "episodes", ")", ":", "\n", "# calculate normalized score", "\n", "            ", "states", "=", "np", ".", "array", "(", "sar_buf", "[", "i", "]", ",", "dtype", "=", "\"object\"", ")", "[", ":", ",", "0", "]", ".", "tolist", "(", ")", "\n", "actions", "=", "np", ".", "array", "(", "sar_buf", "[", "i", "]", ",", "dtype", "=", "\"object\"", ")", "[", ":", ",", "1", "]", ".", "tolist", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "state", "=", "torch", ".", "FloatTensor", "(", "states", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "action", "=", "torch", ".", "FloatTensor", "(", "actions", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "if", "self", ".", "method", "==", "\"redq\"", ":", "\n", "                    ", "q", "=", "self", ".", "critic", ".", "averageQ", "(", "state", ",", "action", ")", "\n", "", "else", ":", "\n", "                    ", "q1", ",", "q2", "=", "self", ".", "critic", "(", "state", ",", "action", ")", "\n", "q", "=", "0.5", "*", "(", "q1", "+", "q2", ")", "\n", "", "qs", "=", "q", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "", "for", "j", "in", "range", "(", "len", "(", "sar_buf", "[", "i", "]", ")", ")", ":", "\n", "                ", "score", "=", "(", "qs", "[", "j", "]", "[", "0", "]", "-", "mc_discounted_return", "[", "i", "]", "[", "j", "]", ")", "/", "norm_coef", "\n", "norm_scores", "[", "i", "]", ".", "append", "(", "score", ")", "\n", "# calculate std", "\n", "", "", "flatten_norm_score", "=", "list", "(", "itertools", ".", "chain", ".", "from_iterable", "(", "norm_scores", ")", ")", "\n", "mean_norm_score", "=", "np", ".", "mean", "(", "flatten_norm_score", ")", "\n", "std_norm_score", "=", "np", ".", "std", "(", "flatten_norm_score", ")", "\n", "print", "(", "\"mean norm score \"", "+", "str", "(", "mean_norm_score", ")", ")", "\n", "print", "(", "\"std norm score \"", "+", "str", "(", "std_norm_score", ")", ")", "\n", "\n", "self", ".", "writer", ".", "add_scalar", "(", "\n", "'reward/test'", ",", "mean_return", ",", "self", ".", "steps", ")", "\n", "print", "(", "'-'", "*", "60", ")", "\n", "print", "(", "f'Num steps: {self.steps:<5}  '", "\n", "f'reward: {mean_return:<5.1f}'", ")", "\n", "print", "(", "'-'", "*", "60", ")", "\n", "#", "\n", "with", "open", "(", "self", ".", "log_dir", "+", "\"/\"", "+", "\"reward.csv\"", ",", "\"a\"", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "str", "(", "self", ".", "steps", ")", "+", "\",\"", "+", "str", "(", "mean_return", ")", "+", "\",\\n\"", ")", "\n", "f", ".", "flush", "(", ")", "\n", "#", "\n", "", "with", "open", "(", "self", ".", "log_dir", "+", "\"/\"", "+", "\"avrbias.csv\"", ",", "\"a\"", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "str", "(", "self", ".", "steps", ")", "+", "\",\"", "+", "str", "(", "mean_norm_score", ")", "+", "\",\\n\"", ")", "\n", "f", ".", "flush", "(", ")", "\n", "#", "\n", "", "with", "open", "(", "self", ".", "log_dir", "+", "\"/\"", "+", "\"stdbias.csv\"", ",", "\"a\"", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "str", "(", "self", ".", "steps", ")", "+", "\",\"", "+", "str", "(", "std_norm_score", ")", "+", "\",\\n\"", ")", "\n", "f", ".", "flush", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.save_models": [[432, 437], ["agent.SacAgent.policy.save", "agent.SacAgent.critic.save", "agent.SacAgent.critic_target.save", "os.path.join", "os.path.join", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.model.BaseNetwork.save", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.model.BaseNetwork.save", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.model.BaseNetwork.save"], ["", "", "def", "save_models", "(", "self", ")", ":", "\n", "        ", "self", ".", "policy", ".", "save", "(", "os", ".", "path", ".", "join", "(", "self", ".", "model_dir", ",", "'policy.pth'", ")", ")", "\n", "self", ".", "critic", ".", "save", "(", "os", ".", "path", ".", "join", "(", "self", ".", "model_dir", ",", "'critic.pth'", ")", ")", "\n", "self", ".", "critic_target", ".", "save", "(", "\n", "os", ".", "path", ".", "join", "(", "self", ".", "model_dir", ",", "'critic_target.pth'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.__del__": [[438, 441], ["agent.SacAgent.writer.close", "agent.SacAgent.env.close"], "methods", ["None"], ["", "def", "__del__", "(", "self", ")", ":", "\n", "        ", "self", ".", "writer", ".", "close", "(", ")", "\n", "self", ".", "env", ".", "close", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.model.BaseNetwork.save": [[9, 11], ["torch.save", "torch.save", "torch.save", "torch.save", "model.BaseNetwork.state_dict"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.model.BaseNetwork.save", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.model.BaseNetwork.save", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.model.BaseNetwork.save", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.model.BaseNetwork.save"], ["    ", "def", "save", "(", "self", ",", "path", ")", ":", "\n", "        ", "torch", ".", "save", "(", "self", ".", "state_dict", "(", ")", ",", "path", ")", "\n", "", "def", "load", "(", "self", ",", "path", ")", ":", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.model.BaseNetwork.load": [[11, 13], ["model.BaseNetwork.load_state_dict", "torch.load", "torch.load", "torch.load", "torch.load"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.model.BaseNetwork.load", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.model.BaseNetwork.load", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.model.BaseNetwork.load", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.model.BaseNetwork.load"], ["", "def", "load", "(", "self", ",", "path", ")", ":", "\n", "        ", "self", ".", "load_state_dict", "(", "torch", ".", "load", "(", "path", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.model.QNetwork.__init__": [[15, 35], ["torch.Module.__init__", "rltorch.network.create_linear_network", "enumerate", "torch.Sequential", "torch.Sequential", "model.QNetwork.Q._modules.values", "new_q_networks.append", "new_q_networks.append", "new_q_networks.append", "len", "torch.Dropout", "torch.Dropout", "torch.LayerNorm", "torch.LayerNorm", "list", "model.QNetwork.Q._modules.values"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.EpochLogger.__init__", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.RunningMeanStats.append", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.RunningMeanStats.append", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.RunningMeanStats.append"], ["    ", "def", "__init__", "(", "self", ",", "num_inputs", ",", "num_actions", ",", "hidden_units", "=", "[", "256", ",", "256", "]", ",", "\n", "initializer", "=", "'xavier'", ",", "layer_norm", "=", "0", ",", "drop_rate", "=", "0.0", ")", ":", "\n", "        ", "super", "(", "QNetwork", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# https://github.com/ku2482/rltorch/blob/master/rltorch/network/builder.py", "\n", "self", ".", "Q", "=", "create_linear_network", "(", "\n", "num_inputs", "+", "num_actions", ",", "1", ",", "hidden_units", "=", "hidden_units", ",", "\n", "initializer", "=", "initializer", ")", "\n", "\n", "# override network architecture (dropout and layer normalization). TH 20210731", "\n", "new_q_networks", "=", "[", "]", "\n", "for", "i", ",", "mod", "in", "enumerate", "(", "self", ".", "Q", ".", "_modules", ".", "values", "(", ")", ")", ":", "\n", "            ", "new_q_networks", ".", "append", "(", "mod", ")", "\n", "if", "(", "(", "i", "%", "2", ")", "==", "0", ")", "and", "(", "i", "<", "(", "len", "(", "list", "(", "self", ".", "Q", ".", "_modules", ".", "values", "(", ")", ")", ")", ")", "-", "1", ")", ":", "\n", "                ", "if", "drop_rate", ">", "0.0", ":", "\n", "                    ", "new_q_networks", ".", "append", "(", "nn", ".", "Dropout", "(", "p", "=", "drop_rate", ")", ")", "# dropout", "\n", "", "if", "layer_norm", ":", "\n", "                    ", "new_q_networks", ".", "append", "(", "nn", ".", "LayerNorm", "(", "mod", ".", "out_features", ")", ")", "# layer norm", "\n", "", "", "i", "+=", "1", "\n", "", "self", ".", "Q", "=", "nn", ".", "Sequential", "(", "*", "new_q_networks", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.model.QNetwork.forward": [[36, 39], ["model.QNetwork.Q"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "q", "=", "self", ".", "Q", "(", "x", ")", "\n", "return", "q", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.model.TwinnedQNetwork.__init__": [[42, 50], ["torch.Module.__init__", "model.QNetwork", "model.QNetwork"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.EpochLogger.__init__"], ["    ", "def", "__init__", "(", "self", ",", "num_inputs", ",", "num_actions", ",", "hidden_units", "=", "[", "256", ",", "256", "]", ",", "\n", "initializer", "=", "'xavier'", ",", "layer_norm", "=", "0", ",", "drop_rate", "=", "0.0", ")", ":", "\n", "        ", "super", "(", "TwinnedQNetwork", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "Q1", "=", "QNetwork", "(", "\n", "num_inputs", ",", "num_actions", ",", "hidden_units", ",", "initializer", ",", "layer_norm", "=", "layer_norm", ",", "drop_rate", "=", "drop_rate", ")", "\n", "self", ".", "Q2", "=", "QNetwork", "(", "\n", "num_inputs", ",", "num_actions", ",", "hidden_units", ",", "initializer", ",", "layer_norm", "=", "layer_norm", ",", "drop_rate", "=", "drop_rate", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.model.TwinnedQNetwork.forward": [[51, 56], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "model.TwinnedQNetwork.Q1", "model.TwinnedQNetwork.Q2"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "states", ",", "actions", ")", ":", "\n", "        ", "x", "=", "torch", ".", "cat", "(", "[", "states", ",", "actions", "]", ",", "dim", "=", "1", ")", "\n", "q1", "=", "self", ".", "Q1", "(", "x", ")", "\n", "q2", "=", "self", ".", "Q2", "(", "x", ")", "\n", "return", "q1", ",", "q2", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.model.RandomizedEnsembleNetwork.__init__": [[60, 70], ["torch.Module.__init__", "list", "range", "range", "setattr", "model.QNetwork", "str"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.EpochLogger.__init__"], ["    ", "def", "__init__", "(", "self", ",", "num_inputs", ",", "num_actions", ",", "hidden_units", "=", "[", "256", ",", "256", "]", ",", "\n", "initializer", "=", "'xavier'", ",", "layer_norm", "=", "0", ",", "drop_rate", "=", "0.0", ",", "N", "=", "10", ")", ":", "\n", "        ", "super", "(", "RandomizedEnsembleNetwork", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "N", "=", "N", "\n", "self", ".", "indices", "=", "list", "(", "range", "(", "N", ")", ")", "\n", "for", "i", "in", "range", "(", "N", ")", ":", "\n", "            ", "setattr", "(", "self", ",", "\"Q\"", "+", "str", "(", "i", ")", ",", "\n", "QNetwork", "(", "num_inputs", ",", "num_actions", ",", "hidden_units", ",", "initializer", ",", "\n", "layer_norm", "=", "layer_norm", ",", "drop_rate", "=", "drop_rate", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.model.RandomizedEnsembleNetwork.forward": [[71, 79], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "random.shuffle", "getattr", "getattr", "str", "str"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "states", ",", "actions", ")", ":", "\n", "        ", "x", "=", "torch", ".", "cat", "(", "[", "states", ",", "actions", "]", ",", "dim", "=", "1", ")", "\n", "\n", "random", ".", "shuffle", "(", "self", ".", "indices", ")", "\n", "q1", "=", "getattr", "(", "self", ",", "\"Q\"", "+", "str", "(", "self", ".", "indices", "[", "0", "]", ")", ")", "(", "x", ")", "\n", "q2", "=", "getattr", "(", "self", ",", "\"Q\"", "+", "str", "(", "self", ".", "indices", "[", "1", "]", ")", ")", "(", "x", ")", "\n", "\n", "return", "q1", ",", "q2", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.model.RandomizedEnsembleNetwork.allQs": [[80, 87], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "range", "Qs.append", "getattr", "str"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.RunningMeanStats.append"], ["", "def", "allQs", "(", "self", ",", "states", ",", "actions", ")", ":", "\n", "        ", "x", "=", "torch", ".", "cat", "(", "[", "states", ",", "actions", "]", ",", "dim", "=", "1", ")", "\n", "\n", "Qs", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "N", ")", ":", "\n", "            ", "Qs", ".", "append", "(", "getattr", "(", "self", ",", "\"Q\"", "+", "str", "(", "i", ")", ")", "(", "x", ")", ")", "\n", "", "return", "Qs", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.model.RandomizedEnsembleNetwork.averageQ": [[88, 97], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "range", "getattr", "str", "getattr", "str"], "methods", ["None"], ["", "def", "averageQ", "(", "self", ",", "states", ",", "actions", ")", ":", "\n", "        ", "x", "=", "torch", ".", "cat", "(", "[", "states", ",", "actions", "]", ",", "dim", "=", "1", ")", "\n", "\n", "q", "=", "getattr", "(", "self", ",", "\"Q\"", "+", "str", "(", "0", ")", ")", "(", "x", ")", "\n", "for", "i", "in", "range", "(", "1", ",", "self", ".", "N", ")", ":", "\n", "            ", "q", "=", "q", "+", "getattr", "(", "self", ",", "\"Q\"", "+", "str", "(", "i", ")", ")", "(", "x", ")", "\n", "", "q", "=", "q", "/", "self", ".", "N", "\n", "\n", "return", "q", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.model.GaussianPolicy.__init__": [[103, 111], ["torch.Module.__init__", "rltorch.network.create_linear_network"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.EpochLogger.__init__"], ["def", "__init__", "(", "self", ",", "num_inputs", ",", "num_actions", ",", "hidden_units", "=", "[", "256", ",", "256", "]", ",", "\n", "initializer", "=", "'xavier'", ")", ":", "\n", "        ", "super", "(", "GaussianPolicy", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# https://github.com/ku2482/rltorch/blob/master/rltorch/network/builder.py", "\n", "self", ".", "policy", "=", "create_linear_network", "(", "\n", "num_inputs", ",", "num_actions", "*", "2", ",", "hidden_units", "=", "hidden_units", ",", "\n", "initializer", "=", "initializer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.model.GaussianPolicy.forward": [[112, 118], ["torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "model.GaussianPolicy.policy"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "states", ")", ":", "\n", "        ", "mean", ",", "log_std", "=", "torch", ".", "chunk", "(", "self", ".", "policy", "(", "states", ")", ",", "2", ",", "dim", "=", "-", "1", ")", "\n", "log_std", "=", "torch", ".", "clamp", "(", "\n", "log_std", ",", "min", "=", "self", ".", "LOG_STD_MIN", ",", "max", "=", "self", ".", "LOG_STD_MAX", ")", "\n", "\n", "return", "mean", ",", "log_std", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.model.GaussianPolicy.sample": [[119, 133], ["model.GaussianPolicy.forward", "log_stds.exp", "torch.distributions.Normal", "torch.distributions.Normal", "torch.distributions.Normal.rsample", "torch.distributions.Normal.rsample", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.distributions.Normal.log_prob", "torch.distributions.Normal.log_prob", "torch.log", "torch.log", "torch.log", "torch.log", "log_probs.sum", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh.pow", "torch.tanh.pow"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.TanhGaussianPolicy.forward", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.TanhNormal.rsample", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.TanhNormal.rsample", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.TanhNormal.log_prob", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.TanhNormal.log_prob", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.Logger.log", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.Logger.log", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.Logger.log", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.Logger.log"], ["", "def", "sample", "(", "self", ",", "states", ")", ":", "\n", "# calculate Gaussian distribusion of (mean, std)", "\n", "        ", "means", ",", "log_stds", "=", "self", ".", "forward", "(", "states", ")", "\n", "stds", "=", "log_stds", ".", "exp", "(", ")", "\n", "normals", "=", "Normal", "(", "means", ",", "stds", ")", "\n", "# sample actions", "\n", "xs", "=", "normals", ".", "rsample", "(", ")", "\n", "actions", "=", "torch", ".", "tanh", "(", "xs", ")", "\n", "# calculate entropies", "\n", "log_probs", "=", "normals", ".", "log_prob", "(", "xs", ")", "-", "torch", ".", "log", "(", "1", "-", "actions", ".", "pow", "(", "2", ")", "+", "self", ".", "eps", ")", "\n", "entropies", "=", "-", "log_probs", ".", "sum", "(", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "\n", "\n", "return", "actions", ",", "entropies", ",", "torch", ".", "tanh", "(", "means", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.RunningMeanStats.__init__": [[40, 43], ["collections.deque"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "n", "=", "10", ")", ":", "\n", "        ", "self", ".", "n", "=", "n", "\n", "self", ".", "stats", "=", "deque", "(", "maxlen", "=", "n", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.RunningMeanStats.append": [[44, 46], ["utils.RunningMeanStats.stats.append"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.RunningMeanStats.append"], ["", "def", "append", "(", "self", ",", "x", ")", ":", "\n", "        ", "self", ".", "stats", ".", "append", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.RunningMeanStats.get": [[47, 49], ["numpy.mean"], "methods", ["None"], ["", "def", "get", "(", "self", ")", ":", "\n", "        ", "return", "np", ".", "mean", "(", "self", ".", "stats", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.to_batch": [[6, 13], ["torch.FloatTensor().unsqueeze().to", "torch.FloatTensor().view().to", "torch.FloatTensor().unsqueeze().to", "torch.FloatTensor().unsqueeze().to", "torch.FloatTensor().unsqueeze().to", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().view", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor"], "function", ["None"], ["def", "to_batch", "(", "state", ",", "action", ",", "reward", ",", "next_state", ",", "done", ",", "device", ")", ":", "\n", "    ", "state", "=", "torch", ".", "FloatTensor", "(", "state", ")", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "device", ")", "\n", "action", "=", "torch", ".", "FloatTensor", "(", "[", "action", "]", ")", ".", "view", "(", "1", ",", "-", "1", ")", ".", "to", "(", "device", ")", "\n", "reward", "=", "torch", ".", "FloatTensor", "(", "[", "reward", "]", ")", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "device", ")", "\n", "next_state", "=", "torch", ".", "FloatTensor", "(", "next_state", ")", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "device", ")", "\n", "done", "=", "torch", ".", "FloatTensor", "(", "[", "done", "]", ")", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "device", ")", "\n", "return", "state", ",", "action", ",", "reward", ",", "next_state", ",", "done", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.update_params": [[15, 22], ["optim.zero_grad", "loss.backward", "optim.step", "network.modules", "torch.nn.utils.clip_grad_norm_", "p.parameters"], "function", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.customenvs.humanoid.HumanoidTruncatedObsEnv.step"], ["", "def", "update_params", "(", "optim", ",", "network", ",", "loss", ",", "grad_clip", "=", "None", ",", "retain_graph", "=", "False", ")", ":", "\n", "    ", "optim", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", "retain_graph", "=", "retain_graph", ")", "\n", "if", "grad_clip", "is", "not", "None", ":", "\n", "        ", "for", "p", "in", "network", ".", "modules", "(", ")", ":", "\n", "            ", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "p", ".", "parameters", "(", ")", ",", "grad_clip", ")", "\n", "", "", "optim", ".", "step", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.soft_update": [[24, 27], ["zip", "target.parameters", "source.parameters", "t.data.copy_"], "function", ["None"], ["", "def", "soft_update", "(", "target", ",", "source", ",", "tau", ")", ":", "\n", "    ", "for", "t", ",", "s", "in", "zip", "(", "target", ".", "parameters", "(", ")", ",", "source", ".", "parameters", "(", ")", ")", ":", "\n", "        ", "t", ".", "data", ".", "copy_", "(", "t", ".", "data", "*", "(", "1.0", "-", "tau", ")", "+", "s", ".", "data", "*", "tau", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.hard_update": [[29, 31], ["target.load_state_dict", "source.state_dict"], "function", ["None"], ["", "", "def", "hard_update", "(", "target", ",", "source", ")", ":", "\n", "    ", "target", ".", "load_state_dict", "(", "source", ".", "state_dict", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.grad_false": [[33, 36], ["network.parameters"], "function", ["None"], ["", "def", "grad_false", "(", "network", ")", ":", "\n", "    ", "for", "param", "in", "network", ".", "parameters", "(", ")", ":", "\n", "        ", "param", ".", "requires_grad", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.play.run": [[12, 49], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "os.path.join", "gym.make", "torch.device", "model.GaussianPolicy().to", "GaussianPolicy().to.load", "utils.grad_false", "gym.make.reset", "os.path.join", "torch.FloatTensor().unsqueeze().to", "exploit.cpu().numpy().reshape", "gym.make.render", "play.run.exploit"], "function", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.model.BaseNetwork.load", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.grad_false", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.util.utilsTH.SparseRewardEnv.reset", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.util.utilsTH.ProxyEnv.render", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.agent.SacAgent.exploit"], ["def", "run", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'--env_id'", ",", "type", "=", "str", ",", "default", "=", "'HalfCheetah-v2'", ")", "\n", "parser", ".", "add_argument", "(", "'--log_name'", ",", "type", "=", "str", ",", "default", "=", "'sac-seed0-datetime'", ")", "\n", "parser", ".", "add_argument", "(", "'--cuda'", ",", "action", "=", "'store_true'", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "type", "=", "int", ",", "default", "=", "0", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "log_dir", "=", "os", ".", "path", ".", "join", "(", "'logs'", ",", "args", ".", "env_id", ",", "args", ".", "log_name", ")", "\n", "\n", "env", "=", "gym", ".", "make", "(", "args", ".", "env_id", ")", "\n", "device", "=", "torch", ".", "device", "(", "\n", "\"cuda\"", "if", "args", ".", "cuda", "and", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "\n", "policy", "=", "GaussianPolicy", "(", "\n", "env", ".", "observation_space", ".", "shape", "[", "0", "]", ",", "\n", "env", ".", "action_space", ".", "shape", "[", "0", "]", ",", "\n", "hidden_units", "=", "[", "256", ",", "256", "]", ")", ".", "to", "(", "device", ")", "\n", "\n", "policy", ".", "load", "(", "os", ".", "path", ".", "join", "(", "log_dir", ",", "'model'", ",", "'policy.pth'", ")", ")", "\n", "grad_false", "(", "policy", ")", "\n", "\n", "def", "exploit", "(", "state", ")", ":", "\n", "        ", "state", "=", "torch", ".", "FloatTensor", "(", "state", ")", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "device", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "_", ",", "_", ",", "action", "=", "policy", ".", "sample", "(", "state", ")", "\n", "", "return", "action", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "reshape", "(", "-", "1", ")", "\n", "\n", "", "state", "=", "env", ".", "reset", "(", ")", "\n", "episode_reward", "=", "0.", "\n", "done", "=", "False", "\n", "while", "not", "done", ":", "\n", "        ", "env", ".", "render", "(", ")", "\n", "action", "=", "exploit", "(", "state", ")", "\n", "next_state", ",", "reward", ",", "done", ",", "_", "=", "env", ".", "step", "(", "action", ")", "\n", "episode_reward", "+=", "reward", "\n", "state", "=", "next_state", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.util.utilsTH.ProxyEnv.__init__": [[13, 15], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "wrapped_env", ")", ":", "\n", "        ", "self", ".", "_wrapped_env", "=", "wrapped_env", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.util.utilsTH.ProxyEnv.spec": [[16, 19], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "spec", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_wrapped_env", ".", "spec", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.util.utilsTH.ProxyEnv.wrapped_env": [[20, 23], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "wrapped_env", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_wrapped_env", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.util.utilsTH.ProxyEnv.reset": [[24, 26], ["utilsTH.ProxyEnv._wrapped_env.reset"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.util.utilsTH.SparseRewardEnv.reset"], ["", "def", "reset", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "self", ".", "_wrapped_env", ".", "reset", "(", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.util.utilsTH.ProxyEnv.action_space": [[27, 30], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "action_space", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_wrapped_env", ".", "action_space", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.util.utilsTH.ProxyEnv.observation_space": [[31, 34], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "observation_space", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_wrapped_env", ".", "observation_space", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.util.utilsTH.ProxyEnv.step": [[35, 37], ["utilsTH.ProxyEnv._wrapped_env.step"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.customenvs.humanoid.HumanoidTruncatedObsEnv.step"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "return", "self", ".", "_wrapped_env", ".", "step", "(", "action", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.util.utilsTH.ProxyEnv.render": [[38, 40], ["utilsTH.ProxyEnv._wrapped_env.render"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.util.utilsTH.ProxyEnv.render"], ["", "def", "render", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "self", ".", "_wrapped_env", ".", "render", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.util.utilsTH.ProxyEnv.log_diagnostics": [[41, 43], ["utilsTH.ProxyEnv._wrapped_env.log_diagnostics"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.util.utilsTH.ProxyEnv.log_diagnostics"], ["", "def", "log_diagnostics", "(", "self", ",", "paths", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "_wrapped_env", ".", "log_diagnostics", "(", "paths", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.util.utilsTH.ProxyEnv.horizon": [[44, 47], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "horizon", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_wrapped_env", ".", "horizon", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.util.utilsTH.ProxyEnv.terminate": [[48, 50], ["utilsTH.ProxyEnv._wrapped_env.terminate"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.util.utilsTH.ProxyEnv.terminate"], ["", "def", "terminate", "(", "self", ")", ":", "\n", "        ", "self", ".", "_wrapped_env", ".", "terminate", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.util.utilsTH.ProxyEnv.get_param_values": [[51, 53], ["utilsTH.ProxyEnv._wrapped_env.get_param_values"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.util.utilsTH.ProxyEnv.get_param_values"], ["", "def", "get_param_values", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_wrapped_env", ".", "get_param_values", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.util.utilsTH.ProxyEnv.set_param_values": [[54, 56], ["utilsTH.ProxyEnv._wrapped_env.set_param_values"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.util.utilsTH.ProxyEnv.set_param_values"], ["", "def", "set_param_values", "(", "self", ",", "params", ")", ":", "\n", "        ", "self", ".", "_wrapped_env", ".", "set_param_values", "(", "params", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.util.utilsTH.SparseRewardEnv.__init__": [[68, 72], ["utilsTH.ProxyEnv.__init__"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.EpochLogger.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", ",", "rew_thresh", ")", ":", "\n", "        ", "self", ".", "rew_thresh", "=", "rew_thresh", "\n", "self", ".", "initial_x", "=", "None", "# 0.0", "\n", "ProxyEnv", ".", "__init__", "(", "self", ",", "env", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.util.utilsTH.SparseRewardEnv.reset": [[74, 78], ["utilsTH.SparseRewardEnv.wrapped_env.reset"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.util.utilsTH.SparseRewardEnv.reset"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "obs", "=", "self", ".", "wrapped_env", ".", "reset", "(", ")", "\n", "self", ".", "initial_x", "=", "self", ".", "wrapped_env", ".", "env", ".", "sim", ".", "data", ".", "qpos", "[", "0", "]", "\n", "return", "obs", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.util.utilsTH.SparseRewardEnv.step": [[80, 94], ["utilsTH.SparseRewardEnv.wrapped_env.step", "math.floor", "math.fabs"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.customenvs.humanoid.HumanoidTruncatedObsEnv.step"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "next_obs", ",", "reward", ",", "done", ",", "info", "=", "self", ".", "wrapped_env", ".", "step", "(", "action", ")", "\n", "\n", "current_x_pos", "=", "self", ".", "wrapped_env", ".", "env", ".", "sim", ".", "data", ".", "qpos", "[", "0", "]", "\n", "if", "(", "(", "current_x_pos", "-", "self", ".", "initial_x", ")", ">=", "self", ".", "rew_thresh", ")", ":", "\n", "            ", "reward", "=", "1.0", "+", "0.0", "*", "reward", "# use previous reward value to keep resulting value type as float 64", "\n", "# unlike PolyRL paper setting, base x position is updated like Ant nav.", "\n", "# self.initial_x = current_x_pos", "\n", "# step reward 20210624", "\n", "reward", "=", "0.0", "*", "reward", "+", "math", ".", "floor", "(", "(", "math", ".", "fabs", "(", "current_x_pos", "-", "self", ".", "initial_x", ")", ")", "/", "self", ".", "rew_thresh", ")", "\n", "", "else", ":", "\n", "            ", "reward", "=", "0.0", "+", "0.0", "*", "reward", "# ditto", "\n", "\n", "", "return", "next_obs", ",", "reward", ",", "done", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.customenvs.ant.AntTruncatedObsEnv.__init__": [[13, 16], ["gym.envs.mujoco.mujoco_env.MujocoEnv.__init__", "gym.utils.EzPickle.__init__"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.EpochLogger.__init__", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.EpochLogger.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "mujoco_env", ".", "MujocoEnv", ".", "__init__", "(", "self", ",", "'ant.xml'", ",", "5", ")", "\n", "utils", ".", "EzPickle", ".", "__init__", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.customenvs.ant.AntTruncatedObsEnv.step": [[17, 37], ["ant.AntTruncatedObsEnv.do_simulation", "ant.AntTruncatedObsEnv.state_vector", "ant.AntTruncatedObsEnv._get_obs", "ant.AntTruncatedObsEnv.get_body_com", "ant.AntTruncatedObsEnv.get_body_com", "numpy.square().sum", "numpy.sum", "numpy.isfinite().all", "dict", "numpy.square", "numpy.square", "numpy.clip", "numpy.isfinite"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.customenvs.humanoid.HumanoidTruncatedObsEnv._get_obs"], ["", "def", "step", "(", "self", ",", "a", ")", ":", "\n", "        ", "xposbefore", "=", "self", ".", "get_body_com", "(", "\"torso\"", ")", "[", "0", "]", "\n", "self", ".", "do_simulation", "(", "a", ",", "self", ".", "frame_skip", ")", "\n", "xposafter", "=", "self", ".", "get_body_com", "(", "\"torso\"", ")", "[", "0", "]", "\n", "forward_reward", "=", "(", "xposafter", "-", "xposbefore", ")", "/", "self", ".", "dt", "\n", "ctrl_cost", "=", ".5", "*", "np", ".", "square", "(", "a", ")", ".", "sum", "(", ")", "\n", "contact_cost", "=", "0.5", "*", "1e-3", "*", "np", ".", "sum", "(", "\n", "np", ".", "square", "(", "np", ".", "clip", "(", "self", ".", "sim", ".", "data", ".", "cfrc_ext", ",", "-", "1", ",", "1", ")", ")", ")", "\n", "survive_reward", "=", "1.0", "\n", "reward", "=", "forward_reward", "-", "ctrl_cost", "-", "contact_cost", "+", "survive_reward", "\n", "state", "=", "self", ".", "state_vector", "(", ")", "\n", "notdone", "=", "np", ".", "isfinite", "(", "state", ")", ".", "all", "(", ")", "and", "state", "[", "2", "]", ">=", "0.2", "and", "state", "[", "2", "]", "<=", "1.0", "\n", "done", "=", "not", "notdone", "\n", "ob", "=", "self", ".", "_get_obs", "(", ")", "\n", "return", "ob", ",", "reward", ",", "done", ",", "dict", "(", "\n", "reward_forward", "=", "forward_reward", ",", "\n", "reward_ctrl", "=", "-", "ctrl_cost", ",", "\n", "reward_contact", "=", "-", "contact_cost", ",", "\n", "reward_survive", "=", "survive_reward", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.customenvs.ant.AntTruncatedObsEnv._get_obs": [[38, 42], ["numpy.concatenate"], "methods", ["None"], ["", "def", "_get_obs", "(", "self", ")", ":", "\n", "        ", "return", "np", ".", "concatenate", "(", "[", "\n", "self", ".", "sim", ".", "data", ".", "qpos", ".", "flat", "[", "2", ":", "]", ",", "\n", "self", ".", "sim", ".", "data", ".", "qvel", ".", "flat", ",", "\n", "# np.clip(self.sim.data.cfrc_ext, -1, 1).flat,", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.customenvs.ant.AntTruncatedObsEnv.reset_model": [[45, 50], ["ant.AntTruncatedObsEnv.set_state", "ant.AntTruncatedObsEnv._get_obs", "ant.AntTruncatedObsEnv.np_random.uniform", "ant.AntTruncatedObsEnv.np_random.randn"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.customenvs.humanoid.HumanoidTruncatedObsEnv._get_obs"], ["", "def", "reset_model", "(", "self", ")", ":", "\n", "        ", "qpos", "=", "self", ".", "init_qpos", "+", "self", ".", "np_random", ".", "uniform", "(", "size", "=", "self", ".", "model", ".", "nq", ",", "low", "=", "-", ".1", ",", "high", "=", ".1", ")", "\n", "qvel", "=", "self", ".", "init_qvel", "+", "self", ".", "np_random", ".", "randn", "(", "self", ".", "model", ".", "nv", ")", "*", ".1", "\n", "self", ".", "set_state", "(", "qpos", ",", "qvel", ")", "\n", "return", "self", ".", "_get_obs", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.customenvs.ant.AntTruncatedObsEnv.viewer_setup": [[51, 53], ["None"], "methods", ["None"], ["", "def", "viewer_setup", "(", "self", ")", ":", "\n", "        ", "self", ".", "viewer", ".", "cam", ".", "distance", "=", "self", ".", "model", ".", "stat", ".", "extent", "*", "0.5", "", "", "", ""]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.customenvs.humanoid.HumanoidTruncatedObsEnv.__init__": [[17, 20], ["gym.envs.mujoco.mujoco_env.MujocoEnv.__init__", "gym.utils.EzPickle.__init__"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.EpochLogger.__init__", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.EpochLogger.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "mujoco_env", ".", "MujocoEnv", ".", "__init__", "(", "self", ",", "'humanoid.xml'", ",", "5", ")", "\n", "utils", ".", "EzPickle", ".", "__init__", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.customenvs.humanoid.HumanoidTruncatedObsEnv._get_obs": [[21, 25], ["numpy.concatenate"], "methods", ["None"], ["", "def", "_get_obs", "(", "self", ")", ":", "\n", "        ", "data", "=", "self", ".", "sim", ".", "data", "\n", "return", "np", ".", "concatenate", "(", "[", "data", ".", "qpos", ".", "flat", "[", "2", ":", "]", ",", "\n", "data", ".", "qvel", ".", "flat", ",", "\n", "# data.cinert.flat,", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.customenvs.humanoid.HumanoidTruncatedObsEnv.step": [[31, 45], ["humanoid.mass_center", "humanoid.HumanoidTruncatedObsEnv.do_simulation", "humanoid.mass_center", "min", "bool", "numpy.square().sum", "numpy.square().sum", "humanoid.HumanoidTruncatedObsEnv._get_obs", "dict", "numpy.square", "numpy.square"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.customenvs.humanoid.mass_center", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.customenvs.humanoid.mass_center", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.customenvs.humanoid.HumanoidTruncatedObsEnv._get_obs"], ["", "def", "step", "(", "self", ",", "a", ")", ":", "\n", "        ", "pos_before", "=", "mass_center", "(", "self", ".", "model", ",", "self", ".", "sim", ")", "\n", "self", ".", "do_simulation", "(", "a", ",", "self", ".", "frame_skip", ")", "\n", "pos_after", "=", "mass_center", "(", "self", ".", "model", ",", "self", ".", "sim", ")", "\n", "alive_bonus", "=", "5.0", "\n", "data", "=", "self", ".", "sim", ".", "data", "\n", "lin_vel_cost", "=", "0.25", "*", "(", "pos_after", "-", "pos_before", ")", "/", "self", ".", "model", ".", "opt", ".", "timestep", "\n", "quad_ctrl_cost", "=", "0.1", "*", "np", ".", "square", "(", "data", ".", "ctrl", ")", ".", "sum", "(", ")", "\n", "quad_impact_cost", "=", ".5e-6", "*", "np", ".", "square", "(", "data", ".", "cfrc_ext", ")", ".", "sum", "(", ")", "\n", "quad_impact_cost", "=", "min", "(", "quad_impact_cost", ",", "10", ")", "\n", "reward", "=", "lin_vel_cost", "-", "quad_ctrl_cost", "-", "quad_impact_cost", "+", "alive_bonus", "\n", "qpos", "=", "self", ".", "sim", ".", "data", ".", "qpos", "\n", "done", "=", "bool", "(", "(", "qpos", "[", "2", "]", "<", "1.0", ")", "or", "(", "qpos", "[", "2", "]", ">", "2.0", ")", ")", "\n", "return", "self", ".", "_get_obs", "(", ")", ",", "reward", ",", "done", ",", "dict", "(", "reward_linvel", "=", "lin_vel_cost", ",", "reward_quadctrl", "=", "-", "quad_ctrl_cost", ",", "reward_alive", "=", "alive_bonus", ",", "reward_impact", "=", "-", "quad_impact_cost", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.customenvs.humanoid.HumanoidTruncatedObsEnv.reset_model": [[46, 53], ["humanoid.HumanoidTruncatedObsEnv.set_state", "humanoid.HumanoidTruncatedObsEnv._get_obs", "humanoid.HumanoidTruncatedObsEnv.np_random.uniform", "humanoid.HumanoidTruncatedObsEnv.np_random.uniform"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.customenvs.humanoid.HumanoidTruncatedObsEnv._get_obs"], ["", "def", "reset_model", "(", "self", ")", ":", "\n", "        ", "c", "=", "0.01", "\n", "self", ".", "set_state", "(", "\n", "self", ".", "init_qpos", "+", "self", ".", "np_random", ".", "uniform", "(", "low", "=", "-", "c", ",", "high", "=", "c", ",", "size", "=", "self", ".", "model", ".", "nq", ")", ",", "\n", "self", ".", "init_qvel", "+", "self", ".", "np_random", ".", "uniform", "(", "low", "=", "-", "c", ",", "high", "=", "c", ",", "size", "=", "self", ".", "model", ".", "nv", ",", ")", "\n", ")", "\n", "return", "self", ".", "_get_obs", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.customenvs.humanoid.HumanoidTruncatedObsEnv.viewer_setup": [[54, 59], ["None"], "methods", ["None"], ["", "def", "viewer_setup", "(", "self", ")", ":", "\n", "        ", "self", ".", "viewer", ".", "cam", ".", "trackbodyid", "=", "1", "\n", "self", ".", "viewer", ".", "cam", ".", "distance", "=", "self", ".", "model", ".", "stat", ".", "extent", "*", "1.0", "\n", "self", ".", "viewer", ".", "cam", ".", "lookat", "[", "2", "]", "=", "2.0", "\n", "self", ".", "viewer", ".", "cam", ".", "elevation", "=", "-", "20", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.customenvs.humanoid.mass_center": [[5, 9], ["numpy.expand_dims", "numpy.sum", "numpy.sum"], "function", ["None"], ["def", "mass_center", "(", "model", ",", "sim", ")", ":", "\n", "    ", "mass", "=", "np", ".", "expand_dims", "(", "model", ".", "body_mass", ",", "1", ")", "\n", "xpos", "=", "sim", ".", "data", ".", "xipos", "\n", "return", "(", "np", ".", "sum", "(", "mass", "*", "xpos", ",", "0", ")", "/", "np", ".", "sum", "(", "mass", ")", ")", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.customenvs.__init__.register_mbpo_environments": [[20, 29], ["tuple", "gym.register"], "function", ["None"], ["def", "register_mbpo_environments", "(", ")", ":", "\n", "    ", "for", "mbpo_environment", "in", "MBPO_ENVIRONMENT_SPECS", ":", "\n", "        ", "gym", ".", "register", "(", "**", "mbpo_environment", ")", "\n", "\n", "", "gym_ids", "=", "tuple", "(", "\n", "environment_spec", "[", "'id'", "]", "\n", "for", "environment_spec", "in", "MBPO_ENVIRONMENT_SPECS", ")", "\n", "\n", "return", "gym_ids", "", "", ""]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.OriginalREDQCodebase.main.redq_sac": [[18, 216], ["dict", "int", "torch.device", "redq.utils.logx.EpochLogger", "redq.utils.logx.EpochLogger.save_config", "torch.manual_seed", "numpy.random.seed", "main.redq_sac.seed_all"], "function", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.Logger.save_config"], ["\n", "\n", "def", "run", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "# replaced wtih SAC-extention args 20210705", "\n", "parser", ".", "add_argument", "(", "\"-env\"", ",", "type", "=", "str", ",", "default", "=", "\"HalfCheetah-v2\"", ",", "\n", "help", "=", "\"Environment name, default = HalfCheetahBulletEnv-v0\"", ")", "\n", "parser", ".", "add_argument", "(", "'-seed'", ",", "type", "=", "int", ",", "default", "=", "0", ")", "\n", "#added byTH 20210705", "\n", "# common", "\n", "parser", ".", "add_argument", "(", "\"-info\"", ",", "type", "=", "str", ",", "help", "=", "\"Information or name of the run\"", ")", "\n", "parser", ".", "add_argument", "(", "\"-frames\"", ",", "type", "=", "int", ",", "default", "=", "1_000_000", ",", "\n", "help", "=", "\"The amount of training interactions with the environment, default is 1mio\"", ")", "\n", "parser", ".", "add_argument", "(", "\"-gpu_id\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"GPU device ID to be used in GPU experiment, default is 1e6\"", ")", "\n", "# evaluation", "\n", "parser", ".", "add_argument", "(", "\"-eval_every\"", ",", "type", "=", "int", ",", "default", "=", "1000", ",", "\n", "help", "=", "\"Number of interactions after which the evaluation runs are performed, default = 1000\"", ")", "\n", "parser", ".", "add_argument", "(", "\"-eval_runs\"", ",", "type", "=", "int", ",", "default", "=", "3", ",", "help", "=", "\"Number of evaluation runs performed, default = 1\"", ")", "\n", "# sparse env", "\n", "parser", ".", "add_argument", "(", "\"-sparsity_th\"", ",", "type", "=", "float", ",", "default", "=", "0.0", ",", "\n", "help", "=", "\"threshold for make reward sparse (i.e., lambda in PolyRL paper), default is 0.0\"", ")", "\n", "# stabilization", "\n", "parser", ".", "add_argument", "(", "\"-huber\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "choices", "=", "[", "0", ",", "1", "]", ",", "\n", "help", "=", "\"Using Huber loss for training critics if set to 1 (TH), default=0\"", ")", "# TODO remove", "\n", "parser", ".", "add_argument", "(", "\"-layer_norm\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "choices", "=", "[", "0", ",", "1", "]", ",", "\n", "help", "=", "\"Using layer normalization for training critics if set to 1 (TH), default=0\"", ")", "\n", "# multi-step and per", "\n", "parser", ".", "add_argument", "(", "\"-n_step\"", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "\"Using n-step bootstrapping, default=1\"", ")", "\n", "parser", ".", "add_argument", "(", "\"-per\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "choices", "=", "[", "0", ",", "1", "]", ",", "\n", "help", "=", "\"Adding Priorizied Experience Replay to the agent if set to 1, default = 0\"", ")", "\n", "# dist RL added @ 20210711", "\n", "parser", ".", "add_argument", "(", "\"-dist\"", ",", "\"--distributional\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "choices", "=", "[", "0", ",", "1", "]", ",", "\n", "help", "=", "\"Using a distributional IQN Critic if set to 1, default=0\"", ")", "# TODO remove", "\n", "# learning per steps", "\n", "parser", ".", "add_argument", "(", "\"-updates_per_step\"", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "\"Number of training updates per one environment step, default = 1\"", ")", "\n", "# th 20210724", "\n", "parser", ".", "add_argument", "(", "\"-target_entropy\"", ",", "type", "=", "float", ",", "default", "=", "None", ",", "help", "=", "\"target entropy , default=Num action\"", ")", "\n", "# for MBPO and redq setting, Hopper: -1, HC: -3, Walker: -3, Ant: -4, Humaoid: -2", "\n", "#", "\n", "parser", ".", "add_argument", "(", "\"-method\"", ",", "default", "=", "\"sac\"", ",", "choices", "=", "[", "\"sac\"", ",", "\"redq\"", ",", "\"duvn\"", ",", "\"monosac\"", "]", ",", "help", "=", "\"method, default=sac\"", ")", "\n", "# learning per steps", "\n", "parser", ".", "add_argument", "(", "\"-batch_size\"", ",", "type", "=", "int", ",", "default", "=", "256", ",", "\n", "help", "=", "\"Number of training batch, default = 256\"", ")", "\n", "#", "\n", "parser", ".", "add_argument", "(", "\"-target_drop_rate\"", ",", "type", "=", "float", ",", "default", "=", "0.0", ",", "help", "=", "\"drop out rate of target value function, default=0\"", ")", "\n", "#", "\n", "parser", ".", "add_argument", "(", "\"-critic_update_delay\"", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "\"number of critic learning delay (tau and UDP is rescaled), default=1 (no delay)\"", ")", "# TODO remove", "\n", "\n", "# 20210813", "\n", "# dist RL added @ 20210711", "\n", "parser", ".", "add_argument", "(", "\"-profile\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "choices", "=", "[", "0", ",", "1", "]", ",", "\n", "help", "=", "\"Using profile for cpu/gpu speed and memory usage if set to 1, default=0\"", ")", "\n", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "# You can define configs in the external json or yaml file.", "\n", "configs", "=", "{", "\n", "'num_steps'", ":", "args", ".", "frames", ",", "#3000000,", "\n", "'batch_size'", ":", "args", ".", "batch_size", ",", "#, 256,", "\n", "'lr'", ":", "0.0003", ",", "\n", "'hidden_units'", ":", "[", "256", ",", "256", "]", ",", "\n", "'memory_size'", ":", "1e6", ",", "\n", "'gamma'", ":", "0.99", ",", "\n", "'tau'", ":", "0.005", ",", "\n", "'entropy_tuning'", ":", "True", ",", "\n", "'ent_coef'", ":", "0.2", ",", "# It's ignored when entropy_tuning=True.", "\n", "'multi_step'", ":", "args", ".", "n_step", ",", "#1,", "\n", "'per'", ":", "args", ".", "per", ",", "#False,  # prioritized experience replay", "\n", "'alpha'", ":", "0.6", ",", "# It's ignored when per=False.", "\n", "'beta'", ":", "0.4", ",", "# It's ignored when per=False.", "\n", "'beta_annealing'", ":", "(", "1.0", "-", "0.4", ")", "/", "(", "1.0", "*", "args", ".", "frames", "*", "args", ".", "updates_per_step", ")", ",", "# 0.0001,  # It's ignored when per=False.", "\n", "'grad_clip'", ":", "None", ",", "\n", "'updates_per_step'", ":", "args", ".", "updates_per_step", "*", "args", ".", "critic_update_delay", ",", "# args.updates_per_step, #1,", "\n", "'start_steps'", ":", "5000", ",", "#10000,", "\n", "'log_interval'", ":", "10", ",", "\n", "'target_update_interval'", ":", "1", ",", "\n", "'eval_interval'", ":", "args", ".", "eval_every", ",", "# 10000,", "\n", "'cuda'", ":", "args", ".", "gpu_id", ",", "# args.cuda,", "\n", "'seed'", ":", "args", ".", "seed", ",", "\n", "\n", "# adde by TH", "\n", "'eval_runs'", ":", "args", ".", "eval_runs", ",", "\n", "'huber'", ":", "args", ".", "huber", ",", "# TODO remove", "\n", "'layer_norm'", ":", "args", ".", "layer_norm", ",", "\n", "#", "\n", "'target_entropy'", ":", "args", ".", "target_entropy", ",", "\n", "'method'", ":", "args", ".", "method", ",", "\n", "'target_drop_rate'", ":", "args", ".", "target_drop_rate", ",", "\n", "'critic_update_delay'", ":", "args", ".", "critic_update_delay", "\n", "}", "\n", "\n", "env", "=", "gym", ".", "make", "(", "args", ".", "env", ")", "\n", "\n", "# make sparse en: TH 20210705", "\n", "if", "args", ".", "sparsity_th", ">", "0.0", ":", "\n", "        ", "print", "(", "\"Evaluation in sparse reward setting with lambda = \"", "+", "str", "(", "args", ".", "sparsity_th", ")", ")", "\n", "env", "=", "SparseRewardEnv", "(", "env", ",", "rew_thresh", "=", "args", ".", "sparsity_th", ")", "\n", "env", ".", "_max_episode_steps", "=", "env", ".", "wrapped_env", ".", "_max_episode_steps", "\n", "\n", "", "label", "=", "args", ".", "env", "+", "\"_\"", "+", "str", "(", "datetime", ".", "datetime", ".", "now", "(", ")", ".", "isoformat", "(", ")", ")", "\n", "log_dir", "=", "os", ".", "path", ".", "join", "(", "'runs'", ",", "args", ".", "info", ",", "label", ")", "\n", "\n", "if", "args", ".", "distributional", ":", "# TODO remove", "\n", "        ", "raise", "NotImplementedError", "(", ")", "\n", "#print(\" Use IQN agent\")", "\n", "#agent = IQNSacAgent(env=env, log_dir=log_dir, **configs)", "\n", "", "else", ":", "\n", "        ", "if", "args", ".", "profile", ":", "\n", "            ", "agent", "=", "SacAgent4Profile", "(", "env", "=", "env", ",", "log_dir", "=", "log_dir", ",", "**", "configs", ")", "\n", "", "else", ":", "\n", "            ", "agent", "=", "SacAgent", "(", "env", "=", "env", ",", "log_dir", "=", "log_dir", ",", "**", "configs", ")", "\n", "", "", "agent", ".", "run", "(", ")", "\n", "\n", "\n", "", "if", "__name__", "==", "'__main__'", ":", "\n", "    ", "run", "(", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.ReplayBuffer.__init__": [[24, 37], ["numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "obs_dim", ",", "act_dim", ",", "size", ")", ":", "\n", "        ", "\"\"\"\n        :param obs_dim: size of observation\n        :param act_dim: size of the action\n        :param size: size of the buffer\n        \"\"\"", "\n", "## init buffers as numpy arrays", "\n", "self", ".", "obs1_buf", "=", "np", ".", "zeros", "(", "[", "size", ",", "obs_dim", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "obs2_buf", "=", "np", ".", "zeros", "(", "[", "size", ",", "obs_dim", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "acts_buf", "=", "np", ".", "zeros", "(", "[", "size", ",", "act_dim", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "rews_buf", "=", "np", ".", "zeros", "(", "size", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "done_buf", "=", "np", ".", "zeros", "(", "size", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "ptr", ",", "self", ".", "size", ",", "self", ".", "max_size", "=", "0", ",", "0", ",", "size", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.ReplayBuffer.store": [[38, 51], ["min"], "methods", ["None"], ["", "def", "store", "(", "self", ",", "obs", ",", "act", ",", "rew", ",", "next_obs", ",", "done", ")", ":", "\n", "        ", "\"\"\"\n        data will get stored in the pointer's location\n        \"\"\"", "\n", "self", ".", "obs1_buf", "[", "self", ".", "ptr", "]", "=", "obs", "\n", "self", ".", "obs2_buf", "[", "self", ".", "ptr", "]", "=", "next_obs", "\n", "self", ".", "acts_buf", "[", "self", ".", "ptr", "]", "=", "act", "\n", "self", ".", "rews_buf", "[", "self", ".", "ptr", "]", "=", "rew", "\n", "self", ".", "done_buf", "[", "self", ".", "ptr", "]", "=", "done", "\n", "## move the pointer to store in next location in buffer", "\n", "self", ".", "ptr", "=", "(", "self", ".", "ptr", "+", "1", ")", "%", "self", ".", "max_size", "\n", "## keep track of the current buffer size", "\n", "self", ".", "size", "=", "min", "(", "self", ".", "size", "+", "1", ",", "self", ".", "max_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.ReplayBuffer.sample_batch": [[52, 66], ["dict", "numpy.random.randint"], "methods", ["None"], ["", "def", "sample_batch", "(", "self", ",", "batch_size", "=", "32", ",", "idxs", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        :param batch_size: size of minibatch\n        :param idxs: specify indexes if you want specific data points\n        :return: mini-batch data as a dictionary\n        \"\"\"", "\n", "if", "idxs", "is", "None", ":", "\n", "            ", "idxs", "=", "np", ".", "random", ".", "randint", "(", "0", ",", "self", ".", "size", ",", "size", "=", "batch_size", ")", "\n", "", "return", "dict", "(", "obs1", "=", "self", ".", "obs1_buf", "[", "idxs", "]", ",", "\n", "obs2", "=", "self", ".", "obs2_buf", "[", "idxs", "]", ",", "\n", "acts", "=", "self", ".", "acts_buf", "[", "idxs", "]", ",", "\n", "rews", "=", "self", ".", "rews_buf", "[", "idxs", "]", ",", "\n", "done", "=", "self", ".", "done_buf", "[", "idxs", "]", ",", "\n", "idxs", "=", "idxs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.Mlp.__init__": [[69, 111], ["torch.Module.__init__", "torch.ModuleList", "torch.ModuleList", "enumerate", "torch.Linear", "torch.Linear", "core.Mlp.apply", "torch.Linear", "torch.Linear", "core.Mlp.hidden_layers.append", "core.Mlp.hidden_layers.append", "core.Mlp.hidden_layers.append", "torch.Dropout", "torch.Dropout", "torch.LayerNorm", "torch.LayerNorm"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.EpochLogger.__init__", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.RunningMeanStats.append", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.RunningMeanStats.append", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.RunningMeanStats.append"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "input_size", ",", "\n", "output_size", ",", "\n", "hidden_sizes", ",", "\n", "hidden_activation", "=", "F", ".", "relu", ",", "\n", "#", "\n", "target_drop_rate", "=", "0.0", ",", "layer_norm", "=", "False", "\n", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "input_size", "=", "input_size", "\n", "self", ".", "output_size", "=", "output_size", "\n", "self", ".", "hidden_activation", "=", "hidden_activation", "\n", "## here we use ModuleList so that the layers in it can be", "\n", "## detected by .parameters() call", "\n", "self", ".", "hidden_layers", "=", "nn", ".", "ModuleList", "(", ")", "\n", "in_size", "=", "input_size", "\n", "\n", "## initialize each hidden layer", "\n", "for", "i", ",", "next_size", "in", "enumerate", "(", "hidden_sizes", ")", ":", "\n", "            ", "fc_layer", "=", "nn", ".", "Linear", "(", "in_size", ",", "next_size", ")", "\n", "in_size", "=", "next_size", "\n", "self", ".", "hidden_layers", ".", "append", "(", "fc_layer", ")", "\n", "\n", "# added 20211206", "\n", "if", "target_drop_rate", ">", "0.0", ":", "\n", "                ", "self", ".", "hidden_layers", ".", "append", "(", "nn", ".", "Dropout", "(", "p", "=", "target_drop_rate", ")", ")", "# dropout", "\n", "", "if", "layer_norm", ":", "\n", "                ", "self", ".", "hidden_layers", ".", "append", "(", "nn", ".", "LayerNorm", "(", "fc_layer", ".", "out_features", ")", ")", "# layer norm", "\n", "\n", "# added to fix bug 20211207", "\n", "", "", "self", ".", "apply_activation_per", "=", "1", "\n", "if", "target_drop_rate", ">", "0.0", ":", "\n", "            ", "self", ".", "apply_activation_per", "+=", "1", "\n", "", "if", "layer_norm", ":", "\n", "            ", "self", ".", "apply_activation_per", "+=", "1", "\n", "\n", "## init last fully connected layer with small weight and bias", "\n", "", "self", ".", "last_fc_layer", "=", "nn", ".", "Linear", "(", "in_size", ",", "output_size", ")", "\n", "self", ".", "apply", "(", "weights_init_", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.Mlp.forward": [[112, 121], ["enumerate", "core.Mlp.last_fc_layer", "fc_layer", "core.Mlp.hidden_activation"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "h", "=", "input", "\n", "for", "i", ",", "fc_layer", "in", "enumerate", "(", "self", ".", "hidden_layers", ")", ":", "\n", "            ", "h", "=", "fc_layer", "(", "h", ")", "\n", "#h = self.hidden_activation(h)", "\n", "if", "(", "(", "i", "+", "1", ")", "%", "self", ".", "apply_activation_per", ")", "==", "0", ":", "\n", "                ", "h", "=", "self", ".", "hidden_activation", "(", "h", ")", "\n", "", "", "output", "=", "self", ".", "last_fc_layer", "(", "h", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.TanhNormal.__init__": [[129, 139], ["torch.distributions.Normal", "torch.distributions.Normal"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "normal_mean", ",", "normal_std", ",", "epsilon", "=", "1e-6", ")", ":", "\n", "        ", "\"\"\"\n        :param normal_mean: Mean of the normal distribution\n        :param normal_std: Std of the normal distribution\n        :param epsilon: Numerical stability epsilon when computing log-prob.\n        \"\"\"", "\n", "self", ".", "normal_mean", "=", "normal_mean", "\n", "self", ".", "normal_std", "=", "normal_std", "\n", "self", ".", "normal", "=", "Normal", "(", "normal_mean", ",", "normal_std", ")", "\n", "self", ".", "epsilon", "=", "epsilon", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.TanhNormal.log_prob": [[140, 154], ["core.TanhNormal.normal.log_prob", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.TanhNormal.log_prob", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.Logger.log", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.Logger.log", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.Logger.log", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.Logger.log", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.Logger.log", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.Logger.log", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.Logger.log", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.Logger.log"], ["", "def", "log_prob", "(", "self", ",", "value", ",", "pre_tanh_value", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        return the log probability of a value\n        :param value: some value, x\n        :param pre_tanh_value: arctanh(x)\n        :return:\n        \"\"\"", "\n", "# use arctanh formula to compute arctanh(value)", "\n", "if", "pre_tanh_value", "is", "None", ":", "\n", "            ", "pre_tanh_value", "=", "torch", ".", "log", "(", "\n", "(", "1", "+", "value", ")", "/", "(", "1", "-", "value", ")", "\n", ")", "/", "2", "\n", "", "return", "self", ".", "normal", ".", "log_prob", "(", "pre_tanh_value", ")", "-", "torch", ".", "log", "(", "1", "-", "value", "*", "value", "+", "self", ".", "epsilon", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.TanhNormal.sample": [[155, 166], ["core.TanhNormal.normal.sample().detach", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "core.TanhNormal.normal.sample", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.TanhNormal.sample"], ["", "def", "sample", "(", "self", ",", "return_pretanh_value", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Gradients will and should *not* pass through this operation.\n        See https://github.com/pytorch/pytorch/issues/4620 for discussion.\n        \"\"\"", "\n", "z", "=", "self", ".", "normal", ".", "sample", "(", ")", ".", "detach", "(", ")", "\n", "\n", "if", "return_pretanh_value", ":", "\n", "            ", "return", "torch", ".", "tanh", "(", "z", ")", ",", "z", "\n", "", "else", ":", "\n", "            ", "return", "torch", ".", "tanh", "(", "z", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.TanhNormal.rsample": [[167, 186], ["torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.distributions.Normal().sample", "torch.distributions.Normal().sample", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.distributions.Normal", "torch.distributions.Normal", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "core.TanhNormal.normal_mean.size", "core.TanhNormal.normal_std.size"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.TanhNormal.sample", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.TanhNormal.sample"], ["", "", "def", "rsample", "(", "self", ",", "return_pretanh_value", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Sampling in the reparameterization case.\n        Implement: tanh(mu + sigma * eksee)\n        with eksee~N(0,1)\n        z here is mu+sigma+eksee\n        \"\"\"", "\n", "z", "=", "(", "\n", "self", ".", "normal_mean", "+", "\n", "self", ".", "normal_std", "*", "\n", "Normal", "(", "## this part is eksee~N(0,1)", "\n", "torch", ".", "zeros", "(", "self", ".", "normal_mean", ".", "size", "(", ")", ")", ",", "\n", "torch", ".", "ones", "(", "self", ".", "normal_std", ".", "size", "(", ")", ")", "\n", ")", ".", "sample", "(", ")", "\n", ")", "\n", "if", "return_pretanh_value", ":", "\n", "            ", "return", "torch", ".", "tanh", "(", "z", ")", ",", "z", "\n", "", "else", ":", "\n", "            ", "return", "torch", ".", "tanh", "(", "z", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.TanhGaussianPolicy.__init__": [[191, 213], ["core.Mlp.__init__", "torch.Linear", "torch.Linear", "core.TanhGaussianPolicy.apply", "len"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.EpochLogger.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "obs_dim", ",", "\n", "action_dim", ",", "\n", "hidden_sizes", ",", "\n", "hidden_activation", "=", "F", ".", "relu", ",", "\n", "action_limit", "=", "1.0", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "input_size", "=", "obs_dim", ",", "\n", "output_size", "=", "action_dim", ",", "\n", "hidden_sizes", "=", "hidden_sizes", ",", "\n", "hidden_activation", "=", "hidden_activation", ",", "\n", ")", "\n", "last_hidden_size", "=", "obs_dim", "\n", "if", "len", "(", "hidden_sizes", ")", ">", "0", ":", "\n", "            ", "last_hidden_size", "=", "hidden_sizes", "[", "-", "1", "]", "\n", "## this is the layer that gives log_std, init this layer with small weight and bias", "\n", "", "self", ".", "last_fc_log_std", "=", "nn", ".", "Linear", "(", "last_hidden_size", ",", "action_dim", ")", "\n", "## action limit: for example, humanoid has an action limit of -0.4 to 0.4", "\n", "self", ".", "action_limit", "=", "action_limit", "\n", "self", ".", "apply", "(", "weights_init_", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.TanhGaussianPolicy.forward": [[214, 253], ["core.TanhGaussianPolicy.last_fc_layer", "core.TanhGaussianPolicy.last_fc_log_std", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.distributions.Normal", "torch.distributions.Normal", "core.TanhGaussianPolicy.hidden_activation", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.distributions.Normal.rsample", "torch.distributions.Normal.rsample", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.distributions.Normal.log_prob", "torch.distributions.Normal.log_prob", "torch.log", "torch.log", "torch.log", "torch.log", "log_prob.sum.sum.sum", "fc_layer", "torch.tanh.pow", "torch.tanh.pow"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.TanhNormal.rsample", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.TanhNormal.rsample", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.TanhNormal.log_prob", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.TanhNormal.log_prob", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.Logger.log", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.Logger.log", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.Logger.log", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.Logger.log"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "obs", ",", "\n", "deterministic", "=", "False", ",", "\n", "return_log_prob", "=", "True", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        :param obs: Observation\n        :param reparameterize: if True, use the reparameterization trick\n        :param deterministic: If True, take determinisitc (test) action\n        :param return_log_prob: If True, return a sample and its log probability\n        \"\"\"", "\n", "h", "=", "obs", "\n", "for", "fc_layer", "in", "self", ".", "hidden_layers", ":", "\n", "            ", "h", "=", "self", ".", "hidden_activation", "(", "fc_layer", "(", "h", ")", ")", "\n", "", "mean", "=", "self", ".", "last_fc_layer", "(", "h", ")", "\n", "\n", "log_std", "=", "self", ".", "last_fc_log_std", "(", "h", ")", "\n", "log_std", "=", "torch", ".", "clamp", "(", "log_std", ",", "LOG_SIG_MIN", ",", "LOG_SIG_MAX", ")", "\n", "std", "=", "torch", ".", "exp", "(", "log_std", ")", "\n", "\n", "normal", "=", "Normal", "(", "mean", ",", "std", ")", "\n", "\n", "if", "deterministic", ":", "\n", "            ", "pre_tanh_value", "=", "mean", "\n", "action", "=", "torch", ".", "tanh", "(", "mean", ")", "\n", "", "else", ":", "\n", "            ", "pre_tanh_value", "=", "normal", ".", "rsample", "(", ")", "\n", "action", "=", "torch", ".", "tanh", "(", "pre_tanh_value", ")", "\n", "\n", "", "if", "return_log_prob", ":", "\n", "            ", "log_prob", "=", "normal", ".", "log_prob", "(", "pre_tanh_value", ")", "\n", "log_prob", "-=", "torch", ".", "log", "(", "1", "-", "action", ".", "pow", "(", "2", ")", "+", "ACTION_BOUND_EPSILON", ")", "\n", "log_prob", "=", "log_prob", ".", "sum", "(", "1", ",", "keepdim", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "log_prob", "=", "None", "\n", "\n", "", "return", "(", "\n", "action", "*", "self", ".", "action_limit", ",", "mean", ",", "log_std", ",", "log_prob", ",", "std", ",", "pre_tanh_value", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.weights_init_": [[14, 19], ["isinstance", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.constant_", "torch.nn.init.constant_"], "function", ["None"], ["def", "weights_init_", "(", "m", ")", ":", "\n", "# weight init helper function", "\n", "    ", "if", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", ":", "\n", "        ", "torch", ".", "nn", ".", "init", ".", "xavier_uniform_", "(", "m", ".", "weight", ",", "gain", "=", "1", ")", "\n", "torch", ".", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.soft_update_model1_with_model2": [[255, 264], ["zip", "model1.parameters", "model2.parameters", "model1_param.data.copy_"], "function", ["None"], ["", "", "def", "soft_update_model1_with_model2", "(", "model1", ",", "model2", ",", "rou", ")", ":", "\n", "    ", "\"\"\"\n    used to polyak update a target network\n    :param model1: a pytorch model\n    :param model2: a pytorch model of the same class\n    :param rou: the update is model1 <- rou*model1 + (1-rou)model2\n    \"\"\"", "\n", "for", "model1_param", ",", "model2_param", "in", "zip", "(", "model1", ".", "parameters", "(", ")", ",", "model2", ".", "parameters", "(", ")", ")", ":", "\n", "        ", "model1_param", ".", "data", ".", "copy_", "(", "rou", "*", "model1_param", ".", "data", "+", "(", "1", "-", "rou", ")", "*", "model2_param", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.test_agent": [[265, 290], ["numpy.zeros", "range", "test_env.reset", "agent.get_test_action", "test_env.step", "logger.store"], "function", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.util.utilsTH.SparseRewardEnv.reset", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.redq_sac.REDQSACAgent.get_test_action", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.customenvs.humanoid.HumanoidTruncatedObsEnv.step", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.EpochLogger.store"], ["", "", "def", "test_agent", "(", "agent", ",", "test_env", ",", "max_ep_len", ",", "logger", ",", "n_eval", "=", "1", ")", ":", "\n", "    ", "\"\"\"\n    This will test the agent's performance by running <n_eval> episodes\n    During the runs, the agent should only take deterministic action\n    This function assumes the agent has a <get_test_action()> function\n    :param agent: agent instance\n    :param test_env: the environment used for testing\n    :param max_ep_len: max length of an episode\n    :param logger: logger to store info in\n    :param n_eval: number of episodes to run the agent\n    :return: test return for each episode as a numpy array\n    \"\"\"", "\n", "ep_return_list", "=", "np", ".", "zeros", "(", "n_eval", ")", "\n", "for", "j", "in", "range", "(", "n_eval", ")", ":", "\n", "        ", "o", ",", "r", ",", "d", ",", "ep_ret", ",", "ep_len", "=", "test_env", ".", "reset", "(", ")", ",", "0", ",", "False", ",", "0", ",", "0", "\n", "while", "not", "(", "d", "or", "(", "ep_len", "==", "max_ep_len", ")", ")", ":", "\n", "# Take deterministic actions at test time", "\n", "            ", "a", "=", "agent", ".", "get_test_action", "(", "o", ")", "\n", "o", ",", "r", ",", "d", ",", "_", "=", "test_env", ".", "step", "(", "a", ")", "\n", "ep_ret", "+=", "r", "\n", "ep_len", "+=", "1", "\n", "", "ep_return_list", "[", "j", "]", "=", "ep_ret", "\n", "if", "logger", "is", "not", "None", ":", "\n", "            ", "logger", ".", "store", "(", "TestEpRet", "=", "ep_ret", ",", "TestEpLen", "=", "ep_len", ")", "\n", "", "", "return", "ep_return_list", "\n", "", ""]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.redq_sac.REDQSACAgent.__init__": [[30, 97], ["int", "redq.algos.core.TanhGaussianPolicy().to", "range", "torch.Adam", "torch.Adam", "torch.Adam", "range", "redq.algos.core.ReplayBuffer", "torch.MSELoss", "torch.MSELoss", "torch.MSELoss", "redq.algos.core.Mlp().to", "redq_sac.REDQSACAgent.q_net_list.append", "redq.algos.core.Mlp().to", "redq.algos.core.Mlp().to.load_state_dict", "redq_sac.REDQSACAgent.q_target_net_list.append", "redq_sac.REDQSACAgent.policy_net.parameters", "redq_sac.REDQSACAgent.q_optimizer_list.append", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.Adam", "torch.Adam", "torch.Adam", "redq_sac.REDQSACAgent.log_alpha.cpu().exp().item", "redq.algos.core.TanhGaussianPolicy", "redq.algos.core.Mlp().to.state_dict", "torch.Adam", "torch.Adam", "torch.Adam", "redq.algos.core.Mlp", "redq.algos.core.Mlp", "redq_sac.REDQSACAgent.q_net_list[].parameters", "redq_sac.REDQSACAgent.log_alpha.cpu().exp", "redq_sac.REDQSACAgent.log_alpha.cpu"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.RunningMeanStats.append", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.RunningMeanStats.append", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.RunningMeanStats.append"], ["def", "__init__", "(", "self", ",", "env_name", ",", "obs_dim", ",", "act_dim", ",", "act_limit", ",", "device", ",", "\n", "hidden_sizes", "=", "(", "256", ",", "256", ")", ",", "replay_size", "=", "int", "(", "1e6", ")", ",", "batch_size", "=", "256", ",", "\n", "lr", "=", "3e-4", ",", "gamma", "=", "0.99", ",", "polyak", "=", "0.995", ",", "\n", "alpha", "=", "0.2", ",", "auto_alpha", "=", "True", ",", "target_entropy", "=", "'mbpo'", ",", "\n", "start_steps", "=", "5000", ",", "delay_update_steps", "=", "'auto'", ",", "\n", "utd_ratio", "=", "20", ",", "num_Q", "=", "10", ",", "num_min", "=", "2", ",", "q_target_mode", "=", "'min'", ",", "\n", "policy_update_delay", "=", "20", ",", "\n", "# added 20211206", "\n", "target_drop_rate", "=", "0.0", ",", "layer_norm", "=", "False", "\n", ")", ":", "\n", "# set up networks", "\n", "        ", "self", ".", "policy_net", "=", "TanhGaussianPolicy", "(", "obs_dim", ",", "act_dim", ",", "hidden_sizes", ",", "action_limit", "=", "act_limit", ")", ".", "to", "(", "device", ")", "\n", "self", ".", "q_net_list", ",", "self", ".", "q_target_net_list", "=", "[", "]", ",", "[", "]", "\n", "for", "q_i", "in", "range", "(", "num_Q", ")", ":", "\n", "# new_q_net = Mlp(obs_dim + act_dim, 1, hidden_sizes).to(device)", "\n", "            ", "new_q_net", "=", "Mlp", "(", "obs_dim", "+", "act_dim", ",", "1", ",", "hidden_sizes", ",", "target_drop_rate", "=", "target_drop_rate", ",", "layer_norm", "=", "layer_norm", ")", ".", "to", "(", "device", ")", "\n", "self", ".", "q_net_list", ".", "append", "(", "new_q_net", ")", "\n", "# new_q_target_net = Mlp(obs_dim + act_dim, 1, hidden_sizes).to(device)", "\n", "new_q_target_net", "=", "Mlp", "(", "obs_dim", "+", "act_dim", ",", "1", ",", "hidden_sizes", ",", "target_drop_rate", "=", "target_drop_rate", ",", "layer_norm", "=", "layer_norm", ")", ".", "to", "(", "device", ")", "\n", "new_q_target_net", ".", "load_state_dict", "(", "new_q_net", ".", "state_dict", "(", ")", ")", "\n", "self", ".", "q_target_net_list", ".", "append", "(", "new_q_target_net", ")", "\n", "# set up optimizers", "\n", "", "self", ".", "policy_optimizer", "=", "optim", ".", "Adam", "(", "self", ".", "policy_net", ".", "parameters", "(", ")", ",", "lr", "=", "lr", ")", "\n", "self", ".", "q_optimizer_list", "=", "[", "]", "\n", "for", "q_i", "in", "range", "(", "num_Q", ")", ":", "\n", "            ", "self", ".", "q_optimizer_list", ".", "append", "(", "optim", ".", "Adam", "(", "self", ".", "q_net_list", "[", "q_i", "]", ".", "parameters", "(", ")", ",", "lr", "=", "lr", ")", ")", "\n", "# set up adaptive entropy (SAC adaptive)", "\n", "", "self", ".", "auto_alpha", "=", "auto_alpha", "\n", "if", "auto_alpha", ":", "\n", "            ", "if", "target_entropy", "==", "'auto'", ":", "\n", "                ", "self", ".", "target_entropy", "=", "-", "act_dim", "\n", "", "if", "target_entropy", "==", "'mbpo'", ":", "\n", "# add 20211206", "\n", "                ", "mbpo_target_entropy_dict", "[", "'AntTruncatedObs-v2'", "]", "=", "-", "4", "\n", "mbpo_target_entropy_dict", "[", "'HumanoidTruncatedObs-v2'", "]", "=", "-", "2", "\n", "\n", "self", ".", "target_entropy", "=", "mbpo_target_entropy_dict", "[", "env_name", "]", "\n", "", "self", ".", "log_alpha", "=", "torch", ".", "zeros", "(", "1", ",", "requires_grad", "=", "True", ",", "device", "=", "device", ")", "\n", "self", ".", "alpha_optim", "=", "optim", ".", "Adam", "(", "[", "self", ".", "log_alpha", "]", ",", "lr", "=", "lr", ")", "\n", "self", ".", "alpha", "=", "self", ".", "log_alpha", ".", "cpu", "(", ")", ".", "exp", "(", ")", ".", "item", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "alpha", "=", "alpha", "\n", "self", ".", "target_entropy", ",", "self", ".", "log_alpha", ",", "self", ".", "alpha_optim", "=", "None", ",", "None", ",", "None", "\n", "# set up replay buffer", "\n", "", "self", ".", "replay_buffer", "=", "ReplayBuffer", "(", "obs_dim", "=", "obs_dim", ",", "act_dim", "=", "act_dim", ",", "size", "=", "replay_size", ")", "\n", "# set up other things", "\n", "self", ".", "mse_criterion", "=", "nn", ".", "MSELoss", "(", ")", "\n", "\n", "# store other hyperparameters", "\n", "self", ".", "start_steps", "=", "start_steps", "\n", "self", ".", "obs_dim", "=", "obs_dim", "\n", "self", ".", "act_dim", "=", "act_dim", "\n", "self", ".", "act_limit", "=", "act_limit", "\n", "self", ".", "lr", "=", "lr", "\n", "self", ".", "hidden_sizes", "=", "hidden_sizes", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "polyak", "=", "polyak", "\n", "self", ".", "replay_size", "=", "replay_size", "\n", "self", ".", "alpha", "=", "alpha", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "num_min", "=", "num_min", "\n", "self", ".", "num_Q", "=", "num_Q", "\n", "self", ".", "utd_ratio", "=", "utd_ratio", "\n", "self", ".", "delay_update_steps", "=", "self", ".", "start_steps", "if", "delay_update_steps", "==", "'auto'", "else", "delay_update_steps", "\n", "self", ".", "q_target_mode", "=", "q_target_mode", "\n", "self", ".", "policy_update_delay", "=", "policy_update_delay", "\n", "self", ".", "device", "=", "device", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.redq_sac.REDQSACAgent.__get_current_num_data": [[98, 101], ["None"], "methods", ["None"], ["", "def", "__get_current_num_data", "(", "self", ")", ":", "\n", "# used to determine whether we should get action from policy or take random starting actions", "\n", "        ", "return", "self", ".", "replay_buffer", ".", "size", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.redq_sac.REDQSACAgent.get_exploration_action": [[102, 113], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "redq_sac.REDQSACAgent.__get_current_num_data", "torch.Tensor().unsqueeze().to", "torch.Tensor().unsqueeze().to", "torch.Tensor().unsqueeze().to", "torch.Tensor().unsqueeze().to", "torch.Tensor().unsqueeze().to", "torch.Tensor().unsqueeze().to", "torch.Tensor().unsqueeze().to", "torch.Tensor().unsqueeze().to", "torch.Tensor().unsqueeze().to", "action_tensor.cpu().numpy().reshape", "env.action_space.sample", "redq_sac.REDQSACAgent.policy_net.forward", "torch.Tensor().unsqueeze", "torch.Tensor().unsqueeze", "torch.Tensor().unsqueeze", "torch.Tensor().unsqueeze", "torch.Tensor().unsqueeze", "torch.Tensor().unsqueeze", "torch.Tensor().unsqueeze", "torch.Tensor().unsqueeze", "torch.Tensor().unsqueeze", "action_tensor.cpu().numpy", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "action_tensor.cpu"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.redq_sac.REDQSACAgent.__get_current_num_data", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.TanhNormal.sample", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.TanhGaussianPolicy.forward"], ["", "def", "get_exploration_action", "(", "self", ",", "obs", ",", "env", ")", ":", "\n", "# given an observation, output a sampled action in numpy form", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "if", "self", ".", "__get_current_num_data", "(", ")", ">", "self", ".", "start_steps", ":", "\n", "                ", "obs_tensor", "=", "torch", ".", "Tensor", "(", "obs", ")", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "action_tensor", "=", "self", ".", "policy_net", ".", "forward", "(", "obs_tensor", ",", "deterministic", "=", "False", ",", "\n", "return_log_prob", "=", "False", ")", "[", "0", "]", "\n", "action", "=", "action_tensor", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "reshape", "(", "-", "1", ")", "\n", "", "else", ":", "\n", "                ", "action", "=", "env", ".", "action_space", ".", "sample", "(", ")", "\n", "", "", "return", "action", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.redq_sac.REDQSACAgent.get_test_action": [[114, 122], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.Tensor().unsqueeze().to", "torch.Tensor().unsqueeze().to", "torch.Tensor().unsqueeze().to", "torch.Tensor().unsqueeze().to", "torch.Tensor().unsqueeze().to", "torch.Tensor().unsqueeze().to", "torch.Tensor().unsqueeze().to", "torch.Tensor().unsqueeze().to", "torch.Tensor().unsqueeze().to", "action_tensor.cpu().numpy().reshape", "redq_sac.REDQSACAgent.policy_net.forward", "torch.Tensor().unsqueeze", "torch.Tensor().unsqueeze", "torch.Tensor().unsqueeze", "torch.Tensor().unsqueeze", "torch.Tensor().unsqueeze", "torch.Tensor().unsqueeze", "torch.Tensor().unsqueeze", "torch.Tensor().unsqueeze", "torch.Tensor().unsqueeze", "action_tensor.cpu().numpy", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "action_tensor.cpu"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.TanhGaussianPolicy.forward"], ["", "def", "get_test_action", "(", "self", ",", "obs", ")", ":", "\n", "# given an observation, output a deterministic action in numpy form", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "obs_tensor", "=", "torch", ".", "Tensor", "(", "obs", ")", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "action_tensor", "=", "self", ".", "policy_net", ".", "forward", "(", "obs_tensor", ",", "deterministic", "=", "True", ",", "\n", "return_log_prob", "=", "False", ")", "[", "0", "]", "\n", "action", "=", "action_tensor", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "reshape", "(", "-", "1", ")", "\n", "", "return", "action", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.redq_sac.REDQSACAgent.get_action_and_logprob_for_bias_evaluation": [[123, 131], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.Tensor().unsqueeze().to", "torch.Tensor().unsqueeze().to", "torch.Tensor().unsqueeze().to", "torch.Tensor().unsqueeze().to", "torch.Tensor().unsqueeze().to", "torch.Tensor().unsqueeze().to", "torch.Tensor().unsqueeze().to", "torch.Tensor().unsqueeze().to", "torch.Tensor().unsqueeze().to", "redq_sac.REDQSACAgent.policy_net.forward", "action_tensor.cpu().numpy().reshape", "torch.Tensor().unsqueeze", "torch.Tensor().unsqueeze", "torch.Tensor().unsqueeze", "torch.Tensor().unsqueeze", "torch.Tensor().unsqueeze", "torch.Tensor().unsqueeze", "torch.Tensor().unsqueeze", "torch.Tensor().unsqueeze", "torch.Tensor().unsqueeze", "action_tensor.cpu().numpy", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "action_tensor.cpu"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.TanhGaussianPolicy.forward"], ["", "def", "get_action_and_logprob_for_bias_evaluation", "(", "self", ",", "obs", ")", ":", "#TODO modify the readme here", "\n", "# given an observation, output a sampled action in numpy form", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "obs_tensor", "=", "torch", ".", "Tensor", "(", "obs", ")", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "action_tensor", ",", "_", ",", "_", ",", "log_prob_a_tilda", ",", "_", ",", "_", ",", "=", "self", ".", "policy_net", ".", "forward", "(", "obs_tensor", ",", "deterministic", "=", "False", ",", "\n", "return_log_prob", "=", "True", ")", "\n", "action", "=", "action_tensor", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "reshape", "(", "-", "1", ")", "\n", "", "return", "action", ",", "log_prob_a_tilda", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.redq_sac.REDQSACAgent.get_ave_q_prediction_for_bias_evaluation": [[132, 141], ["range", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "q_prediction_list.append", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.RunningMeanStats.append"], ["", "def", "get_ave_q_prediction_for_bias_evaluation", "(", "self", ",", "obs_tensor", ",", "acts_tensor", ")", ":", "\n", "# given obs_tensor and act_tensor, output Q prediction", "\n", "        ", "q_prediction_list", "=", "[", "]", "\n", "for", "q_i", "in", "range", "(", "self", ".", "num_Q", ")", ":", "\n", "            ", "q_prediction", "=", "self", ".", "q_net_list", "[", "q_i", "]", "(", "torch", ".", "cat", "(", "[", "obs_tensor", ",", "acts_tensor", "]", ",", "1", ")", ")", "\n", "q_prediction_list", ".", "append", "(", "q_prediction", ")", "\n", "", "q_prediction_cat", "=", "torch", ".", "cat", "(", "q_prediction_list", ",", "dim", "=", "1", ")", "\n", "average_q_prediction", "=", "torch", ".", "mean", "(", "q_prediction_cat", ",", "dim", "=", "1", ")", "\n", "return", "average_q_prediction", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.redq_sac.REDQSACAgent.store_data": [[142, 145], ["redq_sac.REDQSACAgent.replay_buffer.store"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.EpochLogger.store"], ["", "def", "store_data", "(", "self", ",", "o", ",", "a", ",", "r", ",", "o2", ",", "d", ")", ":", "\n", "# store one transition to the buffer", "\n", "        ", "self", ".", "replay_buffer", ".", "store", "(", "o", ",", "a", ",", "r", ",", "o2", ",", "d", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.redq_sac.REDQSACAgent.sample_data": [[146, 155], ["redq_sac.REDQSACAgent.replay_buffer.sample_batch", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().unsqueeze().to", "torch.Tensor().unsqueeze().to", "torch.Tensor().unsqueeze().to", "torch.Tensor().unsqueeze().to", "torch.Tensor().unsqueeze().to", "torch.Tensor().unsqueeze().to", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor().unsqueeze", "torch.Tensor().unsqueeze", "torch.Tensor().unsqueeze", "torch.Tensor().unsqueeze", "torch.Tensor().unsqueeze", "torch.Tensor().unsqueeze", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.ReplayBuffer.sample_batch"], ["", "def", "sample_data", "(", "self", ",", "batch_size", ")", ":", "\n", "# sample data from replay buffer", "\n", "        ", "batch", "=", "self", ".", "replay_buffer", ".", "sample_batch", "(", "batch_size", ")", "\n", "obs_tensor", "=", "Tensor", "(", "batch", "[", "'obs1'", "]", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "obs_next_tensor", "=", "Tensor", "(", "batch", "[", "'obs2'", "]", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "acts_tensor", "=", "Tensor", "(", "batch", "[", "'acts'", "]", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "rews_tensor", "=", "Tensor", "(", "batch", "[", "'rews'", "]", ")", ".", "unsqueeze", "(", "1", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "done_tensor", "=", "Tensor", "(", "batch", "[", "'done'", "]", ")", ".", "unsqueeze", "(", "1", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "return", "obs_tensor", ",", "obs_next_tensor", ",", "acts_tensor", ",", "rews_tensor", ",", "done_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.redq_sac.REDQSACAgent.get_redq_q_target_no_grad": [[156, 199], ["redq_sac.get_probabilistic_num_min", "numpy.random.choice", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "redq_sac.REDQSACAgent.policy_net.forward", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "redq_sac.REDQSACAgent.policy_net.forward", "range", "torch.cat().mean().reshape", "torch.cat().mean().reshape", "torch.cat().mean().reshape", "torch.cat().mean().reshape", "torch.cat().mean().reshape", "torch.cat().mean().reshape", "torch.cat().mean().reshape", "torch.cat().mean().reshape", "torch.cat().mean().reshape", "redq_sac.REDQSACAgent.policy_net.forward", "range", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to.sum().reshape().expand", "q_prediction_next_list.append", "q_prediction_next_list.append", "q_prediction_next_list.append", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat().mean", "torch.cat().mean", "torch.cat().mean", "torch.cat().mean", "torch.cat().mean", "torch.cat().mean", "torch.cat().mean", "torch.cat().mean", "torch.cat().mean", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor().to.sum().reshape", "numpy.random.uniform", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.Tensor().to.sum"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.redq_sac.get_probabilistic_num_min", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.TanhGaussianPolicy.forward", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.TanhGaussianPolicy.forward", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.TanhGaussianPolicy.forward", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.RunningMeanStats.append", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.RunningMeanStats.append", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.RunningMeanStats.append"], ["", "def", "get_redq_q_target_no_grad", "(", "self", ",", "obs_next_tensor", ",", "rews_tensor", ",", "done_tensor", ")", ":", "\n", "# compute REDQ Q target, depending on the agent's Q target mode", "\n", "# allow min as a float:", "\n", "        ", "num_mins_to_use", "=", "get_probabilistic_num_min", "(", "self", ".", "num_min", ")", "\n", "sample_idxs", "=", "np", ".", "random", ".", "choice", "(", "self", ".", "num_Q", ",", "num_mins_to_use", ",", "replace", "=", "False", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "if", "self", ".", "q_target_mode", "==", "'min'", ":", "\n", "                ", "\"\"\"Q target is min of a subset of Q values\"\"\"", "\n", "a_tilda_next", ",", "_", ",", "_", ",", "log_prob_a_tilda_next", ",", "_", ",", "_", "=", "self", ".", "policy_net", ".", "forward", "(", "obs_next_tensor", ")", "\n", "q_prediction_next_list", "=", "[", "]", "\n", "for", "sample_idx", "in", "sample_idxs", ":", "\n", "                    ", "q_prediction_next", "=", "self", ".", "q_target_net_list", "[", "sample_idx", "]", "(", "torch", ".", "cat", "(", "[", "obs_next_tensor", ",", "a_tilda_next", "]", ",", "1", ")", ")", "\n", "q_prediction_next_list", ".", "append", "(", "q_prediction_next", ")", "\n", "", "q_prediction_next_cat", "=", "torch", ".", "cat", "(", "q_prediction_next_list", ",", "1", ")", "\n", "min_q", ",", "min_indices", "=", "torch", ".", "min", "(", "q_prediction_next_cat", ",", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "\n", "next_q_with_log_prob", "=", "min_q", "-", "self", ".", "alpha", "*", "log_prob_a_tilda_next", "\n", "y_q", "=", "rews_tensor", "+", "self", ".", "gamma", "*", "(", "1", "-", "done_tensor", ")", "*", "next_q_with_log_prob", "\n", "", "if", "self", ".", "q_target_mode", "==", "'ave'", ":", "\n", "                ", "\"\"\"Q target is average of all Q values\"\"\"", "\n", "a_tilda_next", ",", "_", ",", "_", ",", "log_prob_a_tilda_next", ",", "_", ",", "_", "=", "self", ".", "policy_net", ".", "forward", "(", "obs_next_tensor", ")", "\n", "q_prediction_next_list", "=", "[", "]", "\n", "for", "q_i", "in", "range", "(", "self", ".", "num_Q", ")", ":", "\n", "                    ", "q_prediction_next", "=", "self", ".", "q_target_net_list", "[", "q_i", "]", "(", "torch", ".", "cat", "(", "[", "obs_next_tensor", ",", "a_tilda_next", "]", ",", "1", ")", ")", "\n", "q_prediction_next_list", ".", "append", "(", "q_prediction_next", ")", "\n", "", "q_prediction_next_ave", "=", "torch", ".", "cat", "(", "q_prediction_next_list", ",", "1", ")", ".", "mean", "(", "dim", "=", "1", ")", ".", "reshape", "(", "-", "1", ",", "1", ")", "\n", "next_q_with_log_prob", "=", "q_prediction_next_ave", "-", "self", ".", "alpha", "*", "log_prob_a_tilda_next", "\n", "y_q", "=", "rews_tensor", "+", "self", ".", "gamma", "*", "(", "1", "-", "done_tensor", ")", "*", "next_q_with_log_prob", "\n", "", "if", "self", ".", "q_target_mode", "==", "'rem'", ":", "\n", "                ", "\"\"\"Q target is random ensemble mixture of Q values\"\"\"", "\n", "a_tilda_next", ",", "_", ",", "_", ",", "log_prob_a_tilda_next", ",", "_", ",", "_", "=", "self", ".", "policy_net", ".", "forward", "(", "obs_next_tensor", ")", "\n", "q_prediction_next_list", "=", "[", "]", "\n", "for", "q_i", "in", "range", "(", "self", ".", "num_Q", ")", ":", "\n", "                    ", "q_prediction_next", "=", "self", ".", "q_target_net_list", "[", "q_i", "]", "(", "torch", ".", "cat", "(", "[", "obs_next_tensor", ",", "a_tilda_next", "]", ",", "1", ")", ")", "\n", "q_prediction_next_list", ".", "append", "(", "q_prediction_next", ")", "\n", "# apply rem here", "\n", "", "q_prediction_next_cat", "=", "torch", ".", "cat", "(", "q_prediction_next_list", ",", "1", ")", "\n", "rem_weight", "=", "Tensor", "(", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ",", "q_prediction_next_cat", ".", "shape", ")", ")", ".", "to", "(", "device", "=", "self", ".", "device", ")", "\n", "normalize_sum", "=", "rem_weight", ".", "sum", "(", "1", ")", ".", "reshape", "(", "-", "1", ",", "1", ")", ".", "expand", "(", "-", "1", ",", "self", ".", "num_Q", ")", "\n", "rem_weight", "=", "rem_weight", "/", "normalize_sum", "\n", "q_prediction_next_rem", "=", "(", "q_prediction_next_cat", "*", "rem_weight", ")", ".", "sum", "(", "dim", "=", "1", ")", ".", "reshape", "(", "-", "1", ",", "1", ")", "\n", "next_q_with_log_prob", "=", "q_prediction_next_rem", "-", "self", ".", "alpha", "*", "log_prob_a_tilda_next", "\n", "y_q", "=", "rews_tensor", "+", "self", ".", "gamma", "*", "(", "1", "-", "done_tensor", ")", "*", "next_q_with_log_prob", "\n", "", "", "return", "y_q", ",", "sample_idxs", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.redq_sac.REDQSACAgent.train": [[200, 269], ["range", "redq_sac.REDQSACAgent.sample_data", "redq_sac.REDQSACAgent.get_redq_q_target_no_grad", "range", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "range", "q_loss_all.backward", "range", "range", "logger.store", "redq_sac.REDQSACAgent.__get_current_num_data", "q_prediction_list.append", "y_q.expand", "redq_sac.REDQSACAgent.mse_criterion", "redq_sac.REDQSACAgent.q_optimizer_list[].zero_grad", "redq_sac.REDQSACAgent.policy_net.forward", "range", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "redq_sac.REDQSACAgent.policy_optimizer.zero_grad", "policy_loss.backward", "range", "redq_sac.REDQSACAgent.q_optimizer_list[].step", "redq_sac.REDQSACAgent.policy_optimizer.step", "redq.algos.core.soft_update_model1_with_model2", "logger.store", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "redq_sac.REDQSACAgent.q_net_list[].requires_grad_", "q_a_tilda_list.append", "redq_sac.REDQSACAgent.q_net_list[].requires_grad_", "redq_sac.REDQSACAgent.alpha_optim.zero_grad", "torch.Tensor.backward", "torch.Tensor.backward", "torch.Tensor.backward", "redq_sac.REDQSACAgent.alpha_optim.step", "redq_sac.REDQSACAgent.log_alpha.cpu().exp().item", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "policy_loss.cpu().item", "torch.Tensor.cpu().item", "torch.Tensor.cpu().item", "torch.Tensor.cpu().item", "q_prediction.detach().cpu().numpy", "log_prob_a_tilda.detach().cpu().numpy", "pretanh.abs().detach().cpu().numpy().reshape", "redq_sac.REDQSACAgent.log_alpha.cpu().exp", "q_loss_all.cpu().item", "policy_loss.cpu", "torch.Tensor.cpu", "torch.Tensor.cpu", "torch.Tensor.cpu", "q_prediction.detach().cpu", "log_prob_a_tilda.detach().cpu", "pretanh.abs().detach().cpu().numpy", "redq_sac.REDQSACAgent.log_alpha.cpu", "q_loss_all.cpu", "q_prediction.detach", "log_prob_a_tilda.detach", "pretanh.abs().detach().cpu", "pretanh.abs().detach", "pretanh.abs"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.redq_sac.REDQSACAgent.sample_data", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.redq_sac.REDQSACAgent.get_redq_q_target_no_grad", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.EpochLogger.store", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.redq_sac.REDQSACAgent.__get_current_num_data", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.RunningMeanStats.append", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.TanhGaussianPolicy.forward", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.customenvs.humanoid.HumanoidTruncatedObsEnv.step", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.customenvs.humanoid.HumanoidTruncatedObsEnv.step", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.core.soft_update_model1_with_model2", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.EpochLogger.store", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.RunningMeanStats.append", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.customenvs.humanoid.HumanoidTruncatedObsEnv.step"], ["", "def", "train", "(", "self", ",", "logger", ")", ":", "\n", "# this function is called after each datapoint collected.", "\n", "# when we only have very limited data, we don't make updates", "\n", "        ", "num_update", "=", "0", "if", "self", ".", "__get_current_num_data", "(", ")", "<=", "self", ".", "delay_update_steps", "else", "self", ".", "utd_ratio", "\n", "for", "i_update", "in", "range", "(", "num_update", ")", ":", "\n", "            ", "obs_tensor", ",", "obs_next_tensor", ",", "acts_tensor", ",", "rews_tensor", ",", "done_tensor", "=", "self", ".", "sample_data", "(", "self", ".", "batch_size", ")", "\n", "\n", "\"\"\"Q loss\"\"\"", "\n", "y_q", ",", "sample_idxs", "=", "self", ".", "get_redq_q_target_no_grad", "(", "obs_next_tensor", ",", "rews_tensor", ",", "done_tensor", ")", "\n", "q_prediction_list", "=", "[", "]", "\n", "for", "q_i", "in", "range", "(", "self", ".", "num_Q", ")", ":", "\n", "                ", "q_prediction", "=", "self", ".", "q_net_list", "[", "q_i", "]", "(", "torch", ".", "cat", "(", "[", "obs_tensor", ",", "acts_tensor", "]", ",", "1", ")", ")", "\n", "q_prediction_list", ".", "append", "(", "q_prediction", ")", "\n", "", "q_prediction_cat", "=", "torch", ".", "cat", "(", "q_prediction_list", ",", "dim", "=", "1", ")", "\n", "y_q", "=", "y_q", ".", "expand", "(", "(", "-", "1", ",", "self", ".", "num_Q", ")", ")", "if", "y_q", ".", "shape", "[", "1", "]", "==", "1", "else", "y_q", "\n", "q_loss_all", "=", "self", ".", "mse_criterion", "(", "q_prediction_cat", ",", "y_q", ")", "*", "self", ".", "num_Q", "\n", "\n", "for", "q_i", "in", "range", "(", "self", ".", "num_Q", ")", ":", "\n", "                ", "self", ".", "q_optimizer_list", "[", "q_i", "]", ".", "zero_grad", "(", ")", "\n", "", "q_loss_all", ".", "backward", "(", ")", "\n", "\n", "\"\"\"policy and alpha loss\"\"\"", "\n", "if", "(", "(", "i_update", "+", "1", ")", "%", "self", ".", "policy_update_delay", "==", "0", ")", "or", "i_update", "==", "num_update", "-", "1", ":", "\n", "# get policy loss", "\n", "                ", "a_tilda", ",", "mean_a_tilda", ",", "log_std_a_tilda", ",", "log_prob_a_tilda", ",", "_", ",", "pretanh", "=", "self", ".", "policy_net", ".", "forward", "(", "obs_tensor", ")", "\n", "q_a_tilda_list", "=", "[", "]", "\n", "for", "sample_idx", "in", "range", "(", "self", ".", "num_Q", ")", ":", "\n", "                    ", "self", ".", "q_net_list", "[", "sample_idx", "]", ".", "requires_grad_", "(", "False", ")", "\n", "q_a_tilda", "=", "self", ".", "q_net_list", "[", "sample_idx", "]", "(", "torch", ".", "cat", "(", "[", "obs_tensor", ",", "a_tilda", "]", ",", "1", ")", ")", "\n", "q_a_tilda_list", ".", "append", "(", "q_a_tilda", ")", "\n", "", "q_a_tilda_cat", "=", "torch", ".", "cat", "(", "q_a_tilda_list", ",", "1", ")", "\n", "ave_q", "=", "torch", ".", "mean", "(", "q_a_tilda_cat", ",", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "\n", "policy_loss", "=", "(", "self", ".", "alpha", "*", "log_prob_a_tilda", "-", "ave_q", ")", ".", "mean", "(", ")", "\n", "self", ".", "policy_optimizer", ".", "zero_grad", "(", ")", "\n", "policy_loss", ".", "backward", "(", ")", "\n", "for", "sample_idx", "in", "range", "(", "self", ".", "num_Q", ")", ":", "\n", "                    ", "self", ".", "q_net_list", "[", "sample_idx", "]", ".", "requires_grad_", "(", "True", ")", "\n", "\n", "# get alpha loss", "\n", "", "if", "self", ".", "auto_alpha", ":", "\n", "                    ", "alpha_loss", "=", "-", "(", "self", ".", "log_alpha", "*", "(", "log_prob_a_tilda", "+", "self", ".", "target_entropy", ")", ".", "detach", "(", ")", ")", ".", "mean", "(", ")", "\n", "self", ".", "alpha_optim", ".", "zero_grad", "(", ")", "\n", "alpha_loss", ".", "backward", "(", ")", "\n", "self", ".", "alpha_optim", ".", "step", "(", ")", "\n", "self", ".", "alpha", "=", "self", ".", "log_alpha", ".", "cpu", "(", ")", ".", "exp", "(", ")", ".", "item", "(", ")", "\n", "", "else", ":", "\n", "                    ", "alpha_loss", "=", "Tensor", "(", "[", "0", "]", ")", "\n", "\n", "", "", "\"\"\"update networks\"\"\"", "\n", "for", "q_i", "in", "range", "(", "self", ".", "num_Q", ")", ":", "\n", "                ", "self", ".", "q_optimizer_list", "[", "q_i", "]", ".", "step", "(", ")", "\n", "\n", "", "if", "(", "(", "i_update", "+", "1", ")", "%", "self", ".", "policy_update_delay", "==", "0", ")", "or", "i_update", "==", "num_update", "-", "1", ":", "\n", "                ", "self", ".", "policy_optimizer", ".", "step", "(", ")", "\n", "\n", "# polyak averaged Q target networks", "\n", "", "for", "q_i", "in", "range", "(", "self", ".", "num_Q", ")", ":", "\n", "                ", "soft_update_model1_with_model2", "(", "self", ".", "q_target_net_list", "[", "q_i", "]", ",", "self", ".", "q_net_list", "[", "q_i", "]", ",", "self", ".", "polyak", ")", "\n", "\n", "# by default only log for the last update out of <num_update> updates", "\n", "", "if", "i_update", "==", "num_update", "-", "1", ":", "\n", "                ", "logger", ".", "store", "(", "LossPi", "=", "policy_loss", ".", "cpu", "(", ")", ".", "item", "(", ")", ",", "LossQ1", "=", "q_loss_all", ".", "cpu", "(", ")", ".", "item", "(", ")", "/", "self", ".", "num_Q", ",", "\n", "LossAlpha", "=", "alpha_loss", ".", "cpu", "(", ")", ".", "item", "(", ")", ",", "Q1Vals", "=", "q_prediction", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "\n", "Alpha", "=", "self", ".", "alpha", ",", "LogPi", "=", "log_prob_a_tilda", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "\n", "PreTanh", "=", "pretanh", ".", "abs", "(", ")", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "reshape", "(", "-", "1", ")", ")", "\n", "\n", "# if there is no update, log 0 to prevent logging problems", "\n", "", "", "if", "num_update", "==", "0", ":", "\n", "            ", "logger", ".", "store", "(", "LossPi", "=", "0", ",", "LossQ1", "=", "0", ",", "LossAlpha", "=", "0", ",", "Q1Vals", "=", "0", ",", "Alpha", "=", "0", ",", "LogPi", "=", "0", ",", "PreTanh", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.redq_sac.get_probabilistic_num_min": [[9, 20], ["numpy.floor", "numpy.random.uniform", "int", "int"], "function", ["None"], ["def", "get_probabilistic_num_min", "(", "num_mins", ")", ":", "\n", "# allows the number of min to be a float", "\n", "    ", "floored_num_mins", "=", "np", ".", "floor", "(", "num_mins", ")", "\n", "if", "num_mins", "-", "floored_num_mins", ">", "0.001", ":", "\n", "        ", "prob_for_higher_value", "=", "num_mins", "-", "floored_num_mins", "\n", "if", "np", ".", "random", ".", "uniform", "(", "0", ",", "1", ")", "<", "prob_for_higher_value", ":", "\n", "            ", "return", "int", "(", "floored_num_mins", "+", "1", ")", "\n", "", "else", ":", "\n", "            ", "return", "int", "(", "floored_num_mins", ")", "\n", "", "", "else", ":", "\n", "        ", "return", "num_mins", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.run_utils.setup_logger_kwargs": [[10, 72], ["dict", "time.strftime", "os.join", "time.strftime", "os.join", "str", "str"], "function", ["None"], ["def", "setup_logger_kwargs", "(", "exp_name", ",", "seed", "=", "None", ",", "data_dir", "=", "None", ",", "datestamp", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Sets up the output_dir for a logger and returns a dict for logger kwargs.\n\n    If no seed is given and datestamp is false, \n\n    ::\n\n        output_dir = data_dir/exp_name\n\n    If a seed is given and datestamp is false,\n\n    ::\n\n        output_dir = data_dir/exp_name/exp_name_s[seed]\n\n    If datestamp is true, amend to\n\n    ::\n\n        output_dir = data_dir/YY-MM-DD_exp_name/YY-MM-DD_HH-MM-SS_exp_name_s[seed]\n\n    You can force datestamp=True by setting ``FORCE_DATESTAMP=True`` in \n    ``spinup/user_config.py``. \n\n    Args:\n\n        exp_name (string): Name for experiment.\n\n        seed (int): Seed for random number generators used by experiment.\n\n        data_dir (string): Path to folder where results should be saved.\n            Default is the ``DEFAULT_DATA_DIR`` in ``spinup/user_config.py``.\n\n        datestamp (bool): Whether to include a date and timestamp in the\n            name of the save directory.\n\n    Returns:\n\n        logger_kwargs, a dict containing output_dir and exp_name.\n    \"\"\"", "\n", "\n", "# Datestamp forcing", "\n", "datestamp", "=", "datestamp", "or", "FORCE_DATESTAMP", "\n", "\n", "# Make base path", "\n", "ymd_time", "=", "time", ".", "strftime", "(", "\"%Y-%m-%d_\"", ")", "if", "datestamp", "else", "''", "\n", "relpath", "=", "''", ".", "join", "(", "[", "ymd_time", ",", "exp_name", "]", ")", "\n", "\n", "if", "seed", "is", "not", "None", ":", "\n", "# Make a seed-specific subfolder in the experiment directory.", "\n", "        ", "if", "datestamp", ":", "\n", "            ", "hms_time", "=", "time", ".", "strftime", "(", "\"%Y-%m-%d_%H-%M-%S\"", ")", "\n", "subfolder", "=", "''", ".", "join", "(", "[", "hms_time", ",", "'-'", ",", "exp_name", ",", "'_s'", ",", "str", "(", "seed", ")", "]", ")", "\n", "", "else", ":", "\n", "            ", "subfolder", "=", "''", ".", "join", "(", "[", "exp_name", ",", "'_s'", ",", "str", "(", "seed", ")", "]", ")", "\n", "", "relpath", "=", "osp", ".", "join", "(", "relpath", ",", "subfolder", ")", "\n", "\n", "", "data_dir", "=", "data_dir", "or", "DEFAULT_DATA_DIR", "\n", "logger_kwargs", "=", "dict", "(", "output_dir", "=", "osp", ".", "join", "(", "data_dir", ",", "relpath", ")", ",", "\n", "exp_name", "=", "exp_name", ")", "\n", "return", "logger_kwargs", "\n", "", ""]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.Logger.__init__": [[45, 76], ["os.exists", "os.exists", "open", "atexit.register", "print", "print", "os.makedirs", "os.makedirs", "os.makedirs", "os.makedirs", "os.join", "os.join", "logx.colorize", "int", "time.time"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.colorize"], ["def", "__init__", "(", "self", ",", "output_dir", "=", "None", ",", "output_fname", "=", "'progress.txt'", ",", "exp_name", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Initialize a Logger.\n\n        Args:\n            output_dir (string): A directory for saving results to. If \n                ``None``, defaults to a temp directory of the form\n                ``/tmp/experiments/somerandomnumber``.\n\n            output_fname (string): Name for the tab-separated-value file \n                containing metrics logged throughout a training run. \n                Defaults to ``progress.txt``. \n\n            exp_name (string): Experiment name. If you run multiple training\n                runs and give them all the same ``exp_name``, the plotter\n                will know to group them. (Use case: if you run the same\n                hyperparameter configuration with multiple random seeds, you\n                should give them all the same ``exp_name``.)\n        \"\"\"", "\n", "self", ".", "output_dir", "=", "output_dir", "or", "\"/tmp/experiments/%i\"", "%", "int", "(", "time", ".", "time", "(", ")", ")", "\n", "if", "osp", ".", "exists", "(", "self", ".", "output_dir", ")", ":", "\n", "            ", "print", "(", "\"Warning: Log dir %s already exists! Storing info there anyway.\"", "%", "self", ".", "output_dir", ")", "\n", "", "else", ":", "\n", "            ", "os", ".", "makedirs", "(", "self", ".", "output_dir", ")", "\n", "", "self", ".", "output_file", "=", "open", "(", "osp", ".", "join", "(", "self", ".", "output_dir", ",", "output_fname", ")", ",", "'w'", ")", "\n", "atexit", ".", "register", "(", "self", ".", "output_file", ".", "close", ")", "\n", "print", "(", "colorize", "(", "\"Logging data to %s\"", "%", "self", ".", "output_file", ".", "name", ",", "'green'", ",", "bold", "=", "True", ")", ")", "\n", "self", ".", "first_row", "=", "True", "\n", "self", ".", "log_headers", "=", "[", "]", "\n", "self", ".", "log_current_row", "=", "{", "}", "\n", "self", ".", "exp_name", "=", "exp_name", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.Logger.log": [[77, 80], ["print", "logx.colorize"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.colorize"], ["", "def", "log", "(", "self", ",", "msg", ",", "color", "=", "'green'", ")", ":", "\n", "        ", "\"\"\"Print a colorized message to stdout.\"\"\"", "\n", "print", "(", "colorize", "(", "msg", ",", "color", ",", "bold", "=", "True", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.Logger.log_tabular": [[81, 96], ["logx.Logger.log_headers.append"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.RunningMeanStats.append"], ["", "def", "log_tabular", "(", "self", ",", "key", ",", "val", ")", ":", "\n", "        ", "\"\"\"\n        Log a value of some diagnostic.\n\n        Call this only once for each diagnostic quantity, each iteration.\n        After using ``log_tabular`` to store values for each diagnostic,\n        make sure to call ``dump_tabular`` to write them out to file and\n        stdout (otherwise they will not get saved anywhere).\n        \"\"\"", "\n", "if", "self", ".", "first_row", ":", "\n", "            ", "self", ".", "log_headers", ".", "append", "(", "key", ")", "\n", "", "else", ":", "\n", "            ", "assert", "key", "in", "self", ".", "log_headers", ",", "\"Trying to introduce a new key %s that you didn't include in the first iteration\"", "%", "key", "\n", "", "assert", "key", "not", "in", "self", ".", "log_current_row", ",", "\"You already set %s this iteration. Maybe you forgot to call dump_tabular()\"", "%", "key", "\n", "self", ".", "log_current_row", "[", "key", "]", "=", "val", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.Logger.save_config": [[97, 121], ["redq.utils.serialization_utils.convert_json", "json.dumps", "print", "print", "logx.colorize", "open", "out.write", "os.join", "os.join"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.serialization_utils.convert_json", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.colorize"], ["", "def", "save_config", "(", "self", ",", "config", ")", ":", "\n", "        ", "\"\"\"\n        Log an experiment configuration.\n\n        Call this once at the top of your experiment, passing in all important\n        config vars as a dict. This will serialize the config to JSON, while\n        handling anything which can't be serialized in a graceful way (writing\n        as informative a string as possible). \n\n        Example use:\n\n        .. code-block:: python\n\n            logger = EpochLogger(**logger_kwargs)\n            logger.save_config(locals())\n        \"\"\"", "\n", "config_json", "=", "convert_json", "(", "config", ")", "\n", "if", "self", ".", "exp_name", "is", "not", "None", ":", "\n", "            ", "config_json", "[", "'exp_name'", "]", "=", "self", ".", "exp_name", "\n", "", "output", "=", "json", ".", "dumps", "(", "config_json", ",", "separators", "=", "(", "','", ",", "':\\t'", ")", ",", "indent", "=", "4", ",", "sort_keys", "=", "True", ")", "\n", "print", "(", "colorize", "(", "'Saving config:\\n'", ",", "color", "=", "'cyan'", ",", "bold", "=", "True", ")", ")", "\n", "print", "(", "output", ")", "\n", "with", "open", "(", "osp", ".", "join", "(", "self", ".", "output_dir", ",", "\"config.json\"", ")", ",", "'w'", ")", "as", "out", ":", "\n", "            ", "out", ".", "write", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.Logger.save_state": [[122, 148], ["joblib.dump", "os.join", "os.join", "logx.Logger.log"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.Logger.log"], ["", "", "def", "save_state", "(", "self", ",", "state_dict", ",", "itr", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Saves the state of an experiment.\n\n        To be clear: this is about saving *state*, not logging diagnostics.\n        All diagnostic logging is separate from this function. This function\n        will save whatever is in ``state_dict``---usually just a copy of the\n        environment---and the most recent parameters for the model you \n        previously set up saving for with ``setup_tf_saver``. \n\n        Call with any frequency you prefer. If you only want to maintain a\n        single state and overwrite it at each call with the most recent \n        version, leave ``itr=None``. If you want to keep all of the states you\n        save, provide unique (increasing) values for 'itr'.\n\n        Args:\n            state_dict (dict): Dictionary containing essential elements to\n                describe the current state of training.\n\n            itr: An int, or None. Current iteration of training.\n        \"\"\"", "\n", "fname", "=", "'vars.pkl'", "if", "itr", "is", "None", "else", "'vars%d.pkl'", "%", "itr", "\n", "try", ":", "\n", "            ", "joblib", ".", "dump", "(", "state_dict", ",", "osp", ".", "join", "(", "self", ".", "output_dir", ",", "fname", ")", ")", "\n", "", "except", ":", "\n", "            ", "self", ".", "log", "(", "'Warning: could not pickle state_dict.'", ",", "color", "=", "'red'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.Logger.dump_tabular": [[149, 175], ["max", "print", "print", "logx.Logger.log_current_row.clear", "len", "max", "logx.Logger.log_current_row.get", "print", "vals.append", "logx.Logger.output_file.write", "logx.Logger.output_file.flush", "hasattr", "logx.Logger.output_file.write", "map"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.RunningMeanStats.get", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.RunningMeanStats.append"], ["", "", "def", "dump_tabular", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Write all of the diagnostics from the current iteration.\n\n        Writes both to stdout, and to the output file.\n        \"\"\"", "\n", "vals", "=", "[", "]", "\n", "key_lens", "=", "[", "len", "(", "key", ")", "for", "key", "in", "self", ".", "log_headers", "]", "\n", "max_key_len", "=", "max", "(", "15", ",", "max", "(", "key_lens", ")", ")", "\n", "keystr", "=", "'%'", "+", "'%d'", "%", "max_key_len", "\n", "fmt", "=", "\"| \"", "+", "keystr", "+", "\"s | %15s |\"", "\n", "n_slashes", "=", "22", "+", "max_key_len", "\n", "print", "(", "\"-\"", "*", "n_slashes", ")", "\n", "for", "key", "in", "self", ".", "log_headers", ":", "\n", "            ", "val", "=", "self", ".", "log_current_row", ".", "get", "(", "key", ",", "\"\"", ")", "\n", "valstr", "=", "\"%8.3g\"", "%", "val", "if", "hasattr", "(", "val", ",", "\"__float__\"", ")", "else", "val", "\n", "print", "(", "fmt", "%", "(", "key", ",", "valstr", ")", ")", "\n", "vals", ".", "append", "(", "val", ")", "\n", "", "print", "(", "\"-\"", "*", "n_slashes", ")", "\n", "if", "self", ".", "output_file", "is", "not", "None", ":", "\n", "            ", "if", "self", ".", "first_row", ":", "\n", "                ", "self", ".", "output_file", ".", "write", "(", "\"\\t\"", ".", "join", "(", "self", ".", "log_headers", ")", "+", "\"\\n\"", ")", "\n", "", "self", ".", "output_file", ".", "write", "(", "\"\\t\"", ".", "join", "(", "map", "(", "str", ",", "vals", ")", ")", "+", "\"\\n\"", ")", "\n", "self", ".", "output_file", ".", "flush", "(", ")", "\n", "", "self", ".", "log_current_row", ".", "clear", "(", ")", "\n", "self", ".", "first_row", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.EpochLogger.__init__": [[220, 223], ["logx.Logger.__init__", "dict"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.EpochLogger.__init__"], ["def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "self", ".", "epoch_dict", "=", "dict", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.EpochLogger.store": [[224, 239], ["kwargs.items", "isinstance", "logx.EpochLogger.epoch_dict[].append", "v.reshape.reshape.reshape"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.RunningMeanStats.append"], ["", "def", "store", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Save something into the epoch_logger's current state.\n\n        Provide an arbitrary number of keyword arguments with numerical \n        values.\n\n        To prevent problems, let value be either a numpy array, or a single scalar value\n        \"\"\"", "\n", "for", "k", ",", "v", "in", "kwargs", ".", "items", "(", ")", ":", "\n", "            ", "if", "not", "k", "in", "self", ".", "epoch_dict", ":", "\n", "                ", "self", ".", "epoch_dict", "[", "k", "]", "=", "[", "]", "\n", "", "if", "isinstance", "(", "v", ",", "np", ".", "ndarray", ")", ":", "# used to prevent problems due to shape issues", "\n", "                ", "v", "=", "v", ".", "reshape", "(", "-", "1", ")", "\n", "", "self", ".", "epoch_dict", "[", "k", "]", ".", "append", "(", "v", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.EpochLogger.log_tabular": [[240, 272], ["logx.Logger.log_tabular", "logx.get_statistics_scalar", "logx.Logger.log_tabular", "numpy.concatenate", "logx.Logger.log_tabular", "logx.Logger.log_tabular", "logx.Logger.log_tabular", "isinstance", "len"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.EpochLogger.log_tabular", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.get_statistics_scalar", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.EpochLogger.log_tabular", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.EpochLogger.log_tabular", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.EpochLogger.log_tabular", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.EpochLogger.log_tabular"], ["", "", "def", "log_tabular", "(", "self", ",", "key", ",", "val", "=", "None", ",", "with_min_and_max", "=", "False", ",", "average_only", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Log a value or possibly the mean/std/min/max values of a diagnostic.\n\n        Args:\n            key (string): The name of the diagnostic. If you are logging a\n                diagnostic whose state has previously been saved with \n                ``store``, the key here has to match the key you used there.\n\n            val: A value for the diagnostic. If you have previously saved\n                values for this key via ``store``, do *not* provide a ``val``\n                here.\n\n            with_min_and_max (bool): If true, log min and max values of the \n                diagnostic over the epoch.\n\n            average_only (bool): If true, do not log the standard deviation\n                of the diagnostic over the epoch.\n        \"\"\"", "\n", "if", "val", "is", "not", "None", ":", "\n", "            ", "super", "(", ")", ".", "log_tabular", "(", "key", ",", "val", ")", "\n", "", "else", ":", "\n", "            ", "v", "=", "self", ".", "epoch_dict", "[", "key", "]", "\n", "vals", "=", "np", ".", "concatenate", "(", "v", ")", "if", "isinstance", "(", "v", "[", "0", "]", ",", "np", ".", "ndarray", ")", "and", "len", "(", "v", "[", "0", "]", ".", "shape", ")", ">", "0", "else", "v", "\n", "stats", "=", "get_statistics_scalar", "(", "vals", ",", "with_min_and_max", "=", "with_min_and_max", ")", "\n", "super", "(", ")", ".", "log_tabular", "(", "key", "if", "average_only", "else", "'Average'", "+", "key", ",", "stats", "[", "0", "]", ")", "\n", "if", "not", "(", "average_only", ")", ":", "\n", "                ", "super", "(", ")", ".", "log_tabular", "(", "'Std'", "+", "key", ",", "stats", "[", "1", "]", ")", "\n", "", "if", "with_min_and_max", ":", "\n", "                ", "super", "(", ")", ".", "log_tabular", "(", "'Max'", "+", "key", ",", "stats", "[", "3", "]", ")", "\n", "super", "(", ")", ".", "log_tabular", "(", "'Min'", "+", "key", ",", "stats", "[", "2", "]", ")", "\n", "", "", "self", ".", "epoch_dict", "[", "key", "]", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.EpochLogger.get_stats": [[273, 280], ["logx.get_statistics_scalar", "numpy.concatenate", "isinstance", "len"], "methods", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.get_statistics_scalar"], ["", "def", "get_stats", "(", "self", ",", "key", ")", ":", "\n", "        ", "\"\"\"\n        Lets an algorithm ask the logger for mean/std/min/max of a diagnostic.\n        \"\"\"", "\n", "v", "=", "self", ".", "epoch_dict", "[", "key", "]", "\n", "vals", "=", "np", ".", "concatenate", "(", "v", ")", "if", "isinstance", "(", "v", "[", "0", "]", ",", "np", ".", "ndarray", ")", "and", "len", "(", "v", "[", "0", "]", ".", "shape", ")", ">", "0", "else", "v", "\n", "return", "get_statistics_scalar", "(", "vals", ",", "with_min_and_max", "=", "True", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.colorize": [[24, 36], ["attr.append", "str", "attr.append"], "function", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.RunningMeanStats.append", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.RunningMeanStats.append"], ["def", "colorize", "(", "string", ",", "color", ",", "bold", "=", "False", ",", "highlight", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Colorize a string.\n\n    This function was originally written by John Schulman.\n    \"\"\"", "\n", "attr", "=", "[", "]", "\n", "num", "=", "color2num", "[", "color", "]", "\n", "if", "highlight", ":", "num", "+=", "10", "\n", "attr", ".", "append", "(", "str", "(", "num", ")", ")", "\n", "if", "bold", ":", "attr", ".", "append", "(", "'1'", ")", "\n", "return", "'\\x1b[%sm%s\\x1b[0m'", "%", "(", "';'", ".", "join", "(", "attr", ")", ",", "string", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.get_statistics_scalar": [[176, 194], ["numpy.array", "np.array.mean", "np.array.std", "np.array.min", "np.array.max"], "function", ["None"], ["", "", "def", "get_statistics_scalar", "(", "x", ",", "with_min_and_max", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Get mean/std and optional min/max of x\n\n    Args:\n        x: An array containing samples of the scalar to produce statistics\n            for.\n\n        with_min_and_max (bool): If true, return min and max of x in\n            addition to mean and std.\n    \"\"\"", "\n", "x", "=", "np", ".", "array", "(", "x", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "mean", ",", "std", "=", "x", ".", "mean", "(", ")", ",", "x", ".", "std", "(", ")", "\n", "if", "with_min_and_max", ":", "\n", "        ", "min_v", "=", "x", ".", "min", "(", ")", "\n", "max_v", "=", "x", ".", "max", "(", ")", "\n", "return", "mean", ",", "std", ",", "min_v", ",", "max_v", "\n", "", "return", "mean", ",", "std", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.serialization_utils.convert_json": [[6, 30], ["serialization_utils.is_json_serializable", "isinstance", "str", "isinstance", "serialization_utils.convert_json", "serialization_utils.convert_json", "isinstance", "obj.items", "serialization_utils.convert_json", "serialization_utils.convert_json", "hasattr", "serialization_utils.convert_json", "hasattr", "serialization_utils.convert_json", "serialization_utils.convert_json", "str", "obj.__dict__.items"], "function", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.serialization_utils.is_json_serializable", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.serialization_utils.convert_json", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.serialization_utils.convert_json", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.serialization_utils.convert_json", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.serialization_utils.convert_json", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.serialization_utils.convert_json", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.serialization_utils.convert_json", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.serialization_utils.convert_json"], ["def", "convert_json", "(", "obj", ")", ":", "\n", "    ", "\"\"\" Convert obj to a version which can be serialized with JSON. \"\"\"", "\n", "if", "is_json_serializable", "(", "obj", ")", ":", "\n", "        ", "return", "obj", "\n", "", "else", ":", "\n", "        ", "if", "isinstance", "(", "obj", ",", "dict", ")", ":", "\n", "            ", "return", "{", "convert_json", "(", "k", ")", ":", "convert_json", "(", "v", ")", "\n", "for", "k", ",", "v", "in", "obj", ".", "items", "(", ")", "}", "\n", "\n", "", "elif", "isinstance", "(", "obj", ",", "tuple", ")", ":", "\n", "            ", "return", "(", "convert_json", "(", "x", ")", "for", "x", "in", "obj", ")", "\n", "\n", "", "elif", "isinstance", "(", "obj", ",", "list", ")", ":", "\n", "            ", "return", "[", "convert_json", "(", "x", ")", "for", "x", "in", "obj", "]", "\n", "\n", "", "elif", "hasattr", "(", "obj", ",", "'__name__'", ")", "and", "not", "(", "'lambda'", "in", "obj", ".", "__name__", ")", ":", "\n", "            ", "return", "convert_json", "(", "obj", ".", "__name__", ")", "\n", "\n", "", "elif", "hasattr", "(", "obj", ",", "'__dict__'", ")", "and", "obj", ".", "__dict__", ":", "\n", "            ", "obj_dict", "=", "{", "convert_json", "(", "k", ")", ":", "convert_json", "(", "v", ")", "\n", "for", "k", ",", "v", "in", "obj", ".", "__dict__", ".", "items", "(", ")", "}", "\n", "return", "{", "str", "(", "obj", ")", ":", "obj_dict", "}", "\n", "\n", "", "return", "str", "(", "obj", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.serialization_utils.is_json_serializable": [[31, 37], ["json.dumps"], "function", ["None"], ["", "", "def", "is_json_serializable", "(", "v", ")", ":", "\n", "    ", "try", ":", "\n", "        ", "json", ".", "dumps", "(", "v", ")", "\n", "return", "True", "\n", "", "except", ":", "\n", "        ", "return", "False", "", "", "", ""]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.bias_utils.get_mc_return_with_entropy_on_reset": [[5, 49], ["numpy.zeros", "numpy.zeros", "bias_eval_env.reset", "range", "numpy.zeros", "numpy.zeros", "range", "numpy.concatenate", "numpy.concatenate", "numpy.array", "numpy.array", "obs_list.append", "act_list.append", "bias_eval_env.step", "reward_list.append", "log_prob_a_tilda_list.append", "torch.no_grad", "agent.get_action_and_logprob_for_bias_evaluation", "log_prob_a_tilda.item"], "function", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.util.utilsTH.SparseRewardEnv.reset", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.RunningMeanStats.append", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.RunningMeanStats.append", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.customenvs.humanoid.HumanoidTruncatedObsEnv.step", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.RunningMeanStats.append", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.code.utils.RunningMeanStats.append", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.redq_sac.REDQSACAgent.get_action_and_logprob_for_bias_evaluation"], ["def", "get_mc_return_with_entropy_on_reset", "(", "bias_eval_env", ",", "agent", ",", "max_ep_len", ",", "alpha", ",", "gamma", ",", "n_mc_eval", ",", "n_mc_cutoff", ")", ":", "\n", "# since we want to also compute bias, so we need to", "\n", "    ", "final_mc_list", "=", "np", ".", "zeros", "(", "0", ")", "\n", "final_mc_entropy_list", "=", "np", ".", "zeros", "(", "0", ")", "\n", "final_obs_list", "=", "[", "]", "\n", "final_act_list", "=", "[", "]", "\n", "while", "final_mc_list", ".", "shape", "[", "0", "]", "<", "n_mc_eval", ":", "\n", "# we continue if haven't collected enough data", "\n", "        ", "o", "=", "bias_eval_env", ".", "reset", "(", ")", "\n", "# temporary lists", "\n", "reward_list", ",", "log_prob_a_tilda_list", ",", "obs_list", ",", "act_list", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "r", ",", "d", ",", "ep_ret", ",", "ep_len", "=", "0", ",", "False", ",", "0", ",", "0", "\n", "discounted_return", "=", "0", "\n", "discounted_return_with_entropy", "=", "0", "\n", "for", "i_step", "in", "range", "(", "max_ep_len", ")", ":", "# run an episode", "\n", "            ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "a", ",", "log_prob_a_tilda", "=", "agent", ".", "get_action_and_logprob_for_bias_evaluation", "(", "o", ")", "\n", "", "obs_list", ".", "append", "(", "o", ")", "\n", "act_list", ".", "append", "(", "a", ")", "\n", "o", ",", "r", ",", "d", ",", "_", "=", "bias_eval_env", ".", "step", "(", "a", ")", "\n", "ep_ret", "+=", "r", "\n", "ep_len", "+=", "1", "\n", "reward_list", ".", "append", "(", "r", ")", "\n", "log_prob_a_tilda_list", ".", "append", "(", "log_prob_a_tilda", ".", "item", "(", ")", ")", "\n", "if", "d", "or", "(", "ep_len", "==", "max_ep_len", ")", ":", "\n", "                ", "break", "\n", "", "", "discounted_return_list", "=", "np", ".", "zeros", "(", "ep_len", ")", "\n", "discounted_return_with_entropy_list", "=", "np", ".", "zeros", "(", "ep_len", ")", "\n", "for", "i_step", "in", "range", "(", "ep_len", "-", "1", ",", "-", "1", ",", "-", "1", ")", ":", "\n", "# backwards compute discounted return and with entropy for all s-a visited", "\n", "            ", "if", "i_step", "==", "ep_len", "-", "1", ":", "\n", "                ", "discounted_return_list", "[", "i_step", "]", "=", "reward_list", "[", "i_step", "]", "\n", "discounted_return_with_entropy_list", "[", "i_step", "]", "=", "reward_list", "[", "i_step", "]", "\n", "", "else", ":", "\n", "                ", "discounted_return_list", "[", "i_step", "]", "=", "reward_list", "[", "i_step", "]", "+", "gamma", "*", "discounted_return_list", "[", "i_step", "+", "1", "]", "\n", "discounted_return_with_entropy_list", "[", "i_step", "]", "=", "reward_list", "[", "i_step", "]", "+", "gamma", "*", "(", "discounted_return_with_entropy_list", "[", "i_step", "+", "1", "]", "-", "alpha", "*", "log_prob_a_tilda_list", "[", "i_step", "+", "1", "]", ")", "\n", "# now we take the first few of these.", "\n", "", "", "final_mc_list", "=", "np", ".", "concatenate", "(", "(", "final_mc_list", ",", "discounted_return_list", "[", ":", "n_mc_cutoff", "]", ")", ")", "\n", "final_mc_entropy_list", "=", "np", ".", "concatenate", "(", "\n", "(", "final_mc_entropy_list", ",", "discounted_return_with_entropy_list", "[", ":", "n_mc_cutoff", "]", ")", ")", "\n", "final_obs_list", "+=", "obs_list", "[", ":", "n_mc_cutoff", "]", "\n", "final_act_list", "+=", "act_list", "[", ":", "n_mc_cutoff", "]", "\n", "", "return", "final_mc_list", ",", "final_mc_entropy_list", ",", "np", ".", "array", "(", "final_obs_list", ")", ",", "np", ".", "array", "(", "final_act_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.bias_utils.log_bias_evaluation": [[50, 72], ["bias_utils.get_mc_return_with_entropy_on_reset", "logger.store", "logger.store", "torch.Tensor().to", "torch.Tensor().to", "numpy.abs", "logger.store", "logger.store", "logger.store", "logger.store", "final_mc_entropy_list.copy", "numpy.abs", "logger.store", "logger.store", "torch.no_grad", "agent.get_ave_q_prediction_for_bias_evaluation().cpu().numpy().reshape", "torch.Tensor", "torch.Tensor", "agent.get_ave_q_prediction_for_bias_evaluation().cpu().numpy", "agent.get_ave_q_prediction_for_bias_evaluation().cpu", "agent.get_ave_q_prediction_for_bias_evaluation"], "function", ["home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.bias_utils.get_mc_return_with_entropy_on_reset", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.EpochLogger.store", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.EpochLogger.store", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.EpochLogger.store", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.EpochLogger.store", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.EpochLogger.store", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.EpochLogger.store", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.EpochLogger.store", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.utils.logx.EpochLogger.store", "home.repos.pwc.inspect_result.TakuyaHiraoka_Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning.algos.redq_sac.REDQSACAgent.get_ave_q_prediction_for_bias_evaluation"], ["", "def", "log_bias_evaluation", "(", "bias_eval_env", ",", "agent", ",", "logger", ",", "max_ep_len", ",", "alpha", ",", "gamma", ",", "n_mc_eval", ",", "n_mc_cutoff", ")", ":", "\n", "    ", "final_mc_list", ",", "final_mc_entropy_list", ",", "final_obs_list", ",", "final_act_list", "=", "get_mc_return_with_entropy_on_reset", "(", "bias_eval_env", ",", "agent", ",", "max_ep_len", ",", "alpha", ",", "gamma", ",", "n_mc_eval", ",", "n_mc_cutoff", ")", "\n", "logger", ".", "store", "(", "MCDisRet", "=", "final_mc_list", ")", "\n", "logger", ".", "store", "(", "MCDisRetEnt", "=", "final_mc_entropy_list", ")", "\n", "obs_tensor", "=", "Tensor", "(", "final_obs_list", ")", ".", "to", "(", "agent", ".", "device", ")", "\n", "acts_tensor", "=", "Tensor", "(", "final_act_list", ")", ".", "to", "(", "agent", ".", "device", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "q_prediction", "=", "agent", ".", "get_ave_q_prediction_for_bias_evaluation", "(", "obs_tensor", ",", "acts_tensor", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "reshape", "(", "-", "1", ")", "\n", "", "bias", "=", "q_prediction", "-", "final_mc_entropy_list", "\n", "bias_abs", "=", "np", ".", "abs", "(", "bias", ")", "\n", "bias_squared", "=", "bias", "**", "2", "\n", "logger", ".", "store", "(", "QPred", "=", "q_prediction", ")", "\n", "logger", ".", "store", "(", "QBias", "=", "bias", ")", "\n", "logger", ".", "store", "(", "QBiasAbs", "=", "bias_abs", ")", "\n", "logger", ".", "store", "(", "QBiasSqr", "=", "bias_squared", ")", "\n", "final_mc_entropy_list_normalize_base", "=", "final_mc_entropy_list", ".", "copy", "(", ")", "\n", "final_mc_entropy_list_normalize_base", "=", "np", ".", "abs", "(", "final_mc_entropy_list_normalize_base", ")", "\n", "final_mc_entropy_list_normalize_base", "[", "final_mc_entropy_list_normalize_base", "<", "10", "]", "=", "10", "\n", "normalized_bias_per_state", "=", "bias", "/", "final_mc_entropy_list_normalize_base", "\n", "logger", ".", "store", "(", "NormQBias", "=", "normalized_bias_per_state", ")", "\n", "normalized_bias_sqr_per_state", "=", "bias_squared", "/", "final_mc_entropy_list_normalize_base", "\n", "logger", ".", "store", "(", "NormQBiasSqr", "=", "normalized_bias_sqr_per_state", ")", "\n", "", ""]]}