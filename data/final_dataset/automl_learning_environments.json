{"home.repos.pwc.inspect_result.automl_learning_environments.None.utils.ReplayBuffer.__init__": [[10, 23], ["int", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "state_dim", ",", "action_dim", ",", "device", ",", "max_size", "=", "int", "(", "1e6", ")", ")", ":", "\n", "        ", "self", ".", "device", "=", "device", "\n", "self", ".", "max_size", "=", "max_size", "\n", "self", ".", "state_dim", "=", "state_dim", "\n", "self", ".", "action_dim", "=", "action_dim", "\n", "self", ".", "ptr", "=", "0", "\n", "self", ".", "size", "=", "0", "\n", "\n", "self", ".", "state", "=", "torch", ".", "zeros", "(", "(", "max_size", ",", "state_dim", ")", ")", "\n", "self", ".", "action", "=", "torch", ".", "zeros", "(", "(", "max_size", ",", "action_dim", ")", ")", "\n", "self", ".", "next_state", "=", "torch", ".", "zeros", "(", "(", "max_size", ",", "state_dim", ")", ")", "\n", "self", ".", "reward", "=", "torch", ".", "zeros", "(", "(", "max_size", ",", "1", ")", ")", "\n", "self", ".", "done", "=", "torch", ".", "zeros", "(", "(", "max_size", ",", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.ReplayBuffer.add": [[24, 33], ["state.detach", "action.detach", "next_state.detach", "reward.squeeze().detach", "done.squeeze().detach", "min", "reward.squeeze", "done.squeeze"], "methods", ["None"], ["", "def", "add", "(", "self", ",", "state", ",", "action", ",", "next_state", ",", "reward", ",", "done", ")", ":", "\n", "        ", "self", ".", "state", "[", "self", ".", "ptr", "]", "=", "state", ".", "detach", "(", ")", "\n", "self", ".", "action", "[", "self", ".", "ptr", "]", "=", "action", ".", "detach", "(", ")", "\n", "self", ".", "next_state", "[", "self", ".", "ptr", "]", "=", "next_state", ".", "detach", "(", ")", "\n", "self", ".", "reward", "[", "self", ".", "ptr", "]", "=", "reward", ".", "squeeze", "(", ")", ".", "detach", "(", ")", "\n", "self", ".", "done", "[", "self", ".", "ptr", "]", "=", "done", ".", "squeeze", "(", ")", ".", "detach", "(", ")", "\n", "\n", "self", ".", "ptr", "=", "(", "self", ".", "ptr", "+", "1", ")", "%", "self", ".", "max_size", "\n", "self", ".", "size", "=", "min", "(", "self", ".", "size", "+", "1", ",", "self", ".", "max_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.ReplayBuffer.sample": [[34, 37], ["numpy.random.randint", "utils.ReplayBuffer._sample_idx"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.None.utils.ReplayBuffer._sample_idx"], ["", "def", "sample", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "idx", "=", "np", ".", "random", ".", "randint", "(", "0", ",", "self", ".", "size", ",", "size", "=", "batch_size", ")", "\n", "return", "self", ".", "_sample_idx", "(", "idx", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.ReplayBuffer._sample_idx": [[38, 45], ["utils.ReplayBuffer.state[].to().detach", "utils.ReplayBuffer.action[].to().detach", "utils.ReplayBuffer.next_state[].to().detach", "utils.ReplayBuffer.reward[].to().detach", "utils.ReplayBuffer.done[].to().detach", "utils.ReplayBuffer.state[].to", "utils.ReplayBuffer.action[].to", "utils.ReplayBuffer.next_state[].to", "utils.ReplayBuffer.reward[].to", "utils.ReplayBuffer.done[].to"], "methods", ["None"], ["", "def", "_sample_idx", "(", "self", ",", "idx", ")", ":", "\n", "        ", "return", "(", "\n", "self", ".", "state", "[", "idx", "]", ".", "to", "(", "self", ".", "device", ")", ".", "detach", "(", ")", ",", "\n", "self", ".", "action", "[", "idx", "]", ".", "to", "(", "self", ".", "device", ")", ".", "detach", "(", ")", ",", "\n", "self", ".", "next_state", "[", "idx", "]", ".", "to", "(", "self", ".", "device", ")", ".", "detach", "(", ")", ",", "\n", "self", ".", "reward", "[", "idx", "]", ".", "to", "(", "self", ".", "device", ")", ".", "detach", "(", ")", ",", "\n", "self", ".", "done", "[", "idx", "]", ".", "to", "(", "self", ".", "device", ")", ".", "detach", "(", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.ReplayBuffer.get_all": [[47, 54], ["utils.ReplayBuffer.state[].to().detach", "utils.ReplayBuffer.action[].to().detach", "utils.ReplayBuffer.next_state[].to().detach", "utils.ReplayBuffer.reward[].to().detach", "utils.ReplayBuffer.done[].to().detach", "utils.ReplayBuffer.state[].to", "utils.ReplayBuffer.action[].to", "utils.ReplayBuffer.next_state[].to", "utils.ReplayBuffer.reward[].to", "utils.ReplayBuffer.done[].to"], "methods", ["None"], ["", "def", "get_all", "(", "self", ")", ":", "\n", "        ", "return", "(", "\n", "self", ".", "state", "[", ":", "self", ".", "size", "]", ".", "to", "(", "self", ".", "device", ")", ".", "detach", "(", ")", ",", "\n", "self", ".", "action", "[", ":", "self", ".", "size", "]", ".", "to", "(", "self", ".", "device", ")", ".", "detach", "(", ")", ",", "\n", "self", ".", "next_state", "[", ":", "self", ".", "size", "]", ".", "to", "(", "self", ".", "device", ")", ".", "detach", "(", ")", ",", "\n", "self", ".", "reward", "[", ":", "self", ".", "size", "]", ".", "to", "(", "self", ".", "device", ")", ".", "detach", "(", ")", ",", "\n", "self", ".", "done", "[", ":", "self", ".", "size", "]", ".", "to", "(", "self", ".", "device", ")", ".", "detach", "(", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.ReplayBuffer.merge_buffer": [[56, 59], ["other_replay_buffer.get_all", "utils.ReplayBuffer.merge_vectors"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.None.utils.ReplayBuffer.get_all", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.ReplayBuffer.merge_vectors"], ["", "def", "merge_buffer", "(", "self", ",", "other_replay_buffer", ")", ":", "\n", "        ", "states", ",", "actions", ",", "next_states", ",", "rewards", ",", "dones", "=", "other_replay_buffer", ".", "get_all", "(", ")", "\n", "self", ".", "merge_vectors", "(", "states", "=", "states", ",", "actions", "=", "actions", ",", "next_states", "=", "next_states", ",", "rewards", "=", "rewards", ",", "dones", "=", "dones", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.ReplayBuffer.merge_vectors": [[60, 63], ["range", "len", "utils.ReplayBuffer.add"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.None.utils.ReplayBuffer.add"], ["", "def", "merge_vectors", "(", "self", ",", "states", ",", "actions", ",", "next_states", ",", "rewards", ",", "dones", ")", ":", "\n", "        ", "for", "i", "in", "range", "(", "len", "(", "states", ")", ")", ":", "\n", "            ", "self", ".", "add", "(", "states", "[", "i", "]", ",", "actions", "[", "i", "]", ",", "next_states", "[", "i", "]", ",", "rewards", "[", "i", "]", ",", "dones", "[", "i", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.ReplayBuffer.get_size": [[64, 66], ["None"], "methods", ["None"], ["", "", "def", "get_size", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "size", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.ReplayBuffer.clear": [[68, 70], ["utils.ReplayBuffer.__init__"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__"], ["", "def", "clear", "(", "self", ")", ":", "\n", "        ", "self", ".", "__init__", "(", "state_dim", "=", "self", ".", "state_dim", ",", "action_dim", "=", "self", ".", "action_dim", ",", "device", "=", "self", ".", "device", ",", "max_size", "=", "self", ".", "max_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.ReplayBuffer.make_one_hot_buffer": [[71, 73], ["torch.argmax"], "methods", ["None"], ["", "def", "make_one_hot_buffer", "(", "self", ",", "dim", "=", "1", ")", ":", "\n", "        ", "self", ".", "action", "=", "torch", ".", "argmax", "(", "self", ".", "action", ",", "dim", "=", "dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.AverageMeter.__init__": [[76, 80], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "print_str", ")", ":", "\n", "        ", "self", ".", "print_str", "=", "print_str", "\n", "self", ".", "vals", "=", "[", "]", "\n", "self", ".", "it", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.AverageMeter.update": [[81, 93], ["torch.is_tensor", "utils.AverageMeter.vals.append", "val.item.item.item", "utils.AverageMeter._mean", "print"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.None.utils.AverageMeter._mean"], ["", "def", "update", "(", "self", ",", "val", ",", "print_rate", "=", "10", ")", ":", "\n", "# formatting from torch/numpy to float", "\n", "        ", "if", "torch", ".", "is_tensor", "(", "val", ")", ":", "\n", "            ", "val", "=", "val", ".", "item", "(", ")", "\n", "\n", "", "self", ".", "vals", ".", "append", "(", "val", ")", "\n", "self", ".", "it", "+=", "1", "\n", "\n", "if", "self", ".", "it", "%", "print_rate", "==", "0", ":", "\n", "            ", "mean_val", "=", "self", ".", "_mean", "(", "num", "=", "print_rate", ",", "ignore_last", "=", "0", ")", "\n", "out", "=", "self", ".", "print_str", "+", "\"{:15.6f} {:>25} {}\"", ".", "format", "(", "mean_val", ",", "\"Total updates: \"", ",", "self", ".", "it", ")", "\n", "print", "(", "out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.AverageMeter.get_mean": [[94, 96], ["utils.AverageMeter._mean"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.None.utils.AverageMeter._mean"], ["", "", "def", "get_mean", "(", "self", ",", "num", "=", "10", ")", ":", "\n", "        ", "return", "self", ".", "_mean", "(", "num", ",", "ignore_last", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.AverageMeter.get_mean_last": [[97, 99], ["utils.AverageMeter._mean"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.None.utils.AverageMeter._mean"], ["", "def", "get_mean_last", "(", "self", ",", "num", "=", "10", ")", ":", "\n", "        ", "return", "self", ".", "_mean", "(", "num", ",", "ignore_last", "=", "num", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.AverageMeter.get_raw_data": [[100, 102], ["None"], "methods", ["None"], ["", "def", "get_raw_data", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "vals", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.AverageMeter._mean": [[103, 106], ["sum", "max", "max", "len", "len", "len"], "methods", ["None"], ["", "def", "_mean", "(", "self", ",", "num", ",", "ignore_last", ")", ":", "\n", "        ", "vals", "=", "self", ".", "vals", "[", "max", "(", "len", "(", "self", ".", "vals", ")", "-", "num", "-", "ignore_last", ",", "0", ")", ":", "max", "(", "len", "(", "self", ".", "vals", ")", "-", "ignore_last", ",", "0", ")", "]", "\n", "return", "sum", "(", "vals", ")", "/", "(", "len", "(", "vals", ")", "+", "1e-9", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.to_one_hot_encoding": [[108, 127], ["torch.is_tensor", "normal.squeeze.squeeze", "torch.is_tensor", "torch.zeros", "torch.zeros", "int", "normal.squeeze.dim", "normal.squeeze.dim", "len", "torch.zeros", "range", "NotImplementedError", "normal.squeeze.dim", "len", "int", "utils.to_one_hot_encoding", "normal.squeeze.item"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.None.utils.to_one_hot_encoding"], ["", "", "def", "to_one_hot_encoding", "(", "normal", ",", "one_hot_dim", ")", ":", "\n", "    ", "if", "torch", ".", "is_tensor", "(", "normal", ")", ":", "\n", "        ", "normal", "=", "normal", ".", "squeeze", "(", ")", "\n", "\n", "", "if", "not", "torch", ".", "is_tensor", "(", "normal", ")", ":", "\n", "        ", "one_hot", "=", "torch", ".", "zeros", "(", "one_hot_dim", ")", "\n", "one_hot", "[", "int", "(", "normal", ")", "]", "=", "1", "\n", "", "elif", "normal", ".", "dim", "(", ")", "==", "0", "or", "(", "normal", ".", "dim", "(", ")", "==", "1", "and", "len", "(", "normal", ")", "==", "1", ")", ":", "# single torch value", "\n", "        ", "one_hot", "=", "torch", ".", "zeros", "(", "one_hot_dim", ")", "\n", "one_hot", "[", "int", "(", "normal", ".", "item", "(", ")", ")", "]", "=", "1", "\n", "", "elif", "normal", ".", "dim", "(", ")", "==", "1", ":", "# vector of values", "\n", "        ", "n", "=", "len", "(", "normal", ")", "\n", "one_hot", "=", "torch", ".", "zeros", "(", "n", ",", "one_hot_dim", ")", "\n", "for", "i", "in", "range", "(", "n", ")", ":", "\n", "            ", "one_hot", "[", "i", "]", "=", "to_one_hot_encoding", "(", "normal", "[", "i", "]", ",", "one_hot_dim", ")", "\n", "", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "(", "'One hot encoding supported only for scalar values and 1D vectors'", ")", "\n", "\n", "", "return", "one_hot", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.from_one_hot_encoding": [[129, 131], ["torch.tensor", "torch.argmax"], "function", ["None"], ["", "def", "from_one_hot_encoding", "(", "one_hot", ")", ":", "\n", "    ", "return", "torch", ".", "tensor", "(", "[", "torch", ".", "argmax", "(", "one_hot", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.calc_abs_param_sum": [[133, 138], ["model.parameters", "abs().sum", "abs"], "function", ["None"], ["", "def", "calc_abs_param_sum", "(", "model", ")", ":", "\n", "    ", "sm", "=", "0", "\n", "for", "param", "in", "model", ".", "parameters", "(", ")", ":", "\n", "        ", "sm", "+=", "abs", "(", "param", ")", ".", "sum", "(", ")", "\n", "", "return", "sm", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.print_abs_param_sum": [[140, 142], ["print", "str", "utils.calc_abs_param_sum"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.None.utils.calc_abs_param_sum"], ["", "def", "print_abs_param_sum", "(", "model", ",", "name", "=", "\"\"", ")", ":", "\n", "    ", "print", "(", "name", "+", "\" \"", "+", "str", "(", "calc_abs_param_sum", "(", "model", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.save_lists": [[144, 161], ["os.path.join", "pandas.DataFrame.from_dict", "torch.save", "os.getcwd", "str"], "function", ["None"], ["", "def", "save_lists", "(", "mode", ",", "config", ",", "reward_list", ",", "train_steps_needed", ",", "episode_length_needed", ",", "env_reward_overview", ",", "experiment_name", "=", "None", ",", "\n", "out_dir", "=", "None", ")", ":", "\n", "    ", "if", "experiment_name", "is", "None", ":", "\n", "        ", "experiment_name", "=", "\"_experiment_\"", "\n", "\n", "", "if", "out_dir", "is", "None", ":", "\n", "        ", "out_dir", "=", "os", ".", "getcwd", "(", ")", "\n", "\n", "", "file_name", "=", "os", ".", "path", ".", "join", "(", "out_dir", ",", "str", "(", "mode", ")", "+", "'_'", "+", "experiment_name", "+", "'.pt'", ")", "\n", "save_dict", "=", "{", "}", "\n", "save_dict", "[", "'config'", "]", "=", "config", "\n", "save_dict", "[", "'reward_list'", "]", "=", "reward_list", "\n", "save_dict", "[", "'train_steps_needed'", "]", "=", "train_steps_needed", "\n", "save_dict", "[", "'episode_length_needed'", "]", "=", "episode_length_needed", "\n", "save_dict", "[", "'env_reward_overview'", "]", "=", "pd", ".", "DataFrame", ".", "from_dict", "(", "env_reward_overview", ",", "orient", "=", "\"index\"", ")", "\n", "\n", "torch", ".", "save", "(", "save_dict", ",", "file_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.barplot_err": [[163, 180], ["pandas.concat", "seaborn.barplot", "pandas.concat", "pd.concat.append"], "function", ["None"], ["", "def", "barplot_err", "(", "x", ",", "y", ",", "xerr", "=", "None", ",", "yerr", "=", "None", ",", "data", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "    ", "_data", "=", "[", "]", "\n", "for", "_i", "in", "data", ".", "index", ":", "\n", "\n", "        ", "_data_i", "=", "pd", ".", "concat", "(", "[", "data", ".", "loc", "[", "_i", ":", "_i", "]", "]", "*", "3", ",", "ignore_index", "=", "True", ",", "sort", "=", "False", ")", "\n", "_row", "=", "data", ".", "loc", "[", "_i", "]", "\n", "if", "xerr", "is", "not", "None", ":", "\n", "            ", "_data_i", "[", "x", "]", "=", "[", "_row", "[", "x", "]", "-", "_row", "[", "xerr", "]", ",", "_row", "[", "x", "]", ",", "_row", "[", "x", "]", "+", "_row", "[", "xerr", "]", "]", "\n", "", "if", "yerr", "is", "not", "None", ":", "\n", "            ", "_data_i", "[", "y", "]", "=", "[", "_row", "[", "y", "]", "-", "_row", "[", "yerr", "]", ",", "_row", "[", "y", "]", ",", "_row", "[", "y", "]", "+", "_row", "[", "yerr", "]", "]", "\n", "", "_data", ".", "append", "(", "_data_i", ")", "\n", "\n", "", "_data", "=", "pd", ".", "concat", "(", "_data", ",", "ignore_index", "=", "True", ",", "sort", "=", "False", ")", "\n", "\n", "_ax", "=", "sns", ".", "barplot", "(", "x", "=", "x", ",", "y", "=", "y", ",", "data", "=", "_data", ",", "ci", "=", "'sd'", ",", "**", "kwargs", ")", "\n", "\n", "return", "_ax", "\n", "", ""]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_halfcheetah_compare_reward_envs.get_data": [[75, 125], ["float", "print", "print", "max", "torch.load", "list_data.append", "sums_eps_len_per_model.append", "sum", "sum", "numpy.zeros", "enumerate", "numpy.mean", "numpy.std", "proc_data.append", "print", "sums_eps_len.append", "sums_eps_len_per_model_i.append", "min", "zip", "range", "numpy.array", "sum", "sum", "sum", "sum", "numpy.asarray", "numpy.asarray", "len", "len", "concat_list.append"], "function", ["None"], ["def", "get_data", "(", ")", ":", "\n", "    ", "list_data", "=", "[", "]", "\n", "for", "log_file", "in", "LOG_FILES", ":", "\n", "        ", "data", "=", "torch", ".", "load", "(", "log_file", ")", "\n", "list_data", ".", "append", "(", "(", "data", "[", "'reward_list'", "]", ",", "data", "[", "'episode_length_list'", "]", ")", ")", "\n", "model_num", "=", "data", "[", "'model_num'", "]", "\n", "model_agents", "=", "data", "[", "'model_agents'", "]", "\n", "\n", "", "sums_eps_len", "=", "[", "]", "\n", "sums_eps_len_per_model", "=", "[", "]", "\n", "min_steps", "=", "float", "(", "'Inf'", ")", "\n", "# get minimum number of evaluations", "\n", "for", "reward_list", ",", "episode_length_list", "in", "list_data", ":", "\n", "        ", "sums_eps_len_per_model_i", "=", "[", "]", "\n", "for", "episode_lengths", "in", "episode_length_list", ":", "\n", "            ", "print", "(", "sum", "(", "episode_lengths", ")", ")", "\n", "sums_eps_len", ".", "append", "(", "sum", "(", "episode_lengths", ")", ")", "\n", "sums_eps_len_per_model_i", ".", "append", "(", "sum", "(", "episode_lengths", ")", ")", "\n", "min_steps", "=", "min", "(", "min_steps", ",", "sum", "(", "episode_lengths", ")", ")", "\n", "", "sums_eps_len_per_model", ".", "append", "(", "sums_eps_len_per_model_i", ")", "\n", "\n", "", "print", "(", "\"# of episode lens > MIN_STEPS: \"", ",", "sum", "(", "np", ".", "asarray", "(", "sums_eps_len", ")", ">=", "MIN_STEPS", ")", ")", "\n", "print", "(", "\"# of episode lens < MIN_STEPS: \"", ",", "sum", "(", "np", ".", "asarray", "(", "sums_eps_len", ")", "<", "MIN_STEPS", ")", ")", "\n", "min_steps", "=", "max", "(", "min_steps", ",", "MIN_STEPS", ")", "\n", "# convert data from episodes to steps", "\n", "proc_data", "=", "[", "]", "\n", "\n", "for", "reward_list", ",", "episode_length_list", "in", "list_data", ":", "\n", "        ", "np_data", "=", "np", ".", "zeros", "(", "[", "model_num", "*", "model_agents", ",", "min_steps", "]", ")", "\n", "\n", "for", "it", ",", "data", "in", "enumerate", "(", "zip", "(", "reward_list", ",", "episode_length_list", ")", ")", ":", "\n", "            ", "rewards", ",", "episode_lengths", "=", "data", "\n", "\n", "concat_list", "=", "[", "]", "\n", "rewards", "=", "rewards", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "episode_lengths", ")", ")", ":", "\n", "                ", "concat_list", "+=", "[", "rewards", "[", "i", "]", "]", "*", "episode_lengths", "[", "i", "]", "\n", "\n", "", "while", "len", "(", "concat_list", ")", "<", "min_steps", ":", "\n", "                ", "concat_list", ".", "append", "(", "concat_list", "[", "-", "1", "]", ")", "\n", "\n", "", "np_data", "[", "it", "]", "=", "np", ".", "array", "(", "concat_list", "[", ":", "min_steps", "]", ")", "\n", "\n", "", "mean", "=", "np", ".", "mean", "(", "np_data", ",", "axis", "=", "0", ")", "\n", "std", "=", "np", ".", "std", "(", "np_data", ",", "axis", "=", "0", ")", "\n", "\n", "proc_data", ".", "append", "(", "(", "mean", ",", "std", ")", ")", "\n", "\n", "", "return", "proc_data", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_halfcheetah_compare_reward_envs.plot_data": [[127, 177], ["matplotlib.subplots", "enumerate", "enumerate", "matplotlib.legend", "matplotlib.subplots_adjust", "matplotlib.title", "matplotlib.xlabel", "matplotlib.xlim", "matplotlib.ylim", "matplotlib.ylabel", "os.path.dirname", "matplotlib.savefig", "matplotlib.show", "legobj.set_linewidth", "os.path.join", "matplotlib.plot", "matplotlib.fill_between", "matplotlib.plot", "matplotlib.fill_between", "matplotlib.plot", "matplotlib.plot", "range", "matplotlib.fill_between", "matplotlib.fill_between", "len", "range", "len", "range", "range", "len", "len"], "function", ["None"], ["", "def", "plot_data", "(", "proc_data", ",", "savefig_name", ")", ":", "\n", "    ", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", "dpi", "=", "600", ",", "figsize", "=", "(", "5", ",", "4", ")", ")", "\n", "#", "\n", "# for mean, _ in data_w:", "\n", "#     plt.plot(mean_w)", "\n", "#     colors.append(plt.gca().lines[-1].get_color())", "\n", "\n", "styl_list", "=", "[", "'-'", ",", "'--'", ",", "'-.'", ",", "':'", "]", "# list of basic linestyles", "\n", "# colors = [\"darkturquoise\", \"cadetblue\", \"lightseagreen\", \"darkcyan\", \"deepskyblue\", \"lightskyblue\", \"steelblue\", \"cyan\"]", "\n", "\n", "for", "i", ",", "data", "in", "enumerate", "(", "proc_data", ")", ":", "\n", "        ", "mean", ",", "std", "=", "data", "\n", "# if i >= 7:", "\n", "#     plt.plot(mean, linewidth=.5, color=colors[i-7])", "\n", "if", "i", "==", "10", ":", "\n", "            ", "plt", ".", "plot", "(", "mean", ",", "color", "=", "'#575757'", ",", "linewidth", "=", "1", ")", "\n", "", "elif", "i", "==", "11", ":", "\n", "            ", "plt", ".", "plot", "(", "mean", ",", "color", "=", "'#EBBB00'", ",", "linewidth", "=", "1", ")", "\n", "", "elif", "i", "==", "12", ":", "\n", "            ", "plt", ".", "plot", "(", "mean", ",", "color", "=", "\"darkcyan\"", ",", "linewidth", "=", "1", ")", "\n", "", "else", ":", "\n", "            ", "plt", ".", "plot", "(", "mean", ",", "linewidth", "=", "1", ")", "\n", "\n", "", "", "for", "i", ",", "data", "in", "enumerate", "(", "proc_data", ")", ":", "\n", "        ", "mean", ",", "std", "=", "data", "\n", "if", "i", "==", "10", ":", "\n", "            ", "plt", ".", "fill_between", "(", "x", "=", "range", "(", "len", "(", "mean", ")", ")", ",", "y1", "=", "mean", "-", "std", "*", "STD_MULT", ",", "y2", "=", "mean", "+", "std", "*", "STD_MULT", ",", "alpha", "=", "0.2", ",", "color", "=", "'#575757'", ")", "\n", "", "elif", "i", "==", "11", ":", "\n", "            ", "plt", ".", "fill_between", "(", "x", "=", "range", "(", "len", "(", "mean", ")", ")", ",", "y1", "=", "mean", "-", "std", "*", "STD_MULT", ",", "y2", "=", "mean", "+", "std", "*", "STD_MULT", ",", "alpha", "=", "0.2", ",", "color", "=", "'#EBBB00'", ")", "\n", "", "elif", "i", "==", "12", ":", "\n", "            ", "plt", ".", "fill_between", "(", "x", "=", "range", "(", "len", "(", "mean", ")", ")", ",", "y1", "=", "mean", "-", "std", "*", "STD_MULT", ",", "y2", "=", "mean", "+", "std", "*", "STD_MULT", ",", "alpha", "=", "0.2", ",", "color", "=", "\"darkcyan\"", ")", "\n", "", "else", ":", "\n", "            ", "plt", ".", "fill_between", "(", "x", "=", "range", "(", "len", "(", "mean", ")", ")", ",", "y1", "=", "mean", "-", "std", "*", "STD_MULT", ",", "y2", "=", "mean", "+", "std", "*", "STD_MULT", ",", "alpha", "=", "0.2", ")", "\n", "\n", "", "", "leg", "=", "plt", ".", "legend", "(", "LEGEND", ",", "fontsize", "=", "9", ")", "\n", "\n", "for", "legobj", "in", "leg", ".", "legendHandles", ":", "\n", "        ", "legobj", ".", "set_linewidth", "(", "2.0", ")", "\n", "\n", "# plt.xlim(0,99)", "\n", "", "plt", ".", "subplots_adjust", "(", "bottom", "=", "0.15", ",", "left", "=", "0.15", ")", "\n", "plt", ".", "title", "(", "'HalfCheetah-v3'", ")", "\n", "plt", ".", "xlabel", "(", "'steps'", ")", "\n", "plt", ".", "xlim", "(", "0", ",", "MIN_STEPS", ")", "\n", "plt", ".", "ylim", "(", "-", "1000", ",", "6000", ")", "\n", "# plt.ylim(-1000, 5000)", "\n", "plt", ".", "ylabel", "(", "'cumulative reward'", ")", "\n", "base_dir", "=", "os", ".", "path", ".", "dirname", "(", "LOG_FILES", "[", "0", "]", ")", "\n", "plt", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "base_dir", ",", "savefig_name", ")", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_halfcheetah_vary_hp.ExperimentWrapper.get_bohb_parameters": [[17, 26], ["None"], "methods", ["None"], ["    ", "def", "get_bohb_parameters", "(", "self", ")", ":", "\n", "        ", "params", "=", "{", "}", "\n", "params", "[", "'min_budget'", "]", "=", "1", "\n", "params", "[", "'max_budget'", "]", "=", "1", "\n", "params", "[", "'eta'", "]", "=", "2", "\n", "params", "[", "'random_fraction'", "]", "=", "1", "\n", "params", "[", "'iterations'", "]", "=", "10000", "\n", "\n", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_halfcheetah_vary_hp.ExperimentWrapper.get_configspace": [[27, 33], ["ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.CategoricalHyperparameter"], "methods", ["None"], ["", "def", "get_configspace", "(", "self", ")", ":", "\n", "        ", "cs", "=", "CS", ".", "ConfigurationSpace", "(", ")", "\n", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "CategoricalHyperparameter", "(", "name", "=", "'td3_vary_vary_hp'", ",", "choices", "=", "[", "False", ",", "True", "]", ",", "default_value", "=", "False", ")", ")", "\n", "\n", "return", "cs", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_halfcheetah_vary_hp.ExperimentWrapper.get_specific_config": [[34, 40], ["copy.deepcopy"], "methods", ["None"], ["", "def", "get_specific_config", "(", "self", ",", "cso", ",", "default_config", ",", "budget", ")", ":", "\n", "        ", "config", "=", "deepcopy", "(", "default_config", ")", "\n", "global", "reward_env_type", "\n", "config", "[", "\"envs\"", "]", "[", "'HalfCheetah-v3'", "]", "[", "'reward_env_type'", "]", "=", "reward_env_type", "\n", "config", "[", "\"agents\"", "]", "[", "'td3_vary'", "]", "[", "'vary_hp'", "]", "=", "cso", "[", "\"td3_vary_vary_hp\"", "]", "\n", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_halfcheetah_vary_hp.ExperimentWrapper.compute": [[41, 81], ["GTNC_evaluate_halfcheetah_vary_hp.ExperimentWrapper.get_specific_config", "print", "print", "print", "print", "print", "print", "str", "str", "str", "str", "print", "print", "print", "print", "print", "open", "yaml.safe_load", "agents.GTN.GTN_Master", "agents.GTN.GTN_Master.run", "str", "str", "str", "float", "traceback.format_exc", "print", "str", "str", "sorted"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_step_size.ExperimentWrapper.get_specific_config", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.run"], ["", "def", "compute", "(", "self", ",", "working_dir", ",", "bohb_id", ",", "config_id", ",", "cso", ",", "budget", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "with", "open", "(", "\"default_config_halfcheetah_reward_env.yaml\"", ",", "'r'", ")", "as", "stream", ":", "\n", "            ", "default_config", "=", "yaml", ".", "safe_load", "(", "stream", ")", "\n", "\n", "", "config", "=", "self", ".", "get_specific_config", "(", "cso", ",", "default_config", ",", "budget", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "\"START BOHB ITERATION\"", ")", "\n", "print", "(", "'CONFIG: '", "+", "str", "(", "config", ")", ")", "\n", "print", "(", "'CSO:    '", "+", "str", "(", "cso", ")", ")", "\n", "print", "(", "'BUDGET: '", "+", "str", "(", "budget", ")", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "try", ":", "\n", "            ", "gtn", "=", "GTN_Master", "(", "config", ",", "bohb_id", "=", "bohb_id", ",", "bohb_working_dir", "=", "working_dir", ")", "\n", "_", ",", "score_list", ",", "model_name", "=", "gtn", ".", "run", "(", ")", "\n", "score", "=", "-", "sorted", "(", "score_list", ")", "[", "-", "1", "]", "\n", "error", "=", "\"\"", "\n", "", "except", ":", "\n", "            ", "score", "=", "float", "(", "'-Inf'", ")", "\n", "score_list", "=", "[", "]", "\n", "model_name", "=", "None", "\n", "error", "=", "traceback", ".", "format_exc", "(", ")", "\n", "print", "(", "error", ")", "\n", "\n", "", "info", "=", "{", "}", "\n", "info", "[", "'error'", "]", "=", "str", "(", "error", ")", "\n", "info", "[", "'config'", "]", "=", "str", "(", "config", ")", "\n", "info", "[", "'score_list'", "]", "=", "str", "(", "score_list", ")", "\n", "info", "[", "'model_name'", "]", "=", "str", "(", "model_name", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "'FINAL SCORE: '", "+", "str", "(", "score", ")", ")", "\n", "print", "(", "'SCORE LIST:  '", "+", "str", "(", "score_list", ")", ")", "\n", "print", "(", "\"END BOHB ITERATION\"", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "return", "{", "\n", "\"loss\"", ":", "score", ",", "\n", "\"info\"", ":", "info", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.bohb_params_PPO_ICM_halfcheetah.ExperimentWrapper.get_bohb_parameters": [[16, 25], ["None"], "methods", ["None"], ["    ", "def", "get_bohb_parameters", "(", "self", ")", ":", "\n", "        ", "params", "=", "{", "}", "\n", "params", "[", "'min_budget'", "]", "=", "1", "\n", "params", "[", "'max_budget'", "]", "=", "2", "\n", "params", "[", "'eta'", "]", "=", "2", "\n", "params", "[", "'iterations'", "]", "=", "1000", "\n", "params", "[", "'random_fraction'", "]", "=", "0.3", "\n", "\n", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.bohb_params_PPO_ICM_halfcheetah.ExperimentWrapper.get_configspace": [[26, 36], ["ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter"], "methods", ["None"], ["", "def", "get_configspace", "(", "self", ")", ":", "\n", "        ", "cs", "=", "CS", ".", "ConfigurationSpace", "(", ")", "\n", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'lr'", ",", "lower", "=", "1e-6", ",", "upper", "=", "1e-3", ",", "log", "=", "True", ",", "default_value", "=", "1e-4", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'beta'", ",", "lower", "=", "0.001", ",", "upper", "=", "1.0", ",", "log", "=", "True", ",", "default_value", "=", "0.2", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'eta'", ",", "lower", "=", "0.001", ",", "upper", "=", "1.0", ",", "log", "=", "True", ",", "default_value", "=", "0.5", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'feature_dim'", ",", "lower", "=", "16", ",", "upper", "=", "256", ",", "log", "=", "True", ",", "default_value", "=", "64", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'hidden_size'", ",", "lower", "=", "16", ",", "upper", "=", "256", ",", "log", "=", "True", ",", "default_value", "=", "128", ")", ")", "\n", "\n", "return", "cs", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.bohb_params_PPO_ICM_halfcheetah.ExperimentWrapper.get_specific_config": [[37, 74], ["copy.deepcopy"], "methods", ["None"], ["", "def", "get_specific_config", "(", "self", ",", "cso", ",", "default_config", ",", "budget", ")", ":", "\n", "        ", "config", "=", "deepcopy", "(", "default_config", ")", "\n", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "=", "{", "}", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'test_episodes'", "]", "=", "1", "\n", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'print_rate'", "]", "=", "100", "\n", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'init_episodes'", "]", "=", "0", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'update_episodes'", "]", "=", "1", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'ppo_epochs'", "]", "=", "10", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'gamma'", "]", "=", "0.99", "\n", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'lr'", "]", "=", "1e-5", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'vf_coef'", "]", "=", "1", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'ent_coef'", "]", "=", "0.001", "\n", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'eps_clip'", "]", "=", "0.2", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'rb_size'", "]", "=", "1000000", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'same_action_num'", "]", "=", "1", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'activation_fn'", "]", "=", "'tanh'", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'hidden_size'", "]", "=", "128", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'hidden_layer'", "]", "=", "2", "\n", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'action_std'", "]", "=", "0.1", "\n", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'early_out_num'", "]", "=", "50", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'early_out_virtual_diff'", "]", "=", "0.02", "\n", "\n", "config", "[", "\"agents\"", "]", "[", "\"icm\"", "]", "[", "\"lr\"", "]", "=", "cso", "[", "\"lr\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"icm\"", "]", "[", "\"beta\"", "]", "=", "cso", "[", "\"beta\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"icm\"", "]", "[", "\"eta\"", "]", "=", "cso", "[", "\"eta\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"icm\"", "]", "[", "\"feature_dim\"", "]", "=", "cso", "[", "\"feature_dim\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"icm\"", "]", "[", "\"hidden_size\"", "]", "=", "cso", "[", "\"hidden_size\"", "]", "\n", "config", "[", "\"device\"", "]", "=", "'cuda'", "\n", "\n", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.bohb_params_PPO_ICM_halfcheetah.ExperimentWrapper.compute": [[75, 117], ["bohb_params_PPO_ICM_halfcheetah.ExperimentWrapper.get_specific_config", "print", "print", "print", "print", "print", "print", "envs.env_factory.EnvFactory", "envs.env_factory.EnvFactory.generate_real_env", "range", "str", "print", "print", "print", "print", "open", "yaml.safe_load", "agents.PPO.PPO", "agents.PPO.PPO.train", "rewards_list.append", "numpy.mean", "str", "str", "str", "sum", "str"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_step_size.ExperimentWrapper.get_specific_config", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_real_env", "home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.train"], ["", "def", "compute", "(", "self", ",", "working_dir", ",", "bohb_id", ",", "config_id", ",", "cso", ",", "budget", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "with", "open", "(", "\"default_config_halfcheetah.yaml\"", ",", "'r'", ")", "as", "stream", ":", "\n", "            ", "default_config", "=", "yaml", ".", "safe_load", "(", "stream", ")", "\n", "\n", "", "config", "=", "self", ".", "get_specific_config", "(", "cso", ",", "default_config", ",", "budget", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "\"START BOHB ITERATION\"", ")", "\n", "print", "(", "'CONFIG: '", "+", "str", "(", "config", ")", ")", "\n", "print", "(", "'CSO:    '", "+", "str", "(", "cso", ")", ")", "\n", "print", "(", "'BUDGET: '", "+", "str", "(", "budget", ")", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "config", "[", "\"agents\"", "]", "[", "\"ppo\"", "]", "[", "\"train_episodes\"", "]", "=", "3000", "\n", "\n", "info", "=", "{", "}", "\n", "\n", "# generate environment", "\n", "env_fac", "=", "EnvFactory", "(", "config", ")", "\n", "real_env", "=", "env_fac", ".", "generate_real_env", "(", ")", "\n", "\n", "# score = 0", "\n", "rewards_list", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "NUM_EVALS", ")", ":", "\n", "            ", "ppo", "=", "PPO", "(", "env", "=", "real_env", ",", "\n", "config", "=", "config", ",", "\n", "icm", "=", "True", ")", "\n", "rewards", ",", "_", ",", "_", "=", "ppo", ".", "train", "(", "real_env", ")", "\n", "rewards_list", ".", "append", "(", "sum", "(", "rewards", ")", ")", "\n", "# score += len(rewards)", "\n", "\n", "# score = score/NUM_EVALS", "\n", "", "score", "=", "-", "np", ".", "mean", "(", "rewards_list", ")", "\n", "info", "[", "'config'", "]", "=", "str", "(", "config", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "'FINAL SCORE: '", "+", "str", "(", "score", ")", ")", "\n", "print", "(", "\"END BOHB ITERATION\"", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "return", "{", "\n", "\"loss\"", ":", "score", ",", "\n", "\"info\"", ":", "info", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_params_bohb_gridworld_initial.str2bool": [[24, 26], ["v.lower"], "function", ["None"], ["def", "str2bool", "(", "v", ")", ":", "\n", "    ", "return", "v", ".", "lower", "(", ")", "in", "(", "\"yes\"", ",", "\"true\"", ",", "\"t\"", ",", "\"1\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_params_bohb_gridworld_initial.analyze_bohb": [[28, 39], ["hpbandster.logged_results_to_HBS_result", "GTNC_visualize_params_bohb_gridworld_initial.transform_result", "GTNC_visualize_params_bohb_gridworld_initial.plot_parallel_scatter", "GTNC_visualize_params_bohb_gridworld_initial.plot_parallel_scatter", "GTNC_visualize_params_bohb_gridworld_initial.plot_parallel_scatter", "GTNC_visualize_params_bohb_gridworld_initial.plot_parallel_scatter"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_params_bohb_gridworld_initial.transform_result", "home.repos.pwc.inspect_result.automl_learning_environments.automl.analyze_bohb.plot_parallel_scatter", "home.repos.pwc.inspect_result.automl_learning_environments.automl.analyze_bohb.plot_parallel_scatter", "home.repos.pwc.inspect_result.automl_learning_environments.automl.analyze_bohb.plot_parallel_scatter", "home.repos.pwc.inspect_result.automl_learning_environments.automl.analyze_bohb.plot_parallel_scatter"], ["", "def", "analyze_bohb", "(", "log_dir", ")", ":", "\n", "# load the example run from the log files", "\n", "    ", "result", "=", "hpres", ".", "logged_results_to_HBS_result", "(", "log_dir", ")", "\n", "\n", "result", "=", "transform_result", "(", "result", ",", "min_success_reward", "=", "MIN_SUCCESS_REWARD", ")", "\n", "# result = remove_outliers(result)", "\n", "\n", "plot_parallel_scatter", "(", "result", ",", "with_mirrored_sampling", "=", "False", ",", "with_nes_step_size", "=", "False", ")", "\n", "plot_parallel_scatter", "(", "result", ",", "with_mirrored_sampling", "=", "False", ",", "with_nes_step_size", "=", "True", ")", "\n", "plot_parallel_scatter", "(", "result", ",", "with_mirrored_sampling", "=", "True", ",", "with_nes_step_size", "=", "False", ")", "\n", "plot_parallel_scatter", "(", "result", ",", "with_mirrored_sampling", "=", "True", ",", "with_nes_step_size", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_params_bohb_gridworld_initial.transform_result": [[41, 51], ["result.data.items", "value1.results.values", "ast.literal_eval", "range", "len"], "function", ["None"], ["", "def", "transform_result", "(", "result", ",", "min_success_reward", ")", ":", "\n", "    ", "for", "key", ",", "value1", "in", "result", ".", "data", ".", "items", "(", ")", ":", "\n", "        ", "for", "value2", "in", "value1", ".", "results", ".", "values", "(", ")", ":", "\n", "            ", "score_list", "=", "value2", "[", "'info'", "]", "[", "'score_list'", "]", "\n", "score_list", "=", "ast", ".", "literal_eval", "(", "score_list", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "score_list", ")", ")", ":", "\n", "                ", "if", "score_list", "[", "i", "]", ">", "min_success_reward", ":", "\n", "                    ", "value2", "[", "'loss'", "]", "=", "i", "\n", "break", "\n", "", "", "", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_params_bohb_gridworld_initial.remove_outliers": [[53, 94], ["result.data.items", "range", "lut.sort", "math.ceil", "math.ceil", "range", "range", "value1.results.values", "len", "lut.pop", "result.data.pop", "lut.pop", "result.data.pop", "lut.append", "math.isfinite", "sorted", "abs", "abs", "result.data[].results.keys", "len", "len", "float", "math.isfinite"], "function", ["None"], ["", "def", "remove_outliers", "(", "result", ")", ":", "\n", "    ", "lut", "=", "[", "]", "\n", "for", "key", ",", "value1", "in", "result", ".", "data", ".", "items", "(", ")", ":", "\n", "        ", "for", "value2", "in", "value1", ".", "results", ".", "values", "(", ")", ":", "\n", "            ", "if", "value2", "==", "None", ":", "\n", "                ", "loss", "=", "float", "(", "'nan'", ")", "\n", "", "else", ":", "\n", "                ", "loss", "=", "value2", "[", "'loss'", "]", "\n", "", "lut", ".", "append", "(", "[", "loss", ",", "key", "]", ")", "\n", "\n", "", "", "filtered_lut", "=", "[", "x", "for", "x", "in", "lut", "if", "math", ".", "isfinite", "(", "x", "[", "0", "]", ")", "]", "\n", "worst_loss", "=", "sorted", "(", "filtered_lut", ",", "reverse", "=", "REVERSE_LOSS", ")", "[", "0", "]", "[", "0", "]", "\n", "\n", "if", "REVERSE_LOSS", ":", "\n", "        ", "worst_loss", "+=", "0.01", "*", "abs", "(", "worst_loss", ")", "\n", "", "else", ":", "\n", "        ", "worst_loss", "-=", "0.01", "*", "abs", "(", "worst_loss", ")", "\n", "\n", "# remove NaN's", "\n", "", "for", "i", "in", "range", "(", "len", "(", "lut", ")", ")", ":", "\n", "        ", "if", "not", "math", ".", "isfinite", "(", "lut", "[", "i", "]", "[", "0", "]", ")", "or", "lut", "[", "i", "]", "[", "0", "]", "==", "0", ":", "\n", "            ", "lut", "[", "i", "]", "[", "0", "]", "=", "worst_loss", "\n", "for", "key", "in", "result", ".", "data", "[", "lut", "[", "i", "]", "[", "1", "]", "]", ".", "results", ".", "keys", "(", ")", ":", "\n", "                ", "result", ".", "data", "[", "lut", "[", "i", "]", "[", "1", "]", "]", ".", "results", "[", "key", "]", "[", "'loss'", "]", "=", "worst_loss", "\n", "# result.data.pop(elem[1], None)", "\n", "\n", "", "", "", "lut", ".", "sort", "(", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ",", "reverse", "=", "REVERSE_LOSS", ")", "\n", "n_remove_worst", "=", "math", ".", "ceil", "(", "len", "(", "lut", ")", "*", "OUTLIER_PERC_WORST", ")", "\n", "n_remove_best", "=", "math", ".", "ceil", "(", "len", "(", "lut", ")", "*", "OUTLIER_PERC_BEST", ")", "\n", "\n", "# remove percentage of worst values", "\n", "for", "i", "in", "range", "(", "n_remove_worst", ")", ":", "\n", "        ", "elem", "=", "lut", ".", "pop", "(", "0", ")", "\n", "result", ".", "data", ".", "pop", "(", "elem", "[", "1", "]", ",", "None", ")", "\n", "\n", "# remove percentage of best values", "\n", "", "for", "i", "in", "range", "(", "n_remove_best", ")", ":", "\n", "        ", "elem", "=", "lut", ".", "pop", "(", ")", "\n", "result", ".", "data", ".", "pop", "(", "elem", "[", "1", "]", ",", "None", ")", "\n", "\n", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_params_bohb_gridworld_initial.filter_values": [[96, 111], ["result.data.items", "result.data.pop", "del_list.append"], "function", ["None"], ["", "def", "filter_values", "(", "result", ")", ":", "\n", "    ", "del_list", "=", "[", "]", "\n", "for", "key", ",", "value1", "in", "result", ".", "data", ".", "items", "(", ")", ":", "\n", "        ", "id", "=", "key", "\n", "if", "value1", ".", "config", "[", "'gtn_score_transform_type'", "]", "!=", "7", ":", "\n", "            ", "del_list", ".", "append", "(", "id", ")", "\n", "# print('++++')", "\n", "# print(key)", "\n", "# print('----')", "\n", "# print(value1)", "\n", "\n", "", "", "for", "id", "in", "del_list", ":", "\n", "        ", "result", ".", "data", ".", "pop", "(", "id", ",", "None", ")", "\n", "\n", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_params_bohb_gridworld_initial.plot_parallel_scatter": [[113, 195], ["matplotlib.subplots", "result.data.values", "range", "matplotlib.title", "matplotlib.yticks", "matplotlib.xticks", "matplotlib.show", "GTNC_visualize_params_bohb_gridworld_initial.str2bool", "GTNC_visualize_params_bohb_gridworld_initial.str2bool", "value.results.values", "len", "numpy.zeros", "numpy.zeros", "numpy.zeros", "range", "matplotlib.scatter", "numpy.exp", "GTNC_visualize_params_bohb_gridworld_initial.linear_interpolation", "range", "len", "len", "range", "range", "len", "GTNC_visualize_params_bohb_gridworld_initial.map_to_zero_one_range", "GTNC_visualize_params_bohb_gridworld_initial.get_color", "str", "str", "str", "numpy.arange", "values[].append", "min", "max", "len", "len", "GTNC_visualize_params_bohb_gridworld_initial.linear_interpolation", "len", "GTNC_visualize_params_bohb_gridworld_initial.linear_interpolation", "numpy.random.uniform", "numpy.log", "numpy.log", "numpy.log", "numpy.random.uniform", "numpy.log", "numpy.log", "decimal.Decimal", "decimal.Decimal", "decimal.Decimal"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_params_bohb_gridworld_initial.str2bool", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_params_bohb_gridworld_initial.str2bool", "home.repos.pwc.inspect_result.automl_learning_environments.automl.analyze_bohb.linear_interpolation", "home.repos.pwc.inspect_result.automl_learning_environments.automl.analyze_bohb.map_to_zero_one_range", "home.repos.pwc.inspect_result.automl_learning_environments.automl.analyze_bohb.get_color", "home.repos.pwc.inspect_result.automl_learning_environments.automl.analyze_bohb.linear_interpolation", "home.repos.pwc.inspect_result.automl_learning_environments.automl.analyze_bohb.linear_interpolation"], ["", "def", "plot_parallel_scatter", "(", "result", ",", "with_mirrored_sampling", ",", "with_nes_step_size", ")", ":", "\n", "    ", "plt", ".", "subplots", "(", "dpi", "=", "300", ",", "figsize", "=", "(", "4", ",", "4", ")", ")", "\n", "\n", "min_step_size", "=", "1e9", "\n", "max_step_size", "=", "-", "1e9", "\n", "\n", "# get all possible keys", "\n", "values", "=", "[", "[", "]", "for", "_", "in", "range", "(", "8", ")", "]", "\n", "for", "value", "in", "result", ".", "data", ".", "values", "(", ")", ":", "\n", "        ", "config", "=", "value", ".", "config", "\n", "mirrored_sampling", "=", "str2bool", "(", "config", "[", "'gtn_mirrored_sampling'", "]", ")", "\n", "nes_step_size", "=", "str2bool", "(", "config", "[", "'gtn_nes_step_size'", "]", ")", "\n", "score_transform_type", "=", "config", "[", "'gtn_score_transform_type'", "]", "\n", "step_size", "=", "config", "[", "'gtn_step_size'", "]", "\n", "\n", "for", "value2", "in", "value", ".", "results", ".", "values", "(", ")", ":", "\n", "            ", "loss", "=", "value2", "[", "'loss'", "]", "\n", "\n", "if", "mirrored_sampling", "==", "with_mirrored_sampling", "and", "nes_step_size", "==", "with_nes_step_size", ":", "\n", "                ", "values", "[", "score_transform_type", "]", ".", "append", "(", "(", "step_size", ",", "loss", ")", ")", "\n", "\n", "min_step_size", "=", "min", "(", "min_step_size", ",", "step_size", ")", "\n", "max_step_size", "=", "max", "(", "max_step_size", ",", "step_size", ")", "\n", "\n", "", "", "", "loss_m", "=", "0", "\n", "loss_M", "=", "50", "\n", "\n", "x_dev", "=", "0.2", "\n", "rad", "=", "25", "\n", "alpha", "=", "0.4", "\n", "text_x_offset", "=", "0", "\n", "text_y_offset", "=", "0", "\n", "size_text", "=", "6", "\n", "\n", "xs", "=", "[", "]", "\n", "ys", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "values", ")", ")", ":", "\n", "        ", "xs", "=", "np", ".", "zeros", "(", "len", "(", "values", "[", "i", "]", ")", ")", "\n", "ys", "=", "np", ".", "zeros", "(", "len", "(", "values", "[", "i", "]", ")", ")", "\n", "colors", "=", "np", ".", "zeros", "(", "[", "len", "(", "values", "[", "i", "]", ")", ",", "3", "]", ")", "\n", "\n", "# log scale if min/max value differs to much", "\n", "if", "max_step_size", "/", "min_step_size", ">", "10", ":", "\n", "            ", "for", "k", "in", "range", "(", "len", "(", "values", "[", "i", "]", ")", ")", ":", "\n", "                ", "step_size", ",", "loss", "=", "values", "[", "i", "]", "[", "k", "]", "\n", "xs", "[", "k", "]", "=", "i", "+", "1", "+", "np", ".", "random", ".", "uniform", "(", "-", "x_dev", ",", "x_dev", ")", "\n", "ys", "[", "k", "]", "=", "linear_interpolation", "(", "np", ".", "log", "(", "step_size", ")", ",", "np", ".", "log", "(", "min_step_size", ")", ",", "np", ".", "log", "(", "max_step_size", ")", ",", "0", ",", "1", ")", "\n", "# linear scale", "\n", "", "", "else", ":", "\n", "            ", "for", "k", "in", "range", "(", "len", "(", "values", "[", "i", "]", ")", ")", ":", "\n", "                ", "step_size", ",", "loss", "=", "values", "[", "i", "]", "[", "k", "]", "\n", "xs", "[", "k", "]", "=", "i", "+", "1", "+", "np", ".", "random", ".", "uniform", "(", "-", "x_dev", ",", "x_dev", ")", "\n", "ys", "[", "k", "]", "=", "linear_interpolation", "(", "step_size", ",", "min_step_size", ",", "max_step_size", ",", "0", ",", "1", ")", "\n", "\n", "", "", "for", "k", "in", "range", "(", "len", "(", "values", "[", "i", "]", ")", ")", ":", "\n", "            ", "step_size", ",", "loss", "=", "values", "[", "i", "]", "[", "k", "]", "\n", "acc", "=", "map_to_zero_one_range", "(", "loss", ",", "loss_m", ",", "loss_M", ")", "\n", "colors", "[", "k", ",", ":", "]", "=", "get_color", "(", "acc", ")", "\n", "\n", "", "plt", ".", "scatter", "(", "xs", ",", "ys", ",", "s", "=", "rad", ",", "c", "=", "colors", ",", "alpha", "=", "alpha", ",", "edgecolors", "=", "'none'", ")", "\n", "\n", "", "if", "max_step_size", "/", "min_step_size", ">", "10", ":", "\n", "        ", "val050", "=", "np", ".", "exp", "(", "(", "np", ".", "log", "(", "min_step_size", ")", "+", "np", ".", "log", "(", "max_step_size", ")", ")", "/", "2", ")", "\n", "", "else", ":", "\n", "        ", "val050", "=", "linear_interpolation", "(", "0.50", ",", "0", ",", "1", ",", "min_step_size", ",", "max_step_size", ")", "\n", "\n", "", "if", "with_nes_step_size", ":", "\n", "        ", "nes_string", "=", "'w/ NES step size'", "\n", "", "else", ":", "\n", "        ", "nes_string", "=", "'w/o NES step size'", "\n", "\n", "", "if", "with_mirrored_sampling", ":", "\n", "        ", "mir_string", "=", "'w/ mirrored sampling'", "\n", "", "else", ":", "\n", "        ", "mir_string", "=", "'w/o mirrored sampling'", "\n", "\n", "", "plt", ".", "title", "(", "mir_string", "+", "', '", "+", "nes_string", ")", "\n", "plt", ".", "yticks", "(", "[", "0", ",", "0.5", ",", "1", "]", ",", "[", "str", "(", "f\"{Decimal(min_step_size):.1E}\"", ")", ",", "str", "(", "f\"{Decimal(val050):.1E}\"", ")", ",", "str", "(", "f\"{Decimal(max_step_size):.1E}\"", ")", "]", ")", "\n", "plt", ".", "xticks", "(", "np", ".", "arange", "(", "8", ")", "+", "1", ",", "(", "'linear transf.'", ",", "'rank transf.'", ",", "'NES'", ",", "'NES unnorm.'", ",", "'single best'", ",", "'single better'", ",", "'all better 1'", ",", "'all better 2'", ")", ",", "rotation", "=", "90", ")", "\n", "\n", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_params_bohb_gridworld_initial.linear_interpolation": [[197, 200], ["None"], "function", ["None"], ["", "def", "linear_interpolation", "(", "x", ",", "x0", ",", "x1", ",", "y0", ",", "y1", ")", ":", "\n", "# linearly interpolate between two x/y values for a given x value", "\n", "    ", "return", "y0", "+", "(", "y1", "-", "y0", ")", "*", "(", "x", "-", "x0", ")", "/", "(", "x1", "-", "x0", "+", "1e-9", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_params_bohb_gridworld_initial.map_to_zero_one_range": [[202, 218], ["None"], "function", ["None"], ["", "def", "map_to_zero_one_range", "(", "loss", ",", "loss_m", ",", "loss_M", ")", ":", "\n", "    ", "if", "loss_M", "<", "1", "and", "loss_m", ">", "0", "and", "REVERSE_LOSS", "==", "False", ":", "\n", "# if we have already a loss in the [0,1] range, there is no need to normalize anything", "\n", "        ", "acc", "=", "loss", "\n", "", "elif", "loss_M", "<", "0", "and", "loss_m", ">", "-", "1", "and", "REVERSE_LOSS", "==", "True", ":", "\n", "# if we have a loss in the [-1,0] range, simply revert its sign", "\n", "        ", "acc", "=", "-", "loss", "\n", "", "else", ":", "\n", "# normalize loss to the 0 (bad) - 1(good) range", "\n", "        ", "acc", "=", "(", "loss", "-", "loss_m", ")", "/", "(", "loss_M", "-", "loss_m", ")", "\n", "if", "REVERSE_LOSS", ":", "\n", "            ", "acc", "=", "1", "-", "acc", "\n", "\n", "", "", "acc", "=", "acc", "**", "EXP_LOSS", "\n", "\n", "return", "acc", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_params_bohb_gridworld_initial.get_color": [[220, 229], ["numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array"], "function", ["None"], ["", "def", "get_color", "(", "acc", ")", ":", "\n", "    ", "if", "acc", "<=", "0", ":", "\n", "        ", "return", "np", ".", "array", "(", "[", "[", "1", ",", "0", ",", "0", "]", "]", ")", "\n", "", "elif", "acc", "<=", "0.5", ":", "\n", "        ", "return", "np", ".", "array", "(", "[", "[", "1", ",", "0", ",", "0", "]", "]", ")", "+", "2", "*", "acc", "*", "np", ".", "array", "(", "[", "[", "0", ",", "1", ",", "0", "]", "]", ")", "\n", "", "elif", "acc", "<=", "1", ":", "\n", "        ", "return", "np", ".", "array", "(", "[", "[", "1", ",", "1", ",", "0", "]", "]", ")", "+", "2", "*", "(", "acc", "-", "0.5", ")", "*", "np", ".", "array", "(", "[", "[", "-", "1", ",", "0", ",", "0", "]", "]", ")", "\n", "", "else", ":", "\n", "        ", "return", "np", ".", "array", "(", "[", "[", "0", ",", "1", ",", "0", "]", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_params_bohb_gridworld_initial.get_bright_random_color": [[231, 234], ["colorsys.hls_to_rgb", "random.random"], "function", ["None"], ["", "", "def", "get_bright_random_color", "(", ")", ":", "\n", "    ", "h", ",", "s", ",", "l", "=", "random", ".", "random", "(", ")", ",", "1", ",", "0.5", "\n", "return", "colorsys", ".", "hls_to_rgb", "(", "h", ",", "l", ",", "s", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_halfcheetah_se_params.ExperimentWrapper.get_bohb_parameters": [[18, 27], ["None"], "methods", ["None"], ["    ", "def", "get_bohb_parameters", "(", "self", ")", ":", "\n", "        ", "params", "=", "{", "}", "\n", "params", "[", "'min_budget'", "]", "=", "1", "\n", "params", "[", "'max_budget'", "]", "=", "3", "\n", "params", "[", "'eta'", "]", "=", "3", "\n", "params", "[", "'random_fraction'", "]", "=", "0.3", "\n", "params", "[", "'iterations'", "]", "=", "10000", "\n", "\n", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_halfcheetah_se_params.ExperimentWrapper.get_configspace": [[28, 58], ["ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter"], "methods", ["None"], ["", "def", "get_configspace", "(", "self", ")", ":", "\n", "        ", "cs", "=", "CS", ".", "ConfigurationSpace", "(", ")", "\n", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'gtn_score_transform_type'", ",", "lower", "=", "0", ",", "upper", "=", "7", ",", "log", "=", "False", ",", "default_value", "=", "7", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'gtn_step_size'", ",", "lower", "=", "0.1", ",", "upper", "=", "1", ",", "log", "=", "True", ",", "default_value", "=", "0.5", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "CategoricalHyperparameter", "(", "name", "=", "'gtn_mirrored_sampling'", ",", "choices", "=", "[", "False", ",", "True", "]", ",", "default_value", "=", "True", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'gtn_noise_std'", ",", "lower", "=", "0.001", ",", "upper", "=", "1", ",", "log", "=", "True", ",", "default_value", "=", "0.01", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "CategoricalHyperparameter", "(", "name", "=", "'gtn_nes_step_size'", ",", "choices", "=", "[", "True", ",", "False", "]", ",", "default_value", "=", "False", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'gtn_num_grad_evals'", ",", "lower", "=", "1", ",", "upper", "=", "5", ",", "log", "=", "False", ",", "default_value", "=", "1", ")", ")", "\n", "\n", "# cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(name='td3_init_episodes', lower=1, upper=20, log=True, default_value=10))", "\n", "# cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(name='td3_batch_size', lower=64, upper=256, log=False, default_value=128))", "\n", "# cs.add_hyperparameter(CSH.UniformFloatHyperparameter(name='td3_gamma', lower=0.001, upper=0.1, log=True, default_value=0.01))", "\n", "# cs.add_hyperparameter(CSH.UniformFloatHyperparameter(name='td3_lr', lower=1e-4, upper=5e-3, log=True, default_value=1e-3))", "\n", "# cs.add_hyperparameter(CSH.UniformFloatHyperparameter(name='td3_tau', lower=0.005, upper=0.05, log=True, default_value=0.01))", "\n", "# cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(name='td3_policy_delay', lower=1, upper=3, log=False, default_value=2))", "\n", "# cs.add_hyperparameter(CSH.CategoricalHyperparameter(name='td3_activation_fn', choices=['tanh', 'relu', 'leakyrelu', 'prelu'], default_value='relu'))", "\n", "# cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(name='td3_hidden_size', lower=48, upper=192, log=True, default_value=128))", "\n", "# cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(name='td3_hidden_layer', lower=1, upper=2, log=False, default_value=2))", "\n", "# cs.add_hyperparameter(CSH.UniformFloatHyperparameter(name='td3_action_std', lower=0.05, upper=0.2, log=True, default_value=0.1))", "\n", "# cs.add_hyperparameter(CSH.UniformFloatHyperparameter(name='td3_policy_std', lower=0.1, upper=0.4, log=True, default_value=0.2))", "\n", "# cs.add_hyperparameter(CSH.UniformFloatHyperparameter(name='td3_policy_std_clip', lower=0.25, upper=1, log=True, default_value=0.5))", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_early_out_virtual_diff'", ",", "lower", "=", "1e-2", ",", "upper", "=", "1e-1", ",", "log", "=", "True", ",", "default_value", "=", "3e-2", ")", ")", "\n", "#", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "CategoricalHyperparameter", "(", "name", "=", "'halfcheetah_activation_fn'", ",", "choices", "=", "[", "'tanh'", ",", "'relu'", ",", "'leakyrelu'", ",", "'prelu'", "]", ",", "default_value", "=", "'leakyrelu'", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'halfcheetah_hidden_size'", ",", "lower", "=", "32", ",", "upper", "=", "256", ",", "log", "=", "True", ",", "default_value", "=", "128", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'halfcheetah_hidden_layer'", ",", "lower", "=", "1", ",", "upper", "=", "4", ",", "log", "=", "False", ",", "\n", "default_value", "=", "2", ")", ")", "\n", "\n", "return", "cs", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_halfcheetah_se_params.ExperimentWrapper.get_specific_config": [[59, 90], ["copy.deepcopy"], "methods", ["None"], ["", "def", "get_specific_config", "(", "self", ",", "cso", ",", "default_config", ",", "budget", ")", ":", "\n", "        ", "config", "=", "deepcopy", "(", "default_config", ")", "\n", "\n", "config", "[", "\"agents\"", "]", "[", "'gtn'", "]", "[", "'score_transform_type'", "]", "=", "cso", "[", "\"gtn_score_transform_type\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'gtn'", "]", "[", "'step_size'", "]", "=", "cso", "[", "\"gtn_step_size\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'gtn'", "]", "[", "'mirrored_sampling'", "]", "=", "cso", "[", "\"gtn_mirrored_sampling\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'gtn'", "]", "[", "'noise_std'", "]", "=", "cso", "[", "\"gtn_noise_std\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'gtn'", "]", "[", "'nes_step_size'", "]", "=", "cso", "[", "\"gtn_nes_step_size\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'gtn'", "]", "[", "'num_grad_evals'", "]", "=", "cso", "[", "\"gtn_num_grad_evals\"", "]", "\n", "\n", "config", "[", "\"agents\"", "]", "[", "'gtn'", "]", "[", "'synthetic_env_type'", "]", "=", "0", "\n", "\n", "# config[\"agents\"]['td3']['init_episodes'] = cso[\"td3_init_episodes\"]", "\n", "# config[\"agents\"]['td3']['batch_size'] = cso[\"td3_batch_size\"]", "\n", "# config[\"agents\"]['td3']['gamma'] = 1-cso[\"td3_gamma\"]", "\n", "# config[\"agents\"]['td3']['lr'] = cso[\"td3_lr\"]", "\n", "# config[\"agents\"]['td3']['tau'] = cso[\"td3_tau\"]", "\n", "# config[\"agents\"]['td3']['policy_delay'] = cso[\"td3_policy_delay\"]", "\n", "# config[\"agents\"]['td3']['activation_fn'] = cso[\"td3_activation_fn\"]", "\n", "# config[\"agents\"]['td3']['hidden_size'] = cso[\"td3_hidden_size\"]", "\n", "# config[\"agents\"]['td3']['hidden_layer'] = cso[\"td3_hidden_layer\"]", "\n", "# config[\"agents\"]['td3']['action_std'] = cso[\"td3_action_std\"]", "\n", "# config[\"agents\"]['td3']['policy_std'] = cso[\"td3_policy_std\"]", "\n", "# config[\"agents\"]['td3']['policy_std_clip'] = cso[\"td3_policy_std_clip\"]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'early_out_virtual_diff'", "]", "=", "cso", "[", "\"td3_early_out_virtual_diff\"", "]", "\n", "\n", "config", "[", "\"envs\"", "]", "[", "'HalfCheetah-v3'", "]", "[", "'activation_fn'", "]", "=", "cso", "[", "\"halfcheetah_activation_fn\"", "]", "\n", "config", "[", "\"envs\"", "]", "[", "'HalfCheetah-v3'", "]", "[", "'hidden_size'", "]", "=", "cso", "[", "\"halfcheetah_hidden_size\"", "]", "\n", "config", "[", "\"envs\"", "]", "[", "'HalfCheetah-v3'", "]", "[", "'hidden_layer'", "]", "=", "cso", "[", "\"halfcheetah_hidden_layer\"", "]", "\n", "\n", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_halfcheetah_se_params.ExperimentWrapper.compute": [[91, 131], ["GTNC_evaluate_halfcheetah_se_params.ExperimentWrapper.get_specific_config", "print", "print", "print", "print", "print", "print", "str", "str", "str", "str", "print", "print", "print", "print", "print", "open", "yaml.safe_load", "agents.GTN.GTN_Master", "agents.GTN.GTN_Master.run", "str", "str", "str", "float", "traceback.format_exc", "print", "str", "str", "sorted"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_step_size.ExperimentWrapper.get_specific_config", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.run"], ["", "def", "compute", "(", "self", ",", "working_dir", ",", "bohb_id", ",", "config_id", ",", "cso", ",", "budget", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "with", "open", "(", "\"default_config_halfcheetah_td3_se_opt.yaml\"", ",", "'r'", ")", "as", "stream", ":", "\n", "            ", "default_config", "=", "yaml", ".", "safe_load", "(", "stream", ")", "\n", "\n", "", "config", "=", "self", ".", "get_specific_config", "(", "cso", ",", "default_config", ",", "budget", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "\"START BOHB ITERATION\"", ")", "\n", "print", "(", "'CONFIG: '", "+", "str", "(", "config", ")", ")", "\n", "print", "(", "'CSO:    '", "+", "str", "(", "cso", ")", ")", "\n", "print", "(", "'BUDGET: '", "+", "str", "(", "budget", ")", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "try", ":", "\n", "            ", "gtn", "=", "GTN_Master", "(", "config", ",", "bohb_id", "=", "bohb_id", ",", "bohb_working_dir", "=", "working_dir", ")", "\n", "_", ",", "score_list", ",", "model_name", "=", "gtn", ".", "run", "(", ")", "\n", "score", "=", "-", "sorted", "(", "score_list", ")", "[", "-", "1", "]", "\n", "error", "=", "\"\"", "\n", "", "except", ":", "\n", "            ", "score", "=", "float", "(", "'-Inf'", ")", "\n", "score_list", "=", "[", "]", "\n", "model_name", "=", "None", "\n", "error", "=", "traceback", ".", "format_exc", "(", ")", "\n", "print", "(", "error", ")", "\n", "\n", "", "info", "=", "{", "}", "\n", "info", "[", "'error'", "]", "=", "str", "(", "error", ")", "\n", "info", "[", "'config'", "]", "=", "str", "(", "config", ")", "\n", "info", "[", "'score_list'", "]", "=", "str", "(", "score_list", ")", "\n", "info", "[", "'model_name'", "]", "=", "str", "(", "model_name", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "'FINAL SCORE: '", "+", "str", "(", "score", ")", ")", "\n", "print", "(", "'SCORE LIST:  '", "+", "str", "(", "score_list", ")", ")", "\n", "print", "(", "\"END BOHB ITERATION\"", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "return", "{", "\n", "\"loss\"", ":", "score", ",", "\n", "\"info\"", ":", "info", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_acrobot_vary_hp_2_DuelingDDQN.load_envs_and_config": [[13, 31], ["os.path.join", "torch.load", "envs.env_factory.EnvFactory", "envs.env_factory.EnvFactory.generate_virtual_env", "env_factory.generate_virtual_env.load_state_dict", "envs.env_factory.EnvFactory.generate_real_env", "open", "yaml.safe_load"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_virtual_env", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_real_env"], ["def", "load_envs_and_config", "(", "file_name", ",", "model_dir", ",", "device", ")", ":", "\n", "    ", "file_path", "=", "os", ".", "path", ".", "join", "(", "model_dir", ",", "file_name", ")", "\n", "save_dict", "=", "torch", ".", "load", "(", "file_path", ")", "\n", "config", "=", "save_dict", "[", "'config'", "]", "\n", "config", "[", "'device'", "]", "=", "device", "\n", "env_factory", "=", "EnvFactory", "(", "config", "=", "config", ")", "\n", "virtual_env", "=", "env_factory", ".", "generate_virtual_env", "(", ")", "\n", "virtual_env", ".", "load_state_dict", "(", "save_dict", "[", "'model'", "]", ")", "\n", "real_env", "=", "env_factory", ".", "generate_real_env", "(", ")", "\n", "\n", "# load additional agent configs", "\n", "with", "open", "(", "\"../default_config_acrobot.yaml\"", ",", "\"r\"", ")", "as", "stream", ":", "\n", "        ", "config_new", "=", "yaml", ".", "safe_load", "(", "stream", ")", "[", "\"agents\"", "]", "\n", "\n", "", "config", "[", "\"agents\"", "]", "[", "\"duelingddqn\"", "]", "=", "config_new", "[", "\"duelingddqn\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"duelingddqn_vary\"", "]", "=", "config_new", "[", "\"duelingddqn_vary\"", "]", "\n", "\n", "return", "virtual_env", ",", "real_env", ",", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_acrobot_vary_hp_2_DuelingDDQN.train_test_agents": [[33, 57], ["range", "agents.agent_utils.select_agent", "agents.agent_utils.select_agent.train", "agents.agent_utils.select_agent.test", "print", "reward_list.append", "train_steps_needed.append", "episodes_needed.append", "str", "sum", "len"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.agents.agent_utils.select_agent", "home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.train", "home.repos.pwc.inspect_result.automl_learning_environments.agents.base_agent.BaseAgent.test"], ["", "def", "train_test_agents", "(", "train_env", ",", "test_env", ",", "config", ",", "agents_num", ")", ":", "\n", "    ", "reward_list", "=", "[", "]", "\n", "train_steps_needed", "=", "[", "]", "\n", "episodes_needed", "=", "[", "]", "\n", "\n", "# settings for comparability", "\n", "config", "[", "'agents'", "]", "[", "'duelingddqn_vary'", "]", "[", "'vary_hp'", "]", "=", "True", "\n", "config", "[", "'agents'", "]", "[", "'duelingddqn'", "]", "[", "'print_rate'", "]", "=", "10", "\n", "config", "[", "'agents'", "]", "[", "'duelingddqn'", "]", "[", "'early_out_num'", "]", "=", "10", "\n", "config", "[", "'agents'", "]", "[", "'duelingddqn'", "]", "[", "'train_episodes'", "]", "=", "1000", "\n", "config", "[", "'agents'", "]", "[", "'duelingddqn'", "]", "[", "'init_episodes'", "]", "=", "10", "\n", "config", "[", "'agents'", "]", "[", "'duelingddqn'", "]", "[", "'test_episodes'", "]", "=", "10", "\n", "config", "[", "'agents'", "]", "[", "'duelingddqn'", "]", "[", "'early_out_virtual_diff'", "]", "=", "0.01", "\n", "\n", "for", "i", "in", "range", "(", "agents_num", ")", ":", "\n", "        ", "agent", "=", "select_agent", "(", "config", "=", "config", ",", "agent_name", "=", "'DuelingDDQN_vary'", ")", "\n", "reward_train", ",", "episode_length", ",", "_", "=", "agent", ".", "train", "(", "env", "=", "train_env", ")", "\n", "reward", ",", "_", ",", "_", "=", "agent", ".", "test", "(", "env", "=", "test_env", ")", "\n", "print", "(", "'reward: '", "+", "str", "(", "reward", ")", ")", "\n", "reward_list", ".", "append", "(", "reward", ")", "\n", "train_steps_needed", ".", "append", "(", "[", "sum", "(", "episode_length", ")", "]", ")", "\n", "episodes_needed", ".", "append", "(", "[", "(", "len", "(", "reward_train", ")", ")", "]", ")", "\n", "\n", "", "return", "reward_list", ",", "train_steps_needed", ",", "episodes_needed", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cmc_transfer_algo.get_best_models_from_log": [[29, 58], ["hpbandster.logged_results_to_HBS_result", "hpres.logged_results_to_HBS_result.data.values", "print", "best_models.sort", "os.path.isdir", "log_dir.replace.replace", "best_models.append", "os.path.isfile", "model_name.replace.replace"], "function", ["None"], ["def", "get_best_models_from_log", "(", "log_dir", ")", ":", "\n", "    ", "if", "not", "os", ".", "path", ".", "isdir", "(", "log_dir", ")", ":", "\n", "        ", "log_dir", "=", "log_dir", ".", "replace", "(", "'nierhoff'", ",", "'dingsda'", ")", "\n", "\n", "", "result", "=", "hpres", ".", "logged_results_to_HBS_result", "(", "log_dir", ")", "\n", "\n", "best_models", "=", "[", "]", "\n", "\n", "for", "value", "in", "result", ".", "data", ".", "values", "(", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "loss", "=", "value", ".", "results", "[", "1.0", "]", "[", "'loss'", "]", "\n", "model_name", "=", "value", ".", "results", "[", "1.0", "]", "[", "'info'", "]", "[", "'model_name'", "]", "\n", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "model_name", ")", ":", "\n", "                ", "model_name", "=", "model_name", ".", "replace", "(", "'nierhoff'", ",", "'dingsda'", ")", "\n", "", "best_models", ".", "append", "(", "(", "loss", ",", "model_name", ")", ")", "\n", "", "except", ":", "\n", "            ", "continue", "\n", "\n", "# before AUC (objective minimized)", "\n", "# print(\"sorting from low to high values (non-AUC)\")", "\n", "# best_models.sort(key=lambda x: x[0])", "\n", "\n", "# AUC (objective maximized)", "\n", "", "", "print", "(", "\"sorting from high to low values (AUC)\"", ")", "\n", "best_models", ".", "sort", "(", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ",", "reverse", "=", "True", ")", "\n", "best_models", "=", "best_models", "[", ":", "MODEL_NUM", "]", "\n", "\n", "return", "best_models", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cmc_transfer_algo.load_envs_and_config": [[60, 73], ["torch.load", "envs.env_factory.EnvFactory", "envs.env_factory.EnvFactory.generate_reward_env", "env_factory.generate_reward_env.load_state_dict", "envs.env_factory.EnvFactory.generate_real_env"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_reward_env", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_real_env"], ["", "def", "load_envs_and_config", "(", "model_file", ")", ":", "\n", "    ", "save_dict", "=", "torch", ".", "load", "(", "model_file", ")", "\n", "\n", "config", "=", "save_dict", "[", "'config'", "]", "\n", "config", "[", "'device'", "]", "=", "'cpu'", "\n", "config", "[", "'envs'", "]", "[", "'MountainCarContinuous-v0'", "]", "[", "'solved_reward'", "]", "=", "100000", "# something big enough to prevent early out triggering", "\n", "\n", "env_factory", "=", "EnvFactory", "(", "config", "=", "config", ")", "\n", "reward_env", "=", "env_factory", ".", "generate_reward_env", "(", ")", "\n", "reward_env", ".", "load_state_dict", "(", "save_dict", "[", "'model'", "]", ")", "\n", "real_env", "=", "env_factory", ".", "generate_real_env", "(", ")", "\n", "\n", "return", "reward_env", ",", "real_env", ",", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cmc_transfer_algo.train_test_agents": [[75, 129], ["range", "agents.agent_utils.select_agent.train", "print", "rewards.append", "episode_lengths.append", "agents.agent_utils.select_agent", "agents.agent_utils.select_agent", "str"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.train", "home.repos.pwc.inspect_result.automl_learning_environments.agents.agent_utils.select_agent", "home.repos.pwc.inspect_result.automl_learning_environments.agents.agent_utils.select_agent"], ["", "def", "train_test_agents", "(", "mode", ",", "env", ",", "real_env", ",", "config", ")", ":", "\n", "    ", "rewards", "=", "[", "]", "\n", "episode_lengths", "=", "[", "]", "\n", "\n", "# settings for comparability", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "=", "{", "}", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'test_episodes'", "]", "=", "1", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'train_episodes'", "]", "=", "3000", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'print_rate'", "]", "=", "100", "\n", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'init_episodes'", "]", "=", "0", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'update_episodes'", "]", "=", "10", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'ppo_epochs'", "]", "=", "80", "# due to reward sparsity and to speed up training", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'gamma'", "]", "=", "0.99", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'lr'", "]", "=", "3e-4", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'vf_coef'", "]", "=", "1", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'ent_coef'", "]", "=", "0.01", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'eps_clip'", "]", "=", "0.2", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'rb_size'", "]", "=", "1000000", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'same_action_num'", "]", "=", "5", "# due to reward sparsity and on-policy", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'activation_fn'", "]", "=", "'relu'", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'hidden_size'", "]", "=", "64", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'hidden_layer'", "]", "=", "2", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'action_std'", "]", "=", "0.5", "\n", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'early_out_num'", "]", "=", "10", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'early_out_virtual_diff'", "]", "=", "0.02", "\n", "\n", "# optimized ICM HPs:", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "=", "{", "}", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "[", "'beta'", "]", "=", "0.1", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "[", "'eta'", "]", "=", "0.01", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "[", "'feature_dim'", "]", "=", "32", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "[", "'hidden_size'", "]", "=", "128", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "[", "'lr'", "]", "=", "5e-4", "\n", "\n", "# default ICM HPs:", "\n", "# config['agents']['icm'] = {}", "\n", "# config['agents']['icm']['beta'] = 0.2", "\n", "# config['agents']['icm']['eta'] = 0.5", "\n", "# config['agents']['icm']['feature_dim'] = 64", "\n", "# config['agents']['icm']['hidden_size'] = 128", "\n", "# config['agents']['icm']['lr'] = 1e-4", "\n", "\n", "for", "i", "in", "range", "(", "MODEL_AGENTS", ")", ":", "\n", "        ", "if", "mode", "==", "'-1'", ":", "\n", "            ", "agent", "=", "select_agent", "(", "config", "=", "config", ",", "agent_name", "=", "'ppo_icm'", ")", "\n", "", "else", ":", "\n", "            ", "agent", "=", "select_agent", "(", "config", "=", "config", ",", "agent_name", "=", "'ppo'", ")", "\n", "", "reward", ",", "episode_length", ",", "_", "=", "agent", ".", "train", "(", "env", "=", "env", ",", "test_env", "=", "real_env", ")", "\n", "print", "(", "'reward: '", "+", "str", "(", "reward", ")", ")", "\n", "rewards", ".", "append", "(", "reward", ")", "\n", "episode_lengths", ".", "append", "(", "episode_length", ")", "\n", "", "return", "rewards", ",", "episode_lengths", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cmc_transfer_algo.save_list": [[131, 141], ["os.makedirs", "os.path.join", "torch.save", "str"], "function", ["None"], ["", "def", "save_list", "(", "mode", ",", "config", ",", "reward_list", ",", "episode_length_list", ")", ":", "\n", "    ", "os", ".", "makedirs", "(", "SAVE_DIR", ",", "exist_ok", "=", "True", ")", "\n", "file_name", "=", "os", ".", "path", ".", "join", "(", "SAVE_DIR", ",", "'best_transfer_algo'", "+", "str", "(", "mode", ")", "+", "'.pt'", ")", "\n", "save_dict", "=", "{", "}", "\n", "save_dict", "[", "'config'", "]", "=", "config", "\n", "save_dict", "[", "'model_num'", "]", "=", "MODEL_NUM", "\n", "save_dict", "[", "'model_agents'", "]", "=", "MODEL_AGENTS", "\n", "save_dict", "[", "'reward_list'", "]", "=", "reward_list", "\n", "save_dict", "[", "'episode_length_list'", "]", "=", "episode_length_list", "\n", "torch", ".", "save", "(", "save_dict", ",", "file_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cmc_transfer_algo.eval_models": [[143, 158], ["GTNC_evaluate_cmc_transfer_algo.get_best_models_from_log", "GTNC_evaluate_cmc_transfer_algo.save_list", "print", "print", "GTNC_evaluate_cmc_transfer_algo.load_envs_and_config", "GTNC_evaluate_cmc_transfer_algo.train_test_agents", "str", "str"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.get_best_models_from_log", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.save_list", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.load_envs_and_config", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.train_test_agents"], ["", "def", "eval_models", "(", "mode", ",", "log_dir", ")", ":", "\n", "    ", "best_models", "=", "get_best_models_from_log", "(", "log_dir", ")", "\n", "\n", "reward_list", "=", "[", "]", "\n", "episode_length_list", "=", "[", "]", "\n", "\n", "for", "best_reward", ",", "model_file", "in", "best_models", ":", "\n", "        ", "print", "(", "'best reward: '", "+", "str", "(", "best_reward", ")", ")", "\n", "print", "(", "'model file: '", "+", "str", "(", "model_file", ")", ")", "\n", "reward_env", ",", "real_env", ",", "config", "=", "load_envs_and_config", "(", "model_file", ")", "\n", "rewards", ",", "episode_lengths", "=", "train_test_agents", "(", "mode", "=", "mode", ",", "env", "=", "reward_env", ",", "real_env", "=", "real_env", ",", "config", "=", "config", ")", "\n", "reward_list", "+=", "rewards", "\n", "episode_length_list", "+=", "episode_lengths", "\n", "\n", "", "save_list", "(", "mode", ",", "config", ",", "reward_list", ",", "episode_length_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cmc_transfer_algo.eval_base": [[160, 173], ["GTNC_evaluate_cmc_transfer_algo.get_best_models_from_log", "range", "GTNC_evaluate_cmc_transfer_algo.save_list", "GTNC_evaluate_cmc_transfer_algo.load_envs_and_config", "GTNC_evaluate_cmc_transfer_algo.train_test_agents"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.get_best_models_from_log", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.save_list", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.load_envs_and_config", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.train_test_agents"], ["", "def", "eval_base", "(", "mode", ",", "log_dir", ")", ":", "\n", "    ", "best_models", "=", "get_best_models_from_log", "(", "log_dir", ")", "\n", "\n", "reward_list", "=", "[", "]", "\n", "episode_length_list", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "MODEL_NUM", ")", ":", "\n", "        ", "reward_env", ",", "real_env", ",", "config", "=", "load_envs_and_config", "(", "best_models", "[", "0", "]", "[", "1", "]", ")", "\n", "rewards", ",", "episode_lengths", "=", "train_test_agents", "(", "mode", "=", "mode", ",", "env", "=", "real_env", ",", "real_env", "=", "real_env", ",", "config", "=", "config", ")", "\n", "reward_list", "+=", "rewards", "\n", "episode_length_list", "+=", "episode_lengths", "\n", "\n", "", "save_list", "(", "mode", ",", "config", ",", "reward_list", ",", "episode_length_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cmc_transfer_algo.eval_icm": [[175, 188], ["GTNC_evaluate_cmc_transfer_algo.get_best_models_from_log", "range", "GTNC_evaluate_cmc_transfer_algo.save_list", "GTNC_evaluate_cmc_transfer_algo.load_envs_and_config", "GTNC_evaluate_cmc_transfer_algo.train_test_agents"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.get_best_models_from_log", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.save_list", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.load_envs_and_config", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.train_test_agents"], ["", "def", "eval_icm", "(", "mode", ",", "log_dir", ")", ":", "\n", "    ", "best_models", "=", "get_best_models_from_log", "(", "log_dir", ")", "\n", "\n", "reward_list", "=", "[", "]", "\n", "episode_length_list", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "MODEL_NUM", ")", ":", "\n", "        ", "reward_env", ",", "real_env", ",", "config", "=", "load_envs_and_config", "(", "best_models", "[", "0", "]", "[", "1", "]", ")", "\n", "rewards", ",", "episode_lengths", "=", "train_test_agents", "(", "mode", "=", "mode", ",", "env", "=", "real_env", ",", "real_env", "=", "real_env", ",", "config", "=", "config", ")", "\n", "reward_list", "+=", "rewards", "\n", "episode_length_list", "+=", "episode_lengths", "\n", "\n", "", "save_list", "(", "mode", ",", "config", ",", "reward_list", ",", "episode_length_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cmc_compare_reward_envs_print_baseline.get_data": [[8, 19], ["torch.load", "print", "print", "len", "max"], "function", ["None"], ["def", "get_data", "(", ")", ":", "\n", "    ", "data", "=", "torch", ".", "load", "(", "LOG_FILE", ")", "\n", "reward_list", "=", "data", "[", "'reward_list'", "]", "\n", "\n", "print", "(", "len", "(", "reward_list", ")", ")", "\n", "count", "=", "0", "\n", "for", "rewards", "in", "reward_list", ":", "\n", "        ", "if", "max", "(", "rewards", ")", ">", "90", ":", "\n", "            ", "count", "+=", "1", "\n", "\n", "", "", "print", "(", "count", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.bohb_params_PPO_pendulum.ExperimentWrapper.get_bohb_parameters": [[15, 24], ["None"], "methods", ["None"], ["    ", "def", "get_bohb_parameters", "(", "self", ")", ":", "\n", "        ", "params", "=", "{", "}", "\n", "params", "[", "'min_budget'", "]", "=", "1000", "\n", "params", "[", "'max_budget'", "]", "=", "8000", "\n", "params", "[", "'eta'", "]", "=", "2", "\n", "params", "[", "'iterations'", "]", "=", "1000", "\n", "params", "[", "'random_fraction'", "]", "=", "0.3", "\n", "\n", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.bohb_params_PPO_pendulum.ExperimentWrapper.get_configspace": [[25, 42], ["ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter"], "methods", ["None"], ["", "def", "get_configspace", "(", "self", ")", ":", "\n", "        ", "cs", "=", "CS", ".", "ConfigurationSpace", "(", ")", "\n", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'ppo_update_episodes'", ",", "lower", "=", "1", ",", "upper", "=", "100", ",", "log", "=", "True", ",", "default_value", "=", "20", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'ppo_ppo_epochs'", ",", "lower", "=", "20", ",", "upper", "=", "200", ",", "log", "=", "True", ",", "default_value", "=", "100", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'ppo_gamma'", ",", "lower", "=", "0.001", ",", "upper", "=", "0.1", ",", "log", "=", "True", ",", "default_value", "=", "0.01", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'ppo_lr'", ",", "lower", "=", "1e-4", ",", "upper", "=", "1e-2", ",", "log", "=", "True", ",", "default_value", "=", "3e-4", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'ppo_vf_coef'", ",", "lower", "=", "0.1", ",", "upper", "=", "2", ",", "log", "=", "True", ",", "default_value", "=", "0.5", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'ppo_ent_coef'", ",", "lower", "=", "0.002", ",", "upper", "=", "0.05", ",", "log", "=", "True", ",", "default_value", "=", "0.01", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'ppo_eps_clip'", ",", "lower", "=", "0.05", ",", "upper", "=", "1", ",", "log", "=", "True", ",", "default_value", "=", "0.2", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "CategoricalHyperparameter", "(", "name", "=", "'ppo_activation_fn'", ",", "choices", "=", "[", "'relu'", ",", "'tanh'", ",", "'leakyrelu'", ",", "'prelu'", "]", ",", "default_value", "=", "'relu'", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'ppo_hidden_size'", ",", "lower", "=", "48", ",", "upper", "=", "192", ",", "log", "=", "True", ",", "default_value", "=", "128", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'ppo_hidden_layer'", ",", "lower", "=", "1", ",", "upper", "=", "2", ",", "log", "=", "False", ",", "default_value", "=", "2", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'ppo_action_std'", ",", "lower", "=", "0.1", ",", "upper", "=", "2", ",", "log", "=", "True", ",", "default_value", "=", "0.5", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'ppo_early_out_virtual_diff'", ",", "lower", "=", "1e-2", ",", "upper", "=", "1e-1", ",", "log", "=", "True", ",", "default_value", "=", "3e-2", ")", ")", "\n", "\n", "return", "cs", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.bohb_params_PPO_pendulum.ExperimentWrapper.get_specific_config": [[43, 61], ["copy.deepcopy"], "methods", ["None"], ["", "def", "get_specific_config", "(", "self", ",", "cso", ",", "default_config", ",", "budget", ")", ":", "\n", "        ", "config", "=", "deepcopy", "(", "default_config", ")", "\n", "\n", "config", "[", "\"agents\"", "]", "[", "\"ppo\"", "]", "[", "\"update_episodes\"", "]", "=", "cso", "[", "\"ppo_update_episodes\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"ppo\"", "]", "[", "\"ppo_epochs\"", "]", "=", "cso", "[", "\"ppo_ppo_epochs\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"ppo\"", "]", "[", "\"gamma\"", "]", "=", "1", "-", "cso", "[", "\"ppo_gamma\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"ppo\"", "]", "[", "\"lr\"", "]", "=", "cso", "[", "\"ppo_lr\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"ppo\"", "]", "[", "\"vf_coef\"", "]", "=", "cso", "[", "\"ppo_vf_coef\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"ppo\"", "]", "[", "\"ent_coef\"", "]", "=", "cso", "[", "\"ppo_ent_coef\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"ppo\"", "]", "[", "\"eps_clip\"", "]", "=", "cso", "[", "\"ppo_eps_clip\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"ppo\"", "]", "[", "\"activation_fn\"", "]", "=", "cso", "[", "\"ppo_activation_fn\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"ppo\"", "]", "[", "\"hidden_size\"", "]", "=", "cso", "[", "\"ppo_hidden_size\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"ppo\"", "]", "[", "\"hidden_layer\"", "]", "=", "cso", "[", "\"ppo_hidden_layer\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"ppo\"", "]", "[", "\"action_std\"", "]", "=", "cso", "[", "\"ppo_action_std\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"ppo\"", "]", "[", "\"early_out_virtual_diff\"", "]", "=", "cso", "[", "\"ppo_early_out_virtual_diff\"", "]", "\n", "config", "[", "\"device\"", "]", "=", "'cuda'", "\n", "\n", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.bohb_params_PPO_pendulum.ExperimentWrapper.compute": [[62, 101], ["bohb_params_PPO_pendulum.ExperimentWrapper.get_specific_config", "print", "print", "print", "print", "print", "print", "int", "envs.env_factory.EnvFactory", "envs.env_factory.EnvFactory.generate_real_env", "range", "str", "print", "print", "print", "print", "open", "yaml.safe_load", "agents.PPO.PPO", "agents.PPO.PPO.train", "len", "str", "str", "str", "str"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_step_size.ExperimentWrapper.get_specific_config", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_real_env", "home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.train"], ["", "def", "compute", "(", "self", ",", "working_dir", ",", "bohb_id", ",", "config_id", ",", "cso", ",", "budget", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "with", "open", "(", "\"default_config_pendulum.yaml\"", ",", "'r'", ")", "as", "stream", ":", "\n", "            ", "default_config", "=", "yaml", ".", "safe_load", "(", "stream", ")", "\n", "\n", "", "config", "=", "self", ".", "get_specific_config", "(", "cso", ",", "default_config", ",", "budget", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "\"START BOHB ITERATION\"", ")", "\n", "print", "(", "'CONFIG: '", "+", "str", "(", "config", ")", ")", "\n", "print", "(", "'CSO:    '", "+", "str", "(", "cso", ")", ")", "\n", "print", "(", "'BUDGET: '", "+", "str", "(", "budget", ")", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "config", "[", "\"agents\"", "]", "[", "\"ppo\"", "]", "[", "\"train_episodes\"", "]", "=", "int", "(", "budget", ")", "\n", "\n", "info", "=", "{", "}", "\n", "\n", "# generate environment", "\n", "env_fac", "=", "EnvFactory", "(", "config", ")", "\n", "real_env", "=", "env_fac", ".", "generate_real_env", "(", ")", "\n", "\n", "score", "=", "0", "\n", "for", "i", "in", "range", "(", "NUM_EVALS", ")", ":", "\n", "            ", "ppo", "=", "PPO", "(", "env", "=", "real_env", ",", "\n", "config", "=", "config", ")", "\n", "rewards", ",", "_", ",", "_", "=", "ppo", ".", "train", "(", "real_env", ")", "\n", "score", "+=", "len", "(", "rewards", ")", "\n", "\n", "", "score", "=", "score", "/", "NUM_EVALS", "\n", "\n", "info", "[", "'config'", "]", "=", "str", "(", "config", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "'FINAL SCORE: '", "+", "str", "(", "score", ")", ")", "\n", "print", "(", "\"END BOHB ITERATION\"", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "return", "{", "\n", "\"loss\"", ":", "score", ",", "\n", "\"info\"", ":", "info", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cmc_compare_reward_envs_debug_td3.get_data": [[29, 78], ["float", "max", "torch.load", "list_data.append", "[].items", "list_hyperparameters.append", "numpy.zeros", "enumerate", "numpy.mean", "numpy.std", "proc_data.append", "print", "min", "zip", "range", "numpy.array", "sum", "sum", "len", "len", "concat_list.append"], "function", ["None"], ["def", "get_data", "(", ")", ":", "\n", "    ", "list_data", "=", "[", "]", "\n", "list_hyperparameters", "=", "[", "]", "\n", "for", "log_file", "in", "LOG_FILES", ":", "\n", "        ", "file_name_hyperparameters", "=", "\"\"", "\n", "data", "=", "torch", ".", "load", "(", "log_file", ")", "\n", "list_data", ".", "append", "(", "(", "data", "[", "'reward_list'", "]", ",", "data", "[", "'episode_length_list'", "]", ")", ")", "\n", "model_num", "=", "data", "[", "'model_num'", "]", "\n", "model_agents", "=", "data", "[", "'model_agents'", "]", "\n", "for", "k", ",", "v", "in", "data", "[", "\"config\"", "]", "[", "\"agents\"", "]", "[", "\"td3\"", "]", ".", "items", "(", ")", ":", "\n", "            ", "if", "k", "in", "IMPORTANT_KEYS", ":", "\n", "                ", "file_name_hyperparameters", "+=", "f\"{k}: {v} \"", "\n", "\n", "", "", "list_hyperparameters", ".", "append", "(", "file_name_hyperparameters", ")", "\n", "\n", "", "min_steps", "=", "float", "(", "'Inf'", ")", "\n", "# get minimum number of evaluations", "\n", "for", "reward_list", ",", "episode_length_list", "in", "list_data", ":", "\n", "        ", "for", "episode_lengths", "in", "episode_length_list", ":", "\n", "            ", "print", "(", "sum", "(", "episode_lengths", ")", ")", "\n", "min_steps", "=", "min", "(", "min_steps", ",", "sum", "(", "episode_lengths", ")", ")", "\n", "\n", "", "", "min_steps", "=", "max", "(", "min_steps", ",", "MIN_STEPS", ")", "\n", "# convert data from episodes to steps", "\n", "proc_data", "=", "[", "]", "\n", "\n", "for", "reward_list", ",", "episode_length_list", "in", "list_data", ":", "\n", "        ", "np_data", "=", "np", ".", "zeros", "(", "[", "model_num", "*", "model_agents", ",", "min_steps", "]", ")", "\n", "\n", "for", "it", ",", "data", "in", "enumerate", "(", "zip", "(", "reward_list", ",", "episode_length_list", ")", ")", ":", "\n", "            ", "rewards", ",", "episode_lengths", "=", "data", "\n", "\n", "concat_list", "=", "[", "]", "\n", "rewards", "=", "rewards", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "episode_lengths", ")", ")", ":", "\n", "                ", "concat_list", "+=", "[", "rewards", "[", "i", "]", "]", "*", "episode_lengths", "[", "i", "]", "\n", "\n", "", "while", "len", "(", "concat_list", ")", "<", "min_steps", ":", "\n", "                ", "concat_list", ".", "append", "(", "concat_list", "[", "-", "1", "]", ")", "\n", "\n", "", "np_data", "[", "it", "]", "=", "np", ".", "array", "(", "concat_list", "[", ":", "min_steps", "]", ")", "\n", "\n", "", "mean", "=", "np", ".", "mean", "(", "np_data", ",", "axis", "=", "0", ")", "\n", "std", "=", "np", ".", "std", "(", "np_data", ",", "axis", "=", "0", ")", "\n", "\n", "proc_data", ".", "append", "(", "(", "mean", ",", "std", ")", ")", "\n", "\n", "", "return", "proc_data", ",", "list_hyperparameters", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cmc_compare_reward_envs_debug_td3.plot_data": [[80, 111], ["matplotlib.figure", "plt.figure.add_subplot", "matplotlib.legend", "matplotlib.tight_layout", "matplotlib.subplots_adjust", "matplotlib.title", "matplotlib.xlabel", "matplotlib.xlim", "matplotlib.ylim", "matplotlib.ylabel", "matplotlib.savefig", "matplotlib.show", "matplotlib.plot", "matplotlib.fill_between", "range", "len"], "function", ["None"], ["", "def", "plot_data", "(", "proc_data", ",", "savefig_name", ",", "list_hyperparameters", ")", ":", "\n", "# fig, ax = plt.subplots(dpi=600, figsize=(10,10))", "\n", "# colors = []", "\n", "#", "\n", "# for mean, _ in data_w:", "\n", "#     plt.plot(mean_w)", "\n", "#     colors.append(plt.gca().lines[-1].get_color())", "\n", "    ", "f", "=", "plt", ".", "figure", "(", "figsize", "=", "(", "15", ",", "15", ")", ")", "\n", "ax", "=", "f", ".", "add_subplot", "(", "111", ")", "\n", "\n", "for", "mean", ",", "std", "in", "proc_data", ":", "\n", "        ", "plt", ".", "plot", "(", "mean", ")", "\n", "\n", "", "for", "mean", ",", "std", "in", "proc_data", ":", "\n", "        ", "plt", ".", "fill_between", "(", "x", "=", "range", "(", "len", "(", "mean", ")", ")", ",", "y1", "=", "mean", "-", "std", "*", "STD_MULT", ",", "y2", "=", "mean", "+", "std", "*", "STD_MULT", ",", "alpha", "=", "0.1", ")", "\n", "# plt.legend(loc=(1.04, 0))", "\n", "", "plt", ".", "legend", "(", "list_hyperparameters", ",", "fontsize", "=", "9", ",", "loc", "=", "'best'", ")", "\n", "plt", ".", "tight_layout", "(", ")", "\n", "# plt.legend(bbox_to_anchor=(1.04, 1), borderaxespad=0)", "\n", "# plt.subplots_adjust(right=0.7)", "\n", "# plt.legend(('TD3'), fontsize=7)", "\n", "# plt.xlim(0,99)", "\n", "plt", ".", "subplots_adjust", "(", "bottom", "=", "0.15", ",", "left", "=", "0.15", ")", "\n", "plt", ".", "title", "(", "'MountainCarContinuous-v0'", ")", "\n", "plt", ".", "xlabel", "(", "'steps'", ")", "\n", "plt", ".", "xlim", "(", "0", ",", "100000", ")", "\n", "# plt.xlim(0, 60000)", "\n", "plt", ".", "ylim", "(", "-", "75", ",", "100", ")", "\n", "plt", ".", "ylabel", "(", "'cumulative reward'", ")", "\n", "plt", ".", "savefig", "(", "savefig_name", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_pendulum_compare_reward_envs.get_data": [[13, 35], ["len", "torch.load", "list_data.append", "numpy.zeros", "range", "numpy.mean", "numpy.std", "proc_data.append", "len", "numpy.array"], "function", ["None"], ["def", "get_data", "(", ")", ":", "\n", "    ", "list_data", "=", "[", "]", "\n", "for", "log_file", "in", "LOG_FILES", ":", "\n", "        ", "data", "=", "torch", ".", "load", "(", "log_file", ")", "\n", "list_data", ".", "append", "(", "data", "[", "'reward_list'", "]", ")", "\n", "\n", "# copy from list to numpy array", "\n", "", "proc_data", "=", "[", "]", "\n", "\n", "n", "=", "len", "(", "list_data", "[", "0", "]", "[", "0", "]", ")", "\n", "for", "data", "in", "list_data", ":", "\n", "        ", "np_data", "=", "np", ".", "zeros", "(", "[", "MAX_VALS", ",", "n", "]", ")", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "np_data", ")", ")", ":", "\n", "            ", "np_data", "[", "i", "]", "=", "np", ".", "array", "(", "data", "[", "i", "]", ")", "\n", "\n", "", "mean", "=", "np", ".", "mean", "(", "np_data", ",", "axis", "=", "0", ")", "\n", "std", "=", "np", ".", "std", "(", "np_data", ",", "axis", "=", "0", ")", "\n", "\n", "proc_data", ".", "append", "(", "(", "mean", ",", "std", ")", ")", "\n", "\n", "", "return", "proc_data", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_pendulum_compare_reward_envs.plot_data": [[37, 58], ["matplotlib.subplots", "matplotlib.legend", "matplotlib.xlim", "matplotlib.title", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.savefig", "matplotlib.show", "matplotlib.plot", "matplotlib.fill_between", "range", "len"], "function", ["None"], ["", "def", "plot_data", "(", "proc_data", ",", "savefig_name", ")", ":", "\n", "    ", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", "dpi", "=", "600", ",", "figsize", "=", "(", "5", ",", "4", ")", ")", "\n", "colors", "=", "[", "]", "\n", "#", "\n", "# for mean, _ in data_w:", "\n", "#     plt.plot(mean_w)", "\n", "#     colors.append(plt.gca().lines[-1].get_color())", "\n", "\n", "for", "mean", ",", "std", "in", "proc_data", ":", "\n", "        ", "plt", ".", "plot", "(", "mean", ")", "\n", "\n", "", "for", "mean", ",", "std", "in", "proc_data", ":", "\n", "        ", "plt", ".", "fill_between", "(", "x", "=", "range", "(", "len", "(", "mean", ")", ")", ",", "y1", "=", "mean", "-", "std", "*", "STD_MULT", ",", "y2", "=", "mean", "+", "std", "*", "STD_MULT", ",", "alpha", "=", "0.1", ")", "\n", "\n", "", "plt", ".", "legend", "(", "(", "'baseline ICM'", ",", "'baseline naive'", ",", "'best NN w/o info vector'", ")", ")", "\n", "plt", ".", "xlim", "(", "0", ",", "49", ")", "\n", "plt", ".", "title", "(", "'Pendulum-v0'", ")", "\n", "plt", ".", "xlabel", "(", "'episode'", ")", "\n", "plt", ".", "ylabel", "(", "'average reward'", ")", "\n", "plt", ".", "savefig", "(", "savefig_name", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld_learned_reward_env.idx_to_xy": [[20, 24], ["None"], "function", ["None"], ["def", "idx_to_xy", "(", "idx", ",", "n", ")", ":", "\n", "    ", "x", "=", "idx", "//", "n", "\n", "y", "=", "idx", "%", "n", "\n", "return", "y", ",", "-", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld_learned_reward_env.xy_to_idx": [[26, 30], ["None"], "function", ["None"], ["", "def", "xy_to_idx", "(", "xy", ",", "n", ")", ":", "\n", "    ", "y", ",", "x", "=", "xy", "\n", "obs", "=", "-", "x", "*", "n", "+", "y", "\n", "return", "obs", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld_learned_reward_env.get_best_models_from_log": [[32, 54], ["hpbandster.logged_results_to_HBS_result", "hpres.logged_results_to_HBS_result.data.values", "best_models.sort", "os.path.isdir", "log_dir.replace.replace", "best_models.append", "os.path.isfile", "model_name.replace.replace"], "function", ["None"], ["", "def", "get_best_models_from_log", "(", "log_dir", ")", ":", "\n", "    ", "if", "not", "os", ".", "path", ".", "isdir", "(", "log_dir", ")", ":", "\n", "        ", "log_dir", "=", "log_dir", ".", "replace", "(", "'nierhoff'", ",", "'dingsda'", ")", "\n", "\n", "", "result", "=", "hpres", ".", "logged_results_to_HBS_result", "(", "log_dir", ")", "\n", "\n", "best_models", "=", "[", "]", "\n", "\n", "for", "value", "in", "result", ".", "data", ".", "values", "(", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "model_name", "=", "value", ".", "results", "[", "1.0", "]", "[", "'info'", "]", "[", "'model_name'", "]", "\n", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "model_name", ")", ":", "\n", "                ", "model_name", "=", "model_name", ".", "replace", "(", "'nierhoff'", ",", "'dingsda'", ")", "\n", "", "best_models", ".", "append", "(", "model_name", ")", "\n", "", "except", ":", "\n", "            ", "continue", "\n", "\n", "", "", "best_models", ".", "sort", "(", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ")", "\n", "best_models", "=", "best_models", "[", ":", "MODEL_NUM", "]", "\n", "\n", "return", "best_models", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld_learned_reward_env.eval_models": [[56, 89], ["GTNC_visualize_gridworld_learned_reward_env.get_best_models_from_log", "GTNC_visualize_gridworld_learned_reward_env.load_envs_and_config", "len", "len", "range", "reward_env.get_state_dim", "range", "reward_env.get_action_dim", "reward_env.set_agent_params", "reward_env.env.real_env.env._obs_to_state", "reward_env.step", "torch.tensor", "reward.item", "reward_dict[].append", "reward_dict[].append", "reward.item", "reward.item", "reward.item", "reward.item"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.get_best_models_from_log", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.load_envs_and_config", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.get_state_dim", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.get_action_dim", "home.repos.pwc.inspect_result.automl_learning_environments.envs.reward_env.RewardEnv.set_agent_params", "home.repos.pwc.inspect_result.automl_learning_environments.envs.gridworld.GridworldEnv._obs_to_state", "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditEnv.step"], ["", "def", "eval_models", "(", "log_dir", ")", ":", "\n", "    ", "best_models", "=", "get_best_models_from_log", "(", "log_dir", ")", "\n", "\n", "info_dict", "=", "{", "}", "\n", "reward_dict", "=", "{", "}", "\n", "\n", "info_dict", "[", "'mode'", "]", "=", "log_dir", "[", "-", "1", "]", "\n", "\n", "for", "model_file", "in", "best_models", ":", "\n", "        ", "reward_env", ",", "real_env", ",", "config", "=", "load_envs_and_config", "(", "model_file", ")", "\n", "info_dict", "[", "'m'", "]", "=", "len", "(", "real_env", ".", "env", ".", "grid", ")", "\n", "info_dict", "[", "'n'", "]", "=", "len", "(", "real_env", ".", "env", ".", "grid", "[", "0", "]", ")", "\n", "\n", "for", "state", "in", "range", "(", "reward_env", ".", "get_state_dim", "(", ")", ")", ":", "\n", "            ", "for", "action", "in", "range", "(", "reward_env", ".", "get_action_dim", "(", ")", ")", ":", "\n", "                ", "reward_env", ".", "set_agent_params", "(", "same_action_num", "=", "1", ",", "gamma", "=", "config", "[", "'agents'", "]", "[", "'ql'", "]", "[", "'gamma'", "]", ")", "\n", "reward_env", ".", "env", ".", "real_env", ".", "env", ".", "state", "=", "reward_env", ".", "env", ".", "real_env", ".", "env", ".", "_obs_to_state", "(", "state", ")", "\n", "reward_env", ".", "env", ".", "state", "=", "state", "\n", "next_state", ",", "reward", ",", "_", "=", "reward_env", ".", "step", "(", "action", "=", "torch", ".", "tensor", "(", "[", "action", "]", ")", ")", "\n", "\n", "if", "SIMPLIFY", ":", "\n", "                    ", "if", "reward", ".", "item", "(", ")", ">", "-", "50", ":", "\n", "                        ", "if", "(", "state", ",", "action", ")", "not", "in", "reward_dict", ":", "\n", "                            ", "reward_dict", "[", "(", "state", ",", "action", ")", "]", "=", "[", "reward", ".", "item", "(", ")", "]", "\n", "", "else", ":", "\n", "                            ", "reward_dict", "[", "(", "state", ",", "action", ")", "]", ".", "append", "(", "reward", ".", "item", "(", ")", ")", "\n", "", "", "", "else", ":", "\n", "                    ", "if", "(", "state", ",", "action", ")", "not", "in", "reward_dict", ":", "\n", "                        ", "reward_dict", "[", "(", "state", ",", "action", ")", "]", "=", "[", "reward", ".", "item", "(", ")", "]", "\n", "", "else", ":", "\n", "                        ", "reward_dict", "[", "(", "state", ",", "action", ")", "]", ".", "append", "(", "reward", ".", "item", "(", ")", ")", "\n", "\n", "", "", "", "", "", "return", "reward_dict", ",", "info_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld_learned_reward_env.map_intensity_to_color": [[91, 100], ["numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array"], "function", ["None"], ["", "def", "map_intensity_to_color", "(", "intensity", ")", ":", "\n", "    ", "if", "intensity", "<=", "0", ":", "\n", "        ", "return", "np", ".", "array", "(", "[", "1", ",", "0", ",", "0", "]", ")", "\n", "", "elif", "intensity", "<=", "0.5", ":", "\n", "        ", "return", "np", ".", "array", "(", "[", "1", ",", "0", ",", "0", "]", ")", "+", "2", "*", "intensity", "*", "np", ".", "array", "(", "[", "0", ",", "1", ",", "0", "]", ")", "\n", "", "elif", "intensity", "<=", "1", ":", "\n", "        ", "return", "np", ".", "array", "(", "[", "1", ",", "1", ",", "0", "]", ")", "+", "2", "*", "(", "intensity", "-", "0.5", ")", "*", "np", ".", "array", "(", "[", "-", "1", ",", "0", ",", "0", "]", ")", "\n", "", "else", ":", "\n", "        ", "return", "np", ".", "array", "(", "[", "0", ",", "1", ",", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld_learned_reward_env.draw_filled_polygon": [[102, 119], ["GTNC_visualize_gridworld_learned_reward_env.idx_to_xy", "matplotlib.fill", "GTNC_visualize_gridworld_learned_reward_env.map_intensity_to_color"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld_heatmap.idx_to_xy", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld.map_intensity_to_color"], ["", "", "def", "draw_filled_polygon", "(", "state", ",", "action", ",", "n", ",", "intensity", ")", ":", "\n", "    ", "x", ",", "y", "=", "idx_to_xy", "(", "state", ",", "n", ")", "\n", "\n", "if", "action", "==", "G_RIGHT", ":", "\n", "        ", "xs", "=", "[", "x", "+", "0.5", ",", "x", "+", "0.5", ",", "x", "]", "\n", "ys", "=", "[", "y", "-", "0.5", ",", "y", "+", "0.5", ",", "y", "]", "\n", "", "elif", "action", "==", "G_LEFT", ":", "\n", "        ", "xs", "=", "[", "x", "-", "0.5", ",", "x", "-", "0.5", ",", "x", "]", "\n", "ys", "=", "[", "y", "-", "0.5", ",", "y", "+", "0.5", ",", "y", "]", "\n", "", "elif", "action", "==", "G_DOWN", ":", "\n", "        ", "xs", "=", "[", "x", "-", "0.5", ",", "x", "+", "0.5", ",", "x", "]", "\n", "ys", "=", "[", "y", "-", "0.5", ",", "y", "-", "0.5", ",", "y", "]", "\n", "", "elif", "action", "==", "G_UP", ":", "\n", "        ", "xs", "=", "[", "x", "-", "0.5", ",", "x", "+", "0.5", ",", "x", "]", "\n", "ys", "=", "[", "y", "+", "0.5", ",", "y", "+", "0.5", ",", "y", "]", "\n", "\n", "", "plt", ".", "fill", "(", "xs", ",", "ys", ",", "facecolor", "=", "map_intensity_to_color", "(", "intensity", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld_learned_reward_env.plot_models": [[121, 186], ["float", "float", "reward_dict.items", "print", "print", "matplotlib.subplots", "reward_avg_dict.items", "range", "range", "matplotlib.plot", "matplotlib.text", "matplotlib.text", "matplotlib.text", "ax.axis", "ax.axis", "matplotlib.show", "statistics.mean", "min", "max", "GTNC_visualize_gridworld_learned_reward_env.draw_filled_polygon", "matplotlib.plot", "matplotlib.plot", "matplotlib.savefig", "matplotlib.savefig", "matplotlib.title", "matplotlib.title", "matplotlib.title", "matplotlib.title", "matplotlib.title", "matplotlib.title", "str", "str", "matplotlib.title", "matplotlib.title"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld_learned_reward_env.draw_filled_polygon"], ["", "def", "plot_models", "(", "reward_dict", ",", "info_dict", ")", ":", "\n", "# create average reward_dict", "\n", "    ", "min_val", "=", "float", "(", "'Inf'", ")", "\n", "max_val", "=", "float", "(", "'-Inf'", ")", "\n", "reward_avg_dict", "=", "{", "}", "\n", "for", "key", ",", "value", "in", "reward_dict", ".", "items", "(", ")", ":", "\n", "        ", "reward_avg_dict", "[", "key", "]", "=", "statistics", ".", "mean", "(", "value", ")", "\n", "min_val", "=", "min", "(", "min_val", ",", "reward_avg_dict", "[", "key", "]", ")", "\n", "max_val", "=", "max", "(", "max_val", ",", "reward_avg_dict", "[", "key", "]", ")", "\n", "\n", "", "print", "(", "min_val", ")", "\n", "print", "(", "max_val", ")", "\n", "n", "=", "info_dict", "[", "'n'", "]", "\n", "mode", "=", "info_dict", "[", "'mode'", "]", "\n", "\n", "# plot individual rewards", "\n", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", "dpi", "=", "600", ",", "figsize", "=", "(", "7", ",", "2.5", ")", ")", "\n", "\n", "for", "key", ",", "value", "in", "reward_avg_dict", ".", "items", "(", ")", ":", "\n", "        ", "intensity", "=", "(", "value", "-", "min_val", ")", "/", "(", "max_val", "-", "min_val", ")", "\n", "state", "=", "key", "[", "0", "]", "\n", "action", "=", "key", "[", "1", "]", "\n", "draw_filled_polygon", "(", "state", ",", "action", ",", "n", ",", "intensity", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "5", ")", ":", "\n", "        ", "plt", ".", "plot", "(", "[", "-", "0.5", ",", "11.5", "]", ",", "[", "-", "i", "+", "0.5", ",", "-", "i", "+", "0.5", "]", ",", "linewidth", "=", "0.5", ",", "color", "=", "'black'", ")", "\n", "", "for", "i", "in", "range", "(", "13", ")", ":", "\n", "        ", "plt", ".", "plot", "(", "[", "i", "-", "0.5", ",", "i", "-", "0.5", "]", ",", "[", "0.5", ",", "-", "3.5", "]", ",", "linewidth", "=", "0.5", ",", "color", "=", "'black'", ")", "\n", "\n", "# plot additional information", "\n", "", "x_water", "=", "[", "0.5", ",", "10.5", ",", "10.5", ",", "0.5", ",", "0.5", "]", "\n", "y_water", "=", "[", "-", "2.5", ",", "-", "2.5", ",", "-", "3.5", ",", "-", "3.5", ",", "-", "2.5", "]", "\n", "plt", ".", "plot", "(", "x_water", ",", "y_water", ",", "linewidth", "=", "2", ",", "color", "=", "'black'", ")", "\n", "plt", ".", "text", "(", "5.5", ",", "-", "3", ",", "'cliff'", ",", "size", "=", "12", ",", "color", "=", "'black'", ",", "ha", "=", "'center'", ",", "va", "=", "'center'", ")", "\n", "plt", ".", "text", "(", "0", ",", "-", "3", ",", "'(S)'", ",", "size", "=", "12", ",", "ha", "=", "'center'", ",", "va", "=", "'center'", ")", "\n", "plt", ".", "text", "(", "11", ",", "-", "3", ",", "'(G)'", ",", "size", "=", "12", ",", "ha", "=", "'center'", ",", "va", "=", "'center'", ")", "\n", "\n", "if", "SIMPLIFY", ":", "\n", "        ", "if", "mode", "==", "'1'", ":", "\n", "            ", "plt", ".", "title", "(", "'exclusive potential reward network (only rewards > -50)'", ")", "\n", "", "elif", "mode", "==", "'2'", ":", "\n", "            ", "plt", ".", "title", "(", "'additive potential reward network (only rewards > -50)'", ")", "\n", "", "elif", "mode", "==", "'5'", ":", "\n", "            ", "plt", ".", "title", "(", "'exclusive non-potential reward network (only rewards > -50)'", ")", "\n", "", "elif", "mode", "==", "'6'", ":", "\n", "            ", "plt", ".", "title", "(", "'additive non-potential reward network (only rewards > -50)'", ")", "\n", "", "", "else", ":", "\n", "        ", "if", "mode", "==", "'1'", ":", "\n", "            ", "plt", ".", "title", "(", "'exclusive potential reward network'", ")", "\n", "", "elif", "mode", "==", "'2'", ":", "\n", "            ", "plt", ".", "title", "(", "'additive potential reward network'", ")", "\n", "", "elif", "mode", "==", "'5'", ":", "\n", "            ", "plt", ".", "title", "(", "'exclusive non-potential reward network'", ")", "\n", "", "elif", "mode", "==", "'6'", ":", "\n", "            ", "plt", ".", "title", "(", "'additive non-potential reward network'", ")", "\n", "\n", "", "", "ax", ".", "axis", "(", "'equal'", ")", "\n", "ax", ".", "axis", "(", "'off'", ")", "\n", "\n", "if", "SIMPLIFY", ":", "\n", "        ", "plt", ".", "savefig", "(", "'cliff_learned_rewards_'", "+", "str", "(", "mode", ")", "+", "'_simplified.svg'", ",", "bbox_inches", "=", "'tight'", ")", "\n", "", "else", ":", "\n", "        ", "plt", ".", "savefig", "(", "'cliff_learned_rewards_'", "+", "str", "(", "mode", ")", "+", "'.svg'", ",", "bbox_inches", "=", "'tight'", ")", "\n", "\n", "", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld_learned_reward_env.load_envs_and_config": [[188, 201], ["torch.load", "envs.env_factory.EnvFactory", "envs.env_factory.EnvFactory.generate_reward_env", "env_factory.generate_reward_env.load_state_dict", "envs.env_factory.EnvFactory.generate_real_env"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_reward_env", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_real_env"], ["", "def", "load_envs_and_config", "(", "model_file", ")", ":", "\n", "    ", "save_dict", "=", "torch", ".", "load", "(", "model_file", ")", "\n", "\n", "config", "=", "save_dict", "[", "'config'", "]", "\n", "# config['device'] = 'cuda'", "\n", "config", "[", "'envs'", "]", "[", "'Cliff'", "]", "[", "'solved_reward'", "]", "=", "100000", "# something big enough to prevent early out triggering", "\n", "\n", "env_factory", "=", "EnvFactory", "(", "config", "=", "config", ")", "\n", "reward_env", "=", "env_factory", ".", "generate_reward_env", "(", ")", "\n", "reward_env", ".", "load_state_dict", "(", "save_dict", "[", "'model'", "]", ")", "\n", "real_env", "=", "env_factory", ".", "generate_real_env", "(", ")", "\n", "\n", "return", "reward_env", ",", "real_env", ",", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_reward_env.ExperimentWrapper.get_bohb_parameters": [[17, 26], ["None"], "methods", ["None"], ["    ", "def", "get_bohb_parameters", "(", "self", ")", ":", "\n", "        ", "params", "=", "{", "}", "\n", "params", "[", "'min_budget'", "]", "=", "1", "\n", "params", "[", "'max_budget'", "]", "=", "1", "\n", "params", "[", "'eta'", "]", "=", "2", "\n", "params", "[", "'random_fraction'", "]", "=", "1", "\n", "params", "[", "'iterations'", "]", "=", "10000", "\n", "\n", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_reward_env.ExperimentWrapper.get_configspace": [[27, 30], ["ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace"], "methods", ["None"], ["", "def", "get_configspace", "(", "self", ")", ":", "\n", "        ", "cs", "=", "CS", ".", "ConfigurationSpace", "(", ")", "\n", "return", "cs", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_reward_env.ExperimentWrapper.get_specific_config": [[31, 38], ["copy.deepcopy"], "methods", ["None"], ["", "def", "get_specific_config", "(", "self", ",", "cso", ",", "default_config", ",", "budget", ")", ":", "\n", "        ", "config", "=", "deepcopy", "(", "default_config", ")", "\n", "global", "reward_env_type", "\n", "config", "[", "\"envs\"", "]", "[", "'Cliff'", "]", "[", "'reward_env_type'", "]", "=", "reward_env_type", "\n", "# config[\"envs\"]['Cliff']['solved_reward'] = sys.maxsize  # AUC", "\n", "config", "[", "'agents'", "]", "[", "'gtn'", "]", "[", "'max_iterations'", "]", "=", "200", "\n", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_reward_env.ExperimentWrapper.compute": [[39, 79], ["GTNC_evaluate_gridworld_reward_env.ExperimentWrapper.get_specific_config", "print", "print", "print", "print", "print", "print", "str", "str", "str", "str", "print", "print", "print", "print", "print", "open", "yaml.safe_load", "agents.GTN.GTN_Master", "agents.GTN.GTN_Master.run", "str", "str", "str", "float", "traceback.format_exc", "print", "str", "str", "sorted"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_step_size.ExperimentWrapper.get_specific_config", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.run"], ["", "def", "compute", "(", "self", ",", "working_dir", ",", "bohb_id", ",", "config_id", ",", "cso", ",", "budget", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "with", "open", "(", "\"default_config_gridworld_reward_env.yaml\"", ",", "'r'", ")", "as", "stream", ":", "\n", "            ", "default_config", "=", "yaml", ".", "safe_load", "(", "stream", ")", "\n", "\n", "", "config", "=", "self", ".", "get_specific_config", "(", "cso", ",", "default_config", ",", "budget", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "\"START BOHB ITERATION\"", ")", "\n", "print", "(", "'CONFIG: '", "+", "str", "(", "config", ")", ")", "\n", "print", "(", "'CSO:    '", "+", "str", "(", "cso", ")", ")", "\n", "print", "(", "'BUDGET: '", "+", "str", "(", "budget", ")", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "try", ":", "\n", "            ", "gtn", "=", "GTN_Master", "(", "config", ",", "bohb_id", "=", "bohb_id", ",", "bohb_working_dir", "=", "working_dir", ")", "\n", "_", ",", "score_list", ",", "model_name", "=", "gtn", ".", "run", "(", ")", "\n", "score", "=", "-", "sorted", "(", "score_list", ")", "[", "-", "1", "]", "\n", "error", "=", "\"\"", "\n", "", "except", ":", "\n", "            ", "score", "=", "float", "(", "'-Inf'", ")", "\n", "score_list", "=", "[", "]", "\n", "model_name", "=", "None", "\n", "error", "=", "traceback", ".", "format_exc", "(", ")", "\n", "print", "(", "error", ")", "\n", "\n", "", "info", "=", "{", "}", "\n", "info", "[", "'error'", "]", "=", "str", "(", "error", ")", "\n", "info", "[", "'config'", "]", "=", "str", "(", "config", ")", "\n", "info", "[", "'score_list'", "]", "=", "str", "(", "score_list", ")", "\n", "info", "[", "'model_name'", "]", "=", "str", "(", "model_name", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "'FINAL SCORE: '", "+", "str", "(", "score", ")", ")", "\n", "print", "(", "'SCORE LIST:  '", "+", "str", "(", "score_list", ")", ")", "\n", "print", "(", "\"END BOHB ITERATION\"", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "return", "{", "\n", "\"loss\"", ":", "score", ",", "\n", "\"info\"", ":", "info", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cmc_compare_reward_envs.get_best_models_from_log": [[29, 59], ["hpbandster.logged_results_to_HBS_result", "hpres.logged_results_to_HBS_result.data.values", "print", "best_models.sort", "os.path.isdir", "log_dir.replace.replace", "best_models.append", "os.path.isfile", "model_name.replace.replace"], "function", ["None"], ["def", "get_best_models_from_log", "(", "log_dir", ")", ":", "\n", "    ", "if", "not", "os", ".", "path", ".", "isdir", "(", "log_dir", ")", ":", "\n", "        ", "log_dir", "=", "log_dir", ".", "replace", "(", "'nierhoff'", ",", "'dingsda'", ")", "\n", "\n", "", "result", "=", "hpres", ".", "logged_results_to_HBS_result", "(", "log_dir", ")", "\n", "\n", "best_models", "=", "[", "]", "\n", "\n", "for", "value", "in", "result", ".", "data", ".", "values", "(", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "loss", "=", "value", ".", "results", "[", "1.0", "]", "[", "'loss'", "]", "\n", "model_name", "=", "value", ".", "results", "[", "1.0", "]", "[", "'info'", "]", "[", "'model_name'", "]", "\n", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "model_name", ")", ":", "\n", "                ", "model_name", "=", "model_name", ".", "replace", "(", "'/home/nierhoff/master_thesis/learning_environments/results'", ",", "\n", "'/home/ferreira/Projects/learning_environments/results/2_thomas_results'", ")", "\n", "", "best_models", ".", "append", "(", "(", "loss", ",", "model_name", ")", ")", "\n", "", "except", ":", "\n", "            ", "continue", "\n", "\n", "# before AUC (objective minimized)", "\n", "# print(\"sorting from low to high values (non-AUC)\")", "\n", "# best_models.sort(key=lambda x: x[0])", "\n", "\n", "# AUC (objective maximized)", "\n", "", "", "print", "(", "\"sorting from high to low values (AUC)\"", ")", "\n", "best_models", ".", "sort", "(", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ",", "reverse", "=", "True", ")", "\n", "best_models", "=", "best_models", "[", ":", "MODEL_NUM", "]", "\n", "\n", "return", "best_models", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cmc_compare_reward_envs.load_envs_and_config": [[61, 74], ["torch.load", "envs.env_factory.EnvFactory", "envs.env_factory.EnvFactory.generate_reward_env", "env_factory.generate_reward_env.load_state_dict", "envs.env_factory.EnvFactory.generate_real_env"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_reward_env", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_real_env"], ["", "def", "load_envs_and_config", "(", "model_file", ")", ":", "\n", "    ", "save_dict", "=", "torch", ".", "load", "(", "model_file", ")", "\n", "\n", "config", "=", "save_dict", "[", "'config'", "]", "\n", "config", "[", "'device'", "]", "=", "'cpu'", "\n", "config", "[", "'envs'", "]", "[", "'MountainCarContinuous-v0'", "]", "[", "'solved_reward'", "]", "=", "100000", "# something big enough to prevent early out triggering", "\n", "\n", "env_factory", "=", "EnvFactory", "(", "config", "=", "config", ")", "\n", "reward_env", "=", "env_factory", ".", "generate_reward_env", "(", ")", "\n", "reward_env", ".", "load_state_dict", "(", "save_dict", "[", "'model'", "]", ")", "\n", "real_env", "=", "env_factory", ".", "generate_real_env", "(", ")", "\n", "\n", "return", "reward_env", ",", "real_env", ",", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cmc_compare_reward_envs.train_test_agents": [[76, 129], ["range", "agents.agent_utils.select_agent.train", "print", "rewards.append", "episode_lengths.append", "agents.agent_utils.select_agent", "agents.agent_utils.select_agent", "str"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.train", "home.repos.pwc.inspect_result.automl_learning_environments.agents.agent_utils.select_agent", "home.repos.pwc.inspect_result.automl_learning_environments.agents.agent_utils.select_agent"], ["", "def", "train_test_agents", "(", "mode", ",", "env", ",", "real_env", ",", "config", ")", ":", "\n", "    ", "rewards", "=", "[", "]", "\n", "episode_lengths", "=", "[", "]", "\n", "\n", "# settings for comparability", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'test_episodes'", "]", "=", "1", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'train_episodes'", "]", "=", "3000", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'print_rate'", "]", "=", "100", "\n", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'lr'", "]", "=", "3e-4", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'tau'", "]", "=", "0.005", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'activation_fn'", "]", "=", "'relu'", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'same_action_num'", "]", "=", "2", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'policy_delay'", "]", "=", "2", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'policy_std_clip'", "]", "=", "0.5", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'policy_std'", "]", "=", "0.2", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'action_std'", "]", "=", "0.1", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'batch_size'", "]", "=", "256", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'hidden_layer'", "]", "=", "2", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'hidden_size'", "]", "=", "128", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'gamma'", "]", "=", "0.99", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'rb_size'", "]", "=", "1000000", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'init_episodes'", "]", "=", "50", "# set via time steps in original td3 implementation", "\n", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'early_out_num'", "]", "=", "10", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'early_out_virtual_diff'", "]", "=", "1e-2", "\n", "\n", "# optimized ICM HPs:", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "=", "{", "}", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "[", "'beta'", "]", "=", "0.1", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "[", "'eta'", "]", "=", "0.01", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "[", "'feature_dim'", "]", "=", "32", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "[", "'hidden_size'", "]", "=", "128", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "[", "'lr'", "]", "=", "5e-4", "\n", "\n", "# default ICM HPs:", "\n", "# config['agents']['icm'] = {}", "\n", "# config['agents']['icm']['beta'] = 0.2", "\n", "# config['agents']['icm']['eta'] = 0.5", "\n", "# config['agents']['icm']['feature_dim'] = 64", "\n", "# config['agents']['icm']['hidden_size'] = 128", "\n", "# config['agents']['icm']['lr'] = 1e-4", "\n", "\n", "for", "i", "in", "range", "(", "MODEL_AGENTS", ")", ":", "\n", "        ", "if", "mode", "==", "'-1'", ":", "\n", "            ", "agent", "=", "select_agent", "(", "config", "=", "config", ",", "agent_name", "=", "'td3_icm'", ")", "\n", "", "else", ":", "\n", "            ", "agent", "=", "select_agent", "(", "config", "=", "config", ",", "agent_name", "=", "'td3'", ")", "\n", "", "reward", ",", "episode_length", ",", "_", "=", "agent", ".", "train", "(", "env", "=", "env", ",", "test_env", "=", "real_env", ")", "\n", "print", "(", "'reward: '", "+", "str", "(", "reward", ")", ")", "\n", "rewards", ".", "append", "(", "reward", ")", "\n", "episode_lengths", ".", "append", "(", "episode_length", ")", "\n", "", "return", "rewards", ",", "episode_lengths", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cmc_compare_reward_envs.save_list": [[131, 141], ["os.makedirs", "os.path.join", "torch.save", "str"], "function", ["None"], ["", "def", "save_list", "(", "mode", ",", "config", ",", "reward_list", ",", "episode_length_list", ")", ":", "\n", "    ", "os", ".", "makedirs", "(", "SAVE_DIR", ",", "exist_ok", "=", "True", ")", "\n", "file_name", "=", "os", ".", "path", ".", "join", "(", "SAVE_DIR", ",", "'best'", "+", "str", "(", "mode", ")", "+", "'.pt'", ")", "# fine-tuned by bohb", "\n", "save_dict", "=", "{", "}", "\n", "save_dict", "[", "'config'", "]", "=", "config", "\n", "save_dict", "[", "'model_num'", "]", "=", "MODEL_NUM", "\n", "save_dict", "[", "'model_agents'", "]", "=", "MODEL_AGENTS", "\n", "save_dict", "[", "'reward_list'", "]", "=", "reward_list", "\n", "save_dict", "[", "'episode_length_list'", "]", "=", "episode_length_list", "\n", "torch", ".", "save", "(", "save_dict", ",", "file_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cmc_compare_reward_envs.eval_models": [[143, 158], ["GTNC_evaluate_cmc_compare_reward_envs.get_best_models_from_log", "GTNC_evaluate_cmc_compare_reward_envs.save_list", "print", "print", "GTNC_evaluate_cmc_compare_reward_envs.load_envs_and_config", "GTNC_evaluate_cmc_compare_reward_envs.train_test_agents", "str", "str"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.get_best_models_from_log", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.save_list", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.load_envs_and_config", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.train_test_agents"], ["", "def", "eval_models", "(", "mode", ",", "log_dir", ")", ":", "\n", "    ", "best_models", "=", "get_best_models_from_log", "(", "log_dir", ")", "\n", "\n", "reward_list", "=", "[", "]", "\n", "episode_length_list", "=", "[", "]", "\n", "\n", "for", "best_reward", ",", "model_file", "in", "best_models", ":", "\n", "        ", "print", "(", "'best reward: '", "+", "str", "(", "best_reward", ")", ")", "\n", "print", "(", "'model file: '", "+", "str", "(", "model_file", ")", ")", "\n", "reward_env", ",", "real_env", ",", "config", "=", "load_envs_and_config", "(", "model_file", ")", "\n", "rewards", ",", "episode_lengths", "=", "train_test_agents", "(", "mode", "=", "mode", ",", "env", "=", "reward_env", ",", "real_env", "=", "real_env", ",", "config", "=", "config", ")", "\n", "reward_list", "+=", "rewards", "\n", "episode_length_list", "+=", "episode_lengths", "\n", "\n", "", "save_list", "(", "mode", ",", "config", ",", "reward_list", ",", "episode_length_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cmc_compare_reward_envs.eval_base": [[160, 173], ["GTNC_evaluate_cmc_compare_reward_envs.get_best_models_from_log", "range", "GTNC_evaluate_cmc_compare_reward_envs.save_list", "GTNC_evaluate_cmc_compare_reward_envs.load_envs_and_config", "GTNC_evaluate_cmc_compare_reward_envs.train_test_agents"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.get_best_models_from_log", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.save_list", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.load_envs_and_config", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.train_test_agents"], ["", "def", "eval_base", "(", "mode", ",", "log_dir", ")", ":", "\n", "    ", "best_models", "=", "get_best_models_from_log", "(", "log_dir", ")", "\n", "\n", "reward_list", "=", "[", "]", "\n", "episode_length_list", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "MODEL_NUM", ")", ":", "\n", "        ", "reward_env", ",", "real_env", ",", "config", "=", "load_envs_and_config", "(", "best_models", "[", "0", "]", "[", "1", "]", ")", "\n", "rewards", ",", "episode_lengths", "=", "train_test_agents", "(", "mode", "=", "mode", ",", "env", "=", "real_env", ",", "real_env", "=", "real_env", ",", "config", "=", "config", ")", "\n", "reward_list", "+=", "rewards", "\n", "episode_length_list", "+=", "episode_lengths", "\n", "\n", "", "save_list", "(", "mode", ",", "config", ",", "reward_list", ",", "episode_length_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cmc_compare_reward_envs.eval_icm": [[175, 188], ["GTNC_evaluate_cmc_compare_reward_envs.get_best_models_from_log", "range", "GTNC_evaluate_cmc_compare_reward_envs.save_list", "GTNC_evaluate_cmc_compare_reward_envs.load_envs_and_config", "GTNC_evaluate_cmc_compare_reward_envs.train_test_agents"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.get_best_models_from_log", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.save_list", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.load_envs_and_config", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.train_test_agents"], ["", "def", "eval_icm", "(", "mode", ",", "log_dir", ")", ":", "\n", "    ", "best_models", "=", "get_best_models_from_log", "(", "log_dir", ")", "\n", "\n", "reward_list", "=", "[", "]", "\n", "episode_length_list", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "MODEL_NUM", ")", ":", "\n", "        ", "reward_env", ",", "real_env", ",", "config", "=", "load_envs_and_config", "(", "best_models", "[", "0", "]", "[", "1", "]", ")", "\n", "rewards", ",", "episode_lengths", "=", "train_test_agents", "(", "mode", "=", "mode", ",", "env", "=", "real_env", ",", "real_env", "=", "real_env", ",", "config", "=", "config", ")", "\n", "reward_list", "+=", "rewards", "\n", "episode_length_list", "+=", "episode_lengths", "\n", "\n", "", "save_list", "(", "mode", ",", "config", ",", "reward_list", ",", "episode_length_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_params.ExperimentWrapper.get_bohb_parameters": [[17, 26], ["None"], "methods", ["None"], ["    ", "def", "get_bohb_parameters", "(", "self", ")", ":", "\n", "        ", "params", "=", "{", "}", "\n", "params", "[", "'min_budget'", "]", "=", "1", "\n", "params", "[", "'max_budget'", "]", "=", "3", "\n", "params", "[", "'eta'", "]", "=", "3", "\n", "params", "[", "'random_fraction'", "]", "=", "0.3", "\n", "params", "[", "'iterations'", "]", "=", "10000", "\n", "\n", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_params.ExperimentWrapper.get_configspace": [[27, 52], ["ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter"], "methods", ["None"], ["", "def", "get_configspace", "(", "self", ")", ":", "\n", "        ", "cs", "=", "CS", ".", "ConfigurationSpace", "(", ")", "\n", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'gtn_score_transform_type'", ",", "lower", "=", "0", ",", "upper", "=", "7", ",", "log", "=", "False", ",", "default_value", "=", "7", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'gtn_step_size'", ",", "lower", "=", "0.1", ",", "upper", "=", "1", ",", "log", "=", "True", ",", "default_value", "=", "0.5", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "CategoricalHyperparameter", "(", "name", "=", "'gtn_mirrored_sampling'", ",", "choices", "=", "[", "False", ",", "True", "]", ",", "default_value", "=", "True", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'gtn_noise_std'", ",", "lower", "=", "0.01", ",", "upper", "=", "1", ",", "log", "=", "True", ",", "default_value", "=", "0.1", ")", ")", "\n", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'ddqn_init_episodes'", ",", "lower", "=", "1", ",", "upper", "=", "20", ",", "log", "=", "True", ",", "default_value", "=", "10", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'ddqn_batch_size'", ",", "lower", "=", "64", ",", "upper", "=", "256", ",", "log", "=", "False", ",", "default_value", "=", "128", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'ddqn_gamma'", ",", "lower", "=", "0.001", ",", "upper", "=", "0.1", ",", "log", "=", "True", ",", "default_value", "=", "0.01", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'ddqn_lr'", ",", "lower", "=", "1e-4", ",", "upper", "=", "5e-3", ",", "log", "=", "True", ",", "default_value", "=", "1e-3", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'ddqn_tau'", ",", "lower", "=", "0.005", ",", "upper", "=", "0.05", ",", "log", "=", "True", ",", "default_value", "=", "0.01", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'ddqn_eps_init'", ",", "lower", "=", "0.8", ",", "upper", "=", "1", ",", "log", "=", "True", ",", "default_value", "=", "0.9", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'ddqn_eps_min'", ",", "lower", "=", "0.005", ",", "upper", "=", "0.05", ",", "log", "=", "True", ",", "default_value", "=", "0.05", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'ddqn_eps_decay'", ",", "lower", "=", "0.01", ",", "upper", "=", "0.2", ",", "log", "=", "True", ",", "default_value", "=", "0.1", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "CategoricalHyperparameter", "(", "name", "=", "'ddqn_activation_fn'", ",", "choices", "=", "[", "'tanh'", ",", "'relu'", ",", "'leakyrelu'", ",", "'prelu'", "]", ",", "default_value", "=", "'relu'", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'ddqn_hidden_size'", ",", "lower", "=", "48", ",", "upper", "=", "192", ",", "log", "=", "True", ",", "default_value", "=", "128", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'ddqn_hidden_layer'", ",", "lower", "=", "1", ",", "upper", "=", "2", ",", "log", "=", "False", ",", "default_value", "=", "2", ")", ")", "\n", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "CategoricalHyperparameter", "(", "name", "=", "'cartpole_activation_fn'", ",", "choices", "=", "[", "'tanh'", ",", "'relu'", ",", "'leakyrelu'", ",", "'prelu'", "]", ",", "default_value", "=", "'leakyrelu'", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'cartpole_hidden_size'", ",", "lower", "=", "48", ",", "upper", "=", "192", ",", "log", "=", "True", ",", "default_value", "=", "128", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'cartpole_hidden_layer'", ",", "lower", "=", "1", ",", "upper", "=", "2", ",", "log", "=", "False", ",", "default_value", "=", "1", ")", ")", "\n", "\n", "return", "cs", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_params.ExperimentWrapper.get_specific_config": [[53, 78], ["copy.deepcopy"], "methods", ["None"], ["", "def", "get_specific_config", "(", "self", ",", "cso", ",", "default_config", ",", "budget", ")", ":", "\n", "        ", "config", "=", "deepcopy", "(", "default_config", ")", "\n", "\n", "config", "[", "\"agents\"", "]", "[", "'gtn'", "]", "[", "'score_transform_type'", "]", "=", "cso", "[", "\"gtn_score_transform_type\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'gtn'", "]", "[", "'step_size'", "]", "=", "cso", "[", "\"gtn_step_size\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'gtn'", "]", "[", "'mirrored_sampling'", "]", "=", "cso", "[", "\"gtn_mirrored_sampling\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'gtn'", "]", "[", "'noise_std'", "]", "=", "cso", "[", "\"gtn_noise_std\"", "]", "\n", "\n", "config", "[", "\"agents\"", "]", "[", "'ddqn'", "]", "[", "'init_episodes'", "]", "=", "cso", "[", "\"ddqn_init_episodes\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'ddqn'", "]", "[", "'batch_size'", "]", "=", "cso", "[", "\"ddqn_batch_size\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'ddqn'", "]", "[", "'gamma'", "]", "=", "1", "-", "cso", "[", "\"ddqn_gamma\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'ddqn'", "]", "[", "'lr'", "]", "=", "cso", "[", "\"ddqn_lr\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'ddqn'", "]", "[", "'tau'", "]", "=", "cso", "[", "\"ddqn_tau\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'ddqn'", "]", "[", "'eps_init'", "]", "=", "cso", "[", "\"ddqn_eps_init\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'ddqn'", "]", "[", "'eps_min'", "]", "=", "cso", "[", "\"ddqn_eps_min\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'ddqn'", "]", "[", "'eps_decay'", "]", "=", "1", "-", "cso", "[", "\"ddqn_eps_decay\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'ddqn'", "]", "[", "'activation_fn'", "]", "=", "cso", "[", "\"ddqn_activation_fn\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'ddqn'", "]", "[", "'hidden_size'", "]", "=", "cso", "[", "\"ddqn_hidden_size\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'ddqn'", "]", "[", "'hidden_layer'", "]", "=", "cso", "[", "\"ddqn_hidden_layer\"", "]", "\n", "\n", "config", "[", "\"envs\"", "]", "[", "'CartPole-v0'", "]", "[", "'activation_fn'", "]", "=", "cso", "[", "\"cartpole_activation_fn\"", "]", "\n", "config", "[", "\"envs\"", "]", "[", "'CartPole-v0'", "]", "[", "'hidden_size'", "]", "=", "cso", "[", "\"cartpole_hidden_size\"", "]", "\n", "config", "[", "\"envs\"", "]", "[", "'CartPole-v0'", "]", "[", "'hidden_layer'", "]", "=", "cso", "[", "\"cartpole_hidden_layer\"", "]", "\n", "\n", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_params.ExperimentWrapper.compute": [[79, 119], ["GTNC_evaluate_cartpole_params.ExperimentWrapper.get_specific_config", "print", "print", "print", "print", "print", "print", "str", "str", "str", "print", "print", "print", "print", "print", "open", "yaml.safe_load", "range", "str", "str", "str", "agents.GTN.GTN_Master", "agents.GTN.GTN_Master.run", "len", "float", "traceback.format_exc", "print", "str", "str"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_step_size.ExperimentWrapper.get_specific_config", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.run"], ["", "def", "compute", "(", "self", ",", "working_dir", ",", "bohb_id", ",", "config_id", ",", "cso", ",", "budget", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "with", "open", "(", "\"default_config_cartpole.yaml\"", ",", "'r'", ")", "as", "stream", ":", "\n", "            ", "default_config", "=", "yaml", ".", "safe_load", "(", "stream", ")", "\n", "\n", "", "config", "=", "self", ".", "get_specific_config", "(", "cso", ",", "default_config", ",", "budget", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "\"START BOHB ITERATION\"", ")", "\n", "print", "(", "'CONFIG: '", "+", "str", "(", "config", ")", ")", "\n", "print", "(", "'CSO:    '", "+", "str", "(", "cso", ")", ")", "\n", "print", "(", "'BUDGET: '", "+", "str", "(", "budget", ")", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "try", ":", "\n", "            ", "score", "=", "0", "\n", "for", "_", "in", "range", "(", "3", ")", ":", "\n", "                ", "gtn", "=", "GTN_Master", "(", "config", ",", "bohb_id", "=", "bohb_id", ",", "bohb_working_dir", "=", "working_dir", ")", "\n", "_", ",", "score_list", "=", "gtn", ".", "run", "(", ")", "\n", "score", "+=", "len", "(", "score_list", ")", "\n", "", "error", "=", "\"\"", "\n", "", "except", ":", "\n", "            ", "score", "=", "float", "(", "'Inf'", ")", "\n", "score_list", "=", "[", "]", "\n", "error", "=", "traceback", ".", "format_exc", "(", ")", "\n", "print", "(", "error", ")", "\n", "\n", "", "info", "=", "{", "}", "\n", "info", "[", "'error'", "]", "=", "str", "(", "error", ")", "\n", "info", "[", "'config'", "]", "=", "str", "(", "config", ")", "\n", "info", "[", "'score_list'", "]", "=", "str", "(", "score_list", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "'FINAL SCORE: '", "+", "str", "(", "score", ")", ")", "\n", "print", "(", "'SCORE LIST:  '", "+", "str", "(", "score_list", ")", ")", "\n", "print", "(", "\"END BOHB ITERATION\"", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "return", "{", "\n", "\"loss\"", ":", "score", ",", "\n", "\"info\"", ":", "info", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.bohb_params_DuelingDDQN_cartpole.ExperimentWrapper.get_bohb_parameters": [[19, 28], ["None"], "methods", ["None"], ["    ", "def", "get_bohb_parameters", "(", "self", ")", ":", "\n", "        ", "params", "=", "{", "}", "\n", "params", "[", "'min_budget'", "]", "=", "1", "\n", "params", "[", "'max_budget'", "]", "=", "2", "\n", "params", "[", "'eta'", "]", "=", "2", "\n", "params", "[", "'iterations'", "]", "=", "10000", "\n", "params", "[", "'random_fraction'", "]", "=", "0.3", "\n", "\n", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.bohb_params_DuelingDDQN_cartpole.ExperimentWrapper.get_configspace": [[29, 48], ["ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter"], "methods", ["None"], ["", "def", "get_configspace", "(", "self", ")", ":", "\n", "        ", "cs", "=", "CS", ".", "ConfigurationSpace", "(", ")", "\n", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'init_episodes'", ",", "lower", "=", "1", ",", "upper", "=", "20", ",", "log", "=", "True", ",", "default_value", "=", "10", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'batch_size'", ",", "lower", "=", "64", ",", "upper", "=", "512", ",", "log", "=", "True", ",", "default_value", "=", "256", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'gamma'", ",", "lower", "=", "0.001", ",", "upper", "=", "0.1", ",", "log", "=", "True", ",", "default_value", "=", "0.01", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'lr'", ",", "lower", "=", "1e-4", ",", "upper", "=", "1e-1", ",", "log", "=", "True", ",", "default_value", "=", "3e-4", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'tau'", ",", "lower", "=", "0.001", ",", "upper", "=", "0.1", ",", "log", "=", "True", ",", "default_value", "=", "0.01", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'eps_init'", ",", "lower", "=", "0.8", ",", "upper", "=", "1", ",", "log", "=", "True", ",", "default_value", "=", "0.9", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'eps_min'", ",", "lower", "=", "0.005", ",", "upper", "=", "0.05", ",", "log", "=", "True", ",", "default_value", "=", "0.05", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'eps_decay'", ",", "lower", "=", "0.01", ",", "upper", "=", "0.2", ",", "log", "=", "True", ",", "default_value", "=", "0.1", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'rb_size'", ",", "lower", "=", "1000", ",", "upper", "=", "1000000", ",", "log", "=", "True", ",", "default_value", "=", "100000", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'hidden_size'", ",", "lower", "=", "32", ",", "upper", "=", "256", ",", "log", "=", "True", ",", "default_value", "=", "128", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'feature_dim'", ",", "lower", "=", "32", ",", "upper", "=", "256", ",", "log", "=", "True", ",", "default_value", "=", "128", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'hidden_layer'", ",", "lower", "=", "1", ",", "upper", "=", "2", ",", "log", "=", "False", ",", "default_value", "=", "2", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "CategoricalHyperparameter", "(", "name", "=", "'activation_fn'", ",", "choices", "=", "[", "'relu'", ",", "'tanh'", ",", "'leakyrelu'", ",", "'prelu'", "]", ",", "default_value", "=", "'relu'", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'early_out_num'", ",", "lower", "=", "1", ",", "upper", "=", "5", ",", "log", "=", "True", ",", "default_value", "=", "3", ")", ")", "\n", "\n", "return", "cs", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.bohb_params_DuelingDDQN_cartpole.ExperimentWrapper.get_specific_config": [[49, 68], ["copy.deepcopy"], "methods", ["None"], ["", "def", "get_specific_config", "(", "self", ",", "cso", ",", "default_config", ",", "budget", ")", ":", "\n", "        ", "config", "=", "deepcopy", "(", "default_config", ")", "\n", "\n", "config", "[", "\"agents\"", "]", "[", "\"duelingddqn\"", "]", "[", "\"init_episodes\"", "]", "=", "cso", "[", "\"init_episodes\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"duelingddqn\"", "]", "[", "\"batch_size\"", "]", "=", "cso", "[", "\"batch_size\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"duelingddqn\"", "]", "[", "\"gamma\"", "]", "=", "1", "-", "cso", "[", "\"gamma\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"duelingddqn\"", "]", "[", "\"lr\"", "]", "=", "cso", "[", "\"lr\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"duelingddqn\"", "]", "[", "\"tau\"", "]", "=", "cso", "[", "\"tau\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"duelingddqn\"", "]", "[", "\"eps_init\"", "]", "=", "cso", "[", "\"eps_init\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"duelingddqn\"", "]", "[", "\"eps_min\"", "]", "=", "cso", "[", "\"eps_min\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'duelingddqn'", "]", "[", "'eps_decay'", "]", "=", "1", "-", "cso", "[", "\"eps_decay\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"duelingddqn\"", "]", "[", "\"rb_size\"", "]", "=", "cso", "[", "\"rb_size\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"duelingddqn\"", "]", "[", "\"hidden_size\"", "]", "=", "cso", "[", "\"hidden_size\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"duelingddqn\"", "]", "[", "\"hidden_layer\"", "]", "=", "cso", "[", "\"hidden_layer\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"duelingddqn\"", "]", "[", "\"feature_dim\"", "]", "=", "cso", "[", "\"feature_dim\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"duelingddqn\"", "]", "[", "\"activation_fn\"", "]", "=", "cso", "[", "\"activation_fn\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"duelingddqn\"", "]", "[", "\"early_out_num\"", "]", "=", "cso", "[", "\"early_out_num\"", "]", "\n", "\n", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.bohb_params_DuelingDDQN_cartpole.ExperimentWrapper.compute": [[69, 102], ["bohb_params_DuelingDDQN_cartpole.ExperimentWrapper.get_specific_config", "print", "print", "print", "print", "print", "print", "envs.env_factory.EnvFactory", "envs.env_factory.EnvFactory.generate_real_env", "agents.DuelingDDQN.DuelingDDQN", "agents.DuelingDDQN.DuelingDDQN.train", "len", "str", "print", "print", "print", "print", "open", "yaml.safe_load", "str", "str", "str", "str"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_step_size.ExperimentWrapper.get_specific_config", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_real_env", "home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.train"], ["", "def", "compute", "(", "self", ",", "working_dir", ",", "bohb_id", ",", "config_id", ",", "cso", ",", "budget", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "with", "open", "(", "\"default_config_cartpole.yaml\"", ",", "'r'", ")", "as", "stream", ":", "\n", "            ", "default_config", "=", "yaml", ".", "safe_load", "(", "stream", ")", "\n", "\n", "", "config", "=", "self", ".", "get_specific_config", "(", "cso", ",", "default_config", ",", "budget", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "\"START BOHB ITERATION\"", ")", "\n", "print", "(", "'CONFIG: '", "+", "str", "(", "config", ")", ")", "\n", "print", "(", "'CSO:    '", "+", "str", "(", "cso", ")", ")", "\n", "print", "(", "'BUDGET: '", "+", "str", "(", "budget", ")", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "info", "=", "{", "}", "\n", "\n", "# generate environment", "\n", "env_fac", "=", "EnvFactory", "(", "config", ")", "\n", "env", "=", "env_fac", ".", "generate_real_env", "(", ")", "\n", "\n", "dueling_ddqn", "=", "DuelingDDQN", "(", "env", "=", "env", ",", "\n", "config", "=", "config", ")", "\n", "rewards", ",", "_", ",", "_", "=", "dueling_ddqn", ".", "train", "(", "env", ")", "\n", "score", "=", "len", "(", "rewards", ")", "\n", "\n", "info", "[", "'config'", "]", "=", "str", "(", "config", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "'FINAL SCORE: '", "+", "str", "(", "score", ")", ")", "\n", "print", "(", "\"END BOHB ITERATION\"", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "return", "{", "\n", "\"loss\"", ":", "score", ",", "\n", "\"info\"", ":", "info", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.calc_avg_bohb_time.analyze_bohb": [[12, 18], ["hpbandster.logged_results_to_HBS_result", "hpres.logged_results_to_HBS_result.get_all_runs"], "function", ["None"], ["def", "analyze_bohb", "(", "log_dir", ")", ":", "\n", "# load the example run from the log files", "\n", "    ", "result", "=", "hpres", ".", "logged_results_to_HBS_result", "(", "log_dir", ")", "\n", "\n", "# get all executed runs", "\n", "all_runs", "=", "result", ".", "get_all_runs", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_reward_env.ExperimentWrapper.get_bohb_parameters": [[17, 26], ["None"], "methods", ["None"], ["    ", "def", "get_bohb_parameters", "(", "self", ")", ":", "\n", "        ", "params", "=", "{", "}", "\n", "params", "[", "'min_budget'", "]", "=", "1", "\n", "params", "[", "'max_budget'", "]", "=", "1", "\n", "params", "[", "'eta'", "]", "=", "2", "\n", "params", "[", "'random_fraction'", "]", "=", "1", "\n", "params", "[", "'iterations'", "]", "=", "10000", "\n", "\n", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_reward_env.ExperimentWrapper.get_configspace": [[27, 30], ["ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace"], "methods", ["None"], ["", "def", "get_configspace", "(", "self", ")", ":", "\n", "        ", "cs", "=", "CS", ".", "ConfigurationSpace", "(", ")", "\n", "return", "cs", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_reward_env.ExperimentWrapper.get_specific_config": [[31, 38], ["copy.deepcopy"], "methods", ["None"], ["", "def", "get_specific_config", "(", "self", ",", "cso", ",", "default_config", ",", "budget", ")", ":", "\n", "        ", "config", "=", "deepcopy", "(", "default_config", ")", "\n", "global", "reward_env_type", "\n", "config", "[", "\"envs\"", "]", "[", "'CartPole-v0'", "]", "[", "'reward_env_type'", "]", "=", "reward_env_type", "\n", "# config[\"envs\"]['CartPole-v0']['solved_reward'] = sys.maxsize  # AUC", "\n", "config", "[", "'agents'", "]", "[", "'gtn'", "]", "[", "'max_iterations'", "]", "=", "200", "\n", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_reward_env.ExperimentWrapper.compute": [[39, 79], ["GTNC_evaluate_cartpole_reward_env.ExperimentWrapper.get_specific_config", "print", "print", "print", "print", "print", "print", "str", "str", "str", "str", "print", "print", "print", "print", "print", "open", "yaml.safe_load", "agents.GTN.GTN_Master", "agents.GTN.GTN_Master.run", "str", "str", "str", "float", "traceback.format_exc", "print", "str", "str", "sorted"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_step_size.ExperimentWrapper.get_specific_config", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.run"], ["", "def", "compute", "(", "self", ",", "working_dir", ",", "bohb_id", ",", "config_id", ",", "cso", ",", "budget", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "with", "open", "(", "\"default_config_cartpole_reward_env.yaml\"", ",", "'r'", ")", "as", "stream", ":", "\n", "            ", "default_config", "=", "yaml", ".", "safe_load", "(", "stream", ")", "\n", "\n", "", "config", "=", "self", ".", "get_specific_config", "(", "cso", ",", "default_config", ",", "budget", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "\"START BOHB ITERATION\"", ")", "\n", "print", "(", "'CONFIG: '", "+", "str", "(", "config", ")", ")", "\n", "print", "(", "'CSO:    '", "+", "str", "(", "cso", ")", ")", "\n", "print", "(", "'BUDGET: '", "+", "str", "(", "budget", ")", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "try", ":", "\n", "            ", "gtn", "=", "GTN_Master", "(", "config", ",", "bohb_id", "=", "bohb_id", ",", "bohb_working_dir", "=", "working_dir", ")", "\n", "_", ",", "score_list", ",", "model_name", "=", "gtn", ".", "run", "(", ")", "\n", "score", "=", "-", "sorted", "(", "score_list", ")", "[", "-", "1", "]", "\n", "error", "=", "\"\"", "\n", "", "except", ":", "\n", "            ", "score", "=", "float", "(", "'-Inf'", ")", "\n", "score_list", "=", "[", "]", "\n", "model_name", "=", "None", "\n", "error", "=", "traceback", ".", "format_exc", "(", ")", "\n", "print", "(", "error", ")", "\n", "\n", "", "info", "=", "{", "}", "\n", "info", "[", "'error'", "]", "=", "str", "(", "error", ")", "\n", "info", "[", "'config'", "]", "=", "str", "(", "config", ")", "\n", "info", "[", "'score_list'", "]", "=", "str", "(", "score_list", ")", "\n", "info", "[", "'model_name'", "]", "=", "str", "(", "model_name", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "'FINAL SCORE: '", "+", "str", "(", "score", ")", ")", "\n", "print", "(", "'SCORE LIST:  '", "+", "str", "(", "score_list", ")", ")", "\n", "print", "(", "\"END BOHB ITERATION\"", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "return", "{", "\n", "\"loss\"", ":", "score", ",", "\n", "\"info\"", ":", "info", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_acrobot.ExperimentWrapper.get_bohb_parameters": [[17, 26], ["None"], "methods", ["None"], ["    ", "def", "get_bohb_parameters", "(", "self", ")", ":", "\n", "        ", "params", "=", "{", "}", "\n", "params", "[", "'min_budget'", "]", "=", "1", "\n", "params", "[", "'max_budget'", "]", "=", "1", "\n", "params", "[", "'eta'", "]", "=", "2", "\n", "params", "[", "'random_fraction'", "]", "=", "1", "\n", "params", "[", "'iterations'", "]", "=", "10000", "\n", "\n", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_acrobot.ExperimentWrapper.get_configspace": [[27, 30], ["ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace"], "methods", ["None"], ["", "def", "get_configspace", "(", "self", ")", ":", "\n", "        ", "cs", "=", "CS", ".", "ConfigurationSpace", "(", ")", "\n", "return", "cs", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_acrobot.ExperimentWrapper.get_specific_config": [[31, 34], ["copy.deepcopy"], "methods", ["None"], ["", "def", "get_specific_config", "(", "self", ",", "cso", ",", "default_config", ",", "budget", ")", ":", "\n", "        ", "config", "=", "deepcopy", "(", "default_config", ")", "\n", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_acrobot.ExperimentWrapper.compute": [[35, 74], ["GTNC_evaluate_acrobot.ExperimentWrapper.get_specific_config", "print", "print", "print", "print", "print", "print", "str", "str", "str", "print", "print", "print", "print", "print", "open", "yaml.safe_load", "agents.GTN.GTN_Master", "agents.GTN.GTN_Master.run", "len", "str", "str", "str", "float", "traceback.format_exc", "print", "str", "str"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_step_size.ExperimentWrapper.get_specific_config", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.run"], ["", "def", "compute", "(", "self", ",", "working_dir", ",", "bohb_id", ",", "config_id", ",", "cso", ",", "budget", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "with", "open", "(", "\"default_config_acrobot.yaml\"", ",", "'r'", ")", "as", "stream", ":", "\n", "            ", "default_config", "=", "yaml", ".", "safe_load", "(", "stream", ")", "\n", "\n", "", "config", "=", "self", ".", "get_specific_config", "(", "cso", ",", "default_config", ",", "budget", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "\"START BOHB ITERATION\"", ")", "\n", "print", "(", "'CONFIG: '", "+", "str", "(", "config", ")", ")", "\n", "print", "(", "'CSO:    '", "+", "str", "(", "cso", ")", ")", "\n", "print", "(", "'BUDGET: '", "+", "str", "(", "budget", ")", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "try", ":", "\n", "            ", "gtn", "=", "GTN_Master", "(", "config", ",", "bohb_id", "=", "bohb_id", ",", "bohb_working_dir", "=", "working_dir", ")", "\n", "_", ",", "score_list", ",", "model_name", "=", "gtn", ".", "run", "(", ")", "\n", "score", "=", "len", "(", "score_list", ")", "\n", "error", "=", "\"\"", "\n", "", "except", ":", "\n", "            ", "score", "=", "float", "(", "'Inf'", ")", "\n", "score_list", "=", "[", "]", "\n", "model_name", "=", "None", "\n", "error", "=", "traceback", ".", "format_exc", "(", ")", "\n", "print", "(", "error", ")", "\n", "\n", "", "info", "=", "{", "}", "\n", "info", "[", "'error'", "]", "=", "str", "(", "error", ")", "\n", "info", "[", "'score_list'", "]", "=", "str", "(", "score_list", ")", "\n", "info", "[", "'model_name'", "]", "=", "str", "(", "model_name", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "'FINAL SCORE: '", "+", "str", "(", "score", ")", ")", "\n", "print", "(", "'SCORE LIST:  '", "+", "str", "(", "score_list", ")", ")", "\n", "print", "(", "\"END BOHB ITERATION\"", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "return", "{", "\n", "\"loss\"", ":", "score", ",", "\n", "\"info\"", ":", "info", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld.ExperimentWrapper.get_bohb_parameters": [[17, 26], ["None"], "methods", ["None"], ["    ", "def", "get_bohb_parameters", "(", "self", ")", ":", "\n", "        ", "params", "=", "{", "}", "\n", "params", "[", "'min_budget'", "]", "=", "1", "\n", "params", "[", "'max_budget'", "]", "=", "1", "\n", "params", "[", "'eta'", "]", "=", "2", "\n", "params", "[", "'random_fraction'", "]", "=", "1", "\n", "params", "[", "'iterations'", "]", "=", "100", "\n", "\n", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld.ExperimentWrapper.get_configspace": [[27, 31], ["ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace"], "methods", ["None"], ["", "def", "get_configspace", "(", "self", ")", ":", "\n", "        ", "cs", "=", "CS", ".", "ConfigurationSpace", "(", ")", "\n", "\n", "return", "cs", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld.ExperimentWrapper.get_specific_config": [[32, 36], ["copy.deepcopy"], "methods", ["None"], ["", "def", "get_specific_config", "(", "self", ",", "cso", ",", "default_config", ",", "budget", ")", ":", "\n", "        ", "config", "=", "deepcopy", "(", "default_config", ")", "\n", "\n", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld.ExperimentWrapper.compute": [[37, 74], ["GTNC_evaluate_gridworld.ExperimentWrapper.get_specific_config", "print", "print", "print", "print", "print", "print", "str", "str", "print", "print", "print", "print", "print", "open", "yaml.safe_load", "agents.GTN.GTN_Master", "agents.GTN.GTN_Master.run", "len", "str", "str", "str", "float", "traceback.format_exc", "print", "str", "str"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_step_size.ExperimentWrapper.get_specific_config", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.run"], ["", "def", "compute", "(", "self", ",", "working_dir", ",", "bohb_id", ",", "config_id", ",", "cso", ",", "budget", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "with", "open", "(", "\"default_config_gridworld.yaml\"", ",", "'r'", ")", "as", "stream", ":", "\n", "            ", "default_config", "=", "yaml", ".", "safe_load", "(", "stream", ")", "\n", "\n", "", "config", "=", "self", ".", "get_specific_config", "(", "cso", ",", "default_config", ",", "budget", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "\"START BOHB ITERATION\"", ")", "\n", "print", "(", "'CONFIG: '", "+", "str", "(", "config", ")", ")", "\n", "print", "(", "'CSO:    '", "+", "str", "(", "cso", ")", ")", "\n", "print", "(", "'BUDGET: '", "+", "str", "(", "budget", ")", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "try", ":", "\n", "            ", "gtn", "=", "GTN_Master", "(", "config", ",", "bohb_id", "=", "bohb_id", ",", "bohb_working_dir", "=", "working_dir", ")", "\n", "_", ",", "score_list", "=", "gtn", ".", "run", "(", ")", "\n", "score", "=", "len", "(", "score_list", ")", "\n", "error", "=", "\"\"", "\n", "", "except", ":", "\n", "            ", "score", "=", "float", "(", "'Inf'", ")", "\n", "score_list", "=", "[", "]", "\n", "error", "=", "traceback", ".", "format_exc", "(", ")", "\n", "print", "(", "error", ")", "\n", "\n", "", "info", "=", "{", "}", "\n", "info", "[", "'error'", "]", "=", "str", "(", "error", ")", "\n", "info", "[", "'score_list'", "]", "=", "str", "(", "score_list", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "'FINAL SCORE: '", "+", "str", "(", "score", ")", ")", "\n", "print", "(", "'SCORE LIST:  '", "+", "str", "(", "score_list", ")", ")", "\n", "print", "(", "\"END BOHB ITERATION\"", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "return", "{", "\n", "\"loss\"", ":", "score", ",", "\n", "\"info\"", ":", "info", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cmc_compare_reward_envs.get_data": [[48, 98], ["float", "print", "print", "max", "torch.load", "list_data.append", "sums_eps_len_per_model.append", "sum", "sum", "numpy.zeros", "enumerate", "numpy.mean", "numpy.std", "proc_data.append", "print", "sums_eps_len.append", "sums_eps_len_per_model_i.append", "min", "zip", "range", "numpy.array", "sum", "sum", "sum", "sum", "numpy.asarray", "numpy.asarray", "len", "len", "concat_list.append"], "function", ["None"], ["def", "get_data", "(", ")", ":", "\n", "    ", "list_data", "=", "[", "]", "\n", "for", "log_file", "in", "LOG_FILES", ":", "\n", "        ", "data", "=", "torch", ".", "load", "(", "log_file", ")", "\n", "list_data", ".", "append", "(", "(", "data", "[", "'reward_list'", "]", ",", "data", "[", "'episode_length_list'", "]", ")", ")", "\n", "model_num", "=", "data", "[", "'model_num'", "]", "\n", "model_agents", "=", "data", "[", "'model_agents'", "]", "\n", "\n", "", "sums_eps_len", "=", "[", "]", "\n", "sums_eps_len_per_model", "=", "[", "]", "\n", "min_steps", "=", "float", "(", "'Inf'", ")", "\n", "# get minimum number of evaluations", "\n", "for", "reward_list", ",", "episode_length_list", "in", "list_data", ":", "\n", "        ", "sums_eps_len_per_model_i", "=", "[", "]", "\n", "for", "episode_lengths", "in", "episode_length_list", ":", "\n", "            ", "print", "(", "sum", "(", "episode_lengths", ")", ")", "\n", "sums_eps_len", ".", "append", "(", "sum", "(", "episode_lengths", ")", ")", "\n", "sums_eps_len_per_model_i", ".", "append", "(", "sum", "(", "episode_lengths", ")", ")", "\n", "min_steps", "=", "min", "(", "min_steps", ",", "sum", "(", "episode_lengths", ")", ")", "\n", "", "sums_eps_len_per_model", ".", "append", "(", "sums_eps_len_per_model_i", ")", "\n", "\n", "", "print", "(", "\"# of episode lens > MIN_STEPS: \"", ",", "sum", "(", "np", ".", "asarray", "(", "sums_eps_len", ")", ">=", "MIN_STEPS", ")", ")", "\n", "print", "(", "\"# of episode lens < MIN_STEPS: \"", ",", "sum", "(", "np", ".", "asarray", "(", "sums_eps_len", ")", "<", "MIN_STEPS", ")", ")", "\n", "min_steps", "=", "max", "(", "min_steps", ",", "MIN_STEPS", ")", "\n", "# convert data from episodes to steps", "\n", "proc_data", "=", "[", "]", "\n", "\n", "for", "reward_list", ",", "episode_length_list", "in", "list_data", ":", "\n", "        ", "np_data", "=", "np", ".", "zeros", "(", "[", "model_num", "*", "model_agents", ",", "min_steps", "]", ")", "\n", "\n", "for", "it", ",", "data", "in", "enumerate", "(", "zip", "(", "reward_list", ",", "episode_length_list", ")", ")", ":", "\n", "            ", "rewards", ",", "episode_lengths", "=", "data", "\n", "\n", "concat_list", "=", "[", "]", "\n", "rewards", "=", "rewards", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "episode_lengths", ")", ")", ":", "\n", "                ", "concat_list", "+=", "[", "rewards", "[", "i", "]", "]", "*", "episode_lengths", "[", "i", "]", "\n", "\n", "", "while", "len", "(", "concat_list", ")", "<", "min_steps", ":", "\n", "                ", "concat_list", ".", "append", "(", "concat_list", "[", "-", "1", "]", ")", "\n", "\n", "", "np_data", "[", "it", "]", "=", "np", ".", "array", "(", "concat_list", "[", ":", "min_steps", "]", ")", "\n", "\n", "", "mean", "=", "np", ".", "mean", "(", "np_data", ",", "axis", "=", "0", ")", "\n", "std", "=", "np", ".", "std", "(", "np_data", ",", "axis", "=", "0", ")", "\n", "\n", "proc_data", ".", "append", "(", "(", "mean", ",", "std", ")", ")", "\n", "\n", "", "return", "proc_data", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cmc_compare_reward_envs.plot_data": [[100, 131], ["matplotlib.subplots", "matplotlib.legend", "matplotlib.subplots_adjust", "matplotlib.title", "matplotlib.xlabel", "matplotlib.xlim", "matplotlib.ylim", "matplotlib.ylabel", "os.path.dirname", "matplotlib.savefig", "matplotlib.show", "matplotlib.plot", "matplotlib.fill_between", "legobj.set_linewidth", "os.path.join", "range", "len"], "function", ["None"], ["", "def", "plot_data", "(", "proc_data", ",", "savefig_name", ")", ":", "\n", "    ", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", "dpi", "=", "600", ",", "figsize", "=", "(", "5", ",", "4", ")", ")", "\n", "colors", "=", "[", "]", "\n", "#", "\n", "# for mean, _ in data_w:", "\n", "#     plt.plot(mean_w)", "\n", "#     colors.append(plt.gca().lines[-1].get_color())", "\n", "\n", "for", "mean", ",", "std", "in", "proc_data", ":", "\n", "        ", "plt", ".", "plot", "(", "mean", ",", "linewidth", "=", "1", ")", "\n", "\n", "", "for", "mean", ",", "std", "in", "proc_data", ":", "\n", "        ", "plt", ".", "fill_between", "(", "x", "=", "range", "(", "len", "(", "mean", ")", ")", ",", "y1", "=", "mean", "-", "std", "*", "STD_MULT", ",", "y2", "=", "mean", "+", "std", "*", "STD_MULT", ",", "alpha", "=", "0.3", ")", "\n", "\n", "", "leg", "=", "plt", ".", "legend", "(", "LEGEND", ",", "fontsize", "=", "9", ")", "\n", "\n", "for", "legobj", "in", "leg", ".", "legendHandles", ":", "\n", "        ", "legobj", ".", "set_linewidth", "(", "2.0", ")", "\n", "\n", "# plt.xlim(0,99)", "\n", "", "plt", ".", "subplots_adjust", "(", "bottom", "=", "0.15", ",", "left", "=", "0.15", ")", "\n", "plt", ".", "title", "(", "'MountainCarContinuous-v0'", ")", "\n", "plt", ".", "xlabel", "(", "'steps'", ")", "\n", "plt", ".", "xlim", "(", "0", ",", "MIN_STEPS", ")", "\n", "# ax.xaxis.set_tick_params(labelsize='small')", "\n", "# plt.xlim(0, 60000)", "\n", "plt", ".", "ylim", "(", "-", "75", ",", "100", ")", "\n", "plt", ".", "ylabel", "(", "'cumulative reward'", ")", "\n", "base_dir", "=", "os", ".", "path", ".", "dirname", "(", "LOG_FILES", "[", "0", "]", ")", "\n", "plt", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "base_dir", ",", "savefig_name", ")", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.bohb_params_TD3_pendulum.ExperimentWrapper.get_bohb_parameters": [[18, 27], ["None"], "methods", ["None"], ["    ", "def", "get_bohb_parameters", "(", "self", ")", ":", "\n", "        ", "params", "=", "{", "}", "\n", "params", "[", "'min_budget'", "]", "=", "1", "\n", "params", "[", "'max_budget'", "]", "=", "8", "\n", "params", "[", "'eta'", "]", "=", "2", "\n", "params", "[", "'iterations'", "]", "=", "1000", "\n", "params", "[", "'random_fraction'", "]", "=", "0.3", "\n", "\n", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.bohb_params_TD3_pendulum.ExperimentWrapper.get_configspace": [[28, 45], ["ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter"], "methods", ["None"], ["", "def", "get_configspace", "(", "self", ")", ":", "\n", "        ", "cs", "=", "CS", ".", "ConfigurationSpace", "(", ")", "\n", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'td3_batch_size'", ",", "lower", "=", "64", ",", "upper", "=", "256", ",", "log", "=", "False", ",", "default_value", "=", "128", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_gamma'", ",", "lower", "=", "0.001", ",", "upper", "=", "0.1", ",", "log", "=", "True", ",", "default_value", "=", "0.01", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_lr'", ",", "lower", "=", "1e-4", ",", "upper", "=", "5e-3", ",", "log", "=", "True", ",", "default_value", "=", "1e-3", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_tau'", ",", "lower", "=", "0.005", ",", "upper", "=", "0.05", ",", "log", "=", "True", ",", "default_value", "=", "0.01", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'td3_policy_delay'", ",", "lower", "=", "1", ",", "upper", "=", "3", ",", "log", "=", "False", ",", "default_value", "=", "2", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "CategoricalHyperparameter", "(", "name", "=", "'td3_activation_fn'", ",", "choices", "=", "[", "'tanh'", ",", "'relu'", ",", "'leakyrelu'", ",", "'prelu'", "]", ",", "default_value", "=", "'relu'", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'td3_hidden_size'", ",", "lower", "=", "48", ",", "upper", "=", "192", ",", "log", "=", "True", ",", "default_value", "=", "128", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'td3_hidden_layer'", ",", "lower", "=", "1", ",", "upper", "=", "2", ",", "log", "=", "False", ",", "default_value", "=", "2", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_action_std'", ",", "lower", "=", "0.05", ",", "upper", "=", "0.2", ",", "log", "=", "True", ",", "default_value", "=", "0.1", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_policy_std'", ",", "lower", "=", "0.1", ",", "upper", "=", "0.4", ",", "log", "=", "True", ",", "default_value", "=", "0.2", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_policy_std_clip'", ",", "lower", "=", "0.25", ",", "upper", "=", "1", ",", "log", "=", "True", ",", "default_value", "=", "0.5", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_early_out_virtual_diff'", ",", "lower", "=", "1e-2", ",", "upper", "=", "1e-1", ",", "log", "=", "True", ",", "default_value", "=", "3e-2", ")", ")", "\n", "\n", "return", "cs", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.bohb_params_TD3_pendulum.ExperimentWrapper.get_specific_config": [[46, 64], ["copy.deepcopy"], "methods", ["None"], ["", "def", "get_specific_config", "(", "self", ",", "cso", ",", "default_config", ",", "budget", ")", ":", "\n", "        ", "config", "=", "deepcopy", "(", "default_config", ")", "\n", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'batch_size'", "]", "=", "cso", "[", "\"td3_batch_size\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'gamma'", "]", "=", "1", "-", "cso", "[", "\"td3_gamma\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'lr'", "]", "=", "cso", "[", "\"td3_lr\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'tau'", "]", "=", "cso", "[", "\"td3_tau\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'policy_delay'", "]", "=", "cso", "[", "\"td3_policy_delay\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'activation_fn'", "]", "=", "cso", "[", "\"td3_activation_fn\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'hidden_size'", "]", "=", "cso", "[", "\"td3_hidden_size\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'hidden_layer'", "]", "=", "cso", "[", "\"td3_hidden_layer\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'action_std'", "]", "=", "cso", "[", "\"td3_action_std\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'policy_std'", "]", "=", "cso", "[", "\"td3_policy_std\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'policy_std_clip'", "]", "=", "cso", "[", "\"td3_policy_std_clip\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'early_out_virtual_diff'", "]", "=", "cso", "[", "\"td3_early_out_virtual_diff\"", "]", "\n", "config", "[", "\"device\"", "]", "=", "'cuda'", "\n", "\n", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.bohb_params_TD3_pendulum.ExperimentWrapper.compute": [[65, 103], ["bohb_params_TD3_pendulum.ExperimentWrapper.get_specific_config", "print", "print", "print", "print", "print", "print", "envs.env_factory.EnvFactory", "envs.env_factory.EnvFactory.generate_real_env", "range", "str", "print", "print", "print", "print", "open", "yaml.safe_load", "agents.TD3.TD3", "agents.TD3.TD3.train", "len", "str", "str", "str", "str", "envs.env_factory.EnvFactory.generate_real_env.get_max_action"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_step_size.ExperimentWrapper.get_specific_config", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_real_env", "home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.train", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.get_max_action"], ["", "def", "compute", "(", "self", ",", "working_dir", ",", "bohb_id", ",", "config_id", ",", "cso", ",", "budget", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "with", "open", "(", "\"default_config_pendulum.yaml\"", ",", "'r'", ")", "as", "stream", ":", "\n", "            ", "default_config", "=", "yaml", ".", "safe_load", "(", "stream", ")", "\n", "\n", "", "config", "=", "self", ".", "get_specific_config", "(", "cso", ",", "default_config", ",", "budget", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "\"START BOHB ITERATION\"", ")", "\n", "print", "(", "'CONFIG: '", "+", "str", "(", "config", ")", ")", "\n", "print", "(", "'CSO:    '", "+", "str", "(", "cso", ")", ")", "\n", "print", "(", "'BUDGET: '", "+", "str", "(", "budget", ")", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "info", "=", "{", "}", "\n", "\n", "# generate environment", "\n", "env_fac", "=", "EnvFactory", "(", "config", ")", "\n", "real_env", "=", "env_fac", ".", "generate_real_env", "(", ")", "\n", "\n", "score", "=", "0", "\n", "for", "i", "in", "range", "(", "NUM_EVALS", ")", ":", "\n", "            ", "td3", "=", "TD3", "(", "env", "=", "real_env", ",", "\n", "max_action", "=", "real_env", ".", "get_max_action", "(", ")", ",", "\n", "config", "=", "config", ")", "\n", "rewards", ",", "_", ",", "_", "=", "td3", ".", "train", "(", "real_env", ")", "\n", "score", "+=", "len", "(", "rewards", ")", "\n", "\n", "", "score", "=", "score", "/", "NUM_EVALS", "\n", "\n", "info", "[", "'config'", "]", "=", "str", "(", "config", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "'FINAL SCORE: '", "+", "str", "(", "score", ")", ")", "\n", "print", "(", "\"END BOHB ITERATION\"", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "return", "{", "\n", "\"loss\"", ":", "score", ",", "\n", "\"info\"", ":", "info", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cmc_se_params.ExperimentWrapper.get_bohb_parameters": [[19, 28], ["None"], "methods", ["None"], ["    ", "def", "get_bohb_parameters", "(", "self", ")", ":", "\n", "        ", "params", "=", "{", "}", "\n", "params", "[", "'min_budget'", "]", "=", "1", "\n", "params", "[", "'max_budget'", "]", "=", "3", "\n", "params", "[", "'eta'", "]", "=", "3", "\n", "params", "[", "'random_fraction'", "]", "=", "0.3", "\n", "params", "[", "'iterations'", "]", "=", "10000", "\n", "\n", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cmc_se_params.ExperimentWrapper.get_configspace": [[29, 57], ["ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter"], "methods", ["None"], ["", "def", "get_configspace", "(", "self", ")", ":", "\n", "        ", "cs", "=", "CS", ".", "ConfigurationSpace", "(", ")", "\n", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'gtn_score_transform_type'", ",", "lower", "=", "0", ",", "upper", "=", "7", ",", "log", "=", "False", ",", "default_value", "=", "7", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'gtn_step_size'", ",", "lower", "=", "0.1", ",", "upper", "=", "1", ",", "log", "=", "True", ",", "default_value", "=", "0.5", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "CategoricalHyperparameter", "(", "name", "=", "'gtn_mirrored_sampling'", ",", "choices", "=", "[", "False", ",", "True", "]", ",", "default_value", "=", "True", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'gtn_noise_std'", ",", "lower", "=", "0.001", ",", "upper", "=", "1", ",", "log", "=", "True", ",", "default_value", "=", "0.01", ")", ")", "\n", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'td3_init_episodes'", ",", "lower", "=", "1", ",", "upper", "=", "100", ",", "log", "=", "True", ",", "default_value", "=", "20", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'td3_batch_size'", ",", "lower", "=", "32", ",", "upper", "=", "256", ",", "log", "=", "False", ",", "default_value", "=", "128", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_gamma'", ",", "lower", "=", "0.001", ",", "upper", "=", "0.1", ",", "log", "=", "True", ",", "default_value", "=", "0.01", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_lr'", ",", "lower", "=", "1e-5", ",", "upper", "=", "5e-2", ",", "log", "=", "True", ",", "default_value", "=", "1e-3", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_tau'", ",", "lower", "=", "0.005", ",", "upper", "=", "0.05", ",", "log", "=", "True", ",", "default_value", "=", "0.01", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'td3_policy_delay'", ",", "lower", "=", "1", ",", "upper", "=", "3", ",", "log", "=", "False", ",", "default_value", "=", "2", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "CategoricalHyperparameter", "(", "name", "=", "'td3_activation_fn'", ",", "choices", "=", "[", "'tanh'", ",", "'relu'", ",", "'leakyrelu'", ",", "'prelu'", "]", ",", "default_value", "=", "'relu'", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'td3_hidden_size'", ",", "lower", "=", "48", ",", "upper", "=", "192", ",", "log", "=", "True", ",", "default_value", "=", "128", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'td3_hidden_layer'", ",", "lower", "=", "1", ",", "upper", "=", "2", ",", "log", "=", "False", ",", "default_value", "=", "2", ")", ")", "\n", "# cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(name='td3_same_action_num', lower=2, upper=3, log=False, default_value=2))", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_action_std'", ",", "lower", "=", "0.05", ",", "upper", "=", "0.6", ",", "log", "=", "True", ",", "default_value", "=", "0.1", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_policy_std'", ",", "lower", "=", "0.1", ",", "upper", "=", "0.6", ",", "log", "=", "True", ",", "default_value", "=", "0.2", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_policy_std_clip'", ",", "lower", "=", "0.25", ",", "upper", "=", "1", ",", "log", "=", "True", ",", "default_value", "=", "0.5", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_early_out_virtual_diff'", ",", "lower", "=", "1e-2", ",", "upper", "=", "1e-1", ",", "log", "=", "True", ",", "default_value", "=", "3e-2", ")", ")", "\n", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "CategoricalHyperparameter", "(", "name", "=", "'cmc_activation_fn'", ",", "choices", "=", "[", "'tanh'", ",", "'relu'", ",", "'leakyrelu'", ",", "'prelu'", "]", ",", "default_value", "=", "'leakyrelu'", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'cmc_hidden_size'", ",", "lower", "=", "48", ",", "upper", "=", "192", ",", "log", "=", "True", ",", "default_value", "=", "128", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'cmc_hidden_layer'", ",", "lower", "=", "1", ",", "upper", "=", "3", ",", "log", "=", "False", ",", "default_value", "=", "1", ")", ")", "\n", "\n", "return", "cs", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cmc_se_params.ExperimentWrapper.get_specific_config": [[58, 89], ["copy.deepcopy"], "methods", ["None"], ["", "def", "get_specific_config", "(", "self", ",", "cso", ",", "default_config", ",", "budget", ")", ":", "\n", "        ", "config", "=", "deepcopy", "(", "default_config", ")", "\n", "\n", "config", "[", "\"agents\"", "]", "[", "'gtn'", "]", "[", "'score_transform_type'", "]", "=", "cso", "[", "\"gtn_score_transform_type\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'gtn'", "]", "[", "'step_size'", "]", "=", "cso", "[", "\"gtn_step_size\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'gtn'", "]", "[", "'mirrored_sampling'", "]", "=", "cso", "[", "\"gtn_mirrored_sampling\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'gtn'", "]", "[", "'noise_std'", "]", "=", "cso", "[", "\"gtn_noise_std\"", "]", "\n", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'init_episodes'", "]", "=", "cso", "[", "\"td3_init_episodes\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'batch_size'", "]", "=", "cso", "[", "\"td3_batch_size\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'gamma'", "]", "=", "1", "-", "cso", "[", "\"td3_gamma\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'lr'", "]", "=", "cso", "[", "\"td3_lr\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'tau'", "]", "=", "cso", "[", "\"td3_tau\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'policy_delay'", "]", "=", "cso", "[", "\"td3_policy_delay\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'activation_fn'", "]", "=", "cso", "[", "\"td3_activation_fn\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'hidden_size'", "]", "=", "cso", "[", "\"td3_hidden_size\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'hidden_layer'", "]", "=", "cso", "[", "\"td3_hidden_layer\"", "]", "\n", "# config[\"agents\"]['td3']['same_action_num'] = cso[\"td3_same_action_num\"]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'action_std'", "]", "=", "cso", "[", "\"td3_action_std\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'policy_std'", "]", "=", "cso", "[", "\"td3_policy_std\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'policy_std_clip'", "]", "=", "cso", "[", "\"td3_policy_std_clip\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'early_out_virtual_diff'", "]", "=", "cso", "[", "\"td3_early_out_virtual_diff\"", "]", "\n", "\n", "config", "[", "\"envs\"", "]", "[", "'MountainCarContinuous-v0'", "]", "[", "'activation_fn'", "]", "=", "cso", "[", "\"cmc_activation_fn\"", "]", "\n", "config", "[", "\"envs\"", "]", "[", "'MountainCarContinuous-v0'", "]", "[", "'hidden_size'", "]", "=", "cso", "[", "\"cmc_hidden_size\"", "]", "\n", "config", "[", "\"envs\"", "]", "[", "'MountainCarContinuous-v0'", "]", "[", "'hidden_layer'", "]", "=", "cso", "[", "\"cmc_hidden_layer\"", "]", "\n", "\n", "global", "reward_env_type", "\n", "config", "[", "\"agents\"", "]", "[", "'gtn'", "]", "[", "'synthetic_env_type'", "]", "=", "0", "\n", "\n", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cmc_se_params.ExperimentWrapper.compute": [[90, 132], ["GTNC_evaluate_cmc_se_params.ExperimentWrapper.get_specific_config", "print", "print", "print", "print", "print", "print", "str", "str", "str", "str", "print", "print", "print", "print", "print", "open", "yaml.safe_load", "agents.GTN.GTN_Master", "agents.GTN.GTN_Master.run", "math.isnan", "str", "str", "str", "float", "float", "traceback.format_exc", "print", "str", "str", "sorted"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_step_size.ExperimentWrapper.get_specific_config", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.run"], ["", "def", "compute", "(", "self", ",", "working_dir", ",", "bohb_id", ",", "config_id", ",", "cso", ",", "budget", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "with", "open", "(", "\"default_config_cmc.yaml\"", ",", "'r'", ")", "as", "stream", ":", "\n", "            ", "default_config", "=", "yaml", ".", "safe_load", "(", "stream", ")", "\n", "\n", "", "config", "=", "self", ".", "get_specific_config", "(", "cso", ",", "default_config", ",", "budget", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "\"START BOHB ITERATION\"", ")", "\n", "print", "(", "'CONFIG: '", "+", "str", "(", "config", ")", ")", "\n", "print", "(", "'CSO:    '", "+", "str", "(", "cso", ")", ")", "\n", "print", "(", "'BUDGET: '", "+", "str", "(", "budget", ")", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "try", ":", "\n", "            ", "gtn", "=", "GTN_Master", "(", "config", ",", "bohb_id", "=", "bohb_id", ",", "bohb_working_dir", "=", "working_dir", ")", "\n", "_", ",", "score_list", ",", "model_name", "=", "gtn", ".", "run", "(", ")", "\n", "score", "=", "-", "sorted", "(", "score_list", ")", "[", "-", "1", "]", "\n", "if", "math", ".", "isnan", "(", "score", ")", ":", "\n", "                ", "score", "=", "float", "(", "'Inf'", ")", "\n", "", "error", "=", "\"\"", "\n", "", "except", ":", "\n", "            ", "score", "=", "float", "(", "'Inf'", ")", "\n", "score_list", "=", "[", "]", "\n", "model_name", "=", "None", "\n", "error", "=", "traceback", ".", "format_exc", "(", ")", "\n", "print", "(", "error", ")", "\n", "\n", "", "info", "=", "{", "}", "\n", "info", "[", "'error'", "]", "=", "str", "(", "error", ")", "\n", "info", "[", "'config'", "]", "=", "str", "(", "config", ")", "\n", "info", "[", "'score_list'", "]", "=", "str", "(", "score_list", ")", "\n", "info", "[", "'model_name'", "]", "=", "str", "(", "model_name", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "'FINAL SCORE: '", "+", "str", "(", "score", ")", ")", "\n", "print", "(", "'SCORE LIST:  '", "+", "str", "(", "score_list", ")", ")", "\n", "print", "(", "\"END BOHB ITERATION\"", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "return", "{", "\n", "\"loss\"", ":", "score", ",", "\n", "\"info\"", ":", "info", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_acrobot_vary_hp_2_TD3_discrete.load_envs_and_config": [[19, 36], ["os.path.join", "torch.load", "envs.env_factory.EnvFactory", "envs.env_factory.EnvFactory.generate_virtual_env", "env_factory.generate_virtual_env.load_state_dict", "envs.env_factory.EnvFactory.generate_real_env", "open", "yaml.safe_load"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_virtual_env", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_real_env"], ["def", "load_envs_and_config", "(", "file_name", ",", "model_dir", ",", "device", ")", ":", "\n", "    ", "file_path", "=", "os", ".", "path", ".", "join", "(", "model_dir", ",", "file_name", ")", "\n", "save_dict", "=", "torch", ".", "load", "(", "file_path", ")", "\n", "config", "=", "save_dict", "[", "'config'", "]", "\n", "config", "[", "'device'", "]", "=", "device", "\n", "env_factory", "=", "EnvFactory", "(", "config", "=", "config", ")", "\n", "virtual_env", "=", "env_factory", ".", "generate_virtual_env", "(", ")", "\n", "virtual_env", ".", "load_state_dict", "(", "save_dict", "[", "'model'", "]", ")", "\n", "real_env", "=", "env_factory", ".", "generate_real_env", "(", ")", "\n", "\n", "# load additional agent configs", "\n", "with", "open", "(", "\"../default_config_acrobot.yaml\"", ",", "\"r\"", ")", "as", "stream", ":", "\n", "        ", "config_new", "=", "yaml", ".", "safe_load", "(", "stream", ")", "[", "\"agents\"", "]", "\n", "\n", "", "config", "[", "\"agents\"", "]", "[", "\"td3_discrete_vary\"", "]", "=", "config_new", "[", "\"td3_discrete_vary_layer_norm_2\"", "]", "\n", "\n", "return", "virtual_env", ",", "real_env", ",", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_acrobot_vary_hp_2_TD3_discrete.train_test_agents": [[38, 62], ["range", "agents.agent_utils.select_agent", "agents.agent_utils.select_agent.train", "agents.agent_utils.select_agent.test", "print", "reward_list.append", "train_steps_needed.append", "episodes_needed.append", "str", "sum", "len"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.agents.agent_utils.select_agent", "home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.train", "home.repos.pwc.inspect_result.automl_learning_environments.agents.base_agent.BaseAgent.test"], ["", "def", "train_test_agents", "(", "train_env", ",", "test_env", ",", "config", ",", "agents_num", ")", ":", "\n", "    ", "reward_list", "=", "[", "]", "\n", "train_steps_needed", "=", "[", "]", "\n", "episodes_needed", "=", "[", "]", "\n", "\n", "# settings for comparability", "\n", "config", "[", "'agents'", "]", "[", "'td3_discrete_vary'", "]", "[", "'vary_hp'", "]", "=", "True", "\n", "config", "[", "'agents'", "]", "[", "'td3_discrete_vary'", "]", "[", "'print_rate'", "]", "=", "10", "\n", "config", "[", "'agents'", "]", "[", "'td3_discrete_vary'", "]", "[", "'early_out_num'", "]", "=", "10", "\n", "config", "[", "'agents'", "]", "[", "'td3_discrete_vary'", "]", "[", "'train_episodes'", "]", "=", "500", "\n", "config", "[", "'agents'", "]", "[", "'td3_discrete_vary'", "]", "[", "'init_episodes'", "]", "=", "10", "\n", "config", "[", "'agents'", "]", "[", "'td3_discrete_vary'", "]", "[", "'test_episodes'", "]", "=", "10", "\n", "config", "[", "'agents'", "]", "[", "'td3_discrete_vary'", "]", "[", "'early_out_virtual_diff'", "]", "=", "0.01", "\n", "\n", "for", "i", "in", "range", "(", "agents_num", ")", ":", "\n", "        ", "agent", "=", "select_agent", "(", "config", "=", "config", ",", "agent_name", "=", "'TD3_discrete_vary'", ")", "\n", "reward_train", ",", "episode_length", ",", "_", "=", "agent", ".", "train", "(", "env", "=", "train_env", ")", "\n", "reward", ",", "_", ",", "_", "=", "agent", ".", "test", "(", "env", "=", "test_env", ")", "\n", "print", "(", "'reward: '", "+", "str", "(", "reward", ")", ")", "\n", "reward_list", ".", "append", "(", "reward", ")", "\n", "train_steps_needed", ".", "append", "(", "[", "sum", "(", "episode_length", ")", "]", ")", "\n", "episodes_needed", ".", "append", "(", "[", "(", "len", "(", "reward_train", ")", ")", "]", ")", "\n", "\n", "", "return", "reward_list", ",", "train_steps_needed", ",", "episodes_needed", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_pendulum_compare_reward_envs.get_best_models_from_log": [[20, 44], ["hpbandster.logged_results_to_HBS_result", "hpres.logged_results_to_HBS_result.data.values", "best_models.sort", "os.path.isdir", "log_dir.replace.replace", "best_models.append", "os.path.isfile", "model_name.replace.replace"], "function", ["None"], ["def", "get_best_models_from_log", "(", "log_dir", ")", ":", "\n", "    ", "if", "not", "os", ".", "path", ".", "isdir", "(", "log_dir", ")", ":", "\n", "        ", "log_dir", "=", "log_dir", ".", "replace", "(", "'nierhoff'", ",", "'dingsda'", ")", "\n", "\n", "", "result", "=", "hpres", ".", "logged_results_to_HBS_result", "(", "log_dir", ")", "\n", "\n", "best_models", "=", "[", "]", "\n", "\n", "for", "value", "in", "result", ".", "data", ".", "values", "(", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "loss", "=", "value", ".", "results", "[", "1.0", "]", "[", "'loss'", "]", "\n", "model_name", "=", "value", ".", "results", "[", "1.0", "]", "[", "'info'", "]", "[", "'model_name'", "]", "\n", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "model_name", ")", ":", "\n", "                ", "model_name", "=", "model_name", ".", "replace", "(", "'nierhoff'", ",", "'dingsda'", ")", "\n", "", "best_models", ".", "append", "(", "(", "loss", ",", "model_name", ")", ")", "\n", "", "except", ":", "\n", "            ", "continue", "\n", "\n", "", "", "best_models", ".", "sort", "(", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ")", "\n", "# best_models.sort(key=lambda x: x[0], reverse=True)", "\n", "best_models", "=", "best_models", "[", ":", "MODEL_NUM", "]", "\n", "\n", "return", "best_models", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_pendulum_compare_reward_envs.load_envs_and_config": [[46, 59], ["torch.load", "envs.env_factory.EnvFactory", "envs.env_factory.EnvFactory.generate_reward_env", "env_factory.generate_reward_env.load_state_dict", "envs.env_factory.EnvFactory.generate_real_env"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_reward_env", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_real_env"], ["", "def", "load_envs_and_config", "(", "model_file", ")", ":", "\n", "    ", "save_dict", "=", "torch", ".", "load", "(", "model_file", ")", "\n", "\n", "config", "=", "save_dict", "[", "'config'", "]", "\n", "config", "[", "'device'", "]", "=", "'cuda'", "\n", "config", "[", "'envs'", "]", "[", "'Pendulum-v0'", "]", "[", "'solved_reward'", "]", "=", "100000", "# something big enough to prevent early out triggering", "\n", "\n", "env_factory", "=", "EnvFactory", "(", "config", "=", "config", ")", "\n", "reward_env", "=", "env_factory", ".", "generate_reward_env", "(", ")", "\n", "reward_env", ".", "load_state_dict", "(", "save_dict", "[", "'model'", "]", ")", "\n", "real_env", "=", "env_factory", ".", "generate_real_env", "(", ")", "\n", "\n", "return", "reward_env", ",", "real_env", ",", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_pendulum_compare_reward_envs.train_test_agents": [[61, 79], ["range", "agents.agent_utils.select_agent.train", "print", "rewards.append", "agents.agent_utils.select_agent", "agents.agent_utils.select_agent", "str"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.train", "home.repos.pwc.inspect_result.automl_learning_environments.agents.agent_utils.select_agent", "home.repos.pwc.inspect_result.automl_learning_environments.agents.agent_utils.select_agent"], ["", "def", "train_test_agents", "(", "mode", ",", "env", ",", "real_env", ",", "config", ")", ":", "\n", "    ", "rewards", "=", "[", "]", "\n", "\n", "# settings for comparability", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'test_episodes'", "]", "=", "1", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'train_episodes'", "]", "=", "50", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'print_rate'", "]", "=", "1", "\n", "\n", "for", "i", "in", "range", "(", "MODEL_AGENTS", ")", ":", "\n", "        ", "if", "mode", "==", "'-1'", ":", "\n", "            ", "agent", "=", "select_agent", "(", "config", "=", "config", ",", "agent_name", "=", "'td3_icm'", ")", "\n", "", "else", ":", "\n", "            ", "agent", "=", "select_agent", "(", "config", "=", "config", ",", "agent_name", "=", "'td3'", ")", "\n", "", "reward", ",", "_", ",", "_", "=", "agent", ".", "train", "(", "env", "=", "env", ",", "test_env", "=", "real_env", ")", "\n", "print", "(", "'reward: '", "+", "str", "(", "reward", ")", ")", "\n", "rewards", ".", "append", "(", "reward", ")", "\n", "\n", "", "return", "rewards", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_pendulum_compare_reward_envs.save_list": [[81, 89], ["os.makedirs", "os.path.join", "print", "torch.save", "str", "str"], "function", ["None"], ["", "def", "save_list", "(", "mode", ",", "config", ",", "reward_list", ")", ":", "\n", "    ", "os", ".", "makedirs", "(", "SAVE_DIR", ",", "exist_ok", "=", "True", ")", "\n", "file_name", "=", "os", ".", "path", ".", "join", "(", "SAVE_DIR", ",", "'best'", "+", "str", "(", "MODEL_NUM", ")", "+", "'_'", "+", "str", "(", "mode", ")", "+", "'.pt'", ")", "\n", "save_dict", "=", "{", "}", "\n", "save_dict", "[", "'config'", "]", "=", "config", "\n", "save_dict", "[", "'reward_list'", "]", "=", "reward_list", "\n", "print", "(", "reward_list", ")", "\n", "torch", ".", "save", "(", "save_dict", ",", "file_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_pendulum_compare_reward_envs.eval_models": [[91, 102], ["GTNC_evaluate_pendulum_compare_reward_envs.get_best_models_from_log", "GTNC_evaluate_pendulum_compare_reward_envs.save_list", "print", "print", "GTNC_evaluate_pendulum_compare_reward_envs.load_envs_and_config", "GTNC_evaluate_pendulum_compare_reward_envs.train_test_agents", "str", "str"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.get_best_models_from_log", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.save_list", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.load_envs_and_config", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.train_test_agents"], ["", "def", "eval_models", "(", "mode", ",", "log_dir", ")", ":", "\n", "    ", "best_models", "=", "get_best_models_from_log", "(", "log_dir", ")", "\n", "\n", "reward_list", "=", "[", "]", "\n", "for", "best_reward", ",", "model_file", "in", "best_models", ":", "\n", "        ", "print", "(", "'best reward: '", "+", "str", "(", "best_reward", ")", ")", "\n", "print", "(", "'model file: '", "+", "str", "(", "model_file", ")", ")", "\n", "reward_env", ",", "real_env", ",", "config", "=", "load_envs_and_config", "(", "model_file", ")", "\n", "rewards", "=", "train_test_agents", "(", "mode", "=", "mode", ",", "env", "=", "reward_env", ",", "real_env", "=", "real_env", ",", "config", "=", "config", ")", "\n", "reward_list", "+=", "rewards", "\n", "", "save_list", "(", "mode", ",", "config", ",", "reward_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_pendulum_compare_reward_envs.eval_base": [[104, 113], ["GTNC_evaluate_pendulum_compare_reward_envs.get_best_models_from_log", "range", "GTNC_evaluate_pendulum_compare_reward_envs.save_list", "GTNC_evaluate_pendulum_compare_reward_envs.load_envs_and_config", "GTNC_evaluate_pendulum_compare_reward_envs.train_test_agents"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.get_best_models_from_log", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.save_list", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.load_envs_and_config", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.train_test_agents"], ["", "def", "eval_base", "(", "mode", ",", "log_dir", ")", ":", "\n", "    ", "best_models", "=", "get_best_models_from_log", "(", "log_dir", ")", "\n", "\n", "reward_list", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "MODEL_NUM", ")", ":", "\n", "        ", "reward_env", ",", "real_env", ",", "config", "=", "load_envs_and_config", "(", "best_models", "[", "0", "]", "[", "1", "]", ")", "\n", "rewards", "=", "train_test_agents", "(", "mode", "=", "mode", ",", "env", "=", "real_env", ",", "real_env", "=", "real_env", ",", "config", "=", "config", ")", "\n", "reward_list", "+=", "rewards", "\n", "", "save_list", "(", "mode", ",", "config", ",", "reward_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_pendulum_compare_reward_envs.eval_icm": [[115, 124], ["GTNC_evaluate_pendulum_compare_reward_envs.get_best_models_from_log", "range", "GTNC_evaluate_pendulum_compare_reward_envs.save_list", "GTNC_evaluate_pendulum_compare_reward_envs.load_envs_and_config", "GTNC_evaluate_pendulum_compare_reward_envs.train_test_agents"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.get_best_models_from_log", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.save_list", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.load_envs_and_config", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.train_test_agents"], ["", "def", "eval_icm", "(", "mode", ",", "log_dir", ")", ":", "\n", "    ", "best_models", "=", "get_best_models_from_log", "(", "log_dir", ")", "\n", "\n", "reward_list", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "MODEL_NUM", ")", ":", "\n", "        ", "reward_env", ",", "real_env", ",", "config", "=", "load_envs_and_config", "(", "best_models", "[", "0", "]", "[", "1", "]", ")", "\n", "rewards", "=", "train_test_agents", "(", "mode", "=", "mode", ",", "env", "=", "real_env", ",", "real_env", "=", "real_env", ",", "config", "=", "config", ")", "\n", "reward_list", "+=", "rewards", "\n", "", "save_list", "(", "mode", ",", "config", ",", "reward_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_transfer_algo.get_best_models_from_log": [[42, 72], ["hpbandster.logged_results_to_HBS_result", "hpres.logged_results_to_HBS_result.data.values", "print", "best_models.sort", "os.path.isdir", "log_dir.replace.replace", "best_models.append", "os.path.isfile", "model_name.replace.replace"], "function", ["None"], ["def", "get_best_models_from_log", "(", "log_dir", ")", ":", "\n", "    ", "if", "not", "os", ".", "path", ".", "isdir", "(", "log_dir", ")", ":", "\n", "        ", "log_dir", "=", "log_dir", ".", "replace", "(", "'nierhoff'", ",", "'dingsda'", ")", "\n", "\n", "", "result", "=", "hpres", ".", "logged_results_to_HBS_result", "(", "log_dir", ")", "\n", "\n", "best_models", "=", "[", "]", "\n", "\n", "for", "value", "in", "result", ".", "data", ".", "values", "(", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "loss", "=", "value", ".", "results", "[", "1.0", "]", "[", "'loss'", "]", "\n", "model_name", "=", "value", ".", "results", "[", "1.0", "]", "[", "'info'", "]", "[", "'model_name'", "]", "\n", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "model_name", ")", ":", "\n", "                ", "model_name", "=", "model_name", ".", "replace", "(", "'nierhoff'", ",", "'dingsda'", ")", "\n", "", "best_models", ".", "append", "(", "(", "loss", ",", "model_name", ")", ")", "\n", "", "except", ":", "\n", "            ", "continue", "\n", "\n", "# before AUC (objective minimized)", "\n", "# print(\"sorting from low to high values (non-AUC)\")", "\n", "# best_models.sort(key=lambda x: x[0])", "\n", "# best_models = best_models[:MODEL_NUM]", "\n", "\n", "# AUC (objective maximized)", "\n", "", "", "print", "(", "\"sorting from high to low values (AUC)\"", ")", "\n", "best_models", ".", "sort", "(", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ",", "reverse", "=", "True", ")", "\n", "best_models", "=", "best_models", "[", ":", "MODEL_NUM", "]", "\n", "\n", "return", "best_models", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_transfer_algo.load_envs_and_config": [[74, 87], ["torch.load", "envs.env_factory.EnvFactory", "envs.env_factory.EnvFactory.generate_reward_env", "env_factory.generate_reward_env.load_state_dict", "envs.env_factory.EnvFactory.generate_real_env"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_reward_env", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_real_env"], ["", "def", "load_envs_and_config", "(", "model_file", ")", ":", "\n", "    ", "save_dict", "=", "torch", ".", "load", "(", "model_file", ")", "\n", "\n", "config", "=", "save_dict", "[", "'config'", "]", "\n", "# config['device'] = 'cuda'", "\n", "config", "[", "'envs'", "]", "[", "'Cliff'", "]", "[", "'solved_reward'", "]", "=", "100000", "# something big enough to prevent early out triggering", "\n", "\n", "env_factory", "=", "EnvFactory", "(", "config", "=", "config", ")", "\n", "reward_env", "=", "env_factory", ".", "generate_reward_env", "(", ")", "\n", "reward_env", ".", "load_state_dict", "(", "save_dict", "[", "'model'", "]", ")", "\n", "real_env", "=", "env_factory", ".", "generate_real_env", "(", ")", "\n", "\n", "return", "reward_env", ",", "real_env", ",", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_transfer_algo.train_test_agents": [[89, 127], ["range", "agents.agent_utils.select_agent.train", "rewards.append", "episode_lengths.append", "agents.agent_utils.select_agent", "agents.agent_utils.select_agent"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.train", "home.repos.pwc.inspect_result.automl_learning_environments.agents.agent_utils.select_agent", "home.repos.pwc.inspect_result.automl_learning_environments.agents.agent_utils.select_agent"], ["", "def", "train_test_agents", "(", "mode", ",", "env", ",", "real_env", ",", "config", ")", ":", "\n", "    ", "rewards", "=", "[", "]", "\n", "episode_lengths", "=", "[", "]", "\n", "\n", "# settings for comparability", "\n", "config", "[", "'agents'", "]", "[", "'sarsa'", "]", "=", "{", "}", "\n", "config", "[", "'agents'", "]", "[", "'sarsa'", "]", "[", "'test_episodes'", "]", "=", "1", "\n", "config", "[", "'agents'", "]", "[", "'sarsa'", "]", "[", "'train_episodes'", "]", "=", "500", "\n", "config", "[", "'agents'", "]", "[", "'sarsa'", "]", "[", "'print_rate'", "]", "=", "100", "\n", "\n", "config", "[", "'agents'", "]", "[", "'sarsa'", "]", "[", "'alpha'", "]", "=", "1.0", "\n", "config", "[", "'agents'", "]", "[", "'sarsa'", "]", "[", "'eps_decay'", "]", "=", "0.0", "\n", "config", "[", "'agents'", "]", "[", "'sarsa'", "]", "[", "'eps_init'", "]", "=", "0.01", "# 0.01", "\n", "config", "[", "'agents'", "]", "[", "'sarsa'", "]", "[", "'eps_min'", "]", "=", "0.01", "# 0.01", "\n", "config", "[", "'agents'", "]", "[", "'sarsa'", "]", "[", "'gamma'", "]", "=", "0.8", "\n", "config", "[", "'agents'", "]", "[", "'sarsa'", "]", "[", "'same_action_num'", "]", "=", "1", "\n", "config", "[", "'agents'", "]", "[", "'sarsa'", "]", "[", "'rb_size'", "]", "=", "1", "# custom to reward env and gridworld", "\n", "config", "[", "'agents'", "]", "[", "'sarsa'", "]", "[", "'init_episodes'", "]", "=", "0", "\n", "config", "[", "'agents'", "]", "[", "'sarsa'", "]", "[", "'batch_size'", "]", "=", "1", "\n", "\n", "config", "[", "'agents'", "]", "[", "'sarsa'", "]", "[", "'early_out_num'", "]", "=", "10", "\n", "config", "[", "'agents'", "]", "[", "'sarsa'", "]", "[", "'early_out_virtual_diff'", "]", "=", "0.02", "\n", "\n", "# for count-based q-learning", "\n", "config", "[", "'agents'", "]", "[", "'sarsa'", "]", "[", "'beta'", "]", "=", "0.1", "\n", "\n", "# for count-based q-learning (tuned)", "\n", "# config['agents']['sarsa']['beta'] = 0.005  # 0.01 also works fine", "\n", "\n", "for", "i", "in", "range", "(", "MODEL_AGENTS", ")", ":", "\n", "        ", "if", "mode", "==", "'-1'", ":", "\n", "            ", "agent", "=", "select_agent", "(", "config", "=", "config", ",", "agent_name", "=", "'sarsa_cb'", ")", "\n", "", "else", ":", "\n", "            ", "agent", "=", "select_agent", "(", "config", "=", "config", ",", "agent_name", "=", "'sarsa'", ")", "\n", "", "reward", ",", "episode_length", ",", "_", "=", "agent", ".", "train", "(", "env", "=", "env", ",", "test_env", "=", "real_env", ")", "\n", "rewards", ".", "append", "(", "reward", ")", "\n", "episode_lengths", ".", "append", "(", "episode_length", ")", "\n", "", "return", "rewards", ",", "episode_lengths", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_transfer_algo.save_list": [[129, 139], ["os.makedirs", "os.path.join", "torch.save", "str"], "function", ["None"], ["", "def", "save_list", "(", "mode", ",", "config", ",", "reward_list", ",", "episode_length_list", ")", ":", "\n", "    ", "os", ".", "makedirs", "(", "SAVE_DIR", ",", "exist_ok", "=", "True", ")", "\n", "file_name", "=", "os", ".", "path", ".", "join", "(", "SAVE_DIR", ",", "'best_transfer_algo'", "+", "str", "(", "mode", ")", "+", "'.pt'", ")", "\n", "save_dict", "=", "{", "}", "\n", "save_dict", "[", "'config'", "]", "=", "config", "\n", "save_dict", "[", "'model_num'", "]", "=", "MODEL_NUM", "\n", "save_dict", "[", "'model_agents'", "]", "=", "MODEL_AGENTS", "\n", "save_dict", "[", "'reward_list'", "]", "=", "reward_list", "\n", "save_dict", "[", "'episode_length_list'", "]", "=", "episode_length_list", "\n", "torch", ".", "save", "(", "save_dict", ",", "file_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_transfer_algo.eval_models": [[141, 156], ["GTNC_evaluate_gridworld_transfer_algo.get_best_models_from_log", "GTNC_evaluate_gridworld_transfer_algo.save_list", "print", "print", "GTNC_evaluate_gridworld_transfer_algo.load_envs_and_config", "GTNC_evaluate_gridworld_transfer_algo.train_test_agents", "str", "str"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.get_best_models_from_log", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.save_list", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.load_envs_and_config", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.train_test_agents"], ["", "def", "eval_models", "(", "mode", ",", "log_dir", ")", ":", "\n", "    ", "best_models", "=", "get_best_models_from_log", "(", "log_dir", ")", "\n", "\n", "reward_list", "=", "[", "]", "\n", "episode_length_list", "=", "[", "]", "\n", "\n", "for", "best_reward", ",", "model_file", "in", "best_models", ":", "\n", "        ", "print", "(", "'best reward: '", "+", "str", "(", "best_reward", ")", ")", "\n", "print", "(", "'model file: '", "+", "str", "(", "model_file", ")", ")", "\n", "reward_env", ",", "real_env", ",", "config", "=", "load_envs_and_config", "(", "model_file", ")", "\n", "rewards", ",", "episode_lengths", "=", "train_test_agents", "(", "mode", "=", "mode", ",", "env", "=", "reward_env", ",", "real_env", "=", "real_env", ",", "config", "=", "config", ")", "\n", "reward_list", "+=", "rewards", "\n", "episode_length_list", "+=", "episode_lengths", "\n", "\n", "", "save_list", "(", "mode", ",", "config", ",", "reward_list", ",", "episode_length_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_transfer_algo.eval_base": [[158, 177], ["GTNC_evaluate_gridworld_transfer_algo.get_best_models_from_log", "range", "GTNC_evaluate_gridworld_transfer_algo.save_list", "GTNC_evaluate_gridworld_transfer_algo.load_envs_and_config", "GTNC_evaluate_gridworld_transfer_algo.train_test_agents"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.get_best_models_from_log", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.save_list", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.load_envs_and_config", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.train_test_agents"], ["", "def", "eval_base", "(", "mode", ",", "log_dir", ")", ":", "\n", "    ", "best_models", "=", "get_best_models_from_log", "(", "log_dir", ")", "\n", "\n", "reward_list", "=", "[", "]", "\n", "episode_length_list", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "MODEL_NUM", ")", ":", "\n", "# if not os.path.isdir(best_models[0][1]):", "\n", "#     model_file = best_models[0][1].replace('/home/dingsda/master_thesis/learning_environments/results',", "\n", "#                                            '/home/ferreira/Projects/learning_environments/results/thomas_results')", "\n", "# else:", "\n", "#     model_file = best_models[0][1]", "\n", "\n", "        ", "reward_env", ",", "real_env", ",", "config", "=", "load_envs_and_config", "(", "best_models", "[", "0", "]", "[", "1", "]", ")", "\n", "rewards", ",", "episode_lengths", "=", "train_test_agents", "(", "mode", "=", "mode", ",", "env", "=", "real_env", ",", "real_env", "=", "real_env", ",", "config", "=", "config", ")", "\n", "reward_list", "+=", "rewards", "\n", "episode_length_list", "+=", "episode_lengths", "\n", "\n", "", "save_list", "(", "mode", ",", "config", ",", "reward_list", ",", "episode_length_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.gridworld_ql_params_bohb.ExperimentWrapper.get_bohb_parameters": [[15, 24], ["None"], "methods", ["None"], ["    ", "def", "get_bohb_parameters", "(", "self", ")", ":", "\n", "        ", "params", "=", "{", "}", "\n", "params", "[", "'min_budget'", "]", "=", "1", "\n", "params", "[", "'max_budget'", "]", "=", "8", "\n", "params", "[", "'eta'", "]", "=", "2", "\n", "params", "[", "'iterations'", "]", "=", "1000", "\n", "params", "[", "'random_fraction'", "]", "=", "0.3", "\n", "\n", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.gridworld_ql_params_bohb.ExperimentWrapper.get_configspace": [[25, 35], ["ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter"], "methods", ["None"], ["", "def", "get_configspace", "(", "self", ")", ":", "\n", "        ", "cs", "=", "CS", ".", "ConfigurationSpace", "(", ")", "\n", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'ql_alpha'", ",", "lower", "=", "0.001", ",", "upper", "=", "1", ",", "log", "=", "True", ",", "default_value", "=", "0.1", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'ql_gamma'", ",", "lower", "=", "0.001", ",", "upper", "=", "1", ",", "log", "=", "True", ",", "default_value", "=", "0.1", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'ql_eps_init'", ",", "lower", "=", "0.01", ",", "upper", "=", "1", ",", "log", "=", "True", ",", "default_value", "=", "0.01", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'ql_eps_min'", ",", "lower", "=", "0.01", ",", "upper", "=", "1", ",", "log", "=", "True", ",", "default_value", "=", "0.01", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'ql_eps_decay'", ",", "lower", "=", "0.001", ",", "upper", "=", "1", ",", "log", "=", "True", ",", "default_value", "=", "0.01", ")", ")", "\n", "\n", "return", "cs", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.gridworld_ql_params_bohb.ExperimentWrapper.get_specific_config": [[36, 50], ["copy.deepcopy"], "methods", ["None"], ["", "def", "get_specific_config", "(", "self", ",", "cso", ",", "default_config", ",", "budget", ")", ":", "\n", "        ", "config", "=", "deepcopy", "(", "default_config", ")", "\n", "\n", "config", "[", "\"agents\"", "]", "[", "'ql'", "]", "[", "'alpha'", "]", "=", "1", "-", "cso", "[", "\"ql_alpha\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'ql'", "]", "[", "'gamma'", "]", "=", "1", "-", "cso", "[", "\"ql_gamma\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'ql'", "]", "[", "'eps_init'", "]", "=", "cso", "[", "\"ql_eps_init\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'ql'", "]", "[", "'eps_min'", "]", "=", "cso", "[", "\"ql_eps_min\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'ql'", "]", "[", "'eps_decay'", "]", "=", "1", "-", "cso", "[", "\"ql_eps_decay\"", "]", "\n", "\n", "config", "[", "\"agents\"", "]", "[", "'ql'", "]", "[", "'test_episodes'", "]", "=", "1", "\n", "\n", "config", "[", "\"device\"", "]", "=", "'cuda'", "\n", "\n", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.gridworld_ql_params_bohb.ExperimentWrapper.compute": [[51, 87], ["gridworld_ql_params_bohb.ExperimentWrapper.get_specific_config", "print", "print", "print", "print", "print", "print", "envs.env_factory.EnvFactory", "envs.env_factory.EnvFactory.generate_real_env", "range", "str", "print", "print", "print", "print", "open", "yaml.safe_load", "agents.QL.QL", "agents.QL.QL.train", "len", "str", "str", "str", "str"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_step_size.ExperimentWrapper.get_specific_config", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_real_env", "home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.train"], ["", "def", "compute", "(", "self", ",", "working_dir", ",", "bohb_id", ",", "config_id", ",", "cso", ",", "budget", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "with", "open", "(", "\"default_config_gridworld.yaml\"", ",", "'r'", ")", "as", "stream", ":", "\n", "            ", "default_config", "=", "yaml", ".", "safe_load", "(", "stream", ")", "\n", "\n", "", "config", "=", "self", ".", "get_specific_config", "(", "cso", ",", "default_config", ",", "budget", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "\"START BOHB ITERATION\"", ")", "\n", "print", "(", "'CONFIG: '", "+", "str", "(", "config", ")", ")", "\n", "print", "(", "'CSO:    '", "+", "str", "(", "cso", ")", ")", "\n", "print", "(", "'BUDGET: '", "+", "str", "(", "budget", ")", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "info", "=", "{", "}", "\n", "\n", "# generate environment", "\n", "env_fac", "=", "EnvFactory", "(", "config", ")", "\n", "real_env", "=", "env_fac", ".", "generate_real_env", "(", ")", "\n", "\n", "score", "=", "0", "\n", "for", "i", "in", "range", "(", "NUM_EVALS", ")", ":", "\n", "            ", "ql", "=", "QL", "(", "env", "=", "real_env", ",", "config", "=", "config", ")", "\n", "rewards", ",", "_", ",", "_", "=", "ql", ".", "train", "(", "env", "=", "real_env", ",", "test_env", "=", "real_env", ")", "\n", "score", "+=", "len", "(", "rewards", ")", "\n", "\n", "", "score", "=", "score", "/", "NUM_EVALS", "\n", "\n", "info", "[", "'config'", "]", "=", "str", "(", "config", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "'FINAL SCORE: '", "+", "str", "(", "score", ")", ")", "\n", "print", "(", "\"END BOHB ITERATION\"", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "return", "{", "\n", "\"loss\"", ":", "score", ",", "\n", "\"info\"", ":", "info", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.bohb_params_QL_CB.ExperimentWrapper.get_bohb_parameters": [[19, 28], ["None"], "methods", ["None"], ["    ", "def", "get_bohb_parameters", "(", "self", ")", ":", "\n", "        ", "params", "=", "{", "}", "\n", "params", "[", "'min_budget'", "]", "=", "1", "\n", "params", "[", "'max_budget'", "]", "=", "2", "\n", "params", "[", "'eta'", "]", "=", "2", "\n", "params", "[", "'iterations'", "]", "=", "10000", "\n", "params", "[", "'random_fraction'", "]", "=", "0.3", "\n", "\n", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.bohb_params_QL_CB.ExperimentWrapper.get_configspace": [[29, 35], ["ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter"], "methods", ["None"], ["", "def", "get_configspace", "(", "self", ")", ":", "\n", "        ", "cs", "=", "CS", ".", "ConfigurationSpace", "(", ")", "\n", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'beta'", ",", "lower", "=", "0.0001", ",", "upper", "=", "2", ",", "log", "=", "True", ",", "default_value", "=", "0.1", ")", ")", "\n", "\n", "return", "cs", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.bohb_params_QL_CB.ExperimentWrapper.get_specific_config": [[36, 42], ["copy.deepcopy"], "methods", ["None"], ["", "def", "get_specific_config", "(", "self", ",", "cso", ",", "default_config", ",", "budget", ")", ":", "\n", "        ", "config", "=", "deepcopy", "(", "default_config", ")", "\n", "\n", "config", "[", "\"agents\"", "]", "[", "\"ql\"", "]", "[", "\"beta\"", "]", "=", "cso", "[", "\"beta\"", "]", "\n", "\n", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.bohb_params_QL_CB.ExperimentWrapper.compute": [[43, 77], ["bohb_params_QL_CB.ExperimentWrapper.get_specific_config", "print", "print", "print", "print", "print", "print", "envs.env_factory.EnvFactory", "envs.env_factory.EnvFactory.generate_real_env", "agents.QL.QL", "agents.QL.QL.train", "len", "str", "print", "print", "print", "print", "open", "yaml.safe_load", "str", "str", "str", "str"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_step_size.ExperimentWrapper.get_specific_config", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_real_env", "home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.train"], ["", "def", "compute", "(", "self", ",", "working_dir", ",", "bohb_id", ",", "config_id", ",", "cso", ",", "budget", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "with", "open", "(", "\"default_config_gridworld.yaml\"", ",", "'r'", ")", "as", "stream", ":", "\n", "            ", "default_config", "=", "yaml", ".", "safe_load", "(", "stream", ")", "\n", "\n", "", "config", "=", "self", ".", "get_specific_config", "(", "cso", ",", "default_config", ",", "budget", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "\"START BOHB ITERATION\"", ")", "\n", "print", "(", "'CONFIG: '", "+", "str", "(", "config", ")", ")", "\n", "print", "(", "'CSO:    '", "+", "str", "(", "cso", ")", ")", "\n", "print", "(", "'BUDGET: '", "+", "str", "(", "budget", ")", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "info", "=", "{", "}", "\n", "\n", "# generate environment", "\n", "env_fac", "=", "EnvFactory", "(", "config", ")", "\n", "env", "=", "env_fac", ".", "generate_real_env", "(", ")", "\n", "\n", "ql", "=", "QL", "(", "env", "=", "env", ",", "\n", "config", "=", "config", ",", "\n", "count_based", "=", "True", ")", "\n", "rewards", ",", "_", ",", "_", "=", "ql", ".", "train", "(", "env", ")", "\n", "score", "=", "len", "(", "rewards", ")", "\n", "\n", "info", "[", "'config'", "]", "=", "str", "(", "config", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "'FINAL SCORE: '", "+", "str", "(", "score", ")", ")", "\n", "print", "(", "\"END BOHB ITERATION\"", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "return", "{", "\n", "\"loss\"", ":", "score", ",", "\n", "\"info\"", ":", "info", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole.ExperimentWrapper.get_bohb_parameters": [[17, 26], ["None"], "methods", ["None"], ["    ", "def", "get_bohb_parameters", "(", "self", ")", ":", "\n", "        ", "params", "=", "{", "}", "\n", "params", "[", "'min_budget'", "]", "=", "1", "\n", "params", "[", "'max_budget'", "]", "=", "1", "\n", "params", "[", "'eta'", "]", "=", "2", "\n", "params", "[", "'random_fraction'", "]", "=", "1", "\n", "params", "[", "'iterations'", "]", "=", "10000", "\n", "\n", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole.ExperimentWrapper.get_configspace": [[27, 31], ["ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace"], "methods", ["None"], ["", "def", "get_configspace", "(", "self", ")", ":", "\n", "        ", "cs", "=", "CS", ".", "ConfigurationSpace", "(", ")", "\n", "\n", "return", "cs", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole.ExperimentWrapper.get_specific_config": [[32, 35], ["copy.deepcopy"], "methods", ["None"], ["", "def", "get_specific_config", "(", "self", ",", "cso", ",", "default_config", ",", "budget", ")", ":", "\n", "        ", "config", "=", "deepcopy", "(", "default_config", ")", "\n", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole.ExperimentWrapper.compute": [[36, 73], ["GTNC_evaluate_cartpole.ExperimentWrapper.get_specific_config", "print", "print", "print", "print", "print", "print", "str", "str", "print", "print", "print", "print", "print", "open", "yaml.safe_load", "agents.GTN.GTN_Master", "agents.GTN.GTN_Master.run", "len", "str", "str", "str", "float", "traceback.format_exc", "print", "str", "str"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_step_size.ExperimentWrapper.get_specific_config", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.run"], ["", "def", "compute", "(", "self", ",", "working_dir", ",", "bohb_id", ",", "config_id", ",", "cso", ",", "budget", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "with", "open", "(", "\"default_config_cartpole_syn_env.yaml\"", ",", "'r'", ")", "as", "stream", ":", "\n", "            ", "default_config", "=", "yaml", ".", "safe_load", "(", "stream", ")", "\n", "\n", "", "config", "=", "self", ".", "get_specific_config", "(", "cso", ",", "default_config", ",", "budget", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "\"START BOHB ITERATION\"", ")", "\n", "print", "(", "'CONFIG: '", "+", "str", "(", "config", ")", ")", "\n", "print", "(", "'CSO:    '", "+", "str", "(", "cso", ")", ")", "\n", "print", "(", "'BUDGET: '", "+", "str", "(", "budget", ")", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "try", ":", "\n", "            ", "gtn", "=", "GTN_Master", "(", "config", ",", "bohb_id", "=", "bohb_id", ",", "bohb_working_dir", "=", "working_dir", ")", "\n", "_", ",", "score_list", "=", "gtn", ".", "run", "(", ")", "\n", "score", "=", "len", "(", "score_list", ")", "\n", "error", "=", "\"\"", "\n", "", "except", ":", "\n", "            ", "score", "=", "float", "(", "'Inf'", ")", "\n", "score_list", "=", "[", "]", "\n", "error", "=", "traceback", ".", "format_exc", "(", ")", "\n", "print", "(", "error", ")", "\n", "\n", "", "info", "=", "{", "}", "\n", "info", "[", "'error'", "]", "=", "str", "(", "error", ")", "\n", "info", "[", "'score_list'", "]", "=", "str", "(", "score_list", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "'FINAL SCORE: '", "+", "str", "(", "score", ")", ")", "\n", "print", "(", "'SCORE LIST:  '", "+", "str", "(", "score_list", ")", ")", "\n", "print", "(", "\"END BOHB ITERATION\"", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "return", "{", "\n", "\"loss\"", ":", "score", ",", "\n", "\"info\"", ":", "info", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cartpole_transfer_vary_hp.auc": [[10, 19], ["None"], "function", ["None"], ["def", "auc", "(", ")", ":", "\n", "    ", "return", "[", "\n", "'../results/3_rn_auc/cartpole_compare_reward_envs/best_transfer_vary_hp1.pt'", ",", "\n", "'../results/3_rn_auc/cartpole_compare_reward_envs/best_transfer_vary_hp2.pt'", ",", "\n", "'../results/3_rn_auc/cartpole_compare_reward_envs/best_transfer_vary_hp5.pt'", ",", "\n", "'../results/3_rn_auc/cartpole_compare_reward_envs/best_transfer_vary_hp6.pt'", ",", "\n", "'../results/0_before_auc/cartpole_compare_reward_envs/best_transfer_vary_hp0.pt'", ",", "\n", "# '../results/cartpole_compare_reward_envs/best_transfer_vary_hp-1.pt',", "\n", "'../results/0_before_auc/cartpole_compare_reward_envs/best_transfer_vary_hp-1_icm_opt.pt'", "# optimizing ICM HPs did not help", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cartpole_transfer_vary_hp.normal": [[22, 31], ["None"], "function", ["None"], ["", "def", "normal", "(", ")", ":", "\n", "    ", "return", "[", "\n", "'../results/0_before_auc/cartpole_compare_reward_envs/best_transfer_vary_hp1.pt'", ",", "\n", "'../results/0_before_auc/cartpole_compare_reward_envs/best_transfer_vary_hp2.pt'", ",", "\n", "'../results/0_before_auc/cartpole_compare_reward_envs/best_transfer_vary_hp5.pt'", ",", "\n", "'../results/0_before_auc/cartpole_compare_reward_envs/best_transfer_vary_hp6.pt'", ",", "\n", "'../results/0_before_auc/cartpole_compare_reward_envs/best_transfer_vary_hp0.pt'", ",", "\n", "# '../results/0_before_auc/cartpole_compare_reward_envs/best_transfer_vary_hp-1.pt',", "\n", "'../results/0_before_auc/cartpole_compare_reward_envs/best_transfer_vary_hp-1_icm_opt.pt'", "# optimizing ICM HPs did not help", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cartpole_transfer_vary_hp.reward_maximization": [[34, 43], ["None"], "function", ["None"], ["", "def", "reward_maximization", "(", ")", ":", "\n", "    ", "return", "[", "\n", "'../results/4_rn_reward/cartpole_compare_reward_envs/best_transfer_vary_hp1.pt'", ",", "\n", "'../results/4_rn_reward/cartpole_compare_reward_envs/best_transfer_vary_hp2.pt'", ",", "\n", "'../results/4_rn_reward/cartpole_compare_reward_envs/best_transfer_vary_hp5.pt'", ",", "\n", "'../results/4_rn_reward/cartpole_compare_reward_envs/best_transfer_vary_hp6.pt'", ",", "\n", "'../results/0_before_auc/cartpole_compare_reward_envs/best_transfer_vary_hp0.pt'", ",", "\n", "# '../results/0_before_auc/cartpole_compare_reward_envs/best_transfer_vary_hp-1.pt',", "\n", "'../results/0_before_auc/cartpole_compare_reward_envs/best_transfer_vary_hp-1_icm_opt.pt'", "# optimizing ICM HPs did not help", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cartpole_transfer_vary_hp.get_data": [[67, 117], ["float", "print", "print", "max", "torch.load", "list_data.append", "sums_eps_len_per_model.append", "sum", "sum", "numpy.zeros", "enumerate", "numpy.mean", "numpy.std", "proc_data.append", "print", "sums_eps_len.append", "sums_eps_len_per_model_i.append", "min", "zip", "range", "numpy.array", "sum", "sum", "sum", "sum", "numpy.asarray", "numpy.asarray", "len", "len", "concat_list.append"], "function", ["None"], ["def", "get_data", "(", ")", ":", "\n", "    ", "list_data", "=", "[", "]", "\n", "for", "log_file", "in", "LOG_FILES", ":", "\n", "        ", "data", "=", "torch", ".", "load", "(", "log_file", ")", "\n", "list_data", ".", "append", "(", "(", "data", "[", "'reward_list'", "]", ",", "data", "[", "'episode_length_list'", "]", ")", ")", "\n", "model_num", "=", "data", "[", "'model_num'", "]", "\n", "model_agents", "=", "data", "[", "'model_agents'", "]", "\n", "\n", "", "sums_eps_len", "=", "[", "]", "\n", "sums_eps_len_per_model", "=", "[", "]", "\n", "min_steps", "=", "float", "(", "'Inf'", ")", "\n", "# get minimum number of evaluations", "\n", "for", "reward_list", ",", "episode_length_list", "in", "list_data", ":", "\n", "        ", "sums_eps_len_per_model_i", "=", "[", "]", "\n", "for", "episode_lengths", "in", "episode_length_list", ":", "\n", "            ", "print", "(", "sum", "(", "episode_lengths", ")", ")", "\n", "sums_eps_len", ".", "append", "(", "sum", "(", "episode_lengths", ")", ")", "\n", "sums_eps_len_per_model_i", ".", "append", "(", "sum", "(", "episode_lengths", ")", ")", "\n", "min_steps", "=", "min", "(", "min_steps", ",", "sum", "(", "episode_lengths", ")", ")", "\n", "", "sums_eps_len_per_model", ".", "append", "(", "sums_eps_len_per_model_i", ")", "\n", "\n", "", "print", "(", "\"# of episode lens > MIN_STEPS: \"", ",", "sum", "(", "np", ".", "asarray", "(", "sums_eps_len", ")", ">=", "MIN_STEPS", ")", ")", "\n", "print", "(", "\"# of episode lens < MIN_STEPS: \"", ",", "sum", "(", "np", ".", "asarray", "(", "sums_eps_len", ")", "<", "MIN_STEPS", ")", ")", "\n", "min_steps", "=", "max", "(", "min_steps", ",", "MIN_STEPS", ")", "\n", "# convert data from episodes to steps", "\n", "proc_data", "=", "[", "]", "\n", "\n", "for", "reward_list", ",", "episode_length_list", "in", "list_data", ":", "\n", "        ", "np_data", "=", "np", ".", "zeros", "(", "[", "model_num", "*", "model_agents", ",", "min_steps", "]", ")", "\n", "\n", "for", "it", ",", "data", "in", "enumerate", "(", "zip", "(", "reward_list", ",", "episode_length_list", ")", ")", ":", "\n", "            ", "rewards", ",", "episode_lengths", "=", "data", "\n", "\n", "concat_list", "=", "[", "]", "\n", "rewards", "=", "rewards", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "episode_lengths", ")", ")", ":", "\n", "                ", "concat_list", "+=", "[", "rewards", "[", "i", "]", "]", "*", "episode_lengths", "[", "i", "]", "\n", "\n", "", "while", "len", "(", "concat_list", ")", "<", "min_steps", ":", "\n", "                ", "concat_list", ".", "append", "(", "concat_list", "[", "-", "1", "]", ")", "\n", "\n", "", "np_data", "[", "it", "]", "=", "np", ".", "array", "(", "concat_list", "[", ":", "min_steps", "]", ")", "\n", "\n", "", "mean", "=", "np", ".", "mean", "(", "np_data", ",", "axis", "=", "0", ")", "\n", "std", "=", "np", ".", "std", "(", "np_data", ",", "axis", "=", "0", ")", "\n", "\n", "proc_data", ".", "append", "(", "(", "mean", ",", "std", ")", ")", "\n", "\n", "", "return", "proc_data", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cartpole_transfer_vary_hp.plot_data": [[119, 143], ["matplotlib.subplots", "matplotlib.legend", "matplotlib.subplots_adjust", "matplotlib.title", "matplotlib.xlabel", "matplotlib.xlim", "matplotlib.ylim", "matplotlib.ylabel", "os.path.dirname", "matplotlib.savefig", "matplotlib.show", "matplotlib.plot", "matplotlib.fill_between", "legobj.set_linewidth", "os.path.join", "range", "len"], "function", ["None"], ["", "def", "plot_data", "(", "proc_data", ",", "savefig_name", ")", ":", "\n", "    ", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", "dpi", "=", "600", ",", "figsize", "=", "(", "5", ",", "4", ")", ")", "\n", "\n", "for", "mean", ",", "std", "in", "proc_data", ":", "\n", "        ", "plt", ".", "plot", "(", "mean", ",", "linewidth", "=", "1", ")", "\n", "\n", "", "for", "mean", ",", "std", "in", "proc_data", ":", "\n", "        ", "plt", ".", "fill_between", "(", "x", "=", "range", "(", "len", "(", "mean", ")", ")", ",", "y1", "=", "mean", "-", "std", "*", "STD_MULT", ",", "y2", "=", "mean", "+", "std", "*", "STD_MULT", ",", "alpha", "=", "0.3", ")", "\n", "\n", "", "leg", "=", "plt", ".", "legend", "(", "LEGEND", ",", "fontsize", "=", "9", ")", "\n", "\n", "for", "legobj", "in", "leg", ".", "legendHandles", ":", "\n", "        ", "legobj", ".", "set_linewidth", "(", "2.0", ")", "\n", "\n", "# plt.xlim(0,99)", "\n", "", "plt", ".", "subplots_adjust", "(", "bottom", "=", "0.15", ",", "left", "=", "0.15", ")", "\n", "plt", ".", "title", "(", "'CartPole-v0 Varied Hyperparameters'", ")", "\n", "plt", ".", "xlabel", "(", "'steps'", ")", "\n", "plt", ".", "xlim", "(", "0", ",", "MIN_STEPS", ")", "\n", "plt", ".", "ylim", "(", "0", ",", "210", ")", "\n", "plt", ".", "ylabel", "(", "'cumulative reward'", ")", "\n", "base_dir", "=", "os", ".", "path", ".", "dirname", "(", "LOG_FILES", "[", "0", "]", ")", "\n", "plt", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "base_dir", ",", "savefig_name", ")", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.bohb_params_TD3_halfcheetah_after.ExperimentWrapper.get_bohb_parameters": [[33, 42], ["None"], "methods", ["None"], ["    ", "def", "get_bohb_parameters", "(", "self", ")", ":", "\n", "        ", "params", "=", "{", "}", "\n", "params", "[", "'min_budget'", "]", "=", "1", "\n", "params", "[", "'max_budget'", "]", "=", "8", "\n", "params", "[", "'eta'", "]", "=", "2", "\n", "params", "[", "'iterations'", "]", "=", "1000", "\n", "params", "[", "'random_fraction'", "]", "=", "0.3", "\n", "\n", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.bohb_params_TD3_halfcheetah_after.ExperimentWrapper.get_configspace": [[43, 61], ["ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter"], "methods", ["None"], ["", "def", "get_configspace", "(", "self", ")", ":", "\n", "        ", "cs", "=", "CS", ".", "ConfigurationSpace", "(", ")", "\n", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'td3_batch_size'", ",", "lower", "=", "64", ",", "upper", "=", "256", ",", "log", "=", "False", ",", "default_value", "=", "128", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_gamma'", ",", "lower", "=", "0.001", ",", "upper", "=", "0.1", ",", "log", "=", "True", ",", "default_value", "=", "0.01", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_lr'", ",", "lower", "=", "1e-4", ",", "upper", "=", "5e-3", ",", "log", "=", "True", ",", "default_value", "=", "1e-3", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_tau'", ",", "lower", "=", "0.005", ",", "upper", "=", "0.05", ",", "log", "=", "True", ",", "default_value", "=", "0.01", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'td3_policy_delay'", ",", "lower", "=", "1", ",", "upper", "=", "3", ",", "log", "=", "False", ",", "default_value", "=", "2", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "CategoricalHyperparameter", "(", "name", "=", "'td3_activation_fn'", ",", "choices", "=", "[", "'tanh'", ",", "'relu'", ",", "'leakyrelu'", ",", "'prelu'", "]", ",", "default_value", "=", "'relu'", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'td3_hidden_size'", ",", "lower", "=", "48", ",", "upper", "=", "192", ",", "log", "=", "True", ",", "default_value", "=", "128", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'td3_hidden_layer'", ",", "lower", "=", "1", ",", "upper", "=", "2", ",", "log", "=", "False", ",", "default_value", "=", "2", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'td3_same_action_num'", ",", "lower", "=", "1", ",", "upper", "=", "3", ",", "log", "=", "False", ",", "default_value", "=", "1", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_action_std'", ",", "lower", "=", "0.05", ",", "upper", "=", "0.2", ",", "log", "=", "True", ",", "default_value", "=", "0.1", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_policy_std'", ",", "lower", "=", "0.1", ",", "upper", "=", "0.4", ",", "log", "=", "True", ",", "default_value", "=", "0.2", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_policy_std_clip'", ",", "lower", "=", "0.25", ",", "upper", "=", "1", ",", "log", "=", "True", ",", "default_value", "=", "0.5", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_early_out_virtual_diff'", ",", "lower", "=", "1e-2", ",", "upper", "=", "1e-1", ",", "log", "=", "True", ",", "default_value", "=", "3e-2", ")", ")", "\n", "\n", "return", "cs", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.bohb_params_TD3_halfcheetah_after.ExperimentWrapper.get_specific_config": [[62, 81], ["copy.deepcopy"], "methods", ["None"], ["", "def", "get_specific_config", "(", "self", ",", "cso", ",", "default_config", ",", "budget", ")", ":", "\n", "        ", "config", "=", "deepcopy", "(", "default_config", ")", "\n", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'batch_size'", "]", "=", "cso", "[", "\"td3_batch_size\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'gamma'", "]", "=", "1", "-", "cso", "[", "\"td3_gamma\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'lr'", "]", "=", "cso", "[", "\"td3_lr\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'tau'", "]", "=", "cso", "[", "\"td3_tau\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'policy_delay'", "]", "=", "cso", "[", "\"td3_policy_delay\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'activation_fn'", "]", "=", "cso", "[", "\"td3_activation_fn\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'hidden_size'", "]", "=", "cso", "[", "\"td3_hidden_size\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'hidden_layer'", "]", "=", "cso", "[", "\"td3_hidden_layer\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'same_action_num'", "]", "=", "cso", "[", "\"td3_same_action_num\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'action_std'", "]", "=", "cso", "[", "\"td3_action_std\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'policy_std'", "]", "=", "cso", "[", "\"td3_policy_std\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'policy_std_clip'", "]", "=", "cso", "[", "\"td3_policy_std_clip\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'early_out_virtual_diff'", "]", "=", "cso", "[", "\"td3_early_out_virtual_diff\"", "]", "\n", "# config[\"device\"] = 'cuda'", "\n", "\n", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.bohb_params_TD3_halfcheetah_after.ExperimentWrapper.compute": [[82, 129], ["bohb_params_TD3_halfcheetah_after.ExperimentWrapper.get_specific_config", "print", "print", "print", "print", "print", "print", "envs.env_factory.EnvFactory", "envs.env_factory.EnvFactory.generate_real_env", "envs.env_factory.EnvFactory.generate_reward_env", "torch.load", "envs.env_factory.EnvFactory.generate_reward_env.load_state_dict", "range", "str", "print", "print", "print", "print", "open", "yaml.safe_load", "agents.TD3.TD3", "agents.TD3.TD3.train", "agents.TD3.TD3.test", "statistics.mean", "str", "str", "str", "len", "str", "envs.env_factory.EnvFactory.generate_reward_env.get_max_action", "max", "envs.env_factory.EnvFactory.generate_real_env.get_solved_reward"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_step_size.ExperimentWrapper.get_specific_config", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_real_env", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_reward_env", "home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.train", "home.repos.pwc.inspect_result.automl_learning_environments.agents.base_agent.BaseAgent.test", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.get_max_action", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.get_solved_reward"], ["", "def", "compute", "(", "self", ",", "working_dir", ",", "bohb_id", ",", "config_id", ",", "cso", ",", "budget", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "with", "open", "(", "CONFIG_FILE", ",", "'r'", ")", "as", "stream", ":", "\n", "            ", "default_config", "=", "yaml", ".", "safe_load", "(", "stream", ")", "\n", "\n", "", "config", "=", "self", ".", "get_specific_config", "(", "cso", ",", "default_config", ",", "budget", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "\"START BOHB ITERATION\"", ")", "\n", "print", "(", "'CONFIG: '", "+", "str", "(", "config", ")", ")", "\n", "print", "(", "'CSO:    '", "+", "str", "(", "cso", ")", ")", "\n", "print", "(", "'BUDGET: '", "+", "str", "(", "budget", ")", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "info", "=", "{", "}", "\n", "\n", "# generate environment", "\n", "env_fac", "=", "EnvFactory", "(", "config", ")", "\n", "\n", "real_env", "=", "env_fac", ".", "generate_real_env", "(", ")", "\n", "reward_env", "=", "env_fac", ".", "generate_reward_env", "(", ")", "\n", "save_dict", "=", "torch", ".", "load", "(", "SAVE_FILE", ")", "\n", "# config = save_dict['config']", "\n", "reward_env", ".", "load_state_dict", "(", "save_dict", "[", "'model'", "]", ")", "\n", "\n", "score", "=", "0", "\n", "for", "i", "in", "range", "(", "NUM_EVALS", ")", ":", "\n", "            ", "td3", "=", "TD3", "(", "env", "=", "reward_env", ",", "\n", "max_action", "=", "reward_env", ".", "get_max_action", "(", ")", ",", "\n", "config", "=", "config", ")", "\n", "reward_list_train", ",", "_", ",", "_", "=", "td3", ".", "train", "(", "reward_env", ",", "test_env", "=", "real_env", ")", "\n", "reward_list_test", ",", "_", ",", "_", "=", "td3", ".", "test", "(", "real_env", ")", "\n", "avg_reward_test", "=", "statistics", ".", "mean", "(", "reward_list_test", ")", "\n", "\n", "unsolved_weight", "=", "config", "[", "\"agents\"", "]", "[", "\"gtn\"", "]", "[", "\"unsolved_weight\"", "]", "\n", "score", "+=", "len", "(", "reward_list_train", ")", "+", "max", "(", "0", ",", "(", "real_env", ".", "get_solved_reward", "(", ")", "-", "avg_reward_test", ")", ")", "*", "unsolved_weight", "\n", "\n", "", "score", "=", "score", "/", "NUM_EVALS", "\n", "\n", "info", "[", "'config'", "]", "=", "str", "(", "config", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "'FINAL SCORE: '", "+", "str", "(", "score", ")", ")", "\n", "print", "(", "\"END BOHB ITERATION\"", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "return", "{", "\n", "\"loss\"", ":", "score", ",", "\n", "\"info\"", ":", "info", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cartpole_compare_reward_envs.auc": [[10, 19], ["None"], "function", ["None"], ["def", "auc", "(", ")", ":", "\n", "    ", "return", "[", "\n", "'../results/3_rn_auc/cartpole_compare_reward_envs/best1.pt'", ",", "\n", "'../results/3_rn_auc/cartpole_compare_reward_envs/best2.pt'", ",", "\n", "'../results/3_rn_auc/cartpole_compare_reward_envs/best5.pt'", ",", "\n", "'../results/3_rn_auc/cartpole_compare_reward_envs/best6.pt'", ",", "\n", "'../results/0_before_auc/cartpole_compare_reward_envs/best0.pt'", ",", "\n", "# '../results/0_before_auc/cartpole_compare_reward_envs/best-1.pt',", "\n", "'../results/0_before_auc/cartpole_compare_reward_envs/best-1_icm_opt.pt'", "# optimizing ICM HPs did not help", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cartpole_compare_reward_envs.normal": [[22, 31], ["None"], "function", ["None"], ["", "def", "normal", "(", ")", ":", "\n", "    ", "return", "[", "\n", "'../results/0_before_auc/cartpole_compare_reward_envs/best1.pt'", ",", "\n", "'../results/0_before_auc/cartpole_compare_reward_envs/best2.pt'", ",", "\n", "'../results/0_before_auc/cartpole_compare_reward_envs/best5.pt'", ",", "\n", "'../results/0_before_auc/cartpole_compare_reward_envs/best6.pt'", ",", "\n", "'../results/0_before_auc/cartpole_compare_reward_envs/best0.pt'", ",", "\n", "# '../results/0_before_auc/cartpole_compare_reward_envs/best-1.pt',", "\n", "'../results/0_before_auc/cartpole_compare_reward_envs/best-1_icm_opt.pt'", "# optimizing ICM HPs did not help", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cartpole_compare_reward_envs.reward_maximization": [[34, 43], ["None"], "function", ["None"], ["", "def", "reward_maximization", "(", ")", ":", "\n", "    ", "return", "[", "\n", "'../results/4_rn_reward/cartpole_compare_reward_envs/best1.pt'", ",", "\n", "'../results/4_rn_reward/cartpole_compare_reward_envs/best2.pt'", ",", "\n", "'../results/4_rn_reward/cartpole_compare_reward_envs/best5.pt'", ",", "\n", "'../results/4_rn_reward/cartpole_compare_reward_envs/best6.pt'", ",", "\n", "'../results/0_before_auc/cartpole_compare_reward_envs/best0.pt'", ",", "\n", "# '../results/0_before_auc/cartpole_compare_reward_envs/best-1.pt',", "\n", "'../results/0_before_auc/cartpole_compare_reward_envs/best-1_icm_opt.pt'", "# optimizing ICM HPs did not help", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cartpole_compare_reward_envs.get_data": [[67, 117], ["float", "print", "print", "max", "torch.load", "list_data.append", "sums_eps_len_per_model.append", "sum", "sum", "numpy.zeros", "enumerate", "numpy.mean", "numpy.std", "proc_data.append", "print", "sums_eps_len.append", "sums_eps_len_per_model_i.append", "min", "zip", "range", "numpy.array", "sum", "sum", "sum", "sum", "numpy.asarray", "numpy.asarray", "len", "len", "concat_list.append"], "function", ["None"], ["def", "get_data", "(", ")", ":", "\n", "    ", "list_data", "=", "[", "]", "\n", "for", "log_file", "in", "LOG_FILES", ":", "\n", "        ", "data", "=", "torch", ".", "load", "(", "log_file", ")", "\n", "list_data", ".", "append", "(", "(", "data", "[", "'reward_list'", "]", ",", "data", "[", "'episode_length_list'", "]", ")", ")", "\n", "model_num", "=", "data", "[", "'model_num'", "]", "\n", "model_agents", "=", "data", "[", "'model_agents'", "]", "\n", "\n", "", "sums_eps_len", "=", "[", "]", "\n", "sums_eps_len_per_model", "=", "[", "]", "\n", "min_steps", "=", "float", "(", "'Inf'", ")", "\n", "# get minimum number of evaluations", "\n", "for", "reward_list", ",", "episode_length_list", "in", "list_data", ":", "\n", "        ", "sums_eps_len_per_model_i", "=", "[", "]", "\n", "for", "episode_lengths", "in", "episode_length_list", ":", "\n", "            ", "print", "(", "sum", "(", "episode_lengths", ")", ")", "\n", "sums_eps_len", ".", "append", "(", "sum", "(", "episode_lengths", ")", ")", "\n", "sums_eps_len_per_model_i", ".", "append", "(", "sum", "(", "episode_lengths", ")", ")", "\n", "min_steps", "=", "min", "(", "min_steps", ",", "sum", "(", "episode_lengths", ")", ")", "\n", "", "sums_eps_len_per_model", ".", "append", "(", "sums_eps_len_per_model_i", ")", "\n", "\n", "", "print", "(", "\"# of episode lens > MIN_STEPS: \"", ",", "sum", "(", "np", ".", "asarray", "(", "sums_eps_len", ")", ">=", "MIN_STEPS", ")", ")", "\n", "print", "(", "\"# of episode lens < MIN_STEPS: \"", ",", "sum", "(", "np", ".", "asarray", "(", "sums_eps_len", ")", "<", "MIN_STEPS", ")", ")", "\n", "min_steps", "=", "max", "(", "min_steps", ",", "MIN_STEPS", ")", "\n", "# convert data from episodes to steps", "\n", "proc_data", "=", "[", "]", "\n", "\n", "for", "reward_list", ",", "episode_length_list", "in", "list_data", ":", "\n", "        ", "np_data", "=", "np", ".", "zeros", "(", "[", "model_num", "*", "model_agents", ",", "min_steps", "]", ")", "\n", "\n", "for", "it", ",", "data", "in", "enumerate", "(", "zip", "(", "reward_list", ",", "episode_length_list", ")", ")", ":", "\n", "            ", "rewards", ",", "episode_lengths", "=", "data", "\n", "\n", "concat_list", "=", "[", "]", "\n", "rewards", "=", "rewards", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "episode_lengths", ")", ")", ":", "\n", "                ", "concat_list", "+=", "[", "rewards", "[", "i", "]", "]", "*", "episode_lengths", "[", "i", "]", "\n", "\n", "", "while", "len", "(", "concat_list", ")", "<", "min_steps", ":", "\n", "                ", "concat_list", ".", "append", "(", "concat_list", "[", "-", "1", "]", ")", "\n", "\n", "", "np_data", "[", "it", "]", "=", "np", ".", "array", "(", "concat_list", "[", ":", "min_steps", "]", ")", "\n", "\n", "", "mean", "=", "np", ".", "mean", "(", "np_data", ",", "axis", "=", "0", ")", "\n", "std", "=", "np", ".", "std", "(", "np_data", ",", "axis", "=", "0", ")", "\n", "\n", "proc_data", ".", "append", "(", "(", "mean", ",", "std", ")", ")", "\n", "\n", "", "return", "proc_data", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cartpole_compare_reward_envs.plot_data": [[119, 142], ["matplotlib.subplots", "matplotlib.legend", "matplotlib.subplots_adjust", "matplotlib.title", "matplotlib.xlabel", "matplotlib.xlim", "matplotlib.ylim", "matplotlib.ylabel", "os.path.dirname", "matplotlib.savefig", "matplotlib.show", "matplotlib.plot", "matplotlib.fill_between", "legobj.set_linewidth", "os.path.join", "range", "len"], "function", ["None"], ["", "def", "plot_data", "(", "proc_data", ",", "savefig_name", ")", ":", "\n", "    ", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", "dpi", "=", "600", ",", "figsize", "=", "(", "5", ",", "4", ")", ")", "\n", "\n", "for", "mean", ",", "std", "in", "proc_data", ":", "\n", "        ", "plt", ".", "plot", "(", "mean", ",", "linewidth", "=", "1", ")", "\n", "\n", "", "for", "mean", ",", "std", "in", "proc_data", ":", "\n", "        ", "plt", ".", "fill_between", "(", "x", "=", "range", "(", "len", "(", "mean", ")", ")", ",", "y1", "=", "mean", "-", "std", "*", "STD_MULT", ",", "y2", "=", "mean", "+", "std", "*", "STD_MULT", ",", "alpha", "=", "0.3", ")", "\n", "\n", "", "leg", "=", "plt", ".", "legend", "(", "LEGEND", ",", "fontsize", "=", "9", ")", "\n", "\n", "for", "legobj", "in", "leg", ".", "legendHandles", ":", "\n", "        ", "legobj", ".", "set_linewidth", "(", "2.0", ")", "\n", "\n", "", "plt", ".", "subplots_adjust", "(", "bottom", "=", "0.15", ",", "left", "=", "0.15", ")", "\n", "plt", ".", "title", "(", "'CartPole-v0'", ")", "\n", "plt", ".", "xlabel", "(", "'steps'", ")", "\n", "plt", ".", "xlim", "(", "0", ",", "MIN_STEPS", ")", "\n", "plt", ".", "ylim", "(", "0", ",", "210", ")", "\n", "plt", ".", "ylabel", "(", "'cumulative reward'", ")", "\n", "base_dir", "=", "os", ".", "path", ".", "dirname", "(", "LOG_FILES", "[", "0", "]", ")", "\n", "plt", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "base_dir", ",", "savefig_name", ")", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.bohb_params_PPO_halfcheetah.ExperimentWrapper.get_bohb_parameters": [[16, 25], ["None"], "methods", ["None"], ["    ", "def", "get_bohb_parameters", "(", "self", ")", ":", "\n", "        ", "params", "=", "{", "}", "\n", "params", "[", "'min_budget'", "]", "=", "1", "\n", "params", "[", "'max_budget'", "]", "=", "2", "\n", "params", "[", "'eta'", "]", "=", "2", "\n", "params", "[", "'iterations'", "]", "=", "1000", "\n", "params", "[", "'random_fraction'", "]", "=", "0.3", "\n", "\n", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.bohb_params_PPO_halfcheetah.ExperimentWrapper.get_configspace": [[26, 44], ["ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter"], "methods", ["None"], ["", "def", "get_configspace", "(", "self", ")", ":", "\n", "        ", "cs", "=", "CS", ".", "ConfigurationSpace", "(", ")", "\n", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'update_episodes'", ",", "lower", "=", "1", ",", "upper", "=", "100", ",", "log", "=", "True", ",", "default_value", "=", "20", ")", ")", "\n", "# cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(name='ppo_epochs', lower=1, upper=100, log=True, default_value=10))", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'ppo_epochs'", ",", "lower", "=", "1", ",", "upper", "=", "80", ",", "log", "=", "True", ",", "default_value", "=", "10", ")", ")", "\n", "# 100 ppo epochs might lead to action NaNs", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'gamma'", ",", "lower", "=", "0.001", ",", "upper", "=", "0.1", ",", "log", "=", "True", ",", "default_value", "=", "0.01", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'lr'", ",", "lower", "=", "1e-5", ",", "upper", "=", "1e-2", ",", "log", "=", "True", ",", "default_value", "=", "3e-4", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'vf_coef'", ",", "lower", "=", "0.1", ",", "upper", "=", "2", ",", "log", "=", "True", ",", "default_value", "=", "1.0", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'ent_coef'", ",", "lower", "=", "0.001", ",", "upper", "=", "0.1", ",", "log", "=", "True", ",", "default_value", "=", "0.01", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'eps_clip'", ",", "lower", "=", "0.05", ",", "upper", "=", "1", ",", "log", "=", "True", ",", "default_value", "=", "0.2", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "CategoricalHyperparameter", "(", "name", "=", "'activation_fn'", ",", "choices", "=", "[", "'relu'", ",", "'tanh'", ",", "'leakyrelu'", ",", "'prelu'", "]", ",", "default_value", "=", "'relu'", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'hidden_size'", ",", "lower", "=", "48", ",", "upper", "=", "192", ",", "log", "=", "True", ",", "default_value", "=", "128", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'hidden_layer'", ",", "lower", "=", "1", ",", "upper", "=", "2", ",", "log", "=", "False", ",", "default_value", "=", "2", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'action_std'", ",", "lower", "=", "0.01", ",", "upper", "=", "2", ",", "log", "=", "True", ",", "default_value", "=", "0.2", ")", ")", "\n", "\n", "return", "cs", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.bohb_params_PPO_halfcheetah.ExperimentWrapper.get_specific_config": [[45, 62], ["copy.deepcopy"], "methods", ["None"], ["", "def", "get_specific_config", "(", "self", ",", "cso", ",", "default_config", ",", "budget", ")", ":", "\n", "        ", "config", "=", "deepcopy", "(", "default_config", ")", "\n", "\n", "config", "[", "\"agents\"", "]", "[", "\"ppo\"", "]", "[", "\"update_episodes\"", "]", "=", "cso", "[", "\"update_episodes\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"ppo\"", "]", "[", "\"ppo_epochs\"", "]", "=", "cso", "[", "\"ppo_epochs\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"ppo\"", "]", "[", "\"gamma\"", "]", "=", "1", "-", "cso", "[", "\"gamma\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"ppo\"", "]", "[", "\"lr\"", "]", "=", "cso", "[", "\"lr\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"ppo\"", "]", "[", "\"vf_coef\"", "]", "=", "cso", "[", "\"vf_coef\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"ppo\"", "]", "[", "\"ent_coef\"", "]", "=", "cso", "[", "\"ent_coef\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"ppo\"", "]", "[", "\"eps_clip\"", "]", "=", "cso", "[", "\"eps_clip\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"ppo\"", "]", "[", "\"activation_fn\"", "]", "=", "cso", "[", "\"activation_fn\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"ppo\"", "]", "[", "\"hidden_size\"", "]", "=", "cso", "[", "\"hidden_size\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"ppo\"", "]", "[", "\"hidden_layer\"", "]", "=", "cso", "[", "\"hidden_layer\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"ppo\"", "]", "[", "\"action_std\"", "]", "=", "cso", "[", "\"action_std\"", "]", "\n", "config", "[", "\"device\"", "]", "=", "'cuda'", "\n", "\n", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.bohb_params_PPO_halfcheetah.ExperimentWrapper.compute": [[63, 104], ["bohb_params_PPO_halfcheetah.ExperimentWrapper.get_specific_config", "print", "print", "print", "print", "print", "print", "envs.env_factory.EnvFactory", "envs.env_factory.EnvFactory.generate_real_env", "range", "str", "print", "print", "print", "print", "open", "yaml.safe_load", "agents.PPO.PPO", "agents.PPO.PPO.train", "rewards_list.append", "numpy.mean", "str", "str", "str", "sum", "str"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_step_size.ExperimentWrapper.get_specific_config", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_real_env", "home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.train"], ["", "def", "compute", "(", "self", ",", "working_dir", ",", "bohb_id", ",", "config_id", ",", "cso", ",", "budget", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "with", "open", "(", "\"default_config_halfcheetah.yaml\"", ",", "'r'", ")", "as", "stream", ":", "\n", "            ", "default_config", "=", "yaml", ".", "safe_load", "(", "stream", ")", "\n", "\n", "", "config", "=", "self", ".", "get_specific_config", "(", "cso", ",", "default_config", ",", "budget", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "\"START BOHB ITERATION\"", ")", "\n", "print", "(", "'CONFIG: '", "+", "str", "(", "config", ")", ")", "\n", "print", "(", "'CSO:    '", "+", "str", "(", "cso", ")", ")", "\n", "print", "(", "'BUDGET: '", "+", "str", "(", "budget", ")", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "config", "[", "\"agents\"", "]", "[", "\"ppo\"", "]", "[", "\"train_episodes\"", "]", "=", "3000", "\n", "\n", "info", "=", "{", "}", "\n", "\n", "# generate environment", "\n", "env_fac", "=", "EnvFactory", "(", "config", ")", "\n", "real_env", "=", "env_fac", ".", "generate_real_env", "(", ")", "\n", "\n", "# score = 0", "\n", "rewards_list", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "NUM_EVALS", ")", ":", "\n", "            ", "ppo", "=", "PPO", "(", "env", "=", "real_env", ",", "\n", "config", "=", "config", ")", "\n", "rewards", ",", "_", ",", "_", "=", "ppo", ".", "train", "(", "real_env", ")", "\n", "rewards_list", ".", "append", "(", "sum", "(", "rewards", ")", ")", "\n", "# score += len(rewards)", "\n", "\n", "# score = score/NUM_EVALS", "\n", "", "score", "=", "-", "np", ".", "mean", "(", "rewards_list", ")", "\n", "info", "[", "'config'", "]", "=", "str", "(", "config", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "'FINAL SCORE: '", "+", "str", "(", "score", ")", ")", "\n", "print", "(", "\"END BOHB ITERATION\"", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "return", "{", "\n", "\"loss\"", ":", "score", ",", "\n", "\"info\"", ":", "info", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld_transfer_algo.auc": [[10, 18], ["None"], "function", ["None"], ["def", "auc", "(", ")", ":", "\n", "    ", "return", "[", "\n", "'../results/3_rn_auc/cliff_compare_reward_envs/best_transfer_algo1.pt'", ",", "\n", "'../results/3_rn_auc/cliff_compare_reward_envs/best_transfer_algo2.pt'", ",", "\n", "'../results/3_rn_auc/cliff_compare_reward_envs/best_transfer_algo5.pt'", ",", "\n", "'../results/3_rn_auc/cliff_compare_reward_envs/best_transfer_algo6.pt'", ",", "\n", "'../results/0_before_auc/cliff_compare_reward_envs/best_transfer_algo0.pt'", ",", "\n", "'../results/0_before_auc/cliff_compare_reward_envs/best_transfer_algo-1_opt.pt'", ",", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld_transfer_algo.normal": [[21, 29], ["None"], "function", ["None"], ["", "def", "normal", "(", ")", ":", "\n", "    ", "return", "[", "\n", "'../results/0_before_auc/cliff_compare_reward_envs/best_transfer_algo1.pt'", ",", "\n", "'../results/0_before_auc/cliff_compare_reward_envs/best_transfer_algo2.pt'", ",", "\n", "'../results/0_before_auc/cliff_compare_reward_envs/best_transfer_algo5.pt'", ",", "\n", "'../results/0_before_auc/cliff_compare_reward_envs/best_transfer_algo6.pt'", ",", "\n", "'../results/0_before_auc/cliff_compare_reward_envs/best_transfer_algo0.pt'", ",", "\n", "'../results/0_before_auc/cliff_compare_reward_envs/best_transfer_algo-1_opt.pt'", ",", "\n", "# '../results/0_before_auc/cliff_compare_reward_envs/best_transfer_algo-1.pt'", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld_transfer_algo.reward_maximization": [[33, 41], ["None"], "function", ["None"], ["", "def", "reward_maximization", "(", ")", ":", "\n", "    ", "return", "[", "\n", "'../results/4_rn_reward/cliff_compare_reward_envs/best_transfer_algo1.pt'", ",", "\n", "'../results/4_rn_reward/cliff_compare_reward_envs/best_transfer_algo2.pt'", ",", "\n", "'../results/4_rn_reward/cliff_compare_reward_envs/best_transfer_algo5.pt'", ",", "\n", "'../results/4_rn_reward/cliff_compare_reward_envs/best_transfer_algo6.pt'", ",", "\n", "'../results/0_before_auc/cliff_compare_reward_envs/best_transfer_algo0.pt'", ",", "\n", "'../results/0_before_auc/cliff_compare_reward_envs/best_transfer_algo-1_opt.pt'", ",", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld_transfer_algo.get_data": [[65, 115], ["float", "print", "print", "max", "torch.load", "list_data.append", "sums_eps_len_per_model.append", "sum", "sum", "numpy.zeros", "enumerate", "numpy.mean", "numpy.std", "proc_data.append", "print", "sums_eps_len.append", "sums_eps_len_per_model_i.append", "min", "zip", "range", "numpy.array", "sum", "sum", "sum", "sum", "numpy.asarray", "numpy.asarray", "len", "len", "concat_list.append"], "function", ["None"], ["def", "get_data", "(", ")", ":", "\n", "    ", "list_data", "=", "[", "]", "\n", "for", "log_file", "in", "LOG_FILES", ":", "\n", "        ", "data", "=", "torch", ".", "load", "(", "log_file", ")", "\n", "list_data", ".", "append", "(", "(", "data", "[", "'reward_list'", "]", ",", "data", "[", "'episode_length_list'", "]", ")", ")", "\n", "model_num", "=", "data", "[", "'model_num'", "]", "\n", "model_agents", "=", "data", "[", "'model_agents'", "]", "\n", "\n", "", "sums_eps_len", "=", "[", "]", "\n", "sums_eps_len_per_model", "=", "[", "]", "\n", "min_steps", "=", "float", "(", "'Inf'", ")", "\n", "# get minimum number of evaluations", "\n", "for", "reward_list", ",", "episode_length_list", "in", "list_data", ":", "\n", "        ", "sums_eps_len_per_model_i", "=", "[", "]", "\n", "for", "episode_lengths", "in", "episode_length_list", ":", "\n", "            ", "print", "(", "sum", "(", "episode_lengths", ")", ")", "\n", "sums_eps_len", ".", "append", "(", "sum", "(", "episode_lengths", ")", ")", "\n", "sums_eps_len_per_model_i", ".", "append", "(", "sum", "(", "episode_lengths", ")", ")", "\n", "min_steps", "=", "min", "(", "min_steps", ",", "sum", "(", "episode_lengths", ")", ")", "\n", "", "sums_eps_len_per_model", ".", "append", "(", "sums_eps_len_per_model_i", ")", "\n", "\n", "", "print", "(", "\"# of episode lens > MIN_STEPS: \"", ",", "sum", "(", "np", ".", "asarray", "(", "sums_eps_len", ")", ">=", "MIN_STEPS", ")", ")", "\n", "print", "(", "\"# of episode lens < MIN_STEPS: \"", ",", "sum", "(", "np", ".", "asarray", "(", "sums_eps_len", ")", "<", "MIN_STEPS", ")", ")", "\n", "min_steps", "=", "max", "(", "min_steps", ",", "MIN_STEPS", ")", "\n", "# convert data from episodes to steps", "\n", "proc_data", "=", "[", "]", "\n", "\n", "for", "reward_list", ",", "episode_length_list", "in", "list_data", ":", "\n", "        ", "np_data", "=", "np", ".", "zeros", "(", "[", "model_num", "*", "model_agents", ",", "min_steps", "]", ")", "\n", "\n", "for", "it", ",", "data", "in", "enumerate", "(", "zip", "(", "reward_list", ",", "episode_length_list", ")", ")", ":", "\n", "            ", "rewards", ",", "episode_lengths", "=", "data", "\n", "\n", "concat_list", "=", "[", "]", "\n", "rewards", "=", "rewards", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "episode_lengths", ")", ")", ":", "\n", "                ", "concat_list", "+=", "[", "rewards", "[", "i", "]", "]", "*", "episode_lengths", "[", "i", "]", "\n", "\n", "", "while", "len", "(", "concat_list", ")", "<", "min_steps", ":", "\n", "                ", "concat_list", ".", "append", "(", "concat_list", "[", "-", "1", "]", ")", "\n", "\n", "", "np_data", "[", "it", "]", "=", "np", ".", "array", "(", "concat_list", "[", ":", "min_steps", "]", ")", "\n", "\n", "", "mean", "=", "np", ".", "mean", "(", "np_data", ",", "axis", "=", "0", ")", "\n", "std", "=", "np", ".", "std", "(", "np_data", ",", "axis", "=", "0", ")", "\n", "\n", "proc_data", ".", "append", "(", "(", "mean", ",", "std", ")", ")", "\n", "\n", "", "return", "proc_data", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld_transfer_algo.plot_data": [[117, 141], ["matplotlib.subplots", "matplotlib.legend", "matplotlib.subplots_adjust", "matplotlib.title", "matplotlib.xlabel", "matplotlib.xlim", "matplotlib.ylim", "matplotlib.ylabel", "os.path.dirname", "matplotlib.savefig", "matplotlib.show", "matplotlib.plot", "matplotlib.fill_between", "legobj.set_linewidth", "os.path.join", "range", "len"], "function", ["None"], ["", "def", "plot_data", "(", "proc_data", ",", "savefig_name", ")", ":", "\n", "    ", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", "dpi", "=", "600", ",", "figsize", "=", "(", "5", ",", "4", ")", ")", "\n", "\n", "for", "mean", ",", "std", "in", "proc_data", ":", "\n", "        ", "plt", ".", "plot", "(", "mean", ",", "linewidth", "=", "1", ")", "\n", "\n", "", "for", "mean", ",", "std", "in", "proc_data", ":", "\n", "        ", "plt", ".", "fill_between", "(", "x", "=", "range", "(", "len", "(", "mean", ")", ")", ",", "y1", "=", "mean", "-", "std", "*", "STD_MULT", ",", "y2", "=", "mean", "+", "std", "*", "STD_MULT", ",", "alpha", "=", "0.3", ")", "\n", "\n", "", "leg", "=", "plt", ".", "legend", "(", "LEGEND", ",", "fontsize", "=", "9", ")", "\n", "\n", "for", "legobj", "in", "leg", ".", "legendHandles", ":", "\n", "        ", "legobj", ".", "set_linewidth", "(", "2.0", ")", "\n", "\n", "# plt.xlim(0,99)", "\n", "", "plt", ".", "subplots_adjust", "(", "bottom", "=", "0.15", ",", "left", "=", "0.15", ")", "\n", "plt", ".", "title", "(", "'Cliff Walking Transfer'", ")", "\n", "plt", ".", "xlabel", "(", "'steps'", ")", "\n", "plt", ".", "xlim", "(", "0", ",", "MIN_STEPS", ")", "\n", "plt", ".", "ylim", "(", "-", "100", ",", "-", "10", ")", "\n", "plt", ".", "ylabel", "(", "'cumulative reward'", ")", "\n", "base_dir", "=", "os", ".", "path", ".", "dirname", "(", "LOG_FILES", "[", "0", "]", ")", "\n", "plt", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "base_dir", ",", "savefig_name", ")", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.bohb_params_TD3_mountaincar_halfcheetah_ICM.ExperimentWrapper.get_bohb_parameters": [[19, 28], ["None"], "methods", ["None"], ["    ", "def", "get_bohb_parameters", "(", "self", ")", ":", "\n", "        ", "params", "=", "{", "}", "\n", "params", "[", "'min_budget'", "]", "=", "1", "\n", "params", "[", "'max_budget'", "]", "=", "2", "\n", "params", "[", "'eta'", "]", "=", "2", "\n", "params", "[", "'iterations'", "]", "=", "10000", "\n", "params", "[", "'random_fraction'", "]", "=", "0.3", "\n", "\n", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.bohb_params_TD3_mountaincar_halfcheetah_ICM.ExperimentWrapper.get_configspace": [[29, 39], ["ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter"], "methods", ["None"], ["", "def", "get_configspace", "(", "self", ")", ":", "\n", "        ", "cs", "=", "CS", ".", "ConfigurationSpace", "(", ")", "\n", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'lr'", ",", "lower", "=", "1e-6", ",", "upper", "=", "1e-3", ",", "log", "=", "True", ",", "default_value", "=", "1e-4", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'beta'", ",", "lower", "=", "0.001", ",", "upper", "=", "1.0", ",", "log", "=", "True", ",", "default_value", "=", "0.2", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'eta'", ",", "lower", "=", "0.001", ",", "upper", "=", "1.0", ",", "log", "=", "True", ",", "default_value", "=", "0.5", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'feature_dim'", ",", "lower", "=", "16", ",", "upper", "=", "256", ",", "log", "=", "True", ",", "default_value", "=", "64", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'hidden_size'", ",", "lower", "=", "16", ",", "upper", "=", "256", ",", "log", "=", "True", ",", "default_value", "=", "128", ")", ")", "\n", "\n", "return", "cs", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.bohb_params_TD3_mountaincar_halfcheetah_ICM.ExperimentWrapper.get_specific_config": [[40, 50], ["copy.deepcopy"], "methods", ["None"], ["", "def", "get_specific_config", "(", "self", ",", "cso", ",", "default_config", ",", "budget", ")", ":", "\n", "        ", "config", "=", "deepcopy", "(", "default_config", ")", "\n", "\n", "config", "[", "\"agents\"", "]", "[", "\"icm\"", "]", "[", "\"lr\"", "]", "=", "cso", "[", "\"lr\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"icm\"", "]", "[", "\"beta\"", "]", "=", "cso", "[", "\"beta\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"icm\"", "]", "[", "\"eta\"", "]", "=", "cso", "[", "\"eta\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"icm\"", "]", "[", "\"feature_dim\"", "]", "=", "cso", "[", "\"feature_dim\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"icm\"", "]", "[", "\"hidden_size\"", "]", "=", "cso", "[", "\"hidden_size\"", "]", "\n", "\n", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.bohb_params_TD3_mountaincar_halfcheetah_ICM.ExperimentWrapper.compute": [[51, 97], ["bohb_params_TD3_mountaincar_halfcheetah_ICM.ExperimentWrapper.get_specific_config", "print", "print", "print", "print", "print", "print", "envs.env_factory.EnvFactory", "envs.env_factory.EnvFactory.generate_real_env", "agents.TD3.TD3", "range", "str", "print", "print", "print", "print", "open", "yaml.safe_load", "agents.TD3.TD3.train", "rewards_list.append", "numpy.mean", "str", "str", "str", "envs.env_factory.EnvFactory.generate_real_env.get_max_action", "sum", "str"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_step_size.ExperimentWrapper.get_specific_config", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_real_env", "home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.train", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.get_max_action"], ["", "def", "compute", "(", "self", ",", "working_dir", ",", "bohb_id", ",", "config_id", ",", "cso", ",", "budget", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "# with open(\"default_config_halfcheetah.yaml\", 'r') as stream:", "\n", "#     default_config = yaml.safe_load(stream)", "\n", "        ", "with", "open", "(", "\"default_config_cmc.yaml\"", ",", "'r'", ")", "as", "stream", ":", "\n", "            ", "default_config", "=", "yaml", ".", "safe_load", "(", "stream", ")", "\n", "\n", "", "config", "=", "self", ".", "get_specific_config", "(", "cso", ",", "default_config", ",", "budget", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "\"START BOHB ITERATION\"", ")", "\n", "print", "(", "'CONFIG: '", "+", "str", "(", "config", ")", ")", "\n", "print", "(", "'CSO:    '", "+", "str", "(", "cso", ")", ")", "\n", "print", "(", "'BUDGET: '", "+", "str", "(", "budget", ")", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "info", "=", "{", "}", "\n", "\n", "# generate environment", "\n", "env_fac", "=", "EnvFactory", "(", "config", ")", "\n", "env", "=", "env_fac", ".", "generate_real_env", "(", ")", "\n", "\n", "td3", "=", "TD3", "(", "env", "=", "env", ",", "\n", "max_action", "=", "env", ".", "get_max_action", "(", ")", ",", "\n", "config", "=", "config", ",", "\n", "icm", "=", "True", ")", "\n", "\n", "# score_list = []", "\n", "rewards_list", "=", "[", "]", "\n", "for", "_", "in", "range", "(", "5", ")", ":", "\n", "            ", "rewards", ",", "_", ",", "_", "=", "td3", ".", "train", "(", "env", ")", "\n", "rewards_list", ".", "append", "(", "sum", "(", "rewards", ")", ")", "\n", "# score_i = len(rewards)", "\n", "# score_list.append(score_i)", "\n", "\n", "", "score", "=", "-", "np", ".", "mean", "(", "rewards_list", ")", "\n", "# score = np.mean(score_list)", "\n", "\n", "info", "[", "'config'", "]", "=", "str", "(", "config", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "'FINAL SCORE: '", "+", "str", "(", "score", ")", ")", "\n", "print", "(", "\"END BOHB ITERATION\"", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "return", "{", "\n", "\"loss\"", ":", "score", ",", "\n", "\"info\"", ":", "info", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld_transfer_vary_hp.auc": [[10, 18], ["None"], "function", ["None"], ["def", "auc", "(", ")", ":", "\n", "    ", "return", "[", "\n", "'../results/3_rn_auc/cliff_compare_reward_envs/best_transfer_vary_hp1.pt'", ",", "\n", "'../results/3_rn_auc/cliff_compare_reward_envs/best_transfer_vary_hp2.pt'", ",", "\n", "'../results/3_rn_auc/cliff_compare_reward_envs/best_transfer_vary_hp5.pt'", ",", "\n", "'../results/3_rn_auc/cliff_compare_reward_envs/best_transfer_vary_hp6.pt'", ",", "\n", "'../results/0_before_auc/cliff_compare_reward_envs/best_transfer_vary_hp0.pt'", ",", "\n", "'../results/0_before_auc/cliff_compare_reward_envs/best_transfer_vary_hp-1.pt'", ",", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld_transfer_vary_hp.normal": [[21, 30], ["None"], "function", ["None"], ["", "def", "normal", "(", ")", ":", "\n", "    ", "return", "[", "\n", "'../results/0_before_auc/cliff_compare_reward_envs/best_transfer_vary_hp1.pt'", ",", "\n", "'../results/0_before_auc/cliff_compare_reward_envs/best_transfer_vary_hp2.pt'", ",", "\n", "'../results/0_before_auc/cliff_compare_reward_envs/best_transfer_vary_hp5.pt'", ",", "\n", "'../results/0_before_auc/cliff_compare_reward_envs/best_transfer_vary_hp6.pt'", ",", "\n", "'../results/0_before_auc/cliff_compare_reward_envs/best_transfer_vary_hp0.pt'", ",", "\n", "# '../results/0_before_auc/cliff_compare_reward_envs/best_transfer_vary_hp-1.pt',", "\n", "'../results/0_before_auc/cliff_compare_reward_envs/best_transfer_vary_hp-1_opt.pt'", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld_transfer_vary_hp.reward_maximization": [[33, 41], ["None"], "function", ["None"], ["", "def", "reward_maximization", "(", ")", ":", "\n", "    ", "return", "[", "\n", "'../results/4_rn_reward/cliff_compare_reward_envs/best_transfer_vary_hp1.pt'", ",", "\n", "'../results/4_rn_reward/cliff_compare_reward_envs/best_transfer_vary_hp2.pt'", ",", "\n", "'../results/4_rn_reward/cliff_compare_reward_envs/best_transfer_vary_hp5.pt'", ",", "\n", "'../results/4_rn_reward/cliff_compare_reward_envs/best_transfer_vary_hp6.pt'", ",", "\n", "'../results/0_before_auc/cliff_compare_reward_envs/best_transfer_vary_hp0.pt'", ",", "\n", "'../results/0_before_auc/cliff_compare_reward_envs/best_transfer_vary_hp-1_opt.pt'", ",", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld_transfer_vary_hp.get_data": [[65, 115], ["float", "print", "print", "max", "torch.load", "list_data.append", "sums_eps_len_per_model.append", "sum", "sum", "numpy.zeros", "enumerate", "numpy.mean", "numpy.std", "proc_data.append", "print", "sums_eps_len.append", "sums_eps_len_per_model_i.append", "min", "zip", "range", "numpy.array", "sum", "sum", "sum", "sum", "numpy.asarray", "numpy.asarray", "len", "len", "concat_list.append"], "function", ["None"], ["def", "get_data", "(", ")", ":", "\n", "    ", "list_data", "=", "[", "]", "\n", "for", "log_file", "in", "LOG_FILES", ":", "\n", "        ", "data", "=", "torch", ".", "load", "(", "log_file", ")", "\n", "list_data", ".", "append", "(", "(", "data", "[", "'reward_list'", "]", ",", "data", "[", "'episode_length_list'", "]", ")", ")", "\n", "model_num", "=", "data", "[", "'model_num'", "]", "\n", "model_agents", "=", "data", "[", "'model_agents'", "]", "\n", "\n", "", "sums_eps_len", "=", "[", "]", "\n", "sums_eps_len_per_model", "=", "[", "]", "\n", "min_steps", "=", "float", "(", "'Inf'", ")", "\n", "# get minimum number of evaluations", "\n", "for", "reward_list", ",", "episode_length_list", "in", "list_data", ":", "\n", "        ", "sums_eps_len_per_model_i", "=", "[", "]", "\n", "for", "episode_lengths", "in", "episode_length_list", ":", "\n", "            ", "print", "(", "sum", "(", "episode_lengths", ")", ")", "\n", "sums_eps_len", ".", "append", "(", "sum", "(", "episode_lengths", ")", ")", "\n", "sums_eps_len_per_model_i", ".", "append", "(", "sum", "(", "episode_lengths", ")", ")", "\n", "min_steps", "=", "min", "(", "min_steps", ",", "sum", "(", "episode_lengths", ")", ")", "\n", "", "sums_eps_len_per_model", ".", "append", "(", "sums_eps_len_per_model_i", ")", "\n", "\n", "", "print", "(", "\"# of episode lens > MIN_STEPS: \"", ",", "sum", "(", "np", ".", "asarray", "(", "sums_eps_len", ")", ">=", "MIN_STEPS", ")", ")", "\n", "print", "(", "\"# of episode lens < MIN_STEPS: \"", ",", "sum", "(", "np", ".", "asarray", "(", "sums_eps_len", ")", "<", "MIN_STEPS", ")", ")", "\n", "min_steps", "=", "max", "(", "min_steps", ",", "MIN_STEPS", ")", "\n", "# convert data from episodes to steps", "\n", "proc_data", "=", "[", "]", "\n", "\n", "for", "reward_list", ",", "episode_length_list", "in", "list_data", ":", "\n", "        ", "np_data", "=", "np", ".", "zeros", "(", "[", "model_num", "*", "model_agents", ",", "min_steps", "]", ")", "\n", "\n", "for", "it", ",", "data", "in", "enumerate", "(", "zip", "(", "reward_list", ",", "episode_length_list", ")", ")", ":", "\n", "            ", "rewards", ",", "episode_lengths", "=", "data", "\n", "\n", "concat_list", "=", "[", "]", "\n", "rewards", "=", "rewards", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "episode_lengths", ")", ")", ":", "\n", "                ", "concat_list", "+=", "[", "rewards", "[", "i", "]", "]", "*", "episode_lengths", "[", "i", "]", "\n", "\n", "", "while", "len", "(", "concat_list", ")", "<", "min_steps", ":", "\n", "                ", "concat_list", ".", "append", "(", "concat_list", "[", "-", "1", "]", ")", "\n", "\n", "", "np_data", "[", "it", "]", "=", "np", ".", "array", "(", "concat_list", "[", ":", "min_steps", "]", ")", "\n", "\n", "", "mean", "=", "np", ".", "mean", "(", "np_data", ",", "axis", "=", "0", ")", "\n", "std", "=", "np", ".", "std", "(", "np_data", ",", "axis", "=", "0", ")", "\n", "\n", "proc_data", ".", "append", "(", "(", "mean", ",", "std", ")", ")", "\n", "\n", "", "return", "proc_data", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld_transfer_vary_hp.plot_data": [[117, 141], ["matplotlib.subplots", "matplotlib.legend", "matplotlib.subplots_adjust", "matplotlib.title", "matplotlib.xlabel", "matplotlib.xlim", "matplotlib.ylim", "matplotlib.ylabel", "os.path.dirname", "matplotlib.savefig", "matplotlib.show", "matplotlib.plot", "matplotlib.fill_between", "legobj.set_linewidth", "os.path.join", "range", "len"], "function", ["None"], ["", "def", "plot_data", "(", "proc_data", ",", "savefig_name", ")", ":", "\n", "    ", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", "dpi", "=", "600", ",", "figsize", "=", "(", "5", ",", "4", ")", ")", "\n", "\n", "for", "mean", ",", "std", "in", "proc_data", ":", "\n", "        ", "plt", ".", "plot", "(", "mean", ",", "linewidth", "=", "1", ")", "\n", "\n", "", "for", "mean", ",", "std", "in", "proc_data", ":", "\n", "        ", "plt", ".", "fill_between", "(", "x", "=", "range", "(", "len", "(", "mean", ")", ")", ",", "y1", "=", "mean", "-", "std", "*", "STD_MULT", ",", "y2", "=", "mean", "+", "std", "*", "STD_MULT", ",", "alpha", "=", "0.3", ")", "\n", "\n", "", "leg", "=", "plt", ".", "legend", "(", "LEGEND", ",", "fontsize", "=", "9", ")", "\n", "\n", "for", "legobj", "in", "leg", ".", "legendHandles", ":", "\n", "        ", "legobj", ".", "set_linewidth", "(", "2.0", ")", "\n", "\n", "# plt.xlim(0,99)", "\n", "", "plt", ".", "subplots_adjust", "(", "bottom", "=", "0.15", ",", "left", "=", "0.15", ")", "\n", "plt", ".", "title", "(", "'Cliff Walking Varied Hyperparameters'", ")", "\n", "plt", ".", "xlabel", "(", "'steps'", ")", "\n", "plt", ".", "xlim", "(", "0", ",", "MIN_STEPS", ")", "\n", "plt", ".", "ylim", "(", "-", "100", ",", "-", "10", ")", "\n", "plt", ".", "ylabel", "(", "'cumulative reward'", ")", "\n", "base_dir", "=", "os", ".", "path", ".", "dirname", "(", "LOG_FILES", "[", "0", "]", ")", "\n", "plt", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "base_dir", ",", "savefig_name", ")", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_compare_reward_envs.get_best_models_from_log": [[41, 71], ["hpbandster.logged_results_to_HBS_result", "hpres.logged_results_to_HBS_result.data.values", "print", "best_models.sort", "os.path.isdir", "log_dir.replace.replace", "best_models.append", "os.path.isfile", "model_name.replace.replace"], "function", ["None"], ["def", "get_best_models_from_log", "(", "log_dir", ")", ":", "\n", "    ", "if", "not", "os", ".", "path", ".", "isdir", "(", "log_dir", ")", ":", "\n", "        ", "log_dir", "=", "log_dir", ".", "replace", "(", "'nierhoff'", ",", "'dingsda'", ")", "\n", "\n", "", "result", "=", "hpres", ".", "logged_results_to_HBS_result", "(", "log_dir", ")", "\n", "\n", "best_models", "=", "[", "]", "\n", "\n", "for", "value", "in", "result", ".", "data", ".", "values", "(", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "loss", "=", "value", ".", "results", "[", "1.0", "]", "[", "'loss'", "]", "\n", "model_name", "=", "value", ".", "results", "[", "1.0", "]", "[", "'info'", "]", "[", "'model_name'", "]", "\n", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "model_name", ")", ":", "\n", "                ", "model_name", "=", "model_name", ".", "replace", "(", "'nierhoff'", ",", "'dingsda'", ")", "\n", "", "best_models", ".", "append", "(", "(", "loss", ",", "model_name", ")", ")", "\n", "", "except", ":", "\n", "            ", "continue", "\n", "\n", "# before AUC (objective minimized), example BOHB result -> -sorted([-150,-200, -250,-100])[-1] --> 100", "\n", "# print(\"sorting from low to high values (non-AUC)\")", "\n", "# best_models.sort(key=lambda x: x[0])", "\n", "# best_models = best_models[:MODEL_NUM]", "\n", "\n", "# AUC (objective maximized)", "\n", "", "", "print", "(", "\"sorting from high to low values (AUC)\"", ")", "\n", "best_models", ".", "sort", "(", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ",", "reverse", "=", "True", ")", "\n", "best_models", "=", "best_models", "[", ":", "MODEL_NUM", "]", "\n", "\n", "return", "best_models", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_compare_reward_envs.load_envs_and_config": [[73, 86], ["torch.load", "envs.env_factory.EnvFactory", "envs.env_factory.EnvFactory.generate_reward_env", "env_factory.generate_reward_env.load_state_dict", "envs.env_factory.EnvFactory.generate_real_env"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_reward_env", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_real_env"], ["", "def", "load_envs_and_config", "(", "model_file", ")", ":", "\n", "    ", "save_dict", "=", "torch", ".", "load", "(", "model_file", ")", "\n", "\n", "config", "=", "save_dict", "[", "'config'", "]", "\n", "config", "[", "'device'", "]", "=", "'cpu'", "\n", "config", "[", "'envs'", "]", "[", "'Cliff'", "]", "[", "'solved_reward'", "]", "=", "100000", "# something big enough to prevent early out triggering", "\n", "\n", "env_factory", "=", "EnvFactory", "(", "config", "=", "config", ")", "\n", "reward_env", "=", "env_factory", ".", "generate_reward_env", "(", ")", "\n", "reward_env", ".", "load_state_dict", "(", "save_dict", "[", "'model'", "]", ")", "\n", "real_env", "=", "env_factory", ".", "generate_real_env", "(", ")", "\n", "\n", "return", "reward_env", ",", "real_env", ",", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_compare_reward_envs.train_test_agents": [[88, 125], ["range", "agents.agent_utils.select_agent.train", "rewards.append", "episode_lengths.append", "agents.agent_utils.select_agent", "agents.agent_utils.select_agent"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.train", "home.repos.pwc.inspect_result.automl_learning_environments.agents.agent_utils.select_agent", "home.repos.pwc.inspect_result.automl_learning_environments.agents.agent_utils.select_agent"], ["", "def", "train_test_agents", "(", "mode", ",", "env", ",", "real_env", ",", "config", ")", ":", "\n", "    ", "rewards", "=", "[", "]", "\n", "episode_lengths", "=", "[", "]", "\n", "\n", "# settings for comparability", "\n", "config", "[", "'agents'", "]", "[", "'ql'", "]", "[", "'test_episodes'", "]", "=", "1", "\n", "config", "[", "'agents'", "]", "[", "'ql'", "]", "[", "'train_episodes'", "]", "=", "500", "\n", "config", "[", "'agents'", "]", "[", "'ql'", "]", "[", "'print_rate'", "]", "=", "100", "\n", "\n", "config", "[", "'agents'", "]", "[", "'ql'", "]", "[", "'alpha'", "]", "=", "1.0", "\n", "config", "[", "'agents'", "]", "[", "'ql'", "]", "[", "'eps_decay'", "]", "=", "0.0", "\n", "config", "[", "'agents'", "]", "[", "'ql'", "]", "[", "'eps_init'", "]", "=", "0.1", "\n", "config", "[", "'agents'", "]", "[", "'ql'", "]", "[", "'eps_min'", "]", "=", "0.1", "\n", "config", "[", "'agents'", "]", "[", "'ql'", "]", "[", "'gamma'", "]", "=", "0.8", "\n", "config", "[", "'agents'", "]", "[", "'ql'", "]", "[", "'same_action_num'", "]", "=", "1", "\n", "config", "[", "'agents'", "]", "[", "'ql'", "]", "[", "'rb_size'", "]", "=", "1", "# custom to reward env and gridworld", "\n", "config", "[", "'agents'", "]", "[", "'ql'", "]", "[", "'init_episodes'", "]", "=", "0", "\n", "config", "[", "'agents'", "]", "[", "'ql'", "]", "[", "'batch_size'", "]", "=", "1", "\n", "\n", "config", "[", "'agents'", "]", "[", "'ql'", "]", "[", "'early_out_num'", "]", "=", "10", "\n", "config", "[", "'agents'", "]", "[", "'ql'", "]", "[", "'early_out_virtual_diff'", "]", "=", "0.02", "\n", "\n", "# for count-based q-learning", "\n", "config", "[", "'agents'", "]", "[", "'ql'", "]", "[", "'beta'", "]", "=", "0.1", "\n", "\n", "# for count-based q-learning (tuned)", "\n", "# config['agents']['ql']['beta'] = 0.005  # 0.01 also works fine", "\n", "\n", "for", "i", "in", "range", "(", "MODEL_AGENTS", ")", ":", "\n", "        ", "if", "mode", "==", "'-1'", ":", "\n", "            ", "agent", "=", "select_agent", "(", "config", "=", "config", ",", "agent_name", "=", "'ql_cb'", ")", "\n", "", "else", ":", "\n", "            ", "agent", "=", "select_agent", "(", "config", "=", "config", ",", "agent_name", "=", "'ql'", ")", "\n", "", "reward", ",", "episode_length", ",", "_", "=", "agent", ".", "train", "(", "env", "=", "env", ",", "test_env", "=", "real_env", ")", "\n", "rewards", ".", "append", "(", "reward", ")", "\n", "episode_lengths", ".", "append", "(", "episode_length", ")", "\n", "", "return", "rewards", ",", "episode_lengths", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_compare_reward_envs.save_list": [[127, 137], ["os.makedirs", "os.path.join", "torch.save", "str"], "function", ["None"], ["", "def", "save_list", "(", "mode", ",", "config", ",", "reward_list", ",", "episode_length_list", ")", ":", "\n", "    ", "os", ".", "makedirs", "(", "SAVE_DIR", ",", "exist_ok", "=", "True", ")", "\n", "file_name", "=", "os", ".", "path", ".", "join", "(", "SAVE_DIR", ",", "'best'", "+", "str", "(", "mode", ")", "+", "'.pt'", ")", "\n", "save_dict", "=", "{", "}", "\n", "save_dict", "[", "'config'", "]", "=", "config", "\n", "save_dict", "[", "'model_num'", "]", "=", "MODEL_NUM", "\n", "save_dict", "[", "'model_agents'", "]", "=", "MODEL_AGENTS", "\n", "save_dict", "[", "'reward_list'", "]", "=", "reward_list", "\n", "save_dict", "[", "'episode_length_list'", "]", "=", "episode_length_list", "\n", "torch", ".", "save", "(", "save_dict", ",", "file_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_compare_reward_envs.eval_models": [[139, 154], ["GTNC_evaluate_gridworld_compare_reward_envs.get_best_models_from_log", "GTNC_evaluate_gridworld_compare_reward_envs.save_list", "print", "print", "GTNC_evaluate_gridworld_compare_reward_envs.load_envs_and_config", "GTNC_evaluate_gridworld_compare_reward_envs.train_test_agents", "str", "str"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.get_best_models_from_log", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.save_list", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.load_envs_and_config", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.train_test_agents"], ["", "def", "eval_models", "(", "mode", ",", "log_dir", ")", ":", "\n", "    ", "best_models", "=", "get_best_models_from_log", "(", "log_dir", ")", "\n", "\n", "reward_list", "=", "[", "]", "\n", "episode_length_list", "=", "[", "]", "\n", "\n", "for", "best_reward", ",", "model_file", "in", "best_models", ":", "\n", "        ", "print", "(", "'best reward: '", "+", "str", "(", "best_reward", ")", ")", "\n", "print", "(", "'model file: '", "+", "str", "(", "model_file", ")", ")", "\n", "reward_env", ",", "real_env", ",", "config", "=", "load_envs_and_config", "(", "model_file", ")", "\n", "rewards", ",", "episode_lengths", "=", "train_test_agents", "(", "mode", "=", "mode", ",", "env", "=", "reward_env", ",", "real_env", "=", "real_env", ",", "config", "=", "config", ")", "\n", "reward_list", "+=", "rewards", "\n", "episode_length_list", "+=", "episode_lengths", "\n", "\n", "", "save_list", "(", "mode", ",", "config", ",", "reward_list", ",", "episode_length_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_compare_reward_envs.eval_base": [[156, 175], ["GTNC_evaluate_gridworld_compare_reward_envs.get_best_models_from_log", "range", "GTNC_evaluate_gridworld_compare_reward_envs.save_list", "GTNC_evaluate_gridworld_compare_reward_envs.load_envs_and_config", "GTNC_evaluate_gridworld_compare_reward_envs.train_test_agents"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.get_best_models_from_log", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.save_list", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.load_envs_and_config", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.train_test_agents"], ["", "def", "eval_base", "(", "mode", ",", "log_dir", ")", ":", "\n", "    ", "best_models", "=", "get_best_models_from_log", "(", "log_dir", ")", "\n", "\n", "reward_list", "=", "[", "]", "\n", "episode_length_list", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "MODEL_NUM", ")", ":", "\n", "# if not os.path.isdir(best_models[0][1]):", "\n", "#     model_file = best_models[0][1].replace('/home/dingsda/master_thesis/learning_environments/results',", "\n", "#                                            '/home/ferreira/Projects/learning_environments/results/thomas_results')", "\n", "# else:", "\n", "#     model_file = best_models[0][1]", "\n", "\n", "        ", "reward_env", ",", "real_env", ",", "config", "=", "load_envs_and_config", "(", "best_models", "[", "0", "]", "[", "1", "]", ")", "\n", "rewards", ",", "episode_lengths", "=", "train_test_agents", "(", "mode", "=", "mode", ",", "env", "=", "real_env", ",", "real_env", "=", "real_env", ",", "config", "=", "config", ")", "\n", "reward_list", "+=", "rewards", "\n", "episode_length_list", "+=", "episode_lengths", "\n", "\n", "", "save_list", "(", "mode", ",", "config", ",", "reward_list", ",", "episode_length_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_halfcheetah_params.ExperimentWrapper.get_bohb_parameters": [[18, 27], ["None"], "methods", ["None"], ["    ", "def", "get_bohb_parameters", "(", "self", ")", ":", "\n", "        ", "params", "=", "{", "}", "\n", "params", "[", "'min_budget'", "]", "=", "1", "\n", "params", "[", "'max_budget'", "]", "=", "3", "\n", "params", "[", "'eta'", "]", "=", "3", "\n", "params", "[", "'random_fraction'", "]", "=", "0.3", "\n", "params", "[", "'iterations'", "]", "=", "10000", "\n", "\n", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_halfcheetah_params.ExperimentWrapper.get_configspace": [[28, 55], ["ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter"], "methods", ["None"], ["", "def", "get_configspace", "(", "self", ")", ":", "\n", "        ", "cs", "=", "CS", ".", "ConfigurationSpace", "(", ")", "\n", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'gtn_score_transform_type'", ",", "lower", "=", "0", ",", "upper", "=", "7", ",", "log", "=", "False", ",", "default_value", "=", "7", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'gtn_step_size'", ",", "lower", "=", "0.1", ",", "upper", "=", "1", ",", "log", "=", "True", ",", "default_value", "=", "0.5", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "CategoricalHyperparameter", "(", "name", "=", "'gtn_mirrored_sampling'", ",", "choices", "=", "[", "False", ",", "True", "]", ",", "default_value", "=", "True", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'gtn_noise_std'", ",", "lower", "=", "0.001", ",", "upper", "=", "1", ",", "log", "=", "True", ",", "default_value", "=", "0.01", ")", ")", "\n", "\n", "# cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(name='td3_init_episodes', lower=1, upper=20, log=True, default_value=10))", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'td3_batch_size'", ",", "lower", "=", "64", ",", "upper", "=", "256", ",", "log", "=", "False", ",", "default_value", "=", "128", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_gamma'", ",", "lower", "=", "0.001", ",", "upper", "=", "0.1", ",", "log", "=", "True", ",", "default_value", "=", "0.01", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_lr'", ",", "lower", "=", "1e-4", ",", "upper", "=", "5e-3", ",", "log", "=", "True", ",", "default_value", "=", "1e-3", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_tau'", ",", "lower", "=", "0.005", ",", "upper", "=", "0.05", ",", "log", "=", "True", ",", "default_value", "=", "0.01", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'td3_policy_delay'", ",", "lower", "=", "1", ",", "upper", "=", "3", ",", "log", "=", "False", ",", "default_value", "=", "2", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "CategoricalHyperparameter", "(", "name", "=", "'td3_activation_fn'", ",", "choices", "=", "[", "'tanh'", ",", "'relu'", ",", "'leakyrelu'", ",", "'prelu'", "]", ",", "default_value", "=", "'relu'", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'td3_hidden_size'", ",", "lower", "=", "48", ",", "upper", "=", "192", ",", "log", "=", "True", ",", "default_value", "=", "128", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'td3_hidden_layer'", ",", "lower", "=", "1", ",", "upper", "=", "2", ",", "log", "=", "False", ",", "default_value", "=", "2", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_action_std'", ",", "lower", "=", "0.05", ",", "upper", "=", "0.2", ",", "log", "=", "True", ",", "default_value", "=", "0.1", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_policy_std'", ",", "lower", "=", "0.1", ",", "upper", "=", "0.4", ",", "log", "=", "True", ",", "default_value", "=", "0.2", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_policy_std_clip'", ",", "lower", "=", "0.25", ",", "upper", "=", "1", ",", "log", "=", "True", ",", "default_value", "=", "0.5", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_early_out_virtual_diff'", ",", "lower", "=", "1e-2", ",", "upper", "=", "1e-1", ",", "log", "=", "True", ",", "default_value", "=", "3e-2", ")", ")", "\n", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "CategoricalHyperparameter", "(", "name", "=", "'halfcheetah_activation_fn'", ",", "choices", "=", "[", "'tanh'", ",", "'relu'", ",", "'leakyrelu'", ",", "'prelu'", "]", ",", "default_value", "=", "'leakyrelu'", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'halfcheetah_hidden_size'", ",", "lower", "=", "48", ",", "upper", "=", "192", ",", "log", "=", "True", ",", "default_value", "=", "128", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'halfcheetah_hidden_layer'", ",", "lower", "=", "1", ",", "upper", "=", "2", ",", "log", "=", "False", ",", "default_value", "=", "1", ")", ")", "\n", "\n", "return", "cs", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_halfcheetah_params.ExperimentWrapper.get_specific_config": [[56, 87], ["copy.deepcopy"], "methods", ["None"], ["", "def", "get_specific_config", "(", "self", ",", "cso", ",", "default_config", ",", "budget", ")", ":", "\n", "        ", "config", "=", "deepcopy", "(", "default_config", ")", "\n", "\n", "config", "[", "\"agents\"", "]", "[", "'gtn'", "]", "[", "'score_transform_type'", "]", "=", "cso", "[", "\"gtn_score_transform_type\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'gtn'", "]", "[", "'step_size'", "]", "=", "cso", "[", "\"gtn_step_size\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'gtn'", "]", "[", "'mirrored_sampling'", "]", "=", "cso", "[", "\"gtn_mirrored_sampling\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'gtn'", "]", "[", "'noise_std'", "]", "=", "cso", "[", "\"gtn_noise_std\"", "]", "\n", "\n", "# config[\"agents\"]['td3']['init_episodes'] = cso[\"td3_init_episodes\"]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'batch_size'", "]", "=", "cso", "[", "\"td3_batch_size\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'gamma'", "]", "=", "1", "-", "cso", "[", "\"td3_gamma\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'lr'", "]", "=", "cso", "[", "\"td3_lr\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'tau'", "]", "=", "cso", "[", "\"td3_tau\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'policy_delay'", "]", "=", "cso", "[", "\"td3_policy_delay\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'activation_fn'", "]", "=", "cso", "[", "\"td3_activation_fn\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'hidden_size'", "]", "=", "cso", "[", "\"td3_hidden_size\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'hidden_layer'", "]", "=", "cso", "[", "\"td3_hidden_layer\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'action_std'", "]", "=", "cso", "[", "\"td3_action_std\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'policy_std'", "]", "=", "cso", "[", "\"td3_policy_std\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'policy_std_clip'", "]", "=", "cso", "[", "\"td3_policy_std_clip\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'early_out_virtual_diff'", "]", "=", "cso", "[", "\"td3_early_out_virtual_diff\"", "]", "\n", "\n", "config", "[", "\"envs\"", "]", "[", "'HalfCheetah-v3'", "]", "[", "'activation_fn'", "]", "=", "cso", "[", "\"halfcheetah_activation_fn\"", "]", "\n", "config", "[", "\"envs\"", "]", "[", "'HalfCheetah-v3'", "]", "[", "'hidden_size'", "]", "=", "cso", "[", "\"halfcheetah_hidden_size\"", "]", "\n", "config", "[", "\"envs\"", "]", "[", "'HalfCheetah-v3'", "]", "[", "'hidden_layer'", "]", "=", "cso", "[", "\"halfcheetah_hidden_layer\"", "]", "\n", "\n", "global", "reward_env_type", "\n", "config", "[", "\"agents\"", "]", "[", "'gtn'", "]", "[", "'synthetic_env_type'", "]", "=", "1", "\n", "config", "[", "\"envs\"", "]", "[", "'HalfCheetah-v3'", "]", "[", "'reward_env_type'", "]", "=", "reward_env_type", "\n", "\n", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_halfcheetah_params.ExperimentWrapper.compute": [[88, 128], ["GTNC_evaluate_halfcheetah_params.ExperimentWrapper.get_specific_config", "print", "print", "print", "print", "print", "print", "str", "str", "str", "str", "print", "print", "print", "print", "print", "open", "yaml.safe_load", "agents.GTN.GTN_Master", "agents.GTN.GTN_Master.run", "str", "str", "str", "float", "traceback.format_exc", "print", "str", "str", "sorted"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_step_size.ExperimentWrapper.get_specific_config", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.run"], ["", "def", "compute", "(", "self", ",", "working_dir", ",", "bohb_id", ",", "config_id", ",", "cso", ",", "budget", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "with", "open", "(", "\"default_config_halfcheetah_td3_se_opt.yaml\"", ",", "'r'", ")", "as", "stream", ":", "\n", "            ", "default_config", "=", "yaml", ".", "safe_load", "(", "stream", ")", "\n", "\n", "", "config", "=", "self", ".", "get_specific_config", "(", "cso", ",", "default_config", ",", "budget", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "\"START BOHB ITERATION\"", ")", "\n", "print", "(", "'CONFIG: '", "+", "str", "(", "config", ")", ")", "\n", "print", "(", "'CSO:    '", "+", "str", "(", "cso", ")", ")", "\n", "print", "(", "'BUDGET: '", "+", "str", "(", "budget", ")", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "try", ":", "\n", "            ", "gtn", "=", "GTN_Master", "(", "config", ",", "bohb_id", "=", "bohb_id", ",", "bohb_working_dir", "=", "working_dir", ")", "\n", "_", ",", "score_list", ",", "model_name", "=", "gtn", ".", "run", "(", ")", "\n", "score", "=", "-", "sorted", "(", "score_list", ")", "[", "-", "1", "]", "\n", "error", "=", "\"\"", "\n", "", "except", ":", "\n", "            ", "score", "=", "float", "(", "'-Inf'", ")", "\n", "score_list", "=", "[", "]", "\n", "model_name", "=", "None", "\n", "error", "=", "traceback", ".", "format_exc", "(", ")", "\n", "print", "(", "error", ")", "\n", "\n", "", "info", "=", "{", "}", "\n", "info", "[", "'error'", "]", "=", "str", "(", "error", ")", "\n", "info", "[", "'config'", "]", "=", "str", "(", "config", ")", "\n", "info", "[", "'score_list'", "]", "=", "str", "(", "score_list", ")", "\n", "info", "[", "'model_name'", "]", "=", "str", "(", "model_name", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "'FINAL SCORE: '", "+", "str", "(", "score", ")", ")", "\n", "print", "(", "'SCORE LIST:  '", "+", "str", "(", "score_list", ")", ")", "\n", "print", "(", "\"END BOHB ITERATION\"", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "return", "{", "\n", "\"loss\"", ":", "score", ",", "\n", "\"info\"", ":", "info", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_transfer_vary_hp.get_best_models_from_log": [[40, 69], ["hpbandster.logged_results_to_HBS_result", "hpres.logged_results_to_HBS_result.data.values", "print", "best_models.sort", "os.path.isdir", "log_dir.replace.replace", "best_models.append", "os.path.isfile", "model_name.replace.replace"], "function", ["None"], ["def", "get_best_models_from_log", "(", "log_dir", ")", ":", "\n", "    ", "if", "not", "os", ".", "path", ".", "isdir", "(", "log_dir", ")", ":", "\n", "        ", "log_dir", "=", "log_dir", ".", "replace", "(", "'nierhoff'", ",", "'dingsda'", ")", "\n", "\n", "", "result", "=", "hpres", ".", "logged_results_to_HBS_result", "(", "log_dir", ")", "\n", "\n", "best_models", "=", "[", "]", "\n", "\n", "for", "value", "in", "result", ".", "data", ".", "values", "(", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "loss", "=", "value", ".", "results", "[", "1.0", "]", "[", "'loss'", "]", "\n", "model_name", "=", "value", ".", "results", "[", "1.0", "]", "[", "'info'", "]", "[", "'model_name'", "]", "\n", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "model_name", ")", ":", "\n", "                ", "model_name", "=", "model_name", ".", "replace", "(", "'nierhoff'", ",", "'dingsda'", ")", "\n", "", "best_models", ".", "append", "(", "(", "loss", ",", "model_name", ")", ")", "\n", "", "except", ":", "\n", "            ", "continue", "\n", "\n", "# before AUC (objective minimized)", "\n", "# print(\"sorting from low to high values (non-AUC)\")", "\n", "# best_models.sort(key=lambda x: x[0])", "\n", "\n", "# AUC (objective maximized)", "\n", "", "", "print", "(", "\"sorting from high to low values (AUC)\"", ")", "\n", "best_models", ".", "sort", "(", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ",", "reverse", "=", "True", ")", "\n", "best_models", "=", "best_models", "[", ":", "MODEL_NUM", "]", "\n", "\n", "return", "best_models", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_transfer_vary_hp.load_envs_and_config": [[71, 84], ["torch.load", "envs.env_factory.EnvFactory", "envs.env_factory.EnvFactory.generate_reward_env", "env_factory.generate_reward_env.load_state_dict", "envs.env_factory.EnvFactory.generate_real_env"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_reward_env", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_real_env"], ["", "def", "load_envs_and_config", "(", "model_file", ")", ":", "\n", "    ", "save_dict", "=", "torch", ".", "load", "(", "model_file", ")", "\n", "\n", "config", "=", "save_dict", "[", "'config'", "]", "\n", "config", "[", "'device'", "]", "=", "'cpu'", "\n", "config", "[", "'envs'", "]", "[", "'CartPole-v0'", "]", "[", "'solved_reward'", "]", "=", "100000", "# something big enough to prevent early out triggering", "\n", "\n", "env_factory", "=", "EnvFactory", "(", "config", "=", "config", ")", "\n", "reward_env", "=", "env_factory", ".", "generate_reward_env", "(", ")", "\n", "reward_env", ".", "load_state_dict", "(", "save_dict", "[", "'model'", "]", ")", "\n", "real_env", "=", "env_factory", ".", "generate_real_env", "(", ")", "\n", "\n", "return", "reward_env", ",", "real_env", ",", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_transfer_vary_hp.vary_hp": [[86, 136], ["ConfigSpace.ConfigurationSpace", "CS.ConfigurationSpace.add_hyperparameter", "CS.ConfigurationSpace.add_hyperparameter", "CS.ConfigurationSpace.add_hyperparameter", "CS.ConfigurationSpace.add_hyperparameter", "CS.ConfigurationSpace.sample_configuration", "print", "copy.deepcopy", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "int", "int", "int", "int"], "function", ["None"], ["", "def", "vary_hp", "(", "config", ")", ":", "\n", "    ", "lr", "=", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'lr'", "]", "\n", "batch_size", "=", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'batch_size'", "]", "\n", "hidden_size", "=", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'hidden_size'", "]", "\n", "hidden_layer", "=", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'hidden_layer'", "]", "\n", "\n", "cs", "=", "CS", ".", "ConfigurationSpace", "(", ")", "\n", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'lr'", ",", "\n", "lower", "=", "lr", "/", "3", ",", "\n", "upper", "=", "lr", "*", "3", ",", "\n", "log", "=", "True", ",", "\n", "default_value", "=", "lr", ")", "\n", ")", "\n", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'batch_size'", ",", "\n", "lower", "=", "int", "(", "batch_size", "/", "3", ")", ",", "\n", "upper", "=", "int", "(", "batch_size", "*", "3", ")", ",", "\n", "log", "=", "True", ",", "\n", "default_value", "=", "batch_size", ")", "\n", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'hidden_size'", ",", "\n", "lower", "=", "int", "(", "hidden_size", "/", "3", ")", ",", "\n", "upper", "=", "int", "(", "hidden_size", "*", "3", ")", ",", "\n", "log", "=", "True", ",", "\n", "default_value", "=", "hidden_size", ")", "\n", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'hidden_layer'", ",", "\n", "lower", "=", "hidden_layer", "-", "1", ",", "\n", "upper", "=", "hidden_layer", "+", "1", ",", "\n", "log", "=", "False", ",", "\n", "default_value", "=", "hidden_layer", ")", "\n", ")", "\n", "\n", "sample", "=", "cs", ".", "sample_configuration", "(", ")", "\n", "\n", "print", "(", "f\"sampled part of config: \"", "\n", "f\"lr: {sample['lr']}, \"", "\n", "f\"batch_size: {sample['batch_size']}, \"", "\n", "f\"hidden_size: {sample['hidden_size']}, \"", "\n", "f\"hidden_layer: {sample['hidden_layer']}\"", "\n", ")", "\n", "\n", "config_mod", "=", "deepcopy", "(", "config", ")", "\n", "config_mod", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'lr'", "]", "=", "sample", "[", "'lr'", "]", "\n", "config_mod", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'batch_size'", "]", "=", "sample", "[", "'batch_size'", "]", "\n", "config_mod", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'hidden_size'", "]", "=", "sample", "[", "'hidden_size'", "]", "\n", "config_mod", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'hidden_layer'", "]", "=", "sample", "[", "'hidden_layer'", "]", "\n", "\n", "return", "config_mod", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_transfer_vary_hp.train_test_agents": [[138, 191], ["range", "GTNC_evaluate_cartpole_transfer_vary_hp.vary_hp", "agents.agent_utils.select_agent.train", "rewards.append", "episode_lengths.append", "agents.agent_utils.select_agent", "agents.agent_utils.select_agent"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_transfer_vary_hp.vary_hp", "home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.train", "home.repos.pwc.inspect_result.automl_learning_environments.agents.agent_utils.select_agent", "home.repos.pwc.inspect_result.automl_learning_environments.agents.agent_utils.select_agent"], ["", "def", "train_test_agents", "(", "mode", ",", "env", ",", "real_env", ",", "config", ")", ":", "\n", "    ", "rewards", "=", "[", "]", "\n", "episode_lengths", "=", "[", "]", "\n", "\n", "# settings for comparability", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'test_episodes'", "]", "=", "1", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'train_episodes'", "]", "=", "1000", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'print_rate'", "]", "=", "100", "\n", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'lr'", "]", "=", "0.00025", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'eps_init'", "]", "=", "1.0", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'eps_min'", "]", "=", "0.1", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'eps_decay'", "]", "=", "0.9", "# original DDQN paper uses linear decay over 1M N's", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'gamma'", "]", "=", "0.99", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'batch_size'", "]", "=", "32", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'same_action_num'", "]", "=", "1", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'activation_fn'", "]", "=", "\"relu\"", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'tau'", "]", "=", "0.01", "# original DDQN paper has hard update every N steps", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'hidden_size'", "]", "=", "64", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'hidden_layer'", "]", "=", "1", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'rb_size'", "]", "=", "1000000", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'init_episodes'", "]", "=", "1", "\n", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'early_out_num'", "]", "=", "10", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'early_out_virtual_diff'", "]", "=", "0.02", "\n", "\n", "# optimized ICM HPs:", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "=", "{", "}", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "[", "'beta'", "]", "=", "0.05", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "[", "'eta'", "]", "=", "0.03", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "[", "'feature_dim'", "]", "=", "32", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "[", "'hidden_size'", "]", "=", "128", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "[", "'lr'", "]", "=", "1e-5", "\n", "\n", "# default ICM HPs:", "\n", "# config['agents']['icm'] = {}", "\n", "# config['agents']['icm']['beta'] = 0.2", "\n", "# config['agents']['icm']['eta'] = 0.5", "\n", "# config['agents']['icm']['feature_dim'] = 64", "\n", "# config['agents']['icm']['hidden_size'] = 128", "\n", "# config['agents']['icm']['lr'] = 1e-4", "\n", "\n", "for", "i", "in", "range", "(", "MODEL_AGENTS", ")", ":", "\n", "        ", "config_mod", "=", "vary_hp", "(", "config", ")", "\n", "\n", "if", "mode", "==", "'-1'", ":", "\n", "            ", "agent", "=", "select_agent", "(", "config", "=", "config_mod", ",", "agent_name", "=", "'ddqn_icm'", ")", "\n", "", "else", ":", "\n", "            ", "agent", "=", "select_agent", "(", "config", "=", "config_mod", ",", "agent_name", "=", "'ddqn'", ")", "\n", "", "reward", ",", "episode_length", ",", "_", "=", "agent", ".", "train", "(", "env", "=", "env", ",", "test_env", "=", "real_env", ")", "\n", "rewards", ".", "append", "(", "reward", ")", "\n", "episode_lengths", ".", "append", "(", "episode_length", ")", "\n", "", "return", "rewards", ",", "episode_lengths", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_transfer_vary_hp.save_list": [[193, 203], ["os.makedirs", "os.path.join", "torch.save", "str"], "function", ["None"], ["", "def", "save_list", "(", "mode", ",", "config", ",", "reward_list", ",", "episode_length_list", ")", ":", "\n", "    ", "os", ".", "makedirs", "(", "SAVE_DIR", ",", "exist_ok", "=", "True", ")", "\n", "file_name", "=", "os", ".", "path", ".", "join", "(", "SAVE_DIR", ",", "'best_transfer_vary_hp'", "+", "str", "(", "mode", ")", "+", "'.pt'", ")", "\n", "save_dict", "=", "{", "}", "\n", "save_dict", "[", "'config'", "]", "=", "config", "\n", "save_dict", "[", "'model_num'", "]", "=", "MODEL_NUM", "\n", "save_dict", "[", "'model_agents'", "]", "=", "MODEL_AGENTS", "\n", "save_dict", "[", "'reward_list'", "]", "=", "reward_list", "\n", "save_dict", "[", "'episode_length_list'", "]", "=", "episode_length_list", "\n", "torch", ".", "save", "(", "save_dict", ",", "file_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_transfer_vary_hp.eval_models": [[205, 220], ["GTNC_evaluate_cartpole_transfer_vary_hp.get_best_models_from_log", "GTNC_evaluate_cartpole_transfer_vary_hp.save_list", "print", "print", "GTNC_evaluate_cartpole_transfer_vary_hp.load_envs_and_config", "GTNC_evaluate_cartpole_transfer_vary_hp.train_test_agents", "str", "str"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.get_best_models_from_log", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.save_list", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.load_envs_and_config", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.train_test_agents"], ["", "def", "eval_models", "(", "mode", ",", "log_dir", ")", ":", "\n", "    ", "best_models", "=", "get_best_models_from_log", "(", "log_dir", ")", "\n", "\n", "reward_list", "=", "[", "]", "\n", "episode_length_list", "=", "[", "]", "\n", "\n", "for", "best_reward", ",", "model_file", "in", "best_models", ":", "\n", "        ", "print", "(", "'best reward: '", "+", "str", "(", "best_reward", ")", ")", "\n", "print", "(", "'model file: '", "+", "str", "(", "model_file", ")", ")", "\n", "reward_env", ",", "real_env", ",", "config", "=", "load_envs_and_config", "(", "model_file", ")", "\n", "rewards", ",", "episode_lengths", "=", "train_test_agents", "(", "mode", "=", "mode", ",", "env", "=", "reward_env", ",", "real_env", "=", "real_env", ",", "config", "=", "config", ")", "\n", "reward_list", "+=", "rewards", "\n", "episode_length_list", "+=", "episode_lengths", "\n", "\n", "", "save_list", "(", "mode", ",", "config", ",", "reward_list", ",", "episode_length_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_transfer_vary_hp.eval_base": [[222, 235], ["GTNC_evaluate_cartpole_transfer_vary_hp.get_best_models_from_log", "range", "GTNC_evaluate_cartpole_transfer_vary_hp.save_list", "GTNC_evaluate_cartpole_transfer_vary_hp.load_envs_and_config", "GTNC_evaluate_cartpole_transfer_vary_hp.train_test_agents"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.get_best_models_from_log", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.save_list", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.load_envs_and_config", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.train_test_agents"], ["", "def", "eval_base", "(", "mode", ",", "log_dir", ")", ":", "\n", "    ", "best_models", "=", "get_best_models_from_log", "(", "log_dir", ")", "\n", "\n", "reward_list", "=", "[", "]", "\n", "episode_length_list", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "MODEL_NUM", ")", ":", "\n", "        ", "reward_env", ",", "real_env", ",", "config", "=", "load_envs_and_config", "(", "best_models", "[", "0", "]", "[", "1", "]", ")", "\n", "rewards", ",", "episode_lengths", "=", "train_test_agents", "(", "mode", "=", "mode", ",", "env", "=", "real_env", ",", "real_env", "=", "real_env", ",", "config", "=", "config", ")", "\n", "reward_list", "+=", "rewards", "\n", "episode_length_list", "+=", "episode_lengths", "\n", "\n", "", "save_list", "(", "mode", ",", "config", ",", "reward_list", ",", "episode_length_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_transfer_vary_hp.eval_icm": [[237, 250], ["GTNC_evaluate_cartpole_transfer_vary_hp.get_best_models_from_log", "range", "GTNC_evaluate_cartpole_transfer_vary_hp.save_list", "GTNC_evaluate_cartpole_transfer_vary_hp.load_envs_and_config", "GTNC_evaluate_cartpole_transfer_vary_hp.train_test_agents"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.get_best_models_from_log", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.save_list", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.load_envs_and_config", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.train_test_agents"], ["", "def", "eval_icm", "(", "mode", ",", "log_dir", ")", ":", "\n", "    ", "best_models", "=", "get_best_models_from_log", "(", "log_dir", ")", "\n", "\n", "reward_list", "=", "[", "]", "\n", "episode_length_list", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "MODEL_NUM", ")", ":", "\n", "        ", "reward_env", ",", "real_env", ",", "config", "=", "load_envs_and_config", "(", "best_models", "[", "0", "]", "[", "1", "]", ")", "\n", "rewards", ",", "episode_lengths", "=", "train_test_agents", "(", "mode", "=", "mode", ",", "env", "=", "real_env", ",", "real_env", "=", "real_env", ",", "config", "=", "config", ")", "\n", "reward_list", "+=", "rewards", "\n", "episode_length_list", "+=", "episode_lengths", "\n", "\n", "", "save_list", "(", "mode", ",", "config", ",", "reward_list", ",", "episode_length_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cmc.ExperimentWrapper.get_bohb_parameters": [[17, 26], ["None"], "methods", ["None"], ["    ", "def", "get_bohb_parameters", "(", "self", ")", ":", "\n", "        ", "params", "=", "{", "}", "\n", "params", "[", "'min_budget'", "]", "=", "1", "\n", "params", "[", "'max_budget'", "]", "=", "1", "\n", "params", "[", "'eta'", "]", "=", "2", "\n", "params", "[", "'random_fraction'", "]", "=", "1", "\n", "params", "[", "'iterations'", "]", "=", "10000", "\n", "\n", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cmc.ExperimentWrapper.get_configspace": [[27, 30], ["ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace"], "methods", ["None"], ["", "def", "get_configspace", "(", "self", ")", ":", "\n", "        ", "cs", "=", "CS", ".", "ConfigurationSpace", "(", ")", "\n", "return", "cs", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cmc.ExperimentWrapper.get_specific_config": [[31, 37], ["copy.deepcopy"], "methods", ["None"], ["", "def", "get_specific_config", "(", "self", ",", "cso", ",", "default_config", ",", "budget", ")", ":", "\n", "        ", "config", "=", "deepcopy", "(", "default_config", ")", "\n", "global", "reward_env_type", "\n", "config", "[", "\"envs\"", "]", "[", "'MountainCarContinuous-v0'", "]", "[", "'reward_env_type'", "]", "=", "reward_env_type", "\n", "config", "[", "\"envs\"", "]", "[", "'MountainCarContinuous-v0'", "]", "[", "'solved_reward'", "]", "=", "sys", ".", "maxsize", "# AUC", "\n", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cmc.ExperimentWrapper.compute": [[38, 78], ["GTNC_evaluate_cmc.ExperimentWrapper.get_specific_config", "print", "print", "print", "print", "print", "print", "str", "str", "str", "str", "print", "print", "print", "print", "print", "open", "yaml.safe_load", "agents.GTN.GTN_Master", "agents.GTN.GTN_Master.run", "str", "str", "str", "float", "traceback.format_exc", "print", "str", "str", "sorted"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_step_size.ExperimentWrapper.get_specific_config", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.run"], ["", "def", "compute", "(", "self", ",", "working_dir", ",", "bohb_id", ",", "config_id", ",", "cso", ",", "budget", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "with", "open", "(", "\"default_config_cmc_reward_env.yaml\"", ",", "'r'", ")", "as", "stream", ":", "\n", "            ", "default_config", "=", "yaml", ".", "safe_load", "(", "stream", ")", "\n", "\n", "", "config", "=", "self", ".", "get_specific_config", "(", "cso", ",", "default_config", ",", "budget", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "\"START BOHB ITERATION\"", ")", "\n", "print", "(", "'CONFIG: '", "+", "str", "(", "config", ")", ")", "\n", "print", "(", "'CSO:    '", "+", "str", "(", "cso", ")", ")", "\n", "print", "(", "'BUDGET: '", "+", "str", "(", "budget", ")", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "try", ":", "\n", "            ", "gtn", "=", "GTN_Master", "(", "config", ",", "bohb_id", "=", "bohb_id", ",", "bohb_working_dir", "=", "working_dir", ")", "\n", "_", ",", "score_list", ",", "model_name", "=", "gtn", ".", "run", "(", ")", "\n", "score", "=", "-", "sorted", "(", "score_list", ")", "[", "-", "1", "]", "\n", "error", "=", "\"\"", "\n", "", "except", ":", "\n", "            ", "score", "=", "float", "(", "'-Inf'", ")", "\n", "score_list", "=", "[", "]", "\n", "model_name", "=", "None", "\n", "error", "=", "traceback", ".", "format_exc", "(", ")", "\n", "print", "(", "error", ")", "\n", "\n", "", "info", "=", "{", "}", "\n", "info", "[", "'error'", "]", "=", "str", "(", "error", ")", "\n", "info", "[", "'config'", "]", "=", "str", "(", "config", ")", "\n", "info", "[", "'score_list'", "]", "=", "str", "(", "score_list", ")", "\n", "info", "[", "'model_name'", "]", "=", "str", "(", "model_name", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "'FINAL SCORE: '", "+", "str", "(", "score", ")", ")", "\n", "print", "(", "'SCORE LIST:  '", "+", "str", "(", "score_list", ")", ")", "\n", "print", "(", "\"END BOHB ITERATION\"", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "return", "{", "\n", "\"loss\"", ":", "score", ",", "\n", "\"info\"", ":", "info", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld.idx_to_xy": [[37, 42], ["None"], "function", ["None"], ["def", "idx_to_xy", "(", "idx", ")", ":", "\n", "    ", "n", "=", "N", "\n", "x", "=", "idx", "//", "n", "\n", "y", "=", "idx", "%", "n", "\n", "return", "y", ",", "-", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld.xy_to_idx": [[44, 49], ["None"], "function", ["None"], ["", "def", "xy_to_idx", "(", "xy", ")", ":", "\n", "    ", "n", "=", "N", "\n", "y", ",", "x", "=", "xy", "\n", "obs", "=", "-", "x", "*", "n", "+", "y", "\n", "return", "obs", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld.map_intensity_to_color": [[51, 61], ["int", "int", "int", "hex", "hex", "hex", "int", "int", "int"], "function", ["None"], ["", "def", "map_intensity_to_color", "(", "intensity", ")", ":", "\n", "    ", "base_r", "=", "int", "(", "COLOR_R", ",", "16", ")", "\n", "base_g", "=", "int", "(", "COLOR_G", ",", "16", ")", "\n", "base_b", "=", "int", "(", "COLOR_B", ",", "16", ")", "\n", "\n", "inter_r", "=", "hex", "(", "int", "(", "255.0", "+", "(", "base_r", "-", "255.0", ")", "*", "intensity", ")", ")", "[", "2", ":", "]", "\n", "inter_g", "=", "hex", "(", "int", "(", "255.0", "+", "(", "base_g", "-", "255.0", ")", "*", "intensity", ")", ")", "[", "2", ":", "]", "\n", "inter_b", "=", "hex", "(", "int", "(", "255.0", "+", "(", "base_b", "-", "255.0", ")", "*", "intensity", ")", ")", "[", "2", ":", "]", "\n", "\n", "return", "'#'", "+", "inter_r", "+", "inter_g", "+", "inter_b", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld.plot_tiles": [[63, 71], ["range", "GTNC_visualize_gridworld.idx_to_xy", "matplotlib.plot"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld_heatmap.idx_to_xy"], ["", "def", "plot_tiles", "(", "length", ",", "n_tot", ")", ":", "\n", "    ", "for", "idx", "in", "range", "(", "n_tot", ")", ":", "\n", "        ", "center", "=", "idx_to_xy", "(", "idx", ")", "\n", "x1", "=", "center", "[", "0", "]", "-", "length", "/", "2", "\n", "x2", "=", "center", "[", "0", "]", "+", "length", "/", "2", "\n", "y1", "=", "center", "[", "1", "]", "-", "length", "/", "2", "\n", "y2", "=", "center", "[", "1", "]", "+", "length", "/", "2", "\n", "plt", ".", "plot", "(", "[", "x1", ",", "x1", ",", "x2", ",", "x2", ",", "x1", "]", ",", "[", "y1", ",", "y2", ",", "y2", ",", "y1", ",", "y1", "]", ",", "color", "=", "COLOR", ",", "linewidth", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld.plot_filled_rectangle": [[73, 78], ["matplotlib.Rectangle", "matplotlib.gca().add_patch", "GTNC_visualize_gridworld.map_intensity_to_color", "matplotlib.gca"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld.map_intensity_to_color"], ["", "", "def", "plot_filled_rectangle", "(", "center", ",", "length", ",", "intensity", ")", ":", "\n", "    ", "x", "=", "center", "[", "0", "]", "-", "length", "/", "2", "\n", "y", "=", "center", "[", "1", "]", "-", "length", "/", "2", "\n", "rect", "=", "plt", ".", "Rectangle", "(", "(", "x", ",", "y", ")", ",", "length", ",", "length", ",", "facecolor", "=", "map_intensity_to_color", "(", "intensity", ")", ",", "fill", "=", "True", ")", "\n", "plt", ".", "gca", "(", ")", ".", "add_patch", "(", "rect", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld.plot_filled_circle": [[80, 83], ["matplotlib.Circle", "matplotlib.gca().add_patch", "GTNC_visualize_gridworld.map_intensity_to_color", "matplotlib.gca"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld.map_intensity_to_color"], ["", "def", "plot_filled_circle", "(", "center", ",", "diameter", ",", "intensity", ")", ":", "\n", "    ", "rect", "=", "plt", ".", "Circle", "(", "center", ",", "diameter", "/", "2", ",", "edgecolor", "=", "'k'", ",", "linewidth", "=", "0.2", ",", "facecolor", "=", "map_intensity_to_color", "(", "intensity", ")", ",", "fill", "=", "True", ")", "\n", "plt", ".", "gca", "(", ")", ".", "add_patch", "(", "rect", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld.plot_tile_numbers": [[85, 94], ["range", "GTNC_visualize_gridworld.idx_to_xy", "str", "matplotlib.gca().text", "str", "matplotlib.gca", "str"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld_heatmap.idx_to_xy"], ["", "def", "plot_tile_numbers", "(", "n_tot", ")", ":", "\n", "    ", "for", "idx", "in", "range", "(", "n_tot", ")", ":", "\n", "        ", "center", "=", "idx_to_xy", "(", "idx", ")", "\n", "text", "=", "str", "(", "idx", ")", "\n", "if", "idx", "==", "0", ":", "\n", "            ", "text", "=", "str", "(", "idx", ")", "+", "'(S)'", "\n", "", "elif", "idx", "==", "n_tot", "-", "1", ":", "\n", "            ", "text", "=", "str", "(", "idx", ")", "+", "'(G)'", "\n", "", "plt", ".", "gca", "(", ")", ".", "text", "(", "center", "[", "0", "]", ",", "center", "[", "1", "]", ",", "text", ",", "fontsize", "=", "FONTSIZE_LARGE", ",", "fontweight", "=", "'bold'", ",", "ha", "=", "'center'", ",", "va", "=", "'center'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld.load_envs_and_config": [[96, 109], ["os.path.join", "torch.load", "envs.env_factory.EnvFactory", "envs.env_factory.EnvFactory.generate_virtual_env", "env_factory.generate_virtual_env.load_state_dict", "envs.env_factory.EnvFactory.generate_real_env", "file_name.split", "grid_size.split", "int", "int"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_virtual_env", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_real_env"], ["", "", "def", "load_envs_and_config", "(", "dir", ",", "file_name", ")", ":", "\n", "    ", "file_path", "=", "os", ".", "path", ".", "join", "(", "dir", ",", "file_name", ")", "\n", "save_dict", "=", "torch", ".", "load", "(", "file_path", ")", "\n", "config", "=", "save_dict", "[", "'config'", "]", "\n", "\n", "env_factory", "=", "EnvFactory", "(", "config", "=", "config", ")", "\n", "virtual_env", "=", "env_factory", ".", "generate_virtual_env", "(", ")", "\n", "virtual_env", ".", "load_state_dict", "(", "save_dict", "[", "'model'", "]", ")", "\n", "real_env", "=", "env_factory", ".", "generate_real_env", "(", ")", "\n", "\n", "grid_size", ",", "_", ",", "_", "=", "file_name", ".", "split", "(", "'_'", ")", "\n", "M", ",", "N", "=", "grid_size", ".", "split", "(", "'x'", ")", "\n", "return", "virtual_env", ",", "real_env", ",", "config", ",", "int", "(", "M", ")", ",", "int", "(", "N", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld.convert_to_int_list": [[111, 113], ["elem.item", "tensor_list.int"], "function", ["None"], ["", "def", "convert_to_int_list", "(", "tensor_list", ")", ":", "\n", "    ", "return", "[", "elem", ".", "item", "(", ")", "for", "elem", "in", "tensor_list", ".", "int", "(", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld.convert_to_float_list": [[115, 117], ["elem.item", "tensor_list.float"], "function", ["None"], ["", "def", "convert_to_float_list", "(", "tensor_list", ")", ":", "\n", "    ", "return", "[", "elem", ".", "item", "(", ")", "for", "elem", "in", "tensor_list", ".", "float", "(", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld.convert_replay_buffer": [[119, 155], ["collections.defaultdict", "replay_buffer.get_all", "GTNC_visualize_gridworld.convert_to_int_list", "GTNC_visualize_gridworld.convert_to_int_list", "GTNC_visualize_gridworld.convert_to_int_list", "GTNC_visualize_gridworld.convert_to_float_list", "zip", "collections.defaultdict.items", "state_dict.items", "[].append", "[].append", "set", "numpy.array", "numpy.array", "numpy.sum", "numpy.sum", "numpy.sum", "len"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.None.utils.ReplayBuffer.get_all", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld.convert_to_int_list", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld.convert_to_int_list", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld.convert_to_int_list", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld.convert_to_float_list"], ["", "def", "convert_replay_buffer", "(", "replay_buffer", ")", ":", "\n", "    ", "rb_dict", "=", "defaultdict", "(", "dict", ")", "\n", "\n", "states", ",", "actions", ",", "next_states", ",", "rewards", ",", "dones", "=", "replay_buffer", ".", "get_all", "(", ")", "\n", "\n", "states", "=", "convert_to_int_list", "(", "states", ")", "\n", "actions", "=", "convert_to_int_list", "(", "actions", ")", "\n", "next_states", "=", "convert_to_int_list", "(", "next_states", ")", "\n", "rewards", "=", "convert_to_float_list", "(", "rewards", ")", "\n", "\n", "for", "state", ",", "action", ",", "next_state", ",", "reward", "in", "zip", "(", "states", ",", "actions", ",", "next_states", ",", "rewards", ")", ":", "\n", "        ", "if", "action", "not", "in", "rb_dict", "[", "state", "]", ":", "\n", "            ", "rb_dict", "[", "state", "]", "[", "action", "]", "=", "{", "'next_states'", ":", "[", "next_state", "]", ",", "'rewards'", ":", "[", "reward", "]", "}", "\n", "", "else", ":", "\n", "            ", "rb_dict", "[", "state", "]", "[", "action", "]", "[", "'next_states'", "]", ".", "append", "(", "next_state", ")", "\n", "rb_dict", "[", "state", "]", "[", "action", "]", "[", "'rewards'", "]", ".", "append", "(", "reward", ")", "\n", "\n", "", "", "for", "state", ",", "state_dict", "in", "rb_dict", ".", "items", "(", ")", ":", "\n", "        ", "for", "action", ",", "action_dict", "in", "state_dict", ".", "items", "(", ")", ":", "\n", "            ", "action_dict", "[", "'avg_rewards'", "]", "=", "{", "}", "\n", "action_dict", "[", "'next_states_prob'", "]", "=", "{", "}", "\n", "next_states", "=", "action_dict", "[", "'next_states'", "]", "\n", "rewards", "=", "action_dict", "[", "'rewards'", "]", "\n", "next_states_set", "=", "set", "(", "next_states", ")", "\n", "\n", "next_states_numpy", "=", "np", ".", "array", "(", "next_states", ",", "dtype", "=", "int", ")", "\n", "rewards_numpy", "=", "np", ".", "array", "(", "rewards", ")", "\n", "\n", "for", "next_state", "in", "next_states_set", ":", "\n", "                ", "next_state_idx", "=", "next_states_numpy", "==", "next_state", "\n", "next_state_prob", "=", "np", ".", "sum", "(", "next_state_idx", ")", "/", "len", "(", "states", ")", "*", "NEXT_STATE_PROB_FACTOR", "\n", "avg_reward", "=", "np", ".", "sum", "(", "next_state_idx", "*", "rewards_numpy", ")", "/", "np", ".", "sum", "(", "next_state_idx", ")", "\n", "action_dict", "[", "'next_states_prob'", "]", "[", "next_state", "]", "=", "next_state_prob", "\n", "action_dict", "[", "'avg_rewards'", "]", "[", "next_state", "]", "=", "avg_reward", "\n", "\n", "", "", "", "return", "rb_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld.is_correct_transition": [[157, 161], ["real_env.env.env._obs_to_state", "real_env.env.step"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.gridworld.GridworldEnv._obs_to_state", "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditEnv.step"], ["", "def", "is_correct_transition", "(", "state", ",", "action", ",", "next_state", ",", "real_env", ")", ":", "\n", "    ", "real_env", ".", "env", ".", "env", ".", "state", "=", "real_env", ".", "env", ".", "env", ".", "_obs_to_state", "(", "state", ")", "\n", "next_state_real", ",", "_", ",", "_", ",", "_", "=", "real_env", ".", "env", ".", "step", "(", "action", ")", "\n", "return", "next_state_real", "==", "next_state", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld.plot_agent_behaviour": [[163, 225], ["rb_dict.items", "rb_dict.items", "state_dict.items", "min", "max", "GTNC_visualize_gridworld.idx_to_xy", "state_dict.items", "min", "max", "min", "max", "len", "enumerate", "min", "max", "sorted", "GTNC_visualize_gridworld.plot_filled_circle", "GTNC_visualize_gridworld.plot_filled_circle", "GTNC_visualize_gridworld.plot_filled_circle", "GTNC_visualize_gridworld.is_correct_transition", "matplotlib.gca().text", "action_dict[].values", "action_dict[].values", "range", "matplotlib.gca", "NotImplementedError", "str"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld_heatmap.idx_to_xy", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld.plot_filled_circle", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld.plot_filled_circle", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld.plot_filled_circle", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld.is_correct_transition"], ["", "def", "plot_agent_behaviour", "(", "rb_dict", ",", "q_table", ",", "real_env", ")", ":", "\n", "    ", "min_reward", "=", "+", "1e9", "\n", "max_reward", "=", "-", "1e9", "\n", "for", "state", ",", "state_dict", "in", "rb_dict", ".", "items", "(", ")", ":", "\n", "        ", "for", "action", ",", "action_dict", "in", "state_dict", ".", "items", "(", ")", ":", "\n", "            ", "min_reward", "=", "min", "(", "min_reward", ",", "min", "(", "action_dict", "[", "'avg_rewards'", "]", ".", "values", "(", ")", ")", ")", "\n", "max_reward", "=", "max", "(", "max_reward", ",", "max", "(", "action_dict", "[", "'avg_rewards'", "]", ".", "values", "(", ")", ")", ")", "\n", "\n", "# print(min_reward)", "\n", "# print(max_reward)", "\n", "\n", "", "", "min_q", "=", "+", "1e9", "\n", "max_q", "=", "-", "1e9", "\n", "\n", "for", "q_vals", "in", "q_table", ":", "\n", "        ", "min_q", "=", "min", "(", "min_q", ",", "min", "(", "q_vals", ")", ")", "\n", "max_q", "=", "max", "(", "max_q", ",", "max", "(", "q_vals", ")", ")", "\n", "\n", "# print(min_q)", "\n", "# print(max_q)", "\n", "\n", "", "for", "state", ",", "state_dict", "in", "rb_dict", ".", "items", "(", ")", ":", "\n", "        ", "x_state", ",", "y_state", "=", "idx_to_xy", "(", "state", ")", "\n", "\n", "for", "action", ",", "action_dict", "in", "state_dict", ".", "items", "(", ")", ":", "\n", "            ", "x_offset", "=", "0", "\n", "y_offset", "=", "0", "\n", "if", "action", "==", "G_UP", ":", "\n", "                ", "y_offset", "=", "+", "NS_OFFSET", "\n", "", "elif", "action", "==", "G_DOWN", ":", "\n", "                ", "y_offset", "=", "-", "NS_OFFSET", "\n", "", "elif", "action", "==", "G_RIGHT", ":", "\n", "                ", "x_offset", "=", "+", "NS_OFFSET", "\n", "", "elif", "action", "==", "G_LEFT", ":", "\n", "                ", "x_offset", "=", "-", "NS_OFFSET", "\n", "", "else", ":", "\n", "                ", "raise", "NotImplementedError", "(", "'Unknown action: '", "+", "str", "(", "action", ")", ")", "\n", "\n", "", "next_states_prob", "=", "action_dict", "[", "'next_states_prob'", "]", "\n", "avg_rewards", "=", "action_dict", "[", "'avg_rewards'", "]", "\n", "\n", "n", "=", "len", "(", "next_states_prob", ")", "\n", "offset", "=", "[", "NS_DIFF", "*", "(", "elem", "-", "(", "n", "-", "1", ")", "/", "2", ")", "for", "elem", "in", "range", "(", "n", ")", "]", "\n", "# print('{} {} {}'.format(state, action, next_states_prob))", "\n", "\n", "for", "i", ",", "next_state", "in", "enumerate", "(", "sorted", "(", "next_states_prob", ")", ")", ":", "\n", "                ", "x_pos", "=", "x_state", "+", "x_offset", "+", "(", "y_offset", "!=", "0", ")", "*", "offset", "[", "i", "]", "\n", "y_pos", "=", "y_state", "+", "y_offset", "+", "(", "y_offset", "==", "0", ")", "*", "(", "-", "offset", "[", "i", "]", ")", "\n", "\n", "mapped_reward", "=", "(", "avg_rewards", "[", "next_state", "]", "-", "min_reward", ")", "/", "(", "max_reward", "-", "min_reward", ")", "\n", "mapped_q", "=", "(", "q_table", "[", "state", "]", "[", "action", "]", "-", "min_q", ")", "/", "(", "max_q", "-", "min_q", ")", "\n", "# print('----')", "\n", "# print(mapped_reward)", "\n", "# print(next_states_prob[next_state])", "\n", "plot_filled_circle", "(", "(", "x_pos", ",", "y_pos", ")", ",", "RADIUS_NEXT_STATE_PERC", ",", "intensity", "=", "next_states_prob", "[", "next_state", "]", "*", "INTENSITY_FACTOR", ")", "\n", "plot_filled_circle", "(", "(", "x_pos", ",", "y_pos", ")", ",", "RADIUS_REWARD", ",", "intensity", "=", "mapped_reward", "*", "INTENSITY_FACTOR", ")", "\n", "plot_filled_circle", "(", "(", "x_pos", ",", "y_pos", ")", ",", "RADIUS_Q", ",", "intensity", "=", "mapped_q", "*", "INTENSITY_FACTOR", ")", "\n", "if", "is_correct_transition", "(", "real_env", "=", "real_env", ",", "state", "=", "state", ",", "action", "=", "action", ",", "next_state", "=", "next_state", ")", ":", "\n", "                    ", "text_color", "=", "'r'", "\n", "", "else", ":", "\n", "                    ", "text_color", "=", "'k'", "\n", "", "plt", ".", "gca", "(", ")", ".", "text", "(", "x_pos", ",", "y_pos", "-", "0.008", ",", "next_state", ",", "color", "=", "text_color", ",", "fontsize", "=", "FONTSIZE_SMALL", ",", "ha", "=", "'center'", ",", "va", "=", "'center'", ")", "\n", "# print(next_states_prob)", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld.merge_q_tables": [[229, 242], ["len", "range", "print", "range", "len", "range", "len", "range", "len", "range", "len", "len", "len"], "function", ["None"], ["", "", "", "", "def", "merge_q_tables", "(", "q_tables", ")", ":", "\n", "    ", "n", "=", "len", "(", "q_tables", ")", "\n", "q_table", "=", "[", "[", "0", "]", "*", "len", "(", "q_tables", "[", "0", "]", "[", "0", "]", ")", "for", "_", "in", "range", "(", "len", "(", "q_tables", "[", "0", "]", ")", ")", "]", "\n", "for", "table", "in", "q_tables", ":", "\n", "        ", "print", "(", "table", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "table", ")", ")", ":", "\n", "            ", "for", "k", "in", "range", "(", "len", "(", "table", "[", "0", "]", ")", ")", ":", "\n", "                ", "q_table", "[", "i", "]", "[", "k", "]", "+=", "table", "[", "i", "]", "[", "k", "]", "\n", "", "", "", "for", "i", "in", "range", "(", "len", "(", "q_table", ")", ")", ":", "\n", "        ", "for", "k", "in", "range", "(", "len", "(", "q_table", "[", "0", "]", ")", ")", ":", "\n", "            ", "q_table", "[", "i", "]", "[", "k", "]", "/=", "n", "\n", "\n", "", "", "return", "q_table", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_halfcheetah_transfer_algo.get_data": [[74, 123], ["float", "print", "print", "torch.load", "list_data.append", "sums_eps_len_per_model.append", "sum", "sum", "numpy.zeros", "enumerate", "numpy.mean", "numpy.std", "proc_data.append", "print", "sums_eps_len.append", "sums_eps_len_per_model_i.append", "min", "zip", "range", "numpy.array", "sum", "sum", "sum", "sum", "numpy.asarray", "numpy.asarray", "len", "len", "concat_list.append"], "function", ["None"], ["def", "get_data", "(", ")", ":", "\n", "    ", "list_data", "=", "[", "]", "\n", "for", "log_file", "in", "LOG_FILES", ":", "\n", "        ", "data", "=", "torch", ".", "load", "(", "log_file", ")", "\n", "list_data", ".", "append", "(", "(", "data", "[", "'reward_list'", "]", ",", "data", "[", "'episode_length_list'", "]", ")", ")", "\n", "model_num", "=", "data", "[", "'model_num'", "]", "\n", "model_agents", "=", "data", "[", "'model_agents'", "]", "\n", "\n", "", "sums_eps_len", "=", "[", "]", "\n", "sums_eps_len_per_model", "=", "[", "]", "\n", "min_steps", "=", "float", "(", "'Inf'", ")", "\n", "# get minimum number of evaluations", "\n", "for", "reward_list", ",", "episode_length_list", "in", "list_data", ":", "\n", "        ", "sums_eps_len_per_model_i", "=", "[", "]", "\n", "for", "episode_lengths", "in", "episode_length_list", ":", "\n", "            ", "print", "(", "sum", "(", "episode_lengths", ")", ")", "\n", "sums_eps_len", ".", "append", "(", "sum", "(", "episode_lengths", ")", ")", "\n", "sums_eps_len_per_model_i", ".", "append", "(", "sum", "(", "episode_lengths", ")", ")", "\n", "min_steps", "=", "min", "(", "min_steps", ",", "sum", "(", "episode_lengths", ")", ")", "\n", "", "sums_eps_len_per_model", ".", "append", "(", "sums_eps_len_per_model_i", ")", "\n", "\n", "", "print", "(", "\"# of episode lens > MIN_STEPS: \"", ",", "sum", "(", "np", ".", "asarray", "(", "sums_eps_len", ")", ">=", "MIN_STEPS", ")", ")", "\n", "print", "(", "\"# of episode lens < MIN_STEPS: \"", ",", "sum", "(", "np", ".", "asarray", "(", "sums_eps_len", ")", "<", "MIN_STEPS", ")", ")", "\n", "# convert data from episodes to steps", "\n", "proc_data", "=", "[", "]", "\n", "\n", "for", "reward_list", ",", "episode_length_list", "in", "list_data", ":", "\n", "        ", "np_data", "=", "np", ".", "zeros", "(", "[", "model_num", "*", "model_agents", ",", "min_steps", "]", ")", "\n", "\n", "for", "it", ",", "data", "in", "enumerate", "(", "zip", "(", "reward_list", ",", "episode_length_list", ")", ")", ":", "\n", "            ", "rewards", ",", "episode_lengths", "=", "data", "\n", "\n", "concat_list", "=", "[", "]", "\n", "rewards", "=", "rewards", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "episode_lengths", ")", ")", ":", "\n", "                ", "concat_list", "+=", "[", "rewards", "[", "i", "]", "]", "*", "episode_lengths", "[", "i", "]", "\n", "\n", "", "while", "len", "(", "concat_list", ")", "<", "min_steps", ":", "\n", "                ", "concat_list", ".", "append", "(", "concat_list", "[", "-", "1", "]", ")", "\n", "\n", "", "np_data", "[", "it", "]", "=", "np", ".", "array", "(", "concat_list", "[", ":", "min_steps", "]", ")", "\n", "\n", "", "mean", "=", "np", ".", "mean", "(", "np_data", ",", "axis", "=", "0", ")", "\n", "std", "=", "np", ".", "std", "(", "np_data", ",", "axis", "=", "0", ")", "\n", "\n", "proc_data", ".", "append", "(", "(", "mean", ",", "std", ")", ")", "\n", "\n", "", "return", "proc_data", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_halfcheetah_transfer_algo.plot_data": [[125, 188], ["matplotlib.subplots", "enumerate", "enumerate", "matplotlib.legend", "matplotlib.subplots_adjust", "matplotlib.title", "matplotlib.xlabel", "matplotlib.xlim", "matplotlib.ylim", "matplotlib.ylabel", "os.path.dirname", "matplotlib.savefig", "matplotlib.show", "numpy.linspace", "numpy.linspace", "make_interp_spline", "make_interp_spline.", "legobj.set_linewidth", "os.path.join", "len", "len", "matplotlib.plot", "matplotlib.fill_between", "len", "matplotlib.plot", "matplotlib.plot", "matplotlib.fill_between", "matplotlib.fill_between", "range", "len", "range", "range", "len", "len"], "function", ["None"], ["", "def", "plot_data", "(", "proc_data", ",", "savefig_name", ")", ":", "\n", "    ", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", "dpi", "=", "600", ",", "figsize", "=", "(", "5", ",", "4", ")", ")", "\n", "colors", "=", "[", "]", "\n", "#", "\n", "# for mean, _ in data_w:", "\n", "#     plt.plot(mean_w)", "\n", "#     colors.append(plt.gca().lines[-1].get_color())", "\n", "\n", "from", "scipy", ".", "interpolate", "import", "make_interp_spline", "\n", "\n", "for", "i", ",", "data", "in", "enumerate", "(", "proc_data", ")", ":", "\n", "        ", "mean", ",", "std", "=", "data", "\n", "x", "=", "np", ".", "linspace", "(", "1", ",", "len", "(", "mean", ")", ",", "num", "=", "len", "(", "mean", ")", ")", "\n", "xnew", "=", "np", ".", "linspace", "(", "1", ",", "len", "(", "mean", ")", ",", "num", "=", "300", ")", "\n", "spl", "=", "make_interp_spline", "(", "x", ",", "mean", ",", "k", "=", "3", ")", "\n", "mean_smooth", "=", "spl", "(", "xnew", ")", "\n", "\n", "if", "i", "==", "10", ":", "\n", "# plt.plot(mean, color='#575757', linewidth=.8)", "\n", "            ", "plt", ".", "plot", "(", "xnew", ",", "mean_smooth", ",", "color", "=", "'#575757'", ",", "linewidth", "=", "1", ")", "\n", "", "elif", "i", "==", "11", ":", "\n", "# plt.plot(mean, color='#EBBB00', linewidth=.8)", "\n", "            ", "plt", ".", "plot", "(", "xnew", ",", "mean_smooth", ",", "color", "=", "'#EBBB00'", ",", "linewidth", "=", "1", ")", "\n", "", "else", ":", "\n", "# plt.plot(mean, linewidth=.8)", "\n", "            ", "plt", ".", "plot", "(", "xnew", ",", "mean_smooth", ",", "linewidth", "=", "1", ")", "\n", "\n", "", "", "for", "i", ",", "data", "in", "enumerate", "(", "proc_data", ")", ":", "\n", "        ", "mean", ",", "std", "=", "data", "\n", "\n", "# x = np.linspace(1, len(mean), num=len(mean))", "\n", "# xnew = np.linspace(1, len(mean), num=300)", "\n", "# spl = make_interp_spline(x, mean, k=3)", "\n", "# mean_smooth = spl(xnew)", "\n", "#", "\n", "# spl_std = make_interp_spline(x, std, k=3)", "\n", "# std_smooth = spl_std(xnew)", "\n", "\n", "if", "i", "==", "10", ":", "\n", "            ", "plt", ".", "fill_between", "(", "x", "=", "range", "(", "len", "(", "mean", ")", ")", ",", "y1", "=", "mean", "-", "std", "*", "STD_MULT", ",", "y2", "=", "mean", "+", "std", "*", "STD_MULT", ",", "alpha", "=", "0.2", ",", "color", "=", "'#575757'", ")", "\n", "# plt.fill_between(x=range(len(mean)), y1=mean - std * STD_MULT, y2=mean + std * STD_MULT, alpha=0.1, color='#575757')", "\n", "", "elif", "i", "==", "11", ":", "\n", "            ", "plt", ".", "fill_between", "(", "x", "=", "range", "(", "len", "(", "mean", ")", ")", ",", "y1", "=", "mean", "-", "std", "*", "STD_MULT", ",", "y2", "=", "mean", "+", "std", "*", "STD_MULT", ",", "alpha", "=", "0.2", ",", "color", "=", "'#EBBB00'", ")", "\n", "# plt.fill_between(x=range(len(mean)), y1=mean - std * STD_MULT, y2=mean + std * STD_MULT, alpha=0.1, color='#EBBB00')", "\n", "", "else", ":", "\n", "            ", "plt", ".", "fill_between", "(", "x", "=", "range", "(", "len", "(", "mean", ")", ")", ",", "y1", "=", "mean", "-", "std", "*", "STD_MULT", ",", "y2", "=", "mean", "+", "std", "*", "STD_MULT", ",", "alpha", "=", "0.2", ")", "\n", "# plt.fill_between(x=range(len(mean)), y1=mean - std * STD_MULT, y2=mean + std * STD_MULT, alpha=0.1)", "\n", "\n", "", "", "leg", "=", "plt", ".", "legend", "(", "LEGEND", ",", "fontsize", "=", "7", ")", "\n", "\n", "for", "legobj", "in", "leg", ".", "legendHandles", ":", "\n", "        ", "legobj", ".", "set_linewidth", "(", "2.0", ")", "\n", "\n", "", "plt", ".", "subplots_adjust", "(", "bottom", "=", "0.15", ",", "left", "=", "0.15", ")", "\n", "plt", ".", "title", "(", "'HalfCheetah-v3 Transfer'", ")", "\n", "plt", ".", "xlabel", "(", "'steps'", ")", "\n", "plt", ".", "xlim", "(", "0", ",", "MIN_STEPS", ")", "\n", "# plt.ylim(-1000, 6000)", "\n", "plt", ".", "ylim", "(", "-", "1000", ",", "3000", ")", "\n", "plt", ".", "ylabel", "(", "'cumulative reward'", ")", "\n", "base_dir", "=", "os", ".", "path", ".", "dirname", "(", "LOG_FILES", "[", "0", "]", ")", "\n", "plt", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "base_dir", ",", "savefig_name", ")", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cmc_compare_reward_envs_debug_td3.get_best_models_from_log": [[26, 50], ["hpbandster.logged_results_to_HBS_result", "hpres.logged_results_to_HBS_result.data.values", "best_models.sort", "os.path.isdir", "log_dir.replace.replace", "best_models.append", "os.path.isfile", "model_name.replace.replace"], "function", ["None"], ["def", "get_best_models_from_log", "(", "log_dir", ")", ":", "\n", "    ", "if", "not", "os", ".", "path", ".", "isdir", "(", "log_dir", ")", ":", "\n", "        ", "log_dir", "=", "log_dir", ".", "replace", "(", "'nierhoff'", ",", "'dingsda'", ")", "\n", "\n", "", "result", "=", "hpres", ".", "logged_results_to_HBS_result", "(", "log_dir", ")", "\n", "\n", "best_models", "=", "[", "]", "\n", "\n", "for", "value", "in", "result", ".", "data", ".", "values", "(", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "loss", "=", "value", ".", "results", "[", "1.0", "]", "[", "'loss'", "]", "\n", "model_name", "=", "value", ".", "results", "[", "1.0", "]", "[", "'info'", "]", "[", "'model_name'", "]", "\n", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "model_name", ")", ":", "\n", "                ", "model_name", "=", "model_name", ".", "replace", "(", "'nierhoff'", ",", "'dingsda'", ")", "\n", "", "best_models", ".", "append", "(", "(", "loss", ",", "model_name", ")", ")", "\n", "", "except", ":", "\n", "            ", "continue", "\n", "\n", "", "", "best_models", ".", "sort", "(", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ")", "\n", "# best_models.sort(key=lambda x: x[0], reverse=True)", "\n", "best_models", "=", "best_models", "[", ":", "MODEL_NUM", "]", "\n", "\n", "return", "best_models", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cmc_compare_reward_envs_debug_td3.load_envs_and_config": [[52, 65], ["torch.load", "envs.env_factory.EnvFactory", "envs.env_factory.EnvFactory.generate_reward_env", "env_factory.generate_reward_env.load_state_dict", "envs.env_factory.EnvFactory.generate_real_env"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_reward_env", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_real_env"], ["", "def", "load_envs_and_config", "(", "model_file", ")", ":", "\n", "    ", "save_dict", "=", "torch", ".", "load", "(", "model_file", ")", "\n", "\n", "config", "=", "save_dict", "[", "'config'", "]", "\n", "config", "[", "'device'", "]", "=", "'cuda'", "\n", "config", "[", "'envs'", "]", "[", "'MountainCarContinuous-v0'", "]", "[", "'solved_reward'", "]", "=", "100000", "# something big enough to prevent early out triggering", "\n", "\n", "env_factory", "=", "EnvFactory", "(", "config", "=", "config", ")", "\n", "reward_env", "=", "env_factory", ".", "generate_reward_env", "(", ")", "\n", "reward_env", ".", "load_state_dict", "(", "save_dict", "[", "'model'", "]", ")", "\n", "real_env", "=", "env_factory", ".", "generate_real_env", "(", ")", "\n", "\n", "return", "reward_env", ",", "real_env", ",", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cmc_compare_reward_envs_debug_td3.train_test_agents": [[67, 95], ["range", "agents.agent_utils.select_agent.train", "print", "rewards.append", "episode_lengths.append", "agents.agent_utils.select_agent", "agents.agent_utils.select_agent", "str"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.train", "home.repos.pwc.inspect_result.automl_learning_environments.agents.agent_utils.select_agent", "home.repos.pwc.inspect_result.automl_learning_environments.agents.agent_utils.select_agent"], ["", "def", "train_test_agents", "(", "mode", ",", "env", ",", "real_env", ",", "config", ")", ":", "\n", "    ", "rewards", "=", "[", "]", "\n", "episode_lengths", "=", "[", "]", "\n", "\n", "# settings for comparability", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'test_episodes'", "]", "=", "1", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'train_episodes'", "]", "=", "500", "\n", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'activation_fn'", "]", "=", "\"relu\"", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'lr'", "]", "=", "0.001", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'tau'", "]", "=", "0.005", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'same_action_num'", "]", "=", "2", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'policy_delay'", "]", "=", "2", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'rb_size'", "]", "=", "10000000", "\n", "# config['agents']['td3']['policy_std_clip'] = 0.9", "\n", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'print_rate'", "]", "=", "100", "\n", "\n", "for", "i", "in", "range", "(", "MODEL_AGENTS", ")", ":", "\n", "        ", "if", "mode", "==", "'-1'", ":", "\n", "            ", "agent", "=", "select_agent", "(", "config", "=", "config", ",", "agent_name", "=", "'td3_icm'", ")", "\n", "", "else", ":", "\n", "            ", "agent", "=", "select_agent", "(", "config", "=", "config", ",", "agent_name", "=", "'td3'", ")", "\n", "", "reward", ",", "episode_length", ",", "_", "=", "agent", ".", "train", "(", "env", "=", "env", ",", "test_env", "=", "real_env", ")", "\n", "print", "(", "'reward: '", "+", "str", "(", "reward", ")", ")", "\n", "rewards", ".", "append", "(", "reward", ")", "\n", "episode_lengths", ".", "append", "(", "episode_length", ")", "\n", "", "return", "rewards", ",", "episode_lengths", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cmc_compare_reward_envs_debug_td3.save_list": [[97, 108], ["os.makedirs", "datetime.datetime.now().strftime", "os.path.join", "torch.save", "datetime.datetime.now", "str"], "function", ["None"], ["", "def", "save_list", "(", "mode", ",", "config", ",", "reward_list", ",", "episode_length_list", ")", ":", "\n", "    ", "os", ".", "makedirs", "(", "SAVE_DIR", ",", "exist_ok", "=", "True", ")", "\n", "time", "=", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "\"%Y_%m_%d_%I_%M_%S\"", ")", "\n", "file_name", "=", "os", ".", "path", ".", "join", "(", "SAVE_DIR", ",", "time", "+", "\"_\"", "+", "'best'", "+", "str", "(", "mode", ")", "+", "'.pt'", ")", "\n", "save_dict", "=", "{", "}", "\n", "save_dict", "[", "'config'", "]", "=", "config", "\n", "save_dict", "[", "'model_num'", "]", "=", "MODEL_NUM", "\n", "save_dict", "[", "'model_agents'", "]", "=", "MODEL_AGENTS", "\n", "save_dict", "[", "'reward_list'", "]", "=", "reward_list", "\n", "save_dict", "[", "'episode_length_list'", "]", "=", "episode_length_list", "\n", "torch", ".", "save", "(", "save_dict", ",", "file_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cmc_compare_reward_envs_debug_td3.eval_models": [[110, 125], ["GTNC_evaluate_cmc_compare_reward_envs_debug_td3.get_best_models_from_log", "GTNC_evaluate_cmc_compare_reward_envs_debug_td3.save_list", "print", "print", "GTNC_evaluate_cmc_compare_reward_envs_debug_td3.load_envs_and_config", "GTNC_evaluate_cmc_compare_reward_envs_debug_td3.train_test_agents", "str", "str"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.get_best_models_from_log", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.save_list", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.load_envs_and_config", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.train_test_agents"], ["", "def", "eval_models", "(", "mode", ",", "log_dir", ")", ":", "\n", "    ", "best_models", "=", "get_best_models_from_log", "(", "log_dir", ")", "\n", "\n", "reward_list", "=", "[", "]", "\n", "episode_length_list", "=", "[", "]", "\n", "\n", "for", "best_reward", ",", "model_file", "in", "best_models", ":", "\n", "        ", "print", "(", "'best reward: '", "+", "str", "(", "best_reward", ")", ")", "\n", "print", "(", "'model file: '", "+", "str", "(", "model_file", ")", ")", "\n", "reward_env", ",", "real_env", ",", "config", "=", "load_envs_and_config", "(", "model_file", ")", "\n", "rewards", ",", "episode_lengths", "=", "train_test_agents", "(", "mode", "=", "mode", ",", "env", "=", "reward_env", ",", "real_env", "=", "real_env", ",", "config", "=", "config", ")", "\n", "reward_list", "+=", "rewards", "\n", "episode_length_list", "+=", "episode_lengths", "\n", "\n", "", "save_list", "(", "mode", ",", "config", ",", "reward_list", ",", "episode_length_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cmc_compare_reward_envs_debug_td3.eval_base": [[127, 140], ["GTNC_evaluate_cmc_compare_reward_envs_debug_td3.get_best_models_from_log", "range", "GTNC_evaluate_cmc_compare_reward_envs_debug_td3.save_list", "GTNC_evaluate_cmc_compare_reward_envs_debug_td3.load_envs_and_config", "GTNC_evaluate_cmc_compare_reward_envs_debug_td3.train_test_agents"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.get_best_models_from_log", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.save_list", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.load_envs_and_config", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.train_test_agents"], ["", "def", "eval_base", "(", "mode", ",", "log_dir", ")", ":", "\n", "    ", "best_models", "=", "get_best_models_from_log", "(", "log_dir", ")", "\n", "\n", "reward_list", "=", "[", "]", "\n", "episode_length_list", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "MODEL_NUM", ")", ":", "\n", "        ", "reward_env", ",", "real_env", ",", "config", "=", "load_envs_and_config", "(", "best_models", "[", "0", "]", "[", "1", "]", ")", "\n", "rewards", ",", "episode_lengths", "=", "train_test_agents", "(", "mode", "=", "mode", ",", "env", "=", "real_env", ",", "real_env", "=", "real_env", ",", "config", "=", "config", ")", "\n", "reward_list", "+=", "rewards", "\n", "episode_length_list", "+=", "episode_lengths", "\n", "\n", "", "save_list", "(", "mode", ",", "config", ",", "reward_list", ",", "episode_length_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cmc_compare_reward_envs_debug_td3.eval_icm": [[142, 155], ["GTNC_evaluate_cmc_compare_reward_envs_debug_td3.get_best_models_from_log", "range", "GTNC_evaluate_cmc_compare_reward_envs_debug_td3.save_list", "GTNC_evaluate_cmc_compare_reward_envs_debug_td3.load_envs_and_config", "GTNC_evaluate_cmc_compare_reward_envs_debug_td3.train_test_agents"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.get_best_models_from_log", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.save_list", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.load_envs_and_config", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.train_test_agents"], ["", "def", "eval_icm", "(", "mode", ",", "log_dir", ")", ":", "\n", "    ", "best_models", "=", "get_best_models_from_log", "(", "log_dir", ")", "\n", "\n", "reward_list", "=", "[", "]", "\n", "episode_length_list", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "MODEL_NUM", ")", ":", "\n", "        ", "reward_env", ",", "real_env", ",", "config", "=", "load_envs_and_config", "(", "best_models", "[", "0", "]", "[", "1", "]", ")", "\n", "rewards", ",", "episode_lengths", "=", "train_test_agents", "(", "mode", "=", "mode", ",", "env", "=", "real_env", ",", "real_env", "=", "real_env", ",", "config", "=", "config", ")", "\n", "reward_list", "+=", "rewards", "\n", "episode_length_list", "+=", "episode_lengths", "\n", "\n", "", "save_list", "(", "mode", ",", "config", ",", "reward_list", ",", "episode_length_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld_compare_reward_envs.auc": [[10, 18], ["None"], "function", ["None"], ["def", "auc", "(", ")", ":", "\n", "    ", "return", "[", "\n", "'../results/3_rn_auc/cliff_compare_reward_envs/best1.pt'", ",", "\n", "'../results/3_rn_auc/cliff_compare_reward_envs/best2.pt'", ",", "\n", "'../results/3_rn_auc/cliff_compare_reward_envs/best5.pt'", ",", "\n", "'../results/3_rn_auc/cliff_compare_reward_envs/best6.pt'", ",", "\n", "'../results/0_before_auc/cliff_compare_reward_envs/best0.pt'", ",", "\n", "'../results/0_before_auc/cliff_compare_reward_envs/best-1_opt.pt'", ",", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld_compare_reward_envs.normal": [[21, 29], ["None"], "function", ["None"], ["", "def", "normal", "(", ")", ":", "\n", "    ", "return", "[", "\n", "'../results/0_before_auc/cliff_compare_reward_envs/best1.pt'", ",", "\n", "'../results/0_before_auc/cliff_compare_reward_envs/best2.pt'", ",", "\n", "'../results/0_before_auc/cliff_compare_reward_envs/best5.pt'", ",", "\n", "'../results/0_before_auc/cliff_compare_reward_envs/best6.pt'", ",", "\n", "'../results/0_before_auc/cliff_compare_reward_envs/best0.pt'", ",", "\n", "'../results/0_before_auc/cliff_compare_reward_envs/best-1_opt.pt'", ",", "\n", "# '../results/0_before_auc/cliff_compare_reward_envs/best-1.pt'", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld_compare_reward_envs.reward_maximization": [[33, 41], ["None"], "function", ["None"], ["", "def", "reward_maximization", "(", ")", ":", "\n", "    ", "return", "[", "\n", "'../results/4_rn_reward/cliff_compare_reward_envs/best1.pt'", ",", "\n", "'../results/4_rn_reward/cliff_compare_reward_envs/best2.pt'", ",", "\n", "'../results/4_rn_reward/cliff_compare_reward_envs/best5.pt'", ",", "\n", "'../results/4_rn_reward/cliff_compare_reward_envs/best6.pt'", ",", "\n", "'../results/0_before_auc/cliff_compare_reward_envs/best0.pt'", ",", "\n", "'../results/0_before_auc/cliff_compare_reward_envs/best-1_opt.pt'", ",", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld_compare_reward_envs.get_data": [[65, 115], ["float", "print", "print", "max", "torch.load", "list_data.append", "sums_eps_len_per_model.append", "sum", "sum", "numpy.zeros", "enumerate", "numpy.mean", "numpy.std", "proc_data.append", "print", "sums_eps_len.append", "sums_eps_len_per_model_i.append", "min", "zip", "range", "numpy.array", "sum", "sum", "sum", "sum", "numpy.asarray", "numpy.asarray", "len", "len", "concat_list.append"], "function", ["None"], ["def", "get_data", "(", ")", ":", "\n", "    ", "list_data", "=", "[", "]", "\n", "for", "log_file", "in", "LOG_FILES", ":", "\n", "        ", "data", "=", "torch", ".", "load", "(", "log_file", ")", "\n", "list_data", ".", "append", "(", "(", "data", "[", "'reward_list'", "]", ",", "data", "[", "'episode_length_list'", "]", ")", ")", "\n", "model_num", "=", "data", "[", "'model_num'", "]", "\n", "model_agents", "=", "data", "[", "'model_agents'", "]", "\n", "\n", "", "sums_eps_len", "=", "[", "]", "\n", "sums_eps_len_per_model", "=", "[", "]", "\n", "min_steps", "=", "float", "(", "'Inf'", ")", "\n", "# get minimum number of evaluations", "\n", "for", "reward_list", ",", "episode_length_list", "in", "list_data", ":", "\n", "        ", "sums_eps_len_per_model_i", "=", "[", "]", "\n", "for", "episode_lengths", "in", "episode_length_list", ":", "\n", "            ", "print", "(", "sum", "(", "episode_lengths", ")", ")", "\n", "sums_eps_len", ".", "append", "(", "sum", "(", "episode_lengths", ")", ")", "\n", "sums_eps_len_per_model_i", ".", "append", "(", "sum", "(", "episode_lengths", ")", ")", "\n", "min_steps", "=", "min", "(", "min_steps", ",", "sum", "(", "episode_lengths", ")", ")", "\n", "", "sums_eps_len_per_model", ".", "append", "(", "sums_eps_len_per_model_i", ")", "\n", "\n", "", "print", "(", "\"# of episode lens > MIN_STEPS: \"", ",", "sum", "(", "np", ".", "asarray", "(", "sums_eps_len", ")", ">=", "MIN_STEPS", ")", ")", "\n", "print", "(", "\"# of episode lens < MIN_STEPS: \"", ",", "sum", "(", "np", ".", "asarray", "(", "sums_eps_len", ")", "<", "MIN_STEPS", ")", ")", "\n", "min_steps", "=", "max", "(", "min_steps", ",", "MIN_STEPS", ")", "\n", "# convert data from episodes to steps", "\n", "proc_data", "=", "[", "]", "\n", "\n", "for", "reward_list", ",", "episode_length_list", "in", "list_data", ":", "\n", "        ", "np_data", "=", "np", ".", "zeros", "(", "[", "model_num", "*", "model_agents", ",", "min_steps", "]", ")", "\n", "\n", "for", "it", ",", "data", "in", "enumerate", "(", "zip", "(", "reward_list", ",", "episode_length_list", ")", ")", ":", "\n", "            ", "rewards", ",", "episode_lengths", "=", "data", "\n", "\n", "concat_list", "=", "[", "]", "\n", "rewards", "=", "rewards", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "episode_lengths", ")", ")", ":", "\n", "                ", "concat_list", "+=", "[", "rewards", "[", "i", "]", "]", "*", "episode_lengths", "[", "i", "]", "\n", "\n", "", "while", "len", "(", "concat_list", ")", "<", "min_steps", ":", "\n", "                ", "concat_list", ".", "append", "(", "concat_list", "[", "-", "1", "]", ")", "\n", "\n", "", "np_data", "[", "it", "]", "=", "np", ".", "array", "(", "concat_list", "[", ":", "min_steps", "]", ")", "\n", "\n", "", "mean", "=", "np", ".", "mean", "(", "np_data", ",", "axis", "=", "0", ")", "\n", "std", "=", "np", ".", "std", "(", "np_data", ",", "axis", "=", "0", ")", "\n", "\n", "proc_data", ".", "append", "(", "(", "mean", ",", "std", ")", ")", "\n", "\n", "", "return", "proc_data", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld_compare_reward_envs.plot_data": [[117, 141], ["matplotlib.subplots", "matplotlib.legend", "matplotlib.subplots_adjust", "matplotlib.title", "matplotlib.xlabel", "matplotlib.xlim", "matplotlib.ylim", "matplotlib.ylabel", "os.path.dirname", "matplotlib.savefig", "matplotlib.show", "matplotlib.plot", "matplotlib.fill_between", "legobj.set_linewidth", "os.path.join", "range", "len"], "function", ["None"], ["", "def", "plot_data", "(", "proc_data", ",", "savefig_name", ")", ":", "\n", "    ", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", "dpi", "=", "600", ",", "figsize", "=", "(", "5", ",", "4", ")", ")", "\n", "\n", "for", "mean", ",", "std", "in", "proc_data", ":", "\n", "        ", "plt", ".", "plot", "(", "mean", ",", "linewidth", "=", "1", ")", "\n", "\n", "", "for", "mean", ",", "std", "in", "proc_data", ":", "\n", "        ", "plt", ".", "fill_between", "(", "x", "=", "range", "(", "len", "(", "mean", ")", ")", ",", "y1", "=", "mean", "-", "std", "*", "STD_MULT", ",", "y2", "=", "mean", "+", "std", "*", "STD_MULT", ",", "alpha", "=", "0.3", ")", "\n", "\n", "", "leg", "=", "plt", ".", "legend", "(", "LEGEND", ",", "fontsize", "=", "9", ")", "\n", "\n", "for", "legobj", "in", "leg", ".", "legendHandles", ":", "\n", "        ", "legobj", ".", "set_linewidth", "(", "2.0", ")", "\n", "\n", "# plt.xlim(0,99)", "\n", "", "plt", ".", "subplots_adjust", "(", "bottom", "=", "0.15", ",", "left", "=", "0.15", ")", "\n", "plt", ".", "title", "(", "'Cliff Walking'", ")", "\n", "plt", ".", "xlabel", "(", "'steps'", ")", "\n", "plt", ".", "xlim", "(", "0", ",", "MIN_STEPS", ")", "\n", "plt", ".", "ylim", "(", "-", "100", ",", "-", "10", ")", "\n", "plt", ".", "ylabel", "(", "'cumulative reward'", ")", "\n", "base_dir", "=", "os", ".", "path", ".", "dirname", "(", "LOG_FILES", "[", "0", "]", ")", "\n", "plt", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "base_dir", ",", "savefig_name", ")", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_mbrl_baseline.fit_mbrl_baseline": [[15, 22], ["envs.env_factory.EnvFactory", "envs.env_factory.EnvFactory.generate_virtual_env", "open", "yaml.safe_load"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_virtual_env"], ["def", "fit_mbrl_baseline", "(", ")", ":", "\n", "    ", "with", "open", "(", "\"default_config_cartpole.yaml\"", ",", "'r'", ")", "as", "stream", ":", "\n", "        ", "default_config", "=", "yaml", ".", "safe_load", "(", "stream", ")", "\n", "", "env_factory", "=", "EnvFactory", "(", "config", "=", "default_config", ")", "\n", "synthetic_env", "=", "env_factory", ".", "generate_virtual_env", "(", ")", "\n", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_mbrl_baseline.evaluate_mbrl_baseline": [[24, 32], ["envs.env_factory.EnvFactory", "envs.env_factory.EnvFactory.generate_virtual_env", "env_factory.generate_virtual_env.load_state_dict", "envs.env_factory.EnvFactory.generate_real_env", "open", "yaml.safe_load", "mbrl_model.parameters"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_virtual_env", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_real_env"], ["", "def", "evaluate_mbrl_baseline", "(", "mbrl_model", ")", ":", "\n", "    ", "with", "open", "(", "\"default_config_cartpole.yaml\"", ",", "'r'", ")", "as", "stream", ":", "\n", "        ", "default_config", "=", "yaml", ".", "safe_load", "(", "stream", ")", "\n", "\n", "", "env_factory", "=", "EnvFactory", "(", "config", "=", "default_config", ")", "\n", "synthetic_env", "=", "env_factory", ".", "generate_virtual_env", "(", ")", "\n", "synthetic_env", ".", "load_state_dict", "(", "mbrl_model", ".", "parameters", "(", ")", ")", "\n", "real_env", "=", "env_factory", ".", "generate_real_env", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_mbrl_baseline.load_envs_and_config": [[34, 45], ["os.path.join", "torch.load", "envs.env_factory.EnvFactory", "envs.env_factory.EnvFactory.generate_virtual_env", "env_factory.generate_virtual_env.load_state_dict", "envs.env_factory.EnvFactory.generate_real_env"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_virtual_env", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_real_env"], ["", "def", "load_envs_and_config", "(", "file_name", ",", "model_dir", ",", "device", ")", ":", "\n", "    ", "file_path", "=", "os", ".", "path", ".", "join", "(", "model_dir", ",", "file_name", ")", "\n", "save_dict", "=", "torch", ".", "load", "(", "file_path", ")", "\n", "config", "=", "save_dict", "[", "'config'", "]", "\n", "config", "[", "'device'", "]", "=", "device", "\n", "env_factory", "=", "EnvFactory", "(", "config", "=", "config", ")", "\n", "virtual_env", "=", "env_factory", ".", "generate_virtual_env", "(", ")", "\n", "virtual_env", ".", "load_state_dict", "(", "save_dict", "[", "'model'", "]", ")", "\n", "real_env", "=", "env_factory", ".", "generate_real_env", "(", ")", "\n", "\n", "return", "virtual_env", ",", "real_env", ",", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_mbrl_baseline.train_test_agents": [[47, 71], ["range", "agents.agent_utils.select_agent", "agents.agent_utils.select_agent.train", "agents.agent_utils.select_agent.test", "print", "reward_list.append", "train_steps_needed.append", "episodes_needed.append", "str", "sum", "len"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.agents.agent_utils.select_agent", "home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.train", "home.repos.pwc.inspect_result.automl_learning_environments.agents.base_agent.BaseAgent.test"], ["", "def", "train_test_agents", "(", "train_env", ",", "test_env", ",", "config", ",", "agents_num", ")", ":", "\n", "    ", "reward_list", "=", "[", "]", "\n", "train_steps_needed", "=", "[", "]", "\n", "episodes_needed", "=", "[", "]", "\n", "\n", "# settings for comparability", "\n", "config", "[", "'agents'", "]", "[", "'ddqn_vary'", "]", "[", "'vary_hp'", "]", "=", "True", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'print_rate'", "]", "=", "10", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'early_out_num'", "]", "=", "10", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'train_episodes'", "]", "=", "1000", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'init_episodes'", "]", "=", "10", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'test_episodes'", "]", "=", "10", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'early_out_virtual_diff'", "]", "=", "0.01", "\n", "\n", "for", "i", "in", "range", "(", "agents_num", ")", ":", "\n", "        ", "agent", "=", "select_agent", "(", "config", "=", "config", ",", "agent_name", "=", "'DDQN_vary'", ")", "\n", "reward_train", ",", "episode_length", ",", "_", "=", "agent", ".", "train", "(", "env", "=", "train_env", ")", "\n", "reward", ",", "_", ",", "_", "=", "agent", ".", "test", "(", "env", "=", "test_env", ")", "\n", "print", "(", "'reward: '", "+", "str", "(", "reward", ")", ")", "\n", "reward_list", ".", "append", "(", "reward", ")", "\n", "train_steps_needed", ".", "append", "(", "[", "sum", "(", "episode_length", ")", "]", ")", "\n", "episodes_needed", ".", "append", "(", "[", "(", "len", "(", "reward_train", ")", ")", "]", ")", "\n", "\n", "", "return", "reward_list", ",", "train_steps_needed", ",", "episodes_needed", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2_correlation.load_envs_and_config": [[12, 23], ["os.path.join", "torch.load", "envs.env_factory.EnvFactory", "envs.env_factory.EnvFactory.generate_virtual_env", "env_factory.generate_virtual_env.load_state_dict", "envs.env_factory.EnvFactory.generate_real_env"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_virtual_env", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_real_env"], ["def", "load_envs_and_config", "(", "file_name", ",", "model_dir", ",", "device", ")", ":", "\n", "    ", "file_path", "=", "os", ".", "path", ".", "join", "(", "model_dir", ",", "file_name", ")", "\n", "save_dict", "=", "torch", ".", "load", "(", "file_path", ")", "\n", "config", "=", "save_dict", "[", "'config'", "]", "\n", "config", "[", "'device'", "]", "=", "device", "\n", "env_factory", "=", "EnvFactory", "(", "config", "=", "config", ")", "\n", "virtual_env", "=", "env_factory", ".", "generate_virtual_env", "(", ")", "\n", "virtual_env", ".", "load_state_dict", "(", "save_dict", "[", "'model'", "]", ")", "\n", "real_env", "=", "env_factory", ".", "generate_real_env", "(", ")", "\n", "\n", "return", "virtual_env", ",", "real_env", ",", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2_correlation.train_test_agents": [[25, 88], ["range", "agents.agent_utils.select_agent", "range", "agents.agent_utils.select_agent", "print", "agents.agent_utils.select_agent.train", "agents.agent_utils.select_agent.test", "print", "reward_list_synthetic.append", "train_steps_needed_synthetic.append", "episodes_needed_synthetic.append", "agents.agent_utils.select_agent", "print", "agents.agent_utils.select_agent.train", "agents.agent_utils.select_agent.test", "print", "reward_list_real.append", "train_steps_needed_real.append", "episodes_needed_real.append", "str", "sum", "len", "str", "sum", "len"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.agents.agent_utils.select_agent", "home.repos.pwc.inspect_result.automl_learning_environments.agents.agent_utils.select_agent", "home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.train", "home.repos.pwc.inspect_result.automl_learning_environments.agents.base_agent.BaseAgent.test", "home.repos.pwc.inspect_result.automl_learning_environments.agents.agent_utils.select_agent", "home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.train", "home.repos.pwc.inspect_result.automl_learning_environments.agents.base_agent.BaseAgent.test"], ["", "def", "train_test_agents", "(", "train_env", ",", "test_env", ",", "config", ",", "agents_num", ")", ":", "\n", "    ", "reward_list_synthetic", "=", "[", "]", "\n", "reward_list_real", "=", "[", "]", "\n", "\n", "train_steps_needed_synthetic", "=", "[", "]", "\n", "episodes_needed_synthetic", "=", "[", "]", "\n", "\n", "train_steps_needed_real", "=", "[", "]", "\n", "episodes_needed_real", "=", "[", "]", "\n", "\n", "# settings for comparability", "\n", "config", "[", "'agents'", "]", "[", "'ddqn_vary'", "]", "[", "'vary_hp'", "]", "=", "True", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'print_rate'", "]", "=", "10", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'early_out_num'", "]", "=", "10", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'train_episodes'", "]", "=", "1000", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'init_episodes'", "]", "=", "10", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'test_episodes'", "]", "=", "100", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'early_out_virtual_diff'", "]", "=", "0.01", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'early_out_num'", "]", "=", "1000", "# todo: remove eventually (new)", "\n", "\n", "for", "i", "in", "range", "(", "agents_num", ")", ":", "\n", "# synthetic", "\n", "        ", "agent", "=", "select_agent", "(", "config", "=", "config", ",", "agent_name", "=", "'DDQN_vary'", ")", "\n", "config_varied", "=", "agent", ".", "full_config", "\n", "for", "j", "in", "range", "(", "100", ")", ":", "\n", "            ", "agent", "=", "select_agent", "(", "config", "=", "config_varied", ",", "agent_name", "=", "'DDQN'", ")", "\n", "print", "(", "f\"training on syn env with config (agent {j}): {agent.full_config}\"", ")", "\n", "reward_train", ",", "episode_length", ",", "_", "=", "agent", ".", "train", "(", "env", "=", "train_env", ")", "\n", "reward", ",", "_", ",", "_", "=", "agent", ".", "test", "(", "env", "=", "test_env", ")", "\n", "print", "(", "'reward when trained on synth. env: '", "+", "str", "(", "reward", ")", ")", "\n", "reward_list_synthetic", ".", "append", "(", "reward", ")", "\n", "train_steps_needed_synthetic", ".", "append", "(", "[", "sum", "(", "episode_length", ")", "]", ")", "\n", "episodes_needed_synthetic", ".", "append", "(", "[", "(", "len", "(", "reward_train", ")", ")", "]", ")", "\n", "\n", "# real", "\n", "agent", "=", "select_agent", "(", "config", "=", "config_varied", ",", "agent_name", "=", "'DDQN'", ")", "\n", "print", "(", "f\"training on real env with config (agent {j}): {agent.full_config}\"", ")", "\n", "reward_train", ",", "episode_length", ",", "_", "=", "agent", ".", "train", "(", "env", "=", "test_env", ")", "\n", "reward", ",", "_", ",", "_", "=", "agent", ".", "test", "(", "env", "=", "test_env", ")", "\n", "print", "(", "'reward when trained on real env: '", "+", "str", "(", "reward", ")", ")", "\n", "reward_list_real", ".", "append", "(", "reward", ")", "\n", "train_steps_needed_real", ".", "append", "(", "[", "sum", "(", "episode_length", ")", "]", ")", "\n", "episodes_needed_real", ".", "append", "(", "[", "(", "len", "(", "reward_train", ")", ")", "]", ")", "\n", "\n", "", "", "reward_dct", "=", "{", "\n", "\"config\"", ":", "config", ",", "\n", "\"synthetic\"", ":", "reward_list_synthetic", ",", "\n", "\"real\"", ":", "reward_list_real", "\n", "}", "\n", "\n", "train_steps_dct", "=", "{", "\n", "\"config\"", ":", "config", ",", "\n", "\"synthetic\"", ":", "train_steps_needed_synthetic", ",", "\n", "\"real\"", ":", "train_steps_needed_real", "\n", "}", "\n", "\n", "episodes_needed_dct", "=", "{", "\n", "\"config\"", ":", "config", ",", "\n", "\"synthetic\"", ":", "episodes_needed_synthetic", ",", "\n", "\"real\"", ":", "episodes_needed_real", "\n", "}", "\n", "\n", "return", "reward_dct", ",", "train_steps_dct", ",", "episodes_needed_dct", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_halfcheetah.ExperimentWrapper.get_bohb_parameters": [[17, 26], ["None"], "methods", ["None"], ["    ", "def", "get_bohb_parameters", "(", "self", ")", ":", "\n", "        ", "params", "=", "{", "}", "\n", "params", "[", "'min_budget'", "]", "=", "1", "\n", "params", "[", "'max_budget'", "]", "=", "1", "\n", "params", "[", "'eta'", "]", "=", "2", "\n", "params", "[", "'random_fraction'", "]", "=", "1", "\n", "params", "[", "'iterations'", "]", "=", "10000", "\n", "\n", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_halfcheetah.ExperimentWrapper.get_configspace": [[27, 30], ["ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace"], "methods", ["None"], ["", "def", "get_configspace", "(", "self", ")", ":", "\n", "        ", "cs", "=", "CS", ".", "ConfigurationSpace", "(", ")", "\n", "return", "cs", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_halfcheetah.ExperimentWrapper.get_specific_config": [[31, 38], ["copy.deepcopy"], "methods", ["None"], ["", "def", "get_specific_config", "(", "self", ",", "cso", ",", "default_config", ",", "budget", ")", ":", "\n", "        ", "config", "=", "deepcopy", "(", "default_config", ")", "\n", "global", "reward_env_type", "\n", "config", "[", "\"envs\"", "]", "[", "'HalfCheetah-v3'", "]", "[", "'reward_env_type'", "]", "=", "reward_env_type", "\n", "config", "[", "\"envs\"", "]", "[", "'HalfCheetah-v3'", "]", "[", "'solved_reward'", "]", "=", "sys", ".", "maxsize", "# AUC", "\n", "# config[\"envs\"]['HalfCheetah-v3']['device'] = \"cuda:0\"  # AUC", "\n", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_halfcheetah.ExperimentWrapper.compute": [[39, 79], ["GTNC_evaluate_halfcheetah.ExperimentWrapper.get_specific_config", "print", "print", "print", "print", "print", "print", "str", "str", "str", "str", "print", "print", "print", "print", "print", "open", "yaml.safe_load", "agents.GTN.GTN_Master", "agents.GTN.GTN_Master.run", "str", "str", "str", "float", "traceback.format_exc", "print", "str", "str", "sorted"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_step_size.ExperimentWrapper.get_specific_config", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.run"], ["", "def", "compute", "(", "self", ",", "working_dir", ",", "bohb_id", ",", "config_id", ",", "cso", ",", "budget", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "with", "open", "(", "\"default_config_halfcheetah_reward_env.yaml\"", ",", "'r'", ")", "as", "stream", ":", "\n", "            ", "default_config", "=", "yaml", ".", "safe_load", "(", "stream", ")", "\n", "\n", "", "config", "=", "self", ".", "get_specific_config", "(", "cso", ",", "default_config", ",", "budget", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "\"START BOHB ITERATION\"", ")", "\n", "print", "(", "'CONFIG: '", "+", "str", "(", "config", ")", ")", "\n", "print", "(", "'CSO:    '", "+", "str", "(", "cso", ")", ")", "\n", "print", "(", "'BUDGET: '", "+", "str", "(", "budget", ")", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "try", ":", "\n", "            ", "gtn", "=", "GTN_Master", "(", "config", ",", "bohb_id", "=", "bohb_id", ",", "bohb_working_dir", "=", "working_dir", ")", "\n", "_", ",", "score_list", ",", "model_name", "=", "gtn", ".", "run", "(", ")", "\n", "score", "=", "-", "sorted", "(", "score_list", ")", "[", "-", "1", "]", "\n", "error", "=", "\"\"", "\n", "", "except", ":", "\n", "            ", "score", "=", "float", "(", "'-Inf'", ")", "\n", "score_list", "=", "[", "]", "\n", "model_name", "=", "None", "\n", "error", "=", "traceback", ".", "format_exc", "(", ")", "\n", "print", "(", "error", ")", "\n", "\n", "", "info", "=", "{", "}", "\n", "info", "[", "'error'", "]", "=", "str", "(", "error", ")", "\n", "info", "[", "'config'", "]", "=", "str", "(", "config", ")", "\n", "info", "[", "'score_list'", "]", "=", "str", "(", "score_list", ")", "\n", "info", "[", "'model_name'", "]", "=", "str", "(", "model_name", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "'FINAL SCORE: '", "+", "str", "(", "score", ")", ")", "\n", "print", "(", "'SCORE LIST:  '", "+", "str", "(", "score_list", ")", ")", "\n", "print", "(", "\"END BOHB ITERATION\"", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "return", "{", "\n", "\"loss\"", ":", "score", ",", "\n", "\"info\"", ":", "info", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.demo_score_transform.score_transform": [[9, 83], ["numpy.asarray", "numpy.asarray", "scores.astype.tolist", "numpy.argsort", "len", "range", "len", "min", "len", "numpy.argsort", "range", "scores.astype.astype", "range", "max", "max", "min", "max", "sum", "numpy.zeros", "numpy.mean", "numpy.where", "numpy.log", "numpy.log", "numpy.argmax", "sum", "numpy.zeros", "numpy.mean", "numpy.where", "ValueError", "sum", "numpy.argmax", "max", "sum", "str", "max"], "function", ["None"], ["def", "score_transform", "(", "score_list", ",", "score_orig_list", ",", "score_transform_type", ",", "nes_step_size", ")", ":", "\n", "    ", "scores", "=", "np", ".", "asarray", "(", "score_list", ")", "\n", "scores_orig", "=", "np", ".", "asarray", "(", "score_orig_list", ")", "\n", "\n", "if", "score_transform_type", "==", "0", ":", "\n", "# convert [1, 0, 5] to [0.2, 0, 1]", "\n", "        ", "scores", "=", "(", "scores", "-", "min", "(", "scores", ")", ")", "/", "(", "max", "(", "scores", ")", "-", "min", "(", "scores", ")", "+", "1e-9", ")", "\n", "\n", "", "elif", "score_transform_type", "==", "1", ":", "\n", "# convert [1, 0, 5] to [0.5, 0, 1]", "\n", "        ", "s", "=", "np", ".", "argsort", "(", "scores", ")", "\n", "n", "=", "len", "(", "scores", ")", "\n", "for", "i", "in", "range", "(", "n", ")", ":", "\n", "            ", "scores", "[", "s", "[", "i", "]", "]", "=", "i", "/", "(", "n", "-", "1", ")", "\n", "\n", "", "", "elif", "score_transform_type", "==", "2", "or", "score_transform_type", "==", "3", ":", "\n", "# fitness shaping from \"Natural Evolution Strategies\" (Wierstra 2014) paper, either with zero mean (2) or without (3)", "\n", "        ", "lmbda", "=", "len", "(", "scores", ")", "\n", "s", "=", "np", ".", "argsort", "(", "-", "scores", ")", "\n", "for", "i", "in", "range", "(", "lmbda", ")", ":", "\n", "            ", "scores", "[", "s", "[", "i", "]", "]", "=", "i", "+", "1", "\n", "", "scores", "=", "scores", ".", "astype", "(", "float", ")", "\n", "for", "i", "in", "range", "(", "lmbda", ")", ":", "\n", "            ", "scores", "[", "i", "]", "=", "max", "(", "0", ",", "np", ".", "log", "(", "lmbda", "/", "2", "+", "1", ")", "-", "np", ".", "log", "(", "scores", "[", "i", "]", ")", ")", "\n", "\n", "", "scores", "=", "scores", "/", "sum", "(", "scores", ")", "\n", "\n", "if", "score_transform_type", "==", "2", ":", "\n", "            ", "scores", "-=", "1", "/", "lmbda", "\n", "\n", "", "scores", "/=", "max", "(", "scores", ")", "\n", "\n", "", "elif", "score_transform_type", "==", "4", ":", "\n", "# consider single best eps", "\n", "        ", "scores_tmp", "=", "np", ".", "zeros", "(", "scores", ".", "size", ")", "\n", "scores_tmp", "[", "np", ".", "argmax", "(", "scores", ")", "]", "=", "1", "\n", "scores", "=", "scores_tmp", "\n", "\n", "", "elif", "score_transform_type", "==", "5", ":", "\n", "# consider single best eps that is better than the average", "\n", "        ", "avg_score_orig", "=", "np", ".", "mean", "(", "scores_orig", ")", "\n", "\n", "scores_idx", "=", "np", ".", "where", "(", "scores", ">", "avg_score_orig", "+", "1e-6", ",", "1", ",", "0", ")", "# 1e-6 to counter numerical errors", "\n", "if", "sum", "(", "scores_idx", ")", ">", "0", ":", "\n", "            ", "scores_tmp", "=", "np", ".", "zeros", "(", "scores", ".", "size", ")", "\n", "scores_tmp", "[", "np", ".", "argmax", "(", "scores", ")", "]", "=", "1", "\n", "scores", "=", "scores_tmp", "\n", "", "else", ":", "\n", "            ", "scores", "=", "scores_idx", "\n", "\n", "", "", "elif", "score_transform_type", "==", "6", "or", "score_transform_type", "==", "7", ":", "\n", "# consider all eps that are better than the average, normalize weight sum to 1", "\n", "        ", "avg_score_orig", "=", "np", ".", "mean", "(", "scores_orig", ")", "\n", "\n", "scores_idx", "=", "np", ".", "where", "(", "scores", ">", "avg_score_orig", "+", "1e-6", ",", "1", ",", "0", ")", "# 1e-6 to counter numerical errors", "\n", "if", "sum", "(", "scores_idx", ")", ">", "0", ":", "\n", "# if sum(scores_idx) > 0:", "\n", "            ", "scores", "=", "scores_idx", "*", "(", "scores", "-", "avg_score_orig", ")", "/", "(", "max", "(", "scores", ")", "-", "avg_score_orig", "+", "1e-9", ")", "\n", "if", "score_transform_type", "==", "6", ":", "\n", "                ", "scores", "/=", "max", "(", "scores", ")", "\n", "", "else", ":", "\n", "                ", "scores", "/=", "sum", "(", "scores", ")", "\n", "", "", "else", ":", "\n", "            ", "scores", "=", "scores_idx", "\n", "\n", "", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Unknown rank transform type: \"", "+", "str", "(", "score_transform_type", ")", ")", "\n", "\n", "", "if", "nes_step_size", ":", "\n", "        ", "scores", "=", "scores", "/", "len", "(", "score_list", ")", "\n", "\n", "", "score_transform_list", "=", "scores", ".", "tolist", "(", ")", "\n", "\n", "return", "score_transform_list", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.demo_score_transform.plot_score_transform_lists": [[85, 110], ["matplotlib.subplots", "enumerate", "matplotlib.subplots_adjust", "matplotlib.savefig", "zip", "axes[].plot", "axes[].plot", "axes[].bar", "axes[].set_ylim", "axes[].set_title", "len", "range", "axes[].set_xlabel", "axes[].set_ylabel", "axes[].legend", "axes[].set_yticks", "len", "numpy.mean", "numpy.mean", "len"], "function", ["None"], ["", "def", "plot_score_transform_lists", "(", "score_list", ",", "score_orig_list", ",", "score_transform_lists", ",", "score_transform_lists_nes", ")", ":", "\n", "    ", "fix", ",", "axes", "=", "plt", ".", "subplots", "(", "nrows", "=", "1", ",", "ncols", "=", "len", "(", "score_transform_lists", ")", ",", "dpi", "=", "600", ",", "figsize", "=", "(", "10", ",", "3.5", ")", ")", "\n", "\n", "titles", "=", "[", "'linear transf.'", ",", "'rank transf.'", ",", "'NES'", ",", "'NES unnorm.'", ",", "'single best'", ",", "'single better'", ",", "'all better 1'", ",", "'all better 2'", "]", "\n", "\n", "for", "i", ",", "lists", "in", "enumerate", "(", "zip", "(", "score_transform_lists", ",", "score_transform_lists_nes", ")", ")", ":", "\n", "        ", "score_transform_list", ",", "score_transform_list_nes", "=", "lists", "\n", "axes", "[", "i", "]", ".", "plot", "(", "score_list", ",", "linestyle", "=", "''", ",", "marker", "=", "'o'", ")", "\n", "axes", "[", "i", "]", ".", "plot", "(", "[", "0", ",", "len", "(", "score_list", ")", "]", ",", "[", "np", ".", "mean", "(", "score_orig_list", ")", ",", "np", ".", "mean", "(", "score_orig_list", ")", "]", ",", "color", "=", "'r'", ")", "\n", "axes", "[", "i", "]", ".", "bar", "(", "range", "(", "len", "(", "score_transform_list", ")", ")", ",", "score_transform_list", ",", "color", "=", "'#78B0D7'", ")", "\n", "# axes[i].bar(range(len(score_transform_list_nes)), score_transform_list_nes, color='#25587D')", "\n", "axes", "[", "i", "]", ".", "set_ylim", "(", "-", "0.45", ",", "1.05", ")", "\n", "axes", "[", "i", "]", ".", "set_title", "(", "titles", "[", "i", "]", ")", "\n", "\n", "if", "i", "==", "0", ":", "\n", "            ", "axes", "[", "i", "]", ".", "set_xlabel", "(", "'i (population member)'", ")", "\n", "axes", "[", "i", "]", ".", "set_ylabel", "(", "'expected cumulative reward / fitness value'", ")", "\n", "# axes[i].legend([r'$K_i$', r'$K_G$', r'$F_i$', r'$F^\\ast_i$'], loc='lower left')", "\n", "axes", "[", "i", "]", ".", "legend", "(", "[", "r'$K_i$'", ",", "r'$K_G$'", ",", "r'$F_i$'", "]", ",", "loc", "=", "'lower left'", ")", "\n", "", "else", ":", "\n", "            ", "axes", "[", "i", "]", ".", "set_yticks", "(", "[", "]", ")", "\n", "\n", "# plt.show()", "\n", "", "", "plt", ".", "subplots_adjust", "(", "bottom", "=", "0.15", ")", "\n", "plt", ".", "savefig", "(", "'demo_score_transform.svg'", ",", "bbox_inches", "=", "'tight'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_mountaincar_params.ExperimentWrapper.get_bohb_parameters": [[17, 26], ["None"], "methods", ["None"], ["    ", "def", "get_bohb_parameters", "(", "self", ")", ":", "\n", "        ", "params", "=", "{", "}", "\n", "params", "[", "'min_budget'", "]", "=", "1", "\n", "params", "[", "'max_budget'", "]", "=", "3", "\n", "params", "[", "'eta'", "]", "=", "3", "\n", "params", "[", "'random_fraction'", "]", "=", "0.3", "\n", "params", "[", "'iterations'", "]", "=", "10000", "\n", "\n", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_mountaincar_params.ExperimentWrapper.get_configspace": [[27, 53], ["ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter"], "methods", ["None"], ["", "def", "get_configspace", "(", "self", ")", ":", "\n", "        ", "cs", "=", "CS", ".", "ConfigurationSpace", "(", ")", "\n", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'gtn_score_transform_type'", ",", "lower", "=", "0", ",", "upper", "=", "7", ",", "log", "=", "False", ",", "default_value", "=", "7", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'gtn_step_size'", ",", "lower", "=", "0.5", ",", "upper", "=", "1", ",", "log", "=", "True", ",", "default_value", "=", "0.5", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "CategoricalHyperparameter", "(", "name", "=", "'gtn_mirrored_sampling'", ",", "choices", "=", "[", "False", ",", "True", "]", ",", "default_value", "=", "True", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'gtn_noise_std'", ",", "lower", "=", "0.005", ",", "upper", "=", "0.05", ",", "log", "=", "True", ",", "default_value", "=", "0.01", ")", ")", "\n", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'ddqn_init_episodes'", ",", "lower", "=", "1", ",", "upper", "=", "20", ",", "log", "=", "True", ",", "default_value", "=", "10", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'ddqn_batch_size'", ",", "lower", "=", "64", ",", "upper", "=", "256", ",", "log", "=", "False", ",", "default_value", "=", "128", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'ddqn_gamma'", ",", "lower", "=", "0.001", ",", "upper", "=", "0.01", ",", "log", "=", "True", ",", "default_value", "=", "0.005", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'ddqn_lr'", ",", "lower", "=", "1e-4", ",", "upper", "=", "5e-3", ",", "log", "=", "True", ",", "default_value", "=", "1e-3", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'ddqn_tau'", ",", "lower", "=", "0.005", ",", "upper", "=", "0.05", ",", "log", "=", "True", ",", "default_value", "=", "0.01", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'ddqn_eps_init'", ",", "lower", "=", "0.8", ",", "upper", "=", "1", ",", "log", "=", "True", ",", "default_value", "=", "0.9", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'ddqn_eps_min'", ",", "lower", "=", "0.005", ",", "upper", "=", "0.05", ",", "log", "=", "True", ",", "default_value", "=", "0.05", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'ddqn_eps_decay'", ",", "lower", "=", "0.01", ",", "upper", "=", "0.2", ",", "log", "=", "True", ",", "default_value", "=", "0.1", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'ddqn_same_action_num'", ",", "lower", "=", "1", ",", "upper", "=", "5", ",", "log", "=", "False", ",", "default_value", "=", "3", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "CategoricalHyperparameter", "(", "name", "=", "'ddqn_activation_fn'", ",", "choices", "=", "[", "'tanh'", ",", "'relu'", ",", "'leakyrelu'", ",", "'prelu'", "]", ",", "default_value", "=", "'relu'", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'ddqn_hidden_size'", ",", "lower", "=", "48", ",", "upper", "=", "192", ",", "log", "=", "True", ",", "default_value", "=", "128", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'ddqn_hidden_layer'", ",", "lower", "=", "1", ",", "upper", "=", "2", ",", "log", "=", "False", ",", "default_value", "=", "2", ")", ")", "\n", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "CategoricalHyperparameter", "(", "name", "=", "'mountaincar_activation_fn'", ",", "choices", "=", "[", "'tanh'", ",", "'relu'", ",", "'leakyrelu'", ",", "'prelu'", "]", ",", "default_value", "=", "'leakyrelu'", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'mountaincar_hidden_size'", ",", "lower", "=", "48", ",", "upper", "=", "192", ",", "log", "=", "True", ",", "default_value", "=", "128", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'mountaincar_hidden_layer'", ",", "lower", "=", "1", ",", "upper", "=", "2", ",", "log", "=", "False", ",", "default_value", "=", "1", ")", ")", "\n", "\n", "return", "cs", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_mountaincar_params.ExperimentWrapper.get_specific_config": [[54, 80], ["copy.deepcopy"], "methods", ["None"], ["", "def", "get_specific_config", "(", "self", ",", "cso", ",", "default_config", ",", "budget", ")", ":", "\n", "        ", "config", "=", "deepcopy", "(", "default_config", ")", "\n", "\n", "config", "[", "\"agents\"", "]", "[", "'gtn'", "]", "[", "'score_transform_type'", "]", "=", "cso", "[", "\"gtn_score_transform_type\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'gtn'", "]", "[", "'step_size'", "]", "=", "cso", "[", "\"gtn_step_size\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'gtn'", "]", "[", "'mirrored_sampling'", "]", "=", "cso", "[", "\"gtn_mirrored_sampling\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'gtn'", "]", "[", "'noise_std'", "]", "=", "cso", "[", "\"gtn_noise_std\"", "]", "\n", "\n", "config", "[", "\"agents\"", "]", "[", "'ddqn'", "]", "[", "'init_episodes'", "]", "=", "cso", "[", "\"ddqn_init_episodes\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'ddqn'", "]", "[", "'batch_size'", "]", "=", "cso", "[", "\"ddqn_batch_size\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'ddqn'", "]", "[", "'gamma'", "]", "=", "1", "-", "cso", "[", "\"ddqn_gamma\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'ddqn'", "]", "[", "'lr'", "]", "=", "cso", "[", "\"ddqn_lr\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'ddqn'", "]", "[", "'tau'", "]", "=", "cso", "[", "\"ddqn_tau\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'ddqn'", "]", "[", "'eps_init'", "]", "=", "cso", "[", "\"ddqn_eps_init\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'ddqn'", "]", "[", "'eps_min'", "]", "=", "cso", "[", "\"ddqn_eps_min\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'ddqn'", "]", "[", "'eps_decay'", "]", "=", "1", "-", "cso", "[", "\"ddqn_eps_decay\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'ddqn'", "]", "[", "'same_action_num'", "]", "=", "cso", "[", "\"ddqn_same_action_num\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'ddqn'", "]", "[", "'activation_fn'", "]", "=", "cso", "[", "\"ddqn_activation_fn\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'ddqn'", "]", "[", "'hidden_size'", "]", "=", "cso", "[", "\"ddqn_hidden_size\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'ddqn'", "]", "[", "'hidden_layer'", "]", "=", "cso", "[", "\"ddqn_hidden_layer\"", "]", "\n", "\n", "config", "[", "\"envs\"", "]", "[", "'MountainCar-v0'", "]", "[", "'activation_fn'", "]", "=", "cso", "[", "\"mountaincar_activation_fn\"", "]", "\n", "config", "[", "\"envs\"", "]", "[", "'MountainCar-v0'", "]", "[", "'hidden_size'", "]", "=", "cso", "[", "\"mountaincar_hidden_size\"", "]", "\n", "config", "[", "\"envs\"", "]", "[", "'MountainCar-v0'", "]", "[", "'hidden_layer'", "]", "=", "cso", "[", "\"mountaincar_hidden_layer\"", "]", "\n", "\n", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_mountaincar_params.ExperimentWrapper.compute": [[81, 121], ["GTNC_evaluate_mountaincar_params.ExperimentWrapper.get_specific_config", "print", "print", "print", "print", "print", "print", "str", "str", "str", "print", "print", "print", "print", "print", "open", "yaml.safe_load", "range", "str", "str", "str", "agents.GTN.GTN_Master", "agents.GTN.GTN_Master.run", "len", "float", "traceback.format_exc", "print", "str", "str"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_step_size.ExperimentWrapper.get_specific_config", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.run"], ["", "def", "compute", "(", "self", ",", "working_dir", ",", "bohb_id", ",", "config_id", ",", "cso", ",", "budget", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "with", "open", "(", "\"default_config_mountaincar.yaml\"", ",", "'r'", ")", "as", "stream", ":", "\n", "            ", "default_config", "=", "yaml", ".", "safe_load", "(", "stream", ")", "\n", "\n", "", "config", "=", "self", ".", "get_specific_config", "(", "cso", ",", "default_config", ",", "budget", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "\"START BOHB ITERATION\"", ")", "\n", "print", "(", "'CONFIG: '", "+", "str", "(", "config", ")", ")", "\n", "print", "(", "'CSO:    '", "+", "str", "(", "cso", ")", ")", "\n", "print", "(", "'BUDGET: '", "+", "str", "(", "budget", ")", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "try", ":", "\n", "            ", "score", "=", "0", "\n", "for", "_", "in", "range", "(", "3", ")", ":", "\n", "                ", "gtn", "=", "GTN_Master", "(", "config", ",", "bohb_id", "=", "bohb_id", ",", "bohb_working_dir", "=", "working_dir", ")", "\n", "_", ",", "score_list", "=", "gtn", ".", "run", "(", ")", "\n", "score", "+=", "len", "(", "score_list", ")", "\n", "", "error", "=", "\"\"", "\n", "", "except", ":", "\n", "            ", "score", "=", "float", "(", "'Inf'", ")", "\n", "score_list", "=", "[", "]", "\n", "error", "=", "traceback", ".", "format_exc", "(", ")", "\n", "print", "(", "error", ")", "\n", "\n", "", "info", "=", "{", "}", "\n", "info", "[", "'error'", "]", "=", "str", "(", "error", ")", "\n", "info", "[", "'config'", "]", "=", "str", "(", "config", ")", "\n", "info", "[", "'score_list'", "]", "=", "str", "(", "score_list", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "'FINAL SCORE: '", "+", "str", "(", "score", ")", ")", "\n", "print", "(", "'SCORE LIST:  '", "+", "str", "(", "score_list", ")", ")", "\n", "print", "(", "\"END BOHB ITERATION\"", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "return", "{", "\n", "\"loss\"", ":", "score", ",", "\n", "\"info\"", ":", "info", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2_TD3_discrete.load_envs_and_config": [[24, 42], ["os.path.join", "torch.load", "envs.env_factory.EnvFactory", "envs.env_factory.EnvFactory.generate_virtual_env", "env_factory.generate_virtual_env.load_state_dict", "envs.env_factory.EnvFactory.generate_real_env", "open", "yaml.safe_load"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_virtual_env", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_real_env"], ["def", "load_envs_and_config", "(", "file_name", ",", "model_dir", ",", "device", ")", ":", "\n", "    ", "file_path", "=", "os", ".", "path", ".", "join", "(", "model_dir", ",", "file_name", ")", "\n", "save_dict", "=", "torch", ".", "load", "(", "file_path", ")", "\n", "config", "=", "save_dict", "[", "'config'", "]", "\n", "config", "[", "'device'", "]", "=", "device", "\n", "env_factory", "=", "EnvFactory", "(", "config", "=", "config", ")", "\n", "virtual_env", "=", "env_factory", ".", "generate_virtual_env", "(", ")", "\n", "virtual_env", ".", "load_state_dict", "(", "save_dict", "[", "'model'", "]", ")", "\n", "real_env", "=", "env_factory", ".", "generate_real_env", "(", ")", "\n", "\n", "# load additional agent configs", "\n", "with", "open", "(", "\"../default_config_cartpole.yaml\"", ",", "\"r\"", ")", "as", "stream", ":", "\n", "        ", "config_new", "=", "yaml", ".", "safe_load", "(", "stream", ")", "[", "\"agents\"", "]", "\n", "\n", "# config[\"agents\"][\"td3_discrete_vary\"] = config_new[\"td3_discrete_temp_anneal\"]", "\n", "", "config", "[", "\"agents\"", "]", "[", "\"td3_discrete_vary\"", "]", "=", "config_new", "[", "\"td3_discrete_vary_layer_norm_2\"", "]", "\n", "\n", "return", "virtual_env", ",", "real_env", ",", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2_TD3_discrete.train_test_agents": [[44, 68], ["range", "agents.agent_utils.select_agent", "agents.agent_utils.select_agent.train", "agents.agent_utils.select_agent.test", "print", "reward_list.append", "train_steps_needed.append", "episodes_needed.append", "str", "sum", "len"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.agents.agent_utils.select_agent", "home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.train", "home.repos.pwc.inspect_result.automl_learning_environments.agents.base_agent.BaseAgent.test"], ["", "def", "train_test_agents", "(", "train_env", ",", "test_env", ",", "config", ",", "agents_num", ")", ":", "\n", "    ", "reward_list", "=", "[", "]", "\n", "train_steps_needed", "=", "[", "]", "\n", "episodes_needed", "=", "[", "]", "\n", "\n", "# settings for comparability", "\n", "config", "[", "'agents'", "]", "[", "'td3_discrete_vary'", "]", "[", "'vary_hp'", "]", "=", "True", "\n", "config", "[", "'agents'", "]", "[", "'td3_discrete_vary'", "]", "[", "'print_rate'", "]", "=", "10", "\n", "config", "[", "'agents'", "]", "[", "'td3_discrete_vary'", "]", "[", "'early_out_num'", "]", "=", "10", "\n", "config", "[", "'agents'", "]", "[", "'td3_discrete_vary'", "]", "[", "'train_episodes'", "]", "=", "1000", "\n", "config", "[", "'agents'", "]", "[", "'td3_discrete_vary'", "]", "[", "'init_episodes'", "]", "=", "10", "\n", "config", "[", "'agents'", "]", "[", "'td3_discrete_vary'", "]", "[", "'test_episodes'", "]", "=", "10", "\n", "config", "[", "'agents'", "]", "[", "'td3_discrete_vary'", "]", "[", "'early_out_virtual_diff'", "]", "=", "0.01", "\n", "\n", "for", "i", "in", "range", "(", "agents_num", ")", ":", "\n", "        ", "agent", "=", "select_agent", "(", "config", "=", "config", ",", "agent_name", "=", "'td3_discrete_vary'", ")", "\n", "reward_train", ",", "episode_length", ",", "_", "=", "agent", ".", "train", "(", "env", "=", "train_env", ")", "\n", "reward", ",", "_", ",", "_", "=", "agent", ".", "test", "(", "env", "=", "test_env", ")", "\n", "print", "(", "'reward: '", "+", "str", "(", "reward", ")", ")", "\n", "reward_list", ".", "append", "(", "reward", ")", "\n", "train_steps_needed", ".", "append", "(", "[", "sum", "(", "episode_length", ")", "]", ")", "\n", "episodes_needed", ".", "append", "(", "[", "(", "len", "(", "reward_train", ")", ")", "]", ")", "\n", "\n", "", "return", "reward_list", ",", "train_steps_needed", ",", "episodes_needed", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_pendulum_params.ExperimentWrapper.get_bohb_parameters": [[18, 27], ["None"], "methods", ["None"], ["    ", "def", "get_bohb_parameters", "(", "self", ")", ":", "\n", "        ", "params", "=", "{", "}", "\n", "params", "[", "'min_budget'", "]", "=", "1", "\n", "params", "[", "'max_budget'", "]", "=", "3", "\n", "params", "[", "'eta'", "]", "=", "3", "\n", "params", "[", "'random_fraction'", "]", "=", "0.3", "\n", "params", "[", "'iterations'", "]", "=", "10000", "\n", "\n", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_pendulum_params.ExperimentWrapper.get_configspace": [[28, 55], ["ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter"], "methods", ["None"], ["", "def", "get_configspace", "(", "self", ")", ":", "\n", "        ", "cs", "=", "CS", ".", "ConfigurationSpace", "(", ")", "\n", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'gtn_score_transform_type'", ",", "lower", "=", "0", ",", "upper", "=", "7", ",", "log", "=", "False", ",", "default_value", "=", "7", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'gtn_step_size'", ",", "lower", "=", "0.1", ",", "upper", "=", "1", ",", "log", "=", "True", ",", "default_value", "=", "0.5", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "CategoricalHyperparameter", "(", "name", "=", "'gtn_mirrored_sampling'", ",", "choices", "=", "[", "False", ",", "True", "]", ",", "default_value", "=", "True", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'gtn_noise_std'", ",", "lower", "=", "0.001", ",", "upper", "=", "1", ",", "log", "=", "True", ",", "default_value", "=", "0.01", ")", ")", "\n", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'td3_init_episodes'", ",", "lower", "=", "1", ",", "upper", "=", "20", ",", "log", "=", "True", ",", "default_value", "=", "10", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'td3_batch_size'", ",", "lower", "=", "64", ",", "upper", "=", "256", ",", "log", "=", "False", ",", "default_value", "=", "128", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_gamma'", ",", "lower", "=", "0.001", ",", "upper", "=", "0.1", ",", "log", "=", "True", ",", "default_value", "=", "0.01", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_lr'", ",", "lower", "=", "1e-4", ",", "upper", "=", "5e-3", ",", "log", "=", "True", ",", "default_value", "=", "1e-3", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_tau'", ",", "lower", "=", "0.005", ",", "upper", "=", "0.05", ",", "log", "=", "True", ",", "default_value", "=", "0.01", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'td3_policy_delay'", ",", "lower", "=", "1", ",", "upper", "=", "3", ",", "log", "=", "False", ",", "default_value", "=", "2", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "CategoricalHyperparameter", "(", "name", "=", "'td3_activation_fn'", ",", "choices", "=", "[", "'tanh'", ",", "'relu'", ",", "'leakyrelu'", ",", "'prelu'", "]", ",", "default_value", "=", "'relu'", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'td3_hidden_size'", ",", "lower", "=", "48", ",", "upper", "=", "192", ",", "log", "=", "True", ",", "default_value", "=", "128", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'td3_hidden_layer'", ",", "lower", "=", "1", ",", "upper", "=", "2", ",", "log", "=", "False", ",", "default_value", "=", "2", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_action_std'", ",", "lower", "=", "0.05", ",", "upper", "=", "0.2", ",", "log", "=", "True", ",", "default_value", "=", "0.1", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_policy_std'", ",", "lower", "=", "0.1", ",", "upper", "=", "0.4", ",", "log", "=", "True", ",", "default_value", "=", "0.2", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_policy_std_clip'", ",", "lower", "=", "0.25", ",", "upper", "=", "1", ",", "log", "=", "True", ",", "default_value", "=", "0.5", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_early_out_virtual_diff'", ",", "lower", "=", "1e-2", ",", "upper", "=", "1e-1", ",", "log", "=", "True", ",", "default_value", "=", "3e-2", ")", ")", "\n", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "CategoricalHyperparameter", "(", "name", "=", "'pendulum_activation_fn'", ",", "choices", "=", "[", "'tanh'", ",", "'relu'", ",", "'leakyrelu'", ",", "'prelu'", "]", ",", "default_value", "=", "'leakyrelu'", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'pendulum_hidden_size'", ",", "lower", "=", "48", ",", "upper", "=", "192", ",", "log", "=", "True", ",", "default_value", "=", "128", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'pendulum_hidden_layer'", ",", "lower", "=", "1", ",", "upper", "=", "2", ",", "log", "=", "False", ",", "default_value", "=", "1", ")", ")", "\n", "\n", "return", "cs", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_pendulum_params.ExperimentWrapper.get_specific_config": [[56, 87], ["copy.deepcopy"], "methods", ["None"], ["", "def", "get_specific_config", "(", "self", ",", "cso", ",", "default_config", ",", "budget", ")", ":", "\n", "        ", "config", "=", "deepcopy", "(", "default_config", ")", "\n", "\n", "config", "[", "\"agents\"", "]", "[", "'gtn'", "]", "[", "'score_transform_type'", "]", "=", "cso", "[", "\"gtn_score_transform_type\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'gtn'", "]", "[", "'step_size'", "]", "=", "cso", "[", "\"gtn_step_size\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'gtn'", "]", "[", "'mirrored_sampling'", "]", "=", "cso", "[", "\"gtn_mirrored_sampling\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'gtn'", "]", "[", "'noise_std'", "]", "=", "cso", "[", "\"gtn_noise_std\"", "]", "\n", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'init_episodes'", "]", "=", "cso", "[", "\"td3_init_episodes\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'batch_size'", "]", "=", "cso", "[", "\"td3_batch_size\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'gamma'", "]", "=", "1", "-", "cso", "[", "\"td3_gamma\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'lr'", "]", "=", "cso", "[", "\"td3_lr\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'tau'", "]", "=", "cso", "[", "\"td3_tau\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'policy_delay'", "]", "=", "cso", "[", "\"td3_policy_delay\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'activation_fn'", "]", "=", "cso", "[", "\"td3_activation_fn\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'hidden_size'", "]", "=", "cso", "[", "\"td3_hidden_size\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'hidden_layer'", "]", "=", "cso", "[", "\"td3_hidden_layer\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'action_std'", "]", "=", "cso", "[", "\"td3_action_std\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'policy_std'", "]", "=", "cso", "[", "\"td3_policy_std\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'policy_std_clip'", "]", "=", "cso", "[", "\"td3_policy_std_clip\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'early_out_virtual_diff'", "]", "=", "cso", "[", "\"td3_early_out_virtual_diff\"", "]", "\n", "\n", "config", "[", "\"envs\"", "]", "[", "'Pendulum-v0'", "]", "[", "'activation_fn'", "]", "=", "cso", "[", "\"pendulum_activation_fn\"", "]", "\n", "config", "[", "\"envs\"", "]", "[", "'Pendulum-v0'", "]", "[", "'hidden_size'", "]", "=", "cso", "[", "\"pendulum_hidden_size\"", "]", "\n", "config", "[", "\"envs\"", "]", "[", "'Pendulum-v0'", "]", "[", "'hidden_layer'", "]", "=", "cso", "[", "\"pendulum_hidden_layer\"", "]", "\n", "\n", "# global reward_env_type", "\n", "config", "[", "\"agents\"", "]", "[", "'gtn'", "]", "[", "'synthetic_env_type'", "]", "=", "0", "\n", "# config[\"envs\"]['Pendulum-v0']['reward_env_type'] = reward_env_type", "\n", "\n", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_pendulum_params.ExperimentWrapper.compute": [[88, 128], ["GTNC_evaluate_pendulum_params.ExperimentWrapper.get_specific_config", "print", "print", "print", "print", "print", "print", "str", "str", "str", "str", "print", "print", "print", "print", "print", "open", "yaml.safe_load", "agents.GTN.GTN_Master", "agents.GTN.GTN_Master.run", "str", "str", "str", "float", "traceback.format_exc", "print", "str", "str", "sorted"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_step_size.ExperimentWrapper.get_specific_config", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.run"], ["", "def", "compute", "(", "self", ",", "working_dir", ",", "bohb_id", ",", "config_id", ",", "cso", ",", "budget", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "with", "open", "(", "\"default_config_pendulum.yaml\"", ",", "'r'", ")", "as", "stream", ":", "\n", "            ", "default_config", "=", "yaml", ".", "safe_load", "(", "stream", ")", "\n", "\n", "", "config", "=", "self", ".", "get_specific_config", "(", "cso", ",", "default_config", ",", "budget", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "\"START BOHB ITERATION\"", ")", "\n", "print", "(", "'CONFIG: '", "+", "str", "(", "config", ")", ")", "\n", "print", "(", "'CSO:    '", "+", "str", "(", "cso", ")", ")", "\n", "print", "(", "'BUDGET: '", "+", "str", "(", "budget", ")", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "try", ":", "\n", "            ", "gtn", "=", "GTN_Master", "(", "config", ",", "bohb_id", "=", "bohb_id", ",", "bohb_working_dir", "=", "working_dir", ")", "\n", "_", ",", "score_list", ",", "model_name", "=", "gtn", ".", "run", "(", ")", "\n", "score", "=", "-", "sorted", "(", "score_list", ")", "[", "-", "1", "]", "\n", "error", "=", "\"\"", "\n", "", "except", ":", "\n", "            ", "score", "=", "float", "(", "'-Inf'", ")", "\n", "score_list", "=", "[", "]", "\n", "model_name", "=", "None", "\n", "error", "=", "traceback", ".", "format_exc", "(", ")", "\n", "print", "(", "error", ")", "\n", "\n", "", "info", "=", "{", "}", "\n", "info", "[", "'error'", "]", "=", "str", "(", "error", ")", "\n", "info", "[", "'config'", "]", "=", "str", "(", "config", ")", "\n", "info", "[", "'score_list'", "]", "=", "str", "(", "score_list", ")", "\n", "info", "[", "'model_name'", "]", "=", "str", "(", "model_name", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "'FINAL SCORE: '", "+", "str", "(", "score", ")", ")", "\n", "print", "(", "'SCORE LIST:  '", "+", "str", "(", "score_list", ")", ")", "\n", "print", "(", "\"END BOHB ITERATION\"", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "return", "{", "\n", "\"loss\"", ":", "score", ",", "\n", "\"info\"", ":", "info", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_learn_halfcheetah.ExperimentWrapper.get_bohb_parameters": [[18, 27], ["None"], "methods", ["None"], ["    ", "def", "get_bohb_parameters", "(", "self", ")", ":", "\n", "        ", "params", "=", "{", "}", "\n", "params", "[", "'min_budget'", "]", "=", "1", "\n", "params", "[", "'max_budget'", "]", "=", "1", "\n", "params", "[", "'eta'", "]", "=", "2", "\n", "params", "[", "'random_fraction'", "]", "=", "1", "\n", "params", "[", "'iterations'", "]", "=", "10000", "\n", "\n", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_learn_halfcheetah.ExperimentWrapper.get_configspace": [[28, 31], ["ConfigSpace.ConfigurationSpace"], "methods", ["None"], ["", "def", "get_configspace", "(", "self", ")", ":", "\n", "        ", "cs", "=", "CS", ".", "ConfigurationSpace", "(", ")", "\n", "return", "cs", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_learn_halfcheetah.ExperimentWrapper.get_specific_config": [[32, 35], ["copy.deepcopy"], "methods", ["None"], ["", "def", "get_specific_config", "(", "self", ",", "cso", ",", "default_config", ",", "budget", ")", ":", "\n", "        ", "config", "=", "deepcopy", "(", "default_config", ")", "\n", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_learn_halfcheetah.ExperimentWrapper.compute": [[36, 77], ["syn_env_learn_halfcheetah.ExperimentWrapper.get_specific_config", "print", "print", "print", "print", "print", "print", "str", "str", "str", "str", "print", "print", "print", "print", "print", "open", "yaml.safe_load", "agents.GTN.GTN_Master", "agents.GTN.GTN_Master.run", "len", "str", "str", "str", "float", "traceback.format_exc", "print", "str", "str"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_step_size.ExperimentWrapper.get_specific_config", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.run"], ["", "def", "compute", "(", "self", ",", "working_dir", ",", "bohb_id", ",", "config_id", ",", "cso", ",", "budget", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "with", "open", "(", "\"default_config_halfcheetah.yaml\"", ",", "'r'", ")", "as", "stream", ":", "\n", "            ", "default_config", "=", "yaml", ".", "safe_load", "(", "stream", ")", "\n", "\n", "", "config", "=", "self", ".", "get_specific_config", "(", "cso", ",", "default_config", ",", "budget", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "\"START BOHB ITERATION\"", ")", "\n", "print", "(", "'CONFIG: '", "+", "str", "(", "config", ")", ")", "\n", "print", "(", "'CSO:    '", "+", "str", "(", "cso", ")", ")", "\n", "print", "(", "'BUDGET: '", "+", "str", "(", "budget", ")", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "try", ":", "\n", "            ", "gtn", "=", "GTN_Master", "(", "config", ",", "bohb_id", "=", "bohb_id", ",", "bohb_working_dir", "=", "working_dir", ")", "\n", "_", ",", "score_list", ",", "model_name", "=", "gtn", ".", "run", "(", ")", "\n", "# score = -sorted(score_list)[-1]", "\n", "score", "=", "len", "(", "score_list", ")", "\n", "error", "=", "\"\"", "\n", "", "except", ":", "\n", "            ", "score", "=", "float", "(", "'-Inf'", ")", "\n", "score_list", "=", "[", "]", "\n", "model_name", "=", "None", "\n", "error", "=", "traceback", ".", "format_exc", "(", ")", "\n", "print", "(", "error", ")", "\n", "\n", "", "info", "=", "{", "}", "\n", "info", "[", "'error'", "]", "=", "str", "(", "error", ")", "\n", "info", "[", "'config'", "]", "=", "str", "(", "config", ")", "\n", "info", "[", "'score_list'", "]", "=", "str", "(", "score_list", ")", "\n", "info", "[", "'model_name'", "]", "=", "str", "(", "model_name", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "'FINAL SCORE: '", "+", "str", "(", "score", ")", ")", "\n", "print", "(", "'SCORE LIST:  '", "+", "str", "(", "score_list", ")", ")", "\n", "print", "(", "\"END BOHB ITERATION\"", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "return", "{", "\n", "\"loss\"", ":", "score", ",", "\n", "\"info\"", ":", "info", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_for_visualization.load_envs_and_config": [[12, 22], ["torch.load", "envs.env_factory.EnvFactory", "envs.env_factory.EnvFactory.generate_virtual_env", "env_factory.generate_virtual_env.load_state_dict", "envs.env_factory.EnvFactory.generate_real_env"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_virtual_env", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_real_env"], ["def", "load_envs_and_config", "(", "file_path", ",", "device", ")", ":", "\n", "    ", "save_dict", "=", "torch", ".", "load", "(", "file_path", ")", "\n", "config", "=", "save_dict", "[", "'config'", "]", "\n", "config", "[", "'device'", "]", "=", "device", "\n", "env_factory", "=", "EnvFactory", "(", "config", "=", "config", ")", "\n", "virtual_env", "=", "env_factory", ".", "generate_virtual_env", "(", ")", "\n", "virtual_env", ".", "load_state_dict", "(", "save_dict", "[", "'model'", "]", ")", "\n", "real_env", "=", "env_factory", ".", "generate_real_env", "(", ")", "\n", "\n", "return", "virtual_env", ",", "real_env", ",", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_for_visualization.train_test_agents": [[24, 42], ["agents.agent_utils.select_agent", "agents.agent_utils.select_agent.train", "agents.agent_utils.select_agent.test", "print", "reward_list.append", "train_steps_needed.append", "episodes_needed.append", "str", "sum", "len"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.agents.agent_utils.select_agent", "home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.train", "home.repos.pwc.inspect_result.automl_learning_environments.agents.base_agent.BaseAgent.test"], ["", "def", "train_test_agents", "(", "train_env", ",", "test_env", ",", "config", ")", ":", "\n", "    ", "reward_list", "=", "[", "]", "\n", "train_steps_needed", "=", "[", "]", "\n", "episodes_needed", "=", "[", "]", "\n", "\n", "config", "[", "\"agents\"", "]", "[", "\"ddqn\"", "]", "[", "\"print_rate\"", "]", "=", "10", "\n", "config", "[", "\"agents\"", "]", "[", "\"ddqn\"", "]", "[", "\"test_episodes\"", "]", "=", "10", "\n", "config", "[", "\"render_env\"", "]", "=", "True", "\n", "\n", "agent", "=", "select_agent", "(", "config", "=", "config", ",", "agent_name", "=", "'DDQN'", ")", "\n", "reward_train", ",", "episode_length", ",", "_", "=", "agent", ".", "train", "(", "env", "=", "train_env", ")", "\n", "reward", ",", "_", ",", "_", "=", "agent", ".", "test", "(", "env", "=", "test_env", ")", "\n", "print", "(", "'reward: '", "+", "str", "(", "reward", ")", ")", "\n", "reward_list", ".", "append", "(", "reward", ")", "\n", "train_steps_needed", ".", "append", "(", "[", "sum", "(", "episode_length", ")", "]", ")", "\n", "episodes_needed", ".", "append", "(", "[", "(", "len", "(", "reward_train", ")", ")", "]", ")", "\n", "\n", "return", "reward_list", ",", "train_steps_needed", ",", "episodes_needed", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.bohb_params_TD3_halfcheetah.ExperimentWrapper.get_bohb_parameters": [[18, 27], ["None"], "methods", ["None"], ["    ", "def", "get_bohb_parameters", "(", "self", ")", ":", "\n", "        ", "params", "=", "{", "}", "\n", "params", "[", "'min_budget'", "]", "=", "1", "\n", "params", "[", "'max_budget'", "]", "=", "8", "\n", "params", "[", "'eta'", "]", "=", "2", "\n", "params", "[", "'iterations'", "]", "=", "1000", "\n", "params", "[", "'random_fraction'", "]", "=", "0.3", "\n", "\n", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.bohb_params_TD3_halfcheetah.ExperimentWrapper.get_configspace": [[28, 47], ["ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter"], "methods", ["None"], ["", "def", "get_configspace", "(", "self", ")", ":", "\n", "        ", "cs", "=", "CS", ".", "ConfigurationSpace", "(", ")", "\n", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'td3_init_episodes'", ",", "lower", "=", "1", ",", "upper", "=", "50", ",", "log", "=", "True", ",", "default_value", "=", "10", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'td3_batch_size'", ",", "lower", "=", "32", ",", "upper", "=", "256", ",", "log", "=", "False", ",", "default_value", "=", "128", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_gamma'", ",", "lower", "=", "0.001", ",", "upper", "=", "0.1", ",", "log", "=", "True", ",", "default_value", "=", "0.01", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_lr'", ",", "lower", "=", "1e-5", ",", "upper", "=", "5e-3", ",", "log", "=", "True", ",", "default_value", "=", "1e-3", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_tau'", ",", "lower", "=", "0.001", ",", "upper", "=", "0.05", ",", "log", "=", "True", ",", "default_value", "=", "0.01", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'td3_policy_delay'", ",", "lower", "=", "1", ",", "upper", "=", "5", ",", "log", "=", "False", ",", "default_value", "=", "2", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "CategoricalHyperparameter", "(", "name", "=", "'td3_activation_fn'", ",", "choices", "=", "[", "'tanh'", ",", "'relu'", ",", "'leakyrelu'", ",", "'prelu'", "]", ",", "default_value", "=", "'relu'", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'td3_hidden_size'", ",", "lower", "=", "48", ",", "upper", "=", "256", ",", "log", "=", "True", ",", "default_value", "=", "128", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'td3_hidden_layer'", ",", "lower", "=", "1", ",", "upper", "=", "3", ",", "log", "=", "False", ",", "default_value", "=", "2", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_action_std'", ",", "lower", "=", "0.01", ",", "upper", "=", "1.0", ",", "log", "=", "True", ",", "default_value", "=", "0.1", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_policy_std'", ",", "lower", "=", "0.01", ",", "upper", "=", "1.0", ",", "log", "=", "True", ",", "default_value", "=", "0.2", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_policy_std_clip'", ",", "lower", "=", "0.01", ",", "upper", "=", "1", ",", "log", "=", "True", ",", "default_value", "=", "0.5", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_early_out_virtual_diff'", ",", "lower", "=", "1e-3", ",", "upper", "=", "1e-1", ",", "log", "=", "True", ",", "\n", "default_value", "=", "3e-2", ")", ")", "\n", "\n", "return", "cs", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.bohb_params_TD3_halfcheetah.ExperimentWrapper.get_specific_config": [[48, 67], ["copy.deepcopy"], "methods", ["None"], ["", "def", "get_specific_config", "(", "self", ",", "cso", ",", "default_config", ",", "budget", ")", ":", "\n", "        ", "config", "=", "deepcopy", "(", "default_config", ")", "\n", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'init_episodes'", "]", "=", "cso", "[", "\"td3_init_episodes\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'batch_size'", "]", "=", "cso", "[", "\"td3_batch_size\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'gamma'", "]", "=", "1", "-", "cso", "[", "\"td3_gamma\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'lr'", "]", "=", "cso", "[", "\"td3_lr\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'tau'", "]", "=", "cso", "[", "\"td3_tau\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'policy_delay'", "]", "=", "cso", "[", "\"td3_policy_delay\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'activation_fn'", "]", "=", "cso", "[", "\"td3_activation_fn\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'hidden_size'", "]", "=", "cso", "[", "\"td3_hidden_size\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'hidden_layer'", "]", "=", "cso", "[", "\"td3_hidden_layer\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'action_std'", "]", "=", "cso", "[", "\"td3_action_std\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'policy_std'", "]", "=", "cso", "[", "\"td3_policy_std\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'policy_std_clip'", "]", "=", "cso", "[", "\"td3_policy_std_clip\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'early_out_virtual_diff'", "]", "=", "cso", "[", "\"td3_early_out_virtual_diff\"", "]", "\n", "# config[\"device\"] = 'cuda'", "\n", "\n", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.bohb_params_TD3_halfcheetah.ExperimentWrapper.compute": [[68, 109], ["bohb_params_TD3_halfcheetah.ExperimentWrapper.get_specific_config", "print", "print", "print", "print", "print", "print", "envs.env_factory.EnvFactory", "envs.env_factory.EnvFactory.generate_real_env", "range", "str", "print", "print", "print", "print", "open", "yaml.safe_load", "agents.TD3.TD3", "agents.TD3.TD3.train", "rewards_list.append", "numpy.median", "str", "str", "str", "numpy.mean", "str", "envs.env_factory.EnvFactory.generate_real_env.get_max_action"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_step_size.ExperimentWrapper.get_specific_config", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_real_env", "home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.train", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.get_max_action"], ["", "def", "compute", "(", "self", ",", "working_dir", ",", "bohb_id", ",", "config_id", ",", "cso", ",", "budget", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "with", "open", "(", "\"default_config_halfcheetah.yaml\"", ",", "'r'", ")", "as", "stream", ":", "\n", "            ", "default_config", "=", "yaml", ".", "safe_load", "(", "stream", ")", "\n", "\n", "", "config", "=", "self", ".", "get_specific_config", "(", "cso", ",", "default_config", ",", "budget", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "\"START BOHB ITERATION\"", ")", "\n", "print", "(", "'CONFIG: '", "+", "str", "(", "config", ")", ")", "\n", "print", "(", "'CSO:    '", "+", "str", "(", "cso", ")", ")", "\n", "print", "(", "'BUDGET: '", "+", "str", "(", "budget", ")", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "info", "=", "{", "}", "\n", "\n", "# generate environment", "\n", "env_fac", "=", "EnvFactory", "(", "config", ")", "\n", "real_env", "=", "env_fac", ".", "generate_real_env", "(", ")", "\n", "\n", "# score = 0", "\n", "rewards_list", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "NUM_EVALS", ")", ":", "\n", "            ", "td3", "=", "TD3", "(", "env", "=", "real_env", ",", "\n", "max_action", "=", "real_env", ".", "get_max_action", "(", ")", ",", "\n", "config", "=", "config", ")", "\n", "rewards", ",", "_", ",", "_", "=", "td3", ".", "train", "(", "real_env", ")", "\n", "rewards_list", ".", "append", "(", "np", ".", "mean", "(", "rewards", ")", ")", "\n", "# score += len(rewards)", "\n", "\n", "# score = score/NUM_EVALS", "\n", "", "score", "=", "-", "np", ".", "median", "(", "rewards_list", ")", "\n", "\n", "info", "[", "'config'", "]", "=", "str", "(", "config", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "'FINAL SCORE: '", "+", "str", "(", "score", ")", ")", "\n", "print", "(", "\"END BOHB ITERATION\"", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "return", "{", "\n", "\"loss\"", ":", "score", ",", "\n", "\"info\"", ":", "info", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_halfcheetah_transfer_vary_hp.get_data": [[77, 126], ["float", "print", "print", "torch.load", "list_data.append", "sums_eps_len_per_model.append", "sum", "sum", "numpy.zeros", "enumerate", "numpy.mean", "numpy.std", "proc_data.append", "print", "sums_eps_len.append", "sums_eps_len_per_model_i.append", "min", "zip", "range", "numpy.array", "sum", "sum", "sum", "sum", "numpy.asarray", "numpy.asarray", "len", "len", "concat_list.append"], "function", ["None"], ["def", "get_data", "(", ")", ":", "\n", "    ", "list_data", "=", "[", "]", "\n", "for", "log_file", "in", "LOG_FILES", ":", "\n", "        ", "data", "=", "torch", ".", "load", "(", "log_file", ")", "\n", "list_data", ".", "append", "(", "(", "data", "[", "'reward_list'", "]", ",", "data", "[", "'episode_length_list'", "]", ")", ")", "\n", "model_num", "=", "data", "[", "'model_num'", "]", "\n", "model_agents", "=", "data", "[", "'model_agents'", "]", "\n", "\n", "", "sums_eps_len", "=", "[", "]", "\n", "sums_eps_len_per_model", "=", "[", "]", "\n", "min_steps", "=", "float", "(", "'Inf'", ")", "\n", "# get minimum number of evaluations", "\n", "for", "reward_list", ",", "episode_length_list", "in", "list_data", ":", "\n", "        ", "sums_eps_len_per_model_i", "=", "[", "]", "\n", "for", "episode_lengths", "in", "episode_length_list", ":", "\n", "            ", "print", "(", "sum", "(", "episode_lengths", ")", ")", "\n", "sums_eps_len", ".", "append", "(", "sum", "(", "episode_lengths", ")", ")", "\n", "sums_eps_len_per_model_i", ".", "append", "(", "sum", "(", "episode_lengths", ")", ")", "\n", "min_steps", "=", "min", "(", "min_steps", ",", "sum", "(", "episode_lengths", ")", ")", "\n", "", "sums_eps_len_per_model", ".", "append", "(", "sums_eps_len_per_model_i", ")", "\n", "\n", "", "print", "(", "\"# of episode lens > MIN_STEPS: \"", ",", "sum", "(", "np", ".", "asarray", "(", "sums_eps_len", ")", ">=", "MIN_STEPS", ")", ")", "\n", "print", "(", "\"# of episode lens < MIN_STEPS: \"", ",", "sum", "(", "np", ".", "asarray", "(", "sums_eps_len", ")", "<", "MIN_STEPS", ")", ")", "\n", "# convert data from episodes to steps", "\n", "proc_data", "=", "[", "]", "\n", "\n", "for", "reward_list", ",", "episode_length_list", "in", "list_data", ":", "\n", "        ", "np_data", "=", "np", ".", "zeros", "(", "[", "model_num", "*", "model_agents", ",", "min_steps", "]", ")", "\n", "\n", "for", "it", ",", "data", "in", "enumerate", "(", "zip", "(", "reward_list", ",", "episode_length_list", ")", ")", ":", "\n", "            ", "rewards", ",", "episode_lengths", "=", "data", "\n", "\n", "concat_list", "=", "[", "]", "\n", "rewards", "=", "rewards", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "episode_lengths", ")", ")", ":", "\n", "                ", "concat_list", "+=", "[", "rewards", "[", "i", "]", "]", "*", "episode_lengths", "[", "i", "]", "\n", "\n", "", "while", "len", "(", "concat_list", ")", "<", "min_steps", ":", "\n", "                ", "concat_list", ".", "append", "(", "concat_list", "[", "-", "1", "]", ")", "\n", "\n", "", "np_data", "[", "it", "]", "=", "np", ".", "array", "(", "concat_list", "[", ":", "min_steps", "]", ")", "\n", "\n", "", "mean", "=", "np", ".", "mean", "(", "np_data", ",", "axis", "=", "0", ")", "\n", "std", "=", "np", ".", "std", "(", "np_data", ",", "axis", "=", "0", ")", "\n", "\n", "proc_data", ".", "append", "(", "(", "mean", ",", "std", ")", ")", "\n", "\n", "", "return", "proc_data", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_halfcheetah_transfer_vary_hp.plot_data": [[128, 173], ["matplotlib.subplots", "enumerate", "enumerate", "matplotlib.legend", "matplotlib.subplots_adjust", "matplotlib.title", "matplotlib.xlabel", "matplotlib.xlim", "matplotlib.ylim", "matplotlib.ylabel", "os.path.dirname", "matplotlib.savefig", "matplotlib.show", "legobj.set_linewidth", "os.path.join", "matplotlib.plot", "matplotlib.fill_between", "matplotlib.plot", "matplotlib.fill_between", "matplotlib.plot", "matplotlib.plot", "range", "matplotlib.fill_between", "matplotlib.fill_between", "len", "range", "len", "range", "range", "len", "len"], "function", ["None"], ["", "def", "plot_data", "(", "proc_data", ",", "savefig_name", ")", ":", "\n", "    ", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", "dpi", "=", "600", ",", "figsize", "=", "(", "5", ",", "4", ")", ")", "\n", "colors", "=", "[", "]", "\n", "#", "\n", "# for mean, _ in data_w:", "\n", "#     plt.plot(mean_w)", "\n", "#     colors.append(plt.gca().lines[-1].get_color())", "\n", "\n", "for", "i", ",", "data", "in", "enumerate", "(", "proc_data", ")", ":", "\n", "        ", "mean", ",", "std", "=", "data", "\n", "if", "i", "==", "10", ":", "\n", "            ", "plt", ".", "plot", "(", "mean", ",", "color", "=", "'#575757'", ",", "linewidth", "=", "1", ")", "\n", "", "elif", "i", "==", "11", ":", "\n", "            ", "plt", ".", "plot", "(", "mean", ",", "color", "=", "'#EBBB00'", ",", "linewidth", "=", "1", ")", "\n", "", "elif", "i", "==", "12", ":", "\n", "            ", "plt", ".", "plot", "(", "mean", ",", "color", "=", "\"darkcyan\"", ",", "linewidth", "=", "1", ")", "\n", "", "else", ":", "\n", "            ", "plt", ".", "plot", "(", "mean", ",", "linewidth", "=", "1", ")", "\n", "\n", "", "", "for", "i", ",", "data", "in", "enumerate", "(", "proc_data", ")", ":", "\n", "        ", "mean", ",", "std", "=", "data", "\n", "if", "i", "==", "10", ":", "\n", "            ", "plt", ".", "fill_between", "(", "x", "=", "range", "(", "len", "(", "mean", ")", ")", ",", "y1", "=", "mean", "-", "std", "*", "STD_MULT", ",", "y2", "=", "mean", "+", "std", "*", "STD_MULT", ",", "alpha", "=", "0.2", ",", "color", "=", "'#575757'", ")", "\n", "", "elif", "i", "==", "11", ":", "\n", "            ", "plt", ".", "fill_between", "(", "x", "=", "range", "(", "len", "(", "mean", ")", ")", ",", "y1", "=", "mean", "-", "std", "*", "STD_MULT", ",", "y2", "=", "mean", "+", "std", "*", "STD_MULT", ",", "alpha", "=", "0.2", ",", "color", "=", "'#EBBB00'", ")", "\n", "", "elif", "i", "==", "12", ":", "\n", "            ", "plt", ".", "fill_between", "(", "x", "=", "range", "(", "len", "(", "mean", ")", ")", ",", "y1", "=", "mean", "-", "std", "*", "STD_MULT", ",", "y2", "=", "mean", "+", "std", "*", "STD_MULT", ",", "alpha", "=", "0.2", ",", "color", "=", "\"darkcyan\"", ")", "\n", "", "else", ":", "\n", "            ", "plt", ".", "fill_between", "(", "x", "=", "range", "(", "len", "(", "mean", ")", ")", ",", "y1", "=", "mean", "-", "std", "*", "STD_MULT", ",", "y2", "=", "mean", "+", "std", "*", "STD_MULT", ",", "alpha", "=", "0.2", ")", "\n", "\n", "", "", "leg", "=", "plt", ".", "legend", "(", "LEGEND", ",", "fontsize", "=", "9", ")", "\n", "\n", "for", "legobj", "in", "leg", ".", "legendHandles", ":", "\n", "        ", "legobj", ".", "set_linewidth", "(", "2.0", ")", "\n", "\n", "", "plt", ".", "subplots_adjust", "(", "bottom", "=", "0.15", ",", "left", "=", "0.15", ")", "\n", "plt", ".", "title", "(", "'HalfCheetah-v3 Varied Hyperparameters'", ")", "\n", "plt", ".", "xlabel", "(", "'steps'", ")", "\n", "plt", ".", "xlim", "(", "0", ",", "MIN_STEPS", ")", "\n", "plt", ".", "ylim", "(", "-", "1000", ",", "6000", ")", "\n", "# plt.ylim(-1000, 5000)", "\n", "plt", ".", "ylabel", "(", "'cumulative reward'", ")", "\n", "base_dir", "=", "os", ".", "path", ".", "dirname", "(", "LOG_FILES", "[", "0", "]", ")", "\n", "plt", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "base_dir", ",", "savefig_name", ")", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cmc_transfer_vary_hp.get_best_models_from_log": [[33, 62], ["hpbandster.logged_results_to_HBS_result", "hpres.logged_results_to_HBS_result.data.values", "print", "best_models.sort", "os.path.isdir", "log_dir.replace.replace", "best_models.append", "os.path.isfile", "model_name.replace.replace"], "function", ["None"], ["def", "get_best_models_from_log", "(", "log_dir", ")", ":", "\n", "    ", "if", "not", "os", ".", "path", ".", "isdir", "(", "log_dir", ")", ":", "\n", "        ", "log_dir", "=", "log_dir", ".", "replace", "(", "'nierhoff'", ",", "'dingsda'", ")", "\n", "\n", "", "result", "=", "hpres", ".", "logged_results_to_HBS_result", "(", "log_dir", ")", "\n", "\n", "best_models", "=", "[", "]", "\n", "\n", "for", "value", "in", "result", ".", "data", ".", "values", "(", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "loss", "=", "value", ".", "results", "[", "1.0", "]", "[", "'loss'", "]", "\n", "model_name", "=", "value", ".", "results", "[", "1.0", "]", "[", "'info'", "]", "[", "'model_name'", "]", "\n", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "model_name", ")", ":", "\n", "                ", "model_name", "=", "model_name", ".", "replace", "(", "'nierhoff'", ",", "'dingsda'", ")", "\n", "", "best_models", ".", "append", "(", "(", "loss", ",", "model_name", ")", ")", "\n", "", "except", ":", "\n", "            ", "continue", "\n", "\n", "# before AUC (objective minimized)", "\n", "# print(\"sorting from low to high values (non-AUC)\")", "\n", "# best_models.sort(key=lambda x: x[0])", "\n", "\n", "# AUC (objective maximized)", "\n", "", "", "print", "(", "\"sorting from high to low values (AUC)\"", ")", "\n", "best_models", ".", "sort", "(", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ",", "reverse", "=", "True", ")", "\n", "best_models", "=", "best_models", "[", ":", "MODEL_NUM", "]", "\n", "\n", "return", "best_models", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cmc_transfer_vary_hp.load_envs_and_config": [[64, 77], ["torch.load", "envs.env_factory.EnvFactory", "envs.env_factory.EnvFactory.generate_reward_env", "env_factory.generate_reward_env.load_state_dict", "envs.env_factory.EnvFactory.generate_real_env"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_reward_env", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_real_env"], ["", "def", "load_envs_and_config", "(", "model_file", ")", ":", "\n", "    ", "save_dict", "=", "torch", ".", "load", "(", "model_file", ")", "\n", "\n", "config", "=", "save_dict", "[", "'config'", "]", "\n", "config", "[", "'device'", "]", "=", "'cpu'", "\n", "config", "[", "'envs'", "]", "[", "'MountainCarContinuous-v0'", "]", "[", "'solved_reward'", "]", "=", "100000", "# something big enough to prevent early out triggering", "\n", "\n", "env_factory", "=", "EnvFactory", "(", "config", "=", "config", ")", "\n", "reward_env", "=", "env_factory", ".", "generate_reward_env", "(", ")", "\n", "reward_env", ".", "load_state_dict", "(", "save_dict", "[", "'model'", "]", ")", "\n", "real_env", "=", "env_factory", ".", "generate_real_env", "(", ")", "\n", "\n", "return", "reward_env", ",", "real_env", ",", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cmc_transfer_vary_hp.vary_hp": [[79, 129], ["ConfigSpace.ConfigurationSpace", "CS.ConfigurationSpace.add_hyperparameter", "CS.ConfigurationSpace.add_hyperparameter", "CS.ConfigurationSpace.add_hyperparameter", "CS.ConfigurationSpace.add_hyperparameter", "CS.ConfigurationSpace.sample_configuration", "print", "copy.deepcopy", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "int", "int", "int", "int"], "function", ["None"], ["", "def", "vary_hp", "(", "config", ")", ":", "\n", "    ", "lr", "=", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'lr'", "]", "\n", "batch_size", "=", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'batch_size'", "]", "\n", "hidden_size", "=", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'hidden_size'", "]", "\n", "hidden_layer", "=", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'hidden_layer'", "]", "\n", "\n", "cs", "=", "CS", ".", "ConfigurationSpace", "(", ")", "\n", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'lr'", ",", "\n", "lower", "=", "lr", "/", "3", ",", "\n", "upper", "=", "lr", "*", "3", ",", "\n", "log", "=", "True", ",", "\n", "default_value", "=", "lr", ")", "\n", ")", "\n", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'batch_size'", ",", "\n", "lower", "=", "int", "(", "batch_size", "/", "3", ")", ",", "\n", "upper", "=", "int", "(", "batch_size", "*", "3", ")", ",", "\n", "log", "=", "True", ",", "\n", "default_value", "=", "batch_size", ")", "\n", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'hidden_size'", ",", "\n", "lower", "=", "int", "(", "hidden_size", "/", "3", ")", ",", "\n", "upper", "=", "int", "(", "hidden_size", "*", "3", ")", ",", "\n", "log", "=", "True", ",", "\n", "default_value", "=", "hidden_size", ")", "\n", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'hidden_layer'", ",", "\n", "lower", "=", "hidden_layer", "-", "1", ",", "\n", "upper", "=", "hidden_layer", "+", "1", ",", "\n", "log", "=", "False", ",", "\n", "default_value", "=", "hidden_layer", ")", "\n", ")", "\n", "\n", "sample", "=", "cs", ".", "sample_configuration", "(", ")", "\n", "\n", "print", "(", "f\"sampled part of config: \"", "\n", "f\"lr: {sample['lr']}, \"", "\n", "f\"batch_size: {sample['batch_size']}, \"", "\n", "f\"hidden_size: {sample['hidden_size']}, \"", "\n", "f\"hidden_layer: {sample['hidden_layer']}\"", "\n", ")", "\n", "\n", "config_mod", "=", "deepcopy", "(", "config", ")", "\n", "config_mod", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'lr'", "]", "=", "sample", "[", "'lr'", "]", "\n", "config_mod", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'batch_size'", "]", "=", "sample", "[", "'batch_size'", "]", "\n", "config_mod", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'hidden_size'", "]", "=", "sample", "[", "'hidden_size'", "]", "\n", "config_mod", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'hidden_layer'", "]", "=", "sample", "[", "'hidden_layer'", "]", "\n", "\n", "return", "config_mod", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cmc_transfer_vary_hp.train_test_agents": [[131, 184], ["range", "GTNC_evaluate_cmc_transfer_vary_hp.vary_hp", "agents.agent_utils.select_agent.train", "print", "rewards.append", "episode_lengths.append", "agents.agent_utils.select_agent", "agents.agent_utils.select_agent", "str"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_transfer_vary_hp.vary_hp", "home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.train", "home.repos.pwc.inspect_result.automl_learning_environments.agents.agent_utils.select_agent", "home.repos.pwc.inspect_result.automl_learning_environments.agents.agent_utils.select_agent"], ["", "def", "train_test_agents", "(", "mode", ",", "env", ",", "real_env", ",", "config", ")", ":", "\n", "    ", "rewards", "=", "[", "]", "\n", "episode_lengths", "=", "[", "]", "\n", "\n", "# settings for comparability", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'test_episodes'", "]", "=", "1", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'train_episodes'", "]", "=", "3000", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'print_rate'", "]", "=", "100", "\n", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'lr'", "]", "=", "3e-4", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'tau'", "]", "=", "0.005", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'activation_fn'", "]", "=", "'relu'", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'same_action_num'", "]", "=", "2", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'policy_delay'", "]", "=", "2", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'policy_std_clip'", "]", "=", "0.5", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'policy_std'", "]", "=", "0.2", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'action_std'", "]", "=", "0.1", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'batch_size'", "]", "=", "256", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'gamma'", "]", "=", "0.99", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'rb_size'", "]", "=", "1000000", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'init_episodes'", "]", "=", "50", "# set via time steps in original td3 implementation", "\n", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'early_out_num'", "]", "=", "10", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'early_out_virtual_diff'", "]", "=", "1e-2", "\n", "\n", "# optimized ICM HPs:", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "=", "{", "}", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "[", "'beta'", "]", "=", "0.1", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "[", "'eta'", "]", "=", "0.01", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "[", "'feature_dim'", "]", "=", "32", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "[", "'hidden_size'", "]", "=", "128", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "[", "'lr'", "]", "=", "5e-4", "\n", "\n", "# default ICM HPs:", "\n", "# config['agents']['icm'] = {}", "\n", "# config['agents']['icm']['beta'] = 0.2", "\n", "# config['agents']['icm']['eta'] = 0.5", "\n", "# config['agents']['icm']['feature_dim'] = 64", "\n", "# config['agents']['icm']['hidden_size'] = 128", "\n", "# config['agents']['icm']['lr'] = 1e-4", "\n", "\n", "for", "i", "in", "range", "(", "MODEL_AGENTS", ")", ":", "\n", "        ", "config_mod", "=", "vary_hp", "(", "config", ")", "\n", "\n", "if", "mode", "==", "'-1'", ":", "\n", "            ", "agent", "=", "select_agent", "(", "config", "=", "config_mod", ",", "agent_name", "=", "'td3_icm'", ")", "\n", "", "else", ":", "\n", "            ", "agent", "=", "select_agent", "(", "config", "=", "config_mod", ",", "agent_name", "=", "'td3'", ")", "\n", "", "reward", ",", "episode_length", ",", "_", "=", "agent", ".", "train", "(", "env", "=", "env", ",", "test_env", "=", "real_env", ")", "\n", "print", "(", "'reward: '", "+", "str", "(", "reward", ")", ")", "\n", "rewards", ".", "append", "(", "reward", ")", "\n", "episode_lengths", ".", "append", "(", "episode_length", ")", "\n", "", "return", "rewards", ",", "episode_lengths", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cmc_transfer_vary_hp.save_list": [[186, 196], ["os.makedirs", "os.path.join", "torch.save", "str"], "function", ["None"], ["", "def", "save_list", "(", "mode", ",", "config", ",", "reward_list", ",", "episode_length_list", ")", ":", "\n", "    ", "os", ".", "makedirs", "(", "SAVE_DIR", ",", "exist_ok", "=", "True", ")", "\n", "file_name", "=", "os", ".", "path", ".", "join", "(", "SAVE_DIR", ",", "'best_transfer_vary_hp'", "+", "str", "(", "mode", ")", "+", "'.pt'", ")", "\n", "save_dict", "=", "{", "}", "\n", "save_dict", "[", "'config'", "]", "=", "config", "\n", "save_dict", "[", "'model_num'", "]", "=", "MODEL_NUM", "\n", "save_dict", "[", "'model_agents'", "]", "=", "MODEL_AGENTS", "\n", "save_dict", "[", "'reward_list'", "]", "=", "reward_list", "\n", "save_dict", "[", "'episode_length_list'", "]", "=", "episode_length_list", "\n", "torch", ".", "save", "(", "save_dict", ",", "file_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cmc_transfer_vary_hp.eval_models": [[198, 213], ["GTNC_evaluate_cmc_transfer_vary_hp.get_best_models_from_log", "GTNC_evaluate_cmc_transfer_vary_hp.save_list", "print", "print", "GTNC_evaluate_cmc_transfer_vary_hp.load_envs_and_config", "GTNC_evaluate_cmc_transfer_vary_hp.train_test_agents", "str", "str"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.get_best_models_from_log", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.save_list", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.load_envs_and_config", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.train_test_agents"], ["", "def", "eval_models", "(", "mode", ",", "log_dir", ")", ":", "\n", "    ", "best_models", "=", "get_best_models_from_log", "(", "log_dir", ")", "\n", "\n", "reward_list", "=", "[", "]", "\n", "episode_length_list", "=", "[", "]", "\n", "\n", "for", "best_reward", ",", "model_file", "in", "best_models", ":", "\n", "        ", "print", "(", "'best reward: '", "+", "str", "(", "best_reward", ")", ")", "\n", "print", "(", "'model file: '", "+", "str", "(", "model_file", ")", ")", "\n", "reward_env", ",", "real_env", ",", "config", "=", "load_envs_and_config", "(", "model_file", ")", "\n", "rewards", ",", "episode_lengths", "=", "train_test_agents", "(", "mode", "=", "mode", ",", "env", "=", "reward_env", ",", "real_env", "=", "real_env", ",", "config", "=", "config", ")", "\n", "reward_list", "+=", "rewards", "\n", "episode_length_list", "+=", "episode_lengths", "\n", "\n", "", "save_list", "(", "mode", ",", "config", ",", "reward_list", ",", "episode_length_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cmc_transfer_vary_hp.eval_base": [[215, 228], ["GTNC_evaluate_cmc_transfer_vary_hp.get_best_models_from_log", "range", "GTNC_evaluate_cmc_transfer_vary_hp.save_list", "GTNC_evaluate_cmc_transfer_vary_hp.load_envs_and_config", "GTNC_evaluate_cmc_transfer_vary_hp.train_test_agents"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.get_best_models_from_log", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.save_list", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.load_envs_and_config", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.train_test_agents"], ["", "def", "eval_base", "(", "mode", ",", "log_dir", ")", ":", "\n", "    ", "best_models", "=", "get_best_models_from_log", "(", "log_dir", ")", "\n", "\n", "reward_list", "=", "[", "]", "\n", "episode_length_list", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "MODEL_NUM", ")", ":", "\n", "        ", "reward_env", ",", "real_env", ",", "config", "=", "load_envs_and_config", "(", "best_models", "[", "0", "]", "[", "1", "]", ")", "\n", "rewards", ",", "episode_lengths", "=", "train_test_agents", "(", "mode", "=", "mode", ",", "env", "=", "real_env", ",", "real_env", "=", "real_env", ",", "config", "=", "config", ")", "\n", "reward_list", "+=", "rewards", "\n", "episode_length_list", "+=", "episode_lengths", "\n", "\n", "", "save_list", "(", "mode", ",", "config", ",", "reward_list", ",", "episode_length_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cmc_transfer_vary_hp.eval_icm": [[230, 243], ["GTNC_evaluate_cmc_transfer_vary_hp.get_best_models_from_log", "range", "GTNC_evaluate_cmc_transfer_vary_hp.save_list", "GTNC_evaluate_cmc_transfer_vary_hp.load_envs_and_config", "GTNC_evaluate_cmc_transfer_vary_hp.train_test_agents"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.get_best_models_from_log", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.save_list", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.load_envs_and_config", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.train_test_agents"], ["", "def", "eval_icm", "(", "mode", ",", "log_dir", ")", ":", "\n", "    ", "best_models", "=", "get_best_models_from_log", "(", "log_dir", ")", "\n", "\n", "reward_list", "=", "[", "]", "\n", "episode_length_list", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "MODEL_NUM", ")", ":", "\n", "        ", "reward_env", ",", "real_env", ",", "config", "=", "load_envs_and_config", "(", "best_models", "[", "0", "]", "[", "1", "]", ")", "\n", "rewards", ",", "episode_lengths", "=", "train_test_agents", "(", "mode", "=", "mode", ",", "env", "=", "real_env", ",", "real_env", "=", "real_env", ",", "config", "=", "config", ")", "\n", "reward_list", "+=", "rewards", "\n", "episode_length_list", "+=", "episode_lengths", "\n", "\n", "", "save_list", "(", "mode", ",", "config", ",", "reward_list", ",", "episode_length_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_vary_hp.ExperimentWrapper.get_bohb_parameters": [[17, 26], ["None"], "methods", ["None"], ["    ", "def", "get_bohb_parameters", "(", "self", ")", ":", "\n", "        ", "params", "=", "{", "}", "\n", "params", "[", "'min_budget'", "]", "=", "1", "\n", "params", "[", "'max_budget'", "]", "=", "1", "\n", "params", "[", "'eta'", "]", "=", "2", "\n", "params", "[", "'random_fraction'", "]", "=", "1", "\n", "params", "[", "'iterations'", "]", "=", "10000", "\n", "\n", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_vary_hp.ExperimentWrapper.get_configspace": [[27, 33], ["ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.CategoricalHyperparameter"], "methods", ["None"], ["", "def", "get_configspace", "(", "self", ")", ":", "\n", "        ", "cs", "=", "CS", ".", "ConfigurationSpace", "(", ")", "\n", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "CategoricalHyperparameter", "(", "name", "=", "'ddqn_vary_vary_hp'", ",", "choices", "=", "[", "False", ",", "True", "]", ",", "default_value", "=", "False", ")", ")", "\n", "\n", "return", "cs", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_vary_hp.ExperimentWrapper.get_specific_config": [[34, 44], ["copy.deepcopy"], "methods", ["None"], ["", "def", "get_specific_config", "(", "self", ",", "cso", ",", "default_config", ",", "budget", ")", ":", "\n", "        ", "config", "=", "deepcopy", "(", "default_config", ")", "\n", "\n", "env_name", "=", "config", "[", "\"env_name\"", "]", "\n", "\n", "config", "[", "\"render_env\"", "]", "=", "False", "\n", "\n", "config", "[", "\"agents\"", "]", "[", "'ddqn_vary'", "]", "[", "'vary_hp'", "]", "=", "cso", "[", "\"ddqn_vary_vary_hp\"", "]", "\n", "\n", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_vary_hp.ExperimentWrapper.compute": [[45, 83], ["GTNC_evaluate_cartpole_vary_hp.ExperimentWrapper.get_specific_config", "print", "print", "print", "print", "print", "print", "str", "str", "str", "print", "print", "print", "print", "print", "open", "yaml.safe_load", "agents.GTN.GTN_Master", "agents.GTN.GTN_Master.run", "len", "str", "str", "str", "float", "traceback.format_exc", "print", "str", "str"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_step_size.ExperimentWrapper.get_specific_config", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.run"], ["", "def", "compute", "(", "self", ",", "working_dir", ",", "bohb_id", ",", "config_id", ",", "cso", ",", "budget", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "with", "open", "(", "\"default_config_cartpole.yaml\"", ",", "'r'", ")", "as", "stream", ":", "\n", "            ", "default_config", "=", "yaml", ".", "safe_load", "(", "stream", ")", "\n", "\n", "", "config", "=", "self", ".", "get_specific_config", "(", "cso", ",", "default_config", ",", "budget", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "\"START BOHB ITERATION\"", ")", "\n", "print", "(", "'CONFIG: '", "+", "str", "(", "config", ")", ")", "\n", "print", "(", "'CSO:    '", "+", "str", "(", "cso", ")", ")", "\n", "print", "(", "'BUDGET: '", "+", "str", "(", "budget", ")", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "try", ":", "\n", "            ", "gtn", "=", "GTN_Master", "(", "config", ",", "bohb_id", "=", "bohb_id", ")", "\n", "_", ",", "score_list", "=", "gtn", ".", "run", "(", ")", "\n", "score", "=", "len", "(", "score_list", ")", "\n", "error", "=", "\"\"", "\n", "", "except", ":", "\n", "            ", "score", "=", "float", "(", "'Inf'", ")", "\n", "score_list", "=", "[", "]", "\n", "error", "=", "traceback", ".", "format_exc", "(", ")", "\n", "print", "(", "error", ")", "\n", "\n", "", "info", "=", "{", "}", "\n", "info", "[", "'error'", "]", "=", "str", "(", "error", ")", "\n", "info", "[", "'score_list'", "]", "=", "str", "(", "score_list", ")", "\n", "info", "[", "'config'", "]", "=", "str", "(", "config", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "'FINAL SCORE: '", "+", "str", "(", "score", ")", ")", "\n", "print", "(", "'SCORE LIST:  '", "+", "str", "(", "score_list", ")", ")", "\n", "print", "(", "\"END BOHB ITERATION\"", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "return", "{", "\n", "\"loss\"", ":", "score", ",", "\n", "\"info\"", ":", "info", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_halfcheetah_transfer_vary_hp.get_best_models_from_log": [[44, 73], ["hpbandster.logged_results_to_HBS_result", "hpres.logged_results_to_HBS_result.data.values", "print", "best_models.sort", "os.path.isdir", "log_dir.replace.replace", "best_models.append", "os.path.isfile", "model_name.replace.replace"], "function", ["None"], ["def", "get_best_models_from_log", "(", "log_dir", ")", ":", "\n", "    ", "if", "not", "os", ".", "path", ".", "isdir", "(", "log_dir", ")", ":", "\n", "        ", "log_dir", "=", "log_dir", ".", "replace", "(", "'nierhoff'", ",", "'dingsda'", ")", "\n", "\n", "", "result", "=", "hpres", ".", "logged_results_to_HBS_result", "(", "log_dir", ")", "\n", "\n", "best_models", "=", "[", "]", "\n", "\n", "for", "value", "in", "result", ".", "data", ".", "values", "(", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "loss", "=", "value", ".", "results", "[", "1.0", "]", "[", "'loss'", "]", "\n", "model_name", "=", "value", ".", "results", "[", "1.0", "]", "[", "'info'", "]", "[", "'model_name'", "]", "\n", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "model_name", ")", ":", "\n", "                ", "model_name", "=", "model_name", ".", "replace", "(", "'nierhoff'", ",", "'dingsda'", ")", "\n", "", "best_models", ".", "append", "(", "(", "loss", ",", "model_name", ")", ")", "\n", "", "except", ":", "\n", "            ", "continue", "\n", "\n", "# before AUC (objective minimized)", "\n", "# print(\"sorting from low to high values (non-AUC)\")", "\n", "# best_models.sort(key=lambda x: x[0])", "\n", "\n", "# AUC (objective maximized)", "\n", "", "", "print", "(", "\"sorting from high to low values (AUC)\"", ")", "\n", "best_models", ".", "sort", "(", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ",", "reverse", "=", "True", ")", "\n", "best_models", "=", "best_models", "[", ":", "MODEL_NUM", "]", "\n", "\n", "return", "best_models", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_halfcheetah_transfer_vary_hp.load_envs_and_config": [[75, 88], ["torch.load", "envs.env_factory.EnvFactory", "envs.env_factory.EnvFactory.generate_reward_env", "env_factory.generate_reward_env.load_state_dict", "envs.env_factory.EnvFactory.generate_real_env"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_reward_env", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_real_env"], ["", "def", "load_envs_and_config", "(", "model_file", ")", ":", "\n", "    ", "save_dict", "=", "torch", ".", "load", "(", "model_file", ")", "\n", "\n", "config", "=", "save_dict", "[", "'config'", "]", "\n", "config", "[", "'device'", "]", "=", "'cpu'", "\n", "config", "[", "'envs'", "]", "[", "'HalfCheetah-v3'", "]", "[", "'solved_reward'", "]", "=", "100000", "# something big enough to prevent early out triggering", "\n", "\n", "env_factory", "=", "EnvFactory", "(", "config", "=", "config", ")", "\n", "reward_env", "=", "env_factory", ".", "generate_reward_env", "(", ")", "\n", "reward_env", ".", "load_state_dict", "(", "save_dict", "[", "'model'", "]", ")", "\n", "real_env", "=", "env_factory", ".", "generate_real_env", "(", ")", "\n", "\n", "return", "reward_env", ",", "real_env", ",", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_halfcheetah_transfer_vary_hp.vary_hp": [[90, 140], ["ConfigSpace.ConfigurationSpace", "CS.ConfigurationSpace.add_hyperparameter", "CS.ConfigurationSpace.add_hyperparameter", "CS.ConfigurationSpace.add_hyperparameter", "CS.ConfigurationSpace.add_hyperparameter", "CS.ConfigurationSpace.sample_configuration", "print", "copy.deepcopy", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "int", "int", "int", "int"], "function", ["None"], ["", "def", "vary_hp", "(", "config", ")", ":", "\n", "    ", "lr", "=", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'lr'", "]", "\n", "batch_size", "=", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'batch_size'", "]", "\n", "hidden_size", "=", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'hidden_size'", "]", "\n", "hidden_layer", "=", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'hidden_layer'", "]", "\n", "\n", "cs", "=", "CS", ".", "ConfigurationSpace", "(", ")", "\n", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'lr'", ",", "\n", "lower", "=", "lr", "/", "3", ",", "\n", "upper", "=", "lr", "*", "3", ",", "\n", "log", "=", "True", ",", "\n", "default_value", "=", "lr", ")", "\n", ")", "\n", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'batch_size'", ",", "\n", "lower", "=", "int", "(", "batch_size", "/", "3", ")", ",", "\n", "upper", "=", "int", "(", "batch_size", "*", "3", ")", ",", "\n", "log", "=", "True", ",", "\n", "default_value", "=", "batch_size", ")", "\n", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'hidden_size'", ",", "\n", "lower", "=", "int", "(", "hidden_size", "/", "3", ")", ",", "\n", "upper", "=", "int", "(", "hidden_size", "*", "3", ")", ",", "\n", "log", "=", "True", ",", "\n", "default_value", "=", "hidden_size", ")", "\n", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'hidden_layer'", ",", "\n", "lower", "=", "hidden_layer", "-", "1", ",", "\n", "upper", "=", "hidden_layer", "+", "1", ",", "\n", "log", "=", "False", ",", "\n", "default_value", "=", "hidden_layer", ")", "\n", ")", "\n", "\n", "sample", "=", "cs", ".", "sample_configuration", "(", ")", "\n", "\n", "print", "(", "f\"sampled part of config: \"", "\n", "f\"lr: {sample['lr']}, \"", "\n", "f\"batch_size: {sample['batch_size']}, \"", "\n", "f\"hidden_size: {sample['hidden_size']}, \"", "\n", "f\"hidden_layer: {sample['hidden_layer']}\"", "\n", ")", "\n", "\n", "config_mod", "=", "deepcopy", "(", "config", ")", "\n", "config_mod", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'lr'", "]", "=", "sample", "[", "'lr'", "]", "\n", "config_mod", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'batch_size'", "]", "=", "sample", "[", "'batch_size'", "]", "\n", "config_mod", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'hidden_size'", "]", "=", "sample", "[", "'hidden_size'", "]", "\n", "config_mod", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'hidden_layer'", "]", "=", "sample", "[", "'hidden_layer'", "]", "\n", "\n", "return", "config_mod", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_halfcheetah_transfer_vary_hp.train_test_agents": [[142, 203], ["range", "GTNC_evaluate_halfcheetah_transfer_vary_hp.vary_hp", "agents.agent_utils.select_agent.train", "print", "rewards.append", "episode_lengths.append", "agents.agent_utils.select_agent", "agents.agent_utils.select_agent", "str"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_transfer_vary_hp.vary_hp", "home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.train", "home.repos.pwc.inspect_result.automl_learning_environments.agents.agent_utils.select_agent", "home.repos.pwc.inspect_result.automl_learning_environments.agents.agent_utils.select_agent"], ["", "def", "train_test_agents", "(", "mode", ",", "env", ",", "real_env", ",", "config", ")", ":", "\n", "    ", "rewards", "=", "[", "]", "\n", "episode_lengths", "=", "[", "]", "\n", "\n", "# settings for comparability", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'test_episodes'", "]", "=", "1", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'train_episodes'", "]", "=", "1000", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'print_rate'", "]", "=", "100", "\n", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'lr'", "]", "=", "3e-4", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'tau'", "]", "=", "0.005", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'activation_fn'", "]", "=", "'relu'", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'same_action_num'", "]", "=", "1", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'policy_delay'", "]", "=", "2", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'policy_std_clip'", "]", "=", "0.5", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'policy_std'", "]", "=", "0.2", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'action_std'", "]", "=", "0.1", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'batch_size'", "]", "=", "256", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'gamma'", "]", "=", "0.99", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'rb_size'", "]", "=", "1000000", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'init_episodes'", "]", "=", "20", "# set via time steps in original td3 implementation", "\n", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'early_out_num'", "]", "=", "50", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'early_out_virtual_diff'", "]", "=", "0.02", "\n", "\n", "# optimized ICM HPs:", "\n", "# config['agents']['icm'] = {}", "\n", "# config['agents']['icm']['beta'] = 0.05", "\n", "# config['agents']['icm']['eta'] = 0.01", "\n", "# config['agents']['icm']['feature_dim'] = 32", "\n", "# config['agents']['icm']['hidden_size'] = 128", "\n", "# config['agents']['icm']['lr'] = 1e-5", "\n", "\n", "# optimized ICM HPs (from max. reward run):", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "=", "{", "}", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "[", "'beta'", "]", "=", "0.001", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "[", "'eta'", "]", "=", "0.1", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "[", "'feature_dim'", "]", "=", "32", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "[", "'hidden_size'", "]", "=", "128", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "[", "'lr'", "]", "=", "1e-5", "\n", "\n", "# default ICM HPs:", "\n", "# config['agents']['icm'] = {}", "\n", "# config['agents']['icm']['beta'] = 0.2", "\n", "# config['agents']['icm']['eta'] = 0.5", "\n", "# config['agents']['icm']['feature_dim'] = 64", "\n", "# config['agents']['icm']['hidden_size'] = 128", "\n", "# config['agents']['icm']['lr'] = 1e-4", "\n", "\n", "for", "i", "in", "range", "(", "MODEL_AGENTS", ")", ":", "\n", "        ", "config_mod", "=", "vary_hp", "(", "config", ")", "\n", "\n", "if", "mode", "==", "'-1'", ":", "\n", "            ", "agent", "=", "select_agent", "(", "config", "=", "config_mod", ",", "agent_name", "=", "'td3_icm'", ")", "\n", "", "else", ":", "\n", "            ", "agent", "=", "select_agent", "(", "config", "=", "config_mod", ",", "agent_name", "=", "'td3'", ")", "\n", "", "reward", ",", "episode_length", ",", "_", "=", "agent", ".", "train", "(", "env", "=", "env", ",", "test_env", "=", "real_env", ")", "\n", "print", "(", "'reward: '", "+", "str", "(", "reward", ")", ")", "\n", "rewards", ".", "append", "(", "reward", ")", "\n", "episode_lengths", ".", "append", "(", "episode_length", ")", "\n", "", "return", "rewards", ",", "episode_lengths", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_halfcheetah_transfer_vary_hp.save_list": [[205, 216], ["os.makedirs", "os.path.join", "torch.save", "str"], "function", ["None"], ["", "def", "save_list", "(", "mode", ",", "config", ",", "reward_list", ",", "episode_length_list", ")", ":", "\n", "    ", "os", ".", "makedirs", "(", "SAVE_DIR", ",", "exist_ok", "=", "True", ")", "\n", "# file_name = os.path.join(SAVE_DIR, 'best_transfer_vary_hp' + str(mode) + \"_bohb_max_reward_run\" + '.pt')", "\n", "file_name", "=", "os", ".", "path", ".", "join", "(", "SAVE_DIR", ",", "'best_transfer_vary_hp'", "+", "str", "(", "mode", ")", "+", "'.pt'", ")", "\n", "save_dict", "=", "{", "}", "\n", "save_dict", "[", "'config'", "]", "=", "config", "\n", "save_dict", "[", "'model_num'", "]", "=", "MODEL_NUM", "\n", "save_dict", "[", "'model_agents'", "]", "=", "MODEL_AGENTS", "\n", "save_dict", "[", "'reward_list'", "]", "=", "reward_list", "\n", "save_dict", "[", "'episode_length_list'", "]", "=", "episode_length_list", "\n", "torch", ".", "save", "(", "save_dict", ",", "file_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_halfcheetah_transfer_vary_hp.eval_models": [[218, 233], ["GTNC_evaluate_halfcheetah_transfer_vary_hp.get_best_models_from_log", "GTNC_evaluate_halfcheetah_transfer_vary_hp.save_list", "print", "print", "GTNC_evaluate_halfcheetah_transfer_vary_hp.load_envs_and_config", "GTNC_evaluate_halfcheetah_transfer_vary_hp.train_test_agents", "str", "str"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.get_best_models_from_log", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.save_list", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.load_envs_and_config", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.train_test_agents"], ["", "def", "eval_models", "(", "mode", ",", "log_dir", ")", ":", "\n", "    ", "best_models", "=", "get_best_models_from_log", "(", "log_dir", ")", "\n", "\n", "reward_list", "=", "[", "]", "\n", "episode_length_list", "=", "[", "]", "\n", "\n", "for", "best_reward", ",", "model_file", "in", "best_models", ":", "\n", "        ", "print", "(", "'best reward: '", "+", "str", "(", "best_reward", ")", ")", "\n", "print", "(", "'model file: '", "+", "str", "(", "model_file", ")", ")", "\n", "reward_env", ",", "real_env", ",", "config", "=", "load_envs_and_config", "(", "model_file", ")", "\n", "rewards", ",", "episode_lengths", "=", "train_test_agents", "(", "mode", "=", "mode", ",", "env", "=", "reward_env", ",", "real_env", "=", "real_env", ",", "config", "=", "config", ")", "\n", "reward_list", "+=", "rewards", "\n", "episode_length_list", "+=", "episode_lengths", "\n", "\n", "", "save_list", "(", "mode", ",", "config", ",", "reward_list", ",", "episode_length_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_halfcheetah_transfer_vary_hp.eval_base": [[235, 248], ["GTNC_evaluate_halfcheetah_transfer_vary_hp.get_best_models_from_log", "range", "GTNC_evaluate_halfcheetah_transfer_vary_hp.save_list", "GTNC_evaluate_halfcheetah_transfer_vary_hp.load_envs_and_config", "GTNC_evaluate_halfcheetah_transfer_vary_hp.train_test_agents"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.get_best_models_from_log", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.save_list", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.load_envs_and_config", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.train_test_agents"], ["", "def", "eval_base", "(", "mode", ",", "log_dir", ")", ":", "\n", "    ", "best_models", "=", "get_best_models_from_log", "(", "log_dir", ")", "\n", "\n", "reward_list", "=", "[", "]", "\n", "episode_length_list", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "MODEL_NUM", ")", ":", "\n", "        ", "reward_env", ",", "real_env", ",", "config", "=", "load_envs_and_config", "(", "best_models", "[", "0", "]", "[", "1", "]", ")", "\n", "rewards", ",", "episode_lengths", "=", "train_test_agents", "(", "mode", "=", "mode", ",", "env", "=", "real_env", ",", "real_env", "=", "real_env", ",", "config", "=", "config", ")", "\n", "reward_list", "+=", "rewards", "\n", "episode_length_list", "+=", "episode_lengths", "\n", "\n", "", "save_list", "(", "mode", ",", "config", ",", "reward_list", ",", "episode_length_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_halfcheetah_transfer_vary_hp.eval_icm": [[250, 263], ["GTNC_evaluate_halfcheetah_transfer_vary_hp.get_best_models_from_log", "range", "GTNC_evaluate_halfcheetah_transfer_vary_hp.save_list", "GTNC_evaluate_halfcheetah_transfer_vary_hp.load_envs_and_config", "GTNC_evaluate_halfcheetah_transfer_vary_hp.train_test_agents"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.get_best_models_from_log", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.save_list", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.load_envs_and_config", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.train_test_agents"], ["", "def", "eval_icm", "(", "mode", ",", "log_dir", ")", ":", "\n", "    ", "best_models", "=", "get_best_models_from_log", "(", "log_dir", ")", "\n", "\n", "reward_list", "=", "[", "]", "\n", "episode_length_list", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "MODEL_NUM", ")", ":", "\n", "        ", "reward_env", ",", "real_env", ",", "config", "=", "load_envs_and_config", "(", "best_models", "[", "0", "]", "[", "1", "]", ")", "\n", "rewards", ",", "episode_lengths", "=", "train_test_agents", "(", "mode", "=", "mode", ",", "env", "=", "real_env", ",", "real_env", "=", "real_env", ",", "config", "=", "config", ")", "\n", "reward_list", "+=", "rewards", "\n", "episode_length_list", "+=", "episode_lengths", "\n", "\n", "", "save_list", "(", "mode", ",", "config", ",", "reward_list", ",", "episode_length_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_acrobot_params.ExperimentWrapper.get_bohb_parameters": [[17, 26], ["None"], "methods", ["None"], ["    ", "def", "get_bohb_parameters", "(", "self", ")", ":", "\n", "        ", "params", "=", "{", "}", "\n", "params", "[", "'min_budget'", "]", "=", "1", "\n", "params", "[", "'max_budget'", "]", "=", "3", "\n", "params", "[", "'eta'", "]", "=", "3", "\n", "params", "[", "'random_fraction'", "]", "=", "0.3", "\n", "params", "[", "'iterations'", "]", "=", "10000", "\n", "\n", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_acrobot_params.ExperimentWrapper.get_configspace": [[27, 52], ["ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter"], "methods", ["None"], ["", "def", "get_configspace", "(", "self", ")", ":", "\n", "        ", "cs", "=", "CS", ".", "ConfigurationSpace", "(", ")", "\n", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'gtn_score_transform_type'", ",", "lower", "=", "0", ",", "upper", "=", "7", ",", "log", "=", "False", ",", "default_value", "=", "7", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'gtn_step_size'", ",", "lower", "=", "0.1", ",", "upper", "=", "1", ",", "log", "=", "True", ",", "default_value", "=", "0.5", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "CategoricalHyperparameter", "(", "name", "=", "'gtn_mirrored_sampling'", ",", "choices", "=", "[", "False", ",", "True", "]", ",", "default_value", "=", "True", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'gtn_noise_std'", ",", "lower", "=", "0.01", ",", "upper", "=", "1", ",", "log", "=", "True", ",", "default_value", "=", "0.1", ")", ")", "\n", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'ddqn_init_episodes'", ",", "lower", "=", "1", ",", "upper", "=", "20", ",", "log", "=", "True", ",", "default_value", "=", "10", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'ddqn_batch_size'", ",", "lower", "=", "64", ",", "upper", "=", "256", ",", "log", "=", "False", ",", "default_value", "=", "128", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'ddqn_gamma'", ",", "lower", "=", "0.001", ",", "upper", "=", "0.1", ",", "log", "=", "True", ",", "default_value", "=", "0.01", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'ddqn_lr'", ",", "lower", "=", "1e-4", ",", "upper", "=", "5e-3", ",", "log", "=", "True", ",", "default_value", "=", "1e-3", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'ddqn_tau'", ",", "lower", "=", "0.005", ",", "upper", "=", "0.05", ",", "log", "=", "True", ",", "default_value", "=", "0.01", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'ddqn_eps_init'", ",", "lower", "=", "0.8", ",", "upper", "=", "1", ",", "log", "=", "True", ",", "default_value", "=", "0.9", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'ddqn_eps_min'", ",", "lower", "=", "0.005", ",", "upper", "=", "0.05", ",", "log", "=", "True", ",", "default_value", "=", "0.05", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'ddqn_eps_decay'", ",", "lower", "=", "0.01", ",", "upper", "=", "0.2", ",", "log", "=", "True", ",", "default_value", "=", "0.1", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "CategoricalHyperparameter", "(", "name", "=", "'ddqn_activation_fn'", ",", "choices", "=", "[", "'tanh'", ",", "'relu'", ",", "'leakyrelu'", ",", "'prelu'", "]", ",", "default_value", "=", "'relu'", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'ddqn_hidden_size'", ",", "lower", "=", "48", ",", "upper", "=", "192", ",", "log", "=", "True", ",", "default_value", "=", "128", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'ddqn_hidden_layer'", ",", "lower", "=", "1", ",", "upper", "=", "2", ",", "log", "=", "False", ",", "default_value", "=", "2", ")", ")", "\n", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "CategoricalHyperparameter", "(", "name", "=", "'acrobot_activation_fn'", ",", "choices", "=", "[", "'tanh'", ",", "'relu'", ",", "'leakyrelu'", ",", "'prelu'", "]", ",", "default_value", "=", "'leakyrelu'", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'acrobot_hidden_size'", ",", "lower", "=", "48", ",", "upper", "=", "192", ",", "log", "=", "True", ",", "default_value", "=", "128", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'acrobot_hidden_layer'", ",", "lower", "=", "1", ",", "upper", "=", "2", ",", "log", "=", "False", ",", "default_value", "=", "1", ")", ")", "\n", "\n", "return", "cs", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_acrobot_params.ExperimentWrapper.get_specific_config": [[53, 78], ["copy.deepcopy"], "methods", ["None"], ["", "def", "get_specific_config", "(", "self", ",", "cso", ",", "default_config", ",", "budget", ")", ":", "\n", "        ", "config", "=", "deepcopy", "(", "default_config", ")", "\n", "\n", "config", "[", "\"agents\"", "]", "[", "'gtn'", "]", "[", "'score_transform_type'", "]", "=", "cso", "[", "\"gtn_score_transform_type\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'gtn'", "]", "[", "'step_size'", "]", "=", "cso", "[", "\"gtn_step_size\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'gtn'", "]", "[", "'mirrored_sampling'", "]", "=", "cso", "[", "\"gtn_mirrored_sampling\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'gtn'", "]", "[", "'noise_std'", "]", "=", "cso", "[", "\"gtn_noise_std\"", "]", "\n", "\n", "config", "[", "\"agents\"", "]", "[", "'ddqn'", "]", "[", "'init_episodes'", "]", "=", "cso", "[", "\"ddqn_init_episodes\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'ddqn'", "]", "[", "'batch_size'", "]", "=", "cso", "[", "\"ddqn_batch_size\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'ddqn'", "]", "[", "'gamma'", "]", "=", "1", "-", "cso", "[", "\"ddqn_gamma\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'ddqn'", "]", "[", "'lr'", "]", "=", "cso", "[", "\"ddqn_lr\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'ddqn'", "]", "[", "'tau'", "]", "=", "cso", "[", "\"ddqn_tau\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'ddqn'", "]", "[", "'eps_init'", "]", "=", "cso", "[", "\"ddqn_eps_init\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'ddqn'", "]", "[", "'eps_min'", "]", "=", "cso", "[", "\"ddqn_eps_min\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'ddqn'", "]", "[", "'eps_decay'", "]", "=", "1", "-", "cso", "[", "\"ddqn_eps_decay\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'ddqn'", "]", "[", "'activation_fn'", "]", "=", "cso", "[", "\"ddqn_activation_fn\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'ddqn'", "]", "[", "'hidden_size'", "]", "=", "cso", "[", "\"ddqn_hidden_size\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'ddqn'", "]", "[", "'hidden_layer'", "]", "=", "cso", "[", "\"ddqn_hidden_layer\"", "]", "\n", "\n", "config", "[", "\"envs\"", "]", "[", "'Acrobot-v1'", "]", "[", "'activation_fn'", "]", "=", "cso", "[", "\"acrobot_activation_fn\"", "]", "\n", "config", "[", "\"envs\"", "]", "[", "'Acrobot-v1'", "]", "[", "'hidden_size'", "]", "=", "cso", "[", "\"acrobot_hidden_size\"", "]", "\n", "config", "[", "\"envs\"", "]", "[", "'Acrobot-v1'", "]", "[", "'hidden_layer'", "]", "=", "cso", "[", "\"acrobot_hidden_layer\"", "]", "\n", "\n", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_acrobot_params.ExperimentWrapper.compute": [[79, 119], ["GTNC_evaluate_acrobot_params.ExperimentWrapper.get_specific_config", "print", "print", "print", "print", "print", "print", "str", "str", "str", "print", "print", "print", "print", "print", "open", "yaml.safe_load", "range", "str", "str", "str", "agents.GTN.GTN_Master", "agents.GTN.GTN_Master.run", "len", "float", "traceback.format_exc", "print", "str", "str"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_step_size.ExperimentWrapper.get_specific_config", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.run"], ["", "def", "compute", "(", "self", ",", "working_dir", ",", "bohb_id", ",", "config_id", ",", "cso", ",", "budget", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "with", "open", "(", "\"default_config_acrobot.yaml\"", ",", "'r'", ")", "as", "stream", ":", "\n", "            ", "default_config", "=", "yaml", ".", "safe_load", "(", "stream", ")", "\n", "\n", "", "config", "=", "self", ".", "get_specific_config", "(", "cso", ",", "default_config", ",", "budget", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "\"START BOHB ITERATION\"", ")", "\n", "print", "(", "'CONFIG: '", "+", "str", "(", "config", ")", ")", "\n", "print", "(", "'CSO:    '", "+", "str", "(", "cso", ")", ")", "\n", "print", "(", "'BUDGET: '", "+", "str", "(", "budget", ")", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "try", ":", "\n", "            ", "score", "=", "0", "\n", "for", "_", "in", "range", "(", "3", ")", ":", "\n", "                ", "gtn", "=", "GTN_Master", "(", "config", ",", "bohb_id", "=", "bohb_id", ",", "bohb_working_dir", "=", "working_dir", ")", "\n", "_", ",", "score_list", "=", "gtn", ".", "run", "(", ")", "\n", "score", "+=", "len", "(", "score_list", ")", "\n", "", "error", "=", "\"\"", "\n", "", "except", ":", "\n", "            ", "score", "=", "float", "(", "'Inf'", ")", "\n", "score_list", "=", "[", "]", "\n", "error", "=", "traceback", ".", "format_exc", "(", ")", "\n", "print", "(", "error", ")", "\n", "\n", "", "info", "=", "{", "}", "\n", "info", "[", "'error'", "]", "=", "str", "(", "error", ")", "\n", "info", "[", "'config'", "]", "=", "str", "(", "config", ")", "\n", "info", "[", "'score_list'", "]", "=", "str", "(", "score_list", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "'FINAL SCORE: '", "+", "str", "(", "score", ")", ")", "\n", "print", "(", "'SCORE LIST:  '", "+", "str", "(", "score_list", ")", ")", "\n", "print", "(", "\"END BOHB ITERATION\"", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "return", "{", "\n", "\"loss\"", ":", "score", ",", "\n", "\"info\"", ":", "info", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.bohb_params_TD3_discrete_cartpole.ExperimentWrapper.get_bohb_parameters": [[101, 110], ["None"], "methods", ["None"], ["    ", "def", "get_bohb_parameters", "(", "self", ")", ":", "\n", "        ", "params", "=", "{", "}", "\n", "params", "[", "'min_budget'", "]", "=", "1", "\n", "params", "[", "'max_budget'", "]", "=", "2", "\n", "params", "[", "'eta'", "]", "=", "2", "\n", "params", "[", "'iterations'", "]", "=", "10000", "\n", "params", "[", "'random_fraction'", "]", "=", "0.3", "\n", "\n", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.bohb_params_TD3_discrete_cartpole.ExperimentWrapper.get_configspace": [[111, 133], ["ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.CategoricalHyperparameter"], "methods", ["None"], ["", "def", "get_configspace", "(", "self", ")", ":", "\n", "        ", "cs", "=", "CS", ".", "ConfigurationSpace", "(", ")", "\n", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'init_episodes'", ",", "lower", "=", "1", ",", "upper", "=", "20", ",", "log", "=", "True", ",", "default_value", "=", "10", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'batch_size'", ",", "lower", "=", "64", ",", "upper", "=", "512", ",", "log", "=", "True", ",", "default_value", "=", "128", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'gamma'", ",", "lower", "=", "0.001", ",", "upper", "=", "0.1", ",", "log", "=", "True", ",", "default_value", "=", "0.01", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'lr'", ",", "lower", "=", "1e-6", ",", "upper", "=", "1e-1", ",", "log", "=", "True", ",", "default_value", "=", "5e-4", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'tau'", ",", "lower", "=", "0.001", ",", "upper", "=", "0.1", ",", "log", "=", "True", ",", "default_value", "=", "0.01", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'policy_delay'", ",", "lower", "=", "1", ",", "upper", "=", "5", ",", "log", "=", "False", ",", "default_value", "=", "2", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'hidden_size'", ",", "lower", "=", "64", ",", "upper", "=", "512", ",", "log", "=", "True", ",", "default_value", "=", "128", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "\n", "CSH", ".", "CategoricalHyperparameter", "(", "name", "=", "'activation_fn'", ",", "choices", "=", "[", "'relu'", ",", "'tanh'", ",", "'leakyrelu'", ",", "'prelu'", "]", ",", "default_value", "=", "'tanh'", ")", ")", "\n", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'action_std'", ",", "lower", "=", "0.01", ",", "upper", "=", "10", ",", "log", "=", "True", ",", "default_value", "=", "0.1", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'policy_std'", ",", "lower", "=", "0.01", ",", "upper", "=", "10", ",", "log", "=", "True", ",", "default_value", "=", "0.2", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'policy_std_clip'", ",", "lower", "=", "0.01", ",", "upper", "=", "10", ",", "log", "=", "True", ",", "default_value", "=", "0.5", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "CategoricalHyperparameter", "(", "name", "=", "'gumbel_softmax_hard'", ",", "choices", "=", "[", "True", ",", "False", "]", ",", "default_value", "=", "False", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'gumbel_softmax_temp'", ",", "lower", "=", "0.001", ",", "upper", "=", "10", ",", "log", "=", "True", ",", "\n", "default_value", "=", "1.0", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "CategoricalHyperparameter", "(", "name", "=", "'use_layer_norm'", ",", "choices", "=", "[", "True", ",", "False", "]", ",", "default_value", "=", "True", ")", ")", "\n", "\n", "return", "cs", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.bohb_params_TD3_discrete_cartpole.ExperimentWrapper.get_specific_config": [[134, 153], ["copy.deepcopy"], "methods", ["None"], ["", "def", "get_specific_config", "(", "self", ",", "cso", ",", "default_config", ",", "budget", ")", ":", "\n", "        ", "config", "=", "deepcopy", "(", "default_config", ")", "\n", "\n", "config", "[", "\"agents\"", "]", "[", "\"td3_discrete_vary\"", "]", "[", "\"init_episodes\"", "]", "=", "cso", "[", "\"init_episodes\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"td3_discrete_vary\"", "]", "[", "\"batch_size\"", "]", "=", "cso", "[", "\"batch_size\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"td3_discrete_vary\"", "]", "[", "\"gamma\"", "]", "=", "1", "-", "cso", "[", "\"gamma\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"td3_discrete_vary\"", "]", "[", "\"lr\"", "]", "=", "cso", "[", "\"lr\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"td3_discrete_vary\"", "]", "[", "\"tau\"", "]", "=", "cso", "[", "\"tau\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"td3_discrete_vary\"", "]", "[", "\"policy_delay\"", "]", "=", "cso", "[", "\"policy_delay\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"td3_discrete_vary\"", "]", "[", "\"hidden_size\"", "]", "=", "cso", "[", "\"hidden_size\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"td3_discrete_vary\"", "]", "[", "\"activation_fn\"", "]", "=", "cso", "[", "\"activation_fn\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"td3_discrete_vary\"", "]", "[", "\"action_std\"", "]", "=", "cso", "[", "\"action_std\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"td3_discrete_vary\"", "]", "[", "\"policy_std\"", "]", "=", "cso", "[", "\"policy_std\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"td3_discrete_vary\"", "]", "[", "\"policy_std_clip\"", "]", "=", "cso", "[", "\"policy_std_clip\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"td3_discrete_vary\"", "]", "[", "\"gumbel_softmax_temp\"", "]", "=", "cso", "[", "\"gumbel_softmax_temp\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"td3_discrete_vary\"", "]", "[", "\"gumbel_softmax_hard\"", "]", "=", "cso", "[", "\"gumbel_softmax_hard\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"td3_discrete_vary\"", "]", "[", "\"use_layer_norm\"", "]", "=", "cso", "[", "\"use_layer_norm\"", "]", "\n", "\n", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.bohb_params_TD3_discrete_cartpole.ExperimentWrapper.compute": [[154, 203], ["bohb_params_TD3_discrete_cartpole.ExperimentWrapper.get_specific_config", "print", "print", "print", "print", "print", "print", "bohb_params_TD3_discrete_cartpole.eval_models", "str", "print", "print", "print", "print", "open", "yaml.safe_load", "str", "str", "str", "str"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_step_size.ExperimentWrapper.get_specific_config", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.eval_models"], ["", "def", "compute", "(", "self", ",", "working_dir", ",", "bohb_id", ",", "config_id", ",", "cso", ",", "budget", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "with", "open", "(", "\"default_config_cartpole.yaml\"", ",", "'r'", ")", "as", "stream", ":", "\n", "            ", "default_config", "=", "yaml", ".", "safe_load", "(", "stream", ")", "\n", "\n", "", "config", "=", "self", ".", "get_specific_config", "(", "cso", ",", "default_config", ",", "budget", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "\"START BOHB ITERATION\"", ")", "\n", "print", "(", "'CONFIG: '", "+", "str", "(", "config", ")", ")", "\n", "print", "(", "'CSO:    '", "+", "str", "(", "cso", ")", ")", "\n", "print", "(", "'BUDGET: '", "+", "str", "(", "budget", ")", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "info", "=", "{", "}", "\n", "\n", "# generate environment", "\n", "# env_fac = EnvFactory(config)", "\n", "# env = env_fac.generate_real_env()", "\n", "\n", "# with BOHB, we want it to specify the variation of HPs", "\n", "config", "[", "\"agents\"", "]", "[", "\"td3_discrete_vary\"", "]", "[", "\"vary_hp\"", "]", "=", "False", "\n", "\n", "mean_reward", "=", "eval_models", "(", "2", ",", "'/home/nierhoff/master_thesis/learning_environments/results/GTNC_evaluate_cartpole_2021-01-28'", "\n", "'-18_1'", ",", "config", ")", "\n", "\n", "# td3 = TD3_discrete_vary(env=env,", "\n", "#                         min_action=env.get_min_action(),", "\n", "#                         max_action=env.get_max_action(),", "\n", "#                         config=config)", "\n", "#", "\n", "#", "\n", "# score_list = []", "\n", "# for _ in range(5):", "\n", "#     rewards, _, _ = td3.train(env)", "\n", "#     score_i = len(rewards)", "\n", "#     score_list.append(score_i)", "\n", "#", "\n", "# score = np.mean(score_list)", "\n", "score", "=", "-", "mean_reward", "# BOHB minimizes -> maximize reward", "\n", "\n", "info", "[", "'config'", "]", "=", "str", "(", "config", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "'FINAL SCORE: '", "+", "str", "(", "score", ")", ")", "\n", "print", "(", "\"END BOHB ITERATION\"", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "return", "{", "\n", "\"loss\"", ":", "score", ",", "\n", "\"info\"", ":", "info", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.bohb_params_TD3_discrete_cartpole.get_best_models_from_log": [[26, 50], ["hpbandster.logged_results_to_HBS_result", "hpres.logged_results_to_HBS_result.data.values", "best_models.sort", "os.path.isdir", "log_dir.replace.replace", "best_models.append", "os.path.isfile", "model_name.replace.replace"], "function", ["None"], ["def", "get_best_models_from_log", "(", "log_dir", ")", ":", "\n", "    ", "if", "not", "os", ".", "path", ".", "isdir", "(", "log_dir", ")", ":", "\n", "        ", "log_dir", "=", "log_dir", ".", "replace", "(", "'nierhoff'", ",", "'dingsda'", ")", "\n", "\n", "", "result", "=", "hpres", ".", "logged_results_to_HBS_result", "(", "log_dir", ")", "\n", "\n", "best_models", "=", "[", "]", "\n", "\n", "for", "value", "in", "result", ".", "data", ".", "values", "(", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "loss", "=", "value", ".", "results", "[", "1.0", "]", "[", "'loss'", "]", "\n", "model_name", "=", "value", ".", "results", "[", "1.0", "]", "[", "'info'", "]", "[", "'model_name'", "]", "\n", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "model_name", ")", ":", "\n", "                ", "model_name", "=", "model_name", ".", "replace", "(", "'nierhoff'", ",", "'dingsda'", ")", "\n", "", "best_models", ".", "append", "(", "(", "loss", ",", "model_name", ")", ")", "\n", "", "except", ":", "\n", "            ", "continue", "\n", "\n", "", "", "best_models", ".", "sort", "(", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ")", "\n", "# best_models.sort(key=lambda x: x[0], reverse=True)", "\n", "best_models", "=", "best_models", "[", ":", "MODEL_NUM", "]", "\n", "\n", "return", "best_models", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.bohb_params_TD3_discrete_cartpole.train_test_agents": [[52, 69], ["range", "agents.agent_utils.select_agent", "agents.agent_utils.select_agent.train", "print", "rewards.append", "episode_lengths.append", "sum", "str"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.agents.agent_utils.select_agent", "home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.train"], ["", "def", "train_test_agents", "(", "mode", ",", "env", ",", "real_env", ",", "config", ")", ":", "\n", "    ", "rewards", "=", "[", "]", "\n", "episode_lengths", "=", "[", "]", "\n", "\n", "config", "[", "'device'", "]", "=", "'cuda'", "\n", "config", "[", "'agents'", "]", "[", "\"td3_discrete_vary\"", "]", "[", "\"print_rate\"", "]", "=", "100", "\n", "config", "[", "'agents'", "]", "[", "\"td3_discrete_vary\"", "]", "[", "\"train_episodes\"", "]", "=", "500", "\n", "config", "[", "'agents'", "]", "[", "\"td3_discrete_vary\"", "]", "[", "\"test_episodes\"", "]", "=", "1", "\n", "config", "[", "'envs'", "]", "[", "'CartPole-v0'", "]", "[", "'solved_reward'", "]", "=", "100000", "# something big enough to prevent early out triggering", "\n", "\n", "for", "i", "in", "range", "(", "MODEL_AGENTS", ")", ":", "\n", "        ", "agent", "=", "select_agent", "(", "config", "=", "config", ",", "agent_name", "=", "'td3_discrete_vary'", ")", "\n", "reward", ",", "episode_length", ",", "_", "=", "agent", ".", "train", "(", "env", "=", "env", ",", "test_env", "=", "real_env", ")", "\n", "print", "(", "'reward: '", "+", "str", "(", "reward", ")", ")", "\n", "rewards", ".", "append", "(", "sum", "(", "reward", ")", ")", "\n", "episode_lengths", ".", "append", "(", "episode_length", ")", "\n", "", "return", "rewards", ",", "episode_lengths", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.bohb_params_TD3_discrete_cartpole.load_envs_and_config": [[71, 83], ["torch.load", "envs.env_factory.EnvFactory", "envs.env_factory.EnvFactory.generate_reward_env", "env_factory.generate_reward_env.load_state_dict", "envs.env_factory.EnvFactory.generate_real_env"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_reward_env", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_real_env"], ["", "def", "load_envs_and_config", "(", "model_file", ")", ":", "\n", "    ", "save_dict", "=", "torch", ".", "load", "(", "model_file", ")", "\n", "\n", "config", "=", "save_dict", "[", "'config'", "]", "\n", "config", "[", "'device'", "]", "=", "'cuda'", "\n", "\n", "env_factory", "=", "EnvFactory", "(", "config", "=", "config", ")", "\n", "reward_env", "=", "env_factory", ".", "generate_reward_env", "(", ")", "\n", "reward_env", ".", "load_state_dict", "(", "save_dict", "[", "'model'", "]", ")", "\n", "real_env", "=", "env_factory", ".", "generate_real_env", "(", ")", "\n", "\n", "return", "reward_env", ",", "real_env", ",", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.bohb_params_TD3_discrete_cartpole.eval_models": [[85, 98], ["bohb_params_TD3_discrete_cartpole.get_best_models_from_log", "numpy.mean", "print", "print", "bohb_params_TD3_discrete_cartpole.load_envs_and_config", "bohb_params_TD3_discrete_cartpole.train_test_agents", "str", "str"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.get_best_models_from_log", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.load_envs_and_config", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.train_test_agents"], ["", "def", "eval_models", "(", "mode", ",", "log_dir", ",", "config", ")", ":", "\n", "    ", "best_models", "=", "get_best_models_from_log", "(", "log_dir", ")", "\n", "\n", "rewards", "=", "[", "]", "\n", "\n", "for", "best_reward", ",", "model_file", "in", "best_models", ":", "\n", "        ", "print", "(", "'best reward: '", "+", "str", "(", "best_reward", ")", ")", "\n", "print", "(", "'model file: '", "+", "str", "(", "model_file", ")", ")", "\n", "reward_env", ",", "real_env", ",", "_", "=", "load_envs_and_config", "(", "model_file", ")", "\n", "reward_list", ",", "_", "=", "train_test_agents", "(", "mode", "=", "mode", ",", "env", "=", "reward_env", ",", "real_env", "=", "real_env", ",", "config", "=", "config", ")", "\n", "rewards", "+=", "reward_list", "\n", "\n", "", "return", "np", ".", "mean", "(", "rewards", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2_eval_generalization_gap.load_envs_and_config": [[12, 23], ["os.path.join", "torch.load", "envs.env_factory.EnvFactory", "envs.env_factory.EnvFactory.generate_virtual_env", "env_factory.generate_virtual_env.load_state_dict", "envs.env_factory.EnvFactory.generate_real_env"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_virtual_env", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_real_env"], ["def", "load_envs_and_config", "(", "file_name", ",", "model_dir", ",", "device", ")", ":", "\n", "    ", "file_path", "=", "os", ".", "path", ".", "join", "(", "model_dir", ",", "file_name", ")", "\n", "save_dict", "=", "torch", ".", "load", "(", "file_path", ")", "\n", "config", "=", "save_dict", "[", "'config'", "]", "\n", "config", "[", "'device'", "]", "=", "device", "\n", "env_factory", "=", "EnvFactory", "(", "config", "=", "config", ")", "\n", "virtual_env", "=", "env_factory", ".", "generate_virtual_env", "(", ")", "\n", "virtual_env", ".", "load_state_dict", "(", "save_dict", "[", "'model'", "]", ")", "\n", "real_env", "=", "env_factory", ".", "generate_real_env", "(", ")", "\n", "\n", "return", "virtual_env", ",", "real_env", ",", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2_eval_generalization_gap.train_test_agents": [[25, 62], ["range", "agents.agent_utils.select_agent", "agents.agent_utils.select_agent.train", "agents.agent_utils.select_agent.test", "print", "reward_list.append", "train_steps_needed.append", "episodes_needed.append", "str", "sum", "len"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.agents.agent_utils.select_agent", "home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.train", "home.repos.pwc.inspect_result.automl_learning_environments.agents.base_agent.BaseAgent.test"], ["", "def", "train_test_agents", "(", "train_env", ",", "test_env", ",", "config", ",", "agents_num", ")", ":", "\n", "    ", "reward_list", "=", "[", "]", "\n", "train_steps_needed", "=", "[", "]", "\n", "episodes_needed", "=", "[", "]", "\n", "\n", "# settings for comparability", "\n", "config", "[", "'agents'", "]", "[", "'ddqn_vary'", "]", "[", "'vary_hp'", "]", "=", "False", "# changed", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'print_rate'", "]", "=", "10", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'early_out_num'", "]", "=", "10", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'train_episodes'", "]", "=", "1000", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'init_episodes'", "]", "=", "10", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'test_episodes'", "]", "=", "10", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'early_out_virtual_diff'", "]", "=", "0.01", "\n", "\n", "# fixed hyperparameters", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'batch_size'", "]", "=", "199", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'gamma'", "]", "=", "0.988", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'lr'", "]", "=", "0.000304", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'tau'", "]", "=", "0.00848", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'eps_init'", "]", "=", "0.809", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'eps_min'", "]", "=", "0.0371", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'eps_decay'", "]", "=", "0.961", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'same_action_num'", "]", "=", "1", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'activation_fn'", "]", "=", "\"tanh\"", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'hidden_size'", "]", "=", "57", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'hidden_layer'", "]", "=", "1", "\n", "\n", "for", "i", "in", "range", "(", "agents_num", ")", ":", "\n", "        ", "agent", "=", "select_agent", "(", "config", "=", "config", ",", "agent_name", "=", "'DDQN_vary'", ")", "\n", "reward_train", ",", "episode_length", ",", "_", "=", "agent", ".", "train", "(", "env", "=", "train_env", ")", "\n", "reward", ",", "_", ",", "_", "=", "agent", ".", "test", "(", "env", "=", "test_env", ")", "\n", "print", "(", "'reward: '", "+", "str", "(", "reward", ")", ")", "\n", "reward_list", ".", "append", "(", "reward", ")", "\n", "train_steps_needed", ".", "append", "(", "[", "sum", "(", "episode_length", ")", "]", ")", "\n", "episodes_needed", ".", "append", "(", "[", "(", "len", "(", "reward_train", ")", ")", "]", ")", "\n", "\n", "", "return", "reward_list", ",", "train_steps_needed", ",", "episodes_needed", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_heatmap.get_best_models_from_log": [[25, 49], ["hpbandster.logged_results_to_HBS_result", "hpres.logged_results_to_HBS_result.data.values", "best_models.sort", "os.path.isdir", "log_dir.replace.replace", "best_models.append", "os.path.isfile", "model_name.replace.replace"], "function", ["None"], ["def", "get_best_models_from_log", "(", "log_dir", ")", ":", "\n", "    ", "if", "not", "os", ".", "path", ".", "isdir", "(", "log_dir", ")", ":", "\n", "        ", "log_dir", "=", "log_dir", ".", "replace", "(", "'nierhoff'", ",", "'dingsda'", ")", "\n", "\n", "", "result", "=", "hpres", ".", "logged_results_to_HBS_result", "(", "log_dir", ")", "\n", "\n", "best_models", "=", "[", "]", "\n", "\n", "for", "value", "in", "result", ".", "data", ".", "values", "(", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "loss", "=", "value", ".", "results", "[", "1.0", "]", "[", "'loss'", "]", "\n", "model_name", "=", "value", ".", "results", "[", "1.0", "]", "[", "'info'", "]", "[", "'model_name'", "]", "\n", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "model_name", ")", ":", "\n", "                ", "model_name", "=", "model_name", ".", "replace", "(", "'nierhoff'", ",", "'dingsda'", ")", "\n", "", "best_models", ".", "append", "(", "(", "loss", ",", "model_name", ")", ")", "\n", "", "except", ":", "\n", "            ", "continue", "\n", "\n", "", "", "best_models", ".", "sort", "(", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ")", "\n", "# best_models.sort(key=lambda x: x[0], reverse=True)", "\n", "best_models", "=", "best_models", "[", ":", "MODEL_NUM", "]", "\n", "\n", "return", "best_models", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_heatmap.load_envs_and_config": [[51, 66], ["torch.load", "envs.env_factory.EnvFactory", "envs.env_factory.EnvFactory.generate_reward_env", "env_factory.generate_reward_env.load_state_dict", "envs.env_factory.EnvFactory.generate_real_env"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_reward_env", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_real_env"], ["", "def", "load_envs_and_config", "(", "model_file", ")", ":", "\n", "    ", "save_dict", "=", "torch", ".", "load", "(", "model_file", ")", "\n", "\n", "config", "=", "save_dict", "[", "'config'", "]", "\n", "if", "BREAK", "==", "'solved'", ":", "\n", "        ", "config", "[", "'envs'", "]", "[", "'Cliff'", "]", "[", "'solved_reward'", "]", "=", "-", "20", "# something big enough to prevent early out triggering", "\n", "", "else", ":", "\n", "        ", "config", "[", "'envs'", "]", "[", "'Cliff'", "]", "[", "'solved_reward'", "]", "=", "100000", "# something big enough to prevent early out triggering", "\n", "\n", "", "env_factory", "=", "EnvFactory", "(", "config", "=", "config", ")", "\n", "reward_env", "=", "env_factory", ".", "generate_reward_env", "(", ")", "\n", "reward_env", ".", "load_state_dict", "(", "save_dict", "[", "'model'", "]", ")", "\n", "real_env", "=", "env_factory", ".", "generate_real_env", "(", ")", "\n", "\n", "return", "reward_env", ",", "real_env", ",", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_heatmap.train_test_agents": [[68, 103], ["range", "agents.agent_utils.select_agent", "agents.agent_utils.select_agent.train", "agents.agent_utils.select_agent.test", "replay_buffer.get_all", "state.tolist.tolist", "next_state.tolist.tolist", "state.tolist.append", "states.append", "int", "int", "len"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.agents.agent_utils.select_agent", "home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.train", "home.repos.pwc.inspect_result.automl_learning_environments.agents.base_agent.BaseAgent.test", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.ReplayBuffer.get_all"], ["", "def", "train_test_agents", "(", "env", ",", "real_env", ",", "config", ")", ":", "\n", "    ", "states", "=", "[", "]", "\n", "\n", "# settings for comparability", "\n", "config", "[", "'agents'", "]", "[", "'sarsa'", "]", "=", "{", "}", "\n", "config", "[", "'agents'", "]", "[", "'sarsa'", "]", "[", "'test_episodes'", "]", "=", "1", "\n", "config", "[", "'agents'", "]", "[", "'sarsa'", "]", "[", "'train_episodes'", "]", "=", "200", "\n", "config", "[", "'agents'", "]", "[", "'sarsa'", "]", "[", "'print_rate'", "]", "=", "100", "\n", "config", "[", "'agents'", "]", "[", "'sarsa'", "]", "[", "'init_episodes'", "]", "=", "config", "[", "'agents'", "]", "[", "'ql'", "]", "[", "'init_episodes'", "]", "\n", "config", "[", "'agents'", "]", "[", "'sarsa'", "]", "[", "'batch_size'", "]", "=", "config", "[", "'agents'", "]", "[", "'ql'", "]", "[", "'batch_size'", "]", "\n", "config", "[", "'agents'", "]", "[", "'sarsa'", "]", "[", "'alpha'", "]", "=", "config", "[", "'agents'", "]", "[", "'ql'", "]", "[", "'alpha'", "]", "\n", "config", "[", "'agents'", "]", "[", "'sarsa'", "]", "[", "'gamma'", "]", "=", "config", "[", "'agents'", "]", "[", "'ql'", "]", "[", "'gamma'", "]", "\n", "config", "[", "'agents'", "]", "[", "'sarsa'", "]", "[", "'eps_init'", "]", "=", "config", "[", "'agents'", "]", "[", "'ql'", "]", "[", "'eps_init'", "]", "\n", "config", "[", "'agents'", "]", "[", "'sarsa'", "]", "[", "'eps_min'", "]", "=", "config", "[", "'agents'", "]", "[", "'ql'", "]", "[", "'eps_min'", "]", "\n", "config", "[", "'agents'", "]", "[", "'sarsa'", "]", "[", "'eps_decay'", "]", "=", "config", "[", "'agents'", "]", "[", "'ql'", "]", "[", "'eps_decay'", "]", "\n", "config", "[", "'agents'", "]", "[", "'sarsa'", "]", "[", "'rb_size'", "]", "=", "config", "[", "'agents'", "]", "[", "'ql'", "]", "[", "'rb_size'", "]", "\n", "config", "[", "'agents'", "]", "[", "'sarsa'", "]", "[", "'same_action_num'", "]", "=", "config", "[", "'agents'", "]", "[", "'ql'", "]", "[", "'same_action_num'", "]", "\n", "config", "[", "'agents'", "]", "[", "'sarsa'", "]", "[", "'early_out_num'", "]", "=", "config", "[", "'agents'", "]", "[", "'ql'", "]", "[", "'early_out_num'", "]", "\n", "config", "[", "'agents'", "]", "[", "'sarsa'", "]", "[", "'early_out_virtual_diff'", "]", "=", "config", "[", "'agents'", "]", "[", "'ql'", "]", "[", "'early_out_virtual_diff'", "]", "\n", "\n", "for", "i", "in", "range", "(", "MODEL_AGENTS", ")", ":", "\n", "        ", "agent", "=", "select_agent", "(", "config", "=", "config", ",", "agent_name", "=", "'sarsa'", ")", "\n", "reward", ",", "_", ",", "_", "=", "agent", ".", "train", "(", "env", "=", "env", ",", "test_env", "=", "real_env", ")", "\n", "_", ",", "_", ",", "replay_buffer", "=", "agent", ".", "test", "(", "env", "=", "real_env", ")", "\n", "state", ",", "_", ",", "next_state", ",", "_", ",", "_", "=", "replay_buffer", ".", "get_all", "(", ")", "\n", "state", "=", "state", ".", "tolist", "(", ")", "\n", "next_state", "=", "next_state", ".", "tolist", "(", ")", "\n", "# skip if we could not solve env", "\n", "if", "len", "(", "reward", ")", "==", "config", "[", "'agents'", "]", "[", "'sarsa'", "]", "[", "'train_episodes'", "]", "and", "BREAK", "==", "'solved'", ":", "\n", "            ", "continue", "\n", "", "state", "=", "[", "int", "(", "elem", "[", "0", "]", ")", "for", "elem", "in", "state", "]", "\n", "state", ".", "append", "(", "int", "(", "next_state", "[", "-", "1", "]", "[", "0", "]", ")", ")", "\n", "states", ".", "append", "(", "state", ")", "\n", "\n", "", "return", "states", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_heatmap.save_list": [[105, 116], ["os.makedirs", "os.path.join", "torch.save", "str", "str"], "function", ["None"], ["", "def", "save_list", "(", "config", ",", "state_list", ")", ":", "\n", "    ", "os", ".", "makedirs", "(", "SAVE_DIR", ",", "exist_ok", "=", "True", ")", "\n", "file_name", "=", "os", ".", "path", ".", "join", "(", "SAVE_DIR", ",", "'heatmap_'", "+", "str", "(", "BREAK", ")", "+", "'_'", "+", "str", "(", "MODE", ")", "+", "'.pt'", ")", "\n", "save_dict", "=", "{", "}", "\n", "save_dict", "[", "'config'", "]", "=", "config", "\n", "save_dict", "[", "'model_num'", "]", "=", "MODEL_NUM", "\n", "save_dict", "[", "'model_agents'", "]", "=", "MODEL_AGENTS", "\n", "save_dict", "[", "'mode'", "]", "=", "MODE", "\n", "save_dict", "[", "'break'", "]", "=", "BREAK", "\n", "save_dict", "[", "'state_list'", "]", "=", "state_list", "\n", "torch", ".", "save", "(", "save_dict", ",", "file_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_heatmap.eval_models": [[118, 133], ["GTNC_evaluate_gridworld_heatmap.get_best_models_from_log", "print", "GTNC_evaluate_gridworld_heatmap.save_list", "print", "print", "GTNC_evaluate_gridworld_heatmap.load_envs_and_config", "GTNC_evaluate_gridworld_heatmap.train_test_agents", "str", "str"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.get_best_models_from_log", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.save_list", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.load_envs_and_config", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.train_test_agents"], ["", "def", "eval_models", "(", "log_dir", ")", ":", "\n", "    ", "best_models", "=", "get_best_models_from_log", "(", "log_dir", ")", "\n", "\n", "state_list", "=", "[", "]", "\n", "\n", "for", "best_reward", ",", "model_file", "in", "best_models", ":", "\n", "        ", "print", "(", "'best reward: '", "+", "str", "(", "best_reward", ")", ")", "\n", "print", "(", "'model file: '", "+", "str", "(", "model_file", ")", ")", "\n", "reward_env", ",", "real_env", ",", "config", "=", "load_envs_and_config", "(", "model_file", ")", "\n", "states", "=", "train_test_agents", "(", "env", "=", "reward_env", ",", "real_env", "=", "real_env", ",", "config", "=", "config", ")", "\n", "state_list", "+=", "states", "\n", "\n", "", "print", "(", "state_list", ")", "\n", "\n", "save_list", "(", "config", ",", "state_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cmc_transfer_vary_hp.get_data": [[46, 97], ["float", "print", "print", "max", "torch.load", "list_data.append", "sums_eps_len_per_model.append", "sum", "sum", "numpy.zeros", "enumerate", "numpy.mean", "numpy.std", "proc_data.append", "print", "sums_eps_len.append", "sums_eps_len_per_model_i.append", "min", "zip", "range", "numpy.array", "sum", "sum", "sum", "sum", "numpy.asarray", "numpy.asarray", "len", "len", "concat_list.append"], "function", ["None"], ["def", "get_data", "(", ")", ":", "\n", "    ", "list_data", "=", "[", "]", "\n", "for", "log_file", "in", "LOG_FILES", ":", "\n", "        ", "data", "=", "torch", ".", "load", "(", "log_file", ")", "\n", "list_data", ".", "append", "(", "(", "data", "[", "'reward_list'", "]", ",", "data", "[", "'episode_length_list'", "]", ")", ")", "\n", "model_num", "=", "data", "[", "'model_num'", "]", "\n", "model_agents", "=", "data", "[", "'model_agents'", "]", "\n", "\n", "", "sums_eps_len", "=", "[", "]", "\n", "sums_eps_len_per_model", "=", "[", "]", "\n", "min_steps", "=", "float", "(", "'Inf'", ")", "\n", "# get minimum number of evaluations", "\n", "for", "reward_list", ",", "episode_length_list", "in", "list_data", ":", "\n", "        ", "sums_eps_len_per_model_i", "=", "[", "]", "\n", "for", "episode_lengths", "in", "episode_length_list", ":", "\n", "            ", "print", "(", "sum", "(", "episode_lengths", ")", ")", "\n", "sums_eps_len", ".", "append", "(", "sum", "(", "episode_lengths", ")", ")", "\n", "sums_eps_len_per_model_i", ".", "append", "(", "sum", "(", "episode_lengths", ")", ")", "\n", "min_steps", "=", "min", "(", "min_steps", ",", "sum", "(", "episode_lengths", ")", ")", "\n", "", "sums_eps_len_per_model", ".", "append", "(", "sums_eps_len_per_model_i", ")", "\n", "\n", "", "print", "(", "\"# of episode lens > MIN_STEPS: \"", ",", "sum", "(", "np", ".", "asarray", "(", "sums_eps_len", ")", ">=", "MIN_STEPS", ")", ")", "\n", "print", "(", "\"# of episode lens < MIN_STEPS: \"", ",", "sum", "(", "np", ".", "asarray", "(", "sums_eps_len", ")", "<", "MIN_STEPS", ")", ")", "\n", "\n", "min_steps", "=", "max", "(", "min_steps", ",", "MIN_STEPS", ")", "\n", "# convert data from episodes to steps", "\n", "proc_data", "=", "[", "]", "\n", "\n", "for", "reward_list", ",", "episode_length_list", "in", "list_data", ":", "\n", "        ", "np_data", "=", "np", ".", "zeros", "(", "[", "model_num", "*", "model_agents", ",", "min_steps", "]", ")", "\n", "\n", "for", "it", ",", "data", "in", "enumerate", "(", "zip", "(", "reward_list", ",", "episode_length_list", ")", ")", ":", "\n", "            ", "rewards", ",", "episode_lengths", "=", "data", "\n", "\n", "concat_list", "=", "[", "]", "\n", "rewards", "=", "rewards", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "episode_lengths", ")", ")", ":", "\n", "                ", "concat_list", "+=", "[", "rewards", "[", "i", "]", "]", "*", "episode_lengths", "[", "i", "]", "\n", "\n", "", "while", "len", "(", "concat_list", ")", "<", "min_steps", ":", "\n", "                ", "concat_list", ".", "append", "(", "concat_list", "[", "-", "1", "]", ")", "\n", "\n", "", "np_data", "[", "it", "]", "=", "np", ".", "array", "(", "concat_list", "[", ":", "min_steps", "]", ")", "\n", "\n", "", "mean", "=", "np", ".", "mean", "(", "np_data", ",", "axis", "=", "0", ")", "\n", "std", "=", "np", ".", "std", "(", "np_data", ",", "axis", "=", "0", ")", "\n", "\n", "proc_data", ".", "append", "(", "(", "mean", ",", "std", ")", ")", "\n", "\n", "", "return", "proc_data", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cmc_transfer_vary_hp.plot_data": [[99, 129], ["matplotlib.subplots", "matplotlib.legend", "matplotlib.subplots_adjust", "matplotlib.title", "matplotlib.xlabel", "matplotlib.xlim", "matplotlib.ylim", "matplotlib.ylabel", "os.path.dirname", "matplotlib.savefig", "matplotlib.show", "matplotlib.plot", "matplotlib.fill_between", "legobj.set_linewidth", "os.path.join", "range", "len"], "function", ["None"], ["", "def", "plot_data", "(", "proc_data", ",", "savefig_name", ")", ":", "\n", "    ", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", "dpi", "=", "600", ",", "figsize", "=", "(", "5", ",", "4", ")", ")", "\n", "colors", "=", "[", "]", "\n", "#", "\n", "# for mean, _ in data_w:", "\n", "#     plt.plot(mean_w)", "\n", "#     colors.append(plt.gca().lines[-1].get_color())", "\n", "\n", "for", "mean", ",", "std", "in", "proc_data", ":", "\n", "        ", "plt", ".", "plot", "(", "mean", ",", "linewidth", "=", "1", ")", "\n", "\n", "", "for", "mean", ",", "std", "in", "proc_data", ":", "\n", "        ", "plt", ".", "fill_between", "(", "x", "=", "range", "(", "len", "(", "mean", ")", ")", ",", "y1", "=", "mean", "-", "std", "*", "STD_MULT", ",", "y2", "=", "mean", "+", "std", "*", "STD_MULT", ",", "alpha", "=", "0.3", ")", "\n", "\n", "", "leg", "=", "plt", ".", "legend", "(", "LEGEND", ",", "fontsize", "=", "9", ")", "\n", "\n", "for", "legobj", "in", "leg", ".", "legendHandles", ":", "\n", "        ", "legobj", ".", "set_linewidth", "(", "2.0", ")", "\n", "\n", "# plt.xlim(0,99)", "\n", "", "plt", ".", "subplots_adjust", "(", "bottom", "=", "0.15", ",", "left", "=", "0.15", ")", "\n", "plt", ".", "title", "(", "'MountainCarContinuous-v0 Varied Hyperparameters'", ")", "\n", "plt", ".", "xlabel", "(", "'steps'", ")", "\n", "plt", ".", "xlim", "(", "0", ",", "MIN_STEPS", ")", "\n", "# ax.xaxis.set_tick_params(labelsize='small')", "\n", "plt", ".", "ylim", "(", "-", "75", ",", "100", ")", "\n", "plt", ".", "ylabel", "(", "'cumulative reward'", ")", "\n", "base_dir", "=", "os", ".", "path", ".", "dirname", "(", "LOG_FILES", "[", "0", "]", ")", "\n", "plt", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "base_dir", ",", "savefig_name", ")", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_acrobot_histogram.load_envs_and_config": [[17, 29], ["os.path.join", "torch.load", "envs.env_factory.EnvFactory", "envs.env_factory.EnvFactory.generate_virtual_env", "env_factory.generate_virtual_env.load_state_dict", "envs.env_factory.EnvFactory.generate_real_env"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_virtual_env", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_real_env"], ["def", "load_envs_and_config", "(", "dir", ",", "file_name", ")", ":", "\n", "    ", "file_path", "=", "os", ".", "path", ".", "join", "(", "dir", ",", "file_name", ")", "\n", "save_dict", "=", "torch", ".", "load", "(", "file_path", ")", "\n", "config", "=", "save_dict", "[", "'config'", "]", "\n", "# config['envs']['CartPole-v0']['solved_reward'] = 195", "\n", "# config['envs']['CartPole-v0']['max_steps'] = 200", "\n", "env_factory", "=", "EnvFactory", "(", "config", "=", "config", ")", "\n", "virtual_env", "=", "env_factory", ".", "generate_virtual_env", "(", ")", "\n", "virtual_env", ".", "load_state_dict", "(", "save_dict", "[", "'model'", "]", ")", "\n", "real_env", "=", "env_factory", ".", "generate_real_env", "(", ")", "\n", "\n", "return", "virtual_env", ",", "real_env", ",", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_acrobot_histogram.plot_hist": [[31, 53], ["matplotlib.figure", "matplotlib.hist", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.yscale", "matplotlib.subplots_adjust", "matplotlib.tick_params", "matplotlib.savefig", "matplotlib.show", "max", "min", "matplotlib.hist", "matplotlib.hist", "matplotlib.hist", "matplotlib.legend", "matplotlib.legend", "max", "int", "max", "max", "max", "str", "int", "int", "int", "max", "min", "max", "min", "max", "min", "max", "min"], "function", ["None"], ["", "def", "plot_hist", "(", "h1", ",", "h2", ",", "h1l", ",", "h2l", ",", "h3", "=", "None", ",", "h3l", "=", "None", ",", "xlabel", "=", "None", ",", "save_idx", "=", "None", ",", "agentname", "=", "\"none\"", ")", ":", "\n", "    ", "plt", ".", "figure", "(", "dpi", "=", "600", ",", "figsize", "=", "(", "3.5", ",", "3", ")", ")", "\n", "plt", ".", "hist", "(", "h1", ",", "alpha", "=", "0.8", ",", "bins", "=", "max", "(", "1", ",", "int", "(", "(", "max", "(", "h1", ")", "-", "min", "(", "h1", ")", ")", "/", "BIN_WIDTH", ")", ")", ")", "\n", "\n", "if", "max", "(", "h2", ")", "==", "min", "(", "h2", ")", ":", "# if we had only a single bin", "\n", "        ", "plt", ".", "hist", "(", "h2", ",", "alpha", "=", "0.6", ",", "bins", "=", "max", "(", "1", ",", "int", "(", "(", "max", "(", "h1", ")", "-", "min", "(", "h1", ")", ")", "/", "BIN_WIDTH", ")", ")", ")", "\n", "", "else", ":", "\n", "        ", "plt", ".", "hist", "(", "h2", ",", "alpha", "=", "0.6", ",", "bins", "=", "max", "(", "1", ",", "int", "(", "(", "max", "(", "h2", ")", "-", "min", "(", "h2", ")", ")", "/", "BIN_WIDTH", ")", ")", ")", "\n", "\n", "", "if", "h3", "is", "not", "None", ":", "\n", "        ", "plt", ".", "hist", "(", "h3", ",", "alpha", "=", "0.4", ",", "bins", "=", "max", "(", "1", ",", "int", "(", "(", "max", "(", "h3", ")", "-", "min", "(", "h3", ")", ")", "/", "BIN_WIDTH", ")", ")", ")", "\n", "", "plt", ".", "xlabel", "(", "xlabel", ",", "fontsize", "=", "12", ")", "\n", "plt", ".", "ylabel", "(", "'occurrence'", ",", "fontsize", "=", "12", ")", "\n", "plt", ".", "yscale", "(", "'log'", ")", "\n", "if", "h3", "is", "not", "None", ":", "\n", "        ", "plt", ".", "legend", "(", "(", "h1l", ",", "h2l", ",", "h3l", ")", ",", "loc", "=", "'upper left'", ",", "fontsize", "=", "10", ")", "\n", "", "else", ":", "\n", "        ", "plt", ".", "legend", "(", "(", "h1l", ",", "h2l", ")", ",", "loc", "=", "'upper left'", ",", "fontsize", "=", "10", ")", "\n", "", "plt", ".", "subplots_adjust", "(", "bottom", "=", "0.15", ",", "left", "=", "0.16", ")", "\n", "plt", ".", "tick_params", "(", "labelsize", "=", "12", ")", "\n", "plt", ".", "savefig", "(", "'acrobot_histogram_'", "+", "agentname", "+", "'_'", "+", "'223R5W'", "+", "'_'", "+", "str", "(", "save_idx", ")", "+", "'.png'", ",", "bbox_inches", "=", "'tight'", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_acrobot_histogram.compare_env_output": [[55, 121], ["replay_buffer_train_all.get_all", "replay_buffer_test_all.get_all", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "range", "range", "GTNC_visualize_acrobot_histogram.plot_hist", "len", "virtual_env.step", "virtual_env.step", "len", "next_states_train[].squeeze().detach().numpy", "next_states_test[].squeeze().detach().numpy", "virt_next_states[].squeeze().detach().numpy", "GTNC_visualize_acrobot_histogram.plot_hist", "rewards_train.squeeze().detach().numpy", "rewards_test.squeeze().detach().numpy", "torch.zeros_like.squeeze().detach().numpy", "next_states_train[].squeeze().detach", "next_states_test[].squeeze().detach", "virt_next_states[].squeeze().detach", "len", "rewards_train.squeeze().detach", "rewards_test.squeeze().detach", "torch.zeros_like.squeeze().detach", "next_states_train[].squeeze", "next_states_test[].squeeze", "virt_next_states[].squeeze", "rewards_train.squeeze", "rewards_test.squeeze", "torch.zeros_like.squeeze"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.None.utils.ReplayBuffer.get_all", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.ReplayBuffer.get_all", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cartpole_histogram.plot_hist", "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditEnv.step", "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditEnv.step", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cartpole_histogram.plot_hist"], ["", "def", "compare_env_output", "(", "virtual_env", ",", "replay_buffer_train_all", ",", "replay_buffer_test_all", ",", "agentname", ")", ":", "\n", "    ", "states_train", ",", "actions_train", ",", "next_states_train", ",", "rewards_train", ",", "dones_train", "=", "replay_buffer_train_all", ".", "get_all", "(", ")", "\n", "states_test", ",", "actions_test", ",", "next_states_test", ",", "rewards_test", ",", "dones_test", "=", "replay_buffer_test_all", ".", "get_all", "(", ")", "\n", "\n", "virt_next_states", "=", "torch", ".", "zeros_like", "(", "next_states_test", ")", "\n", "virt_rewards", "=", "torch", ".", "zeros_like", "(", "rewards_test", ")", "\n", "virt_rewards_incorrect", "=", "torch", ".", "zeros_like", "(", "rewards_test", ")", "\n", "virt_dones", "=", "torch", ".", "zeros_like", "(", "dones_test", ")", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "states_test", ")", ")", ":", "\n", "        ", "state", "=", "states_test", "[", "i", "]", "\n", "action", "=", "actions_test", "[", "i", "]", "\n", "\n", "next_state_virtual", ",", "reward_virtual", ",", "done_virtual", "=", "virtual_env", ".", "step", "(", "action", "=", "action", ",", "state", "=", "state", ")", "\n", "virt_next_states", "[", "i", "]", "=", "next_state_virtual", "\n", "virt_rewards", "[", "i", "]", "=", "reward_virtual", "\n", "virt_dones", "[", "i", "]", "=", "done_virtual", "\n", "\n", "_", ",", "reward_virtual_incorrect", ",", "_", "=", "virtual_env", ".", "step", "(", "action", "=", "1", "-", "action", ",", "state", "=", "state", ")", "\n", "virt_rewards_incorrect", "[", "i", "]", "=", "reward_virtual_incorrect", "\n", "\n", "", "for", "i", "in", "range", "(", "len", "(", "next_states_test", "[", "0", "]", ")", ")", ":", "\n", "        ", "trains", "=", "next_states_train", "[", ":", ",", "i", "]", ".", "squeeze", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "tests", "=", "next_states_test", "[", ":", ",", "i", "]", ".", "squeeze", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "virts", "=", "virt_next_states", "[", ":", ",", "i", "]", ".", "squeeze", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "# diffs = diff_next_states[:,i].squeeze().detach().numpy()", "\n", "\n", "# The state consists of the sin() and cos() of the two rotational joint", "\n", "# angles and the joint angular velocities :", "\n", "# [cos(theta1) sin(theta1) cos(theta2) sin(theta2) thetaDot1 thetaDot2].", "\n", "if", "i", "==", "0", ":", "\n", "            ", "plot_name", "=", "'joint 1 [cos]'", "\n", "", "elif", "i", "==", "1", ":", "\n", "            ", "plot_name", "=", "'joint 1 [sin]'", "\n", "", "elif", "i", "==", "2", ":", "\n", "            ", "plot_name", "=", "'joint 2 [cos]'", "\n", "", "elif", "i", "==", "3", ":", "\n", "            ", "plot_name", "=", "'joint 2 [sin]'", "\n", "", "elif", "i", "==", "4", ":", "\n", "            ", "plot_name", "=", "'joint 1 [1/s]'", "\n", "", "elif", "i", "==", "5", ":", "\n", "            ", "plot_name", "=", "'joint 2 [1/s]'", "\n", "\n", "", "plot_hist", "(", "h1", "=", "trains", ",", "\n", "h2", "=", "tests", ",", "\n", "h3", "=", "virts", ",", "\n", "h1l", "=", "'synth. env. (train)'", ",", "\n", "h2l", "=", "'real env. (test)'", ",", "\n", "h3l", "=", "'synth. env. on real. env data'", ",", "\n", "xlabel", "=", "plot_name", ",", "\n", "save_idx", "=", "i", "+", "1", ",", "\n", "agentname", "=", "agentname", ")", "\n", "\n", "# file_path = os.path.join(path, f\"acrobot_histogram_{agentname}_'223R5W'_{i}.png\")", "\n", "# plt.savefig(file_path, bbox_inches='tight', transparent=True)", "\n", "# plt.show()", "\n", "\n", "", "plot_hist", "(", "h1", "=", "rewards_train", ".", "squeeze", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ",", "\n", "h2", "=", "rewards_test", ".", "squeeze", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ",", "\n", "h3", "=", "virt_rewards", ".", "squeeze", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ",", "\n", "h1l", "=", "'synth. env. (train)'", ",", "\n", "h2l", "=", "'real env. (test)'", ",", "\n", "h3l", "=", "'synth. env. on real. env data'", ",", "\n", "xlabel", "=", "'reward'", ",", "\n", "save_idx", "=", "len", "(", "next_states_test", "[", "0", "]", ")", "+", "1", ",", "\n", "agentname", "=", "agentname", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_run_vary_hp.get_all_files": [[8, 30], ["os.listdir", "sorted", "custom_load_envs_and_config", "ValueError", "print", "print", "sorted.append", "len", "len", "len"], "function", ["None"], ["def", "get_all_files", "(", "with_vary_hp", ",", "model_num", ",", "model_dir", ",", "custom_load_envs_and_config", ",", "env_name", ",", "device", ",", "filter_models_list", "=", "None", ")", ":", "\n", "    ", "file_list", "=", "[", "]", "\n", "for", "file_name", "in", "os", ".", "listdir", "(", "model_dir", ")", ":", "\n", "        ", "if", "env_name", "not", "in", "file_name", ":", "\n", "            ", "continue", "\n", "\n", "", "_", ",", "_", ",", "config", "=", "custom_load_envs_and_config", "(", "file_name", "=", "file_name", ",", "model_dir", "=", "model_dir", ",", "device", "=", "device", ")", "\n", "if", "config", "[", "'agents'", "]", "[", "'ddqn_vary'", "]", "[", "'vary_hp'", "]", "==", "with_vary_hp", ":", "\n", "            ", "file_list", ".", "append", "(", "file_name", ")", "\n", "\n", "# sort file list by random characters/digits -> make randomness deterministic", "\n", "", "", "file_list", "=", "sorted", "(", "file_list", ",", "key", "=", "lambda", "elem", ":", "elem", "[", "-", "9", ":", "]", ")", "\n", "if", "len", "(", "file_list", ")", "<", "model_num", "and", "filter_models_list", "is", "None", ":", "\n", "        ", "raise", "ValueError", "(", "\"Not enough saved models\"", ")", "\n", "\n", "", "if", "filter_models_list", "is", "not", "None", ":", "\n", "        ", "filtered_file_list", "=", "[", "f", "for", "f", "in", "file_list", "if", "f", "in", "filter_models_list", "]", "\n", "print", "(", "f\"model files filtered. Old number of models: {len(file_list)} new number of models: {len(filtered_file_list)}\"", ")", "\n", "print", "(", "\"used models: \"", ",", "filtered_file_list", ")", "\n", "return", "filtered_file_list", "\n", "\n", "", "return", "file_list", "[", ":", "model_num", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_run_vary_hp.run_vary_hp": [[32, 137], ["utils.save_lists", "custom_load_envs_and_config", "syn_env_run_vary_hp.get_all_files", "os.listdir", "range", "zip", "range", "custom_load_envs_and_config", "zip", "print", "custom_train_test_agents", "custom_load_envs_and_config", "print", "custom_train_test_agents", "enumerate", "enumerate", "numpy.hstack", "pool.starmap", "numpy.hstack", "numpy.hstack", "pool.starmap", "reward_list.append", "train_steps_needed.append", "episode_length_needed.append", "numpy.hstack", "str", "str", "str", "range", "str", "str", "custom_load_envs_and_config"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.None.utils.save_lists", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_run_vary_hp.get_all_files"], ["", "def", "run_vary_hp", "(", "mode", ",", "experiment_name", ",", "model_num", ",", "agents_num", ",", "model_dir", ",", "custom_load_envs_and_config", ",", "\n", "custom_train_test_agents", ",", "env_name", ",", "pool", "=", "None", ",", "device", "=", "\"cuda\"", ",", "filter_models_list", "=", "None", ",", "correlation_exp", "=", "False", ")", ":", "\n", "    ", "if", "mode", "==", "0", ":", "\n", "        ", "train_on_venv", "=", "False", "\n", "", "elif", "mode", "==", "1", ":", "\n", "        ", "train_on_venv", "=", "True", "\n", "with_vary_hp", "=", "False", "\n", "", "elif", "mode", "==", "2", ":", "\n", "        ", "train_on_venv", "=", "True", "\n", "with_vary_hp", "=", "True", "\n", "\n", "", "env_reward_overview", "=", "{", "}", "\n", "reward_list", "=", "[", "]", "\n", "train_steps_needed", "=", "[", "]", "\n", "episode_length_needed", "=", "[", "]", "\n", "\n", "if", "not", "train_on_venv", ":", "\n", "        ", "file_name", "=", "os", ".", "listdir", "(", "model_dir", ")", "[", "0", "]", "\n", "_", ",", "real_env", ",", "config", "=", "custom_load_envs_and_config", "(", "file_name", "=", "file_name", ",", "model_dir", "=", "model_dir", ",", "device", "=", "device", ")", "\n", "\n", "if", "pool", "is", "None", ":", "\n", "            ", "for", "i", "in", "range", "(", "model_num", ")", ":", "\n", "                ", "print", "(", "'train on {}-th environment'", ".", "format", "(", "i", ")", ")", "\n", "reward_list_i", ",", "train_steps_needed_i", ",", "episode_length_needed_i", "=", "custom_train_test_agents", "(", "train_env", "=", "real_env", ",", "\n", "test_env", "=", "real_env", ",", "\n", "config", "=", "config", ",", "\n", "agents_num", "=", "agents_num", "\n", ")", "\n", "reward_list", "+=", "reward_list_i", "\n", "train_steps_needed", "+=", "train_steps_needed_i", "\n", "episode_length_needed", "+=", "episode_length_needed_i", "\n", "\n", "if", "correlation_exp", ":", "\n", "                    ", "env_reward_overview", "[", "real_env", ".", "env", ".", "env_name", "+", "\"_\"", "+", "str", "(", "i", ")", "]", "=", "{", "}", "\n", "", "else", ":", "\n", "                    ", "env_reward_overview", "[", "real_env", ".", "env", ".", "env_name", "+", "\"_\"", "+", "str", "(", "i", ")", "]", "=", "np", ".", "hstack", "(", "reward_list_i", ")", "\n", "", "", "", "else", ":", "\n", "            ", "reward_list_tpl", ",", "train_steps_needed_tpl", ",", "episode_length_needed_tpl", "=", "zip", "(", "*", "pool", ".", "starmap", "(", "custom_train_test_agents", ",", "\n", "[", "(", "real_env", ",", "real_env", ",", "config", ",", "\n", "agents_num", ")", "\n", "for", "_", "in", "range", "(", "model_num", ")", "]", ")", "\n", ")", "\n", "# starmap/map preservers order of calling", "\n", "for", "i", "in", "range", "(", "model_num", ")", ":", "\n", "                ", "reward_list", "+=", "reward_list_tpl", "[", "i", "]", "\n", "train_steps_needed", "+=", "train_steps_needed_tpl", "[", "i", "]", "\n", "episode_length_needed", "+=", "episode_length_needed_tpl", "[", "i", "]", "\n", "if", "correlation_exp", ":", "\n", "                    ", "env_reward_overview", "[", "real_env", ".", "env", ".", "env_name", "+", "\"_\"", "+", "str", "(", "i", ")", "]", "=", "{", "}", "\n", "", "else", ":", "\n", "                    ", "env_reward_overview", "[", "real_env", ".", "env", ".", "env_name", "+", "\"_\"", "+", "str", "(", "i", ")", "]", "=", "np", ".", "hstack", "(", "reward_list_tpl", "[", "i", "]", ")", "\n", "", "", "", "", "else", ":", "\n", "        ", "file_list", "=", "get_all_files", "(", "with_vary_hp", "=", "with_vary_hp", ",", "model_num", "=", "model_num", ",", "model_dir", "=", "model_dir", ",", "\n", "custom_load_envs_and_config", "=", "custom_load_envs_and_config", ",", "env_name", "=", "env_name", ",", "device", "=", "device", ",", "\n", "filter_models_list", "=", "filter_models_list", ")", "\n", "\n", "if", "pool", "is", "None", ":", "\n", "            ", "for", "file_name", "in", "file_list", ":", "\n", "                ", "virtual_env", ",", "real_env", ",", "config", "=", "custom_load_envs_and_config", "(", "file_name", "=", "file_name", ",", "model_dir", "=", "model_dir", ",", "device", "=", "device", ")", "\n", "print", "(", "'train agents on '", "+", "str", "(", "file_name", ")", ")", "\n", "\n", "reward_list_i", ",", "train_steps_needed_i", ",", "episode_length_needed_i", "=", "custom_train_test_agents", "(", "train_env", "=", "virtual_env", ",", "\n", "test_env", "=", "real_env", ",", "\n", "config", "=", "config", ",", "\n", "agents_num", "=", "agents_num", "\n", ")", "\n", "reward_list", "+=", "reward_list_i", "\n", "train_steps_needed", "+=", "train_steps_needed_i", "\n", "episode_length_needed", "+=", "episode_length_needed_i", "\n", "if", "correlation_exp", ":", "\n", "                    ", "env_reward_overview", "[", "file_name", "]", "=", "{", "}", "\n", "", "else", ":", "\n", "                    ", "env_reward_overview", "[", "file_name", "]", "=", "np", ".", "hstack", "(", "reward_list_i", ")", "\n", "", "", "", "else", ":", "\n", "            ", "_", ",", "_", ",", "config", "=", "custom_load_envs_and_config", "(", "file_name", "=", "file_list", "[", "0", "]", ",", "model_dir", "=", "model_dir", ",", "device", "=", "device", ")", "\n", "\n", "reward_list_tpl", ",", "train_steps_needed_tpl", ",", "episode_length_needed_tpl", "=", "zip", "(", "*", "pool", ".", "starmap", "(", "custom_train_test_agents", ",", "\n", "[", "(", "*", "custom_load_envs_and_config", "(", "file_name", ",", "\n", "model_dir", ",", "\n", "device", ")", ",", "\n", "agents_num", ")", "\n", "for", "file_name", "in", "file_list", "]", "\n", ")", "\n", ")", "\n", "if", "correlation_exp", ":", "\n", "                ", "for", "i", ",", "file_name", "in", "enumerate", "(", "file_list", ")", ":", "\n", "                    ", "reward_list", ".", "append", "(", "reward_list_tpl", "[", "i", "]", ")", "\n", "train_steps_needed", ".", "append", "(", "train_steps_needed_tpl", "[", "i", "]", ")", "\n", "episode_length_needed", ".", "append", "(", "episode_length_needed_tpl", "[", "i", "]", ")", "\n", "env_reward_overview", "[", "file_name", "]", "=", "{", "}", "\n", "", "", "else", ":", "\n", "# starmap/map preservers order of calling", "\n", "                ", "for", "i", ",", "file_name", "in", "enumerate", "(", "file_list", ")", ":", "\n", "                    ", "reward_list", "+=", "reward_list_tpl", "[", "i", "]", "\n", "train_steps_needed", "+=", "train_steps_needed_tpl", "[", "i", "]", "\n", "episode_length_needed", "+=", "episode_length_needed_tpl", "[", "i", "]", "\n", "env_reward_overview", "[", "file_name", "]", "=", "np", ".", "hstack", "(", "reward_list_tpl", "[", "i", "]", ")", "\n", "\n", "", "", "", "", "save_lists", "(", "mode", "=", "mode", ",", "\n", "config", "=", "config", ",", "\n", "reward_list", "=", "reward_list", ",", "\n", "train_steps_needed", "=", "train_steps_needed", ",", "\n", "episode_length_needed", "=", "episode_length_needed", ",", "\n", "env_reward_overview", "=", "env_reward_overview", ",", "\n", "experiment_name", "=", "experiment_name", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cartpole_transfer_algo.auc": [[10, 19], ["None"], "function", ["None"], ["def", "auc", "(", ")", ":", "\n", "    ", "return", "[", "\n", "'../results/3_rn_auc/cartpole_compare_reward_envs/best_transfer_algo1.pt'", ",", "\n", "'../results/3_rn_auc/cartpole_compare_reward_envs/best_transfer_algo2.pt'", ",", "\n", "'../results/3_rn_auc/cartpole_compare_reward_envs/best_transfer_algo5.pt'", ",", "\n", "'../results/3_rn_auc/cartpole_compare_reward_envs/best_transfer_algo6.pt'", ",", "\n", "'../results/0_before_auc/cartpole_compare_reward_envs/best_transfer_algo0.pt'", ",", "\n", "# '../results/cartpole_compare_reward_envs/best_transfer_algo-1.pt',", "\n", "'../results/0_before_auc/cartpole_compare_reward_envs/best_transfer_algo-1_icm_opt.pt'", ",", "# optimizing ICM HPs did not help", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cartpole_transfer_algo.normal": [[22, 31], ["None"], "function", ["None"], ["", "def", "normal", "(", ")", ":", "\n", "    ", "return", "[", "\n", "'../results/0_before_auc/cartpole_compare_reward_envs/best_transfer_algo1.pt'", ",", "\n", "'../results/0_before_auc/cartpole_compare_reward_envs/best_transfer_algo2.pt'", ",", "\n", "'../results/0_before_auc/cartpole_compare_reward_envs/best_transfer_algo5.pt'", ",", "\n", "'../results/0_before_auc/cartpole_compare_reward_envs/best_transfer_algo6.pt'", ",", "\n", "'../results/0_before_auc/cartpole_compare_reward_envs/best_transfer_algo0.pt'", ",", "\n", "# '../results/0_before_auc/cartpole_compare_reward_envs/best_transfer_algo-1.pt',", "\n", "'../results/0_before_auc/cartpole_compare_reward_envs/best_transfer_algo-1_icm_opt.pt'", ",", "# optimizing ICM HPs did not help", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cartpole_transfer_algo.reward_maximization": [[34, 43], ["None"], "function", ["None"], ["", "def", "reward_maximization", "(", ")", ":", "\n", "    ", "return", "[", "\n", "'../results/4_rn_reward/cartpole_compare_reward_envs/best_transfer_algo1.pt'", ",", "\n", "'../results/4_rn_reward/cartpole_compare_reward_envs/best_transfer_algo2.pt'", ",", "\n", "'../results/4_rn_reward/cartpole_compare_reward_envs/best_transfer_algo5.pt'", ",", "\n", "'../results/4_rn_reward/cartpole_compare_reward_envs/best_transfer_algo6.pt'", ",", "\n", "'../results/0_before_auc/cartpole_compare_reward_envs/best_transfer_algo0.pt'", ",", "\n", "# '../results/0_before_auc/cartpole_compare_reward_envs/best_transfer_algo-1.pt',", "\n", "'../results/0_before_auc/cartpole_compare_reward_envs/best_transfer_algo-1_icm_opt.pt'", ",", "# optimizing ICM HPs did not help", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cartpole_transfer_algo.get_data": [[67, 114], ["float", "print", "print", "torch.load", "list_data.append", "sums_eps_len_per_model.append", "sum", "sum", "numpy.zeros", "enumerate", "numpy.mean", "numpy.std", "proc_data.append", "sums_eps_len.append", "sums_eps_len_per_model_i.append", "min", "zip", "range", "numpy.array", "sum", "sum", "sum", "numpy.asarray", "numpy.asarray", "len"], "function", ["None"], ["def", "get_data", "(", ")", ":", "\n", "    ", "list_data", "=", "[", "]", "\n", "for", "log_file", "in", "LOG_FILES", ":", "\n", "        ", "data", "=", "torch", ".", "load", "(", "log_file", ")", "\n", "list_data", ".", "append", "(", "(", "data", "[", "'reward_list'", "]", ",", "data", "[", "'episode_length_list'", "]", ")", ")", "\n", "model_num", "=", "data", "[", "'model_num'", "]", "\n", "model_agents", "=", "data", "[", "'model_agents'", "]", "\n", "\n", "", "sums_eps_len", "=", "[", "]", "\n", "sums_eps_len_per_model", "=", "[", "]", "\n", "min_steps", "=", "float", "(", "'Inf'", ")", "\n", "# get minimum number of evaluations", "\n", "for", "reward_list", ",", "episode_length_list", "in", "list_data", ":", "\n", "        ", "sums_eps_len_per_model_i", "=", "[", "]", "\n", "for", "episode_lengths", "in", "episode_length_list", ":", "\n", "            ", "sums_eps_len", ".", "append", "(", "sum", "(", "episode_lengths", ")", ")", "\n", "sums_eps_len_per_model_i", ".", "append", "(", "sum", "(", "episode_lengths", ")", ")", "\n", "min_steps", "=", "min", "(", "min_steps", ",", "sum", "(", "episode_lengths", ")", ")", "\n", "\n", "", "sums_eps_len_per_model", ".", "append", "(", "sums_eps_len_per_model_i", ")", "\n", "\n", "", "print", "(", "\"# of episode lens > MIN_STEPS: \"", ",", "sum", "(", "np", ".", "asarray", "(", "sums_eps_len", ")", ">=", "10000", ")", ")", "\n", "print", "(", "\"# of episode lens < MIN_STEPS: \"", ",", "sum", "(", "np", ".", "asarray", "(", "sums_eps_len", ")", "<", "10000", ")", ")", "\n", "\n", "# convert data from episodes to steps", "\n", "proc_data", "=", "[", "]", "\n", "\n", "for", "reward_list", ",", "episode_length_list", "in", "list_data", ":", "\n", "        ", "np_data", "=", "np", ".", "zeros", "(", "[", "model_num", "*", "model_agents", ",", "min_steps", "]", ")", "\n", "\n", "for", "it", ",", "data", "in", "enumerate", "(", "zip", "(", "reward_list", ",", "episode_length_list", ")", ")", ":", "\n", "            ", "rewards", ",", "episode_lengths", "=", "data", "\n", "\n", "concat_list", "=", "[", "]", "\n", "rewards", "=", "rewards", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "episode_lengths", ")", ")", ":", "\n", "                ", "concat_list", "+=", "[", "rewards", "[", "i", "]", "]", "*", "episode_lengths", "[", "i", "]", "\n", "\n", "", "np_data", "[", "it", "]", "=", "np", ".", "array", "(", "concat_list", "[", ":", "min_steps", "]", ")", "\n", "\n", "", "mean", "=", "np", ".", "mean", "(", "np_data", ",", "axis", "=", "0", ")", "\n", "std", "=", "np", ".", "std", "(", "np_data", ",", "axis", "=", "0", ")", "\n", "\n", "proc_data", ".", "append", "(", "(", "mean", ",", "std", ")", ")", "\n", "\n", "", "return", "proc_data", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cartpole_transfer_algo.plot_data": [[116, 140], ["matplotlib.subplots", "matplotlib.legend", "matplotlib.subplots_adjust", "matplotlib.title", "matplotlib.xlabel", "matplotlib.xlim", "matplotlib.ylim", "matplotlib.ylabel", "os.path.dirname", "matplotlib.savefig", "matplotlib.show", "matplotlib.plot", "matplotlib.fill_between", "legobj.set_linewidth", "os.path.join", "range", "len"], "function", ["None"], ["", "def", "plot_data", "(", "proc_data", ",", "savefig_name", ")", ":", "\n", "    ", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", "dpi", "=", "600", ",", "figsize", "=", "(", "5", ",", "4", ")", ")", "\n", "\n", "for", "mean", ",", "std", "in", "proc_data", ":", "\n", "        ", "plt", ".", "plot", "(", "mean", ",", "linewidth", "=", "1", ")", "\n", "\n", "", "for", "mean", ",", "std", "in", "proc_data", ":", "\n", "        ", "plt", ".", "fill_between", "(", "x", "=", "range", "(", "len", "(", "mean", ")", ")", ",", "y1", "=", "mean", "-", "std", "*", "STD_MULT", ",", "y2", "=", "mean", "+", "std", "*", "STD_MULT", ",", "alpha", "=", "0.3", ")", "\n", "\n", "", "leg", "=", "plt", ".", "legend", "(", "LEGEND", ",", "fontsize", "=", "9", ")", "\n", "\n", "for", "legobj", "in", "leg", ".", "legendHandles", ":", "\n", "        ", "legobj", ".", "set_linewidth", "(", "2.0", ")", "\n", "\n", "# plt.xlim(0,99)", "\n", "", "plt", ".", "subplots_adjust", "(", "bottom", "=", "0.15", ",", "left", "=", "0.15", ")", "\n", "plt", ".", "title", "(", "'CartPole-v0 Transfer'", ")", "\n", "plt", ".", "xlabel", "(", "'steps'", ")", "\n", "plt", ".", "xlim", "(", "0", ",", "MIN_STEPS", ")", "\n", "plt", ".", "ylim", "(", "0", ",", "210", ")", "\n", "plt", ".", "ylabel", "(", "'cumulative reward'", ")", "\n", "base_dir", "=", "os", ".", "path", ".", "dirname", "(", "LOG_FILES", "[", "0", "]", ")", "\n", "plt", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "base_dir", ",", "savefig_name", ")", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cartpole_threshold.DDQN_noise.__init__": [[20, 45], ["agents.base_agent.BaseAgent.__init__", "models.actor_critic.Critic_DQN().to", "models.actor_critic.Critic_DQN().to", "GTNC_visualize_cartpole_threshold.DDQN_noise.model_target.load_state_dict", "GTNC_visualize_cartpole_threshold.DDQN_noise.reset_optimizer", "GTNC_visualize_cartpole_threshold.DDQN_noise.model.state_dict", "models.actor_critic.Critic_DQN", "models.actor_critic.Critic_DQN"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__", "home.repos.pwc.inspect_result.automl_learning_environments.agents.SAC.SAC.reset_optimizer"], ["    ", "def", "__init__", "(", "self", ",", "env", ",", "config", ")", ":", "\n", "        ", "agent_name", "=", "\"ddqn\"", "\n", "\n", "super", "(", ")", ".", "__init__", "(", "agent_name", "=", "agent_name", ",", "env", "=", "env", ",", "config", "=", "config", ")", "\n", "\n", "ddqn_config", "=", "config", "[", "\"agents\"", "]", "[", "agent_name", "]", "\n", "\n", "self", ".", "init_episodes", "=", "ddqn_config", "[", "\"init_episodes\"", "]", "\n", "self", ".", "batch_size", "=", "ddqn_config", "[", "\"batch_size\"", "]", "\n", "self", ".", "rb_size", "=", "ddqn_config", "[", "\"rb_size\"", "]", "\n", "self", ".", "gamma", "=", "ddqn_config", "[", "\"gamma\"", "]", "\n", "self", ".", "lr", "=", "ddqn_config", "[", "\"lr\"", "]", "\n", "self", ".", "tau", "=", "ddqn_config", "[", "\"tau\"", "]", "\n", "self", ".", "eps", "=", "ddqn_config", "[", "\"eps_init\"", "]", "\n", "self", ".", "eps_init", "=", "ddqn_config", "[", "\"eps_init\"", "]", "\n", "self", ".", "eps_min", "=", "ddqn_config", "[", "\"eps_min\"", "]", "\n", "self", ".", "eps_decay", "=", "ddqn_config", "[", "\"eps_decay\"", "]", "\n", "\n", "self", ".", "model", "=", "Critic_DQN", "(", "self", ".", "state_dim", ",", "self", ".", "action_dim", ",", "agent_name", ",", "config", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "model_target", "=", "Critic_DQN", "(", "self", ".", "state_dim", ",", "self", ".", "action_dim", ",", "agent_name", ",", "config", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "model_target", ".", "load_state_dict", "(", "self", ".", "model", ".", "state_dict", "(", ")", ")", "\n", "\n", "self", ".", "reset_optimizer", "(", ")", "\n", "\n", "self", ".", "it", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cartpole_threshold.DDQN_noise.train": [[46, 100], ["time.time", "utils.ReplayBuffer", "utils.AverageMeter", "range", "env.close", "env.has_discrete_state_space", "env.has_discrete_action_space", "GTNC_visualize_cartpole_threshold.DDQN_noise.time_is_up", "GTNC_visualize_cartpole_threshold.DDQN_noise.update_parameters_per_episode", "env.reset", "range", "utils.AverageMeter.update", "GTNC_visualize_cartpole_threshold.DDQN_noise.env_solved", "utils.AverageMeter.get_raw_data", "env.max_episode_steps", "GTNC_visualize_cartpole_threshold.DDQN_noise.select_train_action", "env.step", "GTNC_visualize_cartpole_threshold.DDQN_noise.apply_noise", "utils.ReplayBuffer.add", "env.render", "GTNC_visualize_cartpole_threshold.DDQN_noise.learn", "time.time"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.reward_env.RewardEnv.close", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.has_discrete_state_space", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.has_discrete_action_space", "home.repos.pwc.inspect_result.automl_learning_environments.agents.base_agent.BaseAgent.time_is_up", "home.repos.pwc.inspect_result.automl_learning_environments.agents.DDQN.DDQN.update_parameters_per_episode", "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditEnv.reset", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.AverageMeter.update", "home.repos.pwc.inspect_result.automl_learning_environments.agents.base_agent.BaseAgent.env_solved", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.AverageMeter.get_raw_data", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.max_episode_steps", "home.repos.pwc.inspect_result.automl_learning_environments.agents.PPO.PPO.select_train_action", "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditEnv.step", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cartpole_threshold.DDQN_noise.apply_noise", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.ReplayBuffer.add", "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditEnv.render", "home.repos.pwc.inspect_result.automl_learning_environments.agents.PPO.PPO.learn"], ["", "def", "train", "(", "self", ",", "env", ",", "time_remaining", "=", "1e9", ",", "noise_type", "=", "None", ",", "noise_value", "=", "None", ")", ":", "\n", "        ", "time_start", "=", "time", ".", "time", "(", ")", "\n", "\n", "sd", "=", "1", "if", "env", ".", "has_discrete_state_space", "(", ")", "else", "self", ".", "state_dim", "\n", "ad", "=", "1", "if", "env", ".", "has_discrete_action_space", "(", ")", "else", "self", ".", "action_dim", "\n", "replay_buffer", "=", "ReplayBuffer", "(", "state_dim", "=", "sd", ",", "action_dim", "=", "ad", ",", "device", "=", "self", ".", "device", ",", "max_size", "=", "self", ".", "rb_size", ")", "\n", "\n", "avg_meter_reward", "=", "AverageMeter", "(", "print_str", "=", "\"Average reward: \"", ")", "\n", "\n", "# training loop", "\n", "for", "episode", "in", "range", "(", "self", ".", "train_episodes", ")", ":", "\n", "# early out if timeout", "\n", "            ", "if", "self", ".", "time_is_up", "(", "avg_meter_reward", "=", "avg_meter_reward", ",", "\n", "max_episodes", "=", "self", ".", "train_episodes", ",", "\n", "time_elapsed", "=", "time", ".", "time", "(", ")", "-", "time_start", ",", "\n", "time_remaining", "=", "time_remaining", ")", ":", "\n", "                ", "break", "\n", "\n", "", "self", ".", "update_parameters_per_episode", "(", "episode", "=", "episode", ")", "\n", "\n", "state", "=", "env", ".", "reset", "(", ")", "\n", "episode_reward", "=", "0", "\n", "\n", "for", "t", "in", "range", "(", "0", ",", "env", ".", "max_episode_steps", "(", ")", ")", ":", "\n", "                ", "action", "=", "self", ".", "select_train_action", "(", "state", "=", "state", ",", "env", "=", "env", ")", "\n", "\n", "# live view", "\n", "if", "self", ".", "render_env", "and", "episode", "%", "10", "==", "0", ":", "\n", "                    ", "env", ".", "render", "(", ")", "\n", "\n", "# state-action transition", "\n", "", "next_state", ",", "reward", ",", "done", "=", "env", ".", "step", "(", "action", "=", "action", ")", "\n", "next_state", ",", "reward", "=", "self", ".", "apply_noise", "(", "next_state", "=", "next_state", ",", "reward", "=", "reward", ",", "noise_type", "=", "noise_type", ",", "noise_value", "=", "noise_value", ")", "\n", "replay_buffer", ".", "add", "(", "state", "=", "state", ",", "action", "=", "action", ",", "next_state", "=", "next_state", ",", "reward", "=", "reward", ",", "done", "=", "done", ")", "\n", "state", "=", "next_state", "\n", "episode_reward", "+=", "reward", "\n", "\n", "# train", "\n", "if", "episode", ">=", "self", ".", "init_episodes", ":", "\n", "                    ", "self", ".", "learn", "(", "replay_buffer", "=", "replay_buffer", ",", "env", "=", "env", ")", "\n", "\n", "", "if", "done", ">", "0.5", ":", "\n", "                    ", "break", "\n", "\n", "# logging", "\n", "", "", "avg_meter_reward", ".", "update", "(", "episode_reward", ",", "print_rate", "=", "self", ".", "print_rate", ")", "\n", "\n", "# quit training if environment is solved", "\n", "if", "self", ".", "env_solved", "(", "env", "=", "env", ",", "avg_meter_reward", "=", "avg_meter_reward", ",", "episode", "=", "episode", ")", ":", "\n", "                ", "break", "\n", "\n", "", "", "env", ".", "close", "(", ")", "\n", "\n", "return", "avg_meter_reward", ".", "get_raw_data", "(", ")", ",", "replay_buffer", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cartpole_threshold.DDQN_noise.apply_noise": [[101, 110], ["len", "torch.randn().item", "torch.randn().item", "torch.randn().item", "torch.randn().item", "torch.randn().item", "torch.randn().item", "torch.randn().item", "torch.randn().item", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn"], "methods", ["None"], ["", "def", "apply_noise", "(", "self", ",", "next_state", ",", "reward", ",", "noise_type", ",", "noise_value", ")", ":", "\n", "        ", "if", "noise_type", "==", "None", "or", "noise_value", "==", "None", ":", "\n", "            ", "return", "next_state", ",", "reward", "\n", "", "elif", "noise_type", "<", "len", "(", "next_state", ")", ":", "\n", "            ", "next_state", "[", "noise_type", "]", "+=", "torch", ".", "randn", "(", "1", ")", ".", "item", "(", ")", "*", "noise_value", "\n", "", "else", ":", "\n", "            ", "reward", "+=", "torch", ".", "randn", "(", "1", ")", ".", "item", "(", ")", "*", "noise_value", "\n", "\n", "", "return", "next_state", ",", "reward", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cartpole_threshold.DDQN_noise.learn": [[111, 143], ["replay_buffer.sample", "utils.to_one_hot_encoding.squeeze", "actions.squeeze.squeeze.squeeze", "utils.to_one_hot_encoding.squeeze", "rewards.squeeze.squeeze.squeeze", "dones.squeeze.squeeze.squeeze", "env.has_discrete_state_space", "GTNC_visualize_cartpole_threshold.DDQN_noise.model", "GTNC_visualize_cartpole_threshold.DDQN_noise.model", "GTNC_visualize_cartpole_threshold.DDQN_noise.model_target", "GTNC_visualize_cartpole_threshold.DDQN_noise.gather().squeeze", "GTNC_visualize_cartpole_threshold.DDQN_noise.gather().squeeze", "torch.mse_loss", "torch.mse_loss", "GTNC_visualize_cartpole_threshold.DDQN_noise.optimizer.zero_grad", "torch.mse_loss.backward", "GTNC_visualize_cartpole_threshold.DDQN_noise.optimizer.step", "zip", "utils.to_one_hot_encoding", "utils.to_one_hot_encoding", "expected_q_value.detach", "GTNC_visualize_cartpole_threshold.DDQN_noise.model_target.parameters", "GTNC_visualize_cartpole_threshold.DDQN_noise.model.parameters", "target_param.data.copy_", "GTNC_visualize_cartpole_threshold.DDQN_noise.gather", "GTNC_visualize_cartpole_threshold.DDQN_noise.gather", "actions.squeeze.squeeze.long().unsqueeze", "[].unsqueeze", "actions.squeeze.squeeze.long", "GTNC_visualize_cartpole_threshold.DDQN_noise.max"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.None.utils.ReplayBuffer.sample", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.has_discrete_state_space", "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditEnv.step", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.to_one_hot_encoding", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.to_one_hot_encoding"], ["", "def", "learn", "(", "self", ",", "replay_buffer", ",", "env", ")", ":", "\n", "        ", "self", ".", "it", "+=", "1", "\n", "\n", "states", ",", "actions", ",", "next_states", ",", "rewards", ",", "dones", "=", "replay_buffer", ".", "sample", "(", "self", ".", "batch_size", ")", "\n", "\n", "states", "=", "states", ".", "squeeze", "(", ")", "\n", "actions", "=", "actions", ".", "squeeze", "(", ")", "\n", "next_states", "=", "next_states", ".", "squeeze", "(", ")", "\n", "rewards", "=", "rewards", ".", "squeeze", "(", ")", "\n", "dones", "=", "dones", ".", "squeeze", "(", ")", "\n", "\n", "if", "env", ".", "has_discrete_state_space", "(", ")", ":", "\n", "            ", "states", "=", "to_one_hot_encoding", "(", "states", ",", "self", ".", "state_dim", ")", "\n", "next_states", "=", "to_one_hot_encoding", "(", "next_states", ",", "self", ".", "state_dim", ")", "\n", "\n", "", "q_values", "=", "self", ".", "model", "(", "states", ")", "\n", "next_q_values", "=", "self", ".", "model", "(", "next_states", ")", "\n", "next_q_state_values", "=", "self", ".", "model_target", "(", "next_states", ")", "\n", "\n", "q_value", "=", "q_values", ".", "gather", "(", "1", ",", "actions", ".", "long", "(", ")", ".", "unsqueeze", "(", "1", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "next_q_value", "=", "next_q_state_values", ".", "gather", "(", "1", ",", "next_q_values", ".", "max", "(", "1", ")", "[", "1", "]", ".", "unsqueeze", "(", "1", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "expected_q_value", "=", "rewards", "+", "self", ".", "gamma", "*", "next_q_value", "*", "(", "1", "-", "dones", ")", "\n", "loss", "=", "F", ".", "mse_loss", "(", "q_value", ",", "expected_q_value", ".", "detach", "(", ")", ")", "\n", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "# target network update", "\n", "for", "target_param", ",", "param", "in", "zip", "(", "self", ".", "model_target", ".", "parameters", "(", ")", ",", "self", ".", "model", ".", "parameters", "(", ")", ")", ":", "\n", "            ", "target_param", ".", "data", ".", "copy_", "(", "self", ".", "tau", "*", "param", "+", "(", "1", "-", "self", ".", "tau", ")", "*", "target_param", ")", "\n", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cartpole_threshold.DDQN_noise.select_train_action": [[144, 152], ["random.random", "env.get_random_action", "env.has_discrete_state_space", "GTNC_visualize_cartpole_threshold.DDQN_noise.model", "torch.argmax().unsqueeze().detach", "torch.argmax().unsqueeze().detach", "torch.argmax().unsqueeze().detach", "torch.argmax().unsqueeze().detach", "utils.to_one_hot_encoding", "utils.to_one_hot_encoding.to", "torch.argmax().unsqueeze", "torch.argmax().unsqueeze", "torch.argmax().unsqueeze", "torch.argmax().unsqueeze", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.get_random_action", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.has_discrete_state_space", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.to_one_hot_encoding"], ["", "def", "select_train_action", "(", "self", ",", "state", ",", "env", ")", ":", "\n", "        ", "if", "random", ".", "random", "(", ")", "<", "self", ".", "eps", ":", "\n", "            ", "return", "env", ".", "get_random_action", "(", ")", "\n", "", "else", ":", "\n", "            ", "if", "env", ".", "has_discrete_state_space", "(", ")", ":", "\n", "                ", "state", "=", "to_one_hot_encoding", "(", "state", ",", "self", ".", "state_dim", ")", "\n", "", "qvals", "=", "self", ".", "model", "(", "state", ".", "to", "(", "self", ".", "device", ")", ")", "\n", "return", "torch", ".", "argmax", "(", "qvals", ")", ".", "unsqueeze", "(", "0", ")", ".", "detach", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cartpole_threshold.DDQN_noise.select_test_action": [[153, 158], ["env.has_discrete_state_space", "GTNC_visualize_cartpole_threshold.DDQN_noise.model", "torch.argmax().unsqueeze().detach", "torch.argmax().unsqueeze().detach", "torch.argmax().unsqueeze().detach", "torch.argmax().unsqueeze().detach", "utils.to_one_hot_encoding", "utils.to_one_hot_encoding.to", "torch.argmax().unsqueeze", "torch.argmax().unsqueeze", "torch.argmax().unsqueeze", "torch.argmax().unsqueeze", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.has_discrete_state_space", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.to_one_hot_encoding"], ["", "", "def", "select_test_action", "(", "self", ",", "state", ",", "env", ")", ":", "\n", "        ", "if", "env", ".", "has_discrete_state_space", "(", ")", ":", "\n", "            ", "state", "=", "to_one_hot_encoding", "(", "state", ",", "self", ".", "state_dim", ")", "\n", "", "qvals", "=", "self", ".", "model", "(", "state", ".", "to", "(", "self", ".", "device", ")", ")", "\n", "return", "torch", ".", "argmax", "(", "qvals", ")", ".", "unsqueeze", "(", "0", ")", ".", "detach", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cartpole_threshold.DDQN_noise.update_parameters_per_episode": [[159, 165], ["max"], "methods", ["None"], ["", "def", "update_parameters_per_episode", "(", "self", ",", "episode", ")", ":", "\n", "        ", "if", "episode", "==", "0", ":", "\n", "            ", "self", ".", "eps", "=", "self", ".", "eps_init", "\n", "", "else", ":", "\n", "            ", "self", ".", "eps", "*=", "self", ".", "eps_decay", "\n", "self", ".", "eps", "=", "max", "(", "self", ".", "eps", ",", "self", ".", "eps_min", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cartpole_threshold.DDQN_noise.reset_optimizer": [[166, 168], ["torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "GTNC_visualize_cartpole_threshold.DDQN_noise.model.parameters"], "methods", ["None"], ["", "", "def", "reset_optimizer", "(", "self", ")", ":", "\n", "        ", "self", ".", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "self", ".", "model", ".", "parameters", "(", ")", ",", "lr", "=", "self", ".", "lr", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cartpole_threshold.load_envs_and_config": [[170, 182], ["os.path.join", "torch.load", "torch.load", "envs.env_factory.EnvFactory", "envs.env_factory.EnvFactory.generate_virtual_env", "env_factory.generate_virtual_env.load_state_dict", "envs.env_factory.EnvFactory.generate_real_env"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_virtual_env", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_real_env"], ["", "", "def", "load_envs_and_config", "(", "dir", ",", "model_file_name", ")", ":", "\n", "    ", "file_path", "=", "os", ".", "path", ".", "join", "(", "dir", ",", "model_file_name", ")", "\n", "save_dict", "=", "torch", ".", "load", "(", "file_path", ")", "\n", "config", "=", "save_dict", "[", "'config'", "]", "\n", "# config['envs']['CartPole-v0']['solved_reward'] = 195", "\n", "# config['envs']['CartPole-v0']['max_steps'] = 200", "\n", "env_factory", "=", "EnvFactory", "(", "config", "=", "config", ")", "\n", "virtual_env", "=", "env_factory", ".", "generate_virtual_env", "(", ")", "\n", "virtual_env", ".", "load_state_dict", "(", "save_dict", "[", "'model'", "]", ")", "\n", "real_env", "=", "env_factory", ".", "generate_real_env", "(", ")", "\n", "\n", "return", "virtual_env", ",", "real_env", ",", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cartpole_threshold.calc_noisy_reward": [[184, 206], ["numpy.logspace", "torch.save", "torch.save", "range", "reward_list[].append", "range", "print", "GTNC_visualize_cartpole_threshold.DDQN_noise", "print", "GTNC_visualize_cartpole_threshold.DDQN_noise.train", "print", "DDQN_noise.test", "train_length.append", "len"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.train", "home.repos.pwc.inspect_result.automl_learning_environments.agents.base_agent.BaseAgent.test"], ["", "def", "calc_noisy_reward", "(", "virtual_env", ",", "real_env", ",", "config", ",", "reward_file_name", ",", "noise_type", ")", ":", "\n", "    ", "reward_list", "=", "[", "[", "]", "for", "i", "in", "range", "(", "5", ")", "]", "# stupid....", "\n", "\n", "for", "noise_value", "in", "np", ".", "logspace", "(", "-", "2", ",", "1.5", ",", "num", "=", "50", ")", ":", "\n", "        ", "reward_sum", "=", "[", "]", "\n", "train_length", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "100", ")", ":", "\n", "            ", "print", "(", "'{} {} {}'", ".", "format", "(", "noise_type", ",", "noise_value", ",", "i", ")", ")", "\n", "agent", "=", "DDQN_noise", "(", "env", "=", "real_env", ",", "config", "=", "config", ")", "\n", "print", "(", "'train'", ")", "\n", "reward_train", ",", "_", ",", "_", "=", "agent", ".", "train", "(", "env", "=", "virtual_env", ",", "noise_type", "=", "noise_type", ",", "noise_value", "=", "noise_value", ")", "\n", "print", "(", "'test'", ")", "\n", "reward_test", ",", "_", ",", "_", "=", "agent", ".", "test", "(", "env", "=", "real_env", ")", "\n", "reward_sum", "+=", "reward_test", "\n", "train_length", ".", "append", "(", "len", "(", "reward_train", ")", ")", "\n", "\n", "", "reward_list", "[", "noise_type", "]", ".", "append", "(", "(", "noise_value", ",", "reward_sum", ",", "train_length", ")", ")", "\n", "\n", "", "data", "=", "{", "}", "\n", "data", "[", "'reward_list'", "]", "=", "reward_list", "\n", "\n", "torch", ".", "save", "(", "data", ",", "reward_file_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cartpole_threshold.calc_reference_deviation": [[208, 227], ["range", "torch.std().item", "torch.std().item", "agents.DDQN.DDQN", "agents.DDQN.DDQN.train", "replay_buffer_train.get_all", "torch.cat", "torch.cat", "print", "print", "torch.cat", "torch.cat", "torch.std", "torch.std", "torch.std", "torch.std"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.train", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.ReplayBuffer.get_all"], ["", "def", "calc_reference_deviation", "(", "virtual_env", ",", "real_env", ",", "config", ")", ":", "\n", "    ", "state_reward_concat", "=", "None", "\n", "\n", "for", "i", "in", "range", "(", "10", ")", ":", "\n", "        ", "agent", "=", "DDQN", "(", "env", "=", "real_env", ",", "config", "=", "config", ")", "\n", "_", ",", "_", ",", "replay_buffer_train", "=", "agent", ".", "train", "(", "env", "=", "virtual_env", ")", "\n", "\n", "states", ",", "_", ",", "_", ",", "rewards", ",", "_", "=", "replay_buffer_train", ".", "get_all", "(", ")", "\n", "state_reward", "=", "torch", ".", "cat", "(", "(", "states", ",", "rewards", ")", ",", "1", ")", "\n", "\n", "if", "state_reward_concat", "==", "None", ":", "\n", "            ", "state_reward_concat", "=", "state_reward", "\n", "", "else", ":", "\n", "            ", "state_reward_concat", "=", "torch", ".", "cat", "(", "(", "state_reward_concat", ",", "state_reward", ")", ",", "0", ")", "\n", "\n", "", "print", "(", "state_reward_concat", ".", "shape", ")", "\n", "print", "(", "torch", ".", "std", "(", "state_reward_concat", ",", "dim", "=", "0", ")", ")", "\n", "\n", "", "return", "torch", ".", "std", "(", "state_reward_concat", ",", "dim", "=", "0", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cartpole_threshold.plot_threshold": [[229, 257], ["matplotlib.figure", "enumerate", "matplotlib.legend", "matplotlib.xscale", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.savefig", "matplotlib.show", "torch.load", "torch.load", "range", "numpy.array", "numpy.array", "matplotlib.subplots_adjust", "matplotlib.plot", "matplotlib.fill_between", "len", "statistics.mean", "statistics.stdev", "len"], "function", ["None"], ["", "def", "plot_threshold", "(", "std_dev", ",", "reward_file_names", ")", ":", "\n", "    ", "plt", ".", "figure", "(", "dpi", "=", "600", ",", "figsize", "=", "(", "6", ",", "3", ")", ")", "\n", "\n", "for", "i", ",", "reward_file_name", "in", "enumerate", "(", "reward_file_names", ")", ":", "\n", "\n", "        ", "data", "=", "torch", ".", "load", "(", "reward_file_name", ")", "\n", "reward_list", "=", "data", "[", "'reward_list'", "]", "\n", "for", "k", "in", "range", "(", "len", "(", "reward_list", ")", ")", ":", "\n", "            ", "if", "len", "(", "reward_list", "[", "k", "]", ")", ">", "0", ":", "\n", "                ", "reward_list", "=", "reward_list", "[", "k", "]", "\n", "break", "\n", "\n", "", "", "x", "=", "[", "elem", "[", "0", "]", "/", "std_dev", "[", "i", "]", "for", "elem", "in", "reward_list", "]", "\n", "mean", "=", "[", "statistics", ".", "mean", "(", "elem", "[", "1", "]", ")", "for", "elem", "in", "reward_list", "]", "\n", "std", "=", "[", "statistics", ".", "stdev", "(", "elem", "[", "1", "]", ")", "for", "elem", "in", "reward_list", "]", "\n", "mean", "=", "np", ".", "array", "(", "mean", ")", "\n", "std", "=", "np", ".", "array", "(", "std", ")", "\n", "\n", "plt", ".", "subplots_adjust", "(", "bottom", "=", "0.15", ")", "\n", "plt", ".", "plot", "(", "x", ",", "mean", ")", "\n", "plt", ".", "fill_between", "(", "x", "=", "x", ",", "y1", "=", "mean", "-", "std", "*", "STD_MULT", ",", "y2", "=", "mean", "+", "std", "*", "STD_MULT", ",", "alpha", "=", "0.2", ")", "\n", "\n", "", "plt", ".", "legend", "(", "[", "'cart position'", ",", "'cart velocity'", ",", "'pole angle'", ",", "'pole ang. vel.'", ",", "'reward'", "]", ")", "\n", "plt", ".", "xscale", "(", "'log'", ")", "\n", "plt", ".", "xlabel", "(", "'relative noise intensity'", ")", "\n", "plt", ".", "ylabel", "(", "'cumulative reward'", ")", "\n", "plt", ".", "savefig", "(", "'cartpole_threshold.svg'", ",", "bbox_inches", "=", "'tight'", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_inference_time.load_envs_and_config": [[13, 31], ["os.path.join", "torch.load", "envs.env_factory.EnvFactory", "envs.env_factory.EnvFactory.generate_virtual_env", "env_factory.generate_virtual_env.load_state_dict", "envs.env_factory.EnvFactory.generate_real_env", "open", "yaml.safe_load"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_virtual_env", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_real_env"], ["def", "load_envs_and_config", "(", "file_name", ",", "model_dir", ",", "device", ")", ":", "\n", "    ", "file_path", "=", "os", ".", "path", ".", "join", "(", "model_dir", ",", "file_name", ")", "\n", "save_dict", "=", "torch", ".", "load", "(", "file_path", ")", "\n", "config", "=", "save_dict", "[", "'config'", "]", "\n", "config", "[", "'device'", "]", "=", "device", "\n", "env_factory", "=", "EnvFactory", "(", "config", "=", "config", ")", "\n", "virtual_env", "=", "env_factory", ".", "generate_virtual_env", "(", ")", "\n", "virtual_env", ".", "load_state_dict", "(", "save_dict", "[", "'model'", "]", ")", "\n", "real_env", "=", "env_factory", ".", "generate_real_env", "(", ")", "\n", "\n", "# load additional agent configs", "\n", "with", "open", "(", "\"../default_config_cartpole.yaml\"", ",", "\"r\"", ")", "as", "stream", ":", "\n", "        ", "config_new", "=", "yaml", ".", "safe_load", "(", "stream", ")", "[", "\"agents\"", "]", "\n", "\n", "", "config", "[", "\"agents\"", "]", "[", "\"duelingddqn\"", "]", "=", "config_new", "[", "\"duelingddqn\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"duelingddqn_vary\"", "]", "=", "config_new", "[", "\"duelingddqn_vary\"", "]", "\n", "\n", "return", "virtual_env", ",", "real_env", ",", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_transfer_algo.get_best_models_from_log": [[37, 66], ["hpbandster.logged_results_to_HBS_result", "hpres.logged_results_to_HBS_result.data.values", "print", "best_models.sort", "os.path.isdir", "log_dir.replace.replace", "best_models.append", "os.path.isfile", "model_name.replace.replace"], "function", ["None"], ["def", "get_best_models_from_log", "(", "log_dir", ")", ":", "\n", "    ", "if", "not", "os", ".", "path", ".", "isdir", "(", "log_dir", ")", ":", "\n", "        ", "log_dir", "=", "log_dir", ".", "replace", "(", "'nierhoff'", ",", "'dingsda'", ")", "\n", "\n", "", "result", "=", "hpres", ".", "logged_results_to_HBS_result", "(", "log_dir", ")", "\n", "\n", "best_models", "=", "[", "]", "\n", "\n", "for", "value", "in", "result", ".", "data", ".", "values", "(", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "loss", "=", "value", ".", "results", "[", "1.0", "]", "[", "'loss'", "]", "\n", "model_name", "=", "value", ".", "results", "[", "1.0", "]", "[", "'info'", "]", "[", "'model_name'", "]", "\n", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "model_name", ")", ":", "\n", "                ", "model_name", "=", "model_name", ".", "replace", "(", "'nierhoff'", ",", "'dingsda'", ")", "\n", "", "best_models", ".", "append", "(", "(", "loss", ",", "model_name", ")", ")", "\n", "", "except", ":", "\n", "            ", "continue", "\n", "\n", "# before AUC (objective minimized)", "\n", "# print(\"sorting from low to high values (non-AUC)\")", "\n", "# best_models.sort(key=lambda x: x[0])", "\n", "\n", "# AUC (objective maximized)", "\n", "", "", "print", "(", "\"sorting from high to low values (AUC)\"", ")", "\n", "best_models", ".", "sort", "(", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ",", "reverse", "=", "True", ")", "\n", "best_models", "=", "best_models", "[", ":", "MODEL_NUM", "]", "\n", "\n", "return", "best_models", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_transfer_algo.load_envs_and_config": [[68, 81], ["torch.load", "envs.env_factory.EnvFactory", "envs.env_factory.EnvFactory.generate_reward_env", "env_factory.generate_reward_env.load_state_dict", "envs.env_factory.EnvFactory.generate_real_env"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_reward_env", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_real_env"], ["", "def", "load_envs_and_config", "(", "model_file", ")", ":", "\n", "    ", "save_dict", "=", "torch", ".", "load", "(", "model_file", ")", "\n", "\n", "config", "=", "save_dict", "[", "'config'", "]", "\n", "config", "[", "'device'", "]", "=", "'cpu'", "\n", "config", "[", "'envs'", "]", "[", "'CartPole-v0'", "]", "[", "'solved_reward'", "]", "=", "100000", "# something big enough to prevent early out triggering", "\n", "\n", "env_factory", "=", "EnvFactory", "(", "config", "=", "config", ")", "\n", "reward_env", "=", "env_factory", ".", "generate_reward_env", "(", ")", "\n", "reward_env", ".", "load_state_dict", "(", "save_dict", "[", "'model'", "]", ")", "\n", "real_env", "=", "env_factory", ".", "generate_real_env", "(", ")", "\n", "\n", "return", "reward_env", ",", "real_env", ",", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_transfer_algo.train_test_agents": [[83, 136], ["range", "agents.agent_utils.select_agent.train", "rewards.append", "episode_lengths.append", "agents.agent_utils.select_agent", "agents.agent_utils.select_agent"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.train", "home.repos.pwc.inspect_result.automl_learning_environments.agents.agent_utils.select_agent", "home.repos.pwc.inspect_result.automl_learning_environments.agents.agent_utils.select_agent"], ["", "def", "train_test_agents", "(", "mode", ",", "env", ",", "real_env", ",", "config", ")", ":", "\n", "    ", "rewards", "=", "[", "]", "\n", "episode_lengths", "=", "[", "]", "\n", "\n", "# settings for comparability", "\n", "config", "[", "'agents'", "]", "[", "'duelingddqn'", "]", "=", "{", "}", "\n", "config", "[", "'agents'", "]", "[", "'duelingddqn'", "]", "[", "'test_episodes'", "]", "=", "1", "\n", "config", "[", "'agents'", "]", "[", "'duelingddqn'", "]", "[", "'train_episodes'", "]", "=", "1000", "\n", "config", "[", "'agents'", "]", "[", "'duelingddqn'", "]", "[", "'print_rate'", "]", "=", "100", "\n", "\n", "config", "[", "'agents'", "]", "[", "'duelingddqn'", "]", "[", "'lr'", "]", "=", "0.00025", "\n", "config", "[", "'agents'", "]", "[", "'duelingddqn'", "]", "[", "'eps_init'", "]", "=", "1.0", "\n", "config", "[", "'agents'", "]", "[", "'duelingddqn'", "]", "[", "'eps_min'", "]", "=", "0.1", "\n", "config", "[", "'agents'", "]", "[", "'duelingddqn'", "]", "[", "'eps_decay'", "]", "=", "0.9", "# original DDQN paper uses linear decay over 1M N's", "\n", "config", "[", "'agents'", "]", "[", "'duelingddqn'", "]", "[", "'gamma'", "]", "=", "0.99", "\n", "config", "[", "'agents'", "]", "[", "'duelingddqn'", "]", "[", "'batch_size'", "]", "=", "32", "\n", "config", "[", "'agents'", "]", "[", "'duelingddqn'", "]", "[", "'same_action_num'", "]", "=", "1", "\n", "config", "[", "'agents'", "]", "[", "'duelingddqn'", "]", "[", "'activation_fn'", "]", "=", "\"relu\"", "\n", "config", "[", "'agents'", "]", "[", "'duelingddqn'", "]", "[", "'tau'", "]", "=", "0.01", "# original DDQN paper has hard update every N steps", "\n", "config", "[", "'agents'", "]", "[", "'duelingddqn'", "]", "[", "'hidden_size'", "]", "=", "64", "\n", "config", "[", "'agents'", "]", "[", "'duelingddqn'", "]", "[", "'hidden_layer'", "]", "=", "1", "\n", "config", "[", "'agents'", "]", "[", "'duelingddqn'", "]", "[", "'rb_size'", "]", "=", "1000000", "\n", "config", "[", "'agents'", "]", "[", "'duelingddqn'", "]", "[", "'init_episodes'", "]", "=", "1", "\n", "config", "[", "'agents'", "]", "[", "'duelingddqn'", "]", "[", "'feature_dim'", "]", "=", "128", "\n", "\n", "config", "[", "'agents'", "]", "[", "'duelingddqn'", "]", "[", "'early_out_num'", "]", "=", "10", "\n", "config", "[", "'agents'", "]", "[", "'duelingddqn'", "]", "[", "'early_out_virtual_diff'", "]", "=", "0.02", "\n", "\n", "# optimized ICM HPs:", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "=", "{", "}", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "[", "'beta'", "]", "=", "0.05", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "[", "'eta'", "]", "=", "0.03", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "[", "'feature_dim'", "]", "=", "32", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "[", "'hidden_size'", "]", "=", "128", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "[", "'lr'", "]", "=", "1e-5", "\n", "\n", "# default ICM HPs:", "\n", "# config['agents']['icm'] = {}", "\n", "# config['agents']['icm']['beta'] = 0.2", "\n", "# config['agents']['icm']['eta'] = 0.5", "\n", "# config['agents']['icm']['feature_dim'] = 64", "\n", "# config['agents']['icm']['hidden_size'] = 128", "\n", "# config['agents']['icm']['lr'] = 1e-4", "\n", "\n", "for", "i", "in", "range", "(", "MODEL_AGENTS", ")", ":", "\n", "        ", "if", "mode", "==", "'-1'", ":", "\n", "            ", "agent", "=", "select_agent", "(", "config", "=", "config", ",", "agent_name", "=", "'duelingddqn_icm'", ")", "\n", "", "else", ":", "\n", "            ", "agent", "=", "select_agent", "(", "config", "=", "config", ",", "agent_name", "=", "'duelingddqn'", ")", "\n", "", "reward", ",", "episode_length", ",", "_", "=", "agent", ".", "train", "(", "env", "=", "env", ",", "test_env", "=", "real_env", ")", "\n", "rewards", ".", "append", "(", "reward", ")", "\n", "episode_lengths", ".", "append", "(", "episode_length", ")", "\n", "", "return", "rewards", ",", "episode_lengths", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_transfer_algo.save_list": [[138, 148], ["os.makedirs", "os.path.join", "torch.save", "str"], "function", ["None"], ["", "def", "save_list", "(", "mode", ",", "config", ",", "reward_list", ",", "episode_length_list", ")", ":", "\n", "    ", "os", ".", "makedirs", "(", "SAVE_DIR", ",", "exist_ok", "=", "True", ")", "\n", "file_name", "=", "os", ".", "path", ".", "join", "(", "SAVE_DIR", ",", "'best_transfer_algo'", "+", "str", "(", "mode", ")", "+", "'.pt'", ")", "\n", "save_dict", "=", "{", "}", "\n", "save_dict", "[", "'config'", "]", "=", "config", "\n", "save_dict", "[", "'model_num'", "]", "=", "MODEL_NUM", "\n", "save_dict", "[", "'model_agents'", "]", "=", "MODEL_AGENTS", "\n", "save_dict", "[", "'reward_list'", "]", "=", "reward_list", "\n", "save_dict", "[", "'episode_length_list'", "]", "=", "episode_length_list", "\n", "torch", ".", "save", "(", "save_dict", ",", "file_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_transfer_algo.eval_models": [[150, 165], ["GTNC_evaluate_cartpole_transfer_algo.get_best_models_from_log", "GTNC_evaluate_cartpole_transfer_algo.save_list", "print", "print", "GTNC_evaluate_cartpole_transfer_algo.load_envs_and_config", "GTNC_evaluate_cartpole_transfer_algo.train_test_agents", "str", "str"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.get_best_models_from_log", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.save_list", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.load_envs_and_config", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.train_test_agents"], ["", "def", "eval_models", "(", "mode", ",", "log_dir", ")", ":", "\n", "    ", "best_models", "=", "get_best_models_from_log", "(", "log_dir", ")", "\n", "\n", "reward_list", "=", "[", "]", "\n", "episode_length_list", "=", "[", "]", "\n", "\n", "for", "best_reward", ",", "model_file", "in", "best_models", ":", "\n", "        ", "print", "(", "'best reward: '", "+", "str", "(", "best_reward", ")", ")", "\n", "print", "(", "'model file: '", "+", "str", "(", "model_file", ")", ")", "\n", "reward_env", ",", "real_env", ",", "config", "=", "load_envs_and_config", "(", "model_file", ")", "\n", "rewards", ",", "episode_lengths", "=", "train_test_agents", "(", "mode", "=", "mode", ",", "env", "=", "reward_env", ",", "real_env", "=", "real_env", ",", "config", "=", "config", ")", "\n", "reward_list", "+=", "rewards", "\n", "episode_length_list", "+=", "episode_lengths", "\n", "\n", "", "save_list", "(", "mode", ",", "config", ",", "reward_list", ",", "episode_length_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_transfer_algo.eval_base": [[167, 180], ["GTNC_evaluate_cartpole_transfer_algo.get_best_models_from_log", "range", "GTNC_evaluate_cartpole_transfer_algo.save_list", "GTNC_evaluate_cartpole_transfer_algo.load_envs_and_config", "GTNC_evaluate_cartpole_transfer_algo.train_test_agents"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.get_best_models_from_log", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.save_list", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.load_envs_and_config", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.train_test_agents"], ["", "def", "eval_base", "(", "mode", ",", "log_dir", ")", ":", "\n", "    ", "best_models", "=", "get_best_models_from_log", "(", "log_dir", ")", "\n", "\n", "reward_list", "=", "[", "]", "\n", "episode_length_list", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "MODEL_NUM", ")", ":", "\n", "        ", "reward_env", ",", "real_env", ",", "config", "=", "load_envs_and_config", "(", "best_models", "[", "0", "]", "[", "1", "]", ")", "\n", "rewards", ",", "episode_lengths", "=", "train_test_agents", "(", "mode", "=", "mode", ",", "env", "=", "real_env", ",", "real_env", "=", "real_env", ",", "config", "=", "config", ")", "\n", "reward_list", "+=", "rewards", "\n", "episode_length_list", "+=", "episode_lengths", "\n", "\n", "", "save_list", "(", "mode", ",", "config", ",", "reward_list", ",", "episode_length_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_transfer_algo.eval_icm": [[182, 195], ["GTNC_evaluate_cartpole_transfer_algo.get_best_models_from_log", "range", "GTNC_evaluate_cartpole_transfer_algo.save_list", "GTNC_evaluate_cartpole_transfer_algo.load_envs_and_config", "GTNC_evaluate_cartpole_transfer_algo.train_test_agents"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.get_best_models_from_log", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.save_list", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.load_envs_and_config", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.train_test_agents"], ["", "def", "eval_icm", "(", "mode", ",", "log_dir", ")", ":", "\n", "    ", "best_models", "=", "get_best_models_from_log", "(", "log_dir", ")", "\n", "\n", "reward_list", "=", "[", "]", "\n", "episode_length_list", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "MODEL_NUM", ")", ":", "\n", "        ", "reward_env", ",", "real_env", ",", "config", "=", "load_envs_and_config", "(", "best_models", "[", "0", "]", "[", "1", "]", ")", "\n", "rewards", ",", "episode_lengths", "=", "train_test_agents", "(", "mode", "=", "mode", ",", "env", "=", "real_env", ",", "real_env", "=", "real_env", ",", "config", "=", "config", ")", "\n", "reward_list", "+=", "rewards", "\n", "episode_length_list", "+=", "episode_lengths", "\n", "\n", "", "save_list", "(", "mode", ",", "config", ",", "reward_list", ",", "episode_length_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_pendulum.ExperimentWrapper.get_bohb_parameters": [[17, 26], ["None"], "methods", ["None"], ["    ", "def", "get_bohb_parameters", "(", "self", ")", ":", "\n", "        ", "params", "=", "{", "}", "\n", "params", "[", "'min_budget'", "]", "=", "1", "\n", "params", "[", "'max_budget'", "]", "=", "1", "\n", "params", "[", "'eta'", "]", "=", "2", "\n", "params", "[", "'random_fraction'", "]", "=", "1", "\n", "params", "[", "'iterations'", "]", "=", "10000", "\n", "\n", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_pendulum.ExperimentWrapper.get_configspace": [[27, 30], ["ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace"], "methods", ["None"], ["", "def", "get_configspace", "(", "self", ")", ":", "\n", "        ", "cs", "=", "CS", ".", "ConfigurationSpace", "(", ")", "\n", "return", "cs", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_pendulum.ExperimentWrapper.get_specific_config": [[31, 36], ["copy.deepcopy"], "methods", ["None"], ["", "def", "get_specific_config", "(", "self", ",", "cso", ",", "default_config", ",", "budget", ")", ":", "\n", "        ", "config", "=", "deepcopy", "(", "default_config", ")", "\n", "global", "reward_env_type", "\n", "config", "[", "\"envs\"", "]", "[", "'Pendulum-v0'", "]", "[", "'reward_env_type'", "]", "=", "reward_env_type", "\n", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_pendulum.ExperimentWrapper.compute": [[37, 77], ["GTNC_evaluate_pendulum.ExperimentWrapper.get_specific_config", "print", "print", "print", "print", "print", "print", "str", "str", "str", "str", "print", "print", "print", "print", "print", "open", "yaml.safe_load", "agents.GTN.GTN_Master", "agents.GTN.GTN_Master.run", "str", "str", "str", "float", "traceback.format_exc", "print", "str", "str", "sorted"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_step_size.ExperimentWrapper.get_specific_config", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.run"], ["", "def", "compute", "(", "self", ",", "working_dir", ",", "bohb_id", ",", "config_id", ",", "cso", ",", "budget", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "with", "open", "(", "\"default_config_pendulum_reward_env.yaml\"", ",", "'r'", ")", "as", "stream", ":", "\n", "            ", "default_config", "=", "yaml", ".", "safe_load", "(", "stream", ")", "\n", "\n", "", "config", "=", "self", ".", "get_specific_config", "(", "cso", ",", "default_config", ",", "budget", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "\"START BOHB ITERATION\"", ")", "\n", "print", "(", "'CONFIG: '", "+", "str", "(", "config", ")", ")", "\n", "print", "(", "'CSO:    '", "+", "str", "(", "cso", ")", ")", "\n", "print", "(", "'BUDGET: '", "+", "str", "(", "budget", ")", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "try", ":", "\n", "            ", "gtn", "=", "GTN_Master", "(", "config", ",", "bohb_id", "=", "bohb_id", ",", "bohb_working_dir", "=", "working_dir", ")", "\n", "_", ",", "score_list", ",", "model_name", "=", "gtn", ".", "run", "(", ")", "\n", "score", "=", "-", "sorted", "(", "score_list", ")", "[", "-", "1", "]", "\n", "error", "=", "\"\"", "\n", "", "except", ":", "\n", "            ", "score", "=", "float", "(", "'-Inf'", ")", "\n", "score_list", "=", "[", "]", "\n", "model_name", "=", "None", "\n", "error", "=", "traceback", ".", "format_exc", "(", ")", "\n", "print", "(", "error", ")", "\n", "\n", "", "info", "=", "{", "}", "\n", "info", "[", "'error'", "]", "=", "str", "(", "error", ")", "\n", "info", "[", "'config'", "]", "=", "str", "(", "config", ")", "\n", "info", "[", "'score_list'", "]", "=", "str", "(", "score_list", ")", "\n", "info", "[", "'model_name'", "]", "=", "str", "(", "model_name", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "'FINAL SCORE: '", "+", "str", "(", "score", ")", ")", "\n", "print", "(", "'SCORE LIST:  '", "+", "str", "(", "score_list", ")", ")", "\n", "print", "(", "\"END BOHB ITERATION\"", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "return", "{", "\n", "\"loss\"", ":", "score", ",", "\n", "\"info\"", ":", "info", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_halfcheetah_compare_reward_envs.get_best_models_from_log": [[42, 71], ["hpbandster.logged_results_to_HBS_result", "hpres.logged_results_to_HBS_result.data.values", "print", "best_models.sort", "os.path.isdir", "log_dir.replace.replace", "best_models.append", "os.path.isfile", "model_name.replace.replace"], "function", ["None"], ["def", "get_best_models_from_log", "(", "log_dir", ")", ":", "\n", "    ", "if", "not", "os", ".", "path", ".", "isdir", "(", "log_dir", ")", ":", "\n", "        ", "log_dir", "=", "log_dir", ".", "replace", "(", "'nierhoff'", ",", "'dingsda'", ")", "\n", "\n", "", "result", "=", "hpres", ".", "logged_results_to_HBS_result", "(", "log_dir", ")", "\n", "\n", "best_models", "=", "[", "]", "\n", "\n", "for", "value", "in", "result", ".", "data", ".", "values", "(", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "loss", "=", "value", ".", "results", "[", "1.0", "]", "[", "'loss'", "]", "\n", "model_name", "=", "value", ".", "results", "[", "1.0", "]", "[", "'info'", "]", "[", "'model_name'", "]", "\n", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "model_name", ")", ":", "\n", "                ", "model_name", "=", "model_name", ".", "replace", "(", "'nierhoff'", ",", "'dingsda'", ")", "\n", "", "best_models", ".", "append", "(", "(", "loss", ",", "model_name", ")", ")", "\n", "", "except", ":", "\n", "            ", "continue", "\n", "\n", "# before AUC (objective minimized)", "\n", "# print(\"sorting from low to high values (non-AUC)\")", "\n", "# best_models.sort(key=lambda x: x[0])", "\n", "\n", "# AUC (objective maximized)", "\n", "", "", "print", "(", "\"sorting from high to low values (AUC)\"", ")", "\n", "best_models", ".", "sort", "(", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ",", "reverse", "=", "True", ")", "\n", "best_models", "=", "best_models", "[", ":", "MODEL_NUM", "]", "\n", "\n", "return", "best_models", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_halfcheetah_compare_reward_envs.load_envs_and_config": [[73, 87], ["torch.load", "envs.env_factory.EnvFactory", "envs.env_factory.EnvFactory.generate_reward_env", "print", "env_factory.generate_reward_env.load_state_dict", "envs.env_factory.EnvFactory.generate_real_env"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_reward_env", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_real_env"], ["", "def", "load_envs_and_config", "(", "model_file", ")", ":", "\n", "    ", "save_dict", "=", "torch", ".", "load", "(", "model_file", ")", "\n", "\n", "config", "=", "save_dict", "[", "'config'", "]", "\n", "config", "[", "'device'", "]", "=", "'cpu'", "\n", "config", "[", "'envs'", "]", "[", "'HalfCheetah-v3'", "]", "[", "'solved_reward'", "]", "=", "100000", "# something big enough to prevent early out triggering", "\n", "\n", "env_factory", "=", "EnvFactory", "(", "config", "=", "config", ")", "\n", "reward_env", "=", "env_factory", ".", "generate_reward_env", "(", ")", "\n", "print", "(", "save_dict", "[", "'model'", "]", ")", "\n", "reward_env", ".", "load_state_dict", "(", "save_dict", "[", "'model'", "]", ")", "\n", "real_env", "=", "env_factory", ".", "generate_real_env", "(", ")", "\n", "\n", "return", "reward_env", ",", "real_env", ",", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_halfcheetah_compare_reward_envs.train_test_agents": [[89, 150], ["range", "agents.agent_utils.select_agent.train", "print", "rewards.append", "episode_lengths.append", "agents.agent_utils.select_agent", "agents.agent_utils.select_agent", "str"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.train", "home.repos.pwc.inspect_result.automl_learning_environments.agents.agent_utils.select_agent", "home.repos.pwc.inspect_result.automl_learning_environments.agents.agent_utils.select_agent"], ["", "def", "train_test_agents", "(", "mode", ",", "env", ",", "real_env", ",", "config", ")", ":", "\n", "    ", "rewards", "=", "[", "]", "\n", "episode_lengths", "=", "[", "]", "\n", "\n", "# settings for comparability", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'test_episodes'", "]", "=", "1", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'train_episodes'", "]", "=", "1000", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'print_rate'", "]", "=", "100", "\n", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'lr'", "]", "=", "3e-4", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'tau'", "]", "=", "0.005", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'activation_fn'", "]", "=", "'relu'", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'same_action_num'", "]", "=", "1", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'policy_delay'", "]", "=", "2", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'hidden_size'", "]", "=", "128", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'hidden_layer'", "]", "=", "2", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'policy_std_clip'", "]", "=", "0.5", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'policy_std'", "]", "=", "0.2", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'action_std'", "]", "=", "0.1", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'batch_size'", "]", "=", "256", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'gamma'", "]", "=", "0.99", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'rb_size'", "]", "=", "1000000", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'init_episodes'", "]", "=", "20", "# set via time steps in original td3 implementation", "\n", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'early_out_num'", "]", "=", "50", "\n", "config", "[", "'agents'", "]", "[", "'td3'", "]", "[", "'early_out_virtual_diff'", "]", "=", "0.02", "\n", "\n", "# optimized ICM HPs:", "\n", "# config['agents']['icm'] = {}", "\n", "# config['agents']['icm']['beta'] = 0.05", "\n", "# config['agents']['icm']['eta'] = 0.01", "\n", "# config['agents']['icm']['feature_dim'] = 32", "\n", "# config['agents']['icm']['hidden_size'] = 128", "\n", "# config['agents']['icm']['lr'] = 1e-5", "\n", "\n", "# optimized ICM HPs (from max. reward run):", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "=", "{", "}", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "[", "'beta'", "]", "=", "0.001", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "[", "'eta'", "]", "=", "0.1", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "[", "'feature_dim'", "]", "=", "32", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "[", "'hidden_size'", "]", "=", "128", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "[", "'lr'", "]", "=", "1e-5", "\n", "\n", "# default ICM HPs:", "\n", "# config['agents']['icm'] = {}", "\n", "# config['agents']['icm']['beta'] = 0.2", "\n", "# config['agents']['icm']['eta'] = 0.5", "\n", "# config['agents']['icm']['feature_dim'] = 64", "\n", "# config['agents']['icm']['hidden_size'] = 128", "\n", "# config['agents']['icm']['lr'] = 1e-4", "\n", "\n", "for", "i", "in", "range", "(", "MODEL_AGENTS", ")", ":", "\n", "        ", "if", "mode", "==", "'-1'", ":", "\n", "            ", "agent", "=", "select_agent", "(", "config", "=", "config", ",", "agent_name", "=", "'td3_icm'", ")", "\n", "", "else", ":", "\n", "            ", "agent", "=", "select_agent", "(", "config", "=", "config", ",", "agent_name", "=", "'td3'", ")", "\n", "", "reward", ",", "episode_length", ",", "_", "=", "agent", ".", "train", "(", "env", "=", "env", ",", "test_env", "=", "real_env", ")", "\n", "print", "(", "'reward: '", "+", "str", "(", "reward", ")", ")", "\n", "rewards", ".", "append", "(", "reward", ")", "\n", "episode_lengths", ".", "append", "(", "episode_length", ")", "\n", "", "return", "rewards", ",", "episode_lengths", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_halfcheetah_compare_reward_envs.save_list": [[152, 163], ["os.makedirs", "os.path.join", "torch.save", "str"], "function", ["None"], ["", "def", "save_list", "(", "mode", ",", "config", ",", "reward_list", ",", "episode_length_list", ")", ":", "\n", "    ", "os", ".", "makedirs", "(", "SAVE_DIR", ",", "exist_ok", "=", "True", ")", "\n", "# file_name = os.path.join(SAVE_DIR, 'best' + str(mode) + \"_bohb_max_reward_run\" + '.pt')", "\n", "file_name", "=", "os", ".", "path", ".", "join", "(", "SAVE_DIR", ",", "'best'", "+", "str", "(", "mode", ")", "+", "'.pt'", ")", "\n", "save_dict", "=", "{", "}", "\n", "save_dict", "[", "'config'", "]", "=", "config", "\n", "save_dict", "[", "'model_num'", "]", "=", "MODEL_NUM", "\n", "save_dict", "[", "'model_agents'", "]", "=", "MODEL_AGENTS", "\n", "save_dict", "[", "'reward_list'", "]", "=", "reward_list", "\n", "save_dict", "[", "'episode_length_list'", "]", "=", "episode_length_list", "\n", "torch", ".", "save", "(", "save_dict", ",", "file_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_halfcheetah_compare_reward_envs.eval_models": [[165, 180], ["GTNC_evaluate_halfcheetah_compare_reward_envs.get_best_models_from_log", "GTNC_evaluate_halfcheetah_compare_reward_envs.save_list", "print", "print", "GTNC_evaluate_halfcheetah_compare_reward_envs.load_envs_and_config", "GTNC_evaluate_halfcheetah_compare_reward_envs.train_test_agents", "str", "str"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.get_best_models_from_log", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.save_list", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.load_envs_and_config", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.train_test_agents"], ["", "def", "eval_models", "(", "mode", ",", "log_dir", ")", ":", "\n", "    ", "best_models", "=", "get_best_models_from_log", "(", "log_dir", ")", "\n", "\n", "reward_list", "=", "[", "]", "\n", "episode_length_list", "=", "[", "]", "\n", "\n", "for", "best_reward", ",", "model_file", "in", "best_models", ":", "\n", "        ", "print", "(", "'best reward: '", "+", "str", "(", "best_reward", ")", ")", "\n", "print", "(", "'model file: '", "+", "str", "(", "model_file", ")", ")", "\n", "reward_env", ",", "real_env", ",", "config", "=", "load_envs_and_config", "(", "model_file", ")", "\n", "rewards", ",", "episode_lengths", "=", "train_test_agents", "(", "mode", "=", "mode", ",", "env", "=", "reward_env", ",", "real_env", "=", "real_env", ",", "config", "=", "config", ")", "\n", "reward_list", "+=", "rewards", "\n", "episode_length_list", "+=", "episode_lengths", "\n", "\n", "", "save_list", "(", "mode", ",", "config", ",", "reward_list", ",", "episode_length_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_halfcheetah_compare_reward_envs.eval_base": [[182, 195], ["GTNC_evaluate_halfcheetah_compare_reward_envs.get_best_models_from_log", "range", "GTNC_evaluate_halfcheetah_compare_reward_envs.save_list", "GTNC_evaluate_halfcheetah_compare_reward_envs.load_envs_and_config", "GTNC_evaluate_halfcheetah_compare_reward_envs.train_test_agents"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.get_best_models_from_log", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.save_list", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.load_envs_and_config", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.train_test_agents"], ["", "def", "eval_base", "(", "mode", ",", "log_dir", ")", ":", "\n", "    ", "best_models", "=", "get_best_models_from_log", "(", "log_dir", ")", "\n", "\n", "reward_list", "=", "[", "]", "\n", "episode_length_list", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "MODEL_NUM", ")", ":", "\n", "        ", "reward_env", ",", "real_env", ",", "config", "=", "load_envs_and_config", "(", "best_models", "[", "0", "]", "[", "1", "]", ")", "\n", "rewards", ",", "episode_lengths", "=", "train_test_agents", "(", "mode", "=", "mode", ",", "env", "=", "real_env", ",", "real_env", "=", "real_env", ",", "config", "=", "config", ")", "\n", "reward_list", "+=", "rewards", "\n", "episode_length_list", "+=", "episode_lengths", "\n", "\n", "", "save_list", "(", "mode", ",", "config", ",", "reward_list", ",", "episode_length_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_halfcheetah_compare_reward_envs.eval_icm": [[197, 210], ["GTNC_evaluate_halfcheetah_compare_reward_envs.get_best_models_from_log", "range", "GTNC_evaluate_halfcheetah_compare_reward_envs.save_list", "GTNC_evaluate_halfcheetah_compare_reward_envs.load_envs_and_config", "GTNC_evaluate_halfcheetah_compare_reward_envs.train_test_agents"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.get_best_models_from_log", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.save_list", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.load_envs_and_config", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.train_test_agents"], ["", "def", "eval_icm", "(", "mode", ",", "log_dir", ")", ":", "\n", "    ", "best_models", "=", "get_best_models_from_log", "(", "log_dir", ")", "\n", "\n", "reward_list", "=", "[", "]", "\n", "episode_length_list", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "MODEL_NUM", ")", ":", "\n", "        ", "reward_env", ",", "real_env", ",", "config", "=", "load_envs_and_config", "(", "best_models", "[", "0", "]", "[", "1", "]", ")", "\n", "rewards", ",", "episode_lengths", "=", "train_test_agents", "(", "mode", "=", "mode", ",", "env", "=", "real_env", ",", "real_env", "=", "real_env", ",", "config", "=", "config", ")", "\n", "reward_list", "+=", "rewards", "\n", "episode_length_list", "+=", "episode_lengths", "\n", "\n", "", "save_list", "(", "mode", ",", "config", ",", "reward_list", ",", "episode_length_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_exploration_gain.ExperimentWrapper.get_bohb_parameters": [[17, 26], ["None"], "methods", ["None"], ["    ", "def", "get_bohb_parameters", "(", "self", ")", ":", "\n", "        ", "params", "=", "{", "}", "\n", "params", "[", "'min_budget'", "]", "=", "1", "\n", "params", "[", "'max_budget'", "]", "=", "1", "\n", "params", "[", "'eta'", "]", "=", "2", "\n", "params", "[", "'random_fraction'", "]", "=", "1", "\n", "params", "[", "'iterations'", "]", "=", "300", "\n", "\n", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_exploration_gain.ExperimentWrapper.get_configspace": [[27, 33], ["ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.CategoricalHyperparameter"], "methods", ["None"], ["", "def", "get_configspace", "(", "self", ")", ":", "\n", "        ", "cs", "=", "CS", ".", "ConfigurationSpace", "(", ")", "\n", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "CategoricalHyperparameter", "(", "name", "=", "'gtn_exploration_gain'", ",", "choices", "=", "[", "0", ",", "1e-4", "]", ",", "default_value", "=", "0", ")", ")", "\n", "\n", "return", "cs", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_exploration_gain.ExperimentWrapper.get_specific_config": [[34, 41], ["copy.deepcopy"], "methods", ["None"], ["", "def", "get_specific_config", "(", "self", ",", "cso", ",", "default_config", ",", "budget", ")", ":", "\n", "        ", "config", "=", "deepcopy", "(", "default_config", ")", "\n", "\n", "config", "[", "\"render_env\"", "]", "=", "False", "\n", "config", "[", "\"agents\"", "]", "[", "'gtn'", "]", "[", "'exploration_gain'", "]", "=", "cso", "[", "\"gtn_exploration_gain\"", "]", "\n", "\n", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_exploration_gain.ExperimentWrapper.compute": [[42, 79], ["GTNC_evaluate_exploration_gain.ExperimentWrapper.get_specific_config", "print", "print", "print", "print", "print", "print", "str", "str", "print", "print", "print", "print", "print", "open", "yaml.safe_load", "agents.GTN.GTN_Master", "agents.GTN.GTN_Master.run", "len", "str", "str", "str", "float", "traceback.format_exc", "print", "str", "str"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_step_size.ExperimentWrapper.get_specific_config", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.run"], ["", "def", "compute", "(", "self", ",", "working_dir", ",", "bohb_id", ",", "config_id", ",", "cso", ",", "budget", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "with", "open", "(", "\"default_config_gridworld.yaml\"", ",", "'r'", ")", "as", "stream", ":", "\n", "            ", "default_config", "=", "yaml", ".", "safe_load", "(", "stream", ")", "\n", "\n", "", "config", "=", "self", ".", "get_specific_config", "(", "cso", ",", "default_config", ",", "budget", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "\"START BOHB ITERATION\"", ")", "\n", "print", "(", "'CONFIG: '", "+", "str", "(", "config", ")", ")", "\n", "print", "(", "'CSO:    '", "+", "str", "(", "cso", ")", ")", "\n", "print", "(", "'BUDGET: '", "+", "str", "(", "budget", ")", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "try", ":", "\n", "            ", "gtn", "=", "GTN_Master", "(", "config", ",", "bohb_id", "=", "bohb_id", ",", "bohb_working_dir", "=", "working_dir", ")", "\n", "_", ",", "score_list", "=", "gtn", ".", "run", "(", ")", "\n", "score", "=", "len", "(", "score_list", ")", "\n", "error", "=", "\"\"", "\n", "", "except", ":", "\n", "            ", "score", "=", "float", "(", "'Inf'", ")", "\n", "score_list", "=", "[", "]", "\n", "error", "=", "traceback", ".", "format_exc", "(", ")", "\n", "print", "(", "error", ")", "\n", "\n", "", "info", "=", "{", "}", "\n", "info", "[", "'error'", "]", "=", "str", "(", "error", ")", "\n", "info", "[", "'score_list'", "]", "=", "str", "(", "score_list", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "'FINAL SCORE: '", "+", "str", "(", "score", ")", ")", "\n", "print", "(", "'SCORE LIST:  '", "+", "str", "(", "score_list", ")", ")", "\n", "print", "(", "\"END BOHB ITERATION\"", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "return", "{", "\n", "\"loss\"", ":", "score", ",", "\n", "\"info\"", ":", "info", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld_step_size.analyze_bohb": [[24, 31], ["hpbandster.logged_results_to_HBS_result", "GTNC_visualize_gridworld_step_size.plot_parallel_scatter", "GTNC_visualize_gridworld_step_size.plot_parallel_scatter"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.analyze_bohb.plot_parallel_scatter", "home.repos.pwc.inspect_result.automl_learning_environments.automl.analyze_bohb.plot_parallel_scatter"], ["def", "analyze_bohb", "(", "log_dir", ")", ":", "\n", "# load the example run from the log files", "\n", "    ", "result", "=", "hpres", ".", "logged_results_to_HBS_result", "(", "log_dir", ")", "\n", "\n", "plot_parallel_scatter", "(", "result", ",", "with_mirrored_sampling", "=", "False", ",", "with_nes_step_size", "=", "False", ")", "\n", "# plot_parallel_scatter(result, with_mirrored_sampling=False, with_nes_step_size=True)", "\n", "plot_parallel_scatter", "(", "result", ",", "with_mirrored_sampling", "=", "True", ",", "with_nes_step_size", "=", "False", ")", "\n", "# plot_parallel_scatter(result, with_mirrored_sampling=True, with_nes_step_size=True)", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld_step_size.plot_parallel_scatter": [[34, 132], ["matplotlib.figure", "result.data.values", "range", "print", "range", "matplotlib.title", "matplotlib.ylabel", "matplotlib.yticks", "matplotlib.xticks", "savefig_name.replace.replace", "savefig_name.replace.replace", "matplotlib.savefig", "matplotlib.show", "value.results.values", "len", "numpy.zeros", "numpy.zeros", "numpy.zeros", "range", "len", "matplotlib.scatter", "yvals.append", "yticks.append", "range", "len", "len", "range", "range", "len", "GTNC_visualize_gridworld_step_size.map_to_zero_one_range", "GTNC_visualize_gridworld_step_size.get_color", "numpy.exp", "GTNC_visualize_gridworld_step_size.linear_interpolation", "str", "numpy.arange", "values[].append", "min", "max", "len", "len", "GTNC_visualize_gridworld_step_size.linear_interpolation", "len", "GTNC_visualize_gridworld_step_size.linear_interpolation", "numpy.random.uniform", "numpy.log", "numpy.log", "numpy.log", "numpy.random.uniform", "numpy.log", "decimal.Decimal", "numpy.log", "numpy.log"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.analyze_bohb.map_to_zero_one_range", "home.repos.pwc.inspect_result.automl_learning_environments.automl.analyze_bohb.get_color", "home.repos.pwc.inspect_result.automl_learning_environments.automl.analyze_bohb.linear_interpolation", "home.repos.pwc.inspect_result.automl_learning_environments.automl.analyze_bohb.linear_interpolation", "home.repos.pwc.inspect_result.automl_learning_environments.automl.analyze_bohb.linear_interpolation"], ["", "def", "plot_parallel_scatter", "(", "result", ",", "with_mirrored_sampling", ",", "with_nes_step_size", ")", ":", "\n", "    ", "fig", "=", "plt", ".", "figure", "(", "dpi", "=", "300", ",", "figsize", "=", "(", "5", ",", "4", ")", ")", "\n", "\n", "min_step_size", "=", "1e9", "\n", "max_step_size", "=", "-", "1e9", "\n", "\n", "# get all possible keys", "\n", "values", "=", "[", "[", "]", "for", "_", "in", "range", "(", "8", ")", "]", "\n", "for", "value", "in", "result", ".", "data", ".", "values", "(", ")", ":", "\n", "        ", "config", "=", "value", ".", "config", "\n", "mirrored_sampling", "=", "config", "[", "'gtn_mirrored_sampling'", "]", "\n", "nes_step_size", "=", "config", "[", "'gtn_nes_step_size'", "]", "\n", "score_transform_type", "=", "config", "[", "'gtn_score_transform_type'", "]", "\n", "step_size", "=", "config", "[", "'gtn_step_size'", "]", "\n", "\n", "if", "step_size", ">", "10", ":", "\n", "            ", "continue", "\n", "\n", "", "for", "value2", "in", "value", ".", "results", ".", "values", "(", ")", ":", "\n", "            ", "loss", "=", "value2", "[", "'loss'", "]", "\n", "\n", "if", "mirrored_sampling", "==", "with_mirrored_sampling", "and", "nes_step_size", "==", "with_nes_step_size", ":", "\n", "                ", "values", "[", "score_transform_type", "]", ".", "append", "(", "(", "step_size", ",", "loss", ")", ")", "\n", "\n", "min_step_size", "=", "min", "(", "min_step_size", ",", "step_size", ")", "\n", "max_step_size", "=", "max", "(", "max_step_size", ",", "step_size", ")", "\n", "\n", "", "", "", "loss_m", "=", "0", "\n", "loss_M", "=", "50", "\n", "\n", "x_dev", "=", "0.2", "\n", "rad", "=", "20", "\n", "alpha", "=", "1", "\n", "log_diff", "=", "10", "\n", "\n", "tot", "=", "0", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "values", ")", ")", ":", "\n", "        ", "xs", "=", "np", ".", "zeros", "(", "len", "(", "values", "[", "i", "]", ")", ")", "\n", "ys", "=", "np", ".", "zeros", "(", "len", "(", "values", "[", "i", "]", ")", ")", "\n", "colors", "=", "np", ".", "zeros", "(", "[", "len", "(", "values", "[", "i", "]", ")", ",", "3", "]", ")", "\n", "\n", "# log scale if min/max value differs to much", "\n", "if", "max_step_size", "/", "min_step_size", ">", "log_diff", ":", "\n", "            ", "for", "k", "in", "range", "(", "len", "(", "values", "[", "i", "]", ")", ")", ":", "\n", "                ", "step_size", ",", "loss", "=", "values", "[", "i", "]", "[", "k", "]", "\n", "xs", "[", "k", "]", "=", "i", "+", "1", "+", "np", ".", "random", ".", "uniform", "(", "-", "x_dev", ",", "x_dev", ")", "\n", "ys", "[", "k", "]", "=", "linear_interpolation", "(", "np", ".", "log", "(", "step_size", ")", ",", "np", ".", "log", "(", "min_step_size", ")", ",", "np", ".", "log", "(", "max_step_size", ")", ",", "0", ",", "1", ")", "\n", "# linear scale", "\n", "", "", "else", ":", "\n", "            ", "for", "k", "in", "range", "(", "len", "(", "values", "[", "i", "]", ")", ")", ":", "\n", "                ", "step_size", ",", "loss", "=", "values", "[", "i", "]", "[", "k", "]", "\n", "xs", "[", "k", "]", "=", "i", "+", "1", "+", "np", ".", "random", ".", "uniform", "(", "-", "x_dev", ",", "x_dev", ")", "\n", "ys", "[", "k", "]", "=", "linear_interpolation", "(", "step_size", ",", "min_step_size", ",", "max_step_size", ",", "0", ",", "1", ")", "\n", "\n", "", "", "for", "k", "in", "range", "(", "len", "(", "values", "[", "i", "]", ")", ")", ":", "\n", "            ", "step_size", ",", "loss", "=", "values", "[", "i", "]", "[", "k", "]", "\n", "acc", "=", "map_to_zero_one_range", "(", "loss", ",", "loss_m", ",", "loss_M", ")", "\n", "colors", "[", "k", ",", ":", "]", "=", "get_color", "(", "acc", ")", "\n", "\n", "", "tot", "+=", "len", "(", "xs", ")", "\n", "\n", "plt", ".", "scatter", "(", "xs", ",", "ys", ",", "s", "=", "rad", ",", "c", "=", "colors", ",", "alpha", "=", "alpha", ",", "edgecolors", "=", "'none'", ")", "\n", "\n", "", "print", "(", "tot", ")", "\n", "\n", "yvals", "=", "[", "]", "\n", "yticks", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "11", ")", ":", "\n", "        ", "val", "=", "i", "/", "10", "\n", "yvals", ".", "append", "(", "val", ")", "\n", "if", "max_step_size", "/", "min_step_size", ">", "log_diff", ":", "\n", "            ", "ytick", "=", "np", ".", "exp", "(", "np", ".", "log", "(", "min_step_size", ")", "+", "(", "np", ".", "log", "(", "max_step_size", ")", "-", "np", ".", "log", "(", "min_step_size", ")", ")", "*", "val", ")", "\n", "", "else", ":", "\n", "            ", "ytick", "=", "linear_interpolation", "(", "val", ",", "0", ",", "1", ",", "min_step_size", ",", "max_step_size", ")", "\n", "", "yticks", ".", "append", "(", "str", "(", "f\"{Decimal(ytick):.1E}\"", ")", ")", "\n", "\n", "", "if", "with_nes_step_size", ":", "\n", "        ", "nes_string", "=", "'w/ NES step size'", "\n", "", "else", ":", "\n", "        ", "nes_string", "=", "'w/o NES step size'", "\n", "", "nes_string", "=", "''", "\n", "\n", "if", "with_mirrored_sampling", ":", "\n", "        ", "mir_string", "=", "'w/ mirrored sampling'", "\n", "", "else", ":", "\n", "        ", "mir_string", "=", "'w/o mirrored sampling'", "\n", "\n", "", "plt", ".", "title", "(", "mir_string", "+", "' '", "+", "nes_string", ")", "\n", "plt", ".", "ylabel", "(", "'step size'", ")", "\n", "plt", ".", "yticks", "(", "yvals", ",", "yticks", ")", "\n", "plt", ".", "xticks", "(", "np", ".", "arange", "(", "8", ")", "+", "1", ",", "(", "'linear transf.'", ",", "'rank transf.'", ",", "'NES'", ",", "'NES unnorm.'", ",", "'single best'", ",", "'single better'", ",", "'all better 1'", ",", "'all better 2'", ")", ",", "rotation", "=", "90", ")", "\n", "\n", "savefig_name", "=", "'visualize_step_size_'", "+", "nes_string", "[", ":", "3", "]", "+", "' '", "+", "mir_string", "[", ":", "3", "]", "+", "'.svg'", "\n", "savefig_name", "=", "savefig_name", ".", "replace", "(", "' '", ",", "'_'", ")", "\n", "savefig_name", "=", "savefig_name", ".", "replace", "(", "'/'", ",", "'_'", ")", "\n", "plt", ".", "savefig", "(", "savefig_name", ",", "bbox_inches", "=", "'tight'", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld_step_size.linear_interpolation": [[134, 137], ["None"], "function", ["None"], ["", "def", "linear_interpolation", "(", "x", ",", "x0", ",", "x1", ",", "y0", ",", "y1", ")", ":", "\n", "# linearly interpolate between two x/y values for a given x value", "\n", "    ", "return", "y0", "+", "(", "y1", "-", "y0", ")", "*", "(", "x", "-", "x0", ")", "/", "(", "x1", "-", "x0", "+", "1e-9", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld_step_size.map_to_zero_one_range": [[139, 155], ["None"], "function", ["None"], ["", "def", "map_to_zero_one_range", "(", "loss", ",", "loss_m", ",", "loss_M", ")", ":", "\n", "    ", "if", "loss_M", "<", "1", "and", "loss_m", ">", "0", "and", "REVERSE_LOSS", "==", "False", ":", "\n", "# if we have already a loss in the [0,1] range, there is no need to normalize anything", "\n", "        ", "acc", "=", "loss", "\n", "", "elif", "loss_M", "<", "0", "and", "loss_m", ">", "-", "1", "and", "REVERSE_LOSS", "==", "True", ":", "\n", "# if we have a loss in the [-1,0] range, simply revert its sign", "\n", "        ", "acc", "=", "-", "loss", "\n", "", "else", ":", "\n", "# normalize loss to the 0 (bad) - 1(good) range", "\n", "        ", "acc", "=", "(", "loss", "-", "loss_m", ")", "/", "(", "loss_M", "-", "loss_m", ")", "\n", "if", "REVERSE_LOSS", ":", "\n", "            ", "acc", "=", "1", "-", "acc", "\n", "\n", "", "", "acc", "=", "acc", "**", "EXP_LOSS", "\n", "\n", "return", "acc", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld_step_size.get_color": [[157, 166], ["numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array"], "function", ["None"], ["", "def", "get_color", "(", "acc", ")", ":", "\n", "    ", "if", "acc", "<=", "0", ":", "\n", "        ", "return", "np", ".", "array", "(", "[", "[", "1", ",", "0", ",", "0", "]", "]", ")", "\n", "", "elif", "acc", "<=", "0.5", ":", "\n", "        ", "return", "np", ".", "array", "(", "[", "[", "1", ",", "0", ",", "0", "]", "]", ")", "+", "2", "*", "acc", "*", "np", ".", "array", "(", "[", "[", "0", ",", "1", ",", "0", "]", "]", ")", "\n", "", "elif", "acc", "<=", "1", ":", "\n", "        ", "return", "np", ".", "array", "(", "[", "[", "1", ",", "1", ",", "0", "]", "]", ")", "+", "2", "*", "(", "acc", "-", "0.5", ")", "*", "np", ".", "array", "(", "[", "[", "-", "1", ",", "0", ",", "0", "]", "]", ")", "\n", "", "else", ":", "\n", "        ", "return", "np", ".", "array", "(", "[", "[", "0", ",", "1", ",", "0", "]", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld_step_size.get_bright_random_color": [[168, 171], ["colorsys.hls_to_rgb", "random.random"], "function", ["None"], ["", "", "def", "get_bright_random_color", "(", ")", ":", "\n", "    ", "h", ",", "s", ",", "l", "=", "random", ".", "random", "(", ")", ",", "1", ",", "0.5", "\n", "return", "colorsys", ".", "hls_to_rgb", "(", "h", ",", "l", ",", "s", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cmc_params.ExperimentWrapper.get_bohb_parameters": [[18, 27], ["None"], "methods", ["None"], ["    ", "def", "get_bohb_parameters", "(", "self", ")", ":", "\n", "        ", "params", "=", "{", "}", "\n", "params", "[", "'min_budget'", "]", "=", "1", "\n", "params", "[", "'max_budget'", "]", "=", "3", "\n", "params", "[", "'eta'", "]", "=", "3", "\n", "params", "[", "'random_fraction'", "]", "=", "0.3", "\n", "params", "[", "'iterations'", "]", "=", "10000", "\n", "\n", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cmc_params.ExperimentWrapper.get_configspace": [[28, 56], ["ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter"], "methods", ["None"], ["", "def", "get_configspace", "(", "self", ")", ":", "\n", "        ", "cs", "=", "CS", ".", "ConfigurationSpace", "(", ")", "\n", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'gtn_score_transform_type'", ",", "lower", "=", "0", ",", "upper", "=", "7", ",", "log", "=", "False", ",", "default_value", "=", "7", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'gtn_step_size'", ",", "lower", "=", "0.1", ",", "upper", "=", "1", ",", "log", "=", "True", ",", "default_value", "=", "0.5", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "CategoricalHyperparameter", "(", "name", "=", "'gtn_mirrored_sampling'", ",", "choices", "=", "[", "False", ",", "True", "]", ",", "default_value", "=", "True", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'gtn_noise_std'", ",", "lower", "=", "0.001", ",", "upper", "=", "1", ",", "log", "=", "True", ",", "default_value", "=", "0.01", ")", ")", "\n", "\n", "# cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(name='td3_init_episodes', lower=1, upper=20, log=True, default_value=10))", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'td3_batch_size'", ",", "lower", "=", "64", ",", "upper", "=", "256", ",", "log", "=", "False", ",", "default_value", "=", "128", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_gamma'", ",", "lower", "=", "0.001", ",", "upper", "=", "0.1", ",", "log", "=", "True", ",", "default_value", "=", "0.01", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_lr'", ",", "lower", "=", "1e-4", ",", "upper", "=", "5e-3", ",", "log", "=", "True", ",", "default_value", "=", "1e-3", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_tau'", ",", "lower", "=", "0.005", ",", "upper", "=", "0.05", ",", "log", "=", "True", ",", "default_value", "=", "0.01", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'td3_policy_delay'", ",", "lower", "=", "1", ",", "upper", "=", "3", ",", "log", "=", "False", ",", "default_value", "=", "2", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "CategoricalHyperparameter", "(", "name", "=", "'td3_activation_fn'", ",", "choices", "=", "[", "'tanh'", ",", "'relu'", ",", "'leakyrelu'", ",", "'prelu'", "]", ",", "default_value", "=", "'relu'", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'td3_hidden_size'", ",", "lower", "=", "48", ",", "upper", "=", "192", ",", "log", "=", "True", ",", "default_value", "=", "128", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'td3_hidden_layer'", ",", "lower", "=", "1", ",", "upper", "=", "2", ",", "log", "=", "False", ",", "default_value", "=", "2", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'td3_same_action_num'", ",", "lower", "=", "1", ",", "upper", "=", "3", ",", "log", "=", "False", ",", "default_value", "=", "1", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_action_std'", ",", "lower", "=", "0.05", ",", "upper", "=", "0.2", ",", "log", "=", "True", ",", "default_value", "=", "0.1", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_policy_std'", ",", "lower", "=", "0.1", ",", "upper", "=", "0.4", ",", "log", "=", "True", ",", "default_value", "=", "0.2", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_policy_std_clip'", ",", "lower", "=", "0.25", ",", "upper", "=", "1", ",", "log", "=", "True", ",", "default_value", "=", "0.5", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_early_out_virtual_diff'", ",", "lower", "=", "1e-2", ",", "upper", "=", "1e-1", ",", "log", "=", "True", ",", "default_value", "=", "3e-2", ")", ")", "\n", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "CategoricalHyperparameter", "(", "name", "=", "'cmc_activation_fn'", ",", "choices", "=", "[", "'tanh'", ",", "'relu'", ",", "'leakyrelu'", ",", "'prelu'", "]", ",", "default_value", "=", "'leakyrelu'", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'cmc_hidden_size'", ",", "lower", "=", "48", ",", "upper", "=", "192", ",", "log", "=", "True", ",", "default_value", "=", "128", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'cmc_hidden_layer'", ",", "lower", "=", "1", ",", "upper", "=", "2", ",", "log", "=", "False", ",", "default_value", "=", "1", ")", ")", "\n", "\n", "return", "cs", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cmc_params.ExperimentWrapper.get_specific_config": [[57, 89], ["copy.deepcopy"], "methods", ["None"], ["", "def", "get_specific_config", "(", "self", ",", "cso", ",", "default_config", ",", "budget", ")", ":", "\n", "        ", "config", "=", "deepcopy", "(", "default_config", ")", "\n", "\n", "config", "[", "\"agents\"", "]", "[", "'gtn'", "]", "[", "'score_transform_type'", "]", "=", "cso", "[", "\"gtn_score_transform_type\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'gtn'", "]", "[", "'step_size'", "]", "=", "cso", "[", "\"gtn_step_size\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'gtn'", "]", "[", "'mirrored_sampling'", "]", "=", "cso", "[", "\"gtn_mirrored_sampling\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'gtn'", "]", "[", "'noise_std'", "]", "=", "cso", "[", "\"gtn_noise_std\"", "]", "\n", "\n", "# config[\"agents\"]['td3']['init_episodes'] = cso[\"td3_init_episodes\"]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'batch_size'", "]", "=", "cso", "[", "\"td3_batch_size\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'gamma'", "]", "=", "1", "-", "cso", "[", "\"td3_gamma\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'lr'", "]", "=", "cso", "[", "\"td3_lr\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'tau'", "]", "=", "cso", "[", "\"td3_tau\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'policy_delay'", "]", "=", "cso", "[", "\"td3_policy_delay\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'activation_fn'", "]", "=", "cso", "[", "\"td3_activation_fn\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'hidden_size'", "]", "=", "cso", "[", "\"td3_hidden_size\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'hidden_layer'", "]", "=", "cso", "[", "\"td3_hidden_layer\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'same_action_num'", "]", "=", "cso", "[", "\"td3_same_action_num\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'action_std'", "]", "=", "cso", "[", "\"td3_action_std\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'policy_std'", "]", "=", "cso", "[", "\"td3_policy_std\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'policy_std_clip'", "]", "=", "cso", "[", "\"td3_policy_std_clip\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'early_out_virtual_diff'", "]", "=", "cso", "[", "\"td3_early_out_virtual_diff\"", "]", "\n", "\n", "config", "[", "\"envs\"", "]", "[", "'MountainCarContinuous-v0'", "]", "[", "'activation_fn'", "]", "=", "cso", "[", "\"cmc_activation_fn\"", "]", "\n", "config", "[", "\"envs\"", "]", "[", "'MountainCarContinuous-v0'", "]", "[", "'hidden_size'", "]", "=", "cso", "[", "\"cmc_hidden_size\"", "]", "\n", "config", "[", "\"envs\"", "]", "[", "'MountainCarContinuous-v0'", "]", "[", "'hidden_layer'", "]", "=", "cso", "[", "\"cmc_hidden_layer\"", "]", "\n", "\n", "global", "reward_env_type", "\n", "config", "[", "\"agents\"", "]", "[", "'gtn'", "]", "[", "'synthetic_env_type'", "]", "=", "1", "\n", "config", "[", "\"envs\"", "]", "[", "'MountainCarContinuous-v0'", "]", "[", "'reward_env_type'", "]", "=", "reward_env_type", "\n", "\n", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cmc_params.ExperimentWrapper.compute": [[90, 130], ["GTNC_evaluate_cmc_params.ExperimentWrapper.get_specific_config", "print", "print", "print", "print", "print", "print", "str", "str", "str", "str", "print", "print", "print", "print", "print", "open", "yaml.safe_load", "agents.GTN.GTN_Master", "agents.GTN.GTN_Master.run", "str", "str", "str", "float", "traceback.format_exc", "print", "str", "str", "sorted"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_step_size.ExperimentWrapper.get_specific_config", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.run"], ["", "def", "compute", "(", "self", ",", "working_dir", ",", "bohb_id", ",", "config_id", ",", "cso", ",", "budget", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "with", "open", "(", "\"default_config_cmc.yaml\"", ",", "'r'", ")", "as", "stream", ":", "\n", "            ", "default_config", "=", "yaml", ".", "safe_load", "(", "stream", ")", "\n", "\n", "", "config", "=", "self", ".", "get_specific_config", "(", "cso", ",", "default_config", ",", "budget", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "\"START BOHB ITERATION\"", ")", "\n", "print", "(", "'CONFIG: '", "+", "str", "(", "config", ")", ")", "\n", "print", "(", "'CSO:    '", "+", "str", "(", "cso", ")", ")", "\n", "print", "(", "'BUDGET: '", "+", "str", "(", "budget", ")", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "try", ":", "\n", "            ", "gtn", "=", "GTN_Master", "(", "config", ",", "bohb_id", "=", "bohb_id", ",", "bohb_working_dir", "=", "working_dir", ")", "\n", "_", ",", "score_list", ",", "model_name", "=", "gtn", ".", "run", "(", ")", "\n", "score", "=", "-", "sorted", "(", "score_list", ")", "[", "-", "1", "]", "\n", "error", "=", "\"\"", "\n", "", "except", ":", "\n", "            ", "score", "=", "float", "(", "'-Inf'", ")", "\n", "score_list", "=", "[", "]", "\n", "model_name", "=", "None", "\n", "error", "=", "traceback", ".", "format_exc", "(", ")", "\n", "print", "(", "error", ")", "\n", "\n", "", "info", "=", "{", "}", "\n", "info", "[", "'error'", "]", "=", "str", "(", "error", ")", "\n", "info", "[", "'config'", "]", "=", "str", "(", "config", ")", "\n", "info", "[", "'score_list'", "]", "=", "str", "(", "score_list", ")", "\n", "info", "[", "'model_name'", "]", "=", "str", "(", "model_name", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "'FINAL SCORE: '", "+", "str", "(", "score", ")", ")", "\n", "print", "(", "'SCORE LIST:  '", "+", "str", "(", "score_list", ")", ")", "\n", "print", "(", "\"END BOHB ITERATION\"", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "return", "{", "\n", "\"loss\"", ":", "score", ",", "\n", "\"info\"", ":", "info", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld_heatmap.idx_to_xy": [[12, 16], ["None"], "function", ["None"], ["def", "idx_to_xy", "(", "idx", ",", "n", ")", ":", "\n", "    ", "x", "=", "idx", "//", "n", "\n", "y", "=", "idx", "%", "n", "\n", "return", "y", ",", "-", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld_heatmap.xy_to_idx": [[18, 22], ["None"], "function", ["None"], ["", "def", "xy_to_idx", "(", "xy", ",", "n", ")", ":", "\n", "    ", "y", ",", "x", "=", "xy", "\n", "obs", "=", "-", "x", "*", "n", "+", "y", "\n", "return", "obs", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld_heatmap.plot_models": [[24, 80], ["torch.load", "collections.Counter", "matplotlib.subplots", "collections.Counter.items", "range", "range", "matplotlib.plot", "matplotlib.text", "matplotlib.text", "matplotlib.text", "ax.axis", "ax.axis", "matplotlib.savefig", "matplotlib.show", "GTNC_visualize_gridworld_heatmap.idx_to_xy", "matplotlib.fill", "matplotlib.plot", "matplotlib.plot", "matplotlib.title", "max", "numpy.array", "matplotlib.title", "collections.Counter.values", "numpy.array", "matplotlib.title", "str", "matplotlib.title", "matplotlib.title", "str", "matplotlib.title", "matplotlib.title", "matplotlib.title"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld_heatmap.idx_to_xy"], ["", "def", "plot_models", "(", ")", ":", "\n", "    ", "save_dict", "=", "torch", ".", "load", "(", "RESULT_FILE", ")", "\n", "state_list", "=", "save_dict", "[", "'state_list'", "]", "\n", "mode", "=", "save_dict", "[", "'mode'", "]", "\n", "break_cond", "=", "save_dict", "[", "'break'", "]", "\n", "\n", "state_list_flat", "=", "[", "elem", "for", "states", "in", "state_list", "for", "elem", "in", "states", "]", "\n", "state_count", "=", "Counter", "(", "state_list_flat", ")", "\n", "\n", "# plot individual rewards", "\n", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", "dpi", "=", "600", ",", "figsize", "=", "(", "7", ",", "2.5", ")", ")", "\n", "\n", "for", "idx", ",", "count", "in", "state_count", ".", "items", "(", ")", ":", "\n", "        ", "intensity", "=", "count", "/", "max", "(", "state_count", ".", "values", "(", ")", ")", "\n", "x", ",", "y", "=", "idx_to_xy", "(", "idx", ",", "N", ")", "\n", "\n", "xs", "=", "[", "x", "-", "0.5", ",", "x", "-", "0.5", ",", "x", "+", "0.5", ",", "x", "+", "0.5", "]", "\n", "ys", "=", "[", "y", "-", "0.5", ",", "y", "+", "0.5", ",", "y", "+", "0.5", ",", "y", "-", "0.5", "]", "\n", "\n", "color", "=", "np", ".", "array", "(", "[", "1", ",", "1", ",", "1", "]", ")", "-", "intensity", "*", "np", ".", "array", "(", "[", "0", ",", "1", ",", "1", "]", ")", "\n", "plt", ".", "fill", "(", "xs", ",", "ys", ",", "facecolor", "=", "color", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "5", ")", ":", "\n", "        ", "plt", ".", "plot", "(", "[", "-", "0.5", ",", "11.5", "]", ",", "[", "-", "i", "+", "0.5", ",", "-", "i", "+", "0.5", "]", ",", "linewidth", "=", "0.5", ",", "color", "=", "'black'", ")", "\n", "", "for", "i", "in", "range", "(", "13", ")", ":", "\n", "        ", "plt", ".", "plot", "(", "[", "i", "-", "0.5", ",", "i", "-", "0.5", "]", ",", "[", "0.5", ",", "-", "3.5", "]", ",", "linewidth", "=", "0.5", ",", "color", "=", "'black'", ")", "\n", "\n", "# plot additional information", "\n", "", "x_water", "=", "[", "0.5", ",", "10.5", ",", "10.5", ",", "0.5", ",", "0.5", "]", "\n", "y_water", "=", "[", "-", "2.5", ",", "-", "2.5", ",", "-", "3.5", ",", "-", "3.5", ",", "-", "2.5", "]", "\n", "plt", ".", "plot", "(", "x_water", ",", "y_water", ",", "linewidth", "=", "2", ",", "color", "=", "'black'", ")", "\n", "plt", ".", "text", "(", "5.5", ",", "-", "3", ",", "'cliff'", ",", "size", "=", "12", ",", "color", "=", "'black'", ",", "ha", "=", "'center'", ",", "va", "=", "'center'", ")", "\n", "plt", ".", "text", "(", "0", ",", "-", "3", ",", "'(S)'", ",", "size", "=", "12", ",", "ha", "=", "'center'", ",", "va", "=", "'center'", ")", "\n", "plt", ".", "text", "(", "11", ",", "-", "3", ",", "'(G)'", ",", "size", "=", "12", ",", "ha", "=", "'center'", ",", "va", "=", "'center'", ")", "\n", "\n", "if", "mode", "==", "'1'", "and", "break_cond", "==", "'solved'", ":", "\n", "        ", "plt", ".", "title", "(", "'exclusive potential reward network (solved)'", ")", "\n", "", "elif", "mode", "==", "'1'", "and", "break_cond", "==", "'end'", ":", "\n", "        ", "plt", ".", "title", "(", "'exclusive potential reward network (end of training)'", ")", "\n", "", "elif", "mode", "==", "'2'", "and", "break_cond", "==", "'solved'", ":", "\n", "        ", "plt", ".", "title", "(", "'additive potential reward network (solved)'", ")", "\n", "", "elif", "mode", "==", "'2'", "and", "break_cond", "==", "'end'", ":", "\n", "        ", "plt", ".", "title", "(", "'additive potential reward network (end of training)'", ")", "\n", "", "elif", "mode", "==", "'5'", "and", "break_cond", "==", "'solved'", ":", "\n", "        ", "plt", ".", "title", "(", "'exclusive non-potential reward network (solved)'", ")", "\n", "", "elif", "mode", "==", "'5'", "and", "break_cond", "==", "'end'", ":", "\n", "        ", "plt", ".", "title", "(", "'exclusive non-potential reward network (end of training)'", ")", "\n", "", "elif", "mode", "==", "'6'", "and", "break_cond", "==", "'solved'", ":", "\n", "        ", "plt", ".", "title", "(", "'additive non-potential reward network (solved)'", ")", "\n", "", "elif", "mode", "==", "'6'", "and", "break_cond", "==", "'end'", ":", "\n", "        ", "plt", ".", "title", "(", "'additive non-potential reward network (end of training)'", ")", "\n", "\n", "", "ax", ".", "axis", "(", "'equal'", ")", "\n", "ax", ".", "axis", "(", "'off'", ")", "\n", "plt", ".", "savefig", "(", "'cliff_heatmap_'", "+", "str", "(", "break_cond", ")", "+", "'_'", "+", "str", "(", "mode", ")", "+", "'.svg'", ",", "bbox_inches", "=", "'tight'", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld_success_perc.get_data": [[15, 54], ["len", "hpbandster.logged_results_to_HBS_result", "hpres.logged_results_to_HBS_result.get_all_runs", "hpres.logged_results_to_HBS_result.get_id2config_mapping", "list_data.append", "numpy.zeros", "range", "numpy.mean", "numpy.std", "proc_data.append", "ast.literal_eval", "data.append", "len", "numpy.array", "range", "len"], "function", ["None"], ["def", "get_data", "(", "finish_after_solved", ")", ":", "\n", "    ", "list_data", "=", "[", "]", "\n", "for", "log_dir", "in", "LOG_DIRS", ":", "\n", "        ", "result", "=", "hpres", ".", "logged_results_to_HBS_result", "(", "log_dir", ")", "\n", "all_runs", "=", "result", ".", "get_all_runs", "(", ")", "\n", "id2conf", "=", "result", ".", "get_id2config_mapping", "(", ")", "\n", "\n", "# copy data to list", "\n", "data", "=", "[", "]", "\n", "\n", "for", "run", "in", "all_runs", ":", "\n", "            ", "avg_rewards", "=", "ast", ".", "literal_eval", "(", "run", "[", "'info'", "]", "[", "'score_list'", "]", ")", "\n", "config_id", "=", "run", "[", "'config_id'", "]", "\n", "config", "=", "id2conf", "[", "config_id", "]", "[", "'config'", "]", "\n", "\n", "if", "finish_after_solved", ":", "\n", "                ", "for", "k", "in", "range", "(", "1", ",", "len", "(", "avg_rewards", ")", ")", ":", "\n", "                    ", "if", "avg_rewards", "[", "k", "-", "1", "]", ">", "SOLVED_REWARD", ":", "\n", "                        ", "avg_rewards", "[", "k", "]", "=", "avg_rewards", "[", "k", "-", "1", "]", "\n", "\n", "", "", "", "data", ".", "append", "(", "avg_rewards", ")", "\n", "", "list_data", ".", "append", "(", "data", ")", "\n", "\n", "# copy from list to numpy array", "\n", "", "proc_data", "=", "[", "]", "\n", "\n", "n", "=", "len", "(", "list_data", "[", "0", "]", "[", "0", "]", ")", "\n", "for", "data", "in", "list_data", ":", "\n", "        ", "np_data", "=", "np", ".", "zeros", "(", "[", "MAX_VALS", ",", "n", "]", ")", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "np_data", ")", ")", ":", "\n", "            ", "np_data", "[", "i", "]", "=", "np", ".", "array", "(", "data", "[", "i", "]", ")", "\n", "\n", "", "mean", "=", "np", ".", "mean", "(", "np_data", ",", "axis", "=", "0", ")", "\n", "std", "=", "np", ".", "std", "(", "np_data", ",", "axis", "=", "0", ")", "\n", "\n", "proc_data", ".", "append", "(", "(", "mean", ",", "std", ")", ")", "\n", "\n", "", "return", "proc_data", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_gridworld_success_perc.plot_data": [[56, 76], ["matplotlib.subplots", "enumerate", "matplotlib.legend", "matplotlib.xlim", "matplotlib.ylim", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.savefig", "matplotlib.show", "matplotlib.plot", "matplotlib.fill_between", "range", "len"], "function", ["None"], ["", "def", "plot_data", "(", "data", ",", "savefig_name", ")", ":", "\n", "    ", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", "dpi", "=", "600", ",", "figsize", "=", "(", "5", ",", "4", ")", ")", "\n", "colors", "=", "[", "]", "\n", "#", "\n", "# for mean, _ in data_w:", "\n", "#     plt.plot(mean_w)", "\n", "#     colors.append(plt.gca().lines[-1].get_color())", "\n", "\n", "for", "i", ",", "data", "in", "enumerate", "(", "data", ")", ":", "\n", "        ", "mean", ",", "std", "=", "data", "\n", "plt", ".", "plot", "(", "mean", ")", "\n", "plt", ".", "fill_between", "(", "x", "=", "range", "(", "len", "(", "mean", ")", ")", ",", "y1", "=", "mean", "-", "std", "*", "STD_MULT", ",", "y2", "=", "mean", "+", "std", "*", "STD_MULT", ",", "alpha", "=", "0.1", ")", "\n", "\n", "", "plt", ".", "legend", "(", "(", "'2x2 grid world'", ",", "'2x3 grid world'", ",", "'3x3 grid world'", ")", ")", "\n", "plt", ".", "xlim", "(", "0", ",", "49", ")", "\n", "plt", ".", "ylim", "(", "-", "0.2", ",", "1", ")", "\n", "plt", ".", "xlabel", "(", "'ES iteration'", ")", "\n", "plt", ".", "ylabel", "(", "'average reward'", ")", "\n", "plt", ".", "savefig", "(", "savefig_name", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_transfer_vary_hp.get_best_models_from_log": [[45, 75], ["hpbandster.logged_results_to_HBS_result", "hpres.logged_results_to_HBS_result.data.values", "print", "best_models.sort", "os.path.isdir", "log_dir.replace.replace", "best_models.append", "os.path.isfile", "model_name.replace.replace"], "function", ["None"], ["def", "get_best_models_from_log", "(", "log_dir", ")", ":", "\n", "    ", "if", "not", "os", ".", "path", ".", "isdir", "(", "log_dir", ")", ":", "\n", "        ", "log_dir", "=", "log_dir", ".", "replace", "(", "'nierhoff'", ",", "'dingsda'", ")", "\n", "\n", "", "result", "=", "hpres", ".", "logged_results_to_HBS_result", "(", "log_dir", ")", "\n", "\n", "best_models", "=", "[", "]", "\n", "\n", "for", "value", "in", "result", ".", "data", ".", "values", "(", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "loss", "=", "value", ".", "results", "[", "1.0", "]", "[", "'loss'", "]", "\n", "model_name", "=", "value", ".", "results", "[", "1.0", "]", "[", "'info'", "]", "[", "'model_name'", "]", "\n", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "model_name", ")", ":", "\n", "                ", "model_name", "=", "model_name", ".", "replace", "(", "'nierhoff'", ",", "'dingsda'", ")", "\n", "", "best_models", ".", "append", "(", "(", "loss", ",", "model_name", ")", ")", "\n", "", "except", ":", "\n", "            ", "continue", "\n", "\n", "# before AUC (objective minimized)", "\n", "# print(\"sorting from low to high values (non-AUC)\")", "\n", "# best_models.sort(key=lambda x: x[0])", "\n", "# best_models = best_models[:MODEL_NUM]", "\n", "\n", "# AUC (objective maximized)", "\n", "", "", "print", "(", "\"sorting from high to low values (AUC)\"", ")", "\n", "best_models", ".", "sort", "(", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ",", "reverse", "=", "True", ")", "\n", "best_models", "=", "best_models", "[", ":", "MODEL_NUM", "]", "\n", "\n", "return", "best_models", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_transfer_vary_hp.load_envs_and_config": [[77, 90], ["torch.load", "envs.env_factory.EnvFactory", "envs.env_factory.EnvFactory.generate_reward_env", "env_factory.generate_reward_env.load_state_dict", "envs.env_factory.EnvFactory.generate_real_env"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_reward_env", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_real_env"], ["", "def", "load_envs_and_config", "(", "model_file", ")", ":", "\n", "    ", "save_dict", "=", "torch", ".", "load", "(", "model_file", ")", "\n", "\n", "config", "=", "save_dict", "[", "'config'", "]", "\n", "# config['device'] = 'cuda'", "\n", "config", "[", "'envs'", "]", "[", "'Cliff'", "]", "[", "'solved_reward'", "]", "=", "100000", "# something big enough to prevent early out triggering", "\n", "\n", "env_factory", "=", "EnvFactory", "(", "config", "=", "config", ")", "\n", "reward_env", "=", "env_factory", ".", "generate_reward_env", "(", ")", "\n", "reward_env", ".", "load_state_dict", "(", "save_dict", "[", "'model'", "]", ")", "\n", "real_env", "=", "env_factory", ".", "generate_real_env", "(", ")", "\n", "\n", "return", "reward_env", ",", "real_env", ",", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_transfer_vary_hp.vary_hp": [[92, 124], ["ConfigSpace.ConfigurationSpace", "CS.ConfigurationSpace.add_hyperparameter", "CS.ConfigurationSpace.add_hyperparameter", "CS.ConfigurationSpace.sample_configuration", "print", "copy.deepcopy", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter"], "function", ["None"], ["", "def", "vary_hp", "(", "config", ")", ":", "\n", "    ", "alpha", "=", "config", "[", "'agents'", "]", "[", "'ql'", "]", "[", "'alpha'", "]", "\n", "gamma", "=", "config", "[", "'agents'", "]", "[", "'ql'", "]", "[", "'gamma'", "]", "\n", "\n", "cs", "=", "CS", ".", "ConfigurationSpace", "(", ")", "\n", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'alpha'", ",", "\n", "lower", "=", "0.1", ",", "\n", "upper", "=", "1", ",", "\n", "log", "=", "False", ",", "\n", "default_value", "=", "alpha", ")", "\n", ")", "\n", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'gamma'", ",", "\n", "lower", "=", "0.1", ",", "\n", "upper", "=", "1", ",", "\n", "log", "=", "False", ",", "\n", "default_value", "=", "gamma", ")", "\n", ")", "\n", "\n", "sample", "=", "cs", ".", "sample_configuration", "(", ")", "\n", "\n", "print", "(", "f\"sampled part of config: \"", "\n", "f\"alpha: {sample['alpha']}, \"", "\n", "f\"gamma: {sample['gamma']}, \"", "\n", ")", "\n", "\n", "config_mod", "=", "deepcopy", "(", "config", ")", "\n", "config_mod", "[", "'agents'", "]", "[", "'ql'", "]", "[", "'alpha'", "]", "=", "sample", "[", "'alpha'", "]", "\n", "config_mod", "[", "'agents'", "]", "[", "'ql'", "]", "[", "'gamma'", "]", "=", "sample", "[", "'gamma'", "]", "\n", "\n", "return", "config_mod", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_transfer_vary_hp.train_test_agents": [[126, 164], ["range", "GTNC_evaluate_gridworld_transfer_vary_hp.vary_hp", "agents.agent_utils.select_agent.train", "rewards.append", "episode_lengths.append", "agents.agent_utils.select_agent", "agents.agent_utils.select_agent"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_transfer_vary_hp.vary_hp", "home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.train", "home.repos.pwc.inspect_result.automl_learning_environments.agents.agent_utils.select_agent", "home.repos.pwc.inspect_result.automl_learning_environments.agents.agent_utils.select_agent"], ["", "def", "train_test_agents", "(", "mode", ",", "env", ",", "real_env", ",", "config", ")", ":", "\n", "    ", "rewards", "=", "[", "]", "\n", "episode_lengths", "=", "[", "]", "\n", "\n", "# settings for comparability", "\n", "config", "[", "'agents'", "]", "[", "'ql'", "]", "[", "'test_episodes'", "]", "=", "1", "\n", "config", "[", "'agents'", "]", "[", "'ql'", "]", "[", "'train_episodes'", "]", "=", "500", "\n", "config", "[", "'agents'", "]", "[", "'ql'", "]", "[", "'print_rate'", "]", "=", "100", "\n", "\n", "config", "[", "'agents'", "]", "[", "'ql'", "]", "[", "'alpha'", "]", "=", "1.0", "\n", "config", "[", "'agents'", "]", "[", "'ql'", "]", "[", "'eps_decay'", "]", "=", "0.0", "\n", "config", "[", "'agents'", "]", "[", "'ql'", "]", "[", "'eps_init'", "]", "=", "0.1", "\n", "config", "[", "'agents'", "]", "[", "'ql'", "]", "[", "'eps_min'", "]", "=", "0.1", "\n", "config", "[", "'agents'", "]", "[", "'ql'", "]", "[", "'gamma'", "]", "=", "0.8", "\n", "config", "[", "'agents'", "]", "[", "'ql'", "]", "[", "'same_action_num'", "]", "=", "1", "\n", "config", "[", "'agents'", "]", "[", "'ql'", "]", "[", "'rb_size'", "]", "=", "1", "# custom to reward env and gridworld", "\n", "config", "[", "'agents'", "]", "[", "'ql'", "]", "[", "'init_episodes'", "]", "=", "0", "\n", "config", "[", "'agents'", "]", "[", "'ql'", "]", "[", "'batch_size'", "]", "=", "1", "\n", "\n", "config", "[", "'agents'", "]", "[", "'ql'", "]", "[", "'early_out_num'", "]", "=", "10", "\n", "config", "[", "'agents'", "]", "[", "'ql'", "]", "[", "'early_out_virtual_diff'", "]", "=", "0.02", "\n", "\n", "# for count-based q-learning", "\n", "config", "[", "'agents'", "]", "[", "'ql'", "]", "[", "'beta'", "]", "=", "0.1", "\n", "\n", "# for count-based q-learning (tuned)", "\n", "# config['agents']['ql']['beta'] = 0.005  # 0.01 also works fine", "\n", "\n", "for", "i", "in", "range", "(", "MODEL_AGENTS", ")", ":", "\n", "        ", "config_mod", "=", "vary_hp", "(", "config", ")", "\n", "if", "mode", "==", "'-1'", ":", "\n", "            ", "agent", "=", "select_agent", "(", "config", "=", "config_mod", ",", "agent_name", "=", "'ql_cb'", ")", "\n", "", "else", ":", "\n", "            ", "agent", "=", "select_agent", "(", "config", "=", "config_mod", ",", "agent_name", "=", "'ql'", ")", "\n", "", "reward", ",", "episode_length", ",", "_", "=", "agent", ".", "train", "(", "env", "=", "env", ",", "test_env", "=", "real_env", ")", "\n", "rewards", ".", "append", "(", "reward", ")", "\n", "episode_lengths", ".", "append", "(", "episode_length", ")", "\n", "", "return", "rewards", ",", "episode_lengths", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_transfer_vary_hp.save_list": [[166, 176], ["os.makedirs", "os.path.join", "torch.save", "str"], "function", ["None"], ["", "def", "save_list", "(", "mode", ",", "config", ",", "reward_list", ",", "episode_length_list", ")", ":", "\n", "    ", "os", ".", "makedirs", "(", "SAVE_DIR", ",", "exist_ok", "=", "True", ")", "\n", "file_name", "=", "os", ".", "path", ".", "join", "(", "SAVE_DIR", ",", "'best_transfer_vary_hp'", "+", "str", "(", "mode", ")", "+", "'.pt'", ")", "\n", "save_dict", "=", "{", "}", "\n", "save_dict", "[", "'config'", "]", "=", "config", "\n", "save_dict", "[", "'model_num'", "]", "=", "MODEL_NUM", "\n", "save_dict", "[", "'model_agents'", "]", "=", "MODEL_AGENTS", "\n", "save_dict", "[", "'reward_list'", "]", "=", "reward_list", "\n", "save_dict", "[", "'episode_length_list'", "]", "=", "episode_length_list", "\n", "torch", ".", "save", "(", "save_dict", ",", "file_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_transfer_vary_hp.eval_models": [[178, 193], ["GTNC_evaluate_gridworld_transfer_vary_hp.get_best_models_from_log", "GTNC_evaluate_gridworld_transfer_vary_hp.save_list", "print", "print", "GTNC_evaluate_gridworld_transfer_vary_hp.load_envs_and_config", "GTNC_evaluate_gridworld_transfer_vary_hp.train_test_agents", "str", "str"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.get_best_models_from_log", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.save_list", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.load_envs_and_config", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.train_test_agents"], ["", "def", "eval_models", "(", "mode", ",", "log_dir", ")", ":", "\n", "    ", "best_models", "=", "get_best_models_from_log", "(", "log_dir", ")", "\n", "\n", "reward_list", "=", "[", "]", "\n", "episode_length_list", "=", "[", "]", "\n", "\n", "for", "best_reward", ",", "model_file", "in", "best_models", ":", "\n", "        ", "print", "(", "'best reward: '", "+", "str", "(", "best_reward", ")", ")", "\n", "print", "(", "'model file: '", "+", "str", "(", "model_file", ")", ")", "\n", "reward_env", ",", "real_env", ",", "config", "=", "load_envs_and_config", "(", "model_file", ")", "\n", "rewards", ",", "episode_lengths", "=", "train_test_agents", "(", "mode", "=", "mode", ",", "env", "=", "reward_env", ",", "real_env", "=", "real_env", ",", "config", "=", "config", ")", "\n", "reward_list", "+=", "rewards", "\n", "episode_length_list", "+=", "episode_lengths", "\n", "\n", "", "save_list", "(", "mode", ",", "config", ",", "reward_list", ",", "episode_length_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_transfer_vary_hp.eval_base": [[195, 214], ["GTNC_evaluate_gridworld_transfer_vary_hp.get_best_models_from_log", "range", "GTNC_evaluate_gridworld_transfer_vary_hp.save_list", "GTNC_evaluate_gridworld_transfer_vary_hp.load_envs_and_config", "GTNC_evaluate_gridworld_transfer_vary_hp.train_test_agents"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.get_best_models_from_log", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.save_list", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.load_envs_and_config", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.train_test_agents"], ["", "def", "eval_base", "(", "mode", ",", "log_dir", ")", ":", "\n", "    ", "best_models", "=", "get_best_models_from_log", "(", "log_dir", ")", "\n", "\n", "reward_list", "=", "[", "]", "\n", "episode_length_list", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "MODEL_NUM", ")", ":", "\n", "# if not os.path.isdir(best_models[0][1]):", "\n", "#     model_file = best_models[0][1].replace('/home/dingsda/master_thesis/learning_environments/results',", "\n", "#                                            '/home/ferreira/Projects/learning_environments/results/thomas_results')", "\n", "# else:", "\n", "#     model_file = best_models[0][1]", "\n", "\n", "        ", "reward_env", ",", "real_env", ",", "config", "=", "load_envs_and_config", "(", "best_models", "[", "0", "]", "[", "1", "]", ")", "\n", "rewards", ",", "episode_lengths", "=", "train_test_agents", "(", "mode", "=", "mode", ",", "env", "=", "real_env", ",", "real_env", "=", "real_env", ",", "config", "=", "config", ")", "\n", "reward_list", "+=", "rewards", "\n", "episode_length_list", "+=", "episode_lengths", "\n", "\n", "", "save_list", "(", "mode", ",", "config", ",", "reward_list", ",", "episode_length_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_different_sizes.ExperimentWrapper.get_bohb_parameters": [[17, 26], ["None"], "methods", ["None"], ["    ", "def", "get_bohb_parameters", "(", "self", ")", ":", "\n", "        ", "params", "=", "{", "}", "\n", "params", "[", "'min_budget'", "]", "=", "1", "\n", "params", "[", "'max_budget'", "]", "=", "1", "\n", "params", "[", "'eta'", "]", "=", "2", "\n", "params", "[", "'random_fraction'", "]", "=", "1", "\n", "params", "[", "'iterations'", "]", "=", "150", "\n", "\n", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_different_sizes.ExperimentWrapper.get_configspace": [[27, 31], ["ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace"], "methods", ["None"], ["", "def", "get_configspace", "(", "self", ")", ":", "\n", "        ", "cs", "=", "CS", ".", "ConfigurationSpace", "(", ")", "\n", "\n", "return", "cs", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_different_sizes.ExperimentWrapper.get_specific_config": [[32, 38], ["copy.deepcopy"], "methods", ["None"], ["", "def", "get_specific_config", "(", "self", ",", "cso", ",", "default_config", ",", "budget", ")", ":", "\n", "        ", "config", "=", "deepcopy", "(", "default_config", ")", "\n", "\n", "config", "[", "'agents'", "]", "[", "'gtn'", "]", "[", "'quit_when_solved'", "]", "=", "False", "\n", "\n", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_different_sizes.ExperimentWrapper.compute": [[39, 76], ["GTNC_evaluate_gridworld_different_sizes.ExperimentWrapper.get_specific_config", "print", "print", "print", "print", "print", "print", "str", "str", "print", "print", "print", "print", "print", "open", "yaml.safe_load", "agents.GTN.GTN_Master", "agents.GTN.GTN_Master.run", "len", "str", "str", "str", "float", "traceback.format_exc", "print", "str", "str"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_step_size.ExperimentWrapper.get_specific_config", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.run"], ["", "def", "compute", "(", "self", ",", "working_dir", ",", "bohb_id", ",", "config_id", ",", "cso", ",", "budget", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "with", "open", "(", "\"default_config_gridworld.yaml\"", ",", "'r'", ")", "as", "stream", ":", "\n", "            ", "default_config", "=", "yaml", ".", "safe_load", "(", "stream", ")", "\n", "\n", "", "config", "=", "self", ".", "get_specific_config", "(", "cso", ",", "default_config", ",", "budget", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "\"START BOHB ITERATION\"", ")", "\n", "print", "(", "'CONFIG: '", "+", "str", "(", "config", ")", ")", "\n", "print", "(", "'CSO:    '", "+", "str", "(", "cso", ")", ")", "\n", "print", "(", "'BUDGET: '", "+", "str", "(", "budget", ")", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "try", ":", "\n", "            ", "gtn", "=", "GTN_Master", "(", "config", ",", "bohb_id", "=", "bohb_id", ",", "bohb_working_dir", "=", "working_dir", ")", "\n", "_", ",", "score_list", "=", "gtn", ".", "run", "(", ")", "\n", "score", "=", "len", "(", "score_list", ")", "\n", "error", "=", "\"\"", "\n", "", "except", ":", "\n", "            ", "score", "=", "float", "(", "'Inf'", ")", "\n", "score_list", "=", "[", "]", "\n", "error", "=", "traceback", ".", "format_exc", "(", ")", "\n", "print", "(", "error", ")", "\n", "\n", "", "info", "=", "{", "}", "\n", "info", "[", "'error'", "]", "=", "str", "(", "error", ")", "\n", "info", "[", "'score_list'", "]", "=", "str", "(", "score_list", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "'FINAL SCORE: '", "+", "str", "(", "score", ")", ")", "\n", "print", "(", "'SCORE LIST:  '", "+", "str", "(", "score_list", ")", ")", "\n", "print", "(", "\"END BOHB ITERATION\"", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "return", "{", "\n", "\"loss\"", ":", "score", ",", "\n", "\"info\"", ":", "info", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cmc_se_params_v2.ExperimentWrapper.get_bohb_parameters": [[19, 28], ["None"], "methods", ["None"], ["    ", "def", "get_bohb_parameters", "(", "self", ")", ":", "\n", "        ", "params", "=", "{", "}", "\n", "params", "[", "'min_budget'", "]", "=", "1", "\n", "params", "[", "'max_budget'", "]", "=", "3", "\n", "params", "[", "'eta'", "]", "=", "3", "\n", "params", "[", "'random_fraction'", "]", "=", "0.3", "\n", "params", "[", "'iterations'", "]", "=", "10000", "\n", "\n", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cmc_se_params_v2.ExperimentWrapper.get_configspace": [[29, 57], ["ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter"], "methods", ["None"], ["", "def", "get_configspace", "(", "self", ")", ":", "\n", "        ", "cs", "=", "CS", ".", "ConfigurationSpace", "(", ")", "\n", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'gtn_score_transform_type'", ",", "lower", "=", "0", ",", "upper", "=", "7", ",", "log", "=", "False", ",", "default_value", "=", "7", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'gtn_step_size'", ",", "lower", "=", "0.1", ",", "upper", "=", "1", ",", "log", "=", "True", ",", "default_value", "=", "0.5", ")", ")", "\n", "# cs.add_hyperparameter(CSH.CategoricalHyperparameter(name='gtn_mirrored_sampling', choices=[False, True], default_value=True))", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'gtn_noise_std'", ",", "lower", "=", "0.001", ",", "upper", "=", "1", ",", "log", "=", "True", ",", "default_value", "=", "0.01", ")", ")", "\n", "\n", "# cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(name='td3_init_episodes', lower=1, upper=100, log=True, default_value=20))", "\n", "# cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(name='td3_batch_size', lower=32, upper=256, log=False, default_value=128))", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_gamma'", ",", "lower", "=", "0.001", ",", "upper", "=", "0.1", ",", "log", "=", "True", ",", "default_value", "=", "0.01", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_lr'", ",", "lower", "=", "1e-5", ",", "upper", "=", "5e-2", ",", "log", "=", "True", ",", "default_value", "=", "1e-3", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_tau'", ",", "lower", "=", "0.005", ",", "upper", "=", "0.05", ",", "log", "=", "True", ",", "default_value", "=", "0.01", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'td3_policy_delay'", ",", "lower", "=", "1", ",", "upper", "=", "3", ",", "log", "=", "False", ",", "default_value", "=", "2", ")", ")", "\n", "# cs.add_hyperparameter(CSH.CategoricalHyperparameter(name='td3_activation_fn', choices=['tanh', 'relu', 'leakyrelu', 'prelu'], default_value='relu'))", "\n", "# cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(name='td3_hidden_size', lower=48, upper=192, log=True, default_value=128))", "\n", "# cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(name='td3_hidden_layer', lower=1, upper=2, log=False, default_value=2))", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'td3_same_action_num'", ",", "lower", "=", "2", ",", "upper", "=", "3", ",", "log", "=", "False", ",", "default_value", "=", "2", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_action_std'", ",", "lower", "=", "0.05", ",", "upper", "=", "0.6", ",", "log", "=", "True", ",", "default_value", "=", "0.1", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_policy_std'", ",", "lower", "=", "0.1", ",", "upper", "=", "0.6", ",", "log", "=", "True", ",", "default_value", "=", "0.2", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_policy_std_clip'", ",", "lower", "=", "0.25", ",", "upper", "=", "1", ",", "log", "=", "True", ",", "default_value", "=", "0.5", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'td3_early_out_virtual_diff'", ",", "lower", "=", "1e-2", ",", "upper", "=", "1e-1", ",", "log", "=", "True", ",", "default_value", "=", "3e-2", ")", ")", "\n", "\n", "# cs.add_hyperparameter(CSH.CategoricalHyperparameter(name='cmc_activation_fn', choices=['tanh', 'relu', 'leakyrelu', 'prelu'], default_value='leakyrelu'))", "\n", "# cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(name='cmc_hidden_size', lower=48, upper=192, log=True, default_value=128))", "\n", "# cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(name='cmc_hidden_layer', lower=1, upper=3, log=False, default_value=1))", "\n", "\n", "return", "cs", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cmc_se_params_v2.ExperimentWrapper.get_specific_config": [[58, 89], ["copy.deepcopy"], "methods", ["None"], ["", "def", "get_specific_config", "(", "self", ",", "cso", ",", "default_config", ",", "budget", ")", ":", "\n", "        ", "config", "=", "deepcopy", "(", "default_config", ")", "\n", "\n", "config", "[", "\"agents\"", "]", "[", "'gtn'", "]", "[", "'score_transform_type'", "]", "=", "cso", "[", "\"gtn_score_transform_type\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'gtn'", "]", "[", "'step_size'", "]", "=", "cso", "[", "\"gtn_step_size\"", "]", "\n", "# config[\"agents\"]['gtn']['mirrored_sampling'] = cso[\"gtn_mirrored_sampling\"]", "\n", "config", "[", "\"agents\"", "]", "[", "'gtn'", "]", "[", "'noise_std'", "]", "=", "cso", "[", "\"gtn_noise_std\"", "]", "\n", "\n", "# config[\"agents\"]['td3']['init_episodes'] = cso[\"td3_init_episodes\"]", "\n", "# config[\"agents\"]['td3']['batch_size'] = cso[\"td3_batch_size\"]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'gamma'", "]", "=", "1", "-", "cso", "[", "\"td3_gamma\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'lr'", "]", "=", "cso", "[", "\"td3_lr\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'tau'", "]", "=", "cso", "[", "\"td3_tau\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'policy_delay'", "]", "=", "cso", "[", "\"td3_policy_delay\"", "]", "\n", "# config[\"agents\"]['td3']['activation_fn'] = cso[\"td3_activation_fn\"]", "\n", "# config[\"agents\"]['td3']['hidden_size'] = cso[\"td3_hidden_size\"]", "\n", "# config[\"agents\"]['td3']['hidden_layer'] = cso[\"td3_hidden_layer\"]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'same_action_num'", "]", "=", "cso", "[", "\"td3_same_action_num\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'action_std'", "]", "=", "cso", "[", "\"td3_action_std\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'policy_std'", "]", "=", "cso", "[", "\"td3_policy_std\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'policy_std_clip'", "]", "=", "cso", "[", "\"td3_policy_std_clip\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'td3'", "]", "[", "'early_out_virtual_diff'", "]", "=", "cso", "[", "\"td3_early_out_virtual_diff\"", "]", "\n", "\n", "# config[\"envs\"]['MountainCarContinuous-v0']['activation_fn'] = cso[\"cmc_activation_fn\"]", "\n", "# config[\"envs\"]['MountainCarContinuous-v0']['hidden_size'] = cso[\"cmc_hidden_size\"]", "\n", "# config[\"envs\"]['MountainCarContinuous-v0']['hidden_layer'] = cso[\"cmc_hidden_layer\"]", "\n", "\n", "global", "reward_env_type", "\n", "config", "[", "\"agents\"", "]", "[", "'gtn'", "]", "[", "'synthetic_env_type'", "]", "=", "0", "\n", "\n", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cmc_se_params_v2.ExperimentWrapper.compute": [[90, 132], ["GTNC_evaluate_cmc_se_params_v2.ExperimentWrapper.get_specific_config", "print", "print", "print", "print", "print", "print", "str", "str", "str", "str", "print", "print", "print", "print", "print", "open", "yaml.safe_load", "agents.GTN.GTN_Master", "agents.GTN.GTN_Master.run", "math.isnan", "str", "str", "str", "float", "float", "traceback.format_exc", "print", "str", "str", "sorted"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_step_size.ExperimentWrapper.get_specific_config", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.run"], ["", "def", "compute", "(", "self", ",", "working_dir", ",", "bohb_id", ",", "config_id", ",", "cso", ",", "budget", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "with", "open", "(", "\"default_config_cmc_syn_env_opt.yaml\"", ",", "'r'", ")", "as", "stream", ":", "\n", "            ", "default_config", "=", "yaml", ".", "safe_load", "(", "stream", ")", "\n", "\n", "", "config", "=", "self", ".", "get_specific_config", "(", "cso", ",", "default_config", ",", "budget", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "\"START BOHB ITERATION\"", ")", "\n", "print", "(", "'CONFIG: '", "+", "str", "(", "config", ")", ")", "\n", "print", "(", "'CSO:    '", "+", "str", "(", "cso", ")", ")", "\n", "print", "(", "'BUDGET: '", "+", "str", "(", "budget", ")", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "try", ":", "\n", "            ", "gtn", "=", "GTN_Master", "(", "config", ",", "bohb_id", "=", "bohb_id", ",", "bohb_working_dir", "=", "working_dir", ")", "\n", "_", ",", "score_list", ",", "model_name", "=", "gtn", ".", "run", "(", ")", "\n", "score", "=", "-", "sorted", "(", "score_list", ")", "[", "-", "1", "]", "\n", "if", "math", ".", "isnan", "(", "score", ")", ":", "\n", "                ", "score", "=", "float", "(", "'Inf'", ")", "\n", "", "error", "=", "\"\"", "\n", "", "except", ":", "\n", "            ", "score", "=", "float", "(", "'Inf'", ")", "\n", "score_list", "=", "[", "]", "\n", "model_name", "=", "None", "\n", "error", "=", "traceback", ".", "format_exc", "(", ")", "\n", "print", "(", "error", ")", "\n", "\n", "", "info", "=", "{", "}", "\n", "info", "[", "'error'", "]", "=", "str", "(", "error", ")", "\n", "info", "[", "'config'", "]", "=", "str", "(", "config", ")", "\n", "info", "[", "'score_list'", "]", "=", "str", "(", "score_list", ")", "\n", "info", "[", "'model_name'", "]", "=", "str", "(", "model_name", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "'FINAL SCORE: '", "+", "str", "(", "score", ")", ")", "\n", "print", "(", "'SCORE LIST:  '", "+", "str", "(", "score_list", ")", ")", "\n", "print", "(", "\"END BOHB ITERATION\"", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "return", "{", "\n", "\"loss\"", ":", "score", ",", "\n", "\"info\"", ":", "info", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_halfcheetah_transfer_algo.get_best_models_from_log": [[40, 69], ["hpbandster.logged_results_to_HBS_result", "hpres.logged_results_to_HBS_result.data.values", "print", "best_models.sort", "os.path.isdir", "log_dir.replace.replace", "best_models.append", "os.path.isfile", "model_name.replace.replace"], "function", ["None"], ["def", "get_best_models_from_log", "(", "log_dir", ")", ":", "\n", "    ", "if", "not", "os", ".", "path", ".", "isdir", "(", "log_dir", ")", ":", "\n", "        ", "log_dir", "=", "log_dir", ".", "replace", "(", "'nierhoff'", ",", "'dingsda'", ")", "\n", "\n", "", "result", "=", "hpres", ".", "logged_results_to_HBS_result", "(", "log_dir", ")", "\n", "\n", "best_models", "=", "[", "]", "\n", "\n", "for", "value", "in", "result", ".", "data", ".", "values", "(", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "loss", "=", "value", ".", "results", "[", "1.0", "]", "[", "'loss'", "]", "\n", "model_name", "=", "value", ".", "results", "[", "1.0", "]", "[", "'info'", "]", "[", "'model_name'", "]", "\n", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "model_name", ")", ":", "\n", "                ", "model_name", "=", "model_name", ".", "replace", "(", "'nierhoff'", ",", "'dingsda'", ")", "\n", "", "best_models", ".", "append", "(", "(", "loss", ",", "model_name", ")", ")", "\n", "", "except", ":", "\n", "            ", "continue", "\n", "\n", "# before AUC (objective minimized)", "\n", "# print(\"sorting from low to high values (non-AUC)\")", "\n", "# best_models.sort(key=lambda x: x[0])", "\n", "\n", "# AUC (objective maximized)", "\n", "", "", "print", "(", "\"sorting from high to low values (AUC)\"", ")", "\n", "best_models", ".", "sort", "(", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ",", "reverse", "=", "True", ")", "\n", "best_models", "=", "best_models", "[", ":", "MODEL_NUM", "]", "\n", "\n", "return", "best_models", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_halfcheetah_transfer_algo.load_envs_and_config": [[71, 84], ["torch.load", "envs.env_factory.EnvFactory", "envs.env_factory.EnvFactory.generate_reward_env", "env_factory.generate_reward_env.load_state_dict", "envs.env_factory.EnvFactory.generate_real_env"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_reward_env", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_real_env"], ["", "def", "load_envs_and_config", "(", "model_file", ")", ":", "\n", "    ", "save_dict", "=", "torch", ".", "load", "(", "model_file", ")", "\n", "\n", "config", "=", "save_dict", "[", "'config'", "]", "\n", "config", "[", "'device'", "]", "=", "'cpu'", "\n", "config", "[", "'envs'", "]", "[", "'HalfCheetah-v3'", "]", "[", "'solved_reward'", "]", "=", "100000", "# something big enough to prevent early out triggering", "\n", "\n", "env_factory", "=", "EnvFactory", "(", "config", "=", "config", ")", "\n", "reward_env", "=", "env_factory", ".", "generate_reward_env", "(", ")", "\n", "reward_env", ".", "load_state_dict", "(", "save_dict", "[", "'model'", "]", ")", "\n", "real_env", "=", "env_factory", ".", "generate_real_env", "(", ")", "\n", "\n", "return", "reward_env", ",", "real_env", ",", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_halfcheetah_transfer_algo.train_test_agents": [[86, 141], ["range", "agents.agent_utils.select_agent.train", "print", "rewards.append", "episode_lengths.append", "agents.agent_utils.select_agent", "agents.agent_utils.select_agent", "str"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.train", "home.repos.pwc.inspect_result.automl_learning_environments.agents.agent_utils.select_agent", "home.repos.pwc.inspect_result.automl_learning_environments.agents.agent_utils.select_agent"], ["", "def", "train_test_agents", "(", "mode", ",", "env", ",", "real_env", ",", "config", ")", ":", "\n", "    ", "rewards", "=", "[", "]", "\n", "episode_lengths", "=", "[", "]", "\n", "\n", "# settings for comparability", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "=", "{", "}", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'test_episodes'", "]", "=", "1", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'train_episodes'", "]", "=", "5000", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'print_rate'", "]", "=", "100", "\n", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'init_episodes'", "]", "=", "0", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'update_episodes'", "]", "=", "1", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'ppo_epochs'", "]", "=", "10", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'gamma'", "]", "=", "0.99", "\n", "# config['agents']['ppo']['lr'] = 3e-4", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'lr'", "]", "=", "1e-5", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'vf_coef'", "]", "=", "1", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'ent_coef'", "]", "=", "0.001", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'eps_clip'", "]", "=", "0.2", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'rb_size'", "]", "=", "1000000", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'same_action_num'", "]", "=", "1", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'activation_fn'", "]", "=", "'tanh'", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'hidden_size'", "]", "=", "128", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'hidden_layer'", "]", "=", "2", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'action_std'", "]", "=", "0.1", "\n", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'early_out_num'", "]", "=", "50", "\n", "config", "[", "'agents'", "]", "[", "'ppo'", "]", "[", "'early_out_virtual_diff'", "]", "=", "0.02", "\n", "\n", "# BOHB optimized HPs", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "=", "{", "}", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "[", "'beta'", "]", "=", "0.05", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "[", "'eta'", "]", "=", "0.03", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "[", "'feature_dim'", "]", "=", "32", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "[", "'hidden_size'", "]", "=", "128", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "[", "'lr'", "]", "=", "1e-4", "\n", "\n", "# default ICM HPs:", "\n", "# config['agents']['icm'] = {}", "\n", "# config['agents']['icm']['beta'] = 0.2", "\n", "# config['agents']['icm']['eta'] = 0.5", "\n", "# config['agents']['icm']['feature_dim'] = 64", "\n", "# config['agents']['icm']['hidden_size'] = 128", "\n", "# config['agents']['icm']['lr'] = 1e-4", "\n", "\n", "for", "i", "in", "range", "(", "MODEL_AGENTS", ")", ":", "\n", "        ", "if", "mode", "==", "'-1'", ":", "\n", "            ", "agent", "=", "select_agent", "(", "config", "=", "config", ",", "agent_name", "=", "'ppo_icm'", ")", "\n", "", "else", ":", "\n", "            ", "agent", "=", "select_agent", "(", "config", "=", "config", ",", "agent_name", "=", "'ppo'", ")", "\n", "", "reward", ",", "episode_length", ",", "_", "=", "agent", ".", "train", "(", "env", "=", "env", ",", "test_env", "=", "real_env", ")", "\n", "print", "(", "'reward: '", "+", "str", "(", "reward", ")", ")", "\n", "rewards", ".", "append", "(", "reward", ")", "\n", "episode_lengths", ".", "append", "(", "episode_length", ")", "\n", "", "return", "rewards", ",", "episode_lengths", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_halfcheetah_transfer_algo.save_list": [[143, 154], ["os.makedirs", "os.path.join", "print", "torch.save", "str"], "function", ["None"], ["", "def", "save_list", "(", "mode", ",", "config", ",", "reward_list", ",", "episode_length_list", ")", ":", "\n", "    ", "os", ".", "makedirs", "(", "SAVE_DIR", ",", "exist_ok", "=", "True", ")", "\n", "file_name", "=", "os", ".", "path", ".", "join", "(", "SAVE_DIR", ",", "'best_transfer_algo'", "+", "str", "(", "mode", ")", "+", "'.pt'", ")", "\n", "save_dict", "=", "{", "}", "\n", "save_dict", "[", "'config'", "]", "=", "config", "\n", "save_dict", "[", "'model_num'", "]", "=", "MODEL_NUM", "\n", "save_dict", "[", "'model_agents'", "]", "=", "MODEL_AGENTS", "\n", "save_dict", "[", "'reward_list'", "]", "=", "reward_list", "\n", "save_dict", "[", "'episode_length_list'", "]", "=", "episode_length_list", "\n", "print", "(", "f\"saved dict under {file_name}\"", ")", "\n", "torch", ".", "save", "(", "save_dict", ",", "file_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_halfcheetah_transfer_algo.eval_models": [[156, 171], ["GTNC_evaluate_halfcheetah_transfer_algo.get_best_models_from_log", "GTNC_evaluate_halfcheetah_transfer_algo.save_list", "print", "print", "GTNC_evaluate_halfcheetah_transfer_algo.load_envs_and_config", "GTNC_evaluate_halfcheetah_transfer_algo.train_test_agents", "str", "str"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.get_best_models_from_log", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.save_list", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.load_envs_and_config", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.train_test_agents"], ["", "def", "eval_models", "(", "mode", ",", "log_dir", ")", ":", "\n", "    ", "best_models", "=", "get_best_models_from_log", "(", "log_dir", ")", "\n", "\n", "reward_list", "=", "[", "]", "\n", "episode_length_list", "=", "[", "]", "\n", "\n", "for", "best_reward", ",", "model_file", "in", "best_models", ":", "\n", "        ", "print", "(", "'best reward: '", "+", "str", "(", "best_reward", ")", ")", "\n", "print", "(", "'model file: '", "+", "str", "(", "model_file", ")", ")", "\n", "reward_env", ",", "real_env", ",", "config", "=", "load_envs_and_config", "(", "model_file", ")", "\n", "rewards", ",", "episode_lengths", "=", "train_test_agents", "(", "mode", "=", "mode", ",", "env", "=", "reward_env", ",", "real_env", "=", "real_env", ",", "config", "=", "config", ")", "\n", "reward_list", "+=", "rewards", "\n", "episode_length_list", "+=", "episode_lengths", "\n", "\n", "", "save_list", "(", "mode", ",", "config", ",", "reward_list", ",", "episode_length_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_halfcheetah_transfer_algo.eval_base": [[173, 186], ["GTNC_evaluate_halfcheetah_transfer_algo.get_best_models_from_log", "range", "GTNC_evaluate_halfcheetah_transfer_algo.save_list", "GTNC_evaluate_halfcheetah_transfer_algo.load_envs_and_config", "GTNC_evaluate_halfcheetah_transfer_algo.train_test_agents"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.get_best_models_from_log", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.save_list", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.load_envs_and_config", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.train_test_agents"], ["", "def", "eval_base", "(", "mode", ",", "log_dir", ")", ":", "\n", "    ", "best_models", "=", "get_best_models_from_log", "(", "log_dir", ")", "\n", "\n", "reward_list", "=", "[", "]", "\n", "episode_length_list", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "MODEL_NUM", ")", ":", "\n", "        ", "reward_env", ",", "real_env", ",", "config", "=", "load_envs_and_config", "(", "best_models", "[", "0", "]", "[", "1", "]", ")", "\n", "rewards", ",", "episode_lengths", "=", "train_test_agents", "(", "mode", "=", "mode", ",", "env", "=", "real_env", ",", "real_env", "=", "real_env", ",", "config", "=", "config", ")", "\n", "reward_list", "+=", "rewards", "\n", "episode_length_list", "+=", "episode_lengths", "\n", "\n", "", "save_list", "(", "mode", ",", "config", ",", "reward_list", ",", "episode_length_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_halfcheetah_transfer_algo.eval_icm": [[188, 201], ["GTNC_evaluate_halfcheetah_transfer_algo.get_best_models_from_log", "range", "GTNC_evaluate_halfcheetah_transfer_algo.save_list", "GTNC_evaluate_halfcheetah_transfer_algo.load_envs_and_config", "GTNC_evaluate_halfcheetah_transfer_algo.train_test_agents"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.get_best_models_from_log", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.save_list", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.load_envs_and_config", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.train_test_agents"], ["", "def", "eval_icm", "(", "mode", ",", "log_dir", ")", ":", "\n", "    ", "best_models", "=", "get_best_models_from_log", "(", "log_dir", ")", "\n", "\n", "reward_list", "=", "[", "]", "\n", "episode_length_list", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "MODEL_NUM", ")", ":", "\n", "        ", "reward_env", ",", "real_env", ",", "config", "=", "load_envs_and_config", "(", "best_models", "[", "0", "]", "[", "1", "]", ")", "\n", "rewards", ",", "episode_lengths", "=", "train_test_agents", "(", "mode", "=", "mode", ",", "env", "=", "real_env", ",", "real_env", "=", "real_env", ",", "config", "=", "config", ")", "\n", "reward_list", "+=", "rewards", "\n", "episode_length_list", "+=", "episode_lengths", "\n", "\n", "", "save_list", "(", "mode", ",", "config", ",", "reward_list", ",", "episode_length_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_single_pc.ExperimentWrapper.get_bohb_parameters": [[17, 26], ["None"], "methods", ["None"], ["    ", "def", "get_bohb_parameters", "(", "self", ")", ":", "\n", "        ", "params", "=", "{", "}", "\n", "params", "[", "'min_budget'", "]", "=", "1", "\n", "params", "[", "'max_budget'", "]", "=", "1", "\n", "params", "[", "'eta'", "]", "=", "2", "\n", "params", "[", "'random_fraction'", "]", "=", "1", "\n", "params", "[", "'iterations'", "]", "=", "100", "\n", "\n", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_single_pc.ExperimentWrapper.get_configspace": [[27, 31], ["ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace"], "methods", ["None"], ["", "def", "get_configspace", "(", "self", ")", ":", "\n", "        ", "cs", "=", "CS", ".", "ConfigurationSpace", "(", ")", "\n", "\n", "return", "cs", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_single_pc.ExperimentWrapper.get_specific_config": [[32, 38], ["copy.deepcopy"], "methods", ["None"], ["", "def", "get_specific_config", "(", "self", ",", "cso", ",", "default_config", ",", "budget", ")", ":", "\n", "        ", "config", "=", "deepcopy", "(", "default_config", ")", "\n", "\n", "config", "[", "'agents'", "]", "[", "'gtn'", "]", "[", "'mode'", "]", "=", "'single'", "\n", "\n", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_single_pc.ExperimentWrapper.compute": [[39, 80], ["GTNC_evaluate_gridworld_single_pc.ExperimentWrapper.get_specific_config", "print", "print", "print", "print", "print", "print", "str", "str", "str", "str", "print", "print", "print", "print", "print", "open", "yaml.safe_load", "agents.GTN.GTN_Master", "agents.GTN.GTN_Master.clean_working_dir", "agents.GTN.GTN_Master.run", "str", "str", "str", "float", "traceback.format_exc", "print", "str", "str", "sorted"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_step_size.ExperimentWrapper.get_specific_config", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_base.GTN_Base.clean_working_dir", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.run"], ["", "def", "compute", "(", "self", ",", "working_dir", ",", "bohb_id", ",", "config_id", ",", "cso", ",", "budget", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "with", "open", "(", "\"default_config_gridworld.yaml\"", ",", "'r'", ")", "as", "stream", ":", "\n", "            ", "default_config", "=", "yaml", ".", "safe_load", "(", "stream", ")", "\n", "\n", "", "config", "=", "self", ".", "get_specific_config", "(", "cso", ",", "default_config", ",", "budget", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "\"START BOHB ITERATION\"", ")", "\n", "print", "(", "'CONFIG: '", "+", "str", "(", "config", ")", ")", "\n", "print", "(", "'CSO:    '", "+", "str", "(", "cso", ")", ")", "\n", "print", "(", "'BUDGET: '", "+", "str", "(", "budget", ")", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "try", ":", "\n", "            ", "gtn", "=", "GTN_Master", "(", "config", ",", "bohb_id", "=", "0", ",", "bohb_working_dir", "=", "working_dir", ")", "\n", "gtn", ".", "clean_working_dir", "(", ")", "\n", "_", ",", "score_list", ",", "model_name", "=", "gtn", ".", "run", "(", ")", "\n", "score", "=", "-", "sorted", "(", "score_list", ")", "[", "-", "1", "]", "\n", "error", "=", "\"\"", "\n", "", "except", ":", "\n", "            ", "score", "=", "float", "(", "'Inf'", ")", "\n", "score_list", "=", "[", "]", "\n", "model_name", "=", "None", "\n", "error", "=", "traceback", ".", "format_exc", "(", ")", "\n", "print", "(", "error", ")", "\n", "\n", "", "info", "=", "{", "}", "\n", "info", "[", "'error'", "]", "=", "str", "(", "error", ")", "\n", "info", "[", "'config'", "]", "=", "str", "(", "config", ")", "\n", "info", "[", "'score_list'", "]", "=", "str", "(", "score_list", ")", "\n", "info", "[", "'model_name'", "]", "=", "str", "(", "model_name", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "'FINAL SCORE: '", "+", "str", "(", "score", ")", ")", "\n", "print", "(", "'SCORE LIST:  '", "+", "str", "(", "score_list", ")", ")", "\n", "print", "(", "\"END BOHB ITERATION\"", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "return", "{", "\n", "\"loss\"", ":", "score", ",", "\n", "\"info\"", ":", "info", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_acrobot_vary_hp_2.load_envs_and_config": [[12, 23], ["os.path.join", "torch.load", "envs.env_factory.EnvFactory", "envs.env_factory.EnvFactory.generate_virtual_env", "env_factory.generate_virtual_env.load_state_dict", "envs.env_factory.EnvFactory.generate_real_env"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_virtual_env", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_real_env"], ["def", "load_envs_and_config", "(", "file_name", ",", "model_dir", ",", "device", ")", ":", "\n", "    ", "file_path", "=", "os", ".", "path", ".", "join", "(", "model_dir", ",", "file_name", ")", "\n", "save_dict", "=", "torch", ".", "load", "(", "file_path", ")", "\n", "config", "=", "save_dict", "[", "'config'", "]", "\n", "config", "[", "'device'", "]", "=", "device", "\n", "env_factory", "=", "EnvFactory", "(", "config", "=", "config", ")", "\n", "virtual_env", "=", "env_factory", ".", "generate_virtual_env", "(", ")", "\n", "virtual_env", ".", "load_state_dict", "(", "save_dict", "[", "'model'", "]", ")", "\n", "real_env", "=", "env_factory", ".", "generate_real_env", "(", ")", "\n", "\n", "return", "virtual_env", ",", "real_env", ",", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_acrobot_vary_hp_2.train_test_agents": [[25, 49], ["range", "agents.agent_utils.select_agent", "agents.agent_utils.select_agent.train", "agents.agent_utils.select_agent.test", "print", "reward_list.append", "train_steps_needed.append", "episodes_needed.append", "str", "sum", "len"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.agents.agent_utils.select_agent", "home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.train", "home.repos.pwc.inspect_result.automl_learning_environments.agents.base_agent.BaseAgent.test"], ["", "def", "train_test_agents", "(", "train_env", ",", "test_env", ",", "config", ",", "agents_num", ")", ":", "\n", "    ", "reward_list", "=", "[", "]", "\n", "train_steps_needed", "=", "[", "]", "\n", "episodes_needed", "=", "[", "]", "\n", "\n", "# settings for comparability", "\n", "config", "[", "'agents'", "]", "[", "'ddqn_vary'", "]", "[", "'vary_hp'", "]", "=", "True", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'print_rate'", "]", "=", "10", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'early_out_num'", "]", "=", "10", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'train_episodes'", "]", "=", "1000", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'init_episodes'", "]", "=", "10", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'test_episodes'", "]", "=", "10", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'early_out_virtual_diff'", "]", "=", "0.01", "\n", "\n", "for", "i", "in", "range", "(", "agents_num", ")", ":", "\n", "        ", "agent", "=", "select_agent", "(", "config", "=", "config", ",", "agent_name", "=", "'DDQN_vary'", ")", "\n", "reward_train", ",", "episode_length", ",", "_", "=", "agent", ".", "train", "(", "env", "=", "train_env", ")", "\n", "reward", ",", "_", ",", "_", "=", "agent", ".", "test", "(", "env", "=", "test_env", ")", "\n", "print", "(", "'reward: '", "+", "str", "(", "reward", ")", ")", "\n", "reward_list", ".", "append", "(", "reward", ")", "\n", "train_steps_needed", ".", "append", "(", "[", "sum", "(", "episode_length", ")", "]", ")", "\n", "episodes_needed", ".", "append", "(", "[", "(", "len", "(", "reward_train", ")", ")", "]", ")", "\n", "\n", "", "return", "reward_list", ",", "train_steps_needed", ",", "episodes_needed", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2_DuelingDDQN_correlation.load_envs_and_config": [[13, 31], ["os.path.join", "torch.load", "envs.env_factory.EnvFactory", "envs.env_factory.EnvFactory.generate_virtual_env", "env_factory.generate_virtual_env.load_state_dict", "envs.env_factory.EnvFactory.generate_real_env", "open", "yaml.safe_load"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_virtual_env", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_real_env"], ["def", "load_envs_and_config", "(", "file_name", ",", "model_dir", ",", "device", ")", ":", "\n", "    ", "file_path", "=", "os", ".", "path", ".", "join", "(", "model_dir", ",", "file_name", ")", "\n", "save_dict", "=", "torch", ".", "load", "(", "file_path", ")", "\n", "config", "=", "save_dict", "[", "'config'", "]", "\n", "config", "[", "'device'", "]", "=", "device", "\n", "env_factory", "=", "EnvFactory", "(", "config", "=", "config", ")", "\n", "virtual_env", "=", "env_factory", ".", "generate_virtual_env", "(", ")", "\n", "virtual_env", ".", "load_state_dict", "(", "save_dict", "[", "'model'", "]", ")", "\n", "real_env", "=", "env_factory", ".", "generate_real_env", "(", ")", "\n", "\n", "# load additional agent configs", "\n", "with", "open", "(", "\"default_config_cartpole.yaml\"", ",", "\"r\"", ")", "as", "stream", ":", "\n", "        ", "config_new", "=", "yaml", ".", "safe_load", "(", "stream", ")", "[", "\"agents\"", "]", "\n", "\n", "", "config", "[", "\"agents\"", "]", "[", "\"duelingddqn\"", "]", "=", "config_new", "[", "\"duelingddqn\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"duelingddqn_vary\"", "]", "=", "config_new", "[", "\"duelingddqn_vary\"", "]", "\n", "\n", "return", "virtual_env", ",", "real_env", ",", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2_DuelingDDQN_correlation.train_test_agents": [[33, 96], ["range", "agents.agent_utils.select_agent", "range", "agents.agent_utils.select_agent", "print", "agents.agent_utils.select_agent.train", "agents.agent_utils.select_agent.test", "print", "reward_list_synthetic.append", "train_steps_needed_synthetic.append", "episodes_needed_synthetic.append", "agents.agent_utils.select_agent", "print", "agents.agent_utils.select_agent.train", "agents.agent_utils.select_agent.test", "print", "reward_list_real.append", "train_steps_needed_real.append", "episodes_needed_real.append", "str", "sum", "len", "str", "sum", "len"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.agents.agent_utils.select_agent", "home.repos.pwc.inspect_result.automl_learning_environments.agents.agent_utils.select_agent", "home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.train", "home.repos.pwc.inspect_result.automl_learning_environments.agents.base_agent.BaseAgent.test", "home.repos.pwc.inspect_result.automl_learning_environments.agents.agent_utils.select_agent", "home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.train", "home.repos.pwc.inspect_result.automl_learning_environments.agents.base_agent.BaseAgent.test"], ["", "def", "train_test_agents", "(", "train_env", ",", "test_env", ",", "config", ",", "agents_num", ")", ":", "\n", "    ", "reward_list_synthetic", "=", "[", "]", "\n", "reward_list_real", "=", "[", "]", "\n", "\n", "train_steps_needed_synthetic", "=", "[", "]", "\n", "episodes_needed_synthetic", "=", "[", "]", "\n", "\n", "train_steps_needed_real", "=", "[", "]", "\n", "episodes_needed_real", "=", "[", "]", "\n", "\n", "# settings for comparability", "\n", "config", "[", "'agents'", "]", "[", "'duelingddqn_vary'", "]", "[", "'vary_hp'", "]", "=", "True", "\n", "config", "[", "'agents'", "]", "[", "'duelingddqn'", "]", "[", "'print_rate'", "]", "=", "10", "\n", "config", "[", "'agents'", "]", "[", "'duelingddqn'", "]", "[", "'early_out_num'", "]", "=", "10", "\n", "config", "[", "'agents'", "]", "[", "'duelingddqn'", "]", "[", "'train_episodes'", "]", "=", "1000", "\n", "config", "[", "'agents'", "]", "[", "'duelingddqn'", "]", "[", "'init_episodes'", "]", "=", "10", "\n", "config", "[", "'agents'", "]", "[", "'duelingddqn'", "]", "[", "'test_episodes'", "]", "=", "100", "\n", "config", "[", "'agents'", "]", "[", "'duelingddqn'", "]", "[", "'early_out_virtual_diff'", "]", "=", "0.01", "\n", "config", "[", "'agents'", "]", "[", "'duelingddqn'", "]", "[", "'early_out_num'", "]", "=", "1000", "# todo: remove eventually (new)", "\n", "\n", "for", "i", "in", "range", "(", "agents_num", ")", ":", "\n", "# synthetic", "\n", "        ", "agent", "=", "select_agent", "(", "config", "=", "config", ",", "agent_name", "=", "'DuelingDDQN_vary'", ")", "\n", "config_varied", "=", "agent", ".", "full_config", "\n", "for", "j", "in", "range", "(", "100", ")", ":", "\n", "            ", "agent", "=", "select_agent", "(", "config", "=", "config_varied", ",", "agent_name", "=", "'DuelingDDQN'", ")", "\n", "print", "(", "f\"training on syn env with config (agent {j}): {agent.full_config}\"", ")", "\n", "reward_train", ",", "episode_length", ",", "_", "=", "agent", ".", "train", "(", "env", "=", "train_env", ")", "\n", "reward", ",", "_", ",", "_", "=", "agent", ".", "test", "(", "env", "=", "test_env", ")", "\n", "print", "(", "'reward when trained on synth. env: '", "+", "str", "(", "reward", ")", ")", "\n", "reward_list_synthetic", ".", "append", "(", "reward", ")", "\n", "train_steps_needed_synthetic", ".", "append", "(", "[", "sum", "(", "episode_length", ")", "]", ")", "\n", "episodes_needed_synthetic", ".", "append", "(", "[", "(", "len", "(", "reward_train", ")", ")", "]", ")", "\n", "\n", "# real", "\n", "agent", "=", "select_agent", "(", "config", "=", "config_varied", ",", "agent_name", "=", "'DuelingDDQN'", ")", "\n", "print", "(", "f\"training on real env with config (agent {j}): {agent.full_config}\"", ")", "\n", "reward_train", ",", "episode_length", ",", "_", "=", "agent", ".", "train", "(", "env", "=", "test_env", ")", "\n", "reward", ",", "_", ",", "_", "=", "agent", ".", "test", "(", "env", "=", "test_env", ")", "\n", "print", "(", "'reward when trained on real env: '", "+", "str", "(", "reward", ")", ")", "\n", "reward_list_real", ".", "append", "(", "reward", ")", "\n", "train_steps_needed_real", ".", "append", "(", "[", "sum", "(", "episode_length", ")", "]", ")", "\n", "episodes_needed_real", ".", "append", "(", "[", "(", "len", "(", "reward_train", ")", ")", "]", ")", "\n", "\n", "", "", "reward_dct", "=", "{", "\n", "\"config\"", ":", "config_varied", ",", "\n", "\"synthetic\"", ":", "reward_list_synthetic", ",", "\n", "\"real\"", ":", "reward_list_real", "\n", "}", "\n", "\n", "train_steps_dct", "=", "{", "\n", "\"config\"", ":", "config_varied", ",", "\n", "\"synthetic\"", ":", "train_steps_needed_synthetic", ",", "\n", "\"real\"", ":", "train_steps_needed_real", "\n", "}", "\n", "\n", "episodes_needed_dct", "=", "{", "\n", "\"config\"", ":", "config_varied", ",", "\n", "\"synthetic\"", ":", "episodes_needed_synthetic", ",", "\n", "\"real\"", ":", "episodes_needed_real", "\n", "}", "\n", "\n", "return", "reward_dct", ",", "train_steps_dct", ",", "episodes_needed_dct", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.bohb_params_DDQN_ICM_cartpole.ExperimentWrapper.get_bohb_parameters": [[19, 28], ["None"], "methods", ["None"], ["    ", "def", "get_bohb_parameters", "(", "self", ")", ":", "\n", "        ", "params", "=", "{", "}", "\n", "params", "[", "'min_budget'", "]", "=", "1", "\n", "params", "[", "'max_budget'", "]", "=", "2", "\n", "params", "[", "'eta'", "]", "=", "2", "\n", "params", "[", "'iterations'", "]", "=", "10000", "\n", "params", "[", "'random_fraction'", "]", "=", "0.3", "\n", "\n", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.bohb_params_DDQN_ICM_cartpole.ExperimentWrapper.get_configspace": [[29, 39], ["ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter"], "methods", ["None"], ["", "def", "get_configspace", "(", "self", ")", ":", "\n", "        ", "cs", "=", "CS", ".", "ConfigurationSpace", "(", ")", "\n", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'lr'", ",", "lower", "=", "1e-5", ",", "upper", "=", "1e-3", ",", "log", "=", "True", ",", "default_value", "=", "1e-4", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'beta'", ",", "lower", "=", "0.001", ",", "upper", "=", "1.0", ",", "log", "=", "True", ",", "default_value", "=", "0.2", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'eta'", ",", "lower", "=", "0.001", ",", "upper", "=", "1.0", ",", "log", "=", "True", ",", "default_value", "=", "0.5", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'feature_dim'", ",", "lower", "=", "16", ",", "upper", "=", "256", ",", "log", "=", "True", ",", "default_value", "=", "64", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'hidden_size'", ",", "lower", "=", "16", ",", "upper", "=", "256", ",", "log", "=", "True", ",", "default_value", "=", "128", ")", ")", "\n", "\n", "return", "cs", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.bohb_params_DDQN_ICM_cartpole.ExperimentWrapper.get_specific_config": [[40, 50], ["copy.deepcopy"], "methods", ["None"], ["", "def", "get_specific_config", "(", "self", ",", "cso", ",", "default_config", ",", "budget", ")", ":", "\n", "        ", "config", "=", "deepcopy", "(", "default_config", ")", "\n", "\n", "config", "[", "\"agents\"", "]", "[", "\"icm\"", "]", "[", "\"lr\"", "]", "=", "cso", "[", "\"lr\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"icm\"", "]", "[", "\"beta\"", "]", "=", "cso", "[", "\"beta\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"icm\"", "]", "[", "\"eta\"", "]", "=", "cso", "[", "\"eta\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"icm\"", "]", "[", "\"feature_dim\"", "]", "=", "cso", "[", "\"feature_dim\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"icm\"", "]", "[", "\"hidden_size\"", "]", "=", "cso", "[", "\"hidden_size\"", "]", "\n", "\n", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.bohb_params_DDQN_ICM_cartpole.ExperimentWrapper.compute": [[51, 91], ["bohb_params_DDQN_ICM_cartpole.ExperimentWrapper.get_specific_config", "print", "print", "print", "print", "print", "print", "envs.env_factory.EnvFactory", "envs.env_factory.EnvFactory.generate_real_env", "agents.DDQN.DDQN", "range", "numpy.mean", "str", "print", "print", "print", "print", "open", "yaml.safe_load", "agents.DDQN.DDQN.train", "len", "score_list.append", "str", "str", "str", "str"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_step_size.ExperimentWrapper.get_specific_config", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_real_env", "home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.train"], ["", "def", "compute", "(", "self", ",", "working_dir", ",", "bohb_id", ",", "config_id", ",", "cso", ",", "budget", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "with", "open", "(", "\"default_config_cartpole.yaml\"", ",", "'r'", ")", "as", "stream", ":", "\n", "            ", "default_config", "=", "yaml", ".", "safe_load", "(", "stream", ")", "\n", "\n", "", "config", "=", "self", ".", "get_specific_config", "(", "cso", ",", "default_config", ",", "budget", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "\"START BOHB ITERATION\"", ")", "\n", "print", "(", "'CONFIG: '", "+", "str", "(", "config", ")", ")", "\n", "print", "(", "'CSO:    '", "+", "str", "(", "cso", ")", ")", "\n", "print", "(", "'BUDGET: '", "+", "str", "(", "budget", ")", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "info", "=", "{", "}", "\n", "\n", "# generate environment", "\n", "env_fac", "=", "EnvFactory", "(", "config", ")", "\n", "env", "=", "env_fac", ".", "generate_real_env", "(", ")", "\n", "\n", "ddqn", "=", "DDQN", "(", "env", "=", "env", ",", "\n", "config", "=", "config", ",", "\n", "icm", "=", "True", ")", "\n", "\n", "score_list", "=", "[", "]", "\n", "for", "_", "in", "range", "(", "5", ")", ":", "\n", "            ", "rewards", ",", "_", ",", "_", "=", "ddqn", ".", "train", "(", "env", ")", "\n", "score_i", "=", "len", "(", "rewards", ")", "\n", "score_list", ".", "append", "(", "score_i", ")", "\n", "\n", "", "score", "=", "np", ".", "mean", "(", "score_list", ")", "\n", "\n", "info", "[", "'config'", "]", "=", "str", "(", "config", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "'FINAL SCORE: '", "+", "str", "(", "score", ")", ")", "\n", "print", "(", "\"END BOHB ITERATION\"", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "return", "{", "\n", "\"loss\"", ":", "score", ",", "\n", "\"info\"", ":", "info", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.get_best_models_from_log": [[37, 66], ["hpbandster.logged_results_to_HBS_result", "hpres.logged_results_to_HBS_result.data.values", "print", "best_models.sort", "os.path.isdir", "log_dir.replace.replace", "best_models.append", "os.path.isfile", "model_name.replace.replace"], "function", ["None"], ["def", "get_best_models_from_log", "(", "log_dir", ")", ":", "\n", "    ", "if", "not", "os", ".", "path", ".", "isdir", "(", "log_dir", ")", ":", "\n", "        ", "log_dir", "=", "log_dir", ".", "replace", "(", "'nierhoff'", ",", "'dingsda'", ")", "\n", "\n", "", "result", "=", "hpres", ".", "logged_results_to_HBS_result", "(", "log_dir", ")", "\n", "\n", "best_models", "=", "[", "]", "\n", "\n", "for", "value", "in", "result", ".", "data", ".", "values", "(", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "loss", "=", "value", ".", "results", "[", "1.0", "]", "[", "'loss'", "]", "\n", "model_name", "=", "value", ".", "results", "[", "1.0", "]", "[", "'info'", "]", "[", "'model_name'", "]", "\n", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "model_name", ")", ":", "\n", "                ", "model_name", "=", "model_name", ".", "replace", "(", "'nierhoff'", ",", "'dingsda'", ")", "\n", "", "best_models", ".", "append", "(", "(", "loss", ",", "model_name", ")", ")", "\n", "", "except", ":", "\n", "            ", "continue", "\n", "\n", "# before AUC (objective minimized)", "\n", "# print(\"sorting from low to high values (non-AUC)\")", "\n", "# best_models.sort(key=lambda x: x[0])", "\n", "\n", "# AUC (objective maximized)", "\n", "", "", "print", "(", "\"sorting from high to low values (AUC)\"", ")", "\n", "best_models", ".", "sort", "(", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ",", "reverse", "=", "True", ")", "\n", "best_models", "=", "best_models", "[", ":", "MODEL_NUM", "]", "\n", "\n", "return", "best_models", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.load_envs_and_config": [[68, 81], ["torch.load", "envs.env_factory.EnvFactory", "envs.env_factory.EnvFactory.generate_reward_env", "env_factory.generate_reward_env.load_state_dict", "envs.env_factory.EnvFactory.generate_real_env"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_reward_env", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_real_env"], ["", "def", "load_envs_and_config", "(", "model_file", ")", ":", "\n", "    ", "save_dict", "=", "torch", ".", "load", "(", "model_file", ")", "\n", "\n", "config", "=", "save_dict", "[", "'config'", "]", "\n", "config", "[", "'device'", "]", "=", "'cpu'", "\n", "config", "[", "'envs'", "]", "[", "'CartPole-v0'", "]", "[", "'solved_reward'", "]", "=", "100000", "# something big enough to prevent early out triggering", "\n", "\n", "env_factory", "=", "EnvFactory", "(", "config", "=", "config", ")", "\n", "reward_env", "=", "env_factory", ".", "generate_reward_env", "(", ")", "\n", "reward_env", ".", "load_state_dict", "(", "save_dict", "[", "'model'", "]", ")", "\n", "real_env", "=", "env_factory", ".", "generate_real_env", "(", ")", "\n", "\n", "return", "reward_env", ",", "real_env", ",", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.train_test_agents": [[83, 134], ["range", "agents.agent_utils.select_agent.train", "rewards.append", "episode_lengths.append", "agents.agent_utils.select_agent", "agents.agent_utils.select_agent"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.train", "home.repos.pwc.inspect_result.automl_learning_environments.agents.agent_utils.select_agent", "home.repos.pwc.inspect_result.automl_learning_environments.agents.agent_utils.select_agent"], ["", "def", "train_test_agents", "(", "mode", ",", "env", ",", "real_env", ",", "config", ")", ":", "\n", "    ", "rewards", "=", "[", "]", "\n", "episode_lengths", "=", "[", "]", "\n", "\n", "# settings for comparability", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'test_episodes'", "]", "=", "1", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'train_episodes'", "]", "=", "1000", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'print_rate'", "]", "=", "100", "\n", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'lr'", "]", "=", "0.00025", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'eps_init'", "]", "=", "1.0", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'eps_min'", "]", "=", "0.1", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'eps_decay'", "]", "=", "0.9", "# original DDQN paper uses linear decay over 1M N's", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'gamma'", "]", "=", "0.99", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'batch_size'", "]", "=", "32", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'same_action_num'", "]", "=", "1", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'activation_fn'", "]", "=", "\"relu\"", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'tau'", "]", "=", "0.01", "# original DDQN paper has hard update every N steps", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'hidden_size'", "]", "=", "64", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'hidden_layer'", "]", "=", "1", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'rb_size'", "]", "=", "1000000", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'init_episodes'", "]", "=", "1", "\n", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'early_out_num'", "]", "=", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'early_out_num'", "]", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'early_out_virtual_diff'", "]", "=", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'early_out_virtual_diff'", "]", "\n", "\n", "# optimized ICM HPs:", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "=", "{", "}", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "[", "'beta'", "]", "=", "0.05", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "[", "'eta'", "]", "=", "0.03", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "[", "'feature_dim'", "]", "=", "32", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "[", "'hidden_size'", "]", "=", "128", "\n", "config", "[", "'agents'", "]", "[", "'icm'", "]", "[", "'lr'", "]", "=", "1e-5", "\n", "\n", "# default ICM HPs:", "\n", "# config['agents']['icm'] = {}", "\n", "# config['agents']['icm']['beta'] = 0.2", "\n", "# config['agents']['icm']['eta'] = 0.5", "\n", "# config['agents']['icm']['feature_dim'] = 64", "\n", "# config['agents']['icm']['hidden_size'] = 128", "\n", "# config['agents']['icm']['lr'] = 1e-4", "\n", "\n", "for", "i", "in", "range", "(", "MODEL_AGENTS", ")", ":", "\n", "        ", "if", "mode", "==", "'-1'", ":", "\n", "            ", "agent", "=", "select_agent", "(", "config", "=", "config", ",", "agent_name", "=", "'ddqn_icm'", ")", "\n", "", "else", ":", "\n", "            ", "agent", "=", "select_agent", "(", "config", "=", "config", ",", "agent_name", "=", "'ddqn'", ")", "\n", "", "reward", ",", "episode_length", ",", "_", "=", "agent", ".", "train", "(", "env", "=", "env", ",", "test_env", "=", "real_env", ")", "\n", "rewards", ".", "append", "(", "reward", ")", "\n", "episode_lengths", ".", "append", "(", "episode_length", ")", "\n", "", "return", "rewards", ",", "episode_lengths", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.save_list": [[136, 146], ["os.makedirs", "os.path.join", "torch.save", "str"], "function", ["None"], ["", "def", "save_list", "(", "mode", ",", "config", ",", "reward_list", ",", "episode_length_list", ")", ":", "\n", "    ", "os", ".", "makedirs", "(", "SAVE_DIR", ",", "exist_ok", "=", "True", ")", "\n", "file_name", "=", "os", ".", "path", ".", "join", "(", "SAVE_DIR", ",", "'best'", "+", "str", "(", "mode", ")", "+", "'.pt'", ")", "\n", "save_dict", "=", "{", "}", "\n", "save_dict", "[", "'config'", "]", "=", "config", "\n", "save_dict", "[", "'model_num'", "]", "=", "MODEL_NUM", "\n", "save_dict", "[", "'model_agents'", "]", "=", "MODEL_AGENTS", "\n", "save_dict", "[", "'reward_list'", "]", "=", "reward_list", "\n", "save_dict", "[", "'episode_length_list'", "]", "=", "episode_length_list", "\n", "torch", ".", "save", "(", "save_dict", ",", "file_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.eval_models": [[148, 163], ["GTNC_evaluate_cartpole_compare_reward_envs.get_best_models_from_log", "GTNC_evaluate_cartpole_compare_reward_envs.save_list", "print", "print", "GTNC_evaluate_cartpole_compare_reward_envs.load_envs_and_config", "GTNC_evaluate_cartpole_compare_reward_envs.train_test_agents", "str", "str"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.get_best_models_from_log", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.save_list", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.load_envs_and_config", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.train_test_agents"], ["", "def", "eval_models", "(", "mode", ",", "log_dir", ")", ":", "\n", "    ", "best_models", "=", "get_best_models_from_log", "(", "log_dir", ")", "\n", "\n", "reward_list", "=", "[", "]", "\n", "episode_length_list", "=", "[", "]", "\n", "\n", "for", "best_reward", ",", "model_file", "in", "best_models", ":", "\n", "        ", "print", "(", "'best reward: '", "+", "str", "(", "best_reward", ")", ")", "\n", "print", "(", "'model file: '", "+", "str", "(", "model_file", ")", ")", "\n", "reward_env", ",", "real_env", ",", "config", "=", "load_envs_and_config", "(", "model_file", ")", "\n", "rewards", ",", "episode_lengths", "=", "train_test_agents", "(", "mode", "=", "mode", ",", "env", "=", "reward_env", ",", "real_env", "=", "real_env", ",", "config", "=", "config", ")", "\n", "reward_list", "+=", "rewards", "\n", "episode_length_list", "+=", "episode_lengths", "\n", "\n", "", "save_list", "(", "mode", ",", "config", ",", "reward_list", ",", "episode_length_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.eval_base": [[165, 178], ["GTNC_evaluate_cartpole_compare_reward_envs.get_best_models_from_log", "range", "GTNC_evaluate_cartpole_compare_reward_envs.save_list", "GTNC_evaluate_cartpole_compare_reward_envs.load_envs_and_config", "GTNC_evaluate_cartpole_compare_reward_envs.train_test_agents"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.get_best_models_from_log", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.save_list", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.load_envs_and_config", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.train_test_agents"], ["", "def", "eval_base", "(", "mode", ",", "log_dir", ")", ":", "\n", "    ", "best_models", "=", "get_best_models_from_log", "(", "log_dir", ")", "\n", "\n", "reward_list", "=", "[", "]", "\n", "episode_length_list", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "MODEL_NUM", ")", ":", "\n", "        ", "reward_env", ",", "real_env", ",", "config", "=", "load_envs_and_config", "(", "best_models", "[", "0", "]", "[", "1", "]", ")", "\n", "rewards", ",", "episode_lengths", "=", "train_test_agents", "(", "mode", "=", "mode", ",", "env", "=", "real_env", ",", "real_env", "=", "real_env", ",", "config", "=", "config", ")", "\n", "reward_list", "+=", "rewards", "\n", "episode_length_list", "+=", "episode_lengths", "\n", "\n", "", "save_list", "(", "mode", ",", "config", ",", "reward_list", ",", "episode_length_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.eval_icm": [[180, 193], ["GTNC_evaluate_cartpole_compare_reward_envs.get_best_models_from_log", "range", "GTNC_evaluate_cartpole_compare_reward_envs.save_list", "GTNC_evaluate_cartpole_compare_reward_envs.load_envs_and_config", "GTNC_evaluate_cartpole_compare_reward_envs.train_test_agents"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.get_best_models_from_log", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_cartpole_compare_reward_envs.save_list", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.load_envs_and_config", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.train_test_agents"], ["", "def", "eval_icm", "(", "mode", ",", "log_dir", ")", ":", "\n", "    ", "best_models", "=", "get_best_models_from_log", "(", "log_dir", ")", "\n", "\n", "reward_list", "=", "[", "]", "\n", "episode_length_list", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "MODEL_NUM", ")", ":", "\n", "        ", "reward_env", ",", "real_env", ",", "config", "=", "load_envs_and_config", "(", "best_models", "[", "0", "]", "[", "1", "]", ")", "\n", "rewards", ",", "episode_lengths", "=", "train_test_agents", "(", "mode", "=", "mode", ",", "env", "=", "real_env", ",", "real_env", "=", "real_env", ",", "config", "=", "config", ")", "\n", "reward_list", "+=", "rewards", "\n", "episode_length_list", "+=", "episode_lengths", "\n", "\n", "", "save_list", "(", "mode", ",", "config", ",", "reward_list", ",", "episode_length_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cmc_transfer_algo.get_data": [[45, 95], ["float", "print", "print", "max", "torch.load", "list_data.append", "sums_eps_len_per_model.append", "sum", "sum", "numpy.zeros", "enumerate", "numpy.mean", "numpy.std", "proc_data.append", "print", "sums_eps_len.append", "sums_eps_len_per_model_i.append", "min", "zip", "range", "numpy.array", "len", "sum", "sum", "sum", "numpy.asarray", "numpy.asarray", "len", "len", "concat_list.append"], "function", ["None"], ["def", "get_data", "(", ")", ":", "\n", "    ", "list_data", "=", "[", "]", "\n", "for", "log_file", "in", "LOG_FILES", ":", "\n", "        ", "data", "=", "torch", ".", "load", "(", "log_file", ")", "\n", "list_data", ".", "append", "(", "(", "data", "[", "'reward_list'", "]", ",", "data", "[", "'episode_length_list'", "]", ")", ")", "\n", "model_num", "=", "data", "[", "'model_num'", "]", "\n", "model_agents", "=", "data", "[", "'model_agents'", "]", "\n", "\n", "", "sums_eps_len", "=", "[", "]", "\n", "sums_eps_len_per_model", "=", "[", "]", "\n", "min_steps", "=", "float", "(", "'Inf'", ")", "\n", "# get minimum number of evaluations", "\n", "for", "reward_list", ",", "episode_length_list", "in", "list_data", ":", "\n", "        ", "sums_eps_len_per_model_i", "=", "[", "]", "\n", "for", "episode_lengths", "in", "episode_length_list", ":", "\n", "            ", "print", "(", "len", "(", "episode_lengths", ")", ")", "\n", "sums_eps_len", ".", "append", "(", "sum", "(", "episode_lengths", ")", ")", "\n", "sums_eps_len_per_model_i", ".", "append", "(", "sum", "(", "episode_lengths", ")", ")", "\n", "min_steps", "=", "min", "(", "min_steps", ",", "sum", "(", "episode_lengths", ")", ")", "\n", "", "sums_eps_len_per_model", ".", "append", "(", "sums_eps_len_per_model_i", ")", "\n", "\n", "", "print", "(", "\"# of episode lens > MIN_STEPS: \"", ",", "sum", "(", "np", ".", "asarray", "(", "sums_eps_len", ")", ">=", "MIN_STEPS", ")", ")", "\n", "print", "(", "\"# of episode lens < MIN_STEPS: \"", ",", "sum", "(", "np", ".", "asarray", "(", "sums_eps_len", ")", "<", "MIN_STEPS", ")", ")", "\n", "min_steps", "=", "max", "(", "min_steps", ",", "MIN_STEPS", ")", "\n", "# convert data from episodes to steps", "\n", "proc_data", "=", "[", "]", "\n", "\n", "for", "reward_list", ",", "episode_length_list", "in", "list_data", ":", "\n", "        ", "np_data", "=", "np", ".", "zeros", "(", "[", "model_num", "*", "model_agents", ",", "min_steps", "]", ")", "\n", "\n", "for", "it", ",", "data", "in", "enumerate", "(", "zip", "(", "reward_list", ",", "episode_length_list", ")", ")", ":", "\n", "            ", "rewards", ",", "episode_lengths", "=", "data", "\n", "\n", "concat_list", "=", "[", "]", "\n", "rewards", "=", "rewards", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "episode_lengths", ")", ")", ":", "\n", "                ", "concat_list", "+=", "[", "rewards", "[", "i", "]", "]", "*", "episode_lengths", "[", "i", "]", "\n", "\n", "", "while", "len", "(", "concat_list", ")", "<", "min_steps", ":", "\n", "                ", "concat_list", ".", "append", "(", "concat_list", "[", "-", "1", "]", ")", "\n", "\n", "", "np_data", "[", "it", "]", "=", "np", ".", "array", "(", "concat_list", "[", ":", "min_steps", "]", ")", "\n", "\n", "", "mean", "=", "np", ".", "mean", "(", "np_data", ",", "axis", "=", "0", ")", "\n", "std", "=", "np", ".", "std", "(", "np_data", ",", "axis", "=", "0", ")", "\n", "\n", "proc_data", ".", "append", "(", "(", "mean", ",", "std", ")", ")", "\n", "\n", "", "return", "proc_data", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cmc_transfer_algo.plot_data": [[97, 129], ["matplotlib.subplots", "matplotlib.legend", "matplotlib.subplots_adjust", "matplotlib.title", "matplotlib.xlabel", "matplotlib.xlim", "matplotlib.ylim", "matplotlib.ylabel", "os.path.dirname", "matplotlib.savefig", "matplotlib.show", "matplotlib.plot", "matplotlib.fill_between", "legobj.set_linewidth", "os.path.join", "range", "len"], "function", ["None"], ["", "def", "plot_data", "(", "proc_data", ",", "savefig_name", ")", ":", "\n", "    ", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", "dpi", "=", "600", ",", "figsize", "=", "(", "5", ",", "4", ")", ")", "\n", "colors", "=", "[", "]", "\n", "#", "\n", "# for mean, _ in data_w:", "\n", "#     plt.plot(mean_w)", "\n", "#     colors.append(plt.gca().lines[-1].get_color())", "\n", "\n", "for", "mean", ",", "std", "in", "proc_data", ":", "\n", "        ", "plt", ".", "plot", "(", "mean", ",", "linewidth", "=", "1", ")", "\n", "\n", "", "for", "mean", ",", "std", "in", "proc_data", ":", "\n", "        ", "plt", ".", "fill_between", "(", "x", "=", "range", "(", "len", "(", "mean", ")", ")", ",", "y1", "=", "mean", "-", "std", "*", "STD_MULT", ",", "y2", "=", "mean", "+", "std", "*", "STD_MULT", ",", "alpha", "=", "0.3", ")", "\n", "\n", "", "leg", "=", "plt", ".", "legend", "(", "LEGEND", ",", "fontsize", "=", "9", ")", "\n", "\n", "for", "legobj", "in", "leg", ".", "legendHandles", ":", "\n", "        ", "legobj", ".", "set_linewidth", "(", "2.0", ")", "\n", "\n", "# plt.xlim(0,99)", "\n", "", "plt", ".", "subplots_adjust", "(", "bottom", "=", "0.15", ",", "left", "=", "0.15", ")", "\n", "plt", ".", "title", "(", "'MountainCarContinuous-v0 Transfer'", ")", "\n", "plt", ".", "xlabel", "(", "'steps'", ")", "\n", "plt", ".", "xlim", "(", "0", ",", "MIN_STEPS", ")", "\n", "# freq_x_ticks = np.arange(0, MIN_STEPS, 33000)", "\n", "# plt.xticks(freq_x_ticks)", "\n", "# ax.xaxis.set_tick_params(labelsize='small')", "\n", "plt", ".", "ylim", "(", "-", "75", ",", "100", ")", "\n", "plt", ".", "ylabel", "(", "'cumulative reward'", ")", "\n", "base_dir", "=", "os", ".", "path", ".", "dirname", "(", "LOG_FILES", "[", "0", "]", ")", "\n", "plt", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "base_dir", ",", "savefig_name", ")", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cartpole_histogram.load_envs_and_config": [[17, 29], ["os.path.join", "torch.load", "envs.env_factory.EnvFactory", "envs.env_factory.EnvFactory.generate_virtual_env", "env_factory.generate_virtual_env.load_state_dict", "envs.env_factory.EnvFactory.generate_real_env"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_virtual_env", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_real_env"], ["def", "load_envs_and_config", "(", "dir", ",", "file_name", ")", ":", "\n", "    ", "file_path", "=", "os", ".", "path", ".", "join", "(", "dir", ",", "file_name", ")", "\n", "save_dict", "=", "torch", ".", "load", "(", "file_path", ")", "\n", "config", "=", "save_dict", "[", "'config'", "]", "\n", "# config['envs']['CartPole-v0']['solved_reward'] = 195", "\n", "# config['envs']['CartPole-v0']['max_steps'] = 200", "\n", "env_factory", "=", "EnvFactory", "(", "config", "=", "config", ")", "\n", "virtual_env", "=", "env_factory", ".", "generate_virtual_env", "(", ")", "\n", "virtual_env", ".", "load_state_dict", "(", "save_dict", "[", "'model'", "]", ")", "\n", "real_env", "=", "env_factory", ".", "generate_real_env", "(", ")", "\n", "\n", "return", "virtual_env", ",", "real_env", ",", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cartpole_histogram.plot_hist": [[31, 53], ["matplotlib.figure", "matplotlib.hist", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.yscale", "matplotlib.subplots_adjust", "matplotlib.tick_params", "matplotlib.savefig", "matplotlib.show", "max", "min", "matplotlib.hist", "matplotlib.hist", "matplotlib.hist", "matplotlib.legend", "matplotlib.legend", "max", "int", "max", "max", "max", "str", "int", "int", "int", "max", "min", "max", "min", "max", "min", "max", "min"], "function", ["None"], ["", "def", "plot_hist", "(", "h1", ",", "h2", ",", "h1l", ",", "h2l", ",", "h3", "=", "None", ",", "h3l", "=", "None", ",", "xlabel", "=", "None", ",", "save_idx", "=", "None", ",", "agentname", "=", "\"none\"", ")", ":", "\n", "    ", "plt", ".", "figure", "(", "dpi", "=", "600", ",", "figsize", "=", "(", "3.5", ",", "3", ")", ")", "\n", "plt", ".", "hist", "(", "h1", ",", "alpha", "=", "0.8", ",", "bins", "=", "max", "(", "1", ",", "int", "(", "(", "max", "(", "h1", ")", "-", "min", "(", "h1", ")", ")", "/", "BIN_WIDTH", ")", ")", ")", "\n", "\n", "if", "max", "(", "h2", ")", "==", "min", "(", "h2", ")", ":", "# if we had only a single bin", "\n", "        ", "plt", ".", "hist", "(", "h2", ",", "alpha", "=", "0.6", ",", "bins", "=", "max", "(", "1", ",", "int", "(", "(", "max", "(", "h1", ")", "-", "min", "(", "h1", ")", ")", "/", "BIN_WIDTH", ")", ")", ")", "\n", "", "else", ":", "\n", "        ", "plt", ".", "hist", "(", "h2", ",", "alpha", "=", "0.6", ",", "bins", "=", "max", "(", "1", ",", "int", "(", "(", "max", "(", "h2", ")", "-", "min", "(", "h2", ")", ")", "/", "BIN_WIDTH", ")", ")", ")", "\n", "\n", "", "if", "h3", "is", "not", "None", ":", "\n", "        ", "plt", ".", "hist", "(", "h3", ",", "alpha", "=", "0.4", ",", "bins", "=", "max", "(", "1", ",", "int", "(", "(", "max", "(", "h3", ")", "-", "min", "(", "h3", ")", ")", "/", "BIN_WIDTH", ")", ")", ")", "\n", "", "plt", ".", "xlabel", "(", "xlabel", ",", "fontsize", "=", "12", ")", "\n", "plt", ".", "ylabel", "(", "'occurrence'", ",", "fontsize", "=", "12", ")", "\n", "plt", ".", "yscale", "(", "'log'", ")", "\n", "if", "h3", "is", "not", "None", ":", "\n", "        ", "plt", ".", "legend", "(", "(", "h1l", ",", "h2l", ",", "h3l", ")", ",", "loc", "=", "'upper left'", ",", "fontsize", "=", "10", ")", "\n", "", "else", ":", "\n", "        ", "plt", ".", "legend", "(", "(", "h1l", ",", "h2l", ")", ",", "loc", "=", "'upper left'", ",", "fontsize", "=", "10", ")", "\n", "", "plt", ".", "subplots_adjust", "(", "bottom", "=", "0.15", ",", "left", "=", "0.16", ")", "\n", "plt", ".", "tick_params", "(", "labelsize", "=", "12", ")", "\n", "plt", ".", "savefig", "(", "'cartpole_histogram_'", "+", "agentname", "+", "'_'", "+", "'CW9CSH'", "+", "'_'", "+", "str", "(", "save_idx", ")", "+", "'.png'", ",", "bbox_inches", "=", "'tight'", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cartpole_histogram.compare_env_output": [[55, 110], ["replay_buffer_train_all.get_all", "replay_buffer_test_all.get_all", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "range", "range", "GTNC_visualize_cartpole_histogram.plot_hist", "len", "virtual_env.step", "virtual_env.step", "len", "next_states_train[].squeeze().detach().numpy", "next_states_test[].squeeze().detach().numpy", "virt_next_states[].squeeze().detach().numpy", "GTNC_visualize_cartpole_histogram.plot_hist", "rewards_train.squeeze().detach().numpy", "rewards_test.squeeze().detach().numpy", "torch.zeros_like.squeeze().detach().numpy", "next_states_train[].squeeze().detach", "next_states_test[].squeeze().detach", "virt_next_states[].squeeze().detach", "len", "rewards_train.squeeze().detach", "rewards_test.squeeze().detach", "torch.zeros_like.squeeze().detach", "next_states_train[].squeeze", "next_states_test[].squeeze", "virt_next_states[].squeeze", "rewards_train.squeeze", "rewards_test.squeeze", "torch.zeros_like.squeeze"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.None.utils.ReplayBuffer.get_all", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.ReplayBuffer.get_all", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cartpole_histogram.plot_hist", "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditEnv.step", "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditEnv.step", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cartpole_histogram.plot_hist"], ["", "def", "compare_env_output", "(", "virtual_env", ",", "replay_buffer_train_all", ",", "replay_buffer_test_all", ",", "agentname", ")", ":", "\n", "    ", "states_train", ",", "actions_train", ",", "next_states_train", ",", "rewards_train", ",", "dones_train", "=", "replay_buffer_train_all", ".", "get_all", "(", ")", "\n", "states_test", ",", "actions_test", ",", "next_states_test", ",", "rewards_test", ",", "dones_test", "=", "replay_buffer_test_all", ".", "get_all", "(", ")", "\n", "\n", "virt_next_states", "=", "torch", ".", "zeros_like", "(", "next_states_test", ")", "\n", "virt_rewards", "=", "torch", ".", "zeros_like", "(", "rewards_test", ")", "\n", "virt_rewards_incorrect", "=", "torch", ".", "zeros_like", "(", "rewards_test", ")", "\n", "virt_dones", "=", "torch", ".", "zeros_like", "(", "dones_test", ")", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "states_test", ")", ")", ":", "\n", "        ", "state", "=", "states_test", "[", "i", "]", "\n", "action", "=", "actions_test", "[", "i", "]", "\n", "\n", "next_state_virtual", ",", "reward_virtual", ",", "done_virtual", "=", "virtual_env", ".", "step", "(", "action", "=", "action", ",", "state", "=", "state", ")", "\n", "virt_next_states", "[", "i", "]", "=", "next_state_virtual", "\n", "virt_rewards", "[", "i", "]", "=", "reward_virtual", "\n", "virt_dones", "[", "i", "]", "=", "done_virtual", "\n", "\n", "_", ",", "reward_virtual_incorrect", ",", "_", "=", "virtual_env", ".", "step", "(", "action", "=", "1", "-", "action", ",", "state", "=", "state", ")", "\n", "virt_rewards_incorrect", "[", "i", "]", "=", "reward_virtual_incorrect", "\n", "\n", "", "for", "i", "in", "range", "(", "len", "(", "next_states_test", "[", "0", "]", ")", ")", ":", "\n", "        ", "trains", "=", "next_states_train", "[", ":", ",", "i", "]", ".", "squeeze", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "tests", "=", "next_states_test", "[", ":", ",", "i", "]", ".", "squeeze", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "virts", "=", "virt_next_states", "[", ":", ",", "i", "]", ".", "squeeze", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "# diffs = diff_next_states[:,i].squeeze().detach().numpy()", "\n", "\n", "if", "i", "==", "0", ":", "\n", "            ", "plot_name", "=", "'cart position (next state) [m]'", "\n", "", "elif", "i", "==", "1", ":", "\n", "            ", "plot_name", "=", "'cart velocity (next state) [m/s]'", "\n", "", "elif", "i", "==", "2", ":", "\n", "            ", "plot_name", "=", "'pole angle (next state) [rad]'", "\n", "", "elif", "i", "==", "3", ":", "\n", "            ", "plot_name", "=", "'pole angular vel. (next state) [rad/s]'", "\n", "\n", "", "plot_hist", "(", "h1", "=", "trains", ",", "\n", "h2", "=", "tests", ",", "\n", "h3", "=", "virts", ",", "\n", "h1l", "=", "'synth. env. (train)'", ",", "\n", "h2l", "=", "'real env. (test)'", ",", "\n", "h3l", "=", "'synth. env. on real env. data'", ",", "\n", "xlabel", "=", "plot_name", ",", "\n", "save_idx", "=", "i", "+", "1", ",", "\n", "agentname", "=", "agentname", ")", "\n", "\n", "", "plot_hist", "(", "h1", "=", "rewards_train", ".", "squeeze", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ",", "\n", "h2", "=", "rewards_test", ".", "squeeze", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ",", "\n", "h3", "=", "virt_rewards", ".", "squeeze", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ",", "\n", "h1l", "=", "'synth. env. (train)'", ",", "\n", "h2l", "=", "'real env. (test)'", ",", "\n", "h3l", "=", "'synth. env. on real env. data'", ",", "\n", "xlabel", "=", "'reward'", ",", "\n", "save_idx", "=", "len", "(", "next_states_test", "[", "0", "]", ")", "+", "1", ",", "\n", "agentname", "=", "agentname", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_acrobot_vary_hp.ExperimentWrapper.get_bohb_parameters": [[17, 26], ["None"], "methods", ["None"], ["    ", "def", "get_bohb_parameters", "(", "self", ")", ":", "\n", "        ", "params", "=", "{", "}", "\n", "params", "[", "'min_budget'", "]", "=", "1", "\n", "params", "[", "'max_budget'", "]", "=", "1", "\n", "params", "[", "'eta'", "]", "=", "2", "\n", "params", "[", "'random_fraction'", "]", "=", "1", "\n", "params", "[", "'iterations'", "]", "=", "10000", "\n", "\n", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_acrobot_vary_hp.ExperimentWrapper.get_configspace": [[27, 33], ["ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.CategoricalHyperparameter"], "methods", ["None"], ["", "def", "get_configspace", "(", "self", ")", ":", "\n", "        ", "cs", "=", "CS", ".", "ConfigurationSpace", "(", ")", "\n", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "CategoricalHyperparameter", "(", "name", "=", "'ddqn_vary_vary_hp'", ",", "choices", "=", "[", "False", ",", "True", "]", ",", "default_value", "=", "False", ")", ")", "\n", "\n", "return", "cs", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_acrobot_vary_hp.ExperimentWrapper.get_specific_config": [[34, 44], ["copy.deepcopy"], "methods", ["None"], ["", "def", "get_specific_config", "(", "self", ",", "cso", ",", "default_config", ",", "budget", ")", ":", "\n", "        ", "config", "=", "deepcopy", "(", "default_config", ")", "\n", "\n", "env_name", "=", "config", "[", "\"env_name\"", "]", "\n", "\n", "config", "[", "\"render_env\"", "]", "=", "False", "\n", "\n", "config", "[", "\"agents\"", "]", "[", "'ddqn_vary'", "]", "[", "'vary_hp'", "]", "=", "cso", "[", "\"ddqn_vary_vary_hp\"", "]", "\n", "\n", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_acrobot_vary_hp.ExperimentWrapper.compute": [[45, 83], ["GTNC_evaluate_acrobot_vary_hp.ExperimentWrapper.get_specific_config", "print", "print", "print", "print", "print", "print", "str", "str", "str", "print", "print", "print", "print", "print", "open", "yaml.safe_load", "agents.GTN.GTN_Master", "agents.GTN.GTN_Master.run", "len", "str", "str", "str", "float", "traceback.format_exc", "print", "str", "str"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_step_size.ExperimentWrapper.get_specific_config", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.run"], ["", "def", "compute", "(", "self", ",", "working_dir", ",", "bohb_id", ",", "config_id", ",", "cso", ",", "budget", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "with", "open", "(", "\"default_config_acrobot.yaml\"", ",", "'r'", ")", "as", "stream", ":", "\n", "            ", "default_config", "=", "yaml", ".", "safe_load", "(", "stream", ")", "\n", "\n", "", "config", "=", "self", ".", "get_specific_config", "(", "cso", ",", "default_config", ",", "budget", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "\"START BOHB ITERATION\"", ")", "\n", "print", "(", "'CONFIG: '", "+", "str", "(", "config", ")", ")", "\n", "print", "(", "'CSO:    '", "+", "str", "(", "cso", ")", ")", "\n", "print", "(", "'BUDGET: '", "+", "str", "(", "budget", ")", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "try", ":", "\n", "            ", "gtn", "=", "GTN_Master", "(", "config", ",", "bohb_id", "=", "bohb_id", ",", "bohb_working_dir", "=", "working_dir", ")", "\n", "_", ",", "score_list", "=", "gtn", ".", "run", "(", ")", "\n", "score", "=", "len", "(", "score_list", ")", "\n", "error", "=", "\"\"", "\n", "", "except", ":", "\n", "            ", "score", "=", "float", "(", "'Inf'", ")", "\n", "score_list", "=", "[", "]", "\n", "error", "=", "traceback", ".", "format_exc", "(", ")", "\n", "print", "(", "error", ")", "\n", "\n", "", "info", "=", "{", "}", "\n", "info", "[", "'error'", "]", "=", "str", "(", "error", ")", "\n", "info", "[", "'score_list'", "]", "=", "str", "(", "score_list", ")", "\n", "info", "[", "'config'", "]", "=", "str", "(", "config", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "'FINAL SCORE: '", "+", "str", "(", "score", ")", ")", "\n", "print", "(", "'SCORE LIST:  '", "+", "str", "(", "score_list", ")", ")", "\n", "print", "(", "\"END BOHB ITERATION\"", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "return", "{", "\n", "\"loss\"", ":", "score", ",", "\n", "\"info\"", ":", "info", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_step_size.ExperimentWrapper.get_bohb_parameters": [[17, 26], ["None"], "methods", ["None"], ["    ", "def", "get_bohb_parameters", "(", "self", ")", ":", "\n", "        ", "params", "=", "{", "}", "\n", "params", "[", "'min_budget'", "]", "=", "1", "\n", "params", "[", "'max_budget'", "]", "=", "1", "\n", "params", "[", "'eta'", "]", "=", "2", "\n", "params", "[", "'random_fraction'", "]", "=", "1", "\n", "params", "[", "'iterations'", "]", "=", "10000", "\n", "\n", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_step_size.ExperimentWrapper.get_configspace": [[27, 36], ["ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.CategoricalHyperparameter", "ConfigSpace.CategoricalHyperparameter"], "methods", ["None"], ["", "def", "get_configspace", "(", "self", ")", ":", "\n", "        ", "cs", "=", "CS", ".", "ConfigurationSpace", "(", ")", "\n", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'gtn_score_transform_type'", ",", "lower", "=", "0", ",", "upper", "=", "7", ",", "log", "=", "False", ",", "default_value", "=", "7", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'gtn_step_size'", ",", "lower", "=", "1e-2", ",", "upper", "=", "100", ",", "log", "=", "True", ",", "default_value", "=", "1", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "CategoricalHyperparameter", "(", "name", "=", "'gtn_mirrored_sampling'", ",", "choices", "=", "[", "False", ",", "True", "]", ",", "default_value", "=", "False", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "CategoricalHyperparameter", "(", "name", "=", "'gtn_nes_step_size'", ",", "choices", "=", "[", "False", ",", "True", "]", ",", "default_value", "=", "False", ")", ")", "\n", "\n", "return", "cs", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_step_size.ExperimentWrapper.get_specific_config": [[37, 50], ["copy.deepcopy"], "methods", ["None"], ["", "def", "get_specific_config", "(", "self", ",", "cso", ",", "default_config", ",", "budget", ")", ":", "\n", "        ", "config", "=", "deepcopy", "(", "default_config", ")", "\n", "\n", "env_name", "=", "config", "[", "\"env_name\"", "]", "\n", "\n", "config", "[", "\"render_env\"", "]", "=", "False", "\n", "\n", "config", "[", "\"agents\"", "]", "[", "'gtn'", "]", "[", "'score_transform_type'", "]", "=", "cso", "[", "\"gtn_score_transform_type\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'gtn'", "]", "[", "'step_size'", "]", "=", "cso", "[", "\"gtn_step_size\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'gtn'", "]", "[", "'mirrored_sampling'", "]", "=", "cso", "[", "\"gtn_mirrored_sampling\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "'gtn'", "]", "[", "'nes_step_size'", "]", "=", "cso", "[", "\"gtn_nes_step_size\"", "]", "\n", "\n", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_step_size.ExperimentWrapper.compute": [[51, 89], ["GTNC_evaluate_gridworld_step_size.ExperimentWrapper.get_specific_config", "print", "print", "print", "print", "print", "print", "str", "str", "str", "print", "print", "print", "print", "print", "open", "yaml.safe_load", "agents.GTN.GTN_Master", "agents.GTN.GTN_Master.run", "len", "str", "str", "str", "float", "traceback.format_exc", "print", "str", "str"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_step_size.ExperimentWrapper.get_specific_config", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.run"], ["", "def", "compute", "(", "self", ",", "working_dir", ",", "bohb_id", ",", "config_id", ",", "cso", ",", "budget", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "with", "open", "(", "\"default_config_gridworld.yaml\"", ",", "'r'", ")", "as", "stream", ":", "\n", "            ", "default_config", "=", "yaml", ".", "safe_load", "(", "stream", ")", "\n", "\n", "", "config", "=", "self", ".", "get_specific_config", "(", "cso", ",", "default_config", ",", "budget", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "\"START BOHB ITERATION\"", ")", "\n", "print", "(", "'CONFIG: '", "+", "str", "(", "config", ")", ")", "\n", "print", "(", "'CSO:    '", "+", "str", "(", "cso", ")", ")", "\n", "print", "(", "'BUDGET: '", "+", "str", "(", "budget", ")", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "try", ":", "\n", "            ", "gtn", "=", "GTN_Master", "(", "config", ",", "bohb_id", "=", "bohb_id", ",", "bohb_working_dir", "=", "working_dir", ")", "\n", "_", ",", "score_list", "=", "gtn", ".", "run", "(", ")", "\n", "score", "=", "len", "(", "score_list", ")", "\n", "error", "=", "\"\"", "\n", "", "except", ":", "\n", "            ", "score", "=", "float", "(", "'Inf'", ")", "\n", "score_list", "=", "[", "]", "\n", "error", "=", "traceback", ".", "format_exc", "(", ")", "\n", "print", "(", "error", ")", "\n", "\n", "", "info", "=", "{", "}", "\n", "info", "[", "'error'", "]", "=", "str", "(", "error", ")", "\n", "info", "[", "'score_list'", "]", "=", "str", "(", "score_list", ")", "\n", "info", "[", "'config'", "]", "=", "str", "(", "config", ")", "\n", "\n", "print", "(", "'----------------------------'", ")", "\n", "print", "(", "'FINAL SCORE: '", "+", "str", "(", "score", ")", ")", "\n", "print", "(", "'SCORE LIST:  '", "+", "str", "(", "score_list", ")", ")", "\n", "print", "(", "\"END BOHB ITERATION\"", ")", "\n", "print", "(", "'----------------------------'", ")", "\n", "\n", "return", "{", "\n", "\"loss\"", ":", "score", ",", "\n", "\"info\"", ":", "info", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2_DuelingDDQN.load_envs_and_config": [[13, 31], ["os.path.join", "torch.load", "envs.env_factory.EnvFactory", "envs.env_factory.EnvFactory.generate_virtual_env", "env_factory.generate_virtual_env.load_state_dict", "envs.env_factory.EnvFactory.generate_real_env", "open", "yaml.safe_load"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_virtual_env", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_real_env"], ["def", "load_envs_and_config", "(", "file_name", ",", "model_dir", ",", "device", ")", ":", "\n", "    ", "file_path", "=", "os", ".", "path", ".", "join", "(", "model_dir", ",", "file_name", ")", "\n", "save_dict", "=", "torch", ".", "load", "(", "file_path", ")", "\n", "config", "=", "save_dict", "[", "'config'", "]", "\n", "config", "[", "'device'", "]", "=", "device", "\n", "env_factory", "=", "EnvFactory", "(", "config", "=", "config", ")", "\n", "virtual_env", "=", "env_factory", ".", "generate_virtual_env", "(", ")", "\n", "virtual_env", ".", "load_state_dict", "(", "save_dict", "[", "'model'", "]", ")", "\n", "real_env", "=", "env_factory", ".", "generate_real_env", "(", ")", "\n", "\n", "# load additional agent configs", "\n", "with", "open", "(", "\"../default_config_cartpole.yaml\"", ",", "\"r\"", ")", "as", "stream", ":", "\n", "        ", "config_new", "=", "yaml", ".", "safe_load", "(", "stream", ")", "[", "\"agents\"", "]", "\n", "\n", "", "config", "[", "\"agents\"", "]", "[", "\"duelingddqn\"", "]", "=", "config_new", "[", "\"duelingddqn\"", "]", "\n", "config", "[", "\"agents\"", "]", "[", "\"duelingddqn_vary\"", "]", "=", "config_new", "[", "\"duelingddqn_vary\"", "]", "\n", "\n", "return", "virtual_env", ",", "real_env", ",", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2_DuelingDDQN.train_test_agents": [[33, 57], ["range", "agents.agent_utils.select_agent", "agents.agent_utils.select_agent.train", "agents.agent_utils.select_agent.test", "print", "reward_list.append", "train_steps_needed.append", "episodes_needed.append", "str", "sum", "len"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.agents.agent_utils.select_agent", "home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.train", "home.repos.pwc.inspect_result.automl_learning_environments.agents.base_agent.BaseAgent.test"], ["", "def", "train_test_agents", "(", "train_env", ",", "test_env", ",", "config", ",", "agents_num", ")", ":", "\n", "    ", "reward_list", "=", "[", "]", "\n", "train_steps_needed", "=", "[", "]", "\n", "episodes_needed", "=", "[", "]", "\n", "\n", "# settings for comparability", "\n", "config", "[", "'agents'", "]", "[", "'duelingddqn_vary'", "]", "[", "'vary_hp'", "]", "=", "True", "\n", "config", "[", "'agents'", "]", "[", "'duelingddqn'", "]", "[", "'print_rate'", "]", "=", "10", "\n", "config", "[", "'agents'", "]", "[", "'duelingddqn'", "]", "[", "'early_out_num'", "]", "=", "10", "\n", "config", "[", "'agents'", "]", "[", "'duelingddqn'", "]", "[", "'train_episodes'", "]", "=", "1000", "\n", "config", "[", "'agents'", "]", "[", "'duelingddqn'", "]", "[", "'init_episodes'", "]", "=", "10", "\n", "config", "[", "'agents'", "]", "[", "'duelingddqn'", "]", "[", "'test_episodes'", "]", "=", "10", "\n", "config", "[", "'agents'", "]", "[", "'duelingddqn'", "]", "[", "'early_out_virtual_diff'", "]", "=", "0.01", "\n", "\n", "for", "i", "in", "range", "(", "agents_num", ")", ":", "\n", "        ", "agent", "=", "select_agent", "(", "config", "=", "config", ",", "agent_name", "=", "'DuelingDDQN_vary'", ")", "\n", "reward_train", ",", "episode_length", ",", "_", "=", "agent", ".", "train", "(", "env", "=", "train_env", ")", "\n", "reward", ",", "_", ",", "_", "=", "agent", ".", "test", "(", "env", "=", "test_env", ")", "\n", "print", "(", "'reward: '", "+", "str", "(", "reward", ")", ")", "\n", "reward_list", ".", "append", "(", "reward", ")", "\n", "train_steps_needed", ".", "append", "(", "[", "sum", "(", "episode_length", ")", "]", ")", "\n", "episodes_needed", ".", "append", "(", "[", "(", "len", "(", "reward_train", ")", ")", "]", ")", "\n", "\n", "", "return", "reward_list", ",", "train_steps_needed", ",", "episodes_needed", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.load_envs_and_config": [[12, 23], ["os.path.join", "torch.load", "envs.env_factory.EnvFactory", "envs.env_factory.EnvFactory.generate_virtual_env", "env_factory.generate_virtual_env.load_state_dict", "envs.env_factory.EnvFactory.generate_real_env"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_virtual_env", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_real_env"], ["def", "load_envs_and_config", "(", "file_name", ",", "model_dir", ",", "device", ")", ":", "\n", "    ", "file_path", "=", "os", ".", "path", ".", "join", "(", "model_dir", ",", "file_name", ")", "\n", "save_dict", "=", "torch", ".", "load", "(", "file_path", ")", "\n", "config", "=", "save_dict", "[", "'config'", "]", "\n", "config", "[", "'device'", "]", "=", "device", "\n", "env_factory", "=", "EnvFactory", "(", "config", "=", "config", ")", "\n", "virtual_env", "=", "env_factory", ".", "generate_virtual_env", "(", ")", "\n", "virtual_env", ".", "load_state_dict", "(", "save_dict", "[", "'model'", "]", ")", "\n", "real_env", "=", "env_factory", ".", "generate_real_env", "(", ")", "\n", "\n", "return", "virtual_env", ",", "real_env", ",", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.syn_env_evaluate_cartpole_vary_hp_2.train_test_agents": [[25, 49], ["range", "agents.agent_utils.select_agent", "agents.agent_utils.select_agent.train", "agents.agent_utils.select_agent.test", "print", "reward_list.append", "train_steps_needed.append", "episodes_needed.append", "str", "sum", "len"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.agents.agent_utils.select_agent", "home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.train", "home.repos.pwc.inspect_result.automl_learning_environments.agents.base_agent.BaseAgent.test"], ["", "def", "train_test_agents", "(", "train_env", ",", "test_env", ",", "config", ",", "agents_num", ")", ":", "\n", "    ", "reward_list", "=", "[", "]", "\n", "train_steps_needed", "=", "[", "]", "\n", "episodes_needed", "=", "[", "]", "\n", "\n", "# settings for comparability", "\n", "config", "[", "'agents'", "]", "[", "'ddqn_vary'", "]", "[", "'vary_hp'", "]", "=", "True", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'print_rate'", "]", "=", "10", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'early_out_num'", "]", "=", "10", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'train_episodes'", "]", "=", "1000", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'init_episodes'", "]", "=", "10", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'test_episodes'", "]", "=", "10", "\n", "config", "[", "'agents'", "]", "[", "'ddqn'", "]", "[", "'early_out_virtual_diff'", "]", "=", "0.01", "\n", "\n", "for", "i", "in", "range", "(", "agents_num", ")", ":", "\n", "        ", "agent", "=", "select_agent", "(", "config", "=", "config", ",", "agent_name", "=", "'DDQN_vary'", ")", "\n", "reward_train", ",", "episode_length", ",", "_", "=", "agent", ".", "train", "(", "env", "=", "train_env", ")", "\n", "reward", ",", "_", ",", "_", "=", "agent", ".", "test", "(", "env", "=", "test_env", ")", "\n", "print", "(", "'reward: '", "+", "str", "(", "reward", ")", ")", "\n", "reward_list", ".", "append", "(", "reward", ")", "\n", "train_steps_needed", ".", "append", "(", "[", "sum", "(", "episode_length", ")", "]", ")", "\n", "episodes_needed", ".", "append", "(", "[", "(", "len", "(", "reward_train", ")", ")", "]", ")", "\n", "\n", "", "return", "reward_list", ",", "train_steps_needed", ",", "episodes_needed", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cartpole_acrobot_success_perc.get_data": [[14, 70], ["len", "hpbandster.logged_results_to_HBS_result", "hpres.logged_results_to_HBS_result.get_all_runs", "hpres.logged_results_to_HBS_result.get_id2config_mapping", "enumerate", "print", "print", "print", "list_data.append", "numpy.zeros", "range", "numpy.mean", "numpy.std", "proc_data.append", "ts.append", "ast.literal_eval", "range", "data.append", "len", "numpy.array", "str", "str", "len", "statistics.mean", "statistics.stdev"], "function", ["None"], ["def", "get_data", "(", ")", ":", "\n", "    ", "list_data", "=", "[", "]", "\n", "for", "log_dir", "in", "LOG_DIRS", ":", "\n", "        ", "result", "=", "hpres", ".", "logged_results_to_HBS_result", "(", "log_dir", ")", "\n", "all_runs", "=", "result", ".", "get_all_runs", "(", ")", "\n", "id2conf", "=", "result", ".", "get_id2config_mapping", "(", ")", "\n", "\n", "# calculate avg. runtime", "\n", "ts", "=", "[", "]", "\n", "for", "i", ",", "run", "in", "enumerate", "(", "all_runs", ")", ":", "\n", "            ", "t_s", "=", "run", "[", "'time_stamps'", "]", "[", "'started'", "]", "\n", "t_f", "=", "run", "[", "'time_stamps'", "]", "[", "'finished'", "]", "\n", "ts", ".", "append", "(", "t_f", "-", "t_s", ")", "\n", "\n", "if", "i", ">=", "MAX_VALS", ":", "\n", "                ", "break", "\n", "\n", "", "", "print", "(", "log_dir", ")", "\n", "print", "(", "'mean [s]: '", "+", "str", "(", "statistics", ".", "mean", "(", "ts", ")", ")", ")", "\n", "print", "(", "'std [s]: '", "+", "str", "(", "statistics", ".", "stdev", "(", "ts", ")", ")", ")", "\n", "\n", "# copy data to list", "\n", "data", "=", "[", "]", "\n", "\n", "for", "run", "in", "all_runs", ":", "\n", "            ", "avg_rewards", "=", "ast", ".", "literal_eval", "(", "run", "[", "'info'", "]", "[", "'score_list'", "]", ")", "\n", "# print(avg_rewards)", "\n", "\n", "config_id", "=", "run", "[", "'config_id'", "]", "\n", "\n", "# handle timeout cases (impute missing values)", "\n", "if", "avg_rewards", "[", "0", "]", "<", "-", "1e5", "and", "avg_rewards", "[", "1", "]", ">", "-", "1e5", ":", "\n", "                ", "avg_rewards", "[", "0", "]", "=", "avg_rewards", "[", "1", "]", "\n", "", "for", "k", "in", "range", "(", "1", ",", "len", "(", "avg_rewards", ")", ")", ":", "\n", "                ", "if", "avg_rewards", "[", "k", "]", "<", "-", "1e5", "and", "avg_rewards", "[", "k", "-", "1", "]", ">", "-", "1e5", ":", "\n", "                    ", "avg_rewards", "[", "k", "]", "=", "avg_rewards", "[", "k", "-", "1", "]", "\n", "\n", "", "", "data", ".", "append", "(", "avg_rewards", ")", "\n", "", "list_data", ".", "append", "(", "data", ")", "\n", "\n", "# copy from list to numpy array", "\n", "", "proc_data", "=", "[", "]", "\n", "\n", "n", "=", "len", "(", "list_data", "[", "0", "]", "[", "0", "]", ")", "\n", "for", "data", "in", "list_data", ":", "\n", "        ", "np_data", "=", "np", ".", "zeros", "(", "[", "MAX_VALS", ",", "n", "]", ")", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "np_data", ")", ")", ":", "\n", "            ", "np_data", "[", "i", "]", "=", "np", ".", "array", "(", "data", "[", "i", "]", ")", "\n", "\n", "", "mean", "=", "np", ".", "mean", "(", "np_data", ",", "axis", "=", "0", ")", "\n", "std", "=", "np", ".", "std", "(", "np_data", ",", "axis", "=", "0", ")", "\n", "\n", "proc_data", ".", "append", "(", "(", "mean", ",", "std", ")", ")", "\n", "\n", "", "return", "proc_data", ",", "list_data", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cartpole_acrobot_success_perc.plot_data": [[72, 108], ["matplotlib.subplots", "zip", "enumerate", "matplotlib.legend", "matplotlib.xlim", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.savefig", "matplotlib.show", "matplotlib.plot", "matplotlib.plot", "matplotlib.fill_between", "print", "enumerate", "len", "matplotlib.plot", "range", "len"], "function", ["None"], ["", "def", "plot_data", "(", "proc_data", ",", "list_data", ",", "savefig_name", ")", ":", "\n", "    ", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", "dpi", "=", "600", ",", "figsize", "=", "(", "7", ",", "5", ")", ")", "\n", "colors", "=", "[", "'#1F77B4'", ",", "'#FF7F0E'", "]", "\n", "#", "\n", "# for mean, _ in data_w:", "\n", "#     plt.plot(mean_w)", "\n", "#     colors.append(plt.gca().lines[-1].get_color())", "\n", "\n", "thresh_data", "=", "[", "\n", "(", "[", "0", ",", "200", "]", ",", "[", "195", ",", "195", "]", ",", "'--'", ",", "colors", "[", "0", "]", ",", "2", ")", ",", "\n", "(", "[", "0", ",", "200", "]", ",", "[", "-", "100", ",", "-", "100", "]", ",", "'--'", ",", "colors", "[", "1", "]", ",", "2", ")", "\n", "]", "\n", "\n", "for", "(", "mean", ",", "std", ")", ",", "thresh", "in", "zip", "(", "proc_data", ",", "thresh_data", ")", ":", "\n", "        ", "plt", ".", "plot", "(", "thresh", "[", "0", "]", ",", "thresh", "[", "1", "]", ",", "thresh", "[", "2", "]", ",", "color", "=", "thresh", "[", "3", "]", ",", "linewidth", "=", "thresh", "[", "4", "]", ")", "\n", "plt", ".", "plot", "(", "mean", ",", "linewidth", "=", "2", ")", "\n", "\n", "# plt.plot([0, 200], [195, 195], '--', color=colors[0], linewidth=2)", "\n", "# plt.plot([0, 200], [-100, -100], '--', color=colors[1], linewidth=2)", "\n", "\n", "", "for", "mean", ",", "std", "in", "proc_data", ":", "\n", "        ", "plt", ".", "fill_between", "(", "x", "=", "range", "(", "len", "(", "mean", ")", ")", ",", "y1", "=", "mean", "-", "std", "*", "STD_MULT", ",", "y2", "=", "mean", "+", "std", "*", "STD_MULT", ",", "alpha", "=", "0.2", ")", "\n", "\n", "", "for", "i", ",", "dat", "in", "enumerate", "(", "list_data", ")", ":", "\n", "        ", "print", "(", "len", "(", "dat", ")", ")", "\n", "for", "k", ",", "avg_rewards", "in", "enumerate", "(", "dat", ")", ":", "\n", "            ", "if", "k", ">=", "20", ":", "\n", "                ", "continue", "\n", "", "plt", ".", "plot", "(", "avg_rewards", ",", "linewidth", "=", "0.3", ",", "color", "=", "colors", "[", "i", "]", ")", "\n", "\n", "", "", "plt", ".", "legend", "(", "[", "'CartPole-v0 solved threshold'", ",", "'CartPole-v0'", ",", "'Acrobot-v1 solved threshold'", ",", "'Acrobot-v1'", "]", ",", "loc", "=", "'lower right'", ")", "\n", "plt", ".", "xlim", "(", "0", ",", "199", ")", "\n", "plt", ".", "xlabel", "(", "'ES outer loop iteration'", ")", "\n", "plt", ".", "ylabel", "(", "'cumulative reward'", ")", "\n", "plt", ".", "savefig", "(", "savefig_name", ",", "bbox_inches", "=", "'tight'", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN.run_gtn_on_single_pc": [[14, 45], ["agents.GTN_master.GTN_Master", "agents.GTN_master.GTN_Master.clean_working_dir", "time.sleep", "multiprocessing.Process", "mp.Process.start", "p_list.append", "range", "agents.GTN_worker.GTN_Worker", "agents.GTN_master.GTN_Master.run", "agents.GTN_master.GTN_Master", "agents.GTN_master.GTN_Master.run", "multiprocessing.Process", "mp.Process.start", "p_list.append", "mp.Process.join"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_base.GTN_Base.clean_working_dir", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.run", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.run"], ["def", "run_gtn_on_single_pc", "(", "config", ")", ":", "\n", "    ", "def", "run_gtn_worker", "(", "id", ")", ":", "\n", "        ", "gtn", "=", "GTN_Worker", "(", "id", ")", "\n", "gtn", ".", "run", "(", ")", "\n", "\n", "", "def", "run_gtn_master", "(", "config", ")", ":", "\n", "        ", "gtn", "=", "GTN_Master", "(", "config", ")", "\n", "gtn", ".", "run", "(", ")", "\n", "\n", "", "p_list", "=", "[", "]", "\n", "\n", "# cleanup working directory from old files", "\n", "gtn_base", "=", "GTN_Master", "(", "config", ")", "\n", "gtn_base", ".", "clean_working_dir", "(", ")", "\n", "time", ".", "sleep", "(", "2", ")", "\n", "\n", "# first start master", "\n", "p", "=", "mp", ".", "Process", "(", "target", "=", "run_gtn_master", ",", "args", "=", "(", "config", ",", ")", ")", "\n", "p", ".", "start", "(", ")", "\n", "p_list", ".", "append", "(", "p", ")", "\n", "\n", "# then start workers", "\n", "num_workers", "=", "config", "[", "\"agents\"", "]", "[", "\"gtn\"", "]", "[", "\"num_workers\"", "]", "\n", "for", "id", "in", "range", "(", "num_workers", ")", ":", "\n", "        ", "p", "=", "mp", ".", "Process", "(", "target", "=", "run_gtn_worker", ",", "args", "=", "(", "id", ",", ")", ")", "\n", "p", ".", "start", "(", ")", "\n", "p_list", ".", "append", "(", "p", ")", "\n", "\n", "# wait till everything has finished", "\n", "", "for", "p", "in", "p_list", ":", "\n", "        ", "p", ".", "join", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN.run_gtn_on_multiple_pcs": [[47, 57], ["agents.GTN_master.GTN_Master", "agents.GTN_master.GTN_Master.clean_working_dir", "agents.GTN_master.GTN_Master.run", "agents.GTN_worker.GTN_Worker", "agents.GTN_worker.GTN_Worker.run", "ValueError"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_base.GTN_Base.clean_working_dir", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.run", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.run"], ["", "", "def", "run_gtn_on_multiple_pcs", "(", "config", ",", "id", ")", ":", "\n", "    ", "if", "id", "==", "-", "1", ":", "\n", "        ", "gtn_master", "=", "GTN_Master", "(", "config", ")", "\n", "gtn_master", ".", "clean_working_dir", "(", ")", "\n", "gtn_master", ".", "run", "(", ")", "\n", "", "elif", "id", ">=", "0", ":", "\n", "        ", "gtn_worker", "=", "GTN_Worker", "(", "id", ")", "\n", "gtn_worker", ".", "run", "(", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Invalid ID\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.DuelingDDQN.DuelingDDQN.__init__": [[15, 58], ["agents.base_agent.BaseAgent.__init__", "models.actor_critic.Critic_DuelingDQN().to", "models.actor_critic.Critic_DuelingDQN().to", "DuelingDDQN.DuelingDDQN.model_target.load_state_dict", "DuelingDDQN.DuelingDDQN.reset_optimizer", "DuelingDDQN.DuelingDDQN.model.state_dict", "models.icm_baseline.ICM", "models.actor_critic.Critic_DuelingDQN", "models.actor_critic.Critic_DuelingDQN", "env.has_discrete_action_space"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__", "home.repos.pwc.inspect_result.automl_learning_environments.agents.SAC.SAC.reset_optimizer", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.has_discrete_action_space"], ["    ", "def", "__init__", "(", "self", ",", "env", ",", "config", ",", "icm", "=", "False", ")", ":", "\n", "        ", "self", ".", "agent_name", "=", "\"duelingddqn\"", "\n", "\n", "super", "(", ")", ".", "__init__", "(", "agent_name", "=", "self", ".", "agent_name", ",", "env", "=", "env", ",", "config", "=", "config", ")", "\n", "\n", "ddqn_config", "=", "config", "[", "\"agents\"", "]", "[", "self", ".", "agent_name", "]", "\n", "self", ".", "full_config", "=", "config", "\n", "\n", "self", ".", "batch_size", "=", "ddqn_config", "[", "\"batch_size\"", "]", "\n", "self", ".", "rb_size", "=", "ddqn_config", "[", "\"rb_size\"", "]", "\n", "self", ".", "gamma", "=", "ddqn_config", "[", "\"gamma\"", "]", "\n", "self", ".", "lr", "=", "ddqn_config", "[", "\"lr\"", "]", "\n", "self", ".", "tau", "=", "ddqn_config", "[", "\"tau\"", "]", "\n", "self", ".", "eps", "=", "ddqn_config", "[", "\"eps_init\"", "]", "\n", "self", ".", "eps_init", "=", "ddqn_config", "[", "\"eps_init\"", "]", "\n", "self", ".", "eps_min", "=", "ddqn_config", "[", "\"eps_min\"", "]", "\n", "self", ".", "eps_decay", "=", "ddqn_config", "[", "\"eps_decay\"", "]", "\n", "\n", "self", ".", "model", "=", "Critic_DuelingDQN", "(", "self", ".", "state_dim", ",", "self", ".", "action_dim", ",", "self", ".", "agent_name", ",", "config", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "model_target", "=", "Critic_DuelingDQN", "(", "self", ".", "state_dim", ",", "self", ".", "action_dim", ",", "self", ".", "agent_name", ",", "config", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "model_target", ".", "load_state_dict", "(", "self", ".", "model", ".", "state_dict", "(", ")", ")", "\n", "\n", "self", ".", "reset_optimizer", "(", ")", "\n", "\n", "self", ".", "it", "=", "0", "\n", "\n", "self", ".", "icm", "=", "None", "\n", "if", "icm", ":", "\n", "            ", "icm_config", "=", "config", "[", "\"agents\"", "]", "[", "\"icm\"", "]", "\n", "self", ".", "icm_lr", "=", "icm_config", "[", "\"lr\"", "]", "\n", "self", ".", "icm_beta", "=", "icm_config", "[", "\"beta\"", "]", "\n", "self", ".", "icm_eta", "=", "icm_config", "[", "\"eta\"", "]", "\n", "self", ".", "icm_feature_dim", "=", "icm_config", "[", "\"feature_dim\"", "]", "\n", "self", ".", "icm_hidden_dim", "=", "icm_config", "[", "\"hidden_size\"", "]", "\n", "self", ".", "icm", "=", "ICM", "(", "state_dim", "=", "self", ".", "state_dim", ",", "\n", "action_dim", "=", "self", ".", "action_dim", ",", "\n", "has_discrete_actions", "=", "env", ".", "has_discrete_action_space", "(", ")", ",", "\n", "learning_rate", "=", "self", ".", "icm_lr", ",", "\n", "beta", "=", "self", ".", "icm_beta", ",", "\n", "eta", "=", "self", ".", "icm_eta", ",", "\n", "feature_dim", "=", "self", ".", "icm_feature_dim", ",", "\n", "hidden_size", "=", "self", ".", "icm_hidden_dim", ",", "\n", "device", "=", "self", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.DuelingDDQN.DuelingDDQN.learn": [[59, 95], ["replay_buffer.sample", "utils.to_one_hot_encoding.squeeze", "actions.squeeze.squeeze.squeeze", "utils.to_one_hot_encoding.squeeze", "rewards.squeeze.squeeze.squeeze", "dones.squeeze.squeeze.squeeze", "env.has_discrete_state_space", "DuelingDDQN.DuelingDDQN.model", "DuelingDDQN.DuelingDDQN.model", "DuelingDDQN.DuelingDDQN.model_target", "DuelingDDQN.DuelingDDQN.gather().squeeze", "DuelingDDQN.DuelingDDQN.gather().squeeze", "torch.mse_loss", "torch.mse_loss", "DuelingDDQN.DuelingDDQN.optimizer.zero_grad", "torch.mse_loss.backward", "DuelingDDQN.DuelingDDQN.optimizer.step", "zip", "utils.to_one_hot_encoding", "utils.to_one_hot_encoding", "DuelingDDQN.DuelingDDQN.icm.train", "DuelingDDQN.DuelingDDQN.icm.compute_intrinsic_rewards().squeeze", "expected_q_value.detach", "DuelingDDQN.DuelingDDQN.model_target.parameters", "DuelingDDQN.DuelingDDQN.model.parameters", "target_param.data.copy_", "DuelingDDQN.DuelingDDQN.gather", "DuelingDDQN.DuelingDDQN.gather", "DuelingDDQN.DuelingDDQN.icm.compute_intrinsic_rewards", "actions.squeeze.squeeze.long().unsqueeze", "[].unsqueeze", "actions.squeeze.squeeze.long", "DuelingDDQN.DuelingDDQN.max"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.None.utils.ReplayBuffer.sample", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.has_discrete_state_space", "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditEnv.step", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.to_one_hot_encoding", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.to_one_hot_encoding", "home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.train", "home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.compute_intrinsic_rewards"], ["", "", "def", "learn", "(", "self", ",", "replay_buffer", ",", "env", ",", "episode", ")", ":", "\n", "        ", "self", ".", "it", "+=", "1", "\n", "\n", "states", ",", "actions", ",", "next_states", ",", "rewards", ",", "dones", "=", "replay_buffer", ".", "sample", "(", "self", ".", "batch_size", ")", "\n", "\n", "states", "=", "states", ".", "squeeze", "(", ")", "\n", "actions", "=", "actions", ".", "squeeze", "(", ")", "\n", "next_states", "=", "next_states", ".", "squeeze", "(", ")", "\n", "rewards", "=", "rewards", ".", "squeeze", "(", ")", "\n", "dones", "=", "dones", ".", "squeeze", "(", ")", "\n", "\n", "if", "env", ".", "has_discrete_state_space", "(", ")", ":", "\n", "            ", "states", "=", "to_one_hot_encoding", "(", "states", ",", "self", ".", "state_dim", ")", "\n", "next_states", "=", "to_one_hot_encoding", "(", "next_states", ",", "self", ".", "state_dim", ")", "\n", "\n", "", "if", "self", ".", "icm", ":", "\n", "            ", "self", ".", "icm", ".", "train", "(", "states", ",", "next_states", ",", "actions", ")", "\n", "rewards", "+=", "self", ".", "icm", ".", "compute_intrinsic_rewards", "(", "states", ",", "next_states", ",", "actions", ")", ".", "squeeze", "(", ")", "\n", "\n", "", "q_values", "=", "self", ".", "model", "(", "states", ")", "\n", "next_q_values", "=", "self", ".", "model", "(", "next_states", ")", "\n", "next_q_values_target", "=", "self", ".", "model_target", "(", "next_states", ")", "\n", "\n", "q_value", "=", "q_values", ".", "gather", "(", "1", ",", "actions", ".", "long", "(", ")", ".", "unsqueeze", "(", "1", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "next_q_value", "=", "next_q_values_target", ".", "gather", "(", "1", ",", "next_q_values", ".", "max", "(", "1", ")", "[", "1", "]", ".", "unsqueeze", "(", "1", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "expected_q_value", "=", "rewards", "+", "self", ".", "gamma", "*", "next_q_value", "*", "(", "1", "-", "dones", ")", "\n", "loss", "=", "F", ".", "mse_loss", "(", "expected_q_value", ".", "detach", "(", ")", ",", "q_value", ")", "\n", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "# target network update", "\n", "for", "target_param", ",", "param", "in", "zip", "(", "self", ".", "model_target", ".", "parameters", "(", ")", ",", "self", ".", "model", ".", "parameters", "(", ")", ")", ":", "\n", "            ", "target_param", ".", "data", ".", "copy_", "(", "self", ".", "tau", "*", "param", "+", "(", "1", "-", "self", ".", "tau", ")", "*", "target_param", ")", "\n", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.DuelingDDQN.DuelingDDQN.select_train_action": [[96, 104], ["random.random", "env.get_random_action", "env.has_discrete_state_space", "DuelingDDQN.DuelingDDQN.model", "torch.argmax().unsqueeze().detach", "torch.argmax().unsqueeze().detach", "torch.argmax().unsqueeze().detach", "torch.argmax().unsqueeze().detach", "utils.to_one_hot_encoding", "utils.to_one_hot_encoding.to", "torch.argmax().unsqueeze", "torch.argmax().unsqueeze", "torch.argmax().unsqueeze", "torch.argmax().unsqueeze", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.get_random_action", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.has_discrete_state_space", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.to_one_hot_encoding"], ["", "def", "select_train_action", "(", "self", ",", "state", ",", "env", ",", "episode", ")", ":", "\n", "        ", "if", "random", ".", "random", "(", ")", "<", "self", ".", "eps", ":", "\n", "            ", "return", "env", ".", "get_random_action", "(", ")", "\n", "", "else", ":", "\n", "            ", "if", "env", ".", "has_discrete_state_space", "(", ")", ":", "\n", "                ", "state", "=", "to_one_hot_encoding", "(", "state", ",", "self", ".", "state_dim", ")", "\n", "", "qvals", "=", "self", ".", "model", "(", "state", ".", "to", "(", "self", ".", "device", ")", ")", "\n", "return", "torch", ".", "argmax", "(", "qvals", ")", ".", "unsqueeze", "(", "0", ")", ".", "detach", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.DuelingDDQN.DuelingDDQN.select_test_action": [[105, 110], ["env.has_discrete_state_space", "DuelingDDQN.DuelingDDQN.model", "torch.argmax().unsqueeze().detach", "torch.argmax().unsqueeze().detach", "torch.argmax().unsqueeze().detach", "torch.argmax().unsqueeze().detach", "utils.to_one_hot_encoding", "utils.to_one_hot_encoding.to", "torch.argmax().unsqueeze", "torch.argmax().unsqueeze", "torch.argmax().unsqueeze", "torch.argmax().unsqueeze", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.has_discrete_state_space", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.to_one_hot_encoding"], ["", "", "def", "select_test_action", "(", "self", ",", "state", ",", "env", ")", ":", "\n", "        ", "if", "env", ".", "has_discrete_state_space", "(", ")", ":", "\n", "            ", "state", "=", "to_one_hot_encoding", "(", "state", ",", "self", ".", "state_dim", ")", "\n", "", "qvals", "=", "self", ".", "model", "(", "state", ".", "to", "(", "self", ".", "device", ")", ")", "\n", "return", "torch", ".", "argmax", "(", "qvals", ")", ".", "unsqueeze", "(", "0", ")", ".", "detach", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.DuelingDDQN.DuelingDDQN.update_parameters_per_episode": [[111, 117], ["max"], "methods", ["None"], ["", "def", "update_parameters_per_episode", "(", "self", ",", "episode", ")", ":", "\n", "        ", "if", "episode", "==", "0", ":", "\n", "            ", "self", ".", "eps", "=", "self", ".", "eps_init", "\n", "", "else", ":", "\n", "            ", "self", ".", "eps", "*=", "self", ".", "eps_decay", "\n", "self", ".", "eps", "=", "max", "(", "self", ".", "eps", ",", "self", ".", "eps_min", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.DuelingDDQN.DuelingDDQN.reset_optimizer": [[118, 120], ["torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "DuelingDDQN.DuelingDDQN.model.parameters"], "methods", ["None"], ["", "", "def", "reset_optimizer", "(", "self", ")", ":", "\n", "        ", "self", ".", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "self", ".", "model", ".", "parameters", "(", ")", ",", "lr", "=", "self", ".", "lr", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.SARSA.SARSA.__init__": [[13, 35], ["agents.base_agent.BaseAgent.__init__", "numpy.zeros", "numpy.zeros", "range"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", ",", "config", ",", "count_based", "=", "False", ")", ":", "\n", "        ", "self", ".", "agent_name", "=", "\"sarsa\"", "\n", "super", "(", ")", ".", "__init__", "(", "agent_name", "=", "self", ".", "agent_name", ",", "env", "=", "env", ",", "config", "=", "config", ")", "\n", "\n", "ql_config", "=", "config", "[", "\"agents\"", "]", "[", "self", ".", "agent_name", "]", "\n", "\n", "self", ".", "batch_size", "=", "ql_config", "[", "\"batch_size\"", "]", "\n", "self", ".", "alpha", "=", "ql_config", "[", "\"alpha\"", "]", "\n", "self", ".", "gamma", "=", "ql_config", "[", "\"gamma\"", "]", "\n", "self", ".", "eps_init", "=", "ql_config", "[", "\"eps_init\"", "]", "\n", "self", ".", "eps_min", "=", "ql_config", "[", "\"eps_min\"", "]", "\n", "self", ".", "eps_decay", "=", "ql_config", "[", "\"eps_decay\"", "]", "\n", "self", ".", "q_table", "=", "[", "[", "0", "]", "*", "self", ".", "action_dim", "for", "_", "in", "range", "(", "self", ".", "state_dim", ")", "]", "\n", "\n", "self", ".", "count_based", "=", "count_based", "\n", "\n", "self", ".", "it", "=", "0", "\n", "\n", "if", "self", ".", "count_based", ":", "\n", "            ", "self", ".", "beta", "=", "ql_config", "[", "\"beta\"", "]", "\n", "self", ".", "visitation_table", "=", "np", ".", "zeros", "(", "(", "self", ".", "state_dim", ",", "self", ".", "action_dim", ")", ")", "# n(s,a)", "\n", "self", ".", "t_hat", "=", "np", ".", "zeros", "(", "(", "self", ".", "state_dim", ",", "self", ".", "action_dim", ",", "self", ".", "state_dim", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.SARSA.SARSA.learn": [[36, 61], ["range", "replay_buffer.clear", "replay_buffer.sample", "int", "int", "SARSA.SARSA.select_train_action", "int", "int", "reward.item.item.item", "done.item.item.item", "int.item", "int.item", "int.item", "int.item", "math.sqrt"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.None.utils.ReplayBuffer.clear", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.ReplayBuffer.sample", "home.repos.pwc.inspect_result.automl_learning_environments.agents.PPO.PPO.select_train_action"], ["", "", "def", "learn", "(", "self", ",", "replay_buffer", ",", "env", ",", "episode", ")", ":", "\n", "        ", "self", ".", "it", "+=", "1", "\n", "\n", "# if self.it % 5000 == 0:", "\n", "#     self.plot_q_function(env)", "\n", "\n", "for", "_", "in", "range", "(", "self", ".", "batch_size", ")", ":", "\n", "            ", "state", ",", "action", ",", "next_state", ",", "reward", ",", "done", "=", "replay_buffer", ".", "sample", "(", "1", ")", "\n", "state", "=", "int", "(", "state", ".", "item", "(", ")", ")", "\n", "action", "=", "int", "(", "action", ".", "item", "(", ")", ")", "\n", "next_action", "=", "self", ".", "select_train_action", "(", "state", "=", "next_state", ",", "env", "=", "env", ",", "episode", "=", "episode", ")", "\n", "next_state", "=", "int", "(", "next_state", ".", "item", "(", ")", ")", "\n", "next_action", "=", "int", "(", "next_action", ".", "item", "(", ")", ")", "\n", "reward", "=", "reward", ".", "item", "(", ")", "\n", "done", "=", "done", ".", "item", "(", ")", "\n", "\n", "if", "self", ".", "count_based", ":", "\n", "                ", "self", ".", "visitation_table", "[", "state", "]", "[", "action", "]", "+=", "1", "\n", "intrinsic_reward", "=", "self", ".", "beta", "/", "(", "math", ".", "sqrt", "(", "self", ".", "visitation_table", "[", "state", "]", "[", "action", "]", ")", "+", "1e-9", ")", "\n", "reward", "+=", "intrinsic_reward", "\n", "\n", "", "delta", "=", "reward", "+", "self", ".", "gamma", "*", "self", ".", "q_table", "[", "next_state", "]", "[", "next_action", "]", "*", "(", "done", "<", "0.5", ")", "-", "self", ".", "q_table", "[", "state", "]", "[", "action", "]", "\n", "self", ".", "q_table", "[", "state", "]", "[", "action", "]", "+=", "self", ".", "alpha", "*", "delta", "\n", "\n", "", "replay_buffer", ".", "clear", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.SARSA.SARSA.plot_q_function": [[62, 74], ["print", "range", "range", "print", "max"], "methods", ["None"], ["", "def", "plot_q_function", "(", "self", ",", "env", ")", ":", "\n", "# m = len(env.env.grid)", "\n", "# n = len(env.env.grid[0])", "\n", "        ", "m", "=", "3", "\n", "n", "=", "4", "\n", "\n", "print", "(", "'----'", ")", "\n", "for", "i", "in", "range", "(", "m", ")", ":", "\n", "            ", "strng", "=", "''", "\n", "for", "k", "in", "range", "(", "n", ")", ":", "\n", "                ", "strng", "+=", "' {:3f}'", ".", "format", "(", "max", "(", "self", ".", "q_table", "[", "i", "*", "n", "+", "k", "]", ")", ")", "\n", "", "print", "(", "strng", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.SARSA.SARSA.select_train_action": [[75, 82], ["random.random", "env.get_random_action", "torch.tensor", "torch.argmax().unsqueeze().detach", "torch.argmax().unsqueeze", "int", "state.item", "torch.argmax"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.get_random_action"], ["", "", "def", "select_train_action", "(", "self", ",", "state", ",", "env", ",", "episode", ")", ":", "\n", "        ", "if", "random", ".", "random", "(", ")", "<", "self", ".", "eps", ":", "\n", "            ", "action", "=", "env", ".", "get_random_action", "(", ")", "\n", "return", "action", "\n", "", "else", ":", "\n", "            ", "q_vals", "=", "torch", ".", "tensor", "(", "self", ".", "q_table", "[", "int", "(", "state", ".", "item", "(", ")", ")", "]", ")", "\n", "return", "torch", ".", "argmax", "(", "q_vals", ")", ".", "unsqueeze", "(", "0", ")", ".", "detach", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.SARSA.SARSA.select_test_action": [[83, 86], ["torch.tensor", "torch.argmax().unsqueeze().detach", "torch.argmax().unsqueeze", "int", "state.item", "torch.argmax"], "methods", ["None"], ["", "", "def", "select_test_action", "(", "self", ",", "state", ",", "env", ")", ":", "\n", "        ", "q_vals", "=", "torch", ".", "tensor", "(", "self", ".", "q_table", "[", "int", "(", "state", ".", "item", "(", ")", ")", "]", ")", "\n", "return", "torch", ".", "argmax", "(", "q_vals", ")", ".", "unsqueeze", "(", "0", ")", ".", "detach", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.SARSA.SARSA.update_parameters_per_episode": [[87, 93], ["max"], "methods", ["None"], ["", "def", "update_parameters_per_episode", "(", "self", ",", "episode", ")", ":", "\n", "        ", "if", "episode", "==", "0", ":", "\n", "            ", "self", ".", "eps", "=", "self", ".", "eps_init", "\n", "", "else", ":", "\n", "            ", "self", ".", "eps", "*=", "self", ".", "eps_decay", "\n", "self", ".", "eps", "=", "max", "(", "self", ".", "eps", ",", "self", ".", "eps_min", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_master.GTN_Master.__init__": [[16, 75], ["agents.GTN_base.GTN_Base.__init__", "envs.env_factory.EnvFactory", "generate_synthetic_env_fn", "GTN_master.GTN_Master.env_factory.generate_real_env", "GTN_master.GTN_Master.get_model_file_name", "os.makedirs", "print", "generate_synthetic_env_fn", "generate_synthetic_env_fn", "str", "str", "float", "NotImplementedError", "range", "range", "os.path.join", "os.path.join", "os.getcwd", "str", "random.choices"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_real_env", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_master.GTN_Master.get_model_file_name"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "bohb_id", "=", "-", "1", ",", "bohb_working_dir", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "bohb_id", ")", "\n", "self", ".", "config", "=", "config", "\n", "self", ".", "device", "=", "config", "[", "\"device\"", "]", "\n", "self", ".", "env_name", "=", "config", "[", "'env_name'", "]", "\n", "\n", "gtn_config", "=", "config", "[", "\"agents\"", "]", "[", "\"gtn\"", "]", "\n", "self", ".", "max_iterations", "=", "gtn_config", "[", "\"max_iterations\"", "]", "\n", "self", ".", "agent_name", "=", "gtn_config", "[", "\"agent_name\"", "]", "\n", "self", ".", "num_workers", "=", "gtn_config", "[", "\"num_workers\"", "]", "\n", "self", ".", "step_size", "=", "gtn_config", "[", "\"step_size\"", "]", "\n", "self", ".", "nes_step_size", "=", "gtn_config", "[", "\"nes_step_size\"", "]", "\n", "self", ".", "weight_decay", "=", "gtn_config", "[", "\"weight_decay\"", "]", "\n", "self", ".", "score_transform_type", "=", "gtn_config", "[", "\"score_transform_type\"", "]", "\n", "self", ".", "time_mult", "=", "gtn_config", "[", "\"time_mult\"", "]", "\n", "self", ".", "time_max", "=", "gtn_config", "[", "\"time_max\"", "]", "\n", "self", ".", "time_sleep_master", "=", "gtn_config", "[", "\"time_sleep_master\"", "]", "\n", "self", ".", "quit_when_solved", "=", "gtn_config", "[", "\"quit_when_solved\"", "]", "\n", "self", ".", "synthetic_env_type", "=", "gtn_config", "[", "\"synthetic_env_type\"", "]", "\n", "self", ".", "unsolved_weight", "=", "gtn_config", "[", "\"unsolved_weight\"", "]", "\n", "\n", "# make it faster on single PC", "\n", "if", "gtn_config", "[", "\"mode\"", "]", "==", "'single'", ":", "\n", "            ", "self", ".", "time_sleep_master", "/=", "10", "\n", "\n", "# to store results from workers", "\n", "", "self", ".", "time_elapsed_list", "=", "[", "None", "]", "*", "self", ".", "num_workers", "# for debugging", "\n", "self", ".", "score_list", "=", "[", "None", "]", "*", "self", ".", "num_workers", "\n", "self", ".", "score_orig_list", "=", "[", "None", "]", "*", "self", ".", "num_workers", "# for debugging", "\n", "self", ".", "score_transform_list", "=", "[", "None", "]", "*", "self", ".", "num_workers", "\n", "\n", "# to keep track of the reference virtual env", "\n", "self", ".", "env_factory", "=", "EnvFactory", "(", "config", ")", "\n", "if", "self", ".", "synthetic_env_type", "==", "0", ":", "\n", "            ", "generate_synthetic_env_fn", "=", "self", ".", "env_factory", ".", "generate_virtual_env", "\n", "", "elif", "self", ".", "synthetic_env_type", "==", "1", ":", "\n", "            ", "generate_synthetic_env_fn", "=", "self", ".", "env_factory", ".", "generate_reward_env", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\"Unknown synthetic_env_type value: \"", "+", "str", "(", "self", ".", "synthetic_env_type", ")", ")", "\n", "\n", "", "self", ".", "synthetic_env_orig", "=", "generate_synthetic_env_fn", "(", "print_str", "=", "'GTN_Base: '", ")", "\n", "self", ".", "synthetic_env_list", "=", "[", "generate_synthetic_env_fn", "(", "print_str", "=", "'GTN_Master: '", ")", "for", "_", "in", "range", "(", "self", ".", "num_workers", ")", "]", "\n", "self", ".", "eps_list", "=", "[", "generate_synthetic_env_fn", "(", "print_str", "=", "'GTN_Master: '", ")", "for", "_", "in", "range", "(", "self", ".", "num_workers", ")", "]", "\n", "\n", "# for early out", "\n", "self", ".", "avg_runtime", "=", "None", "\n", "self", ".", "real_env", "=", "self", ".", "env_factory", ".", "generate_real_env", "(", ")", "\n", "\n", "# to store models", "\n", "if", "bohb_working_dir", ":", "\n", "            ", "self", ".", "model_dir", "=", "str", "(", "os", ".", "path", ".", "join", "(", "bohb_working_dir", ",", "'GTN_models_'", "+", "self", ".", "env_name", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "model_dir", "=", "str", "(", "os", ".", "path", ".", "join", "(", "os", ".", "getcwd", "(", ")", ",", "\"results\"", ",", "'GTN_models_'", "+", "self", ".", "env_name", ")", ")", "\n", "", "self", ".", "model_name", "=", "self", ".", "get_model_file_name", "(", "self", ".", "env_name", "+", "'_'", "+", "''", ".", "join", "(", "random", ".", "choices", "(", "string", ".", "ascii_uppercase", "+", "string", ".", "digits", ",", "k", "=", "6", ")", ")", "+", "'.pt'", ")", "\n", "self", ".", "best_score", "=", "-", "float", "(", "'Inf'", ")", "\n", "\n", "os", ".", "makedirs", "(", "self", ".", "model_dir", ",", "exist_ok", "=", "True", ")", "\n", "\n", "print", "(", "'Starting GTN Master with bohb_id {}'", ".", "format", "(", "bohb_id", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_master.GTN_Master.get_model_file_name": [[78, 80], ["os.path.join"], "methods", ["None"], ["", "def", "get_model_file_name", "(", "self", ",", "file_name", ")", ":", "\n", "        ", "return", "os", ".", "path", ".", "join", "(", "self", ".", "model_dir", ",", "file_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_master.GTN_Master.run": [[81, 117], ["range", "print", "GTN_master.GTN_Master.print_statistics", "time.time", "print", "print", "GTN_master.GTN_Master.write_worker_inputs", "print", "GTN_master.GTN_Master.read_worker_results", "numpy.mean", "mean_score_orig_list.append", "GTN_master.GTN_Master.save_good_model", "print", "GTN_master.GTN_Master.score_transform", "print", "GTN_master.GTN_Master.update_env", "print", "GTN_master.GTN_Master.print_statistics", "len", "print", "numpy.mean", "str", "str", "str", "str", "str", "str", "time.time", "str", "time.time", "time.time", "time.time", "time.time", "time.time", "time.time"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_master.GTN_Master.print_statistics", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_master.GTN_Master.write_worker_inputs", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_master.GTN_Master.read_worker_results", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_master.GTN_Master.save_good_model", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_master.GTN_Master.score_transform", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_master.GTN_Master.update_env", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_master.GTN_Master.print_statistics"], ["", "def", "run", "(", "self", ")", ":", "\n", "        ", "mean_score_orig_list", "=", "[", "]", "\n", "\n", "for", "it", "in", "range", "(", "self", ".", "max_iterations", ")", ":", "\n", "            ", "t1", "=", "time", ".", "time", "(", ")", "\n", "print", "(", "'-- Master: Iteration '", "+", "str", "(", "it", ")", "+", "' '", "+", "str", "(", "time", ".", "time", "(", ")", "-", "t1", ")", ")", "\n", "print", "(", "'-- Master: write worker inputs'", "+", "' '", "+", "str", "(", "time", ".", "time", "(", ")", "-", "t1", ")", ")", "\n", "self", ".", "write_worker_inputs", "(", "it", ")", "\n", "print", "(", "'-- Master: read worker results'", "+", "' '", "+", "str", "(", "time", ".", "time", "(", ")", "-", "t1", ")", ")", "\n", "self", ".", "read_worker_results", "(", ")", "\n", "\n", "mean_score", "=", "np", ".", "mean", "(", "self", ".", "score_orig_list", ")", "\n", "mean_score_orig_list", ".", "append", "(", "mean_score", ")", "\n", "solved_flag", "=", "self", ".", "save_good_model", "(", "mean_score", ")", "\n", "\n", "if", "solved_flag", "and", "self", ".", "quit_when_solved", ":", "\n", "                ", "print", "(", "'ENV SOLVED'", ")", "\n", "# self.bohb_next_run_counter += 1", "\n", "break", "\n", "\n", "", "print", "(", "'-- Master: rank transform'", "+", "' '", "+", "str", "(", "time", ".", "time", "(", ")", "-", "t1", ")", ")", "\n", "self", ".", "score_transform", "(", ")", "\n", "print", "(", "'-- Master: update env'", "+", "' '", "+", "str", "(", "time", ".", "time", "(", ")", "-", "t1", ")", ")", "\n", "self", ".", "update_env", "(", ")", "\n", "print", "(", "'-- Master: print statistics'", "+", "' '", "+", "str", "(", "time", ".", "time", "(", ")", "-", "t1", ")", ")", "\n", "self", ".", "print_statistics", "(", "it", "=", "it", ",", "time_elapsed", "=", "time", ".", "time", "(", ")", "-", "t1", ")", "\n", "\n", "", "print", "(", "'Master quitting'", ")", "\n", "\n", "self", ".", "print_statistics", "(", "it", "=", "-", "1", ",", "time_elapsed", "=", "-", "1", ")", "\n", "\n", "# error handling", "\n", "if", "len", "(", "mean_score_orig_list", ")", ">", "0", ":", "\n", "            ", "return", "np", ".", "mean", "(", "self", ".", "score_orig_list", ")", ",", "mean_score_orig_list", ",", "self", ".", "model_name", "\n", "", "else", ":", "\n", "            ", "return", "1e9", ",", "mean_score_orig_list", ",", "self", ".", "model_name", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_master.GTN_Master.save_good_model": [[118, 132], ["GTN_master.GTN_Master.synthetic_env_orig.is_virtual_env", "GTN_master.GTN_Master.save_model", "GTN_master.GTN_Master.save_model", "GTN_master.GTN_Master.real_env.get_solved_reward"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.is_virtual_env", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_master.GTN_Master.save_model", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_master.GTN_Master.save_model", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.get_solved_reward"], ["", "", "def", "save_good_model", "(", "self", ",", "mean_score", ")", ":", "\n", "        ", "if", "self", ".", "synthetic_env_orig", ".", "is_virtual_env", "(", ")", ":", "\n", "            ", "if", "mean_score", ">", "self", ".", "real_env", ".", "get_solved_reward", "(", ")", "and", "mean_score", ">", "self", ".", "best_score", ":", "\n", "                ", "self", ".", "save_model", "(", ")", "\n", "self", ".", "best_score", "=", "mean_score", "\n", "return", "True", "\n", "", "", "else", ":", "\n", "# we save all models and select the best from the log", "\n", "# whether we can solve an environment is irrelevant for reward_env since we optimize for speed here", "\n", "            ", "if", "mean_score", ">", "self", ".", "best_score", ":", "\n", "                ", "self", ".", "save_model", "(", ")", "\n", "self", ".", "best_score", "=", "mean_score", "\n", "\n", "", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_master.GTN_Master.save_model": [[133, 140], ["GTN_master.GTN_Master.synthetic_env_orig.state_dict", "os.path.join", "print", "torch.save", "torch.save", "torch.save", "torch.save", "str"], "methods", ["None"], ["", "def", "save_model", "(", "self", ")", ":", "\n", "        ", "save_dict", "=", "{", "}", "\n", "save_dict", "[", "'model'", "]", "=", "self", ".", "synthetic_env_orig", ".", "state_dict", "(", ")", "\n", "save_dict", "[", "'config'", "]", "=", "self", ".", "config", "\n", "save_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "model_dir", ",", "self", ".", "model_name", ")", "\n", "print", "(", "'save model: '", "+", "str", "(", "save_path", ")", ")", "\n", "torch", ".", "save", "(", "save_dict", ",", "save_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_master.GTN_Master.calc_worker_timeout": [[141, 146], ["statistics.mean"], "methods", ["None"], ["", "def", "calc_worker_timeout", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "time_elapsed_list", "[", "0", "]", "is", "None", ":", "\n", "            ", "return", "self", ".", "time_max", "\n", "", "else", ":", "\n", "            ", "return", "statistics", ".", "mean", "(", "self", ".", "time_elapsed_list", ")", "*", "self", ".", "time_mult", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_master.GTN_Master.write_worker_inputs": [[147, 177], ["GTN_master.GTN_Master.calc_worker_timeout", "print", "range", "GTN_master.GTN_Master.get_input_file_name", "GTN_master.GTN_Master.get_input_check_file_name", "os.path.isfile", "time.sleep", "GTN_master.GTN_Master.synthetic_env_orig.state_dict", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "str", "time.sleep"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_master.GTN_Master.calc_worker_timeout", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_base.GTN_Base.get_input_file_name", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_base.GTN_Base.get_input_check_file_name"], ["", "", "def", "write_worker_inputs", "(", "self", ",", "it", ")", ":", "\n", "        ", "timeout", "=", "self", ".", "calc_worker_timeout", "(", ")", "\n", "print", "(", "'timeout: '", "+", "str", "(", "timeout", ")", ")", "\n", "\n", "for", "id", "in", "range", "(", "self", ".", "num_workers", ")", ":", "\n", "\n", "            ", "file_name", "=", "self", ".", "get_input_file_name", "(", "id", "=", "id", ")", "\n", "check_file_name", "=", "self", ".", "get_input_check_file_name", "(", "id", "=", "id", ")", "\n", "\n", "# wait until worker has deleted the file (i.e. acknowledged the previous input)", "\n", "while", "os", ".", "path", ".", "isfile", "(", "file_name", ")", ":", "\n", "                ", "time", ".", "sleep", "(", "self", ".", "time_sleep_master", ")", "\n", "\n", "", "time", ".", "sleep", "(", "self", ".", "time_sleep_master", ")", "\n", "\n", "# if we are not using bohb, shut everything down after last iteration", "\n", "if", "self", ".", "bohb_id", "<", "0", ":", "\n", "                ", "quit_flag", "=", "it", "==", "self", ".", "max_iterations", "-", "1", "\n", "", "else", ":", "\n", "                ", "quit_flag", "=", "False", "\n", "\n", "", "data", "=", "{", "}", "\n", "data", "[", "'timeout'", "]", "=", "timeout", "\n", "data", "[", "'quit_flag'", "]", "=", "quit_flag", "\n", "data", "[", "'config'", "]", "=", "self", ".", "config", "\n", "data", "[", "'synthetic_env_orig'", "]", "=", "self", ".", "synthetic_env_orig", ".", "state_dict", "(", ")", "\n", "# data['bohb_next_run_counter'] = self.bohb_next_run_counter", "\n", "\n", "torch", ".", "save", "(", "data", ",", "file_name", ")", "\n", "torch", ".", "save", "(", "{", "}", ",", "check_file_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_master.GTN_Master.read_worker_results": [[178, 196], ["range", "GTN_master.GTN_Master.get_result_file_name", "GTN_master.GTN_Master.get_result_check_file_name", "torch.load", "torch.load", "torch.load", "torch.load", "GTN_master.GTN_Master.eps_list[].load_state_dict", "GTN_master.GTN_Master.synthetic_env_list[].load_state_dict", "os.remove", "os.remove", "os.path.isfile", "time.sleep"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_base.GTN_Base.get_result_file_name", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_base.GTN_Base.get_result_check_file_name"], ["", "", "def", "read_worker_results", "(", "self", ")", ":", "\n", "        ", "for", "id", "in", "range", "(", "self", ".", "num_workers", ")", ":", "\n", "            ", "file_name", "=", "self", ".", "get_result_file_name", "(", "id", ")", "\n", "check_file_name", "=", "self", ".", "get_result_check_file_name", "(", "id", ")", "\n", "\n", "# wait until worker has finished calculations", "\n", "while", "not", "os", ".", "path", ".", "isfile", "(", "check_file_name", ")", ":", "\n", "                ", "time", ".", "sleep", "(", "self", ".", "time_sleep_master", ")", "\n", "\n", "", "data", "=", "torch", ".", "load", "(", "file_name", ")", "\n", "self", ".", "time_elapsed_list", "[", "id", "]", "=", "data", "[", "'time_elapsed'", "]", "\n", "self", ".", "score_list", "[", "id", "]", "=", "data", "[", "'score'", "]", "\n", "self", ".", "eps_list", "[", "id", "]", ".", "load_state_dict", "(", "data", "[", "'eps'", "]", ")", "\n", "self", ".", "score_orig_list", "[", "id", "]", "=", "data", "[", "'score_orig'", "]", "\n", "self", ".", "synthetic_env_list", "[", "id", "]", ".", "load_state_dict", "(", "data", "[", "'synthetic_env'", "]", ")", "# for debugging", "\n", "\n", "os", ".", "remove", "(", "check_file_name", ")", "\n", "os", ".", "remove", "(", "file_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_master.GTN_Master.score_transform": [[197, 266], ["numpy.asarray", "numpy.asarray", "scores.astype.astype.tolist", "numpy.argsort", "len", "range", "min", "len", "numpy.argsort", "range", "scores.astype.astype.astype", "range", "max", "max", "min", "max", "sum", "numpy.zeros", "numpy.mean", "numpy.where", "numpy.log", "numpy.log", "numpy.argmax", "sum", "numpy.zeros", "numpy.mean", "numpy.where", "ValueError", "sum", "numpy.argmax", "max", "sum", "str", "max"], "methods", ["None"], ["", "", "def", "score_transform", "(", "self", ")", ":", "\n", "        ", "scores", "=", "np", ".", "asarray", "(", "self", ".", "score_list", ")", "\n", "scores_orig", "=", "np", ".", "asarray", "(", "self", ".", "score_orig_list", ")", "\n", "\n", "if", "self", ".", "score_transform_type", "==", "0", ":", "\n", "# convert [1, 0, 5] to [0.2, 0, 1]", "\n", "            ", "scores", "=", "(", "scores", "-", "min", "(", "scores", ")", ")", "/", "(", "max", "(", "scores", ")", "-", "min", "(", "scores", ")", "+", "1e-9", ")", "\n", "\n", "", "elif", "self", ".", "score_transform_type", "==", "1", ":", "\n", "# convert [1, 0, 5] to [0.5, 0, 1]", "\n", "            ", "s", "=", "np", ".", "argsort", "(", "scores", ")", "\n", "n", "=", "len", "(", "scores", ")", "\n", "for", "i", "in", "range", "(", "n", ")", ":", "\n", "                ", "scores", "[", "s", "[", "i", "]", "]", "=", "i", "/", "(", "n", "-", "1", ")", "\n", "\n", "", "", "elif", "self", ".", "score_transform_type", "==", "2", "or", "self", ".", "score_transform_type", "==", "3", ":", "\n", "# fitness shaping from \"Natural Evolution Strategies\" (Wierstra 2014) paper, either with zero mean (2) or without (3)", "\n", "            ", "lmbda", "=", "len", "(", "scores", ")", "\n", "s", "=", "np", ".", "argsort", "(", "-", "scores", ")", "\n", "for", "i", "in", "range", "(", "lmbda", ")", ":", "\n", "                ", "scores", "[", "s", "[", "i", "]", "]", "=", "i", "+", "1", "\n", "", "scores", "=", "scores", ".", "astype", "(", "float", ")", "\n", "for", "i", "in", "range", "(", "lmbda", ")", ":", "\n", "                ", "scores", "[", "i", "]", "=", "max", "(", "0", ",", "np", ".", "log", "(", "lmbda", "/", "2", "+", "1", ")", "-", "np", ".", "log", "(", "scores", "[", "i", "]", ")", ")", "\n", "\n", "", "scores", "=", "scores", "/", "sum", "(", "scores", ")", "\n", "\n", "if", "self", ".", "score_transform_type", "==", "2", ":", "\n", "                ", "scores", "-=", "1", "/", "lmbda", "\n", "\n", "", "scores", "/=", "max", "(", "scores", ")", "\n", "\n", "", "elif", "self", ".", "score_transform_type", "==", "4", ":", "\n", "# consider single best eps", "\n", "            ", "scores_tmp", "=", "np", ".", "zeros", "(", "scores", ".", "size", ")", "\n", "scores_tmp", "[", "np", ".", "argmax", "(", "scores", ")", "]", "=", "1", "\n", "scores", "=", "scores_tmp", "\n", "\n", "", "elif", "self", ".", "score_transform_type", "==", "5", ":", "\n", "# consider single best eps that is better than the average", "\n", "            ", "avg_score_orig", "=", "np", ".", "mean", "(", "scores_orig", ")", "\n", "\n", "scores_idx", "=", "np", ".", "where", "(", "scores", ">", "avg_score_orig", "+", "1e-6", ",", "1", ",", "0", ")", "# 1e-6 to counter numerical errors", "\n", "if", "sum", "(", "scores_idx", ")", ">", "0", ":", "\n", "                ", "scores_tmp", "=", "np", ".", "zeros", "(", "scores", ".", "size", ")", "\n", "scores_tmp", "[", "np", ".", "argmax", "(", "scores", ")", "]", "=", "1", "\n", "scores", "=", "scores_tmp", "\n", "", "else", ":", "\n", "                ", "scores", "=", "scores_idx", "\n", "\n", "", "", "elif", "self", ".", "score_transform_type", "==", "6", "or", "self", ".", "score_transform_type", "==", "7", ":", "\n", "# consider all eps that are better than the average, normalize weight sum to 1", "\n", "            ", "avg_score_orig", "=", "np", ".", "mean", "(", "scores_orig", ")", "\n", "\n", "scores_idx", "=", "np", ".", "where", "(", "scores", ">", "avg_score_orig", "+", "1e-6", ",", "1", ",", "0", ")", "# 1e-6 to counter numerical errors", "\n", "if", "sum", "(", "scores_idx", ")", ">", "0", ":", "\n", "# if sum(scores_idx) > 0:", "\n", "                ", "scores", "=", "scores_idx", "*", "(", "scores", "-", "avg_score_orig", ")", "/", "(", "max", "(", "scores", ")", "-", "avg_score_orig", "+", "1e-9", ")", "\n", "if", "self", ".", "score_transform_type", "==", "6", ":", "\n", "                    ", "scores", "/=", "max", "(", "scores", ")", "\n", "", "else", ":", "\n", "                    ", "scores", "/=", "sum", "(", "scores", ")", "\n", "", "", "else", ":", "\n", "                ", "scores", "=", "scores_idx", "\n", "\n", "", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unknown rank transform type: \"", "+", "str", "(", "self", ".", "score_transform_type", ")", ")", "\n", "\n", "", "self", ".", "score_transform_list", "=", "scores", ".", "tolist", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_master.GTN_Master.update_env": [[267, 299], ["print", "print", "print", "print", "print", "GTN_master.GTN_Master.synthetic_env_orig.modules", "print", "zip", "print", "isinstance", "zip", "str", "str", "str", "str", "str", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "str", "GTN_master.GTN_Master.synthetic_env_orig.modules", "eps.modules", "isinstance", "str", "utils.calc_abs_param_sum().item", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "utils.calc_abs_param_sum().item", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "utils.calc_abs_param_sum().item", "utils.calc_abs_param_sum().item", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "utils.calc_abs_param_sum", "utils.calc_abs_param_sum", "utils.calc_abs_param_sum", "utils.calc_abs_param_sum"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.None.utils.calc_abs_param_sum", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.calc_abs_param_sum", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.calc_abs_param_sum", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.calc_abs_param_sum"], ["", "def", "update_env", "(", "self", ")", ":", "\n", "        ", "ss", "=", "self", ".", "step_size", "\n", "\n", "if", "self", ".", "nes_step_size", ":", "\n", "            ", "ss", "=", "ss", "/", "self", ".", "num_workers", "\n", "\n", "# print('-- update env --')", "\n", "", "print", "(", "'score_orig_list      '", "+", "str", "(", "self", ".", "score_orig_list", ")", ")", "\n", "print", "(", "'score_list           '", "+", "str", "(", "self", ".", "score_list", ")", ")", "\n", "print", "(", "'score_transform_list '", "+", "str", "(", "self", ".", "score_transform_list", ")", ")", "\n", "print", "(", "'venv weights         '", "+", "str", "(", "[", "calc_abs_param_sum", "(", "elem", ")", ".", "item", "(", ")", "for", "elem", "in", "self", ".", "synthetic_env_list", "]", ")", ")", "\n", "\n", "print", "(", "'weights before: '", "+", "str", "(", "calc_abs_param_sum", "(", "self", ".", "synthetic_env_orig", ")", ".", "item", "(", ")", ")", ")", "\n", "\n", "# weight decay", "\n", "for", "l_orig", "in", "self", ".", "synthetic_env_orig", ".", "modules", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "l_orig", ",", "nn", ".", "Linear", ")", ":", "\n", "                ", "l_orig", ".", "weight", "=", "torch", ".", "nn", ".", "Parameter", "(", "l_orig", ".", "weight", "*", "(", "1", "-", "self", ".", "weight_decay", ")", ")", "\n", "if", "l_orig", ".", "bias", "!=", "None", ":", "\n", "                    ", "l_orig", ".", "bias", "=", "torch", ".", "nn", ".", "Parameter", "(", "l_orig", ".", "bias", "*", "(", "1", "-", "self", ".", "weight_decay", ")", ")", "\n", "\n", "", "", "", "print", "(", "'weights after weight decay: '", "+", "str", "(", "calc_abs_param_sum", "(", "self", ".", "synthetic_env_orig", ")", ".", "item", "(", ")", ")", ")", "\n", "\n", "# weight update", "\n", "for", "eps", ",", "score_transform", "in", "zip", "(", "self", ".", "eps_list", ",", "self", ".", "score_transform_list", ")", ":", "\n", "            ", "for", "l_orig", ",", "l_eps", "in", "zip", "(", "self", ".", "synthetic_env_orig", ".", "modules", "(", ")", ",", "eps", ".", "modules", "(", ")", ")", ":", "\n", "                ", "if", "isinstance", "(", "l_orig", ",", "nn", ".", "Linear", ")", ":", "\n", "                    ", "l_orig", ".", "weight", "=", "torch", ".", "nn", ".", "Parameter", "(", "l_orig", ".", "weight", "+", "ss", "*", "score_transform", "*", "l_eps", ".", "weight", ")", "\n", "if", "l_orig", ".", "bias", "!=", "None", ":", "\n", "                        ", "l_orig", ".", "bias", "=", "torch", ".", "nn", ".", "Parameter", "(", "l_orig", ".", "bias", "+", "ss", "*", "score_transform", "*", "l_eps", ".", "bias", ")", "\n", "\n", "", "", "", "", "print", "(", "'weights after update: '", "+", "str", "(", "calc_abs_param_sum", "(", "self", ".", "synthetic_env_orig", ")", ".", "item", "(", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_master.GTN_Master.print_statistics": [[300, 309], ["statistics.mean", "statistics.mean", "print", "print", "print", "print", "print", "print", "str", "str", "str", "str"], "methods", ["None"], ["", "def", "print_statistics", "(", "self", ",", "it", ",", "time_elapsed", ")", ":", "\n", "        ", "orig_score", "=", "statistics", ".", "mean", "(", "self", ".", "score_orig_list", ")", "\n", "mean_time_elapsed", "=", "statistics", ".", "mean", "(", "self", ".", "time_elapsed_list", ")", "\n", "print", "(", "'--------------'", ")", "\n", "print", "(", "'GTN iteration:    '", "+", "str", "(", "it", ")", ")", "\n", "print", "(", "'GTN mstr t_elaps: '", "+", "str", "(", "time_elapsed", ")", ")", "\n", "print", "(", "'GTN avg wo t_elaps: '", "+", "str", "(", "mean_time_elapsed", ")", ")", "\n", "print", "(", "'GTN avg eval score:   '", "+", "str", "(", "orig_score", ")", ")", "\n", "print", "(", "'--------------'", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.DuelingDDQN_vary.DuelingDDQN_vary.__init__": [[13, 23], ["agents.DuelingDDQN.DuelingDDQN.__init__", "copy.deepcopy", "DuelingDDQN_vary.DuelingDDQN_vary.vary_hyperparameters"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__", "home.repos.pwc.inspect_result.automl_learning_environments.agents.DDQN_vary.DDQN_vary.vary_hyperparameters"], ["    ", "def", "__init__", "(", "self", ",", "env", ",", "config", ",", "icm", "=", "False", ")", ":", "\n", "        ", "self", ".", "agent_name", "=", "'duelingddqn'", "\n", "\n", "if", "config", "[", "\"agents\"", "]", "[", "\"duelingddqn_vary\"", "]", "[", "\"vary_hp\"", "]", ":", "\n", "            ", "config_mod", "=", "copy", ".", "deepcopy", "(", "config", ")", "\n", "config_mod", "=", "self", ".", "vary_hyperparameters", "(", "config_mod", ")", "\n", "", "else", ":", "\n", "            ", "config_mod", "=", "config", "\n", "\n", "", "super", "(", ")", ".", "__init__", "(", "env", "=", "env", ",", "config", "=", "config_mod", ",", "icm", "=", "icm", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.DuelingDDQN_vary.DuelingDDQN_vary.vary_hyperparameters": [[24, 76], ["ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.sample_configuration", "print", "print", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "int", "int", "int", "int"], "methods", ["None"], ["", "def", "vary_hyperparameters", "(", "self", ",", "config_mod", ")", ":", "\n", "\n", "        ", "lr", "=", "config_mod", "[", "'agents'", "]", "[", "self", ".", "agent_name", "]", "[", "'lr'", "]", "\n", "batch_size", "=", "config_mod", "[", "'agents'", "]", "[", "self", ".", "agent_name", "]", "[", "'batch_size'", "]", "\n", "hidden_size", "=", "config_mod", "[", "'agents'", "]", "[", "self", ".", "agent_name", "]", "[", "'hidden_size'", "]", "\n", "hidden_layer", "=", "config_mod", "[", "'agents'", "]", "[", "self", ".", "agent_name", "]", "[", "'hidden_layer'", "]", "\n", "\n", "cs", "=", "CS", ".", "ConfigurationSpace", "(", ")", "\n", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'lr'", ",", "\n", "lower", "=", "lr", "/", "3", ",", "\n", "upper", "=", "lr", "*", "3", ",", "\n", "log", "=", "True", ",", "\n", "default_value", "=", "lr", ")", "\n", ")", "\n", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'batch_size'", ",", "\n", "lower", "=", "int", "(", "batch_size", "/", "3", ")", ",", "\n", "upper", "=", "int", "(", "batch_size", "*", "3", ")", ",", "\n", "log", "=", "True", ",", "\n", "default_value", "=", "batch_size", ")", "\n", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'hidden_size'", ",", "\n", "lower", "=", "int", "(", "hidden_size", "/", "3", ")", ",", "\n", "upper", "=", "int", "(", "hidden_size", "*", "3", ")", ",", "\n", "log", "=", "True", ",", "\n", "default_value", "=", "hidden_size", ")", "\n", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'hidden_layer'", ",", "\n", "lower", "=", "hidden_layer", "-", "1", ",", "\n", "upper", "=", "hidden_layer", "+", "1", ",", "\n", "log", "=", "False", ",", "\n", "default_value", "=", "hidden_layer", ")", "\n", ")", "\n", "\n", "config", "=", "cs", ".", "sample_configuration", "(", ")", "\n", "\n", "print", "(", "f\"sampled part of config: \"", "\n", "f\"lr: {config['lr']}, \"", "\n", "f\"batch_size: {config['batch_size']}, \"", "\n", "f\"hidden_size: {config['hidden_size']}, \"", "\n", "f\"hidden_layer: {config['hidden_layer']}\"", "\n", ")", "\n", "\n", "config_mod", "[", "'agents'", "]", "[", "self", ".", "agent_name", "]", "[", "'lr'", "]", "=", "config", "[", "'lr'", "]", "\n", "config_mod", "[", "'agents'", "]", "[", "self", ".", "agent_name", "]", "[", "'batch_size'", "]", "=", "config", "[", "'batch_size'", "]", "\n", "config_mod", "[", "'agents'", "]", "[", "self", ".", "agent_name", "]", "[", "'hidden_size'", "]", "=", "config", "[", "'hidden_size'", "]", "\n", "config_mod", "[", "'agents'", "]", "[", "self", ".", "agent_name", "]", "[", "'hidden_layer'", "]", "=", "config", "[", "'hidden_layer'", "]", "\n", "\n", "print", "(", "\"full config: \"", ",", "config_mod", "[", "'agents'", "]", "[", "self", ".", "agent_name", "]", ")", "\n", "\n", "return", "config_mod", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.TD3_vary.TD3_vary.__init__": [[13, 23], ["agents.TD3.TD3.__init__", "copy.deepcopy", "TD3_vary.TD3_vary.vary_hyperparameters"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__", "home.repos.pwc.inspect_result.automl_learning_environments.agents.DDQN_vary.DDQN_vary.vary_hyperparameters"], ["    ", "def", "__init__", "(", "self", ",", "env", ",", "max_action", ",", "config", ",", "icm", "=", "False", ")", ":", "\n", "        ", "self", ".", "agent_name", "=", "'td3'", "\n", "\n", "if", "config", "[", "\"agents\"", "]", "[", "\"td3_vary\"", "]", "[", "\"vary_hp\"", "]", ":", "\n", "            ", "config_mod", "=", "copy", ".", "deepcopy", "(", "config", ")", "\n", "config_mod", "=", "self", ".", "vary_hyperparameters", "(", "config_mod", ")", "\n", "", "else", ":", "\n", "            ", "config_mod", "=", "config", "\n", "\n", "", "super", "(", ")", ".", "__init__", "(", "env", "=", "env", ",", "max_action", "=", "max_action", ",", "config", "=", "config_mod", ",", "icm", "=", "icm", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.TD3_vary.TD3_vary.vary_hyperparameters": [[24, 60], ["ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.sample_configuration", "print", "print", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "int", "int", "int", "int"], "methods", ["None"], ["", "def", "vary_hyperparameters", "(", "self", ",", "config_mod", ")", ":", "\n", "\n", "        ", "lr", "=", "config_mod", "[", "'agents'", "]", "[", "self", ".", "agent_name", "]", "[", "'lr'", "]", "\n", "batch_size", "=", "config_mod", "[", "'agents'", "]", "[", "self", ".", "agent_name", "]", "[", "'batch_size'", "]", "\n", "hidden_size", "=", "config_mod", "[", "'agents'", "]", "[", "self", ".", "agent_name", "]", "[", "'hidden_size'", "]", "\n", "hidden_layer", "=", "config_mod", "[", "'agents'", "]", "[", "self", ".", "agent_name", "]", "[", "'hidden_layer'", "]", "\n", "\n", "cs", "=", "CS", ".", "ConfigurationSpace", "(", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'lr'", ",", "lower", "=", "lr", "/", "3", ",", "upper", "=", "lr", "*", "3", ",", "log", "=", "True", ",", "default_value", "=", "lr", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "\n", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'batch_size'", ",", "lower", "=", "int", "(", "batch_size", "/", "3", ")", ",", "upper", "=", "int", "(", "batch_size", "*", "3", ")", ",", "log", "=", "True", ",", "\n", "default_value", "=", "batch_size", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "\n", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'hidden_size'", ",", "lower", "=", "int", "(", "hidden_size", "/", "3", ")", ",", "upper", "=", "int", "(", "hidden_size", "*", "3", ")", ",", "log", "=", "True", ",", "\n", "default_value", "=", "hidden_size", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "\n", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'hidden_layer'", ",", "lower", "=", "hidden_layer", "-", "1", ",", "upper", "=", "hidden_layer", "+", "1", ",", "log", "=", "False", ",", "\n", "default_value", "=", "hidden_layer", ")", ")", "\n", "\n", "config", "=", "cs", ".", "sample_configuration", "(", ")", "\n", "\n", "print", "(", "f\"sampled part of config: \"", "\n", "f\"lr: {config['lr']}, \"", "\n", "f\"batch_size: {config['batch_size']}, \"", "\n", "f\"hidden_size: {config['hidden_size']}, \"", "\n", "f\"hidden_layer: {config['hidden_layer']}\"", "\n", ")", "\n", "\n", "config_mod", "[", "'agents'", "]", "[", "self", ".", "agent_name", "]", "[", "'lr'", "]", "=", "config", "[", "'lr'", "]", "\n", "config_mod", "[", "'agents'", "]", "[", "self", ".", "agent_name", "]", "[", "'batch_size'", "]", "=", "config", "[", "'batch_size'", "]", "\n", "config_mod", "[", "'agents'", "]", "[", "self", ".", "agent_name", "]", "[", "'hidden_size'", "]", "=", "config", "[", "'hidden_size'", "]", "\n", "config_mod", "[", "'agents'", "]", "[", "self", ".", "agent_name", "]", "[", "'hidden_layer'", "]", "=", "config", "[", "'hidden_layer'", "]", "\n", "\n", "print", "(", "\"full config: \"", ",", "config_mod", "[", "'agents'", "]", "[", "self", ".", "agent_name", "]", ")", "\n", "\n", "return", "config_mod", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.REPTILE.REPTILE.__init__": [[60, 78], ["torch.Module.__init__", "envs.env_factory.EnvFactory", "agents.agent_utils.select_agent", "range", "REPTILE.REPTILE.envs.append", "REPTILE.REPTILE.env_factory.generate_random_real_env"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__", "home.repos.pwc.inspect_result.automl_learning_environments.agents.agent_utils.select_agent"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "config", "=", "config", "\n", "\n", "reptile_config", "=", "config", "[", "\"agents\"", "]", "[", "\"reptile\"", "]", "\n", "self", ".", "max_iterations", "=", "reptile_config", "[", "\"max_iterations\"", "]", "\n", "self", ".", "step_size", "=", "reptile_config", "[", "\"step_size\"", "]", "\n", "self", ".", "parallel_update", "=", "reptile_config", "[", "\"parallel_update\"", "]", "\n", "self", ".", "env_num", "=", "reptile_config", "[", "\"env_num\"", "]", "\n", "\n", "agent_name", "=", "reptile_config", "[", "\"agent_name\"", "]", "\n", "self", ".", "env_factory", "=", "EnvFactory", "(", "config", ")", "\n", "self", ".", "agent", "=", "select_agent", "(", "config", ",", "agent_name", ")", "\n", "\n", "self", ".", "envs", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "env_num", ")", ":", "\n", "            ", "self", ".", "envs", ".", "append", "(", "self", ".", "env_factory", ".", "generate_random_real_env", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.REPTILE.REPTILE.train": [[79, 89], ["REPTILE.REPTILE.train", "range", "print", "agents.agent_utils.print_stats", "REPTILE.reptile_update_agent_parallel", "REPTILE.reptile_update_agent_serial"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.train", "home.repos.pwc.inspect_result.automl_learning_environments.agents.agent_utils.print_stats", "home.repos.pwc.inspect_result.automl_learning_environments.agents.REPTILE.reptile_update_agent_parallel", "home.repos.pwc.inspect_result.automl_learning_environments.agents.REPTILE.reptile_update_agent_serial"], ["", "", "def", "train", "(", "self", ")", ":", "\n", "        ", "self", ".", "train", "(", ")", "\n", "for", "it", "in", "range", "(", "self", ".", "max_iterations", ")", ":", "\n", "            ", "print", "(", "'-- REPTILE iteration {} --'", ".", "format", "(", "it", ")", ")", "\n", "print_stats", "(", "self", ".", "agent", ")", "\n", "if", "self", ".", "parallel_update", ":", "\n", "                ", "reptile_update_agent_parallel", "(", "agent", "=", "self", ".", "agent", ",", "envs", "=", "self", ".", "envs", ",", "step_size", "=", "self", ".", "step_size", ")", "\n", "", "else", ":", "\n", "                ", "for", "env", "in", "self", ".", "envs", ":", "\n", "                    ", "reptile_update_agent_serial", "(", "agent", "=", "self", ".", "agent", ",", "env", "=", "env", ",", "step_size", "=", "self", ".", "step_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.REPTILE.reptile_update_agent_serial": [[10, 19], ["copy.deepcopy", "agent.train", "copy.deepcopy", "REPTILE.reptile_update_state_dict_serial", "agent.state_dict", "agent.state_dict"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.train", "home.repos.pwc.inspect_result.automl_learning_environments.agents.REPTILE.reptile_update_state_dict_serial"], ["def", "reptile_update_agent_serial", "(", "agent", ",", "env", ",", "step_size", ")", ":", "\n", "    ", "old_state_dict", "=", "copy", ".", "deepcopy", "(", "agent", ".", "state_dict", "(", ")", ")", "\n", "agent", ".", "train", "(", "env", "=", "env", ")", "\n", "new_state_dict", "=", "copy", ".", "deepcopy", "(", "agent", ".", "state_dict", "(", ")", ")", "\n", "\n", "reptile_update_state_dict_serial", "(", "agent", "=", "agent", ",", "\n", "old_state_dict", "=", "old_state_dict", ",", "\n", "new_state_dict", "=", "new_state_dict", ",", "\n", "step_size", "=", "step_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.REPTILE.reptile_update_agent_parallel": [[21, 34], ["copy.deepcopy", "REPTILE.reptile_update_state_dict_parallel", "agent.state_dict", "agent.load_state_dict", "agent.train", "new_state_dicts.append", "copy.deepcopy", "copy.deepcopy", "agent.state_dict"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.agents.REPTILE.reptile_update_state_dict_parallel", "home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.train"], ["", "def", "reptile_update_agent_parallel", "(", "agent", ",", "envs", ",", "step_size", ")", ":", "\n", "    ", "old_state_dict", "=", "copy", ".", "deepcopy", "(", "agent", ".", "state_dict", "(", ")", ")", "\n", "new_state_dicts", "=", "[", "]", "\n", "\n", "for", "env", "in", "envs", ":", "\n", "        ", "agent", ".", "load_state_dict", "(", "copy", ".", "deepcopy", "(", "old_state_dict", ")", ")", "\n", "agent", ".", "train", "(", "env", "=", "env", ")", "\n", "new_state_dicts", ".", "append", "(", "copy", ".", "deepcopy", "(", "agent", ".", "state_dict", "(", ")", ")", ")", "\n", "\n", "", "reptile_update_state_dict_parallel", "(", "agent", "=", "agent", ",", "\n", "old_state_dict", "=", "old_state_dict", ",", "\n", "new_state_dicts", "=", "new_state_dicts", ",", "\n", "step_size", "=", "step_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.REPTILE.reptile_update_state_dict_serial": [[36, 40], ["agent.state_dict", "new_state_dict.items"], "function", ["None"], ["", "def", "reptile_update_state_dict_serial", "(", "agent", ",", "old_state_dict", ",", "new_state_dict", ",", "step_size", ")", ":", "\n", "    ", "agent_state_dict", "=", "agent", ".", "state_dict", "(", ")", "\n", "for", "key", ",", "value", "in", "new_state_dict", ".", "items", "(", ")", ":", "\n", "        ", "agent_state_dict", "[", "key", "]", "=", "old_state_dict", "[", "key", "]", "+", "(", "new_state_dict", "[", "key", "]", "-", "old_state_dict", "[", "key", "]", ")", "*", "step_size", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.REPTILE.reptile_update_state_dict_parallel": [[42, 56], ["len", "agent.load_state_dict", "agent.state_dict", "new_state_dict.items", "print", "copy.deepcopy", "new_state_dict.items", "torch.sum", "torch.sum", "torch.abs", "torch.abs"], "function", ["None"], ["", "", "def", "reptile_update_state_dict_parallel", "(", "agent", ",", "old_state_dict", ",", "new_state_dicts", ",", "step_size", ")", ":", "\n", "    ", "n", "=", "len", "(", "new_state_dicts", ")", "\n", "agent", ".", "load_state_dict", "(", "copy", ".", "deepcopy", "(", "old_state_dict", ")", ")", "\n", "agent_state_dict", "=", "agent", ".", "state_dict", "(", ")", "\n", "\n", "for", "new_state_dict", "in", "new_state_dicts", ":", "\n", "        ", "for", "key", ",", "value", "in", "new_state_dict", ".", "items", "(", ")", ":", "\n", "            ", "agent_state_dict", "[", "key", "]", "+=", "(", "new_state_dict", "[", "key", "]", "-", "old_state_dict", "[", "key", "]", ")", "*", "step_size", "/", "n", "\n", "\n", "", "", "sum_diff", "=", "0", "\n", "for", "key", ",", "value", "in", "new_state_dict", ".", "items", "(", ")", ":", "\n", "        ", "sum_diff", "+=", "torch", ".", "sum", "(", "torch", ".", "abs", "(", "agent_state_dict", "[", "key", "]", "-", "old_state_dict", "[", "key", "]", ")", ")", "\n", "\n", "", "print", "(", "sum_diff", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_base.GTN_Base.__init__": [[8, 18], ["torch.Module.__init__", "os.getcwd", "str", "print", "os.makedirs", "os.path.join", "str"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__"], ["    ", "def", "__init__", "(", "self", ",", "bohb_id", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "bohb_id", "=", "bohb_id", "\n", "\n", "sync_dir_base", "=", "os", ".", "getcwd", "(", ")", "\n", "self", ".", "sync_dir", "=", "str", "(", "os", ".", "path", ".", "join", "(", "sync_dir_base", ",", "'results/GTN_sync'", ")", ")", "\n", "print", "(", "'SYNC DIR: '", "+", "str", "(", "self", ".", "sync_dir", ")", ")", "\n", "\n", "os", ".", "makedirs", "(", "self", ".", "sync_dir", ",", "exist_ok", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_base.GTN_Base.get_input_file_name": [[19, 21], ["os.path.join", "str", "str"], "methods", ["None"], ["", "def", "get_input_file_name", "(", "self", ",", "id", ")", ":", "\n", "        ", "return", "os", ".", "path", ".", "join", "(", "self", ".", "sync_dir", ",", "str", "(", "self", ".", "bohb_id", ")", "+", "'_'", "+", "str", "(", "id", ")", "+", "'_input.pt'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_base.GTN_Base.get_input_check_file_name": [[22, 24], ["os.path.join", "str", "str"], "methods", ["None"], ["", "def", "get_input_check_file_name", "(", "self", ",", "id", ")", ":", "\n", "        ", "return", "os", ".", "path", ".", "join", "(", "self", ".", "sync_dir", ",", "str", "(", "self", ".", "bohb_id", ")", "+", "'_'", "+", "str", "(", "id", ")", "+", "'_input_check.pt'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_base.GTN_Base.get_result_file_name": [[25, 27], ["os.path.join", "str", "str"], "methods", ["None"], ["", "def", "get_result_file_name", "(", "self", ",", "id", ")", ":", "\n", "        ", "return", "os", ".", "path", ".", "join", "(", "self", ".", "sync_dir", ",", "str", "(", "self", ".", "bohb_id", ")", "+", "'_'", "+", "str", "(", "id", ")", "+", "'_result.pt'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_base.GTN_Base.get_result_check_file_name": [[28, 30], ["os.path.join", "str", "str"], "methods", ["None"], ["", "def", "get_result_check_file_name", "(", "self", ",", "id", ")", ":", "\n", "        ", "return", "os", ".", "path", ".", "join", "(", "self", ".", "sync_dir", ",", "str", "(", "self", ".", "bohb_id", ")", "+", "'_'", "+", "str", "(", "id", ")", "+", "'_result_check.pt'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_base.GTN_Base.get_quit_file_name": [[31, 33], ["os.path.join"], "methods", ["None"], ["", "def", "get_quit_file_name", "(", "self", ")", ":", "\n", "        ", "return", "os", ".", "path", ".", "join", "(", "self", ".", "sync_dir", ",", "'quit.flag'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_base.GTN_Base.clean_working_dir": [[34, 38], ["glob.glob", "os.path.join", "os.remove"], "methods", ["None"], ["", "def", "clean_working_dir", "(", "self", ")", ":", "\n", "        ", "files", "=", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "self", ".", "sync_dir", ",", "'*'", ")", ")", "\n", "for", "file", "in", "files", ":", "\n", "            ", "os", ".", "remove", "(", "file", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.TD3_discrete_vary.TD3_discrete_vary.__init__": [[18, 61], ["agents.base_agent.BaseAgent.__init__", "models.actor_critic.Actor_TD3_discrete().to", "models.actor_critic.Actor_TD3_discrete().to", "TD3_discrete_vary.TD3_discrete_vary.actor_target.load_state_dict", "models.actor_critic.Critic_Q().to", "models.actor_critic.Critic_Q().to", "models.actor_critic.Critic_Q().to", "models.actor_critic.Critic_Q().to", "TD3_discrete_vary.TD3_discrete_vary.critic_target_1.load_state_dict", "TD3_discrete_vary.TD3_discrete_vary.critic_target_2.load_state_dict", "TD3_discrete_vary.TD3_discrete_vary.reset_optimizer", "numpy.linspace", "copy.deepcopy", "TD3_discrete_vary.TD3_discrete_vary.vary_hyperparameters", "print", "TD3_discrete_vary.TD3_discrete_vary.actor.state_dict", "TD3_discrete_vary.TD3_discrete_vary.critic_1.state_dict", "TD3_discrete_vary.TD3_discrete_vary.critic_2.state_dict", "models.actor_critic.Actor_TD3_discrete", "models.actor_critic.Actor_TD3_discrete", "models.actor_critic.Critic_Q", "models.actor_critic.Critic_Q", "models.actor_critic.Critic_Q", "models.actor_critic.Critic_Q"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__", "home.repos.pwc.inspect_result.automl_learning_environments.agents.SAC.SAC.reset_optimizer", "home.repos.pwc.inspect_result.automl_learning_environments.agents.DDQN_vary.DDQN_vary.vary_hyperparameters"], ["    ", "def", "__init__", "(", "self", ",", "env", ",", "min_action", ",", "max_action", ",", "config", ")", ":", "\n", "        ", "self", ".", "agent_name", "=", "'td3_discrete_vary'", "\n", "\n", "if", "config", "[", "\"agents\"", "]", "[", "\"td3_discrete_vary\"", "]", "[", "\"vary_hp\"", "]", ":", "\n", "            ", "config_mod", "=", "copy", ".", "deepcopy", "(", "config", ")", "\n", "config_mod", "=", "self", ".", "vary_hyperparameters", "(", "config_mod", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "\"--- configs not varied ---\"", ")", "\n", "config_mod", "=", "config", "\n", "\n", "", "super", "(", ")", ".", "__init__", "(", "agent_name", "=", "self", ".", "agent_name", ",", "env", "=", "env", ",", "config", "=", "config_mod", ")", "\n", "\n", "td3_config", "=", "config_mod", "[", "\"agents\"", "]", "[", "self", ".", "agent_name", "]", "\n", "\n", "self", ".", "max_action", "=", "max_action", "\n", "self", ".", "min_action", "=", "min_action", "\n", "self", ".", "batch_size", "=", "td3_config", "[", "\"batch_size\"", "]", "\n", "self", ".", "rb_size", "=", "td3_config", "[", "\"rb_size\"", "]", "\n", "self", ".", "gamma", "=", "td3_config", "[", "\"gamma\"", "]", "\n", "self", ".", "tau", "=", "td3_config", "[", "\"tau\"", "]", "\n", "self", ".", "policy_delay", "=", "td3_config", "[", "\"policy_delay\"", "]", "\n", "self", ".", "lr", "=", "td3_config", "[", "\"lr\"", "]", "\n", "self", ".", "action_std", "=", "td3_config", "[", "\"action_std\"", "]", "\n", "self", ".", "policy_std", "=", "td3_config", "[", "\"policy_std\"", "]", "\n", "self", ".", "policy_std_clip", "=", "td3_config", "[", "\"policy_std_clip\"", "]", "\n", "\n", "self", ".", "actor", "=", "Actor_TD3_discrete", "(", "self", ".", "state_dim", ",", "self", ".", "action_dim", ",", "max_action", ",", "self", ".", "agent_name", ",", "config_mod", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "actor_target", "=", "Actor_TD3_discrete", "(", "self", ".", "state_dim", ",", "self", ".", "action_dim", ",", "max_action", ",", "self", ".", "agent_name", ",", "config_mod", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "actor_target", ".", "load_state_dict", "(", "self", ".", "actor", ".", "state_dict", "(", ")", ")", "\n", "self", ".", "critic_1", "=", "Critic_Q", "(", "self", ".", "state_dim", ",", "self", ".", "action_dim", ",", "self", ".", "agent_name", ",", "config_mod", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "critic_2", "=", "Critic_Q", "(", "self", ".", "state_dim", ",", "self", ".", "action_dim", ",", "self", ".", "agent_name", ",", "config_mod", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "critic_target_1", "=", "Critic_Q", "(", "self", ".", "state_dim", ",", "self", ".", "action_dim", ",", "self", ".", "agent_name", ",", "config_mod", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "critic_target_2", "=", "Critic_Q", "(", "self", ".", "state_dim", ",", "self", ".", "action_dim", ",", "self", ".", "agent_name", ",", "config_mod", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "critic_target_1", ".", "load_state_dict", "(", "self", ".", "critic_1", ".", "state_dict", "(", ")", ")", "\n", "self", ".", "critic_target_2", ".", "load_state_dict", "(", "self", ".", "critic_2", ".", "state_dict", "(", ")", ")", "\n", "\n", "self", ".", "reset_optimizer", "(", ")", "\n", "\n", "self", ".", "total_it", "=", "0", "\n", "\n", "# Gumbel temperature annealing", "\n", "self", ".", "gumbel_temp_anneal_steps", "=", "np", ".", "linspace", "(", "self", ".", "actor", ".", "gumbel_softmax_temp", ",", "self", ".", "actor", ".", "gumbel_softmax_temp", "/", "20", ",", "2000", ")", "\n", "self", ".", "gumbel_temp_annealed", "=", "self", ".", "gumbel_temp_anneal_steps", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.TD3_discrete_vary.TD3_discrete_vary.learn": [[62, 118], ["replay_buffer.sample", "TD3_discrete_vary.TD3_discrete_vary.critic_1", "TD3_discrete_vary.TD3_discrete_vary.critic_2", "TD3_discrete_vary.TD3_discrete_vary.critic_optimizer.zero_grad", "critic_loss.backward", "TD3_discrete_vary.TD3_discrete_vary.critic_optimizer.step", "len", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "TD3_discrete_vary.TD3_discrete_vary.critic_target_1", "TD3_discrete_vary.TD3_discrete_vary.critic_target_2", "torch.min", "torch.min", "torch.min", "torch.min", "torch.mse_loss", "torch.mse_loss", "torch.mse_loss", "torch.mse_loss", "TD3_discrete_vary.TD3_discrete_vary.actor_optimizer.zero_grad", "actor_loss.backward", "TD3_discrete_vary.TD3_discrete_vary.actor_optimizer.step", "zip", "zip", "zip", "TD3_discrete_vary.TD3_discrete_vary.actor_target", "TD3_discrete_vary.TD3_discrete_vary.critic_1.parameters", "TD3_discrete_vary.TD3_discrete_vary.critic_target_1.parameters", "target_param.data.copy_", "TD3_discrete_vary.TD3_discrete_vary.critic_2.parameters", "TD3_discrete_vary.TD3_discrete_vary.critic_target_2.parameters", "target_param.data.copy_", "TD3_discrete_vary.TD3_discrete_vary.actor.parameters", "TD3_discrete_vary.TD3_discrete_vary.actor_target.parameters", "target_param.data.copy_", "torch.randn_like", "torch.randn_like", "torch.randn_like", "torch.randn_like", "TD3_discrete_vary.TD3_discrete_vary.critic_1", "TD3_discrete_vary.TD3_discrete_vary.actor"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.None.utils.ReplayBuffer.sample", "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditEnv.step", "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditEnv.step"], ["", "def", "learn", "(", "self", ",", "replay_buffer", ",", "env", ",", "episode", ")", ":", "\n", "\n", "        ", "if", "self", ".", "total_it", ">=", "len", "(", "self", ".", "gumbel_temp_anneal_steps", ")", ":", "\n", "            ", "self", ".", "gumbel_temp_annealed", "=", "self", ".", "gumbel_temp_anneal_steps", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "gumbel_temp_annealed", "=", "self", ".", "gumbel_temp_anneal_steps", "[", "self", ".", "total_it", "]", "\n", "\n", "", "self", ".", "total_it", "+=", "1", "\n", "\n", "# Sample replay buffer", "\n", "states", ",", "actions", ",", "next_states", ",", "rewards", ",", "dones", "=", "replay_buffer", ".", "sample", "(", "self", ".", "batch_size", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# Select action according to policy and add clipped noise, no_grad since target will be copied", "\n", "            ", "noise", "=", "(", "torch", ".", "randn_like", "(", "actions", ")", "*", "self", ".", "policy_std", ")", ".", "clamp", "(", "-", "self", ".", "policy_std_clip", ",", "self", ".", "policy_std_clip", ")", "\n", "next_actions", "=", "self", ".", "actor_target", "(", "next_states", ",", "self", ".", "gumbel_temp_annealed", ")", "+", "noise", "\n", "\n", "# Compute the target Q value", "\n", "target_Q1", "=", "self", ".", "critic_target_1", "(", "next_states", ",", "next_actions", ")", "\n", "target_Q2", "=", "self", ".", "critic_target_2", "(", "next_states", ",", "next_actions", ")", "\n", "target_Q", "=", "torch", ".", "min", "(", "target_Q1", ",", "target_Q2", ")", "\n", "target_Q", "=", "rewards", "+", "(", "1", "-", "dones", ")", "*", "self", ".", "gamma", "*", "target_Q", "\n", "# target_Q = rewards + self.gamma * target_Q", "\n", "\n", "# Get current Q estimates", "\n", "", "current_Q1", "=", "self", ".", "critic_1", "(", "states", ",", "actions", ")", "\n", "current_Q2", "=", "self", ".", "critic_2", "(", "states", ",", "actions", ")", "\n", "\n", "# Compute critic loss", "\n", "critic_loss", "=", "F", ".", "mse_loss", "(", "current_Q1", ",", "target_Q", ")", "+", "F", ".", "mse_loss", "(", "current_Q2", ",", "target_Q", ")", "\n", "\n", "# Optimize the critic", "\n", "self", ".", "critic_optimizer", ".", "zero_grad", "(", ")", "\n", "critic_loss", ".", "backward", "(", ")", "\n", "self", ".", "critic_optimizer", ".", "step", "(", ")", "\n", "\n", "# Delayed policy updates", "\n", "if", "self", ".", "total_it", "%", "self", ".", "policy_delay", "==", "0", ":", "\n", "# Compute actor loss", "\n", "# todo: check algorithm 1 in original paper; has additional multiplicative term here", "\n", "            ", "actor_loss", "=", "(", "-", "self", ".", "critic_1", "(", "states", ",", "self", ".", "actor", "(", "states", ",", "self", ".", "gumbel_temp_annealed", ")", ")", ")", ".", "mean", "(", ")", "\n", "\n", "# Optimize the actor", "\n", "self", ".", "actor_optimizer", ".", "zero_grad", "(", ")", "\n", "actor_loss", ".", "backward", "(", ")", "\n", "self", ".", "actor_optimizer", ".", "step", "(", ")", "\n", "\n", "# Update the frozen target models", "\n", "for", "param", ",", "target_param", "in", "zip", "(", "self", ".", "critic_1", ".", "parameters", "(", ")", ",", "self", ".", "critic_target_1", ".", "parameters", "(", ")", ")", ":", "\n", "                ", "target_param", ".", "data", ".", "copy_", "(", "self", ".", "tau", "*", "param", ".", "data", "+", "(", "1", "-", "self", ".", "tau", ")", "*", "target_param", ".", "data", ")", "\n", "\n", "", "for", "param", ",", "target_param", "in", "zip", "(", "self", ".", "critic_2", ".", "parameters", "(", ")", ",", "self", ".", "critic_target_2", ".", "parameters", "(", ")", ")", ":", "\n", "                ", "target_param", ".", "data", ".", "copy_", "(", "self", ".", "tau", "*", "param", ".", "data", "+", "(", "1", "-", "self", ".", "tau", ")", "*", "target_param", ".", "data", ")", "\n", "\n", "", "for", "param", ",", "target_param", "in", "zip", "(", "self", ".", "actor", ".", "parameters", "(", ")", ",", "self", ".", "actor_target", ".", "parameters", "(", ")", ")", ":", "\n", "                ", "target_param", ".", "data", ".", "copy_", "(", "self", ".", "tau", "*", "param", ".", "data", "+", "(", "1", "-", "self", ".", "tau", ")", "*", "target_param", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.TD3_discrete_vary.TD3_discrete_vary.vary_hyperparameters": [[119, 155], ["ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.sample_configuration", "print", "print", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "int", "int", "int", "int"], "methods", ["None"], ["", "", "", "def", "vary_hyperparameters", "(", "self", ",", "config_mod", ")", ":", "\n", "\n", "        ", "lr", "=", "config_mod", "[", "'agents'", "]", "[", "self", ".", "agent_name", "]", "[", "'lr'", "]", "\n", "batch_size", "=", "config_mod", "[", "'agents'", "]", "[", "self", ".", "agent_name", "]", "[", "'batch_size'", "]", "\n", "hidden_size", "=", "config_mod", "[", "'agents'", "]", "[", "self", ".", "agent_name", "]", "[", "'hidden_size'", "]", "\n", "hidden_layer", "=", "config_mod", "[", "'agents'", "]", "[", "self", ".", "agent_name", "]", "[", "'hidden_layer'", "]", "\n", "\n", "cs", "=", "CS", ".", "ConfigurationSpace", "(", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'lr'", ",", "lower", "=", "lr", "/", "3", ",", "upper", "=", "lr", "*", "3", ",", "log", "=", "True", ",", "default_value", "=", "lr", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "\n", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'batch_size'", ",", "lower", "=", "int", "(", "batch_size", "/", "3", ")", ",", "upper", "=", "int", "(", "batch_size", "*", "3", ")", ",", "log", "=", "True", ",", "\n", "default_value", "=", "batch_size", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "\n", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'hidden_size'", ",", "lower", "=", "int", "(", "hidden_size", "/", "3", ")", ",", "upper", "=", "int", "(", "hidden_size", "*", "3", ")", ",", "log", "=", "True", ",", "\n", "default_value", "=", "hidden_size", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "\n", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'hidden_layer'", ",", "lower", "=", "hidden_layer", "-", "1", ",", "upper", "=", "hidden_layer", "+", "1", ",", "log", "=", "False", ",", "\n", "default_value", "=", "hidden_layer", ")", ")", "\n", "\n", "config", "=", "cs", ".", "sample_configuration", "(", ")", "\n", "\n", "print", "(", "f\"sampled part of config: \"", "\n", "f\"lr: {config['lr']}, \"", "\n", "f\"batch_size: {config['batch_size']}, \"", "\n", "f\"hidden_size: {config['hidden_size']}, \"", "\n", "f\"hidden_layer: {config['hidden_layer']}\"", "\n", ")", "\n", "\n", "config_mod", "[", "'agents'", "]", "[", "self", ".", "agent_name", "]", "[", "'lr'", "]", "=", "config", "[", "'lr'", "]", "\n", "config_mod", "[", "'agents'", "]", "[", "self", ".", "agent_name", "]", "[", "'batch_size'", "]", "=", "config", "[", "'batch_size'", "]", "\n", "config_mod", "[", "'agents'", "]", "[", "self", ".", "agent_name", "]", "[", "'hidden_size'", "]", "=", "config", "[", "'hidden_size'", "]", "\n", "config_mod", "[", "'agents'", "]", "[", "self", ".", "agent_name", "]", "[", "'hidden_layer'", "]", "=", "config", "[", "'hidden_layer'", "]", "\n", "\n", "print", "(", "\"full config: \"", ",", "config_mod", "[", "'agents'", "]", "[", "self", ".", "agent_name", "]", ")", "\n", "\n", "return", "config_mod", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.TD3_discrete_vary.TD3_discrete_vary.select_train_action": [[156, 164], ["env.get_random_action", "torch.eye", "torch.eye", "torch.eye", "torch.eye", "TD3_discrete_vary.TD3_discrete_vary.actor().cpu", "TD3_discrete_vary.TD3_discrete_vary.long", "TD3_discrete_vary.TD3_discrete_vary.actor", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "state.to"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.get_random_action"], ["", "def", "select_train_action", "(", "self", ",", "state", ",", "env", ",", "episode", ")", ":", "\n", "        ", "if", "episode", "<", "self", ".", "init_episodes", ":", "\n", "            ", "action", "=", "env", ".", "get_random_action", "(", ")", "\n", "diag", "=", "torch", ".", "eye", "(", "self", ".", "action_dim", ")", "\n", "return", "diag", "[", "action", ".", "long", "(", ")", "]", "# for CartPole: 0 -> [1,0], 1 -> [0,1]", "\n", "", "else", ":", "\n", "            ", "action", "=", "self", ".", "actor", "(", "state", ".", "to", "(", "self", ".", "device", ")", ",", "self", ".", "gumbel_temp_annealed", ")", ".", "cpu", "(", ")", "\n", "return", "action", "+", "(", "torch", ".", "randn", "(", "self", ".", "action_dim", ")", "*", "self", ".", "action_std", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.TD3_discrete_vary.TD3_discrete_vary.select_test_action": [[165, 168], ["TD3_discrete_vary.TD3_discrete_vary.actor().cpu", "TD3_discrete_vary.TD3_discrete_vary.actor", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "state.to"], "methods", ["None"], ["", "", "def", "select_test_action", "(", "self", ",", "state", ",", "env", ")", ":", "\n", "        ", "action", "=", "self", ".", "actor", "(", "state", ".", "to", "(", "self", ".", "device", ")", ",", "self", ".", "gumbel_temp_annealed", ")", ".", "cpu", "(", ")", "\n", "return", "action", "+", "(", "torch", ".", "randn", "(", "self", ".", "action_dim", ")", "*", "self", ".", "action_std", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.TD3_discrete_vary.TD3_discrete_vary.reset_optimizer": [[169, 174], ["list", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "TD3_discrete_vary.TD3_discrete_vary.actor.parameters", "list", "list", "TD3_discrete_vary.TD3_discrete_vary.critic_1.parameters", "TD3_discrete_vary.TD3_discrete_vary.critic_2.parameters"], "methods", ["None"], ["", "def", "reset_optimizer", "(", "self", ")", ":", "\n", "        ", "actor_params", "=", "list", "(", "self", ".", "actor", ".", "parameters", "(", ")", ")", "\n", "critic_params", "=", "list", "(", "self", ".", "critic_1", ".", "parameters", "(", ")", ")", "+", "list", "(", "self", ".", "critic_2", ".", "parameters", "(", ")", ")", "\n", "self", ".", "actor_optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "actor_params", ",", "lr", "=", "self", ".", "lr", ")", "\n", "self", ".", "critic_optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "critic_params", ",", "lr", "=", "self", ".", "lr", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.DDQN_vary.DDQN_vary.__init__": [[13, 25], ["print", "agents.DDQN.DDQN.__init__", "copy.deepcopy", "DDQN_vary.DDQN_vary.vary_hyperparameters"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__", "home.repos.pwc.inspect_result.automl_learning_environments.agents.DDQN_vary.DDQN_vary.vary_hyperparameters"], ["    ", "def", "__init__", "(", "self", ",", "env", ",", "config", ",", "icm", "=", "False", ")", ":", "\n", "        ", "self", ".", "agent_name", "=", "'ddqn'", "\n", "\n", "if", "config", "[", "\"agents\"", "]", "[", "\"ddqn_vary\"", "]", "[", "\"vary_hp\"", "]", ":", "\n", "            ", "config_mod", "=", "copy", ".", "deepcopy", "(", "config", ")", "\n", "config_mod", "=", "self", ".", "vary_hyperparameters", "(", "config_mod", ")", "\n", "", "else", ":", "\n", "            ", "config_mod", "=", "config", "\n", "\n", "", "print", "(", "\"full config: \"", ",", "config_mod", "[", "'agents'", "]", "[", "self", ".", "agent_name", "]", ")", "\n", "\n", "super", "(", ")", ".", "__init__", "(", "env", "=", "env", ",", "config", "=", "config_mod", ",", "icm", "=", "icm", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.DDQN_vary.DDQN_vary.vary_hyperparameters": [[26, 60], ["ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.add_hyperparameter", "ConfigSpace.ConfigurationSpace.sample_configuration", "print", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformFloatHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "ConfigSpace.UniformIntegerHyperparameter", "int", "int", "int", "int"], "methods", ["None"], ["", "def", "vary_hyperparameters", "(", "self", ",", "config_mod", ")", ":", "\n", "\n", "        ", "lr", "=", "config_mod", "[", "'agents'", "]", "[", "self", ".", "agent_name", "]", "[", "'lr'", "]", "\n", "batch_size", "=", "config_mod", "[", "'agents'", "]", "[", "self", ".", "agent_name", "]", "[", "'batch_size'", "]", "\n", "hidden_size", "=", "config_mod", "[", "'agents'", "]", "[", "self", ".", "agent_name", "]", "[", "'hidden_size'", "]", "\n", "hidden_layer", "=", "config_mod", "[", "'agents'", "]", "[", "self", ".", "agent_name", "]", "[", "'hidden_layer'", "]", "\n", "\n", "cs", "=", "CS", ".", "ConfigurationSpace", "(", ")", "\n", "cs", ".", "add_hyperparameter", "(", "CSH", ".", "UniformFloatHyperparameter", "(", "name", "=", "'lr'", ",", "lower", "=", "lr", "/", "3", ",", "upper", "=", "lr", "*", "3", ",", "log", "=", "True", ",", "default_value", "=", "lr", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "\n", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'batch_size'", ",", "lower", "=", "int", "(", "batch_size", "/", "3", ")", ",", "upper", "=", "int", "(", "batch_size", "*", "3", ")", ",", "log", "=", "True", ",", "\n", "default_value", "=", "batch_size", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "\n", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'hidden_size'", ",", "lower", "=", "int", "(", "hidden_size", "/", "3", ")", ",", "upper", "=", "int", "(", "hidden_size", "*", "3", ")", ",", "log", "=", "True", ",", "\n", "default_value", "=", "hidden_size", ")", ")", "\n", "cs", ".", "add_hyperparameter", "(", "\n", "CSH", ".", "UniformIntegerHyperparameter", "(", "name", "=", "'hidden_layer'", ",", "lower", "=", "hidden_layer", "-", "1", ",", "upper", "=", "hidden_layer", "+", "1", ",", "log", "=", "False", ",", "\n", "default_value", "=", "hidden_layer", ")", ")", "\n", "\n", "config", "=", "cs", ".", "sample_configuration", "(", ")", "\n", "\n", "print", "(", "f\"sampled part of config: \"", "\n", "f\"lr: {config['lr']}, \"", "\n", "f\"batch_size: {config['batch_size']}, \"", "\n", "f\"hidden_size: {config['hidden_size']}, \"", "\n", "f\"hidden_layer: {config['hidden_layer']}\"", "\n", ")", "\n", "\n", "config_mod", "[", "'agents'", "]", "[", "self", ".", "agent_name", "]", "[", "'lr'", "]", "=", "config", "[", "'lr'", "]", "\n", "config_mod", "[", "'agents'", "]", "[", "self", ".", "agent_name", "]", "[", "'batch_size'", "]", "=", "config", "[", "'batch_size'", "]", "\n", "config_mod", "[", "'agents'", "]", "[", "self", ".", "agent_name", "]", "[", "'hidden_size'", "]", "=", "config", "[", "'hidden_size'", "]", "\n", "config_mod", "[", "'agents'", "]", "[", "self", ".", "agent_name", "]", "[", "'hidden_layer'", "]", "=", "config", "[", "'hidden_layer'", "]", "\n", "\n", "return", "config_mod", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.QL.QL.__init__": [[13, 37], ["agents.base_agent.BaseAgent.__init__", "print", "numpy.zeros", "numpy.zeros", "range"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", ",", "config", ",", "count_based", "=", "False", ")", ":", "\n", "        ", "self", ".", "agent_name", "=", "\"ql\"", "\n", "super", "(", ")", ".", "__init__", "(", "agent_name", "=", "self", ".", "agent_name", ",", "env", "=", "env", ",", "config", "=", "config", ")", "\n", "\n", "ql_config", "=", "config", "[", "\"agents\"", "]", "[", "self", ".", "agent_name", "]", "\n", "\n", "self", ".", "batch_size", "=", "ql_config", "[", "\"batch_size\"", "]", "\n", "self", ".", "alpha", "=", "ql_config", "[", "\"alpha\"", "]", "\n", "self", ".", "gamma", "=", "ql_config", "[", "\"gamma\"", "]", "\n", "self", ".", "eps_init", "=", "ql_config", "[", "\"eps_init\"", "]", "\n", "self", ".", "eps_min", "=", "ql_config", "[", "\"eps_min\"", "]", "\n", "self", ".", "eps_decay", "=", "ql_config", "[", "\"eps_decay\"", "]", "\n", "self", ".", "q_table", "=", "[", "[", "0", "]", "*", "self", ".", "action_dim", "for", "_", "in", "range", "(", "self", ".", "state_dim", ")", "]", "\n", "self", ".", "count_based", "=", "count_based", "\n", "\n", "self", ".", "it", "=", "0", "\n", "\n", "if", "self", ".", "count_based", ":", "\n", "            ", "self", ".", "beta", "=", "ql_config", "[", "\"beta\"", "]", "\n", "print", "(", "\"beta: \"", ",", "self", ".", "beta", ")", "\n", "self", ".", "visitation_table", "=", "np", ".", "zeros", "(", "(", "self", ".", "state_dim", ",", "self", ".", "action_dim", ")", ")", "# n(s,a)", "\n", "# self.visitation_table_triple = np.zeros((self.state_dim, self.action_dim, self.state_dim))  # n(s,a,s')", "\n", "# self.r_hat = np.zeros((self.state_dim, self.action_dim))", "\n", "self", ".", "t_hat", "=", "np", ".", "zeros", "(", "(", "self", ".", "state_dim", ",", "self", ".", "action_dim", ",", "self", ".", "state_dim", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.QL.QL.learn": [[38, 74], ["range", "replay_buffer.clear", "replay_buffer.sample", "int", "int", "int", "reward.item.item.item", "done.item.item.item", "int.item", "int.item", "int.item", "math.sqrt", "max"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.None.utils.ReplayBuffer.clear", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.ReplayBuffer.sample"], ["", "", "def", "learn", "(", "self", ",", "replay_buffer", ",", "env", ",", "episode", ")", ":", "\n", "        ", "self", ".", "it", "+=", "1", "\n", "\n", "# if self.it % 5000 == 0:", "\n", "#     self.plot_q_function(env)", "\n", "\n", "for", "_", "in", "range", "(", "self", ".", "batch_size", ")", ":", "\n", "            ", "state", ",", "action", ",", "next_state", ",", "reward", ",", "done", "=", "replay_buffer", ".", "sample", "(", "1", ")", "\n", "state", "=", "int", "(", "state", ".", "item", "(", ")", ")", "\n", "action", "=", "int", "(", "action", ".", "item", "(", ")", ")", "\n", "next_state", "=", "int", "(", "next_state", ".", "item", "(", ")", ")", "\n", "reward", "=", "reward", ".", "item", "(", ")", "\n", "done", "=", "done", ".", "item", "(", ")", "\n", "\n", "if", "self", ".", "count_based", ":", "\n", "                ", "self", ".", "visitation_table", "[", "state", "]", "[", "action", "]", "+=", "1", "\n", "intrinsic_reward", "=", "self", ".", "beta", "/", "(", "math", ".", "sqrt", "(", "self", ".", "visitation_table", "[", "state", "]", "[", "action", "]", ")", "+", "1e-9", ")", "\n", "reward", "+=", "intrinsic_reward", "\n", "\n", "# MBIE-EB", "\n", "# self.visitation_table[state][action] += 1", "\n", "# self.visitation_table_triple[state][action][next_state] += 1", "\n", "# # self.r_hat[state][action] = reward  # deterministic & stationary MDP -> mean reward for (s,a) = reward for (s,a)", "\n", "# self.t_hat[state][action][next_state] = self.visitation_table_triple[state][action][next_state] / \\", "\n", "#                                         self.visitation_table[state][action]", "\n", "#", "\n", "# intrinsic_reward = self.beta / (math.sqrt(self.visitation_table[state][action]) + 1e-9)", "\n", "# reward += intrinsic_reward", "\n", "# t = sum(self.t_hat[state][action])", "\n", "# self.q_table[state][action] = reward + self.gamma * t * max(self.q_table[next_state]) * (done < 0.5)", "\n", "# else:", "\n", "\n", "", "delta", "=", "reward", "+", "self", ".", "gamma", "*", "max", "(", "self", ".", "q_table", "[", "next_state", "]", ")", "*", "(", "done", "<", "0.5", ")", "-", "self", ".", "q_table", "[", "state", "]", "[", "action", "]", "\n", "self", ".", "q_table", "[", "state", "]", "[", "action", "]", "+=", "self", ".", "alpha", "*", "delta", "\n", "\n", "", "replay_buffer", ".", "clear", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.QL.QL.plot_q_function": [[75, 87], ["print", "range", "range", "print", "max"], "methods", ["None"], ["", "def", "plot_q_function", "(", "self", ",", "env", ")", ":", "\n", "# m = len(env.env.grid)", "\n", "# n = len(env.env.grid[0])", "\n", "        ", "m", "=", "3", "\n", "n", "=", "4", "\n", "\n", "print", "(", "'----'", ")", "\n", "for", "i", "in", "range", "(", "m", ")", ":", "\n", "            ", "strng", "=", "''", "\n", "for", "k", "in", "range", "(", "n", ")", ":", "\n", "                ", "strng", "+=", "' {:3f}'", ".", "format", "(", "max", "(", "self", ".", "q_table", "[", "i", "*", "n", "+", "k", "]", ")", ")", "\n", "", "print", "(", "strng", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.QL.QL.select_train_action": [[88, 95], ["random.random", "env.get_random_action", "torch.tensor", "torch.argmax().unsqueeze().detach", "torch.argmax().unsqueeze", "int", "state.item", "torch.argmax"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.get_random_action"], ["", "", "def", "select_train_action", "(", "self", ",", "state", ",", "env", ",", "episode", ")", ":", "\n", "        ", "if", "random", ".", "random", "(", ")", "<", "self", ".", "eps", ":", "\n", "            ", "action", "=", "env", ".", "get_random_action", "(", ")", "\n", "return", "action", "\n", "", "else", ":", "\n", "            ", "q_vals", "=", "torch", ".", "tensor", "(", "self", ".", "q_table", "[", "int", "(", "state", ".", "item", "(", ")", ")", "]", ")", "\n", "return", "torch", ".", "argmax", "(", "q_vals", ")", ".", "unsqueeze", "(", "0", ")", ".", "detach", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.QL.QL.select_test_action": [[96, 99], ["torch.tensor", "torch.argmax().unsqueeze().detach", "torch.argmax().unsqueeze", "int", "state.item", "torch.argmax"], "methods", ["None"], ["", "", "def", "select_test_action", "(", "self", ",", "state", ",", "env", ")", ":", "\n", "        ", "q_vals", "=", "torch", ".", "tensor", "(", "self", ".", "q_table", "[", "int", "(", "state", ".", "item", "(", ")", ")", "]", ")", "\n", "return", "torch", ".", "argmax", "(", "q_vals", ")", ".", "unsqueeze", "(", "0", ")", ".", "detach", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.QL.QL.update_parameters_per_episode": [[100, 106], ["max"], "methods", ["None"], ["", "def", "update_parameters_per_episode", "(", "self", ",", "episode", ")", ":", "\n", "        ", "if", "episode", "==", "0", ":", "\n", "            ", "self", ".", "eps", "=", "self", ".", "eps_init", "\n", "", "else", ":", "\n", "            ", "self", ".", "eps", "*=", "self", ".", "eps_decay", "\n", "self", ".", "eps", "=", "max", "(", "self", ".", "eps", ",", "self", ".", "eps_min", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.DDQN.DDQN.__init__": [[15, 59], ["agents.base_agent.BaseAgent.__init__", "models.actor_critic.Critic_DQN().to", "models.actor_critic.Critic_DQN().to", "DDQN.DDQN.model_target.load_state_dict", "DDQN.DDQN.reset_optimizer", "DDQN.DDQN.model.state_dict", "models.icm_baseline.ICM", "models.actor_critic.Critic_DQN", "models.actor_critic.Critic_DQN", "env.has_discrete_action_space"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__", "home.repos.pwc.inspect_result.automl_learning_environments.agents.SAC.SAC.reset_optimizer", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.has_discrete_action_space"], ["    ", "def", "__init__", "(", "self", ",", "env", ",", "config", ",", "icm", "=", "False", ")", ":", "\n", "        ", "self", ".", "agent_name", "=", "\"ddqn\"", "\n", "\n", "super", "(", ")", ".", "__init__", "(", "agent_name", "=", "self", ".", "agent_name", ",", "env", "=", "env", ",", "config", "=", "config", ")", "\n", "\n", "ddqn_config", "=", "config", "[", "\"agents\"", "]", "[", "self", ".", "agent_name", "]", "\n", "\n", "self", ".", "full_config", "=", "config", "\n", "\n", "self", ".", "batch_size", "=", "ddqn_config", "[", "\"batch_size\"", "]", "\n", "self", ".", "rb_size", "=", "ddqn_config", "[", "\"rb_size\"", "]", "\n", "self", ".", "gamma", "=", "ddqn_config", "[", "\"gamma\"", "]", "\n", "self", ".", "lr", "=", "ddqn_config", "[", "\"lr\"", "]", "\n", "self", ".", "tau", "=", "ddqn_config", "[", "\"tau\"", "]", "\n", "self", ".", "eps", "=", "ddqn_config", "[", "\"eps_init\"", "]", "\n", "self", ".", "eps_init", "=", "ddqn_config", "[", "\"eps_init\"", "]", "\n", "self", ".", "eps_min", "=", "ddqn_config", "[", "\"eps_min\"", "]", "\n", "self", ".", "eps_decay", "=", "ddqn_config", "[", "\"eps_decay\"", "]", "\n", "\n", "self", ".", "model", "=", "Critic_DQN", "(", "self", ".", "state_dim", ",", "self", ".", "action_dim", ",", "self", ".", "agent_name", ",", "config", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "model_target", "=", "Critic_DQN", "(", "self", ".", "state_dim", ",", "self", ".", "action_dim", ",", "self", ".", "agent_name", ",", "config", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "model_target", ".", "load_state_dict", "(", "self", ".", "model", ".", "state_dict", "(", ")", ")", "\n", "\n", "self", ".", "reset_optimizer", "(", ")", "\n", "\n", "self", ".", "it", "=", "0", "\n", "\n", "self", ".", "icm", "=", "None", "\n", "if", "icm", ":", "\n", "            ", "icm_config", "=", "config", "[", "\"agents\"", "]", "[", "\"icm\"", "]", "\n", "self", ".", "icm_lr", "=", "icm_config", "[", "\"lr\"", "]", "\n", "self", ".", "icm_beta", "=", "icm_config", "[", "\"beta\"", "]", "\n", "self", ".", "icm_eta", "=", "icm_config", "[", "\"eta\"", "]", "\n", "self", ".", "icm_feature_dim", "=", "icm_config", "[", "\"feature_dim\"", "]", "\n", "self", ".", "icm_hidden_dim", "=", "icm_config", "[", "\"hidden_size\"", "]", "\n", "self", ".", "icm", "=", "ICM", "(", "state_dim", "=", "self", ".", "state_dim", ",", "\n", "action_dim", "=", "self", ".", "action_dim", ",", "\n", "has_discrete_actions", "=", "env", ".", "has_discrete_action_space", "(", ")", ",", "\n", "learning_rate", "=", "self", ".", "icm_lr", ",", "\n", "beta", "=", "self", ".", "icm_beta", ",", "\n", "eta", "=", "self", ".", "icm_eta", ",", "\n", "feature_dim", "=", "self", ".", "icm_feature_dim", ",", "\n", "hidden_size", "=", "self", ".", "icm_hidden_dim", ",", "\n", "device", "=", "self", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.DDQN.DDQN.learn": [[60, 96], ["replay_buffer.sample", "utils.to_one_hot_encoding.squeeze", "actions.squeeze.squeeze.squeeze", "utils.to_one_hot_encoding.squeeze", "rewards.squeeze.squeeze.squeeze", "dones.squeeze.squeeze.squeeze", "env.has_discrete_state_space", "DDQN.DDQN.model", "DDQN.DDQN.model", "DDQN.DDQN.model_target", "DDQN.DDQN.gather().squeeze", "DDQN.DDQN.gather().squeeze", "torch.mse_loss", "torch.mse_loss", "DDQN.DDQN.optimizer.zero_grad", "torch.mse_loss.backward", "DDQN.DDQN.optimizer.step", "zip", "utils.to_one_hot_encoding", "utils.to_one_hot_encoding", "DDQN.DDQN.icm.train", "DDQN.DDQN.icm.compute_intrinsic_rewards().squeeze", "expected_q_value.detach", "DDQN.DDQN.model_target.parameters", "DDQN.DDQN.model.parameters", "target_param.data.copy_", "DDQN.DDQN.gather", "DDQN.DDQN.gather", "DDQN.DDQN.icm.compute_intrinsic_rewards", "actions.squeeze.squeeze.long().unsqueeze", "[].unsqueeze", "actions.squeeze.squeeze.long", "DDQN.DDQN.max"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.None.utils.ReplayBuffer.sample", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.has_discrete_state_space", "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditEnv.step", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.to_one_hot_encoding", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.to_one_hot_encoding", "home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.train", "home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.compute_intrinsic_rewards"], ["", "", "def", "learn", "(", "self", ",", "replay_buffer", ",", "env", ",", "episode", ")", ":", "\n", "        ", "self", ".", "it", "+=", "1", "\n", "\n", "states", ",", "actions", ",", "next_states", ",", "rewards", ",", "dones", "=", "replay_buffer", ".", "sample", "(", "self", ".", "batch_size", ")", "\n", "\n", "states", "=", "states", ".", "squeeze", "(", ")", "\n", "actions", "=", "actions", ".", "squeeze", "(", ")", "\n", "next_states", "=", "next_states", ".", "squeeze", "(", ")", "\n", "rewards", "=", "rewards", ".", "squeeze", "(", ")", "\n", "dones", "=", "dones", ".", "squeeze", "(", ")", "\n", "\n", "if", "env", ".", "has_discrete_state_space", "(", ")", ":", "\n", "            ", "states", "=", "to_one_hot_encoding", "(", "states", ",", "self", ".", "state_dim", ")", "\n", "next_states", "=", "to_one_hot_encoding", "(", "next_states", ",", "self", ".", "state_dim", ")", "\n", "\n", "", "if", "self", ".", "icm", ":", "\n", "            ", "self", ".", "icm", ".", "train", "(", "states", ",", "next_states", ",", "actions", ")", "\n", "rewards", "+=", "self", ".", "icm", ".", "compute_intrinsic_rewards", "(", "states", ",", "next_states", ",", "actions", ")", ".", "squeeze", "(", ")", "\n", "\n", "", "q_values", "=", "self", ".", "model", "(", "states", ")", "\n", "next_q_values", "=", "self", ".", "model", "(", "next_states", ")", "\n", "next_q_state_values", "=", "self", ".", "model_target", "(", "next_states", ")", "\n", "\n", "q_value", "=", "q_values", ".", "gather", "(", "1", ",", "actions", ".", "long", "(", ")", ".", "unsqueeze", "(", "1", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "next_q_value", "=", "next_q_state_values", ".", "gather", "(", "1", ",", "next_q_values", ".", "max", "(", "1", ")", "[", "1", "]", ".", "unsqueeze", "(", "1", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "expected_q_value", "=", "rewards", "+", "self", ".", "gamma", "*", "next_q_value", "*", "(", "1", "-", "dones", ")", "\n", "loss", "=", "F", ".", "mse_loss", "(", "q_value", ",", "expected_q_value", ".", "detach", "(", ")", ")", "\n", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "# target network update", "\n", "for", "target_param", ",", "param", "in", "zip", "(", "self", ".", "model_target", ".", "parameters", "(", ")", ",", "self", ".", "model", ".", "parameters", "(", ")", ")", ":", "\n", "            ", "target_param", ".", "data", ".", "copy_", "(", "self", ".", "tau", "*", "param", "+", "(", "1", "-", "self", ".", "tau", ")", "*", "target_param", ")", "\n", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.DDQN.DDQN.select_train_action": [[97, 105], ["random.random", "env.get_random_action", "env.has_discrete_state_space", "DDQN.DDQN.model", "torch.argmax().unsqueeze().detach", "torch.argmax().unsqueeze().detach", "torch.argmax().unsqueeze().detach", "torch.argmax().unsqueeze().detach", "utils.to_one_hot_encoding", "utils.to_one_hot_encoding.to", "torch.argmax().unsqueeze", "torch.argmax().unsqueeze", "torch.argmax().unsqueeze", "torch.argmax().unsqueeze", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.get_random_action", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.has_discrete_state_space", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.to_one_hot_encoding"], ["", "def", "select_train_action", "(", "self", ",", "state", ",", "env", ",", "episode", ")", ":", "\n", "        ", "if", "random", ".", "random", "(", ")", "<", "self", ".", "eps", ":", "\n", "            ", "return", "env", ".", "get_random_action", "(", ")", "\n", "", "else", ":", "\n", "            ", "if", "env", ".", "has_discrete_state_space", "(", ")", ":", "\n", "                ", "state", "=", "to_one_hot_encoding", "(", "state", ",", "self", ".", "state_dim", ")", "\n", "", "qvals", "=", "self", ".", "model", "(", "state", ".", "to", "(", "self", ".", "device", ")", ")", "\n", "return", "torch", ".", "argmax", "(", "qvals", ")", ".", "unsqueeze", "(", "0", ")", ".", "detach", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.DDQN.DDQN.select_test_action": [[106, 111], ["env.has_discrete_state_space", "DDQN.DDQN.model", "torch.argmax().unsqueeze().detach", "torch.argmax().unsqueeze().detach", "torch.argmax().unsqueeze().detach", "torch.argmax().unsqueeze().detach", "utils.to_one_hot_encoding", "utils.to_one_hot_encoding.to", "torch.argmax().unsqueeze", "torch.argmax().unsqueeze", "torch.argmax().unsqueeze", "torch.argmax().unsqueeze", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.has_discrete_state_space", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.to_one_hot_encoding"], ["", "", "def", "select_test_action", "(", "self", ",", "state", ",", "env", ")", ":", "\n", "        ", "if", "env", ".", "has_discrete_state_space", "(", ")", ":", "\n", "            ", "state", "=", "to_one_hot_encoding", "(", "state", ",", "self", ".", "state_dim", ")", "\n", "", "qvals", "=", "self", ".", "model", "(", "state", ".", "to", "(", "self", ".", "device", ")", ")", "\n", "return", "torch", ".", "argmax", "(", "qvals", ")", ".", "unsqueeze", "(", "0", ")", ".", "detach", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.DDQN.DDQN.update_parameters_per_episode": [[112, 118], ["max"], "methods", ["None"], ["", "def", "update_parameters_per_episode", "(", "self", ",", "episode", ")", ":", "\n", "        ", "if", "episode", "==", "0", ":", "\n", "            ", "self", ".", "eps", "=", "self", ".", "eps_init", "\n", "", "else", ":", "\n", "            ", "self", ".", "eps", "*=", "self", ".", "eps_decay", "\n", "self", ".", "eps", "=", "max", "(", "self", ".", "eps", ",", "self", ".", "eps_min", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.DDQN.DDQN.reset_optimizer": [[119, 121], ["torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "DDQN.DDQN.model.parameters"], "methods", ["None"], ["", "", "def", "reset_optimizer", "(", "self", ")", ":", "\n", "        ", "self", ".", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "self", ".", "model", ".", "parameters", "(", ")", ",", "lr", "=", "self", ".", "lr", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.TD3.TD3.__init__": [[14, 62], ["agents.base_agent.BaseAgent.__init__", "models.actor_critic.Actor_TD3().to", "models.actor_critic.Actor_TD3().to", "TD3.TD3.actor_target.load_state_dict", "models.actor_critic.Critic_Q().to", "models.actor_critic.Critic_Q().to", "models.actor_critic.Critic_Q().to", "models.actor_critic.Critic_Q().to", "TD3.TD3.critic_target_1.load_state_dict", "TD3.TD3.critic_target_2.load_state_dict", "TD3.TD3.reset_optimizer", "TD3.TD3.actor.state_dict", "TD3.TD3.critic_1.state_dict", "TD3.TD3.critic_2.state_dict", "models.icm_baseline.ICM", "models.actor_critic.Actor_TD3", "models.actor_critic.Actor_TD3", "models.actor_critic.Critic_Q", "models.actor_critic.Critic_Q", "models.actor_critic.Critic_Q", "models.actor_critic.Critic_Q", "env.has_discrete_action_space"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__", "home.repos.pwc.inspect_result.automl_learning_environments.agents.SAC.SAC.reset_optimizer", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.has_discrete_action_space"], ["    ", "def", "__init__", "(", "self", ",", "env", ",", "max_action", ",", "config", ",", "icm", "=", "False", ")", ":", "\n", "        ", "agent_name", "=", "\"td3\"", "\n", "super", "(", ")", ".", "__init__", "(", "agent_name", "=", "agent_name", ",", "env", "=", "env", ",", "config", "=", "config", ")", "\n", "\n", "td3_config", "=", "config", "[", "\"agents\"", "]", "[", "agent_name", "]", "\n", "\n", "self", ".", "max_action", "=", "max_action", "\n", "self", ".", "batch_size", "=", "td3_config", "[", "\"batch_size\"", "]", "\n", "self", ".", "rb_size", "=", "td3_config", "[", "\"rb_size\"", "]", "\n", "self", ".", "gamma", "=", "td3_config", "[", "\"gamma\"", "]", "\n", "self", ".", "tau", "=", "td3_config", "[", "\"tau\"", "]", "\n", "self", ".", "policy_delay", "=", "td3_config", "[", "\"policy_delay\"", "]", "\n", "self", ".", "lr", "=", "td3_config", "[", "\"lr\"", "]", "\n", "self", ".", "action_std", "=", "td3_config", "[", "\"action_std\"", "]", "\n", "self", ".", "policy_std", "=", "td3_config", "[", "\"policy_std\"", "]", "\n", "self", ".", "policy_std_clip", "=", "td3_config", "[", "\"policy_std_clip\"", "]", "\n", "\n", "self", ".", "actor", "=", "Actor_TD3", "(", "self", ".", "state_dim", ",", "self", ".", "action_dim", ",", "max_action", ",", "agent_name", ",", "config", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "actor_target", "=", "Actor_TD3", "(", "self", ".", "state_dim", ",", "self", ".", "action_dim", ",", "max_action", ",", "agent_name", ",", "config", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "actor_target", ".", "load_state_dict", "(", "self", ".", "actor", ".", "state_dict", "(", ")", ")", "\n", "self", ".", "critic_1", "=", "Critic_Q", "(", "self", ".", "state_dim", ",", "self", ".", "action_dim", ",", "agent_name", ",", "config", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "critic_2", "=", "Critic_Q", "(", "self", ".", "state_dim", ",", "self", ".", "action_dim", ",", "agent_name", ",", "config", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "critic_target_1", "=", "Critic_Q", "(", "self", ".", "state_dim", ",", "self", ".", "action_dim", ",", "agent_name", ",", "config", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "critic_target_2", "=", "Critic_Q", "(", "self", ".", "state_dim", ",", "self", ".", "action_dim", ",", "agent_name", ",", "config", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "critic_target_1", ".", "load_state_dict", "(", "self", ".", "critic_1", ".", "state_dict", "(", ")", ")", "\n", "self", ".", "critic_target_2", ".", "load_state_dict", "(", "self", ".", "critic_2", ".", "state_dict", "(", ")", ")", "\n", "\n", "self", ".", "reset_optimizer", "(", ")", "\n", "\n", "self", ".", "total_it", "=", "0", "\n", "\n", "self", ".", "icm", "=", "None", "\n", "if", "icm", ":", "\n", "            ", "icm_config", "=", "config", "[", "\"agents\"", "]", "[", "\"icm\"", "]", "\n", "self", ".", "icm_lr", "=", "icm_config", "[", "\"lr\"", "]", "\n", "self", ".", "icm_beta", "=", "icm_config", "[", "\"beta\"", "]", "\n", "self", ".", "icm_eta", "=", "icm_config", "[", "\"eta\"", "]", "\n", "self", ".", "icm_feature_dim", "=", "icm_config", "[", "\"feature_dim\"", "]", "\n", "self", ".", "icm_hidden_dim", "=", "icm_config", "[", "\"hidden_size\"", "]", "\n", "self", ".", "icm", "=", "ICM", "(", "state_dim", "=", "self", ".", "state_dim", ",", "\n", "action_dim", "=", "self", ".", "action_dim", ",", "\n", "has_discrete_actions", "=", "env", ".", "has_discrete_action_space", "(", ")", ",", "\n", "learning_rate", "=", "self", ".", "icm_lr", ",", "\n", "beta", "=", "self", ".", "icm_beta", ",", "\n", "eta", "=", "self", ".", "icm_eta", ",", "\n", "feature_dim", "=", "self", ".", "icm_feature_dim", ",", "\n", "hidden_size", "=", "self", ".", "icm_hidden_dim", ",", "\n", "device", "=", "self", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.TD3.TD3.learn": [[63, 117], ["replay_buffer.sample", "TD3.TD3.critic_1", "TD3.TD3.critic_2", "TD3.TD3.critic_optimizer.zero_grad", "critic_loss.backward", "TD3.TD3.critic_optimizer.step", "TD3.TD3.icm.train", "TD3.TD3.icm.compute_intrinsic_rewards", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "TD3.TD3.critic_target_1", "TD3.TD3.critic_target_2", "torch.min", "torch.min", "torch.min", "torch.min", "torch.mse_loss", "torch.mse_loss", "torch.mse_loss", "torch.mse_loss", "TD3.TD3.actor_optimizer.zero_grad", "actor_loss.backward", "TD3.TD3.actor_optimizer.step", "zip", "zip", "zip", "TD3.TD3.critic_1.parameters", "TD3.TD3.critic_target_1.parameters", "target_param.data.copy_", "TD3.TD3.critic_2.parameters", "TD3.TD3.critic_target_2.parameters", "target_param.data.copy_", "TD3.TD3.actor.parameters", "TD3.TD3.actor_target.parameters", "target_param.data.copy_", "torch.randn_like", "torch.randn_like", "torch.randn_like", "torch.randn_like", "TD3.TD3.actor_target", "TD3.TD3.critic_1", "TD3.TD3.actor"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.None.utils.ReplayBuffer.sample", "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditEnv.step", "home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.train", "home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.compute_intrinsic_rewards", "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditEnv.step"], ["", "", "def", "learn", "(", "self", ",", "replay_buffer", ",", "env", ",", "episode", ")", ":", "\n", "        ", "self", ".", "total_it", "+=", "1", "\n", "\n", "# Sample replay buffer", "\n", "states", ",", "actions", ",", "next_states", ",", "rewards", ",", "dones", "=", "replay_buffer", ".", "sample", "(", "self", ".", "batch_size", ")", "\n", "\n", "if", "self", ".", "icm", ":", "\n", "            ", "self", ".", "icm", ".", "train", "(", "states", ",", "next_states", ",", "actions", ")", "\n", "rewards", "+=", "self", ".", "icm", ".", "compute_intrinsic_rewards", "(", "states", ",", "next_states", ",", "actions", ")", "\n", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# Select action according to policy and add clipped noise, no_grad since target will be copied", "\n", "            ", "noise", "=", "(", "torch", ".", "randn_like", "(", "actions", ")", "*", "self", ".", "policy_std", ")", ".", "clamp", "(", "-", "self", ".", "policy_std_clip", ",", "self", ".", "policy_std_clip", ")", "\n", "next_actions", "=", "(", "self", ".", "actor_target", "(", "next_states", ")", "+", "noise", ")", ".", "clamp", "(", "-", "self", ".", "max_action", ",", "self", ".", "max_action", ")", "\n", "\n", "# Compute the target Q value", "\n", "target_Q1", "=", "self", ".", "critic_target_1", "(", "next_states", ",", "next_actions", ")", "\n", "target_Q2", "=", "self", ".", "critic_target_2", "(", "next_states", ",", "next_actions", ")", "\n", "target_Q", "=", "torch", ".", "min", "(", "target_Q1", ",", "target_Q2", ")", "\n", "target_Q", "=", "rewards", "+", "(", "1", "-", "dones", ")", "*", "self", ".", "gamma", "*", "target_Q", "\n", "# target_Q = rewards + self.gamma * target_Q", "\n", "\n", "# Get current Q estimates", "\n", "", "current_Q1", "=", "self", ".", "critic_1", "(", "states", ",", "actions", ")", "\n", "current_Q2", "=", "self", ".", "critic_2", "(", "states", ",", "actions", ")", "\n", "\n", "# Compute critic loss", "\n", "critic_loss", "=", "F", ".", "mse_loss", "(", "current_Q1", ",", "target_Q", ")", "+", "F", ".", "mse_loss", "(", "current_Q2", ",", "target_Q", ")", "\n", "\n", "# Optimize the critic", "\n", "self", ".", "critic_optimizer", ".", "zero_grad", "(", ")", "\n", "critic_loss", ".", "backward", "(", ")", "\n", "self", ".", "critic_optimizer", ".", "step", "(", ")", "\n", "\n", "# Delayed policy updates", "\n", "if", "self", ".", "total_it", "%", "self", ".", "policy_delay", "==", "0", ":", "\n", "# Compute actor loss", "\n", "# todo: check algorithm 1 in original paper; has additional multiplicative term here", "\n", "            ", "actor_loss", "=", "(", "-", "self", ".", "critic_1", "(", "states", ",", "self", ".", "actor", "(", "states", ")", ")", ")", ".", "mean", "(", ")", "\n", "\n", "# Optimize the actor", "\n", "self", ".", "actor_optimizer", ".", "zero_grad", "(", ")", "\n", "actor_loss", ".", "backward", "(", ")", "\n", "self", ".", "actor_optimizer", ".", "step", "(", ")", "\n", "\n", "# Update the frozen target models", "\n", "for", "param", ",", "target_param", "in", "zip", "(", "self", ".", "critic_1", ".", "parameters", "(", ")", ",", "self", ".", "critic_target_1", ".", "parameters", "(", ")", ")", ":", "\n", "                ", "target_param", ".", "data", ".", "copy_", "(", "self", ".", "tau", "*", "param", ".", "data", "+", "(", "1", "-", "self", ".", "tau", ")", "*", "target_param", ".", "data", ")", "\n", "\n", "", "for", "param", ",", "target_param", "in", "zip", "(", "self", ".", "critic_2", ".", "parameters", "(", ")", ",", "self", ".", "critic_target_2", ".", "parameters", "(", ")", ")", ":", "\n", "                ", "target_param", ".", "data", ".", "copy_", "(", "self", ".", "tau", "*", "param", ".", "data", "+", "(", "1", "-", "self", ".", "tau", ")", "*", "target_param", ".", "data", ")", "\n", "\n", "", "for", "param", ",", "target_param", "in", "zip", "(", "self", ".", "actor", ".", "parameters", "(", ")", ",", "self", ".", "actor_target", ".", "parameters", "(", ")", ")", ":", "\n", "                ", "target_param", ".", "data", ".", "copy_", "(", "self", ".", "tau", "*", "param", ".", "data", "+", "(", "1", "-", "self", ".", "tau", ")", "*", "target_param", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.TD3.TD3.select_train_action": [[118, 125], ["env.get_random_action", "TD3.TD3.actor().cpu", "TD3.TD3.actor", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "state.to"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.get_random_action"], ["", "", "", "def", "select_train_action", "(", "self", ",", "state", ",", "env", ",", "episode", ")", ":", "\n", "        ", "if", "episode", "<", "self", ".", "init_episodes", ":", "\n", "            ", "return", "env", ".", "get_random_action", "(", ")", "\n", "", "else", ":", "\n", "            ", "return", "(", "self", ".", "actor", "(", "state", ".", "to", "(", "self", ".", "device", ")", ")", ".", "cpu", "(", ")", "+", "\n", "torch", ".", "randn", "(", "self", ".", "action_dim", ")", "*", "self", ".", "action_std", "*", "self", ".", "max_action", "\n", ")", ".", "clamp", "(", "-", "self", ".", "max_action", ",", "self", ".", "max_action", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.TD3.TD3.select_test_action": [[126, 130], ["TD3.TD3.actor().cpu", "TD3.TD3.actor", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "state.to"], "methods", ["None"], ["", "", "def", "select_test_action", "(", "self", ",", "state", ",", "env", ")", ":", "\n", "        ", "return", "(", "self", ".", "actor", "(", "state", ".", "to", "(", "self", ".", "device", ")", ")", ".", "cpu", "(", ")", "+", "\n", "torch", ".", "randn", "(", "self", ".", "action_dim", ")", "*", "self", ".", "action_std", "*", "self", ".", "max_action", "\n", ")", ".", "clamp", "(", "-", "self", ".", "max_action", ",", "self", ".", "max_action", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.TD3.TD3.reset_optimizer": [[131, 136], ["list", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "TD3.TD3.actor.parameters", "list", "list", "TD3.TD3.critic_1.parameters", "TD3.TD3.critic_2.parameters"], "methods", ["None"], ["", "def", "reset_optimizer", "(", "self", ")", ":", "\n", "        ", "actor_params", "=", "list", "(", "self", ".", "actor", ".", "parameters", "(", ")", ")", "\n", "critic_params", "=", "list", "(", "self", ".", "critic_1", ".", "parameters", "(", ")", ")", "+", "list", "(", "self", ".", "critic_2", ".", "parameters", "(", ")", ")", "\n", "self", ".", "actor_optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "actor_params", ",", "lr", "=", "self", ".", "lr", ")", "\n", "self", ".", "critic_optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "critic_params", ",", "lr", "=", "self", ".", "lr", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.base_agent.BaseAgent.__init__": [[9, 27], ["torch.Module.__init__", "env.get_state_dim", "env.get_action_dim"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.get_state_dim", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.get_action_dim"], ["    ", "def", "__init__", "(", "self", ",", "agent_name", ",", "env", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "state_dim", "=", "env", ".", "get_state_dim", "(", ")", "\n", "self", ".", "action_dim", "=", "env", ".", "get_action_dim", "(", ")", "\n", "\n", "agent_config", "=", "config", "[", "\"agents\"", "]", "[", "agent_name", "]", "\n", "self", ".", "train_episodes", "=", "agent_config", "[", "\"train_episodes\"", "]", "\n", "self", ".", "test_episodes", "=", "agent_config", "[", "\"test_episodes\"", "]", "\n", "self", ".", "init_episodes", "=", "agent_config", "[", "\"init_episodes\"", "]", "\n", "self", ".", "rb_size", "=", "agent_config", "[", "\"rb_size\"", "]", "\n", "self", ".", "same_action_num", "=", "agent_config", "[", "\"same_action_num\"", "]", "\n", "self", ".", "print_rate", "=", "agent_config", "[", "\"print_rate\"", "]", "\n", "self", ".", "early_out_num", "=", "agent_config", "[", "\"early_out_num\"", "]", "\n", "self", ".", "early_out_virtual_diff", "=", "agent_config", "[", "\"early_out_virtual_diff\"", "]", "\n", "\n", "self", ".", "render_env", "=", "config", "[", "\"render_env\"", "]", "\n", "self", ".", "device", "=", "config", "[", "\"device\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.base_agent.BaseAgent.time_is_up": [[30, 48], ["print", "len", "avg_meter_reward.update", "len", "avg_meter_reward.update", "len", "avg_meter_episode_length.update", "len", "avg_meter_episode_length.update", "avg_meter_reward.get_raw_data", "avg_meter_reward.get_raw_data", "min", "avg_meter_episode_length.get_raw_data", "avg_meter_episode_length.get_raw_data", "max", "avg_meter_reward.get_raw_data", "avg_meter_episode_length.get_raw_data"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.None.utils.AverageMeter.update", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.AverageMeter.update", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.AverageMeter.update", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.AverageMeter.update", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.AverageMeter.get_raw_data", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.AverageMeter.get_raw_data", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.AverageMeter.get_raw_data", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.AverageMeter.get_raw_data", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.AverageMeter.get_raw_data", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.AverageMeter.get_raw_data"], ["", "def", "time_is_up", "(", "self", ",", "avg_meter_reward", ",", "avg_meter_episode_length", ",", "max_episodes", ",", "time_elapsed", ",", "time_remaining", ")", ":", "\n", "        ", "if", "time_elapsed", ">", "time_remaining", ":", "\n", "            ", "print", "(", "\"timeout\"", ")", "\n", "# fill remaining rewards with minimum reward achieved so far", "\n", "if", "len", "(", "avg_meter_reward", ".", "get_raw_data", "(", ")", ")", "==", "0", ":", "\n", "                ", "avg_meter_reward", ".", "update", "(", "-", "1e9", ")", "\n", "", "while", "len", "(", "avg_meter_reward", ".", "get_raw_data", "(", ")", ")", "<", "max_episodes", ":", "\n", "                ", "avg_meter_reward", ".", "update", "(", "min", "(", "avg_meter_reward", ".", "get_raw_data", "(", ")", ")", ",", "print_rate", "=", "1e9", ")", "\n", "\n", "# also fill remaining episode lengths with maximum length achieved so far so that avg_meter_reward and avg_meter_episode_length", "\n", "# have same length for computing AUC", "\n", "", "if", "len", "(", "avg_meter_episode_length", ".", "get_raw_data", "(", ")", ")", "==", "0", ":", "\n", "                ", "avg_meter_episode_length", ".", "update", "(", "1e9", ")", "\n", "", "while", "len", "(", "avg_meter_episode_length", ".", "get_raw_data", "(", ")", ")", "<", "max_episodes", ":", "\n", "                ", "avg_meter_episode_length", ".", "update", "(", "max", "(", "avg_meter_episode_length", ".", "get_raw_data", "(", ")", ")", ",", "print_rate", "=", "1e9", ")", "\n", "", "return", "True", "\n", "", "else", ":", "\n", "            ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.base_agent.BaseAgent.env_solved": [[49, 63], ["avg_meter_reward.get_mean", "avg_meter_reward.get_mean_last", "env.is_virtual_env", "print", "env.get_solved_reward", "print", "abs", "abs"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.None.utils.AverageMeter.get_mean", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.AverageMeter.get_mean_last", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.is_virtual_env", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.get_solved_reward"], ["", "", "def", "env_solved", "(", "self", ",", "env", ",", "avg_meter_reward", ",", "episode", ")", ":", "\n", "        ", "avg_reward", "=", "avg_meter_reward", ".", "get_mean", "(", "num", "=", "self", ".", "early_out_num", ")", "\n", "avg_reward_last", "=", "avg_meter_reward", ".", "get_mean_last", "(", "num", "=", "self", ".", "early_out_num", ")", "\n", "if", "env", ".", "is_virtual_env", "(", ")", ":", "\n", "            ", "if", "abs", "(", "avg_reward", "-", "avg_reward_last", ")", "/", "(", "abs", "(", "avg_reward_last", ")", "+", "1e-9", ")", "<", "self", ".", "early_out_virtual_diff", "and", "episode", ">=", "self", ".", "init_episodes", "+", "self", ".", "early_out_num", ":", "\n", "                ", "print", "(", "\"early out on virtual env after {} episodes with an average reward of {}\"", ".", "format", "(", "episode", "+", "1", ",", "avg_reward", ")", ")", "\n", "return", "True", "\n", "", "", "else", ":", "\n", "            ", "if", "avg_reward", ">=", "env", ".", "get_solved_reward", "(", ")", ":", "\n", "                ", "print", "(", "\"early out on real env after {} episodes with an average reward of {}\"", ".", "format", "(", "episode", "+", "1", ",", "avg_reward", ")", ")", "\n", "return", "True", "\n", "\n", "", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.base_agent.BaseAgent.train": [[64, 154], ["time.time", "env.has_discrete_action_space", "utils.ReplayBuffer", "utils.AverageMeter", "utils.AverageMeter", "env.set_agent_params", "range", "env.close", "env.has_discrete_state_space", "base_agent.BaseAgent.time_is_up", "hasattr", "env.reset", "range", "utils.AverageMeter.update", "utils.AverageMeter.get_raw_data", "utils.AverageMeter.get_raw_data", "env.get_action_dim", "base_agent.BaseAgent.update_parameters_per_episode", "env.max_episode_steps", "base_agent.BaseAgent.select_train_action", "utils.ReplayBuffer.add", "base_agent.BaseAgent.test", "utils.AverageMeter.update", "utils.AverageMeter.update", "base_agent.BaseAgent.env_solved", "env.render", "env.step", "env.step", "base_agent.BaseAgent.learn", "statistics.mean", "print", "time.time", "base_agent.BaseAgent.argmax().unsqueeze", "str", "base_agent.BaseAgent.argmax"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.has_discrete_action_space", "home.repos.pwc.inspect_result.automl_learning_environments.envs.reward_env.RewardEnv.set_agent_params", "home.repos.pwc.inspect_result.automl_learning_environments.envs.reward_env.RewardEnv.close", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.has_discrete_state_space", "home.repos.pwc.inspect_result.automl_learning_environments.agents.base_agent.BaseAgent.time_is_up", "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditEnv.reset", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.AverageMeter.update", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.AverageMeter.get_raw_data", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.AverageMeter.get_raw_data", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.get_action_dim", "home.repos.pwc.inspect_result.automl_learning_environments.agents.DDQN.DDQN.update_parameters_per_episode", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.max_episode_steps", "home.repos.pwc.inspect_result.automl_learning_environments.agents.PPO.PPO.select_train_action", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.ReplayBuffer.add", "home.repos.pwc.inspect_result.automl_learning_environments.agents.base_agent.BaseAgent.test", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.AverageMeter.update", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.AverageMeter.update", "home.repos.pwc.inspect_result.automl_learning_environments.agents.base_agent.BaseAgent.env_solved", "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditEnv.render", "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditEnv.step", "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditEnv.step", "home.repos.pwc.inspect_result.automl_learning_environments.agents.PPO.PPO.learn"], ["", "def", "train", "(", "self", ",", "env", ",", "test_env", "=", "None", ",", "time_remaining", "=", "1e9", ")", ":", "\n", "        ", "time_start", "=", "time", ".", "time", "(", ")", "\n", "\n", "discretize_action", "=", "False", "\n", "\n", "sd", "=", "1", "if", "env", ".", "has_discrete_state_space", "(", ")", "else", "self", ".", "state_dim", "\n", "\n", "# todo: @fabio use \"hasattr\" and custom function in derived class (see below)", "\n", "if", "env", ".", "has_discrete_action_space", "(", ")", ":", "\n", "            ", "ad", "=", "1", "\n", "# in case of td3_discrete, action_dim=1 does not reflect the required action_dim for the gumbel softmax distribution", "\n", "if", "\"td3_discrete\"", "in", "self", ".", "agent_name", ":", "\n", "                ", "ad", "=", "env", ".", "get_action_dim", "(", ")", "\n", "discretize_action", "=", "True", "\n", "", "", "else", ":", "\n", "            ", "ad", "=", "self", ".", "action_dim", "\n", "\n", "", "replay_buffer", "=", "ReplayBuffer", "(", "state_dim", "=", "sd", ",", "action_dim", "=", "ad", ",", "device", "=", "self", ".", "device", ",", "max_size", "=", "self", ".", "rb_size", ")", "\n", "\n", "avg_meter_reward", "=", "AverageMeter", "(", "print_str", "=", "\"Average reward: \"", ")", "\n", "avg_meter_episode_length", "=", "AverageMeter", "(", "print_str", "=", "\"Average episode length: \"", ")", "\n", "\n", "env", ".", "set_agent_params", "(", "same_action_num", "=", "self", ".", "same_action_num", ",", "gamma", "=", "self", ".", "gamma", ")", "\n", "\n", "# training loop", "\n", "for", "episode", "in", "range", "(", "self", ".", "train_episodes", ")", ":", "\n", "# early out if timeout", "\n", "            ", "if", "self", ".", "time_is_up", "(", "avg_meter_reward", "=", "avg_meter_reward", ",", "\n", "avg_meter_episode_length", "=", "avg_meter_episode_length", ",", "\n", "max_episodes", "=", "self", ".", "train_episodes", ",", "\n", "time_elapsed", "=", "time", ".", "time", "(", ")", "-", "time_start", ",", "\n", "time_remaining", "=", "time_remaining", ")", ":", "\n", "                ", "break", "\n", "\n", "", "if", "hasattr", "(", "self", ",", "'update_parameters_per_episode'", ")", ":", "\n", "                ", "self", ".", "update_parameters_per_episode", "(", "episode", "=", "episode", ")", "\n", "\n", "", "state", "=", "env", ".", "reset", "(", ")", "\n", "episode_reward", "=", "0", "\n", "episode_length", "=", "0", "\n", "for", "t", "in", "range", "(", "0", ",", "env", ".", "max_episode_steps", "(", ")", ",", "self", ".", "same_action_num", ")", ":", "\n", "                ", "action", "=", "self", ".", "select_train_action", "(", "state", "=", "state", ",", "env", "=", "env", ",", "episode", "=", "episode", ")", "\n", "\n", "# live view", "\n", "if", "self", ".", "render_env", ":", "\n", "                    ", "env", ".", "render", "(", ")", "\n", "\n", "# state-action transition", "\n", "# required due to gumble softmax in td3 discrete", "\n", "# todo @fabio: move into agent-specific select_train_action, do the same for test", "\n", "", "if", "discretize_action", ":", "\n", "                    ", "next_state", ",", "reward", ",", "done", "=", "env", ".", "step", "(", "action", "=", "action", ".", "argmax", "(", ")", ".", "unsqueeze", "(", "0", ")", ")", "\n", "", "else", ":", "\n", "                    ", "next_state", ",", "reward", ",", "done", "=", "env", ".", "step", "(", "action", "=", "action", ")", "\n", "", "replay_buffer", ".", "add", "(", "state", "=", "state", ",", "action", "=", "action", ",", "next_state", "=", "next_state", ",", "reward", "=", "reward", ",", "done", "=", "done", ")", "\n", "\n", "state", "=", "next_state", "\n", "episode_reward", "+=", "reward", "\n", "episode_length", "+=", "self", ".", "same_action_num", "\n", "\n", "# train", "\n", "if", "episode", ">=", "self", ".", "init_episodes", ":", "\n", "                    ", "self", ".", "learn", "(", "replay_buffer", "=", "replay_buffer", ",", "env", "=", "env", ",", "episode", "=", "episode", ")", "\n", "\n", "", "if", "done", ">", "0.5", ":", "\n", "                    ", "break", "\n", "\n", "# logging", "\n", "", "", "avg_meter_episode_length", ".", "update", "(", "episode_length", ",", "print_rate", "=", "1e9", ")", "\n", "\n", "if", "test_env", "is", "not", "None", ":", "\n", "                ", "avg_reward_test_raw", ",", "_", ",", "_", "=", "self", ".", "test", "(", "test_env", ")", "\n", "avg_meter_reward", ".", "update", "(", "statistics", ".", "mean", "(", "avg_reward_test_raw", ")", ",", "print_rate", "=", "self", ".", "print_rate", ")", "\n", "", "else", ":", "\n", "                ", "avg_meter_reward", ".", "update", "(", "episode_reward", ",", "print_rate", "=", "self", ".", "print_rate", ")", "\n", "\n", "# quit training if environment is solved", "\n", "", "if", "episode", ">=", "self", ".", "init_episodes", ":", "\n", "                ", "if", "test_env", "is", "not", "None", ":", "\n", "                    ", "break_env", "=", "test_env", "\n", "", "else", ":", "\n", "                    ", "break_env", "=", "env", "\n", "", "if", "self", ".", "env_solved", "(", "env", "=", "break_env", ",", "avg_meter_reward", "=", "avg_meter_reward", ",", "episode", "=", "episode", ")", ":", "\n", "                    ", "print", "(", "'early out after '", "+", "str", "(", "episode", ")", "+", "' episodes'", ")", "\n", "break", "\n", "\n", "", "", "", "env", ".", "close", "(", ")", "\n", "\n", "# todo: use dict to reduce confusions and bugs", "\n", "return", "avg_meter_reward", ".", "get_raw_data", "(", ")", ",", "avg_meter_episode_length", ".", "get_raw_data", "(", ")", ",", "replay_buffer", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.base_agent.BaseAgent.test": [[155, 228], ["env.has_discrete_action_space", "utils.ReplayBuffer", "env.set_agent_params", "env.has_discrete_state_space", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "time.time", "utils.AverageMeter", "utils.AverageMeter", "range", "env.close", "utils.AverageMeter.get_raw_data", "utils.AverageMeter.get_raw_data", "env.get_action_dim", "int", "base_agent.BaseAgent.time_is_up", "env.reset", "range", "utils.AverageMeter.update", "utils.AverageMeter.update", "env.max_episode_steps", "base_agent.BaseAgent.select_test_action", "utils.ReplayBuffer.add", "episode_reward.item", "env.render", "env.step", "env.step", "time.time", "base_agent.BaseAgent.argmax().unsqueeze", "base_agent.BaseAgent.argmax"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.has_discrete_action_space", "home.repos.pwc.inspect_result.automl_learning_environments.envs.reward_env.RewardEnv.set_agent_params", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.has_discrete_state_space", "home.repos.pwc.inspect_result.automl_learning_environments.envs.reward_env.RewardEnv.close", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.AverageMeter.get_raw_data", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.AverageMeter.get_raw_data", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.get_action_dim", "home.repos.pwc.inspect_result.automl_learning_environments.agents.base_agent.BaseAgent.time_is_up", "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditEnv.reset", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.AverageMeter.update", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.AverageMeter.update", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.max_episode_steps", "home.repos.pwc.inspect_result.automl_learning_environments.agents.PPO.PPO.select_test_action", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.ReplayBuffer.add", "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditEnv.render", "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditEnv.step", "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditEnv.step"], ["", "def", "test", "(", "self", ",", "env", ",", "time_remaining", "=", "1e9", ")", ":", "\n", "        ", "discretize_action", "=", "False", "\n", "\n", "sd", "=", "1", "if", "env", ".", "has_discrete_state_space", "(", ")", "else", "self", ".", "state_dim", "\n", "\n", "if", "env", ".", "has_discrete_action_space", "(", ")", ":", "\n", "            ", "ad", "=", "1", "\n", "# in case of td3_discrete, action_dim=1 does not reflect the required action_dim for the gumbel softmax distribution", "\n", "if", "self", ".", "agent_name", "==", "\"td3_discrete_vary\"", ":", "\n", "                ", "ad", "=", "env", ".", "get_action_dim", "(", ")", "\n", "discretize_action", "=", "True", "\n", "", "", "else", ":", "\n", "            ", "ad", "=", "self", ".", "action_dim", "\n", "\n", "", "replay_buffer", "=", "ReplayBuffer", "(", "state_dim", "=", "sd", ",", "action_dim", "=", "ad", ",", "device", "=", "self", ".", "device", ",", "max_size", "=", "int", "(", "1e6", ")", ")", "\n", "\n", "env", ".", "set_agent_params", "(", "same_action_num", "=", "self", ".", "same_action_num", ",", "gamma", "=", "self", ".", "gamma", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "time_start", "=", "time", ".", "time", "(", ")", "\n", "\n", "avg_meter_reward", "=", "AverageMeter", "(", "print_str", "=", "\"Average reward: \"", ")", "\n", "avg_meter_episode_length", "=", "AverageMeter", "(", "print_str", "=", "\"Average episode length: \"", ")", "\n", "\n", "# training loop", "\n", "for", "episode", "in", "range", "(", "self", ".", "test_episodes", ")", ":", "\n", "# episode_trajectory = []", "\n", "# early out if timeout", "\n", "                ", "if", "self", ".", "time_is_up", "(", "avg_meter_reward", "=", "avg_meter_reward", ",", "\n", "avg_meter_episode_length", "=", "avg_meter_episode_length", ",", "\n", "max_episodes", "=", "self", ".", "test_episodes", ",", "\n", "time_elapsed", "=", "time", ".", "time", "(", ")", "-", "time_start", ",", "\n", "time_remaining", "=", "time_remaining", ")", ":", "\n", "                    ", "break", "\n", "\n", "", "state", "=", "env", ".", "reset", "(", ")", "\n", "episode_reward", "=", "0", "\n", "episode_length", "=", "0", "\n", "\n", "for", "t", "in", "range", "(", "0", ",", "env", ".", "max_episode_steps", "(", ")", ",", "self", ".", "same_action_num", ")", ":", "\n", "                    ", "action", "=", "self", ".", "select_test_action", "(", "state", ",", "env", ")", "\n", "\n", "# live view", "\n", "if", "self", ".", "render_env", ":", "\n", "                        ", "env", ".", "render", "(", ")", "\n", "\n", "# state-action transition", "\n", "# required due to gumble softmax in td3 discrete", "\n", "", "if", "discretize_action", ":", "\n", "                        ", "next_state", ",", "reward", ",", "done", "=", "env", ".", "step", "(", "action", "=", "action", ".", "argmax", "(", ")", ".", "unsqueeze", "(", "0", ")", ")", "\n", "", "else", ":", "\n", "                        ", "next_state", ",", "reward", ",", "done", "=", "env", ".", "step", "(", "action", "=", "action", ")", "\n", "", "replay_buffer", ".", "add", "(", "state", "=", "state", ",", "action", "=", "action", ",", "next_state", "=", "next_state", ",", "reward", "=", "reward", ",", "done", "=", "done", ")", "\n", "\n", "# episode_trajectory.append([state, action, next_state, reward])", "\n", "\n", "state", "=", "next_state", "\n", "episode_reward", "+=", "reward", "\n", "episode_length", "+=", "1", "\n", "\n", "if", "done", ">", "0.5", ":", "\n", "                        ", "break", "\n", "\n", "# self.trajectories.append(episode_trajectory)", "\n", "\n", "# logging", "\n", "", "", "avg_meter_episode_length", ".", "update", "(", "episode_length", ",", "print_rate", "=", "1e9", ")", "\n", "avg_meter_reward", ".", "update", "(", "episode_reward", ".", "item", "(", ")", ",", "print_rate", "=", "self", ".", "print_rate", ")", "\n", "\n", "", "env", ".", "close", "(", ")", "\n", "\n", "# todo: use dict to reduce confusions and bugs", "\n", "", "return", "avg_meter_reward", ".", "get_raw_data", "(", ")", ",", "avg_meter_episode_length", ".", "get_raw_data", "(", ")", ",", "replay_buffer", "\n", "", "", ""]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.SAC.SAC.__init__": [[12, 42], ["agents.base_agent.BaseAgent.__init__", "models.actor_critic.Actor_SAC", "models.actor_critic.Critic_Q().to", "models.actor_critic.Critic_Q().to", "models.actor_critic.Critic_Q().to", "models.actor_critic.Critic_Q().to", "SAC.SAC.critic_target_1.load_state_dict", "SAC.SAC.critic_target_2.load_state_dict", "SAC.SAC.reset_optimizer", "SAC.SAC.critic_1.state_dict", "SAC.SAC.critic_2.state_dict", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "models.actor_critic.Critic_Q", "models.actor_critic.Critic_Q", "models.actor_critic.Critic_Q", "models.actor_critic.Critic_Q", "numpy.prod().item", "numpy.prod"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__", "home.repos.pwc.inspect_result.automl_learning_environments.agents.SAC.SAC.reset_optimizer"], ["    ", "def", "__init__", "(", "self", ",", "env", ",", "max_action", ",", "config", ")", ":", "\n", "        ", "agent_name", "=", "'sac'", "\n", "\n", "super", "(", ")", ".", "__init__", "(", "agent_name", "=", "agent_name", ",", "env", "=", "env", ",", "config", "=", "config", ")", "\n", "\n", "sac_config", "=", "config", "[", "\"agents\"", "]", "[", "agent_name", "]", "\n", "\n", "self", ".", "max_action", "=", "max_action", "\n", "self", ".", "gamma", "=", "sac_config", "[", "\"gamma\"", "]", "\n", "self", ".", "alpha", "=", "sac_config", "[", "\"alpha\"", "]", "\n", "self", ".", "tau", "=", "sac_config", "[", "\"tau\"", "]", "\n", "self", ".", "automatic_entropy_tuning", "=", "sac_config", "[", "\"automatic_entropy_tuning\"", "]", "\n", "self", ".", "rb_size", "=", "sac_config", "[", "\"rb_size\"", "]", "\n", "self", ".", "batch_size", "=", "sac_config", "[", "\"batch_size\"", "]", "\n", "self", ".", "lr", "=", "sac_config", "[", "\"lr\"", "]", "\n", "\n", "self", ".", "actor", "=", "Actor_SAC", "(", "self", ".", "state_dim", ",", "self", ".", "action_dim", ",", "max_action", ",", "agent_name", ",", "config", ")", "\n", "self", ".", "critic_1", "=", "Critic_Q", "(", "self", ".", "state_dim", ",", "self", ".", "action_dim", ",", "agent_name", ",", "config", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "critic_2", "=", "Critic_Q", "(", "self", ".", "state_dim", ",", "self", ".", "action_dim", ",", "agent_name", ",", "config", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "critic_target_1", "=", "Critic_Q", "(", "self", ".", "state_dim", ",", "self", ".", "action_dim", ",", "agent_name", ",", "config", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "critic_target_2", "=", "Critic_Q", "(", "self", ".", "state_dim", ",", "self", ".", "action_dim", ",", "agent_name", ",", "config", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "critic_target_1", ".", "load_state_dict", "(", "self", ".", "critic_1", ".", "state_dict", "(", ")", ")", "\n", "self", ".", "critic_target_2", ".", "load_state_dict", "(", "self", ".", "critic_2", ".", "state_dict", "(", ")", ")", "\n", "\n", "if", "self", ".", "automatic_entropy_tuning", ":", "\n", "            ", "self", ".", "target_entropy", "=", "-", "np", ".", "prod", "(", "(", "self", ".", "action_dim", ",", ")", ")", ".", "item", "(", ")", "\n", "self", ".", "log_alpha", "=", "torch", ".", "zeros", "(", "1", ",", "requires_grad", "=", "True", ",", "device", "=", "self", ".", "device", ")", "\n", "self", ".", "alpha_optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "[", "self", ".", "log_alpha", "]", ",", "lr", "=", "self", ".", "lr", ")", "\n", "\n", "", "self", ".", "reset_optimizer", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.SAC.SAC.learn": [[43, 94], ["replay_buffer.sample", "rewards.squeeze.squeeze.squeeze", "dones.squeeze.squeeze.squeeze", "SAC.SAC.actor", "SAC.SAC.actor", "SAC.SAC.critic_1().squeeze", "SAC.SAC.critic_2().squeeze", "torch.min().squeeze().to", "torch.min().squeeze().to", "torch.min().squeeze().to", "torch.min().squeeze().to", "torch.min().squeeze().to", "torch.min().squeeze().to", "torch.min().squeeze().to", "torch.min().squeeze().to", "q_backup.to", "torch.mse_loss", "torch.mse_loss", "torch.mse_loss", "torch.mse_loss", "SAC.SAC.actor_optimizer.zero_grad", "actor_loss.backward", "SAC.SAC.actor_optimizer.step", "SAC.SAC.critic_optimizer.zero_grad", "critic_loss.backward", "SAC.SAC.critic_optimizer.step", "zip", "zip", "q_backup.detach", "q_backup.detach", "SAC.SAC.alpha_optimizer.zero_grad", "alpha_loss.backward", "SAC.SAC.alpha_optimizer.step", "SAC.SAC.log_alpha.exp", "SAC.SAC.critic_1.parameters", "SAC.SAC.critic_target_1.parameters", "target_param.data.copy_", "SAC.SAC.critic_2.parameters", "SAC.SAC.critic_target_2.parameters", "target_param.data.copy_", "SAC.SAC.critic_1", "SAC.SAC.critic_2", "torch.min().squeeze", "torch.min().squeeze", "torch.min().squeeze", "torch.min().squeeze", "torch.min().squeeze", "torch.min().squeeze", "torch.min().squeeze", "torch.min().squeeze", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "SAC.SAC.critic_1", "SAC.SAC.critic_2", "SAC.SAC.critic_target_1", "SAC.SAC.critic_target_2"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.None.utils.ReplayBuffer.sample", "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditEnv.step", "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditEnv.step", "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditEnv.step"], ["", "def", "learn", "(", "self", ",", "replay_buffer", ",", "env", ",", "episode", ")", ":", "\n", "        ", "states", ",", "actions", ",", "next_states", ",", "rewards", ",", "dones", "=", "replay_buffer", ".", "sample", "(", "self", ".", "batch_size", ")", "\n", "rewards", "=", "rewards", ".", "squeeze", "(", ")", "\n", "dones", "=", "dones", ".", "squeeze", "(", ")", "\n", "\n", "# Prediction \u03c0(a|s), log\u03c0(a|s), \u03c0(a'|s'), log\u03c0(a'|s'), Q1(s,a), Q2(s,a)", "\n", "_", ",", "pi", ",", "log_pi", "=", "self", ".", "actor", "(", "states", ")", "\n", "_", ",", "next_pi", ",", "next_log_pi", "=", "self", ".", "actor", "(", "next_states", ")", "\n", "q1", "=", "self", ".", "critic_1", "(", "states", ",", "actions", ")", ".", "squeeze", "(", "1", ")", "\n", "q2", "=", "self", ".", "critic_2", "(", "states", ",", "actions", ")", ".", "squeeze", "(", "1", ")", "\n", "\n", "# Min Double-Q: min(Q1(s,\u03c0(a|s)), Q2(s,\u03c0(a|s))), min(Q1\u203e(s',\u03c0(a'|s')), Q2\u203e(s',\u03c0(a'|s')))", "\n", "min_q_pi", "=", "torch", ".", "min", "(", "self", ".", "critic_1", "(", "states", ",", "pi", ")", ",", "self", ".", "critic_2", "(", "states", ",", "pi", ")", ")", ".", "squeeze", "(", "1", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "min_q_next_pi", "=", "torch", ".", "min", "(", "self", ".", "critic_target_1", "(", "next_states", ",", "next_pi", ")", ",", "\n", "self", ".", "critic_target_2", "(", "next_states", ",", "next_pi", ")", ")", ".", "squeeze", "(", "1", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "# Targets for Q regression", "\n", "v_backup", "=", "min_q_next_pi", "-", "self", ".", "alpha", "*", "next_log_pi", "\n", "q_backup", "=", "rewards", "+", "self", ".", "gamma", "*", "(", "1", "-", "dones", ")", "*", "v_backup", "\n", "q_backup", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "# SAC losses", "\n", "actor_loss", "=", "(", "self", ".", "alpha", "*", "log_pi", "-", "min_q_pi", ")", ".", "mean", "(", ")", "\n", "critic1_loss", "=", "F", ".", "mse_loss", "(", "q1", ",", "q_backup", ".", "detach", "(", ")", ")", "\n", "critic2_loss", "=", "F", ".", "mse_loss", "(", "q2", ",", "q_backup", ".", "detach", "(", ")", ")", "\n", "critic_loss", "=", "critic1_loss", "+", "critic2_loss", "\n", "\n", "# Update policy network parameter", "\n", "self", ".", "actor_optimizer", ".", "zero_grad", "(", ")", "\n", "actor_loss", ".", "backward", "(", ")", "\n", "self", ".", "actor_optimizer", ".", "step", "(", ")", "\n", "\n", "# Update two Q-network parameter", "\n", "self", ".", "critic_optimizer", ".", "zero_grad", "(", ")", "\n", "critic_loss", ".", "backward", "(", ")", "\n", "self", ".", "critic_optimizer", ".", "step", "(", ")", "\n", "\n", "# If automatic entropy tuning is True, update alpha", "\n", "if", "self", ".", "automatic_entropy_tuning", ":", "\n", "            ", "alpha_loss", "=", "-", "(", "self", ".", "log_alpha", "*", "(", "log_pi", "+", "self", ".", "target_entropy", ")", ".", "detach", "(", ")", ")", ".", "mean", "(", ")", "\n", "self", ".", "alpha_optimizer", ".", "zero_grad", "(", ")", "\n", "alpha_loss", ".", "backward", "(", ")", "\n", "self", ".", "alpha_optimizer", ".", "step", "(", ")", "\n", "self", ".", "alpha", "=", "self", ".", "log_alpha", ".", "exp", "(", ")", "\n", "\n", "# Polyak averaging for target parameter", "\n", "", "for", "param", ",", "target_param", "in", "zip", "(", "self", ".", "critic_1", ".", "parameters", "(", ")", ",", "self", ".", "critic_target_1", ".", "parameters", "(", ")", ")", ":", "\n", "            ", "target_param", ".", "data", ".", "copy_", "(", "self", ".", "tau", "*", "param", ".", "data", "+", "(", "1", "-", "self", ".", "tau", ")", "*", "target_param", ".", "data", ")", "\n", "\n", "", "for", "param", ",", "target_param", "in", "zip", "(", "self", ".", "critic_2", ".", "parameters", "(", ")", ",", "self", ".", "critic_target_2", ".", "parameters", "(", ")", ")", ":", "\n", "            ", "target_param", ".", "data", ".", "copy_", "(", "self", ".", "tau", "*", "param", ".", "data", "+", "(", "1", "-", "self", ".", "tau", ")", "*", "target_param", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.SAC.SAC.select_train_action": [[95, 101], ["env.get_random_action", "SAC.SAC.actor", "action.cpu", "state.to"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.get_random_action"], ["", "", "def", "select_train_action", "(", "self", ",", "state", ",", "env", ",", "episode", ")", ":", "\n", "        ", "if", "episode", "<", "self", ".", "init_episodes", ":", "\n", "            ", "return", "env", ".", "get_random_action", "(", ")", "\n", "", "else", ":", "\n", "            ", "_", ",", "action", ",", "_", "=", "self", ".", "actor", "(", "state", ".", "to", "(", "self", ".", "device", ")", ")", "\n", "return", "action", ".", "cpu", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.SAC.SAC.select_test_action": [[102, 105], ["SAC.SAC.actor", "action.cpu", "state.to"], "methods", ["None"], ["", "", "def", "select_test_action", "(", "self", ",", "state", ",", "env", ")", ":", "\n", "        ", "action", ",", "_", ",", "_", "=", "self", ".", "actor", "(", "state", ".", "to", "(", "self", ".", "device", ")", ")", "\n", "return", "action", ".", "cpu", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.SAC.SAC.reset_optimizer": [[106, 111], ["list", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "SAC.SAC.actor.parameters", "list", "list", "SAC.SAC.critic_1.parameters", "SAC.SAC.critic_2.parameters"], "methods", ["None"], ["", "def", "reset_optimizer", "(", "self", ")", ":", "\n", "        ", "actor_params", "=", "list", "(", "self", ".", "actor", ".", "parameters", "(", ")", ")", "\n", "critic_params", "=", "list", "(", "self", ".", "critic_1", ".", "parameters", "(", ")", ")", "+", "list", "(", "self", ".", "critic_2", ".", "parameters", "(", ")", ")", "\n", "self", ".", "actor_optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "actor_params", ",", "lr", "=", "self", ".", "lr", ")", "\n", "self", ".", "critic_optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "critic_params", ",", "lr", "=", "self", ".", "lr", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.PPO.PPO.__init__": [[15, 63], ["agents.base_agent.BaseAgent.__init__", "models.actor_critic.Actor_PPO().to", "models.actor_critic.Critic_V().to", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "models.actor_critic.Actor_PPO().to", "models.actor_critic.Critic_V().to", "PPO.PPO.actor_old.load_state_dict", "PPO.PPO.critic_old.load_state_dict", "PPO.PPO.actor.state_dict", "PPO.PPO.critic.state_dict", "models.icm_baseline.ICM", "models.actor_critic.Actor_PPO", "models.actor_critic.Critic_V", "list", "list", "models.actor_critic.Actor_PPO", "models.actor_critic.Critic_V", "PPO.PPO.actor.parameters", "PPO.PPO.critic.parameters", "env.has_discrete_action_space"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.has_discrete_action_space"], ["    ", "def", "__init__", "(", "self", ",", "env", ",", "config", ",", "icm", "=", "False", ")", ":", "\n", "        ", "agent_name", "=", "'ppo'", "\n", "super", "(", ")", ".", "__init__", "(", "agent_name", "=", "agent_name", ",", "env", "=", "env", ",", "config", "=", "config", ")", "\n", "\n", "ppo_config", "=", "config", "[", "'agents'", "]", "[", "agent_name", "]", "\n", "\n", "self", ".", "gamma", "=", "ppo_config", "[", "'gamma'", "]", "\n", "self", ".", "vf_coef", "=", "ppo_config", "[", "'vf_coef'", "]", "\n", "self", ".", "ent_coef", "=", "ppo_config", "[", "'ent_coef'", "]", "\n", "self", ".", "eps_clip", "=", "ppo_config", "[", "'eps_clip'", "]", "\n", "self", ".", "ppo_epochs", "=", "ppo_config", "[", "'ppo_epochs'", "]", "\n", "self", ".", "update_episodes", "=", "ppo_config", "[", "'update_episodes'", "]", "\n", "self", ".", "early_out_num", "=", "ppo_config", "[", "'early_out_num'", "]", "\n", "self", ".", "same_action_num", "=", "ppo_config", "[", "'same_action_num'", "]", "\n", "\n", "self", ".", "ppo_config", "=", "ppo_config", "\n", "\n", "self", ".", "render_env", "=", "config", "[", "\"render_env\"", "]", "\n", "self", ".", "device", "=", "config", "[", "\"device\"", "]", "\n", "\n", "self", ".", "actor", "=", "Actor_PPO", "(", "self", ".", "state_dim", ",", "self", ".", "action_dim", ",", "agent_name", ",", "config", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "critic", "=", "Critic_V", "(", "self", ".", "state_dim", ",", "agent_name", ",", "config", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "list", "(", "self", ".", "actor", ".", "parameters", "(", ")", ")", "+", "\n", "list", "(", "self", ".", "critic", ".", "parameters", "(", ")", ")", ",", "\n", "lr", "=", "ppo_config", "[", "'lr'", "]", ")", "\n", "\n", "self", ".", "actor_old", "=", "Actor_PPO", "(", "self", ".", "state_dim", ",", "self", ".", "action_dim", ",", "agent_name", ",", "config", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "critic_old", "=", "Critic_V", "(", "self", ".", "state_dim", ",", "agent_name", ",", "config", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "actor_old", ".", "load_state_dict", "(", "self", ".", "actor", ".", "state_dict", "(", ")", ")", "\n", "self", ".", "critic_old", ".", "load_state_dict", "(", "self", ".", "critic", ".", "state_dict", "(", ")", ")", "\n", "\n", "self", ".", "icm", "=", "None", "\n", "if", "icm", ":", "\n", "            ", "icm_config", "=", "config", "[", "\"agents\"", "]", "[", "\"icm\"", "]", "\n", "self", ".", "icm_lr", "=", "icm_config", "[", "\"lr\"", "]", "\n", "self", ".", "icm_beta", "=", "icm_config", "[", "\"beta\"", "]", "\n", "self", ".", "icm_eta", "=", "icm_config", "[", "\"eta\"", "]", "\n", "self", ".", "icm_feature_dim", "=", "icm_config", "[", "\"feature_dim\"", "]", "\n", "self", ".", "icm_hidden_dim", "=", "icm_config", "[", "\"hidden_size\"", "]", "\n", "self", ".", "icm", "=", "ICM", "(", "state_dim", "=", "self", ".", "state_dim", ",", "\n", "action_dim", "=", "self", ".", "action_dim", ",", "\n", "has_discrete_actions", "=", "env", ".", "has_discrete_action_space", "(", ")", ",", "\n", "learning_rate", "=", "self", ".", "icm_lr", ",", "\n", "beta", "=", "self", ".", "icm_beta", ",", "\n", "eta", "=", "self", ".", "icm_eta", ",", "\n", "feature_dim", "=", "self", ".", "icm_feature_dim", ",", "\n", "hidden_size", "=", "self", ".", "icm_hidden_dim", ",", "\n", "device", "=", "self", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.PPO.PPO.train": [[64, 129], ["utils.ReplayBuffer", "utils.AverageMeter", "utils.AverageMeter", "env.set_agent_params", "range", "env.close", "env.has_discrete_state_space", "env.has_discrete_action_space", "env.reset", "range", "utils.AverageMeter.update", "utils.AverageMeter.get_raw_data", "utils.AverageMeter.get_raw_data", "env.max_episode_steps", "PPO.PPO.actor_old().cpu", "env.step", "utils.ReplayBuffer.add", "PPO.PPO.test", "utils.AverageMeter.update", "utils.AverageMeter.update", "PPO.PPO.env_solved", "env.render", "PPO.PPO.learn", "utils.ReplayBuffer.clear", "statistics.mean", "print", "PPO.PPO.actor_old", "env.max_episode_steps", "env.reset.to", "str"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.reward_env.RewardEnv.set_agent_params", "home.repos.pwc.inspect_result.automl_learning_environments.envs.reward_env.RewardEnv.close", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.has_discrete_state_space", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.has_discrete_action_space", "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditEnv.reset", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.AverageMeter.update", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.AverageMeter.get_raw_data", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.AverageMeter.get_raw_data", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.max_episode_steps", "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditEnv.step", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.ReplayBuffer.add", "home.repos.pwc.inspect_result.automl_learning_environments.agents.base_agent.BaseAgent.test", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.AverageMeter.update", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.AverageMeter.update", "home.repos.pwc.inspect_result.automl_learning_environments.agents.base_agent.BaseAgent.env_solved", "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditEnv.render", "home.repos.pwc.inspect_result.automl_learning_environments.agents.PPO.PPO.learn", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.ReplayBuffer.clear", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.max_episode_steps"], ["", "", "def", "train", "(", "self", ",", "env", ",", "time_remaining", "=", "1e9", ",", "test_env", "=", "None", ")", ":", "\n", "\n", "        ", "sd", "=", "1", "if", "env", ".", "has_discrete_state_space", "(", ")", "else", "self", ".", "state_dim", "\n", "ad", "=", "1", "if", "env", ".", "has_discrete_action_space", "(", ")", "else", "self", ".", "action_dim", "\n", "replay_buffer", "=", "ReplayBuffer", "(", "state_dim", "=", "sd", ",", "action_dim", "=", "ad", ",", "device", "=", "self", ".", "device", ",", "max_size", "=", "self", ".", "rb_size", ")", "\n", "\n", "avg_meter_reward", "=", "AverageMeter", "(", "print_str", "=", "\"Average reward: \"", ")", "\n", "avg_meter_episode_length", "=", "AverageMeter", "(", "print_str", "=", "\"Average episode length: \"", ")", "\n", "\n", "env", ".", "set_agent_params", "(", "same_action_num", "=", "self", ".", "same_action_num", ",", "gamma", "=", "self", ".", "gamma", ")", "\n", "\n", "time_step", "=", "0", "\n", "\n", "# training loop", "\n", "for", "episode", "in", "range", "(", "self", ".", "train_episodes", ")", ":", "\n", "            ", "state", "=", "env", ".", "reset", "(", ")", "\n", "episode_reward", "=", "0", "\n", "episode_length", "=", "0", "\n", "\n", "for", "t", "in", "range", "(", "0", ",", "env", ".", "max_episode_steps", "(", ")", ",", "self", ".", "same_action_num", ")", ":", "\n", "                ", "time_step", "+=", "self", ".", "same_action_num", "\n", "\n", "# run old policy", "\n", "action", "=", "self", ".", "actor_old", "(", "state", ".", "to", "(", "self", ".", "device", ")", ")", ".", "cpu", "(", ")", "\n", "next_state", ",", "reward", ",", "done", "=", "env", ".", "step", "(", "action", "=", "action", ")", "\n", "\n", "# live view", "\n", "if", "self", ".", "render_env", "and", "episode", "%", "100", "==", "0", ":", "\n", "                    ", "env", ".", "render", "(", ")", "\n", "\n", "", "replay_buffer", ".", "add", "(", "state", "=", "state", ",", "action", "=", "action", ",", "next_state", "=", "next_state", ",", "reward", "=", "reward", ",", "done", "=", "done", ")", "\n", "state", "=", "next_state", "\n", "episode_reward", "+=", "reward", "\n", "episode_length", "+=", "self", ".", "same_action_num", "\n", "\n", "# train after certain amount of timesteps", "\n", "if", "time_step", "/", "env", ".", "max_episode_steps", "(", ")", ">", "self", ".", "update_episodes", ":", "\n", "                    ", "self", ".", "learn", "(", "replay_buffer", ")", "\n", "replay_buffer", ".", "clear", "(", ")", "\n", "time_step", "=", "0", "\n", "", "if", "done", ">", "0.5", ":", "\n", "                    ", "break", "\n", "\n", "# logging", "\n", "", "", "avg_meter_episode_length", ".", "update", "(", "episode_length", ",", "print_rate", "=", "1e9", ")", "\n", "\n", "if", "test_env", "is", "not", "None", ":", "\n", "                ", "avg_reward_test_raw", ",", "_", ",", "_", "=", "self", ".", "test", "(", "test_env", ")", "\n", "avg_meter_reward", ".", "update", "(", "statistics", ".", "mean", "(", "avg_reward_test_raw", ")", ",", "print_rate", "=", "self", ".", "print_rate", ")", "\n", "", "else", ":", "\n", "                ", "avg_meter_reward", ".", "update", "(", "episode_reward", ",", "print_rate", "=", "self", ".", "print_rate", ")", "\n", "\n", "# quit training if environment is solved", "\n", "", "if", "episode", ">=", "self", ".", "init_episodes", ":", "\n", "                ", "if", "test_env", "is", "not", "None", ":", "\n", "                    ", "break_env", "=", "test_env", "\n", "", "else", ":", "\n", "                    ", "break_env", "=", "env", "\n", "", "if", "self", ".", "env_solved", "(", "env", "=", "break_env", ",", "avg_meter_reward", "=", "avg_meter_reward", ",", "episode", "=", "episode", ")", ":", "\n", "                    ", "print", "(", "'early out after '", "+", "str", "(", "episode", ")", "+", "' episodes'", ")", "\n", "break", "\n", "\n", "", "", "", "env", ".", "close", "(", ")", "\n", "\n", "return", "avg_meter_reward", ".", "get_raw_data", "(", ")", ",", "avg_meter_episode_length", ".", "get_raw_data", "(", ")", ",", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.PPO.PPO.select_train_action": [[130, 132], ["PPO.PPO.actor_old().cpu", "PPO.PPO.actor_old", "state.to"], "methods", ["None"], ["", "def", "select_train_action", "(", "self", ",", "state", ",", "env", ")", ":", "\n", "        ", "return", "self", ".", "actor_old", "(", "state", ".", "to", "(", "self", ".", "device", ")", ")", ".", "cpu", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.PPO.PPO.select_test_action": [[133, 135], ["PPO.PPO.actor_old().cpu", "PPO.PPO.actor_old", "state.to"], "methods", ["None"], ["", "def", "select_test_action", "(", "self", ",", "state", ",", "env", ")", ":", "\n", "        ", "return", "self", ".", "actor_old", "(", "state", ".", "to", "(", "self", ".", "device", ")", ")", ".", "cpu", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.PPO.PPO.learn": [[136, 189], ["replay_buffer.get_all", "PPO.PPO.actor_old.evaluate", "old_logprobs.detach.detach.detach", "zip", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "range", "PPO.PPO.actor_old.load_state_dict", "PPO.PPO.critic_old.load_state_dict", "PPO.PPO.icm.compute_intrinsic_rewards", "reversed", "reversed", "torch.FloatTensor().to.insert", "torch.FloatTensor().to.insert", "PPO.PPO.actor.evaluate", "PPO.PPO.critic().squeeze", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "PPO.PPO.optimizer.zero_grad", "loss.mean().backward", "PPO.PPO.optimizer.step", "PPO.PPO.icm.train", "PPO.PPO.actor.state_dict", "PPO.PPO.critic.state_dict", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor().to.mean", "torch.FloatTensor().to.mean", "torch.FloatTensor().to.std", "torch.FloatTensor().to.std", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "PPO.PPO.critic", "loss.mean", "torch.min", "torch.min", "torch.min", "torch.min", "torch.mse_loss", "torch.mse_loss"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.None.utils.ReplayBuffer.get_all", "home.repos.pwc.inspect_result.automl_learning_environments.models.actor_critic.Actor_PPO.evaluate", "home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.compute_intrinsic_rewards", "home.repos.pwc.inspect_result.automl_learning_environments.models.actor_critic.Actor_PPO.evaluate", "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditEnv.step", "home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.train", "home.repos.pwc.inspect_result.automl_learning_environments.models.actor_critic.Actor_PPO.clamp", "home.repos.pwc.inspect_result.automl_learning_environments.models.actor_critic.Actor_PPO.clamp", "home.repos.pwc.inspect_result.automl_learning_environments.models.actor_critic.Actor_PPO.clamp", "home.repos.pwc.inspect_result.automl_learning_environments.models.actor_critic.Actor_PPO.clamp"], ["", "def", "learn", "(", "self", ",", "replay_buffer", ")", ":", "\n", "# Monte Carlo estimate of rewards:", "\n", "        ", "new_rewards", "=", "[", "]", "\n", "discounted_reward", "=", "0", "\n", "\n", "# get states from replay buffer", "\n", "states", ",", "actions", ",", "next_states", ",", "rewards", ",", "dones", "=", "replay_buffer", ".", "get_all", "(", ")", "\n", "\n", "if", "self", ".", "icm", ":", "\n", "            ", "rewards", "+=", "self", ".", "icm", ".", "compute_intrinsic_rewards", "(", "states", ",", "next_states", ",", "actions", ")", "\n", "\n", "", "old_logprobs", ",", "_", "=", "self", ".", "actor_old", ".", "evaluate", "(", "states", ",", "actions", ")", "\n", "old_logprobs", "=", "old_logprobs", ".", "detach", "(", ")", "\n", "\n", "# calculate rewards", "\n", "for", "reward", ",", "done", "in", "zip", "(", "reversed", "(", "rewards", ")", ",", "reversed", "(", "dones", ")", ")", ":", "\n", "            ", "if", "done", ">", "0.5", ":", "\n", "                ", "discounted_reward", "=", "0", "\n", "", "discounted_reward", "=", "reward", "+", "(", "self", ".", "gamma", "*", "discounted_reward", ")", "\n", "new_rewards", ".", "insert", "(", "0", ",", "discounted_reward", ")", "\n", "\n", "# normalize advantage function", "\n", "", "new_rewards", "=", "torch", ".", "FloatTensor", "(", "new_rewards", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "new_rewards", "=", "(", "new_rewards", "-", "new_rewards", ".", "mean", "(", ")", ")", "/", "(", "new_rewards", ".", "std", "(", ")", "+", "1e-5", ")", "\n", "\n", "# optimize policy for ppo_epochs:", "\n", "for", "it", "in", "range", "(", "self", ".", "ppo_epochs", ")", ":", "\n", "# evaluate old actions and values :", "\n", "            ", "logprobs", ",", "dist_entropy", "=", "self", ".", "actor", ".", "evaluate", "(", "states", ",", "actions", ")", "\n", "state_values", "=", "self", ".", "critic", "(", "states", ")", ".", "squeeze", "(", ")", "\n", "\n", "# Finding the ratio (pi_theta / pi_theta__old):", "\n", "ratios", "=", "torch", ".", "exp", "(", "logprobs", "-", "old_logprobs", ")", "\n", "\n", "# Finding Surrogate Loss", "\n", "advantages", "=", "(", "new_rewards", "-", "state_values", ")", ".", "detach", "(", ")", "\n", "\n", "surr1", "=", "ratios", "*", "advantages", "\n", "surr2", "=", "torch", ".", "clamp", "(", "ratios", ",", "1", "-", "self", ".", "eps_clip", ",", "1", "+", "self", ".", "eps_clip", ")", "*", "advantages", "\n", "loss", "=", "-", "torch", ".", "min", "(", "surr1", ",", "surr2", ")", "+", "self", ".", "vf_coef", "*", "F", ".", "mse_loss", "(", "state_values", ",", "new_rewards", ")", "-", "self", ".", "ent_coef", "*", "dist_entropy", "\n", "\n", "# take gradient step", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "mean", "(", ")", ".", "backward", "(", ")", "\n", "# torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 0.5)", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "", "if", "self", ".", "icm", ":", "\n", "            ", "self", ".", "icm", ".", "train", "(", "states", ",", "next_states", ",", "actions", ")", "\n", "\n", "# Copy new weights into old policy:", "\n", "", "self", ".", "actor_old", ".", "load_state_dict", "(", "self", ".", "actor", ".", "state_dict", "(", ")", ")", "\n", "self", ".", "critic_old", ".", "load_state_dict", "(", "self", ".", "critic", ".", "state_dict", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.agent_utils.select_agent": [[15, 67], ["envs.env_factory.EnvFactory", "envs.env_factory.EnvFactory.generate_real_env", "agent_name.lower.lower", "env_factory.generate_real_env.get_max_action", "agents.TD3.TD3", "env_factory.generate_real_env.get_max_action", "agents.TD3.TD3", "env_factory.generate_real_env.get_max_action", "agents.TD3_vary.TD3_vary", "env_factory.generate_real_env.get_max_action", "agents.TD3_vary.TD3_vary", "agents.PPO.PPO", "agents.PPO.PPO", "agents.DDQN.DDQN", "agents.DDQN.DDQN", "agents.DDQN_vary.DDQN_vary", "agents.DDQN_vary.DDQN_vary", "agents.DuelingDDQN.DuelingDDQN", "agents.DuelingDDQN.DuelingDDQN", "agents.DuelingDDQN_vary.DuelingDDQN_vary", "agents.DuelingDDQN_vary.DuelingDDQN_vary", "env_factory.generate_real_env.get_max_action", "env_factory.generate_real_env.get_min_action", "agents.TD3_discrete_vary.TD3_discrete_vary", "agents.QL.QL", "agents.QL.QL", "agents.SARSA.SARSA", "agents.SARSA.SARSA", "NotImplementedError"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_real_env", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.get_max_action", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.get_max_action", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.get_max_action", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.get_max_action", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.get_max_action", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.get_min_action"], ["def", "select_agent", "(", "config", ",", "agent_name", ")", ":", "\n", "    ", "env_factory", "=", "EnvFactory", "(", "config", ")", "\n", "dummy_env", "=", "env_factory", ".", "generate_real_env", "(", "print_str", "=", "'Select Agent: '", ")", "\n", "\n", "agent_name", "=", "agent_name", ".", "lower", "(", ")", "\n", "\n", "if", "agent_name", "==", "\"td3\"", ":", "\n", "        ", "max_action", "=", "dummy_env", ".", "get_max_action", "(", ")", "\n", "return", "TD3", "(", "env", "=", "dummy_env", ",", "max_action", "=", "max_action", ",", "config", "=", "config", ")", "\n", "", "if", "agent_name", "==", "\"td3_icm\"", ":", "\n", "        ", "max_action", "=", "dummy_env", ".", "get_max_action", "(", ")", "\n", "return", "TD3", "(", "env", "=", "dummy_env", ",", "max_action", "=", "max_action", ",", "config", "=", "config", ",", "icm", "=", "True", ")", "\n", "", "elif", "agent_name", "==", "\"td3_vary\"", ":", "\n", "        ", "max_action", "=", "dummy_env", ".", "get_max_action", "(", ")", "\n", "return", "TD3_vary", "(", "env", "=", "dummy_env", ",", "max_action", "=", "max_action", ",", "config", "=", "config", ")", "\n", "", "elif", "agent_name", "==", "\"td3_icm_vary\"", ":", "\n", "        ", "max_action", "=", "dummy_env", ".", "get_max_action", "(", ")", "\n", "return", "TD3_vary", "(", "env", "=", "dummy_env", ",", "max_action", "=", "max_action", ",", "config", "=", "config", ",", "icm", "=", "True", ")", "\n", "", "elif", "agent_name", "==", "\"ppo\"", ":", "\n", "        ", "return", "PPO", "(", "env", "=", "dummy_env", ",", "config", "=", "config", ")", "\n", "", "elif", "agent_name", "==", "\"ppo_icm\"", ":", "\n", "        ", "return", "PPO", "(", "env", "=", "dummy_env", ",", "config", "=", "config", ",", "icm", "=", "True", ")", "\n", "", "elif", "agent_name", "==", "\"ddqn\"", ":", "\n", "        ", "return", "DDQN", "(", "env", "=", "dummy_env", ",", "config", "=", "config", ")", "\n", "", "elif", "agent_name", "==", "\"ddqn_icm\"", ":", "\n", "        ", "return", "DDQN", "(", "env", "=", "dummy_env", ",", "config", "=", "config", ",", "icm", "=", "True", ")", "\n", "", "elif", "agent_name", "==", "\"ddqn_vary\"", ":", "\n", "        ", "return", "DDQN_vary", "(", "env", "=", "dummy_env", ",", "config", "=", "config", ")", "\n", "", "elif", "agent_name", "==", "\"ddqn_icm_vary\"", ":", "\n", "        ", "return", "DDQN_vary", "(", "env", "=", "dummy_env", ",", "config", "=", "config", ",", "icm", "=", "True", ")", "\n", "", "elif", "agent_name", "==", "\"duelingddqn\"", ":", "\n", "        ", "return", "DuelingDDQN", "(", "env", "=", "dummy_env", ",", "config", "=", "config", ")", "\n", "", "elif", "agent_name", "==", "\"duelingddqn_icm\"", ":", "\n", "        ", "return", "DuelingDDQN", "(", "env", "=", "dummy_env", ",", "config", "=", "config", ",", "icm", "=", "True", ")", "\n", "", "elif", "agent_name", "==", "\"duelingddqn_vary\"", ":", "\n", "        ", "return", "DuelingDDQN_vary", "(", "env", "=", "dummy_env", ",", "config", "=", "config", ")", "\n", "", "elif", "agent_name", "==", "\"duelingddqn_icm_vary\"", ":", "\n", "        ", "return", "DuelingDDQN_vary", "(", "env", "=", "dummy_env", ",", "config", "=", "config", ",", "icm", "=", "True", ")", "\n", "", "elif", "agent_name", "==", "\"td3_discrete_vary\"", ":", "\n", "        ", "max_action", "=", "dummy_env", ".", "get_max_action", "(", ")", "\n", "min_action", "=", "dummy_env", ".", "get_min_action", "(", ")", "\n", "return", "TD3_discrete_vary", "(", "env", "=", "dummy_env", ",", "config", "=", "config", ",", "min_action", "=", "min_action", ",", "max_action", "=", "max_action", ")", "\n", "", "elif", "agent_name", "==", "\"ql\"", ":", "\n", "        ", "return", "QL", "(", "env", "=", "dummy_env", ",", "config", "=", "config", ")", "\n", "", "elif", "agent_name", "==", "\"ql_cb\"", ":", "\n", "        ", "return", "QL", "(", "env", "=", "dummy_env", ",", "config", "=", "config", ",", "count_based", "=", "True", ")", "\n", "", "elif", "agent_name", "==", "\"sarsa\"", ":", "\n", "        ", "return", "SARSA", "(", "env", "=", "dummy_env", ",", "config", "=", "config", ")", "\n", "", "elif", "agent_name", "==", "\"sarsa_cb\"", ":", "\n", "        ", "return", "SARSA", "(", "env", "=", "dummy_env", ",", "config", "=", "config", ",", "count_based", "=", "True", ")", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\"Unknownn RL agent\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.agent_utils.print_stats": [[69, 76], ["isinstance", "utils.print_abs_param_sum", "utils.print_abs_param_sum", "utils.print_abs_param_sum", "isinstance", "utils.print_abs_param_sum"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.None.utils.print_abs_param_sum", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.print_abs_param_sum", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.print_abs_param_sum", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.print_abs_param_sum"], ["", "", "def", "print_stats", "(", "agent", ")", ":", "\n", "    ", "if", "isinstance", "(", "agent", ",", "TD3", ")", ":", "\n", "        ", "print_abs_param_sum", "(", "agent", ".", "actor", ",", "\"Actor\"", ")", "\n", "print_abs_param_sum", "(", "agent", ".", "critic_1", ",", "\"Critic1\"", ")", "\n", "print_abs_param_sum", "(", "agent", ".", "critic_2", ",", "\"Critic2\"", ")", "\n", "", "elif", "isinstance", "(", "agent", ",", "DDQN", ")", ":", "\n", "        ", "print_abs_param_sum", "(", "agent", ".", "model", ",", "\"Model\"", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.__init__": [[16, 48], ["agents.GTN_base.GTN_Base.__init__", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all", "print", "GTN_worker.GTN_Worker.get_input_file_name", "GTN_worker.GTN_Worker.get_input_check_file_name", "GTN_worker.GTN_Worker.get_result_file_name", "GTN_worker.GTN_Worker.get_result_check_file_name", "os.path.isfile", "int", "int", "os.remove", "time.time", "time.time"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_base.GTN_Base.get_input_file_name", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_base.GTN_Base.get_input_check_file_name", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_base.GTN_Base.get_result_file_name", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_base.GTN_Base.get_result_check_file_name"], ["    ", "def", "__init__", "(", "self", ",", "id", ",", "bohb_id", "=", "-", "1", ")", ":", "\n", "        ", "\"\"\"\n        params:\n          id: identifies the different workers\n          bohb_id: identifies the different BOHB runs\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", "bohb_id", ")", "\n", "\n", "torch", ".", "manual_seed", "(", "id", "+", "bohb_id", "*", "id", "*", "1000", "+", "int", "(", "time", ".", "time", "(", ")", ")", ")", "\n", "torch", ".", "cuda", ".", "manual_seed_all", "(", "id", "+", "bohb_id", "*", "id", "*", "1000", "+", "int", "(", "time", ".", "time", "(", ")", ")", ")", "\n", "\n", "# for identifying the different workers", "\n", "self", ".", "id", "=", "id", "\n", "self", ".", "test_counter", "=", "0", "\n", "\n", "# flag to stop worker", "\n", "self", ".", "quit_flag", "=", "False", "\n", "self", ".", "time_sleep_worker", "=", "3", "\n", "self", ".", "timeout", "=", "None", "\n", "\n", "# delete corresponding sync files if existent", "\n", "for", "file", "in", "[", "self", ".", "get_input_file_name", "(", "self", ".", "id", ")", ",", "self", ".", "get_input_check_file_name", "(", "self", ".", "id", ")", ",", "\n", "self", ".", "get_result_file_name", "(", "self", ".", "id", ")", ",", "self", ".", "get_result_check_file_name", "(", "self", ".", "id", ")", "]", ":", "\n", "            ", "if", "os", ".", "path", ".", "isfile", "(", "file", ")", ":", "\n", "                ", "os", ".", "remove", "(", "file", ")", "\n", "\n", "# save_dir = \"mbrl_baseline\"", "\n", "# self.save_path = f\"./{save_dir}\"", "\n", "# if not os.path.exists(self.save_path):", "\n", "#     os.mkdir(self.save_path)", "\n", "\n", "", "", "print", "(", "'Starting GTN Worker with bohb_id {} and id {}'", ".", "format", "(", "bohb_id", ",", "id", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.late_init": [[49, 75], ["envs.env_factory.EnvFactory", "generate_synthetic_env_fn", "generate_synthetic_env_fn", "generate_synthetic_env_fn", "NotImplementedError", "str", "str", "str"], "methods", ["None"], ["", "def", "late_init", "(", "self", ",", "config", ")", ":", "\n", "        ", "gtn_config", "=", "config", "[", "\"agents\"", "]", "[", "\"gtn\"", "]", "\n", "self", ".", "noise_std", "=", "gtn_config", "[", "\"noise_std\"", "]", "\n", "self", ".", "num_grad_evals", "=", "gtn_config", "[", "\"num_grad_evals\"", "]", "\n", "self", ".", "grad_eval_type", "=", "gtn_config", "[", "\"grad_eval_type\"", "]", "\n", "self", ".", "mirrored_sampling", "=", "gtn_config", "[", "\"mirrored_sampling\"", "]", "\n", "self", ".", "time_sleep_worker", "=", "gtn_config", "[", "\"time_sleep_worker\"", "]", "\n", "self", ".", "agent_name", "=", "gtn_config", "[", "\"agent_name\"", "]", "\n", "self", ".", "synthetic_env_type", "=", "gtn_config", "[", "\"synthetic_env_type\"", "]", "\n", "self", ".", "unsolved_weight", "=", "gtn_config", "[", "\"unsolved_weight\"", "]", "\n", "\n", "# make it faster on single PC", "\n", "if", "gtn_config", "[", "\"mode\"", "]", "==", "'single'", ":", "\n", "            ", "self", ".", "time_sleep_worker", "/=", "10", "\n", "\n", "", "self", ".", "env_factory", "=", "EnvFactory", "(", "config", ")", "\n", "if", "self", ".", "synthetic_env_type", "==", "0", ":", "\n", "            ", "generate_synthetic_env_fn", "=", "self", ".", "env_factory", ".", "generate_virtual_env", "\n", "", "elif", "self", ".", "synthetic_env_type", "==", "1", ":", "\n", "            ", "generate_synthetic_env_fn", "=", "self", ".", "env_factory", ".", "generate_reward_env", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\"Unknown synthetic_env_type value: \"", "+", "str", "(", "self", ".", "synthetic_env_type", ")", ")", "\n", "\n", "", "self", ".", "synthetic_env_orig", "=", "generate_synthetic_env_fn", "(", "print_str", "=", "'GTN_Base: '", ")", "\n", "self", ".", "synthetic_env", "=", "generate_synthetic_env_fn", "(", "print_str", "=", "'GTN_Worker'", "+", "str", "(", "id", ")", "+", "': '", ")", "\n", "self", ".", "eps", "=", "generate_synthetic_env_fn", "(", "'GTN_Worker'", "+", "str", "(", "id", ")", "+", "': '", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.run": [[76, 115], ["print", "GTN_worker.GTN_Worker.read_worker_input", "time.time", "GTN_worker.GTN_Worker.calc_score", "GTN_worker.GTN_Worker.get_random_noise", "GTN_worker.GTN_Worker.add_noise_to_synthetic_env", "range", "GTN_worker.GTN_Worker.subtract_noise_from_synthetic_env", "range", "GTN_worker.GTN_Worker.calc_best_score", "GTN_worker.GTN_Worker.write_worker_result", "GTN_worker.GTN_Worker.calc_score", "score_add.append", "GTN_worker.GTN_Worker.calc_score", "score_sub.append", "print", "str", "time.time", "time.time", "time.time", "time.time"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.read_worker_input", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.calc_score", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.get_random_noise", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.add_noise_to_synthetic_env", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.subtract_noise_from_synthetic_env", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.calc_best_score", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.write_worker_result", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.calc_score", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.calc_score"], ["", "def", "run", "(", "self", ")", ":", "\n", "# read data from master", "\n", "        ", "while", "not", "self", ".", "quit_flag", ":", "\n", "            ", "self", ".", "read_worker_input", "(", ")", "\n", "\n", "time_start", "=", "time", ".", "time", "(", ")", "\n", "\n", "# get score for network of the last outer loop iteration", "\n", "score_orig", "=", "self", ".", "calc_score", "(", "env", "=", "self", ".", "synthetic_env_orig", ",", "time_remaining", "=", "self", ".", "timeout", "-", "(", "time", ".", "time", "(", ")", "-", "time_start", ")", ")", "\n", "\n", "self", ".", "get_random_noise", "(", ")", "\n", "\n", "# first mirrored noise +N", "\n", "self", ".", "add_noise_to_synthetic_env", "(", ")", "\n", "\n", "score_add", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "num_grad_evals", ")", ":", "\n", "                ", "score", "=", "self", ".", "calc_score", "(", "env", "=", "self", ".", "synthetic_env", ",", "time_remaining", "=", "self", ".", "timeout", "-", "(", "time", ".", "time", "(", ")", "-", "time_start", ")", ")", "\n", "score_add", ".", "append", "(", "score", ")", "\n", "\n", "# # second mirrored noise -N", "\n", "", "self", ".", "subtract_noise_from_synthetic_env", "(", ")", "\n", "\n", "score_sub", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "num_grad_evals", ")", ":", "\n", "                ", "score", "=", "self", ".", "calc_score", "(", "env", "=", "self", ".", "synthetic_env", ",", "time_remaining", "=", "self", ".", "timeout", "-", "(", "time", ".", "time", "(", ")", "-", "time_start", ")", ")", "\n", "score_sub", ".", "append", "(", "score", ")", "\n", "\n", "", "score_best", "=", "self", ".", "calc_best_score", "(", "score_add", "=", "score_add", ",", "score_sub", "=", "score_sub", ")", "\n", "\n", "self", ".", "write_worker_result", "(", "score", "=", "score_best", ",", "\n", "score_orig", "=", "score_orig", ",", "\n", "time_elapsed", "=", "time", ".", "time", "(", ")", "-", "time_start", ")", "\n", "\n", "if", "self", ".", "quit_flag", ":", "\n", "                ", "print", "(", "'QUIT FLAG'", ")", "\n", "break", "\n", "\n", "", "", "print", "(", "'Worker '", "+", "str", "(", "self", ".", "id", ")", "+", "' quitting'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.read_worker_input": [[116, 138], ["GTN_worker.GTN_Worker.get_input_file_name", "GTN_worker.GTN_Worker.get_input_check_file_name", "time.sleep", "torch.load", "torch.load", "torch.load", "torch.load", "GTN_worker.GTN_Worker.late_init", "GTN_worker.GTN_Worker.synthetic_env_orig.load_state_dict", "GTN_worker.GTN_Worker.synthetic_env.load_state_dict", "os.remove", "os.remove", "os.path.isfile", "time.sleep"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_base.GTN_Base.get_input_file_name", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_base.GTN_Base.get_input_check_file_name", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.late_init"], ["", "def", "read_worker_input", "(", "self", ")", ":", "\n", "        ", "file_name", "=", "self", ".", "get_input_file_name", "(", "id", "=", "self", ".", "id", ")", "\n", "check_file_name", "=", "self", ".", "get_input_check_file_name", "(", "id", "=", "self", ".", "id", ")", "\n", "\n", "while", "not", "os", ".", "path", ".", "isfile", "(", "check_file_name", ")", ":", "\n", "            ", "time", ".", "sleep", "(", "self", ".", "time_sleep_worker", ")", "\n", "", "time", ".", "sleep", "(", "self", ".", "time_sleep_worker", ")", "\n", "\n", "data", "=", "torch", ".", "load", "(", "file_name", ")", "\n", "\n", "self", ".", "timeout", "=", "data", "[", "'timeout'", "]", "\n", "self", ".", "quit_flag", "=", "data", "[", "'quit_flag'", "]", "\n", "self", ".", "config", "=", "data", "[", "'config'", "]", "\n", "# self.bohb_next_run_counter = data['bohb_next_run_counter']", "\n", "\n", "self", ".", "late_init", "(", "self", ".", "config", ")", "\n", "\n", "self", ".", "synthetic_env_orig", ".", "load_state_dict", "(", "data", "[", "'synthetic_env_orig'", "]", ")", "\n", "self", ".", "synthetic_env", ".", "load_state_dict", "(", "data", "[", "'synthetic_env_orig'", "]", ")", "\n", "\n", "os", ".", "remove", "(", "check_file_name", ")", "\n", "os", ".", "remove", "(", "file_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.write_worker_result": [[139, 155], ["GTN_worker.GTN_Worker.get_result_file_name", "GTN_worker.GTN_Worker.get_result_check_file_name", "os.path.isfile", "GTN_worker.GTN_Worker.eps.state_dict", "GTN_worker.GTN_Worker.synthetic_env.state_dict", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "time.sleep"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_base.GTN_Base.get_result_file_name", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_base.GTN_Base.get_result_check_file_name"], ["", "def", "write_worker_result", "(", "self", ",", "score", ",", "score_orig", ",", "time_elapsed", ")", ":", "\n", "        ", "file_name", "=", "self", ".", "get_result_file_name", "(", "id", "=", "self", ".", "id", ")", "\n", "check_file_name", "=", "self", ".", "get_result_check_file_name", "(", "id", "=", "self", ".", "id", ")", "\n", "\n", "# wait until master has deleted the file (i.e. acknowledged the previous result)", "\n", "while", "os", ".", "path", ".", "isfile", "(", "file_name", ")", ":", "\n", "            ", "time", ".", "sleep", "(", "self", ".", "time_sleep_worker", ")", "\n", "\n", "", "data", "=", "{", "}", "\n", "data", "[", "\"eps\"", "]", "=", "self", ".", "eps", ".", "state_dict", "(", ")", "\n", "data", "[", "\"synthetic_env\"", "]", "=", "self", ".", "synthetic_env", ".", "state_dict", "(", ")", "# for debugging", "\n", "data", "[", "\"time_elapsed\"", "]", "=", "time_elapsed", "\n", "data", "[", "\"score\"", "]", "=", "score", "\n", "data", "[", "\"score_orig\"", "]", "=", "score_orig", "\n", "torch", ".", "save", "(", "data", ",", "file_name", ")", "\n", "torch", ".", "save", "(", "{", "}", ",", "check_file_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.get_random_noise": [[156, 164], ["zip", "GTN_worker.GTN_Worker.synthetic_env.modules", "GTN_worker.GTN_Worker.eps.modules", "isinstance", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.normal", "torch.normal", "torch.normal", "torch.normal", "torch.normal", "torch.normal", "torch.normal", "torch.normal", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cartpole_transfer_algo.normal", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cartpole_transfer_algo.normal", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cartpole_transfer_algo.normal", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cartpole_transfer_algo.normal", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cartpole_transfer_algo.normal", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cartpole_transfer_algo.normal", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cartpole_transfer_algo.normal", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cartpole_transfer_algo.normal"], ["", "def", "get_random_noise", "(", "self", ")", ":", "\n", "        ", "for", "l_virt", ",", "l_eps", "in", "zip", "(", "self", ".", "synthetic_env", ".", "modules", "(", ")", ",", "self", ".", "eps", ".", "modules", "(", ")", ")", ":", "\n", "            ", "if", "isinstance", "(", "l_virt", ",", "nn", ".", "Linear", ")", ":", "\n", "                ", "l_eps", ".", "weight", "=", "torch", ".", "nn", ".", "Parameter", "(", "torch", ".", "normal", "(", "mean", "=", "torch", ".", "zeros_like", "(", "l_virt", ".", "weight", ")", ",", "\n", "std", "=", "torch", ".", "ones_like", "(", "l_virt", ".", "weight", ")", ")", "*", "self", ".", "noise_std", ")", "\n", "if", "l_eps", ".", "bias", "!=", "None", ":", "\n", "                    ", "l_eps", ".", "bias", "=", "torch", ".", "nn", ".", "Parameter", "(", "torch", ".", "normal", "(", "mean", "=", "torch", ".", "zeros_like", "(", "l_virt", ".", "bias", ")", ",", "\n", "std", "=", "torch", ".", "ones_like", "(", "l_virt", ".", "bias", ")", ")", "*", "self", ".", "noise_std", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.add_noise_to_synthetic_env": [[165, 176], ["zip", "GTN_worker.GTN_Worker.synthetic_env_orig.modules", "GTN_worker.GTN_Worker.synthetic_env.modules", "GTN_worker.GTN_Worker.eps.modules", "isinstance", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter"], "methods", ["None"], ["", "", "", "", "def", "add_noise_to_synthetic_env", "(", "self", ",", "add", "=", "True", ")", ":", "\n", "        ", "for", "l_orig", ",", "l_virt", ",", "l_eps", "in", "zip", "(", "self", ".", "synthetic_env_orig", ".", "modules", "(", ")", ",", "self", ".", "synthetic_env", ".", "modules", "(", ")", ",", "self", ".", "eps", ".", "modules", "(", ")", ")", ":", "\n", "            ", "if", "isinstance", "(", "l_virt", ",", "nn", ".", "Linear", ")", ":", "\n", "                ", "if", "add", ":", "# add eps", "\n", "                    ", "l_virt", ".", "weight", "=", "torch", ".", "nn", ".", "Parameter", "(", "l_orig", ".", "weight", "+", "l_eps", ".", "weight", ")", "\n", "if", "l_virt", ".", "bias", "!=", "None", ":", "\n", "                        ", "l_virt", ".", "bias", "=", "torch", ".", "nn", ".", "Parameter", "(", "l_orig", ".", "bias", "+", "l_eps", ".", "bias", ")", "\n", "", "", "else", ":", "# subtract eps", "\n", "                    ", "l_virt", ".", "weight", "=", "torch", ".", "nn", ".", "Parameter", "(", "l_orig", ".", "weight", "-", "l_eps", ".", "weight", ")", "\n", "if", "l_virt", ".", "bias", "!=", "None", ":", "\n", "                        ", "l_virt", ".", "bias", "=", "torch", ".", "nn", ".", "Parameter", "(", "l_orig", ".", "bias", "-", "l_eps", ".", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.subtract_noise_from_synthetic_env": [[177, 179], ["GTN_worker.GTN_Worker.add_noise_to_synthetic_env"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.add_noise_to_synthetic_env"], ["", "", "", "", "", "def", "subtract_noise_from_synthetic_env", "(", "self", ")", ":", "\n", "        ", "self", ".", "add_noise_to_synthetic_env", "(", "add", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.invert_eps": [[180, 186], ["GTN_worker.GTN_Worker.eps.modules", "isinstance", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter"], "methods", ["None"], ["", "def", "invert_eps", "(", "self", ")", ":", "\n", "        ", "for", "l_eps", "in", "self", ".", "eps", ".", "modules", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "l_eps", ",", "nn", ".", "Linear", ")", ":", "\n", "                ", "l_eps", ".", "weight", "=", "torch", ".", "nn", ".", "Parameter", "(", "-", "l_eps", ".", "weight", ")", "\n", "if", "l_eps", ".", "bias", "!=", "None", ":", "\n", "                    ", "l_eps", ".", "bias", "=", "torch", ".", "nn", ".", "Parameter", "(", "-", "l_eps", ".", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.calc_score": [[187, 222], ["time.time", "agents.agent_utils.select_agent", "GTN_worker.GTN_Worker.env_factory.generate_real_env", "agents.agent_utils.select_agent.train", "agents.agent_utils.select_agent.test", "statistics.mean", "env.is_virtual_env", "print", "time.time", "time.time"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.agents.agent_utils.select_agent", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_real_env", "home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.train", "home.repos.pwc.inspect_result.automl_learning_environments.agents.base_agent.BaseAgent.test", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.is_virtual_env"], ["", "", "", "", "def", "calc_score", "(", "self", ",", "env", ",", "time_remaining", ")", ":", "\n", "        ", "time_start", "=", "time", ".", "time", "(", ")", "\n", "\n", "agent", "=", "select_agent", "(", "config", "=", "self", ".", "config", ",", "agent_name", "=", "self", ".", "agent_name", ")", "\n", "real_env", "=", "self", ".", "env_factory", ".", "generate_real_env", "(", ")", "\n", "\n", "reward_list_train", ",", "episode_length_train", ",", "_", "=", "agent", ".", "train", "(", "env", "=", "env", ",", "test_env", "=", "real_env", ",", "time_remaining", "=", "time_remaining", "-", "(", "time", ".", "time", "(", ")", "-", "time_start", ")", ")", "\n", "\n", "# if len(agent.trajectories) != 0:", "\n", "#     print(\"trajectories should have been cleared\")", "\n", "#     agent.trajectories.clear()", "\n", "\n", "reward_list_test", ",", "_", ",", "_", "=", "agent", ".", "test", "(", "env", "=", "real_env", ",", "time_remaining", "=", "time_remaining", "-", "(", "time", ".", "time", "(", ")", "-", "time_start", ")", ")", "\n", "\n", "# trajectories = copy.deepcopy(agent.trajectories)", "\n", "# GTN_Worker.store_data(trajectories, datasets_dir=self.save_path, id=self.id, test_counter=self.test_counter,", "\n", "#                       bohb_next_run_counter=self.bohb_next_run_counter)", "\n", "\n", "# self.test_counter += 1", "\n", "# agent.trajectories.clear()", "\n", "# assert len(agent.trajectories) == 0", "\n", "\n", "avg_reward_test", "=", "statistics", ".", "mean", "(", "reward_list_test", ")", "\n", "\n", "if", "env", ".", "is_virtual_env", "(", ")", ":", "\n", "            ", "return", "avg_reward_test", "\n", "", "else", ":", "\n", "# # when timeout occurs, reward_list_train is padded (with min. reward values) and episode_length_train is not", "\n", "# if len(episode_length_train) < len(reward_list_train):", "\n", "#     print(\"due to timeout, reward_list_train has been padded\")", "\n", "#     print(f\"shape rewards: {np.shape(reward_list_train)}, shape episode lengths: {np.shape(episode_length_train)}\")", "\n", "#     reward_list_train = reward_list_train[:len(episode_length_train)]", "\n", "\n", "            ", "print", "(", "\"AVG REWARD: \"", ",", "avg_reward_test", ")", "\n", "return", "avg_reward_test", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.calc_best_score": [[234, 255], ["statistics.mean", "statistics.mean", "max", "GTN_worker.GTN_Worker.add_noise_to_synthetic_env", "min", "min", "NotImplementedError", "GTN_worker.GTN_Worker.invert_eps", "GTN_worker.GTN_Worker.add_noise_to_synthetic_env", "str"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.add_noise_to_synthetic_env", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.invert_eps", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.add_noise_to_synthetic_env"], ["", "", "def", "calc_best_score", "(", "self", ",", "score_sub", ",", "score_add", ")", ":", "\n", "        ", "if", "self", ".", "grad_eval_type", "==", "'mean'", ":", "\n", "            ", "score_sub", "=", "statistics", ".", "mean", "(", "score_sub", ")", "\n", "score_add", "=", "statistics", ".", "mean", "(", "score_add", ")", "\n", "", "elif", "self", ".", "grad_eval_type", "==", "'minmax'", ":", "\n", "            ", "score_sub", "=", "min", "(", "score_sub", ")", "\n", "score_add", "=", "min", "(", "score_add", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "'Unknown parameter for grad_eval_type: '", "+", "str", "(", "self", ".", "grad_eval_type", ")", ")", "\n", "\n", "", "if", "self", ".", "mirrored_sampling", ":", "\n", "            ", "score_best", "=", "max", "(", "score_add", ",", "score_sub", ")", "\n", "if", "score_sub", ">", "score_add", ":", "\n", "                ", "self", ".", "invert_eps", "(", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "add_noise_to_synthetic_env", "(", ")", "\n", "", "", "else", ":", "\n", "            ", "score_best", "=", "score_add", "\n", "self", ".", "add_noise_to_synthetic_env", "(", ")", "\n", "\n", "", "return", "score_best", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.__init__": [[11, 15], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "env", "=", "env", "\n", "self", ".", "same_action_num", "=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.step": [[16, 71], ["env_wrapper.EnvWrapper.is_virtual_env", "env_wrapper.EnvWrapper.has_discrete_action_space", "reward_sum.to", "state.to", "done.to.to.to", "env_wrapper.EnvWrapper.has_discrete_state_space", "utils.to_one_hot_encoding.cpu().detach().numpy", "env_wrapper.EnvWrapper.has_discrete_action_space", "range", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "utils.to_one_hot_encoding", "range", "range", "utils.from_one_hot_encoding", "env_wrapper.EnvWrapper.env.step", "next_state_torch.unsqueeze.unsqueeze.dim", "next_state_torch.unsqueeze.unsqueeze.unsqueeze", "env_wrapper.EnvWrapper.get_action_dim", "env_wrapper.EnvWrapper.env.step", "env_wrapper.EnvWrapper.env.step", "utils.to_one_hot_encoding.cpu().detach", "utils.to_one_hot_encoding.astype", "utils.to_one_hot_encoding.to", "utils.to_one_hot_encoding.to", "state.to", "utils.to_one_hot_encoding.cpu"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.is_virtual_env", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.has_discrete_action_space", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.has_discrete_state_space", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.has_discrete_action_space", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.to_one_hot_encoding", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.from_one_hot_encoding", "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditEnv.step", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.get_action_dim", "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditEnv.step", "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditEnv.step"], ["", "def", "step", "(", "self", ",", "action", ",", "state", "=", "None", ")", ":", "\n", "        ", "if", "self", ".", "is_virtual_env", "(", ")", ":", "\n", "            ", "reward_sum", "=", "None", "\n", "\n", "if", "self", ".", "has_discrete_action_space", "(", ")", ":", "\n", "                ", "action", "=", "to_one_hot_encoding", "(", "action", ",", "self", ".", "get_action_dim", "(", ")", ")", "\n", "\n", "", "if", "state", "is", "None", ":", "\n", "                ", "for", "i", "in", "range", "(", "self", ".", "same_action_num", ")", ":", "\n", "                    ", "state", ",", "reward", ",", "done", "=", "self", ".", "env", ".", "step", "(", "action", "=", "action", ".", "to", "(", "self", ".", "env", ".", "device", ")", ")", "\n", "if", "reward_sum", "is", "None", ":", "\n", "                        ", "reward_sum", "=", "reward", "\n", "", "else", ":", "\n", "                        ", "reward_sum", "+=", "reward", "\n", "# TODO: proper handling of the done flag for a batch of states/actions if same_action_num > 1", "\n", "# required for the histogram experiment", "\n", "", "", "", "else", ":", "\n", "                ", "for", "i", "in", "range", "(", "self", ".", "same_action_num", ")", ":", "\n", "                    ", "state", ",", "reward", ",", "done", "=", "self", ".", "env", ".", "step", "(", "action", "=", "action", ".", "to", "(", "self", ".", "env", ".", "device", ")", ",", "state", "=", "state", ".", "to", "(", "self", ".", "env", ".", "device", ")", ")", "\n", "if", "reward_sum", "is", "None", ":", "\n", "                        ", "reward_sum", "=", "reward", "\n", "", "else", ":", "\n", "                        ", "reward_sum", "+=", "reward", "\n", "# TODO: proper handling of the done flag for a batch of states/actions if same_action_num > 1", "\n", "\n", "# todo: to device?", "\n", "", "", "", "reward", "=", "reward_sum", ".", "to", "(", "\"cpu\"", ")", "\n", "next_state", "=", "state", ".", "to", "(", "\"cpu\"", ")", "\n", "done", "=", "done", ".", "to", "(", "\"cpu\"", ")", "\n", "\n", "if", "self", ".", "has_discrete_state_space", "(", ")", ":", "\n", "                ", "next_state", "=", "from_one_hot_encoding", "(", "next_state", ")", "\n", "\n", "", "return", "next_state", ",", "reward", ",", "done", "\n", "\n", "", "else", ":", "\n", "            ", "action", "=", "action", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "if", "self", ".", "has_discrete_action_space", "(", ")", ":", "\n", "                ", "action", "=", "action", ".", "astype", "(", "int", ")", "[", "0", "]", "\n", "\n", "", "reward_sum", "=", "0", "\n", "for", "i", "in", "range", "(", "self", ".", "same_action_num", ")", ":", "\n", "                ", "state", ",", "reward", ",", "done", ",", "_", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "reward_sum", "+=", "reward", "\n", "if", "done", ":", "\n", "                    ", "break", "\n", "\n", "", "", "next_state_torch", "=", "torch", ".", "tensor", "(", "state", ",", "device", "=", "\"cpu\"", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "reward_torch", "=", "torch", ".", "tensor", "(", "reward_sum", ",", "device", "=", "\"cpu\"", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "done_torch", "=", "torch", ".", "tensor", "(", "done", ",", "device", "=", "\"cpu\"", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "\n", "if", "next_state_torch", ".", "dim", "(", ")", "==", "0", ":", "\n", "                ", "next_state_torch", "=", "next_state_torch", ".", "unsqueeze", "(", "0", ")", "\n", "\n", "", "return", "next_state_torch", ",", "reward_torch", ",", "done_torch", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.reset": [[72, 86], ["env_wrapper.EnvWrapper.env.reset", "type", "torch.from_numpy().float().cpu", "torch.from_numpy().float().cpu", "torch.from_numpy().float().cpu", "torch.from_numpy().float().cpu", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "env_wrapper.EnvWrapper.has_discrete_state_space", "env_wrapper.EnvWrapper.is_virtual_env", "utils.from_one_hot_encoding", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditEnv.reset", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.has_discrete_state_space", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.is_virtual_env", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.from_one_hot_encoding"], ["", "", "def", "reset", "(", "self", ")", ":", "\n", "        ", "state", "=", "self", ".", "env", ".", "reset", "(", ")", "\n", "\n", "if", "type", "(", "state", ")", "==", "np", ".", "ndarray", ":", "\n", "            ", "state_torch", "=", "torch", ".", "from_numpy", "(", "state", ")", ".", "float", "(", ")", ".", "cpu", "(", ")", "\n", "", "elif", "torch", ".", "is_tensor", "(", "state", ")", ":", "\n", "            ", "state_torch", "=", "state", "\n", "", "else", ":", "# float", "\n", "            ", "state_torch", "=", "torch", ".", "tensor", "(", "[", "state", "]", ",", "device", "=", "\"cpu\"", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "\n", "", "if", "self", ".", "has_discrete_state_space", "(", ")", "and", "self", ".", "is_virtual_env", "(", ")", ":", "\n", "            ", "return", "from_one_hot_encoding", "(", "state_torch", ")", "\n", "", "else", ":", "\n", "            ", "return", "state_torch", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.get_random_action": [[87, 93], ["env_wrapper.EnvWrapper.env.action_space.sample", "type", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "env_wrapper.EnvWrapper.env.action_space.sample", "type", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.None.utils.ReplayBuffer.sample", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.ReplayBuffer.sample"], ["", "", "def", "get_random_action", "(", "self", ")", ":", "\n", "        ", "action", "=", "self", ".", "env", ".", "action_space", ".", "sample", "(", ")", "\n", "if", "type", "(", "action", ")", "==", "np", ".", "ndarray", ":", "\n", "            ", "return", "torch", ".", "from_numpy", "(", "self", ".", "env", ".", "action_space", ".", "sample", "(", ")", ")", "\n", "", "elif", "type", "(", "action", ")", "==", "int", ":", "\n", "            ", "return", "torch", ".", "tensor", "(", "[", "action", "]", ",", "device", "=", "\"cpu\"", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.get_state_dim": [[94, 99], ["None"], "methods", ["None"], ["", "", "def", "get_state_dim", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "env", ".", "observation_space", ".", "shape", ":", "\n", "            ", "return", "self", ".", "env", ".", "observation_space", ".", "shape", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "env", ".", "observation_space", ".", "n", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.get_action_dim": [[100, 105], ["None"], "methods", ["None"], ["", "", "def", "get_action_dim", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "env", ".", "action_space", ".", "shape", ":", "\n", "            ", "return", "self", ".", "env", ".", "action_space", ".", "shape", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "env", ".", "action_space", ".", "n", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.get_max_action": [[106, 111], ["None"], "methods", ["None"], ["", "", "def", "get_max_action", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "env", ".", "env_name", "==", "'Pendulum-v0'", ":", "\n", "            ", "return", "2", "\n", "", "else", ":", "\n", "            ", "return", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.get_min_action": [[112, 117], ["env_wrapper.EnvWrapper.get_max_action"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.get_max_action"], ["", "", "def", "get_min_action", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "env", ".", "env_name", "==", "'CartPole-v0'", ":", "\n", "            ", "return", "0", "\n", "", "else", ":", "\n", "            ", "return", "-", "self", ".", "get_max_action", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.has_discrete_action_space": [[118, 123], ["isinstance"], "methods", ["None"], ["", "", "def", "has_discrete_action_space", "(", "self", ")", ":", "\n", "        ", "if", "isinstance", "(", "self", ".", "env", ".", "action_space", ",", "Discrete", ")", ":", "\n", "            ", "return", "True", "\n", "", "else", ":", "\n", "            ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.has_discrete_state_space": [[124, 129], ["isinstance"], "methods", ["None"], ["", "", "def", "has_discrete_state_space", "(", "self", ")", ":", "\n", "        ", "if", "isinstance", "(", "self", ".", "env", ".", "observation_space", ",", "Discrete", ")", ":", "\n", "            ", "return", "True", "\n", "", "else", ":", "\n", "            ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.render": [[130, 137], ["env_wrapper.EnvWrapper.is_virtual_env", "env_wrapper.EnvWrapper.env.render", "env_wrapper.EnvWrapper.env.reset_env.render"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.is_virtual_env", "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditEnv.render", "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditEnv.render"], ["", "", "def", "render", "(", "self", ")", ":", "\n", "        ", "if", "not", "self", ".", "is_virtual_env", "(", ")", ":", "\n", "            ", "return", "self", ".", "env", ".", "render", "(", ")", "\n", "", "else", ":", "\n", "# hacky", "\n", "            ", "self", ".", "env", ".", "reset_env", ".", "state", "=", "self", ".", "env", ".", "state", "\n", "return", "self", ".", "env", ".", "reset_env", ".", "render", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.close": [[138, 141], ["env_wrapper.EnvWrapper.is_virtual_env", "env_wrapper.EnvWrapper.env.close"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.is_virtual_env", "home.repos.pwc.inspect_result.automl_learning_environments.envs.reward_env.RewardEnv.close"], ["", "", "def", "close", "(", "self", ")", ":", "\n", "        ", "if", "not", "self", ".", "is_virtual_env", "(", ")", ":", "\n", "            ", "return", "self", ".", "env", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.get_solved_reward": [[142, 144], ["None"], "methods", ["None"], ["", "", "def", "get_solved_reward", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "env", ".", "solved_reward", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.max_episode_steps": [[145, 147], ["None"], "methods", ["None"], ["", "def", "max_episode_steps", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "env", ".", "_max_episode_steps", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.can_be_solved": [[148, 153], ["None"], "methods", ["None"], ["", "def", "can_be_solved", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "env", ".", "solved_reward", "<", "1e9", ":", "\n", "            ", "return", "True", "\n", "", "else", ":", "\n", "            ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.seed": [[154, 160], ["env_wrapper.EnvWrapper.is_virtual_env", "env_wrapper.EnvWrapper.env.seed", "print"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.is_virtual_env", "home.repos.pwc.inspect_result.automl_learning_environments.envs.reward_env.RewardEnv.seed"], ["", "", "def", "seed", "(", "self", ",", "seed", ")", ":", "\n", "        ", "if", "not", "self", ".", "is_virtual_env", "(", ")", ":", "\n", "            ", "return", "self", ".", "env", ".", "seed", "(", "seed", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "\"Setting manuel seed not yet implemented, performance may decrease\"", ")", "\n", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.is_virtual_env": [[161, 163], ["isinstance"], "methods", ["None"], ["", "", "def", "is_virtual_env", "(", "self", ")", ":", "\n", "        ", "return", "isinstance", "(", "self", ".", "env", ",", "VirtualEnv", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.set_agent_params": [[164, 168], ["hasattr", "env_wrapper.EnvWrapper.env.set_agent_params"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.reward_env.RewardEnv.set_agent_params"], ["", "def", "set_agent_params", "(", "self", ",", "same_action_num", ",", "gamma", ")", ":", "\n", "        ", "self", ".", "same_action_num", "=", "same_action_num", "\n", "if", "hasattr", "(", "self", ".", "env", ",", "\"set_agent_params\"", ")", ":", "\n", "            ", "self", ".", "env", ".", "set_agent_params", "(", "gamma", "=", "gamma", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.reward_env.RewardEnv.__init__": [[8, 28], ["torch.Module.__init__", "str", "str", "int", "int", "int", "float", "int", "int", "reward_env.RewardEnv.build_reward_net", "reward_env.RewardEnv.reset"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__", "home.repos.pwc.inspect_result.automl_learning_environments.envs.reward_env.RewardEnv.build_reward_net", "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditEnv.reset"], ["    ", "def", "__init__", "(", "self", ",", "real_env", ",", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "env_name", "=", "str", "(", "kwargs", "[", "\"env_name\"", "]", ")", "\n", "self", ".", "device", "=", "str", "(", "kwargs", "[", "\"device\"", "]", ")", "\n", "self", ".", "state_dim", "=", "int", "(", "kwargs", "[", "\"state_dim\"", "]", ")", "\n", "self", ".", "action_dim", "=", "int", "(", "kwargs", "[", "\"action_dim\"", "]", ")", "\n", "self", ".", "info_dim", "=", "int", "(", "kwargs", "[", "\"info_dim\"", "]", ")", "\n", "self", ".", "solved_reward", "=", "float", "(", "kwargs", "[", "\"solved_reward\"", "]", ")", "\n", "self", ".", "reward_env_type", "=", "int", "(", "kwargs", "[", "\"reward_env_type\"", "]", ")", "\n", "\n", "# for gym compatibility", "\n", "self", ".", "_max_episode_steps", "=", "int", "(", "kwargs", "[", "\"max_steps\"", "]", ")", "\n", "self", ".", "action_space", "=", "kwargs", "[", "\"action_space\"", "]", "\n", "self", ".", "observation_space", "=", "kwargs", "[", "\"observation_space\"", "]", "\n", "\n", "# initialize two environments", "\n", "self", ".", "real_env", "=", "real_env", "\n", "self", ".", "reward_net", "=", "self", ".", "build_reward_net", "(", "kwargs", ")", "\n", "self", ".", "state", "=", "self", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.reward_env.RewardEnv.build_reward_net": [[29, 60], ["models.model_utils.build_nn_from_config().to", "torch.Linear().to", "torch.Linear().to", "NotImplementedError", "models.model_utils.build_nn_from_config", "NotImplementedError", "torch.Linear", "torch.Linear", "str", "str"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.models.model_utils.build_nn_from_config"], ["", "def", "build_reward_net", "(", "self", ",", "kwargs", ")", ":", "\n", "# 0: original reward", "\n", "# 1: potential function (exclusive)", "\n", "# 2: potential function (additive)", "\n", "# 3: potential function with additional info vector (exclusive)", "\n", "# 4: potential function with additional info vector (additive)", "\n", "# 5: non-potential function (exclusive)", "\n", "# 6: non-potential function (additive)", "\n", "# 7: non-potential function with additional info vector (exclusive)", "\n", "# 8: non-potential function with additional info vector (additive)", "\n", "# 101: weighted info vector as baseline (exclusive)", "\n", "# 102: weighted info vector as baseline (additive)", "\n", "\n", "        ", "if", "self", ".", "reward_env_type", "<", "100", ":", "\n", "            ", "if", "self", ".", "reward_env_type", "==", "0", ":", "\n", "                ", "input_dim", "=", "1", "# dummy dimension", "\n", "", "elif", "self", ".", "reward_env_type", "==", "1", "or", "self", ".", "reward_env_type", "==", "2", "or", "self", ".", "reward_env_type", "==", "5", "or", "self", ".", "reward_env_type", "==", "6", ":", "\n", "                ", "input_dim", "=", "self", ".", "state_dim", "\n", "", "elif", "self", ".", "reward_env_type", "==", "3", "or", "self", ".", "reward_env_type", "==", "4", "or", "self", ".", "reward_env_type", "==", "7", "or", "self", ".", "reward_env_type", "==", "8", ":", "\n", "                ", "input_dim", "=", "self", ".", "state_dim", "+", "self", ".", "info_dim", "\n", "", "else", ":", "\n", "                ", "raise", "NotImplementedError", "(", "'Unknown reward_env_type: '", "+", "str", "(", "self", ".", "reward_env_type", ")", ")", "\n", "\n", "", "return", "build_nn_from_config", "(", "input_dim", "=", "input_dim", ",", "\n", "output_dim", "=", "1", ",", "\n", "nn_config", "=", "kwargs", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "", "else", ":", "\n", "            ", "if", "self", ".", "reward_env_type", "==", "101", "or", "self", ".", "reward_env_type", "==", "102", ":", "\n", "                ", "return", "nn", ".", "Linear", "(", "self", ".", "info_dim", ",", "1", ",", "bias", "=", "False", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "", "else", ":", "\n", "                ", "raise", "NotImplementedError", "(", "'Unknown reward_env_type: '", "+", "str", "(", "self", ".", "reward_env_type", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.reward_env.RewardEnv.step": [[61, 67], ["reward_env.RewardEnv.real_env.step", "reward_env.RewardEnv._calc_reward"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditEnv.step", "home.repos.pwc.inspect_result.automl_learning_environments.envs.gridworld.GridworldEnv._calc_reward"], ["", "", "", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "next_state", ",", "reward", ",", "done", ",", "info", "=", "self", ".", "real_env", ".", "step", "(", "action", ")", "\n", "\n", "reward_res", "=", "self", ".", "_calc_reward", "(", "state", "=", "self", ".", "state", ",", "next_state", "=", "next_state", ",", "reward", "=", "reward", ",", "info", "=", "info", ")", "\n", "self", ".", "state", "=", "next_state", "\n", "return", "next_state", ",", "reward_res", ",", "done", ",", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.reward_env.RewardEnv._calc_reward": [[68, 134], ["torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "reward_env.RewardEnv.item", "info.pop", "isinstance", "utils.to_one_hot_encoding", "utils.to_one_hot_encoding", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "len", "reward_env.RewardEnv.reward_net", "reward_env.RewardEnv.reward_net", "reward_env.RewardEnv.reward_net", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "ValueError", "list", "reward_env.RewardEnv.reward_net", "reward_env.RewardEnv.reward_net", "info.values", "torch.tensor.to", "torch.tensor.to", "torch.tensor.to", "torch.tensor.to", "torch.tensor.to", "torch.tensor.to", "torch.tensor.to", "torch.tensor.to", "reward_env.RewardEnv.reward_net", "torch.tensor.dim", "torch.tensor.dim", "torch.tensor.dim", "torch.tensor.dim", "reward_env.RewardEnv.reward_net", "reward_env.RewardEnv.reward_net", "reward_env.RewardEnv.reward_net", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "ValueError", "list", "reward_env.RewardEnv.reward_net", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "reward_env.RewardEnv.reward_net", "info.values", "torch.tensor.to", "torch.tensor.to", "torch.tensor.to", "torch.tensor.to", "torch.tensor.to", "torch.tensor.to", "torch.tensor.to", "torch.tensor.to", "ValueError", "list", "reward_env.RewardEnv.reward_net", "torch.tensor.dim", "torch.tensor.dim", "torch.tensor.dim", "torch.tensor.dim", "reward_env.RewardEnv.reward_net", "info.values", "reward_env.RewardEnv.reward_net"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.None.utils.to_one_hot_encoding", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.to_one_hot_encoding"], ["", "def", "_calc_reward", "(", "self", ",", "state", ",", "next_state", ",", "reward", ",", "info", ")", ":", "\n", "        ", "if", "'TimeLimit.truncated'", "in", "info", ":", "# remove additional information from wrapper", "\n", "            ", "info", ".", "pop", "(", "'TimeLimit.truncated'", ")", "\n", "\n", "", "reward_torch", "=", "torch", ".", "tensor", "(", "reward", ",", "device", "=", "self", ".", "device", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "\n", "if", "isinstance", "(", "state", ",", "int", ")", "or", "len", "(", "state", ")", "<", "self", ".", "state_dim", ":", "\n", "            ", "state_torch", "=", "to_one_hot_encoding", "(", "state", ",", "self", ".", "state_dim", ")", "\n", "next_state_torch", "=", "to_one_hot_encoding", "(", "next_state", ",", "self", ".", "state_dim", ")", "\n", "", "else", ":", "\n", "            ", "state_torch", "=", "torch", ".", "tensor", "(", "state", ",", "device", "=", "self", ".", "device", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "next_state_torch", "=", "torch", ".", "tensor", "(", "next_state", ",", "device", "=", "self", ".", "device", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "\n", "", "if", "self", ".", "reward_env_type", "==", "0", ":", "\n", "            ", "reward_res", "=", "reward_torch", "\n", "\n", "", "elif", "self", ".", "reward_env_type", "==", "1", ":", "\n", "            ", "reward_res", "=", "self", ".", "gamma", "*", "self", ".", "reward_net", "(", "next_state_torch", ")", "-", "self", ".", "reward_net", "(", "state_torch", ")", "\n", "\n", "", "elif", "self", ".", "reward_env_type", "==", "2", ":", "\n", "            ", "reward_res", "=", "reward_torch", "+", "self", ".", "gamma", "*", "self", ".", "reward_net", "(", "next_state_torch", ")", "-", "self", ".", "reward_net", "(", "state_torch", ")", "\n", "\n", "", "elif", "self", ".", "reward_env_type", "==", "3", "or", "self", ".", "reward_env_type", "==", "4", ":", "\n", "            ", "if", "not", "info", ":", "\n", "                ", "raise", "ValueError", "(", "'No info dict provided by environment'", ")", "\n", "\n", "", "info_torch", "=", "torch", ".", "tensor", "(", "list", "(", "info", ".", "values", "(", ")", ")", ",", "device", "=", "self", ".", "device", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "input_state", "=", "torch", ".", "cat", "(", "(", "state_torch", ".", "to", "(", "self", ".", "device", ")", ",", "info_torch", ".", "to", "(", "self", ".", "device", ")", ")", ",", "dim", "=", "state_torch", ".", "dim", "(", ")", "-", "1", ")", "\n", "input_state_next", "=", "torch", ".", "cat", "(", "(", "next_state_torch", ".", "to", "(", "self", ".", "device", ")", ",", "info_torch", ".", "to", "(", "self", ".", "device", ")", ")", ",", "dim", "=", "state_torch", ".", "dim", "(", ")", "-", "1", ")", "\n", "\n", "if", "self", ".", "reward_env_type", "==", "3", ":", "\n", "                ", "reward_res", "=", "self", ".", "gamma", "*", "self", ".", "reward_net", "(", "input_state_next", ")", "-", "self", ".", "reward_net", "(", "input_state", ")", "\n", "", "elif", "self", ".", "reward_env_type", "==", "4", ":", "\n", "                ", "reward_res", "=", "reward_torch", "+", "self", ".", "gamma", "*", "self", ".", "reward_net", "(", "input_state_next", ")", "-", "self", ".", "reward_net", "(", "input_state", ")", "\n", "\n", "", "", "elif", "self", ".", "reward_env_type", "==", "5", ":", "\n", "            ", "reward_res", "=", "self", ".", "reward_net", "(", "next_state_torch", ")", "\n", "\n", "", "elif", "self", ".", "reward_env_type", "==", "6", ":", "\n", "            ", "reward_res", "=", "reward_torch", "+", "self", ".", "reward_net", "(", "next_state_torch", ")", "\n", "\n", "", "elif", "self", ".", "reward_env_type", "==", "7", "or", "self", ".", "reward_env_type", "==", "8", ":", "\n", "            ", "if", "not", "info", ":", "\n", "                ", "raise", "ValueError", "(", "'No info dict provided by environment'", ")", "\n", "\n", "", "info_torch", "=", "torch", ".", "tensor", "(", "list", "(", "info", ".", "values", "(", ")", ")", ",", "device", "=", "self", ".", "device", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "input_state", "=", "torch", ".", "cat", "(", "(", "state_torch", ".", "to", "(", "self", ".", "device", ")", ",", "info_torch", ".", "to", "(", "self", ".", "device", ")", ")", ",", "dim", "=", "state_torch", ".", "dim", "(", ")", "-", "1", ")", "\n", "input_state_next", "=", "torch", ".", "cat", "(", "(", "next_state_torch", ".", "to", "(", "self", ".", "device", ")", ",", "info_torch", ".", "to", "(", "self", ".", "device", ")", ")", ",", "dim", "=", "state_torch", ".", "dim", "(", ")", "-", "1", ")", "\n", "\n", "if", "self", ".", "reward_env_type", "==", "7", ":", "\n", "                ", "reward_res", "=", "self", ".", "reward_net", "(", "input_state_next", ")", "\n", "", "elif", "self", ".", "reward_env_type", "==", "8", ":", "\n", "                ", "reward_res", "=", "reward_torch", "+", "self", ".", "reward_net", "(", "input_state_next", ")", "\n", "\n", "", "", "elif", "self", ".", "reward_env_type", "==", "101", "or", "self", ".", "reward_env_type", "==", "102", ":", "\n", "            ", "if", "not", "info", ":", "\n", "                ", "raise", "ValueError", "(", "'No info dict provided by environment'", ")", "\n", "\n", "", "info_torch", "=", "torch", ".", "tensor", "(", "list", "(", "info", ".", "values", "(", ")", ")", ",", "device", "=", "self", ".", "device", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "\n", "if", "self", ".", "reward_env_type", "==", "101", ":", "\n", "                ", "reward_res", "=", "self", ".", "reward_net", "(", "info_torch", ")", "\n", "", "elif", "self", ".", "reward_env_type", "==", "102", ":", "\n", "                ", "reward_res", "=", "reward_torch", "+", "self", ".", "reward_net", "(", "info_torch", ")", "\n", "\n", "", "", "return", "reward_res", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.reward_env.RewardEnv.seed": [[135, 137], ["reward_env.RewardEnv.real_env.seed"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.reward_env.RewardEnv.seed"], ["", "def", "seed", "(", "self", ",", "seed", ")", ":", "\n", "        ", "return", "self", ".", "real_env", ".", "seed", "(", "seed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.reward_env.RewardEnv.render": [[138, 140], ["reward_env.RewardEnv.real_env.render"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditEnv.render"], ["", "def", "render", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "real_env", ".", "render", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.reward_env.RewardEnv.reset": [[141, 144], ["reward_env.RewardEnv.real_env.reset"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditEnv.reset"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "state", "=", "self", ".", "real_env", ".", "reset", "(", ")", "\n", "return", "self", ".", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.reward_env.RewardEnv.close": [[145, 147], ["reward_env.RewardEnv.real_env.close"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.reward_env.RewardEnv.close"], ["", "def", "close", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "real_env", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.reward_env.RewardEnv.set_agent_params": [[148, 150], ["None"], "methods", ["None"], ["", "def", "set_agent_params", "(", "self", ",", "gamma", ")", ":", "\n", "        ", "self", ".", "gamma", "=", "gamma", "\n", "", "", ""]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.virtual_env.VirtualEnv.__init__": [[8, 34], ["torch.Module.__init__", "str", "str", "int", "int", "float", "int", "models.model_utils.build_nn_from_config().to", "models.model_utils.build_nn_from_config().to", "models.model_utils.build_nn_from_config().to", "virtual_env.VirtualEnv.reset", "models.model_utils.build_nn_from_config", "models.model_utils.build_nn_from_config", "models.model_utils.build_nn_from_config"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__", "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditEnv.reset", "home.repos.pwc.inspect_result.automl_learning_environments.models.model_utils.build_nn_from_config", "home.repos.pwc.inspect_result.automl_learning_environments.models.model_utils.build_nn_from_config", "home.repos.pwc.inspect_result.automl_learning_environments.models.model_utils.build_nn_from_config"], ["    ", "def", "__init__", "(", "self", ",", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "env_name", "=", "str", "(", "kwargs", "[", "\"env_name\"", "]", ")", "\n", "self", ".", "device", "=", "str", "(", "kwargs", "[", "\"device\"", "]", ")", "\n", "self", ".", "state_dim", "=", "int", "(", "kwargs", "[", "\"state_dim\"", "]", ")", "\n", "self", ".", "action_dim", "=", "int", "(", "kwargs", "[", "\"action_dim\"", "]", ")", "\n", "self", ".", "solved_reward", "=", "float", "(", "kwargs", "[", "\"solved_reward\"", "]", ")", "\n", "\n", "# for gym compatibility", "\n", "self", ".", "_max_episode_steps", "=", "int", "(", "kwargs", "[", "\"max_steps\"", "]", ")", "\n", "self", ".", "action_space", "=", "kwargs", "[", "\"action_space\"", "]", "\n", "self", ".", "observation_space", "=", "kwargs", "[", "\"observation_space\"", "]", "\n", "self", ".", "reset_env", "=", "kwargs", "[", "\"reset_env\"", "]", "\n", "\n", "self", ".", "state_net", "=", "build_nn_from_config", "(", "input_dim", "=", "self", ".", "state_dim", "+", "self", ".", "action_dim", ",", "\n", "output_dim", "=", "self", ".", "state_dim", ",", "\n", "nn_config", "=", "kwargs", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "reward_net", "=", "build_nn_from_config", "(", "input_dim", "=", "self", ".", "state_dim", "+", "self", ".", "action_dim", ",", "\n", "output_dim", "=", "1", ",", "\n", "nn_config", "=", "kwargs", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "done_net", "=", "build_nn_from_config", "(", "input_dim", "=", "self", ".", "state_dim", "+", "self", ".", "action_dim", ",", "\n", "output_dim", "=", "1", ",", "\n", "nn_config", "=", "kwargs", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "self", ".", "state", "=", "self", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.virtual_env.VirtualEnv.reset": [[35, 42], ["torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "virtual_env.VirtualEnv.reset_env.reset", "len", "utils.from_one_hot_encoding", "len", "utils.to_one_hot_encoding"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditEnv.reset", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.from_one_hot_encoding", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.to_one_hot_encoding"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "state", "=", "torch", ".", "tensor", "(", "self", ".", "reset_env", ".", "reset", "(", ")", ")", "\n", "if", "len", "(", "self", ".", "state", ")", ">", "self", ".", "state_dim", ":", "\n", "            ", "self", ".", "state", "=", "from_one_hot_encoding", "(", "self", ".", "state", ")", "\n", "", "elif", "len", "(", "self", ".", "state", ")", "<", "self", ".", "state_dim", ":", "\n", "            ", "self", ".", "state", "=", "to_one_hot_encoding", "(", "self", ".", "state", ",", "self", ".", "state_dim", ")", "\n", "", "return", "self", ".", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.virtual_env.VirtualEnv.step": [[43, 55], ["virtual_env.VirtualEnv.state_net", "virtual_env.VirtualEnv.reward_net", "virtual_env.VirtualEnv.done_net", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "action.to", "virtual_env.VirtualEnv.state.to", "action.to", "state.to", "action.dim", "action.dim"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "action", ",", "state", "=", "None", ")", ":", "\n", "        ", "if", "state", "is", "None", ":", "\n", "            ", "input", "=", "torch", ".", "cat", "(", "(", "action", ".", "to", "(", "self", ".", "device", ")", ",", "self", ".", "state", ".", "to", "(", "self", ".", "device", ")", ")", ",", "dim", "=", "action", ".", "dim", "(", ")", "-", "1", ")", "\n", "", "else", ":", "\n", "            ", "input", "=", "torch", ".", "cat", "(", "(", "action", ".", "to", "(", "self", ".", "device", ")", ",", "state", ".", "to", "(", "self", ".", "device", ")", ")", ",", "dim", "=", "action", ".", "dim", "(", ")", "-", "1", ")", "\n", "\n", "", "next_state", "=", "self", ".", "state_net", "(", "input", ")", "\n", "reward", "=", "self", ".", "reward_net", "(", "input", ")", "\n", "done", "=", "self", ".", "done_net", "(", "input", ")", "\n", "self", ".", "state", "=", "next_state", "\n", "\n", "return", "next_state", ",", "reward", ",", "done", "\n", "", "", ""]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.__init__": [[11, 22], ["env_factory.EnvFactory.generate_real_env", "env_factory.EnvFactory.get_state_dim", "env_factory.EnvFactory.get_action_dim"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_real_env", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.get_state_dim", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_wrapper.EnvWrapper.get_action_dim"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "self", ".", "env_name", "=", "config", "[", "\"env_name\"", "]", "\n", "self", ".", "device", "=", "config", "[", "\"device\"", "]", "\n", "self", ".", "env_config", "=", "config", "[", "\"envs\"", "]", "[", "self", ".", "env_name", "]", "\n", "\n", "# for virtual env", "\n", "dummy_env", "=", "self", ".", "generate_real_env", "(", "print_str", "=", "'EnvFactory (dummy_env): '", ")", "\n", "self", ".", "state_dim", "=", "dummy_env", ".", "get_state_dim", "(", ")", "\n", "self", ".", "action_dim", "=", "dummy_env", ".", "get_action_dim", "(", ")", "\n", "self", ".", "observation_space", "=", "dummy_env", ".", "env", ".", "observation_space", "\n", "self", ".", "action_space", "=", "dummy_env", ".", "env", ".", "action_space", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_real_env": [[23, 29], ["env_factory.EnvFactory._get_default_parameters", "env_factory.EnvFactory._generate_real_env_with_kwargs", "envs.env_wrapper.EnvWrapper"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory._get_default_parameters", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory._generate_real_env_with_kwargs"], ["", "def", "generate_real_env", "(", "self", ",", "print_str", "=", "''", ")", ":", "\n", "# generate a real environment with default parameters", "\n", "        ", "kwargs", "=", "self", ".", "_get_default_parameters", "(", "virtual_env", "=", "False", ")", "\n", "# print(print_str + 'Generating default real environment \"{}\" with parameters {}'.format(self.env_name, kwargs))", "\n", "env", "=", "self", ".", "_generate_real_env_with_kwargs", "(", "kwargs", "=", "kwargs", ",", "env_name", "=", "self", ".", "env_name", ")", "\n", "return", "EnvWrapper", "(", "env", "=", "env", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_virtual_env": [[30, 36], ["env_factory.EnvFactory._get_default_parameters", "envs.virtual_env.VirtualEnv", "envs.env_wrapper.EnvWrapper().to", "envs.env_wrapper.EnvWrapper"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory._get_default_parameters"], ["", "def", "generate_virtual_env", "(", "self", ",", "print_str", "=", "''", ")", ":", "\n", "# generate a virtual environment with default parameters", "\n", "        ", "kwargs", "=", "self", ".", "_get_default_parameters", "(", "virtual_env", "=", "True", ")", "\n", "# print(print_str + 'Generating virtual environment \"{}\" with parameters {}'.format(self.env_name, kwargs))", "\n", "env", "=", "VirtualEnv", "(", "kwargs", ")", "\n", "return", "EnvWrapper", "(", "env", "=", "env", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_reward_env": [[37, 44], ["env_factory.EnvFactory._get_default_parameters", "env_factory.EnvFactory._generate_real_env_with_kwargs", "envs.reward_env.RewardEnv", "envs.env_wrapper.EnvWrapper().to", "envs.env_wrapper.EnvWrapper"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory._get_default_parameters", "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory._generate_real_env_with_kwargs"], ["", "def", "generate_reward_env", "(", "self", ",", "print_str", "=", "''", ")", ":", "\n", "# generate a virtual environment with default parameters", "\n", "        ", "kwargs", "=", "self", ".", "_get_default_parameters", "(", "virtual_env", "=", "True", ")", "\n", "# print(print_str + 'Generating reward environment \"{}\" with parameters {}'.format(self.env_name, kwargs))", "\n", "real_env", "=", "self", ".", "_generate_real_env_with_kwargs", "(", "kwargs", "=", "kwargs", ",", "env_name", "=", "self", ".", "env_name", ")", "\n", "reward_env", "=", "RewardEnv", "(", "real_env", "=", "real_env", ",", "kwargs", "=", "kwargs", ")", "\n", "return", "EnvWrapper", "(", "env", "=", "reward_env", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory._get_default_parameters": [[45, 60], ["env_factory.EnvFactory.env_config.items", "env_factory.EnvFactory.generate_real_env", "isinstance", "float"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory.generate_real_env"], ["", "def", "_get_default_parameters", "(", "self", ",", "virtual_env", ")", ":", "\n", "        ", "kwargs", "=", "{", "\"env_name\"", ":", "self", ".", "env_name", ",", "\n", "\"device\"", ":", "self", ".", "device", "}", "\n", "if", "virtual_env", ":", "\n", "            ", "kwargs", "[", "\"state_dim\"", "]", "=", "self", ".", "state_dim", "\n", "kwargs", "[", "\"action_dim\"", "]", "=", "self", ".", "action_dim", "\n", "kwargs", "[", "\"observation_space\"", "]", "=", "self", ".", "observation_space", "\n", "kwargs", "[", "\"action_space\"", "]", "=", "self", ".", "action_space", "\n", "kwargs", "[", "\"reset_env\"", "]", "=", "self", ".", "generate_real_env", "(", ")", "\n", "", "for", "key", ",", "value", "in", "self", ".", "env_config", ".", "items", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "value", ",", "list", ")", ":", "\n", "                ", "kwargs", "[", "key", "]", "=", "float", "(", "value", "[", "1", "]", ")", "\n", "", "else", ":", "\n", "                ", "kwargs", "[", "key", "]", "=", "value", "\n", "", "", "return", "kwargs", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.env_factory.EnvFactory._generate_real_env_with_kwargs": [[61, 94], ["kwargs.items", "int", "gym.wrappers.TimeLimit", "setattr", "BanditFixedPermutedGaussian", "gym.wrappers.TimeLimit", "EmptyRoom22", "gym.wrappers.TimeLimit", "EmptyRoom23", "gym.wrappers.TimeLimit", "EmptyRoom33", "gym.wrappers.TimeLimit", "WallRoom", "gym.wrappers.TimeLimit", "HoleRoom", "gym.wrappers.TimeLimit", "HoleRoomLarge", "gym.wrappers.TimeLimit", "HoleRoomLargeShifted", "gym.wrappers.TimeLimit", "gym.make", "Cliff"], "methods", ["None"], ["", "def", "_generate_real_env_with_kwargs", "(", "self", ",", "kwargs", ",", "env_name", ")", ":", "\n", "# generate environment class", "\n", "# todo: make generic (e.g. check if class is existent in bandit.py or gridworld.py)", "\n", "        ", "if", "env_name", "==", "\"Bandit\"", ":", "\n", "            ", "env", "=", "TimeLimit", "(", "BanditFixedPermutedGaussian", "(", ")", ")", "\n", "", "elif", "env_name", "==", "\"EmptyRoom22\"", ":", "\n", "            ", "env", "=", "TimeLimit", "(", "EmptyRoom22", "(", ")", ")", "\n", "", "elif", "env_name", "==", "\"EmptyRoom23\"", ":", "\n", "            ", "env", "=", "TimeLimit", "(", "EmptyRoom23", "(", ")", ")", "\n", "", "elif", "env_name", "==", "\"EmptyRoom33\"", ":", "\n", "            ", "env", "=", "TimeLimit", "(", "EmptyRoom33", "(", ")", ")", "\n", "", "elif", "env_name", "==", "\"WallRoom\"", ":", "\n", "            ", "env", "=", "TimeLimit", "(", "WallRoom", "(", ")", ")", "\n", "", "elif", "env_name", "==", "\"HoleRoom\"", ":", "\n", "            ", "env", "=", "TimeLimit", "(", "HoleRoom", "(", ")", ")", "\n", "", "elif", "env_name", "==", "\"HoleRoomLarge\"", ":", "\n", "            ", "env", "=", "TimeLimit", "(", "HoleRoomLarge", "(", ")", ")", "\n", "", "elif", "env_name", "==", "\"HoleRoomLargeShifted\"", ":", "\n", "            ", "env", "=", "TimeLimit", "(", "HoleRoomLargeShifted", "(", ")", ")", "\n", "", "elif", "env_name", "==", "\"Cliff\"", ":", "\n", "            ", "env", "=", "TimeLimit", "(", "Cliff", "(", ")", ")", "\n", "", "else", ":", "\n", "            ", "env", "=", "gym", ".", "make", "(", "env_name", ")", "\n", "\n", "", "for", "key", ",", "value", "in", "kwargs", ".", "items", "(", ")", ":", "\n", "            ", "setattr", "(", "env", ",", "key", ",", "value", ")", "\n", "\n", "# for episode termination", "\n", "", "env", ".", "_max_episode_steps", "=", "int", "(", "kwargs", "[", "\"max_steps\"", "]", ")", "\n", "# for model save/load", "\n", "env", ".", "kwargs", "=", "kwargs", "\n", "\n", "return", "env", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.gridworld.GridworldEnv.__init__": [[25, 33], ["len", "len", "gym.spaces.Discrete", "gym.spaces.Discrete", "gridworld.GridworldEnv._seed"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditEnv._seed"], ["    ", "def", "__init__", "(", "self", ",", "grid", ")", ":", "\n", "        ", "m", "=", "len", "(", "grid", ")", "\n", "n", "=", "len", "(", "grid", "[", "0", "]", ")", "\n", "self", ".", "grid", "=", "grid", "\n", "self", ".", "state", "=", "None", "\n", "self", ".", "action_space", "=", "spaces", ".", "Discrete", "(", "4", ")", "\n", "self", ".", "observation_space", "=", "spaces", ".", "Discrete", "(", "m", "*", "n", ")", "\n", "self", ".", "_seed", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.gridworld.GridworldEnv._seed": [[34, 37], ["gym.utils.seeding.np_random"], "methods", ["None"], ["", "def", "_seed", "(", "self", ",", "seed", "=", "None", ")", ":", "\n", "        ", "self", ".", "np_random", ",", "seed", "=", "seeding", ".", "np_random", "(", "seed", ")", "\n", "return", "[", "seed", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.gridworld.GridworldEnv.step": [[38, 46], ["gridworld.GridworldEnv.action_space.contains", "gridworld.GridworldEnv._calc_next_state", "gridworld.GridworldEnv._calc_reward", "gridworld.GridworldEnv._calc_done", "gridworld.GridworldEnv._state_to_obs"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.gridworld.GridworldEnv._calc_next_state", "home.repos.pwc.inspect_result.automl_learning_environments.envs.gridworld.GridworldEnv._calc_reward", "home.repos.pwc.inspect_result.automl_learning_environments.envs.gridworld.GridworldEnv._calc_done", "home.repos.pwc.inspect_result.automl_learning_environments.envs.gridworld.GridworldEnv._state_to_obs"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "assert", "self", ".", "action_space", ".", "contains", "(", "action", ")", "\n", "\n", "self", ".", "state", "=", "self", ".", "_calc_next_state", "(", "action", ")", "\n", "reward", "=", "self", ".", "_calc_reward", "(", ")", "\n", "done", "=", "self", ".", "_calc_done", "(", ")", "\n", "obs", "=", "self", ".", "_state_to_obs", "(", "self", ".", "state", ")", "\n", "return", "obs", ",", "reward", ",", "done", ",", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.gridworld.GridworldEnv.reset": [[47, 59], ["len", "len", "range", "ValueError", "range", "gridworld.GridworldEnv._state_to_obs"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.gridworld.GridworldEnv._state_to_obs"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "m", "=", "len", "(", "self", ".", "grid", ")", "\n", "n", "=", "len", "(", "self", ".", "grid", "[", "0", "]", ")", "\n", "\n", "# find start state", "\n", "for", "x", "in", "range", "(", "m", ")", ":", "\n", "            ", "for", "y", "in", "range", "(", "n", ")", ":", "\n", "                ", "if", "self", ".", "grid", "[", "x", "]", "[", "y", "]", "==", "'S'", ":", "\n", "                    ", "self", ".", "state", "=", "(", "x", ",", "y", ")", "\n", "return", "self", ".", "_state_to_obs", "(", "self", ".", "state", ")", "\n", "\n", "", "", "", "raise", "ValueError", "(", "\"No start state found\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.gridworld.GridworldEnv.render": [[60, 62], ["None"], "methods", ["None"], ["", "def", "render", "(", "self", ",", "mode", "=", "'human'", ",", "close", "=", "False", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.gridworld.GridworldEnv._calc_next_state": [[63, 93], ["len", "len", "min", "min", "max", "max", "ValueError", "str"], "methods", ["None"], ["", "def", "_calc_next_state", "(", "self", ",", "action", ")", ":", "\n", "        ", "m", "=", "len", "(", "self", ".", "grid", ")", "\n", "n", "=", "len", "(", "self", ".", "grid", "[", "0", "]", ")", "\n", "x", ",", "y", "=", "self", ".", "state", "\n", "\n", "# hole -> stuck", "\n", "if", "self", ".", "grid", "[", "x", "]", "[", "y", "]", "==", "'O'", ":", "\n", "            ", "return", "x", ",", "y", "\n", "\n", "# calculate movement direction", "\n", "", "if", "action", "==", "G_LEFT", ":", "\n", "            ", "x_n", ",", "y_n", "=", "x", ",", "y", "-", "1", "\n", "", "elif", "action", "==", "G_UP", ":", "\n", "            ", "x_n", ",", "y_n", "=", "x", "-", "1", ",", "y", "\n", "", "elif", "action", "==", "G_RIGHT", ":", "\n", "            ", "x_n", ",", "y_n", "=", "x", ",", "y", "+", "1", "\n", "", "elif", "action", "==", "G_DOWN", ":", "\n", "            ", "x_n", ",", "y_n", "=", "x", "+", "1", ",", "y", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "'Unknown action: '", "+", "str", "(", "action", ")", ")", "\n", "\n", "# movement limited by map boundaries", "\n", "", "x_n", "=", "min", "(", "max", "(", "x_n", ",", "0", ")", ",", "m", "-", "1", ")", "\n", "y_n", "=", "min", "(", "max", "(", "y_n", ",", "0", ")", ",", "n", "-", "1", ")", "\n", "\n", "# movement limited by wall boundaries", "\n", "if", "self", ".", "grid", "[", "x_n", "]", "[", "y_n", "]", "==", "'#'", ":", "\n", "            ", "x_n", ",", "y_n", "=", "x", ",", "y", "\n", "\n", "", "return", "x_n", ",", "y_n", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.gridworld.GridworldEnv._calc_reward": [[95, 103], ["None"], "methods", ["None"], ["", "def", "_calc_reward", "(", "self", ")", ":", "\n", "        ", "x", ",", "y", "=", "self", ".", "state", "\n", "if", "self", ".", "grid", "[", "x", "]", "[", "y", "]", "==", "'G'", ":", "\n", "            ", "return", "self", ".", "g_reward", "\n", "", "elif", "self", ".", "grid", "[", "x", "]", "[", "y", "]", "==", "'O'", ":", "\n", "            ", "return", "self", ".", "o_reward", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "step_cost", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.gridworld.GridworldEnv._calc_done": [[105, 111], ["None"], "methods", ["None"], ["", "", "def", "_calc_done", "(", "self", ")", ":", "\n", "        ", "x", ",", "y", "=", "self", ".", "state", "\n", "if", "self", ".", "grid", "[", "x", "]", "[", "y", "]", "in", "(", "'O'", ",", "'G'", ")", ":", "\n", "            ", "return", "True", "\n", "", "else", ":", "\n", "            ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.gridworld.GridworldEnv._obs_to_state": [[113, 118], ["len"], "methods", ["None"], ["", "", "def", "_obs_to_state", "(", "self", ",", "obs", ")", ":", "\n", "        ", "n", "=", "len", "(", "self", ".", "grid", "[", "0", "]", ")", "\n", "x", "=", "obs", "//", "n", "\n", "y", "=", "obs", "%", "n", "\n", "return", "x", ",", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.gridworld.GridworldEnv._state_to_obs": [[120, 125], ["len"], "methods", ["None"], ["", "def", "_state_to_obs", "(", "self", ",", "state", ")", ":", "\n", "        ", "n", "=", "len", "(", "self", ".", "grid", "[", "0", "]", ")", "\n", "x", ",", "y", "=", "state", "\n", "obs", "=", "x", "*", "n", "+", "y", "\n", "return", "obs", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.gridworld.EmptyRoom22.__init__": [[128, 135], ["gridworld.GridworldEnv.__init__"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "step_cost", "=", "-", "0.01", "\n", "self", ".", "g_reward", "=", "1", "\n", "self", ".", "o_reward", "=", "-", "1", "\n", "grid", "=", "[", "[", "'S'", ",", "' '", "]", ",", "\n", "[", "' '", ",", "'G'", "]", "]", "\n", "GridworldEnv", ".", "__init__", "(", "self", ",", "grid", "=", "grid", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.gridworld.EmptyRoom23.__init__": [[138, 145], ["gridworld.GridworldEnv.__init__"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "step_cost", "=", "-", "0.01", "\n", "self", ".", "g_reward", "=", "1", "\n", "self", ".", "o_reward", "=", "-", "1", "\n", "grid", "=", "[", "[", "'S'", ",", "' '", ",", "' '", "]", ",", "\n", "[", "' '", ",", "' '", ",", "'G'", "]", "]", "\n", "GridworldEnv", ".", "__init__", "(", "self", ",", "grid", "=", "grid", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.gridworld.EmptyRoom33.__init__": [[148, 156], ["gridworld.GridworldEnv.__init__"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "step_cost", "=", "-", "0.01", "\n", "self", ".", "g_reward", "=", "1", "\n", "self", ".", "o_reward", "=", "-", "1", "\n", "grid", "=", "[", "[", "'S'", ",", "' '", ",", "' '", "]", ",", "\n", "[", "' '", ",", "' '", ",", "' '", "]", ",", "\n", "[", "' '", ",", "' '", ",", "'G'", "]", "]", "\n", "GridworldEnv", ".", "__init__", "(", "self", ",", "grid", "=", "grid", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.gridworld.EmptyRoom.__init__": [[159, 170], ["gridworld.GridworldEnv.__init__"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "step_cost", "=", "-", "0.01", "\n", "self", ".", "g_reward", "=", "1", "\n", "self", ".", "o_reward", "=", "-", "1", "\n", "grid", "=", "[", "[", "'S'", ",", "' '", ",", "' '", ",", "' '", ",", "' '", "]", ",", "\n", "[", "' '", ",", "' '", ",", "' '", ",", "' '", ",", "' '", "]", ",", "\n", "[", "' '", ",", "' '", ",", "' '", ",", "' '", ",", "' '", "]", ",", "\n", "[", "' '", ",", "' '", ",", "' '", ",", "' '", ",", "' '", "]", ",", "\n", "[", "' '", ",", "' '", ",", "' '", ",", "' '", ",", "'G'", "]", "]", "\n", "\n", "GridworldEnv", ".", "__init__", "(", "self", ",", "grid", "=", "grid", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.gridworld.WallRoom.__init__": [[173, 184], ["gridworld.GridworldEnv.__init__"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "step_cost", "=", "-", "0.01", "\n", "self", ".", "g_reward", "=", "1", "\n", "self", ".", "o_reward", "=", "-", "1", "\n", "grid", "=", "[", "[", "'S'", ",", "' '", ",", "'#'", ",", "' '", ",", "' '", "]", ",", "\n", "[", "' '", ",", "' '", ",", "'#'", ",", "' '", ",", "' '", "]", ",", "\n", "[", "' '", ",", "' '", ",", "' '", ",", "' '", ",", "' '", "]", ",", "\n", "[", "' '", ",", "' '", ",", "'#'", ",", "' '", ",", "' '", "]", ",", "\n", "[", "' '", ",", "' '", ",", "'#'", ",", "' '", ",", "'G'", "]", "]", "\n", "\n", "GridworldEnv", ".", "__init__", "(", "self", ",", "grid", "=", "grid", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.gridworld.HoleRoom.__init__": [[187, 198], ["gridworld.GridworldEnv.__init__"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "step_cost", "=", "-", "0.01", "\n", "self", ".", "g_reward", "=", "1", "\n", "self", ".", "o_reward", "=", "-", "1", "\n", "grid", "=", "[", "[", "'S'", ",", "' '", ",", "'O'", ",", "' '", ",", "' '", "]", ",", "\n", "[", "' '", ",", "' '", ",", "'O'", ",", "' '", ",", "' '", "]", ",", "\n", "[", "' '", ",", "' '", ",", "' '", ",", "' '", ",", "' '", "]", ",", "\n", "[", "' '", ",", "' '", ",", "'O'", ",", "' '", ",", "' '", "]", ",", "\n", "[", "' '", ",", "' '", ",", "'O'", ",", "' '", ",", "'G'", "]", "]", "\n", "\n", "GridworldEnv", ".", "__init__", "(", "self", ",", "grid", "=", "grid", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.gridworld.HoleRoomLarge.__init__": [[201, 212], ["gridworld.GridworldEnv.__init__"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "step_cost", "=", "-", "0.01", "\n", "self", ".", "g_reward", "=", "1", "\n", "self", ".", "o_reward", "=", "-", "1", "\n", "grid", "=", "[", "[", "'S'", ",", "' '", ",", "'O'", ",", "' '", ",", "'O'", ",", "' '", ",", "' '", "]", ",", "\n", "[", "' '", ",", "' '", ",", "'O'", ",", "' '", ",", "'O'", ",", "' '", ",", "' '", "]", ",", "\n", "[", "' '", ",", "' '", ",", "' '", ",", "' '", ",", "' '", ",", "' '", ",", "' '", "]", ",", "\n", "[", "' '", ",", "' '", ",", "'O'", ",", "' '", ",", "'O'", ",", "' '", ",", "' '", "]", ",", "\n", "[", "' '", ",", "' '", ",", "'O'", ",", "' '", ",", "'O'", ",", "' '", ",", "'G'", "]", "]", "\n", "\n", "GridworldEnv", ".", "__init__", "(", "self", ",", "grid", "=", "grid", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.gridworld.HoleRoomLargeShifted.__init__": [[215, 226], ["gridworld.GridworldEnv.__init__"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "step_cost", "=", "-", "0.01", "\n", "self", ".", "g_reward", "=", "1", "\n", "self", ".", "o_reward", "=", "-", "1", "\n", "grid", "=", "[", "[", "'S'", ",", "' '", ",", "'O'", ",", "' '", ",", "' '", ",", "' '", ",", "' '", "]", ",", "\n", "[", "' '", ",", "' '", ",", "'O'", ",", "' '", ",", "'O'", ",", "' '", ",", "' '", "]", ",", "\n", "[", "' '", ",", "' '", ",", "'O'", ",", "' '", ",", "'O'", ",", "' '", ",", "' '", "]", ",", "\n", "[", "' '", ",", "' '", ",", "'O'", ",", "' '", ",", "'O'", ",", "' '", ",", "' '", "]", ",", "\n", "[", "' '", ",", "' '", ",", "' '", ",", "' '", ",", "'O'", ",", "' '", ",", "'G'", "]", "]", "\n", "\n", "GridworldEnv", ".", "__init__", "(", "self", ",", "grid", "=", "grid", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.gridworld.Cliff.__init__": [[229, 239], ["gridworld.GridworldEnv.__init__"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "step_cost", "=", "-", "1", "\n", "self", ".", "g_reward", "=", "0", "\n", "self", ".", "o_reward", "=", "-", "100", "\n", "grid", "=", "[", "[", "' '", ",", "' '", ",", "' '", ",", "' '", ",", "' '", ",", "' '", ",", "' '", ",", "' '", ",", "' '", ",", "' '", ",", "' '", ",", "' '", "]", ",", "\n", "[", "' '", ",", "' '", ",", "' '", ",", "' '", ",", "' '", ",", "' '", ",", "' '", ",", "' '", ",", "' '", ",", "' '", ",", "' '", ",", "' '", "]", ",", "\n", "[", "' '", ",", "' '", ",", "' '", ",", "' '", ",", "' '", ",", "' '", ",", "' '", ",", "' '", ",", "' '", ",", "' '", ",", "' '", ",", "' '", "]", ",", "\n", "[", "'S'", ",", "'O'", ",", "'O'", ",", "'O'", ",", "'O'", ",", "'O'", ",", "'O'", ",", "'O'", ",", "'O'", ",", "'O'", ",", "'O'", ",", "'G'", "]", "]", "\n", "\n", "GridworldEnv", ".", "__init__", "(", "self", ",", "grid", "=", "grid", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditEnv.__init__": [[19, 38], ["len", "gym.spaces.Discrete", "gym.spaces.Discrete", "bandit.BanditEnv._seed", "len", "len", "ValueError", "ValueError", "min", "max", "isinstance", "ValueError"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditEnv._seed"], ["def", "__init__", "(", "self", ",", "p_dist", ",", "r_dist", ")", ":", "\n", "        ", "if", "len", "(", "p_dist", ")", "!=", "len", "(", "r_dist", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"Probability and Reward distribution must be the same length\"", ")", "\n", "\n", "", "if", "min", "(", "p_dist", ")", "<", "0", "or", "max", "(", "p_dist", ")", ">", "1", ":", "\n", "            ", "raise", "ValueError", "(", "\"All probabilities must be between 0 and 1\"", ")", "\n", "\n", "", "for", "reward", "in", "r_dist", ":", "\n", "            ", "if", "isinstance", "(", "reward", ",", "list", ")", "and", "reward", "[", "1", "]", "<=", "0", ":", "\n", "                ", "raise", "ValueError", "(", "\"Standard deviation in rewards must all be greater than 0\"", ")", "\n", "\n", "", "", "self", ".", "p_dist", "=", "p_dist", "\n", "self", ".", "r_dist", "=", "r_dist", "\n", "\n", "self", ".", "n_bandits", "=", "len", "(", "p_dist", ")", "\n", "self", ".", "action_space", "=", "spaces", ".", "Discrete", "(", "self", ".", "n_bandits", ")", "\n", "self", ".", "observation_space", "=", "spaces", ".", "Discrete", "(", "1", ")", "\n", "\n", "self", ".", "_seed", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditEnv._seed": [[39, 42], ["gym.utils.seeding.np_random"], "methods", ["None"], ["", "def", "_seed", "(", "self", ",", "seed", "=", "None", ")", ":", "\n", "        ", "self", ".", "np_random", ",", "seed", "=", "seeding", ".", "np_random", "(", "seed", ")", "\n", "return", "[", "seed", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditEnv.step": [[43, 56], ["bandit.BanditEnv.action_space.contains", "numpy.random.uniform", "isinstance", "numpy.random.normal"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cartpole_transfer_algo.normal"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "assert", "self", ".", "action_space", ".", "contains", "(", "action", ")", "\n", "\n", "reward", "=", "0", "\n", "done", "=", "False", "\n", "\n", "if", "np", ".", "random", ".", "uniform", "(", ")", "<", "self", ".", "p_dist", "[", "action", "]", ":", "\n", "            ", "if", "not", "isinstance", "(", "self", ".", "r_dist", "[", "action", "]", ",", "list", ")", ":", "\n", "                ", "reward", "=", "self", ".", "r_dist", "[", "action", "]", "\n", "", "else", ":", "\n", "                ", "reward", "=", "np", ".", "random", ".", "normal", "(", "self", ".", "r_dist", "[", "action", "]", "[", "0", "]", ",", "self", ".", "r_dist", "[", "action", "]", "[", "1", "]", ")", "\n", "\n", "", "", "return", "0", ",", "reward", ",", "done", ",", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditEnv.reset": [[57, 59], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditEnv.render": [[60, 62], ["None"], "methods", ["None"], ["", "def", "render", "(", "self", ",", "mode", "=", "'human'", ",", "close", "=", "False", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditTwoArmedDeterministicFixed.__init__": [[67, 69], ["bandit.BanditEnv.__init__"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "BanditEnv", ".", "__init__", "(", "self", ",", "p_dist", "=", "[", "1", ",", "0", "]", ",", "r_dist", "=", "[", "1", ",", "1", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditTwoArmedHighLowFixed.__init__": [[74, 76], ["bandit.BanditEnv.__init__"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "BanditEnv", ".", "__init__", "(", "self", ",", "p_dist", "=", "[", "0.8", ",", "0.2", "]", ",", "r_dist", "=", "[", "1", ",", "1", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditTwoArmedHighHighFixed.__init__": [[81, 83], ["bandit.BanditEnv.__init__"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "BanditEnv", ".", "__init__", "(", "self", ",", "p_dist", "=", "[", "0.8", ",", "0.9", "]", ",", "r_dist", "=", "[", "1", ",", "1", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditTwoArmedLowLowFixed.__init__": [[88, 90], ["bandit.BanditEnv.__init__"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "BanditEnv", ".", "__init__", "(", "self", ",", "p_dist", "=", "[", "0.1", ",", "0.2", "]", ",", "r_dist", "=", "[", "1", ",", "1", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditTenArmedRandomFixed.__init__": [[95, 99], ["numpy.random.uniform", "numpy.full", "bandit.BanditEnv.__init__"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__"], ["def", "__init__", "(", "self", ",", "bandits", "=", "10", ")", ":", "\n", "        ", "p_dist", "=", "np", ".", "random", ".", "uniform", "(", "size", "=", "bandits", ")", "\n", "r_dist", "=", "np", ".", "full", "(", "bandits", ",", "1", ")", "\n", "BanditEnv", ".", "__init__", "(", "self", ",", "p_dist", "=", "p_dist", ",", "r_dist", "=", "r_dist", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditTenArmedUniformDistributedReward.__init__": [[104, 108], ["numpy.full", "numpy.random.uniform", "bandit.BanditEnv.__init__"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__"], ["def", "__init__", "(", "self", ",", "bandits", "=", "10", ")", ":", "\n", "        ", "p_dist", "=", "np", ".", "full", "(", "bandits", ",", "1", ")", "\n", "r_dist", "=", "np", ".", "random", ".", "uniform", "(", "size", "=", "bandits", ")", "\n", "BanditEnv", ".", "__init__", "(", "self", ",", "p_dist", "=", "p_dist", ",", "r_dist", "=", "r_dist", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditTenArmedRandomRandom.__init__": [[113, 117], ["numpy.random.uniform", "numpy.random.uniform", "bandit.BanditEnv.__init__"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__"], ["def", "__init__", "(", "self", ",", "bandits", "=", "10", ")", ":", "\n", "        ", "p_dist", "=", "np", ".", "random", ".", "uniform", "(", "size", "=", "bandits", ")", "\n", "r_dist", "=", "np", ".", "random", ".", "uniform", "(", "size", "=", "bandits", ")", "\n", "BanditEnv", ".", "__init__", "(", "self", ",", "p_dist", "=", "p_dist", ",", "r_dist", "=", "r_dist", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditTenArmedGaussian.__init__": [[128, 136], ["numpy.full", "range", "bandit.BanditEnv.__init__", "r_dist.append", "numpy.random.normal"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_visualize_cartpole_transfer_algo.normal"], ["def", "__init__", "(", "self", ",", "bandits", "=", "20", ")", ":", "\n", "        ", "p_dist", "=", "np", ".", "full", "(", "bandits", ",", "1", ")", "\n", "r_dist", "=", "[", "]", "\n", "\n", "for", "_", "in", "range", "(", "bandits", ")", ":", "\n", "            ", "r_dist", ".", "append", "(", "[", "np", ".", "random", ".", "normal", "(", "0", ",", "1", ")", ",", "1", "]", ")", "\n", "\n", "", "BanditEnv", ".", "__init__", "(", "self", ",", "p_dist", "=", "p_dist", ",", "r_dist", "=", "r_dist", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditRandomPermutedGaussian.__init__": [[143, 152], ["numpy.full", "range", "random.shuffle", "bandit.BanditEnv.__init__", "r_dist.append"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__"], ["def", "__init__", "(", "self", ",", "bandits", "=", "20", ")", ":", "\n", "        ", "p_dist", "=", "np", ".", "full", "(", "bandits", ",", "1", ")", "\n", "r_dist", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "bandits", ")", ":", "\n", "            ", "r_dist", ".", "append", "(", "[", "(", "i", "+", "1", ")", "/", "bandits", ",", "1", "]", ")", "\n", "", "random", ".", "shuffle", "(", "r_dist", ")", "\n", "\n", "BanditEnv", ".", "__init__", "(", "self", ",", "p_dist", "=", "p_dist", ",", "r_dist", "=", "r_dist", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditFixedPermutedGaussian.__init__": [[159, 168], ["numpy.full", "range", "random.Random().shuffle", "bandit.BanditEnv.__init__", "r_dist.append", "random.Random"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__"], ["def", "__init__", "(", "self", ",", "bandits", "=", "20", ")", ":", "\n", "        ", "p_dist", "=", "np", ".", "full", "(", "bandits", ",", "1", ")", "\n", "r_dist", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "bandits", ")", ":", "\n", "            ", "r_dist", ".", "append", "(", "[", "(", "i", "+", "1", ")", "/", "bandits", ",", "1", "]", ")", "\n", "", "random", ".", "Random", "(", "0", ")", ".", "shuffle", "(", "r_dist", ")", "\n", "\n", "BanditEnv", ".", "__init__", "(", "self", ",", "p_dist", "=", "p_dist", ",", "r_dist", "=", "r_dist", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.automl_learning_environments.models.model_utils.build_nn_from_config": [[4, 40], ["torch.Identity", "modules.append", "modules.append", "range", "modules.append", "torch.Sequential", "torch.PReLU", "torch.Linear", "modules.append", "modules.append", "modules.append", "torch.Linear", "torch.ReLU", "torch.LayerNorm", "torch.Linear", "torch.LeakyReLU", "torch.Tanh", "torch.Identity", "print"], "function", ["None"], ["def", "build_nn_from_config", "(", "input_dim", ",", "output_dim", ",", "nn_config", ")", ":", "\n", "    ", "hidden_size", "=", "nn_config", "[", "'hidden_size'", "]", "\n", "hidden_layer", "=", "nn_config", "[", "'hidden_layer'", "]", "\n", "activation_fn", "=", "nn_config", "[", "'activation_fn'", "]", "\n", "\n", "if", "activation_fn", "==", "'prelu'", ":", "\n", "        ", "act_fn", "=", "nn", ".", "PReLU", "(", ")", "\n", "", "elif", "activation_fn", "==", "'relu'", ":", "\n", "        ", "act_fn", "=", "nn", ".", "ReLU", "(", ")", "\n", "", "elif", "activation_fn", "==", "'leakyrelu'", ":", "\n", "        ", "act_fn", "=", "nn", ".", "LeakyReLU", "(", ")", "\n", "", "elif", "activation_fn", "==", "'tanh'", ":", "\n", "        ", "act_fn", "=", "nn", ".", "Tanh", "(", ")", "\n", "", "elif", "activation_fn", "==", "'identity'", ":", "\n", "        ", "act_fn", "=", "nn", ".", "Identity", "(", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "'Unknown activation function.'", ")", "\n", "\n", "", "norm", "=", "nn", ".", "Identity", "(", ")", "\n", "\n", "try", ":", "\n", "        ", "use_layer_norm", "=", "nn_config", "[", "\"use_layer_norm\"", "]", "\n", "if", "use_layer_norm", ":", "\n", "            ", "norm", "=", "nn", ".", "LayerNorm", "(", "hidden_size", ")", "\n", "", "", "except", "KeyError", ":", "\n", "        ", "pass", "\n", "\n", "", "modules", "=", "[", "]", "\n", "modules", ".", "append", "(", "nn", ".", "Linear", "(", "input_dim", ",", "hidden_size", ")", ")", "\n", "modules", ".", "append", "(", "act_fn", ")", "\n", "for", "i", "in", "range", "(", "hidden_layer", "-", "1", ")", ":", "\n", "        ", "modules", ".", "append", "(", "nn", ".", "Linear", "(", "hidden_size", ",", "hidden_size", ")", ")", "\n", "modules", ".", "append", "(", "norm", ")", "\n", "modules", ".", "append", "(", "act_fn", ")", "\n", "", "modules", ".", "append", "(", "nn", ".", "Linear", "(", "hidden_size", ",", "output_dim", ")", ")", "\n", "return", "nn", ".", "Sequential", "(", "*", "modules", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICMModel.__init__": [[9, 78], ["torch.Module.__init__", "models.model_utils.build_nn_from_config", "models.model_utils.build_nn_from_config", "models.model_utils.build_nn_from_config", "ResidualBlock", "ResidualBlock", "ResidualBlock", "ResidualBlock", "models.model_utils.build_nn_from_config", "torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "icm_baseline.ICMModel.fc1", "icm_baseline.ICMModel.fc2", "torch.Linear", "torch.Linear", "torch.Linear", "torch.LeakyReLU", "torch.LeakyReLU", "torch.LeakyReLU", "torch.Linear", "torch.Linear", "torch.Linear", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__", "home.repos.pwc.inspect_result.automl_learning_environments.models.model_utils.build_nn_from_config", "home.repos.pwc.inspect_result.automl_learning_environments.models.model_utils.build_nn_from_config", "home.repos.pwc.inspect_result.automl_learning_environments.models.model_utils.build_nn_from_config", "home.repos.pwc.inspect_result.automl_learning_environments.models.model_utils.build_nn_from_config", "home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__"], ["    ", "def", "__init__", "(", "self", ",", "state_dim", ",", "action_dim", ",", "has_discrete_actions", ",", "feature_dim", "=", "64", ",", "hidden_size", "=", "128", ",", ")", ":", "\n", "        ", "super", "(", "ICMModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "state_dim", "=", "state_dim", "\n", "self", ".", "action_dim", "=", "action_dim", "\n", "self", ".", "feature_dim", "=", "feature_dim", "\n", "self", ".", "has_discrete_actions", "=", "has_discrete_actions", "\n", "\n", "nn_features_config", "=", "{", "\n", "'hidden_size'", ":", "hidden_size", ",", "\n", "'hidden_layer'", ":", "2", ",", "\n", "'activation_fn'", ":", "\"leakyrelu\"", "\n", "}", "\n", "nn_inverse_config", "=", "{", "\n", "'hidden_size'", ":", "hidden_size", ",", "\n", "'hidden_layer'", ":", "2", ",", "\n", "'activation_fn'", ":", "\"relu\"", "\n", "}", "\n", "nn_forward_pre_config", "=", "{", "\n", "'hidden_size'", ":", "hidden_size", ",", "\n", "'hidden_layer'", ":", "2", ",", "\n", "'activation_fn'", ":", "\"leakyrelu\"", "\n", "}", "\n", "nn_forward_post_config", "=", "{", "\n", "'hidden_size'", ":", "hidden_size", ",", "\n", "'hidden_layer'", ":", "1", ",", "\n", "'activation_fn'", ":", "\"leakyrelu\"", "\n", "}", "\n", "\n", "if", "self", ".", "has_discrete_actions", "and", "self", ".", "action_dim", "==", "2", ":", "\n", "            ", "action_dim", "=", "1", "\n", "\n", "", "self", ".", "features_model", "=", "build_nn_from_config", "(", "input_dim", "=", "state_dim", ",", "\n", "output_dim", "=", "feature_dim", ",", "\n", "nn_config", "=", "nn_features_config", ")", "\n", "\n", "self", ".", "inverse_model", "=", "build_nn_from_config", "(", "input_dim", "=", "feature_dim", "*", "2", ",", "\n", "output_dim", "=", "action_dim", ",", "\n", "nn_config", "=", "nn_inverse_config", ")", "\n", "\n", "self", ".", "forward_pre_model", "=", "build_nn_from_config", "(", "input_dim", "=", "action_dim", "+", "feature_dim", ",", "\n", "output_dim", "=", "feature_dim", ",", "\n", "nn_config", "=", "nn_forward_pre_config", ")", "\n", "\n", "class", "ResidualBlock", "(", "nn", ".", "Module", ")", ":", "\n", "            ", "def", "__init__", "(", "self", ",", "input_dim", ",", "output_dim", ")", ":", "\n", "                ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "fc1", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "input_dim", ",", "output_dim", ")", ",", "\n", "nn", ".", "LeakyReLU", "(", "inplace", "=", "True", ")", ",", "\n", ")", "\n", "self", ".", "fc2", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "input_dim", ",", "output_dim", ")", "\n", ")", "\n", "\n", "", "def", "forward", "(", "self", ",", "feature", ",", "action", ")", ":", "\n", "                ", "x", "=", "feature", "\n", "x", "=", "self", ".", "fc1", "(", "torch", ".", "cat", "(", "[", "x", ",", "action", "]", ",", "dim", "=", "1", ")", ")", "\n", "x", "=", "self", ".", "fc2", "(", "torch", ".", "cat", "(", "[", "x", ",", "action", "]", ",", "dim", "=", "1", ")", ")", "\n", "return", "feature", "+", "x", "\n", "\n", "# original implementation uses residual blocks:", "\n", "# https://github.com/openai/large-scale-curiosity/blob/master/dynamics.py#L55-L61", "\n", "", "", "self", ".", "residual_block1", "=", "ResidualBlock", "(", "input_dim", "=", "action_dim", "+", "feature_dim", ",", "output_dim", "=", "feature_dim", ")", "\n", "self", ".", "residual_block2", "=", "ResidualBlock", "(", "input_dim", "=", "action_dim", "+", "feature_dim", ",", "output_dim", "=", "feature_dim", ")", "\n", "self", ".", "residual_block3", "=", "ResidualBlock", "(", "input_dim", "=", "action_dim", "+", "feature_dim", ",", "output_dim", "=", "feature_dim", ")", "\n", "self", ".", "residual_block4", "=", "ResidualBlock", "(", "input_dim", "=", "action_dim", "+", "feature_dim", ",", "output_dim", "=", "feature_dim", ")", "\n", "\n", "self", ".", "forward_post_model", "=", "build_nn_from_config", "(", "input_dim", "=", "feature_dim", ",", "output_dim", "=", "feature_dim", ",", "nn_config", "=", "nn_forward_post_config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICMModel.forward": [[79, 101], ["icm_baseline.ICMModel.features_model", "icm_baseline.ICMModel.features_model", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "icm_baseline.ICMModel.inverse_model", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "icm_baseline.ICMModel.forward_pre_model", "icm_baseline.ICMModel.residual_block1", "icm_baseline.ICMModel.residual_block2", "icm_baseline.ICMModel.residual_block3", "icm_baseline.ICMModel.residual_block4", "icm_baseline.ICMModel.forward_post_model"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "state", ",", "next_state", ",", "action", "=", "input", "\n", "\n", "state_encoded", "=", "self", ".", "features_model", "(", "state", ")", "\n", "next_state_encoded", "=", "self", ".", "features_model", "(", "next_state", ")", "\n", "\n", "# get predicted action from inverse model", "\n", "state_next_state_concat", "=", "torch", ".", "cat", "(", "(", "state_encoded", ",", "next_state_encoded", ")", ",", "1", ")", "\n", "action_pred", "=", "self", ".", "inverse_model", "(", "state_next_state_concat", ")", "\n", "\n", "# get predicted next state encoding from forward model with residual connections", "\n", "forward_model_input", "=", "torch", ".", "cat", "(", "[", "state_encoded", ",", "action", "]", ",", "1", ")", "\n", "x", "=", "self", ".", "forward_pre_model", "(", "forward_model_input", ")", "\n", "\n", "x", "=", "self", ".", "residual_block1", "(", "feature", "=", "x", ",", "action", "=", "action", ")", "\n", "x", "=", "self", ".", "residual_block2", "(", "feature", "=", "x", ",", "action", "=", "action", ")", "\n", "x", "=", "self", ".", "residual_block3", "(", "feature", "=", "x", ",", "action", "=", "action", ")", "\n", "x", "=", "self", ".", "residual_block4", "(", "feature", "=", "x", ",", "action", "=", "action", ")", "\n", "\n", "next_state_pred_encoded", "=", "self", ".", "forward_post_model", "(", "x", ")", "\n", "\n", "return", "next_state_encoded", ",", "next_state_pred_encoded", ",", "action_pred", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.__init__": [[106, 133], ["ICMModel().to", "list", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.MSELoss", "torch.MSELoss", "torch.MSELoss", "icm_baseline.ICM.model.parameters", "torch.BCEWithLogitsLoss", "torch.BCEWithLogitsLoss", "torch.BCEWithLogitsLoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "icm_baseline.ICMModel"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "state_dim", ",", "action_dim", ",", "has_discrete_actions", ",", "feature_dim", "=", "64", ",", "hidden_size", "=", "128", ",", "learning_rate", "=", "1e-4", ",", "\n", "beta", "=", ".2", ",", "eta", "=", ".5", ",", "device", "=", "\"cpu\"", ")", ":", "\n", "\n", "        ", "self", ".", "device", "=", "device", "\n", "self", ".", "action_dim", "=", "action_dim", "\n", "self", ".", "has_discrete_actions", "=", "has_discrete_actions", "\n", "\n", "if", "self", ".", "has_discrete_actions", ":", "\n", "            ", "if", "self", ".", "action_dim", "==", "2", ":", "\n", "                ", "self", ".", "action_criterion", "=", "nn", ".", "BCEWithLogitsLoss", "(", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "action_criterion", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "action_criterion", "=", "nn", ".", "MSELoss", "(", ")", "\n", "\n", "", "self", ".", "model", "=", "ICMModel", "(", "state_dim", "=", "state_dim", ",", "\n", "action_dim", "=", "action_dim", ",", "\n", "has_discrete_actions", "=", "has_discrete_actions", ",", "\n", "feature_dim", "=", "feature_dim", ",", "\n", "hidden_size", "=", "hidden_size", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "self", ".", "beta", "=", "beta", "\n", "self", ".", "lr", "=", "learning_rate", "\n", "self", ".", "eta", "=", "eta", "\n", "\n", "icm_params", "=", "list", "(", "self", ".", "model", ".", "parameters", "(", ")", ")", "\n", "self", ".", "icm_optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "icm_params", ",", "lr", "=", "self", ".", "lr", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.train": [[134, 159], ["icm_baseline.ICM.model", "icm_baseline.ICM.icm_optimizer.zero_grad", "icm_loss.backward", "icm_baseline.ICM.icm_optimizer.step", "actions.to.to.unsqueeze", "actions.to.to.to", "torch.nn.functional.one_hot", "torch.nn.functional.one_hot", "torch.nn.functional.one_hot", "torch.nn.functional.one_hot", "torch.nn.functional.one_hot", "torch.nn.functional.one_hot", "torch.nn.functional.one_hot", "torch.nn.functional.one_hot", "torch.nn.functional.one_hot", "len", "actions.to.to.to", "icm_baseline.ICM.action_criterion", "torch.mse_loss", "torch.mse_loss", "torch.mse_loss"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.envs.bandit.BanditEnv.step"], ["", "def", "train", "(", "self", ",", "states", ",", "next_states", ",", "actions", ")", ":", "\n", "# necessary for 2d action spaces but which are actually 1d (e.g. CartPole)", "\n", "        ", "if", "len", "(", "actions", ".", "shape", ")", "==", "1", "and", "self", ".", "action_dim", "==", "2", ":", "\n", "            ", "actions", "=", "actions", ".", "unsqueeze", "(", "dim", "=", "1", ")", "\n", "\n", "# necessary since cross entropy needs one hot encoding when action space is discrete and action_dim > 2", "\n", "", "if", "self", ".", "has_discrete_actions", "and", "self", ".", "action_dim", ">", "2", ":", "\n", "            ", "actions", "=", "actions", ".", "to", "(", "torch", ".", "long", ")", "\n", "actions_inp", "=", "torch", ".", "nn", ".", "functional", ".", "one_hot", "(", "actions", ".", "to", "(", "torch", ".", "long", ")", ")", "\n", "", "else", ":", "\n", "            ", "actions_inp", "=", "actions", "\n", "\n", "", "next_states_encoded", ",", "next_states_pred_encoded", ",", "actions_pred", "=", "self", ".", "model", "(", "input", "=", "(", "states", ",", "next_states", ",", "actions_inp", ")", ")", "\n", "\n", "# compute ICM loss", "\n", "icm_loss", "=", "(", "1", "-", "self", ".", "beta", ")", "*", "self", ".", "action_criterion", "(", "actions_pred", ",", "actions", ")", "+", "self", ".", "beta", "*", "F", ".", "mse_loss", "(", "next_states_pred_encoded", ",", "\n", "next_states_encoded", ")", "\n", "\n", "# print(\"action loss: \", self.action_criterion(actions_pred, actions).data.cpu().numpy(),", "\n", "#       \"state loss: \", F.mse_loss(next_states_encoded, next_states_pred_encoded).data.cpu().numpy())", "\n", "\n", "# Optimize ICM", "\n", "self", ".", "icm_optimizer", ".", "zero_grad", "(", ")", "\n", "icm_loss", ".", "backward", "(", ")", "\n", "self", ".", "icm_optimizer", ".", "step", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.models.icm_baseline.ICM.compute_intrinsic_rewards": [[160, 173], ["icm_baseline.ICM.model", "intrinsic_reward.detach().unsqueeze", "action.unsqueeze.unsqueeze.unsqueeze", "torch.nn.functional.one_hot", "torch.nn.functional.one_hot", "torch.nn.functional.one_hot", "torch.nn.functional.one_hot", "torch.nn.functional.one_hot", "torch.nn.functional.one_hot", "torch.nn.functional.one_hot", "torch.nn.functional.one_hot", "torch.nn.functional.one_hot", "torch.mse_loss().mean", "torch.mse_loss().mean", "torch.mse_loss().mean", "len", "action.unsqueeze.unsqueeze.to", "intrinsic_reward.detach", "torch.mse_loss", "torch.mse_loss", "torch.mse_loss"], "methods", ["None"], ["", "def", "compute_intrinsic_rewards", "(", "self", ",", "state", ",", "next_state", ",", "action", ")", ":", "\n", "# necessary for 2d action spaces but which are actually 1d (e.g. CartPole)", "\n", "        ", "if", "len", "(", "action", ".", "shape", ")", "==", "1", "and", "self", ".", "action_dim", "==", "2", ":", "\n", "            ", "action", "=", "action", ".", "unsqueeze", "(", "dim", "=", "1", ")", "\n", "\n", "", "if", "self", ".", "has_discrete_actions", "and", "self", ".", "action_dim", "!=", "2", ":", "\n", "            ", "actions_inp", "=", "torch", ".", "nn", ".", "functional", ".", "one_hot", "(", "action", ".", "to", "(", "torch", ".", "long", ")", ")", "\n", "", "else", ":", "\n", "            ", "actions_inp", "=", "action", "\n", "\n", "", "next_states_encoded", ",", "next_states_pred_encoded", ",", "actions_pred", "=", "self", ".", "model", "(", "input", "=", "(", "state", ",", "next_state", ",", "actions_inp", ")", ")", "\n", "intrinsic_reward", "=", "self", ".", "eta", "*", "F", ".", "mse_loss", "(", "next_states_encoded", ",", "next_states_pred_encoded", ",", "reduction", "=", "\"none\"", ")", ".", "mean", "(", "-", "1", ")", "\n", "return", "intrinsic_reward", ".", "detach", "(", ")", ".", "unsqueeze", "(", "dim", "=", "1", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.automl_learning_environments.models.actor_critic.Actor_TD3.__init__": [[12, 17], ["torch.Module.__init__", "models.model_utils.build_nn_from_config"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__", "home.repos.pwc.inspect_result.automl_learning_environments.models.model_utils.build_nn_from_config"], ["    ", "def", "__init__", "(", "self", ",", "state_dim", ",", "action_dim", ",", "max_action", ",", "agent_name", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "net", "=", "build_nn_from_config", "(", "input_dim", "=", "state_dim", ",", "output_dim", "=", "action_dim", ",", "nn_config", "=", "config", "[", "\"agents\"", "]", "[", "agent_name", "]", ")", "\n", "self", ".", "max_action", "=", "max_action", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.models.actor_critic.Actor_TD3.forward": [[18, 20], ["torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "actor_critic.Actor_TD3.net"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "state", ")", ":", "\n", "        ", "return", "torch", ".", "tanh", "(", "self", ".", "net", "(", "state", ")", ")", "*", "self", ".", "max_action", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.models.actor_critic.Actor_TD3_discrete.__init__": [[23, 32], ["torch.Module.__init__", "models.model_utils.build_nn_from_config"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__", "home.repos.pwc.inspect_result.automl_learning_environments.models.model_utils.build_nn_from_config"], ["    ", "def", "__init__", "(", "self", ",", "state_dim", ",", "action_dim", ",", "max_action", ",", "agent_name", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "net", "=", "build_nn_from_config", "(", "input_dim", "=", "state_dim", ",", "output_dim", "=", "action_dim", ",", "nn_config", "=", "config", "[", "\"agents\"", "]", "[", "agent_name", "]", ")", "\n", "self", ".", "max_action", "=", "max_action", "\n", "# gumbel_softmax_temp = config[\"agents\"][agent_name][\"gumbel_softmax_temp\"]", "\n", "# self.gumbel_softmax_temp = torch.nn.Parameter(torch.tensor(gumbel_softmax_temp), requires_grad=True)", "\n", "self", ".", "gumbel_softmax_temp", "=", "config", "[", "\"agents\"", "]", "[", "agent_name", "]", "[", "\"gumbel_softmax_temp\"", "]", "\n", "self", ".", "gumbel_softmax_hard", "=", "config", "[", "\"agents\"", "]", "[", "agent_name", "]", "[", "\"gumbel_softmax_hard\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.models.actor_critic.Actor_TD3_discrete.forward": [[33, 36], ["torch.gumbel_softmax", "torch.gumbel_softmax", "torch.gumbel_softmax", "actor_critic.Actor_TD3_discrete.net"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "state", ",", "tau", ")", ":", "\n", "        ", "action", "=", "self", ".", "net", "(", "state", ")", "*", "self", ".", "max_action", "\n", "return", "F", ".", "gumbel_softmax", "(", "action", ",", "tau", "=", "tau", ",", "hard", "=", "self", ".", "gumbel_softmax_hard", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.models.actor_critic.Actor_PPO.__init__": [[39, 44], ["torch.Module.__init__", "models.model_utils.build_nn_from_config", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__", "home.repos.pwc.inspect_result.automl_learning_environments.models.model_utils.build_nn_from_config"], ["    ", "def", "__init__", "(", "self", ",", "state_dim", ",", "action_dim", ",", "agent_name", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "net", "=", "build_nn_from_config", "(", "input_dim", "=", "state_dim", ",", "output_dim", "=", "action_dim", ",", "nn_config", "=", "config", "[", "\"agents\"", "]", "[", "agent_name", "]", ")", "\n", "self", ".", "action_std", "=", "torch", ".", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "action_dim", ",", "device", "=", "config", "[", "\"device\"", "]", ")", "*", "config", "[", "\"agents\"", "]", "[", "agent_name", "]", "[", "\"action_std\"", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.models.actor_critic.Actor_PPO.forward": [[45, 50], ["torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "actor_critic.Actor_PPO.clamp", "torch.distributions.normal.Normal", "torch.distributions.normal.Normal", "torch.distributions.normal.Normal", "torch.distributions.normal.Normal.rsample", "torch.distributions.normal.Normal.rsample", "torch.distributions.normal.Normal.rsample", "actor_critic.Actor_PPO.net"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.models.actor_critic.Actor_PPO.clamp"], ["", "def", "forward", "(", "self", ",", "state", ")", ":", "\n", "        ", "action_mean", "=", "torch", ".", "tanh", "(", "self", ".", "net", "(", "state", ")", ")", "\n", "self", ".", "clamp", "(", "self", ".", "action_std", ",", "0.001", ")", "\n", "dist", "=", "Normal", "(", "action_mean", ",", "self", ".", "action_std", ")", "\n", "return", "dist", ".", "rsample", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.models.actor_critic.Actor_PPO.evaluate": [[51, 58], ["torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.distributions.normal.Normal", "torch.distributions.normal.Normal", "torch.distributions.normal.Normal", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "actor_critic.Actor_PPO.clamp", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "actor_critic.Actor_PPO.net", "torch.distributions.normal.Normal.log_prob", "torch.distributions.normal.Normal.log_prob", "torch.distributions.normal.Normal.log_prob", "torch.distributions.normal.Normal.entropy", "torch.distributions.normal.Normal.entropy", "torch.distributions.normal.Normal.entropy"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.models.actor_critic.Actor_PPO.clamp"], ["", "def", "evaluate", "(", "self", ",", "state", ",", "action", ")", ":", "\n", "        ", "action_mean", "=", "torch", ".", "tanh", "(", "self", ".", "net", "(", "state", ")", ")", "\n", "dist", "=", "Normal", "(", "action_mean", ",", "self", ".", "action_std", ")", "\n", "action_logprobs", "=", "torch", ".", "sum", "(", "dist", ".", "log_prob", "(", "action", ")", ",", "dim", "=", "1", ")", "\n", "self", ".", "clamp", "(", "self", ".", "action_std", ",", "0.01", ")", "\n", "dist_entropy", "=", "torch", ".", "sum", "(", "dist", ".", "entropy", "(", ")", ",", "dim", "=", "1", ")", "\n", "return", "action_logprobs", ",", "dist_entropy", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.models.actor_critic.Actor_PPO.clamp": [[59, 62], ["range", "len", "max"], "methods", ["None"], ["", "def", "clamp", "(", "self", ",", "action_std", ",", "min_val", ")", ":", "\n", "        ", "for", "i", "in", "range", "(", "len", "(", "action_std", ")", ")", ":", "\n", "            ", "action_std", ".", "data", "[", "i", "]", "=", "max", "(", "action_std", ".", "data", "[", "i", "]", ",", "min_val", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.models.actor_critic.Critic_Q.__init__": [[65, 69], ["torch.Module.__init__", "models.model_utils.build_nn_from_config"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__", "home.repos.pwc.inspect_result.automl_learning_environments.models.model_utils.build_nn_from_config"], ["    ", "def", "__init__", "(", "self", ",", "state_dim", ",", "action_dim", ",", "agent_name", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "net", "=", "build_nn_from_config", "(", "input_dim", "=", "state_dim", "+", "action_dim", ",", "output_dim", "=", "1", ",", "nn_config", "=", "config", "[", "\"agents\"", "]", "[", "agent_name", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.models.actor_critic.Critic_Q.forward": [[70, 72], ["actor_critic.Critic_Q.net", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "len"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "state", ",", "action", ")", ":", "\n", "        ", "return", "self", ".", "net", "(", "torch", ".", "cat", "(", "[", "state", ",", "action", "]", ",", "dim", "=", "len", "(", "state", ".", "shape", ")", "-", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.models.actor_critic.Critic_V.__init__": [[75, 79], ["torch.Module.__init__", "models.model_utils.build_nn_from_config"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__", "home.repos.pwc.inspect_result.automl_learning_environments.models.model_utils.build_nn_from_config"], ["    ", "def", "__init__", "(", "self", ",", "state_dim", ",", "agent_name", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "net", "=", "build_nn_from_config", "(", "input_dim", "=", "state_dim", ",", "output_dim", "=", "1", ",", "nn_config", "=", "config", "[", "\"agents\"", "]", "[", "agent_name", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.models.actor_critic.Critic_V.forward": [[80, 82], ["actor_critic.Critic_V.net"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "state", ")", ":", "\n", "        ", "return", "self", ".", "net", "(", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.models.actor_critic.Critic_DQN.__init__": [[85, 89], ["torch.Module.__init__", "models.model_utils.build_nn_from_config"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__", "home.repos.pwc.inspect_result.automl_learning_environments.models.model_utils.build_nn_from_config"], ["    ", "def", "__init__", "(", "self", ",", "state_dim", ",", "action_dim", ",", "agent_name", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "net", "=", "build_nn_from_config", "(", "input_dim", "=", "state_dim", ",", "output_dim", "=", "action_dim", ",", "nn_config", "=", "config", "[", "\"agents\"", "]", "[", "agent_name", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.models.actor_critic.Critic_DQN.forward": [[90, 92], ["actor_critic.Critic_DQN.net"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "state", ")", ":", "\n", "        ", "return", "self", ".", "net", "(", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.models.actor_critic.Critic_DuelingDQN.__init__": [[95, 115], ["torch.Module.__init__", "models.model_utils.build_nn_from_config", "copy.copy", "models.model_utils.build_nn_from_config", "models.model_utils.build_nn_from_config"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__", "home.repos.pwc.inspect_result.automl_learning_environments.models.model_utils.build_nn_from_config", "home.repos.pwc.inspect_result.automl_learning_environments.models.model_utils.build_nn_from_config", "home.repos.pwc.inspect_result.automl_learning_environments.models.model_utils.build_nn_from_config"], ["    ", "def", "__init__", "(", "self", ",", "state_dim", ",", "action_dim", ",", "agent_name", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "feature_stream", "=", "build_nn_from_config", "(", "input_dim", "=", "state_dim", ",", "\n", "output_dim", "=", "config", "[", "\"agents\"", "]", "[", "agent_name", "]", "[", "\"feature_dim\"", "]", ",", "\n", "nn_config", "=", "config", "[", "\"agents\"", "]", "[", "agent_name", "]", "\n", ")", "\n", "\n", "heads_config", "=", "copy", ".", "copy", "(", "config", "[", "\"agents\"", "]", "[", "agent_name", "]", ")", "\n", "heads_config", "[", "\"hidden_layer\"", "]", "=", "1", "\n", "heads_config", "[", "\"hidden_size\"", "]", "=", "config", "[", "\"agents\"", "]", "[", "agent_name", "]", "[", "\"feature_dim\"", "]", "\n", "\n", "self", ".", "value_stream", "=", "build_nn_from_config", "(", "input_dim", "=", "config", "[", "\"agents\"", "]", "[", "agent_name", "]", "[", "\"feature_dim\"", "]", ",", "\n", "output_dim", "=", "1", ",", "\n", "nn_config", "=", "heads_config", "\n", ")", "\n", "\n", "self", ".", "advantage_stream", "=", "build_nn_from_config", "(", "input_dim", "=", "config", "[", "\"agents\"", "]", "[", "agent_name", "]", "[", "\"feature_dim\"", "]", ",", "\n", "output_dim", "=", "action_dim", ",", "\n", "nn_config", "=", "heads_config", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.models.actor_critic.Critic_DuelingDQN.forward": [[117, 123], ["actor_critic.Critic_DuelingDQN.feature_stream", "actor_critic.Critic_DuelingDQN.value_stream", "actor_critic.Critic_DuelingDQN.advantage_stream", "actor_critic.Critic_DuelingDQN.mean"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "state", ")", ":", "\n", "        ", "features", "=", "self", ".", "feature_stream", "(", "state", ")", "\n", "values", "=", "self", ".", "value_stream", "(", "features", ")", "\n", "advantages", "=", "self", ".", "advantage_stream", "(", "features", ")", "\n", "q_values", "=", "values", "+", "(", "advantages", "-", "advantages", ".", "mean", "(", ")", ")", "\n", "return", "q_values", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.models.actor_critic.Actor_SAC.__init__": [[126, 137], ["torch.Module.__init__", "models.model_utils.build_nn_from_config", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__", "home.repos.pwc.inspect_result.automl_learning_environments.models.model_utils.build_nn_from_config"], ["    ", "def", "__init__", "(", "self", ",", "state_dim", ",", "action_dim", ",", "max_action", ",", "agent_name", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "net", "=", "build_nn_from_config", "(", "input_dim", "=", "state_dim", ",", "output_dim", "=", "1", ",", "nn_config", "=", "config", "[", "\"agents\"", "]", "[", "agent_name", "]", ")", "\n", "self", ".", "output_limit", "=", "max_action", "\n", "self", ".", "log_std_min", "=", "config", "[", "\"agents\"", "]", "[", "agent_name", "]", "[", "'log_std_min'", "]", "\n", "self", ".", "log_std_max", "=", "config", "[", "\"agents\"", "]", "[", "agent_name", "]", "[", "'log_std_max'", "]", "\n", "\n", "# Set output layers", "\n", "self", ".", "mu_layer", "=", "nn", ".", "Linear", "(", "action_dim", ",", "action_dim", ")", "\n", "self", ".", "log_std_layer", "=", "nn", ".", "Linear", "(", "action_dim", ",", "action_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.models.actor_critic.Actor_SAC.clip_but_pass_gradient": [[138, 143], ["clip_value.detach"], "methods", ["None"], ["", "def", "clip_but_pass_gradient", "(", "self", ",", "x", ",", "l", "=", "-", "1.", ",", "u", "=", "1.", ")", ":", "\n", "        ", "clip_up", "=", "(", "x", ">", "u", ")", ".", "float", "(", ")", "\n", "clip_low", "=", "(", "x", "<", "l", ")", ".", "float", "(", ")", "\n", "clip_value", "=", "(", "u", "-", "x", ")", "*", "clip_up", "+", "(", "l", "-", "x", ")", "*", "clip_low", "\n", "return", "x", "+", "clip_value", ".", "detach", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.models.actor_critic.Actor_SAC.apply_squashing_func": [[144, 150], ["torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "actor_critic.Actor_SAC.clip_but_pass_gradient", "torch.tanh.pow", "torch.tanh.pow", "torch.tanh.pow"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.models.actor_critic.Actor_SAC.clip_but_pass_gradient"], ["", "def", "apply_squashing_func", "(", "self", ",", "mu", ",", "pi", ",", "log_pi", ")", ":", "\n", "        ", "mu", "=", "torch", ".", "tanh", "(", "mu", ")", "\n", "pi", "=", "torch", ".", "tanh", "(", "pi", ")", "\n", "# To avoid evil machine precision error, strictly clip 1-pi**2 to [0,1] range.", "\n", "log_pi", "-=", "torch", ".", "sum", "(", "torch", ".", "log", "(", "self", ".", "clip_but_pass_gradient", "(", "1", "-", "pi", ".", "pow", "(", "2", ")", ",", "l", "=", "0.", ",", "u", "=", "1.", ")", "+", "1e-6", ")", ",", "dim", "=", "-", "1", ")", "\n", "return", "mu", ",", "pi", ",", "log_pi", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.models.actor_critic.Actor_SAC.forward": [[151, 169], ["actor_critic.Actor_SAC.net", "actor_critic.Actor_SAC.mu_layer", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.distributions.normal.Normal", "torch.distributions.normal.Normal", "torch.distributions.normal.Normal", "torch.distributions.normal.Normal.rsample", "torch.distributions.normal.Normal.rsample", "torch.distributions.normal.Normal.rsample", "torch.distributions.normal.Normal.log_prob().sum", "torch.distributions.normal.Normal.log_prob().sum", "torch.distributions.normal.Normal.log_prob().sum", "actor_critic.Actor_SAC.apply_squashing_func", "actor_critic.Actor_SAC.log_std_layer", "torch.distributions.normal.Normal.log_prob", "torch.distributions.normal.Normal.log_prob", "torch.distributions.normal.Normal.log_prob"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.models.actor_critic.Actor_SAC.apply_squashing_func"], ["", "def", "forward", "(", "self", ",", "state", ")", ":", "\n", "        ", "x", "=", "self", ".", "net", "(", "state", ")", "\n", "\n", "mu", "=", "self", ".", "mu_layer", "(", "x", ")", "\n", "log_std", "=", "torch", ".", "tanh", "(", "self", ".", "log_std_layer", "(", "x", ")", ")", "\n", "log_std", "=", "self", ".", "log_std_min", "+", "0.5", "*", "(", "self", ".", "log_std_max", "-", "self", ".", "log_std_min", ")", "*", "(", "log_std", "+", "1", ")", "\n", "std", "=", "torch", ".", "exp", "(", "log_std", ")", "\n", "\n", "# https://pytorch.org/docs/stable/distributions.html#normal", "\n", "dist", "=", "Normal", "(", "mu", ",", "std", ")", "\n", "pi", "=", "dist", ".", "rsample", "(", ")", "# Reparameterization trick (mean + std * N(0,1))", "\n", "log_pi", "=", "dist", ".", "log_prob", "(", "pi", ")", ".", "sum", "(", "dim", "=", "-", "1", ")", "\n", "mu", ",", "pi", ",", "log_pi", "=", "self", ".", "apply_squashing_func", "(", "mu", ",", "pi", ",", "log_pi", ")", "\n", "\n", "# Make sure outputs are in correct range", "\n", "mu", "=", "mu", "*", "self", ".", "output_limit", "\n", "pi", "=", "pi", "*", "self", ".", "output_limit", "\n", "return", "mu", ",", "pi", ",", "log_pi", "\n", "", "", ""]], "home.repos.pwc.inspect_result.automl_learning_environments.automl.analyze_bohb.analyze_bohb": [[19, 79], ["hpbandster.logged_results_to_HBS_result", "remove_outliers.get_all_runs", "remove_outliers.get_id2config_mapping", "remove_outliers.get_incumbent_id", "remove_outliers.get_runs_by_id", "print", "print", "print", "analyze_bohb.remove_outliers", "analyze_bohb.plot_parallel_scatter", "matplotlib.title", "matplotlib.show", "str().strip().replace().lower", "matplotlib.savefig", "os.path.join", "str", "str", "str", "str().strip().replace", "str().strip", "str"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.analyze_bohb.remove_outliers", "home.repos.pwc.inspect_result.automl_learning_environments.automl.analyze_bohb.plot_parallel_scatter"], ["def", "analyze_bohb", "(", "log_dir", ",", "title", ")", ":", "\n", "# load the example run from the log files", "\n", "    ", "result", "=", "hpres", ".", "logged_results_to_HBS_result", "(", "log_dir", ")", "\n", "\n", "# get all executed runs", "\n", "all_runs", "=", "result", ".", "get_all_runs", "(", ")", "\n", "\n", "# get the 'dict' that translates config ids to the actual configurations", "\n", "id2conf", "=", "result", ".", "get_id2config_mapping", "(", ")", "\n", "\n", "# Here is how you get he incumbent (best configuration)", "\n", "inc_id", "=", "result", ".", "get_incumbent_id", "(", ")", "\n", "\n", "# let's grab the run on the highest budget", "\n", "inc_runs", "=", "result", ".", "get_runs_by_id", "(", "inc_id", ")", "\n", "inc_run", "=", "inc_runs", "[", "-", "1", "]", "\n", "\n", "# We have access to all information: the config, the loss observed during", "\n", "# optimization, and all the additional information", "\n", "inc_valid_score", "=", "inc_run", ".", "loss", "\n", "inc_config", "=", "id2conf", "[", "inc_id", "]", "[", "'config'", "]", "\n", "inc_info", "=", "inc_run", "[", "'info'", "]", "\n", "\n", "print", "(", "'Best found configuration :'", "+", "str", "(", "inc_config", ")", ")", "\n", "print", "(", "'Score: '", "+", "str", "(", "inc_valid_score", ")", ")", "\n", "print", "(", "'Info: '", "+", "str", "(", "inc_info", ")", ")", "\n", "# print('It achieved accuracies of %f (validation) and %f (test).' % (-inc_valid_score, inc_test_score))", "\n", "\n", "# # Let's plot the observed losses grouped by budget,", "\n", "# hpvis.losses_over_time(all_runs)", "\n", "#", "\n", "# # the number of concurent runs,", "\n", "# hpvis.concurrent_runs_over_time(all_runs)", "\n", "#", "\n", "# # and the number of finished runs.", "\n", "# hpvis.finished_runs_over_time(all_runs)", "\n", "#", "\n", "# # This one visualizes the spearman rank correlation coefficients of the losses", "\n", "# # between different budgets.", "\n", "# hpvis.correlation_across_budgets(result)", "\n", "#", "\n", "# # For model based optimizers, one might wonder how much the model actually helped.", "\n", "# # The next plot compares the performance of configs picked by the model vs. random ones", "\n", "# hpvis.performance_histogram_model_vs_random(all_runs, id2conf)", "\n", "\n", "result", "=", "remove_outliers", "(", "result", ")", "\n", "\n", "# result = filter_values(result)", "\n", "\n", "# print_configs_sorted_by_loss(result)", "\n", "\n", "# print_stats_per_value(result)", "\n", "\n", "# plot_accuracy_over_budget(result)", "\n", "\n", "plot_parallel_scatter", "(", "result", ")", "\n", "plt", ".", "title", "(", "title", ")", "\n", "plt", ".", "show", "(", ")", "\n", "file_name", "=", "str", "(", "title", ")", ".", "strip", "(", ")", ".", "replace", "(", "' '", ",", "'_'", ")", ".", "lower", "(", ")", "\n", "plt", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "\"../experiments/automl_plots/\"", ",", "file_name", "+", "\".png\"", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.automl.analyze_bohb.print_configs_sorted_by_loss": [[81, 94], ["result.data.items", "lst.sort", "v1.results.items", "print", "lst.append"], "function", ["None"], ["", "def", "print_configs_sorted_by_loss", "(", "result", ")", ":", "\n", "    ", "lst", "=", "[", "]", "\n", "\n", "for", "k1", ",", "v1", "in", "result", ".", "data", ".", "items", "(", ")", ":", "\n", "        ", "for", "k2", ",", "v2", "in", "v1", ".", "results", ".", "items", "(", ")", ":", "\n", "            ", "loss", "=", "v2", "[", "'loss'", "]", "\n", "config", "=", "v1", ".", "config", "\n", "lst", ".", "append", "(", "(", "loss", ",", "config", ")", ")", "\n", "\n", "", "", "lst", ".", "sort", "(", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ")", "\n", "\n", "for", "elem", "in", "lst", ":", "\n", "        ", "print", "(", "elem", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.automl.analyze_bohb.print_stats_per_value": [[96, 134], ["float", "result.data.values", "dict().items", "value.config.items", "print", "set", "sorted", "value.results.items", "dict", "list", "len", "print", "sorted", "min", "config_params.items", "losses.append", "numpy.mean", "numpy.std", "len", "config_params.keys", "config_params[].append", "print"], "function", ["None"], ["", "", "def", "print_stats_per_value", "(", "result", ")", ":", "\n", "# get all possible keys", "\n", "    ", "min_epoch", "=", "float", "(", "'Inf'", ")", "\n", "\n", "config_params", "=", "{", "}", "\n", "for", "value", "in", "result", ".", "data", ".", "values", "(", ")", ":", "\n", "        ", "for", "config_param", ",", "config_param_val", "in", "value", ".", "config", ".", "items", "(", ")", ":", "\n", "            ", "for", "epoch", ",", "epoch_result", "in", "value", ".", "results", ".", "items", "(", ")", ":", "\n", "                ", "try", ":", "\n", "                    ", "loss", "=", "epoch_result", "[", "\"loss\"", "]", "\n", "\n", "min_epoch", "=", "min", "(", "min_epoch", ",", "epoch", ")", "\n", "\n", "if", "config_param", "in", "config_params", ".", "keys", "(", ")", ":", "\n", "                        ", "config_params", "[", "config_param", "]", ".", "append", "(", "(", "config_param_val", ",", "epoch", ",", "loss", ")", ")", "\n", "", "else", ":", "\n", "                        ", "config_params", "[", "config_param", "]", "=", "[", "(", "config_param_val", ",", "epoch", ",", "loss", ")", "]", "\n", "", "", "except", ":", "\n", "                    ", "print", "(", "'Error in get_avg_per_value, continuing'", ")", "\n", "\n", "", "", "", "", "for", "config_param", ",", "data", "in", "(", "dict", "(", "sorted", "(", "config_params", ".", "items", "(", ")", ")", ")", ")", ".", "items", "(", ")", ":", "\n", "        ", "print", "(", "config_param", ")", "\n", "\n", "# get all unique possible values for each config parameter", "\n", "values", "=", "set", "(", "elem", "[", "0", "]", "for", "elem", "in", "data", ")", "\n", "values", "=", "sorted", "(", "list", "(", "values", ")", ")", "\n", "\n", "if", "len", "(", "values", ")", ">", "20", ":", "\n", "            ", "continue", "\n", "\n", "", "for", "value", "in", "values", ":", "\n", "            ", "losses", "=", "[", "]", "\n", "for", "elem", "in", "data", ":", "\n", "                ", "val", ",", "epoch", ",", "loss", "=", "elem", "\n", "if", "val", "==", "value", "and", "epoch", "==", "min_epoch", ":", "\n", "                    ", "losses", ".", "append", "(", "loss", ")", "\n", "\n", "", "", "print", "(", "'{}  {}  {} {}'", ".", "format", "(", "value", ",", "np", ".", "mean", "(", "losses", ")", ",", "np", ".", "std", "(", "losses", ")", ",", "len", "(", "losses", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.automl.analyze_bohb.remove_outliers": [[136, 181], ["result.data.items", "range", "lut.sort", "math.ceil", "math.ceil", "range", "range", "value1.results.values", "len", "lut.pop", "result.data.pop", "lut.pop", "result.data.pop", "lut.append", "math.isfinite", "sorted", "abs", "abs", "result.data[].results.keys", "len", "len", "float", "math.isfinite"], "function", ["None"], ["", "", "", "def", "remove_outliers", "(", "result", ")", ":", "\n", "    ", "lut", "=", "[", "]", "\n", "for", "key", ",", "value1", "in", "result", ".", "data", ".", "items", "(", ")", ":", "\n", "        ", "for", "value2", "in", "value1", ".", "results", ".", "values", "(", ")", ":", "\n", "            ", "if", "value2", "==", "None", ":", "\n", "                ", "loss", "=", "float", "(", "'nan'", ")", "\n", "", "else", ":", "\n", "                ", "loss", "=", "value2", "[", "'loss'", "]", "\n", "", "lut", ".", "append", "(", "[", "loss", ",", "key", "]", ")", "\n", "\n", "", "", "filtered_lut", "=", "[", "x", "for", "x", "in", "lut", "if", "math", ".", "isfinite", "(", "x", "[", "0", "]", ")", "]", "\n", "worst_loss", "=", "sorted", "(", "filtered_lut", ",", "reverse", "=", "REVERSE_LOSS", ")", "[", "0", "]", "[", "0", "]", "\n", "\n", "if", "REVERSE_LOSS", ":", "\n", "        ", "worst_loss", "+=", "0.01", "*", "abs", "(", "worst_loss", ")", "\n", "", "else", ":", "\n", "        ", "worst_loss", "-=", "0.01", "*", "abs", "(", "worst_loss", ")", "\n", "\n", "# remove NaN's", "\n", "", "for", "i", "in", "range", "(", "len", "(", "lut", ")", ")", ":", "\n", "        ", "if", "not", "math", ".", "isfinite", "(", "lut", "[", "i", "]", "[", "0", "]", ")", "or", "lut", "[", "i", "]", "[", "0", "]", "==", "0", ":", "\n", "            ", "lut", "[", "i", "]", "[", "0", "]", "=", "worst_loss", "\n", "for", "key", "in", "result", ".", "data", "[", "lut", "[", "i", "]", "[", "1", "]", "]", ".", "results", ".", "keys", "(", ")", ":", "\n", "# hacky but sometimes some budgets are missing (presumably when terminating ongoing runs)", "\n", "                ", "if", "result", ".", "data", "[", "lut", "[", "i", "]", "[", "1", "]", "]", ".", "results", "[", "key", "]", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "else", ":", "\n", "                    ", "result", ".", "data", "[", "lut", "[", "i", "]", "[", "1", "]", "]", ".", "results", "[", "key", "]", "[", "'loss'", "]", "=", "worst_loss", "\n", "# result.data.pop(elem[1], None)", "\n", "\n", "", "", "", "", "lut", ".", "sort", "(", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ",", "reverse", "=", "REVERSE_LOSS", ")", "\n", "n_remove_worst", "=", "math", ".", "ceil", "(", "len", "(", "lut", ")", "*", "OUTLIER_PERC_WORST", ")", "\n", "n_remove_best", "=", "math", ".", "ceil", "(", "len", "(", "lut", ")", "*", "OUTLIER_PERC_BEST", ")", "\n", "\n", "# remove percentage of worst values", "\n", "for", "i", "in", "range", "(", "n_remove_worst", ")", ":", "\n", "        ", "elem", "=", "lut", ".", "pop", "(", "0", ")", "\n", "result", ".", "data", ".", "pop", "(", "elem", "[", "1", "]", ",", "None", ")", "\n", "\n", "# remove percentage of best values", "\n", "", "for", "i", "in", "range", "(", "n_remove_best", ")", ":", "\n", "        ", "elem", "=", "lut", ".", "pop", "(", ")", "\n", "result", ".", "data", ".", "pop", "(", "elem", "[", "1", "]", ",", "None", ")", "\n", "\n", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.automl.analyze_bohb.filter_values": [[183, 200], ["result.data.items", "result.data.pop"], "function", ["None"], ["", "def", "filter_values", "(", "result", ")", ":", "\n", "    ", "del_list", "=", "[", "]", "\n", "for", "key", ",", "value1", "in", "result", ".", "data", ".", "items", "(", ")", ":", "\n", "        ", "id", "=", "key", "\n", "config", "=", "value1", ".", "config", "\n", "\n", "rep_env_num", "=", "config", "[", "'rep_env_num'", "]", "\n", "ddqn_dropout", "=", "config", "[", "'ddqn_dropout'", "]", "\n", "# if not ddqn_dropout == 0:", "\n", "#     del_list.append(id)", "\n", "# if not rep_env_num == 5:", "\n", "#     del_list.append(id)", "\n", "\n", "", "for", "id", "in", "del_list", ":", "\n", "        ", "result", ".", "data", ".", "pop", "(", "id", ",", "None", ")", "\n", "\n", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.automl.analyze_bohb.plot_accuracy_over_budget": [[202, 227], ["matplotlib.subplots", "result.data.items", "ax.set_title", "ax.set_xlabel", "ax.set_ylabel", "analyze_bohb.get_bright_random_color", "value1.results.items", "matplotlib.semilogx", "x.append", "y.append", "print"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.analyze_bohb.get_bright_random_color"], ["", "def", "plot_accuracy_over_budget", "(", "result", ")", ":", "\n", "    ", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", ")", "\n", "\n", "# plot hyperband plot", "\n", "index", "=", "None", "\n", "color", "=", "None", "\n", "\n", "for", "key", ",", "value1", "in", "result", ".", "data", ".", "items", "(", ")", ":", "\n", "        ", "if", "key", "[", "0", "]", "is", "not", "index", ":", "\n", "            ", "index", "=", "key", "[", "0", "]", "\n", "color", "=", "get_bright_random_color", "(", ")", "\n", "\n", "", "try", ":", "\n", "            ", "x", "=", "[", "]", "\n", "y", "=", "[", "]", "\n", "for", "key2", ",", "value2", "in", "value1", ".", "results", ".", "items", "(", ")", ":", "\n", "                ", "x", ".", "append", "(", "key2", ")", "\n", "y", ".", "append", "(", "value2", "[", "\"loss\"", "]", ")", "\n", "", "plt", ".", "semilogx", "(", "x", ",", "y", ",", "color", "=", "color", ")", "\n", "", "except", ":", "\n", "            ", "print", "(", "'Error in plot_accuracy_over_budget, continuing'", ")", "\n", "\n", "", "", "ax", ".", "set_title", "(", "'Score for different configurations'", ")", "\n", "ax", ".", "set_xlabel", "(", "'epochs'", ")", "\n", "ax", ".", "set_ylabel", "(", "'score'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.automl.analyze_bohb.plot_parallel_scatter": [[229, 365], ["matplotlib.subplots", "result.data.values", "dict().items", "matplotlib.yticks", "matplotlib.xticks", "matplotlib.subplots_adjust", "value.config.items", "set", "sorted", "len", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "range", "matplotlib.scatter", "numpy.arange", "tuple", "value.results.items", "dict", "list", "len", "range", "type", "range", "sorted", "sorted", "len", "len", "matplotlib.text", "range", "type", "min", "range", "config_params.keys", "min", "max", "min", "max", "config_params.items", "analyze_bohb.map_to_zero_one_range", "analyze_bohb.get_color", "str", "len", "len", "matplotlib.text", "range", "type", "min", "range", "min", "max", "matplotlib.text", "matplotlib.text", "matplotlib.text", "config_params.keys", "config_params[].append", "print", "analyze_bohb.linear_interpolation", "len", "len", "min", "max", "matplotlib.text", "matplotlib.text", "len", "range", "numpy.exp", "range", "analyze_bohb.linear_interpolation", "range", "str", "str", "str", "numpy.log", "numpy.log", "numpy.log", "numpy.random.uniform", "numpy.random.uniform", "len", "str", "str", "matplotlib.text", "len", "len", "range", "len", "range", "max", "numpy.random.uniform", "numpy.random.uniform", "len", "str", "len", "len", "max", "max", "numpy.random.uniform", "numpy.random.uniform", "numpy.log", "numpy.log", "analyze_bohb.linear_interpolation", "analyze_bohb.linear_interpolation", "decimal.Decimal", "decimal.Decimal", "decimal.Decimal", "len", "decimal.Decimal", "decimal.Decimal", "max", "numpy.random.uniform", "numpy.log", "numpy.log", "numpy.log", "numpy.random.uniform", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.analyze_bohb.map_to_zero_one_range", "home.repos.pwc.inspect_result.automl_learning_environments.automl.analyze_bohb.get_color", "home.repos.pwc.inspect_result.automl_learning_environments.automl.analyze_bohb.linear_interpolation", "home.repos.pwc.inspect_result.automl_learning_environments.automl.analyze_bohb.linear_interpolation", "home.repos.pwc.inspect_result.automl_learning_environments.automl.analyze_bohb.linear_interpolation", "home.repos.pwc.inspect_result.automl_learning_environments.automl.analyze_bohb.linear_interpolation"], ["", "def", "plot_parallel_scatter", "(", "result", ")", ":", "\n", "    ", "plt", ".", "subplots", "(", "dpi", "=", "300", ",", "figsize", "=", "(", "8", ",", "4", ")", ")", "\n", "\n", "ep_m", "=", "1e9", "\n", "ep_M", "=", "-", "1e9", "\n", "loss_m", "=", "1e9", "\n", "loss_M", "=", "-", "1e9", "\n", "\n", "# get all possible keys", "\n", "config_params", "=", "{", "}", "\n", "for", "value", "in", "result", ".", "data", ".", "values", "(", ")", ":", "\n", "        ", "for", "config_param", ",", "config_param_val", "in", "value", ".", "config", ".", "items", "(", ")", ":", "\n", "            ", "for", "epoch", ",", "epoch_result", "in", "value", ".", "results", ".", "items", "(", ")", ":", "\n", "                ", "try", ":", "\n", "                    ", "loss", "=", "epoch_result", "[", "\"loss\"", "]", "\n", "ep_m", "=", "min", "(", "ep_m", ",", "epoch", ")", "\n", "ep_M", "=", "max", "(", "ep_M", ",", "epoch", ")", "\n", "loss_m", "=", "min", "(", "loss_m", ",", "loss", ")", "\n", "loss_M", "=", "max", "(", "loss_M", ",", "loss", ")", "\n", "\n", "if", "config_param", "in", "config_params", ".", "keys", "(", ")", ":", "\n", "                        ", "config_params", "[", "config_param", "]", ".", "append", "(", "(", "config_param_val", ",", "epoch", ",", "loss", ")", ")", "\n", "", "else", ":", "\n", "                        ", "config_params", "[", "config_param", "]", "=", "[", "(", "config_param_val", ",", "epoch", ",", "loss", ")", "]", "\n", "", "", "except", ":", "\n", "                    ", "print", "(", "'Error in plot_parallel_scatter, continuing'", ")", "\n", "\n", "", "", "", "", "x_dev", "=", "0.2", "\n", "r_min", "=", "3", "\n", "r_max", "=", "4", "\n", "alpha", "=", "0.4", "\n", "text_x_offset", "=", "-", "0.1", "\n", "text_y_offset", "=", "-", "0.1", "\n", "size_text", "=", "6", "\n", "\n", "index", "=", "0", "\n", "for", "config_param", ",", "data", "in", "(", "dict", "(", "sorted", "(", "config_params", ".", "items", "(", ")", ")", ")", ")", ".", "items", "(", ")", ":", "\n", "# get all unique possible values for each config parameter", "\n", "        ", "values", "=", "set", "(", "elem", "[", "0", "]", "for", "elem", "in", "data", ")", "\n", "values", "=", "sorted", "(", "list", "(", "values", ")", ")", "\n", "\n", "n", "=", "len", "(", "data", ")", "\n", "xs", "=", "np", ".", "zeros", "(", "n", ")", "\n", "ys", "=", "np", ".", "zeros", "(", "n", ")", "\n", "rads", "=", "np", ".", "zeros", "(", "n", ")", "\n", "colors", "=", "np", ".", "zeros", "(", "[", "n", ",", "3", "]", ")", "\n", "\n", "# extract common features", "\n", "for", "i", "in", "range", "(", "len", "(", "values", ")", ")", ":", "\n", "            ", "for", "k", "in", "range", "(", "len", "(", "data", ")", ")", ":", "\n", "                ", "if", "data", "[", "k", "]", "[", "0", "]", "==", "values", "[", "i", "]", ":", "\n", "                    ", "ep", "=", "data", "[", "k", "]", "[", "1", "]", "\n", "acc", "=", "map_to_zero_one_range", "(", "data", "[", "k", "]", "[", "2", "]", ",", "loss_m", ",", "loss_M", ")", "\n", "\n", "# test:", "\n", "# loss_b = -1233125.5410615604", "\n", "# loss_a = -5233125.5410615604 #(we minimize the negative reward)", "\n", "# print(loss_b, \"->\", map_to_zero_one_range(loss_b, loss_m, loss_M))", "\n", "# print(loss_a, \"->\", map_to_zero_one_range(loss_a, loss_m, loss_M))", "\n", "\n", "rads", "[", "k", "]", "=", "linear_interpolation", "(", "np", ".", "log", "(", "ep", ")", ",", "np", ".", "log", "(", "ep_m", ")", ",", "np", ".", "log", "(", "ep_M", ")", ",", "r_min", ",", "r_max", ")", "**", "2", "\n", "colors", "[", "k", ",", ":", "]", "=", "get_color", "(", "acc", ")", "\n", "\n", "# check for type (categorical,int,float,log)", "\n", "", "", "", "if", "type", "(", "values", "[", "0", "]", ")", "is", "bool", ":", "\n", "            ", "y_dev", "=", "x_dev", "/", "2", "\n", "for", "i", "in", "range", "(", "len", "(", "values", ")", ")", ":", "\n", "                ", "plt", ".", "text", "(", "index", "+", "text_x_offset", ",", "values", "[", "i", "]", "+", "text_y_offset", ",", "str", "(", "values", "[", "i", "]", ")", ",", "rotation", "=", "90", ",", "\n", "size", "=", "size_text", ")", "\n", "for", "k", "in", "range", "(", "len", "(", "data", ")", ")", ":", "\n", "                    ", "if", "data", "[", "k", "]", "[", "0", "]", "==", "values", "[", "i", "]", ":", "\n", "                        ", "xs", "[", "k", "]", "=", "index", "+", "np", ".", "random", ".", "uniform", "(", "-", "x_dev", ",", "x_dev", ")", "\n", "ys", "[", "k", "]", "=", "values", "[", "i", "]", "+", "np", ".", "random", ".", "uniform", "(", "-", "y_dev", ",", "y_dev", ")", "\n", "\n", "", "", "", "", "elif", "type", "(", "values", "[", "0", "]", ")", "is", "str", ":", "\n", "            ", "y_dev", "=", "min", "(", "1", "/", "len", "(", "values", ")", "/", "2.5", ",", "x_dev", "/", "2", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "values", ")", ")", ":", "\n", "                ", "plt", ".", "text", "(", "index", "+", "text_x_offset", ",", "i", "/", "(", "max", "(", "len", "(", "values", ")", "-", "1", ",", "1", ")", ")", "+", "text_y_offset", ",", "values", "[", "i", "]", ",", "\n", "rotation", "=", "90", ",", "size", "=", "size_text", ")", "\n", "for", "k", "in", "range", "(", "len", "(", "data", ")", ")", ":", "\n", "                    ", "if", "data", "[", "k", "]", "[", "0", "]", "==", "values", "[", "i", "]", ":", "\n", "                        ", "xs", "[", "k", "]", "=", "index", "+", "np", ".", "random", ".", "uniform", "(", "-", "x_dev", ",", "x_dev", ")", "\n", "ys", "[", "k", "]", "=", "i", "/", "(", "max", "(", "len", "(", "values", ")", "-", "1", ",", "1", ")", ")", "+", "np", ".", "random", ".", "uniform", "(", "-", "y_dev", ",", "y_dev", ")", "\n", "\n", "", "", "", "", "elif", "type", "(", "values", "[", "0", "]", ")", "is", "int", ":", "\n", "            ", "y_dev", "=", "min", "(", "1", "/", "len", "(", "values", ")", "/", "2.5", ",", "x_dev", "/", "2", ")", "\n", "\n", "plotAllStr", "=", "len", "(", "values", ")", "<", "20", "\n", "\n", "if", "not", "plotAllStr", ":", "\n", "                ", "min_val", "=", "min", "(", "values", ")", "\n", "max_val", "=", "max", "(", "values", ")", "\n", "plt", ".", "text", "(", "index", "+", "text_x_offset", ",", "0", "+", "text_y_offset", ",", "str", "(", "f\"{Decimal(min_val):.1E}\"", ")", ",", "rotation", "=", "90", ",", "size", "=", "size_text", ")", "\n", "plt", ".", "text", "(", "index", "+", "text_x_offset", ",", "1", "+", "text_y_offset", ",", "str", "(", "f\"{Decimal(max_val):.1E}\"", ")", ",", "rotation", "=", "90", ",", "size", "=", "size_text", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "len", "(", "values", ")", ")", ":", "\n", "                ", "if", "plotAllStr", ":", "\n", "                    ", "plt", ".", "text", "(", "index", "+", "text_x_offset", ",", "i", "/", "(", "max", "(", "len", "(", "values", ")", "-", "1", ",", "1", ")", ")", ",", "str", "(", "values", "[", "i", "]", ")", ",", "rotation", "=", "90", ",", "\n", "size", "=", "size_text", ")", "\n", "", "for", "k", "in", "range", "(", "len", "(", "data", ")", ")", ":", "\n", "                    ", "if", "data", "[", "k", "]", "[", "0", "]", "==", "values", "[", "i", "]", ":", "\n", "                        ", "xs", "[", "k", "]", "=", "index", "+", "np", ".", "random", ".", "uniform", "(", "-", "x_dev", ",", "x_dev", ")", "\n", "ys", "[", "k", "]", "=", "i", "/", "(", "max", "(", "len", "(", "values", ")", "-", "1", ",", "1", ")", ")", "+", "np", ".", "random", ".", "uniform", "(", "-", "y_dev", ",", "y_dev", ")", "\n", "\n", "", "", "", "", "else", ":", "# float", "\n", "            ", "min_val", "=", "min", "(", "values", ")", "\n", "max_val", "=", "max", "(", "values", ")", "\n", "\n", "# log scale if min/max value differs to much", "\n", "if", "max_val", "/", "min_val", ">", "100", ":", "\n", "                ", "val050", "=", "np", ".", "exp", "(", "(", "np", ".", "log", "(", "min_val", ")", "+", "np", ".", "log", "(", "max_val", ")", ")", "/", "2", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "values", ")", ")", ":", "\n", "                    ", "for", "k", "in", "range", "(", "len", "(", "data", ")", ")", ":", "\n", "                        ", "if", "data", "[", "k", "]", "[", "0", "]", "==", "values", "[", "i", "]", ":", "\n", "                            ", "xs", "[", "k", "]", "=", "index", "+", "np", ".", "random", ".", "uniform", "(", "-", "x_dev", ",", "x_dev", ")", "\n", "ys", "[", "k", "]", "=", "linear_interpolation", "(", "np", ".", "log", "(", "data", "[", "k", "]", "[", "0", "]", ")", ",", "np", ".", "log", "(", "min_val", ")", ",", "np", ".", "log", "(", "max_val", ")", ",", "0", ",", "1", ")", "\n", "\n", "# linear scale", "\n", "", "", "", "", "else", ":", "\n", "                ", "val050", "=", "linear_interpolation", "(", "0.50", ",", "0", ",", "1", ",", "min_val", ",", "max_val", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "values", ")", ")", ":", "\n", "                    ", "for", "k", "in", "range", "(", "len", "(", "data", ")", ")", ":", "\n", "                        ", "if", "data", "[", "k", "]", "[", "0", "]", "==", "values", "[", "i", "]", ":", "\n", "                            ", "xs", "[", "k", "]", "=", "index", "+", "np", ".", "random", ".", "uniform", "(", "-", "x_dev", ",", "x_dev", ")", "\n", "ys", "[", "k", "]", "=", "linear_interpolation", "(", "data", "[", "k", "]", "[", "0", "]", ",", "min_val", ",", "max_val", ",", "0", ",", "1", ")", "\n", "\n", "", "", "", "", "plt", ".", "text", "(", "index", "+", "text_x_offset", ",", "0", "+", "text_y_offset", ",", "str", "(", "f\"{Decimal(min_val):.1E}\"", ")", ",", "rotation", "=", "90", ",", "size", "=", "size_text", ")", "\n", "plt", ".", "text", "(", "index", "+", "text_x_offset", ",", "0.5", "+", "text_y_offset", ",", "str", "(", "f\"{Decimal(val050):.1E}\"", ")", ",", "rotation", "=", "90", ",", "size", "=", "size_text", ")", "\n", "plt", ".", "text", "(", "index", "+", "text_x_offset", ",", "1", "+", "text_y_offset", ",", "str", "(", "f\"{Decimal(max_val):.1E}\"", ")", ",", "rotation", "=", "90", ",", "size", "=", "size_text", ")", "\n", "\n", "", "plt", ".", "scatter", "(", "xs", ",", "ys", ",", "s", "=", "rads", ",", "c", "=", "colors", ",", "alpha", "=", "alpha", ",", "edgecolors", "=", "'none'", ")", "\n", "index", "+=", "1", "\n", "\n", "", "plt", ".", "yticks", "(", "[", "]", ",", "[", "]", ")", "\n", "plt", ".", "xticks", "(", "np", ".", "arange", "(", "index", ")", ",", "(", "tuple", "(", "sorted", "(", "config_params", ".", "keys", "(", ")", ")", ")", ")", ",", "rotation", "=", "90", ",", "fontsize", "=", "size_text", ")", "\n", "plt", ".", "subplots_adjust", "(", "bottom", "=", "0.25", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.automl.analyze_bohb.linear_interpolation": [[367, 370], ["None"], "function", ["None"], ["", "def", "linear_interpolation", "(", "x", ",", "x0", ",", "x1", ",", "y0", ",", "y1", ")", ":", "\n", "# linearly interpolate between two x/y values for a given x value", "\n", "    ", "return", "y0", "+", "(", "y1", "-", "y0", ")", "*", "(", "x", "-", "x0", ")", "/", "(", "x1", "-", "x0", "+", "1e-9", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.automl.analyze_bohb.map_to_zero_one_range": [[372, 388], ["None"], "function", ["None"], ["", "def", "map_to_zero_one_range", "(", "loss", ",", "loss_m", ",", "loss_M", ")", ":", "\n", "    ", "if", "loss_M", "<", "1", "and", "loss_m", ">", "0", "and", "REVERSE_LOSS", "==", "False", ":", "\n", "# if we have already a loss in the [0,1] range, there is no need to normalize anything", "\n", "        ", "acc", "=", "loss", "\n", "", "elif", "loss_M", "<", "0", "and", "loss_m", ">", "-", "1", "and", "REVERSE_LOSS", "==", "True", ":", "\n", "# if we have a loss in the [-1,0] range, simply revert its sign", "\n", "        ", "acc", "=", "-", "loss", "\n", "", "else", ":", "\n", "# normalize loss to the 0 (bad) - 1(good) range", "\n", "        ", "acc", "=", "(", "loss", "-", "loss_m", ")", "/", "(", "loss_M", "-", "loss_m", "+", "1e-9", ")", "\n", "if", "REVERSE_LOSS", ":", "\n", "            ", "acc", "=", "1", "-", "acc", "\n", "\n", "", "", "acc", "=", "acc", "**", "EXP_LOSS", "\n", "\n", "return", "acc", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.automl.analyze_bohb.get_color": [[390, 404], ["numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array"], "function", ["None"], ["", "def", "get_color", "(", "acc", ")", ":", "\n", "# print(\"acc: \", acc)", "\n", "    ", "if", "acc", "<=", "0", ":", "\n", "# print(\"color: \", np.array([[1, 0, 0]]))", "\n", "        ", "return", "np", ".", "array", "(", "[", "[", "1", ",", "0", ",", "0", "]", "]", ")", "\n", "", "elif", "acc", "<=", "0.5", ":", "\n", "# print(\"color: \", np.array([[1, 0, 0]]) + 2 * acc * np.array([[0, 1, 0]]))", "\n", "        ", "return", "np", ".", "array", "(", "[", "[", "1", ",", "0", ",", "0", "]", "]", ")", "+", "2", "*", "acc", "*", "np", ".", "array", "(", "[", "[", "0", ",", "1", ",", "0", "]", "]", ")", "\n", "", "elif", "acc", "<=", "1", ":", "\n", "# print(\"color: \", np.array([[1, 1, 0]]) + 2 * (acc - 0.5) * np.array([[-1, 0, 0]]))", "\n", "        ", "return", "np", ".", "array", "(", "[", "[", "1", ",", "1", ",", "0", "]", "]", ")", "+", "2", "*", "(", "acc", "-", "0.5", ")", "*", "np", ".", "array", "(", "[", "[", "-", "1", ",", "0", ",", "0", "]", "]", ")", "\n", "", "else", ":", "\n", "# print(\"color: \", np.array([[0, 1, 0]]))", "\n", "        ", "return", "np", ".", "array", "(", "[", "[", "0", ",", "1", ",", "0", "]", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.automl.analyze_bohb.get_bright_random_color": [[406, 409], ["colorsys.hls_to_rgb", "random.random"], "function", ["None"], ["", "", "def", "get_bright_random_color", "(", ")", ":", "\n", "    ", "h", ",", "s", ",", "l", "=", "random", ".", "random", "(", ")", ",", "1", ",", "0.5", "\n", "return", "colorsys", ".", "hls_to_rgb", "(", "h", ",", "l", ",", "s", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWorker.__init__": [[23, 30], ["hpbandster.core.worker.Worker.__init__", "print"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__"], ["    ", "def", "__init__", "(", "self", ",", "id", ",", "working_dir", ",", "experiment_wrapper", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "BohbWorker", ",", "self", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "print", "(", "kwargs", ")", "\n", "\n", "self", ".", "id", "=", "id", "\n", "self", ".", "working_dir", "=", "working_dir", "\n", "self", ".", "experiment_wrapper", "=", "experiment_wrapper", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWorker.compute": [[31, 33], ["bohb_optim.BohbWorker.experiment_wrapper.compute"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWorker.compute"], ["", "def", "compute", "(", "self", ",", "config_id", ",", "config", ",", "budget", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "self", ".", "experiment_wrapper", ".", "compute", "(", "self", ".", "working_dir", ",", "self", ".", "id", ",", "config_id", ",", "config", ",", "budget", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__": [[36, 79], ["hpbandster.optimizers.config_generators.bohb.BOHB", "hpbandster.optimizers.config_generators.bohb.BOHB", "hpbandster.core.master.Master.__init__", "bohb_optim.BohbWrapper.config.update", "ValueError", "numpy.power", "int", "numpy.linspace", "numpy.log", "numpy.log"], "methods", ["home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.__init__", "home.repos.pwc.inspect_result.automl_learning_environments.None.utils.AverageMeter.update"], ["    ", "def", "__init__", "(", "self", ",", "configspace", "=", "None", ",", "\n", "eta", "=", "3", ",", "min_budget", "=", "0.01", ",", "max_budget", "=", "1", ",", "\n", "min_points_in_model", "=", "None", ",", "top_n_percent", "=", "15", ",", "\n", "num_samples", "=", "64", ",", "random_fraction", "=", "1", "/", "3", ",", "bandwidth_factor", "=", "3", ",", "\n", "min_bandwidth", "=", "1e-3", ",", "\n", "**", "kwargs", ")", ":", "\n", "# TODO: Proper check for ConfigSpace object!", "\n", "        ", "if", "configspace", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"You have to provide a valid CofigSpace object\"", ")", "\n", "\n", "", "cg", "=", "BOHB", "(", "configspace", "=", "configspace", ",", "\n", "min_points_in_model", "=", "min_points_in_model", ",", "\n", "top_n_percent", "=", "top_n_percent", ",", "\n", "num_samples", "=", "num_samples", ",", "\n", "random_fraction", "=", "random_fraction", ",", "\n", "bandwidth_factor", "=", "bandwidth_factor", ",", "\n", "min_bandwidth", "=", "min_bandwidth", "\n", ")", "\n", "\n", "super", "(", ")", ".", "__init__", "(", "config_generator", "=", "cg", ",", "**", "kwargs", ")", "\n", "\n", "# Hyperband related stuff", "\n", "self", ".", "eta", "=", "eta", "\n", "self", ".", "min_budget", "=", "min_budget", "\n", "self", ".", "max_budget", "=", "max_budget", "\n", "\n", "# precompute some HB stuff", "\n", "self", ".", "max_SH_iter", "=", "-", "int", "(", "np", ".", "log", "(", "min_budget", "/", "max_budget", ")", "/", "np", ".", "log", "(", "eta", ")", ")", "+", "1", "\n", "self", ".", "budgets", "=", "max_budget", "*", "np", ".", "power", "(", "eta", ",", "-", "np", ".", "linspace", "(", "self", ".", "max_SH_iter", "-", "1", ",", "0", ",", "\n", "self", ".", "max_SH_iter", ")", ")", "\n", "\n", "self", ".", "config", ".", "update", "(", "{", "\n", "'eta'", ":", "eta", ",", "\n", "'min_budget'", ":", "min_budget", ",", "\n", "'max_budget'", ":", "max_budget", ",", "\n", "'budgets'", ":", "self", ".", "budgets", ",", "\n", "'max_SH_iter'", ":", "self", ".", "max_SH_iter", ",", "\n", "'min_points_in_model'", ":", "min_points_in_model", ",", "\n", "'top_n_percent'", ":", "top_n_percent", ",", "\n", "'num_samples'", ":", "num_samples", ",", "\n", "'random_fraction'", ":", "random_fraction", ",", "\n", "'bandwidth_factor'", ":", "bandwidth_factor", ",", "\n", "'min_bandwidth'", ":", "min_bandwidth", "\n", "}", ")", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.BohbWrapper.get_next_iteration": [[81, 92], ["int", "hpbandster.optimizers.iterations.SuccessiveHalving", "hpbandster.optimizers.iterations.SuccessiveHalving", "max", "numpy.floor", "int", "range"], "methods", ["None"], ["", "def", "get_next_iteration", "(", "self", ",", "iteration", ",", "iteration_kwargs", "=", "{", "}", ")", ":", "\n", "# number of 'SH rungs'", "\n", "        ", "s", "=", "self", ".", "max_SH_iter", "-", "1", "\n", "# number of configurations in that bracket", "\n", "n0", "=", "int", "(", "np", ".", "floor", "(", "(", "self", ".", "max_SH_iter", ")", "/", "(", "s", "+", "1", ")", ")", "*", "self", ".", "eta", "**", "s", ")", "\n", "ns", "=", "[", "max", "(", "int", "(", "n0", "*", "(", "self", ".", "eta", "**", "(", "-", "i", ")", ")", ")", ",", "1", ")", "for", "i", "in", "range", "(", "s", "+", "1", ")", "]", "\n", "\n", "return", "(", "SuccessiveHalving", "(", "HPB_iter", "=", "iteration", ",", "num_configs", "=", "ns", ",", "\n", "budgets", "=", "self", ".", "budgets", "[", "(", "-", "s", "-", "1", ")", ":", "]", ",", "\n", "config_sampler", "=", "self", ".", "config_generator", ".", "get_config", ",", "\n", "**", "iteration_kwargs", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.get_bohb_interface": [[94, 108], ["psutil.net_if_addrs", "psutil.net_if_addrs.keys", "print", "psutil.net_if_addrs.keys", "print", "psutil.net_if_addrs.keys", "print", "print"], "function", ["None"], ["", "", "def", "get_bohb_interface", "(", ")", ":", "\n", "    ", "addrs", "=", "psutil", ".", "net_if_addrs", "(", ")", "\n", "if", "'eth0'", "in", "addrs", ".", "keys", "(", ")", ":", "\n", "        ", "print", "(", "'FOUND eth0 INTERFACE'", ")", "\n", "return", "'eth0'", "\n", "", "elif", "'eno1'", "in", "addrs", ".", "keys", "(", ")", ":", "\n", "        ", "print", "(", "'FOUND eno1 INTERFACE'", ")", "\n", "return", "'eno1'", "\n", "", "elif", "'ib0'", "in", "addrs", ".", "keys", "(", ")", ":", "\n", "        ", "print", "(", "'FOUND ib0 INTERFACE'", ")", "\n", "return", "'ib0'", "\n", "", "else", ":", "\n", "        ", "print", "(", "'FOUND lo INTERFACE'", ")", "\n", "return", "'lo'", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.get_working_dir": [[110, 112], ["str", "os.path.join", "os.getcwd"], "function", ["None"], ["", "", "def", "get_working_dir", "(", "run_id", ")", ":", "\n", "    ", "return", "str", "(", "os", ".", "path", ".", "join", "(", "os", ".", "getcwd", "(", ")", ",", "\"results\"", ",", "run_id", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.run_bohb_parallel": [[114, 180], ["experiment_wrapper.get_bohb_parameters", "bohb_optim.get_bohb_interface", "bohb_optim.get_working_dir", "hpbandster.nic_name_to_host", "os.makedirs", "print", "hpbandster.NameServer", "hpns.NameServer.start", "bohb_optim.BohbWorker", "BohbWorker.run", "hpbandster.json_result_logger", "bohb_optim.BohbWrapper", "BohbWrapper.run", "BohbWrapper.shutdown", "hpns.NameServer.shutdown", "print", "time.sleep", "bohb_optim.BohbWorker", "BohbWorker.load_nameserver_credentials", "BohbWorker.run", "exit", "int", "experiment_wrapper.get_configspace", "int"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_step_size.ExperimentWrapper.get_bohb_parameters", "home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.get_bohb_interface", "home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.get_working_dir", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.run", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.run", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.run", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_step_size.ExperimentWrapper.get_configspace"], ["", "def", "run_bohb_parallel", "(", "id", ",", "run_id", ",", "bohb_workers", ",", "experiment_wrapper", ")", ":", "\n", "# get bohb params", "\n", "    ", "bohb_params", "=", "experiment_wrapper", ".", "get_bohb_parameters", "(", ")", "\n", "\n", "# get suitable interface (eth0 or lo)", "\n", "bohb_interface", "=", "get_bohb_interface", "(", ")", "\n", "\n", "# get BOHB log directory", "\n", "working_dir", "=", "get_working_dir", "(", "run_id", ")", "\n", "\n", "# every process has to lookup the hostname", "\n", "host", "=", "hpns", ".", "nic_name_to_host", "(", "bohb_interface", ")", "\n", "\n", "os", ".", "makedirs", "(", "working_dir", ",", "exist_ok", "=", "True", ")", "\n", "\n", "if", "int", "(", "id", ")", "%", "1000", "!=", "0", ":", "\n", "        ", "print", "(", "'START NEW WORKER'", ")", "\n", "time", ".", "sleep", "(", "10", ")", "\n", "w", "=", "BohbWorker", "(", "host", "=", "host", ",", "\n", "id", "=", "id", ",", "\n", "run_id", "=", "run_id", ",", "\n", "working_dir", "=", "working_dir", ",", "\n", "experiment_wrapper", "=", "experiment_wrapper", ")", "\n", "w", ".", "load_nameserver_credentials", "(", "working_directory", "=", "working_dir", ")", "\n", "w", ".", "run", "(", "background", "=", "False", ")", "\n", "exit", "(", "0", ")", "\n", "\n", "", "print", "(", "'START NEW MASTER'", ")", "\n", "ns", "=", "hpns", ".", "NameServer", "(", "run_id", "=", "run_id", ",", "\n", "host", "=", "host", ",", "\n", "port", "=", "0", ",", "\n", "working_directory", "=", "working_dir", ")", "\n", "ns_host", ",", "ns_port", "=", "ns", ".", "start", "(", ")", "\n", "\n", "w", "=", "BohbWorker", "(", "host", "=", "host", ",", "\n", "nameserver", "=", "ns_host", ",", "\n", "nameserver_port", "=", "ns_port", ",", "\n", "id", "=", "id", ",", "\n", "run_id", "=", "run_id", ",", "\n", "working_dir", "=", "working_dir", ",", "\n", "experiment_wrapper", "=", "experiment_wrapper", ")", "\n", "w", ".", "run", "(", "background", "=", "True", ")", "\n", "\n", "result_logger", "=", "hpres", ".", "json_result_logger", "(", "directory", "=", "working_dir", ",", "\n", "overwrite", "=", "True", ")", "\n", "\n", "bohb", "=", "BohbWrapper", "(", "\n", "configspace", "=", "experiment_wrapper", ".", "get_configspace", "(", ")", ",", "\n", "run_id", "=", "run_id", ",", "\n", "eta", "=", "bohb_params", "[", "'eta'", "]", ",", "\n", "host", "=", "host", ",", "\n", "nameserver", "=", "ns_host", ",", "\n", "nameserver_port", "=", "ns_port", ",", "\n", "min_budget", "=", "bohb_params", "[", "'min_budget'", "]", ",", "\n", "max_budget", "=", "bohb_params", "[", "'max_budget'", "]", ",", "\n", "random_fraction", "=", "bohb_params", "[", "'random_fraction'", "]", ",", "\n", "result_logger", "=", "result_logger", ")", "\n", "\n", "# res = bohb.run(n_iterations=bohb_params['iterations'])", "\n", "res", "=", "bohb", ".", "run", "(", "n_iterations", "=", "bohb_params", "[", "'iterations'", "]", ",", "\n", "min_n_workers", "=", "int", "(", "bohb_workers", ")", ")", "\n", "\n", "bohb", ".", "shutdown", "(", "shutdown_workers", "=", "True", ")", "\n", "ns", ".", "shutdown", "(", ")", "\n", "\n", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.run_bohb_serial": [[182, 222], ["experiment_wrapper.get_bohb_parameters", "bohb_optim.get_working_dir", "int", "hpbandster.NameServer", "hpns.NameServer.start", "bohb_optim.BohbWorker", "BohbWorker.run", "hpbandster.json_result_logger", "bohb_optim.BohbWrapper", "BohbWrapper.run", "BohbWrapper.shutdown", "hpns.NameServer.shutdown", "experiment_wrapper.get_configspace", "random.random"], "function", ["home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_step_size.ExperimentWrapper.get_bohb_parameters", "home.repos.pwc.inspect_result.automl_learning_environments.automl.bohb_optim.get_working_dir", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.run", "home.repos.pwc.inspect_result.automl_learning_environments.agents.GTN_worker.GTN_Worker.run", "home.repos.pwc.inspect_result.automl_learning_environments.experiments.GTNC_evaluate_gridworld_step_size.ExperimentWrapper.get_configspace"], ["", "def", "run_bohb_serial", "(", "run_id", ",", "experiment_wrapper", ")", ":", "\n", "# get bohb parameters", "\n", "    ", "bohb_params", "=", "experiment_wrapper", ".", "get_bohb_parameters", "(", ")", "\n", "\n", "# get BOHB log directory", "\n", "working_dir", "=", "get_working_dir", "(", "run_id", ")", "\n", "\n", "# assign random port in the 30000-40000 range to avoid using a blocked port because of a previous improper bohb shutdown", "\n", "port", "=", "int", "(", "30000", "+", "random", ".", "random", "(", ")", "*", "10000", ")", "\n", "\n", "ns", "=", "hpns", ".", "NameServer", "(", "run_id", "=", "run_id", ",", "host", "=", "\"127.0.0.1\"", ",", "port", "=", "port", ")", "\n", "ns", ".", "start", "(", ")", "\n", "\n", "w", "=", "BohbWorker", "(", "nameserver", "=", "\"127.0.0.1\"", ",", "\n", "id", "=", "0", ",", "\n", "run_id", "=", "run_id", ",", "\n", "nameserver_port", "=", "port", ",", "\n", "working_dir", "=", "working_dir", ",", "\n", "experiment_wrapper", "=", "experiment_wrapper", ")", "\n", "w", ".", "run", "(", "background", "=", "True", ")", "\n", "\n", "result_logger", "=", "hpres", ".", "json_result_logger", "(", "directory", "=", "working_dir", ",", "\n", "overwrite", "=", "True", ")", "\n", "\n", "bohb", "=", "BohbWrapper", "(", "\n", "configspace", "=", "experiment_wrapper", ".", "get_configspace", "(", ")", ",", "\n", "run_id", "=", "run_id", ",", "\n", "eta", "=", "bohb_params", "[", "'eta'", "]", ",", "\n", "min_budget", "=", "bohb_params", "[", "'min_budget'", "]", ",", "\n", "max_budget", "=", "bohb_params", "[", "'max_budget'", "]", ",", "\n", "random_fraction", "=", "bohb_params", "[", "'random_fraction'", "]", ",", "\n", "nameserver", "=", "\"127.0.0.1\"", ",", "\n", "nameserver_port", "=", "port", ",", "\n", "result_logger", "=", "result_logger", ")", "\n", "\n", "res", "=", "bohb", ".", "run", "(", "n_iterations", "=", "bohb_params", "[", "'iterations'", "]", ")", "\n", "bohb", ".", "shutdown", "(", "shutdown_workers", "=", "True", ")", "\n", "ns", ".", "shutdown", "(", ")", "\n", "\n", "return", "res", "\n", "", ""]]}