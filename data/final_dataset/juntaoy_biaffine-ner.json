{"home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.get_char_vocab.get_char_vocab": [[8, 21], ["set", "sorted", "print", "list", "open", "open", "f.readlines", "f.write", "len", "json.loads", "sorted.update"], "function", ["None"], ["def", "get_char_vocab", "(", "input_filenames", ",", "output_filename", ")", ":", "\n", "  ", "vocab", "=", "set", "(", ")", "\n", "for", "filename", "in", "input_filenames", ":", "\n", "    ", "with", "open", "(", "filename", ")", "as", "f", ":", "\n", "      ", "for", "line", "in", "f", ".", "readlines", "(", ")", ":", "\n", "        ", "for", "sentence", "in", "json", ".", "loads", "(", "line", ")", "[", "\"sentences\"", "]", ":", "\n", "          ", "for", "word", "in", "sentence", ":", "\n", "            ", "vocab", ".", "update", "(", "word", ")", "\n", "", "", "", "", "", "vocab", "=", "sorted", "(", "list", "(", "vocab", ")", ")", "\n", "with", "open", "(", "output_filename", ",", "\"w\"", ")", "as", "f", ":", "\n", "    ", "for", "char", "in", "vocab", ":", "\n", "      ", "f", ".", "write", "(", "u\"{}\\n\"", ".", "format", "(", "char", ")", ".", "encode", "(", "\"utf8\"", ")", ")", "\n", "", "", "print", "(", "\"Wrote {} characters to {}\"", ".", "format", "(", "len", "(", "vocab", ")", ",", "output_filename", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.biaffine_ner_model.BiaffineNERModel.__init__": [[15, 62], ["util.EmbeddingDictionary", "util.load_char_dict", "h5py.File", "len", "input_props.append", "input_props.append", "input_props.append", "input_props.append", "input_props.append", "input_props.append", "input_props.append", "zip", "tensorflow.PaddingFIFOQueue", "tensorflow.PaddingFIFOQueue.enqueue", "tensorflow.PaddingFIFOQueue.dequeue", "biaffine_ner_model.BiaffineNERModel.get_predictions_and_loss", "tensorflow.Variable", "tensorflow.assign", "tensorflow.train.exponential_decay", "tensorflow.trainable_variables", "tensorflow.gradients", "tensorflow.clip_by_global_norm", "optimizer.apply_gradients", "tensorflow.placeholder", "zip", "enumerate"], "methods", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.load_char_dict", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.biaffine_ner_model.BiaffineNERModel.get_predictions_and_loss"], ["  ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "    ", "self", ".", "config", "=", "config", "\n", "self", ".", "context_embeddings", "=", "util", ".", "EmbeddingDictionary", "(", "config", "[", "\"context_embeddings\"", "]", ")", "\n", "self", ".", "context_embeddings_size", "=", "self", ".", "context_embeddings", ".", "size", "\n", "\n", "self", ".", "char_embedding_size", "=", "config", "[", "\"char_embedding_size\"", "]", "\n", "self", ".", "char_dict", "=", "util", ".", "load_char_dict", "(", "config", "[", "\"char_vocab_path\"", "]", ")", "\n", "\n", "self", ".", "lm_file", "=", "h5py", ".", "File", "(", "self", ".", "config", "[", "\"lm_path\"", "]", ",", "\"r\"", ")", "\n", "self", ".", "lm_layers", "=", "self", ".", "config", "[", "\"lm_layers\"", "]", "\n", "self", ".", "lm_size", "=", "self", ".", "config", "[", "\"lm_size\"", "]", "\n", "\n", "self", ".", "eval_data", "=", "None", "# Load eval data lazily.", "\n", "self", ".", "ner_types", "=", "self", ".", "config", "[", "'ner_types'", "]", "\n", "self", ".", "ner_maps", "=", "{", "ner", ":", "(", "i", "+", "1", ")", "for", "i", ",", "ner", "in", "enumerate", "(", "self", ".", "ner_types", ")", "}", "\n", "self", ".", "num_types", "=", "len", "(", "self", ".", "ner_types", ")", "\n", "\n", "input_props", "=", "[", "]", "\n", "input_props", ".", "append", "(", "(", "tf", ".", "string", ",", "[", "None", ",", "None", "]", ")", ")", "# Tokens.", "\n", "input_props", ".", "append", "(", "(", "tf", ".", "float32", ",", "[", "None", ",", "None", ",", "self", ".", "context_embeddings_size", "]", ")", ")", "# Context embeddings.", "\n", "input_props", ".", "append", "(", "(", "tf", ".", "float32", ",", "[", "None", ",", "None", ",", "self", ".", "lm_size", ",", "self", ".", "lm_layers", "]", ")", ")", "# LM embeddings.", "\n", "input_props", ".", "append", "(", "(", "tf", ".", "int32", ",", "[", "None", ",", "None", ",", "None", "]", ")", ")", "# Character indices.", "\n", "input_props", ".", "append", "(", "(", "tf", ".", "int32", ",", "[", "None", "]", ")", ")", "# Text lengths.", "\n", "input_props", ".", "append", "(", "(", "tf", ".", "bool", ",", "[", "]", ")", ")", "# Is training.", "\n", "input_props", ".", "append", "(", "(", "tf", ".", "int32", ",", "[", "None", "]", ")", ")", "# Gold NER Label", "\n", "\n", "self", ".", "queue_input_tensors", "=", "[", "tf", ".", "placeholder", "(", "dtype", ",", "shape", ")", "for", "dtype", ",", "shape", "in", "input_props", "]", "\n", "dtypes", ",", "shapes", "=", "zip", "(", "*", "input_props", ")", "\n", "queue", "=", "tf", ".", "PaddingFIFOQueue", "(", "capacity", "=", "10", ",", "dtypes", "=", "dtypes", ",", "shapes", "=", "shapes", ")", "\n", "self", ".", "enqueue_op", "=", "queue", ".", "enqueue", "(", "self", ".", "queue_input_tensors", ")", "\n", "self", ".", "input_tensors", "=", "queue", ".", "dequeue", "(", ")", "\n", "\n", "self", ".", "predictions", ",", "self", ".", "loss", "=", "self", ".", "get_predictions_and_loss", "(", "self", ".", "input_tensors", ")", "\n", "self", ".", "global_step", "=", "tf", ".", "Variable", "(", "0", ",", "name", "=", "\"global_step\"", ",", "trainable", "=", "False", ")", "\n", "self", ".", "reset_global_step", "=", "tf", ".", "assign", "(", "self", ".", "global_step", ",", "0", ")", "\n", "learning_rate", "=", "tf", ".", "train", ".", "exponential_decay", "(", "self", ".", "config", "[", "\"learning_rate\"", "]", ",", "self", ".", "global_step", ",", "\n", "self", ".", "config", "[", "\"decay_frequency\"", "]", ",", "self", ".", "config", "[", "\"decay_rate\"", "]", ",", "\n", "staircase", "=", "True", ")", "\n", "trainable_params", "=", "tf", ".", "trainable_variables", "(", ")", "\n", "gradients", "=", "tf", ".", "gradients", "(", "self", ".", "loss", ",", "trainable_params", ")", "\n", "gradients", ",", "_", "=", "tf", ".", "clip_by_global_norm", "(", "gradients", ",", "self", ".", "config", "[", "\"max_gradient_norm\"", "]", ")", "\n", "optimizers", "=", "{", "\n", "\"adam\"", ":", "tf", ".", "train", ".", "AdamOptimizer", ",", "\n", "\"sgd\"", ":", "tf", ".", "train", ".", "GradientDescentOptimizer", "\n", "}", "\n", "optimizer", "=", "optimizers", "[", "self", ".", "config", "[", "\"optimizer\"", "]", "]", "(", "learning_rate", ")", "\n", "self", ".", "train_op", "=", "optimizer", ".", "apply_gradients", "(", "zip", "(", "gradients", ",", "trainable_params", ")", ",", "global_step", "=", "self", ".", "global_step", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.biaffine_ner_model.BiaffineNERModel.start_enqueue_thread": [[63, 77], ["threading.Thread", "threading.Thread.start", "open", "json.loads", "random.shuffle", "f.readlines", "biaffine_ner_model.BiaffineNERModel.tensorize_example", "dict", "session.run", "zip"], "methods", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.biaffine_ner_model.BiaffineNERModel.tensorize_example"], ["", "def", "start_enqueue_thread", "(", "self", ",", "session", ")", ":", "\n", "    ", "with", "open", "(", "self", ".", "config", "[", "\"train_path\"", "]", ")", "as", "f", ":", "\n", "      ", "train_examples", "=", "[", "json", ".", "loads", "(", "jsonline", ")", "for", "jsonline", "in", "f", ".", "readlines", "(", ")", "]", "\n", "\n", "", "def", "_enqueue_loop", "(", ")", ":", "\n", "      ", "while", "True", ":", "\n", "        ", "random", ".", "shuffle", "(", "train_examples", ")", "\n", "for", "example", "in", "train_examples", ":", "\n", "          ", "tensorized_example", "=", "self", ".", "tensorize_example", "(", "example", ",", "is_training", "=", "True", ")", "\n", "feed_dict", "=", "dict", "(", "zip", "(", "self", ".", "queue_input_tensors", ",", "tensorized_example", ")", ")", "\n", "session", ".", "run", "(", "self", ".", "enqueue_op", ",", "feed_dict", "=", "feed_dict", ")", "\n", "", "", "", "enqueue_thread", "=", "threading", ".", "Thread", "(", "target", "=", "_enqueue_loop", ")", "\n", "enqueue_thread", ".", "daemon", "=", "True", "\n", "enqueue_thread", ".", "start", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.biaffine_ner_model.BiaffineNERModel.restore": [[78, 86], ["tensorflow.train.Saver", "os.path.join", "print", "session.run", "tensorflow.train.Saver.restore", "tensorflow.global_variables_initializer", "tensorflow.global_variables"], "methods", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.biaffine_ner_model.BiaffineNERModel.restore"], ["", "def", "restore", "(", "self", ",", "session", ")", ":", "\n", "# Don't try to restore unused variables from the TF-Hub ELMo module.", "\n", "    ", "vars_to_restore", "=", "[", "v", "for", "v", "in", "tf", ".", "global_variables", "(", ")", "if", "\"module/\"", "not", "in", "v", ".", "name", "]", "\n", "saver", "=", "tf", ".", "train", ".", "Saver", "(", "vars_to_restore", ")", "\n", "checkpoint_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "config", "[", "\"log_dir\"", "]", ",", "\"model.max.ckpt\"", ")", "\n", "print", "(", "\"Restoring from {}\"", ".", "format", "(", "checkpoint_path", ")", ")", "\n", "session", ".", "run", "(", "tf", ".", "global_variables_initializer", "(", ")", ")", "\n", "saver", ".", "restore", "(", "session", ",", "checkpoint_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.biaffine_ner_model.BiaffineNERModel.load_lm_embeddings": [[87, 100], ["doc_key.replace", "len", "numpy.zeros", "enumerate", "numpy.zeros", "list", "group.keys", "range", "max", "str"], "methods", ["None"], ["", "def", "load_lm_embeddings", "(", "self", ",", "doc_key", ")", ":", "\n", "    ", "if", "self", ".", "lm_file", "is", "None", ":", "\n", "      ", "return", "np", ".", "zeros", "(", "[", "0", ",", "0", ",", "self", ".", "lm_size", ",", "self", ".", "lm_layers", "]", ")", "\n", "", "file_key", "=", "doc_key", ".", "replace", "(", "\"/\"", ",", "\":\"", ")", "\n", "if", "not", "file_key", "in", "self", ".", "lm_file", "and", "file_key", "[", ":", "-", "2", "]", "in", "self", ".", "lm_file", ":", "\n", "      ", "file_key", "=", "file_key", "[", ":", "-", "2", "]", "\n", "", "group", "=", "self", ".", "lm_file", "[", "file_key", "]", "\n", "num_sentences", "=", "len", "(", "list", "(", "group", ".", "keys", "(", ")", ")", ")", "\n", "sentences", "=", "[", "group", "[", "str", "(", "i", ")", "]", "[", "...", "]", "for", "i", "in", "range", "(", "num_sentences", ")", "]", "\n", "lm_emb", "=", "np", ".", "zeros", "(", "[", "num_sentences", ",", "max", "(", "s", ".", "shape", "[", "0", "]", "for", "s", "in", "sentences", ")", ",", "self", ".", "lm_size", ",", "self", ".", "lm_layers", "]", ")", "\n", "for", "i", ",", "s", "in", "enumerate", "(", "sentences", ")", ":", "\n", "      ", "lm_emb", "[", "i", ",", ":", "s", ".", "shape", "[", "0", "]", ",", ":", ",", ":", "]", "=", "s", "\n", "", "return", "lm_emb", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.biaffine_ner_model.BiaffineNERModel.tensorize_example": [[101, 141], ["max", "max", "numpy.array", "numpy.zeros", "numpy.zeros", "enumerate", "numpy.array", "biaffine_ner_model.BiaffineNERModel.load_lm_embeddings", "numpy.array", "max", "max", "enumerate", "enumerate", "len", "len", "len", "len", "biaffine_ner_model.BiaffineNERModel.context_embeddings.is_in_embeddings", "xrange", "max", "len", "xrange", "biaffine_ner_model.BiaffineNERModel.context_embeddings.is_in_embeddings", "len", "numpy.array.append", "len", "len", "ner.get"], "methods", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.biaffine_ner_model.BiaffineNERModel.load_lm_embeddings", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.EmbeddingDictionary.is_in_embeddings", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.EmbeddingDictionary.is_in_embeddings"], ["", "def", "tensorize_example", "(", "self", ",", "example", ",", "is_training", ")", ":", "\n", "    ", "ners", "=", "example", "[", "\"ners\"", "]", "\n", "sentences", "=", "example", "[", "\"sentences\"", "]", "\n", "\n", "max_sentence_length", "=", "max", "(", "len", "(", "s", ")", "for", "s", "in", "sentences", ")", "\n", "max_word_length", "=", "max", "(", "max", "(", "max", "(", "len", "(", "w", ")", "for", "w", "in", "s", ")", "for", "s", "in", "sentences", ")", ",", "max", "(", "self", ".", "config", "[", "\"filter_widths\"", "]", ")", ")", "\n", "text_len", "=", "np", ".", "array", "(", "[", "len", "(", "s", ")", "for", "s", "in", "sentences", "]", ")", "\n", "tokens", "=", "[", "[", "\"\"", "]", "*", "max_sentence_length", "for", "_", "in", "sentences", "]", "\n", "char_index", "=", "np", ".", "zeros", "(", "[", "len", "(", "sentences", ")", ",", "max_sentence_length", ",", "max_word_length", "]", ")", "\n", "context_word_emb", "=", "np", ".", "zeros", "(", "[", "len", "(", "sentences", ")", ",", "max_sentence_length", ",", "self", ".", "context_embeddings_size", "]", ")", "\n", "lemmas", "=", "[", "]", "\n", "if", "\"lemmas\"", "in", "example", ":", "\n", "      ", "lemmas", "=", "example", "[", "\"lemmas\"", "]", "\n", "", "for", "i", ",", "sentence", "in", "enumerate", "(", "sentences", ")", ":", "\n", "      ", "for", "j", ",", "word", "in", "enumerate", "(", "sentence", ")", ":", "\n", "        ", "tokens", "[", "i", "]", "[", "j", "]", "=", "word", "\n", "if", "self", ".", "context_embeddings", ".", "is_in_embeddings", "(", "word", ")", ":", "\n", "          ", "context_word_emb", "[", "i", ",", "j", "]", "=", "self", ".", "context_embeddings", "[", "word", "]", "\n", "", "elif", "lemmas", "and", "self", ".", "context_embeddings", ".", "is_in_embeddings", "(", "lemmas", "[", "i", "]", "[", "j", "]", ")", ":", "\n", "          ", "context_word_emb", "[", "i", ",", "j", "]", "=", "self", ".", "context_embeddings", "[", "lemmas", "[", "i", "]", "[", "j", "]", "]", "\n", "", "char_index", "[", "i", ",", "j", ",", ":", "len", "(", "word", ")", "]", "=", "[", "self", ".", "char_dict", "[", "c", "]", "for", "c", "in", "word", "]", "\n", "\n", "", "", "tokens", "=", "np", ".", "array", "(", "tokens", ")", "\n", "\n", "doc_key", "=", "example", "[", "\"doc_key\"", "]", "\n", "\n", "lm_emb", "=", "self", ".", "load_lm_embeddings", "(", "doc_key", ")", "\n", "\n", "gold_labels", "=", "[", "]", "\n", "if", "is_training", ":", "\n", "      ", "for", "sid", ",", "sent", "in", "enumerate", "(", "sentences", ")", ":", "\n", "        ", "ner", "=", "{", "(", "s", ",", "e", ")", ":", "self", ".", "ner_maps", "[", "t", "]", "for", "s", ",", "e", ",", "t", "in", "ners", "[", "sid", "]", "}", "\n", "for", "s", "in", "xrange", "(", "len", "(", "sent", ")", ")", ":", "\n", "          ", "for", "e", "in", "xrange", "(", "s", ",", "len", "(", "sent", ")", ")", ":", "\n", "            ", "gold_labels", ".", "append", "(", "ner", ".", "get", "(", "(", "s", ",", "e", ")", ",", "0", ")", ")", "\n", "", "", "", "", "gold_labels", "=", "np", ".", "array", "(", "gold_labels", ")", "\n", "\n", "example_tensors", "=", "(", "tokens", ",", "context_word_emb", ",", "lm_emb", ",", "char_index", ",", "text_len", ",", "is_training", ",", "gold_labels", ")", "\n", "\n", "return", "example_tensors", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.biaffine_ner_model.BiaffineNERModel.get_dropout": [[142, 144], ["tensorflow.to_float"], "methods", ["None"], ["", "def", "get_dropout", "(", "self", ",", "dropout_rate", ",", "is_training", ")", ":", "\n", "    ", "return", "1", "-", "(", "tf", ".", "to_float", "(", "is_training", ")", "*", "dropout_rate", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.biaffine_ner_model.BiaffineNERModel.lstm_contextualize": [[145, 178], ["range", "tensorflow.shape", "tensorflow.variable_scope", "tensorflow.contrib.rnn.LSTMStateTuple", "tensorflow.contrib.rnn.LSTMStateTuple", "tensorflow.nn.bidirectional_dynamic_rnn", "tensorflow.concat", "tensorflow.nn.dropout", "tensorflow.variable_scope", "util.CustomLSTMCell", "tensorflow.variable_scope", "util.CustomLSTMCell", "tensorflow.tile", "tensorflow.tile", "tensorflow.tile", "tensorflow.tile", "tensorflow.sigmoid", "util.projection", "util.shape"], "methods", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.shape", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.dropout", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.projection", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.shape"], ["", "def", "lstm_contextualize", "(", "self", ",", "text_emb", ",", "text_len", ",", "lstm_dropout", ")", ":", "\n", "    ", "num_sentences", "=", "tf", ".", "shape", "(", "text_emb", ")", "[", "0", "]", "\n", "\n", "current_inputs", "=", "text_emb", "# [num_sentences, max_sentence_length, emb]", "\n", "\n", "for", "layer", "in", "range", "(", "self", ".", "config", "[", "\"contextualization_layers\"", "]", ")", ":", "\n", "      ", "with", "tf", ".", "variable_scope", "(", "\"layer_{}\"", ".", "format", "(", "layer", ")", ",", "reuse", "=", "tf", ".", "AUTO_REUSE", ")", ":", "\n", "        ", "with", "tf", ".", "variable_scope", "(", "\"fw_cell\"", ")", ":", "\n", "          ", "cell_fw", "=", "util", ".", "CustomLSTMCell", "(", "self", ".", "config", "[", "\"contextualization_size\"", "]", ",", "num_sentences", ",", "lstm_dropout", ")", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"bw_cell\"", ")", ":", "\n", "          ", "cell_bw", "=", "util", ".", "CustomLSTMCell", "(", "self", ".", "config", "[", "\"contextualization_size\"", "]", ",", "num_sentences", ",", "lstm_dropout", ")", "\n", "", "state_fw", "=", "tf", ".", "contrib", ".", "rnn", ".", "LSTMStateTuple", "(", "tf", ".", "tile", "(", "cell_fw", ".", "initial_state", ".", "c", ",", "[", "num_sentences", ",", "1", "]", ")", ",", "\n", "tf", ".", "tile", "(", "cell_fw", ".", "initial_state", ".", "h", ",", "[", "num_sentences", ",", "1", "]", ")", ")", "\n", "state_bw", "=", "tf", ".", "contrib", ".", "rnn", ".", "LSTMStateTuple", "(", "tf", ".", "tile", "(", "cell_bw", ".", "initial_state", ".", "c", ",", "[", "num_sentences", ",", "1", "]", ")", ",", "\n", "tf", ".", "tile", "(", "cell_bw", ".", "initial_state", ".", "h", ",", "[", "num_sentences", ",", "1", "]", ")", ")", "\n", "\n", "(", "fw_outputs", ",", "bw_outputs", ")", ",", "(", "(", "_", ",", "fw_final_state", ")", ",", "(", "_", ",", "bw_final_state", ")", ")", "=", "tf", ".", "nn", ".", "bidirectional_dynamic_rnn", "(", "\n", "cell_fw", "=", "cell_fw", ",", "\n", "cell_bw", "=", "cell_bw", ",", "\n", "inputs", "=", "current_inputs", ",", "\n", "sequence_length", "=", "text_len", ",", "\n", "initial_state_fw", "=", "state_fw", ",", "\n", "initial_state_bw", "=", "state_bw", ")", "\n", "\n", "text_outputs", "=", "tf", ".", "concat", "(", "[", "fw_outputs", ",", "bw_outputs", "]", ",", "2", ")", "# [num_sentences, max_sentence_length, emb]", "\n", "text_outputs", "=", "tf", ".", "nn", ".", "dropout", "(", "text_outputs", ",", "lstm_dropout", ")", "\n", "if", "layer", ">", "0", ":", "\n", "          ", "highway_gates", "=", "tf", ".", "sigmoid", "(", "\n", "util", ".", "projection", "(", "text_outputs", ",", "util", ".", "shape", "(", "text_outputs", ",", "2", ")", ")", ")", "# [num_sentences, max_sentence_length, emb]", "\n", "text_outputs", "=", "highway_gates", "*", "text_outputs", "+", "(", "1", "-", "highway_gates", ")", "*", "current_inputs", "\n", "", "current_inputs", "=", "text_outputs", "\n", "\n", "", "", "return", "text_outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.biaffine_ner_model.BiaffineNERModel.get_predictions_and_loss": [[179, 239], ["biaffine_ner_model.BiaffineNERModel.get_dropout", "biaffine_ner_model.BiaffineNERModel.get_dropout", "biaffine_ner_model.BiaffineNERModel.get_dropout", "context_emb_list.append", "tensorflow.gather", "tensorflow.reshape", "util.cnn", "tensorflow.reshape", "context_emb_list.append", "util.shape", "util.shape", "tensorflow.reshape", "tensorflow.matmul", "tensorflow.reshape", "context_emb_list.append", "tensorflow.concat", "tensorflow.nn.dropout", "tensorflow.sequence_mask", "tensorflow.logical_and", "tensorflow.tile", "tensorflow.logical_and", "tensorflow.reshape", "biaffine_ner_model.BiaffineNERModel.lstm_contextualize", "util.bilinear_classifier", "tensorflow.boolean_mask", "tensorflow.nn.sparse_softmax_cross_entropy_with_logits", "tensorflow.reduce_sum", "tensorflow.shape", "tensorflow.shape", "tensorflow.get_variable", "tensorflow.variable_scope", "tensorflow.nn.softmax", "tensorflow.get_variable", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.variable_scope", "util.projection", "tensorflow.variable_scope", "util.projection", "tensorflow.reshape", "util.shape", "util.shape", "util.shape", "tensorflow.get_variable", "tensorflow.logical_not", "len", "tensorflow.constant_initializer", "tensorflow.sequence_mask", "tensorflow.constant_initializer", "tensorflow.range"], "methods", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.biaffine_ner_model.BiaffineNERModel.get_dropout", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.biaffine_ner_model.BiaffineNERModel.get_dropout", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.biaffine_ner_model.BiaffineNERModel.get_dropout", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.cnn", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.shape", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.shape", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.dropout", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.biaffine_ner_model.BiaffineNERModel.lstm_contextualize", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.bilinear_classifier", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.shape", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.shape", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.projection", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.projection", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.shape", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.shape", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.shape"], ["", "def", "get_predictions_and_loss", "(", "self", ",", "inputs", ")", ":", "\n", "    ", "tokens", ",", "context_word_emb", ",", "lm_emb", ",", "char_index", ",", "text_len", ",", "is_training", ",", "gold_labels", "=", "inputs", "\n", "self", ".", "dropout", "=", "self", ".", "get_dropout", "(", "self", ".", "config", "[", "\"dropout_rate\"", "]", ",", "is_training", ")", "\n", "self", ".", "lexical_dropout", "=", "self", ".", "get_dropout", "(", "self", ".", "config", "[", "\"lexical_dropout_rate\"", "]", ",", "is_training", ")", "\n", "self", ".", "lstm_dropout", "=", "self", ".", "get_dropout", "(", "self", ".", "config", "[", "\"lstm_dropout_rate\"", "]", ",", "is_training", ")", "\n", "\n", "num_sentences", "=", "tf", ".", "shape", "(", "tokens", ")", "[", "0", "]", "\n", "max_sentence_length", "=", "tf", ".", "shape", "(", "tokens", ")", "[", "1", "]", "\n", "\n", "context_emb_list", "=", "[", "]", "\n", "context_emb_list", ".", "append", "(", "context_word_emb", ")", "\n", "char_emb", "=", "tf", ".", "gather", "(", "tf", ".", "get_variable", "(", "\"char_embeddings\"", ",", "[", "len", "(", "self", ".", "char_dict", ")", ",", "self", ".", "config", "[", "\"char_embedding_size\"", "]", "]", ")", ",", "char_index", ")", "# [num_sentences, max_sentence_length, max_word_length, emb]", "\n", "flattened_char_emb", "=", "tf", ".", "reshape", "(", "char_emb", ",", "[", "num_sentences", "*", "max_sentence_length", ",", "util", ".", "shape", "(", "char_emb", ",", "2", ")", ",", "util", ".", "shape", "(", "char_emb", ",", "3", ")", "]", ")", "# [num_sentences * max_sentence_length, max_word_length, emb]", "\n", "flattened_aggregated_char_emb", "=", "util", ".", "cnn", "(", "flattened_char_emb", ",", "self", ".", "config", "[", "\"filter_widths\"", "]", ",", "self", ".", "config", "[", "\"filter_size\"", "]", ")", "# [num_sentences * max_sentence_length, emb]", "\n", "aggregated_char_emb", "=", "tf", ".", "reshape", "(", "flattened_aggregated_char_emb", ",", "[", "num_sentences", ",", "max_sentence_length", ",", "util", ".", "shape", "(", "flattened_aggregated_char_emb", ",", "1", ")", "]", ")", "# [num_sentences, max_sentence_length, emb]", "\n", "context_emb_list", ".", "append", "(", "aggregated_char_emb", ")", "\n", "\n", "\n", "lm_emb_size", "=", "util", ".", "shape", "(", "lm_emb", ",", "2", ")", "\n", "lm_num_layers", "=", "util", ".", "shape", "(", "lm_emb", ",", "3", ")", "\n", "with", "tf", ".", "variable_scope", "(", "\"lm_aggregation\"", ")", ":", "\n", "      ", "self", ".", "lm_weights", "=", "tf", ".", "nn", ".", "softmax", "(", "tf", ".", "get_variable", "(", "\"lm_scores\"", ",", "[", "lm_num_layers", "]", ",", "initializer", "=", "tf", ".", "constant_initializer", "(", "0.0", ")", ")", ")", "\n", "self", ".", "lm_scaling", "=", "tf", ".", "get_variable", "(", "\"lm_scaling\"", ",", "[", "]", ",", "initializer", "=", "tf", ".", "constant_initializer", "(", "1.0", ")", ")", "\n", "\n", "", "flattened_lm_emb", "=", "tf", ".", "reshape", "(", "lm_emb", ",", "[", "num_sentences", "*", "max_sentence_length", "*", "lm_emb_size", ",", "lm_num_layers", "]", ")", "\n", "flattened_aggregated_lm_emb", "=", "tf", ".", "matmul", "(", "flattened_lm_emb", ",", "tf", ".", "expand_dims", "(", "self", ".", "lm_weights", ",", "1", ")", ")", "# [num_sentences * max_sentence_length * emb, 1]", "\n", "aggregated_lm_emb", "=", "tf", ".", "reshape", "(", "flattened_aggregated_lm_emb", ",", "[", "num_sentences", ",", "max_sentence_length", ",", "lm_emb_size", "]", ")", "\n", "aggregated_lm_emb", "*=", "self", ".", "lm_scaling", "\n", "context_emb_list", ".", "append", "(", "aggregated_lm_emb", ")", "\n", "\n", "context_emb", "=", "tf", ".", "concat", "(", "context_emb_list", ",", "2", ")", "# [num_sentences, max_sentence_length, emb]", "\n", "context_emb", "=", "tf", ".", "nn", ".", "dropout", "(", "context_emb", ",", "self", ".", "lexical_dropout", ")", "# [num_sentences, max_sentence_length, emb]", "\n", "\n", "text_len_mask", "=", "tf", ".", "sequence_mask", "(", "text_len", ",", "maxlen", "=", "max_sentence_length", ")", "# [num_sentence, max_sentence_length]", "\n", "\n", "candidate_scores_mask", "=", "tf", ".", "logical_and", "(", "tf", ".", "expand_dims", "(", "text_len_mask", ",", "[", "1", "]", ")", ",", "tf", ".", "expand_dims", "(", "text_len_mask", ",", "[", "2", "]", ")", ")", "#[num_sentence, max_sentence_length,max_sentence_length]", "\n", "sentence_ends_leq_starts", "=", "tf", ".", "tile", "(", "tf", ".", "expand_dims", "(", "tf", ".", "logical_not", "(", "tf", ".", "sequence_mask", "(", "tf", ".", "range", "(", "max_sentence_length", ")", ",", "max_sentence_length", ")", ")", ",", "0", ")", ",", "[", "num_sentences", ",", "1", ",", "1", "]", ")", "#[num_sentence, max_sentence_length,max_sentence_length]", "\n", "candidate_scores_mask", "=", "tf", ".", "logical_and", "(", "candidate_scores_mask", ",", "sentence_ends_leq_starts", ")", "\n", "\n", "flattened_candidate_scores_mask", "=", "tf", ".", "reshape", "(", "candidate_scores_mask", ",", "[", "-", "1", "]", ")", "#[num_sentence * max_sentence_length * max_sentence_length]", "\n", "\n", "\n", "context_outputs", "=", "self", ".", "lstm_contextualize", "(", "context_emb", ",", "text_len", ",", "self", ".", "lstm_dropout", ")", "# [num_sentence, max_sentence_length, emb]", "\n", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "\"candidate_starts_ffnn\"", ")", ":", "\n", "      ", "candidate_starts_emb", "=", "util", ".", "projection", "(", "context_outputs", ",", "self", ".", "config", "[", "\"ffnn_size\"", "]", ")", "#[num_sentences, max_sentences_length,emb]", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"candidate_ends_ffnn\"", ")", ":", "\n", "      ", "candidate_ends_emb", "=", "util", ".", "projection", "(", "context_outputs", ",", "self", ".", "config", "[", "\"ffnn_size\"", "]", ")", "#[num_sentences, max_sentences_length, emb]", "\n", "\n", "\n", "", "candidate_ner_scores", "=", "util", ".", "bilinear_classifier", "(", "candidate_starts_emb", ",", "candidate_ends_emb", ",", "self", ".", "dropout", ",", "output_size", "=", "self", ".", "num_types", "+", "1", ")", "#[num_sentence, max_sentence_length,max_sentence_length,types+1]", "\n", "candidate_ner_scores", "=", "tf", ".", "boolean_mask", "(", "tf", ".", "reshape", "(", "candidate_ner_scores", ",", "[", "-", "1", ",", "self", ".", "num_types", "+", "1", "]", ")", ",", "flattened_candidate_scores_mask", ")", "\n", "\n", "\n", "loss", "=", "tf", ".", "nn", ".", "sparse_softmax_cross_entropy_with_logits", "(", "labels", "=", "gold_labels", ",", "logits", "=", "candidate_ner_scores", ")", "\n", "loss", "=", "tf", ".", "reduce_sum", "(", "loss", ")", "\n", "\n", "\n", "return", "candidate_ner_scores", ",", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.biaffine_ner_model.BiaffineNERModel.get_pred_ner": [[242, 271], ["enumerate", "enumerate", "enumerate", "set", "xrange", "numpy.argmax", "sorted", "len", "xrange", "xrange", "top_spans[].append", "xrange", "len", "candidates.append", "len", "len", "sent_pred_mentions[].append", "enumerate"], "methods", ["None"], ["", "def", "get_pred_ner", "(", "self", ",", "sentences", ",", "span_scores", ",", "is_flat_ner", ")", ":", "\n", "    ", "candidates", "=", "[", "]", "\n", "for", "sid", ",", "sent", "in", "enumerate", "(", "sentences", ")", ":", "\n", "      ", "for", "s", "in", "xrange", "(", "len", "(", "sent", ")", ")", ":", "\n", "        ", "for", "e", "in", "xrange", "(", "s", ",", "len", "(", "sent", ")", ")", ":", "\n", "          ", "candidates", ".", "append", "(", "(", "sid", ",", "s", ",", "e", ")", ")", "\n", "\n", "", "", "", "top_spans", "=", "[", "[", "]", "for", "_", "in", "xrange", "(", "len", "(", "sentences", ")", ")", "]", "\n", "for", "i", ",", "type", "in", "enumerate", "(", "np", ".", "argmax", "(", "span_scores", ",", "axis", "=", "1", ")", ")", ":", "\n", "      ", "if", "type", ">", "0", ":", "\n", "        ", "sid", ",", "s", ",", "e", "=", "candidates", "[", "i", "]", "\n", "top_spans", "[", "sid", "]", ".", "append", "(", "(", "s", ",", "e", ",", "type", ",", "span_scores", "[", "i", ",", "type", "]", ")", ")", "\n", "\n", "\n", "", "", "top_spans", "=", "[", "sorted", "(", "top_span", ",", "reverse", "=", "True", ",", "key", "=", "lambda", "x", ":", "x", "[", "3", "]", ")", "for", "top_span", "in", "top_spans", "]", "\n", "sent_pred_mentions", "=", "[", "[", "]", "for", "_", "in", "xrange", "(", "len", "(", "sentences", ")", ")", "]", "\n", "for", "sid", ",", "top_span", "in", "enumerate", "(", "top_spans", ")", ":", "\n", "      ", "for", "ns", ",", "ne", ",", "t", ",", "_", "in", "top_span", ":", "\n", "        ", "for", "ts", ",", "te", ",", "_", "in", "sent_pred_mentions", "[", "sid", "]", ":", "\n", "          ", "if", "ns", "<", "ts", "<=", "ne", "<", "te", "or", "ts", "<", "ns", "<=", "te", "<", "ne", ":", "\n", "#for both nested and flat ner no clash is allowed", "\n", "            ", "break", "\n", "", "if", "is_flat_ner", "and", "(", "ns", "<=", "ts", "<=", "te", "<=", "ne", "or", "ts", "<=", "ns", "<=", "ne", "<=", "te", ")", ":", "\n", "#for flat ner nested mentions are not allowed", "\n", "            ", "break", "\n", "", "", "else", ":", "\n", "          ", "sent_pred_mentions", "[", "sid", "]", ".", "append", "(", "(", "ns", ",", "ne", ",", "t", ")", ")", "\n", "", "", "", "pred_mentions", "=", "set", "(", "(", "sid", ",", "s", ",", "e", ",", "t", ")", "for", "sid", ",", "spr", "in", "enumerate", "(", "sent_pred_mentions", ")", "for", "s", ",", "e", ",", "t", "in", "spr", ")", "\n", "return", "pred_mentions", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.biaffine_ner_model.BiaffineNERModel.load_eval_data": [[272, 282], ["print", "json.loads", "open", "biaffine_ner_model.BiaffineNERModel.tensorize_example", "biaffine_ner_model.BiaffineNERModel.load_eval_data.load_line"], "methods", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.biaffine_ner_model.BiaffineNERModel.tensorize_example"], ["", "def", "load_eval_data", "(", "self", ")", ":", "\n", "    ", "if", "self", ".", "eval_data", "is", "None", ":", "\n", "      ", "def", "load_line", "(", "line", ")", ":", "\n", "        ", "example", "=", "json", ".", "loads", "(", "line", ")", "\n", "return", "self", ".", "tensorize_example", "(", "example", ",", "is_training", "=", "False", ")", ",", "example", "\n", "\n", "", "with", "open", "(", "self", ".", "config", "[", "\"eval_path\"", "]", ")", "as", "f", ":", "\n", "        ", "self", ".", "eval_data", "=", "[", "load_line", "(", "l", ")", "for", "l", "in", "f", ".", "readlines", "(", ")", "]", "\n", "\n", "", "print", "(", "\"Loaded {} eval examples.\"", ".", "format", "(", "len", "(", "self", ".", "eval_data", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.biaffine_ner_model.BiaffineNERModel.evaluate": [[285, 349], ["biaffine_ner_model.BiaffineNERModel.load_eval_data", "time.time", "enumerate", "print", "print", "print", "print", "session.run", "sum", "set", "biaffine_ner_model.BiaffineNERModel.get_pred_ner", "len", "len", "len", "time.time", "print", "xrange", "util.make_summary", "xrange", "print", "float", "float", "print", "print", "print", "zip", "len", "set", "set", "len", "len", "len", "enumerate", "len", "float", "float"], "methods", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.biaffine_ner_model.BiaffineNERModel.load_eval_data", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.biaffine_ner_model.BiaffineNERModel.get_pred_ner", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.make_summary"], ["", "", "def", "evaluate", "(", "self", ",", "session", ",", "is_final_test", "=", "False", ")", ":", "\n", "    ", "self", ".", "load_eval_data", "(", ")", "\n", "\n", "tp", ",", "fn", ",", "fp", "=", "0", ",", "0", ",", "0", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "num_words", "=", "0", "\n", "sub_tp", ",", "sub_fn", ",", "sub_fp", "=", "[", "0", "]", "*", "self", ".", "num_types", ",", "[", "0", "]", "*", "self", ".", "num_types", ",", "[", "0", "]", "*", "self", ".", "num_types", "\n", "\n", "is_flat_ner", "=", "'flat_ner'", "in", "self", ".", "config", "and", "self", ".", "config", "[", "'flat_ner'", "]", "\n", "\n", "for", "example_num", ",", "(", "tensorized_example", ",", "example", ")", "in", "enumerate", "(", "self", ".", "eval_data", ")", ":", "\n", "      ", "feed_dict", "=", "{", "i", ":", "t", "for", "i", ",", "t", "in", "zip", "(", "self", ".", "input_tensors", ",", "tensorized_example", ")", "}", "\n", "candidate_ner_scores", "=", "session", ".", "run", "(", "self", ".", "predictions", ",", "feed_dict", "=", "feed_dict", ")", "\n", "\n", "num_words", "+=", "sum", "(", "len", "(", "tok", ")", "for", "tok", "in", "example", "[", "\"sentences\"", "]", ")", "\n", "\n", "\n", "gold_ners", "=", "set", "(", "[", "(", "sid", ",", "s", ",", "e", ",", "self", ".", "ner_maps", "[", "t", "]", ")", "for", "sid", ",", "ner", "in", "enumerate", "(", "example", "[", "'ners'", "]", ")", "for", "s", ",", "e", ",", "t", "in", "ner", "]", ")", "\n", "pred_ners", "=", "self", ".", "get_pred_ner", "(", "example", "[", "\"sentences\"", "]", ",", "candidate_ner_scores", ",", "is_flat_ner", ")", "\n", "\n", "tp", "+=", "len", "(", "gold_ners", "&", "pred_ners", ")", "\n", "fn", "+=", "len", "(", "gold_ners", "-", "pred_ners", ")", "\n", "fp", "+=", "len", "(", "pred_ners", "-", "gold_ners", ")", "\n", "\n", "if", "is_final_test", ":", "\n", "        ", "for", "i", "in", "xrange", "(", "self", ".", "num_types", ")", ":", "\n", "          ", "sub_gm", "=", "set", "(", "(", "sid", ",", "s", ",", "e", ")", "for", "sid", ",", "s", ",", "e", ",", "t", "in", "gold_ners", "if", "t", "==", "i", "+", "1", ")", "\n", "sub_pm", "=", "set", "(", "(", "sid", ",", "s", ",", "e", ")", "for", "sid", ",", "s", ",", "e", ",", "t", "in", "pred_ners", "if", "t", "==", "i", "+", "1", ")", "\n", "sub_tp", "[", "i", "]", "+=", "len", "(", "sub_gm", "&", "sub_pm", ")", "\n", "sub_fn", "[", "i", "]", "+=", "len", "(", "sub_gm", "-", "sub_pm", ")", "\n", "sub_fp", "[", "i", "]", "+=", "len", "(", "sub_pm", "-", "sub_gm", ")", "\n", "\n", "\n", "", "", "if", "example_num", "%", "10", "==", "0", ":", "\n", "        ", "print", "(", "\"Evaluated {}/{} examples.\"", ".", "format", "(", "example_num", "+", "1", ",", "len", "(", "self", ".", "eval_data", ")", ")", ")", "\n", "\n", "", "", "used_time", "=", "time", ".", "time", "(", ")", "-", "start_time", "\n", "print", "(", "\"Time used: %d second, %.2f w/s \"", "%", "(", "used_time", ",", "num_words", "*", "1.0", "/", "used_time", ")", ")", "\n", "\n", "m_r", "=", "0", "if", "tp", "==", "0", "else", "float", "(", "tp", ")", "/", "(", "tp", "+", "fn", ")", "\n", "m_p", "=", "0", "if", "tp", "==", "0", "else", "float", "(", "tp", ")", "/", "(", "tp", "+", "fp", ")", "\n", "m_f1", "=", "0", "if", "m_p", "==", "0", "else", "2.0", "*", "m_r", "*", "m_p", "/", "(", "m_r", "+", "m_p", ")", "\n", "\n", "print", "(", "\"Mention F1: {:.2f}%\"", ".", "format", "(", "m_f1", "*", "100", ")", ")", "\n", "print", "(", "\"Mention recall: {:.2f}%\"", ".", "format", "(", "m_r", "*", "100", ")", ")", "\n", "print", "(", "\"Mention precision: {:.2f}%\"", ".", "format", "(", "m_p", "*", "100", ")", ")", "\n", "\n", "if", "is_final_test", ":", "\n", "      ", "print", "(", "\"****************SUB NER TYPES********************\"", ")", "\n", "for", "i", "in", "xrange", "(", "self", ".", "num_types", ")", ":", "\n", "        ", "sub_r", "=", "0", "if", "sub_tp", "[", "i", "]", "==", "0", "else", "float", "(", "sub_tp", "[", "i", "]", ")", "/", "(", "sub_tp", "[", "i", "]", "+", "sub_fn", "[", "i", "]", ")", "\n", "sub_p", "=", "0", "if", "sub_tp", "[", "i", "]", "==", "0", "else", "float", "(", "sub_tp", "[", "i", "]", ")", "/", "(", "sub_tp", "[", "i", "]", "+", "sub_fp", "[", "i", "]", ")", "\n", "sub_f1", "=", "0", "if", "sub_p", "==", "0", "else", "2.0", "*", "sub_r", "*", "sub_p", "/", "(", "sub_r", "+", "sub_p", ")", "\n", "\n", "print", "(", "\"{} F1: {:.2f}%\"", ".", "format", "(", "self", ".", "ner_types", "[", "i", "]", ",", "sub_f1", "*", "100", ")", ")", "\n", "print", "(", "\"{} recall: {:.2f}%\"", ".", "format", "(", "self", ".", "ner_types", "[", "i", "]", ",", "sub_r", "*", "100", ")", ")", "\n", "print", "(", "\"{} precision: {:.2f}%\"", ".", "format", "(", "self", ".", "ner_types", "[", "i", "]", ",", "sub_p", "*", "100", ")", ")", "\n", "\n", "", "", "summary_dict", "=", "{", "}", "\n", "summary_dict", "[", "\"Mention F1\"", "]", "=", "m_f1", "\n", "summary_dict", "[", "\"Mention recall\"", "]", "=", "m_r", "\n", "summary_dict", "[", "\"Mention precision\"", "]", "=", "m_p", "\n", "\n", "return", "util", ".", "make_summary", "(", "summary_dict", ")", ",", "m_f1", "\n", "", "", ""]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.EmbeddingDictionary.__init__": [[226, 235], ["util.EmbeddingDictionary.load_embedding_dict"], "methods", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.EmbeddingDictionary.load_embedding_dict"], ["  ", "def", "__init__", "(", "self", ",", "info", ",", "normalize", "=", "True", ",", "maybe_cache", "=", "None", ")", ":", "\n", "    ", "self", ".", "_size", "=", "info", "[", "\"size\"", "]", "\n", "self", ".", "_normalize", "=", "normalize", "\n", "self", ".", "_path", "=", "info", "[", "\"path\"", "]", "\n", "if", "maybe_cache", "is", "not", "None", "and", "maybe_cache", ".", "_path", "==", "self", ".", "_path", ":", "\n", "      ", "assert", "self", ".", "_size", "==", "maybe_cache", ".", "_size", "\n", "self", ".", "_embeddings", "=", "maybe_cache", ".", "_embeddings", "\n", "", "else", ":", "\n", "      ", "self", ".", "_embeddings", "=", "self", ".", "load_embedding_dict", "(", "self", ".", "_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.EmbeddingDictionary.size": [[236, 239], ["None"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "size", "(", "self", ")", ":", "\n", "    ", "return", "self", ".", "_size", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.EmbeddingDictionary.load_embedding_dict": [[240, 257], ["print", "numpy.zeros", "collections.defaultdict", "len", "print", "open", "enumerate", "f.readlines", "line.find", "numpy.fromstring", "len", "len"], "methods", ["None"], ["", "def", "load_embedding_dict", "(", "self", ",", "path", ")", ":", "\n", "    ", "print", "(", "\"Loading word embeddings from {}...\"", ".", "format", "(", "path", ")", ")", "\n", "default_embedding", "=", "np", ".", "zeros", "(", "self", ".", "size", ")", "\n", "embedding_dict", "=", "collections", ".", "defaultdict", "(", "lambda", ":", "default_embedding", ")", "\n", "if", "len", "(", "path", ")", ">", "0", ":", "\n", "      ", "vocab_size", "=", "None", "\n", "with", "open", "(", "path", ")", "as", "f", ":", "\n", "        ", "for", "i", ",", "line", "in", "enumerate", "(", "f", ".", "readlines", "(", ")", ")", ":", "\n", "          ", "word_end", "=", "line", ".", "find", "(", "\" \"", ")", "\n", "word", "=", "line", "[", ":", "word_end", "]", "\n", "embedding", "=", "np", ".", "fromstring", "(", "line", "[", "word_end", "+", "1", ":", "]", ",", "np", ".", "float32", ",", "sep", "=", "\" \"", ")", "\n", "assert", "len", "(", "embedding", ")", "==", "self", ".", "size", "\n", "embedding_dict", "[", "word", "]", "=", "embedding", "\n", "", "", "if", "vocab_size", "is", "not", "None", ":", "\n", "        ", "assert", "vocab_size", "==", "len", "(", "embedding_dict", ")", "\n", "", "print", "(", "\"Done loading word embeddings.\"", ")", "\n", "", "return", "embedding_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.EmbeddingDictionary.is_in_embeddings": [[258, 260], ["util.EmbeddingDictionary._embeddings.has_key"], "methods", ["None"], ["", "def", "is_in_embeddings", "(", "self", ",", "key", ")", ":", "\n", "    ", "return", "self", ".", "_embeddings", ".", "has_key", "(", "key", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.EmbeddingDictionary.__getitem__": [[261, 266], ["util.EmbeddingDictionary.normalize"], "methods", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.EmbeddingDictionary.normalize"], ["", "def", "__getitem__", "(", "self", ",", "key", ")", ":", "\n", "    ", "embedding", "=", "self", ".", "_embeddings", "[", "key", "]", "\n", "if", "self", ".", "_normalize", ":", "\n", "      ", "embedding", "=", "self", ".", "normalize", "(", "embedding", ")", "\n", "", "return", "embedding", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.EmbeddingDictionary.normalize": [[267, 273], ["numpy.linalg.norm"], "methods", ["None"], ["", "def", "normalize", "(", "self", ",", "v", ")", ":", "\n", "    ", "norm", "=", "np", ".", "linalg", ".", "norm", "(", "v", ")", "\n", "if", "norm", ">", "0", ":", "\n", "      ", "return", "v", "/", "norm", "\n", "", "else", ":", "\n", "      ", "return", "v", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.CustomLSTMCell.__init__": [[275, 283], ["tensorflow.nn.dropout", "util.CustomLSTMCell._block_orthonormal_initializer", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.contrib.rnn.LSTMStateTuple", "tensorflow.ones"], "methods", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.dropout", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.CustomLSTMCell._block_orthonormal_initializer"], ["  ", "def", "__init__", "(", "self", ",", "num_units", ",", "batch_size", ",", "dropout", ")", ":", "\n", "    ", "self", ".", "_num_units", "=", "num_units", "\n", "self", ".", "_dropout", "=", "dropout", "\n", "self", ".", "_dropout_mask", "=", "tf", ".", "nn", ".", "dropout", "(", "tf", ".", "ones", "(", "[", "batch_size", ",", "self", ".", "output_size", "]", ")", ",", "dropout", ")", "\n", "self", ".", "_initializer", "=", "self", ".", "_block_orthonormal_initializer", "(", "[", "self", ".", "output_size", "]", "*", "3", ")", "\n", "initial_cell_state", "=", "tf", ".", "get_variable", "(", "\"lstm_initial_cell_state\"", ",", "[", "1", ",", "self", ".", "output_size", "]", ")", "\n", "initial_hidden_state", "=", "tf", ".", "get_variable", "(", "\"lstm_initial_hidden_state\"", ",", "[", "1", ",", "self", ".", "output_size", "]", ")", "\n", "self", ".", "_initial_state", "=", "tf", ".", "contrib", ".", "rnn", ".", "LSTMStateTuple", "(", "initial_cell_state", ",", "initial_hidden_state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.CustomLSTMCell.state_size": [[284, 287], ["tensorflow.contrib.rnn.LSTMStateTuple"], "methods", ["None"], ["", "@", "property", "\n", "def", "state_size", "(", "self", ")", ":", "\n", "    ", "return", "tf", ".", "contrib", ".", "rnn", ".", "LSTMStateTuple", "(", "self", ".", "output_size", ",", "self", ".", "output_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.CustomLSTMCell.output_size": [[288, 291], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "output_size", "(", "self", ")", ":", "\n", "    ", "return", "self", ".", "_num_units", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.CustomLSTMCell.initial_state": [[292, 295], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "initial_state", "(", "self", ")", ":", "\n", "    ", "return", "self", ".", "_initial_state", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.CustomLSTMCell.__call__": [[296, 308], ["tensorflow.variable_scope", "util.projection", "tensorflow.split", "tensorflow.sigmoid", "tensorflow.contrib.rnn.LSTMStateTuple", "tensorflow.concat", "tensorflow.tanh", "tensorflow.sigmoid", "tensorflow.tanh", "type"], "methods", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.projection"], ["", "def", "__call__", "(", "self", ",", "inputs", ",", "state", ",", "scope", "=", "None", ")", ":", "\n", "    ", "\"\"\"Long short-term memory cell (LSTM).\"\"\"", "\n", "with", "tf", ".", "variable_scope", "(", "scope", "or", "type", "(", "self", ")", ".", "__name__", ")", ":", "# \"CustomLSTMCell\"", "\n", "      ", "c", ",", "h", "=", "state", "\n", "h", "*=", "self", ".", "_dropout_mask", "\n", "concat", "=", "projection", "(", "tf", ".", "concat", "(", "[", "inputs", ",", "h", "]", ",", "1", ")", ",", "3", "*", "self", ".", "output_size", ",", "initializer", "=", "self", ".", "_initializer", ")", "\n", "i", ",", "j", ",", "o", "=", "tf", ".", "split", "(", "concat", ",", "num_or_size_splits", "=", "3", ",", "axis", "=", "1", ")", "\n", "i", "=", "tf", ".", "sigmoid", "(", "i", ")", "\n", "new_c", "=", "(", "1", "-", "i", ")", "*", "c", "+", "i", "*", "tf", ".", "tanh", "(", "j", ")", "\n", "new_h", "=", "tf", ".", "tanh", "(", "new_c", ")", "*", "tf", ".", "sigmoid", "(", "o", ")", "\n", "new_state", "=", "tf", ".", "contrib", ".", "rnn", ".", "LSTMStateTuple", "(", "new_c", ",", "new_h", ")", "\n", "return", "new_h", ",", "new_state", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.CustomLSTMCell._orthonormal_initializer": [[309, 321], ["numpy.random.randn().astype", "numpy.random.randn().astype", "numpy.linalg.qr", "numpy.linalg.qr", "min", "numpy.sign", "numpy.sign", "numpy.dot", "numpy.random.randn", "numpy.random.randn", "numpy.diag", "numpy.diag"], "methods", ["None"], ["", "", "def", "_orthonormal_initializer", "(", "self", ",", "scale", "=", "1.0", ")", ":", "\n", "    ", "def", "_initializer", "(", "shape", ",", "dtype", "=", "tf", ".", "float32", ",", "partition_info", "=", "None", ")", ":", "\n", "      ", "M1", "=", "np", ".", "random", ".", "randn", "(", "shape", "[", "0", "]", ",", "shape", "[", "0", "]", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "M2", "=", "np", ".", "random", ".", "randn", "(", "shape", "[", "1", "]", ",", "shape", "[", "1", "]", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "Q1", ",", "R1", "=", "np", ".", "linalg", ".", "qr", "(", "M1", ")", "\n", "Q2", ",", "R2", "=", "np", ".", "linalg", ".", "qr", "(", "M2", ")", "\n", "Q1", "=", "Q1", "*", "np", ".", "sign", "(", "np", ".", "diag", "(", "R1", ")", ")", "\n", "Q2", "=", "Q2", "*", "np", ".", "sign", "(", "np", ".", "diag", "(", "R2", ")", ")", "\n", "n_min", "=", "min", "(", "shape", "[", "0", "]", ",", "shape", "[", "1", "]", ")", "\n", "params", "=", "np", ".", "dot", "(", "Q1", "[", ":", ",", ":", "n_min", "]", ",", "Q2", "[", ":", "n_min", ",", ":", "]", ")", "*", "scale", "\n", "return", "params", "\n", "", "return", "_initializer", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.CustomLSTMCell._block_orthonormal_initializer": [[322, 330], ["util.CustomLSTMCell._orthonormal_initializer", "numpy.concatenate", "len", "sum", "util.CustomLSTMCell."], "methods", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.CustomLSTMCell._orthonormal_initializer"], ["", "def", "_block_orthonormal_initializer", "(", "self", ",", "output_sizes", ")", ":", "\n", "    ", "def", "_initializer", "(", "shape", ",", "dtype", "=", "np", ".", "float32", ",", "partition_info", "=", "None", ")", ":", "\n", "      ", "assert", "len", "(", "shape", ")", "==", "2", "\n", "assert", "sum", "(", "output_sizes", ")", "==", "shape", "[", "1", "]", "\n", "initializer", "=", "self", ".", "_orthonormal_initializer", "(", ")", "\n", "params", "=", "np", ".", "concatenate", "(", "[", "initializer", "(", "[", "shape", "[", "0", "]", ",", "o", "]", ",", "dtype", ",", "partition_info", ")", "for", "o", "in", "output_sizes", "]", ",", "1", ")", "\n", "return", "params", "\n", "", "return", "_initializer", "\n", "", "", ""]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.initialize_from_env": [[16, 25], ["print", "util.mkdirs", "print", "pyhocon.ConfigFactory.parse_file", "os.path.join", "pyhocon.HOCONConverter.convert"], "function", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.mkdirs"], ["def", "initialize_from_env", "(", ")", ":", "\n", "  ", "name", "=", "sys", ".", "argv", "[", "1", "]", "\n", "print", "(", "\"Running experiment: {}\"", ".", "format", "(", "name", ")", ")", "\n", "\n", "config", "=", "pyhocon", ".", "ConfigFactory", ".", "parse_file", "(", "\"experiments.conf\"", ")", "[", "name", "]", "\n", "config", "[", "\"log_dir\"", "]", "=", "mkdirs", "(", "os", ".", "path", ".", "join", "(", "config", "[", "\"log_root\"", "]", ",", "name", ")", ")", "\n", "\n", "print", "(", "pyhocon", ".", "HOCONConverter", ".", "convert", "(", "config", ",", "\"hocon\"", ")", ")", "\n", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.copy_checkpoint": [[26, 29], ["shutil.copyfile"], "function", ["None"], ["", "def", "copy_checkpoint", "(", "source", ",", "target", ")", ":", "\n", "  ", "for", "ext", "in", "(", "\".index\"", ",", "\".data-00000-of-00001\"", ")", ":", "\n", "    ", "shutil", ".", "copyfile", "(", "source", "+", "ext", ",", "target", "+", "ext", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.make_summary": [[30, 32], ["tensorflow.Summary", "tensorflow.Summary.Value", "value_dict.items"], "function", ["None"], ["", "", "def", "make_summary", "(", "value_dict", ")", ":", "\n", "  ", "return", "tf", ".", "Summary", "(", "value", "=", "[", "tf", ".", "Summary", ".", "Value", "(", "tag", "=", "k", ",", "simple_value", "=", "v", ")", "for", "k", ",", "v", "in", "value_dict", ".", "items", "(", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.flatten": [[33, 35], ["None"], "function", ["None"], ["", "def", "flatten", "(", "l", ")", ":", "\n", "  ", "return", "[", "item", "for", "sublist", "in", "l", "for", "item", "in", "sublist", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.mkdirs": [[37, 44], ["os.makedirs"], "function", ["None"], ["", "def", "mkdirs", "(", "path", ")", ":", "\n", "  ", "try", ":", "\n", "    ", "os", ".", "makedirs", "(", "path", ")", "\n", "", "except", "OSError", "as", "exception", ":", "\n", "    ", "if", "exception", ".", "errno", "!=", "errno", ".", "EEXIST", ":", "\n", "      ", "raise", "\n", "", "", "return", "path", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.load_char_dict": [[45, 52], ["collections.defaultdict", "collections.defaultdict.update", "codecs.open", "vocab.extend", "l.strip", "enumerate", "f.readlines"], "function", ["None"], ["", "def", "load_char_dict", "(", "char_vocab_path", ")", ":", "\n", "  ", "vocab", "=", "[", "u\"<unk>\"", "]", "\n", "with", "codecs", ".", "open", "(", "char_vocab_path", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "    ", "vocab", ".", "extend", "(", "l", ".", "strip", "(", ")", "for", "l", "in", "f", ".", "readlines", "(", ")", ")", "\n", "", "char_dict", "=", "collections", ".", "defaultdict", "(", "int", ")", "\n", "char_dict", ".", "update", "(", "{", "c", ":", "i", "for", "i", ",", "c", "in", "enumerate", "(", "vocab", ")", "}", ")", "\n", "return", "char_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.maybe_divide": [[53, 55], ["float"], "function", ["None"], ["", "def", "maybe_divide", "(", "x", ",", "y", ")", ":", "\n", "  ", "return", "0", "if", "y", "==", "0", "else", "x", "/", "float", "(", "y", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.projection": [[56, 58], ["util.ffnn"], "function", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.ffnn"], ["", "def", "projection", "(", "inputs", ",", "output_size", ",", "initializer", "=", "None", ")", ":", "\n", "  ", "return", "ffnn", "(", "inputs", ",", "0", ",", "-", "1", ",", "output_size", ",", "dropout", "=", "None", ",", "output_weights_initializer", "=", "initializer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.highway": [[59, 69], ["range", "tensorflow.variable_scope", "tensorflow.split", "tensorflow.sigmoid", "tensorflow.nn.relu", "util.projection", "tensorflow.nn.dropout", "util.shape"], "function", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.projection", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.dropout", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.shape"], ["", "def", "highway", "(", "inputs", ",", "num_layers", ",", "dropout", ")", ":", "\n", "  ", "for", "i", "in", "range", "(", "num_layers", ")", ":", "\n", "    ", "with", "tf", ".", "variable_scope", "(", "\"highway_{}\"", ".", "format", "(", "i", ")", ")", ":", "\n", "      ", "j", ",", "f", "=", "tf", ".", "split", "(", "projection", "(", "inputs", ",", "2", "*", "shape", "(", "inputs", ",", "-", "1", ")", ")", ",", "2", ",", "-", "1", ")", "\n", "f", "=", "tf", ".", "sigmoid", "(", "f", ")", "\n", "j", "=", "tf", ".", "nn", ".", "relu", "(", "j", ")", "\n", "if", "dropout", "is", "not", "None", ":", "\n", "        ", "j", "=", "tf", ".", "nn", ".", "dropout", "(", "j", ",", "dropout", ")", "\n", "", "inputs", "=", "f", "*", "j", "+", "(", "1", "-", "f", ")", "*", "inputs", "\n", "", "", "return", "inputs", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.shape": [[70, 72], ["tensorflow.shape", "x.get_shape"], "function", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.shape"], ["", "def", "shape", "(", "x", ",", "dim", ")", ":", "\n", "  ", "return", "x", ".", "get_shape", "(", ")", "[", "dim", "]", ".", "value", "or", "tf", ".", "shape", "(", "x", ")", "[", "dim", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.ffnn": [[73, 101], ["range", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.nn.xw_plus_b", "len", "ValueError", "len", "util.shape", "util.shape", "util.shape", "tensorflow.reshape", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.nn.relu", "len", "tensorflow.reshape", "inputs.get_shape", "inputs.get_shape", "tensorflow.nn.xw_plus_b", "tensorflow.nn.dropout", "util.shape", "inputs.get_shape", "len", "util.shape", "inputs.get_shape"], "function", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.shape", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.shape", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.shape", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.dropout", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.shape", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.shape"], ["", "def", "ffnn", "(", "inputs", ",", "num_hidden_layers", ",", "hidden_size", ",", "output_size", ",", "dropout", ",", "output_weights_initializer", "=", "None", ")", ":", "\n", "  ", "if", "len", "(", "inputs", ".", "get_shape", "(", ")", ")", ">", "3", ":", "\n", "    ", "raise", "ValueError", "(", "\"FFNN with rank {} not supported\"", ".", "format", "(", "len", "(", "inputs", ".", "get_shape", "(", ")", ")", ")", ")", "\n", "\n", "", "if", "len", "(", "inputs", ".", "get_shape", "(", ")", ")", "==", "3", ":", "\n", "    ", "batch_size", "=", "shape", "(", "inputs", ",", "0", ")", "\n", "seqlen", "=", "shape", "(", "inputs", ",", "1", ")", "\n", "emb_size", "=", "shape", "(", "inputs", ",", "2", ")", "\n", "current_inputs", "=", "tf", ".", "reshape", "(", "inputs", ",", "[", "batch_size", "*", "seqlen", ",", "emb_size", "]", ")", "\n", "", "else", ":", "\n", "    ", "current_inputs", "=", "inputs", "\n", "\n", "", "for", "i", "in", "range", "(", "num_hidden_layers", ")", ":", "\n", "    ", "hidden_weights", "=", "tf", ".", "get_variable", "(", "\"hidden_weights_{}\"", ".", "format", "(", "i", ")", ",", "[", "shape", "(", "current_inputs", ",", "1", ")", ",", "hidden_size", "]", ")", "\n", "hidden_bias", "=", "tf", ".", "get_variable", "(", "\"hidden_bias_{}\"", ".", "format", "(", "i", ")", ",", "[", "hidden_size", "]", ")", "\n", "current_outputs", "=", "tf", ".", "nn", ".", "relu", "(", "tf", ".", "nn", ".", "xw_plus_b", "(", "current_inputs", ",", "hidden_weights", ",", "hidden_bias", ")", ")", "\n", "\n", "if", "dropout", "is", "not", "None", ":", "\n", "      ", "current_outputs", "=", "tf", ".", "nn", ".", "dropout", "(", "current_outputs", ",", "dropout", ")", "\n", "", "current_inputs", "=", "current_outputs", "\n", "\n", "", "output_weights", "=", "tf", ".", "get_variable", "(", "\"output_weights\"", ",", "[", "shape", "(", "current_inputs", ",", "1", ")", ",", "output_size", "]", ",", "initializer", "=", "output_weights_initializer", ")", "\n", "output_bias", "=", "tf", ".", "get_variable", "(", "\"output_bias\"", ",", "[", "output_size", "]", ")", "\n", "outputs", "=", "tf", ".", "nn", ".", "xw_plus_b", "(", "current_inputs", ",", "output_weights", ",", "output_bias", ")", "\n", "\n", "if", "len", "(", "inputs", ".", "get_shape", "(", ")", ")", "==", "3", ":", "\n", "    ", "outputs", "=", "tf", ".", "reshape", "(", "outputs", ",", "[", "batch_size", ",", "seqlen", ",", "output_size", "]", ")", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.cnn": [[102, 114], ["util.shape", "enumerate", "tensorflow.concat", "tensorflow.nn.conv1d", "tensorflow.nn.relu", "tensorflow.reduce_max", "outputs.append", "tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.get_variable", "tensorflow.nn.bias_add"], "function", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.shape"], ["", "def", "cnn", "(", "inputs", ",", "filter_sizes", ",", "num_filters", ")", ":", "\n", "  ", "input_size", "=", "shape", "(", "inputs", ",", "2", ")", "\n", "outputs", "=", "[", "]", "\n", "for", "i", ",", "filter_size", "in", "enumerate", "(", "filter_sizes", ")", ":", "\n", "    ", "with", "tf", ".", "variable_scope", "(", "\"conv_{}\"", ".", "format", "(", "i", ")", ")", ":", "\n", "      ", "w", "=", "tf", ".", "get_variable", "(", "\"w\"", ",", "[", "filter_size", ",", "input_size", ",", "num_filters", "]", ")", "\n", "b", "=", "tf", ".", "get_variable", "(", "\"b\"", ",", "[", "num_filters", "]", ")", "\n", "", "conv", "=", "tf", ".", "nn", ".", "conv1d", "(", "inputs", ",", "w", ",", "stride", "=", "1", ",", "padding", "=", "\"VALID\"", ")", "# [num_words, num_chars - filter_size, num_filters]", "\n", "h", "=", "tf", ".", "nn", ".", "relu", "(", "tf", ".", "nn", ".", "bias_add", "(", "conv", ",", "b", ")", ")", "# [num_words, num_chars - filter_size, num_filters]", "\n", "pooled", "=", "tf", ".", "reduce_max", "(", "h", ",", "1", ")", "# [num_words, num_filters]", "\n", "outputs", ".", "append", "(", "pooled", ")", "\n", "", "return", "tf", ".", "concat", "(", "outputs", ",", "1", ")", "# [num_words, num_filters * len(filter_sizes)]", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.bilinear_classifier": [[115, 140], ["tensorflow.nn.dropout", "tensorflow.nn.dropout", "util.biaffine_mapping", "tf.nn.dropout.get_shape().as_list", "tensorflow.shape", "tensorflow.squeeze", "tensorflow.transpose", "tensorflow.zeros_initializer", "tf.nn.dropout.get_shape"], "function", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.dropout", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.dropout", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.biaffine_mapping", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.shape"], ["", "def", "bilinear_classifier", "(", "x_bnv", ",", "y_bnv", ",", "keep_prob", ",", "output_size", "=", "1", ",", "add_bias_1", "=", "True", ",", "add_bias_2", "=", "True", ")", ":", "\n", "  ", "\"\"\"biaffine_mapping() with dropout.\"\"\"", "\n", "\n", "\n", "# Statically known input dimensions.", "\n", "input_size", "=", "x_bnv", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "-", "1", "]", "\n", "\n", "# Dynamically known input dimensions", "\n", "batch_size", "=", "tf", ".", "shape", "(", "x_bnv", ")", "[", "0", "]", "\n", "noise_shape", "=", "[", "batch_size", ",", "1", ",", "input_size", "]", "\n", "x_bnv", "=", "tf", ".", "nn", ".", "dropout", "(", "x_bnv", ",", "keep_prob", ",", "noise_shape", "=", "noise_shape", ")", "\n", "y_bnv", "=", "tf", ".", "nn", ".", "dropout", "(", "y_bnv", ",", "keep_prob", ",", "noise_shape", "=", "noise_shape", ")", "\n", "\n", "biaffine", "=", "biaffine_mapping", "(", "\n", "x_bnv", ",", "\n", "y_bnv", ",", "\n", "output_size", ",", "\n", "add_bias_1", "=", "add_bias_1", ",", "\n", "add_bias_2", "=", "add_bias_2", ",", "\n", "initializer", "=", "tf", ".", "zeros_initializer", "(", ")", ")", "\n", "if", "output_size", "==", "1", ":", "\n", "    ", "output", "=", "tf", ".", "squeeze", "(", "biaffine", ",", "axis", "=", "2", ")", "\n", "", "else", ":", "\n", "    ", "output", "=", "tf", ".", "transpose", "(", "biaffine", ",", "[", "0", ",", "1", ",", "3", ",", "2", "]", ")", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.biaffine_mapping": [[141, 224], ["tensorflow.variable_scope", "tensorflow.get_variable", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.matmul", "tensorflow.reshape", "tensorflow.matmul", "tensorflow.reshape", "tensorflow.shape", "tensorflow.shape", "tensorflow.concat", "tensorflow.concat", "tf.concat.get_shape().as_list", "tf.concat.get_shape().as_list", "tensorflow.orthogonal_initializer", "tensorflow.ones", "tensorflow.ones", "tf.concat.get_shape", "tf.concat.get_shape"], "function", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.shape", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.shape"], ["", "def", "biaffine_mapping", "(", "vector_set_1", ",", "\n", "vector_set_2", ",", "\n", "output_size", ",", "\n", "add_bias_1", "=", "True", ",", "\n", "add_bias_2", "=", "True", ",", "\n", "initializer", "=", "None", ")", ":", "\n", "  ", "\"\"\"Bilinear mapping: maps two vector spaces to a third vector space.\n\n  The input vector spaces are two 3d matrices: batch size x bucket size x values\n  A typical application of the function is to compute a square matrix\n  representing a dependency tree. The output is for each bucket a square\n  matrix of the form [bucket size, output size, bucket size]. If the output size\n  is set to 1 then results is [bucket size, 1, bucket size] equivalent to\n  a square matrix where the bucket for instance represent the tokens on\n  the x-axis and y-axis. In this way represent the adjacency matrix of a\n  dependency graph (see https://arxiv.org/abs/1611.01734).\n\n  Args:\n     vector_set_1: vectors of space one\n     vector_set_2: vectors of space two\n     output_size: number of output labels (e.g. edge labels)\n     add_bias_1: Whether to add a bias for input one\n     add_bias_2: Whether to add a bias for input two\n     initializer: Initializer for the bilinear weight map\n\n  Returns:\n    Output vector space as 4d matrix:\n    batch size x bucket size x output size x bucket size\n    The output could represent an unlabeled dependency tree when\n    the output size is 1 or a labeled tree otherwise.\n\n  \"\"\"", "\n", "with", "tf", ".", "variable_scope", "(", "'Bilinear'", ")", ":", "\n", "# Dynamic shape info", "\n", "    ", "batch_size", "=", "tf", ".", "shape", "(", "vector_set_1", ")", "[", "0", "]", "\n", "bucket_size", "=", "tf", ".", "shape", "(", "vector_set_1", ")", "[", "1", "]", "\n", "\n", "if", "add_bias_1", ":", "\n", "      ", "vector_set_1", "=", "tf", ".", "concat", "(", "\n", "[", "vector_set_1", ",", "tf", ".", "ones", "(", "[", "batch_size", ",", "bucket_size", ",", "1", "]", ")", "]", ",", "axis", "=", "2", ")", "\n", "", "if", "add_bias_2", ":", "\n", "      ", "vector_set_2", "=", "tf", ".", "concat", "(", "\n", "[", "vector_set_2", ",", "tf", ".", "ones", "(", "[", "batch_size", ",", "bucket_size", ",", "1", "]", ")", "]", ",", "axis", "=", "2", ")", "\n", "\n", "# Static shape info", "\n", "", "vector_set_1_size", "=", "vector_set_1", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "-", "1", "]", "\n", "vector_set_2_size", "=", "vector_set_2", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "-", "1", "]", "\n", "\n", "if", "not", "initializer", ":", "\n", "      ", "initializer", "=", "tf", ".", "orthogonal_initializer", "(", ")", "\n", "\n", "# Mapping matrix", "\n", "", "bilinear_map", "=", "tf", ".", "get_variable", "(", "\n", "'bilinear_map'", ",", "[", "vector_set_1_size", ",", "output_size", ",", "vector_set_2_size", "]", ",", "\n", "initializer", "=", "initializer", ")", "\n", "\n", "# The matrix operations and reshapings for bilinear mapping.", "\n", "# b: batch size (batch of buckets)", "\n", "# v1, v2: values (size of vectors)", "\n", "# n: tokens (size of bucket)", "\n", "# r: labels (output size), e.g. 1 if unlabeled or number of edge labels.", "\n", "\n", "# [b, n, v1] -> [b*n, v1]", "\n", "vector_set_1", "=", "tf", ".", "reshape", "(", "vector_set_1", ",", "[", "-", "1", ",", "vector_set_1_size", "]", ")", "\n", "\n", "# [v1, r, v2] -> [v1, r*v2]", "\n", "bilinear_map", "=", "tf", ".", "reshape", "(", "bilinear_map", ",", "[", "vector_set_1_size", ",", "-", "1", "]", ")", "\n", "\n", "# [b*n, v1] x [v1, r*v2] -> [b*n, r*v2]", "\n", "bilinear_mapping", "=", "tf", ".", "matmul", "(", "vector_set_1", ",", "bilinear_map", ")", "\n", "\n", "# [b*n, r*v2] -> [b, n*r, v2]", "\n", "bilinear_mapping", "=", "tf", ".", "reshape", "(", "\n", "bilinear_mapping", ",", "\n", "[", "batch_size", ",", "bucket_size", "*", "output_size", ",", "vector_set_2_size", "]", ")", "\n", "\n", "# [b, n*r, v2] x [b, n, v2]T -> [b, n*r, n]", "\n", "bilinear_mapping", "=", "tf", ".", "matmul", "(", "bilinear_mapping", ",", "vector_set_2", ",", "adjoint_b", "=", "True", ")", "\n", "\n", "# [b, n*r, n] -> [b, n, r, n]", "\n", "bilinear_mapping", "=", "tf", ".", "reshape", "(", "\n", "bilinear_mapping", ",", "[", "batch_size", ",", "bucket_size", ",", "output_size", ",", "bucket_size", "]", ")", "\n", "return", "bilinear_mapping", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.BertConfig.__init__": [[33, 80], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "vocab_size", ",", "\n", "hidden_size", "=", "768", ",", "\n", "num_hidden_layers", "=", "12", ",", "\n", "num_attention_heads", "=", "12", ",", "\n", "intermediate_size", "=", "3072", ",", "\n", "hidden_act", "=", "\"gelu\"", ",", "\n", "hidden_dropout_prob", "=", "0.1", ",", "\n", "attention_probs_dropout_prob", "=", "0.1", ",", "\n", "max_position_embeddings", "=", "512", ",", "\n", "type_vocab_size", "=", "16", ",", "\n", "initializer_range", "=", "0.02", ")", ":", "\n", "        ", "\"\"\"Constructs BertConfig.\n\n        Args:\n          vocab_size: Vocabulary size of `inputs_ids` in `BertModel`.\n          hidden_size: Size of the encoder layers and the pooler layer.\n          num_hidden_layers: Number of hidden layers in the Transformer encoder.\n          num_attention_heads: Number of attention heads for each attention layer in\n            the Transformer encoder.\n          intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n            layer in the Transformer encoder.\n          hidden_act: The non-linear activation function (function or string) in the\n            encoder and pooler.\n          hidden_dropout_prob: The dropout probability for all fully connected\n            layers in the embeddings, encoder, and pooler.\n          attention_probs_dropout_prob: The dropout ratio for the attention\n            probabilities.\n          max_position_embeddings: The maximum sequence length that this model might\n            ever be used with. Typically set this to something large just in case\n            (e.g., 512 or 1024 or 2048).\n          type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n            `BertModel`.\n          initializer_range: The stdev of the truncated_normal_initializer for\n            initializing all weight matrices.\n        \"\"\"", "\n", "self", ".", "vocab_size", "=", "vocab_size", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "num_hidden_layers", "=", "num_hidden_layers", "\n", "self", ".", "num_attention_heads", "=", "num_attention_heads", "\n", "self", ".", "hidden_act", "=", "hidden_act", "\n", "self", ".", "intermediate_size", "=", "intermediate_size", "\n", "self", ".", "hidden_dropout_prob", "=", "hidden_dropout_prob", "\n", "self", ".", "attention_probs_dropout_prob", "=", "attention_probs_dropout_prob", "\n", "self", ".", "max_position_embeddings", "=", "max_position_embeddings", "\n", "self", ".", "type_vocab_size", "=", "type_vocab_size", "\n", "self", ".", "initializer_range", "=", "initializer_range", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.BertConfig.from_dict": [[81, 88], ["modeling.BertConfig", "six.iteritems"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "from_dict", "(", "cls", ",", "json_object", ")", ":", "\n", "        ", "\"\"\"Constructs a `BertConfig` from a Python dictionary of parameters.\"\"\"", "\n", "config", "=", "BertConfig", "(", "vocab_size", "=", "None", ")", "\n", "for", "(", "key", ",", "value", ")", "in", "six", ".", "iteritems", "(", "json_object", ")", ":", "\n", "            ", "config", ".", "__dict__", "[", "key", "]", "=", "value", "\n", "", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.BertConfig.from_json_file": [[89, 95], ["cls.from_dict", "tensorflow.gfile.GFile", "reader.read", "json.loads"], "methods", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.BertConfig.from_dict"], ["", "@", "classmethod", "\n", "def", "from_json_file", "(", "cls", ",", "json_file", ")", ":", "\n", "        ", "\"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"", "\n", "with", "tf", ".", "gfile", ".", "GFile", "(", "json_file", ",", "\"r\"", ")", "as", "reader", ":", "\n", "            ", "text", "=", "reader", ".", "read", "(", ")", "\n", "", "return", "cls", ".", "from_dict", "(", "json", ".", "loads", "(", "text", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.BertConfig.to_dict": [[96, 100], ["copy.deepcopy"], "methods", ["None"], ["", "def", "to_dict", "(", "self", ")", ":", "\n", "        ", "\"\"\"Serializes this instance to a Python dictionary.\"\"\"", "\n", "output", "=", "copy", ".", "deepcopy", "(", "self", ".", "__dict__", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.BertConfig.to_json_string": [[101, 104], ["json.dumps", "modeling.BertConfig.to_dict"], "methods", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.BertConfig.to_dict"], ["", "def", "to_json_string", "(", "self", ")", ":", "\n", "        ", "\"\"\"Serializes this instance to a JSON string.\"\"\"", "\n", "return", "json", ".", "dumps", "(", "self", ".", "to_dict", "(", ")", ",", "indent", "=", "2", ",", "sort_keys", "=", "True", ")", "+", "\"\\n\"", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.BertModel.__init__": [[130, 236], ["copy.deepcopy", "modeling.get_shape_list", "tensorflow.ones", "tensorflow.zeros", "tensorflow.variable_scope", "tensorflow.variable_scope", "modeling.embedding_lookup", "modeling.embedding_postprocessor", "tensorflow.variable_scope", "modeling.create_attention_mask_from_input_mask", "modeling.transformer_model", "tensorflow.variable_scope", "tensorflow.squeeze", "tensorflow.layers.dense", "modeling.get_activation", "modeling.create_initializer"], "methods", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.get_shape_list", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.embedding_lookup", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.embedding_postprocessor", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.create_attention_mask_from_input_mask", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.transformer_model", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.get_activation", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.create_initializer"], ["def", "__init__", "(", "self", ",", "\n", "config", ",", "\n", "is_training", ",", "\n", "input_ids", ",", "\n", "input_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "use_one_hot_embeddings", "=", "True", ",", "\n", "scope", "=", "None", ")", ":", "\n", "        ", "\"\"\"Constructor for BertModel.\n\n        Args:\n          config: `BertConfig` instance.\n          is_training: bool. true for training model, false for eval model. Controls\n            whether dropout will be applied.\n          input_ids: int32 Tensor of shape [batch_size, seq_length].\n          input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].\n          token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n          use_one_hot_embeddings: (optional) bool. Whether to use one-hot word\n            embeddings or tf.embedding_lookup() for the word embeddings. On the TPU,\n            it is much faster if this is True, on the CPU or GPU, it is faster if\n            this is False.\n          scope: (optional) variable scope. Defaults to \"bert\".\n\n        Raises:\n          ValueError: The config is invalid or one of the input tensor shapes\n            is invalid.\n        \"\"\"", "\n", "config", "=", "copy", ".", "deepcopy", "(", "config", ")", "\n", "# if not is_training:", "\n", "#     config.hidden_dropout_prob = 0.0", "\n", "#     config.attention_probs_dropout_prob = 0.0", "\n", "\n", "input_shape", "=", "get_shape_list", "(", "input_ids", ",", "expected_rank", "=", "2", ")", "\n", "batch_size", "=", "input_shape", "[", "0", "]", "\n", "seq_length", "=", "input_shape", "[", "1", "]", "\n", "\n", "if", "input_mask", "is", "None", ":", "\n", "            ", "input_mask", "=", "tf", ".", "ones", "(", "shape", "=", "[", "batch_size", ",", "seq_length", "]", ",", "dtype", "=", "tf", ".", "int32", ")", "\n", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "tf", ".", "zeros", "(", "shape", "=", "[", "batch_size", ",", "seq_length", "]", ",", "dtype", "=", "tf", ".", "int32", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "scope", ",", "default_name", "=", "\"bert\"", ")", ":", "\n", "            ", "with", "tf", ".", "variable_scope", "(", "\"embeddings\"", ")", ":", "\n", "# Perform embedding lookup on the word ids.", "\n", "                ", "(", "self", ".", "embedding_output", ",", "self", ".", "embedding_table", ")", "=", "embedding_lookup", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "vocab_size", "=", "config", ".", "vocab_size", ",", "\n", "embedding_size", "=", "config", ".", "hidden_size", ",", "\n", "initializer_range", "=", "config", ".", "initializer_range", ",", "\n", "word_embedding_name", "=", "\"word_embeddings\"", ",", "\n", "use_one_hot_embeddings", "=", "use_one_hot_embeddings", ")", "\n", "\n", "# Add positional embeddings and token type embeddings, then layer", "\n", "# normalize and perform dropout.", "\n", "self", ".", "embedding_output", "=", "embedding_postprocessor", "(", "\n", "input_tensor", "=", "self", ".", "embedding_output", ",", "\n", "use_token_type", "=", "True", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "token_type_vocab_size", "=", "config", ".", "type_vocab_size", ",", "\n", "token_type_embedding_name", "=", "\"token_type_embeddings\"", ",", "\n", "use_position_embeddings", "=", "True", ",", "\n", "position_embedding_name", "=", "\"position_embeddings\"", ",", "\n", "initializer_range", "=", "config", ".", "initializer_range", ",", "\n", "max_position_embeddings", "=", "config", ".", "max_position_embeddings", ",", "\n", "dropout_prob", "=", "config", ".", "hidden_dropout_prob", ",", "\n", "is_training", "=", "is_training", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"encoder\"", ")", ":", "\n", "# This converts a 2D mask of shape [batch_size, seq_length] to a 3D", "\n", "# mask of shape [batch_size, seq_length, seq_length] which is used", "\n", "# for the attention scores.", "\n", "                ", "attention_mask", "=", "create_attention_mask_from_input_mask", "(", "\n", "input_ids", ",", "input_mask", ")", "\n", "\n", "# Run the stacked transformer.", "\n", "# `sequence_output` shape = [batch_size, seq_length, hidden_size].", "\n", "self", ".", "all_encoder_layers", "=", "transformer_model", "(", "\n", "input_tensor", "=", "self", ".", "embedding_output", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "hidden_size", "=", "config", ".", "hidden_size", ",", "\n", "num_hidden_layers", "=", "config", ".", "num_hidden_layers", ",", "\n", "num_attention_heads", "=", "config", ".", "num_attention_heads", ",", "\n", "intermediate_size", "=", "config", ".", "intermediate_size", ",", "\n", "intermediate_act_fn", "=", "get_activation", "(", "config", ".", "hidden_act", ")", ",", "\n", "hidden_dropout_prob", "=", "config", ".", "hidden_dropout_prob", ",", "\n", "attention_probs_dropout_prob", "=", "config", ".", "attention_probs_dropout_prob", ",", "\n", "initializer_range", "=", "config", ".", "initializer_range", ",", "\n", "do_return_all_layers", "=", "True", ",", "\n", "is_training", "=", "is_training", ")", "\n", "\n", "", "self", ".", "sequence_output", "=", "self", ".", "all_encoder_layers", "[", "-", "1", "]", "\n", "# The \"pooler\" converts the encoded sequence tensor of shape", "\n", "# [batch_size, seq_length, hidden_size] to a tensor of shape", "\n", "# [batch_size, hidden_size]. This is necessary for segment-level", "\n", "# (or segment-pair-level) classification tasks where we need a fixed", "\n", "# dimensional representation of the segment.", "\n", "with", "tf", ".", "variable_scope", "(", "\"pooler\"", ")", ":", "\n", "# We \"pool\" the model by simply taking the hidden state corresponding", "\n", "# to the first token. We assume that this has been pre-trained", "\n", "                ", "first_token_tensor", "=", "tf", ".", "squeeze", "(", "self", ".", "sequence_output", "[", ":", ",", "0", ":", "1", ",", ":", "]", ",", "axis", "=", "1", ")", "\n", "self", ".", "pooled_output", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "first_token_tensor", ",", "\n", "config", ".", "hidden_size", ",", "\n", "activation", "=", "tf", ".", "tanh", ",", "\n", "kernel_initializer", "=", "create_initializer", "(", "config", ".", "initializer_range", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.BertModel.get_pooled_output": [[237, 239], ["None"], "methods", ["None"], ["", "", "", "def", "get_pooled_output", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.BertModel.get_sequence_output": [[240, 248], ["None"], "methods", ["None"], ["", "def", "get_sequence_output", "(", "self", ")", ":", "\n", "        ", "\"\"\"Gets final hidden layer of encoder.\n\n        Returns:\n          float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n          to the final hidden of the transformer encoder.\n        \"\"\"", "\n", "return", "self", ".", "sequence_output", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.BertModel.get_all_encoder_layers": [[249, 251], ["None"], "methods", ["None"], ["", "def", "get_all_encoder_layers", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "all_encoder_layers", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.BertModel.get_embedding_output": [[252, 262], ["None"], "methods", ["None"], ["", "def", "get_embedding_output", "(", "self", ")", ":", "\n", "        ", "\"\"\"Gets output of the embedding lookup (i.e., input to the transformer).\n\n        Returns:\n          float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n          to the output of the embedding layer, after summing the word\n          embeddings with the positional embeddings and the token type embeddings,\n          then performing layer normalization. This is the input to the transformer.\n        \"\"\"", "\n", "return", "self", ".", "embedding_output", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.BertModel.get_embedding_table": [[263, 265], ["None"], "methods", ["None"], ["", "def", "get_embedding_table", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "embedding_table", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.gelu": [[267, 281], ["tensorflow.erf", "tensorflow.sqrt"], "function", ["None"], ["", "", "def", "gelu", "(", "input_tensor", ")", ":", "\n", "    ", "\"\"\"Gaussian Error Linear Unit.\n\n    This is a smoother version of the RELU.\n    Original paper: https://arxiv.org/abs/1606.08415\n\n    Args:\n      input_tensor: float Tensor to perform activation.\n\n    Returns:\n      `input_tensor` with the GELU activation applied.\n    \"\"\"", "\n", "cdf", "=", "0.5", "*", "(", "1.0", "+", "tf", ".", "erf", "(", "input_tensor", "/", "tf", ".", "sqrt", "(", "2.0", ")", ")", ")", "\n", "return", "input_tensor", "*", "cdf", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.get_activation": [[283, 318], ["activation_string.lower", "isinstance", "ValueError"], "function", ["None"], ["", "def", "get_activation", "(", "activation_string", ")", ":", "\n", "    ", "\"\"\"Maps a string to a Python function, e.g., \"relu\" => `tf.nn.relu`.\n\n    Args:\n      activation_string: String name of the activation function.\n\n    Returns:\n      A Python function corresponding to the activation function. If\n      `activation_string` is None, empty, or \"linear\", this will return None.\n      If `activation_string` is not a string, it will return `activation_string`.\n\n    Raises:\n      ValueError: The `activation_string` does not correspond to a known\n        activation.\n    \"\"\"", "\n", "\n", "# We assume that anything that\"s not a string is already an activation", "\n", "# function, so we just return it.", "\n", "if", "not", "isinstance", "(", "activation_string", ",", "six", ".", "string_types", ")", ":", "\n", "        ", "return", "activation_string", "\n", "\n", "", "if", "not", "activation_string", ":", "\n", "        ", "return", "None", "\n", "\n", "", "act", "=", "activation_string", ".", "lower", "(", ")", "\n", "if", "act", "==", "\"linear\"", ":", "\n", "        ", "return", "None", "\n", "", "elif", "act", "==", "\"relu\"", ":", "\n", "        ", "return", "tf", ".", "nn", ".", "relu", "\n", "", "elif", "act", "==", "\"gelu\"", ":", "\n", "        ", "return", "gelu", "\n", "", "elif", "act", "==", "\"tanh\"", ":", "\n", "        ", "return", "tf", ".", "tanh", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Unsupported activation: %s\"", "%", "act", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.get_assignment_map_from_checkpoint": [[320, 345], ["collections.OrderedDict", "tensorflow.train.list_variables", "collections.OrderedDict", "re.match", "re.match.group"], "function", ["None"], ["", "", "def", "get_assignment_map_from_checkpoint", "(", "tvars", ",", "init_checkpoint", ")", ":", "\n", "    ", "\"\"\"Compute the union of the current variables and checkpoint variables.\"\"\"", "\n", "assignment_map", "=", "{", "}", "\n", "initialized_variable_names", "=", "{", "}", "\n", "\n", "name_to_variable", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "for", "var", "in", "tvars", ":", "\n", "        ", "name", "=", "var", ".", "name", "\n", "m", "=", "re", ".", "match", "(", "\"^(.*):\\\\d+$\"", ",", "name", ")", "\n", "if", "m", "is", "not", "None", ":", "\n", "            ", "name", "=", "m", ".", "group", "(", "1", ")", "\n", "", "name_to_variable", "[", "name", "]", "=", "var", "\n", "\n", "", "init_vars", "=", "tf", ".", "train", ".", "list_variables", "(", "init_checkpoint", ")", "\n", "\n", "assignment_map", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "for", "x", "in", "init_vars", ":", "\n", "        ", "(", "name", ",", "var", ")", "=", "(", "x", "[", "0", "]", ",", "x", "[", "1", "]", ")", "\n", "if", "name", "not", "in", "name_to_variable", ":", "\n", "            ", "continue", "\n", "", "assignment_map", "[", "name", "]", "=", "name", "\n", "initialized_variable_names", "[", "name", "]", "=", "1", "\n", "initialized_variable_names", "[", "name", "+", "\":0\"", "]", "=", "1", "\n", "\n", "", "return", "(", "assignment_map", ",", "initialized_variable_names", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.dropout": [[347, 363], ["tensorflow.nn.dropout"], "function", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.dropout"], ["", "def", "dropout", "(", "input_tensor", ",", "dropout_prob", ")", ":", "\n", "    ", "\"\"\"Perform dropout.\n\n    Args:\n      input_tensor: float Tensor.\n      dropout_prob: Python float. The probability of dropping out a value (NOT of\n        *keeping* a dimension as in `tf.nn.dropout`).\n\n    Returns:\n      A version of `input_tensor` with dropout applied.\n    \"\"\"", "\n", "if", "dropout_prob", "is", "None", "or", "dropout_prob", "==", "0.0", ":", "\n", "        ", "return", "input_tensor", "\n", "\n", "", "output", "=", "tf", ".", "nn", ".", "dropout", "(", "input_tensor", ",", "1.0", "-", "dropout_prob", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.layer_norm": [[365, 369], ["tensorflow.contrib.layers.layer_norm"], "function", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.layer_norm"], ["", "def", "layer_norm", "(", "input_tensor", ",", "name", "=", "None", ")", ":", "\n", "    ", "\"\"\"Run layer normalization on the last dimension of the tensor.\"\"\"", "\n", "return", "tf", ".", "contrib", ".", "layers", ".", "layer_norm", "(", "\n", "inputs", "=", "input_tensor", ",", "begin_norm_axis", "=", "-", "1", ",", "begin_params_axis", "=", "-", "1", ",", "scope", "=", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.layer_norm_and_dropout": [[371, 376], ["modeling.layer_norm", "tensorflow.layers.dropout"], "function", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.layer_norm", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.dropout"], ["", "def", "layer_norm_and_dropout", "(", "input_tensor", ",", "dropout_prob", ",", "is_training", ",", "name", "=", "None", ")", ":", "\n", "    ", "\"\"\"Runs layer normalization followed by dropout.\"\"\"", "\n", "output_tensor", "=", "layer_norm", "(", "input_tensor", ",", "name", ")", "\n", "output_tensor", "=", "tf", ".", "layers", ".", "dropout", "(", "output_tensor", ",", "dropout_prob", ",", "training", "=", "is_training", ")", "\n", "return", "output_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.create_initializer": [[378, 381], ["tensorflow.truncated_normal_initializer"], "function", ["None"], ["", "def", "create_initializer", "(", "initializer_range", "=", "0.02", ")", ":", "\n", "    ", "\"\"\"Creates a `truncated_normal_initializer` with the given range.\"\"\"", "\n", "return", "tf", ".", "truncated_normal_initializer", "(", "stddev", "=", "initializer_range", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.embedding_lookup": [[383, 430], ["tensorflow.get_variable", "modeling.get_shape_list", "tensorflow.reshape", "tensorflow.expand_dims", "tensorflow.reshape", "tensorflow.one_hot", "tensorflow.matmul", "tensorflow.nn.embedding_lookup", "modeling.create_initializer"], "function", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.get_shape_list", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.embedding_lookup", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.create_initializer"], ["", "def", "embedding_lookup", "(", "input_ids", ",", "\n", "vocab_size", ",", "\n", "embedding_size", "=", "128", ",", "\n", "initializer_range", "=", "0.02", ",", "\n", "word_embedding_name", "=", "\"word_embeddings\"", ",", "\n", "use_one_hot_embeddings", "=", "False", ")", ":", "\n", "    ", "\"\"\"Looks up words embeddings for id tensor.\n\n    Args:\n      input_ids: int32 Tensor of shape [batch_size, seq_length] containing word\n        ids.\n      vocab_size: int. Size of the embedding vocabulary.\n      embedding_size: int. Width of the word embeddings.\n      initializer_range: float. Embedding initialization range.\n      word_embedding_name: string. Name of the embedding table.\n      use_one_hot_embeddings: bool. If True, use one-hot method for word\n        embeddings. If False, use `tf.nn.embedding_lookup()`. One hot is better\n        for TPUs.\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, embedding_size].\n    \"\"\"", "\n", "# This function assumes that the input is of shape [batch_size, seq_length,", "\n", "# num_inputs].", "\n", "#", "\n", "# If the input is a 2D tensor of shape [batch_size, seq_length], we", "\n", "# reshape to [batch_size, seq_length, 1].", "\n", "if", "input_ids", ".", "shape", ".", "ndims", "==", "2", ":", "\n", "        ", "input_ids", "=", "tf", ".", "expand_dims", "(", "input_ids", ",", "axis", "=", "[", "-", "1", "]", ")", "\n", "\n", "", "embedding_table", "=", "tf", ".", "get_variable", "(", "\n", "name", "=", "word_embedding_name", ",", "\n", "shape", "=", "[", "vocab_size", ",", "embedding_size", "]", ",", "\n", "initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "\n", "if", "use_one_hot_embeddings", ":", "\n", "        ", "flat_input_ids", "=", "tf", ".", "reshape", "(", "input_ids", ",", "[", "-", "1", "]", ")", "\n", "one_hot_input_ids", "=", "tf", ".", "one_hot", "(", "flat_input_ids", ",", "depth", "=", "vocab_size", ")", "\n", "output", "=", "tf", ".", "matmul", "(", "one_hot_input_ids", ",", "embedding_table", ")", "\n", "", "else", ":", "\n", "        ", "output", "=", "tf", ".", "nn", ".", "embedding_lookup", "(", "embedding_table", ",", "input_ids", ")", "\n", "\n", "", "input_shape", "=", "get_shape_list", "(", "input_ids", ")", "\n", "\n", "output", "=", "tf", ".", "reshape", "(", "output", ",", "\n", "input_shape", "[", "0", ":", "-", "1", "]", "+", "[", "input_shape", "[", "-", "1", "]", "*", "embedding_size", "]", ")", "\n", "return", "(", "output", ",", "embedding_table", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.embedding_postprocessor": [[432, 527], ["modeling.get_shape_list", "modeling.layer_norm_and_dropout", "tensorflow.get_variable", "tensorflow.reshape", "tensorflow.one_hot", "tensorflow.matmul", "tensorflow.reshape", "tensorflow.assert_less_equal", "ValueError", "tensorflow.control_dependencies", "tensorflow.get_variable", "tensorflow.slice", "len", "range", "position_broadcast_shape.extend", "tensorflow.reshape", "modeling.create_initializer", "layer_norm_and_dropout.shape.as_list", "position_broadcast_shape.append", "modeling.create_initializer"], "function", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.get_shape_list", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.layer_norm_and_dropout", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.create_initializer", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.create_initializer"], ["", "def", "embedding_postprocessor", "(", "input_tensor", ",", "\n", "use_token_type", "=", "False", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "token_type_vocab_size", "=", "16", ",", "\n", "token_type_embedding_name", "=", "\"token_type_embeddings\"", ",", "\n", "use_position_embeddings", "=", "True", ",", "\n", "position_embedding_name", "=", "\"position_embeddings\"", ",", "\n", "initializer_range", "=", "0.02", ",", "\n", "max_position_embeddings", "=", "512", ",", "\n", "dropout_prob", "=", "0.1", ",", "\n", "is_training", "=", "True", ")", ":", "\n", "    ", "\"\"\"Performs various post-processing on a word embedding tensor.\n\n    Args:\n      input_tensor: float Tensor of shape [batch_size, seq_length,\n        embedding_size].\n      use_token_type: bool. Whether to add embeddings for `token_type_ids`.\n      token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n        Must be specified if `use_token_type` is True.\n      token_type_vocab_size: int. The vocabulary size of `token_type_ids`.\n      token_type_embedding_name: string. The name of the embedding table variable\n        for token type ids.\n      use_position_embeddings: bool. Whether to add position embeddings for the\n        position of each token in the sequence.\n      position_embedding_name: string. The name of the embedding table variable\n        for positional embeddings.\n      initializer_range: float. Range of the weight initialization.\n      max_position_embeddings: int. Maximum sequence length that might ever be\n        used with this model. This can be longer than the sequence length of\n        input_tensor, but cannot be shorter.\n      dropout_prob: float. Dropout probability applied to the final output tensor.\n\n    Returns:\n      float tensor with same shape as `input_tensor`.\n\n    Raises:\n      ValueError: One of the tensor shapes or input values is invalid.\n    \"\"\"", "\n", "input_shape", "=", "get_shape_list", "(", "input_tensor", ",", "expected_rank", "=", "3", ")", "\n", "batch_size", "=", "input_shape", "[", "0", "]", "\n", "seq_length", "=", "input_shape", "[", "1", "]", "\n", "width", "=", "input_shape", "[", "2", "]", "\n", "\n", "output", "=", "input_tensor", "\n", "\n", "if", "use_token_type", ":", "\n", "        ", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"`token_type_ids` must be specified if\"", "\n", "\"`use_token_type` is True.\"", ")", "\n", "", "token_type_table", "=", "tf", ".", "get_variable", "(", "\n", "name", "=", "token_type_embedding_name", ",", "\n", "shape", "=", "[", "token_type_vocab_size", ",", "width", "]", ",", "\n", "initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "# This vocab will be small so we always do one-hot here, since it is always", "\n", "# faster for a small vocabulary.", "\n", "flat_token_type_ids", "=", "tf", ".", "reshape", "(", "token_type_ids", ",", "[", "-", "1", "]", ")", "\n", "one_hot_ids", "=", "tf", ".", "one_hot", "(", "flat_token_type_ids", ",", "depth", "=", "token_type_vocab_size", ")", "\n", "token_type_embeddings", "=", "tf", ".", "matmul", "(", "one_hot_ids", ",", "token_type_table", ")", "\n", "token_type_embeddings", "=", "tf", ".", "reshape", "(", "token_type_embeddings", ",", "\n", "[", "batch_size", ",", "seq_length", ",", "width", "]", ")", "\n", "output", "+=", "token_type_embeddings", "\n", "\n", "", "if", "use_position_embeddings", ":", "\n", "        ", "assert_op", "=", "tf", ".", "assert_less_equal", "(", "seq_length", ",", "max_position_embeddings", ")", "\n", "with", "tf", ".", "control_dependencies", "(", "[", "assert_op", "]", ")", ":", "\n", "            ", "full_position_embeddings", "=", "tf", ".", "get_variable", "(", "\n", "name", "=", "position_embedding_name", ",", "\n", "shape", "=", "[", "max_position_embeddings", ",", "width", "]", ",", "\n", "initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "# Since the position embedding table is a learned variable, we create it", "\n", "# using a (long) sequence length `max_position_embeddings`. The actual", "\n", "# sequence length might be shorter than this, for faster training of", "\n", "# tasks that do not have long sequences.", "\n", "#", "\n", "# So `full_position_embeddings` is effectively an embedding table", "\n", "# for position [0, 1, 2, ..., max_position_embeddings-1], and the current", "\n", "# sequence has positions [0, 1, 2, ... seq_length-1], so we can just", "\n", "# perform a slice.", "\n", "position_embeddings", "=", "tf", ".", "slice", "(", "full_position_embeddings", ",", "[", "0", ",", "0", "]", ",", "\n", "[", "seq_length", ",", "-", "1", "]", ")", "\n", "num_dims", "=", "len", "(", "output", ".", "shape", ".", "as_list", "(", ")", ")", "\n", "\n", "# Only the last two dimensions are relevant (`seq_length` and `width`), so", "\n", "# we broadcast among the first dimensions, which is typically just", "\n", "# the batch size.", "\n", "position_broadcast_shape", "=", "[", "]", "\n", "for", "_", "in", "range", "(", "num_dims", "-", "2", ")", ":", "\n", "                ", "position_broadcast_shape", ".", "append", "(", "1", ")", "\n", "", "position_broadcast_shape", ".", "extend", "(", "[", "seq_length", ",", "width", "]", ")", "\n", "position_embeddings", "=", "tf", ".", "reshape", "(", "position_embeddings", ",", "\n", "position_broadcast_shape", ")", "\n", "output", "+=", "position_embeddings", "\n", "\n", "", "", "output", "=", "layer_norm_and_dropout", "(", "output", ",", "dropout_prob", ",", "is_training", "=", "is_training", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.create_attention_mask_from_input_mask": [[529, 561], ["modeling.get_shape_list", "modeling.get_shape_list", "tensorflow.cast", "tensorflow.ones", "tensorflow.reshape"], "function", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.get_shape_list", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.get_shape_list"], ["", "def", "create_attention_mask_from_input_mask", "(", "from_tensor", ",", "to_mask", ")", ":", "\n", "    ", "\"\"\"Create 3D attention mask from a 2D tensor mask.\n\n    Args:\n      from_tensor: 2D or 3D Tensor of shape [batch_size, from_seq_length, ...].\n      to_mask: int32 Tensor of shape [batch_size, to_seq_length].\n\n    Returns:\n      float Tensor of shape [batch_size, from_seq_length, to_seq_length].\n    \"\"\"", "\n", "from_shape", "=", "get_shape_list", "(", "from_tensor", ",", "expected_rank", "=", "[", "2", ",", "3", "]", ")", "\n", "batch_size", "=", "from_shape", "[", "0", "]", "\n", "from_seq_length", "=", "from_shape", "[", "1", "]", "\n", "\n", "to_shape", "=", "get_shape_list", "(", "to_mask", ",", "expected_rank", "=", "2", ")", "\n", "to_seq_length", "=", "to_shape", "[", "1", "]", "\n", "\n", "to_mask", "=", "tf", ".", "cast", "(", "\n", "tf", ".", "reshape", "(", "to_mask", ",", "[", "batch_size", ",", "1", ",", "to_seq_length", "]", ")", ",", "tf", ".", "float32", ")", "\n", "\n", "# We don't assume that `from_tensor` is a mask (although it could be). We", "\n", "# don't actually care if we attend *from* padding tokens (only *to* padding)", "\n", "# tokens so we create a tensor of all ones.", "\n", "#", "\n", "# `broadcast_ones` = [batch_size, from_seq_length, 1]", "\n", "broadcast_ones", "=", "tf", ".", "ones", "(", "\n", "shape", "=", "[", "batch_size", ",", "from_seq_length", ",", "1", "]", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "\n", "# Here we broadcast along two dimensions to create the mask.", "\n", "mask", "=", "broadcast_ones", "*", "to_mask", "\n", "\n", "return", "mask", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.attention_scores_layer": [[563, 711], ["modeling.get_shape_list", "modeling.get_shape_list", "modeling.reshape_to_matrix", "modeling.reshape_to_matrix", "tensorflow.layers.dense", "modeling.attention_scores_layer.transpose_for_scores"], "function", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.get_shape_list", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.get_shape_list", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.reshape_to_matrix", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.reshape_to_matrix"], ["", "def", "attention_scores_layer", "(", "from_tensor", ",", "\n", "to_tensor", ",", "\n", "attention_mask", "=", "None", ",", "\n", "num_attention_heads", "=", "1", ",", "\n", "size_per_head", "=", "512", ",", "\n", "query_act", "=", "None", ",", "\n", "key_act", "=", "None", ",", "\n", "initializer_range", "=", "0.02", ",", "\n", "batch_size", "=", "None", ",", "\n", "from_seq_length", "=", "None", ",", "\n", "to_seq_length", "=", "None", ",", "\n", "query_equals_key", "=", "False", ",", "\n", "return_features", "=", "False", ")", ":", "\n", "    ", "\"\"\"Calculate multi-headed attention probabilities from `from_tensor` to `to_tensor`.\n\n    This is an implementation of multi-headed attention based on \"Attention\n    is all you Need\". If `from_tensor` and `to_tensor` are the same, then\n    this is self-attention. Each timestep in `from_tensor` attends to the\n    corresponding sequence in `to_tensor`, and returns a fixed-with vector.\n\n    This function first projects `from_tensor` into a \"query\" tensor and\n    `to_tensor` into \"key\" and \"value\" tensors. These are (effectively) a list\n    of tensors of length `num_attention_heads`, where each tensor is of shape\n    [batch_size, seq_length, size_per_head].\n\n    Then, the query and key tensors are dot-producted and scaled. These are\n    softmaxed to obtain attention probabilities. The value tensors are then\n    interpolated by these probabilities, then concatenated back to a single\n    tensor and returned.\n\n    In practice, the multi-headed attention are done with transposes and\n    reshapes rather than actual separate tensors.\n\n    Args:\n      from_tensor: float Tensor of shape [batch_size, from_seq_length,\n        from_width].\n      to_tensor: float Tensor of shape [batch_size, to_seq_length, to_width].\n      attention_mask: (optional) int32 Tensor of shape [batch_size,\n        from_seq_length, to_seq_length]. The values should be 1 or 0. The\n        attention scores will effectively be set to -infinity for any positions in\n        the mask that are 0, and will be unchanged for positions that are 1.\n      num_attention_heads: int. Number of attention heads.\n      size_per_head: int. Size of each attention head.\n      query_act: (optional) Activation function for the query transform.\n      key_act: (optional) Activation function for the key transform.\n      initializer_range: float. Range of the weight initializer.\n      batch_size: (Optional) int. If the input is 2D, this might be the batch size\n        of the 3D version of the `from_tensor` and `to_tensor`.\n      from_seq_length: (Optional) If the input is 2D, this might be the seq length\n        of the 3D version of the `from_tensor`.\n      to_seq_length: (Optional) If the input is 2D, this might be the seq length\n        of the 3D version of the `to_tensor`.\n\n    Returns:\n      float Tensor of shape [batch_size, num_attention_heads, from_seq_length, to_seq_length].\n\n    Raises:\n      ValueError: Any of the arguments or tensor shapes are invalid.\n    \"\"\"", "\n", "\n", "def", "transpose_for_scores", "(", "input_tensor", ",", "batch_size", ",", "num_attention_heads", ",", "\n", "seq_length", ",", "width", ")", ":", "\n", "        ", "output_tensor", "=", "tf", ".", "reshape", "(", "\n", "input_tensor", ",", "[", "batch_size", ",", "seq_length", ",", "num_attention_heads", ",", "width", "]", ")", "\n", "\n", "output_tensor", "=", "tf", ".", "transpose", "(", "output_tensor", ",", "[", "0", ",", "2", ",", "1", ",", "3", "]", ")", "\n", "return", "output_tensor", "\n", "\n", "", "from_shape", "=", "get_shape_list", "(", "from_tensor", ",", "expected_rank", "=", "[", "2", ",", "3", "]", ")", "\n", "to_shape", "=", "get_shape_list", "(", "to_tensor", ",", "expected_rank", "=", "[", "2", ",", "3", "]", ")", "\n", "\n", "if", "len", "(", "from_shape", ")", "!=", "len", "(", "to_shape", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"The rank of `from_tensor` must match the rank of `to_tensor`.\"", ")", "\n", "\n", "", "if", "len", "(", "from_shape", ")", "==", "3", ":", "\n", "        ", "batch_size", "=", "from_shape", "[", "0", "]", "\n", "from_seq_length", "=", "from_shape", "[", "1", "]", "\n", "to_seq_length", "=", "to_shape", "[", "1", "]", "\n", "", "elif", "len", "(", "from_shape", ")", "==", "2", ":", "\n", "        ", "if", "(", "batch_size", "is", "None", "or", "from_seq_length", "is", "None", "or", "to_seq_length", "is", "None", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"When passing in rank 2 tensors to attention_layer, the values \"", "\n", "\"for `batch_size`, `from_seq_length`, and `to_seq_length` \"", "\n", "\"must all be specified.\"", ")", "\n", "\n", "# Scalar dimensions referenced here:", "\n", "#   B = batch size (number of sequences)", "\n", "#   F = `from_tensor` sequence length", "\n", "#   T = `to_tensor` sequence length", "\n", "#   N = `num_attention_heads`", "\n", "#   H = `size_per_head`", "\n", "\n", "", "", "from_tensor_2d", "=", "reshape_to_matrix", "(", "from_tensor", ")", "\n", "to_tensor_2d", "=", "reshape_to_matrix", "(", "to_tensor", ")", "\n", "\n", "# `query_layer` = [B*F, N*H]", "\n", "query_layer", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "from_tensor_2d", ",", "\n", "num_attention_heads", "*", "size_per_head", ",", "\n", "activation", "=", "query_act", ",", "\n", "name", "=", "\"query\"", ",", "\n", "kernel_initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "\n", "# `key_layer` = [B*T, N*H]", "\n", "if", "query_equals_key", ":", "\n", "        ", "key_layer", "=", "query_layer", "\n", "", "else", ":", "\n", "        ", "key_layer", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "to_tensor_2d", ",", "\n", "num_attention_heads", "*", "size_per_head", ",", "\n", "activation", "=", "key_act", ",", "\n", "name", "=", "\"key\"", ",", "\n", "kernel_initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "\n", "# `query_layer` = [B, N, F, H]", "\n", "", "query_layer", "=", "transpose_for_scores", "(", "query_layer", ",", "batch_size", ",", "\n", "num_attention_heads", ",", "from_seq_length", ",", "\n", "size_per_head", ")", "\n", "\n", "# `key_layer` = [B, N, T, H]", "\n", "key_layer", "=", "transpose_for_scores", "(", "key_layer", ",", "batch_size", ",", "num_attention_heads", ",", "\n", "to_seq_length", ",", "size_per_head", ")", "\n", "\n", "# Take the dot product between \"query\" and \"key\" to get the raw", "\n", "# attention scores.", "\n", "# `attention_scores` = [B, N, F, T]", "\n", "attention_scores", "=", "tf", ".", "matmul", "(", "query_layer", ",", "key_layer", ",", "transpose_b", "=", "True", ")", "\n", "attention_scores", "=", "tf", ".", "multiply", "(", "attention_scores", ",", "\n", "1.0", "/", "math", ".", "sqrt", "(", "float", "(", "size_per_head", ")", ")", ")", "\n", "\n", "if", "attention_mask", "is", "not", "None", ":", "\n", "# `attention_mask` = [B, 1, F, T]", "\n", "        ", "attention_mask", "=", "tf", ".", "expand_dims", "(", "attention_mask", ",", "axis", "=", "[", "1", "]", ")", "\n", "\n", "# Since attention_mask is 1.0 for positions we want to attend and 0.0 for", "\n", "# masked positions, this operation will create a tensor which is 0.0 for", "\n", "# positions we want to attend and -10000.0 for masked positions.", "\n", "adder", "=", "(", "1.0", "-", "tf", ".", "cast", "(", "attention_mask", ",", "tf", ".", "float32", ")", ")", "*", "-", "10000.0", "\n", "\n", "# Since we are adding it to the raw scores before the softmax, this is", "\n", "# effectively the same as removing these entirely.", "\n", "attention_scores", "=", "attention_scores", "*", "tf", ".", "cast", "(", "attention_mask", ",", "tf", ".", "float32", ")", "+", "adder", "\n", "\n", "", "if", "return_features", ":", "\n", "        ", "return", "attention_scores", ",", "query_layer", ",", "key_layer", "\n", "", "else", ":", "\n", "        ", "return", "attention_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.attention_layer": [[713, 843], ["modeling.reshape_to_matrix", "tensorflow.layers.dense", "modeling.attention_scores_layer", "tensorflow.nn.softmax", "tensorflow.layers.dropout", "tensorflow.reshape", "tensorflow.transpose", "tensorflow.matmul", "tensorflow.transpose", "tensorflow.reshape", "tensorflow.reshape", "modeling.create_initializer"], "function", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.reshape_to_matrix", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.attention_scores_layer", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.dropout", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.create_initializer"], ["", "", "def", "attention_layer", "(", "from_tensor", ",", "\n", "to_tensor", ",", "\n", "attention_mask", "=", "None", ",", "\n", "num_attention_heads", "=", "1", ",", "\n", "size_per_head", "=", "512", ",", "\n", "query_act", "=", "None", ",", "\n", "key_act", "=", "None", ",", "\n", "value_act", "=", "None", ",", "\n", "attention_probs_dropout_prob", "=", "0.0", ",", "\n", "initializer_range", "=", "0.02", ",", "\n", "do_return_2d_tensor", "=", "False", ",", "\n", "batch_size", "=", "None", ",", "\n", "from_seq_length", "=", "None", ",", "\n", "to_seq_length", "=", "None", ",", "\n", "is_training", "=", "True", ")", ":", "\n", "    ", "\"\"\"Performs multi-headed attention from `from_tensor` to `to_tensor`.\n\n    This is an implementation of multi-headed attention based on \"Attention\n    is all you Need\". If `from_tensor` and `to_tensor` are the same, then\n    this is self-attention. Each timestep in `from_tensor` attends to the\n    corresponding sequence in `to_tensor`, and returns a fixed-with vector.\n\n    This function first projects `from_tensor` into a \"query\" tensor and\n    `to_tensor` into \"key\" and \"value\" tensors. These are (effectively) a list\n    of tensors of length `num_attention_heads`, where each tensor is of shape\n    [batch_size, seq_length, size_per_head].\n\n    Then, the query and key tensors are dot-producted and scaled. These are\n    softmaxed to obtain attention probabilities. The value tensors are then\n    interpolated by these probabilities, then concatenated back to a single\n    tensor and returned.\n\n    In practice, the multi-headed attention are done with transposes and\n    reshapes rather than actual separate tensors.\n\n    Args:\n      from_tensor: float Tensor of shape [batch_size, from_seq_length,\n        from_width].\n      to_tensor: float Tensor of shape [batch_size, to_seq_length, to_width].\n      attention_mask: (optional) int32 Tensor of shape [batch_size,\n        from_seq_length, to_seq_length]. The values should be 1 or 0. The\n        attention scores will effectively be set to -infinity for any positions in\n        the mask that are 0, and will be unchanged for positions that are 1.\n      num_attention_heads: int. Number of attention heads.\n      size_per_head: int. Size of each attention head.\n      query_act: (optional) Activation function for the query transform.\n      key_act: (optional) Activation function for the key transform.\n      value_act: (optional) Activation function for the value transform.\n      attention_probs_dropout_prob: (optional) float. Dropout probability of the\n        attention probabilities.\n      initializer_range: float. Range of the weight initializer.\n      do_return_2d_tensor: bool. If True, the output will be of shape [batch_size\n        * from_seq_length, num_attention_heads * size_per_head]. If False, the\n        output will be of shape [batch_size, from_seq_length, num_attention_heads\n        * size_per_head].\n      batch_size: (Optional) int. If the input is 2D, this might be the batch size\n        of the 3D version of the `from_tensor` and `to_tensor`.\n      from_seq_length: (Optional) If the input is 2D, this might be the seq length\n        of the 3D version of the `from_tensor`.\n      to_seq_length: (Optional) If the input is 2D, this might be the seq length\n        of the 3D version of the `to_tensor`.\n\n    Returns:\n      float Tensor of shape [batch_size, from_seq_length,\n        num_attention_heads * size_per_head]. (If `do_return_2d_tensor` is\n        true, this will be of shape [batch_size * from_seq_length,\n        num_attention_heads * size_per_head]).\n\n    Raises:\n      ValueError: Any of the arguments or tensor shapes are invalid.\n    \"\"\"", "\n", "\n", "to_tensor_2d", "=", "reshape_to_matrix", "(", "to_tensor", ")", "\n", "\n", "# `value_layer` = [B*T, N*H]", "\n", "value_layer", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "to_tensor_2d", ",", "\n", "num_attention_heads", "*", "size_per_head", ",", "\n", "activation", "=", "value_act", ",", "\n", "name", "=", "\"value\"", ",", "\n", "kernel_initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "\n", "# Normalize the attention scores to probabilities.", "\n", "# `attention_probs` = [B, N, F, T]", "\n", "attention_scores", "=", "attention_scores_layer", "(", "from_tensor", ",", "\n", "to_tensor", ",", "\n", "attention_mask", ",", "\n", "num_attention_heads", ",", "\n", "size_per_head", ",", "\n", "query_act", ",", "\n", "key_act", ",", "\n", "initializer_range", ",", "\n", "batch_size", ",", "\n", "from_seq_length", ",", "\n", "to_seq_length", ")", "\n", "\n", "# Normalize the attention scores to probabilities.", "\n", "# `attention_probs` = [B, N, F, T]", "\n", "attention_probs", "=", "tf", ".", "nn", ".", "softmax", "(", "attention_scores", ")", "\n", "\n", "# This is actually dropping out entire tokens to attend to, which might", "\n", "# seem a bit unusual, but is taken from the original Transformer paper.", "\n", "attention_probs", "=", "tf", ".", "layers", ".", "dropout", "(", "attention_probs", ",", "attention_probs_dropout_prob", ",", "training", "=", "is_training", ")", "\n", "\n", "# `value_layer` = [B, T, N, H]", "\n", "value_layer", "=", "tf", ".", "reshape", "(", "\n", "value_layer", ",", "\n", "[", "batch_size", ",", "to_seq_length", ",", "num_attention_heads", ",", "size_per_head", "]", ")", "\n", "\n", "# `value_layer` = [B, N, T, H]", "\n", "value_layer", "=", "tf", ".", "transpose", "(", "value_layer", ",", "[", "0", ",", "2", ",", "1", ",", "3", "]", ")", "\n", "\n", "# `context_layer` = [B, N, F, H]", "\n", "context_layer", "=", "tf", ".", "matmul", "(", "attention_probs", ",", "value_layer", ")", "\n", "\n", "# `context_layer` = [B, F, N, H]", "\n", "context_layer", "=", "tf", ".", "transpose", "(", "context_layer", ",", "[", "0", ",", "2", ",", "1", ",", "3", "]", ")", "\n", "\n", "if", "do_return_2d_tensor", ":", "\n", "# `context_layer` = [B*F, N*H]", "\n", "        ", "context_layer", "=", "tf", ".", "reshape", "(", "\n", "context_layer", ",", "\n", "[", "batch_size", "*", "from_seq_length", ",", "num_attention_heads", "*", "size_per_head", "]", ")", "\n", "", "else", ":", "\n", "# `context_layer` = [B, F, N*H]", "\n", "        ", "context_layer", "=", "tf", ".", "reshape", "(", "\n", "context_layer", ",", "\n", "[", "batch_size", ",", "from_seq_length", ",", "num_attention_heads", "*", "size_per_head", "]", ")", "\n", "\n", "", "return", "context_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.transformer_model": [[845, 986], ["int", "modeling.get_shape_list", "modeling.reshape_to_matrix", "range", "ValueError", "ValueError", "modeling.reshape_from_matrix", "tensorflow.variable_scope", "modeling.reshape_from_matrix", "final_outputs.append", "tensorflow.variable_scope", "tensorflow.variable_scope", "tensorflow.layers.dense", "tensorflow.variable_scope", "tensorflow.layers.dense", "tensorflow.layers.dropout", "modeling.layer_norm", "all_layer_outputs.append", "tensorflow.variable_scope", "modeling.attention_layer", "attention_heads.append", "len", "tensorflow.concat", "tensorflow.variable_scope", "tensorflow.layers.dense", "tensorflow.layers.dropout", "modeling.layer_norm", "modeling.create_initializer", "modeling.create_initializer", "modeling.create_initializer"], "function", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.get_shape_list", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.reshape_to_matrix", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.reshape_from_matrix", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.reshape_from_matrix", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.dropout", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.layer_norm", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.attention_layer", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.dropout", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.layer_norm", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.create_initializer", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.create_initializer", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.create_initializer"], ["", "def", "transformer_model", "(", "input_tensor", ",", "\n", "attention_mask", "=", "None", ",", "\n", "hidden_size", "=", "768", ",", "\n", "num_hidden_layers", "=", "12", ",", "\n", "num_attention_heads", "=", "12", ",", "\n", "intermediate_size", "=", "3072", ",", "\n", "intermediate_act_fn", "=", "gelu", ",", "\n", "hidden_dropout_prob", "=", "0.1", ",", "\n", "attention_probs_dropout_prob", "=", "0.1", ",", "\n", "initializer_range", "=", "0.02", ",", "\n", "do_return_all_layers", "=", "False", ",", "\n", "is_training", "=", "True", ")", ":", "\n", "    ", "\"\"\"Multi-headed, multi-layer Transformer from \"Attention is All You Need\".\n\n    This is almost an exact implementation of the original Transformer encoder.\n\n    See the original paper:\n    https://arxiv.org/abs/1706.03762\n\n    Also see:\n    https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py\n\n    Args:\n      input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].\n      attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length,\n        seq_length], with 1 for positions that can be attended to and 0 in\n        positions that should not be.\n      hidden_size: int. Hidden size of the Transformer.\n      num_hidden_layers: int. Number of layers (blocks) in the Transformer.\n      num_attention_heads: int. Number of attention heads in the Transformer.\n      intermediate_size: int. The size of the \"intermediate\" (a.k.a., feed\n        forward) layer.\n      intermediate_act_fn: function. The non-linear activation function to apply\n        to the output of the intermediate/feed-forward layer.\n      hidden_dropout_prob: float. Dropout probability for the hidden layers.\n      attention_probs_dropout_prob: float. Dropout probability of the attention\n        probabilities.\n      initializer_range: float. Range of the initializer (stddev of truncated\n        normal).\n      do_return_all_layers: Whether to also return all layers or just the final\n        layer.\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size], the final\n      hidden layer of the Transformer.\n\n    Raises:\n      ValueError: A Tensor shape or parameter is invalid.\n    \"\"\"", "\n", "if", "hidden_size", "%", "num_attention_heads", "!=", "0", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"The hidden size (%d) is not a multiple of the number of attention \"", "\n", "\"heads (%d)\"", "%", "(", "hidden_size", ",", "num_attention_heads", ")", ")", "\n", "\n", "", "attention_head_size", "=", "int", "(", "hidden_size", "/", "num_attention_heads", ")", "\n", "input_shape", "=", "get_shape_list", "(", "input_tensor", ",", "expected_rank", "=", "3", ")", "\n", "batch_size", "=", "input_shape", "[", "0", "]", "\n", "seq_length", "=", "input_shape", "[", "1", "]", "\n", "input_width", "=", "input_shape", "[", "2", "]", "\n", "\n", "# The Transformer performs sum residuals on all layers so the input needs", "\n", "# to be the same as the hidden size.", "\n", "if", "input_width", "!=", "hidden_size", ":", "\n", "        ", "raise", "ValueError", "(", "\"The width of the input tensor (%d) != hidden size (%d)\"", "%", "\n", "(", "input_width", ",", "hidden_size", ")", ")", "\n", "\n", "# We keep the representation as a 2D tensor to avoid re-shaping it back and", "\n", "# forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on", "\n", "# the GPU/CPU but may not be free on the TPU, so we want to minimize them to", "\n", "# help the optimizer.", "\n", "", "prev_output", "=", "reshape_to_matrix", "(", "input_tensor", ")", "\n", "\n", "all_layer_outputs", "=", "[", "]", "\n", "for", "layer_idx", "in", "range", "(", "num_hidden_layers", ")", ":", "\n", "        ", "with", "tf", ".", "variable_scope", "(", "\"layer_%d\"", "%", "layer_idx", ")", ":", "\n", "            ", "layer_input", "=", "prev_output", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "\"attention\"", ")", ":", "\n", "                ", "attention_heads", "=", "[", "]", "\n", "with", "tf", ".", "variable_scope", "(", "\"self\"", ")", ":", "\n", "                    ", "attention_head", "=", "attention_layer", "(", "\n", "from_tensor", "=", "layer_input", ",", "\n", "to_tensor", "=", "layer_input", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "num_attention_heads", "=", "num_attention_heads", ",", "\n", "size_per_head", "=", "attention_head_size", ",", "\n", "attention_probs_dropout_prob", "=", "attention_probs_dropout_prob", ",", "\n", "initializer_range", "=", "initializer_range", ",", "\n", "do_return_2d_tensor", "=", "True", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "from_seq_length", "=", "seq_length", ",", "\n", "to_seq_length", "=", "seq_length", ",", "\n", "is_training", "=", "is_training", ")", "\n", "attention_heads", ".", "append", "(", "attention_head", ")", "\n", "\n", "", "attention_output", "=", "None", "\n", "if", "len", "(", "attention_heads", ")", "==", "1", ":", "\n", "                    ", "attention_output", "=", "attention_heads", "[", "0", "]", "\n", "", "else", ":", "\n", "# In the case where we have other sequences, we just concatenate", "\n", "# them to the self-attention head before the projection.", "\n", "                    ", "attention_output", "=", "tf", ".", "concat", "(", "attention_heads", ",", "axis", "=", "-", "1", ")", "\n", "\n", "# Run a linear projection of `hidden_size` then add a residual", "\n", "# with `layer_input`.", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"output\"", ")", ":", "\n", "                    ", "attention_output", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "attention_output", ",", "\n", "hidden_size", ",", "\n", "kernel_initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "attention_output", "=", "tf", ".", "layers", ".", "dropout", "(", "attention_output", ",", "hidden_dropout_prob", ",", "training", "=", "is_training", ")", "\n", "attention_output", "=", "layer_norm", "(", "attention_output", "+", "layer_input", ")", "\n", "\n", "# The activation is only applied to the \"intermediate\" hidden layer.", "\n", "", "", "with", "tf", ".", "variable_scope", "(", "\"intermediate\"", ")", ":", "\n", "                ", "intermediate_output", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "attention_output", ",", "\n", "intermediate_size", ",", "\n", "activation", "=", "intermediate_act_fn", ",", "\n", "kernel_initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "\n", "# Down-project back to `hidden_size` then add the residual.", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"output\"", ")", ":", "\n", "                ", "layer_output", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "intermediate_output", ",", "\n", "hidden_size", ",", "\n", "kernel_initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "layer_output", "=", "tf", ".", "layers", ".", "dropout", "(", "layer_output", ",", "hidden_dropout_prob", ",", "training", "=", "is_training", ")", "\n", "layer_output", "=", "layer_norm", "(", "layer_output", "+", "attention_output", ")", "\n", "prev_output", "=", "layer_output", "\n", "all_layer_outputs", ".", "append", "(", "layer_output", ")", "\n", "\n", "", "", "", "if", "do_return_all_layers", ":", "\n", "        ", "final_outputs", "=", "[", "]", "\n", "for", "layer_output", "in", "all_layer_outputs", ":", "\n", "            ", "final_output", "=", "reshape_from_matrix", "(", "layer_output", ",", "input_shape", ")", "\n", "final_outputs", ".", "append", "(", "final_output", ")", "\n", "", "return", "final_outputs", "\n", "", "else", ":", "\n", "        ", "final_output", "=", "reshape_from_matrix", "(", "prev_output", ",", "input_shape", ")", "\n", "return", "final_output", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.get_shape_list": [[988, 1023], ["tensor.shape.as_list", "enumerate", "tensorflow.shape", "modeling.assert_rank", "non_static_indexes.append"], "function", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.shape", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.assert_rank"], ["", "", "def", "get_shape_list", "(", "tensor", ",", "expected_rank", "=", "None", ",", "name", "=", "None", ")", ":", "\n", "    ", "\"\"\"Returns a list of the shape of tensor, preferring static dimensions.\n\n    Args:\n      tensor: A tf.Tensor object to find the shape of.\n      expected_rank: (optional) int. The expected rank of `tensor`. If this is\n        specified and the `tensor` has a different rank, and exception will be\n        thrown.\n      name: Optional name of the tensor for the error message.\n\n    Returns:\n      A list of dimensions of the shape of tensor. All static dimensions will\n      be returned as python integers, and dynamic dimensions will be returned\n      as tf.Tensor scalars.\n    \"\"\"", "\n", "if", "name", "is", "None", ":", "\n", "        ", "name", "=", "tensor", ".", "name", "\n", "\n", "", "if", "expected_rank", "is", "not", "None", ":", "\n", "        ", "assert_rank", "(", "tensor", ",", "expected_rank", ",", "name", ")", "\n", "\n", "", "shape", "=", "tensor", ".", "shape", ".", "as_list", "(", ")", "\n", "\n", "non_static_indexes", "=", "[", "]", "\n", "for", "(", "index", ",", "dim", ")", "in", "enumerate", "(", "shape", ")", ":", "\n", "        ", "if", "dim", "is", "None", ":", "\n", "            ", "non_static_indexes", ".", "append", "(", "index", ")", "\n", "\n", "", "", "if", "not", "non_static_indexes", ":", "\n", "        ", "return", "shape", "\n", "\n", "", "dyn_shape", "=", "tf", ".", "shape", "(", "tensor", ")", "\n", "for", "index", "in", "non_static_indexes", ":", "\n", "        ", "shape", "[", "index", "]", "=", "dyn_shape", "[", "index", "]", "\n", "", "return", "shape", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.reshape_to_matrix": [[1025, 1037], ["tensorflow.reshape", "ValueError"], "function", ["None"], ["", "def", "reshape_to_matrix", "(", "input_tensor", ")", ":", "\n", "    ", "\"\"\"Reshapes a >= rank 2 tensor to a rank 2 tensor (i.e., a matrix).\"\"\"", "\n", "ndims", "=", "input_tensor", ".", "shape", ".", "ndims", "\n", "if", "ndims", "<", "2", ":", "\n", "        ", "raise", "ValueError", "(", "\"Input tensor must have at least rank 2. Shape = %s\"", "%", "\n", "(", "input_tensor", ".", "shape", ")", ")", "\n", "", "if", "ndims", "==", "2", ":", "\n", "        ", "return", "input_tensor", "\n", "\n", "", "width", "=", "input_tensor", ".", "shape", "[", "-", "1", "]", "\n", "output_tensor", "=", "tf", ".", "reshape", "(", "input_tensor", ",", "[", "-", "1", ",", "width", "]", ")", "\n", "return", "output_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.reshape_from_matrix": [[1039, 1050], ["modeling.get_shape_list", "tensorflow.reshape", "len"], "function", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.get_shape_list"], ["", "def", "reshape_from_matrix", "(", "output_tensor", ",", "orig_shape_list", ")", ":", "\n", "    ", "\"\"\"Reshapes a rank 2 tensor back to its original rank >= 2 tensor.\"\"\"", "\n", "if", "len", "(", "orig_shape_list", ")", "==", "2", ":", "\n", "        ", "return", "output_tensor", "\n", "\n", "", "output_shape", "=", "get_shape_list", "(", "output_tensor", ")", "\n", "\n", "orig_dims", "=", "orig_shape_list", "[", "0", ":", "-", "1", "]", "\n", "width", "=", "output_shape", "[", "-", "1", "]", "\n", "\n", "return", "tf", ".", "reshape", "(", "output_tensor", ",", "orig_dims", "+", "[", "width", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.assert_rank": [[1052, 1080], ["isinstance", "ValueError", "tensorflow.get_variable_scope", "str", "str"], "function", ["None"], ["", "def", "assert_rank", "(", "tensor", ",", "expected_rank", ",", "name", "=", "None", ")", ":", "\n", "    ", "\"\"\"Raises an exception if the tensor rank is not of the expected rank.\n\n    Args:\n      tensor: A tf.Tensor to check the rank of.\n      expected_rank: Python integer or list of integers, expected rank.\n      name: Optional name of the tensor for the error message.\n\n    Raises:\n      ValueError: If the expected shape doesn't match the actual shape.\n    \"\"\"", "\n", "if", "name", "is", "None", ":", "\n", "        ", "name", "=", "tensor", ".", "name", "\n", "\n", "", "expected_rank_dict", "=", "{", "}", "\n", "if", "isinstance", "(", "expected_rank", ",", "six", ".", "integer_types", ")", ":", "\n", "        ", "expected_rank_dict", "[", "expected_rank", "]", "=", "True", "\n", "", "else", ":", "\n", "        ", "for", "x", "in", "expected_rank", ":", "\n", "            ", "expected_rank_dict", "[", "x", "]", "=", "True", "\n", "\n", "", "", "actual_rank", "=", "tensor", ".", "shape", ".", "ndims", "\n", "if", "actual_rank", "not", "in", "expected_rank_dict", ":", "\n", "        ", "scope_name", "=", "tf", ".", "get_variable_scope", "(", ")", ".", "name", "\n", "raise", "ValueError", "(", "\n", "\"For the tensor `%s` in scope `%s`, the actual rank \"", "\n", "\"`%d` (shape = %s) is not equal to the expected rank `%s`\"", "%", "\n", "(", "name", ",", "scope_name", ",", "actual_rank", ",", "str", "(", "tensor", ".", "shape", ")", ",", "str", "(", "expected_rank", ")", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.extract_features.input_fn_builder": [[93, 121], ["tensorflow.data.Dataset.from_generator", "d.batch.batch", "functools.partial", "dict", "dict", "tensorflow.TensorShape", "tensorflow.TensorShape", "tensorflow.TensorShape", "tensorflow.TensorShape", "tensorflow.TensorShape"], "function", ["None"], ["def", "input_fn_builder", "(", "examples", ",", "window_size", ",", "stride", ",", "tokenizer", ")", ":", "\n", "    ", "\"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"", "\n", "\n", "def", "input_fn", "(", "params", ")", ":", "\n", "        ", "\"\"\"The actual input function.\"\"\"", "\n", "batch_size", "=", "params", "[", "\"batch_size\"", "]", "\n", "\n", "d", "=", "tf", ".", "data", ".", "Dataset", ".", "from_generator", "(", "\n", "functools", ".", "partial", "(", "convert_examples_to_features", ",", "\n", "examples", "=", "examples", ",", "\n", "window_size", "=", "window_size", ",", "\n", "stride", "=", "stride", ",", "\n", "tokenizer", "=", "tokenizer", ")", ",", "\n", "dict", "(", "unique_ids", "=", "tf", ".", "int32", ",", "\n", "input_ids", "=", "tf", ".", "int32", ",", "\n", "input_mask", "=", "tf", ".", "int32", ",", "\n", "input_type_ids", "=", "tf", ".", "int32", ",", "\n", "extract_indices", "=", "tf", ".", "int32", ")", ",", "\n", "dict", "(", "unique_ids", "=", "tf", ".", "TensorShape", "(", "[", "]", ")", ",", "\n", "input_ids", "=", "tf", ".", "TensorShape", "(", "[", "window_size", "]", ")", ",", "\n", "input_mask", "=", "tf", ".", "TensorShape", "(", "[", "window_size", "]", ")", ",", "\n", "input_type_ids", "=", "tf", ".", "TensorShape", "(", "[", "window_size", "]", ")", ",", "\n", "extract_indices", "=", "tf", ".", "TensorShape", "(", "[", "window_size", "]", ")", ")", ")", "\n", "\n", "d", "=", "d", ".", "batch", "(", "batch_size", "=", "batch_size", ",", "drop_remainder", "=", "False", ")", "\n", "return", "d", "\n", "\n", "", "return", "input_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.extract_features.model_fn_builder": [[123, 185], ["modeling.BertModel", "tensorflow.trainable_variables", "modeling.get_assignment_map_from_checkpoint", "tensorflow.logging.info", "modeling.BertModel.get_all_encoder_layers", "enumerate", "tensorflow.contrib.tpu.TPUEstimatorSpec", "ValueError", "tensorflow.train.init_from_checkpoint", "tensorflow.logging.info", "tensorflow.train.init_from_checkpoint", "tensorflow.train.Scaffold"], "function", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.get_assignment_map_from_checkpoint", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.BertModel.get_all_encoder_layers"], ["", "def", "model_fn_builder", "(", "bert_config", ",", "init_checkpoint", ",", "layer_indexes", ",", "use_tpu", ",", "\n", "use_one_hot_embeddings", ")", ":", "\n", "    ", "\"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"", "\n", "\n", "def", "model_fn", "(", "features", ",", "labels", ",", "mode", ",", "params", ")", ":", "# pylint: disable=unused-argument", "\n", "        ", "\"\"\"The `model_fn` for TPUEstimator.\"\"\"", "\n", "\n", "unique_ids", "=", "features", "[", "\"unique_ids\"", "]", "\n", "input_ids", "=", "features", "[", "\"input_ids\"", "]", "\n", "input_mask", "=", "features", "[", "\"input_mask\"", "]", "\n", "input_type_ids", "=", "features", "[", "\"input_type_ids\"", "]", "\n", "extract_indices", "=", "features", "[", "\"extract_indices\"", "]", "\n", "\n", "model", "=", "modeling", ".", "BertModel", "(", "\n", "config", "=", "bert_config", ",", "\n", "is_training", "=", "False", ",", "\n", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "token_type_ids", "=", "input_type_ids", ",", "\n", "use_one_hot_embeddings", "=", "use_one_hot_embeddings", ")", "\n", "\n", "if", "mode", "!=", "tf", ".", "estimator", ".", "ModeKeys", ".", "PREDICT", ":", "\n", "            ", "raise", "ValueError", "(", "\"Only PREDICT modes are supported: %s\"", "%", "(", "mode", ")", ")", "\n", "\n", "", "tvars", "=", "tf", ".", "trainable_variables", "(", ")", "\n", "scaffold_fn", "=", "None", "\n", "(", "assignment_map", ",", "\n", "initialized_variable_names", ")", "=", "modeling", ".", "get_assignment_map_from_checkpoint", "(", "\n", "tvars", ",", "init_checkpoint", ")", "\n", "if", "use_tpu", ":", "\n", "\n", "            ", "def", "tpu_scaffold", "(", ")", ":", "\n", "                ", "tf", ".", "train", ".", "init_from_checkpoint", "(", "init_checkpoint", ",", "assignment_map", ")", "\n", "return", "tf", ".", "train", ".", "Scaffold", "(", ")", "\n", "\n", "", "scaffold_fn", "=", "tpu_scaffold", "\n", "", "else", ":", "\n", "            ", "tf", ".", "train", ".", "init_from_checkpoint", "(", "init_checkpoint", ",", "assignment_map", ")", "\n", "\n", "", "tf", ".", "logging", ".", "info", "(", "\"**** Trainable Variables ****\"", ")", "\n", "for", "var", "in", "tvars", ":", "\n", "            ", "init_string", "=", "\"\"", "\n", "if", "var", ".", "name", "in", "initialized_variable_names", ":", "\n", "                ", "init_string", "=", "\", *INIT_FROM_CKPT*\"", "\n", "", "tf", ".", "logging", ".", "info", "(", "\"  name = %s, shape = %s%s\"", ",", "var", ".", "name", ",", "var", ".", "shape", ",", "\n", "init_string", ")", "\n", "\n", "", "all_layers", "=", "model", ".", "get_all_encoder_layers", "(", ")", "\n", "\n", "predictions", "=", "{", "\n", "\"unique_ids\"", ":", "unique_ids", ",", "\n", "\"extract_indices\"", ":", "extract_indices", "\n", "}", "\n", "\n", "for", "(", "i", ",", "layer_index", ")", "in", "enumerate", "(", "layer_indexes", ")", ":", "\n", "            ", "predictions", "[", "\"layer_output_%d\"", "%", "i", "]", "=", "all_layers", "[", "layer_index", "]", "\n", "\n", "", "output_spec", "=", "tf", ".", "contrib", ".", "tpu", ".", "TPUEstimatorSpec", "(", "\n", "mode", "=", "mode", ",", "predictions", "=", "predictions", ",", "scaffold_fn", "=", "scaffold_fn", ")", "\n", "return", "output_spec", "\n", "\n", "", "return", "model_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.extract_features._convert_example_to_features": [[187, 222], ["tokenizer.convert_tokens_to_ids", "dict", "tokens.append", "input_type_ids.append", "len", "len", "tokenizer.convert_tokens_to_ids.append", "input_mask.append", "input_type_ids.append", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.tokenization.convert_tokens_to_ids"], ["", "def", "_convert_example_to_features", "(", "example", ",", "window_start", ",", "window_end", ",", "tokens_ids_to_extract", ",", "tokenizer", ",", "seq_length", ")", ":", "\n", "    ", "window_tokens", "=", "example", ".", "tokens", "[", "window_start", ":", "window_end", "]", "\n", "\n", "tokens", "=", "[", "]", "\n", "input_type_ids", "=", "[", "]", "\n", "for", "token", "in", "window_tokens", ":", "\n", "        ", "tokens", ".", "append", "(", "token", ")", "\n", "input_type_ids", ".", "append", "(", "0", ")", "\n", "\n", "", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "\n", "# The mask has 1 for real tokens and 0 for padding tokens. Only real", "\n", "# tokens are attended to.", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "# Zero-pad up to the sequence length.", "\n", "while", "len", "(", "input_ids", ")", "<", "seq_length", ":", "\n", "        ", "input_ids", ".", "append", "(", "0", ")", "\n", "input_mask", ".", "append", "(", "0", ")", "\n", "input_type_ids", ".", "append", "(", "0", ")", "\n", "\n", "", "extract_indices", "=", "[", "-", "1", "]", "*", "seq_length", "\n", "for", "i", "in", "tokens_ids_to_extract", ":", "\n", "        ", "assert", "i", "-", "window_start", ">=", "0", "\n", "extract_indices", "[", "i", "-", "window_start", "]", "=", "i", "\n", "\n", "", "assert", "len", "(", "input_ids", ")", "==", "seq_length", "\n", "assert", "len", "(", "input_mask", ")", "==", "seq_length", "\n", "assert", "len", "(", "input_type_ids", ")", "==", "seq_length", "\n", "\n", "return", "dict", "(", "unique_ids", "=", "example", ".", "document_index", ",", "\n", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "input_type_ids", "=", "input_type_ids", ",", "\n", "extract_indices", "=", "extract_indices", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.extract_features.convert_examples_to_features": [[224, 253], ["range", "len", "int", "int", "token_ids_to_extract.extend", "numpy.clip", "numpy.clip", "token_ids_to_extract.extend", "range", "len", "token_ids_to_extract.extend", "extract_features._convert_example_to_features", "len", "len", "range", "range", "min", "len", "len"], "function", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.extract_features._convert_example_to_features"], ["", "def", "convert_examples_to_features", "(", "examples", ",", "window_size", ",", "stride", ",", "tokenizer", ")", ":", "\n", "    ", "\"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"", "\n", "\n", "assert", "window_size", "%", "2", "==", "1", "\n", "assert", "stride", "%", "2", "==", "1", "\n", "\n", "for", "example", "in", "examples", ":", "\n", "        ", "for", "i", "in", "range", "(", "0", ",", "len", "(", "example", ".", "tokens", ")", ",", "stride", ")", ":", "\n", "            ", "window_center", "=", "i", "+", "window_size", "//", "2", "\n", "token_ids_to_extract", "=", "[", "]", "\n", "extract_start", "=", "int", "(", "np", ".", "clip", "(", "window_center", "-", "stride", "//", "2", ",", "0", ",", "len", "(", "example", ".", "tokens", ")", ")", ")", "\n", "extract_end", "=", "int", "(", "np", ".", "clip", "(", "window_center", "+", "stride", "//", "2", "+", "1", ",", "extract_start", ",", "len", "(", "example", ".", "tokens", ")", ")", ")", "\n", "\n", "if", "i", "==", "0", ":", "\n", "                ", "token_ids_to_extract", ".", "extend", "(", "range", "(", "extract_start", ")", ")", "\n", "\n", "", "token_ids_to_extract", ".", "extend", "(", "range", "(", "extract_start", ",", "extract_end", ")", ")", "\n", "\n", "if", "i", "+", "stride", ">=", "len", "(", "example", ".", "tokens", ")", ":", "\n", "                ", "token_ids_to_extract", ".", "extend", "(", "range", "(", "extract_end", ",", "len", "(", "example", ".", "tokens", ")", ")", ")", "\n", "\n", "", "token_ids_to_extract", "=", "[", "t", "for", "t", "in", "token_ids_to_extract", "if", "example", ".", "bert_to_orig_map", "[", "t", "]", ">=", "0", "]", "\n", "\n", "yield", "_convert_example_to_features", "(", "example", ",", "\n", "i", ",", "\n", "min", "(", "i", "+", "window_size", ",", "len", "(", "example", ".", "tokens", ")", ")", ",", "\n", "token_ids_to_extract", ",", "\n", "tokenizer", ",", "\n", "window_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.extract_features.main": [[255, 331], ["tensorflow.logging.set_verbosity", "modeling.BertConfig.from_json_file", "tokenization.FullTokenizer", "tensorflow.contrib.tpu.RunConfig", "FLAGS.input_file.split", "enumerate", "extract_features.model_fn_builder", "tensorflow.contrib.tpu.TPUEstimator", "extract_features.input_fn_builder", "h5py.File", "h5py.File.close", "int", "data.process_example", "orig_examples.append", "bert_examples.append", "tqdm.tqdm", "tf.contrib.tpu.TPUEstimator.predict", "FLAGS.layers.split", "tensorflow.contrib.tpu.TPUConfig", "open", "json_examples.extend", "data.process_example.bertify", "int", "bert_example.doc_key.replace", "t.update", "enumerate", "sum", "orig_example.unravel_token_index", "enumerate", "json.loads", "h5py.File.create_dataset", "f.readlines", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.modeling.BertConfig.from_json_file", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.extract_features.model_fn_builder", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.extract_features.input_fn_builder", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.data.process_example", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.data.Example.bertify", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.data.Example.unravel_token_index"], ["", "", "", "def", "main", "(", "_", ")", ":", "\n", "    ", "tf", ".", "logging", ".", "set_verbosity", "(", "tf", ".", "logging", ".", "INFO", ")", "\n", "\n", "layer_indexes", "=", "[", "int", "(", "x", ")", "for", "x", "in", "FLAGS", ".", "layers", ".", "split", "(", "\",\"", ")", "]", "\n", "\n", "bert_config", "=", "modeling", ".", "BertConfig", ".", "from_json_file", "(", "FLAGS", ".", "bert_config_file", ")", "\n", "\n", "tokenizer", "=", "tokenization", ".", "FullTokenizer", "(", "\n", "vocab_file", "=", "FLAGS", ".", "vocab_file", ",", "do_lower_case", "=", "FLAGS", ".", "do_lower_case", ")", "\n", "\n", "is_per_host", "=", "tf", ".", "contrib", ".", "tpu", ".", "InputPipelineConfig", ".", "PER_HOST_V2", "\n", "run_config", "=", "tf", ".", "contrib", ".", "tpu", ".", "RunConfig", "(", "\n", "master", "=", "FLAGS", ".", "master", ",", "\n", "tpu_config", "=", "tf", ".", "contrib", ".", "tpu", ".", "TPUConfig", "(", "\n", "num_shards", "=", "FLAGS", ".", "num_tpu_cores", ",", "\n", "per_host_input_for_training", "=", "is_per_host", ")", ")", "\n", "\n", "json_examples", "=", "[", "]", "\n", "for", "file", "in", "FLAGS", ".", "input_file", ".", "split", "(", "';'", ")", ":", "\n", "        ", "with", "open", "(", "file", ")", "as", "f", ":", "\n", "            ", "json_examples", ".", "extend", "(", "(", "json", ".", "loads", "(", "jsonline", ")", "for", "jsonline", "in", "f", ".", "readlines", "(", ")", ")", ")", "\n", "\n", "", "", "orig_examples", "=", "[", "]", "\n", "bert_examples", "=", "[", "]", "\n", "for", "i", ",", "json_e", "in", "enumerate", "(", "json_examples", ")", ":", "\n", "        ", "e", "=", "process_example", "(", "json_e", ",", "i", ")", "\n", "orig_examples", ".", "append", "(", "e", ")", "\n", "bert_examples", ".", "append", "(", "e", ".", "bertify", "(", "tokenizer", ")", ")", "\n", "\n", "", "model_fn", "=", "model_fn_builder", "(", "\n", "bert_config", "=", "bert_config", ",", "\n", "init_checkpoint", "=", "FLAGS", ".", "init_checkpoint", ",", "\n", "layer_indexes", "=", "layer_indexes", ",", "\n", "use_tpu", "=", "FLAGS", ".", "use_tpu", ",", "\n", "use_one_hot_embeddings", "=", "FLAGS", ".", "use_one_hot_embeddings", ",", "\n", ")", "\n", "\n", "# If TPU is not available, this will fall back to normal Estimator on CPU", "\n", "# or GPU.", "\n", "estimator", "=", "tf", ".", "contrib", ".", "tpu", ".", "TPUEstimator", "(", "\n", "use_tpu", "=", "FLAGS", ".", "use_tpu", ",", "\n", "model_fn", "=", "model_fn", ",", "\n", "config", "=", "run_config", ",", "\n", "predict_batch_size", "=", "FLAGS", ".", "batch_size", ")", "\n", "\n", "input_fn", "=", "input_fn_builder", "(", "\n", "examples", "=", "bert_examples", ",", "window_size", "=", "FLAGS", ".", "window_size", ",", "stride", "=", "FLAGS", ".", "stride", ",", "tokenizer", "=", "tokenizer", ")", "\n", "\n", "writer", "=", "h5py", ".", "File", "(", "FLAGS", ".", "output_file", ",", "'w'", ")", "\n", "with", "tqdm", "(", "total", "=", "sum", "(", "len", "(", "e", ".", "tokens", ")", "for", "e", "in", "orig_examples", ")", ")", "as", "t", ":", "\n", "        ", "for", "result", "in", "estimator", ".", "predict", "(", "input_fn", ",", "yield_single_examples", "=", "True", ")", ":", "\n", "            ", "document_index", "=", "int", "(", "result", "[", "\"unique_ids\"", "]", ")", "\n", "bert_example", "=", "bert_examples", "[", "document_index", "]", "\n", "orig_example", "=", "orig_examples", "[", "document_index", "]", "\n", "file_key", "=", "bert_example", ".", "doc_key", ".", "replace", "(", "'/'", ",", "':'", ")", "\n", "\n", "t", ".", "update", "(", "n", "=", "(", "result", "[", "'extract_indices'", "]", ">=", "0", ")", ".", "sum", "(", ")", ")", "\n", "\n", "for", "output_index", ",", "bert_token_index", "in", "enumerate", "(", "result", "[", "'extract_indices'", "]", ")", ":", "\n", "                ", "if", "bert_token_index", "<", "0", ":", "\n", "                    ", "continue", "\n", "\n", "", "token_index", "=", "bert_example", ".", "bert_to_orig_map", "[", "bert_token_index", "]", "\n", "sentence_index", ",", "token_index", "=", "orig_example", ".", "unravel_token_index", "(", "token_index", ")", "\n", "\n", "dataset_key", "=", "\"{}/{}\"", ".", "format", "(", "file_key", ",", "sentence_index", ")", "\n", "if", "dataset_key", "not", "in", "writer", ":", "\n", "                    ", "writer", ".", "create_dataset", "(", "dataset_key", ",", "\n", "(", "len", "(", "orig_example", ".", "sentence_tokens", "[", "sentence_index", "]", ")", ",", "bert_config", ".", "hidden_size", ",", "len", "(", "layer_indexes", ")", ")", ",", "\n", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "", "dset", "=", "writer", "[", "dataset_key", "]", "\n", "for", "j", ",", "layer_index", "in", "enumerate", "(", "layer_indexes", ")", ":", "\n", "                    ", "layer_output", "=", "result", "[", "\"layer_output_%d\"", "%", "j", "]", "\n", "dset", "[", "token_index", ",", ":", ",", "j", "]", "=", "layer_output", "[", "output_index", "]", "\n", "", "", "", "", "writer", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.data.Example.__init__": [[9, 17], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "doc_key", ",", "tokens", ",", "sentence_tokens", ",", "document_index", ",", "\n", "offset", "=", "0", ",", "bert_to_orig_map", "=", "None", ")", ":", "\n", "        ", "self", ".", "doc_key", "=", "doc_key", "\n", "self", ".", "tokens", "=", "tokens", "\n", "self", ".", "sentence_tokens", "=", "sentence_tokens", "\n", "self", ".", "document_index", "=", "document_index", "\n", "self", ".", "offset", "=", "offset", "\n", "self", ".", "bert_to_orig_map", "=", "bert_to_orig_map", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.data.Example.bertify": [[19, 40], ["enumerate", "data.Example", "tokenizer.tokenize", "orig_to_bert_map.append", "orig_to_bert_end_map.append", "bert_tokens.extend", "tokenizer.tokenize", "len", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.tokenization.WordpieceTokenizer.tokenize"], ["", "def", "bertify", "(", "self", ",", "tokenizer", ")", ":", "\n", "        ", "assert", "self", ".", "offset", "==", "0", "\n", "\n", "bert_tokens", "=", "[", "]", "\n", "orig_to_bert_map", "=", "[", "]", "\n", "orig_to_bert_end_map", "=", "[", "]", "\n", "for", "t", "in", "self", ".", "tokens", ":", "\n", "            ", "bert_t", "=", "tokenizer", ".", "tokenize", "(", "t", ")", "\n", "orig_to_bert_map", ".", "append", "(", "len", "(", "bert_tokens", ")", ")", "\n", "orig_to_bert_end_map", ".", "append", "(", "len", "(", "bert_tokens", ")", "+", "len", "(", "bert_t", ")", "-", "1", ")", "\n", "bert_tokens", ".", "extend", "(", "bert_t", ")", "\n", "\n", "", "bert_sentence_tokens", "=", "[", "tokenizer", ".", "tokenize", "(", "' '", ".", "join", "(", "s", ")", ")", "for", "s", "in", "self", ".", "sentence_tokens", "]", "\n", "\n", "bert_to_orig_map", "=", "[", "-", "1", "]", "*", "len", "(", "bert_tokens", ")", "\n", "for", "i", ",", "bert_i", "in", "enumerate", "(", "orig_to_bert_map", ")", ":", "\n", "            ", "bert_to_orig_map", "[", "bert_i", "]", "=", "i", "\n", "\n", "\n", "\n", "", "return", "Example", "(", "self", ".", "doc_key", ",", "bert_tokens", ",", "bert_sentence_tokens", ",", "self", ".", "document_index", ",", "bert_to_orig_map", "=", "bert_to_orig_map", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.data.Example.unravel_token_index": [[41, 50], ["enumerate", "ValueError", "len", "len", "len"], "methods", ["None"], ["", "def", "unravel_token_index", "(", "self", ",", "token_index", ")", ":", "\n", "        ", "prev_sentences_len", "=", "0", "\n", "for", "i", ",", "s", "in", "enumerate", "(", "self", ".", "sentence_tokens", ")", ":", "\n", "            ", "if", "token_index", "<", "prev_sentences_len", "+", "len", "(", "s", ")", ":", "\n", "                ", "token_index_in_sentence", "=", "token_index", "-", "prev_sentences_len", "\n", "return", "i", ",", "token_index_in_sentence", "\n", "", "prev_sentences_len", "+=", "len", "(", "s", ")", "\n", "\n", "", "raise", "ValueError", "(", "'token_index is out of range ({} >= {})'", ",", "token_index", ",", "len", "(", "self", ".", "tokens", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.data.process_example": [[54, 61], ["sum", "data.Example", "tokenization.convert_to_unicode"], "function", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.tokenization.convert_to_unicode"], ["", "", "def", "process_example", "(", "example", ",", "index", ")", ":", "\n", "    ", "sentences", "=", "example", "[", "\"sentences\"", "]", "\n", "sentence_tokens", "=", "[", "[", "tokenization", ".", "convert_to_unicode", "(", "w", ")", "for", "w", "in", "s", "]", "for", "s", "in", "sentences", "]", "\n", "tokens", "=", "sum", "(", "sentence_tokens", ",", "[", "]", ")", "\n", "doc_key", "=", "example", "[", "\"doc_key\"", "]", "\n", "\n", "return", "Example", "(", "doc_key", ",", "tokens", ",", "sentence_tokens", ",", "index", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.tokenization.FullTokenizer.__init__": [[164, 169], ["tokenization.load_vocab", "tokenization.BasicTokenizer", "tokenization.WordpieceTokenizer", "tokenization.FullTokenizer.vocab.items"], "methods", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.tokenization.load_vocab"], ["def", "__init__", "(", "self", ",", "vocab_file", ",", "do_lower_case", "=", "True", ")", ":", "\n", "    ", "self", ".", "vocab", "=", "load_vocab", "(", "vocab_file", ")", "\n", "self", ".", "inv_vocab", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "self", ".", "vocab", ".", "items", "(", ")", "}", "\n", "self", ".", "basic_tokenizer", "=", "BasicTokenizer", "(", "do_lower_case", "=", "do_lower_case", ")", "\n", "self", ".", "wordpiece_tokenizer", "=", "WordpieceTokenizer", "(", "vocab", "=", "self", ".", "vocab", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.tokenization.FullTokenizer.tokenize": [[170, 177], ["tokenization.FullTokenizer.basic_tokenizer.tokenize", "tokenization.FullTokenizer.wordpiece_tokenizer.tokenize", "split_tokens.append"], "methods", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.tokenization.WordpieceTokenizer.tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "    ", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "self", ".", "basic_tokenizer", ".", "tokenize", "(", "text", ")", ":", "\n", "      ", "for", "sub_token", "in", "self", ".", "wordpiece_tokenizer", ".", "tokenize", "(", "token", ")", ":", "\n", "        ", "split_tokens", ".", "append", "(", "sub_token", ")", "\n", "\n", "", "", "return", "split_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.tokenization.FullTokenizer.convert_tokens_to_ids": [[178, 180], ["tokenization.convert_by_vocab"], "methods", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.tokenization.convert_by_vocab"], ["", "def", "convert_tokens_to_ids", "(", "self", ",", "tokens", ")", ":", "\n", "    ", "return", "convert_by_vocab", "(", "self", ".", "vocab", ",", "tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.tokenization.FullTokenizer.convert_ids_to_tokens": [[181, 183], ["tokenization.convert_by_vocab"], "methods", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.tokenization.convert_by_vocab"], ["", "def", "convert_ids_to_tokens", "(", "self", ",", "ids", ")", ":", "\n", "    ", "return", "convert_by_vocab", "(", "self", ".", "inv_vocab", ",", "ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.tokenization.BasicTokenizer.__init__": [[188, 195], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "do_lower_case", "=", "True", ")", ":", "\n", "    ", "\"\"\"Constructs a BasicTokenizer.\n\n    Args:\n      do_lower_case: Whether to lower case the input.\n    \"\"\"", "\n", "self", ".", "do_lower_case", "=", "do_lower_case", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.tokenization.BasicTokenizer.tokenize": [[196, 219], ["tokenization.convert_to_unicode", "tokenization.BasicTokenizer._clean_text", "tokenization.BasicTokenizer._tokenize_chinese_chars", "tokenization.whitespace_tokenize", "tokenization.whitespace_tokenize", "split_tokens.extend", "tokenization.BasicTokenizer.lower", "tokenization.BasicTokenizer._run_strip_accents", "tokenization.BasicTokenizer._run_split_on_punc"], "methods", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.tokenization.convert_to_unicode", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.tokenization.BasicTokenizer._clean_text", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.tokenization.BasicTokenizer._tokenize_chinese_chars", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.tokenization.whitespace_tokenize", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.tokenization.whitespace_tokenize", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.tokenization.BasicTokenizer._run_strip_accents", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.tokenization.BasicTokenizer._run_split_on_punc"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Tokenizes a piece of text.\"\"\"", "\n", "text", "=", "convert_to_unicode", "(", "text", ")", "\n", "text", "=", "self", ".", "_clean_text", "(", "text", ")", "\n", "\n", "# This was added on November 1st, 2018 for the multilingual and Chinese", "\n", "# models. This is also applied to the English models now, but it doesn't", "\n", "# matter since the English models were not trained on any Chinese data", "\n", "# and generally don't have any Chinese data in them (there are Chinese", "\n", "# characters in the vocabulary because Wikipedia does have some Chinese", "\n", "# words in the English Wikipedia.).", "\n", "text", "=", "self", ".", "_tokenize_chinese_chars", "(", "text", ")", "\n", "\n", "orig_tokens", "=", "whitespace_tokenize", "(", "text", ")", "\n", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "orig_tokens", ":", "\n", "      ", "if", "self", ".", "do_lower_case", ":", "\n", "        ", "token", "=", "token", ".", "lower", "(", ")", "\n", "token", "=", "self", ".", "_run_strip_accents", "(", "token", ")", "\n", "", "split_tokens", ".", "extend", "(", "self", ".", "_run_split_on_punc", "(", "token", ")", ")", "\n", "\n", "", "output_tokens", "=", "whitespace_tokenize", "(", "\" \"", ".", "join", "(", "split_tokens", ")", ")", "\n", "return", "output_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.tokenization.BasicTokenizer._run_strip_accents": [[220, 230], ["unicodedata.normalize", "unicodedata.category", "output.append"], "methods", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.None.util.EmbeddingDictionary.normalize"], ["", "def", "_run_strip_accents", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Strips accents from a piece of text.\"\"\"", "\n", "text", "=", "unicodedata", ".", "normalize", "(", "\"NFD\"", ",", "text", ")", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "      ", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Mn\"", ":", "\n", "        ", "continue", "\n", "", "output", ".", "append", "(", "char", ")", "\n", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.tokenization.BasicTokenizer._run_split_on_punc": [[231, 250], ["list", "len", "tokenization._is_punctuation", "output.append", "output[].append", "output.append"], "methods", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.tokenization._is_punctuation"], ["", "def", "_run_split_on_punc", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Splits punctuation on a piece of text.\"\"\"", "\n", "chars", "=", "list", "(", "text", ")", "\n", "i", "=", "0", "\n", "start_new_word", "=", "True", "\n", "output", "=", "[", "]", "\n", "while", "i", "<", "len", "(", "chars", ")", ":", "\n", "      ", "char", "=", "chars", "[", "i", "]", "\n", "if", "_is_punctuation", "(", "char", ")", ":", "\n", "        ", "output", ".", "append", "(", "[", "char", "]", ")", "\n", "start_new_word", "=", "True", "\n", "", "else", ":", "\n", "        ", "if", "start_new_word", ":", "\n", "          ", "output", ".", "append", "(", "[", "]", ")", "\n", "", "start_new_word", "=", "False", "\n", "output", "[", "-", "1", "]", ".", "append", "(", "char", ")", "\n", "", "i", "+=", "1", "\n", "\n", "", "return", "[", "\"\"", ".", "join", "(", "x", ")", "for", "x", "in", "output", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.tokenization.BasicTokenizer._tokenize_chinese_chars": [[251, 263], ["ord", "tokenization.BasicTokenizer._is_chinese_char", "output.append", "output.append", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.tokenization.BasicTokenizer._is_chinese_char"], ["", "def", "_tokenize_chinese_chars", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Adds whitespace around any CJK character.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "      ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "self", ".", "_is_chinese_char", "(", "cp", ")", ":", "\n", "        ", "output", ".", "append", "(", "\" \"", ")", "\n", "output", ".", "append", "(", "char", ")", "\n", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n", "        ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.tokenization.BasicTokenizer._is_chinese_char": [[264, 285], ["None"], "methods", ["None"], ["", "def", "_is_chinese_char", "(", "self", ",", "cp", ")", ":", "\n", "    ", "\"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"", "\n", "# This defines a \"chinese character\" as anything in the CJK Unicode block:", "\n", "#   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)", "\n", "#", "\n", "# Note that the CJK Unicode block is NOT all Japanese and Korean characters,", "\n", "# despite its name. The modern Korean Hangul alphabet is a different block,", "\n", "# as is Japanese Hiragana and Katakana. Those alphabets are used to write", "\n", "# space-separated words, so they are not treated specially and handled", "\n", "# like the all of the other languages.", "\n", "if", "(", "(", "cp", ">=", "0x4E00", "and", "cp", "<=", "0x9FFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x3400", "and", "cp", "<=", "0x4DBF", ")", "or", "#", "\n", "(", "cp", ">=", "0x20000", "and", "cp", "<=", "0x2A6DF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2A700", "and", "cp", "<=", "0x2B73F", ")", "or", "#", "\n", "(", "cp", ">=", "0x2B740", "and", "cp", "<=", "0x2B81F", ")", "or", "#", "\n", "(", "cp", ">=", "0x2B820", "and", "cp", "<=", "0x2CEAF", ")", "or", "\n", "(", "cp", ">=", "0xF900", "and", "cp", "<=", "0xFAFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2F800", "and", "cp", "<=", "0x2FA1F", ")", ")", ":", "#", "\n", "      ", "return", "True", "\n", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.tokenization.BasicTokenizer._clean_text": [[286, 298], ["ord", "tokenization._is_whitespace", "tokenization._is_control", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.tokenization._is_whitespace", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.tokenization._is_control"], ["", "def", "_clean_text", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "      ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "cp", "==", "0", "or", "cp", "==", "0xfffd", "or", "_is_control", "(", "char", ")", ":", "\n", "        ", "continue", "\n", "", "if", "_is_whitespace", "(", "char", ")", ":", "\n", "        ", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n", "        ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.tokenization.WordpieceTokenizer.__init__": [[303, 307], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "vocab", ",", "unk_token", "=", "\"[UNK]\"", ",", "max_input_chars_per_word", "=", "200", ")", ":", "\n", "    ", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "unk_token", "=", "unk_token", "\n", "self", ".", "max_input_chars_per_word", "=", "max_input_chars_per_word", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.tokenization.WordpieceTokenizer.tokenize": [[308, 360], ["tokenization.convert_to_unicode", "tokenization.whitespace_tokenize", "list", "len", "output_tokens.append", "len", "len", "sub_tokens.append", "output_tokens.append", "output_tokens.extend"], "methods", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.tokenization.convert_to_unicode", "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.tokenization.whitespace_tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Tokenizes a piece of text into its word pieces.\n\n    This uses a greedy longest-match-first algorithm to perform tokenization\n    using the given vocabulary.\n\n    For example:\n      input = \"unaffable\"\n      output = [\"un\", \"##aff\", \"##able\"]\n\n    Args:\n      text: A single token or whitespace separated tokens. This should have\n        already been passed through `BasicTokenizer.\n\n    Returns:\n      A list of wordpiece tokens.\n    \"\"\"", "\n", "\n", "text", "=", "convert_to_unicode", "(", "text", ")", "\n", "\n", "output_tokens", "=", "[", "]", "\n", "for", "token", "in", "whitespace_tokenize", "(", "text", ")", ":", "\n", "      ", "chars", "=", "list", "(", "token", ")", "\n", "if", "len", "(", "chars", ")", ">", "self", ".", "max_input_chars_per_word", ":", "\n", "        ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "continue", "\n", "\n", "", "is_bad", "=", "False", "\n", "start", "=", "0", "\n", "sub_tokens", "=", "[", "]", "\n", "while", "start", "<", "len", "(", "chars", ")", ":", "\n", "        ", "end", "=", "len", "(", "chars", ")", "\n", "cur_substr", "=", "None", "\n", "while", "start", "<", "end", ":", "\n", "          ", "substr", "=", "\"\"", ".", "join", "(", "chars", "[", "start", ":", "end", "]", ")", "\n", "if", "start", ">", "0", ":", "\n", "            ", "substr", "=", "\"##\"", "+", "substr", "\n", "", "if", "substr", "in", "self", ".", "vocab", ":", "\n", "            ", "cur_substr", "=", "substr", "\n", "break", "\n", "", "end", "-=", "1", "\n", "", "if", "cur_substr", "is", "None", ":", "\n", "          ", "is_bad", "=", "True", "\n", "break", "\n", "", "sub_tokens", ".", "append", "(", "cur_substr", ")", "\n", "start", "=", "end", "\n", "\n", "", "if", "is_bad", ":", "\n", "        ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "", "else", ":", "\n", "        ", "output_tokens", ".", "extend", "(", "sub_tokens", ")", "\n", "", "", "return", "output_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.tokenization.validate_case_matches_checkpoint": [[28, 76], ["re.match", "re.match.group", "ValueError"], "function", ["None"], ["def", "validate_case_matches_checkpoint", "(", "do_lower_case", ",", "init_checkpoint", ")", ":", "\n", "  ", "\"\"\"Checks whether the casing config is consistent with the checkpoint name.\"\"\"", "\n", "\n", "# The casing has to be passed in by the user and there is no explicit check", "\n", "# as to whether it matches the checkpoint. The casing information probably", "\n", "# should have been stored in the bert_config.json file, but it's not, so", "\n", "# we have to heuristically detect it to validate.", "\n", "\n", "if", "not", "init_checkpoint", ":", "\n", "    ", "return", "\n", "\n", "", "m", "=", "re", ".", "match", "(", "\"^.*?([A-Za-z0-9_-]+)/bert_model.ckpt\"", ",", "init_checkpoint", ")", "\n", "if", "m", "is", "None", ":", "\n", "    ", "return", "\n", "\n", "", "model_name", "=", "m", ".", "group", "(", "1", ")", "\n", "\n", "lower_models", "=", "[", "\n", "\"uncased_L-24_H-1024_A-16\"", ",", "\"uncased_L-12_H-768_A-12\"", ",", "\n", "\"multilingual_L-12_H-768_A-12\"", ",", "\"chinese_L-12_H-768_A-12\"", "\n", "]", "\n", "\n", "cased_models", "=", "[", "\n", "\"cased_L-12_H-768_A-12\"", ",", "\"cased_L-24_H-1024_A-16\"", ",", "\n", "\"multi_cased_L-12_H-768_A-12\"", "\n", "]", "\n", "\n", "is_bad_config", "=", "False", "\n", "if", "model_name", "in", "lower_models", "and", "not", "do_lower_case", ":", "\n", "    ", "is_bad_config", "=", "True", "\n", "actual_flag", "=", "\"False\"", "\n", "case_name", "=", "\"lowercased\"", "\n", "opposite_flag", "=", "\"True\"", "\n", "\n", "", "if", "model_name", "in", "cased_models", "and", "do_lower_case", ":", "\n", "    ", "is_bad_config", "=", "True", "\n", "actual_flag", "=", "\"True\"", "\n", "case_name", "=", "\"cased\"", "\n", "opposite_flag", "=", "\"False\"", "\n", "\n", "", "if", "is_bad_config", ":", "\n", "    ", "raise", "ValueError", "(", "\n", "\"You passed in `--do_lower_case=%s` with `--init_checkpoint=%s`. \"", "\n", "\"However, `%s` seems to be a %s model, so you \"", "\n", "\"should pass in `--do_lower_case=%s` so that the fine-tuning matches \"", "\n", "\"how the model was pre-training. If this error is wrong, please \"", "\n", "\"just comment out this check.\"", "%", "(", "actual_flag", ",", "init_checkpoint", ",", "\n", "model_name", ",", "case_name", ",", "opposite_flag", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.tokenization.convert_to_unicode": [[78, 96], ["isinstance", "isinstance", "isinstance", "ValueError", "text.decode", "ValueError", "text.decode", "isinstance", "ValueError", "type", "type"], "function", ["None"], ["", "", "def", "convert_to_unicode", "(", "text", ")", ":", "\n", "  ", "\"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"", "\n", "if", "six", ".", "PY3", ":", "\n", "    ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "      ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "bytes", ")", ":", "\n", "      ", "return", "text", ".", "decode", "(", "\"utf-8\"", ",", "\"ignore\"", ")", "\n", "", "else", ":", "\n", "      ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "elif", "six", ".", "PY2", ":", "\n", "    ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "      ", "return", "text", ".", "decode", "(", "\"utf-8\"", ",", "\"ignore\"", ")", "\n", "", "elif", "isinstance", "(", "text", ",", "unicode", ")", ":", "\n", "      ", "return", "text", "\n", "", "else", ":", "\n", "      ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "else", ":", "\n", "    ", "raise", "ValueError", "(", "\"Not running on Python2 or Python 3?\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.tokenization.printable_text": [[98, 119], ["isinstance", "isinstance", "isinstance", "ValueError", "text.decode", "ValueError", "isinstance", "text.encode", "ValueError", "type", "type"], "function", ["None"], ["", "", "def", "printable_text", "(", "text", ")", ":", "\n", "  ", "\"\"\"Returns text encoded in a way suitable for print or `tf.logging`.\"\"\"", "\n", "\n", "# These functions want `str` for both Python2 and Python3, but in one case", "\n", "# it's a Unicode string and in the other it's a byte string.", "\n", "if", "six", ".", "PY3", ":", "\n", "    ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "      ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "bytes", ")", ":", "\n", "      ", "return", "text", ".", "decode", "(", "\"utf-8\"", ",", "\"ignore\"", ")", "\n", "", "else", ":", "\n", "      ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "elif", "six", ".", "PY2", ":", "\n", "    ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "      ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "unicode", ")", ":", "\n", "      ", "return", "text", ".", "encode", "(", "\"utf-8\"", ")", "\n", "", "else", ":", "\n", "      ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "else", ":", "\n", "    ", "raise", "ValueError", "(", "\"Not running on Python2 or Python 3?\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.tokenization.load_vocab": [[121, 134], ["collections.OrderedDict", "tensorflow.gfile.GFile", "tokenization.convert_to_unicode", "token.strip.strip", "reader.readline"], "function", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.tokenization.convert_to_unicode"], ["", "", "def", "load_vocab", "(", "vocab_file", ")", ":", "\n", "  ", "\"\"\"Loads a vocabulary file into a dictionary.\"\"\"", "\n", "vocab", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "index", "=", "0", "\n", "with", "tf", ".", "gfile", ".", "GFile", "(", "vocab_file", ",", "\"r\"", ")", "as", "reader", ":", "\n", "    ", "while", "True", ":", "\n", "      ", "token", "=", "convert_to_unicode", "(", "reader", ".", "readline", "(", ")", ")", "\n", "if", "not", "token", ":", "\n", "        ", "break", "\n", "", "token", "=", "token", ".", "strip", "(", ")", "\n", "vocab", "[", "token", "]", "=", "index", "\n", "index", "+=", "1", "\n", "", "", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.tokenization.convert_by_vocab": [[136, 142], ["output.append"], "function", ["None"], ["", "def", "convert_by_vocab", "(", "vocab", ",", "items", ")", ":", "\n", "  ", "\"\"\"Converts a sequence of [tokens|ids] using the vocab.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "item", "in", "items", ":", "\n", "    ", "output", ".", "append", "(", "vocab", "[", "item", "]", ")", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.tokenization.convert_tokens_to_ids": [[144, 146], ["tokenization.convert_by_vocab"], "function", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.tokenization.convert_by_vocab"], ["", "def", "convert_tokens_to_ids", "(", "vocab", ",", "tokens", ")", ":", "\n", "  ", "return", "convert_by_vocab", "(", "vocab", ",", "tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.tokenization.convert_ids_to_tokens": [[148, 150], ["tokenization.convert_by_vocab"], "function", ["home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.tokenization.convert_by_vocab"], ["", "def", "convert_ids_to_tokens", "(", "inv_vocab", ",", "ids", ")", ":", "\n", "  ", "return", "convert_by_vocab", "(", "inv_vocab", ",", "ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.tokenization.whitespace_tokenize": [[152, 159], ["text.strip.strip", "text.strip.split"], "function", ["None"], ["", "def", "whitespace_tokenize", "(", "text", ")", ":", "\n", "  ", "\"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"", "\n", "text", "=", "text", ".", "strip", "(", ")", "\n", "if", "not", "text", ":", "\n", "    ", "return", "[", "]", "\n", "", "tokens", "=", "text", ".", "split", "(", ")", "\n", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.tokenization._is_whitespace": [[362, 372], ["unicodedata.category"], "function", ["None"], ["", "", "def", "_is_whitespace", "(", "char", ")", ":", "\n", "  ", "\"\"\"Checks whether `chars` is a whitespace character.\"\"\"", "\n", "# \\t, \\n, and \\r are technically contorl characters but we treat them", "\n", "# as whitespace since they are generally considered as such.", "\n", "if", "char", "==", "\" \"", "or", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "    ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Zs\"", ":", "\n", "    ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.tokenization._is_control": [[374, 384], ["unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "def", "_is_control", "(", "char", ")", ":", "\n", "  ", "\"\"\"Checks whether `chars` is a control character.\"\"\"", "\n", "# These are technically control characters but we count them as whitespace", "\n", "# characters.", "\n", "if", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "    ", "return", "False", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "\"C\"", ")", ":", "\n", "    ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.juntaoy_biaffine-ner.extract_bert_features.tokenization._is_punctuation": [[386, 400], ["ord", "unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "def", "_is_punctuation", "(", "char", ")", ":", "\n", "  ", "\"\"\"Checks whether `chars` is a punctuation character.\"\"\"", "\n", "cp", "=", "ord", "(", "char", ")", "\n", "# We treat all non-letter/number ASCII as punctuation.", "\n", "# Characters such as \"^\", \"$\", and \"`\" are not in the Unicode", "\n", "# Punctuation class but we treat them as punctuation anyways, for", "\n", "# consistency.", "\n", "if", "(", "(", "cp", ">=", "33", "and", "cp", "<=", "47", ")", "or", "(", "cp", ">=", "58", "and", "cp", "<=", "64", ")", "or", "\n", "(", "cp", ">=", "91", "and", "cp", "<=", "96", ")", "or", "(", "cp", ">=", "123", "and", "cp", "<=", "126", ")", ")", ":", "\n", "    ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "\"P\"", ")", ":", "\n", "    ", "return", "True", "\n", "", "return", "False", "\n", "", ""]]}