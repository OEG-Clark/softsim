{"home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.None.delin_inoSWAP.tree": [[3, 53], ["len", "len", "len", "range", "len", "print", "range", "print", "print", "btree.append", "len", "print", "len", "btree.append", "btree.insert", "openidx.append", "len", "btree.append", "len"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["def", "tree", "(", "acts", ",", "words", ",", "pos", ")", ":", "\n", "\t", "btree", "=", "[", "]", "\n", "openidx", "=", "[", "]", "\n", "wid", "=", "0", "\n", "\n", "previous_act", "=", "'N'", "\n", "\n", "size_tree", "=", "0", "\n", "max_size_tree", "=", "len", "(", "words", ")", "\n", "\n", "for", "act", "in", "acts", ":", "\n", "\t\t", "if", "act", "[", "0", "]", "==", "'S'", "and", "act", "[", "1", "]", "==", "'H'", ":", "\n", "\t\t\t", "if", "len", "(", "words", ")", "!=", "0", ":", "\n", "\t\t\t\t", "btree", ".", "append", "(", "\"(\"", "+", "pos", "[", "0", "]", "+", "\" \"", "+", "words", "[", "0", "]", "+", "\")\"", ")", "\n", "del", "words", "[", "0", "]", "\n", "del", "pos", "[", "0", "]", "\n", "wid", "+=", "1", "\n", "", "previous_act", "=", "'S'", "\n", "size_tree", "+=", "1", "\n", "\n", "", "elif", "act", "[", "0", "]", "==", "'N'", ":", "\n", "\t\t\t", "btree", ".", "insert", "(", "-", "1", ",", "\"(\"", "+", "act", "[", "3", ":", "-", "1", "]", ")", "\n", "openidx", ".", "append", "(", "len", "(", "btree", ")", "-", "2", ")", "\n", "previous_act", "=", "'N'", "\n", "", "else", ":", "#REDUCE", "\n", "\n", "\t\t\t", "if", "len", "(", "openidx", ")", ">", "0", ":", "\n", "\t\t\t\t", "tmp", "=", "\" \"", ".", "join", "(", "btree", "[", "openidx", "[", "-", "1", "]", ":", "]", ")", "+", "\")\"", "\n", "btree", "=", "btree", "[", ":", "openidx", "[", "-", "1", "]", "]", "\n", "btree", ".", "append", "(", "tmp", ")", "\n", "openidx", "=", "openidx", "[", ":", "-", "1", "]", "\n", "", "previous_act", "=", "'R'", "\n", "\n", "\n", "", "", "if", "len", "(", "openidx", ")", ">", "0", ":", "\n", "\t\t", "tope", "=", "len", "(", "openidx", ")", "\n", "for", "i", "in", "range", "(", "tope", ")", ":", "\n", "\t\t\t", "tmp", "=", "\" \"", ".", "join", "(", "btree", "[", "openidx", "[", "-", "1", "]", ":", "]", ")", "+", "\")\"", "\n", "btree", "=", "btree", "[", ":", "openidx", "[", "-", "1", "]", "]", "\n", "btree", ".", "append", "(", "tmp", ")", "\n", "openidx", "=", "openidx", "[", ":", "-", "1", "]", "\n", "\n", "#print(btree)", "\n", "", "", "if", "len", "(", "btree", ")", ">", "1", ":", "\n", "\t\t", "print", "(", "'(ROOT'", ",", "end", "=", "''", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "btree", ")", ")", ":", "\n", "\t\t\t\t", "print", "(", "btree", "[", "i", "]", ",", "end", "=", "''", ")", "\n", "", "print", "(", "')'", ")", "\n", "", "else", ":", "\n", "\t\t", "print", "(", "btree", "[", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.None.delin_inoSWAP.reorder_text": [[55, 66], ["reversed", "range", "buffer.append", "len", "stack.append", "buffer.append", "buffer.pop", "stack.pop"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop"], ["", "", "def", "reorder_text", "(", "actions", ",", "text", ")", ":", "\n", "\t", "stack", "=", "[", "]", "\n", "buffer", "=", "[", "]", "\n", "for", "t", "in", "reversed", "(", "range", "(", "len", "(", "text", ")", ")", ")", ":", "\n", "\t\t", "buffer", ".", "append", "(", "text", "[", "t", "]", ")", "\n", "\n", "", "for", "a", "in", "actions", ":", "\n", "\t\t", "if", "a", "[", "1", "]", "==", "'H'", ":", "stack", ".", "append", "(", "buffer", ".", "pop", "(", ")", ")", "\n", "if", "a", "[", "1", "]", "==", "'W'", ":", "buffer", ".", "append", "(", "stack", ".", "pop", "(", "-", "2", ")", ")", "\n", "\n", "", "return", "stack", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.create_fairseq_dicts.argument_parser": [[7, 24], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["def", "argument_parser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Add fairseq dicts'", ")", "\n", "# The idea here is to keep pretraining indices to re-use model embeddings", "\n", "# when loading", "\n", "parser", ".", "add_argument", "(", "\n", "\"-i\"", ",", "\"--in-pretrain-dict\"", ",", "\n", "help", "=", "\"Pretrainign fairseq dict (indices are preserved)\"", ",", "\n", "required", "=", "True", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"-t\"", ",", "\"--in-fine-tune-data\"", ",", "\n", "help", "=", "\"Fine tuning data, tokens tab separated\"", ",", "\n", "required", "=", "True", "\n", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.create_fairseq_dicts.read_fairseq_dict": [[26, 35], ["re.compile", "open", "re.compile.match", "re.compile.match().groups", "items.append", "line.strip", "re.compile.match", "int"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "read_fairseq_dict", "(", "file_path", ")", ":", "\n", "    ", "line_re", "=", "re", ".", "compile", "(", "'(.*) ([0-9]+)'", ")", "\n", "with", "open", "(", "file_path", ")", "as", "fid", ":", "\n", "        ", "items", "=", "[", "]", "\n", "for", "line", "in", "fid", ":", "\n", "            ", "assert", "line_re", ".", "match", "(", "line", ".", "strip", "(", ")", ")", ",", "\"Not a fairseq dict\"", "\n", "word", ",", "count", "=", "line_re", ".", "match", "(", "line", ")", ".", "groups", "(", ")", "\n", "items", ".", "append", "(", "[", "word", ",", "int", "(", "count", ")", "]", ")", "\n", "", "", "return", "items", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.create_fairseq_dicts.write_fairseq_dict": [[37, 41], ["open", "fid.write"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write"], ["", "def", "write_fairseq_dict", "(", "file_path", ",", "out_entries", ")", ":", "\n", "    ", "with", "open", "(", "file_path", ",", "'w'", ")", "as", "fid", ":", "\n", "        ", "for", "key", ",", "count", "in", "out_entries", ":", "\n", "            ", "fid", ".", "write", "(", "f'{key} {count}\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.create_fairseq_dicts.read_data_into_dict": [[43, 47], ["open", "collections.Counter", "line.strip().split", "line.strip"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["", "", "", "def", "read_data_into_dict", "(", "file_path", ")", ":", "\n", "    ", "with", "open", "(", "file_path", ")", "as", "fid", ":", "\n", "        ", "return", "Counter", "(", "[", "\n", "token", "for", "line", "in", "fid", "for", "token", "in", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "]", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.merge_restored_vocabulary.update_state_emb": [[6, 70], ["model.named_parameters", "value.clone().detach().requires_grad_", "enumerate", "print", "value.clone().detach", "enumerate", "enumerate", "value.clone"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["def", "update_state_emb", "(", "\n", "model", ",", "# Trainer.get_model()", "\n", "state", ",", "# state", "\n", "task", ",", "# Trainer.task", "\n", "state_task", "# tasks.setup_task(state['args'])", "\n", ")", ":", "\n", "    ", "''' \n    Update checkpointy state to have partial embeddings overloading\n    '''", "\n", "\n", "state_model", "=", "state", "[", "'model'", "]", "\n", "\n", "# model weights that are overloadable", "\n", "source_dict_weights", "=", "[", "'encoder.embed_tokens.weight'", "]", "\n", "target_dict_weights", "=", "[", "'decoder.embed_out'", ",", "'decoder.embed_tokens.weight'", "]", "\n", "overwritable_weights", "=", "source_dict_weights", "+", "target_dict_weights", "\n", "for", "name", ",", "value", "in", "model", ".", "named_parameters", "(", ")", ":", "\n", "        ", "if", "(", "\n", "name", "in", "overwritable_weights", "\n", "and", "state_model", "[", "name", "]", ".", "shape", "[", "0", "]", "<", "value", ".", "shape", "[", "0", "]", "\n", ")", ":", "\n", "\n", "# Alow overloading of sub-sets of the vocabulary", "\n", "            ", "pre_voc_size", ",", "pre_emb_dim", "=", "state_model", "[", "name", "]", ".", "shape", "\n", "finet_voc_size", ",", "finet_emb_dim", "=", "value", ".", "shape", "\n", "\n", "assert", "pre_emb_dim", "==", "finet_emb_dim", ",", "f\"Embeddings sizes of models do not match \"", "f\"({pre_emb_dim}, {finet_emb_dim})\"", "\n", "\n", "# Get the embedings of the load model", "\n", "if", "name", "in", "target_dict_weights", ":", "\n", "                ", "pretr_model_idx_by_symbol", "=", "{", "\n", "key", ":", "index", "\n", "for", "index", ",", "key", "in", "enumerate", "(", "\n", "state_task", ".", "target_dictionary", ".", "symbols", "\n", ")", "\n", "}", "\n", "model_symbols", "=", "task", ".", "target_dictionary", ".", "symbols", "\n", "", "else", ":", "\n", "                ", "pretr_model_idx_by_symbol", "=", "{", "\n", "key", ":", "index", "\n", "for", "index", ",", "key", "in", "enumerate", "(", "\n", "state_task", ".", "source_dictionary", ".", "symbols", "\n", ")", "\n", "}", "\n", "model_symbols", "=", "task", ".", "source_dictionary", ".", "symbols", "\n", "\n", "# start by setting new state equal to the random", "\n", "# initialized model", "\n", "", "new_state", "=", "value", ".", "clone", "(", ")", ".", "detach", "(", ")", ".", "requires_grad_", "(", "True", ")", "\n", "\n", "# Assign those embeddings to the new model if found", "\n", "count_load", "=", "0", "\n", "for", "index", ",", "symbol", "in", "enumerate", "(", "model_symbols", ")", ":", "\n", "                ", "if", "symbol", "in", "pretr_model_idx_by_symbol", ":", "\n", "                    ", "state_index", "=", "pretr_model_idx_by_symbol", "[", "symbol", "]", "\n", "count_load", "+=", "1", "\n", "new_state", "[", "index", ",", ":", "]", "=", "state", "[", "'model'", "]", "[", "name", "]", "[", "state_index", ",", ":", "]", "\n", "", "", "state", "[", "'model'", "]", "[", "name", "]", "=", "new_state", "\n", "print", "(", "f'{name} {count_load}/{new_state.shape[0]} embeddings load from checkpoint'", ")", "\n", "\n", "", "", "return", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.merge_restored_vocabulary.argument_parser": [[72, 75], ["fairseq.options.get_training_parser", "fairseq.options.parse_args_and_arch"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.get_training_parser", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.parse_args_and_arch"], ["", "def", "argument_parser", "(", ")", ":", "\n", "    ", "parser", "=", "options", ".", "get_training_parser", "(", ")", "\n", "return", "options", ".", "parse_args_and_arch", "(", "parser", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.merge_restored_vocabulary.main": [[77, 102], ["fairseq.checkpoint_utils.load_checkpoint_to_cpu", "fairseq.tasks.setup_task", "fairseq.tasks.setup_task", "tasks.setup_task.build_model", "merge_restored_vocabulary.update_state_emb", "torch.save", "os.path.dirname", "Exception"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.checkpoint_utils.load_checkpoint_to_cpu", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.sentence_prediction.SentencePredictionTask.setup_task", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.sentence_prediction.SentencePredictionTask.setup_task", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv_lm.LightConvLanguageModel.build_model", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.merge_restored_vocabulary.update_state_emb", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.save"], ["", "def", "main", "(", "args", ")", ":", "\n", "\n", "# read pretrained model and task", "\n", "    ", "state", "=", "checkpoint_utils", ".", "load_checkpoint_to_cpu", "(", "args", ".", "restore_file", ")", "\n", "state_task", "=", "tasks", ".", "setup_task", "(", "state", "[", "'args'", "]", ")", "\n", "\n", "# build fine-tuning model and task", "\n", "task", "=", "tasks", ".", "setup_task", "(", "args", ")", "\n", "model", "=", "task", ".", "build_model", "(", "args", ")", "\n", "\n", "# Sanity check, this is not a model on the original pretraining folder ", "\n", "# We will modify this checkpoint and we do not want to spoil other models", "\n", "path", "=", "args", ".", "data", "[", ":", "-", "1", "]", "if", "args", ".", "data", "[", "-", "1", "]", "==", "'/'", "else", "args", ".", "data", "\n", "if", "os", ".", "path", ".", "dirname", "(", "args", ".", "restore_file", ")", "!=", "path", ":", "\n", "        ", "raise", "Exception", "(", "\n", "f'Expected --restore-model to be in {args.data}\\n'", "\n", "'(reason: will modify this checkpoint)'", "\n", ")", "\n", "\n", "# Update state merging new vocabulary", "\n", "", "state", "=", "update_state_emb", "(", "model", ",", "state", ",", "task", ",", "state_task", ")", "\n", "\n", "# with open(, 'w') as fid:", "\n", "# with open(args.restore_file, 'w') as fid:", "\n", "torch", ".", "save", "(", "state", ",", "args", ".", "restore_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.alignment_analysis.fix_alignments": [[12, 108], ["amr.alignmentsToken2Node", "amr.alignmentsToken2Node", "amr.findSubGraph", "amr.tokens[].lower", "len", "len", "amr.alignmentsToken2Node", "print", "len", "amr.alignmentsToken2Node", "print", "len", "len", "any", "any", "print", "enumerate", "len", "print", "len", "r.startswith", "print", "amr.alignments[].append", "any", "print", "tok.lower", "any", "print", "len", "tok.startswith", "tok.startswith", "tok.startswith"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.alignmentsToken2Node", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.alignmentsToken2Node", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.findSubGraph", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.alignmentsToken2Node", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.alignmentsToken2Node", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["def", "fix_alignments", "(", "amr", ")", ":", "\n", "    ", "changes", "=", "0", "\n", "for", "node_id", "in", "amr", ".", "nodes", ":", "\n", "        ", "if", "node_id", "not", "in", "amr", ".", "alignments", ":", "\n", "            ", "amr", ".", "alignments", "[", "node_id", "]", "=", "[", "]", "\n", "# fix false positives", "\n", "", "", "for", "node_id", "in", "amr", ".", "nodes", ":", "\n", "        ", "token_ids", "=", "amr", ".", "alignments", "[", "node_id", "]", "\n", "if", "not", "token_ids", ":", "\n", "            ", "continue", "\n", "", "tokens", "=", "[", "amr", ".", "tokens", "[", "t", "-", "1", "]", ".", "lower", "(", ")", "for", "t", "in", "token_ids", "if", "0", "<=", "t", "<=", "len", "(", "amr", ".", "tokens", ")", "]", "\n", "nodes", "=", "amr", ".", "alignmentsToken2Node", "(", "token_ids", "[", "0", "]", ")", "\n", "\n", "if", "amr", ".", "nodes", "[", "node_id", "]", "==", "'and'", ":", "\n", "            ", "if", "len", "(", "nodes", ")", ">", "1", "and", "not", "any", "(", "tok", "in", "[", "'and'", ",", "'&'", ",", "';'", ",", "','", "]", "for", "tok", "in", "tokens", ")", ":", "\n", "                ", "amr", ".", "alignments", "[", "node_id", "]", "=", "[", "]", "\n", "print", "(", "'[removing alignment]'", ",", "'and'", ",", "f'({\" \".join(tokens)})'", ")", "\n", "changes", "+=", "1", "\n", "", "", "elif", "amr", ".", "nodes", "[", "node_id", "]", "==", "'i'", ":", "\n", "            ", "if", "not", "any", "(", "tok", "in", "[", "'i'", ",", "'me'", ",", "'myself'", ",", "'my'", ",", "'mine'", ",", "'im'", ",", "'ill'", ",", "'i\\'m'", ",", "'i\\'ll'", "]", "for", "tok", "in", "tokens", ")", ":", "\n", "                ", "amr", ".", "alignments", "[", "node_id", "]", "=", "[", "]", "\n", "print", "(", "'[removing alignment]'", ",", "'i'", ",", "f'({\" \".join(tokens)})'", ")", "\n", "changes", "+=", "1", "\n", "", "", "elif", "amr", ".", "nodes", "[", "node_id", "]", "==", "'you'", ":", "\n", "            ", "if", "not", "any", "(", "tok", ".", "startswith", "(", "'you'", ")", "or", "tok", ".", "startswith", "(", "'ya'", ")", "or", "tok", "==", "'u'", "for", "tok", "in", "tokens", ")", ":", "\n", "                ", "amr", ".", "alignments", "[", "node_id", "]", "=", "[", "]", "\n", "print", "(", "'[removing alignment]'", ",", "'you'", ",", "f'({\" \".join(tokens)})'", ")", "\n", "changes", "+=", "1", "\n", "", "", "elif", "amr", ".", "nodes", "[", "node_id", "]", "==", "'imperative'", ":", "\n", "            ", "if", "not", "any", "(", "tok", ".", "startswith", "(", "'let'", ")", "or", "'!'", "in", "tok", "or", "tok", "==", "'imperative'", "for", "tok", "in", "tokens", ")", ":", "\n", "                ", "amr", ".", "alignments", "[", "node_id", "]", "=", "[", "]", "\n", "print", "(", "'[removing alignment]'", ",", "'imperative'", ",", "f'({\" \".join(tokens)})'", ")", "\n", "changes", "+=", "1", "\n", "", "", "", "amr", ".", "token2node_memo", "=", "{", "}", "\n", "# fix false negatives", "\n", "for", "node_id", "in", "amr", ".", "nodes", ":", "\n", "        ", "token_ids", "=", "amr", ".", "alignments", "[", "node_id", "]", "\n", "if", "token_ids", ":", "\n", "            ", "continue", "\n", "\n", "", "if", "amr", ".", "nodes", "[", "node_id", "]", "==", "'-'", ":", "\n", "            ", "possible_align", "=", "[", "(", "i", "+", "1", ",", "tok", ")", "for", "i", ",", "tok", "in", "enumerate", "(", "amr", ".", "tokens", ")", "if", "tok", ".", "lower", "(", ")", "in", "[", "'not'", ",", "'n\\'t'", "]", "]", "\n", "if", "len", "(", "possible_align", ")", "==", "1", ":", "\n", "                ", "id", "=", "possible_align", "[", "0", "]", "[", "0", "]", "\n", "nodes", "=", "amr", ".", "alignmentsToken2Node", "(", "id", ")", "\n", "if", "len", "(", "nodes", ")", "==", "0", ":", "\n", "                    ", "amr", ".", "alignments", "[", "node_id", "]", "=", "[", "id", "]", "\n", "print", "(", "'[adding alignment]'", ",", "'-'", ",", "f'({possible_align[0][1]})'", ")", "\n", "changes", "+=", "1", "\n", "", "", "", "", "amr", ".", "token2node_memo", "=", "{", "}", "\n", "for", "node_id", "in", "amr", ".", "nodes", ":", "\n", "        ", "token_ids", "=", "amr", ".", "alignments", "[", "node_id", "]", "\n", "if", "not", "token_ids", ":", "\n", "            ", "continue", "\n", "", "nodes", "=", "amr", ".", "alignmentsToken2Node", "(", "token_ids", "[", "0", "]", ")", "\n", "if", "len", "(", "nodes", ")", "<=", "1", ":", "\n", "            ", "continue", "\n", "", "entity_sg", "=", "amr", ".", "findSubGraph", "(", "nodes", ")", "\n", "edges", "=", "entity_sg", ".", "edges", "\n", "tokens", "=", "[", "amr", ".", "tokens", "[", "t", "-", "1", "]", "for", "t", "in", "token_ids", "if", "0", "<=", "t", "<=", "len", "(", "amr", ".", "tokens", ")", "]", "\n", "node_label", "=", "','", ".", "join", "(", "amr", ".", "nodes", "[", "n", "]", "for", "n", "in", "nodes", ")", "\n", "if", "amr", ".", "nodes", "[", "node_id", "]", "==", "'name'", ":", "\n", "            ", "for", "s", ",", "r", ",", "t", "in", "amr", ".", "edges", ":", "\n", "                ", "if", "(", "s", ",", "r", ",", "t", ")", "in", "edges", ":", "\n", "                    ", "continue", "\n", "", "if", "not", "r", ".", "startswith", "(", "':op'", ")", ":", "\n", "                    ", "continue", "\n", "", "if", "node_id", "==", "s", "and", "t", "not", "in", "nodes", ":", "\n", "                    ", "amr", ".", "alignments", "[", "t", "]", "=", "amr", ".", "alignments", "[", "node_id", "]", "\n", "print", "(", "'[adding alignment]'", ",", "'name'", ",", "amr", ".", "nodes", "[", "t", "]", ",", "f'({\" \".join(tokens)} {node_label})'", ")", "\n", "changes", "+=", "1", "\n", "", "", "", "root", "=", "entity_sg", ".", "root", "\n", "i", "=", "token_ids", "[", "-", "1", "]", "+", "1", "\n", "while", "i", "-", "1", "<", "len", "(", "amr", ".", "tokens", ")", ":", "\n", "            ", "align", "=", "amr", ".", "alignmentsToken2Node", "(", "i", ")", "\n", "tok", "=", "amr", ".", "tokens", "[", "i", "-", "1", "]", "\n", "if", "not", "align", "and", "'\"'", "+", "tok", "+", "'\"'", "in", "[", "amr", ".", "nodes", "[", "n", "]", "for", "n", "in", "nodes", "]", "and", "tok", "not", "in", "tokens", ":", "\n", "                ", "for", "n", "in", "nodes", ":", "\n", "                    ", "amr", ".", "alignments", "[", "n", "]", ".", "append", "(", "i", ")", "\n", "", "token_ids", "=", "amr", ".", "alignments", "[", "node_id", "]", "\n", "tokens", "=", "[", "amr", ".", "tokens", "[", "t", "-", "1", "]", "for", "t", "in", "token_ids", "if", "0", "<=", "t", "<=", "len", "(", "amr", ".", "tokens", ")", "]", "\n", "print", "(", "'[adding alignment]'", ",", "'token'", ",", "tok", ",", "f'({\" \".join(tokens)} {node_label})'", ")", "\n", "", "else", ":", "\n", "                ", "break", "\n", "", "i", "+=", "1", "\n", "", "if", "not", "node_id", "==", "root", ":", "\n", "            ", "continue", "\n", "# for s, r, t in amr.edges:", "\n", "#     if (s, r, t) in edges:", "\n", "#         continue", "\n", "#     if node_id == s and t not in nodes and not amr.alignments[t]:", "\n", "#         amr.alignments[t].extend(amr.alignments[node_id])", "\n", "#         print('[adding alignment]', 'target', amr.nodes[t], f'({\" \".join(tokens)} {node_label})')", "\n", "#         changes += 1", "\n", "", "", "amr", ".", "token2node_memo", "=", "{", "}", "\n", "return", "changes", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.alignment_analysis.main": [[112, 296], ["amr.JAMR_CorpusReader", "amr.JAMR_CorpusReader.load_amrs", "dict", "collections.Counter", "collections.Counter", "collections.Counter", "collections.Counter", "collections.Counter", "sorted", "print", "sorted", "print", "print", "print", "sorted", "print", "print", "print", "sorted", "print", "print", "print", "sorted", "print", "print", "print", "sorted", "print", "alignment_analysis.fix_alignments", "str", "sorted.split", "any", "dict.keys", "print", "print", "print", "sorted", "len", "sum", "len", "sum", "type.isdigit", "len", "sum", "len", "sum", "str", "sorted.split", "any", "open", "json.dump", "amr.alignmentsToken2Node", "amr.findSubGraph", "sorted", "all_entities.append", "collections.Counter", "len", "sum", "collections.Counter.values", "collections.Counter.values", "collections.Counter.values", "collections.Counter.values", "[].append", "[].append", "[].append", "unaligned_nodes.append", "unaligned_nodes.append", "len", "sorted.count", "len", "int", "size_counters[].values", "len", "sum", "len", "sum", "len", "sum", "len", "sum", "len", "len", "[].append", "sorted.count", "len", "len", "[].append", "len", "[].append", "[].append", "str", "len", "unrooted_entities.append", "unrooted_entities.append", "sorted.count", "amr.alignmentsToken2Node.count", "len", "sum", "collections.Counter.values", "collections.Counter.values", "collections.Counter.values", "collections.Counter.values", "sorted.count", "len", "len", "[].append", "amr.alignmentsToken2Node.count", "len", "len", "[].append", "len", "amr.nodes[].isdigit", "amr.nodes[].startswith", "size_counters[].values", "str", "str"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.JAMR_CorpusReader.load_amrs", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.alignment_analysis.fix_alignments", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.Stack.dump", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.alignmentsToken2Node", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.findSubGraph", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "main", "(", ")", ":", "\n", "    ", "cr", "=", "JAMR_CorpusReader", "(", ")", "\n", "cr", ".", "load_amrs", "(", "sys", ".", "argv", "[", "1", "]", ",", "verbose", "=", "False", ")", "\n", "\n", "json", "=", "{", "'size'", ":", "{", "}", ",", "'unaligned'", ":", "{", "}", ",", "'unconnected'", ":", "{", "}", ",", "'unrooted'", ":", "{", "}", ",", "'repeats'", ":", "{", "}", ",", "'stats'", ":", "{", "}", "}", "\n", "\n", "all_entities", "=", "[", "]", "\n", "unaligned_nodes", "=", "[", "]", "\n", "unrooted_entities", "=", "[", "]", "\n", "changes", "=", "0", "\n", "amrs_changed", "=", "0", "\n", "for", "amr", "in", "cr", ".", "amrs", ":", "\n", "        ", "change", "=", "fix_alignments", "(", "amr", ")", "\n", "changes", "+=", "change", "\n", "if", "change", ">", "0", ":", "\n", "            ", "amrs_changed", "+=", "1", "\n", "", "for", "node_id", "in", "amr", ".", "nodes", ":", "\n", "# get entity info", "\n", "            ", "if", "node_id", "not", "in", "amr", ".", "alignments", ":", "\n", "                ", "unaligned_nodes", ".", "append", "(", "amr", ".", "nodes", "[", "node_id", "]", ")", "\n", "continue", "\n", "", "token_ids", "=", "amr", ".", "alignments", "[", "node_id", "]", "\n", "if", "not", "token_ids", ":", "\n", "                ", "unaligned_nodes", ".", "append", "(", "amr", ".", "nodes", "[", "node_id", "]", ")", "\n", "continue", "\n", "", "nodes", "=", "amr", ".", "alignmentsToken2Node", "(", "token_ids", "[", "0", "]", ")", "\n", "if", "len", "(", "nodes", ")", "<=", "1", ":", "\n", "                ", "continue", "\n", "", "entity_sg", "=", "amr", ".", "findSubGraph", "(", "nodes", ")", "\n", "root", "=", "entity_sg", ".", "root", "\n", "if", "not", "node_id", "==", "root", ":", "\n", "                ", "continue", "\n", "", "edges", "=", "entity_sg", ".", "edges", "\n", "\n", "tokens", "=", "[", "amr", ".", "tokens", "[", "t", "-", "1", "]", "for", "t", "in", "token_ids", "if", "0", "<=", "t", "<=", "len", "(", "amr", ".", "tokens", ")", "]", "\n", "special_nodes", "=", "[", "n", "for", "n", "in", "nodes", "if", "(", "amr", ".", "nodes", "[", "n", "]", ".", "isdigit", "(", ")", "or", "amr", ".", "nodes", "[", "n", "]", ".", "startswith", "(", "'\"'", ")", ")", "]", "\n", "\n", "entity_type", "=", "sorted", "(", "[", "amr", ".", "nodes", "[", "id", "]", "for", "id", "in", "nodes", "if", "id", "not", "in", "special_nodes", "]", ")", "\n", "entity_type", "=", "','", ".", "join", "(", "entity_type", ")", "\n", "\n", "nodes", "=", "{", "n", ":", "amr", ".", "nodes", "[", "n", "]", "for", "n", "in", "nodes", "}", "\n", "all_entities", ".", "append", "(", "(", "amr", ",", "entity_type", ",", "tokens", ",", "root", ",", "nodes", ",", "edges", ",", "str", "(", "amr", ")", ")", ")", "\n", "for", "s", ",", "r", ",", "t", "in", "amr", ".", "edges", ":", "\n", "                ", "if", "(", "s", ",", "r", ",", "t", ")", "in", "edges", ":", "\n", "                    ", "continue", "\n", "", "if", "len", "(", "edges", ")", "==", "0", ":", "\n", "                    ", "continue", "\n", "", "if", "s", "in", "nodes", "and", "s", "!=", "root", ":", "\n", "                    ", "if", "t", "not", "in", "amr", ".", "alignments", "or", "not", "amr", ".", "alignments", "[", "t", "]", ":", "\n", "                        ", "continue", "\n", "", "label", "=", "f'{amr.nodes[root]} {amr.nodes[s]}'", "\n", "unrooted_entities", ".", "append", "(", "(", "entity_type", ",", "tokens", ",", "label", ",", "str", "(", "amr", ")", ")", ")", "\n", "", "if", "t", "in", "nodes", "and", "t", "!=", "root", ":", "\n", "                    ", "if", "s", "not", "in", "amr", ".", "alignments", "or", "not", "amr", ".", "alignments", "[", "s", "]", ":", "\n", "                        ", "continue", "\n", "", "label", "=", "f'{amr.nodes[root]} {amr.nodes[t]}'", "\n", "unrooted_entities", ".", "append", "(", "(", "entity_type", ",", "tokens", ",", "label", ",", "str", "(", "amr", ")", ")", ")", "\n", "\n", "\n", "", "", "", "", "size_counters", "=", "dict", "(", ")", "\n", "unconnected_counter", "=", "Counter", "(", ")", "\n", "unaligned_counter", "=", "Counter", "(", ")", "\n", "unrooted_counter", "=", "Counter", "(", ")", "\n", "repeated_counter", "=", "Counter", "(", ")", "\n", "attachment_counter", "=", "Counter", "(", ")", "\n", "for", "node", "in", "unaligned_nodes", ":", "\n", "        ", "unaligned_counter", "[", "node", "]", "+=", "1", "\n", "", "for", "entity_type", ",", "tokens", ",", "label", ",", "string", "in", "unrooted_entities", ":", "\n", "        ", "unrooted_counter", "[", "entity_type", "]", "+=", "1", "\n", "attachment_counter", "[", "label", "]", "+=", "1", "\n", "", "json", "[", "'stats'", "]", "[", "'unrooted-attachments'", "]", "=", "{", "}", "\n", "for", "node", "in", "sorted", "(", "attachment_counter", ",", "reverse", "=", "True", ",", "key", "=", "lambda", "x", ":", "attachment_counter", "[", "x", "]", ")", ":", "\n", "        ", "json", "[", "'stats'", "]", "[", "'unrooted-attachments'", "]", "[", "node", "]", "=", "attachment_counter", "[", "node", "]", "\n", "", "for", "amr", ",", "entity_type", ",", "tokens", ",", "root", ",", "nodes", ",", "edges", ",", "string", "in", "all_entities", ":", "\n", "        ", "label", "=", "str", "(", "entity_type", ".", "count", "(", "','", ")", "+", "1", ")", "\n", "if", "label", "not", "in", "size_counters", ":", "\n", "            ", "size_counters", "[", "label", "]", "=", "Counter", "(", ")", "\n", "", "size_counters", "[", "label", "]", "[", "entity_type", "]", "+=", "1", "\n", "if", "entity_type", ".", "count", "(", "','", ")", "+", "1", ">", "1", "and", "len", "(", "edges", ")", "==", "0", ":", "\n", "            ", "unconnected_counter", "[", "entity_type", "]", "+=", "1", "\n", "", "nodes", "=", "entity_type", ".", "split", "(", "','", ")", "\n", "if", "any", "(", "nodes", ".", "count", "(", "n", ")", ">", "1", "for", "n", "in", "nodes", ")", ":", "\n", "            ", "repeated_counter", "[", "entity_type", "]", "+=", "1", "\n", "\n", "", "", "print", "(", "'Changes:'", ",", "changes", ",", "'AMRs changed:'", ",", "amrs_changed", ")", "\n", "for", "label", "in", "sorted", "(", "size_counters", ".", "keys", "(", ")", ",", "key", "=", "lambda", "x", ":", "int", "(", "x", ")", ")", ":", "\n", "        ", "print", "(", "'size'", ",", "label", ")", "\n", "print", "(", "f'({len(size_counters[label])} types, {sum(size_counters[label].values())} items)'", ")", "\n", "json", "[", "'stats'", "]", "[", "'size '", "+", "label", "]", "=", "{", "'types'", ":", "len", "(", "size_counters", "[", "label", "]", ")", ",", "\n", "'items'", ":", "sum", "(", "size_counters", "[", "label", "]", ".", "values", "(", ")", ")", "}", "\n", "print", "(", "size_counters", "[", "label", "]", ")", "\n", "json", "[", "'size'", "]", "[", "label", "]", "=", "{", "}", "\n", "for", "type", "in", "sorted", "(", "size_counters", "[", "label", "]", ",", "reverse", "=", "True", ",", "key", "=", "lambda", "x", ":", "size_counters", "[", "label", "]", "[", "x", "]", ")", ":", "\n", "            ", "d", "=", "{", "\n", "'count'", ":", "size_counters", "[", "label", "]", "[", "type", "]", ",", "\n", "'tokens'", ":", "[", "]", ",", "\n", "'graphs'", ":", "[", "]", ",", "\n", "}", "\n", "json", "[", "'size'", "]", "[", "label", "]", "[", "type", "]", "=", "d", "\n", "", "", "print", "(", "'unconnected'", ")", "\n", "print", "(", "f'({len(unconnected_counter)} types, {sum(unconnected_counter.values())} items)'", ")", "\n", "json", "[", "'stats'", "]", "[", "'unconnected'", "]", "=", "{", "'types'", ":", "len", "(", "unconnected_counter", ")", ",", "\n", "'items'", ":", "sum", "(", "unconnected_counter", ".", "values", "(", ")", ")", "}", "\n", "print", "(", "unconnected_counter", ")", "\n", "json", "[", "'unconnected'", "]", "=", "{", "}", "\n", "for", "type", "in", "sorted", "(", "unconnected_counter", ",", "reverse", "=", "True", ",", "key", "=", "lambda", "x", ":", "unconnected_counter", "[", "x", "]", ")", ":", "\n", "        ", "d", "=", "{", "\n", "'count'", ":", "unconnected_counter", "[", "type", "]", ",", "\n", "'tokens'", ":", "[", "]", ",", "\n", "'graphs'", ":", "[", "]", ",", "\n", "}", "\n", "json", "[", "'unconnected'", "]", "[", "type", "]", "=", "d", "\n", "", "print", "(", "'unaligned'", ")", "\n", "print", "(", "f'({len(unaligned_counter)} types, {sum(unaligned_counter.values())} items)'", ")", "\n", "json", "[", "'stats'", "]", "[", "'unaligned'", "]", "=", "{", "'types'", ":", "len", "(", "unaligned_counter", ")", ",", "\n", "'items'", ":", "sum", "(", "unaligned_counter", ".", "values", "(", ")", ")", "}", "\n", "print", "(", "unaligned_counter", ")", "\n", "json", "[", "'unaligned'", "]", "=", "{", "}", "\n", "for", "type", "in", "sorted", "(", "unaligned_counter", ",", "reverse", "=", "True", ",", "key", "=", "lambda", "x", ":", "unaligned_counter", "[", "x", "]", ")", ":", "\n", "        ", "d", "=", "{", "\n", "'count'", ":", "unaligned_counter", "[", "type", "]", ",", "\n", "}", "\n", "if", "type", ".", "isdigit", "(", ")", ":", "\n", "            ", "type", "=", "'<NUM>'", "+", "type", "\n", "", "json", "[", "'unaligned'", "]", "[", "type", "]", "=", "d", "\n", "", "print", "(", "'unrooted'", ")", "\n", "print", "(", "f'({len(unrooted_counter)} types, {sum(unrooted_counter.values())} items)'", ")", "\n", "json", "[", "'stats'", "]", "[", "'unrooted'", "]", "=", "{", "'types'", ":", "len", "(", "unrooted_counter", ")", ",", "\n", "'items'", ":", "sum", "(", "unrooted_counter", ".", "values", "(", ")", ")", "}", "\n", "print", "(", "unrooted_counter", ")", "\n", "json", "[", "'unrooted'", "]", "=", "{", "}", "\n", "for", "type", "in", "sorted", "(", "unrooted_counter", ",", "reverse", "=", "True", ",", "key", "=", "lambda", "x", ":", "unrooted_counter", "[", "x", "]", ")", ":", "\n", "        ", "d", "=", "{", "\n", "'count'", ":", "unrooted_counter", "[", "type", "]", ",", "\n", "'tokens'", ":", "[", "]", ",", "\n", "'graphs'", ":", "[", "]", ",", "\n", "'attachments'", ":", "[", "]", "\n", "}", "\n", "json", "[", "'unrooted'", "]", "[", "type", "]", "=", "d", "\n", "", "print", "(", "'repeats'", ")", "\n", "print", "(", "f'({len(repeated_counter)} types, {sum(repeated_counter.values())} items)'", ")", "\n", "json", "[", "'stats'", "]", "[", "'repeats'", "]", "=", "{", "'types'", ":", "len", "(", "repeated_counter", ")", ",", "\n", "'items'", ":", "sum", "(", "repeated_counter", ".", "values", "(", ")", ")", "}", "\n", "print", "(", "repeated_counter", ")", "\n", "json", "[", "'repeats'", "]", "=", "{", "}", "\n", "for", "type", "in", "sorted", "(", "repeated_counter", ",", "reverse", "=", "True", ",", "key", "=", "lambda", "x", ":", "repeated_counter", "[", "x", "]", ")", ":", "\n", "        ", "d", "=", "{", "\n", "'count'", ":", "repeated_counter", "[", "type", "]", ",", "\n", "'tokens'", ":", "[", "]", ",", "\n", "'graphs'", ":", "[", "]", ",", "\n", "}", "\n", "json", "[", "'repeats'", "]", "[", "type", "]", "=", "d", "\n", "", "print", "(", ")", "\n", "\n", "\n", "for", "entity_type", ",", "tokens", ",", "label", ",", "string", "in", "unrooted_entities", ":", "\n", "        ", "tokens", "=", "' '", ".", "join", "(", "tokens", ")", "\n", "if", "tokens", "not", "in", "json", "[", "'unrooted'", "]", "[", "entity_type", "]", "[", "'tokens'", "]", "and", "len", "(", "json", "[", "'unrooted'", "]", "[", "entity_type", "]", "[", "'tokens'", "]", ")", "<", "100", ":", "\n", "            ", "json", "[", "'unrooted'", "]", "[", "entity_type", "]", "[", "'tokens'", "]", ".", "append", "(", "tokens", ")", "\n", "if", "len", "(", "json", "[", "'unrooted'", "]", "[", "entity_type", "]", "[", "'graphs'", "]", ")", "<", "1", ":", "\n", "                ", "json", "[", "'unrooted'", "]", "[", "entity_type", "]", "[", "'graphs'", "]", ".", "append", "(", "string", ")", "\n", "", "", "if", "label", "not", "in", "json", "[", "'unrooted'", "]", "[", "entity_type", "]", "[", "'attachments'", "]", ":", "\n", "            ", "json", "[", "'unrooted'", "]", "[", "entity_type", "]", "[", "'attachments'", "]", ".", "append", "(", "label", ")", "\n", "", "", "for", "amr", ",", "entity_type", ",", "tokens", ",", "root", ",", "nodes", ",", "edges", ",", "string", "in", "all_entities", ":", "\n", "        ", "tokens", "=", "' '", ".", "join", "(", "tokens", ")", "\n", "size", "=", "str", "(", "entity_type", ".", "count", "(", "','", ")", "+", "1", ")", "\n", "if", "tokens", "not", "in", "json", "[", "'size'", "]", "[", "size", "]", "[", "entity_type", "]", "[", "'tokens'", "]", "and", "len", "(", "json", "[", "'size'", "]", "[", "size", "]", "[", "entity_type", "]", "[", "'tokens'", "]", ")", "<", "100", ":", "\n", "            ", "json", "[", "'size'", "]", "[", "size", "]", "[", "entity_type", "]", "[", "'tokens'", "]", ".", "append", "(", "tokens", ")", "\n", "if", "len", "(", "json", "[", "'size'", "]", "[", "size", "]", "[", "entity_type", "]", "[", "'graphs'", "]", ")", "<", "1", ":", "\n", "                ", "json", "[", "'size'", "]", "[", "size", "]", "[", "entity_type", "]", "[", "'graphs'", "]", ".", "append", "(", "string", ")", "\n", "", "", "if", "entity_type", ".", "count", "(", "','", ")", "+", "1", ">", "1", "and", "len", "(", "edges", ")", "==", "0", ":", "\n", "            ", "if", "tokens", "not", "in", "json", "[", "'unconnected'", "]", "[", "entity_type", "]", "[", "'tokens'", "]", "and", "len", "(", "json", "[", "'unconnected'", "]", "[", "entity_type", "]", "[", "'tokens'", "]", ")", "<", "100", ":", "\n", "                ", "json", "[", "'unconnected'", "]", "[", "entity_type", "]", "[", "'tokens'", "]", ".", "append", "(", "tokens", ")", "\n", "if", "len", "(", "json", "[", "'unconnected'", "]", "[", "entity_type", "]", "[", "'graphs'", "]", ")", "<", "1", ":", "\n", "                    ", "json", "[", "'unconnected'", "]", "[", "entity_type", "]", "[", "'graphs'", "]", ".", "append", "(", "string", ")", "\n", "", "", "", "nodes", "=", "entity_type", ".", "split", "(", "','", ")", "\n", "if", "any", "(", "nodes", ".", "count", "(", "n", ")", ">", "1", "for", "n", "in", "nodes", ")", ":", "\n", "            ", "if", "tokens", "not", "in", "json", "[", "'repeats'", "]", "[", "entity_type", "]", "[", "'tokens'", "]", "and", "len", "(", "json", "[", "'repeats'", "]", "[", "entity_type", "]", "[", "'tokens'", "]", ")", "<", "100", ":", "\n", "                ", "json", "[", "'repeats'", "]", "[", "entity_type", "]", "[", "'tokens'", "]", ".", "append", "(", "tokens", ")", "\n", "if", "len", "(", "json", "[", "'repeats'", "]", "[", "entity_type", "]", "[", "'graphs'", "]", ")", "<", "1", ":", "\n", "                    ", "json", "[", "'repeats'", "]", "[", "entity_type", "]", "[", "'graphs'", "]", ".", "append", "(", "string", ")", "\n", "\n", "", "", "", "", "with", "open", "(", "'alignment_analysis.json'", ",", "'w+'", ",", "encoding", "=", "'utf8'", ")", "as", "f", ":", "\n", "        ", "J", ".", "dump", "(", "json", ",", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.format_score.red": [[18, 20], ["None"], "function", ["None"], ["def", "red", "(", "text", ")", ":", "\n", "    ", "return", "\"\\033[%dm%s\\033[0m\"", "%", "(", "91", ",", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.format_score.green": [[22, 24], ["None"], "function", ["None"], ["", "def", "green", "(", "text", ")", ":", "\n", "    ", "return", "\"\\033[%dm%s\\033[0m\"", "%", "(", "92", ",", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.amr2.argument_parser": [[8, 43], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.amr2.read_amr_as_raw": [[45, 56], ["open", "fid.readlines", "line.strip", "raw_amr.append", "raw_amrs.append", "line.strip"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.readlines", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.amr2.write_amr_from_raw": [[58, 63], ["open", "fid.write", "fid.write"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.tasks_from_amr_tags.yellow_font": [[8, 10], ["None"], "function", ["None"], ["def", "yellow_font", "(", "string", ")", ":", "\n", "    ", "return", "\"\\033[93m%s\\033[0m\"", "%", "string", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.tasks_from_amr_tags.read_bio": [[12, 23], ["open", "fid.readlines", "line.strip", "raw_bio.append", "raw_bios.append", "line.strip"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.readlines", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "read_bio", "(", "file_path", ")", ":", "\n", "    ", "with", "open", "(", "file_path", ")", "as", "fid", ":", "\n", "        ", "raw_bios", "=", "[", "]", "\n", "raw_bio", "=", "[", "]", "\n", "for", "line", "in", "fid", ".", "readlines", "(", ")", ":", "\n", "            ", "if", "line", ".", "strip", "(", ")", ":", "\n", "                ", "raw_bio", ".", "append", "(", "line", ".", "strip", "(", ")", ")", "\n", "", "else", ":", "\n", "                ", "raw_bios", ".", "append", "(", "raw_bio", ")", "\n", "raw_bio", "=", "[", "]", "\n", "", "", "", "return", "raw_bios", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.tasks_from_amr_tags.get_filtered_labeled_token": [[25, 72], ["sense_regex.match", "sense_regex.match().groups", "token_by_sense[].update", "pred_regex.match", "pred_regex.match().groups", "token_by_lemma[].update", "addnode_regex.match", "sense_regex.match", "addnode_regex.match().groups", "token_by_addnode[].update", "blank_regex.match", "pred_regex.match", "ipdb.set_trace", "print", "addnode_regex.match"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.pdb.set_trace", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["", "def", "get_filtered_labeled_token", "(", "labeled_token", ")", ":", "\n", "    ", "'''\n    Given <token label> , filter out labels not satisfying regex\n    '''", "\n", "\n", "if", "sense_regex", ".", "match", "(", "labeled_token", ")", ":", "\n", "# node with sense", "\n", "# extract label and token", "\n", "        ", "token", ",", "sense", "=", "sense_regex", ".", "match", "(", "labeled_token", ")", ".", "groups", "(", ")", "\n", "token_by_sense", "[", "sense", "]", ".", "update", "(", "[", "token", "]", ")", "\n", "# labeled tokens for each task", "\n", "wsd_labeled_token", "=", "labeled_token", "\n", "mcr_labeled_token", "=", "f'{token} O'", "\n", "ner_labeled_token", "=", "f'{token} O'", "\n", "\n", "", "elif", "pred_regex", ".", "match", "(", "labeled_token", ")", ":", "\n", "# node with lemma (ignored)", "\n", "        ", "token", ",", "lemma", "=", "pred_regex", ".", "match", "(", "labeled_token", ")", ".", "groups", "(", ")", "\n", "token_by_lemma", "[", "lemma", "]", ".", "update", "(", "[", "token", "]", ")", "\n", "# labeled tokens for each task", "\n", "wsd_labeled_token", "=", "f'{token} O'", "\n", "mcr_labeled_token", "=", "f'{token} O'", "\n", "ner_labeled_token", "=", "f'{token} O'", "\n", "\n", "", "elif", "addnode_regex", ".", "match", "(", "labeled_token", ")", ":", "\n", "# subgraph from addnode", "\n", "        ", "token", ",", "addnode", "=", "addnode_regex", ".", "match", "(", "labeled_token", ")", ".", "groups", "(", ")", "\n", "token_by_addnode", "[", "addnode", "]", ".", "update", "(", "[", "token", "]", ")", "\n", "# labeled tokens for each task", "\n", "wsd_labeled_token", "=", "f'{token} O'", "\n", "mcr_labeled_token", "=", "labeled_token", "\n", "if", "',name'", "in", "addnode", ":", "\n", "            ", "ner_labeled_token", "=", "labeled_token", "\n", "", "else", ":", "\n", "            ", "ner_labeled_token", "=", "f'{token} O'", "\n", "\n", "", "", "elif", "blank_regex", ".", "match", "(", "labeled_token", ")", ":", "\n", "# labeled tokens for each task", "\n", "        ", "wsd_labeled_token", "=", "labeled_token", "\n", "mcr_labeled_token", "=", "labeled_token", "\n", "ner_labeled_token", "=", "labeled_token", "\n", "\n", "", "else", ":", "\n", "        ", "import", "ipdb", ";", "ipdb", ".", "set_trace", "(", "context", "=", "30", ")", "\n", "print", "(", ")", "\n", "\n", "", "return", "wsd_labeled_token", ",", "mcr_labeled_token", ",", "ner_labeled_token", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.Blinker.LinkCache.__init__": [[32, 48], ["dict", "glob.glob", "dict", "pathtocachedirectory.endswith", "open", "json.load", "open.close", "Blinker.LinkCache.linkcache.update"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.GoldMiner.close", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update"], ["   ", "def", "__init__", "(", "self", ",", "pathtocachedirectory", ",", "fastmode", "=", "True", ")", ":", "\n", "      ", "if", "not", "pathtocachedirectory", ".", "endswith", "(", "'/'", ")", ":", "\n", "         ", "pathtocachedirectory", "+=", "'/'", "\n", "", "self", ".", "pathtocachedirectory", "=", "pathtocachedirectory", "\n", "self", ".", "fastmode", "=", "fastmode", "\n", "self", ".", "linkcache", "=", "dict", "(", ")", "\n", "jsonfileslist", "=", "glob", ".", "glob", "(", "f'{pathtocachedirectory}{CACHEFILEPATTERN}'", ")", "\n", "for", "fname", "in", "jsonfileslist", ":", "\n", "         ", "jfil", "=", "open", "(", "fname", ",", "'r'", ")", "\n", "tmpcache", "=", "json", ".", "load", "(", "jfil", ")", "\n", "jfil", ".", "close", "(", ")", "\n", "if", "not", "CACHEIDKEY", "in", "tmpcache", ":", "\n", "            ", "continue", "\n", "", "self", ".", "linkcache", ".", "update", "(", "tmpcache", "[", "CACHEDATAKEY", "]", ")", "\n", "\n", "", "self", ".", "localcache", "=", "dict", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.Blinker.LinkCache.lookup": [[49, 65], ["list", "Blinker.LinkCache.hash_key", "Blinker.LinkCache.hash_key", "list.append", "list.append", "Blinker.LinkCache.generalize_key", "Blinker.LinkCache.generalize_key"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.Blinker.LinkCache.hash_key", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.Blinker.LinkCache.hash_key", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.Blinker.LinkCache.generalize_key", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.Blinker.LinkCache.generalize_key"], ["", "def", "lookup", "(", "self", ",", "left", ",", "mention", ",", "right", ")", ":", "\n", "      ", "gkl", "=", "list", "(", ")", "\n", "gkslow", "=", "f'{SLOWTAG}#{self.generalize_key(left, mention, right)}'", "\n", "gkfast", "=", "f'{FASTTAG}#{self.generalize_key(left, mention, right)}'", "\n", "gkslowhashed", "=", "self", ".", "hash_key", "(", "gkslow", ")", "\n", "gkfasthashed", "=", "self", ".", "hash_key", "(", "gkfast", ")", "\n", "gkl", ".", "append", "(", "gkslowhashed", ")", "# allow cached results from slow mode even if caller is in fast mode", "\n", "if", "self", ".", "fastmode", ":", "# but don't allow fast mode results if caller is in slow mode", "\n", "         ", "gkl", ".", "append", "(", "gkfasthashed", ")", "\n", "", "for", "genkey", "in", "gkl", ":", "\n", "         ", "if", "self", ".", "localcache", ":", "\n", "            ", "if", "genkey", "in", "self", ".", "localcache", "[", "CACHEDATAKEY", "]", ":", "\n", "               ", "return", "self", ".", "localcache", "[", "CACHEDATAKEY", "]", "[", "genkey", "]", "[", "PREDICTIONSKEY", "]", ",", "self", ".", "localcache", "[", "CACHEDATAKEY", "]", "[", "genkey", "]", "[", "SCORESKEY", "]", "\n", "", "", "if", "genkey", "in", "self", ".", "linkcache", ":", "\n", "            ", "return", "self", ".", "linkcache", "[", "genkey", "]", "[", "PREDICTIONSKEY", "]", ",", "self", ".", "linkcache", "[", "genkey", "]", "[", "SCORESKEY", "]", "\n", "", "", "return", "None", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.Blinker.LinkCache.addtocache": [[66, 74], ["Blinker.LinkCache.hash_key", "dict", "Blinker.LinkCache.generalize_key"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.Blinker.LinkCache.hash_key", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.Blinker.LinkCache.generalize_key"], ["", "def", "addtocache", "(", "self", ",", "left", ",", "mention", ",", "right", ",", "predictions", ",", "scores", ",", "mode", "=", "None", ")", ":", "\n", "      ", "gkmodetag", "=", "mode", "if", "mode", "else", "(", "FASTTAG", "if", "self", ".", "fastmode", "else", "SLOWTAG", ")", "\n", "gk", "=", "f'{gkmodetag}#{self.generalize_key(left, mention, right)}'", "\n", "gkhashed", "=", "self", ".", "hash_key", "(", "gk", ")", "\n", "linkdata", "=", "{", "PREDICTIONSKEY", ":", "predictions", ",", "SCORESKEY", ":", "scores", "}", "\n", "if", "CACHEDATAKEY", "not", "in", "self", ".", "localcache", ":", "\n", "         ", "self", ".", "localcache", "[", "CACHEDATAKEY", "]", "=", "dict", "(", ")", "\n", "", "self", ".", "localcache", "[", "CACHEDATAKEY", "]", "[", "gkhashed", "]", "=", "linkdata", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.Blinker.LinkCache.hash_key": [[75, 77], ["hashlib.sha3_256().hexdigest", "hashlib.sha3_256", "cachekey.encode"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.encode"], ["", "def", "hash_key", "(", "self", ",", "cachekey", ")", ":", "\n", "      ", "return", "hashlib", ".", "sha3_256", "(", "cachekey", ".", "encode", "(", ")", ")", ".", "hexdigest", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.Blinker.LinkCache.generalize_key": [[78, 84], ["leftstr.lower", "mentionstr.lower", "rightstr.lower"], "methods", ["None"], ["", "def", "generalize_key", "(", "self", ",", "left", ",", "mention", ",", "right", ")", ":", "\n", "      ", "leftstr", "=", "''", ".", "join", "(", "[", "ch", "for", "ch", "in", "left", "if", "ch", "not", "in", "DELCHARS", "]", ")", "\n", "mentionstr", "=", "''", ".", "join", "(", "[", "ch", "for", "ch", "in", "mention", "if", "ch", "not", "in", "DELCHARS", "]", ")", "\n", "rightstr", "=", "''", ".", "join", "(", "[", "ch", "for", "ch", "in", "right", "if", "ch", "not", "in", "DELCHARS", "]", ")", "\n", "retstr", "=", "f'{leftstr.lower()}#{mentionstr.lower()}#{rightstr.lower()}'", "\n", "return", "retstr", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.Blinker.LinkCache.done": [[85, 102], ["glob.glob", "len", "random.randint", "os.path.exists", "open", "json.dump", "open.close", "random.randint"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.Stack.dump", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.GoldMiner.close"], ["", "def", "done", "(", "self", ")", ":", "\n", "      ", "if", "self", ".", "localcache", ":", "\n", "         ", "jsonfileslist", "=", "glob", ".", "glob", "(", "f'{self.pathtocachedirectory}/{CACHEFILEPATTERN}'", ")", "\n", "i", "=", "len", "(", "jsonfileslist", ")", "\n", "rn", "=", "random", ".", "randint", "(", "0", ",", "999", ")", "\n", "self", ".", "uid", "=", "f'{i:09d}_{rn:03d}'", "\n", "self", ".", "localcachename", "=", "f'{self.pathtocachedirectory}/linkcache_{self.uid}.json'", "\n", "while", "os", ".", "path", ".", "exists", "(", "self", ".", "localcachename", ")", ":", "\n", "            ", "i", "+=", "1", "\n", "rn", "=", "random", ".", "randint", "(", "0", ",", "999", ")", "\n", "self", ".", "uid", "=", "f'{i:09d}_{rn:03d}'", "\n", "self", ".", "localcachename", "=", "f'{self.pathtocachedirectory}/linkcache_{self.uid}.json'", "\n", "", "self", ".", "localcache", "[", "CACHEIDKEY", "]", "=", "self", ".", "uid", "\n", "self", ".", "localcache", "[", "MODEKEY", "]", "=", "FASTTAG", "if", "self", ".", "fastmode", "else", "SLOWTAG", "\n", "cfil", "=", "open", "(", "self", ".", "localcachename", ",", "'w'", ")", "\n", "json", ".", "dump", "(", "self", ".", "localcache", ",", "cfil", ")", "\n", "cfil", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.Blinker.Blinker.__init__": [[108, 162], ["Blinker.LinkCache", "argparse.Namespace", "logging.getLogger", "Blinker.Blinker.cachedir.endswith", "print", "LDConvert.LDConverter", "torch.cuda.is_available", "pathtomodeldirectory.endswith", "print", "print"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["   ", "def", "__init__", "(", "self", ",", "pathtocachedirectory", "=", "None", ",", "pathtomodeldirectory", "=", "None", ",", "fastmode", "=", "True", ",", "wikititleonly", "=", "False", ")", ":", "\n", "      ", "if", "pathtocachedirectory", ":", "\n", "         ", "self", ".", "cachedir", "=", "pathtocachedirectory", "\n", "", "else", ":", "\n", "         ", "if", "CACHEPATHENVVAR", "in", "os", ".", "environ", ":", "\n", "            ", "self", ".", "cachedir", "=", "os", ".", "environ", "[", "CACHEPATHENVVAR", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "cachedir", "=", "'.'", "\n", "\n", "", "", "if", "not", "self", ".", "cachedir", ".", "endswith", "(", "'/'", ")", ":", "\n", "         ", "self", ".", "cachedir", "+=", "'/'", "\n", "\n", "", "self", ".", "cacheonlymode", "=", "False", "\n", "if", "pathtomodeldirectory", ":", "\n", "         ", "import", "run_blink", "\n", "self", ".", "run_blink", "=", "run_blink", "\n", "if", "not", "pathtomodeldirectory", ".", "endswith", "(", "'/'", ")", ":", "\n", "            ", "pathtomodeldirectory", "+=", "'/'", "\n", "", "", "else", ":", "\n", "         ", "self", ".", "cacheonlymode", "=", "True", "\n", "print", "(", "'Not using BLINK (Blinker in cache-only mode)'", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "\n", "", "self", ".", "linkcache", "=", "LinkCache", "(", "self", ".", "cachedir", ",", "fastmode", "=", "fastmode", ")", "\n", "\n", "self", ".", "ldc", "=", "None", "\n", "if", "not", "wikititleonly", ":", "\n", "         ", "import", "LDConvert", "\n", "self", ".", "ldc", "=", "LDConvert", ".", "LDConverter", "(", ")", "\n", "\n", "", "self", ".", "config", "=", "{", "\n", "\"interactive\"", ":", "False", ",", "\n", "\"fast\"", ":", "fastmode", ",", "\n", "\"top_k\"", ":", "16", ",", "\n", "\"output_path\"", ":", "\"logs/\"", "\n", "}", "\n", "if", "not", "self", ".", "cacheonlymode", ":", "\n", "         ", "self", ".", "config", "[", "'biencoder_model'", "]", "=", "pathtomodeldirectory", "+", "'biencoder_wiki_large.bin'", "\n", "self", ".", "config", "[", "\"biencoder_config\"", "]", "=", "pathtomodeldirectory", "+", "\"biencoder_wiki_large.json\"", "\n", "self", ".", "config", "[", "\"entity_catalogue\"", "]", "=", "pathtomodeldirectory", "+", "\"entity.jsonl\"", "\n", "self", ".", "config", "[", "\"entity_encoding\"", "]", "=", "pathtomodeldirectory", "+", "\"all_entities_large.t7\"", "\n", "self", ".", "config", "[", "\"crossencoder_model\"", "]", "=", "pathtomodeldirectory", "+", "\"crossencoder_wiki_large.bin\"", "\n", "self", ".", "config", "[", "\"crossencoder_config\"", "]", "=", "pathtomodeldirectory", "+", "\"crossencoder_wiki_large.json\"", "\n", "\n", "", "if", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "# override fastmode param if no gpu available", "\n", "         ", "self", ".", "config", "[", "'fast'", "]", "=", "True", "\n", "", "if", "not", "self", ".", "cacheonlymode", ":", "\n", "         ", "if", "self", ".", "config", "[", "'fast'", "]", ":", "\n", "            ", "print", "(", "'Using BLINK in fast mode (no cross-encoder reranking)'", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "'Using BLINK in slow mode (bi-encoder candidate gen + cross-encoder reranking)'", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "\n", "", "", "args", "=", "argparse", ".", "Namespace", "(", "**", "self", ".", "config", ")", "\n", "self", ".", "logger", "=", "logging", ".", "getLogger", "(", ")", "\n", "self", ".", "models", "=", "None", "# load on demand (when mention+context not in cache)", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.Blinker.Blinker.runblink": [[165, 222], ["argparse.Namespace", "dict", "list", "enumerate", "list", "list", "list", "list", "enumerate", "Blinker.Blinker.linkcache.lookup", "all", "Blinker.Blinker.run_blink.run", "list", "enumerate", "list.append", "print", "Blinker.Blinker.run_blink.load_models", "list", "list.append", "Blinker.Blinker.linkcache.addtocache", "list.append", "list.append", "list.append", "list.append", "isinstance", "list.append", "score.item", "isinstance", "scores[].tolist"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.Blinker.LinkCache.lookup", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.service.amr_client.run", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_transformer_amr_parser.load_models", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.Blinker.LinkCache.addtocache", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item"], ["", "def", "runblink", "(", "self", ",", "mentiondata", ")", ":", "\n", "      ", "args", "=", "argparse", ".", "Namespace", "(", "**", "self", ".", "config", ")", "\n", "\n", "results", "=", "dict", "(", ")", "\n", "mentions2blink", "=", "list", "(", ")", "\n", "for", "i", ",", "mstru", "in", "enumerate", "(", "mentiondata", ")", ":", "\n", "         ", "(", "preds", ",", "scores", ")", "=", "self", ".", "linkcache", ".", "lookup", "(", "mstru", "[", "CONTEXTLEFTKEY", "]", ",", "mstru", "[", "MENTIONKEY", "]", ",", "mstru", "[", "CONTEXTRIGHTKEY", "]", ")", "\n", "if", "all", "(", "(", "preds", ",", "scores", ")", ")", ":", "\n", "            ", "results", "[", "i", "]", "=", "(", "preds", ",", "scores", ")", "\n", "", "else", ":", "\n", "            ", "results", "[", "i", "]", "=", "None", "\n", "mentions2blink", ".", "append", "(", "mstru", ")", "\n", "\n", "", "", "predictions", "=", "list", "(", ")", "\n", "scores", "=", "list", "(", ")", "\n", "if", "mentions2blink", "and", "not", "self", ".", "cacheonlymode", ":", "\n", "         ", "if", "not", "self", ".", "models", ":", "\n", "            ", "print", "(", "'Loading BLINK models...'", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "self", ".", "models", "=", "self", ".", "run_blink", ".", "load_models", "(", "args", ",", "logger", "=", "self", ".", "logger", ")", "\n", "\n", "", "_", ",", "_", ",", "_", ",", "_", ",", "_", ",", "predictions", ",", "numpyscores", "=", "self", ".", "run_blink", ".", "run", "(", "args", ",", "self", ".", "logger", ",", "*", "self", ".", "models", ",", "test_data", "=", "mentions2blink", ")", "\n", "scores", "=", "list", "(", ")", "\n", "for", "scoreslist", "in", "numpyscores", ":", "\n", "            ", "sl", "=", "list", "(", ")", "\n", "for", "score", "in", "scoreslist", ":", "\n", "               ", "s", "=", "score", "\n", "if", "isinstance", "(", "score", ",", "numpy", ".", "float32", ")", ":", "# stupid numpy/json hack (float32 not serializable even though other floats are)", "\n", "                  ", "s", "=", "score", ".", "item", "(", ")", "\n", "", "sl", ".", "append", "(", "s", ")", "\n", "", "scores", ".", "append", "(", "sl", ")", "\n", "\n", "", "for", "i", ",", "mstru", "in", "enumerate", "(", "mentions2blink", ")", ":", "\n", "            ", "s", "=", "scores", "[", "i", "]", "\n", "self", ".", "linkcache", ".", "addtocache", "(", "mstru", "[", "CONTEXTLEFTKEY", "]", ",", "mstru", "[", "MENTIONKEY", "]", ",", "mstru", "[", "CONTEXTRIGHTKEY", "]", ",", "predictions", "[", "i", "]", ",", "s", ")", "\n", "\n", "", "", "j", "=", "0", "\n", "retpreds", "=", "list", "(", ")", "\n", "retscores", "=", "list", "(", ")", "\n", "for", "i", ",", "mstru", "in", "enumerate", "(", "mentiondata", ")", ":", "\n", "         ", "if", "results", "[", "i", "]", ":", "\n", "            ", "retpreds", ".", "append", "(", "results", "[", "i", "]", "[", "0", "]", ")", "\n", "retscores", ".", "append", "(", "results", "[", "i", "]", "[", "1", "]", ")", "\n", "", "else", ":", "\n", "            ", "currpreds", "=", "[", "''", "]", "\n", "currscores", "=", "[", "0.0", "]", "\n", "if", "predictions", "and", "scores", ":", "\n", "               ", "if", "predictions", "[", "j", "]", "and", "scores", "[", "j", "]", ":", "\n", "                  ", "currpreds", "=", "predictions", "[", "j", "]", "\n", "flatscores", "=", "scores", "[", "j", "]", "\n", "if", "not", "isinstance", "(", "scores", "[", "j", "]", ",", "list", ")", ":", "\n", "                     ", "flatscores", "=", "scores", "[", "j", "]", ".", "tolist", "(", ")", "\n", "", "currscores", "=", "flatscores", "\n", "", "", "retpreds", ".", "append", "(", "currpreds", ")", "\n", "retscores", ".", "append", "(", "currscores", ")", "\n", "j", "+=", "1", "\n", "\n", "", "", "return", "retpreds", ",", "retscores", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.Blinker.Blinker.runblinks": [[225, 255], ["sentence.strip().split", "list", "list", "list", "dict", "Blinker.Blinker.runblink", "sentence.strip", "list.append", "list.append"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.Blinker.Blinker.runblink", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "runblinks", "(", "self", ",", "sentence", ")", ":", "\n", "      ", "stoks", "=", "sentence", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "lcontext", "=", "list", "(", ")", "\n", "mention", "=", "list", "(", ")", "\n", "rcontext", "=", "list", "(", ")", "\n", "i", "=", "0", "\n", "for", "tok", "in", "stoks", ":", "\n", "         ", "i", "+=", "1", "\n", "if", "tok", "==", "MENTIONSTART", ":", "\n", "            ", "break", "\n", "", "else", ":", "\n", "            ", "lcontext", ".", "append", "(", "tok", ")", "\n", "", "", "for", "tok", "in", "stoks", "[", "i", ":", "]", ":", "\n", "         ", "i", "+=", "1", "\n", "if", "tok", "==", "MENTIONEND", ":", "\n", "            ", "break", "\n", "", "else", ":", "\n", "            ", "mention", ".", "append", "(", "tok", ")", "\n", "", "", "rcontext", "=", "stoks", "[", "i", ":", "]", "\n", "mentionstr", "=", "' '", ".", "join", "(", "mention", ")", ".", "lower", "(", ")", "\n", "\n", "mstru", "=", "dict", "(", ")", "\n", "mstru", "[", "'label'", "]", "=", "'unknown'", "\n", "mstru", "[", "'label_id'", "]", "=", "-", "1", "\n", "mstru", "[", "CONTEXTLEFTKEY", "]", "=", "' '", ".", "join", "(", "lcontext", ")", ".", "lower", "(", ")", "\n", "mstru", "[", "MENTIONKEY", "]", "=", "mentionstr", "\n", "mstru", "[", "CONTEXTRIGHTKEY", "]", "=", "' '", ".", "join", "(", "rcontext", ")", ".", "lower", "(", ")", "\n", "\n", "predictions", ",", "scores", "=", "self", ".", "runblink", "(", "[", "mstru", "]", ")", "\n", "return", "mentionstr", ",", "predictions", ",", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.Blinker.Blinker.getwpinfofortitle": [[258, 268], ["Blinker.Blinker.ldc.get_WPinfo_for_WPtitles"], "methods", ["None"], ["", "def", "getwpinfofortitle", "(", "self", ",", "wptitle", ")", ":", "\n", "      ", "wpid", "=", "''", "\n", "resurl", "=", "''", "\n", "if", "wptitle", ":", "\n", "         ", "if", "self", ".", "ldc", ":", "\n", "            ", "wpinfolist", "=", "self", ".", "ldc", ".", "get_WPinfo_for_WPtitles", "(", "[", "wptitle", "]", ")", "\n", "if", "wpinfolist", ":", "\n", "               ", "wpid", ",", "resurl", "=", "wpinfolist", "[", "0", "]", "\n", "\n", "", "", "", "return", "wpid", ",", "resurl", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.Blinker.Blinker.getdbpuriforwpid": [[269, 277], ["Blinker.Blinker.ldc.get_DBPuris_for_WPids"], "methods", ["None"], ["", "def", "getdbpuriforwpid", "(", "self", ",", "wpid", ")", ":", "\n", "      ", "dbpuri", "=", "''", "\n", "if", "self", ".", "ldc", ":", "\n", "         ", "dbpurilist", "=", "self", ".", "ldc", ".", "get_DBPuris_for_WPids", "(", "[", "wpid", "]", ")", "\n", "if", "dbpurilist", ":", "\n", "            ", "dbpuri", "=", "dbpurilist", "[", "0", "]", "\n", "\n", "", "", "return", "dbpuri", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.Blinker.Blinker.done": [[278, 282], ["Blinker.Blinker.linkcache.done", "Blinker.Blinker.ldc.flush"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.Blinker.Blinker.done"], ["", "def", "done", "(", "self", ")", ":", "\n", "      ", "self", ".", "linkcache", ".", "done", "(", ")", "\n", "if", "self", ".", "ldc", ":", "\n", "         ", "self", ".", "ldc", ".", "flush", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.Blinker.runtxt": [[285, 299], ["open", "open.close", "os.path.exists", "print", "Blinker.linksentenceall", "line.strip", "json.dumps", "print", "sys.stdout.flush"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.GoldMiner.close", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.Blinker.linksentenceall", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["", "", "", "def", "runtxt", "(", "blinker", ",", "inputfilename", ")", ":", "\n", "   ", "if", "not", "os", ".", "path", ".", "exists", "(", "inputfilename", ")", ":", "\n", "      ", "print", "(", "f'input file {inputfilename} not found'", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "return", "\n", "\n", "", "txtfil", "=", "open", "(", "inputfilename", ",", "'r'", ")", "\n", "for", "line", "in", "txtfil", ":", "\n", "      ", "mstrus", "=", "linksentenceall", "(", "blinker", ",", "line", ".", "strip", "(", ")", ")", "\n", "for", "mstru", "in", "mstrus", ":", "\n", "         ", "jstr", "=", "json", ".", "dumps", "(", "mstru", ")", "\n", "print", "(", "jstr", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "\n", "", "", "txtfil", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.Blinker.runIOB": [[303, 329], ["list", "open", "os.path.exists", "print", "line.strip().startswith", "line.strip().split", "list.append", "Blinker.taganddumpiob", "line.strip", "Blinker.taganddumpiob", "list", "len", "line.strip", "line.strip"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.Blinker.taganddumpiob", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.Blinker.taganddumpiob"], ["def", "runIOB", "(", "blinker", ",", "inputfilename", ")", ":", "\n", "   ", "if", "not", "os", ".", "path", ".", "exists", "(", "inputfilename", ")", ":", "\n", "      ", "print", "(", "f'input file {inputfilename} not found'", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "return", "\n", "\n", "", "stoks", "=", "list", "(", ")", "\n", "iobfil", "=", "open", "(", "inputfilename", ",", "'r'", ")", "\n", "snum", "=", "0", "\n", "for", "line", "in", "iobfil", ":", "\n", "      ", "if", "line", ".", "strip", "(", ")", ".", "startswith", "(", "'#'", ")", ":", "\n", "         ", "continue", "\n", "", "if", "not", "line", ".", "strip", "(", ")", ":", "\n", "         ", "taganddumpiob", "(", "blinker", ",", "snum", ",", "stoks", ")", "\n", "snum", "+=", "1", "\n", "stoks", "=", "list", "(", ")", "\n", "continue", "\n", "\n", "", "fields", "=", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "tok", "=", "fields", "[", "0", "]", "\n", "tag", "=", "'O'", "\n", "if", "len", "(", "fields", ")", ">=", "2", ":", "\n", "         ", "tag", "=", "fields", "[", "1", "]", "[", "0", "]", "\n", "", "stoks", ".", "append", "(", "(", "tok", ",", "tag", ")", ")", "\n", "\n", "", "if", "stoks", ":", "\n", "      ", "taganddumpiob", "(", "blinker", ",", "snum", ",", "stoks", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.Blinker.taganddumpiob": [[332, 383], ["list", "enumerate", "sys.stdout.flush", "dict", "blinker.runblinks", "len", "blinker.getwpinfofortitle", "blinker.getdbpuriforwpid", "json.dumps", "print", "list.append", "list.append"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.Blinker.Blinker.runblinks", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.Blinker.Blinker.getwpinfofortitle", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.Blinker.Blinker.getdbpuriforwpid", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "", "def", "taganddumpiob", "(", "blinker", ",", "snum", ",", "stoks", ")", ":", "\n", "   ", "toks", "=", "[", "t", "for", "(", "t", ",", "_", ")", "in", "stoks", "]", "\n", "mentions", "=", "list", "(", ")", "\n", "insidespan", "=", "False", "\n", "spanstart", "=", "0", "\n", "for", "i", ",", "(", "tok", ",", "tag", ")", "in", "enumerate", "(", "stoks", ")", ":", "\n", "      ", "if", "tag", "==", "'B'", ":", "\n", "         ", "if", "insidespan", ":", "\n", "            ", "lcontext", "=", "toks", "[", "0", ":", "spanstart", "]", "\n", "mention", "=", "toks", "[", "spanstart", ":", "i", "]", "\n", "rcontext", "=", "toks", "[", "i", ":", "]", "\n", "mentions", ".", "append", "(", "(", "lcontext", ",", "mention", ",", "rcontext", ")", ")", "\n", "", "insidespan", "=", "True", "\n", "spanstart", "=", "i", "\n", "", "elif", "tag", "==", "'O'", ":", "\n", "         ", "if", "insidespan", ":", "\n", "            ", "lcontext", "=", "toks", "[", "0", ":", "spanstart", "]", "\n", "mention", "=", "toks", "[", "spanstart", ":", "i", "]", "\n", "rcontext", "=", "toks", "[", "i", ":", "]", "\n", "mentions", ".", "append", "(", "(", "lcontext", ",", "mention", ",", "rcontext", ")", ")", "\n", "insidespan", "=", "False", "\n", "\n", "", "", "", "s", "=", "' '", ".", "join", "(", "toks", ")", "\n", "for", "(", "l", ",", "m", ",", "r", ")", "in", "mentions", ":", "\n", "      ", "mstru", "=", "dict", "(", ")", "\n", "lstr", "=", "' '", ".", "join", "(", "l", ")", "\n", "mstr", "=", "' '", ".", "join", "(", "m", ")", "\n", "rstr", "=", "' '", ".", "join", "(", "r", ")", "\n", "stagged", "=", "lstr", "+", "' '", "+", "MENTIONSTART", "+", "' '", "+", "mstr", "+", "' '", "+", "MENTIONEND", "+", "' '", "+", "rstr", "\n", "mention", ",", "predictions", ",", "scores", "=", "blinker", ".", "runblinks", "(", "stagged", ")", "\n", "if", "len", "(", "predictions", ")", ">", "0", ":", "\n", "         ", "wptitle", "=", "predictions", "[", "0", "]", "[", "0", "]", "\n", "wpid", ",", "resurl", "=", "blinker", ".", "getwpinfofortitle", "(", "wptitle", ")", "\n", "\n", "mstru", "[", "'id'", "]", "=", "snum", "\n", "mstru", "[", "'sentence'", "]", "=", "s", "\n", "mstru", "[", "CONTEXTLEFTKEY", "]", "=", "lstr", "\n", "mstru", "[", "MENTIONKEY", "]", "=", "mstr", "\n", "mstru", "[", "CONTEXTRIGHTKEY", "]", "=", "rstr", "\n", "mstru", "[", "'Wikipedia_ID'", "]", "=", "wpid", "\n", "mstru", "[", "'Wikipedia_URL'", "]", "=", "resurl", "\n", "mstru", "[", "'Wikipedia_title'", "]", "=", "wptitle", "\n", "\n", "dbpuri", "=", "blinker", ".", "getdbpuriforwpid", "(", "wpid", ")", "\n", "if", "dbpuri", ":", "\n", "            ", "mstru", "[", "'DBPedia_URI'", "]", "=", "dbpuri", "\n", "\n", "", "jstr", "=", "json", ".", "dumps", "(", "mstru", ")", "\n", "print", "(", "jstr", ")", "\n", "\n", "", "", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.Blinker.linksentenceall": [[386, 481], ["sentence.replace.find", "list", "blinker.runblink", "list", "enumerate", "sentence.replace.split", "sentence.replace.replace", "sentence.replace.replace", "sentence.replace.find", "len", "len", "sentence.replace.split", "enumerate", "dict", "blinker.getwpinfofortitle", "blinker.getdbpuriforwpid", "list.append", "sentence[].replace().replace().strip", "sentence[].replace().replace().strip", "sentence.replace.replace().replace().strip", "dict", "list.append", "sentence[].find", "sentence[].find", "range", "m.replace().strip.replace().strip", "len", "len", "dict", "list.append", "sentence[].replace().replace", "sentence[].replace().replace", "sentence.replace.replace().replace", "len", "m.replace().strip.replace", "len", "sentence[].replace", "sentence[].replace", "sentence.replace.replace"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.Blinker.Blinker.runblink", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.Blinker.Blinker.getwpinfofortitle", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.Blinker.Blinker.getdbpuriforwpid", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "linksentenceall", "(", "blinker", ",", "sentence", ")", ":", "\n", "   ", "sid", "=", "0", "\n", "if", "'\\t'", "in", "sentence", ":", "\n", "      ", "fields", "=", "sentence", ".", "split", "(", "'\\t'", ")", "\n", "sid", "=", "fields", "[", "0", "]", "\n", "sentence", "=", "fields", "[", "1", "]", "\n", "\n", "", "if", "'<'", "in", "sentence", ":", "\n", "      ", "sentence", "=", "sentence", ".", "replace", "(", "'<'", ",", "MENTIONSTART", "+", "' '", ")", "\n", "sentence", "=", "sentence", ".", "replace", "(", "'>'", ",", "' '", "+", "MENTIONEND", ")", "\n", "\n", "", "j", "=", "sentence", ".", "find", "(", "MENTIONSTART", ")", "\n", "k", "=", "sentence", ".", "find", "(", "MENTIONEND", ")", "+", "len", "(", "MENTIONEND", ")", "\n", "if", "k", "<", "0", ":", "\n", "      ", "k", "=", "len", "(", "sentence", ")", "\n", "", "mstrus", "=", "list", "(", ")", "\n", "if", "j", ">=", "0", ":", "\n", "      ", "while", "j", ">=", "0", ":", "\n", "         ", "l", "=", "sentence", "[", ":", "j", "]", ".", "replace", "(", "MENTIONSTART", ",", "''", ")", ".", "replace", "(", "MENTIONEND", ",", "''", ")", ".", "strip", "(", ")", "\n", "m", "=", "sentence", "[", "j", "+", "len", "(", "MENTIONSTART", ")", ":", "k", "]", "\n", "if", "MENTIONEND", "in", "m", ":", "\n", "            ", "m", "=", "m", ".", "replace", "(", "MENTIONEND", ",", "''", ")", ".", "strip", "(", ")", "\n", "", "r", "=", "sentence", "[", "k", ":", "]", ".", "replace", "(", "MENTIONSTART", ",", "''", ")", ".", "replace", "(", "MENTIONEND", ",", "''", ")", ".", "strip", "(", ")", "\n", "s", "=", "sentence", ".", "replace", "(", "MENTIONSTART", ",", "''", ")", ".", "replace", "(", "MENTIONEND", ",", "''", ")", ".", "strip", "(", ")", "\n", "\n", "mstru", "=", "dict", "(", ")", "\n", "mstru", "[", "'id'", "]", "=", "sid", "\n", "mstru", "[", "'sentence'", "]", "=", "s", "\n", "mstru", "[", "'label'", "]", "=", "'unknown'", "\n", "mstru", "[", "'label_id'", "]", "=", "-", "1", "\n", "mstru", "[", "CONTEXTLEFTKEY", "]", "=", "l", "\n", "mstru", "[", "MENTIONKEY", "]", "=", "m", "\n", "mstru", "[", "CONTEXTRIGHTKEY", "]", "=", "r", "\n", "\n", "mstrus", ".", "append", "(", "mstru", ")", "\n", "\n", "j", "=", "sentence", "[", "k", ":", "]", ".", "find", "(", "MENTIONSTART", ")", "\n", "if", "j", ">", "0", ":", "\n", "            ", "j", "+=", "k", "\n", "", "tmpk", "=", "sentence", "[", "k", ":", "]", ".", "find", "(", "MENTIONEND", ")", "\n", "if", "tmpk", "<", "0", ":", "\n", "            ", "k", "=", "len", "(", "sentence", ")", "\n", "", "else", ":", "\n", "            ", "k", "=", "tmpk", "+", "len", "(", "MENTIONEND", ")", "+", "k", "\n", "", "", "", "else", ":", "\n", "      ", "stoks", "=", "sentence", ".", "split", "(", ")", "\n", "for", "i", ",", "tok", "in", "enumerate", "(", "stoks", ")", ":", "\n", "         ", "l", "=", "stoks", "[", ":", "i", "]", "\n", "for", "j", "in", "range", "(", "len", "(", "stoks", "[", "i", ":", "]", ")", ")", ":", "\n", "            ", "k", "=", "i", "+", "j", "+", "1", "\n", "if", "(", "k", "-", "i", ")", ">", "MAXMENTIONWINDOW", ":", "\n", "               ", "break", "\n", "", "m", "=", "stoks", "[", "i", ":", "k", "]", "\n", "r", "=", "stoks", "[", "k", ":", "]", "\n", "\n", "mstru", "=", "dict", "(", ")", "\n", "mstru", "[", "'id'", "]", "=", "sid", "\n", "mstru", "[", "'sentence'", "]", "=", "sentence", "\n", "mstru", "[", "'label'", "]", "=", "'unknown'", "\n", "mstru", "[", "'label_id'", "]", "=", "-", "1", "\n", "mstru", "[", "CONTEXTLEFTKEY", "]", "=", "' '", ".", "join", "(", "l", ")", "\n", "mstru", "[", "MENTIONKEY", "]", "=", "' '", ".", "join", "(", "m", ")", "\n", "mstru", "[", "CONTEXTRIGHTKEY", "]", "=", "' '", ".", "join", "(", "r", ")", "\n", "\n", "mstrus", ".", "append", "(", "mstru", ")", "\n", "\n", "", "", "", "predictions", ",", "scores", "=", "blinker", ".", "runblink", "(", "mstrus", ")", "\n", "\n", "retstrus", "=", "list", "(", ")", "\n", "for", "i", ",", "preds", "in", "enumerate", "(", "predictions", ")", ":", "\n", "      ", "mstru", "=", "dict", "(", ")", "\n", "wptitle", "=", "preds", "[", "0", "]", "\n", "topscore", "=", "scores", "[", "i", "]", "[", "0", "]", "\n", "wpid", ",", "resurl", "=", "blinker", ".", "getwpinfofortitle", "(", "wptitle", ")", "\n", "\n", "mstru", "[", "'id'", "]", "=", "mstrus", "[", "i", "]", "[", "'id'", "]", "\n", "mstru", "[", "'sentence'", "]", "=", "mstrus", "[", "i", "]", "[", "'sentence'", "]", "\n", "mstru", "[", "CONTEXTLEFTKEY", "]", "=", "mstrus", "[", "i", "]", "[", "CONTEXTLEFTKEY", "]", "\n", "mstru", "[", "MENTIONKEY", "]", "=", "mstrus", "[", "i", "]", "[", "MENTIONKEY", "]", "\n", "mstru", "[", "CONTEXTRIGHTKEY", "]", "=", "mstrus", "[", "i", "]", "[", "CONTEXTRIGHTKEY", "]", "\n", "mstru", "[", "'Wikipedia_title'", "]", "=", "wptitle", "\n", "mstru", "[", "'Link_score'", "]", "=", "topscore", "\n", "\n", "if", "wpid", ":", "\n", "         ", "mstru", "[", "'Wikipedia_ID'", "]", "=", "wpid", "\n", "", "if", "resurl", ":", "\n", "         ", "mstru", "[", "'Wikipedia_URL'", "]", "=", "resurl", "\n", "\n", "", "dbpuri", "=", "blinker", ".", "getdbpuriforwpid", "(", "wpid", ")", "\n", "if", "dbpuri", ":", "\n", "         ", "mstru", "[", "'DBPedia_URI'", "]", "=", "dbpuri", "\n", "\n", "", "retstrus", ".", "append", "(", "mstru", ")", "\n", "\n", "", "return", "retstrus", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.Blinker.main": [[484, 570], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "Blinker.Blinker", "Blinker.done", "os.path.exists", "Blinker.runIOB", "os.path.exists", "print", "sys.exit", "os.path.exists", "print", "sys.exit", "os.path.exists", "print", "sys.exit", "parser.parse_args.inputfile.endswith", "print", "sys.exit", "open", "Blinker.runtxt", "parser.parse_args.inputfile.endswith", "print", "sys.exit", "input", "Blinker.linksentenceall", "input.strip", "print", "print", "print", "print", "print", "print"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.Blinker.Blinker.done", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.Blinker.runIOB", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.Blinker.runtxt", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.Blinker.linksentenceall", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["", "def", "main", "(", ")", ":", "\n", "\n", "   ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "'Blinker'", ")", "\n", "parser", ".", "add_argument", "(", "'-i'", ",", "'--inputfile'", ",", "action", "=", "'store'", ",", "help", "=", "'path to inputfile (.txt or .iob format)'", ")", "\n", "parser", ".", "add_argument", "(", "'-o'", ",", "'--outputfile'", ",", "action", "=", "'store'", ",", "help", "=", "'path to output file'", ")", "\n", "parser", ".", "add_argument", "(", "'-m'", ",", "'--blinkmodels'", ",", "action", "=", "'store'", ",", "help", "=", "'path to BLINK models'", ")", "\n", "parser", ".", "add_argument", "(", "'-c'", ",", "'--cachedirectory'", ",", "action", "=", "'store'", ",", "help", "=", "'path to cache directory'", ")", "\n", "parser", ".", "add_argument", "(", "'-r'", ",", "'--rawoutput'", ",", "action", "=", "'store_true'", ",", "help", "=", "'output BLINK links (wikipedia titles) only (no mapping to other vocabularies)'", ")", "\n", "parser", ".", "add_argument", "(", "'-x'", ",", "'--crossencoder'", ",", "action", "=", "'store_true'", ",", "help", "=", "'run cross-encoder reranking (slow mode)'", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "if", "args", ".", "blinkmodels", ":", "\n", "      ", "if", "not", "os", ".", "path", ".", "exists", "(", "args", ".", "blinkmodels", ")", ":", "\n", "         ", "print", "(", "f'path to blink models {args.blinkmodels} not found'", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "sys", ".", "exit", "(", "2", ")", "\n", "\n", "", "", "if", "args", ".", "cachedirectory", ":", "\n", "      ", "if", "not", "os", ".", "path", ".", "exists", "(", "args", ".", "cachedirectory", ")", ":", "\n", "         ", "print", "(", "f'path to cache directory {args.cachedirectory} not found'", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "sys", ".", "exit", "(", "3", ")", "\n", "\n", "", "", "infmt", "=", "''", "\n", "if", "args", ".", "inputfile", ":", "\n", "      ", "if", "not", "os", ".", "path", ".", "exists", "(", "args", ".", "inputfile", ")", ":", "\n", "         ", "print", "(", "f'input file {args.inputfile} not found'", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "sys", ".", "exit", "(", "4", ")", "\n", "", "else", ":", "\n", "         ", "if", "args", ".", "inputfile", ".", "endswith", "(", "'.iob'", ")", ":", "\n", "            ", "infmt", "=", "'iob'", "\n", "", "elif", "args", ".", "inputfile", ".", "endswith", "(", "'.txt'", ")", ":", "\n", "            ", "infmt", "=", "'txt'", "\n", "", "else", ":", "\n", "            ", "print", "(", "f'unknown format for input file {args.inputfile}; should be .iob or .txt'", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "sys", ".", "exit", "(", "5", ")", "\n", "\n", "", "", "", "if", "args", ".", "outputfile", ":", "\n", "      ", "if", "os", ".", "path", ".", "exists", "(", "args", ".", "outputfile", ")", ":", "\n", "         ", "print", "(", "f'output file {args.outputfile} exists'", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "sys", ".", "exit", "(", "6", ")", "\n", "", "else", ":", "\n", "         ", "outfil", "=", "open", "(", "args", ".", "outputfile", ",", "'w'", ")", "\n", "\n", "", "", "wtitlesonly", "=", "False", "\n", "if", "args", ".", "rawoutput", ":", "\n", "      ", "wtitlesonly", "=", "True", "\n", "\n", "", "blinker", "=", "None", "\n", "fastmode", "=", "True", "\n", "if", "args", ".", "crossencoder", ":", "\n", "      ", "fastmode", "=", "False", "\n", "", "blinker", "=", "Blinker", "(", "pathtocachedirectory", "=", "args", ".", "cachedirectory", ",", "pathtomodeldirectory", "=", "args", ".", "blinkmodels", ",", "fastmode", "=", "fastmode", ",", "wikititleonly", "=", "wtitlesonly", ")", "\n", "\n", "if", "infmt", "==", "'iob'", ":", "\n", "      ", "runIOB", "(", "blinker", ",", "args", ".", "inputfile", ")", "\n", "", "elif", "infmt", "==", "'txt'", ":", "\n", "      ", "runtxt", "(", "blinker", ",", "args", ".", "inputfile", ")", "\n", "", "else", ":", "\n", "      ", "while", "True", ":", "\n", "         ", "try", ":", "\n", "            ", "s", "=", "input", "(", "'BLINK> '", ")", "\n", "mstrus", "=", "linksentenceall", "(", "blinker", ",", "s", ".", "strip", "(", ")", ")", "\n", "\n", "for", "mstru", "in", "mstrus", ":", "\n", "               ", "mention", "=", "mstru", "[", "MENTIONKEY", "]", "\n", "wpid", "=", "''", "\n", "if", "'Wikipedia_ID'", "in", "mstru", ":", "\n", "                  ", "wpid", "=", "mstru", "[", "'Wikipedia_ID'", "]", "\n", "", "wpurl", "=", "''", "\n", "if", "'Wikipedia_URL'", "in", "mstru", ":", "\n", "                  ", "wpurl", "=", "mstru", "[", "'Wikipedia_URL'", "]", "\n", "", "wptitle", "=", "mstru", "[", "'Wikipedia_title'", "]", "\n", "linkscore", "=", "mstru", "[", "'Link_score'", "]", "\n", "dbpuri", "=", "''", "\n", "if", "'DBPedia_URI'", "in", "mstru", ":", "\n", "                  ", "dbpuri", "=", "mstru", "[", "'DBPedia_URI'", "]", "\n", "", "print", "(", "f'mention=\"{mention}\"'", ")", "\n", "print", "(", "f'\\twptitle=\"{wptitle}\"'", ")", "\n", "print", "(", "f'\\tlinkscore=\"{linkscore}\"'", ")", "\n", "print", "(", "f'\\twpid=\"{wpid}\"'", ")", "\n", "print", "(", "f'\\twpurl=\"{wpurl}\"'", ")", "\n", "print", "(", "f'\\tdbpuri=\"{dbpuri}\"'", ")", "\n", "\n", "", "", "except", "EOFError", ":", "\n", "            ", "break", "\n", "\n", "", "", "", "blinker", ".", "done", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParamStore.__init__": [[121, 136], ["dict", "list"], "methods", ["None"], ["   ", "def", "__init__", "(", "self", ")", ":", "\n", "      ", "self", ".", "pstore", "=", "dict", "(", ")", "\n", "self", ".", "pstore", "[", "'typejoiner'", "]", "=", "AMRTYPEJOINER", "\n", "self", ".", "pstore", "[", "'typepad'", "]", "=", "''", "\n", "self", ".", "pstore", "[", "'retypeamronly'", "]", "=", "False", "\n", "self", ".", "pstore", "[", "'retypetypes'", "]", "=", "list", "(", ")", "\n", "self", ".", "pstore", "[", "'debug'", "]", "=", "False", "\n", "self", ".", "pstore", "[", "'skipretyper'", "]", "=", "False", "\n", "self", ".", "pstore", "[", "'withdictionary'", "]", "=", "False", "\n", "self", ".", "pstore", "[", "'outputscores'", "]", "=", "False", "\n", "self", ".", "pstore", "[", "'explorethreshold'", "]", "=", "False", "\n", "self", ".", "pstore", "[", "'noclobber'", "]", "=", "False", "\n", "self", ".", "pstore", "[", "'blinkfastmode'", "]", "=", "True", "\n", "self", ".", "pstore", "[", "'scorethreshold'", "]", "=", "0.0", "\n", "self", ".", "pstore", "[", "'blinkthreshold'", "]", "=", "MAXNEGFLOAT", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParamStore.getparam": [[137, 139], ["None"], "methods", ["None"], ["", "def", "getparam", "(", "self", ",", "paramname", ")", ":", "\n", "      ", "return", "self", ".", "pstore", "[", "paramname", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParamStore.setparam": [[140, 142], ["None"], "methods", ["None"], ["", "def", "setparam", "(", "self", ",", "paramname", ",", "paramvalue", ")", ":", "\n", "      ", "self", ".", "pstore", "[", "paramname", "]", "=", "paramvalue", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParamStore.tostring": [[143, 146], ["retyper.ParamStore.pstore.items"], "methods", ["None"], ["", "def", "tostring", "(", "self", ")", ":", "\n", "      ", "s", "=", "'\\n'", ".", "join", "(", "[", "f'pstore[{p}] == {v}'", "for", "p", ",", "v", "in", "self", ".", "pstore", ".", "items", "(", ")", "]", ")", "\n", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.Node.__init__": [[150, 163], ["nstart.isdigit", "nend.isdigit", "int", "int"], "methods", ["None"], ["   ", "def", "__init__", "(", "self", ",", "aid", ",", "ntype", ",", "nstart", ",", "nend", ",", "nodeline", ")", ":", "\n", "      ", "self", ".", "aid", "=", "aid", "\n", "self", ".", "ntype", "=", "ntype", "\n", "if", "nstart", ".", "isdigit", "(", ")", ":", "\n", "         ", "self", ".", "start", "=", "int", "(", "nstart", ")", "\n", "", "if", "nend", ".", "isdigit", "(", ")", ":", "\n", "         ", "self", ".", "end", "=", "int", "(", "nend", ")", "\n", "", "self", ".", "nodeline", "=", "nodeline", "\n", "self", ".", "parsepiece", "=", "-", "1", "\n", "self", ".", "retype", "=", "self", ".", "ntype", "\n", "self", ".", "entitylink", "=", "None", "\n", "self", ".", "namenode", "=", "None", "\n", "self", ".", "score", "=", "1.0", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.Node.setretype": [[164, 166], ["None"], "methods", ["None"], ["", "def", "setretype", "(", "self", ",", "retype", ")", ":", "\n", "      ", "self", ".", "retype", "=", "retype", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.Node.setscore": [[167, 169], ["None"], "methods", ["None"], ["", "def", "setscore", "(", "self", ",", "score", ")", ":", "\n", "      ", "self", ".", "score", "=", "score", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.Node.setparsepiece": [[170, 172], ["None"], "methods", ["None"], ["", "def", "setparsepiece", "(", "self", ",", "pp", ")", ":", "\n", "      ", "self", ".", "parsepiece", "=", "pp", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.Node.setentitylink": [[173, 175], ["None"], "methods", ["None"], ["", "def", "setentitylink", "(", "self", ",", "el", ")", ":", "\n", "      ", "self", ".", "entitylink", "=", "el", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.Node.setnamenode": [[176, 178], ["None"], "methods", ["None"], ["", "def", "setnamenode", "(", "self", ",", "nn", ")", ":", "\n", "      ", "self", ".", "namenode", "=", "nn", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.Node.tostring": [[179, 188], ["None"], "methods", ["None"], ["", "def", "tostring", "(", "self", ")", ":", "\n", "      ", "s", "=", "f'self.aid = {self.aid}\\n'", "\n", "s", "+=", "f'self.ntype = {self.ntype}\\n'", "\n", "s", "+=", "f'self.start = {self.start}\\n'", "\n", "s", "+=", "f'self.end = {self.end}\\n'", "\n", "s", "+=", "f'self.nodeline = {self.nodeline}\\n'", "\n", "s", "+=", "f'self.parsepiece = {self.parsepiece}\\n'", "\n", "s", "+=", "f'self.entitylink = {self.entitylink}\\n'", "\n", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.Edge.__init__": [[192, 202], ["None"], "methods", ["None"], ["   ", "def", "__init__", "(", "self", ",", "etype", ",", "arg1type", ",", "arg1id", ",", "arg2type", ",", "arg2id", ",", "edgelinenum", ")", ":", "\n", "      ", "self", ".", "etype", "=", "etype", "\n", "self", ".", "arg1type", "=", "arg1type", "\n", "self", ".", "arg1id", "=", "arg1id", "\n", "self", ".", "arg2type", "=", "arg2type", "\n", "self", ".", "arg2id", "=", "arg2id", "\n", "self", ".", "edgelinenum", "=", "edgelinenum", "\n", "self", ".", "parsepiece", "=", "-", "1", "\n", "self", ".", "retype", "=", "self", ".", "etype", "\n", "self", ".", "score", "=", "1.0", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.Edge.setretype": [[203, 205], ["None"], "methods", ["None"], ["", "def", "setretype", "(", "self", ",", "retype", ")", ":", "\n", "      ", "self", ".", "retype", "=", "retype", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.Edge.setscore": [[206, 208], ["None"], "methods", ["None"], ["", "def", "setscore", "(", "self", ",", "score", ")", ":", "\n", "      ", "self", ".", "score", "=", "score", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.Edge.setparsepiece": [[209, 211], ["None"], "methods", ["None"], ["", "def", "setparsepiece", "(", "self", ",", "pp", ")", ":", "\n", "      ", "self", ".", "parsepiece", "=", "pp", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.Edge.tostring": [[212, 221], ["None"], "methods", ["None"], ["", "def", "tostring", "(", "self", ")", ":", "\n", "      ", "s", "=", "f'self.etype = {self.etype}\\n'", "\n", "s", "+=", "f'self.arg1type = {self.arg1type}\\n'", "\n", "s", "+=", "f'self.arg1id = {self.arg1id}\\n'", "\n", "s", "+=", "f'self.arg2type = {self.arg2type}\\n'", "\n", "s", "+=", "f'self.arg2id = {self.arg2id}\\n'", "\n", "s", "+=", "f'self.edgelinenum = {self.edgelinenum}\\n'", "\n", "s", "+=", "f'self.parsepiece = {self.parsepiece}\\n'", "\n", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParsePiece.__init__": [[225, 233], ["None"], "methods", ["None"], ["   ", "def", "__init__", "(", "self", ",", "head", ",", "ptype", ",", "tail", ",", "tailparent", ",", "tailparentpath", ")", ":", "\n", "      ", "self", ".", "head", "=", "head", "\n", "self", ".", "ptype", "=", "ptype", "\n", "self", ".", "tail", "=", "tail", "\n", "self", ".", "tailparent", "=", "tailparent", "\n", "self", ".", "tailparentpath", "=", "tailparentpath", "\n", "self", ".", "nestart", "=", "-", "1", "\n", "self", ".", "neend", "=", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParsePiece.setnespan": [[234, 237], ["None"], "methods", ["None"], ["", "def", "setnespan", "(", "self", ",", "nestart", ",", "neend", ")", ":", "\n", "      ", "self", ".", "nestart", "=", "nestart", "\n", "self", ".", "neend", "=", "neend", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParsePiece.tostring": [[238, 246], ["None"], "methods", ["None"], ["", "def", "tostring", "(", "self", ")", ":", "\n", "      ", "s", "=", "f\"self.head = '{self.head}'\\n\"", "\n", "s", "+=", "f\"self.ptype = '{self.ptype}'\\n\"", "\n", "s", "+=", "f\"self.tail = '{self.tail}'\\n\"", "\n", "s", "+=", "f\"self.tailparent = '{self.tailparent}'\\n\"", "\n", "s", "+=", "f\"self.tailparentpath = '{self.tailparentpath}'\\n\"", "\n", "s", "+=", "f\"self.nespan = '{self.nestart}-{self.neend}'\\n\"", "\n", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.Stack.__init__": [[250, 252], ["None"], "methods", ["None"], ["   ", "def", "__init__", "(", "self", ")", ":", "\n", "      ", "self", ".", "stack", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.Stack.push": [[253, 255], ["retyper.Stack.stack.append"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "push", "(", "self", ",", "item", ")", ":", "\n", "      ", "self", ".", "stack", ".", "append", "(", "item", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.Stack.pop": [[256, 263], ["None"], "methods", ["None"], ["", "def", "pop", "(", "self", ")", ":", "\n", "      ", "if", "not", "self", ".", "stack", ":", "\n", "         ", "return", "None", "\n", "", "else", ":", "\n", "         ", "topval", "=", "self", ".", "stack", "[", "-", "1", "]", "\n", "self", ".", "stack", "=", "self", ".", "stack", "[", ":", "-", "1", "]", "\n", "return", "topval", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.Stack.top": [[264, 269], ["None"], "methods", ["None"], ["", "", "def", "top", "(", "self", ")", ":", "\n", "      ", "if", "not", "self", ".", "stack", ":", "\n", "         ", "return", "None", "\n", "", "else", ":", "\n", "         ", "return", "self", ".", "stack", "[", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.Stack.dump": [[270, 272], ["ANCESTORJOINER.join", "str"], "methods", ["None"], ["", "", "def", "dump", "(", "self", ")", ":", "\n", "      ", "return", "ANCESTORJOINER", ".", "join", "(", "[", "str", "(", "i", ")", "for", "i", "in", "self", ".", "stack", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.printerr": [[41, 45], ["print", "sys.stderr.flush"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["def", "printerr", "(", "s", ",", "**", "kwargs", ")", ":", "\n", "   ", "print", "(", "s", ",", "file", "=", "sys", ".", "stderr", ",", "**", "kwargs", ")", "\n", "sys", ".", "stderr", ".", "flush", "(", ")", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.parseclargs": [[49, 109], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "retyper.printerr", "sys.exit", "retyper.printerr", "sys.exit", "os.path.exists", "retyper.printerr", "sys.exit", "os.path.exists", "retyper.printerr", "sys.exit", "os.path.exists", "retyper.printerr", "sys.exit", "os.path.exists", "retyper.printerr", "sys.exit", "os.path.exists", "retyper.printerr", "sys.exit", "os.path.exists", "retyper.printerr", "sys.exit", "os.path.exists", "retyper.printerr", "sys.exit"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.printerr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.printerr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.printerr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.printerr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.printerr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.printerr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.printerr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.printerr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.printerr"], ["", "def", "parseclargs", "(", ")", ":", "\n", "   ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "'retyper'", ")", "\n", "parser", ".", "add_argument", "(", "'-i'", ",", "'--inputfile'", ",", "action", "=", "'store'", ",", "required", "=", "True", ",", "help", "=", "'path to the AMR parse file to retype'", ")", "\n", "parser", ".", "add_argument", "(", "'-m'", ",", "'--modeldirectory'", ",", "action", "=", "'store'", ",", "help", "=", "'path to the directory with the BERT model files'", ")", "\n", "parser", ".", "add_argument", "(", "'-l'", ",", "'--labelsfile'", ",", "action", "=", "'store'", ",", "help", "=", "'path to the labels.txt file for the BERT model'", ")", "\n", "parser", ".", "add_argument", "(", "'-o'", ",", "'--outputfile'", ",", "action", "=", "'store'", ",", "help", "=", "'name of the output file for retyped AMR parses'", ")", "\n", "parser", ".", "add_argument", "(", "'-b'", ",", "'--biooutputfile'", ",", "action", "=", "'store'", ",", "help", "=", "'name of the output file for IOB-formatted sentences from the input AMRs'", ")", "\n", "parser", ".", "add_argument", "(", "'-j'", ",", "'--jsonoutputfile'", ",", "action", "=", "'store'", ",", "help", "=", "'name of the output file for json-formatted mention structures'", ")", "\n", "parser", ".", "add_argument", "(", "'-t'", ",", "'--scorethreshold'", ",", "action", "=", "'store'", ",", "type", "=", "float", ",", "help", "=", "'retyper score threshold (decimal number between 0.0 and 1.0); retyper will ignore retypes if their model score is below threshold)'", ")", "\n", "parser", ".", "add_argument", "(", "'-d'", ",", "'--debug'", ",", "action", "=", "'store_true'", ",", "help", "=", "'turn on verbose debug output'", ")", "\n", "parser", ".", "add_argument", "(", "'-n'", ",", "'--noclobber'", ",", "action", "=", "'store_true'", ",", "help", "=", "'do not overwrite output files if they already exist'", ")", "\n", "parser", ".", "add_argument", "(", "'-x'", ",", "'--explorethreshold'", ",", "action", "=", "'store_true'", ",", "help", "=", "'iterate the retyper over a (fixed) list of threshold scores'", ")", "\n", "parser", ".", "add_argument", "(", "'--skipretyper'", ",", "action", "=", "'store_true'", ",", "help", "=", "\"don't load the retyper model; find NE spans, generate IOB, and reconstruct the AMR parse, but don't retype\"", ")", "\n", "parser", ".", "add_argument", "(", "'--padnodetypes'", ",", "action", "=", "'store_true'", ",", "help", "=", "\"add space after the variable type operator ('/'); if not specified, retyper will determine dynamically whether padding is needed on AMR generation to match input AMRs\"", ")", "\n", "parser", ".", "add_argument", "(", "'--retypeamrtypesonly'", ",", "action", "=", "'store_true'", ",", "help", "=", "\"don't retype if the original type is not among the fixed list of AMR NE types\"", ")", "\n", "parser", ".", "add_argument", "(", "'--withdictionary'", ",", "action", "=", "'store_true'", ",", "help", "=", "'use the dictionary-extended BERT architecture (required if the model was trained with dictionary features)'", ")", "\n", "parser", ".", "add_argument", "(", "'--outputscores'", ",", "action", "=", "'store_true'", ",", "help", "=", "'add model scores to the output AMRs (will show as comments on retyped nodes)'", ")", "\n", "parser", ".", "add_argument", "(", "'--wikify'", ",", "action", "=", "'store_true'", ",", "help", "=", "'use BLINK to add :wiki links to the AMRs'", ")", "\n", "parser", ".", "add_argument", "(", "'--blinkmodels'", ",", "action", "=", "'store'", ",", "help", "=", "'path to the BLINK model directory; if not specified, will attempt to link from cache only'", ")", "\n", "parser", ".", "add_argument", "(", "'--blinkcachepath'", ",", "action", "=", "'store'", ",", "help", "=", "'path to the BLINK cache directory'", ")", "\n", "parser", ".", "add_argument", "(", "'--blinkcrossencoder'", ",", "action", "=", "'store_true'", ",", "help", "=", "'use BLINK cross-encoder reranking'", ")", "\n", "parser", ".", "add_argument", "(", "'--blinkthreshold'", ",", "action", "=", "'store'", ",", "type", "=", "float", ",", "help", "=", "'BLINK score threshold for accepting/rejecting links'", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "if", "not", "args", ".", "skipretyper", ":", "\n", "      ", "if", "not", "args", ".", "modeldirectory", ":", "\n", "         ", "printerr", "(", "'must specify a retyper model directory'", ")", "\n", "sys", ".", "exit", "(", "1", ")", "\n", "", "if", "not", "args", ".", "labelsfile", ":", "\n", "         ", "printerr", "(", "'must specify a retyper model labels file'", ")", "\n", "sys", ".", "exit", "(", "1", ")", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "args", ".", "modeldirectory", ")", ":", "\n", "         ", "printerr", "(", "f'model directory {args.modeldirectory} not found'", ")", "\n", "sys", ".", "exit", "(", "2", ")", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "args", ".", "labelsfile", ")", ":", "\n", "         ", "printerr", "(", "f'labels file {args.labelsfile} not found'", ")", "\n", "sys", ".", "exit", "(", "2", ")", "\n", "\n", "", "", "if", "args", ".", "blinkmodels", ":", "\n", "      ", "if", "not", "os", ".", "path", ".", "exists", "(", "args", ".", "blinkmodels", ")", ":", "\n", "         ", "printerr", "(", "f'BLINK model directory {args.modeldirectory} not found'", ")", "\n", "sys", ".", "exit", "(", "2", ")", "\n", "", "", "if", "args", ".", "blinkcachepath", ":", "\n", "      ", "if", "not", "os", ".", "path", ".", "exists", "(", "args", ".", "blinkcachepath", ")", ":", "\n", "         ", "printerr", "(", "f'BLINK cache directory {args.blinkcachepath} not found'", ")", "\n", "sys", ".", "exit", "(", "3", ")", "\n", "", "", "if", "args", ".", "inputfile", ":", "\n", "      ", "if", "not", "os", ".", "path", ".", "exists", "(", "args", ".", "inputfile", ")", ":", "\n", "         ", "printerr", "(", "f'input file {args.inputfile} not found'", ")", "\n", "sys", ".", "exit", "(", "4", ")", "\n", "", "", "if", "args", ".", "outputfile", ":", "\n", "      ", "if", "args", ".", "noclobber", "and", "os", ".", "path", ".", "exists", "(", "args", ".", "outputfile", ")", ":", "\n", "         ", "printerr", "(", "f'output file {args.outputfile} already exists'", ")", "\n", "sys", ".", "exit", "(", "5", ")", "\n", "", "", "if", "args", ".", "biooutputfile", ":", "\n", "      ", "if", "args", ".", "noclobber", "and", "os", ".", "path", ".", "exists", "(", "args", ".", "biooutputfile", ")", ":", "\n", "         ", "printerr", "(", "f'bio output file {args.biooutputfile} already exists'", ")", "\n", "sys", ".", "exit", "(", "6", ")", "\n", "\n", "", "", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.printoutput": [[112, 117], ["print", "print"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["", "def", "printoutput", "(", "string", ",", "outputfile", ",", "**", "kwargs", ")", ":", "\n", "   ", "if", "outputfile", ":", "\n", "      ", "print", "(", "string", ",", "file", "=", "outputfile", ",", "**", "kwargs", ")", "\n", "", "else", ":", "\n", "      ", "print", "(", "string", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.addnode": [[275, 303], ["line.strip().split", "fields[].strip", "nodetype.startswith", "nspan.split", "retyper.Node", "len", "len", "spanfields[].strip", "spanfields[].strip", "line.strip"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["", "", "def", "addnode", "(", "nodelinenum", ",", "line", ")", ":", "\n", "   ", "NUMFIELDS", "=", "4", "\n", "TYPEFIELD", "=", "0", "\n", "IDFIELD", "=", "1", "\n", "TYPEFIELD", "=", "2", "\n", "SPANFIELD", "=", "3", "\n", "\n", "fields", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "if", "len", "(", "fields", ")", "!=", "NUMFIELDS", ":", "\n", "      ", "return", "None", "\n", "\n", "", "aid", "=", "fields", "[", "IDFIELD", "]", ".", "strip", "(", ")", "\n", "\n", "nodetype", "=", "fields", "[", "TYPEFIELD", "]", "\n", "if", "nodetype", ".", "startswith", "(", "'\"'", ")", ":", "# ignore nodes that don't refer to types", "\n", "      ", "return", "None", "# (such as the string ops of names)", "\n", "\n", "", "start", "=", "0", "\n", "end", "=", "0", "\n", "nspan", "=", "fields", "[", "SPANFIELD", "]", "\n", "spanfields", "=", "nspan", ".", "split", "(", "'-'", ")", "\n", "if", "len", "(", "spanfields", ")", "!=", "2", ":", "\n", "      ", "return", "None", "\n", "", "else", ":", "\n", "      ", "start", "=", "spanfields", "[", "0", "]", ".", "strip", "(", ")", "\n", "end", "=", "spanfields", "[", "1", "]", ".", "strip", "(", ")", "\n", "\n", "", "return", "Node", "(", "aid", ",", "nodetype", ",", "start", ",", "end", ",", "nodelinenum", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.addedge": [[304, 324], ["line.strip().split", "fields[].strip", "fields[].strip", "fields[].strip", "fields[].strip", "fields[].strip", "retyper.Edge", "len", "line.strip"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["", "def", "addedge", "(", "edgelinenum", ",", "line", ")", ":", "\n", "   ", "NUMFIELDS", "=", "6", "\n", "EDGEFIELD", "=", "0", "\n", "TYPEFIELD", "=", "1", "\n", "ARG1TYPEFIELD", "=", "2", "\n", "ARG2TYPEFIELD", "=", "3", "\n", "ARG1IDFIELD", "=", "4", "\n", "ARG2IDFIELD", "=", "5", "\n", "\n", "fields", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "if", "len", "(", "fields", ")", "!=", "NUMFIELDS", ":", "\n", "      ", "return", "None", "\n", "\n", "", "edgetype", "=", "fields", "[", "TYPEFIELD", "]", ".", "strip", "(", ")", "\n", "arg1type", "=", "fields", "[", "ARG1TYPEFIELD", "]", ".", "strip", "(", ")", "\n", "arg2type", "=", "fields", "[", "ARG2TYPEFIELD", "]", ".", "strip", "(", ")", "\n", "arg1id", "=", "fields", "[", "ARG1IDFIELD", "]", ".", "strip", "(", ")", "\n", "arg2id", "=", "fields", "[", "ARG2IDFIELD", "]", ".", "strip", "(", ")", "\n", "\n", "return", "Edge", "(", "edgetype", ",", "arg1type", ",", "arg1id", ",", "arg2type", ",", "arg2id", ",", "edgelinenum", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.protectedcount": [[325, 342], ["None"], "function", ["None"], ["", "def", "protectedcount", "(", "s", ",", "c", ")", ":", "\n", "   ", "DOUBLEQUOTE", "=", "'\"'", "\n", "\n", "if", "not", "c", "in", "s", ":", "\n", "      ", "return", "0", "\n", "\n", "", "count", "=", "0", "\n", "isinsidequote", "=", "False", "\n", "\n", "for", "ch", "in", "s", ":", "\n", "      ", "if", "ch", "==", "DOUBLEQUOTE", ":", "\n", "         ", "isinsidequote", "=", "not", "isinsidequote", "\n", "", "if", "ch", "==", "c", ":", "\n", "         ", "if", "not", "isinsidequote", ":", "\n", "            ", "count", "+=", "1", "\n", "\n", "", "", "", "return", "count", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.protectedsplit": [[345, 377], ["list", "list.append", "string.split", "list.append"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "protectedsplit", "(", "string", ",", "splitter", ")", ":", "\n", "# don't split string on splitter if splitter is inside a double-quoted string", "\n", "   ", "DOUBLEQUOTE", "=", "'\"'", "\n", "\n", "if", "not", "splitter", "in", "string", ":", "\n", "      ", "return", "[", "string", "]", "\n", "\n", "", "if", "splitter", "==", "DOUBLEQUOTE", ":", "\n", "      ", "return", "string", ".", "split", "(", "splitter", ")", "\n", "\n", "", "strlist", "=", "list", "(", ")", "\n", "currstr", "=", "''", "\n", "insidequote", "=", "False", "\n", "for", "ch", "in", "string", ":", "\n", "      ", "if", "ch", "==", "DOUBLEQUOTE", ":", "\n", "         ", "if", "insidequote", ":", "\n", "            ", "insidequote", "=", "False", "\n", "", "else", ":", "\n", "            ", "insidequote", "=", "True", "\n", "\n", "", "", "if", "ch", "==", "splitter", ":", "\n", "         ", "if", "insidequote", ":", "\n", "            ", "currstr", "+=", "ch", "\n", "", "else", ":", "\n", "            ", "strlist", ".", "append", "(", "currstr", ")", "\n", "currstr", "=", "''", "\n", "", "", "else", ":", "\n", "         ", "currstr", "+=", "ch", "\n", "\n", "", "", "strlist", ".", "append", "(", "currstr", ")", "\n", "\n", "return", "strlist", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.getparsepieces": [[380, 425], ["dict", "params.getparam", "retyper.Stack", "retyper.protectedsplit", "retyper.printerr", "retyper.printerr", "retyper.protectedsplit", "params.getparam", "retyper.printerr", "len", "range", "retyper.Stack.push", "tmppt[].split", "params.getparam", "tmppt[].split", "params.setparam", "pptype.join", "range", "retyper.Stack.top", "retyper.Stack.dump", "retyper.ParsePiece", "retyper.protectedcount", "retyper.Stack.pop", "retyper.printerr", "retyper.protectedcount", "retyper.Stack.pop"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParamStore.getparam", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.protectedsplit", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.printerr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.printerr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.protectedsplit", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParamStore.getparam", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.printerr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.push", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParamStore.getparam", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParamStore.setparam", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.Stack.top", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.Stack.dump", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.protectedcount", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.printerr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.protectedcount", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop"], ["", "def", "getparsepieces", "(", "amrparse", ",", "params", ")", ":", "\n", "   ", "parsepieces", "=", "dict", "(", ")", "\n", "tmppp", "=", "protectedsplit", "(", "amrparse", ",", "AMRNODESTART", ")", "[", "1", ":", "]", "# starts with '(', so [0] will be ''", "\n", "\n", "if", "params", ".", "getparam", "(", "'debug'", ")", ":", "\n", "      ", "printerr", "(", "amrparse", ")", "\n", "printerr", "(", "tmppp", ")", "\n", "\n", "", "parents", "=", "Stack", "(", ")", "\n", "\n", "j", "=", "0", "\n", "prevtail", "=", "''", "\n", "for", "n", "in", "tmppp", ":", "\n", "      ", "tmppt", "=", "protectedsplit", "(", "n", ",", "AMRTYPESPLITTER", ")", "\n", "\n", "if", "params", ".", "getparam", "(", "'debug'", ")", ":", "\n", "         ", "printerr", "(", "tmppt", ")", "\n", "\n", "", "if", "len", "(", "tmppt", ")", ">", "1", ":", "\n", "         ", "for", "_", "in", "range", "(", "protectedcount", "(", "prevtail", ",", "AMRNODEEND", ")", ")", ":", "\n", "            ", "ptailparent", "=", "parents", ".", "pop", "(", ")", "\n", "\n", "", "phead", "=", "tmppt", "[", "0", "]", "\n", "parents", ".", "push", "(", "phead", ")", "\n", "tmptail", "=", "tmppt", "[", "1", "]", ".", "split", "(", ")", "\n", "\n", "if", "params", ".", "getparam", "(", "'debug'", ")", ":", "\n", "            ", "printerr", "(", "tmptail", ")", "\n", "\n", "", "pptype", "=", "tmptail", "[", "0", "]", "\n", "tmptail", "=", "tmppt", "[", "1", "]", ".", "split", "(", "pptype", ")", "\n", "params", ".", "setparam", "(", "'typepad'", ",", "tmptail", "[", "0", "]", ")", "\n", "pptail", "=", "pptype", ".", "join", "(", "tmptail", "[", "1", ":", "]", ")", "\n", "\n", "for", "_", "in", "range", "(", "protectedcount", "(", "pptype", ",", "AMRNODEEND", ")", ")", ":", "\n", "            ", "ptailparent", "=", "parents", ".", "pop", "(", ")", "\n", "\n", "", "ptailparent", "=", "parents", ".", "top", "(", ")", "\n", "ptailparentpath", "=", "parents", ".", "dump", "(", ")", "\n", "\n", "parsepieces", "[", "j", "]", "=", "ParsePiece", "(", "head", "=", "phead", ",", "ptype", "=", "pptype", ",", "tail", "=", "pptail", ",", "tailparent", "=", "ptailparent", ",", "tailparentpath", "=", "ptailparentpath", ")", "\n", "j", "+=", "1", "\n", "prevtail", "=", "pptail", "\n", "\n", "", "", "return", "parsepieces", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.getparentfrompath": [[428, 437], ["pathstr.split", "len"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["", "def", "getparentfrompath", "(", "pathstr", ")", ":", "\n", "   ", "if", "not", "pathstr", ":", "\n", "      ", "return", "None", "\n", "\n", "", "ancestors", "=", "pathstr", ".", "split", "(", "ANCESTORJOINER", ")", "\n", "if", "len", "(", "ancestors", ")", "<", "2", ":", "\n", "      ", "return", "None", "\n", "", "else", ":", "\n", "      ", "return", "ancestors", "[", "-", "2", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.getwikiid": [[438, 455], ["pstr.split", "parts[].strip", "parts[].strip.split", "parts[].strip", "len", "parts[].strip.startswith", "len"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["", "", "def", "getwikiid", "(", "pstr", ")", ":", "\n", "   ", "parts", "=", "pstr", ".", "split", "(", "WIKIOP", ")", "\n", "if", "len", "(", "parts", ")", "<=", "1", ":", "\n", "      ", "return", "None", "\n", "\n", "", "tstr", "=", "parts", "[", "1", "]", ".", "strip", "(", ")", "\n", "if", "not", "tstr", ".", "startswith", "(", "'\"'", ")", ":", "\n", "      ", "return", "None", "\n", "\n", "", "parts", "=", "tstr", ".", "split", "(", "'\"'", ")", "\n", "\n", "if", "len", "(", "parts", ")", "<=", "1", ":", "\n", "      ", "return", "None", "\n", "\n", "", "wid", "=", "parts", "[", "1", "]", ".", "strip", "(", ")", "\n", "\n", "return", "wid", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.getnespan": [[456, 522], ["list", "ppstr.split", "dict", "list", "nodelist.items", "retyper.isop", "list", "retyper.isop", "list.append", "pstr.split", "list.append", "nodeids[].append", "len", "len"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.isop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.isop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "getnespan", "(", "ppstr", ",", "edgelist", ",", "nodelist", ")", ":", "\n", "   ", "if", "NESTRINGOP", "not", "in", "ppstr", ":", "\n", "      ", "return", "None", "\n", "\n", "# really long mention spans cause BLINK to crash", "\n", "# (there's a limit, but it's buried in the config", "\n", "# file of the BLINK model, and BLINK just allows", "\n", "# a fatal crash if the limit is exceeded)", "\n", "", "if", "NESTRINGOP", "+", "'10'", "in", "ppstr", ":", "\n", "      ", "return", "None", "\n", "\n", "", "tokstrs", "=", "list", "(", ")", "\n", "pstrs", "=", "ppstr", ".", "split", "(", ")", "\n", "strisop", "=", "False", "\n", "for", "pstr", "in", "pstrs", ":", "\n", "      ", "if", "isop", "(", "pstr", ")", ":", "\n", "         ", "strisop", "=", "True", "\n", "continue", "\n", "", "if", "strisop", ":", "\n", "         ", "if", "pstr", "[", "0", "]", "==", "'\"'", ":", "\n", "            ", "tmpstrs", "=", "pstr", ".", "split", "(", "'\"'", ")", "\n", "tokstr", "=", "'\"'", "+", "tmpstrs", "[", "1", "]", "+", "'\"'", "\n", "tokstrs", ".", "append", "(", "tokstr", ")", "\n", "strisop", "=", "False", "\n", "\n", "", "", "", "if", "not", "tokstrs", ":", "\n", "      ", "return", "None", "\n", "\n", "", "nodeids", "=", "dict", "(", ")", "\n", "for", "tokstr", "in", "tokstrs", ":", "\n", "      ", "nodeids", "[", "tokstr", "]", "=", "list", "(", ")", "\n", "\n", "# don't want to use sets because I want to maintain the order", "\n", "", "for", "edge", "in", "edgelist", ":", "\n", "      ", "if", "isop", "(", "edge", ".", "arg1type", ")", "and", "edge", ".", "arg2type", "in", "tokstrs", ":", "\n", "         ", "if", "edge", ".", "arg1id", "not", "in", "nodeids", "[", "edge", ".", "arg2type", "]", ":", "\n", "            ", "nodeids", "[", "edge", ".", "arg2type", "]", ".", "append", "(", "edge", ".", "arg1id", ")", "\n", "\n", "# find a node id in the first tok's node ids ", "\n", "# that's among the node ids for all the other tokstrs", "\n", "", "", "", "candidatenodes", "=", "list", "(", ")", "\n", "for", "t1node", "in", "nodeids", "[", "tokstrs", "[", "0", "]", "]", ":", "\n", "      ", "t1nodeok", "=", "True", "\n", "for", "tok", "in", "tokstrs", "[", "1", ":", "]", ":", "\n", "         ", "if", "t1node", "not", "in", "nodeids", "[", "tok", "]", ":", "\n", "            ", "t1nodeok", "=", "False", "\n", "break", "\n", "\n", "", "", "if", "t1nodeok", ":", "\n", "         ", "candidatenodes", ".", "append", "(", "t1node", ")", "\n", "\n", "", "", "if", "not", "candidatenodes", ":", "\n", "      ", "return", "None", "\n", "\n", "", "spannode", "=", "None", "\n", "for", "nid", ",", "node", "in", "nodelist", ".", "items", "(", ")", ":", "\n", "      ", "if", "node", ".", "aid", "in", "candidatenodes", "and", "node", ".", "parsepiece", "==", "-", "1", ":", "\n", "         ", "if", "len", "(", "candidatenodes", ")", ">", "1", ":", "# if more than one node has the same string", "\n", "            ", "if", "(", "node", ".", "end", "-", "node", ".", "start", ")", "==", "len", "(", "tokstrs", ")", ":", "# pick the one with the span with the \"right\" length", "\n", "               ", "spannode", "=", "nid", "# (will not work when the number of tokens != number of :ops)", "\n", "break", "\n", "", "", "else", ":", "\n", "            ", "spannode", "=", "nid", "\n", "break", "\n", "\n", "", "", "", "return", "spannode", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.isop": [[523, 543], ["opstr.startswith", "opstr.split", "ch.isdigit"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["", "def", "isop", "(", "opstr", ")", ":", "\n", "   ", "if", "not", "opstr", ":", "\n", "      ", "return", "False", "\n", "\n", "# the first char of NESTRINGOP (':') may be stripped (e.g., in an edge arg)", "\n", "", "if", "opstr", "[", "0", "]", "!=", "NESTRINGOP", "[", "0", "]", ":", "\n", "      ", "opstr", "=", "NESTRINGOP", "[", "0", "]", "+", "opstr", "\n", "\n", "", "if", "not", "opstr", ".", "startswith", "(", "NESTRINGOP", ")", ":", "\n", "      ", "return", "False", "\n", "\n", "", "intpart", "=", "opstr", ".", "split", "(", "NESTRINGOP", ",", "1", ")", "[", "1", "]", "\n", "if", "not", "intpart", ":", "\n", "      ", "return", "False", "\n", "\n", "", "for", "ch", "in", "intpart", ":", "\n", "      ", "if", "not", "ch", ".", "isdigit", "(", ")", ":", "\n", "         ", "return", "False", "\n", "\n", "", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.expandiob": [[544, 562], ["list", "tag.startswith", "list.append", "list.append", "tag.startswith", "list.append", "list.append"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "expandiob", "(", "ioblist", ")", ":", "\n", "   ", "iobexp", "=", "list", "(", ")", "\n", "insidemention", "=", "False", "\n", "for", "tok", ",", "tag", "in", "ioblist", ":", "\n", "      ", "if", "tag", ".", "startswith", "(", "BTAG", ")", ":", "# starting a new mention", "\n", "         ", "if", "insidemention", ":", "# but already in one, so close it", "\n", "            ", "iobexp", ".", "append", "(", "(", "ENDMENTION", ",", "OTAG", ")", ")", "\n", "", "else", ":", "\n", "            ", "insidemention", "=", "True", "\n", "", "iobexp", ".", "append", "(", "(", "STARTMENTION", ",", "OTAG", ")", ")", "\n", "", "elif", "tag", ".", "startswith", "(", "OTAG", ")", ":", "\n", "         ", "if", "insidemention", ":", "\n", "            ", "iobexp", ".", "append", "(", "(", "ENDMENTION", ",", "OTAG", ")", ")", "\n", "insidemention", "=", "False", "\n", "\n", "", "", "iobexp", ".", "append", "(", "(", "tok", ",", "tag", ")", ")", "\n", "\n", "", "return", "iobexp", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.stripiob": [[565, 568], ["tok.startswith"], "function", ["None"], ["", "def", "stripiob", "(", "ioblist", ")", ":", "\n", "   ", "STRIPTAGPREFIX", "=", "'[unused'", "\n", "return", "[", "(", "tok", ",", "tag", ")", "for", "(", "tok", ",", "tag", ")", "in", "ioblist", "if", "not", "tok", ".", "startswith", "(", "STRIPTAGPREFIX", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.retypeiob": [[571, 580], ["retyper.classify_input"], "function", ["None"], ["", "def", "retypeiob", "(", "ioblist", ",", "retyper", ",", "params", ")", ":", "\n", "\n", "   ", "retypediob", "=", "ioblist", "\n", "\n", "if", "retyper", ":", "\n", "      ", "tokens_only", "=", "[", "tok", "for", "(", "tok", ",", "_", ")", "in", "ioblist", "]", "\n", "retypediob", "=", "retyper", ".", "classify_input", "(", "tokens_only", ",", "return_probabilities", "=", "True", ")", "\n", "\n", "", "return", "retypediob", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.dumpbiofile": [[583, 593], ["print", "isinstance", "print", "print"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["", "def", "dumpbiofile", "(", "ioblist", ",", "biofile", ")", ":", "\n", "   ", "if", "biofile", ":", "\n", "      ", "for", "tok", ",", "iob", "in", "ioblist", ":", "\n", "         ", "if", "isinstance", "(", "iob", ",", "tuple", ")", ":", "\n", "            ", "iobtag", ",", "score", "=", "iob", "\n", "print", "(", "f'{tok} {iobtag} ## {score}'", ",", "file", "=", "biofile", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "f'{tok} {iob}'", ",", "file", "=", "biofile", ")", "\n", "\n", "", "", "", "print", "(", "''", ",", "file", "=", "biofile", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.dumpjsonfile": [[596, 657], ["list", "list", "enumerate", "isinstance", "list.append", "iobtag.startswith", "retyper.dumpjsonmention", "iobtag.startswith", "retyper.dumpjsonmention", "list.append", "list", "retyper.dumpjsonmention", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.dumpjsonmention", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.dumpjsonmention", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.dumpjsonmention"], ["", "def", "dumpjsonfile", "(", "strippedioblist", ",", "blinker", ",", "jsonfile", ",", "jcount", ",", "threshold", "=", "MAXNEGFLOAT", ")", ":", "\n", "   ", "nummentions", "=", "0", "\n", "mention", "=", "list", "(", ")", "\n", "linkedioblist", "=", "list", "(", ")", "\n", "tag", "=", "''", "\n", "toklist", "=", "[", "tok", "for", "tok", ",", "_", "in", "strippedioblist", "]", "\n", "bidx", "=", "0", "\n", "prevbidx", "=", "0", "\n", "biobtag", "=", "''", "\n", "bscore", "=", "None", "\n", "prevbiobtag", "=", "''", "\n", "btok", "=", "''", "\n", "for", "i", ",", "(", "tok", ",", "iob", ")", "in", "enumerate", "(", "strippedioblist", ")", ":", "\n", "      ", "score", "=", "None", "\n", "if", "isinstance", "(", "iob", ",", "tuple", ")", ":", "\n", "         ", "iobtag", ",", "score", "=", "iob", "\n", "", "else", ":", "\n", "         ", "iobtag", "=", "iob", "\n", "", "linkedioblist", ".", "append", "(", "(", "tok", ",", "iob", ")", ")", "\n", "elink", "=", "''", "\n", "if", "iobtag", ".", "startswith", "(", "BTAG", ")", ":", "\n", "         ", "if", "mention", ":", "\n", "            ", "elink", "=", "dumpjsonmention", "(", "toklist", "[", ":", "(", "i", "-", "len", "(", "mention", ")", ")", "]", ",", "mention", ",", "tag", ",", "toklist", "[", "i", ":", "]", ",", "jcount", ",", "nummentions", ",", "blinker", ",", "jsonfile", ",", "threshold", ")", "\n", "nummentions", "+=", "1", "\n", "if", "elink", ":", "\n", "               ", "newtag", "=", "biobtag", "\n", "if", "bscore", ":", "\n", "                  ", "newtag", "=", "(", "biobtag", ",", "bscore", ")", "\n", "", "linkedioblist", "[", "bidx", "]", "=", "(", "btok", ",", "newtag", "+", "' :wiki \"'", "+", "elink", "+", "'\"'", ")", "\n", "", "", "bidx", "=", "i", "\n", "biobtag", "=", "iobtag", "\n", "bscore", "=", "score", "\n", "btok", "=", "tok", "\n", "mention", "=", "[", "tok", "]", "\n", "tag", "=", "iobtag", "\n", "", "elif", "iobtag", ".", "startswith", "(", "ITAG", ")", ":", "\n", "         ", "mention", ".", "append", "(", "tok", ")", "\n", "if", "not", "tag", ":", "\n", "            ", "tag", "=", "iobtag", "\n", "", "", "else", ":", "\n", "         ", "if", "mention", ":", "\n", "            ", "elink", "=", "dumpjsonmention", "(", "toklist", "[", ":", "(", "i", "-", "len", "(", "mention", ")", ")", "]", ",", "mention", ",", "tag", ",", "toklist", "[", "i", ":", "]", ",", "jcount", ",", "nummentions", ",", "blinker", ",", "jsonfile", ",", "threshold", ")", "\n", "nummentions", "+=", "1", "\n", "if", "elink", ":", "\n", "               ", "newtag", "=", "biobtag", "\n", "if", "bscore", ":", "\n", "                  ", "newtag", "=", "(", "biobtag", ",", "bscore", ")", "\n", "", "linkedioblist", "[", "bidx", "]", "=", "(", "btok", ",", "newtag", "+", "' :wiki \"'", "+", "elink", "+", "'\"'", ")", "\n", "", "", "mention", "=", "list", "(", ")", "\n", "tag", "=", "''", "\n", "\n", "", "", "if", "mention", ":", "\n", "      ", "elink", "=", "dumpjsonmention", "(", "toklist", "[", ":", "(", "i", "-", "len", "(", "mention", ")", ")", "]", ",", "mention", ",", "tag", ",", "toklist", "[", "i", ":", "]", ",", "jcount", ",", "nummentions", ",", "blinker", ",", "jsonfile", ",", "threshold", ")", "\n", "if", "elink", ":", "\n", "         ", "newtag", "=", "biobtag", "\n", "if", "bscore", ":", "\n", "            ", "newtag", "=", "(", "biobtag", ",", "bscore", ")", "\n", "", "linkedioblist", "[", "bidx", "]", "=", "(", "btok", ",", "newtag", "+", "' :wiki \"'", "+", "elink", "+", "'\"'", ")", "\n", "", "nummentions", "+=", "1", "\n", "\n", "", "return", "nummentions", ",", "linkedioblist", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.dumpjsonmention": [[660, 718], ["dict", "rcstr.endswith", "tag.split", "list", "len", "blinker.runblink", "len", "[].replace", "blinker.getwpinfofortitle", "json.dumps", "print", "jsonfile.flush", "blinker.runblink", "len", "resurl.split"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.Blinker.Blinker.runblink", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.Blinker.Blinker.getwpinfofortitle", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.Blinker.Blinker.runblink", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["", "def", "dumpjsonmention", "(", "leftcontext", ",", "mention", ",", "tag", ",", "rightcontext", ",", "sentcount", ",", "mentcount", ",", "blinker", ",", "jsonfile", ",", "threshold", "=", "MAXNEGFLOAT", ")", ":", "\n", "   ", "wptitle", "=", "''", "\n", "resurl", "=", "''", "\n", "mstru", "=", "dict", "(", ")", "\n", "mstru", "[", "'id'", "]", "=", "f'{sentcount}.{mentcount}'", "\n", "mstru", "[", "'label'", "]", "=", "'unknown'", "\n", "mstru", "[", "'label_id'", "]", "=", "-", "1", "\n", "mstru", "[", "'context_left'", "]", "=", "' '", ".", "join", "(", "leftcontext", ")", ".", "lower", "(", ")", "\n", "mstru", "[", "'mention'", "]", "=", "' '", ".", "join", "(", "mention", ")", ".", "lower", "(", ")", "\n", "rcstr", "=", "' '", ".", "join", "(", "rightcontext", ")", ".", "lower", "(", ")", "\n", "if", "rcstr", ".", "endswith", "(", "ROOTTOKEN", ")", ":", "\n", "      ", "peel", "=", "0", "-", "len", "(", "ROOTTOKEN", ")", "-", "1", "# extra 1 for the space added by join", "\n", "rcstr", "=", "rcstr", "[", ":", "peel", "]", "\n", "", "mstru", "[", "'context_right'", "]", "=", "rcstr", "\n", "\n", "elink", "=", "''", "\n", "tagelems", "=", "tag", ".", "split", "(", ")", "\n", "if", "len", "(", "tagelems", ")", ">", "1", ":", "\n", "      ", "elink", "=", "tagelems", "[", "1", "]", "\n", "\n", "", "if", "elink", ":", "\n", "      ", "mstru", "[", "'Wikipedia_URL'", "]", "=", "'en.wikipedia.org/wiki/'", "+", "elink", "\n", "\n", "# Uncomment temporarily if you want to force context-free spans into the cache", "\n", "#   mstru['context_left'] = ''", "\n", "#   mstru['context_right'] = ''", "\n", "\n", "", "predictions", "=", "list", "(", ")", "\n", "if", "blinker", ":", "\n", "      ", "predictions", ",", "scores", "=", "blinker", ".", "runblink", "(", "[", "mstru", "]", ")", "\n", "if", "not", "predictions", ":", "# back off to no-context linking (should only happen in cache-only mode)", "\n", "         ", "mstru", "[", "'context_left'", "]", "=", "''", "\n", "mstru", "[", "'context_right'", "]", "=", "''", "\n", "predictions", ",", "scores", "=", "blinker", ".", "runblink", "(", "[", "mstru", "]", ")", "\n", "\n", "", "", "topscore", "=", "threshold", "-", "1.0", "\n", "if", "len", "(", "predictions", ")", ">", "0", ":", "\n", "# BLINK returns wptitles, but amr gold wiki links are stripped uris", "\n", "      ", "wptitle", "=", "predictions", "[", "0", "]", "[", "0", "]", ".", "replace", "(", "' '", ",", "'_'", ")", "\n", "topscore", "=", "scores", "[", "0", "]", "[", "0", "]", "\n", "\n", "wpid", ",", "resurl", "=", "blinker", ".", "getwpinfofortitle", "(", "wptitle", ")", "\n", "mstru", "[", "'Wikipedia_URL'", "]", "=", "resurl", "\n", "mstru", "[", "'label'", "]", "=", "wpid", "\n", "mstru", "[", "'label_id'", "]", "=", "wpid", "\n", "\n", "", "if", "jsonfile", ":", "\n", "      ", "s", "=", "json", ".", "dumps", "(", "mstru", ")", "\n", "print", "(", "s", ",", "file", "=", "jsonfile", ")", "\n", "jsonfile", ".", "flush", "(", ")", "\n", "\n", "", "if", "topscore", "<", "threshold", ":", "\n", "      ", "return", "'-'", "\n", "", "else", ":", "\n", "      ", "if", "resurl", ":", "\n", "         ", "return", "resurl", ".", "split", "(", "'/'", ")", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "         ", "return", "wptitle", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.processparse": [[721, 960], ["retyper.getparsepieces", "params.getparam", "list", "getparsepieces.items", "sorted", "list", "params.getparam", "params.getparam", "params.getparam", "retyper.expandiob", "params.getparam", "retyper.retypeiob", "retyper.stripiob", "retyper.dumpjsonfile", "params.getparam", "list", "range", "list.append", "list", "list", "getparsepieces.items", "retyper.printoutput", "getparsepieces.items", "set", "retyper.printoutput", "retyper.printoutput", "nodelist.items", "len", "len", "list", "list", "nodelist.items", "enumerate", "retyper.printerr", "retyper.printerr", "params.getparam", "retyper.printerr", "retyper.printerr", "retyper.printerr", "retyper.dumpbiofile", "len", "len", "retyper.printerr", "retyper.printerr", "len", "isinstance", "retypedbtag.startswith", "retypedbtag.startswith", "n.setretype", "nodelines[].replace", "params.getparam", "nodelines[].replace", "edge.strip().split", "retyper.printoutput", "list.append", "retyper.printoutput", "ofil.flush", "retyper.printerr", "retyper.getnespan", "nodelist[].setparsepiece", "retyper.getparentfrompath", "getparsepieces.items", "retyper.printerr", "retyper.printerr", "params.getparam", "pl.startswith", "nodelines[].replace", "params.getparam", "n.setscore", "retype.split", "params.getparam", "params.getparam", "pl.startswith", "len", "list.append", "fields[].strip", "fields[].strip", "fields[].strip", "fields[].strip", "fields[].strip", "fields[].strip", "list.append", "retyper.printerr", "parsepieces[].setnespan", "list.append", "nodelist[].setparsepiece", "pl.startswith", "retypedbtag.split", "nodelines[].rstrip", "retyper.printerr", "retyper.printerr", "edge.strip", "list.append", "nodelist.items", "retyper.getwikiid", "nodelist[].setentitylink", "params.getparam", "pl.startswith", "pl.startswith", "pl.startswith", "len", "len", "params.getparam", "params.getparam", "pp.tostring", "node.tostring", "edge.tostring", "nodelist[].setnamenode", "parsepieces[].tostring", "parsepieces[].tostring", "n.retype.split", "n.retype.split"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.getparsepieces", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParamStore.getparam", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParamStore.getparam", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParamStore.getparam", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParamStore.getparam", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.expandiob", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParamStore.getparam", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.retypeiob", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.stripiob", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.dumpjsonfile", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParamStore.getparam", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.printoutput", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.printoutput", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.printoutput", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.printerr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.printerr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParamStore.getparam", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.printerr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.printerr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.printerr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.dumpbiofile", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.printerr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.printerr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.Edge.setretype", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParamStore.getparam", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.printoutput", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.printoutput", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.printerr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.getnespan", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.Edge.setparsepiece", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.getparentfrompath", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.printerr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.printerr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParamStore.getparam", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParamStore.getparam", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.Edge.setscore", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParamStore.getparam", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParamStore.getparam", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.printerr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParsePiece.setnespan", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.Edge.setparsepiece", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.printerr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.printerr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.getwikiid", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.Node.setentitylink", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParamStore.getparam", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParamStore.getparam", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParamStore.getparam", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParsePiece.tostring", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParsePiece.tostring", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParsePiece.tostring", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.Node.setnamenode", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParsePiece.tostring", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParsePiece.tostring", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["", "", "", "def", "processparse", "(", "amrparse", ",", "toks", ",", "nodelist", ",", "edgelist", ",", "preamble", ",", "retyper", ",", "blinker", ",", "ofil", ",", "bfil", ",", "jfil", ",", "jcount", ",", "params", ")", ":", "\n", "   ", "numretypes", "=", "0", "\n", "numretypesdiff", "=", "0", "\n", "\n", "parsepieces", "=", "getparsepieces", "(", "amrparse", ",", "params", ")", "\n", "if", "not", "parsepieces", ":", "\n", "      ", "return", "0", ",", "0", ",", "0", "\n", "\n", "", "if", "params", ".", "getparam", "(", "'debug'", ")", ":", "\n", "      ", "for", "pid", ",", "pp", "in", "parsepieces", ".", "items", "(", ")", ":", "\n", "         ", "printerr", "(", "f'pp[{pid}] =\\n{pp.tostring()}'", ")", "\n", "\n", "", "", "parsepiecesneedingretyping", "=", "list", "(", ")", "\n", "parentidx", "=", "-", "1", "\n", "for", "pid", ",", "pp", "in", "parsepieces", ".", "items", "(", ")", ":", "\n", "      ", "if", "pp", ".", "ptype", "==", "NAMETYPE", "and", "NESTRINGOP", "in", "pp", ".", "tail", ":", "\n", "         ", "namespannode", "=", "getnespan", "(", "pp", ".", "tail", ",", "edgelist", ",", "nodelist", ")", "\n", "if", "not", "namespannode", ":", "\n", "            ", "printerr", "(", "f'ERROR: No node with matching span found for\\n{pp.head}{pp.tail}'", ")", "\n", "continue", "\n", "", "ppnestart", "=", "nodelist", "[", "namespannode", "]", ".", "start", "\n", "ppneend", "=", "nodelist", "[", "namespannode", "]", ".", "end", "\n", "nodelist", "[", "namespannode", "]", ".", "setparsepiece", "(", "pid", ")", "\n", "parent", "=", "getparentfrompath", "(", "pp", ".", "tailparentpath", ")", "\n", "for", "idx", ",", "ppiece", "in", "parsepieces", ".", "items", "(", ")", ":", "\n", "            ", "if", "ppiece", ".", "head", "==", "parent", ":", "\n", "               ", "if", "ppiece", ".", "ptype", "not", "in", "NONNERTYPESWITHNAME", ":", "\n", "                  ", "parentidx", "=", "idx", "\n", "", "break", "\n", "", "", "if", "parentidx", ">=", "0", ":", "\n", "            ", "parsepieces", "[", "parentidx", "]", ".", "setnespan", "(", "ppnestart", ",", "ppneend", ")", "\n", "parsepiecesneedingretyping", ".", "append", "(", "parentidx", ")", "\n", "\n", "# parsepieces are in the order of the edges (which I'm using to order the nodes needing retyping)", "\n", "", "", "", "parsepiecesneedingretyping", "=", "sorted", "(", "set", "(", "parsepiecesneedingretyping", ")", ")", "\n", "\n", "if", "not", "parsepiecesneedingretyping", ":", "\n", "      ", "newpreamble", "=", "''", ".", "join", "(", "preamble", ")", "\n", "printoutput", "(", "newpreamble", ",", "ofil", ",", "end", "=", "''", ")", "\n", "printoutput", "(", "amrparse", ",", "ofil", ")", "\n", "return", "0", ",", "0", ",", "0", "\n", "\n", "", "nodeidsneedingretyping", "=", "[", "(", "edge", ".", "arg1id", ",", "edge", ".", "arg2id", ")", "for", "edge", "in", "edgelist", "if", "edge", ".", "arg1type", "==", "NAMETYPE", "and", "edge", ".", "arg2type", "==", "NAMETYPE", "]", "\n", "\n", "# the list comprehension would put the node indexes in the order of nodelist, ", "\n", "# but I need them to be in the order they appear in the edgelist; so sad", "\n", "#   nodesneedingretyping = [nodeindex for nodeindex, node in nodelist.items() if node.aid in nodeidsneedingretyping]", "\n", "nodesneedingretyping", "=", "list", "(", ")", "\n", "for", "(", "nodeid", ",", "nameid", ")", "in", "nodeidsneedingretyping", ":", "\n", "      ", "for", "nodeindex", ",", "node", "in", "nodelist", ".", "items", "(", ")", ":", "\n", "         ", "if", "node", ".", "aid", "==", "nodeid", ":", "\n", "            ", "if", "node", ".", "ntype", "not", "in", "NONNERTYPESWITHNAME", ":", "\n", "               ", "nodesneedingretyping", ".", "append", "(", "nodeindex", ")", "\n", "for", "nnidx", ",", "nnode", "in", "nodelist", ".", "items", "(", ")", ":", "\n", "                  ", "if", "nnode", ".", "aid", "==", "nameid", ":", "\n", "                     ", "nodelist", "[", "nodeindex", "]", ".", "setnamenode", "(", "nnidx", ")", "\n", "\n", "", "", "", "", "", "", "if", "len", "(", "nodesneedingretyping", ")", "!=", "len", "(", "parsepiecesneedingretyping", ")", ":", "\n", "      ", "nodesneedingretyping", "=", "list", "(", ")", "\n", "parsepiecesneedingretyping", "=", "list", "(", ")", "\n", "\n", "", "for", "nid", "in", "nodesneedingretyping", ":", "\n", "      ", "nnid", "=", "nodelist", "[", "nid", "]", ".", "namenode", "\n", "namestart", "=", "nodelist", "[", "nnid", "]", ".", "start", "\n", "nameend", "=", "nodelist", "[", "nnid", "]", ".", "end", "\n", "for", "ppid", "in", "parsepiecesneedingretyping", ":", "\n", "         ", "if", "parsepieces", "[", "ppid", "]", ".", "nestart", "==", "namestart", "and", "parsepieces", "[", "ppid", "]", ".", "neend", "==", "nameend", ":", "\n", "            ", "nodelist", "[", "nid", "]", ".", "setparsepiece", "(", "ppid", ")", "\n", "ptail", "=", "parsepieces", "[", "ppid", "]", ".", "tail", "\n", "if", "WIKIOP", "in", "ptail", ":", "\n", "               ", "wikiid", "=", "getwikiid", "(", "ptail", ")", "\n", "nodelist", "[", "nid", "]", ".", "setentitylink", "(", "wikiid", ")", "\n", "\n", "", "", "", "", "if", "params", ".", "getparam", "(", "'debug'", ")", ":", "\n", "      ", "for", "nid", ",", "node", "in", "nodelist", ".", "items", "(", ")", ":", "\n", "         ", "printerr", "(", "f'nodelist[{nid}] =\\n{node.tostring()}'", ")", "\n", "", "for", "i", ",", "edge", "in", "enumerate", "(", "edgelist", ")", ":", "\n", "         ", "printerr", "(", "f'edgelist[{i}] = \\n{edge.tostring()}'", ")", "\n", "\n", "", "", "if", "params", ".", "getparam", "(", "'debug'", ")", ":", "\n", "      ", "printerr", "(", "f'parsepiecesneedingretyping = {parsepiecesneedingretyping}'", ")", "\n", "printerr", "(", "f'nodesneedingretyping = {nodesneedingretyping}'", ")", "\n", "\n", "", "iob", "=", "[", "(", "tok", ",", "OTAG", ")", "for", "tok", "in", "toks", "]", "\n", "for", "nid", "in", "nodesneedingretyping", ":", "\n", "      ", "n", "=", "nodelist", "[", "nid", "]", "\n", "if", "params", ".", "getparam", "(", "'retypeamronly'", ")", ":", "\n", "         ", "if", "not", "n", ".", "ntype", "in", "params", ".", "getparam", "(", "'retypetypes'", ")", ":", "\n", "            ", "continue", "\n", "", "", "ntyp", "=", "AMRPREFIX", "+", "n", ".", "ntype", "\n", "nent", "=", "''", "\n", "if", "n", ".", "entitylink", ":", "\n", "         ", "nent", "+=", "'\\t'", "+", "n", ".", "entitylink", "\n", "", "if", "n", ".", "namenode", ":", "\n", "         ", "i", "=", "nodelist", "[", "n", ".", "namenode", "]", ".", "start", "\n", "j", "=", "nodelist", "[", "n", ".", "namenode", "]", ".", "end", "\n", "", "else", ":", "\n", "         ", "i", "=", "n", ".", "start", "\n", "j", "=", "n", ".", "end", "\n", "", "tok", ",", "_", "=", "iob", "[", "i", "]", "\n", "iob", "[", "i", "]", "=", "(", "tok", ",", "BTAG", "+", "ntyp", "+", "nent", ")", "\n", "i", "+=", "1", "\n", "while", "i", "<", "j", ":", "\n", "         ", "tok", ",", "_", "=", "iob", "[", "i", "]", "\n", "iob", "[", "i", "]", "=", "(", "tok", ",", "ITAG", "+", "ntyp", "+", "nent", ")", "\n", "i", "+=", "1", "\n", "\n", "", "", "if", "params", ".", "getparam", "(", "'debug'", ")", ":", "\n", "      ", "printerr", "(", "iob", ")", "\n", "\n", "", "iobexp", "=", "expandiob", "(", "iob", ")", "\n", "\n", "if", "params", ".", "getparam", "(", "'debug'", ")", ":", "\n", "      ", "printerr", "(", "toks", ")", "\n", "printerr", "(", "iobexp", ")", "\n", "\n", "", "retypediob", "=", "retypeiob", "(", "iobexp", ",", "retyper", ",", "params", ")", "\n", "\n", "if", "bfil", ":", "\n", "      ", "dumpbiofile", "(", "retypediob", ",", "bfil", ")", "\n", "\n", "", "jadd", "=", "0", "\n", "linkedioblist", "=", "stripiob", "(", "retypediob", ")", "\n", "jadd", ",", "linkedioblist", "=", "dumpjsonfile", "(", "linkedioblist", ",", "blinker", ",", "jfil", ",", "jcount", ",", "threshold", "=", "params", ".", "getparam", "(", "'blinkthreshold'", ")", ")", "\n", "\n", "strippedretypediob", "=", "linkedioblist", "\n", "\n", "if", "len", "(", "strippedretypediob", ")", "!=", "len", "(", "iob", ")", ":", "\n", "      ", "printerr", "(", "f\"retyped iob list doesn't align\\n{iob}\\n{strippedretypediob}\"", ")", "\n", "\n", "", "if", "params", ".", "getparam", "(", "'debug'", ")", ":", "\n", "      ", "printerr", "(", "strippedretypediob", ")", "\n", "\n", "", "newpreamble", "=", "list", "(", ")", "\n", "\n", "newpreamble", "+=", "[", "pl", "for", "pl", "in", "preamble", "if", "not", "(", "pl", ".", "startswith", "(", "LINETYPEPREFIXES", "[", "'node'", "]", ")", "or", "pl", ".", "startswith", "(", "LINETYPEPREFIXES", "[", "'edge'", "]", ")", "or", "pl", ".", "startswith", "(", "LINETYPEPREFIXES", "[", "'root'", "]", ")", ")", "]", "\n", "nodelines", "=", "[", "pl", "for", "pl", "in", "preamble", "if", "pl", ".", "startswith", "(", "LINETYPEPREFIXES", "[", "'node'", "]", ")", "]", "\n", "rootline", "=", "[", "pl", "for", "pl", "in", "preamble", "if", "pl", ".", "startswith", "(", "LINETYPEPREFIXES", "[", "'root'", "]", ")", "]", "[", "0", "]", "\n", "rootisnodenum", "=", "-", "1", "\n", "for", "i", "in", "range", "(", "len", "(", "nodelines", ")", ")", ":", "\n", "      ", "if", "nodelines", "[", "i", "]", ".", "replace", "(", "LINETYPEPREFIXES", "[", "'node'", "]", ",", "LINETYPEPREFIXES", "[", "'root'", "]", ")", "==", "rootline", ":", "\n", "         ", "rootisnodenum", "=", "i", "\n", "\n", "", "", "for", "nid", "in", "nodesneedingretyping", ":", "\n", "      ", "n", "=", "nodelist", "[", "nid", "]", "\n", "origtype", "=", "n", ".", "ntype", "\n", "start", "=", "0", "\n", "if", "n", ".", "namenode", ":", "\n", "         ", "start", "=", "nodelist", "[", "n", ".", "namenode", "]", ".", "start", "\n", "", "else", ":", "\n", "         ", "start", "=", "n", ".", "start", "\n", "", "retypedtoken", ",", "retypedtype", "=", "strippedretypediob", "[", "start", "]", "\n", "score", "=", "1.0", "\n", "if", "isinstance", "(", "retypedtype", ",", "tuple", ")", ":", "\n", "         ", "retypedbtag", ",", "score", "=", "retypedtype", "\n", "", "else", ":", "\n", "         ", "retypedbtag", "=", "retypedtype", "\n", "\n", "", "if", "retypedbtag", ".", "startswith", "(", "BTAG", "+", "AMRPREFIX", ")", ":", "\n", "         ", "retypedbtag", "=", "retypedbtag", "[", "len", "(", "BTAG", "+", "AMRPREFIX", ")", ":", "]", "\n", "", "if", "retypedbtag", ".", "startswith", "(", "ITAG", "+", "AMRPREFIX", ")", ":", "\n", "         ", "retypedbtag", "=", "retypedbtag", "[", "len", "(", "ITAG", "+", "AMRPREFIX", ")", ":", "]", "\n", "\n", "", "retype", "=", "origtype", "\n", "if", "score", ">=", "params", ".", "getparam", "(", "'scorethreshold'", ")", ":", "\n", "         ", "retype", "=", "retypedbtag", ".", "split", "(", "'\\t'", ")", "[", "0", "]", "# remove \\t<entitylink> if it exists", "\n", "n", ".", "setscore", "(", "score", ")", "\n", "\n", "", "n", ".", "setretype", "(", "retype", ")", "\n", "retypenolink", "=", "retype", ".", "split", "(", "' :wiki'", ")", "[", "0", "]", "\n", "nodelines", "[", "n", ".", "nodeline", "]", "=", "nodelines", "[", "n", ".", "nodeline", "]", ".", "replace", "(", "origtype", ",", "retypenolink", ")", "\n", "if", "params", ".", "getparam", "(", "'outputscores'", ")", ":", "\n", "         ", "nodelines", "[", "n", ".", "nodeline", "]", "=", "nodelines", "[", "n", ".", "nodeline", "]", ".", "rstrip", "(", ")", "+", "f'\\t## {n.score:.{OUTPUTPRECISION}f}\\n'", "\n", "", "numretypes", "+=", "1", "\n", "if", "retype", "!=", "origtype", ":", "\n", "         ", "numretypesdiff", "+=", "1", "\n", "\n", "", "ppid", "=", "n", ".", "parsepiece", "\n", "if", "ppid", ">=", "0", ":", "\n", "         ", "if", "params", ".", "getparam", "(", "'debug'", ")", ":", "\n", "            ", "printerr", "(", "f'pp[{ppid}] =\\n{parsepieces[ppid].tostring()}'", ")", "\n", "", "parsepieces", "[", "ppid", "]", ".", "ptype", "=", "retype", "\n", "if", "params", ".", "getparam", "(", "'debug'", ")", ":", "\n", "            ", "printerr", "(", "f'pp[{ppid}] =\\n{parsepieces[ppid].tostring()}'", ")", "\n", "\n", "", "", "", "newpreamble", "+=", "nodelines", "\n", "\n", "if", "rootisnodenum", ">=", "0", ":", "\n", "      ", "rootline", "=", "nodelines", "[", "rootisnodenum", "]", ".", "replace", "(", "LINETYPEPREFIXES", "[", "'node'", "]", ",", "LINETYPEPREFIXES", "[", "'root'", "]", ")", "\n", "\n", "", "newpreamble", ".", "append", "(", "rootline", ")", "\n", "\n", "NUMFIELDS", "=", "6", "\n", "EDGEFIELD", "=", "0", "\n", "EDGENAME", "=", "1", "\n", "ARG1TYPE", "=", "2", "\n", "ARG2TYPE", "=", "3", "\n", "ARG1NODE", "=", "4", "\n", "ARG2NODE", "=", "5", "\n", "edgelines", "=", "[", "pl", "for", "pl", "in", "preamble", "if", "pl", ".", "startswith", "(", "LINETYPEPREFIXES", "[", "'edge'", "]", ")", "]", "\n", "newedgelines", "=", "list", "(", ")", "\n", "for", "edge", "in", "edgelines", ":", "\n", "      ", "fields", "=", "edge", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "if", "len", "(", "fields", ")", "!=", "NUMFIELDS", ":", "\n", "         ", "newedgelines", ".", "append", "(", "edge", ")", "\n", "", "else", ":", "\n", "         ", "epref", "=", "fields", "[", "EDGEFIELD", "]", ".", "strip", "(", ")", "\n", "ename", "=", "fields", "[", "EDGENAME", "]", ".", "strip", "(", ")", "\n", "ea1type", "=", "fields", "[", "ARG1TYPE", "]", ".", "strip", "(", ")", "\n", "ea1node", "=", "fields", "[", "ARG1NODE", "]", ".", "strip", "(", ")", "\n", "ea2type", "=", "fields", "[", "ARG2TYPE", "]", ".", "strip", "(", ")", "\n", "ea2node", "=", "fields", "[", "ARG2NODE", "]", ".", "strip", "(", ")", "\n", "for", "nid", "in", "nodesneedingretyping", ":", "\n", "            ", "n", "=", "nodelist", "[", "nid", "]", "\n", "if", "ea1node", "==", "n", ".", "aid", "and", "ename", "==", "n", ".", "ntype", ":", "\n", "               ", "if", "n", ".", "retype", ":", "\n", "                  ", "ename", "=", "n", ".", "retype", ".", "split", "(", "' :wiki'", ")", "[", "0", "]", "\n", "", "", "if", "ea2node", "==", "n", ".", "aid", "and", "ea2type", "==", "n", ".", "ntype", ":", "\n", "               ", "if", "n", ".", "retype", ":", "\n", "                  ", "ea2type", "=", "n", ".", "retype", ".", "split", "(", "' :wiki'", ")", "[", "0", "]", "\n", "", "", "", "newedgelines", ".", "append", "(", "'\\t'", ".", "join", "(", "[", "epref", ",", "ename", ",", "ea1type", ",", "ea2type", ",", "ea1node", ",", "ea2node", "+", "'\\n'", "]", ")", ")", "\n", "\n", "", "", "newpreamble", "+=", "newedgelines", "\n", "\n", "for", "pl", "in", "newpreamble", ":", "\n", "      ", "printoutput", "(", "pl", ",", "ofil", ",", "end", "=", "''", ")", "\n", "\n", "", "newparsestrings", "=", "list", "(", ")", "\n", "\n", "for", "_", ",", "pp", "in", "parsepieces", ".", "items", "(", ")", ":", "\n", "      ", "newparsestrings", ".", "append", "(", "f'{AMRNODESTART}{pp.head}{params.getparam(\"typejoiner\")}{params.getparam(\"typepad\")}{pp.ptype}{pp.tail}'", ")", "\n", "", "for", "nps", "in", "newparsestrings", ":", "\n", "      ", "printoutput", "(", "nps", ",", "ofil", ",", "end", "=", "''", ")", "\n", "", "printoutput", "(", "''", ",", "ofil", ")", "\n", "\n", "if", "ofil", ":", "\n", "      ", "ofil", ".", "flush", "(", ")", "\n", "\n", "", "return", "numretypes", ",", "numretypesdiff", ",", "jadd", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.main": [[965, 1149], ["retyper.ParamStore", "retyper.parseclargs", "isinstance", "retyper.ParamStore.getparam", "retyper.ParamStore.getparam", "retyper.ParamStore.getparam", "retyper.ParamStore.setparam", "retyper.ParamStore.setparam", "retyper.ParamStore.setparam", "retyper.ParamStore.setparam", "retyper.ParamStore.setparam", "retyper.ParamStore.setparam", "retyper.ParamStore.setparam", "retyper.ParamStore.setparam", "retyper.ParamStore.setparam", "retyper.ParamStore.setparam", "retyper.ParamStore.setparam", "retyper.ParamStore.setparam", "open", "list", "retyper.ParamStore.setparam", "retyper.printerr", "retyper.printerr", "retyper.ParamStore.getparam", "EntityTyper", "Blinker", "retyper.ParamStore.getparam", "retyper.ParamStore.setparam", "list", "list", "dict", "list", "open.close", "retyper.printerr", "retyper.printerr", "l.strip", "l.strip.startswith", "retyper.ParamStore.tostring", "open", "open", "open", "open", "retyper.printerr", "open.close", "open.close", "open.close", "Blinker.done", "l.strip.startswith", "list.append", "retyper.ParamStore.getparam", "retyper.ParamStore.getparam", "retyper.ParamStore.getparam", "list", "list", "dict", "list", "line.startswith", "retyper.processparse", "retyper.printoutput", "retyper.processparse", "retyper.printoutput", "list.append", "line.startswith", "len", "line.startswith", "len", "line[].split", "line.startswith", "len", "retyper.addnode", "line.startswith", "retyper.addedge", "list.append", "len"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.parseclargs", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParamStore.getparam", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParamStore.getparam", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParamStore.getparam", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParamStore.setparam", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParamStore.setparam", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParamStore.setparam", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParamStore.setparam", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParamStore.setparam", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParamStore.setparam", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParamStore.setparam", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParamStore.setparam", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParamStore.setparam", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParamStore.setparam", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParamStore.setparam", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParamStore.setparam", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParamStore.setparam", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.printerr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.printerr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParamStore.getparam", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParamStore.getparam", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParamStore.setparam", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.GoldMiner.close", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.printerr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.printerr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParsePiece.tostring", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.printerr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.GoldMiner.close", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.GoldMiner.close", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.GoldMiner.close", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.Blinker.Blinker.done", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParamStore.getparam", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParamStore.getparam", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.ParamStore.getparam", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.processparse", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.printoutput", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.processparse", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.printoutput", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.addnode", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.addedge", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "main", "(", ")", ":", "\n", "   ", "params", "=", "ParamStore", "(", ")", "\n", "clargs", "=", "parseclargs", "(", ")", "\n", "\n", "if", "clargs", ".", "padnodetypes", ":", "\n", "      ", "params", ".", "setparam", "(", "'typepad'", ",", "AMRTYPEPAD", ")", "\n", "", "if", "clargs", ".", "debug", ":", "\n", "      ", "params", ".", "setparam", "(", "'debug'", ",", "True", ")", "\n", "", "if", "clargs", ".", "skipretyper", ":", "\n", "      ", "params", ".", "setparam", "(", "'skipretyper'", ",", "True", ")", "\n", "", "if", "clargs", ".", "wikify", ":", "\n", "      ", "params", ".", "setparam", "(", "'wikify'", ",", "True", ")", "\n", "", "if", "clargs", ".", "withdictionary", ":", "\n", "      ", "params", ".", "setparam", "(", "'withdictionary'", ",", "True", ")", "\n", "", "if", "clargs", ".", "outputscores", ":", "\n", "      ", "params", ".", "setparam", "(", "'outputscores'", ",", "True", ")", "\n", "", "if", "clargs", ".", "explorethreshold", ":", "\n", "      ", "params", ".", "setparam", "(", "'explorethreshold'", ",", "True", ")", "\n", "", "if", "clargs", ".", "noclobber", ":", "\n", "      ", "params", ".", "setparam", "(", "'noclobber'", ",", "True", ")", "\n", "", "if", "clargs", ".", "blinkcrossencoder", ":", "\n", "      ", "params", ".", "setparam", "(", "'blinkfastmode'", ",", "False", ")", "\n", "", "if", "clargs", ".", "scorethreshold", ":", "\n", "      ", "params", ".", "setparam", "(", "'scorethreshold'", ",", "clargs", ".", "scorethreshold", ")", "\n", "", "if", "isinstance", "(", "clargs", ".", "blinkthreshold", ",", "float", ")", ":", "\n", "      ", "params", ".", "setparam", "(", "'blinkthreshold'", ",", "clargs", ".", "blinkthreshold", ")", "\n", "\n", "", "if", "clargs", ".", "retypeamrtypesonly", ":", "\n", "      ", "params", ".", "setparam", "(", "'retypeamronly'", ",", "True", ")", "\n", "lfile", "=", "open", "(", "clargs", ".", "labelsfile", ",", "'r'", ")", "\n", "retypetypes", "=", "list", "(", ")", "\n", "for", "l", "in", "lfile", ":", "\n", "         ", "atyp", "=", "l", ".", "strip", "(", ")", "\n", "if", "atyp", ".", "startswith", "(", "BTAG", ")", ":", "\n", "            ", "atyp", "=", "atyp", "[", "len", "(", "BTAG", ")", ":", "]", "\n", "if", "atyp", ".", "startswith", "(", "AMRPREFIX", ")", ":", "\n", "               ", "atyp", "=", "atyp", "[", "len", "(", "AMRPREFIX", ")", ":", "]", "\n", "", "retypetypes", ".", "append", "(", "atyp", ")", "\n", "", "", "params", ".", "setparam", "(", "'retypetypes'", ",", "retypetypes", ")", "\n", "\n", "", "if", "params", ".", "getparam", "(", "'debug'", ")", ":", "\n", "      ", "printerr", "(", "params", ".", "tostring", "(", ")", ")", "\n", "\n", "", "if", "params", ".", "getparam", "(", "'skipretyper'", ")", ":", "\n", "      ", "retyper", "=", "None", "\n", "", "else", ":", "\n", "      ", "printerr", "(", "'Loading model...'", ")", "\n", "if", "params", ".", "getparam", "(", "'withdictionary'", ")", ":", "\n", "         ", "from", "etyperwithdictionary", "import", "EntityTyper", "\n", "", "else", ":", "\n", "         ", "from", "etyper", "import", "EntityTyper", "\n", "\n", "", "retyper", "=", "EntityTyper", "(", "pathtoclasslabelsfile", "=", "clargs", ".", "labelsfile", ",", "pathtomodeldirectory", "=", "clargs", ".", "modeldirectory", ")", "\n", "\n", "", "blinker", "=", "None", "\n", "if", "clargs", ".", "wikify", ":", "\n", "      ", "from", "Blinker", "import", "Blinker", "\n", "blinker", "=", "Blinker", "(", "pathtomodeldirectory", "=", "clargs", ".", "blinkmodels", ",", "pathtocachedirectory", "=", "clargs", ".", "blinkcachepath", ",", "fastmode", "=", "params", ".", "getparam", "(", "'blinkfastmode'", ")", ",", "wikititleonly", "=", "True", ")", "\n", "\n", "", "threshlist", "=", "[", "params", ".", "getparam", "(", "'scorethreshold'", ")", "]", "\n", "\n", "if", "params", ".", "getparam", "(", "'explorethreshold'", ")", ":", "\n", "      ", "threshlist", "=", "EXPLORETHRESHOLDLIST", "\n", "\n", "", "for", "thresh", "in", "threshlist", ":", "\n", "\n", "      ", "ifil", "=", "None", "\n", "ofil", "=", "None", "\n", "bfil", "=", "None", "\n", "jfil", "=", "None", "\n", "\n", "if", "clargs", ".", "inputfile", ":", "\n", "         ", "ifil", "=", "open", "(", "clargs", ".", "inputfile", ",", "'r'", ")", "\n", "", "if", "clargs", ".", "biooutputfile", ":", "\n", "         ", "bfil", "=", "open", "(", "clargs", ".", "biooutputfile", ",", "'w'", ")", "\n", "", "if", "clargs", ".", "jsonoutputfile", ":", "\n", "         ", "jfil", "=", "open", "(", "clargs", ".", "jsonoutputfile", ",", "'w'", ")", "\n", "\n", "", "if", "clargs", ".", "outputfile", ":", "\n", "         ", "if", "params", ".", "getparam", "(", "'explorethreshold'", ")", "or", "params", ".", "getparam", "(", "'scorethreshold'", ")", ":", "\n", "            ", "currofilname", "=", "f'{clargs.outputfile}_{thresh}.amr'", "\n", "", "else", ":", "\n", "            ", "currofilname", "=", "clargs", ".", "outputfile", "\n", "\n", "", "ofil", "=", "open", "(", "currofilname", ",", "'w'", ")", "\n", "\n", "", "params", ".", "setparam", "(", "'scorethreshold'", ",", "thresh", ")", "\n", "\n", "numparses", "=", "0", "\n", "numretypes", "=", "0", "\n", "numretypesdiff", "=", "0", "\n", "jcount", "=", "0", "\n", "\n", "if", "retyper", ":", "\n", "         ", "printerr", "(", "f'Retyping with threshold = {thresh}...'", ")", "\n", "\n", "", "current_input", "=", "''", "\n", "needsretyping", "=", "False", "\n", "current_snt", "=", "''", "\n", "current_preamble", "=", "list", "(", ")", "\n", "current_toks", "=", "list", "(", ")", "\n", "nodelist", "=", "dict", "(", ")", "\n", "edgelist", "=", "list", "(", ")", "\n", "current_amrparse", "=", "''", "\n", "i", "=", "0", "\n", "nodelinenum", "=", "0", "\n", "edgelinenum", "=", "0", "\n", "processing", "=", "False", "\n", "\n", "for", "line", "in", "ifil", ":", "\n", "         ", "current_input", "+=", "line", "\n", "\n", "if", "line", "==", "'\\n'", ":", "# must have blank line between amr parses", "\n", "            ", "if", "not", "processing", ":", "# skip extra blank lines if already dumped previous parse", "\n", "               ", "continue", "\n", "\n", "", "numparses", "+=", "1", "\n", "\n", "if", "needsretyping", ":", "\n", "               ", "numretypesadd", ",", "numretypesdiffadd", ",", "numjadd", "=", "processparse", "(", "current_amrparse", ",", "current_toks", ",", "nodelist", ",", "edgelist", ",", "current_preamble", ",", "retyper", ",", "blinker", ",", "ofil", ",", "bfil", ",", "jfil", ",", "jcount", ",", "params", ")", "\n", "numretypes", "+=", "numretypesadd", "\n", "numretypesdiff", "+=", "numretypesdiffadd", "\n", "jcount", "+=", "numjadd", "\n", "", "else", ":", "\n", "               ", "printoutput", "(", "current_input", ",", "ofil", ",", "end", "=", "''", ")", "\n", "\n", "", "current_input", "=", "''", "\n", "needsretyping", "=", "False", "\n", "current_snt", "=", "''", "\n", "current_preamble", "=", "list", "(", ")", "\n", "current_toks", "=", "list", "(", ")", "\n", "nodelist", "=", "dict", "(", ")", "\n", "edgelist", "=", "list", "(", ")", "\n", "current_amrparse", "=", "''", "\n", "i", "=", "0", "\n", "nodelinenum", "=", "0", "\n", "edgelinenum", "=", "0", "\n", "processing", "=", "False", "\n", "", "elif", "line", ".", "startswith", "(", "LINETYPEPREFIXES", "[", "'amrline'", "]", ")", ":", "\n", "            ", "processing", "=", "True", "\n", "if", "NAMEDENTITYOP", "in", "line", ":", "\n", "               ", "needsretyping", "=", "True", "\n", "", "current_amrparse", "+=", "line", "\n", "", "else", ":", "\n", "            ", "current_preamble", ".", "append", "(", "line", ")", "\n", "processing", "=", "True", "\n", "if", "line", ".", "startswith", "(", "LINETYPEPREFIXES", "[", "'snt'", "]", ")", ":", "\n", "               ", "current_snt", "=", "line", "[", "len", "(", "LINETYPEPREFIXES", "[", "'snt'", "]", ")", ":", "]", "\n", "", "elif", "line", ".", "startswith", "(", "LINETYPEPREFIXES", "[", "'tok'", "]", ")", ":", "\n", "               ", "current_toks", "=", "line", "[", "len", "(", "LINETYPEPREFIXES", "[", "'tok'", "]", ")", ":", "]", ".", "split", "(", ")", "\n", "", "elif", "line", ".", "startswith", "(", "LINETYPEPREFIXES", "[", "'node'", "]", ")", ":", "\n", "               ", "tmpnode", "=", "addnode", "(", "nodelinenum", ",", "line", ")", "\n", "nodelinenum", "+=", "1", "\n", "if", "tmpnode", ":", "\n", "                  ", "nodelist", "[", "i", "]", "=", "tmpnode", "\n", "i", "+=", "1", "\n", "", "", "elif", "line", ".", "startswith", "(", "LINETYPEPREFIXES", "[", "'edge'", "]", ")", ":", "\n", "               ", "tmpedge", "=", "addedge", "(", "edgelinenum", ",", "line", ")", "\n", "edgelinenum", "+=", "1", "\n", "if", "tmpedge", ":", "\n", "                  ", "edgelist", ".", "append", "(", "tmpedge", ")", "\n", "\n", "", "", "", "", "ifil", ".", "close", "(", ")", "\n", "\n", "if", "processing", ":", "# there's a parse left over (no blank line at end of file)", "\n", "         ", "if", "needsretyping", ":", "\n", "            ", "numretypesadd", ",", "numretypesdiffadd", ",", "numjadd", "=", "processparse", "(", "current_amrparse", ",", "current_toks", ",", "nodelist", ",", "edgelist", ",", "current_preamble", ",", "retyper", ",", "blinker", ",", "ofil", ",", "bfil", ",", "jfil", ",", "jcount", ",", "params", ")", "\n", "numretypes", "+=", "numretypesadd", "\n", "numretypesdiff", "+=", "numretypesdiffadd", "\n", "jcount", "+=", "numjadd", "\n", "", "else", ":", "\n", "            ", "printoutput", "(", "current_input", ",", "ofil", ")", "\n", "\n", "", "", "printerr", "(", "f'{numparses} AMR parses processed'", ")", "\n", "printerr", "(", "f'{numretypesdiff} / {numretypes} Named Entities processed'", ")", "\n", "\n", "if", "bfil", ":", "\n", "         ", "bfil", ".", "close", "(", ")", "\n", "", "if", "jfil", ":", "\n", "         ", "jfil", ".", "close", "(", ")", "\n", "", "if", "ofil", ":", "\n", "         ", "ofil", ".", "close", "(", ")", "\n", "", "if", "blinker", ":", "\n", "         ", "blinker", ".", "done", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.dep_parsing_score.argument_parser": [[8, 37], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["def", "argument_parser", "(", ")", ":", "\n", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Tool to handle AMR'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--in-tokens\"", ",", "\n", "help", "=", "\"tab separated tokens one sentence per line\"", ",", "\n", "required", "=", "True", ",", "\n", "type", "=", "str", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--in-actions\"", ",", "\n", "help", "=", "\"space separated actions one sentence per line\"", ",", "\n", "required", "=", "True", ",", "\n", "type", "=", "str", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--in-gold-actions\"", ",", "\n", "help", "=", "\"space separated actions one sentence per line\"", ",", "\n", "required", "=", "True", ",", "\n", "type", "=", "str", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--out-score\"", ",", "\n", "help", "=", "\"ARC scores\"", ",", "\n", "type", "=", "str", ",", "\n", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.dep_parsing_score.compute_correct": [[39, 148], ["reversed", "reversed", "range", "dep_parsing_score.compute_correct.clean_tag"], "function", ["None"], ["", "def", "compute_correct", "(", "src_str", ",", "hypo_str", ",", "target_str", ")", ":", "\n", "\n", "    ", "def", "clean_tag", "(", "action", ")", ":", "\n", "        ", "if", "'SHIFT'", "in", "action", ":", "\n", "# remove multi-task in SHIFT", "\n", "            ", "action", "=", "action", ".", "split", "(", "'('", ")", "[", "0", "]", "\n", "# for action ngrams, keep only first action", "\n", "", "return", "action", ".", "split", "(", "','", ")", "[", "0", "]", "\n", "\n", "", "hypo_str", "=", "[", "clean_tag", "(", "x", ")", "for", "x", "in", "hypo_str", "]", "\n", "target_str", "=", "[", "clean_tag", "(", "x", ")", "for", "x", "in", "target_str", "]", "\n", "\n", "\n", "#print('GOLD',target_str)", "\n", "#print('PRED',hypo_str)", "\n", "\n", "\n", "\n", "correct_heads", "=", "0", "\n", "total_heads", "=", "0", "\n", "correct_labels", "=", "0", "\n", "gold_heads", "=", "{", "}", "\n", "gold_labels", "=", "{", "}", "\n", "hyp_heads", "=", "{", "}", "\n", "hyp_labels", "=", "{", "}", "\n", "pbuffer", "=", "[", "]", "\n", "pstack", "=", "[", "]", "\n", "for", "i", "in", "reversed", "(", "range", "(", "len", "(", "src_str", ")", ")", ")", ":", "\n", "        ", "if", "i", "<", "len", "(", "src_str", ")", "-", "1", ":", "\n", "            ", "pbuffer", ".", "append", "(", "i", "+", "1", ")", "\n", "", "else", ":", "\n", "            ", "pbuffer", ".", "append", "(", "0", ")", "\n", "# compute gold", "\n", "", "", "for", "word", "in", "target_str", ":", "\n", "\n", "        ", "if", "\"unk\"", "in", "word", ":", "\n", "            ", "word", "=", "\"RIGHT-ARC(preconj)\"", "\n", "", "if", "\"SHIFT\"", "in", "word", ":", "\n", "            ", "pstack", ".", "append", "(", "pbuffer", ".", "pop", "(", ")", ")", "\n", "", "elif", "\"ARC\"", "in", "word", ":", "\n", "            ", "head", "=", "-", "1", "\n", "dep", "=", "-", "1", "\n", "s0", "=", "pstack", ".", "pop", "(", ")", "\n", "s1", "=", "pstack", ".", "pop", "(", ")", "\n", "if", "\"RIGHT\"", "in", "word", ":", "\n", "                ", "head", "=", "s1", "\n", "dep", "=", "s0", "\n", "", "else", ":", "\n", "                ", "head", "=", "s0", "\n", "dep", "=", "s1", "\n", "", "pstack", ".", "append", "(", "head", ")", "\n", "gold_heads", "[", "dep", "]", "=", "head", "\n", "label", "=", "word", ".", "split", "(", "\"(\"", ")", "[", "1", "]", "\n", "gold_labels", "[", "dep", "]", "=", "label", "\n", "", "elif", "word", "==", "\"SWAP\"", ":", "\n", "            ", "s0", "=", "pstack", ".", "pop", "(", ")", "\n", "s1", "=", "pstack", ".", "pop", "(", ")", "\n", "pbuffer", ".", "append", "(", "s1", ")", "\n", "pstack", ".", "append", "(", "s0", ")", "\n", "\n", "", "", "pbuffer", "=", "[", "]", "\n", "pstack", "=", "[", "]", "\n", "for", "i", "in", "reversed", "(", "range", "(", "len", "(", "src_str", ")", ")", ")", ":", "\n", "        ", "if", "i", "<", "len", "(", "src_str", ")", "-", "1", ":", "\n", "            ", "pbuffer", ".", "append", "(", "i", "+", "1", ")", "\n", "", "else", ":", "\n", "            ", "pbuffer", ".", "append", "(", "0", ")", "\n", "\n", "", "", "for", "word", "in", "hypo_str", ":", "\n", "\n", "        ", "if", "\"SHIFT\"", "in", "word", ":", "\n", "            ", "if", "True", ":", "#len(pstack)>=1:#tiene que haber al menos 1 elemento en la pila", "\n", "                ", "pstack", ".", "append", "(", "pbuffer", ".", "pop", "(", ")", ")", "\n", "", "", "elif", "\"ARC\"", "in", "word", ":", "\n", "            ", "if", "True", ":", "#len(pstack)>=2:#tiene que haber al menos 2 elementos en la pila", "\n", "                ", "head", "=", "-", "1", "\n", "dep", "=", "-", "1", "\n", "s0", "=", "pstack", ".", "pop", "(", ")", "\n", "s1", "=", "pstack", ".", "pop", "(", ")", "\n", "if", "\"RIGHT\"", "in", "word", ":", "\n", "                    ", "head", "=", "s1", "\n", "dep", "=", "s0", "\n", "", "else", ":", "\n", "                    ", "head", "=", "s0", "\n", "dep", "=", "s1", "\n", "", "pstack", ".", "append", "(", "head", ")", "\n", "hyp_heads", "[", "dep", "]", "=", "head", "\n", "label", "=", "word", ".", "split", "(", "\"(\"", ")", "[", "1", "]", "\n", "hyp_labels", "[", "dep", "]", "=", "label", "\n", "", "", "elif", "word", "==", "\"SWAP\"", ":", "\n", "            ", "if", "True", ":", "#len(pstack)>=2:", "\n", "                ", "s0", "=", "pstack", ".", "pop", "(", ")", "\n", "s1", "=", "pstack", ".", "pop", "(", ")", "\n", "pbuffer", ".", "append", "(", "s1", ")", "\n", "pstack", ".", "append", "(", "s0", ")", "\n", "\n", "#print('GOLD',gold_heads)", "\n", "#print('PRED',hyp_heads)", "\n", "", "", "", "for", "i", "in", "range", "(", "len", "(", "src_str", ")", "-", "1", ")", ":", "\n", "        ", "idw", "=", "i", "+", "1", "\n", "total_heads", "+=", "1", "\n", "if", "idw", "in", "gold_heads", "and", "idw", "in", "hyp_heads", ":", "\n", "            ", "if", "hyp_heads", "[", "idw", "]", "==", "gold_heads", "[", "idw", "]", ":", "\n", "                ", "correct_heads", "+=", "1", "\n", "", "if", "hyp_heads", "[", "idw", "]", "==", "gold_heads", "[", "idw", "]", "and", "hyp_labels", "[", "idw", "]", "==", "gold_labels", "[", "idw", "]", ":", "\n", "                ", "correct_labels", "+=", "1", "\n", "\n", "#exit(0)", "\n", "", "", "", "return", "total_heads", ",", "correct_heads", ",", "correct_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.sanity_check_amr.get_propbank_name": [[9, 17], ["amr_pred.split", "prop_pred.replace.endswith", "prop_pred.replace.replace"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["def", "get_propbank_name", "(", "amr_pred", ")", ":", "\n", "    ", "items", "=", "amr_pred", ".", "split", "(", "'-'", ")", "\n", "prop_pred", "=", "'-'", ".", "join", "(", "items", "[", ":", "-", "1", "]", ")", "+", "'.'", "+", "items", "[", "-", "1", "]", "\n", "if", "prop_pred", ".", "endswith", "(", "'.91'", ")", "or", "prop_pred", "in", "[", "'have-half-life.01'", "]", ":", "\n", "        ", "pass", "\n", "", "else", ":", "\n", "        ", "prop_pred", "=", "prop_pred", ".", "replace", "(", "'-'", ",", "'_'", ")", "\n", "", "return", "prop_pred", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.extract_rules.create_fixed_rules": [[12, 109], ["sorted", "list", "sum", "extract_rules.clean_rules", "countries[].split", "countries[].split", "extract_rules.normalize", "all", "extract_rules.normalize_entity", "str", "str", "fixed_counters.keys", "fixed_counters[].values", "rule.split", "len", "print", "edges.append", "edges.append", "print", "countries[].split.isalpha", "collections.Counter", "print", "len"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.extract_rules.clean_rules", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.extract_rules.normalize", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.extract_rules.normalize_entity", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["def", "create_fixed_rules", "(", "all_entities", ")", ":", "\n", "    ", "global", "entity_rules_json", "\n", "\n", "fixed_counters", "=", "{", "}", "\n", "fixed_rules", "=", "{", "}", "\n", "\n", "for", "amr", ",", "entity_type", ",", "tokens", ",", "root", ",", "nodes", ",", "edges", "in", "all_entities", ":", "\n", "\n", "        ", "final_nodes", "=", "[", "n", "for", "n", "in", "nodes", "if", "not", "[", "e", "for", "e", "in", "edges", "if", "e", "[", "0", "]", "==", "n", "]", "]", "\n", "ntokens", "=", "[", "normalize", "(", "t", ")", "for", "t", "in", "tokens", "]", "\n", "\n", "if", "not", "all", "(", "t", "in", "[", "amr", ".", "nodes", "[", "n", "]", "for", "n", "in", "final_nodes", "]", "for", "t", "in", "ntokens", ")", ":", "\n", "# normalize entity subgraph", "\n", "            ", "normalized_entity", "=", "normalize_entity", "(", "root", ",", "nodes", ",", "edges", ")", "\n", "name", "=", "''", ".", "join", "(", "tokens", ")", ".", "lower", "(", ")", "\n", "if", "len", "(", "name", ")", "==", "1", "and", "name", ".", "isalpha", "(", ")", ":", "\n", "                ", "continue", "\n", "\n", "", "rule", "=", "entity_type", "+", "'\\t'", "+", "','", ".", "join", "(", "tokens", ")", ".", "lower", "(", ")", "\n", "node_string", "=", "str", "(", "normalized_entity", ".", "nodes", ")", "\n", "edge_string", "=", "str", "(", "normalized_entity", ".", "edges", ")", "\n", "if", "rule", "not", "in", "fixed_counters", ":", "\n", "                ", "fixed_counters", "[", "rule", "]", "=", "Counter", "(", ")", "\n", "fixed_rules", "[", "rule", "]", "=", "{", "}", "\n", "", "solution_tag", "=", "node_string", "+", "'\\t'", "+", "edge_string", "\n", "if", "solution_tag", "not", "in", "fixed_rules", "[", "rule", "]", ":", "\n", "                ", "fixed_rules", "[", "rule", "]", "[", "solution_tag", "]", "=", "{", "}", "\n", "", "fixed_counters", "[", "rule", "]", "[", "solution_tag", "]", "+=", "1", "\n", "fixed_rules", "[", "rule", "]", "[", "solution_tag", "]", "[", "'nodes'", "]", "=", "normalized_entity", ".", "nodes", "\n", "fixed_rules", "[", "rule", "]", "[", "solution_tag", "]", "[", "'edges'", "]", "=", "normalized_entity", ".", "edges", "\n", "# for s,r,t in normalized_entity.edges:", "\n", "#     if r==':quant' and not normalized_entity.nodes[t].isdigit():", "\n", "#         print(':quant',normalized_entity.nodes[t])", "\n", "#     if entity_type!='date-entity' and normalized_entity.nodes[t].isdigit():", "\n", "#         print(r,normalized_entity.nodes[t])", "\n", "fixed_rules", "[", "rule", "]", "[", "solution_tag", "]", "[", "'root'", "]", "=", "normalized_entity", ".", "root", "\n", "\n", "# clean rules", "\n", "", "", "for", "rule", "in", "sorted", "(", "list", "(", "fixed_counters", ".", "keys", "(", ")", ")", ")", ":", "\n", "        ", "total", "=", "sum", "(", "fixed_counters", "[", "rule", "]", ".", "values", "(", ")", ")", "\n", "name", "=", "rule", ".", "split", "(", "'\\t'", ")", "[", "1", "]", "\n", "if", "len", "(", "name", ")", "<", "3", ":", "\n", "            ", "if", "total", "<", "3", ":", "\n", "                ", "del", "fixed_counters", "[", "rule", "]", "\n", "del", "fixed_rules", "[", "rule", "]", "\n", "# print('[deleting] '+rule)", "\n", "continue", "\n", "", "", "clean_rules", "(", "fixed_counters", "[", "rule", "]", ",", "fixed_rules", "[", "rule", "]", ")", "\n", "if", "verbose", ":", "\n", "            ", "print", "(", "'[fixed rule]'", ",", "rule", ")", "\n", "\n", "", "", "for", "country", "in", "countries", ":", "\n", "\n", "        ", "root", "=", "0", "\n", "nodes", "=", "{", "0", ":", "'country'", ",", "1", ":", "'name'", ",", "}", "\n", "edges", "=", "[", "(", "0", ",", "':name'", ",", "1", ")", "]", "\n", "name", "=", "countries", "[", "country", "]", ".", "split", "(", ")", "\n", "idx", "=", "2", "\n", "for", "n", "in", "name", ":", "\n", "            ", "nodes", "[", "idx", "]", "=", "f'\"{n}\"'", "\n", "edges", ".", "append", "(", "(", "1", ",", "f':op{idx-1}'", ",", "idx", ")", ")", "\n", "idx", "+=", "1", "\n", "\n", "", "for", "rule", "in", "[", "'country,name'", "+", "'\\t'", "+", "country", ",", "'country,name'", "+", "'\\t'", "+", "','", ".", "join", "(", "name", ")", ".", "lower", "(", ")", "]", ":", "\n", "            ", "if", "rule", "in", "fixed_rules", ":", "\n", "                ", "continue", "\n", "", "else", ":", "\n", "                ", "fixed_rules", "[", "rule", "]", "=", "{", "}", "\n", "fixed_rules", "[", "rule", "]", "[", "'count'", "]", "=", "0", "\n", "", "fixed_rules", "[", "rule", "]", "[", "'nodes'", "]", "=", "nodes", "\n", "fixed_rules", "[", "rule", "]", "[", "'edges'", "]", "=", "edges", "\n", "fixed_rules", "[", "rule", "]", "[", "'root'", "]", "=", "root", "\n", "if", "verbose", ":", "\n", "                ", "print", "(", "'[fixed rule]'", ",", "rule", ")", "\n", "\n", "", "", "root", "=", "0", "\n", "nodes", "=", "{", "0", ":", "'person'", ",", "1", ":", "'country'", ",", "2", ":", "'name'", "}", "\n", "edges", "=", "[", "(", "0", ",", "':mod'", ",", "1", ")", ",", "(", "1", ",", "':name'", ",", "2", ")", "]", "\n", "name", "=", "countries", "[", "country", "]", ".", "split", "(", ")", "\n", "idx", "=", "2", "\n", "for", "n", "in", "name", ":", "\n", "            ", "nodes", "[", "idx", "]", "=", "f'\"{n}\"'", "\n", "edges", ".", "append", "(", "(", "2", ",", "f':op{idx - 1}'", ",", "idx", ")", ")", "\n", "idx", "+=", "1", "\n", "", "rule", "=", "'person,country,name'", "+", "'\\t'", "+", "','", ".", "join", "(", "name", ")", ".", "lower", "(", ")", "\n", "if", "rule", "in", "fixed_rules", ":", "\n", "            ", "continue", "\n", "", "else", ":", "\n", "            ", "fixed_rules", "[", "rule", "]", "=", "{", "}", "\n", "fixed_rules", "[", "rule", "]", "[", "'count'", "]", "=", "0", "\n", "", "fixed_rules", "[", "rule", "]", "[", "'nodes'", "]", "=", "nodes", "\n", "fixed_rules", "[", "rule", "]", "[", "'edges'", "]", "=", "edges", "\n", "fixed_rules", "[", "rule", "]", "[", "'root'", "]", "=", "root", "\n", "if", "verbose", ":", "\n", "            ", "print", "(", "'[fixed rule]'", ",", "rule", ")", "\n", "\n", "", "", "entity_rules_json", "[", "'fixed'", "]", "=", "fixed_rules", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.extract_rules.create_var_rules": [[111, 186], ["sorted", "all", "list", "sum", "extract_rules.clean_rules", "print", "print", "extract_rules.normalize", "enumerate", "extract_rules.normalize_entity", "str", "str", "var_counters.keys", "var_counters[].values", "print", "entity_type.endswith", "str", "collections.Counter", "len"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.extract_rules.clean_rules", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.extract_rules.normalize", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.extract_rules.normalize_entity", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["", "def", "create_var_rules", "(", "all_entities", ")", ":", "\n", "    ", "global", "entity_rules_json", "\n", "\n", "var_counters", "=", "{", "}", "\n", "var_rules", "=", "{", "}", "\n", "\n", "for", "amr", ",", "entity_type", ",", "tokens", ",", "root", ",", "nodes", ",", "edges", "in", "all_entities", ":", "\n", "# Update variable rules", "\n", "\n", "        ", "final_nodes", "=", "[", "n", "for", "n", "in", "nodes", "if", "not", "[", "e", "for", "e", "in", "edges", "if", "e", "[", "0", "]", "==", "n", "]", "]", "\n", "ntokens", "=", "[", "normalize", "(", "t", ")", "for", "t", "in", "tokens", "]", "\n", "\n", "if", "all", "(", "t", "in", "[", "amr", ".", "nodes", "[", "n", "]", "for", "n", "in", "final_nodes", "]", "for", "t", "in", "ntokens", ")", ":", "\n", "            ", "if", "entity_type", ".", "endswith", "(", "',name'", ")", "or", "entity_type", "==", "'name'", ":", "\n", "                ", "continue", "\n", "", "rule", "=", "entity_type", "+", "'\\t'", "+", "str", "(", "len", "(", "ntokens", ")", ")", "\n", "# normalize variables", "\n", "var_map", "=", "{", "}", "\n", "for", "idx", ",", "t", "in", "enumerate", "(", "ntokens", ")", ":", "\n", "                ", "for", "n", "in", "nodes", ":", "\n", "                    ", "if", "t", "==", "amr", ".", "nodes", "[", "n", "]", ":", "\n", "                        ", "var_map", "[", "n", "]", "=", "f'X{idx}'", "\n", "", "else", ":", "\n", "                        ", "var_map", "[", "n", "]", "=", "amr", ".", "nodes", "[", "n", "]", "\n", "# renormalize entity subgraph", "\n", "", "", "", "normalized_entity", "=", "normalize_entity", "(", "root", ",", "var_map", ",", "edges", ")", "\n", "node_string", "=", "str", "(", "normalized_entity", ".", "nodes", ")", "\n", "edge_string", "=", "str", "(", "normalized_entity", ".", "edges", ")", "\n", "if", "rule", "not", "in", "var_counters", ":", "\n", "                ", "var_counters", "[", "rule", "]", "=", "Counter", "(", ")", "\n", "var_rules", "[", "rule", "]", "=", "{", "}", "\n", "", "solution_tag", "=", "node_string", "+", "'\\t'", "+", "edge_string", "\n", "if", "solution_tag", "not", "in", "var_rules", "[", "rule", "]", ":", "\n", "                ", "var_rules", "[", "rule", "]", "[", "solution_tag", "]", "=", "{", "}", "\n", "", "var_counters", "[", "rule", "]", "[", "solution_tag", "]", "+=", "1", "\n", "var_rules", "[", "rule", "]", "[", "solution_tag", "]", "[", "'nodes'", "]", "=", "normalized_entity", ".", "nodes", "\n", "var_rules", "[", "rule", "]", "[", "solution_tag", "]", "[", "'edges'", "]", "=", "normalized_entity", ".", "edges", "\n", "var_rules", "[", "rule", "]", "[", "solution_tag", "]", "[", "'root'", "]", "=", "normalized_entity", ".", "root", "\n", "\n", "# clean rules", "\n", "", "", "for", "rule", "in", "sorted", "(", "list", "(", "var_counters", ".", "keys", "(", ")", ")", ")", ":", "\n", "        ", "total", "=", "sum", "(", "var_counters", "[", "rule", "]", ".", "values", "(", ")", ")", "\n", "# if total < 3:", "\n", "#     del var_counters[rule]", "\n", "#     del var_rules[rule]", "\n", "#     continue", "\n", "clean_rules", "(", "var_counters", "[", "rule", "]", ",", "var_rules", "[", "rule", "]", ")", "\n", "if", "verbose", ":", "\n", "            ", "print", "(", "'[variable rule]'", ",", "rule", ")", "\n", "", "", "rule", "=", "'person\\t1'", "\n", "root", "=", "0", "\n", "nodes", "=", "{", "0", ":", "'person'", ",", "1", ":", "'X0'", "}", "\n", "edges", "=", "[", "(", "0", ",", "':ARG0-of'", ",", "1", ")", "]", "\n", "if", "rule", "not", "in", "var_rules", ":", "\n", "        ", "var_rules", "[", "rule", "]", "=", "{", "}", "\n", "var_rules", "[", "rule", "]", "[", "'count'", "]", "=", "0", "\n", "", "var_rules", "[", "rule", "]", "[", "'nodes'", "]", "=", "nodes", "\n", "var_rules", "[", "rule", "]", "[", "'edges'", "]", "=", "edges", "\n", "var_rules", "[", "rule", "]", "[", "'root'", "]", "=", "root", "\n", "if", "verbose", ":", "\n", "        ", "print", "(", "'[variable rule]'", ",", "rule", ")", "\n", "", "rule", "=", "'thing\\t1'", "\n", "root", "=", "0", "\n", "nodes", "=", "{", "0", ":", "'thing'", ",", "1", ":", "'X0'", "}", "\n", "edges", "=", "[", "(", "0", ",", "':ARG1-of'", ",", "1", ")", "]", "\n", "if", "rule", "not", "in", "var_rules", ":", "\n", "        ", "var_rules", "[", "rule", "]", "=", "{", "}", "\n", "var_rules", "[", "rule", "]", "[", "'count'", "]", "=", "0", "\n", "", "var_rules", "[", "rule", "]", "[", "'nodes'", "]", "=", "nodes", "\n", "var_rules", "[", "rule", "]", "[", "'edges'", "]", "=", "edges", "\n", "var_rules", "[", "rule", "]", "[", "'root'", "]", "=", "root", "\n", "if", "verbose", ":", "\n", "        ", "print", "(", "'[variable rule]'", ",", "rule", ")", "\n", "\n", "", "entity_rules_json", "[", "'var'", "]", "=", "var_rules", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.extract_rules.create_date_entity_rules": [[188, 217], ["print", "date_entity[].append", "date_entity[].remove", "date_entity[].append"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "create_date_entity_rules", "(", "all_entities", ")", ":", "\n", "    ", "global", "entity_rules_json", "\n", "\n", "date_entity", "=", "{", "}", "\n", "\n", "for", "amr", ",", "entity_type", ",", "tokens", ",", "root", ",", "nodes", ",", "edges", "in", "all_entities", ":", "\n", "\n", "        ", "if", "entity_type", "==", "'date-entity'", ":", "\n", "            ", "final_nodes", "=", "[", "n", "for", "n", "in", "nodes", "if", "not", "[", "e", "for", "e", "in", "edges", "if", "e", "[", "0", "]", "==", "n", "]", "]", "\n", "final_edges", "=", "[", "e", "for", "e", "in", "edges", "if", "e", "[", "2", "]", "in", "final_nodes", "]", "\n", "for", "s", ",", "r", ",", "t", "in", "final_edges", ":", "\n", "                ", "if", "r", "not", "in", "date_entity", ":", "\n", "                    ", "date_entity", "[", "r", "]", "=", "[", "]", "\n", "", "if", "amr", ".", "nodes", "[", "t", "]", "not", "in", "date_entity", "[", "r", "]", ":", "\n", "                    ", "date_entity", "[", "r", "]", ".", "append", "(", "amr", ".", "nodes", "[", "t", "]", ")", "\n", "", "", "", "if", "':era'", "not", "in", "date_entity", ":", "\n", "            ", "date_entity", "[", "':era'", "]", "=", "[", "]", "\n", "", "for", "t", "in", "[", "'BC'", ",", "'AD'", ",", "'B.C.'", ",", "'A.D.'", ",", "'CE'", ",", "'BCE'", ",", "'C.E.'", ",", "'B.C.E.'", ",", "'Heisei'", ",", "'Reiwa'", "]", ":", "\n", "            ", "if", "t", "not", "in", "date_entity", "[", "':era'", "]", ":", "\n", "                ", "date_entity", "[", "':era'", "]", ".", "append", "(", "t", ")", "\n", "", "", "if", "':dayperiod'", "not", "in", "date_entity", ":", "\n", "            ", "date_entity", "[", "':dayperiod'", "]", "=", "[", "]", "\n", "", "for", "t", "in", "[", "'and'", ",", "'yesterday'", ",", "'today'", "]", ":", "\n", "            ", "if", "t", "in", "date_entity", "[", "':dayperiod'", "]", ":", "\n", "                ", "date_entity", "[", "':dayperiod'", "]", ".", "remove", "(", "t", ")", "\n", "", "", "", "for", "rule", "in", "date_entity", ":", "\n", "        ", "if", "verbose", ":", "\n", "            ", "print", "(", "'[date-entity rule]'", ",", "rule", ")", "\n", "", "", "entity_rules_json", "[", "'date-entity'", "]", "=", "date_entity", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.extract_rules.create_name_rules": [[219, 248], ["amr.findSubGraph", "entity_type.endswith", "extract_rules.normalize_entity", "print"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.findSubGraph", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.extract_rules.normalize_entity", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["", "def", "create_name_rules", "(", "all_entities", ")", ":", "\n", "    ", "global", "entity_rules_json", "\n", "\n", "name_rules", "=", "{", "}", "\n", "\n", "for", "amr", ",", "entity_type", ",", "tokens", ",", "root", ",", "nodes", ",", "edges", "in", "all_entities", ":", "\n", "\n", "        ", "final_nodes", "=", "[", "n", "for", "n", "in", "nodes", "if", "not", "[", "e", "for", "e", "in", "edges", "if", "e", "[", "0", "]", "==", "n", "]", "]", "\n", "rule_nodes", "=", "[", "id", "for", "id", "in", "nodes", "if", "id", "not", "in", "final_nodes", "]", "\n", "subgraph", "=", "amr", ".", "findSubGraph", "(", "rule_nodes", ")", "\n", "\n", "# if len(rule_nodes) < 3:", "\n", "#     continue", "\n", "\n", "if", "entity_type", ".", "endswith", "(", "',name'", ")", "or", "entity_type", "==", "'name'", ":", "\n", "\n", "# renormalize entity subgraph", "\n", "            ", "normalized_entity", "=", "normalize_entity", "(", "subgraph", ".", "root", ",", "subgraph", ".", "nodes", ",", "subgraph", ".", "edges", ")", "\n", "\n", "rule", "=", "entity_type", "\n", "if", "rule", "not", "in", "name_rules", ":", "\n", "                ", "name_rules", "[", "rule", "]", "=", "{", "}", "\n", "", "name_rules", "[", "rule", "]", "[", "'nodes'", "]", "=", "normalized_entity", ".", "nodes", "\n", "name_rules", "[", "rule", "]", "[", "'edges'", "]", "=", "normalized_entity", ".", "edges", "\n", "name_rules", "[", "rule", "]", "[", "'root'", "]", "=", "normalized_entity", ".", "root", "\n", "", "", "for", "rule", "in", "name_rules", ":", "\n", "        ", "if", "verbose", ":", "\n", "            ", "print", "(", "'[names rule]'", ",", "rule", ")", "\n", "", "", "entity_rules_json", "[", "'names'", "]", "=", "name_rules", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.extract_rules.clean_rules": [[250, 262], ["max", "list", "counter.keys", "counter.keys"], "function", ["None"], ["", "def", "clean_rules", "(", "counter", ",", "rule", ")", ":", "\n", "    ", "max_val", "=", "max", "(", "counter", ".", "keys", "(", ")", ",", "key", "=", "lambda", "x", ":", "counter", "[", "x", "]", ")", "\n", "for", "solution", "in", "list", "(", "counter", ".", "keys", "(", ")", ")", ":", "\n", "        ", "if", "solution", "!=", "max_val", ":", "\n", "            ", "del", "counter", "[", "solution", "]", "\n", "del", "rule", "[", "solution", "]", "\n", "", "", "for", "solution", "in", "counter", ":", "\n", "        ", "rule", "[", "'nodes'", "]", "=", "rule", "[", "solution", "]", "[", "'nodes'", "]", "\n", "rule", "[", "'edges'", "]", "=", "rule", "[", "solution", "]", "[", "'edges'", "]", "\n", "rule", "[", "'root'", "]", "=", "rule", "[", "solution", "]", "[", "'root'", "]", "\n", "rule", "[", "'count'", "]", "=", "counter", "[", "solution", "]", "\n", "del", "rule", "[", "solution", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.extract_rules.normalize_entity": [[264, 274], ["transition_amr_parser.amr.AMR", "sorted", "transition_amr_parser.amr.AMR.edges.append", "enumerate", "sorted"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "", "def", "normalize_entity", "(", "root", ",", "nodes", ",", "edges", ")", ":", "\n", "    ", "normalize_ids", "=", "{", "id", ":", "i", "for", "i", ",", "id", "in", "enumerate", "(", "sorted", "(", "nodes", ",", "key", "=", "lambda", "x", ":", "nodes", "[", "x", "]", ")", ")", "}", "\n", "normalized_entity", "=", "AMR", "(", ")", "\n", "for", "n", "in", "nodes", ":", "\n", "        ", "normalized_entity", ".", "nodes", "[", "normalize_ids", "[", "n", "]", "]", "=", "nodes", "[", "n", "]", "\n", "", "for", "s", ",", "r", ",", "t", "in", "edges", ":", "\n", "        ", "normalized_entity", ".", "edges", ".", "append", "(", "(", "normalize_ids", "[", "s", "]", ",", "r", ",", "normalize_ids", "[", "t", "]", ")", ")", "\n", "", "normalized_entity", ".", "edges", "=", "sorted", "(", "normalized_entity", ".", "edges", ")", "\n", "normalized_entity", ".", "root", "=", "normalize_ids", "[", "root", "]", "\n", "return", "normalized_entity", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.extract_rules.main": [[276, 328], ["transition_amr_parser.amr.JAMR_CorpusReader", "transition_amr_parser.amr.JAMR_CorpusReader.load_amrs", "extract_rules.create_fixed_rules", "extract_rules.create_var_rules", "extract_rules.create_name_rules", "extract_rules.create_date_entity_rules", "extract_rules.create_normalization_rules", "print", "open", "json.dump", "print", "print", "print", "print", "print", "print", "amr.alignmentsToken2Node", "amr.findSubGraph", "all_entities.append", "len", "len", "len", "len", "sum", "len", "len", "len", "len", "entity_rules_json[].values"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.JAMR_CorpusReader.load_amrs", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.extract_rules.create_fixed_rules", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.extract_rules.create_var_rules", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.extract_rules.create_name_rules", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.extract_rules.create_date_entity_rules", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.extract_rules.create_normalization_rules", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.Stack.dump", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.alignmentsToken2Node", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.findSubGraph", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "main", "(", ")", ":", "\n", "    ", "cr", "=", "JAMR_CorpusReader", "(", ")", "\n", "cr", ".", "load_amrs", "(", "sys", ".", "argv", "[", "1", "]", ",", "verbose", "=", "False", ")", "\n", "\n", "all_entities", "=", "[", "]", "\n", "for", "amr", "in", "cr", ".", "amrs", ":", "\n", "        ", "for", "node_id", "in", "amr", ".", "alignments", ":", "\n", "\n", "# get entity info", "\n", "            ", "token_ids", "=", "amr", ".", "alignments", "[", "node_id", "]", "\n", "if", "not", "token_ids", ":", "\n", "                ", "continue", "\n", "", "nodes", "=", "amr", ".", "alignmentsToken2Node", "(", "token_ids", "[", "0", "]", ")", "\n", "if", "len", "(", "nodes", ")", "<=", "1", ":", "\n", "                ", "continue", "\n", "", "entity_sg", "=", "amr", ".", "findSubGraph", "(", "nodes", ")", "\n", "root", "=", "entity_sg", ".", "root", "\n", "if", "not", "node_id", "==", "root", ":", "\n", "                ", "continue", "\n", "", "edges", "=", "entity_sg", ".", "edges", "\n", "if", "not", "edges", ":", "\n", "                ", "continue", "\n", "", "if", "len", "(", "edges", ")", "==", "1", "and", "edges", "[", "0", "]", "[", "1", "]", "in", "[", "':polarity'", ",", "':mode'", "]", ":", "\n", "                ", "continue", "\n", "\n", "", "tokens", "=", "[", "amr", ".", "tokens", "[", "t", "-", "1", "]", "for", "t", "in", "token_ids", "if", "0", "<=", "t", "<=", "len", "(", "amr", ".", "tokens", ")", "]", "\n", "final_nodes", "=", "[", "n", "for", "n", "in", "nodes", "if", "not", "[", "e", "for", "e", "in", "edges", "if", "e", "[", "0", "]", "==", "n", "]", "]", "\n", "\n", "entity_type", "=", "[", "amr", ".", "nodes", "[", "id", "]", "for", "id", "in", "nodes", "if", "id", "not", "in", "final_nodes", "]", "\n", "entity_type", "=", "','", ".", "join", "(", "entity_type", ")", "\n", "\n", "nodes", "=", "{", "n", ":", "amr", ".", "nodes", "[", "n", "]", "for", "n", "in", "nodes", "}", "\n", "all_entities", ".", "append", "(", "(", "amr", ",", "entity_type", ",", "tokens", ",", "root", ",", "nodes", ",", "edges", ")", ")", "\n", "\n", "", "", "create_fixed_rules", "(", "all_entities", ")", "\n", "create_var_rules", "(", "all_entities", ")", "\n", "create_name_rules", "(", "all_entities", ")", "\n", "create_date_entity_rules", "(", "all_entities", ")", "\n", "create_normalization_rules", "(", ")", "\n", "\n", "if", "verbose", ":", "\n", "        ", "print", "(", "'[entity rules] Writing rules'", ")", "\n", "", "frules_out", "=", "sys", ".", "argv", "[", "2", "]", "\n", "with", "open", "(", "frules_out", ",", "'w+'", ",", "encoding", "=", "'utf8'", ")", "as", "f", ":", "\n", "        ", "JSON", ".", "dump", "(", "entity_rules_json", ",", "f", ",", "sort_keys", "=", "True", ")", "\n", "", "if", "verbose", ":", "\n", "        ", "print", "(", "'[entity rules] Fixed:'", ",", "len", "(", "entity_rules_json", "[", "'fixed'", "]", ")", ")", "\n", "print", "(", "'[entity rules] Variable:'", ",", "len", "(", "entity_rules_json", "[", "'var'", "]", ")", ")", "\n", "print", "(", "'[entity rules] Date-entity:'", ",", "len", "(", "entity_rules_json", "[", "'date-entity'", "]", ")", ")", "\n", "print", "(", "'[entity rules] Named entity:'", ",", "len", "(", "entity_rules_json", "[", "'names'", "]", ")", ")", "\n", "print", "(", "'[entity rules] Normalize:'", ",", "sum", "(", "len", "(", "x", ")", "for", "x", "in", "entity_rules_json", "[", "'normalize'", "]", ".", "values", "(", ")", ")", ")", "\n", "print", "(", "'[entity rules] Done'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.extract_rules.normalize": [[330, 362], ["string.lower", "NUM_RE.match", "string.lower.replace().replace().replace().replace().replace", "str", "string.lower.endswith", "str", "str", "str", "str", "string.lower.endswith", "str", "units.values", "string.endswith", "len", "units.values", "string.lower.replace().replace().replace().replace", "string.lower.replace().replace().replace", "string.lower.replace().replace", "string.lower.replace"], "function", ["None"], ["", "", "def", "normalize", "(", "string", ")", ":", "\n", "    ", "lstring", "=", "string", ".", "lower", "(", ")", "\n", "\n", "# number or ordinal", "\n", "if", "NUM_RE", ".", "match", "(", "lstring", ")", ":", "\n", "        ", "return", "lstring", ".", "replace", "(", "','", ",", "''", ")", ".", "replace", "(", "'st'", ",", "''", ")", ".", "replace", "(", "'nd'", ",", "''", ")", ".", "replace", "(", "'rd'", ",", "''", ")", ".", "replace", "(", "'th'", ",", "''", ")", "\n", "\n", "# months", "\n", "", "if", "lstring", "in", "months", ":", "\n", "        ", "return", "str", "(", "months", "[", "lstring", "]", ")", "\n", "", "if", "len", "(", "lstring", ")", "==", "4", "and", "lstring", ".", "endswith", "(", "'.'", ")", "and", "lstring", "[", ":", "3", "]", "in", "months", ":", "\n", "        ", "return", "str", "(", "months", "[", "lstring", "[", ":", "3", "]", "]", ")", "\n", "\n", "# cardinal numbers", "\n", "", "if", "lstring", "in", "cardinals", ":", "\n", "        ", "return", "str", "(", "cardinals", "[", "lstring", "]", ")", "\n", "\n", "# ordinal numbers", "\n", "", "if", "lstring", "in", "ordinals", ":", "\n", "        ", "return", "str", "(", "ordinals", "[", "lstring", "]", ")", "\n", "\n", "# unit abbreviations", "\n", "", "if", "lstring", "in", "units", ":", "\n", "        ", "return", "str", "(", "units", "[", "lstring", "]", ")", "\n", "", "if", "lstring", ".", "endswith", "(", "'s'", ")", "and", "lstring", "[", ":", "-", "1", "]", "in", "units", ":", "\n", "        ", "return", "str", "(", "units", "[", "lstring", "[", ":", "-", "1", "]", "]", ")", "\n", "", "if", "lstring", "in", "units", ".", "values", "(", ")", ":", "\n", "        ", "return", "lstring", "\n", "", "if", "string", ".", "endswith", "(", "'s'", ")", "and", "lstring", "[", ":", "-", "1", "]", "in", "units", ".", "values", "(", ")", ":", "\n", "        ", "return", "lstring", "[", ":", "-", "1", "]", "\n", "\n", "", "return", "'\"'", "+", "string", "+", "'\"'", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.extract_rules.create_normalization_rules": [[364, 376], ["print", "print", "print", "print"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["", "def", "create_normalization_rules", "(", ")", ":", "\n", "    ", "entity_rules_json", "[", "'normalize'", "]", "=", "{", "}", "\n", "entity_rules_json", "[", "'normalize'", "]", "[", "'months'", "]", "=", "months", "\n", "entity_rules_json", "[", "'normalize'", "]", "[", "'units'", "]", "=", "units", "\n", "entity_rules_json", "[", "'normalize'", "]", "[", "'cardinals'", "]", "=", "cardinals", "\n", "entity_rules_json", "[", "'normalize'", "]", "[", "'ordinals'", "]", "=", "ordinals", "\n", "entity_rules_json", "[", "'normalize'", "]", "[", "'decades'", "]", "=", "decades", "\n", "entity_rules_json", "[", "'normalize'", "]", "[", "'countries'", "]", "=", "countries", "\n", "print", "(", "'[normalize rules] months'", ")", "\n", "print", "(", "'[normalize rules] units'", ")", "\n", "print", "(", "'[normalize rules] cardinals'", ")", "\n", "print", "(", "'[normalize rules] ordinals'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.unaligned_node_analysis.main": [[6, 19], ["amr.JAMR_CorpusReader", "amr.JAMR_CorpusReader.load_amrs", "collections.Counter", "sorted", "print", "special.strip"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.JAMR_CorpusReader.load_amrs", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["def", "main", "(", ")", ":", "\n", "    ", "cr", "=", "JAMR_CorpusReader", "(", ")", "\n", "cr", ".", "load_amrs", "(", "sys", ".", "argv", "[", "1", "]", ",", "verbose", "=", "False", ")", "\n", "\n", "special_alignments", "=", "Counter", "(", ")", "\n", "\n", "for", "amr", "in", "cr", ".", "amrs", ":", "\n", "        ", "for", "node_id", "in", "amr", ".", "nodes", ":", "\n", "            ", "if", "node_id", "not", "in", "amr", ".", "alignments", "or", "not", "amr", ".", "alignments", "[", "node_id", "]", ":", "\n", "                ", "special_alignments", "[", "amr", ".", "nodes", "[", "node_id", "]", "]", "+=", "1", "\n", "\n", "", "", "", "for", "special", "in", "sorted", "(", "special_alignments", ",", "reverse", "=", "True", ",", "key", "=", "lambda", "x", ":", "special_alignments", "[", "x", "]", ")", ":", "\n", "        ", "print", "(", "special", ".", "strip", "(", ")", ",", "special_alignments", "[", "special", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.multi_node_analysis.main": [[8, 46], ["amr.JAMR_CorpusReader", "amr.JAMR_CorpusReader.load_amrs", "sorted", "print", "print", "print", "amr.alignmentsToken2Node", "amr.findSubGraph", "str", "sum", "special_alignments[].most_common", "len", "re.match", "amr.findSubGraph.nodes[].endswith", "collections.Counter", "sum", "special_alignments[].values", "len", "special_alignments[].values", "len"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.JAMR_CorpusReader.load_amrs", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.alignmentsToken2Node", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.findSubGraph"], ["def", "main", "(", ")", ":", "\n", "    ", "cr", "=", "JAMR_CorpusReader", "(", ")", "\n", "cr", ".", "load_amrs", "(", "sys", ".", "argv", "[", "1", "]", ",", "verbose", "=", "False", ")", "\n", "\n", "special_alignments", "=", "{", "}", "\n", "\n", "for", "amr", "in", "cr", ".", "amrs", ":", "\n", "        ", "for", "node_id", "in", "amr", ".", "alignments", ":", "\n", "            ", "aligned_token_ids", "=", "amr", ".", "alignments", "[", "node_id", "]", "\n", "aligned_node_ids", "=", "amr", ".", "alignmentsToken2Node", "(", "aligned_token_ids", "[", "0", "]", ")", "\n", "aligned_node_ids", "=", "[", "id", "for", "id", "in", "aligned_node_ids", "if", "'\"'", "not", "in", "amr", ".", "nodes", "[", "id", "]", "]", "\n", "if", "len", "(", "aligned_node_ids", ")", "<=", "1", ":", "\n", "                ", "continue", "\n", "\n", "", "subgraph", "=", "amr", ".", "findSubGraph", "(", "aligned_node_ids", ")", "\n", "# normalize named entities", "\n", "if", "len", "(", "subgraph", ".", "edges", ")", "==", "1", "and", "subgraph", ".", "edges", "[", "0", "]", "[", "1", "]", "==", "':name'", ":", "\n", "                ", "subgraph", ".", "nodes", "[", "subgraph", ".", "root", "]", "=", "'[entity]'", "\n", "# normalize numbers", "\n", "", "for", "n", "in", "subgraph", ".", "nodes", ":", "\n", "                ", "if", "re", ".", "match", "(", "'[0-9]+'", ",", "subgraph", ".", "nodes", "[", "n", "]", ")", ":", "\n", "                    ", "subgraph", ".", "nodes", "[", "n", "]", "=", "'[NUM]'", "\n", "", "if", "subgraph", ".", "nodes", "[", "n", "]", ".", "endswith", "(", "'quantity'", ")", ":", "\n", "                    ", "subgraph", ".", "nodes", "[", "n", "]", "=", "'[quantity]'", "\n", "# if subgraph.nodes[n].endswith('entity'):", "\n", "#     subgraph.nodes[n] = '[value]'", "\n", "", "", "aligned_subgraph", "=", "str", "(", "subgraph", ")", "\n", "\n", "aligned_tokens", "=", "' '", ".", "join", "(", "amr", ".", "tokens", "[", "x", "]", "for", "x", "in", "aligned_token_ids", "if", "x", "<", "len", "(", "amr", ".", "tokens", ")", ")", "\n", "\n", "if", "aligned_subgraph", "not", "in", "special_alignments", ":", "\n", "                ", "special_alignments", "[", "aligned_subgraph", "]", "=", "Counter", "(", ")", "\n", "", "special_alignments", "[", "aligned_subgraph", "]", "[", "aligned_tokens", "]", "+=", "1", "\n", "\n", "", "", "for", "special", "in", "sorted", "(", "special_alignments", ",", "key", "=", "lambda", "x", ":", "sum", "(", "special_alignments", "[", "x", "]", ".", "values", "(", ")", ")", ",", "reverse", "=", "True", ")", ":", "\n", "        ", "print", "(", "special", ",", "sum", "(", "special_alignments", "[", "special", "]", ".", "values", "(", ")", ")", ")", "\n", "print", "(", "special_alignments", "[", "special", "]", ".", "most_common", "(", "10", ")", ")", "\n", "print", "(", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.con_parsing_score.argument_parser": [[8, 37], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.con_parsing_score.compute_correct": [[39, 148], ["reversed", "reversed", "range", "con_parsing_score.compute_correct.clean_tag"], "function", ["None"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.vimdiff_amr_files.get_one_amr": [[5, 12], ["fid.readline", "fid.readline.strip", "amr.append", "fid.readline"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["def", "get_one_amr", "(", "fid", ")", ":", "\n", "    ", "amr", "=", "[", "]", "\n", "line", "=", "fid", ".", "readline", "(", ")", "\n", "while", "line", ".", "strip", "(", ")", ":", "\n", "        ", "amr", ".", "append", "(", "line", ")", "\n", "line", "=", "fid", ".", "readline", "(", ")", "\n", "", "return", "amr", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.vimdiff_amr_files.write": [[14, 17], ["open", "fid.write"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write"], ["", "def", "write", "(", "file_name", ",", "content", ")", ":", "\n", "    ", "with", "open", "(", "file_name", ",", "'w'", ")", "as", "fid", ":", "\n", "        ", "fid", ".", "write", "(", "content", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.unparsable_amrs_analysis.get_token": [[6, 11], ["len"], "function", ["None"], ["def", "get_token", "(", "gold_amr", ",", "t", ")", ":", "\n", "    ", "if", "0", "<=", "t", "-", "1", "<", "len", "(", "gold_amr", ".", "tokens", ")", ":", "\n", "        ", "return", "gold_amr", ".", "tokens", "[", "t", "-", "1", "]", "\n", "", "else", ":", "\n", "        ", "return", "'NA'", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_dictionary.is_next_open_bracket": [[1, 8], ["IndexError"], "function", ["None"], ["def", "is_next_open_bracket", "(", "line", ",", "start_idx", ")", ":", "\n", "    ", "for", "char", "in", "line", "[", "(", "start_idx", "+", "1", ")", ":", "]", ":", "\n", "        ", "if", "char", "==", "'('", ":", "\n", "            ", "return", "True", "\n", "", "elif", "char", "==", "')'", ":", "\n", "            ", "return", "False", "\n", "", "", "raise", "IndexError", "(", "'Bracket possibly not balanced, open bracket not followed by closed bracket'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_dictionary.get_between_brackets": [[9, 17], ["output.append"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "get_between_brackets", "(", "line", ",", "start_idx", ")", ":", "\n", "    ", "output", "=", "[", "]", "\n", "for", "char", "in", "line", "[", "(", "start_idx", "+", "1", ")", ":", "]", ":", "\n", "        ", "if", "char", "==", "')'", ":", "\n", "            ", "break", "\n", "", "assert", "not", "(", "char", "==", "'('", ")", "\n", "output", ".", "append", "(", "char", ")", "\n", "", "return", "''", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_dictionary.get_dict": [[18, 43], ["line.rstrip", "range", "terminal.split", "len", "len", "words_list.append", "output.append", "get_dictionary.is_next_open_bracket", "get_dictionary.get_between_brackets"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.linearization.constituent_dict.is_next_open_bracket", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.linearization.constituent_dict.get_between_brackets"], ["", "def", "get_dict", "(", "lines", ")", ":", "\n", "    ", "output", "=", "[", "]", "\n", "for", "line", "in", "lines", ":", "\n", "#print 'curr line', line_strip", "\n", "        ", "line_strip", "=", "line", ".", "rstrip", "(", ")", "\n", "#print 'length of the sentence', len(line_strip)", "\n", "for", "i", "in", "range", "(", "len", "(", "line_strip", ")", ")", ":", "\n", "            ", "if", "i", "==", "0", ":", "\n", "                ", "assert", "line_strip", "[", "i", "]", "==", "'('", "\n", "", "if", "line_strip", "[", "i", "]", "==", "'('", "and", "not", "(", "is_next_open_bracket", "(", "line_strip", ",", "i", ")", ")", ":", "# fulfilling this condition means this is a terminal symbol", "\n", "                ", "output", ".", "append", "(", "get_between_brackets", "(", "line_strip", ",", "i", ")", ")", "\n", "#print 'output:',output", "\n", "", "", "", "words_dict", "=", "{", "}", "\n", "for", "terminal", "in", "output", ":", "\n", "        ", "terminal_split", "=", "terminal", ".", "split", "(", ")", "\n", "assert", "len", "(", "terminal_split", ")", "==", "2", "# each terminal contains a POS tag and word        ", "\n", "if", "not", "(", "terminal_split", "[", "1", "]", "in", "words_dict", ")", ":", "\n", "            ", "words_dict", "[", "terminal_split", "[", "1", "]", "]", "=", "1", "\n", "", "else", ":", "\n", "            ", "words_dict", "[", "terminal_split", "[", "1", "]", "]", "=", "words_dict", "[", "terminal_split", "[", "1", "]", "]", "+", "1", "\n", "", "", "words_list", "=", "[", "]", "\n", "for", "item", "in", "words_dict", ":", "\n", "        ", "if", "words_dict", "[", "item", "]", ">", "1", ":", "\n", "            ", "words_list", ".", "append", "(", "item", ")", "\n", "", "", "return", "words_list", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.constituent_dict.is_next_open_bracket": [[1, 8], ["IndexError"], "function", ["None"], ["def", "is_next_open_bracket", "(", "line", ",", "start_idx", ")", ":", "\n", "    ", "for", "char", "in", "line", "[", "(", "start_idx", "+", "1", ")", ":", "]", ":", "\n", "        ", "if", "char", "==", "'('", ":", "\n", "            ", "return", "True", "\n", "", "elif", "char", "==", "')'", ":", "\n", "            ", "return", "False", "\n", "", "", "raise", "IndexError", "(", "'Bracket possibly not balanced, open bracket not followed by closed bracket'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.constituent_dict.get_between_brackets": [[9, 17], ["output.append"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "get_between_brackets", "(", "line", ",", "start_idx", ")", ":", "\n", "    ", "output", "=", "[", "]", "\n", "for", "char", "in", "line", "[", "(", "start_idx", "+", "1", ")", ":", "]", ":", "\n", "        ", "if", "char", "==", "')'", ":", "\n", "            ", "break", "\n", "", "assert", "not", "(", "char", "==", "'('", ")", "\n", "output", ".", "append", "(", "char", ")", "\n", "", "return", "''", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.constituent_dict.get_dict": [[18, 43], ["line.rstrip", "range", "terminal.split", "len", "len", "words_list.append", "output.append", "constituent_dict.is_next_open_bracket", "constituent_dict.get_between_brackets"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.linearization.constituent_dict.is_next_open_bracket", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.linearization.constituent_dict.get_between_brackets"], ["", "def", "get_dict", "(", "lines", ")", ":", "\n", "    ", "output", "=", "[", "]", "\n", "for", "line", "in", "lines", ":", "\n", "#print 'curr line', line_strip", "\n", "        ", "line_strip", "=", "line", ".", "rstrip", "(", ")", "\n", "#print 'length of the sentence', len(line_strip)", "\n", "for", "i", "in", "range", "(", "len", "(", "line_strip", ")", ")", ":", "\n", "            ", "if", "i", "==", "0", ":", "\n", "                ", "assert", "line_strip", "[", "i", "]", "==", "'('", "\n", "", "if", "line_strip", "[", "i", "]", "==", "'('", "and", "not", "(", "is_next_open_bracket", "(", "line_strip", ",", "i", ")", ")", ":", "# fulfilling this condition means this is a terminal symbol", "\n", "                ", "output", ".", "append", "(", "get_between_brackets", "(", "line_strip", ",", "i", ")", ")", "\n", "#print 'output:',output", "\n", "", "", "", "words_dict", "=", "{", "}", "\n", "for", "terminal", "in", "output", ":", "\n", "        ", "terminal_split", "=", "terminal", ".", "split", "(", ")", "\n", "assert", "len", "(", "terminal_split", ")", "==", "2", "# each terminal contains a POS tag and word        ", "\n", "if", "not", "(", "terminal_split", "[", "1", "]", "in", "words_dict", ")", ":", "\n", "            ", "words_dict", "[", "terminal_split", "[", "1", "]", "]", "=", "1", "\n", "", "else", ":", "\n", "            ", "words_dict", "[", "terminal_split", "[", "1", "]", "]", "=", "words_dict", "[", "terminal_split", "[", "1", "]", "]", "+", "1", "\n", "", "", "words_list", "=", "[", "]", "\n", "for", "item", "in", "words_dict", ":", "\n", "        ", "if", "words_dict", "[", "item", "]", ">", "1", ":", "\n", "            ", "words_list", ".", "append", "(", "item", ")", "\n", "", "", "return", "words_list", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_words.unkify": [[5, 74], ["len", "final.append", "token.rstrip", "token.rstrip", "token.rstrip().lower", "ch0.isupper", "final.append", "final.append", "token.rstrip", "final.append", "char.isdigit", "token.rstrip", "token.rstrip", "token.rstrip", "len", "char.isalpha", "ch0.isalpha", "len", "char.islower", "char.isupper"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["def", "unkify", "(", "tokens", ",", "words_dict", ",", "lang", ")", ":", "\n", "    ", "final", "=", "[", "]", "\n", "for", "token", "in", "tokens", ":", "\n", "# only process the train singletons and unknown words", "\n", "        ", "if", "len", "(", "token", ".", "rstrip", "(", ")", ")", "==", "0", ":", "\n", "            ", "final", ".", "append", "(", "'UNK'", ")", "\n", "", "elif", "not", "(", "token", ".", "rstrip", "(", ")", "in", "words_dict", ")", ":", "\n", "            ", "if", "lang", "==", "\"ch\"", ":", "\n", "                ", "final", ".", "append", "(", "'UNK'", ")", "\n", "continue", ";", "\n", "", "numCaps", "=", "0", "\n", "hasDigit", "=", "False", "\n", "hasDash", "=", "False", "\n", "hasLower", "=", "False", "\n", "for", "char", "in", "token", ".", "rstrip", "(", ")", ":", "\n", "                ", "if", "char", ".", "isdigit", "(", ")", ":", "\n", "                    ", "hasDigit", "=", "True", "\n", "", "elif", "char", "==", "'-'", ":", "\n", "                    ", "hasDash", "=", "True", "\n", "", "elif", "char", ".", "isalpha", "(", ")", ":", "\n", "                    ", "if", "char", ".", "islower", "(", ")", ":", "\n", "                        ", "hasLower", "=", "True", "\n", "", "elif", "char", ".", "isupper", "(", ")", ":", "\n", "                        ", "numCaps", "+=", "1", "\n", "", "", "", "result", "=", "'UNK'", "\n", "lower", "=", "token", ".", "rstrip", "(", ")", ".", "lower", "(", ")", "\n", "ch0", "=", "token", ".", "rstrip", "(", ")", "[", "0", "]", "\n", "if", "ch0", ".", "isupper", "(", ")", ":", "\n", "                ", "if", "numCaps", "==", "1", ":", "\n", "                    ", "result", "=", "result", "+", "'-INITC'", "\n", "if", "lower", "in", "words_dict", ":", "\n", "                        ", "result", "=", "result", "+", "'-KNOWNLC'", "\n", "", "", "else", ":", "\n", "                    ", "result", "=", "result", "+", "'-CAPS'", "\n", "", "", "elif", "not", "(", "ch0", ".", "isalpha", "(", ")", ")", "and", "numCaps", ">", "0", ":", "\n", "                ", "result", "=", "result", "+", "'-CAPS'", "\n", "", "elif", "hasLower", ":", "\n", "                ", "result", "=", "result", "+", "'-LC'", "\n", "", "if", "hasDigit", ":", "\n", "                ", "result", "=", "result", "+", "'-NUM'", "\n", "", "if", "hasDash", ":", "\n", "                ", "result", "=", "result", "+", "'-DASH'", "\n", "", "if", "lower", "[", "-", "1", "]", "==", "'s'", "and", "len", "(", "lower", ")", ">=", "3", ":", "\n", "                ", "ch2", "=", "lower", "[", "-", "2", "]", "\n", "if", "not", "(", "ch2", "==", "'s'", ")", "and", "not", "(", "ch2", "==", "'i'", ")", "and", "not", "(", "ch2", "==", "'u'", ")", ":", "\n", "                    ", "result", "=", "result", "+", "'-s'", "\n", "", "", "elif", "len", "(", "lower", ")", ">=", "5", "and", "not", "(", "hasDash", ")", "and", "not", "(", "hasDigit", "and", "numCaps", ">", "0", ")", ":", "\n", "                ", "if", "lower", "[", "-", "2", ":", "]", "==", "'ed'", ":", "\n", "                    ", "result", "=", "result", "+", "'-ed'", "\n", "", "elif", "lower", "[", "-", "3", ":", "]", "==", "'ing'", ":", "\n", "                    ", "result", "=", "result", "+", "'-ing'", "\n", "", "elif", "lower", "[", "-", "3", ":", "]", "==", "'ion'", ":", "\n", "                    ", "result", "=", "result", "+", "'-ion'", "\n", "", "elif", "lower", "[", "-", "2", ":", "]", "==", "'er'", ":", "\n", "                    ", "result", "=", "result", "+", "'-er'", "\n", "", "elif", "lower", "[", "-", "3", ":", "]", "==", "'est'", ":", "\n", "                    ", "result", "=", "result", "+", "'-est'", "\n", "", "elif", "lower", "[", "-", "2", ":", "]", "==", "'ly'", ":", "\n", "                    ", "result", "=", "result", "+", "'-ly'", "\n", "", "elif", "lower", "[", "-", "3", ":", "]", "==", "'ity'", ":", "\n", "                    ", "result", "=", "result", "+", "'-ity'", "\n", "", "elif", "lower", "[", "-", "1", "]", "==", "'y'", ":", "\n", "                    ", "result", "=", "result", "+", "'-y'", "\n", "", "elif", "lower", "[", "-", "2", ":", "]", "==", "'al'", ":", "\n", "                    ", "result", "=", "result", "+", "'-al'", "\n", "", "", "final", ".", "append", "(", "result", ")", "\n", "", "else", ":", "\n", "            ", "final", ".", "append", "(", "token", ".", "rstrip", "(", ")", ")", "\n", "", "", "return", "final", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_words.is_next_open_bracket": [[75, 82], ["IndexError"], "function", ["None"], ["", "def", "is_next_open_bracket", "(", "line", ",", "start_idx", ")", ":", "\n", "    ", "for", "char", "in", "line", "[", "(", "start_idx", "+", "1", ")", ":", "]", ":", "\n", "        ", "if", "char", "==", "'('", ":", "\n", "            ", "return", "True", "\n", "", "elif", "char", "==", "')'", ":", "\n", "            ", "return", "False", "\n", "", "", "raise", "IndexError", "(", "'Bracket possibly not balanced, open bracket not followed by closed bracket'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_words.get_between_brackets": [[83, 91], ["output.append"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "get_between_brackets", "(", "line", ",", "start_idx", ")", ":", "\n", "    ", "output", "=", "[", "]", "\n", "for", "char", "in", "line", "[", "(", "start_idx", "+", "1", ")", ":", "]", ":", "\n", "        ", "if", "char", "==", "')'", ":", "\n", "            ", "break", "\n", "", "assert", "not", "(", "char", "==", "'('", ")", "\n", "output", ".", "append", "(", "char", ")", "\n", "", "return", "''", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_words.get_tags_tokens_lowercase": [[101, 122], ["line.rstrip", "range", "len", "terminal.split", "output_tags.append", "output_tokens.append", "output_lowercase.append", "output.append", "len", "terminal_split[].lower", "get_words.is_next_open_bracket", "get_words.get_between_brackets"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.linearization.constituent_dict.is_next_open_bracket", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.linearization.constituent_dict.get_between_brackets"], ["", "def", "get_tags_tokens_lowercase", "(", "line", ")", ":", "\n", "    ", "output", "=", "[", "]", "\n", "#print 'curr line', line_strip", "\n", "line_strip", "=", "line", ".", "rstrip", "(", ")", "\n", "#print 'length of the sentence', len(line_strip)", "\n", "for", "i", "in", "range", "(", "len", "(", "line_strip", ")", ")", ":", "\n", "        ", "if", "i", "==", "0", ":", "\n", "            ", "assert", "line_strip", "[", "i", "]", "==", "'('", "\n", "", "if", "line_strip", "[", "i", "]", "==", "'('", "and", "not", "(", "is_next_open_bracket", "(", "line_strip", ",", "i", ")", ")", ":", "# fulfilling this condition means this is a terminal symbol", "\n", "            ", "output", ".", "append", "(", "get_between_brackets", "(", "line_strip", ",", "i", ")", ")", "\n", "#print 'output:',output", "\n", "", "", "output_tags", "=", "[", "]", "\n", "output_tokens", "=", "[", "]", "\n", "output_lowercase", "=", "[", "]", "\n", "for", "terminal", "in", "output", ":", "\n", "        ", "terminal_split", "=", "terminal", ".", "split", "(", ")", "\n", "assert", "len", "(", "terminal_split", ")", "==", "2", "# each terminal contains a POS tag and word        ", "\n", "output_tags", ".", "append", "(", "terminal_split", "[", "0", "]", ")", "\n", "output_tokens", ".", "append", "(", "terminal_split", "[", "1", "]", ")", "\n", "output_lowercase", ".", "append", "(", "terminal_split", "[", "1", "]", ".", "lower", "(", ")", ")", "\n", "", "return", "[", "output_tags", ",", "output_tokens", ",", "output_lowercase", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_words.get_nonterminal": [[123, 132], ["output.append"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "get_nonterminal", "(", "line", ",", "start_idx", ")", ":", "\n", "    ", "assert", "line", "[", "start_idx", "]", "==", "'('", "# make sure it's an open bracket", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "line", "[", "(", "start_idx", "+", "1", ")", ":", "]", ":", "\n", "        ", "if", "char", "==", "' '", ":", "\n", "            ", "break", "\n", "", "assert", "not", "(", "char", "==", "'('", ")", "and", "not", "(", "char", "==", "')'", ")", "\n", "output", ".", "append", "(", "char", ")", "\n", "", "return", "''", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_words.get_word": [[133, 144], ["output.append"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "get_word", "(", "line", ",", "start_idx", ")", ":", "\n", "#assert line[start_idx] == '(' # make sure it's an open bracket", "\n", "    ", "output", "=", "[", "]", "\n", "for", "char", "in", "line", "[", "(", "start_idx", "+", "1", ")", ":", "]", ":", "\n", "        ", "if", "char", "==", "')'", ":", "\n", "            ", "break", "\n", "", "if", "char", "==", "' '", ":", "\n", "            ", "char", "=", "'\\t'", "\n", "#assert not(char == '(') and not(char == ')')", "\n", "", "output", ".", "append", "(", "char", ")", "\n", "", "return", "''", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_words.get_actions": [[145, 207], ["line.rstrip", "len", "get_words.is_next_open_bracket", "output_actions.append", "get_words.get_nonterminal", "output_actions.append", "get_words.get_word", "output_actions.append", "get_word.split", "a1.split", "a1.split", "int", "output_actions.append", "output_actions.append"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.linearization.constituent_dict.is_next_open_bracket", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_inorder_oracle_SWAP.get_nonterminal", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_inorder_oracle_SWAP.get_word", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "get_actions", "(", "line", ")", ":", "\n", "    ", "output_actions", "=", "[", "]", "\n", "line_strip", "=", "line", ".", "rstrip", "(", ")", "\n", "i", "=", "0", "\n", "sentence_order", "=", "0", "\n", "stack", "=", "[", "]", "\n", "max_idx", "=", "(", "len", "(", "line_strip", ")", "-", "1", ")", "\n", "while", "i", "<=", "max_idx", ":", "\n", "        ", "assert", "line_strip", "[", "i", "]", "==", "'('", "or", "line_strip", "[", "i", "]", "==", "')'", "\n", "if", "line_strip", "[", "i", "]", "==", "'('", ":", "\n", "            ", "if", "is_next_open_bracket", "(", "line_strip", ",", "i", ")", ":", "# open non-terminal", "\n", "                ", "curr_NT", "=", "get_nonterminal", "(", "line_strip", ",", "i", ")", "\n", "output_actions", ".", "append", "(", "'NT('", "+", "curr_NT", "+", "')'", ")", "\n", "i", "+=", "1", "\n", "while", "line_strip", "[", "i", "]", "!=", "'('", ":", "# get the next open bracket, which may be a terminal or another non-terminal", "\n", "                    ", "i", "+=", "1", "\n", "", "", "else", ":", "# it's a terminal symbol", "\n", "                ", "word", "=", "get_word", "(", "line_strip", ",", "i", ")", "\n", "#print(word)", "\n", "a1", "=", "word", ".", "split", "(", "'\\t'", ")", "[", "1", "]", "\n", "iw1", "=", "a1", ".", "split", "(", "'='", ")", "[", "0", "]", "\n", "w1", "=", "a1", ".", "split", "(", "'='", ")", "[", "1", "]", "\n", "#print('-------')", "\n", "#print(sentence_order,' ',iw1,w1)", "\n", "#print(stack)", "\n", "output_actions", ".", "append", "(", "'SHIFT'", ")", "\n", "if", "int", "(", "iw1", ")", ">", "sentence_order", ":", "\n", "#stack.append(sentence_order)", "\n", "\n", "#output_actions.append('SHIFT'+str(sentence_order))", "\n", "#for x in range(int(iw1)-sentence_order):", "\n", "#if True:", "\n", "#stack.append(sentence_order)", "\n", "#if w1 == \".\" or w1 == \",\":", "\n", "#    output_actions.append('SHIFT('+w1+')')", "\n", "#else:", "\n", "#output_actions.append('SHIFT'+iw1)", "\n", "#output_actions.append('SWAP'+str(sentence_order))", "\n", "                    ", "output_actions", ".", "append", "(", "'SHIFT'", ")", "\n", "", "else", ":", "\n", "#stack.append(iw1)", "\n", "                    ", "sentence_order", "+=", "1", "\n", "#if w1 == \".\" or w1 == \",\":", "\n", "#     output_actions.append('SHIFT('+w1+')')", "\n", "#else:", "\n", "output_actions", ".", "append", "(", "'SHIFT-REGULAR'", "+", "iw1", ")", "\n", "\n", "#output_actions.append('SHIFT')", "\n", "", "while", "line_strip", "[", "i", "]", "!=", "')'", ":", "\n", "                    ", "i", "+=", "1", "\n", "", "i", "+=", "1", "\n", "while", "line_strip", "[", "i", "]", "!=", "')'", "and", "line_strip", "[", "i", "]", "!=", "'('", ":", "\n", "                    ", "i", "+=", "1", "\n", "", "", "", "else", ":", "\n", "             ", "output_actions", ".", "append", "(", "'REDUCE'", ")", "\n", "if", "i", "==", "max_idx", ":", "\n", "                 ", "break", "\n", "", "i", "+=", "1", "\n", "while", "line_strip", "[", "i", "]", "!=", "')'", "and", "line_strip", "[", "i", "]", "!=", "'('", ":", "\n", "                 ", "i", "+=", "1", "\n", "", "", "", "assert", "i", "==", "max_idx", "\n", "return", "output_actions", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_words.construct": [[208, 225], ["len", "get_words.construct", "trees.append", "trees.append", "trees.append"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_inorder_oracle_SWAP.construct", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "construct", "(", "actions", ",", "trees", ")", ":", "\n", "    ", "while", "len", "(", "actions", ")", ">", "0", ":", "\n", "        ", "act", "=", "actions", "[", "0", "]", "\n", "actions", "=", "actions", "[", "1", ":", "]", "\n", "if", "act", "[", "0", "]", "==", "'N'", ":", "\n", "            ", "tree", "=", "[", "act", "]", "\n", "actions", ",", "tree", "=", "construct", "(", "actions", ",", "tree", ")", "\n", "trees", ".", "append", "(", "tree", ")", "\n", "", "elif", "act", "[", "0", "]", "==", "'S'", "and", "act", "[", "1", "]", "==", "'H'", ":", "\n", "            ", "trees", ".", "append", "(", "act", ")", "\n", "", "elif", "act", "[", "0", "]", "==", "'S'", "and", "act", "[", "1", "]", "==", "'W'", ":", "\n", "            ", "trees", ".", "append", "(", "act", ")", "\n", "", "elif", "act", "[", "0", "]", "==", "'R'", ":", "\n", "            ", "break", ";", "\n", "", "else", ":", "\n", "            ", "assert", "False", "\n", "", "", "return", "actions", ",", "trees", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_words.get_actions2": [[226, 242], ["get_actions2.append", "get_actions2.append", "type", "get_words.get_actions2", "get_actions2.append", "type", "type", "get_words.get_actions2", "get_actions2.append"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_inorder_oracle_SWAP.get_actions2", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_inorder_oracle_SWAP.get_actions2", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "get_actions2", "(", "trees", ",", "actions", ")", ":", "\n", "    ", "if", "type", "(", "trees", "[", "1", "]", ")", "==", "list", ":", "#types.ListType:", "\n", "        ", "actions", "=", "get_actions2", "(", "trees", "[", "1", "]", ",", "actions", ")", "\n", "", "else", ":", "\n", "        ", "actions", ".", "append", "(", "trees", "[", "1", "]", ")", "\n", "\n", "", "assert", "type", "(", "trees", "[", "0", "]", ")", "==", "str", "#types.StringType", "\n", "actions", ".", "append", "(", "\"NT\"", "+", "trees", "[", "0", "]", "[", "2", ":", "]", ")", "\n", "\n", "for", "item", "in", "trees", "[", "2", ":", "]", ":", "\n", "        ", "if", "type", "(", "item", ")", "==", "list", ":", "#types.ListType:", "\n", "            ", "actions", "=", "get_actions2", "(", "item", ",", "actions", ")", "\n", "", "else", ":", "\n", "            ", "actions", ".", "append", "(", "item", ")", "\n", "", "", "actions", ".", "append", "(", "\"RE\"", "+", "trees", "[", "0", "]", "[", "2", ":", "]", ")", "\n", "return", "actions", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_words.get_projective_order": [[244, 257], ["dict", "range", "print", "len", "t.split", "t.split", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["", "def", "get_projective_order", "(", "tokens", ")", ":", "\n", "    ", "order", "=", "dict", "(", ")", "\n", "for", "t", "in", "tokens", ":", "\n", "        ", "i", "=", "t", ".", "split", "(", "'='", ")", "[", "0", "]", "\n", "order", "[", "str", "(", "i", ")", "]", "=", "t", ".", "split", "(", "'='", ")", "[", "1", "]", "\n", "\n", "", "cad", "=", "''", "\n", "for", "a", "in", "range", "(", "len", "(", "tokens", ")", ")", ":", "\n", "        ", "if", "cad", "==", "''", ":", "\n", "            ", "cad", "=", "order", "[", "str", "(", "a", ")", "]", "\n", "", "else", ":", "\n", "            ", "cad", "=", "cad", "+", "'\\t'", "+", "order", "[", "str", "(", "a", ")", "]", "\n", "", "", "print", "(", "cad", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_words.main": [[259, 286], ["open", "open.readlines", "open.close", "open", "open.readlines", "open.close", "get_dictionary.get_dict", "len", "NotImplementedError", "get_words.get_tags_tokens_lowercase", "get_words.get_projective_order", "line.count", "line.count", "NotImplementedError", "len", "len", "len", "len", "str"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.readlines", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.GoldMiner.close", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.readlines", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.GoldMiner.close", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.linearization.constituent_dict.get_dict", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_inorder_oracle_SWAP.get_tags_tokens_lowercase", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_inorder_oracle_SWAP.get_projective_order"], ["", "def", "main", "(", ")", ":", "\n", "    ", "if", "len", "(", "sys", ".", "argv", ")", "!=", "4", ":", "\n", "        ", "raise", "NotImplementedError", "(", "'Program only takes three arguments:  en|ch train file and dev file (for vocabulary mapping purposes)'", ")", "\n", "", "assert", "sys", ".", "argv", "[", "1", "]", "==", "\"ch\"", "or", "sys", ".", "argv", "[", "1", "]", "==", "\"en\"", "\n", "\n", "train_file", "=", "open", "(", "sys", ".", "argv", "[", "2", "]", ",", "'r'", ")", "\n", "lines", "=", "train_file", ".", "readlines", "(", ")", "\n", "train_file", ".", "close", "(", ")", "\n", "dev_file", "=", "open", "(", "sys", ".", "argv", "[", "3", "]", ",", "'r'", ")", "\n", "dev_lines", "=", "dev_file", ".", "readlines", "(", ")", "\n", "dev_file", ".", "close", "(", ")", "\n", "words_list", "=", "get_dictionary", ".", "get_dict", "(", "lines", ")", "\n", "line_ctr", "=", "0", "\n", "# get the oracle for the train file", "\n", "for", "line", "in", "dev_lines", ":", "\n", "        ", "line_ctr", "+=", "1", "\n", "# assert that the parenthesis are balanced", "\n", "if", "line", ".", "count", "(", "'('", ")", "!=", "line", ".", "count", "(", "')'", ")", ":", "\n", "            ", "raise", "NotImplementedError", "(", "'Unbalanced number of parenthesis in line '", "+", "str", "(", "line_ctr", ")", ")", "\n", "# first line: the bracketed tree itself itself ", "\n", "#print('# ' + line.rstrip()) #Ponemos ! para que coincida con lo que envio LIU", "\n", "", "tags", ",", "tokens", ",", "lowercase", "=", "get_tags_tokens_lowercase", "(", "line", ")", "\n", "assert", "len", "(", "tags", ")", "==", "len", "(", "tokens", ")", "\n", "assert", "len", "(", "tokens", ")", "==", "len", "(", "lowercase", ")", "\n", "#print(' '.join(tags))", "\n", "#print(' '.join(tokens))", "\n", "get_projective_order", "(", "tokens", ")", "\n", "#print(' '.join(lowercase))", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_tags.unkify": [[5, 74], ["len", "final.append", "token.rstrip", "token.rstrip", "token.rstrip().lower", "ch0.isupper", "final.append", "final.append", "token.rstrip", "final.append", "char.isdigit", "token.rstrip", "token.rstrip", "token.rstrip", "len", "char.isalpha", "ch0.isalpha", "len", "char.islower", "char.isupper"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["def", "unkify", "(", "tokens", ",", "words_dict", ",", "lang", ")", ":", "\n", "    ", "final", "=", "[", "]", "\n", "for", "token", "in", "tokens", ":", "\n", "# only process the train singletons and unknown words", "\n", "        ", "if", "len", "(", "token", ".", "rstrip", "(", ")", ")", "==", "0", ":", "\n", "            ", "final", ".", "append", "(", "'UNK'", ")", "\n", "", "elif", "not", "(", "token", ".", "rstrip", "(", ")", "in", "words_dict", ")", ":", "\n", "            ", "if", "lang", "==", "\"ch\"", ":", "\n", "                ", "final", ".", "append", "(", "'UNK'", ")", "\n", "continue", ";", "\n", "", "numCaps", "=", "0", "\n", "hasDigit", "=", "False", "\n", "hasDash", "=", "False", "\n", "hasLower", "=", "False", "\n", "for", "char", "in", "token", ".", "rstrip", "(", ")", ":", "\n", "                ", "if", "char", ".", "isdigit", "(", ")", ":", "\n", "                    ", "hasDigit", "=", "True", "\n", "", "elif", "char", "==", "'-'", ":", "\n", "                    ", "hasDash", "=", "True", "\n", "", "elif", "char", ".", "isalpha", "(", ")", ":", "\n", "                    ", "if", "char", ".", "islower", "(", ")", ":", "\n", "                        ", "hasLower", "=", "True", "\n", "", "elif", "char", ".", "isupper", "(", ")", ":", "\n", "                        ", "numCaps", "+=", "1", "\n", "", "", "", "result", "=", "'UNK'", "\n", "lower", "=", "token", ".", "rstrip", "(", ")", ".", "lower", "(", ")", "\n", "ch0", "=", "token", ".", "rstrip", "(", ")", "[", "0", "]", "\n", "if", "ch0", ".", "isupper", "(", ")", ":", "\n", "                ", "if", "numCaps", "==", "1", ":", "\n", "                    ", "result", "=", "result", "+", "'-INITC'", "\n", "if", "lower", "in", "words_dict", ":", "\n", "                        ", "result", "=", "result", "+", "'-KNOWNLC'", "\n", "", "", "else", ":", "\n", "                    ", "result", "=", "result", "+", "'-CAPS'", "\n", "", "", "elif", "not", "(", "ch0", ".", "isalpha", "(", ")", ")", "and", "numCaps", ">", "0", ":", "\n", "                ", "result", "=", "result", "+", "'-CAPS'", "\n", "", "elif", "hasLower", ":", "\n", "                ", "result", "=", "result", "+", "'-LC'", "\n", "", "if", "hasDigit", ":", "\n", "                ", "result", "=", "result", "+", "'-NUM'", "\n", "", "if", "hasDash", ":", "\n", "                ", "result", "=", "result", "+", "'-DASH'", "\n", "", "if", "lower", "[", "-", "1", "]", "==", "'s'", "and", "len", "(", "lower", ")", ">=", "3", ":", "\n", "                ", "ch2", "=", "lower", "[", "-", "2", "]", "\n", "if", "not", "(", "ch2", "==", "'s'", ")", "and", "not", "(", "ch2", "==", "'i'", ")", "and", "not", "(", "ch2", "==", "'u'", ")", ":", "\n", "                    ", "result", "=", "result", "+", "'-s'", "\n", "", "", "elif", "len", "(", "lower", ")", ">=", "5", "and", "not", "(", "hasDash", ")", "and", "not", "(", "hasDigit", "and", "numCaps", ">", "0", ")", ":", "\n", "                ", "if", "lower", "[", "-", "2", ":", "]", "==", "'ed'", ":", "\n", "                    ", "result", "=", "result", "+", "'-ed'", "\n", "", "elif", "lower", "[", "-", "3", ":", "]", "==", "'ing'", ":", "\n", "                    ", "result", "=", "result", "+", "'-ing'", "\n", "", "elif", "lower", "[", "-", "3", ":", "]", "==", "'ion'", ":", "\n", "                    ", "result", "=", "result", "+", "'-ion'", "\n", "", "elif", "lower", "[", "-", "2", ":", "]", "==", "'er'", ":", "\n", "                    ", "result", "=", "result", "+", "'-er'", "\n", "", "elif", "lower", "[", "-", "3", ":", "]", "==", "'est'", ":", "\n", "                    ", "result", "=", "result", "+", "'-est'", "\n", "", "elif", "lower", "[", "-", "2", ":", "]", "==", "'ly'", ":", "\n", "                    ", "result", "=", "result", "+", "'-ly'", "\n", "", "elif", "lower", "[", "-", "3", ":", "]", "==", "'ity'", ":", "\n", "                    ", "result", "=", "result", "+", "'-ity'", "\n", "", "elif", "lower", "[", "-", "1", "]", "==", "'y'", ":", "\n", "                    ", "result", "=", "result", "+", "'-y'", "\n", "", "elif", "lower", "[", "-", "2", ":", "]", "==", "'al'", ":", "\n", "                    ", "result", "=", "result", "+", "'-al'", "\n", "", "", "final", ".", "append", "(", "result", ")", "\n", "", "else", ":", "\n", "            ", "final", ".", "append", "(", "token", ".", "rstrip", "(", ")", ")", "\n", "", "", "return", "final", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_tags.is_next_open_bracket": [[75, 82], ["IndexError"], "function", ["None"], ["", "def", "is_next_open_bracket", "(", "line", ",", "start_idx", ")", ":", "\n", "    ", "for", "char", "in", "line", "[", "(", "start_idx", "+", "1", ")", ":", "]", ":", "\n", "        ", "if", "char", "==", "'('", ":", "\n", "            ", "return", "True", "\n", "", "elif", "char", "==", "')'", ":", "\n", "            ", "return", "False", "\n", "", "", "raise", "IndexError", "(", "'Bracket possibly not balanced, open bracket not followed by closed bracket'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_tags.get_between_brackets": [[83, 91], ["output.append"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "get_between_brackets", "(", "line", ",", "start_idx", ")", ":", "\n", "    ", "output", "=", "[", "]", "\n", "for", "char", "in", "line", "[", "(", "start_idx", "+", "1", ")", ":", "]", ":", "\n", "        ", "if", "char", "==", "')'", ":", "\n", "            ", "break", "\n", "", "assert", "not", "(", "char", "==", "'('", ")", "\n", "output", ".", "append", "(", "char", ")", "\n", "", "return", "''", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_tags.get_tags_tokens_lowercase": [[101, 122], ["line.rstrip", "range", "len", "terminal.split", "output_tags.append", "output_tokens.append", "output_lowercase.append", "output.append", "len", "terminal_split[].lower", "get_tags.is_next_open_bracket", "get_tags.get_between_brackets"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.linearization.constituent_dict.is_next_open_bracket", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.linearization.constituent_dict.get_between_brackets"], ["", "def", "get_tags_tokens_lowercase", "(", "line", ")", ":", "\n", "    ", "output", "=", "[", "]", "\n", "#print 'curr line', line_strip", "\n", "line_strip", "=", "line", ".", "rstrip", "(", ")", "\n", "#print 'length of the sentence', len(line_strip)", "\n", "for", "i", "in", "range", "(", "len", "(", "line_strip", ")", ")", ":", "\n", "        ", "if", "i", "==", "0", ":", "\n", "            ", "assert", "line_strip", "[", "i", "]", "==", "'('", "\n", "", "if", "line_strip", "[", "i", "]", "==", "'('", "and", "not", "(", "is_next_open_bracket", "(", "line_strip", ",", "i", ")", ")", ":", "# fulfilling this condition means this is a terminal symbol", "\n", "            ", "output", ".", "append", "(", "get_between_brackets", "(", "line_strip", ",", "i", ")", ")", "\n", "#print 'output:',output", "\n", "", "", "output_tags", "=", "[", "]", "\n", "output_tokens", "=", "[", "]", "\n", "output_lowercase", "=", "[", "]", "\n", "for", "terminal", "in", "output", ":", "\n", "        ", "terminal_split", "=", "terminal", ".", "split", "(", ")", "\n", "assert", "len", "(", "terminal_split", ")", "==", "2", "# each terminal contains a POS tag and word        ", "\n", "output_tags", ".", "append", "(", "terminal_split", "[", "0", "]", ")", "\n", "output_tokens", ".", "append", "(", "terminal_split", "[", "1", "]", ")", "\n", "output_lowercase", ".", "append", "(", "terminal_split", "[", "1", "]", ".", "lower", "(", ")", ")", "\n", "", "return", "[", "output_tags", ",", "output_tokens", ",", "output_lowercase", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_tags.get_nonterminal": [[123, 132], ["output.append"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "get_nonterminal", "(", "line", ",", "start_idx", ")", ":", "\n", "    ", "assert", "line", "[", "start_idx", "]", "==", "'('", "# make sure it's an open bracket", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "line", "[", "(", "start_idx", "+", "1", ")", ":", "]", ":", "\n", "        ", "if", "char", "==", "' '", ":", "\n", "            ", "break", "\n", "", "assert", "not", "(", "char", "==", "'('", ")", "and", "not", "(", "char", "==", "')'", ")", "\n", "output", ".", "append", "(", "char", ")", "\n", "", "return", "''", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_tags.get_word": [[133, 144], ["output.append"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "get_word", "(", "line", ",", "start_idx", ")", ":", "\n", "#assert line[start_idx] == '(' # make sure it's an open bracket", "\n", "    ", "output", "=", "[", "]", "\n", "for", "char", "in", "line", "[", "(", "start_idx", "+", "1", ")", ":", "]", ":", "\n", "        ", "if", "char", "==", "')'", ":", "\n", "            ", "break", "\n", "", "if", "char", "==", "' '", ":", "\n", "            ", "char", "=", "'\\t'", "\n", "#assert not(char == '(') and not(char == ')')", "\n", "", "output", ".", "append", "(", "char", ")", "\n", "", "return", "''", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_tags.get_actions": [[145, 207], ["line.rstrip", "len", "get_tags.is_next_open_bracket", "output_actions.append", "get_tags.get_nonterminal", "output_actions.append", "get_tags.get_word", "output_actions.append", "get_word.split", "a1.split", "a1.split", "int", "output_actions.append", "output_actions.append"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.linearization.constituent_dict.is_next_open_bracket", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_inorder_oracle_SWAP.get_nonterminal", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_inorder_oracle_SWAP.get_word", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "get_actions", "(", "line", ")", ":", "\n", "    ", "output_actions", "=", "[", "]", "\n", "line_strip", "=", "line", ".", "rstrip", "(", ")", "\n", "i", "=", "0", "\n", "sentence_order", "=", "0", "\n", "stack", "=", "[", "]", "\n", "max_idx", "=", "(", "len", "(", "line_strip", ")", "-", "1", ")", "\n", "while", "i", "<=", "max_idx", ":", "\n", "        ", "assert", "line_strip", "[", "i", "]", "==", "'('", "or", "line_strip", "[", "i", "]", "==", "')'", "\n", "if", "line_strip", "[", "i", "]", "==", "'('", ":", "\n", "            ", "if", "is_next_open_bracket", "(", "line_strip", ",", "i", ")", ":", "# open non-terminal", "\n", "                ", "curr_NT", "=", "get_nonterminal", "(", "line_strip", ",", "i", ")", "\n", "output_actions", ".", "append", "(", "'NT('", "+", "curr_NT", "+", "')'", ")", "\n", "i", "+=", "1", "\n", "while", "line_strip", "[", "i", "]", "!=", "'('", ":", "# get the next open bracket, which may be a terminal or another non-terminal", "\n", "                    ", "i", "+=", "1", "\n", "", "", "else", ":", "# it's a terminal symbol", "\n", "                ", "word", "=", "get_word", "(", "line_strip", ",", "i", ")", "\n", "#print(word)", "\n", "a1", "=", "word", ".", "split", "(", "'\\t'", ")", "[", "1", "]", "\n", "iw1", "=", "a1", ".", "split", "(", "'='", ")", "[", "0", "]", "\n", "w1", "=", "a1", ".", "split", "(", "'='", ")", "[", "1", "]", "\n", "#print('-------')", "\n", "#print(sentence_order,' ',iw1,w1)", "\n", "#print(stack)", "\n", "output_actions", ".", "append", "(", "'SHIFT'", ")", "\n", "if", "int", "(", "iw1", ")", ">", "sentence_order", ":", "\n", "#stack.append(sentence_order)", "\n", "\n", "#output_actions.append('SHIFT'+str(sentence_order))", "\n", "#for x in range(int(iw1)-sentence_order):", "\n", "#if True:", "\n", "#stack.append(sentence_order)", "\n", "#if w1 == \".\" or w1 == \",\":", "\n", "#    output_actions.append('SHIFT('+w1+')')", "\n", "#else:", "\n", "#output_actions.append('SHIFT'+iw1)", "\n", "#output_actions.append('SWAP'+str(sentence_order))", "\n", "                    ", "output_actions", ".", "append", "(", "'SHIFT'", ")", "\n", "", "else", ":", "\n", "#stack.append(iw1)", "\n", "                    ", "sentence_order", "+=", "1", "\n", "#if w1 == \".\" or w1 == \",\":", "\n", "#     output_actions.append('SHIFT('+w1+')')", "\n", "#else:", "\n", "output_actions", ".", "append", "(", "'SHIFT-REGULAR'", "+", "iw1", ")", "\n", "\n", "#output_actions.append('SHIFT')", "\n", "", "while", "line_strip", "[", "i", "]", "!=", "')'", ":", "\n", "                    ", "i", "+=", "1", "\n", "", "i", "+=", "1", "\n", "while", "line_strip", "[", "i", "]", "!=", "')'", "and", "line_strip", "[", "i", "]", "!=", "'('", ":", "\n", "                    ", "i", "+=", "1", "\n", "", "", "", "else", ":", "\n", "             ", "output_actions", ".", "append", "(", "'REDUCE'", ")", "\n", "if", "i", "==", "max_idx", ":", "\n", "                 ", "break", "\n", "", "i", "+=", "1", "\n", "while", "line_strip", "[", "i", "]", "!=", "')'", "and", "line_strip", "[", "i", "]", "!=", "'('", ":", "\n", "                 ", "i", "+=", "1", "\n", "", "", "", "assert", "i", "==", "max_idx", "\n", "return", "output_actions", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_tags.construct": [[208, 225], ["len", "get_tags.construct", "trees.append", "trees.append", "trees.append"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_inorder_oracle_SWAP.construct", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "construct", "(", "actions", ",", "trees", ")", ":", "\n", "    ", "while", "len", "(", "actions", ")", ">", "0", ":", "\n", "        ", "act", "=", "actions", "[", "0", "]", "\n", "actions", "=", "actions", "[", "1", ":", "]", "\n", "if", "act", "[", "0", "]", "==", "'N'", ":", "\n", "            ", "tree", "=", "[", "act", "]", "\n", "actions", ",", "tree", "=", "construct", "(", "actions", ",", "tree", ")", "\n", "trees", ".", "append", "(", "tree", ")", "\n", "", "elif", "act", "[", "0", "]", "==", "'S'", "and", "act", "[", "1", "]", "==", "'H'", ":", "\n", "            ", "trees", ".", "append", "(", "act", ")", "\n", "", "elif", "act", "[", "0", "]", "==", "'S'", "and", "act", "[", "1", "]", "==", "'W'", ":", "\n", "            ", "trees", ".", "append", "(", "act", ")", "\n", "", "elif", "act", "[", "0", "]", "==", "'R'", ":", "\n", "            ", "break", ";", "\n", "", "else", ":", "\n", "            ", "assert", "False", "\n", "", "", "return", "actions", ",", "trees", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_tags.get_actions2": [[226, 242], ["get_actions2.append", "get_actions2.append", "type", "get_tags.get_actions2", "get_actions2.append", "type", "type", "get_tags.get_actions2", "get_actions2.append"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_inorder_oracle_SWAP.get_actions2", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_inorder_oracle_SWAP.get_actions2", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "get_actions2", "(", "trees", ",", "actions", ")", ":", "\n", "    ", "if", "type", "(", "trees", "[", "1", "]", ")", "==", "list", ":", "#types.ListType:", "\n", "        ", "actions", "=", "get_actions2", "(", "trees", "[", "1", "]", ",", "actions", ")", "\n", "", "else", ":", "\n", "        ", "actions", ".", "append", "(", "trees", "[", "1", "]", ")", "\n", "\n", "", "assert", "type", "(", "trees", "[", "0", "]", ")", "==", "str", "#types.StringType", "\n", "actions", ".", "append", "(", "\"NT\"", "+", "trees", "[", "0", "]", "[", "2", ":", "]", ")", "\n", "\n", "for", "item", "in", "trees", "[", "2", ":", "]", ":", "\n", "        ", "if", "type", "(", "item", ")", "==", "list", ":", "#types.ListType:", "\n", "            ", "actions", "=", "get_actions2", "(", "item", ",", "actions", ")", "\n", "", "else", ":", "\n", "            ", "actions", ".", "append", "(", "item", ")", "\n", "", "", "actions", ".", "append", "(", "\"RE\"", "+", "trees", "[", "0", "]", "[", "2", ":", "]", ")", "\n", "return", "actions", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_tags.get_projective_order": [[244, 261], ["enumerate", "dict", "range", "print", "len", "t.split", "t.split", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["", "def", "get_projective_order", "(", "tokens", ",", "tags", ")", ":", "\n", "    ", "for", "i", ",", "t", "in", "enumerate", "(", "tags", ")", ":", "\n", "        ", "tokens", "[", "i", "]", "=", "tokens", "[", "i", "]", "+", "'='", "+", "t", "\n", "\n", "\n", "", "order", "=", "dict", "(", ")", "\n", "for", "t", "in", "tokens", ":", "\n", "        ", "i", "=", "t", ".", "split", "(", "'='", ")", "[", "0", "]", "\n", "order", "[", "str", "(", "i", ")", "]", "=", "t", ".", "split", "(", "'='", ")", "[", "2", "]", "\n", "\n", "", "cad", "=", "''", "\n", "for", "a", "in", "range", "(", "len", "(", "tokens", ")", ")", ":", "\n", "        ", "if", "cad", "==", "''", ":", "\n", "            ", "cad", "=", "order", "[", "str", "(", "a", ")", "]", "\n", "", "else", ":", "\n", "            ", "cad", "=", "cad", "+", "'\\t'", "+", "order", "[", "str", "(", "a", ")", "]", "\n", "", "", "print", "(", "cad", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_tags.main": [[263, 290], ["open", "open.readlines", "open.close", "open", "open.readlines", "open.close", "get_dictionary.get_dict", "len", "NotImplementedError", "get_tags.get_tags_tokens_lowercase", "get_tags.get_projective_order", "line.count", "line.count", "NotImplementedError", "len", "len", "len", "len", "str"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.readlines", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.GoldMiner.close", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.readlines", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.GoldMiner.close", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.linearization.constituent_dict.get_dict", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_inorder_oracle_SWAP.get_tags_tokens_lowercase", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_inorder_oracle_SWAP.get_projective_order"], ["", "def", "main", "(", ")", ":", "\n", "    ", "if", "len", "(", "sys", ".", "argv", ")", "!=", "4", ":", "\n", "        ", "raise", "NotImplementedError", "(", "'Program only takes three arguments:  en|ch train file and dev file (for vocabulary mapping purposes)'", ")", "\n", "", "assert", "sys", ".", "argv", "[", "1", "]", "==", "\"ch\"", "or", "sys", ".", "argv", "[", "1", "]", "==", "\"en\"", "\n", "\n", "train_file", "=", "open", "(", "sys", ".", "argv", "[", "2", "]", ",", "'r'", ")", "\n", "lines", "=", "train_file", ".", "readlines", "(", ")", "\n", "train_file", ".", "close", "(", ")", "\n", "dev_file", "=", "open", "(", "sys", ".", "argv", "[", "3", "]", ",", "'r'", ")", "\n", "dev_lines", "=", "dev_file", ".", "readlines", "(", ")", "\n", "dev_file", ".", "close", "(", ")", "\n", "words_list", "=", "get_dictionary", ".", "get_dict", "(", "lines", ")", "\n", "line_ctr", "=", "0", "\n", "# get the oracle for the train file", "\n", "for", "line", "in", "dev_lines", ":", "\n", "        ", "line_ctr", "+=", "1", "\n", "# assert that the parenthesis are balanced", "\n", "if", "line", ".", "count", "(", "'('", ")", "!=", "line", ".", "count", "(", "')'", ")", ":", "\n", "            ", "raise", "NotImplementedError", "(", "'Unbalanced number of parenthesis in line '", "+", "str", "(", "line_ctr", ")", ")", "\n", "# first line: the bracketed tree itself itself ", "\n", "#print('# ' + line.rstrip()) #Ponemos ! para que coincida con lo que envio LIU", "\n", "", "tags", ",", "tokens", ",", "lowercase", "=", "get_tags_tokens_lowercase", "(", "line", ")", "\n", "assert", "len", "(", "tags", ")", "==", "len", "(", "tokens", ")", "\n", "assert", "len", "(", "tokens", ")", "==", "len", "(", "lowercase", ")", "\n", "#print(' '.join(tags))", "\n", "#print(' '.join(tokens))", "\n", "get_projective_order", "(", "tokens", ",", "tags", ")", "\n", "#print(' '.join(lowercase))", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_words_with_index.unkify": [[5, 74], ["len", "final.append", "token.rstrip", "token.rstrip", "token.rstrip().lower", "ch0.isupper", "final.append", "final.append", "token.rstrip", "final.append", "char.isdigit", "token.rstrip", "token.rstrip", "token.rstrip", "len", "char.isalpha", "ch0.isalpha", "len", "char.islower", "char.isupper"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["def", "unkify", "(", "tokens", ",", "words_dict", ",", "lang", ")", ":", "\n", "    ", "final", "=", "[", "]", "\n", "for", "token", "in", "tokens", ":", "\n", "# only process the train singletons and unknown words", "\n", "        ", "if", "len", "(", "token", ".", "rstrip", "(", ")", ")", "==", "0", ":", "\n", "            ", "final", ".", "append", "(", "'UNK'", ")", "\n", "", "elif", "not", "(", "token", ".", "rstrip", "(", ")", "in", "words_dict", ")", ":", "\n", "            ", "if", "lang", "==", "\"ch\"", ":", "\n", "                ", "final", ".", "append", "(", "'UNK'", ")", "\n", "continue", ";", "\n", "", "numCaps", "=", "0", "\n", "hasDigit", "=", "False", "\n", "hasDash", "=", "False", "\n", "hasLower", "=", "False", "\n", "for", "char", "in", "token", ".", "rstrip", "(", ")", ":", "\n", "                ", "if", "char", ".", "isdigit", "(", ")", ":", "\n", "                    ", "hasDigit", "=", "True", "\n", "", "elif", "char", "==", "'-'", ":", "\n", "                    ", "hasDash", "=", "True", "\n", "", "elif", "char", ".", "isalpha", "(", ")", ":", "\n", "                    ", "if", "char", ".", "islower", "(", ")", ":", "\n", "                        ", "hasLower", "=", "True", "\n", "", "elif", "char", ".", "isupper", "(", ")", ":", "\n", "                        ", "numCaps", "+=", "1", "\n", "", "", "", "result", "=", "'UNK'", "\n", "lower", "=", "token", ".", "rstrip", "(", ")", ".", "lower", "(", ")", "\n", "ch0", "=", "token", ".", "rstrip", "(", ")", "[", "0", "]", "\n", "if", "ch0", ".", "isupper", "(", ")", ":", "\n", "                ", "if", "numCaps", "==", "1", ":", "\n", "                    ", "result", "=", "result", "+", "'-INITC'", "\n", "if", "lower", "in", "words_dict", ":", "\n", "                        ", "result", "=", "result", "+", "'-KNOWNLC'", "\n", "", "", "else", ":", "\n", "                    ", "result", "=", "result", "+", "'-CAPS'", "\n", "", "", "elif", "not", "(", "ch0", ".", "isalpha", "(", ")", ")", "and", "numCaps", ">", "0", ":", "\n", "                ", "result", "=", "result", "+", "'-CAPS'", "\n", "", "elif", "hasLower", ":", "\n", "                ", "result", "=", "result", "+", "'-LC'", "\n", "", "if", "hasDigit", ":", "\n", "                ", "result", "=", "result", "+", "'-NUM'", "\n", "", "if", "hasDash", ":", "\n", "                ", "result", "=", "result", "+", "'-DASH'", "\n", "", "if", "lower", "[", "-", "1", "]", "==", "'s'", "and", "len", "(", "lower", ")", ">=", "3", ":", "\n", "                ", "ch2", "=", "lower", "[", "-", "2", "]", "\n", "if", "not", "(", "ch2", "==", "'s'", ")", "and", "not", "(", "ch2", "==", "'i'", ")", "and", "not", "(", "ch2", "==", "'u'", ")", ":", "\n", "                    ", "result", "=", "result", "+", "'-s'", "\n", "", "", "elif", "len", "(", "lower", ")", ">=", "5", "and", "not", "(", "hasDash", ")", "and", "not", "(", "hasDigit", "and", "numCaps", ">", "0", ")", ":", "\n", "                ", "if", "lower", "[", "-", "2", ":", "]", "==", "'ed'", ":", "\n", "                    ", "result", "=", "result", "+", "'-ed'", "\n", "", "elif", "lower", "[", "-", "3", ":", "]", "==", "'ing'", ":", "\n", "                    ", "result", "=", "result", "+", "'-ing'", "\n", "", "elif", "lower", "[", "-", "3", ":", "]", "==", "'ion'", ":", "\n", "                    ", "result", "=", "result", "+", "'-ion'", "\n", "", "elif", "lower", "[", "-", "2", ":", "]", "==", "'er'", ":", "\n", "                    ", "result", "=", "result", "+", "'-er'", "\n", "", "elif", "lower", "[", "-", "3", ":", "]", "==", "'est'", ":", "\n", "                    ", "result", "=", "result", "+", "'-est'", "\n", "", "elif", "lower", "[", "-", "2", ":", "]", "==", "'ly'", ":", "\n", "                    ", "result", "=", "result", "+", "'-ly'", "\n", "", "elif", "lower", "[", "-", "3", ":", "]", "==", "'ity'", ":", "\n", "                    ", "result", "=", "result", "+", "'-ity'", "\n", "", "elif", "lower", "[", "-", "1", "]", "==", "'y'", ":", "\n", "                    ", "result", "=", "result", "+", "'-y'", "\n", "", "elif", "lower", "[", "-", "2", ":", "]", "==", "'al'", ":", "\n", "                    ", "result", "=", "result", "+", "'-al'", "\n", "", "", "final", ".", "append", "(", "result", ")", "\n", "", "else", ":", "\n", "            ", "final", ".", "append", "(", "token", ".", "rstrip", "(", ")", ")", "\n", "", "", "return", "final", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_words_with_index.is_next_open_bracket": [[75, 82], ["IndexError"], "function", ["None"], ["", "def", "is_next_open_bracket", "(", "line", ",", "start_idx", ")", ":", "\n", "    ", "for", "char", "in", "line", "[", "(", "start_idx", "+", "1", ")", ":", "]", ":", "\n", "        ", "if", "char", "==", "'('", ":", "\n", "            ", "return", "True", "\n", "", "elif", "char", "==", "')'", ":", "\n", "            ", "return", "False", "\n", "", "", "raise", "IndexError", "(", "'Bracket possibly not balanced, open bracket not followed by closed bracket'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_words_with_index.get_between_brackets": [[83, 91], ["output.append"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "get_between_brackets", "(", "line", ",", "start_idx", ")", ":", "\n", "    ", "output", "=", "[", "]", "\n", "for", "char", "in", "line", "[", "(", "start_idx", "+", "1", ")", ":", "]", ":", "\n", "        ", "if", "char", "==", "')'", ":", "\n", "            ", "break", "\n", "", "assert", "not", "(", "char", "==", "'('", ")", "\n", "output", ".", "append", "(", "char", ")", "\n", "", "return", "''", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_words_with_index.get_tags_tokens_lowercase": [[101, 122], ["line.rstrip", "range", "len", "terminal.split", "output_tags.append", "output_tokens.append", "output_lowercase.append", "output.append", "len", "terminal_split[].lower", "get_words_with_index.is_next_open_bracket", "get_words_with_index.get_between_brackets"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.linearization.constituent_dict.is_next_open_bracket", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.linearization.constituent_dict.get_between_brackets"], ["", "def", "get_tags_tokens_lowercase", "(", "line", ")", ":", "\n", "    ", "output", "=", "[", "]", "\n", "#print 'curr line', line_strip", "\n", "line_strip", "=", "line", ".", "rstrip", "(", ")", "\n", "#print 'length of the sentence', len(line_strip)", "\n", "for", "i", "in", "range", "(", "len", "(", "line_strip", ")", ")", ":", "\n", "        ", "if", "i", "==", "0", ":", "\n", "            ", "assert", "line_strip", "[", "i", "]", "==", "'('", "\n", "", "if", "line_strip", "[", "i", "]", "==", "'('", "and", "not", "(", "is_next_open_bracket", "(", "line_strip", ",", "i", ")", ")", ":", "# fulfilling this condition means this is a terminal symbol", "\n", "            ", "output", ".", "append", "(", "get_between_brackets", "(", "line_strip", ",", "i", ")", ")", "\n", "#print 'output:',output", "\n", "", "", "output_tags", "=", "[", "]", "\n", "output_tokens", "=", "[", "]", "\n", "output_lowercase", "=", "[", "]", "\n", "for", "terminal", "in", "output", ":", "\n", "        ", "terminal_split", "=", "terminal", ".", "split", "(", ")", "\n", "assert", "len", "(", "terminal_split", ")", "==", "2", "# each terminal contains a POS tag and word        ", "\n", "output_tags", ".", "append", "(", "terminal_split", "[", "0", "]", ")", "\n", "output_tokens", ".", "append", "(", "terminal_split", "[", "1", "]", ")", "\n", "output_lowercase", ".", "append", "(", "terminal_split", "[", "1", "]", ".", "lower", "(", ")", ")", "\n", "", "return", "[", "output_tags", ",", "output_tokens", ",", "output_lowercase", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_words_with_index.get_nonterminal": [[123, 132], ["output.append"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "get_nonterminal", "(", "line", ",", "start_idx", ")", ":", "\n", "    ", "assert", "line", "[", "start_idx", "]", "==", "'('", "# make sure it's an open bracket", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "line", "[", "(", "start_idx", "+", "1", ")", ":", "]", ":", "\n", "        ", "if", "char", "==", "' '", ":", "\n", "            ", "break", "\n", "", "assert", "not", "(", "char", "==", "'('", ")", "and", "not", "(", "char", "==", "')'", ")", "\n", "output", ".", "append", "(", "char", ")", "\n", "", "return", "''", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_words_with_index.get_word": [[133, 144], ["output.append"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "get_word", "(", "line", ",", "start_idx", ")", ":", "\n", "#assert line[start_idx] == '(' # make sure it's an open bracket", "\n", "    ", "output", "=", "[", "]", "\n", "for", "char", "in", "line", "[", "(", "start_idx", "+", "1", ")", ":", "]", ":", "\n", "        ", "if", "char", "==", "')'", ":", "\n", "            ", "break", "\n", "", "if", "char", "==", "' '", ":", "\n", "            ", "char", "=", "'\\t'", "\n", "#assert not(char == '(') and not(char == ')')", "\n", "", "output", ".", "append", "(", "char", ")", "\n", "", "return", "''", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_words_with_index.get_actions": [[145, 207], ["line.rstrip", "len", "get_words_with_index.is_next_open_bracket", "output_actions.append", "get_words_with_index.get_nonterminal", "output_actions.append", "get_words_with_index.get_word", "output_actions.append", "get_word.split", "a1.split", "a1.split", "int", "output_actions.append", "output_actions.append"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.linearization.constituent_dict.is_next_open_bracket", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_inorder_oracle_SWAP.get_nonterminal", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_inorder_oracle_SWAP.get_word", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "get_actions", "(", "line", ")", ":", "\n", "    ", "output_actions", "=", "[", "]", "\n", "line_strip", "=", "line", ".", "rstrip", "(", ")", "\n", "i", "=", "0", "\n", "sentence_order", "=", "0", "\n", "stack", "=", "[", "]", "\n", "max_idx", "=", "(", "len", "(", "line_strip", ")", "-", "1", ")", "\n", "while", "i", "<=", "max_idx", ":", "\n", "        ", "assert", "line_strip", "[", "i", "]", "==", "'('", "or", "line_strip", "[", "i", "]", "==", "')'", "\n", "if", "line_strip", "[", "i", "]", "==", "'('", ":", "\n", "            ", "if", "is_next_open_bracket", "(", "line_strip", ",", "i", ")", ":", "# open non-terminal", "\n", "                ", "curr_NT", "=", "get_nonterminal", "(", "line_strip", ",", "i", ")", "\n", "output_actions", ".", "append", "(", "'NT('", "+", "curr_NT", "+", "')'", ")", "\n", "i", "+=", "1", "\n", "while", "line_strip", "[", "i", "]", "!=", "'('", ":", "# get the next open bracket, which may be a terminal or another non-terminal", "\n", "                    ", "i", "+=", "1", "\n", "", "", "else", ":", "# it's a terminal symbol", "\n", "                ", "word", "=", "get_word", "(", "line_strip", ",", "i", ")", "\n", "#print(word)", "\n", "a1", "=", "word", ".", "split", "(", "'\\t'", ")", "[", "1", "]", "\n", "iw1", "=", "a1", ".", "split", "(", "'='", ")", "[", "0", "]", "\n", "w1", "=", "a1", ".", "split", "(", "'='", ")", "[", "1", "]", "\n", "#print('-------')", "\n", "#print(sentence_order,' ',iw1,w1)", "\n", "#print(stack)", "\n", "output_actions", ".", "append", "(", "'SHIFT'", ")", "\n", "if", "int", "(", "iw1", ")", ">", "sentence_order", ":", "\n", "#stack.append(sentence_order)", "\n", "\n", "#output_actions.append('SHIFT'+str(sentence_order))", "\n", "#for x in range(int(iw1)-sentence_order):", "\n", "#if True:", "\n", "#stack.append(sentence_order)", "\n", "#if w1 == \".\" or w1 == \",\":", "\n", "#    output_actions.append('SHIFT('+w1+')')", "\n", "#else:", "\n", "#output_actions.append('SHIFT'+iw1)", "\n", "#output_actions.append('SWAP'+str(sentence_order))", "\n", "                    ", "output_actions", ".", "append", "(", "'SHIFT'", ")", "\n", "", "else", ":", "\n", "#stack.append(iw1)", "\n", "                    ", "sentence_order", "+=", "1", "\n", "#if w1 == \".\" or w1 == \",\":", "\n", "#     output_actions.append('SHIFT('+w1+')')", "\n", "#else:", "\n", "output_actions", ".", "append", "(", "'SHIFT-REGULAR'", "+", "iw1", ")", "\n", "\n", "#output_actions.append('SHIFT')", "\n", "", "while", "line_strip", "[", "i", "]", "!=", "')'", ":", "\n", "                    ", "i", "+=", "1", "\n", "", "i", "+=", "1", "\n", "while", "line_strip", "[", "i", "]", "!=", "')'", "and", "line_strip", "[", "i", "]", "!=", "'('", ":", "\n", "                    ", "i", "+=", "1", "\n", "", "", "", "else", ":", "\n", "             ", "output_actions", ".", "append", "(", "'REDUCE'", ")", "\n", "if", "i", "==", "max_idx", ":", "\n", "                 ", "break", "\n", "", "i", "+=", "1", "\n", "while", "line_strip", "[", "i", "]", "!=", "')'", "and", "line_strip", "[", "i", "]", "!=", "'('", ":", "\n", "                 ", "i", "+=", "1", "\n", "", "", "", "assert", "i", "==", "max_idx", "\n", "return", "output_actions", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_words_with_index.construct": [[208, 225], ["len", "get_words_with_index.construct", "trees.append", "trees.append", "trees.append"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_inorder_oracle_SWAP.construct", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "construct", "(", "actions", ",", "trees", ")", ":", "\n", "    ", "while", "len", "(", "actions", ")", ">", "0", ":", "\n", "        ", "act", "=", "actions", "[", "0", "]", "\n", "actions", "=", "actions", "[", "1", ":", "]", "\n", "if", "act", "[", "0", "]", "==", "'N'", ":", "\n", "            ", "tree", "=", "[", "act", "]", "\n", "actions", ",", "tree", "=", "construct", "(", "actions", ",", "tree", ")", "\n", "trees", ".", "append", "(", "tree", ")", "\n", "", "elif", "act", "[", "0", "]", "==", "'S'", "and", "act", "[", "1", "]", "==", "'H'", ":", "\n", "            ", "trees", ".", "append", "(", "act", ")", "\n", "", "elif", "act", "[", "0", "]", "==", "'S'", "and", "act", "[", "1", "]", "==", "'W'", ":", "\n", "            ", "trees", ".", "append", "(", "act", ")", "\n", "", "elif", "act", "[", "0", "]", "==", "'R'", ":", "\n", "            ", "break", ";", "\n", "", "else", ":", "\n", "            ", "assert", "False", "\n", "", "", "return", "actions", ",", "trees", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_words_with_index.get_actions2": [[226, 242], ["get_actions2.append", "get_actions2.append", "type", "get_words_with_index.get_actions2", "get_actions2.append", "type", "type", "get_words_with_index.get_actions2", "get_actions2.append"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_inorder_oracle_SWAP.get_actions2", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_inorder_oracle_SWAP.get_actions2", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "get_actions2", "(", "trees", ",", "actions", ")", ":", "\n", "    ", "if", "type", "(", "trees", "[", "1", "]", ")", "==", "list", ":", "#types.ListType:", "\n", "        ", "actions", "=", "get_actions2", "(", "trees", "[", "1", "]", ",", "actions", ")", "\n", "", "else", ":", "\n", "        ", "actions", ".", "append", "(", "trees", "[", "1", "]", ")", "\n", "\n", "", "assert", "type", "(", "trees", "[", "0", "]", ")", "==", "str", "#types.StringType", "\n", "actions", ".", "append", "(", "\"NT\"", "+", "trees", "[", "0", "]", "[", "2", ":", "]", ")", "\n", "\n", "for", "item", "in", "trees", "[", "2", ":", "]", ":", "\n", "        ", "if", "type", "(", "item", ")", "==", "list", ":", "#types.ListType:", "\n", "            ", "actions", "=", "get_actions2", "(", "item", ",", "actions", ")", "\n", "", "else", ":", "\n", "            ", "actions", ".", "append", "(", "item", ")", "\n", "", "", "actions", ".", "append", "(", "\"RE\"", "+", "trees", "[", "0", "]", "[", "2", ":", "]", ")", "\n", "return", "actions", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_words_with_index.get_projective_order": [[244, 257], ["dict", "range", "print", "len", "t.split", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["", "def", "get_projective_order", "(", "tokens", ")", ":", "\n", "    ", "order", "=", "dict", "(", ")", "\n", "for", "t", "in", "tokens", ":", "\n", "        ", "i", "=", "t", ".", "split", "(", "'='", ")", "[", "0", "]", "\n", "order", "[", "str", "(", "i", ")", "]", "=", "t", "#t.split('=')[1]", "\n", "\n", "", "cad", "=", "''", "\n", "for", "a", "in", "range", "(", "len", "(", "tokens", ")", ")", ":", "\n", "        ", "if", "cad", "==", "''", ":", "\n", "            ", "cad", "=", "order", "[", "str", "(", "a", ")", "]", "\n", "", "else", ":", "\n", "            ", "cad", "=", "cad", "+", "'\\t'", "+", "order", "[", "str", "(", "a", ")", "]", "\n", "", "", "print", "(", "cad", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_words_with_index.main": [[259, 286], ["open", "open.readlines", "open.close", "open", "open.readlines", "open.close", "get_dictionary.get_dict", "len", "NotImplementedError", "get_words_with_index.get_tags_tokens_lowercase", "get_words_with_index.get_projective_order", "line.count", "line.count", "NotImplementedError", "len", "len", "len", "len", "str"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.readlines", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.GoldMiner.close", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.readlines", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.GoldMiner.close", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.linearization.constituent_dict.get_dict", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_inorder_oracle_SWAP.get_tags_tokens_lowercase", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_inorder_oracle_SWAP.get_projective_order"], ["", "def", "main", "(", ")", ":", "\n", "    ", "if", "len", "(", "sys", ".", "argv", ")", "!=", "4", ":", "\n", "        ", "raise", "NotImplementedError", "(", "'Program only takes three arguments:  en|ch train file and dev file (for vocabulary mapping purposes)'", ")", "\n", "", "assert", "sys", ".", "argv", "[", "1", "]", "==", "\"ch\"", "or", "sys", ".", "argv", "[", "1", "]", "==", "\"en\"", "\n", "\n", "train_file", "=", "open", "(", "sys", ".", "argv", "[", "2", "]", ",", "'r'", ")", "\n", "lines", "=", "train_file", ".", "readlines", "(", ")", "\n", "train_file", ".", "close", "(", ")", "\n", "dev_file", "=", "open", "(", "sys", ".", "argv", "[", "3", "]", ",", "'r'", ")", "\n", "dev_lines", "=", "dev_file", ".", "readlines", "(", ")", "\n", "dev_file", ".", "close", "(", ")", "\n", "words_list", "=", "get_dictionary", ".", "get_dict", "(", "lines", ")", "\n", "line_ctr", "=", "0", "\n", "# get the oracle for the train file", "\n", "for", "line", "in", "dev_lines", ":", "\n", "        ", "line_ctr", "+=", "1", "\n", "# assert that the parenthesis are balanced", "\n", "if", "line", ".", "count", "(", "'('", ")", "!=", "line", ".", "count", "(", "')'", ")", ":", "\n", "            ", "raise", "NotImplementedError", "(", "'Unbalanced number of parenthesis in line '", "+", "str", "(", "line_ctr", ")", ")", "\n", "# first line: the bracketed tree itself itself ", "\n", "#print('# ' + line.rstrip()) #Ponemos ! para que coincida con lo que envio LIU", "\n", "", "tags", ",", "tokens", ",", "lowercase", "=", "get_tags_tokens_lowercase", "(", "line", ")", "\n", "assert", "len", "(", "tags", ")", "==", "len", "(", "tokens", ")", "\n", "assert", "len", "(", "tokens", ")", "==", "len", "(", "lowercase", ")", "\n", "#print(' '.join(tags))", "\n", "#print(' '.join(tokens))", "\n", "get_projective_order", "(", "tokens", ")", "\n", "#print(' '.join(lowercase))", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_inorder_oracle_SWAP.unkify": [[5, 74], ["len", "final.append", "token.rstrip", "token.rstrip", "token.rstrip().lower", "ch0.isupper", "final.append", "final.append", "token.rstrip", "final.append", "char.isdigit", "token.rstrip", "token.rstrip", "token.rstrip", "len", "char.isalpha", "ch0.isalpha", "len", "char.islower", "char.isupper"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["def", "unkify", "(", "tokens", ",", "words_dict", ",", "lang", ")", ":", "\n", "    ", "final", "=", "[", "]", "\n", "for", "token", "in", "tokens", ":", "\n", "# only process the train singletons and unknown words", "\n", "        ", "if", "len", "(", "token", ".", "rstrip", "(", ")", ")", "==", "0", ":", "\n", "            ", "final", ".", "append", "(", "'UNK'", ")", "\n", "", "elif", "not", "(", "token", ".", "rstrip", "(", ")", "in", "words_dict", ")", ":", "\n", "            ", "if", "lang", "==", "\"ch\"", ":", "\n", "                ", "final", ".", "append", "(", "'UNK'", ")", "\n", "continue", ";", "\n", "", "numCaps", "=", "0", "\n", "hasDigit", "=", "False", "\n", "hasDash", "=", "False", "\n", "hasLower", "=", "False", "\n", "for", "char", "in", "token", ".", "rstrip", "(", ")", ":", "\n", "                ", "if", "char", ".", "isdigit", "(", ")", ":", "\n", "                    ", "hasDigit", "=", "True", "\n", "", "elif", "char", "==", "'-'", ":", "\n", "                    ", "hasDash", "=", "True", "\n", "", "elif", "char", ".", "isalpha", "(", ")", ":", "\n", "                    ", "if", "char", ".", "islower", "(", ")", ":", "\n", "                        ", "hasLower", "=", "True", "\n", "", "elif", "char", ".", "isupper", "(", ")", ":", "\n", "                        ", "numCaps", "+=", "1", "\n", "", "", "", "result", "=", "'UNK'", "\n", "lower", "=", "token", ".", "rstrip", "(", ")", ".", "lower", "(", ")", "\n", "ch0", "=", "token", ".", "rstrip", "(", ")", "[", "0", "]", "\n", "if", "ch0", ".", "isupper", "(", ")", ":", "\n", "                ", "if", "numCaps", "==", "1", ":", "\n", "                    ", "result", "=", "result", "+", "'-INITC'", "\n", "if", "lower", "in", "words_dict", ":", "\n", "                        ", "result", "=", "result", "+", "'-KNOWNLC'", "\n", "", "", "else", ":", "\n", "                    ", "result", "=", "result", "+", "'-CAPS'", "\n", "", "", "elif", "not", "(", "ch0", ".", "isalpha", "(", ")", ")", "and", "numCaps", ">", "0", ":", "\n", "                ", "result", "=", "result", "+", "'-CAPS'", "\n", "", "elif", "hasLower", ":", "\n", "                ", "result", "=", "result", "+", "'-LC'", "\n", "", "if", "hasDigit", ":", "\n", "                ", "result", "=", "result", "+", "'-NUM'", "\n", "", "if", "hasDash", ":", "\n", "                ", "result", "=", "result", "+", "'-DASH'", "\n", "", "if", "lower", "[", "-", "1", "]", "==", "'s'", "and", "len", "(", "lower", ")", ">=", "3", ":", "\n", "                ", "ch2", "=", "lower", "[", "-", "2", "]", "\n", "if", "not", "(", "ch2", "==", "'s'", ")", "and", "not", "(", "ch2", "==", "'i'", ")", "and", "not", "(", "ch2", "==", "'u'", ")", ":", "\n", "                    ", "result", "=", "result", "+", "'-s'", "\n", "", "", "elif", "len", "(", "lower", ")", ">=", "5", "and", "not", "(", "hasDash", ")", "and", "not", "(", "hasDigit", "and", "numCaps", ">", "0", ")", ":", "\n", "                ", "if", "lower", "[", "-", "2", ":", "]", "==", "'ed'", ":", "\n", "                    ", "result", "=", "result", "+", "'-ed'", "\n", "", "elif", "lower", "[", "-", "3", ":", "]", "==", "'ing'", ":", "\n", "                    ", "result", "=", "result", "+", "'-ing'", "\n", "", "elif", "lower", "[", "-", "3", ":", "]", "==", "'ion'", ":", "\n", "                    ", "result", "=", "result", "+", "'-ion'", "\n", "", "elif", "lower", "[", "-", "2", ":", "]", "==", "'er'", ":", "\n", "                    ", "result", "=", "result", "+", "'-er'", "\n", "", "elif", "lower", "[", "-", "3", ":", "]", "==", "'est'", ":", "\n", "                    ", "result", "=", "result", "+", "'-est'", "\n", "", "elif", "lower", "[", "-", "2", ":", "]", "==", "'ly'", ":", "\n", "                    ", "result", "=", "result", "+", "'-ly'", "\n", "", "elif", "lower", "[", "-", "3", ":", "]", "==", "'ity'", ":", "\n", "                    ", "result", "=", "result", "+", "'-ity'", "\n", "", "elif", "lower", "[", "-", "1", "]", "==", "'y'", ":", "\n", "                    ", "result", "=", "result", "+", "'-y'", "\n", "", "elif", "lower", "[", "-", "2", ":", "]", "==", "'al'", ":", "\n", "                    ", "result", "=", "result", "+", "'-al'", "\n", "", "", "final", ".", "append", "(", "result", ")", "\n", "", "else", ":", "\n", "            ", "final", ".", "append", "(", "token", ".", "rstrip", "(", ")", ")", "\n", "", "", "return", "final", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_inorder_oracle_SWAP.is_next_open_bracket": [[75, 82], ["IndexError"], "function", ["None"], ["", "def", "is_next_open_bracket", "(", "line", ",", "start_idx", ")", ":", "\n", "    ", "for", "char", "in", "line", "[", "(", "start_idx", "+", "1", ")", ":", "]", ":", "\n", "        ", "if", "char", "==", "'('", ":", "\n", "            ", "return", "True", "\n", "", "elif", "char", "==", "')'", ":", "\n", "            ", "return", "False", "\n", "", "", "raise", "IndexError", "(", "'Bracket possibly not balanced, open bracket not followed by closed bracket'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_inorder_oracle_SWAP.get_between_brackets": [[83, 91], ["output.append"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "get_between_brackets", "(", "line", ",", "start_idx", ")", ":", "\n", "    ", "output", "=", "[", "]", "\n", "for", "char", "in", "line", "[", "(", "start_idx", "+", "1", ")", ":", "]", ":", "\n", "        ", "if", "char", "==", "')'", ":", "\n", "            ", "break", "\n", "", "assert", "not", "(", "char", "==", "'('", ")", "\n", "output", ".", "append", "(", "char", ")", "\n", "", "return", "''", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_inorder_oracle_SWAP.get_tags_tokens_lowercase": [[101, 122], ["line.rstrip", "range", "len", "terminal.split", "output_tags.append", "output_tokens.append", "output_lowercase.append", "output.append", "len", "terminal_split[].lower", "get_inorder_oracle_SWAP.is_next_open_bracket", "get_inorder_oracle_SWAP.get_between_brackets"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.linearization.constituent_dict.is_next_open_bracket", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.linearization.constituent_dict.get_between_brackets"], ["", "def", "get_tags_tokens_lowercase", "(", "line", ")", ":", "\n", "    ", "output", "=", "[", "]", "\n", "#print 'curr line', line_strip", "\n", "line_strip", "=", "line", ".", "rstrip", "(", ")", "\n", "#print 'length of the sentence', len(line_strip)", "\n", "for", "i", "in", "range", "(", "len", "(", "line_strip", ")", ")", ":", "\n", "        ", "if", "i", "==", "0", ":", "\n", "            ", "assert", "line_strip", "[", "i", "]", "==", "'('", "\n", "", "if", "line_strip", "[", "i", "]", "==", "'('", "and", "not", "(", "is_next_open_bracket", "(", "line_strip", ",", "i", ")", ")", ":", "# fulfilling this condition means this is a terminal symbol", "\n", "            ", "output", ".", "append", "(", "get_between_brackets", "(", "line_strip", ",", "i", ")", ")", "\n", "#print 'output:',output", "\n", "", "", "output_tags", "=", "[", "]", "\n", "output_tokens", "=", "[", "]", "\n", "output_lowercase", "=", "[", "]", "\n", "for", "terminal", "in", "output", ":", "\n", "        ", "terminal_split", "=", "terminal", ".", "split", "(", ")", "\n", "assert", "len", "(", "terminal_split", ")", "==", "2", "# each terminal contains a POS tag and word        ", "\n", "output_tags", ".", "append", "(", "terminal_split", "[", "0", "]", ")", "\n", "output_tokens", ".", "append", "(", "terminal_split", "[", "1", "]", ")", "\n", "output_lowercase", ".", "append", "(", "terminal_split", "[", "1", "]", ".", "lower", "(", ")", ")", "\n", "", "return", "[", "output_tags", ",", "output_tokens", ",", "output_lowercase", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_inorder_oracle_SWAP.get_nonterminal": [[123, 132], ["output.append"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "get_nonterminal", "(", "line", ",", "start_idx", ")", ":", "\n", "    ", "assert", "line", "[", "start_idx", "]", "==", "'('", "# make sure it's an open bracket", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "line", "[", "(", "start_idx", "+", "1", ")", ":", "]", ":", "\n", "        ", "if", "char", "==", "' '", ":", "\n", "            ", "break", "\n", "", "assert", "not", "(", "char", "==", "'('", ")", "and", "not", "(", "char", "==", "')'", ")", "\n", "output", ".", "append", "(", "char", ")", "\n", "", "return", "''", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_inorder_oracle_SWAP.get_word": [[133, 144], ["output.append"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "get_word", "(", "line", ",", "start_idx", ")", ":", "\n", "#assert line[start_idx] == '(' # make sure it's an open bracket", "\n", "    ", "output", "=", "[", "]", "\n", "for", "char", "in", "line", "[", "(", "start_idx", "+", "1", ")", ":", "]", ":", "\n", "        ", "if", "char", "==", "')'", ":", "\n", "            ", "break", "\n", "", "if", "char", "==", "' '", ":", "\n", "            ", "char", "=", "'\\t'", "\n", "#assert not(char == '(') and not(char == ')')", "\n", "", "output", ".", "append", "(", "char", ")", "\n", "", "return", "''", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_inorder_oracle_SWAP.get_actions": [[145, 220], ["line.rstrip", "reversed", "range", "buffer.append", "len", "len", "int", "get_inorder_oracle_SWAP.is_next_open_bracket", "output_actions.append", "get_inorder_oracle_SWAP.get_nonterminal", "output_actions.append", "get_inorder_oracle_SWAP.get_word", "output_actions.append", "get_word.split", "a1.split", "a1.split", "print", "print", "stack.append", "int", "stack.append", "print", "buffer.pop", "buffer.append", "print", "buffer.pop", "buffer.append", "len", "int", "int", "print", "stack.pop", "len", "int", "int", "print", "stack.pop", "str", "proj_order.get", "proj_order.get", "str", "proj_order.get", "proj_order.get", "str", "str", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.linearization.constituent_dict.is_next_open_bracket", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_inorder_oracle_SWAP.get_nonterminal", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_inorder_oracle_SWAP.get_word", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop"], ["", "def", "get_actions", "(", "line", ",", "proj_order", ")", ":", "\n", "    ", "debug", "=", "False", "\n", "output_actions", "=", "[", "]", "\n", "line_strip", "=", "line", ".", "rstrip", "(", ")", "\n", "i", "=", "0", "\n", "buffer", "=", "[", "]", "\n", "for", "k", "in", "reversed", "(", "range", "(", "len", "(", "proj_order", ")", ")", ")", ":", "\n", "        ", "buffer", ".", "append", "(", "int", "(", "k", ")", ")", "\n", "", "stack", "=", "[", "]", "\n", "max_idx", "=", "(", "len", "(", "line_strip", ")", "-", "1", ")", "\n", "while", "i", "<=", "max_idx", ":", "\n", "        ", "assert", "line_strip", "[", "i", "]", "==", "'('", "or", "line_strip", "[", "i", "]", "==", "')'", "\n", "if", "line_strip", "[", "i", "]", "==", "'('", ":", "\n", "            ", "if", "is_next_open_bracket", "(", "line_strip", ",", "i", ")", ":", "# open non-terminal", "\n", "                ", "curr_NT", "=", "get_nonterminal", "(", "line_strip", ",", "i", ")", "\n", "output_actions", ".", "append", "(", "'NT('", "+", "curr_NT", "+", "')'", ")", "\n", "i", "+=", "1", "\n", "while", "line_strip", "[", "i", "]", "!=", "'('", ":", "# get the next open bracket, which may be a terminal or another non-terminal", "\n", "                    ", "i", "+=", "1", "\n", "", "", "else", ":", "# it's a terminal symbol", "\n", "                ", "word", "=", "get_word", "(", "line_strip", ",", "i", ")", "\n", "#print(word)", "\n", "a1", "=", "word", ".", "split", "(", "'\\t'", ")", "[", "1", "]", "\n", "iw1", "=", "a1", ".", "split", "(", "'='", ")", "[", "0", "]", "\n", "w1", "=", "a1", ".", "split", "(", "'='", ")", "[", "1", "]", "\n", "#print('-------')", "\n", "#print(sentence_order,' ',iw1,w1)", "\n", "if", "debug", ":", "print", "(", "'STACK'", ",", "stack", ")", "\n", "if", "debug", ":", "print", "(", "'BUFFER'", ",", "buffer", ")", "\n", "process_node", "=", "''", "\n", "#output_actions.append('SHIFT')", "\n", "if", "True", ":", "#int(iw1) > buffer[-1]:", "\n", "\n", "                    ", "while", "int", "(", "iw1", ")", "!=", "buffer", "[", "-", "1", "]", ":", "\n", "                        ", "if", "debug", ":", "print", "(", "'SHIFT'", "+", "str", "(", "buffer", "[", "-", "1", "]", ")", ")", "\n", "\n", "if", "process_node", "==", "''", ":", "\n", "                            ", "process_node", "=", "'SHIFT'", "\n", "", "else", ":", "\n", "                            ", "process_node", "=", "process_node", "+", "'\\tSHIFT'", "#output_actions.append('SHIFT')", "\n", "", "stack", ".", "append", "(", "buffer", ".", "pop", "(", ")", ")", "\n", "while", "len", "(", "stack", ")", ">", "1", "and", "int", "(", "proj_order", ".", "get", "(", "str", "(", "stack", "[", "-", "2", "]", ")", ")", ")", ">", "int", "(", "proj_order", ".", "get", "(", "str", "(", "stack", "[", "-", "1", "]", ")", ")", ")", ":", "\n", "                           ", "process_node", "=", "process_node", "+", "'\\tSWAP'", "#output_actions.append('SWAP-'+str(stack[-2]))", "\n", "if", "debug", ":", "print", "(", "'SWAP-'", "+", "str", "(", "stack", "[", "-", "2", "]", ")", ")", "\n", "buffer", ".", "append", "(", "stack", ".", "pop", "(", "-", "2", ")", ")", "\n", "\n", "\n", "", "", "", "if", "True", ":", "#else:", "\n", "                    ", "if", "debug", ":", "print", "(", "'SHIFT REGULAR'", "+", "str", "(", "buffer", "[", "-", "1", "]", ")", ")", "\n", "if", "process_node", "==", "''", ":", "\n", "                        ", "process_node", "=", "'SHIFT'", "\n", "", "else", ":", "\n", "                        ", "process_node", "=", "process_node", "+", "'\\tSHIFT'", "#output_actions.append('SHIFT-REGULAR'+iw1)", "\n", "", "stack", ".", "append", "(", "buffer", ".", "pop", "(", ")", ")", "\n", "while", "len", "(", "stack", ")", ">", "1", "and", "int", "(", "proj_order", ".", "get", "(", "str", "(", "stack", "[", "-", "2", "]", ")", ")", ")", ">", "int", "(", "proj_order", ".", "get", "(", "str", "(", "stack", "[", "-", "1", "]", ")", ")", ")", ":", "\n", "                           ", "process_node", "=", "process_node", "+", "'\\tSWAP'", "#output_actions.append('SWAP-REGULAR-'+str(stack[-2]))", "\n", "if", "debug", ":", "print", "(", "'SWAP-'", "+", "str", "(", "stack", "[", "-", "2", "]", ")", ")", "\n", "buffer", ".", "append", "(", "stack", ".", "pop", "(", "-", "2", ")", ")", "\n", "\n", "\n", "", "", "output_actions", ".", "append", "(", "process_node", ")", "\n", "while", "line_strip", "[", "i", "]", "!=", "')'", ":", "\n", "                    ", "i", "+=", "1", "\n", "", "i", "+=", "1", "\n", "while", "line_strip", "[", "i", "]", "!=", "')'", "and", "line_strip", "[", "i", "]", "!=", "'('", ":", "\n", "                    ", "i", "+=", "1", "\n", "", "", "", "else", ":", "\n", "             ", "output_actions", ".", "append", "(", "'REDUCE'", ")", "\n", "if", "i", "==", "max_idx", ":", "\n", "                 ", "break", "\n", "", "i", "+=", "1", "\n", "while", "line_strip", "[", "i", "]", "!=", "')'", "and", "line_strip", "[", "i", "]", "!=", "'('", ":", "\n", "                 ", "i", "+=", "1", "\n", "", "", "", "assert", "i", "==", "max_idx", "\n", "return", "output_actions", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_inorder_oracle_SWAP.construct": [[221, 238], ["len", "get_inorder_oracle_SWAP.construct", "trees.append", "trees.append", "trees.append"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_inorder_oracle_SWAP.construct", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "construct", "(", "actions", ",", "trees", ")", ":", "\n", "    ", "while", "len", "(", "actions", ")", ">", "0", ":", "\n", "        ", "act", "=", "actions", "[", "0", "]", "\n", "actions", "=", "actions", "[", "1", ":", "]", "\n", "if", "act", "[", "0", "]", "==", "'N'", ":", "\n", "            ", "tree", "=", "[", "act", "]", "\n", "actions", ",", "tree", "=", "construct", "(", "actions", ",", "tree", ")", "\n", "trees", ".", "append", "(", "tree", ")", "\n", "", "elif", "act", "[", "0", "]", "==", "'S'", "and", "act", "[", "1", "]", "==", "'H'", ":", "\n", "            ", "trees", ".", "append", "(", "act", ")", "\n", "", "elif", "act", "[", "0", "]", "==", "'S'", "and", "act", "[", "1", "]", "==", "'W'", ":", "\n", "            ", "trees", ".", "append", "(", "act", ")", "\n", "", "elif", "act", "[", "0", "]", "==", "'R'", ":", "\n", "            ", "break", ";", "\n", "", "else", ":", "\n", "            ", "assert", "False", "\n", "", "", "return", "actions", ",", "trees", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_inorder_oracle_SWAP.get_actions2": [[239, 256], ["get_actions2.append", "get_actions2.append", "type", "get_inorder_oracle_SWAP.get_actions2", "get_actions2.append", "type", "type", "get_inorder_oracle_SWAP.get_actions2", "get_actions2.append"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_inorder_oracle_SWAP.get_actions2", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_inorder_oracle_SWAP.get_actions2", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "get_actions2", "(", "trees", ",", "actions", ")", ":", "\n", "    ", "if", "type", "(", "trees", "[", "1", "]", ")", "==", "list", ":", "#types.ListType:", "\n", "        ", "actions", "=", "get_actions2", "(", "trees", "[", "1", "]", ",", "actions", ")", "\n", "", "else", ":", "\n", "        ", "actions", ".", "append", "(", "trees", "[", "1", "]", ")", "\n", "\n", "", "assert", "type", "(", "trees", "[", "0", "]", ")", "==", "str", "#types.StringType", "\n", "actions", ".", "append", "(", "\"NT\"", "+", "trees", "[", "0", "]", "[", "2", ":", "]", ")", "\n", "\n", "for", "item", "in", "trees", "[", "2", ":", "]", ":", "\n", "        ", "if", "type", "(", "item", ")", "==", "list", ":", "#types.ListType:", "\n", "            ", "actions", "=", "get_actions2", "(", "item", ",", "actions", ")", "\n", "", "else", ":", "\n", "            ", "actions", ".", "append", "(", "item", ")", "\n", "#actions.append(\"RE\"+trees[0][2:])#VINO", "\n", "", "", "actions", ".", "append", "(", "\"RE\"", ")", "#INO", "\n", "return", "actions", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_inorder_oracle_SWAP.get_projective_order": [[257, 265], ["dict", "t.split"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["", "def", "get_projective_order", "(", "tokens", ")", ":", "\n", "    ", "order", "=", "dict", "(", ")", "\n", "proj_order", "=", "0", "\n", "for", "t", "in", "tokens", ":", "\n", "        ", "i", "=", "t", ".", "split", "(", "'='", ")", "[", "0", "]", "\n", "order", "[", "i", "]", "=", "proj_order", "\n", "proj_order", "+=", "1", "\n", "", "return", "order", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_inorder_oracle_SWAP.main": [[266, 308], ["open", "open.readlines", "open.close", "open", "open.readlines", "open.close", "get_dictionary.get_dict", "len", "NotImplementedError", "get_inorder_oracle_SWAP.get_tags_tokens_lowercase", "get_inorder_oracle_SWAP.get_projective_order", "get_inorder_oracle_SWAP.get_actions", "get_inorder_oracle_SWAP.construct", "get_inorder_oracle_SWAP.get_actions2", "print", "line.count", "line.count", "NotImplementedError", "len", "len", "len", "len", "print", "str"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.readlines", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.GoldMiner.close", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.readlines", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.GoldMiner.close", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.linearization.constituent_dict.get_dict", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_inorder_oracle_SWAP.get_tags_tokens_lowercase", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_inorder_oracle_SWAP.get_projective_order", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_inorder_oracle_SWAP.get_actions", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_inorder_oracle_SWAP.construct", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.get_inorder_oracle_SWAP.get_actions2", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["", "def", "main", "(", ")", ":", "\n", "    ", "if", "len", "(", "sys", ".", "argv", ")", "!=", "4", ":", "\n", "        ", "raise", "NotImplementedError", "(", "'Program only takes three arguments:  en|ch train file and dev file (for vocabulary mapping purposes)'", ")", "\n", "", "assert", "sys", ".", "argv", "[", "1", "]", "==", "\"ch\"", "or", "sys", ".", "argv", "[", "1", "]", "==", "\"en\"", "\n", "\n", "train_file", "=", "open", "(", "sys", ".", "argv", "[", "2", "]", ",", "'r'", ")", "\n", "lines", "=", "train_file", ".", "readlines", "(", ")", "\n", "train_file", ".", "close", "(", ")", "\n", "dev_file", "=", "open", "(", "sys", ".", "argv", "[", "3", "]", ",", "'r'", ")", "\n", "dev_lines", "=", "dev_file", ".", "readlines", "(", ")", "\n", "dev_file", ".", "close", "(", ")", "\n", "words_list", "=", "get_dictionary", ".", "get_dict", "(", "lines", ")", "\n", "line_ctr", "=", "0", "\n", "\n", "# get the oracle for the train file", "\n", "for", "line", "in", "dev_lines", ":", "\n", "        ", "line_ctr", "+=", "1", "\n", "# assert that the parenthesis are balanced", "\n", "if", "line", ".", "count", "(", "'('", ")", "!=", "line", ".", "count", "(", "')'", ")", ":", "\n", "            ", "raise", "NotImplementedError", "(", "'Unbalanced number of parenthesis in line '", "+", "str", "(", "line_ctr", ")", ")", "\n", "# first line: the bracketed tree itself itself ", "\n", "#print('# ' + line.rstrip()) #Ponemos ! para que coincida con lo que envio LIU", "\n", "", "tags", ",", "tokens", ",", "lowercase", "=", "get_tags_tokens_lowercase", "(", "line", ")", "\n", "assert", "len", "(", "tags", ")", "==", "len", "(", "tokens", ")", "\n", "assert", "len", "(", "tokens", ")", "==", "len", "(", "lowercase", ")", "\n", "#print(' '.join(tags))", "\n", "#print(' '.join(tokens))", "\n", "\n", "proj_order", "=", "get_projective_order", "(", "tokens", ")", "#print(' '.join(lowercase))", "\n", "#print(proj_order)", "\n", "#unkified = unkify(tokens, words_list, sys.argv[1])    ", "\n", "#print(' '.join(unkified))", "\n", "output_actions", "=", "get_actions", "(", "line", ",", "proj_order", ")", "\n", "#print(output_actions)", "\n", "_", ",", "trees", "=", "construct", "(", "output_actions", ",", "[", "]", ")", "\n", "\n", "#print('Tree',trees)", "\n", "\n", "output_actions2", "=", "get_actions2", "(", "trees", "[", "0", "]", ",", "[", "]", ")", "\n", "for", "action", "in", "output_actions2", ":", "\n", "            ", "print", "(", "action", ",", "end", "=", "'\\t'", ")", "\n", "", "print", "(", "'TERM'", ")", "\n", "#print('')", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.PretrainedWav2VecModel.__init__": [[32, 42], ["torch.nn.Module.__init__", "torch.load", "fairseq.models.wav2vec.Wav2VecModel.build_model", "fairseq.models.wav2vec.Wav2VecModel.build_model.load_state_dict", "fairseq.models.wav2vec.Wav2VecModel.build_model.eval"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv_lm.LightConvLanguageModel.build_model", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.load_state_dict", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval"], ["    ", "def", "__init__", "(", "self", ",", "fname", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "checkpoint", "=", "torch", ".", "load", "(", "fname", ")", "\n", "self", ".", "args", "=", "checkpoint", "[", "\"args\"", "]", "\n", "model", "=", "Wav2VecModel", ".", "build_model", "(", "self", ".", "args", ",", "None", ")", "\n", "model", ".", "load_state_dict", "(", "checkpoint", "[", "\"model\"", "]", ")", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "self", ".", "model", "=", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.PretrainedWav2VecModel.forward": [[43, 50], ["torch.no_grad", "wav2vec_featurize.PretrainedWav2VecModel.model.feature_extractor", "isinstance", "wav2vec_featurize.PretrainedWav2VecModel.model.feature_aggregator"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "z", "=", "self", ".", "model", ".", "feature_extractor", "(", "x", ")", "\n", "if", "isinstance", "(", "z", ",", "tuple", ")", ":", "\n", "                ", "z", "=", "z", "[", "0", "]", "\n", "", "c", "=", "self", ".", "model", ".", "feature_aggregator", "(", "z", ")", "\n", "", "return", "z", ",", "c", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.EmbeddingWriterConfig.__init__": [[54, 76], ["argparse.ArgumentParser.__init__", "wav2vec_featurize.EmbeddingWriterConfig.add_argument", "wav2vec_featurize.EmbeddingWriterConfig.add_argument", "wav2vec_featurize.EmbeddingWriterConfig.add_argument", "wav2vec_featurize.EmbeddingWriterConfig.add_argument", "wav2vec_featurize.EmbeddingWriterConfig.add_argument", "wav2vec_featurize.EmbeddingWriterConfig.add_argument", "wav2vec_featurize.EmbeddingWriterConfig.add_argument", "wav2vec_featurize.EmbeddingWriterConfig.add_argument"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\"Pre-compute embeddings for wav2letter++ datasets\"", ")", "\n", "\n", "kwargs", "=", "{", "\"action\"", ":", "\"store\"", ",", "\"type\"", ":", "str", ",", "\"required\"", ":", "True", "}", "\n", "\n", "self", ".", "add_argument", "(", "\"--input\"", ",", "\"-i\"", ",", "\n", "help", "=", "\"Input Directory\"", ",", "**", "kwargs", ")", "\n", "self", ".", "add_argument", "(", "\"--output\"", ",", "\"-o\"", ",", "\n", "help", "=", "\"Output Directory\"", ",", "**", "kwargs", ")", "\n", "self", ".", "add_argument", "(", "\"--model\"", ",", "\n", "help", "=", "\"Path to model checkpoint\"", ",", "**", "kwargs", ")", "\n", "self", ".", "add_argument", "(", "\"--split\"", ",", "\n", "help", "=", "\"Dataset Splits\"", ",", "nargs", "=", "'+'", ",", "**", "kwargs", ")", "\n", "self", ".", "add_argument", "(", "\"--ext\"", ",", "default", "=", "\"wav\"", ",", "required", "=", "False", ",", "\n", "help", "=", "\"Audio file extension\"", ")", "\n", "\n", "self", ".", "add_argument", "(", "\"--no-copy-labels\"", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"Do not copy label files. Useful for large datasets, use --targetdir in wav2letter then.\"", ")", "\n", "self", ".", "add_argument", "(", "\"--use-feat\"", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"Use the feature vector ('z') instead of context vector ('c') for features\"", ")", "\n", "self", ".", "add_argument", "(", "\"--gpu\"", ",", "\n", "help", "=", "\"GPU to use\"", ",", "default", "=", "0", ",", "type", "=", "int", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.Prediction.__init__": [[81, 84], ["PretrainedWav2VecModel().cuda", "wav2vec_featurize.PretrainedWav2VecModel"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "fname", ",", "gpu", "=", "0", ")", ":", "\n", "        ", "self", ".", "gpu", "=", "gpu", "\n", "self", ".", "model", "=", "PretrainedWav2VecModel", "(", "fname", ")", ".", "cuda", "(", "gpu", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.Prediction.__call__": [[85, 91], ["torch.from_numpy().float().cuda", "torch.no_grad", "wav2vec_featurize.Prediction.model", "z.squeeze().cpu().numpy", "c.squeeze().cpu().numpy", "torch.from_numpy().float", "torch.from_numpy().float().cuda.unsqueeze", "z.squeeze().cpu", "c.squeeze().cpu", "torch.from_numpy", "z.squeeze", "c.squeeze"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.model"], ["", "def", "__call__", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "torch", ".", "from_numpy", "(", "x", ")", ".", "float", "(", ")", ".", "cuda", "(", "self", ".", "gpu", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "z", ",", "c", "=", "self", ".", "model", "(", "x", ".", "unsqueeze", "(", "0", ")", ")", "\n", "\n", "", "return", "z", ".", "squeeze", "(", "0", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "c", ".", "squeeze", "(", "0", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.__init__": [[96, 99], ["os.makedirs", "os.path.dirname"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "fname", ")", ":", "\n", "        ", "self", ".", "fname", "=", "fname", "\n", "os", ".", "makedirs", "(", "os", ".", "path", ".", "dirname", "(", "self", ".", "fname", ")", ",", "exist_ok", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write": [[100, 107], ["h5py.File", "data.T.flatten.T.flatten.T.flatten", "numpy.array"], "methods", ["None"], ["", "def", "write", "(", "self", ",", "data", ")", ":", "\n", "        ", "channel", ",", "T", "=", "data", ".", "shape", "\n", "\n", "with", "h5py", ".", "File", "(", "self", ".", "fname", ",", "\"w\"", ")", "as", "out_ds", ":", "\n", "            ", "data", "=", "data", ".", "T", ".", "flatten", "(", ")", "\n", "out_ds", "[", "\"features\"", "]", "=", "data", "\n", "out_ds", "[", "\"info\"", "]", "=", "np", ".", "array", "(", "[", "16e3", "//", "160", ",", "T", ",", "channel", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.EmbeddingDatasetWriter.__init__": [[121, 143], ["os.path.exists", "wav2vec_featurize.Prediction", "os.path.exists"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists"], ["def", "__init__", "(", "self", ",", "input_root", ",", "output_root", ",", "split", ",", "\n", "model_fname", ",", "\n", "extension", "=", "\"wav\"", ",", "\n", "gpu", "=", "0", ",", "\n", "verbose", "=", "False", ",", "\n", "use_feat", "=", "False", ",", "\n", ")", ":", "\n", "\n", "        ", "assert", "os", ".", "path", ".", "exists", "(", "model_fname", ")", "\n", "\n", "self", ".", "model_fname", "=", "model_fname", "\n", "self", ".", "model", "=", "Prediction", "(", "self", ".", "model_fname", ",", "gpu", ")", "\n", "\n", "self", ".", "input_root", "=", "input_root", "\n", "self", ".", "output_root", "=", "output_root", "\n", "self", ".", "split", "=", "split", "\n", "self", ".", "verbose", "=", "verbose", "\n", "self", ".", "extension", "=", "extension", "\n", "self", ".", "use_feat", "=", "use_feat", "\n", "\n", "assert", "os", ".", "path", ".", "exists", "(", "self", ".", "input_path", ")", ",", "\"Input path '{}' does not exist\"", ".", "format", "(", "self", ".", "input_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.EmbeddingDatasetWriter._progress": [[144, 148], ["tqdm.tqdm"], "methods", ["None"], ["", "def", "_progress", "(", "self", ",", "iterable", ",", "**", "kwargs", ")", ":", "\n", "        ", "if", "self", ".", "verbose", ":", "\n", "            ", "return", "tqdm", ".", "tqdm", "(", "iterable", ",", "**", "kwargs", ")", "\n", "", "return", "iterable", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.EmbeddingDatasetWriter.require_output_path": [[149, 152], ["wav2vec_featurize.EmbeddingDatasetWriter.get_output_path", "os.makedirs"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.EmbeddingDatasetWriter.get_output_path"], ["", "def", "require_output_path", "(", "self", ",", "fname", "=", "None", ")", ":", "\n", "        ", "path", "=", "self", ".", "get_output_path", "(", "fname", ")", "\n", "os", ".", "makedirs", "(", "path", ",", "exist_ok", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.EmbeddingDatasetWriter.input_path": [[153, 156], ["wav2vec_featurize.EmbeddingDatasetWriter.get_input_path"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.EmbeddingDatasetWriter.get_input_path"], ["", "@", "property", "\n", "def", "input_path", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "get_input_path", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.EmbeddingDatasetWriter.output_path": [[157, 160], ["wav2vec_featurize.EmbeddingDatasetWriter.get_output_path"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.EmbeddingDatasetWriter.get_output_path"], ["", "@", "property", "\n", "def", "output_path", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "get_output_path", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.EmbeddingDatasetWriter.get_input_path": [[161, 165], ["os.path.join", "os.path.join", "wav2vec_featurize.EmbeddingDatasetWriter.get_input_path"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.EmbeddingDatasetWriter.get_input_path"], ["", "def", "get_input_path", "(", "self", ",", "fname", "=", "None", ")", ":", "\n", "        ", "if", "fname", "is", "None", ":", "\n", "            ", "return", "os", ".", "path", ".", "join", "(", "self", ".", "input_root", ",", "self", ".", "split", ")", "\n", "", "return", "os", ".", "path", ".", "join", "(", "self", ".", "get_input_path", "(", ")", ",", "fname", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.EmbeddingDatasetWriter.get_output_path": [[166, 170], ["os.path.join", "os.path.join", "wav2vec_featurize.EmbeddingDatasetWriter.get_output_path"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.EmbeddingDatasetWriter.get_output_path"], ["", "def", "get_output_path", "(", "self", ",", "fname", "=", "None", ")", ":", "\n", "        ", "if", "fname", "is", "None", ":", "\n", "            ", "return", "os", ".", "path", ".", "join", "(", "self", ".", "output_root", ",", "self", ".", "split", ")", "\n", "", "return", "os", ".", "path", ".", "join", "(", "self", ".", "get_output_path", "(", ")", ",", "fname", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.EmbeddingDatasetWriter.copy_labels": [[171, 177], ["wav2vec_featurize.EmbeddingDatasetWriter.require_output_path", "list", "tqdm.tqdm", "filter", "shutil.copy", "glob.glob", "wav2vec_featurize.EmbeddingDatasetWriter.get_input_path"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.EmbeddingDatasetWriter.require_output_path", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.copy", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.EmbeddingDatasetWriter.get_input_path"], ["", "def", "copy_labels", "(", "self", ")", ":", "\n", "        ", "self", ".", "require_output_path", "(", ")", "\n", "\n", "labels", "=", "list", "(", "filter", "(", "lambda", "x", ":", "self", ".", "extension", "not", "in", "x", ",", "glob", ".", "glob", "(", "self", ".", "get_input_path", "(", "\"*\"", ")", ")", ")", ")", "\n", "for", "fname", "in", "tqdm", ".", "tqdm", "(", "labels", ")", ":", "\n", "            ", "copy", "(", "fname", ",", "self", ".", "output_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.EmbeddingDatasetWriter.input_fnames": [[178, 181], ["sorted", "glob.glob", "wav2vec_featurize.EmbeddingDatasetWriter.get_input_path"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.EmbeddingDatasetWriter.get_input_path"], ["", "", "@", "property", "\n", "def", "input_fnames", "(", "self", ")", ":", "\n", "        ", "return", "sorted", "(", "glob", ".", "glob", "(", "self", ".", "get_input_path", "(", "\"*.{}\"", ".", "format", "(", "self", ".", "extension", ")", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.EmbeddingDatasetWriter.__len__": [[182, 184], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "input_fnames", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.EmbeddingDatasetWriter.write_features": [[185, 198], ["map", "wav2vec_featurize.EmbeddingDatasetWriter._progress", "map", "zip", "wav2vec_featurize.read_audio", "wav2vec_featurize.EmbeddingDatasetWriter.model", "wav2vec_featurize.H5Writer", "wav2vec_featurize.H5Writer.write", "os.path.join", "len", "x.replace"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.EmbeddingDatasetWriter._progress", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.read_audio", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.model", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write"], ["", "def", "write_features", "(", "self", ")", ":", "\n", "\n", "        ", "paths", "=", "self", ".", "input_fnames", "\n", "\n", "fnames_context", "=", "map", "(", "lambda", "x", ":", "os", ".", "path", ".", "join", "(", "self", ".", "output_path", ",", "x", ".", "replace", "(", "\".\"", "+", "self", ".", "extension", ",", "\".h5context\"", ")", ")", ",", "map", "(", "os", ".", "path", ".", "basename", ",", "paths", ")", ")", "\n", "\n", "for", "name", ",", "target_fname", "in", "self", ".", "_progress", "(", "zip", "(", "paths", ",", "fnames_context", ")", ",", "total", "=", "len", "(", "self", ")", ")", ":", "\n", "            ", "wav", ",", "sr", "=", "read_audio", "(", "name", ")", "\n", "z", ",", "c", "=", "self", ".", "model", "(", "wav", ")", "\n", "feat", "=", "z", "if", "self", ".", "use_feat", "else", "c", "\n", "writer", "=", "H5Writer", "(", "target_fname", ")", "\n", "writer", ".", "write", "(", "feat", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.EmbeddingDatasetWriter.__repr__": [[199, 203], ["len"], "methods", ["None"], ["", "", "def", "__repr__", "(", "self", ")", ":", "\n", "\n", "        ", "return", "\"EmbeddingDatasetWriter ({n_files} files)\\n\\tinput:\\t{input_root}\\n\\toutput:\\t{output_root}\\n\\tsplit:\\t{split})\"", ".", "format", "(", "\n", "n_files", "=", "len", "(", "self", ")", ",", "**", "self", ".", "__dict__", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.read_audio": [[21, 28], ["soundfile.read"], "function", ["None"], ["def", "read_audio", "(", "fname", ")", ":", "\n", "    ", "\"\"\" Load an audio file and return PCM along with the sample rate \"\"\"", "\n", "\n", "wav", ",", "sr", "=", "sf", ".", "read", "(", "fname", ")", "\n", "assert", "sr", "==", "16e3", "\n", "\n", "return", "wav", ",", "16e3", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.shard_docs.main": [[17, 50], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "open", "contextlib.ExitStack", "shard_docs.main.output_doc"], "function", ["None"], ["def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'input'", ")", "\n", "parser", ".", "add_argument", "(", "'--num-shards'", ",", "type", "=", "int", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "assert", "args", ".", "num_shards", "is", "not", "None", "and", "args", ".", "num_shards", ">", "1", "\n", "\n", "with", "open", "(", "args", ".", "input", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "h", ":", "\n", "        ", "with", "contextlib", ".", "ExitStack", "(", ")", "as", "stack", ":", "\n", "            ", "outputs", "=", "[", "\n", "stack", ".", "enter_context", "(", "open", "(", "args", ".", "input", "+", "\".shard\"", "+", "str", "(", "i", ")", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", ")", "\n", "for", "i", "in", "range", "(", "args", ".", "num_shards", ")", "\n", "]", "\n", "\n", "doc", "=", "[", "]", "\n", "first_doc", "=", "[", "True", "]", "*", "args", ".", "num_shards", "\n", "def", "output_doc", "(", "i", ")", ":", "\n", "                ", "if", "not", "first_doc", "[", "i", "]", ":", "\n", "                    ", "outputs", "[", "i", "]", ".", "write", "(", "\"\\n\"", ")", "\n", "", "first_doc", "[", "i", "]", "=", "False", "\n", "for", "line", "in", "doc", ":", "\n", "                    ", "outputs", "[", "i", "]", ".", "write", "(", "line", ")", "\n", "", "doc", ".", "clear", "(", ")", "\n", "\n", "", "num_docs", "=", "0", "\n", "for", "line", "in", "h", ":", "\n", "                ", "if", "line", ".", "strip", "(", ")", "==", "\"\"", ":", "# empty line indicates new document", "\n", "                    ", "output_doc", "(", "num_docs", "%", "args", ".", "num_shards", ")", "\n", "num_docs", "+=", "1", "\n", "", "else", ":", "\n", "                    ", "doc", ".", "append", "(", "line", ")", "\n", "", "", "output_doc", "(", "num_docs", "%", "args", ".", "num_shards", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.spm_encode.main": [[17, 96], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "sentencepiece.SentencePieceProcessor", "spm.SentencePieceProcessor.Load", "len", "len", "contextlib.ExitStack", "enumerate", "print", "print", "spm.SentencePieceProcessor.EncodeAsPieces", "encode.strip", "zip", "list", "list", "stack.enter_context", "stack.enter_context", "len", "spm_encode.main.encode"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.encode"], ["def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--model\"", ",", "required", "=", "True", ",", "\n", "help", "=", "\"sentencepiece model to use for encoding\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--inputs\"", ",", "nargs", "=", "\"+\"", ",", "default", "=", "[", "'-'", "]", ",", "\n", "help", "=", "\"input files to filter/encode\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--outputs\"", ",", "nargs", "=", "\"+\"", ",", "default", "=", "[", "'-'", "]", ",", "\n", "help", "=", "\"path to save encoded outputs\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_format\"", ",", "choices", "=", "[", "\"piece\"", ",", "\"id\"", "]", ",", "default", "=", "\"piece\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--min-len\"", ",", "type", "=", "int", ",", "metavar", "=", "\"N\"", ",", "\n", "help", "=", "\"filter sentence pairs with fewer than N tokens\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max-len\"", ",", "type", "=", "int", ",", "metavar", "=", "\"N\"", ",", "\n", "help", "=", "\"filter sentence pairs with more than N tokens\"", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "assert", "len", "(", "args", ".", "inputs", ")", "==", "len", "(", "args", ".", "outputs", ")", ",", "\"number of input and output paths should match\"", "\n", "\n", "sp", "=", "spm", ".", "SentencePieceProcessor", "(", ")", "\n", "sp", ".", "Load", "(", "args", ".", "model", ")", "\n", "\n", "if", "args", ".", "output_format", "==", "\"piece\"", ":", "\n", "        ", "def", "encode", "(", "l", ")", ":", "\n", "            ", "return", "sp", ".", "EncodeAsPieces", "(", "l", ")", "\n", "", "", "elif", "args", ".", "output_format", "==", "\"id\"", ":", "\n", "        ", "def", "encode", "(", "l", ")", ":", "\n", "            ", "return", "list", "(", "map", "(", "str", ",", "sp", ".", "EncodeAsIds", "(", "l", ")", ")", ")", "\n", "", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n", "", "if", "args", ".", "min_len", "is", "not", "None", "or", "args", ".", "max_len", "is", "not", "None", ":", "\n", "        ", "def", "valid", "(", "line", ")", ":", "\n", "            ", "return", "(", "\n", "(", "args", ".", "min_len", "is", "None", "or", "len", "(", "line", ")", ">=", "args", ".", "min_len", ")", "\n", "and", "(", "args", ".", "max_len", "is", "None", "or", "len", "(", "line", ")", "<=", "args", ".", "max_len", ")", "\n", ")", "\n", "", "", "else", ":", "\n", "        ", "def", "valid", "(", "lines", ")", ":", "\n", "            ", "return", "True", "\n", "\n", "", "", "with", "contextlib", ".", "ExitStack", "(", ")", "as", "stack", ":", "\n", "        ", "inputs", "=", "[", "\n", "stack", ".", "enter_context", "(", "open", "(", "input", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", ")", "if", "input", "!=", "\"-\"", "else", "sys", ".", "stdin", "\n", "for", "input", "in", "args", ".", "inputs", "\n", "]", "\n", "outputs", "=", "[", "\n", "stack", ".", "enter_context", "(", "open", "(", "output", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", ")", "if", "output", "!=", "\"-\"", "else", "sys", ".", "stdout", "\n", "for", "output", "in", "args", ".", "outputs", "\n", "]", "\n", "\n", "stats", "=", "{", "\n", "\"num_empty\"", ":", "0", ",", "\n", "\"num_filtered\"", ":", "0", ",", "\n", "}", "\n", "\n", "def", "encode_line", "(", "line", ")", ":", "\n", "            ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "len", "(", "line", ")", ">", "0", ":", "\n", "                ", "line", "=", "encode", "(", "line", ")", "\n", "if", "valid", "(", "line", ")", ":", "\n", "                    ", "return", "line", "\n", "", "else", ":", "\n", "                    ", "stats", "[", "\"num_filtered\"", "]", "+=", "1", "\n", "", "", "else", ":", "\n", "                ", "stats", "[", "\"num_empty\"", "]", "+=", "1", "\n", "", "return", "None", "\n", "\n", "", "for", "i", ",", "lines", "in", "enumerate", "(", "zip", "(", "*", "inputs", ")", ",", "start", "=", "1", ")", ":", "\n", "            ", "enc_lines", "=", "list", "(", "map", "(", "encode_line", ",", "lines", ")", ")", "\n", "if", "not", "any", "(", "enc_line", "is", "None", "for", "enc_line", "in", "enc_lines", ")", ":", "\n", "                ", "for", "enc_line", ",", "output_h", "in", "zip", "(", "enc_lines", ",", "outputs", ")", ":", "\n", "                    ", "print", "(", "\" \"", ".", "join", "(", "enc_line", ")", ",", "file", "=", "output_h", ")", "\n", "", "", "if", "i", "%", "10000", "==", "0", ":", "\n", "                ", "print", "(", "\"processed {} lines\"", ".", "format", "(", "i", ")", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "\n", "", "", "print", "(", "\"skipped {} empty lines\"", ".", "format", "(", "stats", "[", "\"num_empty\"", "]", ")", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "print", "(", "\"filtered {} lines\"", ".", "format", "(", "stats", "[", "\"num_filtered\"", "]", ")", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.read_binarized.get_parser": [[12, 23], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "fairseq.data.indexed_dataset.get_available_dataset_impl"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.get_available_dataset_impl"], ["def", "get_parser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "description", "=", "'writes text from binarized file to stdout'", ")", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'--dataset-impl'", ",", "help", "=", "'dataset implementation'", ",", "\n", "choices", "=", "indexed_dataset", ".", "get_available_dataset_impl", "(", ")", ")", "\n", "parser", ".", "add_argument", "(", "'--dict'", ",", "metavar", "=", "'FP'", ",", "help", "=", "'dictionary containing known words'", ",", "default", "=", "None", ")", "\n", "parser", ".", "add_argument", "(", "'--input'", ",", "metavar", "=", "'FP'", ",", "required", "=", "True", ",", "help", "=", "'binarized file to read'", ")", "\n", "# fmt: on", "\n", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.read_binarized.main": [[25, 44], ["read_binarized.get_parser", "get_parser.parse_args", "fairseq.data.data_utils.load_indexed_dataset", "fairseq.data.Dictionary.load", "print", "dictionary.string", "str", "int"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.get_parser", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.data_utils.load_indexed_dataset", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.hub_utils.GeneratorHubInterface.string"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "get_parser", "(", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "dictionary", "=", "Dictionary", ".", "load", "(", "args", ".", "dict", ")", "if", "args", ".", "dict", "is", "not", "None", "else", "None", "\n", "dataset", "=", "data_utils", ".", "load_indexed_dataset", "(", "\n", "args", ".", "input", ",", "\n", "dictionary", ",", "\n", "dataset_impl", "=", "args", ".", "dataset_impl", ",", "\n", "default", "=", "'lazy'", ",", "\n", ")", "\n", "\n", "for", "tensor_line", "in", "dataset", ":", "\n", "        ", "if", "dictionary", "is", "None", ":", "\n", "            ", "line", "=", "' '", ".", "join", "(", "[", "str", "(", "int", "(", "x", ")", ")", "for", "x", "in", "tensor_line", "]", ")", "\n", "", "else", ":", "\n", "            ", "line", "=", "dictionary", ".", "string", "(", "tensor_line", ")", "\n", "\n", "", "print", "(", "line", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.compare_namespaces.main": [[7, 41], ["eval", "eval", "compare_namespaces.main.keys"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval"], ["def", "main", "(", ")", ":", "\n", "\n", "    ", "ns1", "=", "eval", "(", "input", "(", "'Namespace 1: '", ")", ")", "\n", "ns2", "=", "eval", "(", "input", "(", "'Namespace 2: '", ")", ")", "\n", "\n", "def", "keys", "(", "ns", ")", ":", "\n", "        ", "ks", "=", "set", "(", ")", "\n", "for", "k", "in", "dir", "(", "ns", ")", ":", "\n", "            ", "if", "not", "k", ".", "startswith", "(", "'_'", ")", ":", "\n", "                ", "ks", ".", "add", "(", "k", ")", "\n", "", "", "return", "ks", "\n", "\n", "", "k1", "=", "keys", "(", "ns1", ")", "\n", "k2", "=", "keys", "(", "ns2", ")", "\n", "\n", "def", "print_keys", "(", "ks", ",", "ns1", ",", "ns2", "=", "None", ")", ":", "\n", "        ", "for", "k", "in", "ks", ":", "\n", "            ", "if", "ns2", "is", "None", ":", "\n", "                ", "print", "(", "'{}\\t{}'", ".", "format", "(", "k", ",", "getattr", "(", "ns1", ",", "k", ",", "None", ")", ")", ")", "\n", "", "else", ":", "\n", "                ", "print", "(", "'{}\\t{}\\t{}'", ".", "format", "(", "k", ",", "getattr", "(", "ns1", ",", "k", ",", "None", ")", ",", "getattr", "(", "ns2", ",", "k", ",", "None", ")", ")", ")", "\n", "\n", "", "", "", "print", "(", "'Keys unique to namespace 1:'", ")", "\n", "print_keys", "(", "k1", "-", "k2", ",", "ns1", ")", "\n", "print", "(", ")", "\n", "\n", "print", "(", "'Keys unique to namespace 2:'", ")", "\n", "print_keys", "(", "k2", "-", "k1", ",", "ns2", ")", "\n", "print", "(", ")", "\n", "\n", "print", "(", "'Overlapping keys with different values:'", ")", "\n", "ks", "=", "[", "k", "for", "k", "in", "k1", "&", "k2", "if", "getattr", "(", "ns1", ",", "k", ",", "'None'", ")", "!=", "getattr", "(", "ns2", ",", "k", ",", "'None'", ")", "]", "\n", "print_keys", "(", "ks", ",", "ns1", ",", "ns2", ")", "\n", "print", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_manifest.get_parser": [[17, 28], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument"], "function", ["None"], ["def", "get_parser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'root'", ",", "metavar", "=", "'DIR'", ",", "help", "=", "'root directory containing flac files to index'", ")", "\n", "parser", ".", "add_argument", "(", "'--valid-percent'", ",", "default", "=", "0.01", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'percentage of data to use as validation set (between 0 and 1)'", ")", "\n", "parser", ".", "add_argument", "(", "'--dest'", ",", "default", "=", "'.'", ",", "type", "=", "str", ",", "metavar", "=", "'DIR'", ",", "help", "=", "'output directory'", ")", "\n", "parser", ".", "add_argument", "(", "'--ext'", ",", "default", "=", "'flac'", ",", "type", "=", "str", ",", "metavar", "=", "'EXT'", ",", "help", "=", "'extension to look for'", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "default", "=", "42", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "help", "=", "'random seed'", ")", "\n", "parser", ".", "add_argument", "(", "'--path-must-contain'", ",", "default", "=", "None", ",", "type", "=", "str", ",", "metavar", "=", "'FRAG'", ",", "\n", "help", "=", "'if set, path must contain this substring for a file to be included in the manifest'", ")", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_manifest.main": [[30, 51], ["os.path.realpath", "os.path.join", "random.Random", "open", "open", "print", "print", "glob.iglob", "os.path.join", "os.path.join", "os.path.realpath", "print", "soundfile.info", "random.Random.random", "os.path.relpath"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["", "def", "main", "(", "args", ")", ":", "\n", "    ", "assert", "args", ".", "valid_percent", ">=", "0", "and", "args", ".", "valid_percent", "<=", "1.", "\n", "\n", "dir_path", "=", "os", ".", "path", ".", "realpath", "(", "args", ".", "root", ")", "\n", "search_path", "=", "os", ".", "path", ".", "join", "(", "dir_path", ",", "'**/*.'", "+", "args", ".", "ext", ")", "\n", "rand", "=", "random", ".", "Random", "(", "args", ".", "seed", ")", "\n", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "dest", ",", "'train.tsv'", ")", ",", "'w'", ")", "as", "train_f", ",", "open", "(", "\n", "os", ".", "path", ".", "join", "(", "args", ".", "dest", ",", "'valid.tsv'", ")", ",", "'w'", ")", "as", "valid_f", ":", "\n", "        ", "print", "(", "dir_path", ",", "file", "=", "train_f", ")", "\n", "print", "(", "dir_path", ",", "file", "=", "valid_f", ")", "\n", "\n", "for", "fname", "in", "glob", ".", "iglob", "(", "search_path", ",", "recursive", "=", "True", ")", ":", "\n", "            ", "file_path", "=", "os", ".", "path", ".", "realpath", "(", "fname", ")", "\n", "\n", "if", "args", ".", "path_must_contain", "and", "args", ".", "path_must_contain", "not", "in", "file_path", ":", "\n", "                ", "continue", "\n", "\n", "", "frames", "=", "soundfile", ".", "info", "(", "fname", ")", ".", "frames", "\n", "dest", "=", "train_f", "if", "rand", ".", "random", "(", ")", ">", "args", ".", "valid_percent", "else", "valid_f", "\n", "print", "(", "'{}\\t{}'", ".", "format", "(", "os", ".", "path", ".", "relpath", "(", "file_path", ",", "dir_path", ")", ",", "frames", ")", ",", "file", "=", "dest", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.count_docs.main": [[19, 56], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "print", "print", "print", "count_docs.main.gopen"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'input'", ")", "\n", "parser", ".", "add_argument", "(", "'--gzip'", ",", "action", "=", "'store_true'", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "def", "gopen", "(", ")", ":", "\n", "        ", "if", "args", ".", "gzip", ":", "\n", "            ", "return", "gzip", ".", "open", "(", "args", ".", "input", ",", "'r'", ")", "\n", "", "else", ":", "\n", "            ", "return", "open", "(", "args", ".", "input", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "\n", "\n", "", "", "num_lines", "=", "[", "]", "\n", "num_toks", "=", "[", "]", "\n", "with", "gopen", "(", ")", "as", "h", ":", "\n", "        ", "num_docs", "=", "1", "\n", "num_lines_in_doc", "=", "0", "\n", "num_toks_in_doc", "=", "0", "\n", "for", "i", ",", "line", "in", "enumerate", "(", "h", ")", ":", "\n", "            ", "if", "len", "(", "line", ".", "strip", "(", ")", ")", "==", "0", ":", "# empty line indicates new document", "\n", "                ", "num_docs", "+=", "1", "\n", "num_lines", ".", "append", "(", "num_lines_in_doc", ")", "\n", "num_toks", ".", "append", "(", "num_toks_in_doc", ")", "\n", "num_lines_in_doc", "=", "0", "\n", "num_toks_in_doc", "=", "0", "\n", "", "else", ":", "\n", "                ", "num_lines_in_doc", "+=", "1", "\n", "num_toks_in_doc", "+=", "len", "(", "line", ".", "rstrip", "(", ")", ".", "split", "(", ")", ")", "\n", "", "if", "i", "%", "1000000", "==", "0", ":", "\n", "                ", "print", "(", "i", ",", "file", "=", "sys", ".", "stderr", ",", "end", "=", "\"\"", ",", "flush", "=", "True", ")", "\n", "", "elif", "i", "%", "100000", "==", "0", ":", "\n", "                ", "print", "(", "\".\"", ",", "file", "=", "sys", ".", "stderr", ",", "end", "=", "\"\"", ",", "flush", "=", "True", ")", "\n", "", "", "print", "(", "file", "=", "sys", ".", "stderr", ",", "flush", "=", "True", ")", "\n", "\n", "", "print", "(", "\"found {} docs\"", ".", "format", "(", "num_docs", ")", ")", "\n", "print", "(", "\"average num lines per doc: {}\"", ".", "format", "(", "np", ".", "mean", "(", "num_lines", ")", ")", ")", "\n", "print", "(", "\"average num toks per doc: {}\"", ".", "format", "(", "np", ".", "mean", "(", "num_toks", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.rm_pt.parse_checkpoints": [[19, 30], ["pt_regexp_epoch_based.fullmatch", "entries.append", "pt_regexp_update_based.fullmatch", "entries.append", "int", "pt_regexp_update_based.fullmatch.group", "pt_regexp_update_based.fullmatch.group", "int", "pt_regexp_update_based.fullmatch.group", "pt_regexp_update_based.fullmatch.group"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["def", "parse_checkpoints", "(", "files", ")", ":", "\n", "    ", "entries", "=", "[", "]", "\n", "for", "f", "in", "files", ":", "\n", "        ", "m", "=", "pt_regexp_epoch_based", ".", "fullmatch", "(", "f", ")", "\n", "if", "m", "is", "not", "None", ":", "\n", "            ", "entries", ".", "append", "(", "(", "int", "(", "m", ".", "group", "(", "1", ")", ")", ",", "m", ".", "group", "(", "0", ")", ")", ")", "\n", "", "else", ":", "\n", "            ", "m", "=", "pt_regexp_update_based", ".", "fullmatch", "(", "f", ")", "\n", "if", "m", "is", "not", "None", ":", "\n", "                ", "entries", ".", "append", "(", "(", "int", "(", "m", ".", "group", "(", "1", ")", ")", ",", "m", ".", "group", "(", "0", ")", ")", ")", "\n", "", "", "", "return", "entries", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.rm_pt.last_n_checkpoints": [[32, 35], ["rm_pt.parse_checkpoints", "sorted"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.rm_pt.parse_checkpoints"], ["", "def", "last_n_checkpoints", "(", "files", ",", "n", ")", ":", "\n", "    ", "entries", "=", "parse_checkpoints", "(", "files", ")", "\n", "return", "[", "x", "[", "1", "]", "for", "x", "in", "sorted", "(", "entries", ",", "reverse", "=", "True", ")", "[", ":", "n", "]", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.rm_pt.every_n_checkpoints": [[37, 40], ["rm_pt.parse_checkpoints", "sorted", "sorted"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.rm_pt.parse_checkpoints"], ["", "def", "every_n_checkpoints", "(", "files", ",", "n", ")", ":", "\n", "    ", "entries", "=", "parse_checkpoints", "(", "files", ")", "\n", "return", "[", "x", "[", "1", "]", "for", "x", "in", "sorted", "(", "sorted", "(", "entries", ")", "[", ":", ":", "-", "n", "]", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.rm_pt.main": [[42, 129], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "sorted", "sorted", "sorted", "print", "print", "os.walk", "print", "sys.exit", "len", "len", "len", "input", "len", "len", "len", "len", "print", "print", "print", "input.strip().lower", "os.path.realpath", "print", "os.remove", "print", "shutil.copyfile", "print", "os.remove", "rm_pt.last_n_checkpoints", "rm_pt.every_n_checkpoints", "os.path.join", "input.strip().lower", "sys.exit", "pt_regexp.fullmatch", "sorted.append", "input.strip", "os.path.islink", "sorted.append", "sorted.append", "input.strip", "os.path.basename().startswith", "os.path.basename"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.average_checkpoints.last_n_checkpoints", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.rm_pt.every_n_checkpoints", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "description", "=", "(", "\n", "'Recursively delete checkpoint files from `root_dir`, '", "\n", "'but preserve checkpoint_best.pt and checkpoint_last.pt'", "\n", ")", "\n", ")", "\n", "parser", ".", "add_argument", "(", "'root_dirs'", ",", "nargs", "=", "'*'", ")", "\n", "parser", ".", "add_argument", "(", "'--save-last'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "help", "=", "'number of last checkpoints to save'", ")", "\n", "parser", ".", "add_argument", "(", "'--save-every'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "help", "=", "'interval of checkpoints to save'", ")", "\n", "parser", ".", "add_argument", "(", "'--preserve-test'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'preserve checkpoints in dirs that start with test_ prefix (default: delete them)'", ")", "\n", "parser", ".", "add_argument", "(", "'--delete-best'", ",", "action", "=", "'store_true'", ",", "help", "=", "'delete checkpoint_best.pt'", ")", "\n", "parser", ".", "add_argument", "(", "'--delete-last'", ",", "action", "=", "'store_true'", ",", "help", "=", "'delete checkpoint_last.pt'", ")", "\n", "parser", ".", "add_argument", "(", "'--no-dereference'", ",", "action", "=", "'store_true'", ",", "help", "=", "'don\\'t dereference symlinks'", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "files_to_desymlink", "=", "[", "]", "\n", "files_to_preserve", "=", "[", "]", "\n", "files_to_delete", "=", "[", "]", "\n", "for", "root_dir", "in", "args", ".", "root_dirs", ":", "\n", "        ", "for", "root", ",", "_subdirs", ",", "files", "in", "os", ".", "walk", "(", "root_dir", ")", ":", "\n", "            ", "if", "args", ".", "save_last", ">", "0", ":", "\n", "                ", "to_save", "=", "last_n_checkpoints", "(", "files", ",", "args", ".", "save_last", ")", "\n", "", "else", ":", "\n", "                ", "to_save", "=", "[", "]", "\n", "", "if", "args", ".", "save_every", ">", "0", ":", "\n", "                ", "to_save", "+=", "every_n_checkpoints", "(", "files", ",", "args", ".", "save_every", ")", "\n", "", "for", "file", "in", "files", ":", "\n", "                ", "if", "not", "pt_regexp", ".", "fullmatch", "(", "file", ")", ":", "\n", "                    ", "continue", "\n", "", "full_path", "=", "os", ".", "path", ".", "join", "(", "root", ",", "file", ")", "\n", "if", "(", "\n", "(", "\n", "not", "os", ".", "path", ".", "basename", "(", "root", ")", ".", "startswith", "(", "'test_'", ")", "\n", "or", "args", ".", "preserve_test", "\n", ")", "\n", "and", "(", "\n", "(", "file", "==", "'checkpoint_last.pt'", "and", "not", "args", ".", "delete_last", ")", "\n", "or", "(", "file", "==", "'checkpoint_best.pt'", "and", "not", "args", ".", "delete_best", ")", "\n", "or", "file", "in", "to_save", "\n", ")", "\n", ")", ":", "\n", "                    ", "if", "os", ".", "path", ".", "islink", "(", "full_path", ")", "and", "not", "args", ".", "no_dereference", ":", "\n", "                        ", "files_to_desymlink", ".", "append", "(", "full_path", ")", "\n", "", "else", ":", "\n", "                        ", "files_to_preserve", ".", "append", "(", "full_path", ")", "\n", "", "", "else", ":", "\n", "                    ", "files_to_delete", ".", "append", "(", "full_path", ")", "\n", "\n", "", "", "", "", "if", "len", "(", "files_to_desymlink", ")", "==", "0", "and", "len", "(", "files_to_delete", ")", "==", "0", ":", "\n", "        ", "print", "(", "'Nothing to do.'", ")", "\n", "sys", ".", "exit", "(", "0", ")", "\n", "\n", "", "files_to_desymlink", "=", "sorted", "(", "files_to_desymlink", ")", "\n", "files_to_preserve", "=", "sorted", "(", "files_to_preserve", ")", "\n", "files_to_delete", "=", "sorted", "(", "files_to_delete", ")", "\n", "\n", "print", "(", "'Operations to perform (in order):'", ")", "\n", "if", "len", "(", "files_to_desymlink", ")", ">", "0", ":", "\n", "        ", "for", "file", "in", "files_to_desymlink", ":", "\n", "            ", "print", "(", "' - preserve (and dereference symlink): '", "+", "file", ")", "\n", "", "", "if", "len", "(", "files_to_preserve", ")", ">", "0", ":", "\n", "        ", "for", "file", "in", "files_to_preserve", ":", "\n", "            ", "print", "(", "' - preserve: '", "+", "file", ")", "\n", "", "", "if", "len", "(", "files_to_delete", ")", ">", "0", ":", "\n", "        ", "for", "file", "in", "files_to_delete", ":", "\n", "            ", "print", "(", "' - delete: '", "+", "file", ")", "\n", "", "", "while", "True", ":", "\n", "        ", "resp", "=", "input", "(", "'Continue? (Y/N): '", ")", "\n", "if", "resp", ".", "strip", "(", ")", ".", "lower", "(", ")", "==", "'y'", ":", "\n", "            ", "break", "\n", "", "elif", "resp", ".", "strip", "(", ")", ".", "lower", "(", ")", "==", "'n'", ":", "\n", "            ", "sys", ".", "exit", "(", "0", ")", "\n", "\n", "", "", "print", "(", "'Executing...'", ")", "\n", "if", "len", "(", "files_to_desymlink", ")", ">", "0", ":", "\n", "        ", "for", "file", "in", "files_to_desymlink", ":", "\n", "            ", "realpath", "=", "os", ".", "path", ".", "realpath", "(", "file", ")", "\n", "print", "(", "'rm '", "+", "file", ")", "\n", "os", ".", "remove", "(", "file", ")", "\n", "print", "(", "'cp {} {}'", ".", "format", "(", "realpath", ",", "file", ")", ")", "\n", "shutil", ".", "copyfile", "(", "realpath", ",", "file", ")", "\n", "", "", "if", "len", "(", "files_to_delete", ")", ">", "0", ":", "\n", "        ", "for", "file", "in", "files_to_delete", ":", "\n", "            ", "print", "(", "'rm '", "+", "file", ")", "\n", "os", ".", "remove", "(", "file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.split_train_valid_docs.main": [[16, 78], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "print", "doc.clear", "open", "enumerate", "len", "open", "open", "len", "sample.append", "random.randrange", "len", "split_train_valid_docs.main.update_sample"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.clear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'input'", ")", "\n", "parser", ".", "add_argument", "(", "'sample_output'", ",", "help", "=", "'train output file'", ")", "\n", "parser", ".", "add_argument", "(", "'remainder_output'", ",", "help", "=", "'valid output file'", ")", "\n", "parser", ".", "add_argument", "(", "'-k'", ",", "type", "=", "int", ",", "help", "=", "\"remainder size\"", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "assert", "args", ".", "k", "is", "not", "None", "\n", "\n", "sample", "=", "[", "]", "\n", "remainder", "=", "[", "]", "\n", "num_docs", "=", "[", "0", "]", "\n", "\n", "def", "update_sample", "(", "doc", ")", ":", "\n", "        ", "if", "len", "(", "sample", ")", "<", "args", ".", "k", ":", "\n", "            ", "sample", ".", "append", "(", "doc", ".", "copy", "(", ")", ")", "\n", "", "else", ":", "\n", "            ", "i", "=", "num_docs", "[", "0", "]", "\n", "j", "=", "random", ".", "randrange", "(", "i", "+", "1", ")", "\n", "if", "j", "<", "args", ".", "k", ":", "\n", "                ", "remainder", ".", "append", "(", "sample", "[", "j", "]", ")", "\n", "sample", "[", "j", "]", "=", "doc", ".", "copy", "(", ")", "\n", "", "else", ":", "\n", "                ", "remainder", ".", "append", "(", "doc", ".", "copy", "(", ")", ")", "\n", "", "", "num_docs", "[", "0", "]", "+=", "1", "\n", "doc", ".", "clear", "(", ")", "\n", "\n", "", "with", "open", "(", "args", ".", "input", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "h", ":", "\n", "        ", "doc", "=", "[", "]", "\n", "for", "i", ",", "line", "in", "enumerate", "(", "h", ")", ":", "\n", "            ", "if", "line", ".", "strip", "(", ")", "==", "\"\"", ":", "# empty line indicates new document", "\n", "                ", "update_sample", "(", "doc", ")", "\n", "", "else", ":", "\n", "                ", "doc", ".", "append", "(", "line", ")", "\n", "", "if", "i", "%", "1000000", "==", "0", ":", "\n", "                ", "print", "(", "i", ",", "file", "=", "sys", ".", "stderr", ",", "end", "=", "\"\"", ",", "flush", "=", "True", ")", "\n", "", "elif", "i", "%", "100000", "==", "0", ":", "\n", "                ", "print", "(", "\".\"", ",", "file", "=", "sys", ".", "stderr", ",", "end", "=", "\"\"", ",", "flush", "=", "True", ")", "\n", "", "", "if", "len", "(", "doc", ")", ">", "0", ":", "\n", "            ", "update_sample", "(", "doc", ")", "\n", "", "", "print", "(", "file", "=", "sys", ".", "stderr", ",", "flush", "=", "True", ")", "\n", "\n", "assert", "len", "(", "sample", ")", "==", "args", ".", "k", "\n", "\n", "with", "open", "(", "args", ".", "sample_output", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "out", ":", "\n", "        ", "first", "=", "True", "\n", "for", "doc", "in", "sample", ":", "\n", "            ", "if", "not", "first", ":", "\n", "                ", "out", ".", "write", "(", "\"\\n\"", ")", "\n", "", "first", "=", "False", "\n", "for", "line", "in", "doc", ":", "\n", "                ", "out", ".", "write", "(", "line", ")", "\n", "\n", "", "", "", "with", "open", "(", "args", ".", "remainder_output", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "out", ":", "\n", "        ", "first", "=", "True", "\n", "for", "doc", "in", "remainder", ":", "\n", "            ", "if", "not", "first", ":", "\n", "                ", "out", ".", "write", "(", "\"\\n\"", ")", "\n", "", "first", "=", "False", "\n", "for", "line", "in", "doc", ":", "\n", "                ", "out", ".", "write", "(", "line", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.build_sym_alignment.main": [[29, 94], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "open", "open", "os.system", "os.system", "os.system", "open", "itertools.zip_longest", "print", "s.strip", "t.strip"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'symmetric alignment builer'", ")", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'--fast_align_dir'", ",", "\n", "help", "=", "'path to fast_align build directory'", ")", "\n", "parser", ".", "add_argument", "(", "'--mosesdecoder_dir'", ",", "\n", "help", "=", "'path to mosesdecoder root directory'", ")", "\n", "parser", ".", "add_argument", "(", "'--sym_heuristic'", ",", "\n", "help", "=", "'heuristic to use for symmetrization'", ",", "\n", "default", "=", "'grow-diag-final-and'", ")", "\n", "parser", ".", "add_argument", "(", "'--source_file'", ",", "\n", "help", "=", "'path to a file with sentences '", "\n", "'in the source language'", ")", "\n", "parser", ".", "add_argument", "(", "'--target_file'", ",", "\n", "help", "=", "'path to a file with sentences '", "\n", "'in the target language'", ")", "\n", "parser", ".", "add_argument", "(", "'--output_dir'", ",", "\n", "help", "=", "'output directory'", ")", "\n", "# fmt: on", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "fast_align_bin", "=", "os", ".", "path", ".", "join", "(", "args", ".", "fast_align_dir", ",", "'fast_align'", ")", "\n", "symal_bin", "=", "os", ".", "path", ".", "join", "(", "args", ".", "mosesdecoder_dir", ",", "'bin'", ",", "'symal'", ")", "\n", "sym_fast_align_bin", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "mosesdecoder_dir", ",", "'scripts'", ",", "'ems'", ",", "\n", "'support'", ",", "'symmetrize-fast-align.perl'", ")", "\n", "\n", "# create joined file", "\n", "joined_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "'text.joined'", ")", "\n", "with", "open", "(", "args", ".", "source_file", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "src", ",", "open", "(", "args", ".", "target_file", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "tgt", ":", "\n", "        ", "with", "open", "(", "joined_file", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "joined", ":", "\n", "            ", "for", "s", ",", "t", "in", "zip_longest", "(", "src", ",", "tgt", ")", ":", "\n", "                ", "print", "(", "'{} ||| {}'", ".", "format", "(", "s", ".", "strip", "(", ")", ",", "t", ".", "strip", "(", ")", ")", ",", "file", "=", "joined", ")", "\n", "\n", "", "", "", "bwd_align_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "'align.backward'", ")", "\n", "\n", "# run forward alignment", "\n", "fwd_align_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "'align.forward'", ")", "\n", "fwd_fast_align_cmd", "=", "'{FASTALIGN} -i {JOINED} -d -o -v > {FWD}'", ".", "format", "(", "\n", "FASTALIGN", "=", "fast_align_bin", ",", "\n", "JOINED", "=", "joined_file", ",", "\n", "FWD", "=", "fwd_align_file", ")", "\n", "assert", "os", ".", "system", "(", "fwd_fast_align_cmd", ")", "==", "0", "\n", "\n", "# run backward alignment", "\n", "bwd_align_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "'align.backward'", ")", "\n", "bwd_fast_align_cmd", "=", "'{FASTALIGN} -i {JOINED} -d -o -v -r > {BWD}'", ".", "format", "(", "\n", "FASTALIGN", "=", "fast_align_bin", ",", "\n", "JOINED", "=", "joined_file", ",", "\n", "BWD", "=", "bwd_align_file", ")", "\n", "assert", "os", ".", "system", "(", "bwd_fast_align_cmd", ")", "==", "0", "\n", "\n", "# run symmetrization", "\n", "sym_out_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "'aligned'", ")", "\n", "sym_cmd", "=", "'{SYMFASTALIGN} {FWD} {BWD} {SRC} {TGT} {OUT} {HEURISTIC} {SYMAL}'", ".", "format", "(", "\n", "SYMFASTALIGN", "=", "sym_fast_align_bin", ",", "\n", "FWD", "=", "fwd_align_file", ",", "\n", "BWD", "=", "bwd_align_file", ",", "\n", "SRC", "=", "args", ".", "source_file", ",", "\n", "TGT", "=", "args", ".", "target_file", ",", "\n", "OUT", "=", "sym_out_file", ",", "\n", "HEURISTIC", "=", "args", ".", "sym_heuristic", ",", "\n", "SYMAL", "=", "symal_bin", "\n", ")", "\n", "assert", "os", ".", "system", "(", "sym_cmd", ")", "==", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.average_checkpoints.average_checkpoints": [[14, 68], ["collections.OrderedDict", "len", "collections.OrderedDict", "collections.OrderedDict.items", "torch.load", "list", "averaged_params[].div_", "model_params.keys", "isinstance", "KeyError", "p.float.float", "p.float.clone", "torch.serialization.default_restore_location"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load"], ["def", "average_checkpoints", "(", "inputs", ")", ":", "\n", "    ", "\"\"\"Loads checkpoints from inputs and returns a model with averaged weights.\n\n    Args:\n      inputs: An iterable of string paths of checkpoints to load from.\n\n    Returns:\n      A dict of string keys mapping to various values. The 'model' key\n      from the returned dict should correspond to an OrderedDict mapping\n      string parameter names to torch Tensors.\n    \"\"\"", "\n", "params_dict", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "params_keys", "=", "None", "\n", "new_state", "=", "None", "\n", "num_models", "=", "len", "(", "inputs", ")", "\n", "\n", "for", "f", "in", "inputs", ":", "\n", "        ", "state", "=", "torch", ".", "load", "(", "\n", "f", ",", "\n", "map_location", "=", "(", "\n", "lambda", "s", ",", "_", ":", "torch", ".", "serialization", ".", "default_restore_location", "(", "s", ",", "'cpu'", ")", "\n", ")", ",", "\n", ")", "\n", "# Copies over the settings from the first checkpoint", "\n", "if", "new_state", "is", "None", ":", "\n", "            ", "new_state", "=", "state", "\n", "\n", "", "model_params", "=", "state", "[", "'model'", "]", "\n", "\n", "model_params_keys", "=", "list", "(", "model_params", ".", "keys", "(", ")", ")", "\n", "if", "params_keys", "is", "None", ":", "\n", "            ", "params_keys", "=", "model_params_keys", "\n", "", "elif", "params_keys", "!=", "model_params_keys", ":", "\n", "            ", "raise", "KeyError", "(", "\n", "'For checkpoint {}, expected list of params: {}, '", "\n", "'but found: {}'", ".", "format", "(", "f", ",", "params_keys", ",", "model_params_keys", ")", "\n", ")", "\n", "\n", "", "for", "k", "in", "params_keys", ":", "\n", "            ", "p", "=", "model_params", "[", "k", "]", "\n", "if", "isinstance", "(", "p", ",", "torch", ".", "HalfTensor", ")", ":", "\n", "                ", "p", "=", "p", ".", "float", "(", ")", "\n", "", "if", "k", "not", "in", "params_dict", ":", "\n", "                ", "params_dict", "[", "k", "]", "=", "p", ".", "clone", "(", ")", "\n", "# NOTE: clone() is needed in case of p is a shared parameter", "\n", "", "else", ":", "\n", "                ", "params_dict", "[", "k", "]", "+=", "p", "\n", "\n", "", "", "", "averaged_params", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "for", "k", ",", "v", "in", "params_dict", ".", "items", "(", ")", ":", "\n", "        ", "averaged_params", "[", "k", "]", "=", "v", "\n", "averaged_params", "[", "k", "]", ".", "div_", "(", "num_models", ")", "\n", "", "new_state", "[", "'model'", "]", "=", "averaged_params", "\n", "return", "new_state", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.average_checkpoints.last_n_checkpoints": [[70, 89], ["os.listdir", "len", "re.compile", "re.compile", "re.compile.fullmatch", "len", "Exception", "os.path.join", "int", "len", "pt_regexp.fullmatch.group", "entries.append", "sorted", "pt_regexp.fullmatch.group"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "last_n_checkpoints", "(", "paths", ",", "n", ",", "update_based", ",", "upper_bound", "=", "None", ")", ":", "\n", "    ", "assert", "len", "(", "paths", ")", "==", "1", "\n", "path", "=", "paths", "[", "0", "]", "\n", "if", "update_based", ":", "\n", "        ", "pt_regexp", "=", "re", ".", "compile", "(", "r'checkpoint_\\d+_(\\d+)\\.pt'", ")", "\n", "", "else", ":", "\n", "        ", "pt_regexp", "=", "re", ".", "compile", "(", "r'checkpoint(\\d+)\\.pt'", ")", "\n", "", "files", "=", "os", ".", "listdir", "(", "path", ")", "\n", "\n", "entries", "=", "[", "]", "\n", "for", "f", "in", "files", ":", "\n", "        ", "m", "=", "pt_regexp", ".", "fullmatch", "(", "f", ")", "\n", "if", "m", "is", "not", "None", ":", "\n", "            ", "sort_key", "=", "int", "(", "m", ".", "group", "(", "1", ")", ")", "\n", "if", "upper_bound", "is", "None", "or", "sort_key", "<=", "upper_bound", ":", "\n", "                ", "entries", ".", "append", "(", "(", "sort_key", ",", "m", ".", "group", "(", "0", ")", ")", ")", "\n", "", "", "", "if", "len", "(", "entries", ")", "<", "n", ":", "\n", "        ", "raise", "Exception", "(", "'Found {} checkpoint files but need at least {}'", ",", "len", "(", "entries", ")", ",", "n", ")", "\n", "", "return", "[", "os", ".", "path", ".", "join", "(", "path", ",", "x", "[", "1", "]", ")", "for", "x", "in", "sorted", "(", "entries", ",", "reverse", "=", "True", ")", "[", ":", "n", "]", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.average_checkpoints.main": [[91, 137], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_mutually_exclusive_group", "parser.add_mutually_exclusive_group.add_argument", "parser.add_mutually_exclusive_group.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "print", "average_checkpoints.average_checkpoints", "torch.save", "print", "average_checkpoints.last_n_checkpoints", "print"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.average_checkpoints.average_checkpoints", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.save", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.average_checkpoints.last_n_checkpoints", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "description", "=", "'Tool to average the params of input checkpoints to '", "\n", "'produce a new checkpoint'", ",", "\n", ")", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'--inputs'", ",", "required", "=", "True", ",", "nargs", "=", "'+'", ",", "\n", "help", "=", "'Input checkpoint file paths.'", ")", "\n", "parser", ".", "add_argument", "(", "'--output'", ",", "required", "=", "True", ",", "metavar", "=", "'FILE'", ",", "\n", "help", "=", "'Write the new checkpoint containing the averaged weights to this path.'", ")", "\n", "num_group", "=", "parser", ".", "add_mutually_exclusive_group", "(", ")", "\n", "num_group", ".", "add_argument", "(", "'--num-epoch-checkpoints'", ",", "type", "=", "int", ",", "\n", "help", "=", "'if set, will try to find checkpoints with names checkpoint_xx.pt in the path specified by input, '", "\n", "'and average last this many of them.'", ")", "\n", "num_group", ".", "add_argument", "(", "'--num-update-checkpoints'", ",", "type", "=", "int", ",", "\n", "help", "=", "'if set, will try to find checkpoints with names checkpoint_ee_xx.pt in the path specified by input, '", "\n", "'and average last this many of them.'", ")", "\n", "parser", ".", "add_argument", "(", "'--checkpoint-upper-bound'", ",", "type", "=", "int", ",", "\n", "help", "=", "'when using --num-epoch-checkpoints, this will set an upper bound on which checkpoint to use, '", "\n", "'e.g., with --num-epoch-checkpoints=10 --checkpoint-upper-bound=50, checkpoints 41-50 would be averaged.'", ")", "\n", "# fmt: on", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "print", "(", "args", ")", "\n", "\n", "num", "=", "None", "\n", "is_update_based", "=", "False", "\n", "if", "args", ".", "num_update_checkpoints", "is", "not", "None", ":", "\n", "        ", "num", "=", "args", ".", "num_update_checkpoints", "\n", "is_update_based", "=", "True", "\n", "", "elif", "args", ".", "num_epoch_checkpoints", "is", "not", "None", ":", "\n", "        ", "num", "=", "args", ".", "num_epoch_checkpoints", "\n", "\n", "", "assert", "args", ".", "checkpoint_upper_bound", "is", "None", "or", "args", ".", "num_epoch_checkpoints", "is", "not", "None", ",", "'--checkpoint-upper-bound requires --num-epoch-checkpoints'", "\n", "assert", "args", ".", "num_epoch_checkpoints", "is", "None", "or", "args", ".", "num_update_checkpoints", "is", "None", ",", "'Cannot combine --num-epoch-checkpoints and --num-update-checkpoints'", "\n", "\n", "if", "num", "is", "not", "None", ":", "\n", "        ", "args", ".", "inputs", "=", "last_n_checkpoints", "(", "\n", "args", ".", "inputs", ",", "num", ",", "is_update_based", ",", "upper_bound", "=", "args", ".", "checkpoint_upper_bound", ",", "\n", ")", "\n", "print", "(", "'averaging checkpoints: '", ",", "args", ".", "inputs", ")", "\n", "\n", "", "new_state", "=", "average_checkpoints", "(", "args", ".", "inputs", ")", "\n", "torch", ".", "save", "(", "new_state", ",", "args", ".", "output", ")", "\n", "print", "(", "'Finished writing averaged checkpoint to {}.'", ".", "format", "(", "args", ".", "output", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.spm_decode.main": [[15, 42], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "sentencepiece.SentencePieceProcessor", "spm.SentencePieceProcessor.Load", "open", "int", "print", "spm.SentencePieceProcessor.DecodePieces", "spm_decode.main.decode"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.decode"], ["def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--model\"", ",", "required", "=", "True", ",", "\n", "help", "=", "\"sentencepiece model to use for decoding\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--input\"", ",", "required", "=", "True", ",", "help", "=", "\"input file to decode\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--input_format\"", ",", "choices", "=", "[", "\"piece\"", ",", "\"id\"", "]", ",", "default", "=", "\"piece\"", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "sp", "=", "spm", ".", "SentencePieceProcessor", "(", ")", "\n", "sp", ".", "Load", "(", "args", ".", "model", ")", "\n", "\n", "if", "args", ".", "input_format", "==", "\"piece\"", ":", "\n", "        ", "def", "decode", "(", "l", ")", ":", "\n", "            ", "return", "\"\"", ".", "join", "(", "sp", ".", "DecodePieces", "(", "l", ")", ")", "\n", "", "", "elif", "args", ".", "input_format", "==", "\"id\"", ":", "\n", "        ", "def", "decode", "(", "l", ")", ":", "\n", "            ", "return", "\"\"", ".", "join", "(", "sp", ".", "DecodeIds", "(", "l", ")", ")", "\n", "", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n", "", "def", "tok2int", "(", "tok", ")", ":", "\n", "# remap reference-side <unk> (represented as <<unk>>) to 0", "\n", "        ", "return", "int", "(", "tok", ")", "if", "tok", "!=", "\"<<unk>>\"", "else", "0", "\n", "\n", "", "with", "open", "(", "args", ".", "input", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "h", ":", "\n", "        ", "for", "line", "in", "h", ":", "\n", "            ", "print", "(", "decode", "(", "list", "(", "map", "(", "tok2int", ",", "line", ".", "rstrip", "(", ")", ".", "split", "(", ")", ")", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.con_rank_model.argument_parsing": [[27, 83], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["def", "argument_parsing", "(", ")", ":", "\n", "\n", "# Argument hanlding", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "description", "=", "'Organize model results'", "\n", ")", "\n", "# jbinfo args", "\n", "parser", ".", "add_argument", "(", "\n", "'--checkpoints'", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "'DATA/AMR/models/'", ",", "\n", "help", "=", "'Folder containing model folders (containing themselves '", "\n", "'checkpoints, config.sh etc)'", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--min-epoch-delta'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "10", ",", "\n", "help", "=", "'Minimum for the difference between best valid epoch and max'", "\n", "' epochs'", "\n", ")", "\n", "# jbinfo args", "\n", "parser", ".", "add_argument", "(", "\n", "'--seed-average'", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "'average results per seed'", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--link-best'", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "'do not link or relink best smatch model'", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--no-print'", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "'do not print'", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--set'", ",", "\n", "default", "=", "'valid'", ",", "\n", "choices", "=", "[", "'valid'", ",", "'test'", "]", ",", "\n", "help", "=", "'Set of the results'", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--score-names'", ",", "\n", "nargs", "=", "'+'", ",", "\n", "default", "=", "[", "'smatch'", ",", "'wiki.smatch'", "]", ",", "\n", "help", "=", "'Set of the results'", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--no-split-name'", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "'do not split model name into components'", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\"--ignore-deleted\"", ",", "action", "=", "'store_true'", ")", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.con_rank_model.yellow": [[85, 87], ["None"], "function", ["None"], ["", "def", "yellow", "(", "string", ")", ":", "\n", "    ", "return", "\"\\033[93m%s\\033[0m\"", "%", "string", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.con_rank_model.red": [[89, 91], ["None"], "function", ["None"], ["", "def", "red", "(", "string", ")", ":", "\n", "    ", "return", "\"\\033[91m%s\\033[0m\"", "%", "string", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.con_rank_model.mean": [[93, 95], ["float", "len", "sum"], "function", ["None"], ["", "def", "mean", "(", "items", ")", ":", "\n", "    ", "return", "float", "(", "sum", "(", "items", ")", ")", "/", "len", "(", "items", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.con_rank_model.std": [[97, 104], ["con_rank_model.mean", "math.sqrt", "len", "float", "sum", "len"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.mean"], ["", "def", "std", "(", "items", ")", ":", "\n", "    ", "mu", "=", "mean", "(", "items", ")", "\n", "if", "(", "len", "(", "items", ")", "-", "1", ")", "==", "0", ":", "\n", "        ", "return", "0.0", "\n", "", "else", ":", "\n", "        ", "var", "=", "float", "(", "sum", "(", "[", "(", "x", "-", "mu", ")", "**", "2", "for", "x", "in", "items", "]", ")", ")", "/", "(", "len", "(", "items", ")", "-", "1", ")", "\n", "return", "sqrt", "(", "var", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.con_rank_model.get_scores_from_folder": [[106, 134], ["os.listdir", "results_regex.match().groups", "epoch_numbers.append", "con_rank_model.get_score_from_log", "results_regex.match", "int", "max", "results_regex.match", "int"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.get_score_from_log"], ["", "", "def", "get_scores_from_folder", "(", "epoch_folder", ",", "score_name", ")", ":", "\n", "    ", "'''\n    Get score files from an epoch folder (<model_folder>/epoch_folder/)\n    '''", "\n", "\n", "# Get results available in this folder ", "\n", "scores", "=", "{", "}", "\n", "epoch_numbers", "=", "[", "]", "\n", "for", "dfile", "in", "os", ".", "listdir", "(", "epoch_folder", ")", ":", "\n", "\n", "# if not a results file, skip", "\n", "        ", "if", "not", "results_regex", ".", "match", "(", "dfile", ")", ":", "\n", "            ", "continue", "\n", "\n", "", "epoch_number", ",", "sname", "=", "results_regex", ".", "match", "(", "dfile", ")", ".", "groups", "(", ")", "\n", "\n", "# store epoch number", "\n", "epoch_numbers", ".", "append", "(", "int", "(", "epoch_number", ")", ")", "\n", "\n", "if", "sname", "!=", "score_name", ":", "\n", "            ", "continue", "\n", "\n", "# get score", "\n", "", "score", "=", "get_score_from_log", "(", "f'{epoch_folder}/{dfile}'", ",", "score_name", ")", "\n", "if", "score", "is", "not", "None", ":", "\n", "            ", "scores", "[", "int", "(", "epoch_number", ")", "]", "=", "score", "\n", "\n", "", "", "return", "scores", ",", "max", "(", "epoch_numbers", ")", "if", "epoch_numbers", "else", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.con_rank_model.get_score_from_log": [[136, 158], ["open", "Exception", "regex.match", "regex.match().groups", "list", "list.append", "map", "regex.match"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "get_score_from_log", "(", "file_path", ",", "score_name", ")", ":", "\n", "\n", "    ", "results", "=", "None", "\n", "\n", "if", "'smatch'", "in", "score_name", ":", "\n", "        ", "regex", "=", "smatch_results_re", "\n", "", "elif", "score_name", "==", "'las'", ":", "\n", "        ", "regex", "=", "las_results_re", "\n", "", "else", ":", "\n", "        ", "raise", "Exception", "(", "f'Unknown score type {score_name}'", ")", "\n", "\n", "", "with", "open", "(", "file_path", ")", "as", "fid", ":", "\n", "        ", "for", "line", "in", "fid", ":", "\n", "            ", "if", "regex", ".", "match", "(", "line", ")", ":", "\n", "                ", "results", "=", "regex", ".", "match", "(", "line", ")", ".", "groups", "(", ")", "\n", "#print(results)", "\n", "results", "=", "list", "(", "map", "(", "float", ",", "results", ")", ")", "\n", "#print(results)", "\n", "results", ".", "append", "(", "results", "[", "0", "]", ")", "\n", "break", "\n", "\n", "", "", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.con_rank_model.rank_scores": [[160, 189], ["sorted", "scores.items", "len", "len"], "function", ["None"], ["", "def", "rank_scores", "(", "scores", ",", "score_name", ")", ":", "\n", "\n", "    ", "if", "score_name", "==", "'las'", ":", "\n", "        ", "sort_idx", "=", "1", "\n", "", "else", ":", "\n", "        ", "sort_idx", "=", "0", "\n", "", "models", "=", "sorted", "(", "scores", ".", "items", "(", ")", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", "[", "sort_idx", "]", ")", "\n", "if", "len", "(", "models", ")", ">=", "3", ":", "\n", "        ", "third_best_score", ",", "second_best_score", ",", "best_score", "=", "models", "[", "-", "3", ":", "]", "\n", "", "elif", "len", "(", "models", ")", "==", "2", ":", "\n", "        ", "second_best_score", ",", "best_score", "=", "models", "\n", "third_best_score", "=", "[", "-", "1", ",", "-", "1", "]", "\n", "", "else", ":", "\n", "        ", "best_score", "=", "models", "[", "0", "]", "\n", "second_best_score", "=", "[", "-", "1", ",", "-", "1", "]", "\n", "third_best_score", "=", "[", "-", "1", ",", "-", "1", "]", "\n", "\n", "# get top 3, but second and third happening before last", "\n", "", "top3_prev", "=", "[", "(", "None", ",", "None", ")", ",", "(", "None", ",", "None", ")", "]", "\n", "idx", "=", "0", "\n", "for", "m", "in", "models", "[", ":", ":", "-", "2", "]", ":", "\n", "        ", "number", ",", "score", "=", "m", "\n", "if", "number", "<", "models", "[", "-", "1", "]", "[", "0", "]", ":", "\n", "            ", "top3_prev", "[", "idx", "]", "=", "m", "\n", "idx", "+=", "1", "\n", "", "if", "idx", "==", "2", ":", "\n", "            ", "break", "\n", "\n", "", "", "return", "best_score", ",", "second_best_score", ",", "third_best_score", ",", "top3_prev", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.con_rank_model.get_max_epoch_from_config": [[191, 201], ["open", "fid.readlines", "config_var_regex.match", "line.strip", "config_var_regex.match().groups", "int", "config_var_regex.match", "line.strip"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.readlines"], ["", "def", "get_max_epoch_from_config", "(", "model_folder", ")", ":", "\n", "    ", "max_epoch", "=", "None", "\n", "with", "open", "(", "f'{model_folder}/config.sh'", ")", "as", "fid", ":", "\n", "        ", "for", "line", "in", "fid", ".", "readlines", "(", ")", ":", "\n", "            ", "if", "config_var_regex", ".", "match", "(", "line", ".", "strip", "(", ")", ")", ":", "\n", "                ", "name", ",", "value", "=", "config_var_regex", ".", "match", "(", "line", ".", "strip", "(", ")", ")", ".", "groups", "(", ")", "\n", "if", "name", "==", "'MAX_EPOCH'", ":", "\n", "                    ", "max_epoch", "=", "int", "(", "value", ")", "\n", "break", "\n", "", "", "", "", "return", "max_epoch", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.con_rank_model.collect_checkpoint_results": [[203, 284], ["epoch_folder.replace", "list", "con_rank_model.get_scores_from_folder", "con_rank_model.rank_scores", "zip", "con_rank_model.get_max_epoch_from_config", "list", "items.append", "os.path.isdir", "filter", "int", "os.listdir", "set", "set", "int", "int", "int", "len", "score_name.upper", "os.path.isfile", "checkpoint_re.match().groups", "scores.keys", "checkpoint_re.match"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.get_scores_from_folder", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.rank_scores", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.get_max_epoch_from_config", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "collect_checkpoint_results", "(", "epoch_folders", ",", "score_name", ")", ":", "\n", "\n", "# loop over those folders", "\n", "    ", "items", "=", "[", "]", "\n", "for", "epoch_folder", "in", "epoch_folders", ":", "\n", "\n", "# data in {epoch_folder}/../", "\n", "# assume containing folder is the model folder", "\n", "        ", "model_folder", "=", "epoch_folder", ".", "replace", "(", "'epoch_tests'", ",", "''", ")", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "model_folder", ")", ":", "\n", "            ", "continue", "\n", "\n", "# Get checkpoints available for this model", "\n", "", "checkpoints", "=", "list", "(", "filter", "(", "checkpoint_re", ".", "match", ",", "os", ".", "listdir", "(", "model_folder", ")", ")", ")", "\n", "\n", "# Get the scores from the result files", "\n", "scores", ",", "max_score_epoch", "=", "get_scores_from_folder", "(", "epoch_folder", ",", "score_name", ")", "\n", "\n", "# If no scores skip this folder", "\n", "if", "not", "scores", ":", "\n", "            ", "continue", "\n", "\n", "# get top 3 scores and epochs", "\n", "", "best_score", ",", "second_best_score", ",", "third_best_score", ",", "top3_prev", "=", "rank_scores", "(", "scores", ",", "score_name", ")", "\n", "\n", "# Find if checkpoints have been deleted and there is no copy (e.g. as", "\n", "# done by remove_checkpoints.sh)", "\n", "deleted_checkpoints", "=", "False", "\n", "for", "label", ",", "score", "in", "zip", "(", "\n", "[", "'best'", ",", "'second_best'", ",", "'third_best'", "]", ",", "\n", "[", "best_score", ",", "second_best_score", ",", "third_best_score", "]", "\n", ")", ":", "\n", "            ", "if", "score", "[", "0", "]", "==", "-", "1", ":", "\n", "                ", "continue", "\n", "\n", "", "saved_checkpoint_name", "=", "(", "\n", "f'{model_folder}/checkpoint_{label}_{score_name.upper()}.pt'", "\n", ")", "\n", "\n", "if", "(", "\n", "f'checkpoint{score[0]}.pt'", "not", "in", "checkpoints", "and", "\n", "not", "os", ".", "path", ".", "isfile", "(", "saved_checkpoint_name", ")", "\n", ")", ":", "\n", "                ", "deleted_checkpoints", "=", "True", "\n", "break", "\n", "\n", "# find out epoch checkpoints that still need to be run", "\n", "", "", "stdout_numbers", "=", "[", "\n", "int", "(", "checkpoint_re", ".", "match", "(", "x", ")", ".", "groups", "(", ")", "[", "0", "]", ")", "for", "x", "in", "checkpoints", "\n", "]", "\n", "\n", "# find out the maximum number of epochs", "\n", "max_epochs", "=", "get_max_epoch_from_config", "(", "model_folder", ")", "\n", "missing_epochs", "=", "list", "(", "set", "(", "stdout_numbers", ")", "-", "set", "(", "scores", ".", "keys", "(", ")", ")", ")", "\n", "\n", "items", ".", "append", "(", "{", "\n", "'folder'", ":", "model_folder", ",", "\n", "# top scores", "\n", "f'best_{score_name}'", ":", "best_score", "[", "1", "]", ",", "\n", "f'second_best_{score_name}'", ":", "second_best_score", "[", "1", "]", ",", "\n", "f'third_best_{score_name}'", ":", "third_best_score", "[", "1", "]", ",", "\n", "f'second_best_before_{score_name}'", ":", "top3_prev", "[", "1", "]", "[", "1", "]", ",", "\n", "f'third_best_before_{score_name}'", ":", "top3_prev", "[", "0", "]", "[", "1", "]", ",", "\n", "# top score epochs", "\n", "f'best_{score_name}_epoch'", ":", "int", "(", "best_score", "[", "0", "]", ")", ",", "\n", "f'second_best_{score_name}_epoch'", ":", "int", "(", "second_best_score", "[", "0", "]", ")", ",", "\n", "f'third_best_{score_name}_epoch'", ":", "int", "(", "third_best_score", "[", "0", "]", ")", ",", "\n", "f'second_best_before_{score_name}_epoch'", ":", "top3_prev", "[", "1", "]", "[", "0", "]", ",", "\n", "f'third_best_before_{score_name}_epoch'", ":", "top3_prev", "[", "0", "]", "[", "0", "]", ",", "\n", "# any top score checkpoints missing", "\n", "'deleted_checkpoints'", ":", "deleted_checkpoints", ",", "\n", "# other", "\n", "'max_epochs'", ":", "max_epochs", ",", "\n", "'num_missing_epochs'", ":", "len", "(", "missing_epochs", ")", ",", "\n", "'num'", ":", "1", ",", "\n", "'ensemble'", ":", "False", "\n", "}", ")", "\n", "\n", "", "return", "items", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.con_rank_model.get_extra_results": [[286, 323], ["epoch_folder.replace", "glob.glob", "os.path.basename", "con_rank_model.get_score_from_log", "os.path.dirname", "items.append"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.get_score_from_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "get_extra_results", "(", "epoch_folders", ",", "sset", ",", "score_name", ")", ":", "\n", "\n", "# loop over those folders", "\n", "    ", "items", "=", "[", "]", "\n", "for", "epoch_folder", "in", "epoch_folders", ":", "\n", "\n", "# data in {epoch_folder}/../", "\n", "# assume containing folder is the model folder", "\n", "        ", "model_folder", "=", "epoch_folder", ".", "replace", "(", "'epoch_tests'", ",", "''", ")", "\n", "\n", "# Extra results", "\n", "for", "extra_exp", "in", "glob", ".", "glob", "(", "\n", "f'{model_folder}/*/{sset}.{score_name}'", "\n", ")", ":", "\n", "\n", "# look for extra experiments", "\n", "            ", "exp_tag", "=", "os", ".", "path", ".", "basename", "(", "os", ".", "path", ".", "dirname", "(", "extra_exp", ")", ")", "\n", "\n", "if", "exp_tag", "==", "'epoch_tests'", ":", "\n", "                ", "continue", "\n", "\n", "", "exp_smatch", "=", "get_score_from_log", "(", "extra_exp", ",", "score_name", ")", "\n", "\n", "if", "exp_smatch", "is", "not", "None", ":", "\n", "                ", "items", ".", "append", "(", "{", "\n", "'folder'", ":", "f'{model_folder}'", ",", "\n", "f'best_{score_name}'", ":", "exp_smatch", ",", "\n", "f'best_{score_name}_epoch'", ":", "0", ",", "\n", "'max_epochs'", ":", "0", ",", "\n", "'num_missing_epochs'", ":", "0", ",", "\n", "'deleted_checkpoints'", ":", "False", ",", "\n", "'num'", ":", "1", ",", "\n", "'ensemble'", ":", "True", ",", "\n", "'extra_exp'", ":", "exp_tag", "\n", "}", ")", "\n", "\n", "", "", "", "return", "items", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.con_rank_model.seed_average": [[325, 390], ["collections.defaultdict", "collections.defaultdict", "collections.defaultdict.items", "model_folder_re.match().groups", "clusters[].append", "seeds[].append", "merged_items.append", "all", "any", "con_rank_model.seed_average.results_map"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "seed_average", "(", "items", ",", "score_name", ")", ":", "\n", "    ", "\"\"\"\n    Aggregate stats for different seeds of same model\n    \"\"\"", "\n", "\n", "# cluster by key", "\n", "clusters", "=", "defaultdict", "(", "list", ")", "\n", "seeds", "=", "defaultdict", "(", "list", ")", "\n", "for", "item", "in", "items", ":", "\n", "        ", "key", ",", "seed", "=", "model_folder_re", ".", "match", "(", "item", "[", "'folder'", "]", ")", ".", "groups", "(", ")", "\n", "if", "'extra_exp'", "in", "item", ":", "\n", "            ", "key", "+=", "' '", "\n", "key", "+=", "item", "[", "'extra_exp'", "]", "\n", "", "clusters", "[", "key", "]", ".", "append", "(", "item", ")", "\n", "seeds", "[", "key", "]", ".", "append", "(", "seed", ")", "\n", "\n", "# merge", "\n", "", "merged_items", "=", "[", "]", "\n", "for", "key", ",", "cluster_items", "in", "clusters", ".", "items", "(", ")", ":", "\n", "\n", "        ", "def", "results_map", "(", "field", ",", "fun", ")", ":", "\n", "            ", "if", "any", "(", "[", "x", "[", "field", "]", "is", "None", "for", "x", "in", "cluster_items", "]", ")", ":", "\n", "                ", "return", "None", "\n", "", "else", ":", "\n", "                ", "if", "isinstance", "(", "cluster_items", "[", "0", "]", "[", "field", "]", ",", "list", ")", ":", "\n", "                    ", "num_types", "=", "len", "(", "cluster_items", "[", "0", "]", "[", "field", "]", ")", "\n", "results", "=", "[", "]", "\n", "for", "t", "in", "range", "(", "num_types", ")", ":", "\n", "                        ", "results", ".", "append", "(", "fun", "(", "\n", "[", "x", "[", "field", "]", "[", "t", "]", "for", "x", "in", "cluster_items", "]", "\n", ")", ")", "\n", "", "return", "results", "\n", "", "else", ":", "\n", "                    ", "return", "fun", "(", "[", "x", "[", "field", "]", "for", "x", "in", "cluster_items", "]", ")", "\n", "\n", "", "", "", "def", "fany", "(", "field", ")", ":", "\n", "            ", "return", "results_map", "(", "field", ",", "any", ")", "\n", "\n", "", "def", "average", "(", "field", ")", ":", "\n", "            ", "return", "results_map", "(", "field", ",", "mean", ")", "\n", "\n", "", "def", "stdev", "(", "field", ")", ":", "\n", "            ", "return", "results_map", "(", "field", ",", "lambda", "x", ":", "std", "(", "x", ")", ")", "\n", "\n", "", "def", "maximum", "(", "field", ")", ":", "\n", "            ", "return", "results_map", "(", "field", ",", "max", ")", "\n", "\n", "", "merged_items", ".", "append", "(", "{", "\n", "'folder'", ":", "key", ",", "\n", "f'best_{score_name}'", ":", "average", "(", "f'best_{score_name}'", ")", ",", "\n", "f'best_{score_name}_std'", ":", "stdev", "(", "f'best_{score_name}'", ")", ",", "\n", "f'best_{score_name}_epoch'", ":", "\n", "ceil", "(", "average", "(", "f'best_{score_name}_epoch'", ")", ")", ",", "\n", "'max_epochs'", ":", "ceil", "(", "average", "(", "'max_epochs'", ")", ")", ",", "\n", "'num_missing_epochs'", ":", "maximum", "(", "'num_missing_epochs'", ")", ",", "\n", "'num'", ":", "len", "(", "cluster_items", ")", ",", "\n", "'seeds'", ":", "seeds", "[", "key", "]", ",", "\n", "'deleted_checkpoints'", ":", "fany", "(", "'deleted_checkpoints'", ")", "\n", "}", ")", "\n", "\n", "if", "all", "(", "[", "'best_CE_{score_name}'", "in", "item", "for", "item", "in", "items", "]", ")", ":", "\n", "            ", "merged_items", "[", "-", "1", "]", "[", "f'best_CE_{score_name}'", "]", "=", "average", "(", "f'best_CE_{score_name}'", ")", "\n", "\n", "", "", "return", "merged_items", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.con_rank_model.get_basic_table_info": [[392, 438], ["centering.append", "row.append", "any", "con_rank_model.yellow", "print", "centering.extend", "row.extend", "centering.append", "row.append", "len", "get_shortname().split", "[].split", "con_rank_model.get_shortname", "get_shortname().split", "con_rank_model.get_shortname"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.status.yellow", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.get_shortname", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.get_shortname"], ["", "def", "get_basic_table_info", "(", "items", ",", "checkpoints", ",", "score_name", ",", "split_name", ")", ":", "\n", "\n", "# check if split will work, other wise override split_name", "\n", "    ", "if", "split_name", "and", "any", "(", "[", "\n", "len", "(", "get_shortname", "(", "item", ",", "checkpoints", ")", ".", "split", "(", ")", "[", "0", "]", ".", "split", "(", "'_'", ")", ")", "!=", "4", "\n", "for", "item", "in", "items", "\n", "if", "get_shortname", "(", "item", ",", "checkpoints", ")", ".", "split", "(", ")", "\n", "]", ")", ":", "\n", "        ", "warn", "=", "yellow", "(", "'WARNING:'", ")", "\n", "split_name", "=", "False", "\n", "print", "(", "f'\\n{warn} Model name not well formmatted (spureous _ ).'", "\n", "' Fix name or use --no-split-name\\n'", ")", "\n", "\n", "# add shortname as folder removing checkpoints root, get max length of", "\n", "# name for padding print", "\n", "# scale of the read results", "\n", "", "if", "score_name", "==", "'las'", ":", "\n", "        ", "sort_idx", "=", "1", "\n", "scale", "=", "1", "\n", "", "elif", "'smatch'", "in", "score_name", ":", "\n", "        ", "sort_idx", "=", "0", "\n", "scale", "=", "100", "\n", "\n", "# Header", "\n", "", "if", "split_name", ":", "\n", "        ", "centering", "=", "[", "'<'", ",", "'<'", ",", "'<'", ",", "'<'", ",", "'<'", ",", "'^'", ",", "'^'", "]", "\n", "row", "=", "[", "\n", "'data'", ",", "'oracle'", ",", "'features'", ",", "'model'", ",", "'extra'", ",", "'seeds'", ",", "'best epoch'", "\n", "]", "\n", "", "else", ":", "\n", "        ", "centering", "=", "[", "'<'", ",", "'^'", ",", "'^'", "]", "\n", "row", "=", "[", "'name'", ",", "'seed'", ",", "'best epoch'", "]", "\n", "\n", "", "if", "score_name", "==", "'las'", ":", "\n", "        ", "centering", ".", "extend", "(", "[", "'^'", ",", "'^'", "]", ")", "\n", "row", ".", "extend", "(", "[", "'UAS'", ",", "'LAS'", "]", ")", "\n", "", "else", ":", "\n", "        ", "centering", ".", "append", "(", "'^'", ")", "\n", "row", ".", "append", "(", "'SMATCH'", ")", "\n", "# extra warning", "\n", "", "centering", ".", "append", "(", "'<'", ")", "\n", "row", ".", "append", "(", "''", ")", "\n", "# style for rows", "\n", "rows", "=", "[", "row", "]", "\n", "\n", "return", "rows", ",", "centering", ",", "scale", ",", "sort_idx", ",", "split_name", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.con_rank_model.get_shortname": [[440, 453], ["item[].replace", "os.path.basename", "os.path.basename"], "function", ["None"], ["", "def", "get_shortname", "(", "item", ",", "checkpoints", ")", ":", "\n", "# name", "\n", "    ", "shortname", "=", "item", "[", "'folder'", "]", ".", "replace", "(", "checkpoints", ",", "''", ")", "\n", "# if we give model folder direcly, shortname will be empty, use the", "\n", "# containing folder", "\n", "if", "shortname", "==", "''", ":", "\n", "        ", "if", "checkpoints", "[", "-", "1", "]", "==", "'/'", ":", "\n", "            ", "shortname", "=", "os", ".", "path", ".", "basename", "(", "checkpoints", "[", ":", "-", "1", "]", ")", "\n", "", "else", ":", "\n", "            ", "shortname", "=", "os", ".", "path", ".", "basename", "(", "checkpoints", ")", "\n", "", "", "else", ":", "\n", "        ", "shortname", "=", "shortname", "[", "1", ":", "]", "if", "shortname", "[", "0", "]", "==", "'/'", "else", "shortname", "\n", "", "return", "shortname", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.con_rank_model.get_name_rows": [[455, 491], ["con_rank_model.get_shortname", "get_shortname.split", "row.extend", "row.append", "len", "get_shortname.split", "len", "main_pieces[].split", "get_shortname.split", "item[].split", "[].split", "get_shortname.split"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.get_shortname", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["", "def", "get_name_rows", "(", "split_name", ",", "item", ",", "checkpoints", ")", ":", "\n", "\n", "    ", "row", "=", "[", "]", "\n", "shortname", "=", "get_shortname", "(", "item", ",", "checkpoints", ")", "\n", "\n", "if", "(", "\n", "split_name", "\n", "and", "(", "\n", "shortname", ".", "split", "(", ")", "==", "[", "]", "\n", "or", "len", "(", "shortname", ".", "split", "(", ")", "[", "0", "]", ".", "split", "(", "'_'", ")", ")", "!=", "4", "\n", ")", "\n", ")", ":", "\n", "        ", "split_name", "=", "False", "\n", "\n", "", "if", "split_name", ":", "\n", "\n", "# Remove slash at start of end", "\n", "# ignore _ on first field", "\n", "        ", "main_pieces", "=", "shortname", ".", "split", "(", ")", "\n", "if", "len", "(", "main_pieces", ")", ">", "1", ":", "\n", "            ", "pieces", "=", "main_pieces", "[", "0", "]", ".", "split", "(", "'_'", ")", "+", "[", "main_pieces", "[", "-", "1", "]", "]", "\n", "", "else", ":", "\n", "            ", "pieces", "=", "shortname", ".", "split", "(", "'_'", ")", "+", "[", "''", "]", "\n", "\n", "", "if", "'extra_exp'", "in", "item", ":", "\n", "            ", "pieces", "[", "-", "1", "]", "+=", "' '", ".", "join", "(", "item", "[", "'extra_exp'", "]", ".", "split", "(", "'_'", ")", ")", "\n", "\n", "", "row", ".", "extend", "(", "pieces", ")", "\n", "", "else", ":", "\n", "\n", "        ", "if", "'extra_exp'", "in", "item", ":", "\n", "            ", "shortname", "+=", "' '", "\n", "shortname", "+=", "item", "[", "'extra_exp'", "]", "\n", "", "row", ".", "append", "(", "shortname", ")", "\n", "\n", "", "return", "row", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.con_rank_model.get_score_rows": [[493, 532], ["row.append", "row.append", "row.append"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "get_score_rows", "(", "score_name", ",", "item", ",", "scale", ")", ":", "\n", "\n", "    ", "row", "=", "[", "]", "\n", "\n", "if", "score_name", "==", "'las'", ":", "\n", "\n", "# first score", "\n", "        ", "cell_str", "=", "'{:2.1f}'", ".", "format", "(", "\n", "scale", "*", "item", "[", "f'best_{score_name}'", "]", "[", "0", "]", "\n", ")", "\n", "if", "f'best_{score_name}_std'", "in", "item", ":", "\n", "            ", "cell_str", "+=", "' ({:3.1f})'", ".", "format", "(", "\n", "scale", "*", "item", "[", "f'best_{score_name}_std'", "]", "[", "0", "]", "\n", ")", "\n", "", "row", ".", "append", "(", "cell_str", ")", "\n", "\n", "# second score", "\n", "cell_str", "=", "'{:2.1f}'", ".", "format", "(", "\n", "scale", "*", "item", "[", "f'best_{score_name}'", "]", "[", "1", "]", "\n", ")", "\n", "if", "f'best_{score_name}_std'", "in", "item", ":", "\n", "            ", "cell_str", "+=", "' ({:3.1f})'", ".", "format", "(", "\n", "scale", "*", "item", "[", "f'best_{score_name}_std'", "]", "[", "1", "]", "\n", ")", "\n", "", "row", ".", "append", "(", "cell_str", ")", "\n", "\n", "", "else", ":", "\n", "\n", "# first score", "\n", "        ", "cell_str", "=", "'{:2.1f}'", ".", "format", "(", "\n", "scale", "*", "item", "[", "f'best_{score_name}'", "]", "[", "0", "]", "\n", ")", "\n", "if", "f'best_{score_name}_std'", "in", "item", ":", "\n", "            ", "cell_str", "+=", "' ({:3.1f})'", ".", "format", "(", "\n", "scale", "*", "item", "[", "f'best_{score_name}_std'", "]", "[", "0", "]", "\n", ")", "\n", "", "row", ".", "append", "(", "cell_str", ")", "\n", "\n", "", "return", "row", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.con_rank_model.print_table": [[534, 574], ["con_rank_model.get_basic_table_info", "sorted", "print", "con_rank_model.ptable", "con_rank_model.get_name_rows", "get_name_rows.append", "get_name_rows.append", "get_name_rows.extend", "rows.append", "con_rank_model.red", "con_rank_model.get_score_rows", "get_name_rows.append", "get_name_rows.append", "con_rank_model.yellow", "con_rank_model.yellow"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.get_basic_table_info", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.ptable", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.get_name_rows", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.status.red", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.get_score_rows", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.status.yellow", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.status.yellow"], ["", "def", "print_table", "(", "checkpoints", ",", "items", ",", "score_name", ",", "min_epoch_delta", ",", "\n", "split_name", "=", "True", ")", ":", "\n", "\n", "    ", "rows", ",", "centering", ",", "scale", ",", "sort_idx", ",", "split_name", "=", "get_basic_table_info", "(", "items", ",", "checkpoints", ",", "score_name", ",", "split_name", ")", "\n", "\n", "# Loop over table rows", "\n", "items", "=", "sorted", "(", "items", ",", "key", "=", "lambda", "x", ":", "x", "[", "f'best_{score_name}'", "]", "[", "sort_idx", "]", ")", "\n", "for", "item", "in", "items", ":", "\n", "\n", "# rows pertaining node name", "\n", "        ", "row", "=", "get_name_rows", "(", "split_name", ",", "item", ",", "checkpoints", ")", "\n", "\n", "# number of seeds row", "\n", "row", ".", "append", "(", "'{}'", ".", "format", "(", "item", "[", "'num'", "]", ")", ")", "\n", "\n", "# best epoch row", "\n", "epoch_delta", "=", "item", "[", "'max_epochs'", "]", "-", "item", "[", "f'best_{score_name}_epoch'", "]", "\n", "convergence_epoch", "=", "'{:d}'", ".", "format", "(", "item", "[", "f'best_{score_name}_epoch'", "]", ")", "\n", "# check if some checkpoint was deleted by", "\n", "if", "item", "[", "'deleted_checkpoints'", "]", ":", "\n", "            ", "convergence_epoch", "=", "red", "(", "f'{convergence_epoch}'", ")", "\n", "", "elif", "epoch_delta", "<", "min_epoch_delta", ":", "\n", "            ", "convergence_epoch", "=", "yellow", "(", "f'{convergence_epoch}'", ")", "\n", "", "row", ".", "append", "(", "'{:s}/{:d}'", ".", "format", "(", "convergence_epoch", ",", "item", "[", "'max_epochs'", "]", ")", ")", "\n", "\n", "# score row", "\n", "row", ".", "extend", "(", "get_score_rows", "(", "score_name", ",", "item", ",", "scale", ")", ")", "\n", "\n", "# missing epochs for test", "\n", "if", "'num_missing_epochs'", "in", "item", "and", "item", "[", "'num_missing_epochs'", "]", ">", "0", ":", "\n", "            ", "row", ".", "append", "(", "yellow", "(", "' {:d}!'", ".", "format", "(", "item", "[", "'num_missing_epochs'", "]", ")", ")", ")", "\n", "", "else", ":", "\n", "            ", "row", ".", "append", "(", "''", ")", "\n", "\n", "# collect", "\n", "", "rows", ".", "append", "(", "row", ")", "\n", "\n", "", "print", "(", "f'\\n{score_name}'", ")", "\n", "ptable", "(", "rows", ",", "centering", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.con_rank_model.ptable": [[576, 607], ["len", "re.compile", "enumerate", "print", "print", "max", "enumerate", "table_str.append", "row_sep.join", "range", "row_str.append", "col_sep.join", "len", "len", "len", "re.compile.sub", "re.compile.sub"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "ptable", "(", "rows", ",", "centering", ")", ":", "\n", "\n", "    ", "num_columns", "=", "len", "(", "rows", "[", "0", "]", ")", "\n", "# bash scape chars (used for formatting, have length 0 on display)", "\n", "BASH_SCAPE", "=", "re", ".", "compile", "(", "r'\\x1b\\[\\d+m|\\x1b\\[0m'", ")", "\n", "column_widths", "=", "[", "\n", "max", "(", "[", "len", "(", "BASH_SCAPE", ".", "sub", "(", "''", ",", "row", "[", "i", "]", ")", ")", "for", "row", "in", "rows", "]", ")", "\n", "for", "i", "in", "range", "(", "num_columns", ")", "\n", "]", "\n", "\n", "table_str", "=", "[", "]", "\n", "col_sep", "=", "' '", "\n", "for", "i", ",", "row", "in", "enumerate", "(", "rows", ")", ":", "\n", "        ", "row_str", "=", "[", "]", "\n", "for", "j", ",", "cell", "in", "enumerate", "(", "row", ")", ":", "\n", "# need to discount for bash scape chars", "\n", "            ", "delta", "=", "len", "(", "cell", ")", "-", "len", "(", "BASH_SCAPE", ".", "sub", "(", "''", ",", "cell", ")", ")", "\n", "if", "i", "==", "0", ":", "\n", "# Header has all cells centered", "\n", "                ", "align", "=", "'^'", "\n", "", "else", ":", "\n", "                ", "align", "=", "centering", "[", "j", "]", "\n", "", "row_str", ".", "append", "(", "\n", "'{:{align}{width}} '", ".", "format", "(", "\n", "cell", ",", "align", "=", "align", ",", "width", "=", "column_widths", "[", "j", "]", "+", "delta", ")", "\n", ")", "\n", "", "table_str", ".", "append", "(", "col_sep", ".", "join", "(", "row_str", ")", ")", "\n", "\n", "", "row_sep", "=", "'\\n'", "\n", "print", "(", "row_sep", ".", "join", "(", "table_str", ")", ")", "\n", "print", "(", "\"\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.con_rank_model.link_top_models": [[609, 668], ["os.path.realpath", "score_name.upper", "os.path.islink", "os.path.basename", "os.path.islink", "os.remove", "os.path.isfile", "os.symlink", "os.path.isfile", "os.path.isfile", "Exception", "os.path.realpath", "os.remove", "os.path.islink", "os.path.isfile"], "function", ["None"], ["", "def", "link_top_models", "(", "items", ",", "score_name", ",", "ignore_deleted", ")", ":", "\n", "\n", "    ", "for", "item", "in", "items", ":", "\n", "\n", "        ", "if", "f'third_best_{score_name}_epoch'", "not", "in", "item", ":", "\n", "            ", "continue", "\n", "\n", "", "model_folder", "=", "os", ".", "path", ".", "realpath", "(", "item", "[", "'folder'", "]", ")", "\n", "# TODO: Decide if we want this disabled or not", "\n", "# for rank in ['best', 'second_best', 'third_best',", "\n", "# 'second_best_before', 'third_best_before']:", "\n", "for", "rank", "in", "[", "'best'", ",", "'second_best'", ",", "'third_best'", "]", ":", "\n", "            ", "epoch", "=", "item", "[", "f'{rank}_{score_name}_epoch'", "]", "\n", "\n", "# skip if no model found", "\n", "if", "epoch", "==", "-", "1", ":", "\n", "                ", "continue", "\n", "\n", "# get names and paths", "\n", "", "score_name_caps", "=", "score_name", ".", "upper", "(", ")", "\n", "target_best", "=", "(", "f'{model_folder}/'", "\n", "f'checkpoint_{rank}_{score_name_caps}.pt'", ")", "\n", "source_best", "=", "f'checkpoint{epoch}.pt'", "\n", "\n", "# if the best checkpoint does not exist but we have not saved it as", "\n", "# a file, we are in trouble", "\n", "if", "(", "\n", "not", "os", ".", "path", ".", "isfile", "(", "f'{model_folder}/{source_best}'", ")", "\n", "and", "not", "os", ".", "path", ".", "isfile", "(", "target_best", ")", "\n", ")", ":", "\n", "                ", "if", "ignore_deleted", ":", "\n", "                    ", "continue", "\n", "", "else", ":", "\n", "                    ", "raise", "Exception", "(", "\n", "f'Best model is {model_folder}/{source_best}, however'", "\n", "', the checkpoint seems to have been removed'", "\n", ")", "\n", "\n", "# get current best model (if exists)", "\n", "", "", "if", "os", ".", "path", ".", "islink", "(", "target_best", ")", ":", "\n", "                ", "current_best", "=", "os", ".", "path", ".", "basename", "(", "os", ".", "path", ".", "realpath", "(", "target_best", ")", ")", "\n", "", "else", ":", "\n", "                ", "current_best", "=", "None", "\n", "\n", "# replace link/checkpoint or create a new one", "\n", "", "if", "os", ".", "path", ".", "islink", "(", "target_best", ")", "and", "current_best", "!=", "source_best", ":", "\n", "# We created a link before to a worse model, remove it", "\n", "                ", "os", ".", "remove", "(", "target_best", ")", "\n", "", "elif", "os", ".", "path", ".", "isfile", "(", "target_best", ")", ":", "\n", "# If we ran remove_checkpoints.sh, we replaced the original", "\n", "# link by copy of the checkpoint. We dont know if this is the", "\n", "# correct checkpoint already", "\n", "                ", "os", ".", "remove", "(", "target_best", ")", "\n", "\n", "", "if", "(", "\n", "not", "os", ".", "path", ".", "islink", "(", "target_best", ")", "\n", "and", "not", "os", ".", "path", ".", "isfile", "(", "target_best", ")", "\n", ")", ":", "\n", "                ", "os", ".", "symlink", "(", "source_best", ",", "target_best", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.con_rank_model.main": [[670, 709], ["con_rank_model.argument_parsing", "print", "seed_average.extend", "os.walk", "con_rank_model.collect_checkpoint_results", "con_rank_model.get_extra_results", "con_rank_model.link_top_models", "con_rank_model.print_table", "con_rank_model.seed_average"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.parse.argument_parsing", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.collect_checkpoint_results", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.get_extra_results", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.link_top_models", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.print_table", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.seed_average"], ["", "", "", "", "def", "main", "(", ")", ":", "\n", "\n", "# ARGUMENT HANDLING", "\n", "    ", "args", "=", "argument_parsing", "(", ")", "\n", "\n", "# Find folders of the form /path/to/epoch_folders/ for all checkpoints", "\n", "epoch_folders", "=", "[", "\n", "x", "[", "0", "]", "\n", "for", "x", "in", "os", ".", "walk", "(", "args", ".", "checkpoints", ")", "\n", "if", "'epoch_tests'", "in", "x", "[", "0", "]", "\n", "]", "\n", "\n", "print", "(", "'IS WORKING'", ",", "args", ".", "score_names", ")", "\n", "\n", "# Separate results with and without wiki", "\n", "for", "score_name", "in", "args", ".", "score_names", ":", "\n", "\n", "# collect results for each model. For validation we have las N", "\n", "# checkpoints, which we use to determine the best model", "\n", "        ", "items", "=", "[", "]", "\n", "if", "args", ".", "set", "==", "'valid'", ":", "\n", "            ", "items", "=", "collect_checkpoint_results", "(", "epoch_folders", ",", "score_name", ")", "\n", "# collect extra results such as beam or weight average experiments", "\n", "", "items", ".", "extend", "(", "get_extra_results", "(", "epoch_folders", ",", "args", ".", "set", ",", "score_name", ")", ")", "\n", "\n", "if", "items", "==", "[", "]", ":", "\n", "            ", "continue", "\n", "\n", "# link best score model", "\n", "", "if", "args", ".", "link_best", ":", "\n", "            ", "link_top_models", "(", "items", ",", "score_name", ",", "args", ".", "ignore_deleted", ")", "\n", "\n", "", "if", "items", "!=", "[", "]", "and", "not", "args", ".", "no_print", ":", "\n", "# average over seeds", "\n", "            ", "if", "args", ".", "seed_average", ":", "\n", "                ", "items", "=", "seed_average", "(", "items", ",", "score_name", ")", "\n", "", "print_table", "(", "args", ".", "checkpoints", ",", "items", ",", "\n", "score_name", ",", "args", ".", "min_epoch_delta", ",", "split_name", "=", "not", "\n", "args", ".", "no_split_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.save_fairseq_args.argument_parser": [[6, 31], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["def", "argument_parser", "(", ")", ":", "\n", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'AMR parser oracle'", ")", "\n", "# Single input parameters", "\n", "parser", ".", "add_argument", "(", "\n", "\"--fairseq-preprocess-args\"", ",", "\n", "help", "=", "\"command lines args for fairseq-preprocess\"", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--fairseq-train-args\"", ",", "\n", "help", "=", "\"command lines args for fairseq-train\"", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--out-fairseq-model-config\"", ",", "\n", "help", "=", "\"Config\"", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", "\n", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.parse.argument_parsing": [[9, 46], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["def", "argument_parsing", "(", ")", ":", "\n", "\n", "# Argument hanlding", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "description", "=", "'Call parser from the command line'", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'-i'", ",", "'--in-tokenized-sentences'", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "'File with one __tokenized__ sentence per line'", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'-c'", ",", "'--in-checkpoint'", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "'one fairseq model checkpoint (or various, separated by :)'", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'-o'", ",", "'--out-amr'", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "'File to store AMR in PENNMAN format'", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--roberta-batch-size'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "10", ",", "\n", "help", "=", "'Batch size for roberta computation (watch for OOM)'", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--batch-size'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "128", ",", "\n", "help", "=", "'Batch size for decoding (excluding roberta)'", "\n", ")", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.parse.main": [[48, 85], ["parse.argument_parsing", "transition_amr_parser.io.read_sentences", "print", "time.time", "transition_amr_parser.stack_transformer_amr_parser.AMRParser.from_checkpoint", "time.time", "datetime.timedelta", "print", "time.time", "AMRParser.from_checkpoint.parse_sentences", "time.time", "print", "datetime.timedelta", "print", "split_sentences.append", "len", "len", "open", "range", "fairseq.tokenizer.tokenize_line", "float", "float", "len", "fid.write"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.parse.argument_parsing", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.io.read_sentences", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_transformer_amr_parser.AMRParser.from_checkpoint", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_transformer_amr_parser.AMRParser.parse_sentences", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.tokenizer.tokenize_line", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write"], ["", "def", "main", "(", ")", ":", "\n", "\n", "# argument handling", "\n", "    ", "args", "=", "argument_parsing", "(", ")", "\n", "\n", "# read tokenized sentences", "\n", "sentences", "=", "read_sentences", "(", "args", ".", "in_tokenized_sentences", ")", "\n", "split_sentences", "=", "[", "]", "\n", "for", "sentence", "in", "sentences", ":", "\n", "        ", "split_sentences", ".", "append", "(", "tokenize_line", "(", "sentence", ")", ")", "\n", "", "print", "(", "len", "(", "split_sentences", ")", ")", "\n", "\n", "# load parser", "\n", "start", "=", "time", ".", "time", "(", ")", "\n", "parser", "=", "AMRParser", ".", "from_checkpoint", "(", "args", ".", "in_checkpoint", ")", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "time_secs", "=", "timedelta", "(", "seconds", "=", "float", "(", "end", "-", "start", ")", ")", "\n", "print", "(", "f'Total time taken to load parser: {time_secs}'", ")", "\n", "\n", "# TODO: max batch sizes could be computed from max sentence length", "\n", "\n", "# parse", "\n", "start", "=", "time", ".", "time", "(", ")", "\n", "result", "=", "parser", ".", "parse_sentences", "(", "\n", "split_sentences", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "roberta_batch_size", "=", "args", ".", "roberta_batch_size", ",", "\n", ")", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "print", "(", "len", "(", "result", ")", ")", "\n", "time_secs", "=", "timedelta", "(", "seconds", "=", "float", "(", "end", "-", "start", ")", ")", "\n", "print", "(", "f'Total time taken to parse sentences: {time_secs}'", ")", "\n", "\n", "# write annotations", "\n", "with", "open", "(", "args", ".", "out_amr", ",", "'w'", ")", "as", "fid", ":", "\n", "        ", "for", "i", "in", "range", "(", "0", ",", "len", "(", "sentences", ")", ")", ":", "\n", "            ", "fid", ".", "write", "(", "result", "[", "i", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.argument_parsing": [[27, 83], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["def", "argument_parsing", "(", ")", ":", "\n", "\n", "# Argument hanlding", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "description", "=", "'Organize model results'", "\n", ")", "\n", "# jbinfo args", "\n", "parser", ".", "add_argument", "(", "\n", "'--checkpoints'", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "'DATA/AMR/models/'", ",", "\n", "help", "=", "'Folder containing model folders (containing themselves '", "\n", "'checkpoints, config.sh etc)'", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--min-epoch-delta'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "10", ",", "\n", "help", "=", "'Minimum for the difference between best valid epoch and max'", "\n", "' epochs'", "\n", ")", "\n", "# jbinfo args", "\n", "parser", ".", "add_argument", "(", "\n", "'--seed-average'", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "'average results per seed'", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--link-best'", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "'do not link or relink best smatch model'", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--no-print'", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "'do not print'", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--set'", ",", "\n", "default", "=", "'valid'", ",", "\n", "choices", "=", "[", "'valid'", ",", "'test'", "]", ",", "\n", "help", "=", "'Set of the results'", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--score-names'", ",", "\n", "nargs", "=", "'+'", ",", "\n", "default", "=", "[", "'smatch'", ",", "'wiki.smatch'", "]", ",", "\n", "help", "=", "'Set of the results'", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--no-split-name'", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "'do not split model name into components'", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\"--ignore-deleted\"", ",", "action", "=", "'store_true'", ")", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.yellow": [[85, 87], ["None"], "function", ["None"], ["", "def", "yellow", "(", "string", ")", ":", "\n", "    ", "return", "\"\\033[93m%s\\033[0m\"", "%", "string", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.red": [[89, 91], ["None"], "function", ["None"], ["", "def", "red", "(", "string", ")", ":", "\n", "    ", "return", "\"\\033[91m%s\\033[0m\"", "%", "string", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.mean": [[93, 95], ["float", "len", "sum"], "function", ["None"], ["", "def", "mean", "(", "items", ")", ":", "\n", "    ", "return", "float", "(", "sum", "(", "items", ")", ")", "/", "len", "(", "items", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.std": [[97, 104], ["rank_model.mean", "math.sqrt", "len", "float", "sum", "len"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.mean"], ["", "def", "std", "(", "items", ")", ":", "\n", "    ", "mu", "=", "mean", "(", "items", ")", "\n", "if", "(", "len", "(", "items", ")", "-", "1", ")", "==", "0", ":", "\n", "        ", "return", "0.0", "\n", "", "else", ":", "\n", "        ", "var", "=", "float", "(", "sum", "(", "[", "(", "x", "-", "mu", ")", "**", "2", "for", "x", "in", "items", "]", ")", ")", "/", "(", "len", "(", "items", ")", "-", "1", ")", "\n", "return", "sqrt", "(", "var", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.get_scores_from_folder": [[106, 134], ["os.listdir", "results_regex.match().groups", "epoch_numbers.append", "rank_model.get_score_from_log", "results_regex.match", "int", "max", "results_regex.match", "int"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.get_score_from_log"], ["", "", "def", "get_scores_from_folder", "(", "epoch_folder", ",", "score_name", ")", ":", "\n", "    ", "'''\n    Get score files from an epoch folder (<model_folder>/epoch_folder/)\n    '''", "\n", "\n", "# Get results available in this folder ", "\n", "scores", "=", "{", "}", "\n", "epoch_numbers", "=", "[", "]", "\n", "for", "dfile", "in", "os", ".", "listdir", "(", "epoch_folder", ")", ":", "\n", "\n", "# if not a results file, skip", "\n", "        ", "if", "not", "results_regex", ".", "match", "(", "dfile", ")", ":", "\n", "            ", "continue", "\n", "\n", "", "epoch_number", ",", "sname", "=", "results_regex", ".", "match", "(", "dfile", ")", ".", "groups", "(", ")", "\n", "\n", "# store epoch number", "\n", "epoch_numbers", ".", "append", "(", "int", "(", "epoch_number", ")", ")", "\n", "\n", "if", "sname", "!=", "score_name", ":", "\n", "            ", "continue", "\n", "\n", "# get score", "\n", "", "score", "=", "get_score_from_log", "(", "f'{epoch_folder}/{dfile}'", ",", "score_name", ")", "\n", "if", "score", "is", "not", "None", ":", "\n", "            ", "scores", "[", "int", "(", "epoch_number", ")", "]", "=", "score", "\n", "\n", "", "", "return", "scores", ",", "max", "(", "epoch_numbers", ")", "if", "epoch_numbers", "else", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.get_score_from_log": [[136, 155], ["open", "Exception", "regex.match", "regex.match().groups", "list", "map", "regex.match"], "function", ["None"], ["", "def", "get_score_from_log", "(", "file_path", ",", "score_name", ")", ":", "\n", "\n", "    ", "results", "=", "None", "\n", "\n", "if", "'smatch'", "in", "score_name", ":", "\n", "        ", "regex", "=", "smatch_results_re", "\n", "", "elif", "score_name", "==", "'las'", ":", "\n", "        ", "regex", "=", "las_results_re", "\n", "", "else", ":", "\n", "        ", "raise", "Exception", "(", "f'Unknown score type {score_name}'", ")", "\n", "\n", "", "with", "open", "(", "file_path", ")", "as", "fid", ":", "\n", "        ", "for", "line", "in", "fid", ":", "\n", "            ", "if", "regex", ".", "match", "(", "line", ")", ":", "\n", "                ", "results", "=", "regex", ".", "match", "(", "line", ")", ".", "groups", "(", ")", "\n", "results", "=", "list", "(", "map", "(", "float", ",", "results", ")", ")", "\n", "break", "\n", "\n", "", "", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.rank_scores": [[157, 186], ["sorted", "scores.items", "len", "len"], "function", ["None"], ["", "def", "rank_scores", "(", "scores", ",", "score_name", ")", ":", "\n", "\n", "    ", "if", "score_name", "==", "'las'", ":", "\n", "        ", "sort_idx", "=", "1", "\n", "", "else", ":", "\n", "        ", "sort_idx", "=", "0", "\n", "", "models", "=", "sorted", "(", "scores", ".", "items", "(", ")", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", "[", "sort_idx", "]", ")", "\n", "if", "len", "(", "models", ")", ">=", "3", ":", "\n", "        ", "third_best_score", ",", "second_best_score", ",", "best_score", "=", "models", "[", "-", "3", ":", "]", "\n", "", "elif", "len", "(", "models", ")", "==", "2", ":", "\n", "        ", "second_best_score", ",", "best_score", "=", "models", "\n", "third_best_score", "=", "[", "-", "1", ",", "-", "1", "]", "\n", "", "else", ":", "\n", "        ", "best_score", "=", "models", "[", "0", "]", "\n", "second_best_score", "=", "[", "-", "1", ",", "-", "1", "]", "\n", "third_best_score", "=", "[", "-", "1", ",", "-", "1", "]", "\n", "\n", "# get top 3, but second and third happening before last", "\n", "", "top3_prev", "=", "[", "(", "None", ",", "None", ")", ",", "(", "None", ",", "None", ")", "]", "\n", "idx", "=", "0", "\n", "for", "m", "in", "models", "[", ":", ":", "-", "2", "]", ":", "\n", "        ", "number", ",", "score", "=", "m", "\n", "if", "number", "<", "models", "[", "-", "1", "]", "[", "0", "]", ":", "\n", "            ", "top3_prev", "[", "idx", "]", "=", "m", "\n", "idx", "+=", "1", "\n", "", "if", "idx", "==", "2", ":", "\n", "            ", "break", "\n", "\n", "", "", "return", "best_score", ",", "second_best_score", ",", "third_best_score", ",", "top3_prev", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.get_max_epoch_from_config": [[188, 198], ["open", "fid.readlines", "config_var_regex.match", "line.strip", "config_var_regex.match().groups", "int", "config_var_regex.match", "line.strip"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.readlines"], ["", "def", "get_max_epoch_from_config", "(", "model_folder", ")", ":", "\n", "    ", "max_epoch", "=", "None", "\n", "with", "open", "(", "f'{model_folder}/config.sh'", ")", "as", "fid", ":", "\n", "        ", "for", "line", "in", "fid", ".", "readlines", "(", ")", ":", "\n", "            ", "if", "config_var_regex", ".", "match", "(", "line", ".", "strip", "(", ")", ")", ":", "\n", "                ", "name", ",", "value", "=", "config_var_regex", ".", "match", "(", "line", ".", "strip", "(", ")", ")", ".", "groups", "(", ")", "\n", "if", "name", "==", "'MAX_EPOCH'", ":", "\n", "                    ", "max_epoch", "=", "int", "(", "value", ")", "\n", "break", "\n", "", "", "", "", "return", "max_epoch", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.collect_checkpoint_results": [[200, 281], ["epoch_folder.replace", "list", "rank_model.get_scores_from_folder", "rank_model.rank_scores", "zip", "rank_model.get_max_epoch_from_config", "list", "items.append", "os.path.isdir", "filter", "int", "os.listdir", "set", "set", "int", "int", "int", "len", "score_name.upper", "os.path.isfile", "checkpoint_re.match().groups", "scores.keys", "checkpoint_re.match"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.get_scores_from_folder", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.rank_scores", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.get_max_epoch_from_config", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "collect_checkpoint_results", "(", "epoch_folders", ",", "score_name", ")", ":", "\n", "\n", "# loop over those folders", "\n", "    ", "items", "=", "[", "]", "\n", "for", "epoch_folder", "in", "epoch_folders", ":", "\n", "\n", "# data in {epoch_folder}/../", "\n", "# assume containing folder is the model folder", "\n", "        ", "model_folder", "=", "epoch_folder", ".", "replace", "(", "'epoch_tests'", ",", "''", ")", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "model_folder", ")", ":", "\n", "            ", "continue", "\n", "\n", "# Get checkpoints available for this model", "\n", "", "checkpoints", "=", "list", "(", "filter", "(", "checkpoint_re", ".", "match", ",", "os", ".", "listdir", "(", "model_folder", ")", ")", ")", "\n", "\n", "# Get the scores from the result files", "\n", "scores", ",", "max_score_epoch", "=", "get_scores_from_folder", "(", "epoch_folder", ",", "score_name", ")", "\n", "\n", "# If no scores skip this folder", "\n", "if", "not", "scores", ":", "\n", "            ", "continue", "\n", "\n", "# get top 3 scores and epochs", "\n", "", "best_score", ",", "second_best_score", ",", "third_best_score", ",", "top3_prev", "=", "rank_scores", "(", "scores", ",", "score_name", ")", "\n", "\n", "# Find if checkpoints have been deleted and there is no copy (e.g. as", "\n", "# done by remove_checkpoints.sh)", "\n", "deleted_checkpoints", "=", "False", "\n", "for", "label", ",", "score", "in", "zip", "(", "\n", "[", "'best'", ",", "'second_best'", ",", "'third_best'", "]", ",", "\n", "[", "best_score", ",", "second_best_score", ",", "third_best_score", "]", "\n", ")", ":", "\n", "            ", "if", "score", "[", "0", "]", "==", "-", "1", ":", "\n", "                ", "continue", "\n", "\n", "", "saved_checkpoint_name", "=", "(", "\n", "f'{model_folder}/checkpoint_{label}_{score_name.upper()}.pt'", "\n", ")", "\n", "\n", "if", "(", "\n", "f'checkpoint{score[0]}.pt'", "not", "in", "checkpoints", "and", "\n", "not", "os", ".", "path", ".", "isfile", "(", "saved_checkpoint_name", ")", "\n", ")", ":", "\n", "                ", "deleted_checkpoints", "=", "True", "\n", "break", "\n", "\n", "# find out epoch checkpoints that still need to be run", "\n", "", "", "stdout_numbers", "=", "[", "\n", "int", "(", "checkpoint_re", ".", "match", "(", "x", ")", ".", "groups", "(", ")", "[", "0", "]", ")", "for", "x", "in", "checkpoints", "\n", "]", "\n", "\n", "# find out the maximum number of epochs", "\n", "max_epochs", "=", "get_max_epoch_from_config", "(", "model_folder", ")", "\n", "missing_epochs", "=", "list", "(", "set", "(", "stdout_numbers", ")", "-", "set", "(", "scores", ".", "keys", "(", ")", ")", ")", "\n", "\n", "items", ".", "append", "(", "{", "\n", "'folder'", ":", "model_folder", ",", "\n", "# top scores", "\n", "f'best_{score_name}'", ":", "best_score", "[", "1", "]", ",", "\n", "f'second_best_{score_name}'", ":", "second_best_score", "[", "1", "]", ",", "\n", "f'third_best_{score_name}'", ":", "third_best_score", "[", "1", "]", ",", "\n", "f'second_best_before_{score_name}'", ":", "top3_prev", "[", "1", "]", "[", "1", "]", ",", "\n", "f'third_best_before_{score_name}'", ":", "top3_prev", "[", "0", "]", "[", "1", "]", ",", "\n", "# top score epochs", "\n", "f'best_{score_name}_epoch'", ":", "int", "(", "best_score", "[", "0", "]", ")", ",", "\n", "f'second_best_{score_name}_epoch'", ":", "int", "(", "second_best_score", "[", "0", "]", ")", ",", "\n", "f'third_best_{score_name}_epoch'", ":", "int", "(", "third_best_score", "[", "0", "]", ")", ",", "\n", "f'second_best_before_{score_name}_epoch'", ":", "top3_prev", "[", "1", "]", "[", "0", "]", ",", "\n", "f'third_best_before_{score_name}_epoch'", ":", "top3_prev", "[", "0", "]", "[", "0", "]", ",", "\n", "# any top score checkpoints missing", "\n", "'deleted_checkpoints'", ":", "deleted_checkpoints", ",", "\n", "# other", "\n", "'max_epochs'", ":", "max_epochs", ",", "\n", "'num_missing_epochs'", ":", "len", "(", "missing_epochs", ")", ",", "\n", "'num'", ":", "1", ",", "\n", "'ensemble'", ":", "False", "\n", "}", ")", "\n", "\n", "", "return", "items", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.get_extra_results": [[283, 320], ["epoch_folder.replace", "glob.glob", "os.path.basename", "rank_model.get_score_from_log", "os.path.dirname", "items.append"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.get_score_from_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "get_extra_results", "(", "epoch_folders", ",", "sset", ",", "score_name", ")", ":", "\n", "\n", "# loop over those folders", "\n", "    ", "items", "=", "[", "]", "\n", "for", "epoch_folder", "in", "epoch_folders", ":", "\n", "\n", "# data in {epoch_folder}/../", "\n", "# assume containing folder is the model folder", "\n", "        ", "model_folder", "=", "epoch_folder", ".", "replace", "(", "'epoch_tests'", ",", "''", ")", "\n", "\n", "# Extra results", "\n", "for", "extra_exp", "in", "glob", ".", "glob", "(", "\n", "f'{model_folder}/*/{sset}.{score_name}'", "\n", ")", ":", "\n", "\n", "# look for extra experiments", "\n", "            ", "exp_tag", "=", "os", ".", "path", ".", "basename", "(", "os", ".", "path", ".", "dirname", "(", "extra_exp", ")", ")", "\n", "\n", "if", "exp_tag", "==", "'epoch_tests'", ":", "\n", "                ", "continue", "\n", "\n", "", "exp_smatch", "=", "get_score_from_log", "(", "extra_exp", ",", "score_name", ")", "\n", "\n", "if", "exp_smatch", "is", "not", "None", ":", "\n", "                ", "items", ".", "append", "(", "{", "\n", "'folder'", ":", "f'{model_folder}'", ",", "\n", "f'best_{score_name}'", ":", "exp_smatch", ",", "\n", "f'best_{score_name}_epoch'", ":", "0", ",", "\n", "'max_epochs'", ":", "0", ",", "\n", "'num_missing_epochs'", ":", "0", ",", "\n", "'deleted_checkpoints'", ":", "False", ",", "\n", "'num'", ":", "1", ",", "\n", "'ensemble'", ":", "True", ",", "\n", "'extra_exp'", ":", "exp_tag", "\n", "}", ")", "\n", "\n", "", "", "", "return", "items", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.seed_average": [[322, 387], ["collections.defaultdict", "collections.defaultdict", "collections.defaultdict.items", "model_folder_re.match().groups", "clusters[].append", "seeds[].append", "merged_items.append", "all", "any", "rank_model.seed_average.results_map"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "seed_average", "(", "items", ",", "score_name", ")", ":", "\n", "    ", "\"\"\"\n    Aggregate stats for different seeds of same model\n    \"\"\"", "\n", "\n", "# cluster by key", "\n", "clusters", "=", "defaultdict", "(", "list", ")", "\n", "seeds", "=", "defaultdict", "(", "list", ")", "\n", "for", "item", "in", "items", ":", "\n", "        ", "key", ",", "seed", "=", "model_folder_re", ".", "match", "(", "item", "[", "'folder'", "]", ")", ".", "groups", "(", ")", "\n", "if", "'extra_exp'", "in", "item", ":", "\n", "            ", "key", "+=", "' '", "\n", "key", "+=", "item", "[", "'extra_exp'", "]", "\n", "", "clusters", "[", "key", "]", ".", "append", "(", "item", ")", "\n", "seeds", "[", "key", "]", ".", "append", "(", "seed", ")", "\n", "\n", "# merge", "\n", "", "merged_items", "=", "[", "]", "\n", "for", "key", ",", "cluster_items", "in", "clusters", ".", "items", "(", ")", ":", "\n", "\n", "        ", "def", "results_map", "(", "field", ",", "fun", ")", ":", "\n", "            ", "if", "any", "(", "[", "x", "[", "field", "]", "is", "None", "for", "x", "in", "cluster_items", "]", ")", ":", "\n", "                ", "return", "None", "\n", "", "else", ":", "\n", "                ", "if", "isinstance", "(", "cluster_items", "[", "0", "]", "[", "field", "]", ",", "list", ")", ":", "\n", "                    ", "num_types", "=", "len", "(", "cluster_items", "[", "0", "]", "[", "field", "]", ")", "\n", "results", "=", "[", "]", "\n", "for", "t", "in", "range", "(", "num_types", ")", ":", "\n", "                        ", "results", ".", "append", "(", "fun", "(", "\n", "[", "x", "[", "field", "]", "[", "t", "]", "for", "x", "in", "cluster_items", "]", "\n", ")", ")", "\n", "", "return", "results", "\n", "", "else", ":", "\n", "                    ", "return", "fun", "(", "[", "x", "[", "field", "]", "for", "x", "in", "cluster_items", "]", ")", "\n", "\n", "", "", "", "def", "fany", "(", "field", ")", ":", "\n", "            ", "return", "results_map", "(", "field", ",", "any", ")", "\n", "\n", "", "def", "average", "(", "field", ")", ":", "\n", "            ", "return", "results_map", "(", "field", ",", "mean", ")", "\n", "\n", "", "def", "stdev", "(", "field", ")", ":", "\n", "            ", "return", "results_map", "(", "field", ",", "lambda", "x", ":", "std", "(", "x", ")", ")", "\n", "\n", "", "def", "maximum", "(", "field", ")", ":", "\n", "            ", "return", "results_map", "(", "field", ",", "max", ")", "\n", "\n", "", "merged_items", ".", "append", "(", "{", "\n", "'folder'", ":", "key", ",", "\n", "f'best_{score_name}'", ":", "average", "(", "f'best_{score_name}'", ")", ",", "\n", "f'best_{score_name}_std'", ":", "stdev", "(", "f'best_{score_name}'", ")", ",", "\n", "f'best_{score_name}_epoch'", ":", "\n", "ceil", "(", "average", "(", "f'best_{score_name}_epoch'", ")", ")", ",", "\n", "'max_epochs'", ":", "ceil", "(", "average", "(", "'max_epochs'", ")", ")", ",", "\n", "'num_missing_epochs'", ":", "maximum", "(", "'num_missing_epochs'", ")", ",", "\n", "'num'", ":", "len", "(", "cluster_items", ")", ",", "\n", "'seeds'", ":", "seeds", "[", "key", "]", ",", "\n", "'deleted_checkpoints'", ":", "fany", "(", "'deleted_checkpoints'", ")", "\n", "}", ")", "\n", "\n", "if", "all", "(", "[", "'best_CE_{score_name}'", "in", "item", "for", "item", "in", "items", "]", ")", ":", "\n", "            ", "merged_items", "[", "-", "1", "]", "[", "f'best_CE_{score_name}'", "]", "=", "average", "(", "f'best_CE_{score_name}'", ")", "\n", "\n", "", "", "return", "merged_items", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.get_basic_table_info": [[389, 435], ["centering.append", "row.append", "any", "rank_model.yellow", "print", "centering.extend", "row.extend", "centering.append", "row.append", "len", "get_shortname().split", "[].split", "rank_model.get_shortname", "get_shortname().split", "rank_model.get_shortname"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.status.yellow", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.get_shortname", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.get_shortname"], ["", "def", "get_basic_table_info", "(", "items", ",", "checkpoints", ",", "score_name", ",", "split_name", ")", ":", "\n", "\n", "# check if split will work, other wise override split_name", "\n", "    ", "if", "split_name", "and", "any", "(", "[", "\n", "len", "(", "get_shortname", "(", "item", ",", "checkpoints", ")", ".", "split", "(", ")", "[", "0", "]", ".", "split", "(", "'_'", ")", ")", "!=", "4", "\n", "for", "item", "in", "items", "\n", "if", "get_shortname", "(", "item", ",", "checkpoints", ")", ".", "split", "(", ")", "\n", "]", ")", ":", "\n", "        ", "warn", "=", "yellow", "(", "'WARNING:'", ")", "\n", "split_name", "=", "False", "\n", "print", "(", "f'\\n{warn} Model name not well formmatted (spureous _ ).'", "\n", "' Fix name or use --no-split-name\\n'", ")", "\n", "\n", "# add shortname as folder removing checkpoints root, get max length of", "\n", "# name for padding print", "\n", "# scale of the read results", "\n", "", "if", "score_name", "==", "'las'", ":", "\n", "        ", "sort_idx", "=", "1", "\n", "scale", "=", "1", "\n", "", "elif", "'smatch'", "in", "score_name", ":", "\n", "        ", "sort_idx", "=", "0", "\n", "scale", "=", "100", "\n", "\n", "# Header", "\n", "", "if", "split_name", ":", "\n", "        ", "centering", "=", "[", "'<'", ",", "'<'", ",", "'<'", ",", "'<'", ",", "'<'", ",", "'^'", ",", "'^'", "]", "\n", "row", "=", "[", "\n", "'data'", ",", "'oracle'", ",", "'features'", ",", "'model'", ",", "'extra'", ",", "'seeds'", ",", "'best epoch'", "\n", "]", "\n", "", "else", ":", "\n", "        ", "centering", "=", "[", "'<'", ",", "'^'", ",", "'^'", "]", "\n", "row", "=", "[", "'name'", ",", "'seed'", ",", "'best epoch'", "]", "\n", "\n", "", "if", "score_name", "==", "'las'", ":", "\n", "        ", "centering", ".", "extend", "(", "[", "'^'", ",", "'^'", "]", ")", "\n", "row", ".", "extend", "(", "[", "'UAS'", ",", "'LAS'", "]", ")", "\n", "", "else", ":", "\n", "        ", "centering", ".", "append", "(", "'^'", ")", "\n", "row", ".", "append", "(", "'SMATCH'", ")", "\n", "# extra warning", "\n", "", "centering", ".", "append", "(", "'<'", ")", "\n", "row", ".", "append", "(", "''", ")", "\n", "# style for rows", "\n", "rows", "=", "[", "row", "]", "\n", "\n", "return", "rows", ",", "centering", ",", "scale", ",", "sort_idx", ",", "split_name", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.get_shortname": [[437, 450], ["item[].replace", "os.path.basename", "os.path.basename"], "function", ["None"], ["", "def", "get_shortname", "(", "item", ",", "checkpoints", ")", ":", "\n", "# name", "\n", "    ", "shortname", "=", "item", "[", "'folder'", "]", ".", "replace", "(", "checkpoints", ",", "''", ")", "\n", "# if we give model folder direcly, shortname will be empty, use the", "\n", "# containing folder", "\n", "if", "shortname", "==", "''", ":", "\n", "        ", "if", "checkpoints", "[", "-", "1", "]", "==", "'/'", ":", "\n", "            ", "shortname", "=", "os", ".", "path", ".", "basename", "(", "checkpoints", "[", ":", "-", "1", "]", ")", "\n", "", "else", ":", "\n", "            ", "shortname", "=", "os", ".", "path", ".", "basename", "(", "checkpoints", ")", "\n", "", "", "else", ":", "\n", "        ", "shortname", "=", "shortname", "[", "1", ":", "]", "if", "shortname", "[", "0", "]", "==", "'/'", "else", "shortname", "\n", "", "return", "shortname", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.get_name_rows": [[452, 488], ["rank_model.get_shortname", "get_shortname.split", "row.extend", "row.append", "len", "get_shortname.split", "len", "main_pieces[].split", "get_shortname.split", "item[].split", "[].split", "get_shortname.split"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.get_shortname", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["", "def", "get_name_rows", "(", "split_name", ",", "item", ",", "checkpoints", ")", ":", "\n", "\n", "    ", "row", "=", "[", "]", "\n", "shortname", "=", "get_shortname", "(", "item", ",", "checkpoints", ")", "\n", "\n", "if", "(", "\n", "split_name", "\n", "and", "(", "\n", "shortname", ".", "split", "(", ")", "==", "[", "]", "\n", "or", "len", "(", "shortname", ".", "split", "(", ")", "[", "0", "]", ".", "split", "(", "'_'", ")", ")", "!=", "4", "\n", ")", "\n", ")", ":", "\n", "        ", "split_name", "=", "False", "\n", "\n", "", "if", "split_name", ":", "\n", "\n", "# Remove slash at start of end", "\n", "# ignore _ on first field", "\n", "        ", "main_pieces", "=", "shortname", ".", "split", "(", ")", "\n", "if", "len", "(", "main_pieces", ")", ">", "1", ":", "\n", "            ", "pieces", "=", "main_pieces", "[", "0", "]", ".", "split", "(", "'_'", ")", "+", "[", "main_pieces", "[", "-", "1", "]", "]", "\n", "", "else", ":", "\n", "            ", "pieces", "=", "shortname", ".", "split", "(", "'_'", ")", "+", "[", "''", "]", "\n", "\n", "", "if", "'extra_exp'", "in", "item", ":", "\n", "            ", "pieces", "[", "-", "1", "]", "+=", "' '", ".", "join", "(", "item", "[", "'extra_exp'", "]", ".", "split", "(", "'_'", ")", ")", "\n", "\n", "", "row", ".", "extend", "(", "pieces", ")", "\n", "", "else", ":", "\n", "\n", "        ", "if", "'extra_exp'", "in", "item", ":", "\n", "            ", "shortname", "+=", "' '", "\n", "shortname", "+=", "item", "[", "'extra_exp'", "]", "\n", "", "row", ".", "append", "(", "shortname", ")", "\n", "\n", "", "return", "row", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.get_score_rows": [[490, 529], ["row.append", "row.append", "row.append"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "get_score_rows", "(", "score_name", ",", "item", ",", "scale", ")", ":", "\n", "\n", "    ", "row", "=", "[", "]", "\n", "\n", "if", "score_name", "==", "'las'", ":", "\n", "\n", "# first score", "\n", "        ", "cell_str", "=", "'{:2.1f}'", ".", "format", "(", "\n", "scale", "*", "item", "[", "f'best_{score_name}'", "]", "[", "0", "]", "\n", ")", "\n", "if", "f'best_{score_name}_std'", "in", "item", ":", "\n", "            ", "cell_str", "+=", "' ({:3.1f})'", ".", "format", "(", "\n", "scale", "*", "item", "[", "f'best_{score_name}_std'", "]", "[", "0", "]", "\n", ")", "\n", "", "row", ".", "append", "(", "cell_str", ")", "\n", "\n", "# second score", "\n", "cell_str", "=", "'{:2.1f}'", ".", "format", "(", "\n", "scale", "*", "item", "[", "f'best_{score_name}'", "]", "[", "1", "]", "\n", ")", "\n", "if", "f'best_{score_name}_std'", "in", "item", ":", "\n", "            ", "cell_str", "+=", "' ({:3.1f})'", ".", "format", "(", "\n", "scale", "*", "item", "[", "f'best_{score_name}_std'", "]", "[", "1", "]", "\n", ")", "\n", "", "row", ".", "append", "(", "cell_str", ")", "\n", "\n", "", "else", ":", "\n", "\n", "# first score", "\n", "        ", "cell_str", "=", "'{:2.1f}'", ".", "format", "(", "\n", "scale", "*", "item", "[", "f'best_{score_name}'", "]", "[", "0", "]", "\n", ")", "\n", "if", "f'best_{score_name}_std'", "in", "item", ":", "\n", "            ", "cell_str", "+=", "' ({:3.1f})'", ".", "format", "(", "\n", "scale", "*", "item", "[", "f'best_{score_name}_std'", "]", "[", "0", "]", "\n", ")", "\n", "", "row", ".", "append", "(", "cell_str", ")", "\n", "\n", "", "return", "row", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.print_table": [[531, 571], ["rank_model.get_basic_table_info", "sorted", "print", "rank_model.ptable", "rank_model.get_name_rows", "get_name_rows.append", "get_name_rows.append", "get_name_rows.extend", "rows.append", "rank_model.red", "rank_model.get_score_rows", "get_name_rows.append", "get_name_rows.append", "rank_model.yellow", "rank_model.yellow"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.get_basic_table_info", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.ptable", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.get_name_rows", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.status.red", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.get_score_rows", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.status.yellow", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.status.yellow"], ["", "def", "print_table", "(", "checkpoints", ",", "items", ",", "score_name", ",", "min_epoch_delta", ",", "\n", "split_name", "=", "True", ")", ":", "\n", "\n", "    ", "rows", ",", "centering", ",", "scale", ",", "sort_idx", ",", "split_name", "=", "get_basic_table_info", "(", "items", ",", "checkpoints", ",", "score_name", ",", "split_name", ")", "\n", "\n", "# Loop over table rows", "\n", "items", "=", "sorted", "(", "items", ",", "key", "=", "lambda", "x", ":", "x", "[", "f'best_{score_name}'", "]", "[", "sort_idx", "]", ")", "\n", "for", "item", "in", "items", ":", "\n", "\n", "# rows pertaining node name", "\n", "        ", "row", "=", "get_name_rows", "(", "split_name", ",", "item", ",", "checkpoints", ")", "\n", "\n", "# number of seeds row", "\n", "row", ".", "append", "(", "'{}'", ".", "format", "(", "item", "[", "'num'", "]", ")", ")", "\n", "\n", "# best epoch row", "\n", "epoch_delta", "=", "item", "[", "'max_epochs'", "]", "-", "item", "[", "f'best_{score_name}_epoch'", "]", "\n", "convergence_epoch", "=", "'{:d}'", ".", "format", "(", "item", "[", "f'best_{score_name}_epoch'", "]", ")", "\n", "# check if some checkpoint was deleted by", "\n", "if", "item", "[", "'deleted_checkpoints'", "]", ":", "\n", "            ", "convergence_epoch", "=", "red", "(", "f'{convergence_epoch}'", ")", "\n", "", "elif", "epoch_delta", "<", "min_epoch_delta", ":", "\n", "            ", "convergence_epoch", "=", "yellow", "(", "f'{convergence_epoch}'", ")", "\n", "", "row", ".", "append", "(", "'{:s}/{:d}'", ".", "format", "(", "convergence_epoch", ",", "item", "[", "'max_epochs'", "]", ")", ")", "\n", "\n", "# score row", "\n", "row", ".", "extend", "(", "get_score_rows", "(", "score_name", ",", "item", ",", "scale", ")", ")", "\n", "\n", "# missing epochs for test", "\n", "if", "'num_missing_epochs'", "in", "item", "and", "item", "[", "'num_missing_epochs'", "]", ">", "0", ":", "\n", "            ", "row", ".", "append", "(", "yellow", "(", "' {:d}!'", ".", "format", "(", "item", "[", "'num_missing_epochs'", "]", ")", ")", ")", "\n", "", "else", ":", "\n", "            ", "row", ".", "append", "(", "''", ")", "\n", "\n", "# collect", "\n", "", "rows", ".", "append", "(", "row", ")", "\n", "\n", "", "print", "(", "f'\\n{score_name}'", ")", "\n", "ptable", "(", "rows", ",", "centering", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.ptable": [[573, 604], ["len", "re.compile", "enumerate", "print", "print", "max", "enumerate", "table_str.append", "row_sep.join", "range", "row_str.append", "col_sep.join", "len", "len", "len", "re.compile.sub", "re.compile.sub"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "ptable", "(", "rows", ",", "centering", ")", ":", "\n", "\n", "    ", "num_columns", "=", "len", "(", "rows", "[", "0", "]", ")", "\n", "# bash scape chars (used for formatting, have length 0 on display)", "\n", "BASH_SCAPE", "=", "re", ".", "compile", "(", "r'\\x1b\\[\\d+m|\\x1b\\[0m'", ")", "\n", "column_widths", "=", "[", "\n", "max", "(", "[", "len", "(", "BASH_SCAPE", ".", "sub", "(", "''", ",", "row", "[", "i", "]", ")", ")", "for", "row", "in", "rows", "]", ")", "\n", "for", "i", "in", "range", "(", "num_columns", ")", "\n", "]", "\n", "\n", "table_str", "=", "[", "]", "\n", "col_sep", "=", "' '", "\n", "for", "i", ",", "row", "in", "enumerate", "(", "rows", ")", ":", "\n", "        ", "row_str", "=", "[", "]", "\n", "for", "j", ",", "cell", "in", "enumerate", "(", "row", ")", ":", "\n", "# need to discount for bash scape chars", "\n", "            ", "delta", "=", "len", "(", "cell", ")", "-", "len", "(", "BASH_SCAPE", ".", "sub", "(", "''", ",", "cell", ")", ")", "\n", "if", "i", "==", "0", ":", "\n", "# Header has all cells centered", "\n", "                ", "align", "=", "'^'", "\n", "", "else", ":", "\n", "                ", "align", "=", "centering", "[", "j", "]", "\n", "", "row_str", ".", "append", "(", "\n", "'{:{align}{width}} '", ".", "format", "(", "\n", "cell", ",", "align", "=", "align", ",", "width", "=", "column_widths", "[", "j", "]", "+", "delta", ")", "\n", ")", "\n", "", "table_str", ".", "append", "(", "col_sep", ".", "join", "(", "row_str", ")", ")", "\n", "\n", "", "row_sep", "=", "'\\n'", "\n", "print", "(", "row_sep", ".", "join", "(", "table_str", ")", ")", "\n", "print", "(", "\"\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.link_top_models": [[606, 665], ["os.path.realpath", "score_name.upper", "os.path.islink", "os.path.basename", "os.path.islink", "os.remove", "os.path.isfile", "os.symlink", "os.path.isfile", "os.path.isfile", "Exception", "os.path.realpath", "os.remove", "os.path.islink", "os.path.isfile"], "function", ["None"], ["", "def", "link_top_models", "(", "items", ",", "score_name", ",", "ignore_deleted", ")", ":", "\n", "\n", "    ", "for", "item", "in", "items", ":", "\n", "\n", "        ", "if", "f'third_best_{score_name}_epoch'", "not", "in", "item", ":", "\n", "            ", "continue", "\n", "\n", "", "model_folder", "=", "os", ".", "path", ".", "realpath", "(", "item", "[", "'folder'", "]", ")", "\n", "# TODO: Decide if we want this disabled or not", "\n", "# for rank in ['best', 'second_best', 'third_best',", "\n", "# 'second_best_before', 'third_best_before']:", "\n", "for", "rank", "in", "[", "'best'", ",", "'second_best'", ",", "'third_best'", "]", ":", "\n", "            ", "epoch", "=", "item", "[", "f'{rank}_{score_name}_epoch'", "]", "\n", "\n", "# skip if no model found", "\n", "if", "epoch", "==", "-", "1", ":", "\n", "                ", "continue", "\n", "\n", "# get names and paths", "\n", "", "score_name_caps", "=", "score_name", ".", "upper", "(", ")", "\n", "target_best", "=", "(", "f'{model_folder}/'", "\n", "f'checkpoint_{rank}_{score_name_caps}.pt'", ")", "\n", "source_best", "=", "f'checkpoint{epoch}.pt'", "\n", "\n", "# if the best checkpoint does not exist but we have not saved it as", "\n", "# a file, we are in trouble", "\n", "if", "(", "\n", "not", "os", ".", "path", ".", "isfile", "(", "f'{model_folder}/{source_best}'", ")", "\n", "and", "not", "os", ".", "path", ".", "isfile", "(", "target_best", ")", "\n", ")", ":", "\n", "                ", "if", "ignore_deleted", ":", "\n", "                    ", "continue", "\n", "", "else", ":", "\n", "                    ", "raise", "Exception", "(", "\n", "f'Best model is {model_folder}/{source_best}, however'", "\n", "', the checkpoint seems to have been removed'", "\n", ")", "\n", "\n", "# get current best model (if exists)", "\n", "", "", "if", "os", ".", "path", ".", "islink", "(", "target_best", ")", ":", "\n", "                ", "current_best", "=", "os", ".", "path", ".", "basename", "(", "os", ".", "path", ".", "realpath", "(", "target_best", ")", ")", "\n", "", "else", ":", "\n", "                ", "current_best", "=", "None", "\n", "\n", "# replace link/checkpoint or create a new one", "\n", "", "if", "os", ".", "path", ".", "islink", "(", "target_best", ")", "and", "current_best", "!=", "source_best", ":", "\n", "# We created a link before to a worse model, remove it", "\n", "                ", "os", ".", "remove", "(", "target_best", ")", "\n", "", "elif", "os", ".", "path", ".", "isfile", "(", "target_best", ")", ":", "\n", "# If we ran remove_checkpoints.sh, we replaced the original", "\n", "# link by copy of the checkpoint. We dont know if this is the", "\n", "# correct checkpoint already", "\n", "                ", "os", ".", "remove", "(", "target_best", ")", "\n", "\n", "", "if", "(", "\n", "not", "os", ".", "path", ".", "islink", "(", "target_best", ")", "\n", "and", "not", "os", ".", "path", ".", "isfile", "(", "target_best", ")", "\n", ")", ":", "\n", "                ", "os", ".", "symlink", "(", "source_best", ",", "target_best", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.main": [[667, 706], ["rank_model.argument_parsing", "print", "seed_average.extend", "os.walk", "rank_model.collect_checkpoint_results", "rank_model.get_extra_results", "rank_model.link_top_models", "rank_model.print_table", "rank_model.seed_average"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.parse.argument_parsing", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.collect_checkpoint_results", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.get_extra_results", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.link_top_models", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.print_table", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.seed_average"], ["", "", "", "", "def", "main", "(", ")", ":", "\n", "\n", "# ARGUMENT HANDLING", "\n", "    ", "args", "=", "argument_parsing", "(", ")", "\n", "\n", "# Find folders of the form /path/to/epoch_folders/ for all checkpoints", "\n", "epoch_folders", "=", "[", "\n", "x", "[", "0", "]", "\n", "for", "x", "in", "os", ".", "walk", "(", "args", ".", "checkpoints", ")", "\n", "if", "'epoch_tests'", "in", "x", "[", "0", "]", "\n", "]", "\n", "\n", "print", "(", "'IS WORKING'", ",", "args", ".", "score_names", ")", "\n", "\n", "# Separate results with and without wiki", "\n", "for", "score_name", "in", "args", ".", "score_names", ":", "\n", "\n", "# collect results for each model. For validation we have las N", "\n", "# checkpoints, which we use to determine the best model", "\n", "        ", "items", "=", "[", "]", "\n", "if", "args", ".", "set", "==", "'valid'", ":", "\n", "            ", "items", "=", "collect_checkpoint_results", "(", "epoch_folders", ",", "score_name", ")", "\n", "# collect extra results such as beam or weight average experiments", "\n", "", "items", ".", "extend", "(", "get_extra_results", "(", "epoch_folders", ",", "args", ".", "set", ",", "score_name", ")", ")", "\n", "\n", "if", "items", "==", "[", "]", ":", "\n", "            ", "continue", "\n", "\n", "# link best score model", "\n", "", "if", "args", ".", "link_best", ":", "\n", "            ", "link_top_models", "(", "items", ",", "score_name", ",", "args", ".", "ignore_deleted", ")", "\n", "\n", "", "if", "items", "!=", "[", "]", "and", "not", "args", ".", "no_print", ":", "\n", "# average over seeds", "\n", "            ", "if", "args", ".", "seed_average", ":", "\n", "                ", "items", "=", "seed_average", "(", "items", ",", "score_name", ")", "\n", "", "print_table", "(", "args", ".", "checkpoints", ",", "items", ",", "\n", "score_name", ",", "args", ".", "min_epoch_delta", ",", "split_name", "=", "not", "\n", "args", ".", "no_split_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.remove_checkpoints.get_best_checkpoints": [[12, 38], ["dict", "os.path.islink", "os.path.islink", "Exception", "best_checkpoints[].append", "file_type.upper", "os.path.isfile", "os.readlink", "os.readlink"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["def", "get_best_checkpoints", "(", "model_folder", ",", "file_types", ",", "ignore_deleted", ")", ":", "\n", "    ", "best_checkpoints", "=", "dict", "(", ")", "\n", "for", "file_type", "in", "file_types", ":", "\n", "        ", "best_checkpoints", "[", "file_type", "]", "=", "[", "]", "\n", "for", "label", "in", "[", "'best'", ",", "'second_best'", ",", "'third_best'", "]", ":", "\n", "            ", "link", "=", "f'{model_folder}/checkpoint_{label}_{file_type.upper()}.pt'", "\n", "\n", "# raise if best checkpoint was deleted", "\n", "if", "(", "\n", "os", ".", "path", ".", "islink", "(", "link", ")", "\n", "and", "not", "os", ".", "path", ".", "isfile", "(", "f'{model_folder}/{os.readlink(link)}'", ")", "\n", "and", "not", "ignore_deleted", "\n", ")", ":", "\n", "                ", "raise", "Exception", "(", "\n", "f'Best model was deleted: {original_checkpoint} '", "\n", "'use --ignore-deleted to ignore'", "\n", ")", "\n", "\n", "", "if", "os", ".", "path", ".", "islink", "(", "link", ")", ":", "\n", "                ", "best_checkpoints", "[", "file_type", "]", ".", "append", "(", "\n", "f'{model_folder}/{os.readlink(link)}'", "\n", ")", "\n", "", "else", ":", "\n", "                ", "return", "{", "}", "\n", "\n", "", "", "", "return", "best_checkpoints", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.remove_checkpoints.argument_parser": [[40, 46], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["", "def", "argument_parser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Checkpoint remover'", ")", "\n", "parser", ".", "add_argument", "(", "\"model_folders\"", ",", "nargs", "=", "'+'", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "\"--check\"", ",", "action", "=", "'store_true'", ")", "\n", "parser", ".", "add_argument", "(", "\"--ignore-deleted\"", ",", "action", "=", "'store_true'", ")", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.status.yellow": [[12, 14], ["None"], "function", ["None"], ["def", "yellow", "(", "string", ")", ":", "\n", "    ", "return", "\"\\033[93m%s\\033[0m\"", "%", "string", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.status.green": [[16, 18], ["None"], "function", ["None"], ["", "def", "green", "(", "string", ")", ":", "\n", "    ", "return", "\"\\033[92m%s\\033[0m\"", "%", "string", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.status.red": [[20, 22], ["None"], "function", ["None"], ["", "def", "red", "(", "string", ")", ":", "\n", "    ", "return", "\"\\033[91m%s\\033[0m\"", "%", "string", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.status.argument_parsing": [[24, 39], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["", "def", "argument_parsing", "(", ")", ":", "\n", "\n", "# Argument hanlding", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "description", "=", "'Organize model results'", "\n", ")", "\n", "# jbinfo args", "\n", "parser", ".", "add_argument", "(", "\n", "'--models'", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "'DATA/AMR/models/'", ",", "\n", "help", "=", "'Folder containing model folders (containing themselves '", "\n", "'checkpoints, config.sh etc)'", "\n", ")", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.status.get_status": [[41, 113], ["os.path.isdir", "list", "open", "fid.readlines", "int", "checkpoint_best_re.match().groups", "sorted", "range", "list", "list", "list", "config_var_regex.match", "os.listdir", "checkpoint_re.match", "os.listdir", "checkpoint_best_re.match", "list", "line.strip", "config_var_regex.match().groups", "checkpoint_re.match().groups", "checkpoint_best_re.match", "int", "set", "set", "set", "set", "set", "set", "status.yellow", "int", "os.listdir", "status.red", "set", "set", "status.red", "status.green", "config_var_regex.match", "checkpoint_re.match", "results_regex.match().groups", "results_regex.match", "max", "line.strip", "results_regex.match", "results_regex.match().groups", "len", "status.green", "len", "results_regex.match"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.readlines", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.status.yellow", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.status.red", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.status.red", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.status.green", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.status.green"], ["", "def", "get_status", "(", "models", ",", "model_name", ")", ":", "\n", "\n", "# basic paths", "\n", "    ", "model_folder", "=", "f'{models}/{model_name}'", "\n", "epoch_folder", "=", "f'{model_folder}/epoch_tests/'", "\n", "\n", "# get needed config values", "\n", "max_epoch", "=", "None", "\n", "keep_last_epochs", "=", "40", "\n", "with", "open", "(", "f'{model_folder}/config.sh'", ")", "as", "fid", ":", "\n", "        ", "for", "line", "in", "fid", ".", "readlines", "(", ")", ":", "\n", "            ", "if", "config_var_regex", ".", "match", "(", "line", ".", "strip", "(", ")", ")", ":", "\n", "                ", "name", ",", "value", "=", "config_var_regex", ".", "match", "(", "line", ".", "strip", "(", ")", ")", ".", "groups", "(", ")", "\n", "if", "name", "==", "'MAX_EPOCH'", ":", "\n", "                    ", "max_epoch", "=", "int", "(", "value", ")", "\n", "break", "\n", "\n", "", "", "", "", "assert", "max_epoch", ",", "\"Missing MAX_EPOCH in '{model_folder}/config.sh\"", "\n", "\n", "# Get info from available files", "\n", "checkpoints", "=", "[", "\n", "int", "(", "checkpoint_re", ".", "match", "(", "m", ")", ".", "groups", "(", ")", "[", "0", "]", ")", "\n", "for", "m", "in", "os", ".", "listdir", "(", "model_folder", ")", "if", "checkpoint_re", ".", "match", "(", "m", ")", "\n", "]", "\n", "best_checkpoints", "=", "[", "\n", "checkpoint_best_re", ".", "match", "(", "m", ")", ".", "groups", "(", ")", "\n", "for", "m", "in", "os", ".", "listdir", "(", "model_folder", ")", "if", "checkpoint_best_re", ".", "match", "(", "m", ")", "\n", "]", "\n", "if", "os", ".", "path", ".", "isdir", "(", "epoch_folder", ")", ":", "\n", "        ", "results", "=", "sorted", "(", "[", "\n", "int", "(", "results_regex", ".", "match", "(", "m", ")", ".", "groups", "(", ")", "[", "0", "]", ")", "\n", "for", "m", "in", "os", ".", "listdir", "(", "epoch_folder", ")", "\n", "if", "results_regex", ".", "match", "(", "m", ")", "and", "\n", "results_regex", ".", "match", "(", "m", ")", ".", "groups", "(", ")", "[", "1", "]", "==", "'actions'", "\n", "]", ")", "\n", "", "else", ":", "\n", "        ", "results", "=", "[", "]", "\n", "\n", "# Sanity check", "\n", "", "result_epochs", "=", "list", "(", "range", "(", "max_epoch", "-", "keep_last_epochs", "+", "1", ",", "max_epoch", "+", "1", ")", ")", "\n", "if", "checkpoints", ":", "\n", "\n", "# pre checkpoint removal", "\n", "        ", "missing_epochs", "=", "list", "(", "set", "(", "result_epochs", ")", "-", "set", "(", "checkpoints", ")", ")", "\n", "missing_results", "=", "list", "(", "set", "(", "result_epochs", ")", "-", "set", "(", "results", ")", ")", "\n", "unrecoverable_results", "=", "list", "(", "set", "(", "missing_results", ")", "&", "set", "(", "missing_epochs", ")", ")", "\n", "if", "results", "==", "[", "]", "and", "missing_epochs", ":", "\n", "            ", "return", "model_name", ",", "yellow", "(", "'training'", ")", ",", "f'epoch {max(checkpoints)}/{max_epoch}'", "\n", "", "elif", "unrecoverable_results", ":", "\n", "            ", "return", "model_name", ",", "red", "(", "'broken!'", ")", ",", "'checkpoints deleted but missing results'", "\n", "", "elif", "missing_results", ":", "\n", "            ", "return", "model_name", ",", "'uncompleted'", ",", "f'test {len(missing_results)} models'", "\n", "", "elif", "missing_epochs", ":", "\n", "            ", "return", "model_name", ",", "'uncompleted'", ",", "f'train {len(missing_results)} epochs'", "\n", "", "elif", "best_checkpoints", ":", "\n", "            ", "return", "model_name", ",", "green", "(", "'completed'", ")", ",", "'you could remove checkpoints'", "\n", "", "else", ":", "\n", "            ", "return", "model_name", ",", "'uncompleted'", ",", "'run model ranker and remove checkpoints'", "\n", "\n", "", "", "elif", "best_checkpoints", ":", "\n", "\n", "# post checkpoint removal", "\n", "        ", "missing_results", "=", "list", "(", "set", "(", "result_epochs", ")", "-", "set", "(", "results", ")", ")", "\n", "if", "missing_results", ":", "\n", "            ", "return", "model_name", ",", "red", "(", "'broken!'", ")", ",", "'checkpoints deleted but missing results'", "\n", "", "else", ":", "\n", "            ", "return", "model_name", ",", "green", "(", "'completed'", ")", ",", "''", "\n", "\n", "", "", "else", ":", "\n", "\n", "# not trained", "\n", "        ", "return", "model_name", ",", "'not trained'", ",", "''", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.status.main": [[115, 131], ["status.argument_parsing", "os.listdir", "print", "sorted", "print", "stata.append", "print", "status.get_status"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.parse.argument_parsing", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.status.get_status"], ["", "", "def", "main", "(", ")", ":", "\n", "\n", "# ARGUMENT HANDLING", "\n", "    ", "args", "=", "argument_parsing", "(", ")", "\n", "\n", "# Get status of each model", "\n", "stata", "=", "[", "]", "\n", "for", "model_name", "in", "os", ".", "listdir", "(", "args", ".", "models", ")", ":", "\n", "        ", "stata", ".", "append", "(", "get_status", "(", "args", ".", "models", ",", "model_name", ")", ")", "\n", "\n", "# print", "\n", "", "print", "(", ")", "\n", "for", "status", "in", "sorted", "(", "stata", ",", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ")", ":", "\n", "        ", "model_name", ",", "status", ",", "detail", "=", "status", "\n", "print", "(", "f'{model_name:50s} {status} {detail}'", ")", "\n", "", "print", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.amr_client.argument_parser": [[11, 24], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'AMR parser'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--port\"", ",", "\n", "help", "=", "\"GRPC port\"", ",", "\n", "type", "=", "str", "\n", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "# Sanity checks", "\n", "assert", "args", ".", "port", "\n", "\n", "return", "args", "\n", "\n", "", "def", "get_input_from_sentence", "(", "sentence", ")", ":", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.amr_client.get_input_from_sentences": [[25, 30], ["amr_pb2.AMRBatchInput", "batch.append", "amr_pb2.AMRBatchInput.Sentence", "sentence.split"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["    ", "tokens", "=", "sentence", ".", "split", "(", ")", "\n", "input_tokens", "=", "[", "]", "\n", "for", "token", "in", "tokens", ":", "\n", "        ", "input_tokens", ".", "append", "(", "amr_pb2", ".", "AMRInput", ".", "WordInfo", "(", "token", "=", "token", ")", ")", "\n", "", "return", "amr_pb2", ".", "AMRInput", "(", "word_infos", "=", "input_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.amr_client.run": [[31, 45], ["amr_client.argument_parser", "grpc.insecure_channel", "amr_pb2_grpc.AMRBatchServerStub", "transition_amr_parser.io.read_sentences", "amr_client.get_input_from_sentences", "amr_pb2_grpc.AMRBatchServerStub.process", "open", "fid.write"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.argument_parser", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.io.read_sentences", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.amr_client.get_input_from_sentences", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.service.amr_server.Parser.process", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write"], ["", "def", "run", "(", ")", ":", "\n", "# NOTE(gRPC Python Team): .close() is possible on a channel and should be", "\n", "# used in circumstances in which the with statement does not fit the needs", "\n", "# of the code.", "\n", "# Argument handling", "\n", "    ", "args", "=", "argument_parser", "(", ")", "\n", "channel", "=", "grpc", ".", "insecure_channel", "(", "'localhost:'", "+", "args", ".", "port", ")", "\n", "stub", "=", "amr_pb2_grpc", ".", "AMRServerStub", "(", "channel", ")", "\n", "sentence", "=", "input", "(", "\"Enter the sentence: \"", ")", "\n", "amr_input", "=", "get_input_from_sentence", "(", "sentence", ")", "\n", "response", "=", "stub", ".", "process", "(", "amr_input", ")", "\n", "print", "(", "\"AMR parse received: \\n\"", "+", "response", ".", "amr_parse", ")", "\n", "\n", "", "if", "__name__", "==", "'__main__'", ":", "\n", "    ", "logging", ".", "basicConfig", "(", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.amr_server.Parser.__init__": [[53, 57], ["transition_amr_parser.stack_transformer_amr_parser.AMRParser.from_checkpoint"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_transformer_amr_parser.AMRParser.from_checkpoint"], ["tokens", "=", "[", "word_token", ".", "token", "for", "word_token", "in", "word_tokens", "]", "\n", "amr", "=", "self", ".", "parser", ".", "parse_sentence", "(", "tokens", ")", "\n", "return", "amr_pb2", ".", "AMRResponse", "(", "amr_parse", "=", "amr", ".", "toJAMRString", "(", ")", ")", "\n", "\n", "", "", "def", "serve", "(", ")", ":", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.amr_server.Parser.process": [[58, 65], ["amr_server.Parser.parser.parse_sentences", "amr_pb2.AMRBatchResponse", "batch.append"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_transformer_amr_parser.AMRParser.parse_sentences", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["# Argument handling", "\n", "    ", "args", "=", "argument_parser", "(", ")", "\n", "\n", "server", "=", "grpc", ".", "server", "(", "futures", ".", "ThreadPoolExecutor", "(", "max_workers", "=", "10", ")", ")", "\n", "amr_pb2_grpc", ".", "add_AMRServerServicer_to_server", "(", "Parser", "(", "model_path", "=", "args", ".", "in_model", ",", "roberta_cache_path", "=", "args", ".", "roberta_cache_path", ")", ",", "server", ")", "\n", "server", ".", "add_insecure_port", "(", "'[::]:'", "+", "args", ".", "port", ")", "\n", "server", ".", "start", "(", ")", "\n", "server", ".", "wait_for_termination", "(", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.amr_server.argument_parser": [[14, 50], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["\n", "from", "fairseq", ".", "models", ".", "roberta", "import", "RobertaModel", "\n", "import", "argparse", "\n", "\n", "def", "argument_parser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'AMR parser'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--in-model\"", ",", "\n", "help", "=", "\"path to the AMR parsing model\"", ",", "\n", "type", "=", "str", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--roberta-cache-path\"", ",", "\n", "help", "=", "\"Path to the roberta large model\"", ",", "\n", "type", "=", "str", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--port\"", ",", "\n", "help", "=", "\"GRPC port\"", ",", "\n", "type", "=", "str", "\n", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "# Sanity checks", "\n", "assert", "args", ".", "in_model", "\n", "assert", "args", ".", "port", "\n", "\n", "return", "args", "\n", "\n", "", "class", "Parser", "(", ")", ":", "\n", "\n", "    ", "def", "__init__", "(", "self", ",", "model_path", ",", "roberta_cache_path", "=", "None", ",", "roberta_use_gpu", "=", "False", ",", "model_use_gpu", "=", "False", ")", ":", "\n", "        ", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "roberta_use_gpu", "=", "True", "\n", "model_use_gpu", "=", "True", "\n", "", "self", ".", "parser", "=", "AMRParser", "(", "model_path", ",", "roberta_cache_path", "=", "roberta_cache_path", ",", "roberta_use_gpu", "=", "roberta_use_gpu", ",", "model_use_gpu", "=", "model_use_gpu", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.amr_server.serve": [[66, 75], ["amr_server.argument_parser", "grpc.server", "amr_pb2_grpc.add_AMRBatchServerServicer_to_server", "grpc.server.add_insecure_port", "grpc.server.start", "grpc.server.wait_for_termination", "concurrent.futures.ThreadPoolExecutor", "amr_server.Parser"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.argument_parser", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.meters.StopwatchMeter.start"], ["\n", "", "if", "__name__", "==", "'__main__'", ":", "\n", "    ", "logging", ".", "basicConfig", "(", ")", "\n", "serve", "(", ")", "\n", "\n", "\n", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.amr_pb2_grpc.AMRServerStub.__init__": [[11, 21], ["channel.unary_unary"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "channel", ")", ":", "\n", "    ", "\"\"\"Constructor.\n\n    Args:\n      channel: A grpc.Channel.\n    \"\"\"", "\n", "self", ".", "process", "=", "channel", ".", "unary_unary", "(", "\n", "'/AMRServer/process'", ",", "\n", "request_serializer", "=", "amr__pb2", ".", "AMRInput", ".", "SerializeToString", ",", "\n", "response_deserializer", "=", "amr__pb2", ".", "AMRResponse", ".", "FromString", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.amr_pb2_grpc.AMRServerServicer.process": [[28, 34], ["context.set_code", "context.set_details", "NotImplementedError"], "methods", ["None"], ["def", "process", "(", "self", ",", "request", ",", "context", ")", ":", "\n", "# missing associated documentation comment in .proto file", "\n", "    ", "pass", "\n", "context", ".", "set_code", "(", "grpc", ".", "StatusCode", ".", "UNIMPLEMENTED", ")", "\n", "context", ".", "set_details", "(", "'Method not implemented!'", ")", "\n", "raise", "NotImplementedError", "(", "'Method not implemented!'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.amr_pb2_grpc.add_AMRServerServicer_to_server": [[36, 47], ["grpc.method_handlers_generic_handler", "server.add_generic_rpc_handlers", "grpc.unary_unary_rpc_method_handler"], "function", ["None"], ["", "", "def", "add_AMRServerServicer_to_server", "(", "servicer", ",", "server", ")", ":", "\n", "  ", "rpc_method_handlers", "=", "{", "\n", "'process'", ":", "grpc", ".", "unary_unary_rpc_method_handler", "(", "\n", "servicer", ".", "process", ",", "\n", "request_deserializer", "=", "amr__pb2", ".", "AMRInput", ".", "FromString", ",", "\n", "response_serializer", "=", "amr__pb2", ".", "AMRResponse", ".", "SerializeToString", ",", "\n", ")", ",", "\n", "}", "\n", "generic_handler", "=", "grpc", ".", "method_handlers_generic_handler", "(", "\n", "'AMRServer'", ",", "rpc_method_handlers", ")", "\n", "server", ".", "add_generic_rpc_handlers", "(", "(", "generic_handler", ",", ")", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.fairseq_data_iterator.get_batch_iterator": [[32, 93], ["isinstance", "fairseq.data.data_utils.batch_by_size", "fairseq.data.data_utils.numpy_seed", "dataset.ordered_indices", "fairseq.data.data_utils.filter_by_size"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.data_utils.batch_by_size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.data_utils.numpy_seed", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.ordered_indices", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.data_utils.filter_by_size"], ["def", "get_batch_iterator", "(", "\n", "dataset", ",", "max_tokens", "=", "None", ",", "max_sentences", "=", "None", ",", "max_positions", "=", "None", ",", "\n", "ignore_invalid_inputs", "=", "False", ",", "required_batch_size_multiple", "=", "1", ",", "\n", "seed", "=", "1", ",", "num_shards", "=", "1", ",", "shard_id", "=", "0", ",", "num_workers", "=", "0", ",", "epoch", "=", "0", ",", "\n", "large_sent_first", "=", "False", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Get an iterator that yields batches of data from the given dataset.\n\n    Args:\n        dataset (~fairseq.data.FairseqDataset): dataset to batch\n        max_tokens (int, optional): max number of tokens in each batch\n            (default: None).\n        max_sentences (int, optional): max number of sentences in each\n            batch (default: None).\n        max_positions (optional): max sentence length supported by the\n            model (default: None).\n        ignore_invalid_inputs (bool, optional): don't raise Exception for\n            sentences that are too long (default: False).\n        required_batch_size_multiple (int, optional): require batch size to\n            be a multiple of N (default: 1).\n        seed (int, optional): seed for random number generator for\n            reproducibility (default: 1).\n        num_shards (int, optional): shard the data iterator into N\n            shards (default: 1).\n        shard_id (int, optional): which shard of the data iterator to\n            return (default: 0).\n        num_workers (int, optional): how many subprocesses to use for data\n            loading. 0 means the data will be loaded in the main process\n            (default: 0).\n        epoch (int, optional): the epoch to start the iterator from\n            (default: 0).\n\n    Returns:\n        ~fairseq.iterators.EpochBatchIterator: a batched iterator over the\n            given dataset split\n    \"\"\"", "\n", "assert", "isinstance", "(", "dataset", ",", "FairseqDataset", ")", "\n", "\n", "# get indices ordered by example size", "\n", "with", "data_utils", ".", "numpy_seed", "(", "seed", ")", ":", "\n", "        ", "indices", "=", "dataset", ".", "ordered_indices", "(", ")", "\n", "# invert order to start by bigger ones", "\n", "if", "large_sent_first", ":", "\n", "            ", "indices", "=", "indices", "[", ":", ":", "-", "1", "]", "\n", "\n", "# filter examples that are too large", "\n", "", "", "if", "max_positions", "is", "not", "None", ":", "\n", "        ", "indices", "=", "data_utils", ".", "filter_by_size", "(", "\n", "indices", ",", "dataset", ".", "size", ",", "max_positions", ",", "\n", "raise_exception", "=", "(", "not", "ignore_invalid_inputs", ")", ",", "\n", ")", "\n", "\n", "# create mini-batches with given size constraints", "\n", "", "batch_sampler", "=", "data_utils", ".", "batch_by_size", "(", "\n", "indices", ",", "dataset", ".", "num_tokens", ",", "max_tokens", "=", "max_tokens", ",", "\n", "max_sentences", "=", "max_sentences", ",", "\n", "required_batch_size_multiple", "=", "required_batch_size_multiple", ",", "\n", ")", "\n", "\n", "return", "batch_sampler", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.fairseq_data_iterator.main": [[95, 121], ["fairseq.utils.import_user_module", "fairseq.tasks.setup_task", "tasks.setup_task.load_dataset", "tasks.setup_task.dataset", "fairseq_data_iterator.get_batch_iterator", "tqdm.tqdm", "task.dataset.collater"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.import_user_module", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.sentence_prediction.SentencePredictionTask.setup_task", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.sentence_prediction.SentencePredictionTask.load_dataset", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.fairseq_task.FairseqTask.dataset", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.fairseq_task.FairseqTask.get_batch_iterator", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.collater"], ["", "def", "main", "(", "args", ")", ":", "\n", "\n", "# Load dataset ", "\n", "    ", "utils", ".", "import_user_module", "(", "args", ")", "\n", "task", "=", "tasks", ".", "setup_task", "(", "args", ")", "\n", "task", ".", "load_dataset", "(", "args", ".", "gen_subset", ")", "\n", "dataset", "=", "task", ".", "dataset", "(", "args", ".", "gen_subset", ")", "\n", "\n", "# Get iterator over batches", "\n", "batch_index_iterator", "=", "get_batch_iterator", "(", "\n", "dataset", "=", "dataset", ",", "\n", "max_tokens", "=", "args", ".", "max_tokens", ",", "\n", "max_sentences", "=", "args", ".", "max_sentences", ",", "\n", "max_positions", "=", "None", ",", "\n", "ignore_invalid_inputs", "=", "args", ".", "skip_invalid_size_inputs_valid_test", ",", "\n", "required_batch_size_multiple", "=", "args", ".", "required_batch_size_multiple", ",", "\n", "num_shards", "=", "args", ".", "num_shards", ",", "\n", "shard_id", "=", "args", ".", "shard_id", ",", "\n", "num_workers", "=", "args", ".", "num_workers", ",", "\n", "large_sent_first", "=", "False", "\n", ")", "\n", "\n", "# collate batch of sentences into single tensor for all data", "\n", "for", "batch_ids", "in", "tqdm", "(", "batch_index_iterator", ")", ":", "\n", "        ", "samples", "=", "[", "dataset", "[", "i", "]", "for", "i", "in", "batch_ids", "]", "\n", "dataset", ".", "collater", "(", "samples", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.fairseq_data_iterator.cli_main": [[123, 127], ["fairseq.options.get_generation_parser", "fairseq.options.parse_args_and_arch", "fairseq_data_iterator.main"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.get_generation_parser", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.parse_args_and_arch", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.main"], ["", "", "def", "cli_main", "(", ")", ":", "\n", "    ", "parser", "=", "options", ".", "get_generation_parser", "(", ")", "\n", "args", "=", "options", ".", "parse_args_and_arch", "(", "parser", ")", "\n", "main", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.roberta.argument_parsing": [[18, 48], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["def", "argument_parsing", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "description", "=", "'unit test for roberta unicode handling'", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'-i'", ",", "'--in-tokenized-sentences'", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "'File with one __tokenized__ sentence per line'", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'-p'", ",", "'--pretrained-embed'", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "default", "=", "\"roberta.large\"", ",", "\n", "help", "=", "'roberta model to load'", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'-o'", ",", "'--output-file'", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "'File to store bad unicode sentences'", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--raise-error'", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Set to force exception if unicode error found'", "\n", ")", "\n", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.roberta.main": [[49, 143], ["roberta.argument_parsing", "transition_amr_parser.io.read_sentences", "print", "open", "roberta.main.load_roberta"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.parse.argument_parsing", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.io.read_sentences", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_transformer_amr_parser.load_roberta"], ["", "def", "main", "(", ")", ":", "\n", "    ", "args", "=", "argument_parsing", "(", ")", "\n", "\n", "sentences", "=", "read_sentences", "(", "args", ".", "in_tokenized_sentences", ")", "\n", "split_sentences", "=", "[", "]", "\n", "for", "sentence", "in", "sentences", ":", "\n", "        ", "split_sentences", ".", "append", "(", "tokenize_line", "(", "sentence", ")", ")", "\n", "", "print", "(", "len", "(", "split_sentences", ")", ")", "\n", "\n", "bad_unicode", "=", "open", "(", "args", ".", "output_file", ",", "'w'", ")", "\n", "\n", "def", "load_roberta", "(", "name", "=", "None", ",", "roberta_cache_path", "=", "None", ")", ":", "\n", "        ", "if", "not", "roberta_cache_path", ":", "\n", "            ", "roberta", "=", "torch", ".", "hub", ".", "load", "(", "'pytorch/fairseq'", ",", "name", ")", "\n", "", "else", ":", "\n", "            ", "roberta", "=", "RobertaModel", ".", "from_pretrained", "(", "roberta_cach_path", ",", "checkpoint_file", "=", "'model.pt'", ")", "\n", "\n", "", "roberta", ".", "eval", "(", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "roberta", ".", "cuda", "(", ")", "\n", "", "return", "roberta", "\n", "\n", "", "def", "get_wordpiece_to_word_map", "(", "sentence", ",", "roberta_bpe", ",", "raise_error", ")", ":", "\n", "# Get word and worpiece tokens according to RoBERTa                                           ", "\n", "# sentence = sentence.replace(u'\\x91', u' ')", "\n", "# sentence = sentence.replace(u'\\x96', u' ')", "\n", "        ", "word_tokens", "=", "sentence", ".", "split", "(", ")", "\n", "wordpiece_tokens", "=", "[", "\n", "roberta_bpe", ".", "decode", "(", "wordpiece", ")", "\n", "for", "wordpiece", "in", "roberta_bpe", ".", "encode", "(", "sentence", ")", ".", "split", "(", ")", "\n", "]", "\n", "#print(\"wp_tokens: \", wordpiece_tokens)", "\n", "\n", "assert", "len", "(", "word_tokens", ")", "<=", "len", "(", "wordpiece_tokens", ")", "\n", "assert", "isinstance", "(", "word_tokens", ",", "list", ")", "\n", "assert", "isinstance", "(", "wordpiece_tokens", ",", "list", ")", "\n", "w_index", "=", "0", "\n", "word_to_wordpiece", "=", "[", "]", "\n", "subword_sequence", "=", "[", "]", "\n", "bad_unicode_flag", "=", "0", "\n", "\n", "for", "wp_index", "in", "range", "(", "len", "(", "wordpiece_tokens", ")", ")", ":", "\n", "            ", "if", "w_index", "in", "range", "(", "len", "(", "word_tokens", ")", ")", ":", "\n", "                ", "word", "=", "word_tokens", "[", "w_index", "]", "\n", "if", "word", "==", "wordpiece_tokens", "[", "wp_index", "]", ":", "\n", "                    ", "word_to_wordpiece", ".", "append", "(", "wp_index", ")", "\n", "w_index", "+=", "1", "\n", "", "else", ":", "\n", "                    ", "subword_sequence", ".", "append", "(", "wp_index", ")", "\n", "word_from_pieces", "=", "\"\"", ".", "join", "(", "[", "\n", "# NOTE: Facebooks BPE signals SOW with whitesplace                                ", "\n", "wordpiece_tokens", "[", "i", "]", ".", "lstrip", "(", ")", "\n", "for", "i", "in", "subword_sequence", "\n", "]", ")", "\n", "if", "word", "==", "word_from_pieces", ":", "\n", "                        ", "word_to_wordpiece", ".", "append", "(", "subword_sequence", ")", "\n", "w_index", "+=", "1", "\n", "subword_sequence", "=", "[", "]", "\n", "", "elif", "word_from_pieces", "not", "in", "word", ":", "\n", "                        ", "word_to_wordpiece", ".", "append", "(", "subword_sequence", ")", "\n", "w_index", "+=", "1", "\n", "subword_sequence", "=", "[", "]", "\n", "bad_unicode_flag", "=", "1", "\n", "\n", "", "", "", "", "if", "bad_unicode_flag", "==", "1", ":", "\n", "            ", "bad_unicode", ".", "write", "(", "sentence", ")", "\n", "wp", "=", "\" \"", ".", "join", "(", "wordpiece_tokens", ")", "\n", "print", "(", "\"\\n\\nsentence: \"", ",", "sentence", ")", "\n", "print", "(", "\"wp: \"", ",", "wp", ")", "\n", "print", "(", "\"\\n\"", ")", "\n", "bad_unicode", ".", "write", "(", "\"\\n\"", ")", "\n", "bad_unicode", ".", "write", "(", "wp", ")", "\n", "bad_unicode", ".", "write", "(", "\"\\n\\n\"", ")", "\n", "if", "raise_error", ":", "\n", "                ", "raise", "Exception", "(", "'Unicode splitting failed'", ")", "\n", "\n", "", "", "return", "word_to_wordpiece", "\n", "\n", "", "def", "check_wordpiece_to_word_map", "(", "input_file", ",", "raise_error", ")", ":", "\n", "        ", "num_sents", "=", "0", "\n", "with", "open", "(", "input_file", ",", "'r'", ")", "as", "fid", ":", "\n", "            ", "for", "sentence", "in", "tqdm", "(", "fid", ")", ":", "\n", "                ", "if", "not", "sentence", ":", "\n", "                    ", "break", "\n", "", "sentence", "=", "\" \"", ".", "join", "(", "tokenize_line", "(", "str", "(", "sentence", ".", "rstrip", "(", ")", ")", ")", ")", "\n", "#print(\"input: \", sentence)", "\n", "word2piece", "=", "get_wordpiece_to_word_map", "(", "\n", "sentence", ",", "\n", "roberta", ".", "bpe", ",", "\n", "raise_error", "\n", ")", "\n", "\n", "", "", "", "roberta", "=", "load_roberta", "(", "name", "=", "args", ".", "pretrained_embed", ")", "\n", "check_wordpiece_to_word_map", "(", "args", ".", "in_tokenized_sentences", ",", "args", ".", "raise_error", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.service.amr_client.argument_parser": [[10, 23], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["def", "argument_parser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'AMR parser'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--port\"", ",", "\n", "help", "=", "\"GRPC port\"", ",", "\n", "type", "=", "str", "\n", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "# Sanity checks", "\n", "assert", "args", ".", "port", "\n", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.service.amr_client.get_input_from_sentence": [[24, 30], ["sentence.split", "amr_pb2.AMRInput", "input_tokens.append", "amr_pb2.AMRInput.WordInfo"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "get_input_from_sentence", "(", "sentence", ")", ":", "\n", "    ", "tokens", "=", "sentence", ".", "split", "(", ")", "\n", "input_tokens", "=", "[", "]", "\n", "for", "token", "in", "tokens", ":", "\n", "        ", "input_tokens", ".", "append", "(", "amr_pb2", ".", "AMRInput", ".", "WordInfo", "(", "token", "=", "token", ")", ")", "\n", "", "return", "amr_pb2", ".", "AMRInput", "(", "word_infos", "=", "input_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.service.amr_client.run": [[31, 43], ["amr_client.argument_parser", "grpc.insecure_channel", "amr_pb2_grpc.AMRServerStub", "input", "amr_client.get_input_from_sentence", "amr_pb2_grpc.AMRServerStub.process", "print"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.argument_parser", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.service.amr_client.get_input_from_sentence", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.service.amr_server.Parser.process", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["", "def", "run", "(", ")", ":", "\n", "# NOTE(gRPC Python Team): .close() is possible on a channel and should be", "\n", "# used in circumstances in which the with statement does not fit the needs", "\n", "# of the code.", "\n", "# Argument handling", "\n", "    ", "args", "=", "argument_parser", "(", ")", "\n", "channel", "=", "grpc", ".", "insecure_channel", "(", "'localhost:'", "+", "args", ".", "port", ")", "\n", "stub", "=", "amr_pb2_grpc", ".", "AMRServerStub", "(", "channel", ")", "\n", "sentence", "=", "input", "(", "\"Enter the sentence: \"", ")", "\n", "amr_input", "=", "get_input_from_sentence", "(", "sentence", ")", "\n", "response", "=", "stub", ".", "process", "(", "amr_input", ")", "\n", "print", "(", "\"AMR parse received: \\n\"", "+", "response", ".", "amr_parse", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.service.amr_server.Parser.__init__": [[45, 50], ["torch.cuda.is_available", "transition_amr_parser.amr_parser.AMRParser"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "model_path", ",", "roberta_cache_path", "=", "None", ",", "roberta_use_gpu", "=", "False", ",", "model_use_gpu", "=", "False", ")", ":", "\n", "        ", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "roberta_use_gpu", "=", "True", "\n", "model_use_gpu", "=", "True", "\n", "", "self", ".", "parser", "=", "AMRParser", "(", "model_path", ",", "roberta_cache_path", "=", "roberta_cache_path", ",", "roberta_use_gpu", "=", "roberta_use_gpu", ",", "model_use_gpu", "=", "model_use_gpu", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.service.amr_server.Parser.process": [[51, 56], ["amr_server.Parser.parser.parse_sentence", "amr_pb2.AMRResponse", "amr_server.Parser.toJAMRString"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.fake_parse.FakeAMRParser.parse_sentence", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.toJAMRString"], ["", "def", "process", "(", "self", ",", "request", ",", "context", ")", ":", "\n", "        ", "word_tokens", "=", "request", ".", "word_infos", "\n", "tokens", "=", "[", "word_token", ".", "token", "for", "word_token", "in", "word_tokens", "]", "\n", "amr", "=", "self", ".", "parser", ".", "parse_sentence", "(", "tokens", ")", "\n", "return", "amr_pb2", ".", "AMRResponse", "(", "amr_parse", "=", "amr", ".", "toJAMRString", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.service.amr_server.argument_parser": [[18, 42], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["def", "argument_parser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'AMR parser'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--in-model\"", ",", "\n", "help", "=", "\"path to the AMR parsing model\"", ",", "\n", "type", "=", "str", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--roberta-cache-path\"", ",", "\n", "help", "=", "\"Path to the roberta large model\"", ",", "\n", "type", "=", "str", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--port\"", ",", "\n", "help", "=", "\"GRPC port\"", ",", "\n", "type", "=", "str", "\n", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "# Sanity checks", "\n", "assert", "args", ".", "in_model", "\n", "assert", "args", ".", "port", "\n", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.service.amr_server.serve": [[57, 66], ["amr_server.argument_parser", "grpc.server", "amr_pb2_grpc.add_AMRServerServicer_to_server", "grpc.server.add_insecure_port", "grpc.server.start", "grpc.server.wait_for_termination", "concurrent.futures.ThreadPoolExecutor", "amr_server.Parser"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.argument_parser", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.amr_pb2_grpc.add_AMRServerServicer_to_server", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.meters.StopwatchMeter.start"], ["", "", "def", "serve", "(", ")", ":", "\n", "# Argument handling", "\n", "    ", "args", "=", "argument_parser", "(", ")", "\n", "\n", "server", "=", "grpc", ".", "server", "(", "futures", ".", "ThreadPoolExecutor", "(", "max_workers", "=", "10", ")", ")", "\n", "amr_pb2_grpc", ".", "add_AMRServerServicer_to_server", "(", "Parser", "(", "model_path", "=", "args", ".", "in_model", ",", "roberta_cache_path", "=", "args", ".", "roberta_cache_path", ")", ",", "server", ")", "\n", "server", ".", "add_insecure_port", "(", "'[::]:'", "+", "args", ".", "port", ")", "\n", "server", ".", "start", "(", ")", "\n", "server", ".", "wait_for_termination", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.linearization.get_dictionary.is_next_open_bracket": [[1, 8], ["IndexError"], "function", ["None"], ["def", "is_next_open_bracket", "(", "line", ",", "start_idx", ")", ":", "\n", "    ", "for", "char", "in", "line", "[", "(", "start_idx", "+", "1", ")", ":", "]", ":", "\n", "        ", "if", "char", "==", "'('", ":", "\n", "            ", "return", "True", "\n", "", "elif", "char", "==", "')'", ":", "\n", "            ", "return", "False", "\n", "", "", "raise", "IndexError", "(", "'Bracket possibly not balanced, open bracket not followed by closed bracket'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.linearization.get_dictionary.get_between_brackets": [[9, 17], ["output.append"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "get_between_brackets", "(", "line", ",", "start_idx", ")", ":", "\n", "    ", "output", "=", "[", "]", "\n", "for", "char", "in", "line", "[", "(", "start_idx", "+", "1", ")", ":", "]", ":", "\n", "        ", "if", "char", "==", "')'", ":", "\n", "            ", "break", "\n", "", "assert", "not", "(", "char", "==", "'('", ")", "\n", "output", ".", "append", "(", "char", ")", "\n", "", "return", "''", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.linearization.get_dictionary.get_dict": [[18, 43], ["line.rstrip", "range", "terminal.split", "len", "len", "words_list.append", "output.append", "get_dictionary.is_next_open_bracket", "get_dictionary.get_between_brackets"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.linearization.constituent_dict.is_next_open_bracket", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.linearization.constituent_dict.get_between_brackets"], ["", "def", "get_dict", "(", "lines", ")", ":", "\n", "    ", "output", "=", "[", "]", "\n", "for", "line", "in", "lines", ":", "\n", "#print 'curr line', line_strip", "\n", "        ", "line_strip", "=", "line", ".", "rstrip", "(", ")", "\n", "#print 'length of the sentence', len(line_strip)", "\n", "for", "i", "in", "range", "(", "len", "(", "line_strip", ")", ")", ":", "\n", "            ", "if", "i", "==", "0", ":", "\n", "                ", "assert", "line_strip", "[", "i", "]", "==", "'('", "\n", "", "if", "line_strip", "[", "i", "]", "==", "'('", "and", "not", "(", "is_next_open_bracket", "(", "line_strip", ",", "i", ")", ")", ":", "# fulfilling this condition means this is a terminal symbol", "\n", "                ", "output", ".", "append", "(", "get_between_brackets", "(", "line_strip", ",", "i", ")", ")", "\n", "#print 'output:',output", "\n", "", "", "", "words_dict", "=", "{", "}", "\n", "for", "terminal", "in", "output", ":", "\n", "        ", "terminal_split", "=", "terminal", ".", "split", "(", ")", "\n", "assert", "len", "(", "terminal_split", ")", "==", "2", "# each terminal contains a POS tag and word        ", "\n", "if", "not", "(", "terminal_split", "[", "1", "]", "in", "words_dict", ")", ":", "\n", "            ", "words_dict", "[", "terminal_split", "[", "1", "]", "]", "=", "1", "\n", "", "else", ":", "\n", "            ", "words_dict", "[", "terminal_split", "[", "1", "]", "]", "=", "words_dict", "[", "terminal_split", "[", "1", "]", "]", "+", "1", "\n", "", "", "words_list", "=", "[", "]", "\n", "for", "item", "in", "words_dict", ":", "\n", "        ", "if", "words_dict", "[", "item", "]", ">", "1", ":", "\n", "            ", "words_list", ".", "append", "(", "item", ")", "\n", "", "", "return", "words_list", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.linearization.constituent_dict.is_next_open_bracket": [[1, 8], ["IndexError"], "function", ["None"], ["def", "is_next_open_bracket", "(", "line", ",", "start_idx", ")", ":", "\n", "    ", "for", "char", "in", "line", "[", "(", "start_idx", "+", "1", ")", ":", "]", ":", "\n", "        ", "if", "char", "==", "'('", ":", "\n", "            ", "return", "True", "\n", "", "elif", "char", "==", "')'", ":", "\n", "            ", "return", "False", "\n", "", "", "raise", "IndexError", "(", "'Bracket possibly not balanced, open bracket not followed by closed bracket'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.linearization.constituent_dict.get_between_brackets": [[9, 17], ["output.append"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "get_between_brackets", "(", "line", ",", "start_idx", ")", ":", "\n", "    ", "output", "=", "[", "]", "\n", "for", "char", "in", "line", "[", "(", "start_idx", "+", "1", ")", ":", "]", ":", "\n", "        ", "if", "char", "==", "')'", ":", "\n", "            ", "break", "\n", "", "assert", "not", "(", "char", "==", "'('", ")", "\n", "output", ".", "append", "(", "char", ")", "\n", "", "return", "''", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.linearization.constituent_dict.get_dict": [[18, 43], ["line.rstrip", "range", "terminal.split", "len", "len", "words_list.append", "output.append", "constituent_dict.is_next_open_bracket", "constituent_dict.get_between_brackets"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.linearization.constituent_dict.is_next_open_bracket", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.linearization.constituent_dict.get_between_brackets"], ["", "def", "get_dict", "(", "lines", ")", ":", "\n", "    ", "output", "=", "[", "]", "\n", "for", "line", "in", "lines", ":", "\n", "#print 'curr line', line_strip", "\n", "        ", "line_strip", "=", "line", ".", "rstrip", "(", ")", "\n", "#print 'length of the sentence', len(line_strip)", "\n", "for", "i", "in", "range", "(", "len", "(", "line_strip", ")", ")", ":", "\n", "            ", "if", "i", "==", "0", ":", "\n", "                ", "assert", "line_strip", "[", "i", "]", "==", "'('", "\n", "", "if", "line_strip", "[", "i", "]", "==", "'('", "and", "not", "(", "is_next_open_bracket", "(", "line_strip", ",", "i", ")", ")", ":", "# fulfilling this condition means this is a terminal symbol", "\n", "                ", "output", ".", "append", "(", "get_between_brackets", "(", "line_strip", ",", "i", ")", ")", "\n", "#print 'output:',output", "\n", "", "", "", "words_dict", "=", "{", "}", "\n", "for", "terminal", "in", "output", ":", "\n", "        ", "terminal_split", "=", "terminal", ".", "split", "(", ")", "\n", "assert", "len", "(", "terminal_split", ")", "==", "2", "# each terminal contains a POS tag and word        ", "\n", "if", "not", "(", "terminal_split", "[", "1", "]", "in", "words_dict", ")", ":", "\n", "            ", "words_dict", "[", "terminal_split", "[", "1", "]", "]", "=", "1", "\n", "", "else", ":", "\n", "            ", "words_dict", "[", "terminal_split", "[", "1", "]", "]", "=", "words_dict", "[", "terminal_split", "[", "1", "]", "]", "+", "1", "\n", "", "", "words_list", "=", "[", "]", "\n", "for", "item", "in", "words_dict", ":", "\n", "        ", "if", "words_dict", "[", "item", "]", ">", "1", ":", "\n", "            ", "words_list", ".", "append", "(", "item", ")", "\n", "", "", "return", "words_list", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.linearization.delin_inoSWAP.tree": [[3, 52], ["len", "len", "len", "range", "len", "print", "range", "print", "print", "btree.append", "len", "print", "len", "btree.append", "btree.insert", "openidx.append", "len", "btree.append", "len"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["def", "tree", "(", "acts", ",", "words", ",", "pos", ")", ":", "\n", "\t", "btree", "=", "[", "]", "\n", "openidx", "=", "[", "]", "\n", "wid", "=", "0", "\n", "\n", "previous_act", "=", "'N'", "\n", "\n", "size_tree", "=", "0", "\n", "max_size_tree", "=", "len", "(", "words", ")", "\n", "\n", "for", "act", "in", "acts", ":", "\n", "\t\t", "if", "act", "[", "0", "]", "==", "'S'", "and", "act", "[", "1", "]", "==", "'H'", ":", "\n", "\t\t\t", "if", "len", "(", "words", ")", "!=", "0", ":", "\n", "\t\t\t\t", "btree", ".", "append", "(", "\"(\"", "+", "pos", "[", "0", "]", "+", "\" \"", "+", "words", "[", "0", "]", "+", "\")\"", ")", "\n", "del", "words", "[", "0", "]", "\n", "del", "pos", "[", "0", "]", "\n", "wid", "+=", "1", "\n", "", "previous_act", "=", "'S'", "\n", "size_tree", "+=", "1", "\n", "\n", "", "elif", "act", "[", "0", "]", "==", "'N'", ":", "\n", "\t\t\t", "btree", ".", "insert", "(", "-", "1", ",", "\"(\"", "+", "act", "[", "3", ":", "-", "1", "]", ")", "\n", "openidx", ".", "append", "(", "len", "(", "btree", ")", "-", "2", ")", "\n", "previous_act", "=", "'N'", "\n", "", "else", ":", "#REDUCE", "\n", "\n", "\t\t\t", "if", "len", "(", "openidx", ")", ">", "0", ":", "\n", "\t\t\t\t", "tmp", "=", "\" \"", ".", "join", "(", "btree", "[", "openidx", "[", "-", "1", "]", ":", "]", ")", "+", "\")\"", "\n", "btree", "=", "btree", "[", ":", "openidx", "[", "-", "1", "]", "]", "\n", "btree", ".", "append", "(", "tmp", ")", "\n", "openidx", "=", "openidx", "[", ":", "-", "1", "]", "\n", "", "previous_act", "=", "'R'", "\n", "\n", "\n", "", "", "if", "len", "(", "openidx", ")", ">", "0", ":", "\n", "\t\t", "tope", "=", "len", "(", "openidx", ")", "\n", "for", "i", "in", "range", "(", "tope", ")", ":", "\n", "\t\t\t", "tmp", "=", "\" \"", ".", "join", "(", "btree", "[", "openidx", "[", "-", "1", "]", ":", "]", ")", "+", "\")\"", "\n", "btree", "=", "btree", "[", ":", "openidx", "[", "-", "1", "]", "]", "\n", "btree", ".", "append", "(", "tmp", ")", "\n", "openidx", "=", "openidx", "[", ":", "-", "1", "]", "\n", "\n", "#print(btree)", "\n", "", "", "if", "len", "(", "btree", ")", ">", "1", ":", "\n", "\t\t", "print", "(", "'(ROOT'", ",", "end", "=", "''", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "btree", ")", ")", ":", "\n", "\t\t\t\t", "print", "(", "btree", "[", "i", "]", ",", "end", "=", "''", ")", "\n", "", "print", "(", "')'", ")", "\n", "", "else", ":", "\n", "\t\t", "print", "(", "btree", "[", "0", "]", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.linearization.delin_inoSWAP.reorder_text": [[54, 65], ["reversed", "range", "buffer.append", "len", "stack.append", "buffer.append", "buffer.pop", "stack.pop"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop"], ["\n", "", "", "def", "reorder_text", "(", "actions", ",", "text", ")", ":", "\n", "\t", "stack", "=", "[", "]", "\n", "buffer", "=", "[", "]", "\n", "for", "t", "in", "reversed", "(", "range", "(", "len", "(", "text", ")", ")", ")", ":", "\n", "\t\t", "buffer", ".", "append", "(", "text", "[", "t", "]", ")", "\n", "\n", "", "for", "a", "in", "actions", ":", "\n", "\t\t", "if", "a", "[", "1", "]", "==", "'H'", ":", "stack", ".", "append", "(", "buffer", ".", "pop", "(", ")", ")", "\n", "if", "a", "[", "1", "]", "==", "'W'", ":", "buffer", ".", "append", "(", "stack", ".", "pop", "(", "-", "2", ")", ")", "\n", "\n", "", "return", "stack", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.correctly_installed.check_cuda_torch_binary_vs_bare_metal": [[6, 26], ["subprocess.check_output", "subprocess.check_output.split", "output[].split", "raw_output.split.index", "torch.version.cuda.split", "torch.version.cuda.split", "print"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.index", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["def", "check_cuda_torch_binary_vs_bare_metal", "(", ")", ":", "\n", "\n", "# command line CUDA", "\n", "    ", "cuda_dir", "=", "torch", ".", "utils", ".", "cpp_extension", ".", "CUDA_HOME", "\n", "cuda_call", "=", "[", "cuda_dir", "+", "\"/bin/nvcc\"", ",", "\"-V\"", "]", "\n", "raw_output", "=", "subprocess", ".", "check_output", "(", "cuda_call", ",", "universal_newlines", "=", "True", ")", "\n", "output", "=", "raw_output", ".", "split", "(", ")", "\n", "release_idx", "=", "output", ".", "index", "(", "\"release\"", ")", "+", "1", "\n", "release", "=", "output", "[", "release_idx", "]", ".", "split", "(", "\".\"", ")", "\n", "bare_metal_major", "=", "release", "[", "0", "]", "\n", "bare_metal_minor", "=", "release", "[", "1", "]", "[", "0", "]", "\n", "\n", "# torch compilation CUDA", "\n", "torch_binary_major", "=", "torch", ".", "version", ".", "cuda", ".", "split", "(", "\".\"", ")", "[", "0", "]", "\n", "torch_binary_minor", "=", "torch", ".", "version", ".", "cuda", ".", "split", "(", "\".\"", ")", "[", "1", "]", "\n", "\n", "if", "(", "bare_metal_major", "!=", "torch_binary_major", ")", "or", "(", "bare_metal_minor", "!=", "torch_binary_minor", ")", ":", "\n", "        ", "print", "(", "\n", "\"Pytorch binaries were compiled with Cuda {} but binary {} is {}\"", ".", "format", "(", "\n", "torch", ".", "version", ".", "cuda", ",", "cuda_dir", "+", "\"/bin/nvcc\"", ",", "output", "[", "release_idx", "]", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_concat_dataset.TestConcatDataset.setUp": [[15, 40], ["tests.test_train.mock_dict", "torch.LongTensor().view", "fairseq.data.TokenBlockDataset", "fairseq.data.LanguagePairDataset", "torch.LongTensor().view", "fairseq.data.TokenBlockDataset", "fairseq.data.LanguagePairDataset", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor().view.size", "torch.LongTensor().view.size"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_train.mock_dict", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "d", "=", "mock_dict", "(", ")", "\n", "tokens_1", "=", "torch", ".", "LongTensor", "(", "[", "1", "]", ")", ".", "view", "(", "1", ",", "-", "1", ")", "\n", "tokens_ds1", "=", "TokenBlockDataset", "(", "\n", "tokens_1", ",", "\n", "sizes", "=", "[", "tokens_1", ".", "size", "(", "-", "1", ")", "]", ",", "\n", "block_size", "=", "1", ",", "\n", "pad", "=", "0", ",", "\n", "eos", "=", "1", ",", "\n", "include_targets", "=", "False", ",", "\n", ")", "\n", "self", ".", "dataset_1", "=", "LanguagePairDataset", "(", "\n", "tokens_ds1", ",", "tokens_ds1", ".", "sizes", ",", "d", ",", "shuffle", "=", "False", "\n", ")", "\n", "tokens_2", "=", "torch", ".", "LongTensor", "(", "[", "2", "]", ")", ".", "view", "(", "1", ",", "-", "1", ")", "\n", "tokens_ds2", "=", "TokenBlockDataset", "(", "\n", "tokens_2", ",", "\n", "sizes", "=", "[", "tokens_2", ".", "size", "(", "-", "1", ")", "]", ",", "\n", "block_size", "=", "1", ",", "\n", "pad", "=", "0", ",", "\n", "eos", "=", "1", ",", "\n", "include_targets", "=", "False", ",", "\n", ")", "\n", "self", ".", "dataset_2", "=", "LanguagePairDataset", "(", "\n", "tokens_ds2", ",", "tokens_ds2", ".", "sizes", ",", "d", ",", "shuffle", "=", "False", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_concat_dataset.TestConcatDataset.test_concat_dataset_basics": [[42, 65], ["fairseq.data.concat_dataset.ConcatDataset", "fairseq.data.concat_dataset.ConcatDataset", "fairseq.data.concat_dataset.ConcatDataset", "len", "len", "len"], "methods", ["None"], ["", "def", "test_concat_dataset_basics", "(", "self", ")", ":", "\n", "        ", "d", "=", "ConcatDataset", "(", "\n", "[", "self", ".", "dataset_1", ",", "self", ".", "dataset_2", "]", "\n", ")", "\n", "assert", "(", "len", "(", "d", ")", "==", "2", ")", "\n", "assert", "(", "d", "[", "0", "]", "[", "'source'", "]", "[", "0", "]", "==", "1", ")", "\n", "assert", "(", "d", "[", "1", "]", "[", "'source'", "]", "[", "0", "]", "==", "2", ")", "\n", "\n", "d", "=", "ConcatDataset", "(", "\n", "[", "self", ".", "dataset_1", ",", "self", ".", "dataset_2", "]", ",", "sample_ratios", "=", "[", "1", ",", "2", "]", "\n", ")", "\n", "assert", "(", "len", "(", "d", ")", "==", "3", ")", "\n", "assert", "(", "d", "[", "0", "]", "[", "'source'", "]", "[", "0", "]", "==", "1", ")", "\n", "assert", "(", "d", "[", "1", "]", "[", "'source'", "]", "[", "0", "]", "==", "2", ")", "\n", "assert", "(", "d", "[", "2", "]", "[", "'source'", "]", "[", "0", "]", "==", "2", ")", "\n", "\n", "d", "=", "ConcatDataset", "(", "\n", "[", "self", ".", "dataset_1", ",", "self", ".", "dataset_2", "]", ",", "sample_ratios", "=", "[", "2", ",", "1", "]", "\n", ")", "\n", "assert", "(", "len", "(", "d", ")", "==", "3", ")", "\n", "assert", "(", "d", "[", "0", "]", "[", "'source'", "]", "[", "0", "]", "==", "1", ")", "\n", "assert", "(", "d", "[", "1", "]", "[", "'source'", "]", "[", "0", "]", "==", "1", ")", "\n", "assert", "(", "d", "[", "2", "]", "[", "'source'", "]", "[", "0", "]", "==", "2", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_reproducibility.TestReproducibility._test_reproducibility": [[18, 66], ["tempfile.TemporaryDirectory", "io.StringIO", "stdout.getvalue.getvalue.getvalue", "map", "os.rename", "io.StringIO", "stdout.getvalue.getvalue.getvalue", "map", "contextlib.redirect_stdout", "test_binaries.create_dummy_data", "test_binaries.preprocess_translation_data", "contextlib.redirect_stdout", "test_binaries.train_translation_model", "os.path.join", "os.path.join", "contextlib.redirect_stdout", "test_binaries.train_translation_model", "round", "test_reproducibility.TestReproducibility.assertEqual", "test_reproducibility.TestReproducibility.assertEqual", "io.StringIO", "stdout.getvalue.getvalue.split", "stdout.getvalue.getvalue.split", "float", "test_reproducibility.TestReproducibility._test_reproducibility.cast"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.create_dummy_data", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.preprocess_translation_data", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.train_translation_model", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.train_translation_model", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["    ", "def", "_test_reproducibility", "(", "self", ",", "name", ",", "extra_flags", "=", "None", ")", ":", "\n", "        ", "if", "extra_flags", "is", "None", ":", "\n", "            ", "extra_flags", "=", "[", "]", "\n", "\n", "", "with", "tempfile", ".", "TemporaryDirectory", "(", "name", ")", "as", "data_dir", ":", "\n", "            ", "with", "contextlib", ".", "redirect_stdout", "(", "StringIO", "(", ")", ")", ":", "\n", "                ", "test_binaries", ".", "create_dummy_data", "(", "data_dir", ")", "\n", "test_binaries", ".", "preprocess_translation_data", "(", "data_dir", ")", "\n", "\n", "# train epochs 1 and 2 together", "\n", "", "stdout", "=", "StringIO", "(", ")", "\n", "with", "contextlib", ".", "redirect_stdout", "(", "stdout", ")", ":", "\n", "                ", "test_binaries", ".", "train_translation_model", "(", "\n", "data_dir", ",", "'fconv_iwslt_de_en'", ",", "[", "\n", "'--dropout'", ",", "'0.0'", ",", "\n", "'--log-format'", ",", "'json'", ",", "\n", "'--log-interval'", ",", "'1'", ",", "\n", "'--max-epoch'", ",", "'3'", ",", "\n", "]", "+", "extra_flags", ",", "\n", ")", "\n", "", "stdout", "=", "stdout", ".", "getvalue", "(", ")", "\n", "train_log", ",", "valid_log", "=", "map", "(", "json", ".", "loads", ",", "stdout", ".", "split", "(", "'\\n'", ")", "[", "-", "5", ":", "-", "3", "]", ")", "\n", "\n", "# train epoch 2, resuming from previous checkpoint 1", "\n", "os", ".", "rename", "(", "\n", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'checkpoint1.pt'", ")", ",", "\n", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'checkpoint_last.pt'", ")", ",", "\n", ")", "\n", "stdout", "=", "StringIO", "(", ")", "\n", "with", "contextlib", ".", "redirect_stdout", "(", "stdout", ")", ":", "\n", "                ", "test_binaries", ".", "train_translation_model", "(", "\n", "data_dir", ",", "'fconv_iwslt_de_en'", ",", "[", "\n", "'--dropout'", ",", "'0.0'", ",", "\n", "'--log-format'", ",", "'json'", ",", "\n", "'--log-interval'", ",", "'1'", ",", "\n", "'--max-epoch'", ",", "'3'", ",", "\n", "]", "+", "extra_flags", ",", "\n", ")", "\n", "", "stdout", "=", "stdout", ".", "getvalue", "(", ")", "\n", "train_res_log", ",", "valid_res_log", "=", "map", "(", "json", ".", "loads", ",", "stdout", ".", "split", "(", "'\\n'", ")", "[", "-", "5", ":", "-", "3", "]", ")", "\n", "\n", "def", "cast", "(", "s", ")", ":", "\n", "                ", "return", "round", "(", "float", "(", "s", ")", ",", "3", ")", "\n", "\n", "", "for", "k", "in", "[", "'train_loss'", ",", "'train_ppl'", ",", "'train_num_updates'", ",", "'train_gnorm'", "]", ":", "\n", "                ", "self", ".", "assertEqual", "(", "cast", "(", "train_log", "[", "k", "]", ")", ",", "cast", "(", "train_res_log", "[", "k", "]", ")", ")", "\n", "", "for", "k", "in", "[", "'valid_loss'", ",", "'valid_ppl'", ",", "'valid_num_updates'", ",", "'valid_best_loss'", "]", ":", "\n", "                ", "self", ".", "assertEqual", "(", "cast", "(", "valid_log", "[", "k", "]", ")", ",", "cast", "(", "valid_res_log", "[", "k", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_reproducibility.TestReproducibility.test_reproducibility": [[67, 69], ["test_reproducibility.TestReproducibility._test_reproducibility"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_reproducibility.TestReproducibility._test_reproducibility"], ["", "", "", "def", "test_reproducibility", "(", "self", ")", ":", "\n", "        ", "self", ".", "_test_reproducibility", "(", "'test_reproducibility'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_reproducibility.TestReproducibility.test_reproducibility_fp16": [[70, 74], ["test_reproducibility.TestReproducibility._test_reproducibility"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_reproducibility.TestReproducibility._test_reproducibility"], ["", "def", "test_reproducibility_fp16", "(", "self", ")", ":", "\n", "        ", "self", ".", "_test_reproducibility", "(", "'test_reproducibility_fp16'", ",", "[", "\n", "'--fp16'", ",", "\n", "'--fp16-init-scale'", ",", "'4096'", ",", "\n", "]", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_reproducibility.TestReproducibility.test_reproducibility_memory_efficient_fp16": [[76, 80], ["test_reproducibility.TestReproducibility._test_reproducibility"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_reproducibility.TestReproducibility._test_reproducibility"], ["", "def", "test_reproducibility_memory_efficient_fp16", "(", "self", ")", ":", "\n", "        ", "self", ".", "_test_reproducibility", "(", "'test_reproducibility_memory_efficient_fp16'", ",", "[", "\n", "'--memory-efficient-fp16'", ",", "\n", "'--fp16-init-scale'", ",", "'4096'", ",", "\n", "]", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_memory_efficient_fp16.TestMemoryEfficientFP16.test_load_state_dict": [[17, 56], ["torch.nn.Linear().cuda().half", "list", "fairseq.optim.adam.FairseqAdam", "fairseq.optim.fp16_optimizer.MemoryEfficientFP16Optimizer", "torch.nn.Linear().cuda().half.sum", "fairseq.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.backward", "fairseq.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.step", "fairseq.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.state_dict", "fairseq.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.load_state_dict", "fairseq.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.optimizer.state.items", "torch.nn.Linear().cuda().half.parameters", "argparse.Namespace", "argparse.Namespace", "test_memory_efficient_fp16.TestMemoryEfficientFP16.assertTrue", "v.values", "torch.nn.Linear().cuda", "torch.nn.Linear().cuda().half.", "torch.is_tensor", "torch.rand().cuda().half", "test_memory_efficient_fp16.TestMemoryEfficientFP16.assertTrue", "torch.nn.Linear", "torch.rand().cuda", "torch.rand"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.scalar_bias.ScalarBias.backward", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.polynomial_decay_schedule.PolynomialDecaySchedule.step", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.state_dict", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.load_state_dict", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear"], ["    ", "def", "test_load_state_dict", "(", "self", ")", ":", "\n", "# define simple FP16 model", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "Linear", "(", "5", ",", "5", ")", ".", "cuda", "(", ")", ".", "half", "(", ")", "\n", "params", "=", "list", "(", "model", ".", "parameters", "(", ")", ")", "\n", "\n", "# initialize memory efficient FP16 optimizer", "\n", "optimizer", "=", "FairseqAdam", "(", "\n", "argparse", ".", "Namespace", "(", "\n", "lr", "=", "[", "0.00001", "]", ",", "\n", "adam_betas", "=", "'(0.9, 0.999)'", ",", "\n", "adam_eps", "=", "1e-8", ",", "\n", "weight_decay", "=", "0.0", ",", "\n", ")", ",", "\n", "params", ",", "\n", ")", "\n", "me_optimizer", "=", "MemoryEfficientFP16Optimizer", "(", "\n", "argparse", ".", "Namespace", "(", "\n", "fp16_init_scale", "=", "1", ",", "\n", "fp16_scale_window", "=", "1", ",", "\n", "fp16_scale_tolerance", "=", "1", ",", "\n", "threshold_loss_scale", "=", "1", ",", "\n", ")", ",", "\n", "params", ",", "\n", "optimizer", ",", "\n", ")", "\n", "\n", "# optimizer state is created in the first step", "\n", "loss", "=", "model", "(", "torch", ".", "rand", "(", "5", ")", ".", "cuda", "(", ")", ".", "half", "(", ")", ")", ".", "sum", "(", ")", "\n", "me_optimizer", ".", "backward", "(", "loss", ")", "\n", "me_optimizer", ".", "step", "(", ")", "\n", "\n", "# reload state", "\n", "state", "=", "me_optimizer", ".", "state_dict", "(", ")", "\n", "me_optimizer", ".", "load_state_dict", "(", "state", ")", "\n", "for", "k", ",", "v", "in", "me_optimizer", ".", "optimizer", ".", "state", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "assertTrue", "(", "k", ".", "dtype", "==", "torch", ".", "float16", ")", "\n", "for", "v_i", "in", "v", ".", "values", "(", ")", ":", "\n", "                ", "if", "torch", ".", "is_tensor", "(", "v_i", ")", ":", "\n", "                    ", "self", ".", "assertTrue", "(", "v_i", ".", "dtype", "==", "torch", ".", "float32", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_character_token_embedder.TestCharacterTokenEmbedder.test_character_token_embedder": [[14, 39], ["fairseq.data.Dictionary", "fairseq.data.Dictionary.add_symbol", "fairseq.data.Dictionary.add_symbol", "fairseq.modules.CharacterTokenEmbedder", "max", "torch.LongTensor().fill_", "range", "fairseq.modules.CharacterTokenEmbedder.", "test_character_token_embedder.TestCharacterTokenEmbedder.assertAlmostEqual", "test_character_token_embedder.TestCharacterTokenEmbedder.assertAlmostEqual", "test_character_token_embedder.TestCharacterTokenEmbedder.assertAlmostEqual", "test_character_token_embedder.TestCharacterTokenEmbedder.assertAlmostEqual", "fairseq.modules.CharacterTokenEmbedder.sum().backward", "fairseq.data.Dictionary.pad", "len", "fairseq.data.Dictionary.eos", "range", "fairseq.data.Dictionary.eos", "fairseq.modules.CharacterTokenEmbedder.size", "len", "torch.LongTensor", "len", "fairseq.data.Dictionary.index", "len", "fairseq.modules.CharacterTokenEmbedder.sum", "len"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertAlmostEqual", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertAlmostEqual", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertAlmostEqual", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertAlmostEqual", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.scalar_bias.ScalarBias.backward", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.index"], ["    ", "def", "test_character_token_embedder", "(", "self", ")", ":", "\n", "        ", "vocab", "=", "Dictionary", "(", ")", "\n", "vocab", ".", "add_symbol", "(", "'hello'", ")", "\n", "vocab", ".", "add_symbol", "(", "'there'", ")", "\n", "\n", "embedder", "=", "CharacterTokenEmbedder", "(", "vocab", ",", "[", "(", "2", ",", "16", ")", ",", "(", "4", ",", "32", ")", ",", "(", "8", ",", "64", ")", ",", "(", "16", ",", "2", ")", "]", ",", "64", ",", "5", ",", "2", ")", "\n", "\n", "test_sents", "=", "[", "[", "'hello'", ",", "'unk'", ",", "'there'", "]", ",", "[", "'there'", "]", ",", "[", "'hello'", ",", "'there'", "]", "]", "\n", "max_len", "=", "max", "(", "len", "(", "s", ")", "for", "s", "in", "test_sents", ")", "\n", "input", "=", "torch", ".", "LongTensor", "(", "len", "(", "test_sents", ")", ",", "max_len", "+", "2", ")", ".", "fill_", "(", "vocab", ".", "pad", "(", ")", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "test_sents", ")", ")", ":", "\n", "            ", "input", "[", "i", "]", "[", "0", "]", "=", "vocab", ".", "eos", "(", ")", "\n", "for", "j", "in", "range", "(", "len", "(", "test_sents", "[", "i", "]", ")", ")", ":", "\n", "                ", "input", "[", "i", "]", "[", "j", "+", "1", "]", "=", "vocab", ".", "index", "(", "test_sents", "[", "i", "]", "[", "j", "]", ")", "\n", "", "input", "[", "i", "]", "[", "j", "+", "2", "]", "=", "vocab", ".", "eos", "(", ")", "\n", "", "embs", "=", "embedder", "(", "input", ")", "\n", "\n", "assert", "embs", ".", "size", "(", ")", "==", "(", "len", "(", "test_sents", ")", ",", "max_len", "+", "2", ",", "5", ")", "\n", "self", ".", "assertAlmostEqual", "(", "embs", "[", "0", "]", "[", "0", "]", ",", "embs", "[", "1", "]", "[", "0", "]", ")", "\n", "self", ".", "assertAlmostEqual", "(", "embs", "[", "0", "]", "[", "0", "]", ",", "embs", "[", "0", "]", "[", "-", "1", "]", ")", "\n", "self", ".", "assertAlmostEqual", "(", "embs", "[", "0", "]", "[", "1", "]", ",", "embs", "[", "2", "]", "[", "1", "]", ")", "\n", "self", ".", "assertAlmostEqual", "(", "embs", "[", "0", "]", "[", "3", "]", ",", "embs", "[", "1", "]", "[", "1", "]", ")", "\n", "\n", "embs", ".", "sum", "(", ")", ".", "backward", "(", ")", "\n", "assert", "embedder", ".", "char_embeddings", ".", "weight", ".", "grad", "is", "not", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_character_token_embedder.TestCharacterTokenEmbedder.assertAlmostEqual": [[40, 43], ["test_character_token_embedder.TestCharacterTokenEmbedder.assertEqual", "test_character_token_embedder.TestCharacterTokenEmbedder.assertLess", "t1.size", "t2.size"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "assertAlmostEqual", "(", "self", ",", "t1", ",", "t2", ")", ":", "\n", "        ", "self", ".", "assertEqual", "(", "t1", ".", "size", "(", ")", ",", "t2", ".", "size", "(", ")", ",", "\"size mismatch\"", ")", "\n", "self", ".", "assertLess", "(", "(", "t1", "-", "t2", ")", ".", "abs", "(", ")", ".", "max", "(", ")", ",", "1e-6", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sparse_multihead_attention.TestSparseMultiheadAttention.test_sparse_multihead_attention": [[12, 45], ["torch.randn", "torch.tensor", "fairseq.modules.sparse_multihead_attention.SparseMultiheadAttention", "fairseq.modules.sparse_multihead_attention.SparseMultiheadAttention.buffered_sparse_mask", "torch.all", "torch.tensor", "fairseq.modules.sparse_multihead_attention.SparseMultiheadAttention", "fairseq.modules.sparse_multihead_attention.SparseMultiheadAttention.buffered_sparse_mask", "torch.all", "torch.eq", "torch.eq", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.sparse_multihead_attention.SparseMultiheadAttention.buffered_sparse_mask", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.sparse_multihead_attention.SparseMultiheadAttention.buffered_sparse_mask"], ["    ", "def", "test_sparse_multihead_attention", "(", "self", ")", ":", "\n", "        ", "attn_weights", "=", "torch", ".", "randn", "(", "1", ",", "8", ",", "8", ")", "\n", "bidirectional_sparse_mask", "=", "torch", ".", "tensor", "(", "[", "\n", "[", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "float", "(", "'-inf'", ")", ",", "float", "(", "'-inf'", ")", ",", "0", "]", ",", "\n", "[", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "float", "(", "'-inf'", ")", ",", "float", "(", "'-inf'", ")", ",", "0", "]", ",", "\n", "[", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "float", "(", "'-inf'", ")", ",", "float", "(", "'-inf'", ")", ",", "0", "]", ",", "\n", "[", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "float", "(", "'-inf'", ")", ",", "float", "(", "'-inf'", ")", ",", "0", "]", ",", "\n", "[", "float", "(", "'-inf'", ")", ",", "float", "(", "'-inf'", ")", ",", "float", "(", "'-inf'", ")", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", "]", ",", "\n", "[", "float", "(", "'-inf'", ")", ",", "float", "(", "'-inf'", ")", ",", "float", "(", "'-inf'", ")", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", "]", ",", "\n", "[", "float", "(", "'-inf'", ")", ",", "float", "(", "'-inf'", ")", ",", "float", "(", "'-inf'", ")", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", "]", ",", "\n", "[", "float", "(", "'-inf'", ")", ",", "float", "(", "'-inf'", ")", ",", "float", "(", "'-inf'", ")", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", "]", "\n", "]", ")", "\n", "\n", "bidirectional_attention", "=", "SparseMultiheadAttention", "(", "16", ",", "1", ",", "stride", "=", "4", ",", "expressivity", "=", "1", ",", "is_bidirectional", "=", "True", ")", "\n", "bidirectional_attention_sparse_mask", "=", "bidirectional_attention", ".", "buffered_sparse_mask", "(", "attn_weights", ",", "8", ",", "8", ")", "\n", "torch", ".", "all", "(", "torch", ".", "eq", "(", "bidirectional_attention_sparse_mask", ",", "bidirectional_sparse_mask", ")", ")", "\n", "\n", "sparse_mask", "=", "torch", ".", "tensor", "(", "[", "\n", "[", "0", ",", "float", "(", "'-inf'", ")", ",", "float", "(", "'-inf'", ")", ",", "float", "(", "'-inf'", ")", ",", "float", "(", "'-inf'", ")", ",", "float", "(", "'-inf'", ")", ",", "\n", "float", "(", "'-inf'", ")", ",", "float", "(", "'-inf'", ")", "]", ",", "\n", "[", "0", ",", "0", ",", "float", "(", "'-inf'", ")", ",", "float", "(", "'-inf'", ")", ",", "float", "(", "'-inf'", ")", ",", "float", "(", "'-inf'", ")", ",", "float", "(", "'-inf'", ")", ",", "float", "(", "'-inf'", ")", "]", ",", "\n", "[", "0", ",", "0", ",", "0", ",", "float", "(", "'-inf'", ")", ",", "float", "(", "'-inf'", ")", ",", "float", "(", "'-inf'", ")", ",", "float", "(", "'-inf'", ")", ",", "float", "(", "'-inf'", ")", "]", ",", "\n", "[", "0", ",", "0", ",", "0", ",", "0", ",", "float", "(", "'-inf'", ")", ",", "float", "(", "'-inf'", ")", ",", "float", "(", "'-inf'", ")", ",", "float", "(", "'-inf'", ")", "]", ",", "\n", "[", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "float", "(", "'-inf'", ")", ",", "float", "(", "'-inf'", ")", ",", "float", "(", "'-inf'", ")", "]", ",", "\n", "[", "float", "(", "'-inf'", ")", ",", "float", "(", "'-inf'", ")", ",", "float", "(", "'-inf'", ")", ",", "0", ",", "0", ",", "0", ",", "float", "(", "'-inf'", ")", ",", "float", "(", "'-inf'", ")", "]", ",", "\n", "[", "float", "(", "'-inf'", ")", ",", "float", "(", "'-inf'", ")", ",", "float", "(", "'-inf'", ")", ",", "0", ",", "0", ",", "0", ",", "0", ",", "float", "(", "'-inf'", ")", "]", ",", "\n", "[", "float", "(", "'-inf'", ")", ",", "float", "(", "'-inf'", ")", ",", "float", "(", "'-inf'", ")", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", "]", ",", "\n", "]", ")", "\n", "\n", "attention", "=", "SparseMultiheadAttention", "(", "16", ",", "1", ",", "stride", "=", "4", ",", "expressivity", "=", "1", ",", "is_bidirectional", "=", "False", ")", "\n", "attention_sparse_mask", "=", "attention", ".", "buffered_sparse_mask", "(", "attn_weights", ",", "8", ",", "8", ")", "\n", "\n", "torch", ".", "all", "(", "torch", ".", "eq", "(", "attention_sparse_mask", ",", "sparse_mask", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_label_smoothing.TestLabelSmoothing.setUp": [[20, 49], ["tests.dummy_dictionary", "len", "test_label_smoothing.TestLabelSmoothing.assertEqual", "test_label_smoothing.TestLabelSmoothing.assertEqual", "test_label_smoothing.TestLabelSmoothing.assertEqual", "test_label_smoothing.TestLabelSmoothing.assertEqual", "next", "argparse.Namespace", "torch.FloatTensor().unsqueeze().expand", "tests.TestTranslationTask.setup_task", "test_label_smoothing.TestLabelSmoothing.task.build_model", "test_label_smoothing.TestLabelSmoothing.d.pad", "test_label_smoothing.TestLabelSmoothing.d.eos", "test_label_smoothing.TestLabelSmoothing.d.unk", "tests.dummy_dataloader", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.FloatTensor().unsqueeze", "torch.FloatTensor"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.utils.dummy_dictionary", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.sentence_prediction.SentencePredictionTask.setup_task", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv_lm.LightConvLanguageModel.build_model", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.unk", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.utils.dummy_dataloader"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "# build dictionary", "\n", "        ", "self", ".", "d", "=", "test_utils", ".", "dummy_dictionary", "(", "3", ")", "\n", "vocab", "=", "len", "(", "self", ".", "d", ")", "\n", "self", ".", "assertEqual", "(", "vocab", ",", "4", "+", "3", ")", "# 4 special + 3 tokens", "\n", "self", ".", "assertEqual", "(", "self", ".", "d", ".", "pad", "(", ")", ",", "1", ")", "\n", "self", ".", "assertEqual", "(", "self", ".", "d", ".", "eos", "(", ")", ",", "2", ")", "\n", "self", ".", "assertEqual", "(", "self", ".", "d", ".", "unk", "(", ")", ",", "3", ")", "\n", "pad", ",", "eos", ",", "unk", ",", "w1", ",", "w2", ",", "w3", "=", "1", ",", "2", ",", "3", ",", "4", ",", "5", ",", "6", "# noqa: F841", "\n", "\n", "# build dataset", "\n", "self", ".", "data", "=", "[", "\n", "# the first batch item has padding", "\n", "{", "'source'", ":", "torch", ".", "LongTensor", "(", "[", "w1", ",", "eos", "]", ")", ",", "'target'", ":", "torch", ".", "LongTensor", "(", "[", "w1", ",", "eos", "]", ")", "}", ",", "\n", "{", "'source'", ":", "torch", ".", "LongTensor", "(", "[", "w1", ",", "eos", "]", ")", ",", "'target'", ":", "torch", ".", "LongTensor", "(", "[", "w1", ",", "w1", ",", "eos", "]", ")", "}", ",", "\n", "]", "\n", "self", ".", "sample", "=", "next", "(", "test_utils", ".", "dummy_dataloader", "(", "self", ".", "data", ")", ")", "\n", "\n", "# build model", "\n", "self", ".", "args", "=", "argparse", ".", "Namespace", "(", ")", "\n", "self", ".", "args", ".", "sentence_avg", "=", "False", "\n", "self", ".", "args", ".", "probs", "=", "torch", ".", "FloatTensor", "(", "[", "\n", "#      pad   eos  unk   w1   w2   w3", "\n", "[", "0.05", ",", "0.05", ",", "0.1", ",", "0.05", ",", "0.3", ",", "0.4", ",", "0.05", "]", ",", "\n", "[", "0.05", ",", "0.10", ",", "0.2", ",", "0.05", ",", "0.2", ",", "0.3", ",", "0.10", "]", ",", "\n", "[", "0.05", ",", "0.15", ",", "0.3", ",", "0.05", ",", "0.1", ",", "0.2", ",", "0.15", "]", ",", "\n", "]", ")", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "2", ",", "3", ",", "7", ")", "# add batch dimension", "\n", "self", ".", "task", "=", "test_utils", ".", "TestTranslationTask", ".", "setup_task", "(", "self", ".", "args", ",", "self", ".", "d", ",", "self", ".", "d", ")", "\n", "self", ".", "model", "=", "self", ".", "task", ".", "build_model", "(", "self", ".", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_label_smoothing.TestLabelSmoothing.test_nll_loss": [[50, 58], ["fairseq.criterions.cross_entropy.CrossEntropyCriterion", "fairseq.criterions.label_smoothed_cross_entropy.LabelSmoothedCrossEntropyCriterion", "fairseq.criterions.cross_entropy.CrossEntropyCriterion.", "fairseq.criterions.label_smoothed_cross_entropy.LabelSmoothedCrossEntropyCriterion.", "test_label_smoothing.TestLabelSmoothing.assertLess", "test_label_smoothing.TestLabelSmoothing.assertLess", "abs", "abs"], "methods", ["None"], ["", "def", "test_nll_loss", "(", "self", ")", ":", "\n", "        ", "self", ".", "args", ".", "label_smoothing", "=", "0.1", "\n", "nll_crit", "=", "CrossEntropyCriterion", "(", "self", ".", "args", ",", "self", ".", "task", ")", "\n", "smooth_crit", "=", "LabelSmoothedCrossEntropyCriterion", "(", "self", ".", "args", ",", "self", ".", "task", ")", "\n", "nll_loss", ",", "nll_sample_size", ",", "nll_logging_output", "=", "nll_crit", "(", "self", ".", "model", ",", "self", ".", "sample", ")", "\n", "smooth_loss", ",", "smooth_sample_size", ",", "smooth_logging_output", "=", "smooth_crit", "(", "self", ".", "model", ",", "self", ".", "sample", ")", "\n", "self", ".", "assertLess", "(", "abs", "(", "nll_loss", "-", "nll_logging_output", "[", "'loss'", "]", ")", ",", "1e-6", ")", "\n", "self", ".", "assertLess", "(", "abs", "(", "nll_loss", "-", "smooth_logging_output", "[", "'nll_loss'", "]", ")", ",", "1e-6", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_label_smoothing.TestLabelSmoothing.test_padding": [[59, 77], ["fairseq.criterions.label_smoothed_cross_entropy.LabelSmoothedCrossEntropyCriterion", "fairseq.criterions.label_smoothed_cross_entropy.LabelSmoothedCrossEntropyCriterion.", "test_label_smoothing.TestLabelSmoothing.test_padding.get_one_no_padding"], "methods", ["None"], ["", "def", "test_padding", "(", "self", ")", ":", "\n", "        ", "self", ".", "args", ".", "label_smoothing", "=", "0.1", "\n", "crit", "=", "LabelSmoothedCrossEntropyCriterion", "(", "self", ".", "args", ",", "self", ".", "task", ")", "\n", "loss", ",", "_", ",", "logging_output", "=", "crit", "(", "self", ".", "model", ",", "self", ".", "sample", ")", "\n", "\n", "def", "get_one_no_padding", "(", "idx", ")", ":", "\n", "# create a new sample with just a single batch item so that there's", "\n", "# no padding", "\n", "            ", "sample1", "=", "next", "(", "test_utils", ".", "dummy_dataloader", "(", "[", "self", ".", "data", "[", "idx", "]", "]", ")", ")", "\n", "args1", "=", "copy", ".", "copy", "(", "self", ".", "args", ")", "\n", "args1", ".", "probs", "=", "args1", ".", "probs", "[", "idx", ",", ":", ",", ":", "]", ".", "unsqueeze", "(", "0", ")", "\n", "model1", "=", "self", ".", "task", ".", "build_model", "(", "args1", ")", "\n", "loss1", ",", "_", ",", "_", "=", "crit", "(", "model1", ",", "sample1", ")", "\n", "return", "loss1", "\n", "\n", "", "loss1", "=", "get_one_no_padding", "(", "0", ")", "\n", "loss2", "=", "get_one_no_padding", "(", "1", ")", "\n", "self", ".", "assertAlmostEqual", "(", "loss", ",", "loss1", "+", "loss2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_label_smoothing.TestLabelSmoothing.test_reduction": [[78, 84], ["fairseq.criterions.label_smoothed_cross_entropy.LabelSmoothedCrossEntropyCriterion", "fairseq.criterions.label_smoothed_cross_entropy.LabelSmoothedCrossEntropyCriterion.", "fairseq.criterions.label_smoothed_cross_entropy.LabelSmoothedCrossEntropyCriterion.", "test_label_smoothing.TestLabelSmoothing.assertAlmostEqual", "unreduced_loss.sum"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertAlmostEqual"], ["", "def", "test_reduction", "(", "self", ")", ":", "\n", "        ", "self", ".", "args", ".", "label_smoothing", "=", "0.1", "\n", "crit", "=", "LabelSmoothedCrossEntropyCriterion", "(", "self", ".", "args", ",", "self", ".", "task", ")", "\n", "loss", ",", "_", ",", "logging_output", "=", "crit", "(", "self", ".", "model", ",", "self", ".", "sample", ",", "reduce", "=", "True", ")", "\n", "unreduced_loss", ",", "_", ",", "_", "=", "crit", "(", "self", ".", "model", ",", "self", ".", "sample", ",", "reduce", "=", "False", ")", "\n", "self", ".", "assertAlmostEqual", "(", "loss", ",", "unreduced_loss", ".", "sum", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_label_smoothing.TestLabelSmoothing.test_zero_eps": [[85, 92], ["fairseq.criterions.cross_entropy.CrossEntropyCriterion", "fairseq.criterions.label_smoothed_cross_entropy.LabelSmoothedCrossEntropyCriterion", "fairseq.criterions.cross_entropy.CrossEntropyCriterion.", "fairseq.criterions.label_smoothed_cross_entropy.LabelSmoothedCrossEntropyCriterion.", "test_label_smoothing.TestLabelSmoothing.assertAlmostEqual"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertAlmostEqual"], ["", "def", "test_zero_eps", "(", "self", ")", ":", "\n", "        ", "self", ".", "args", ".", "label_smoothing", "=", "0.0", "\n", "nll_crit", "=", "CrossEntropyCriterion", "(", "self", ".", "args", ",", "self", ".", "task", ")", "\n", "smooth_crit", "=", "LabelSmoothedCrossEntropyCriterion", "(", "self", ".", "args", ",", "self", ".", "task", ")", "\n", "nll_loss", ",", "nll_sample_size", ",", "nll_logging_output", "=", "nll_crit", "(", "self", ".", "model", ",", "self", ".", "sample", ")", "\n", "smooth_loss", ",", "smooth_sample_size", ",", "smooth_logging_output", "=", "smooth_crit", "(", "self", ".", "model", ",", "self", ".", "sample", ")", "\n", "self", ".", "assertAlmostEqual", "(", "nll_loss", ",", "smooth_loss", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_label_smoothing.TestLabelSmoothing.assertAlmostEqual": [[93, 96], ["test_label_smoothing.TestLabelSmoothing.assertEqual", "test_label_smoothing.TestLabelSmoothing.assertLess", "t1.size", "t2.size"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "assertAlmostEqual", "(", "self", ",", "t1", ",", "t2", ")", ":", "\n", "        ", "self", ".", "assertEqual", "(", "t1", ".", "size", "(", ")", ",", "t2", ".", "size", "(", ")", ",", "\"size mismatch\"", ")", "\n", "self", ".", "assertLess", "(", "(", "t1", "-", "t2", ")", ".", "abs", "(", ")", ".", "max", "(", ")", ",", "1e-6", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_multi_corpus_sampled_dataset.TestMultiCorpusSampledDataset.setUp": [[17, 42], ["tests.test_train.mock_dict", "torch.LongTensor().view", "fairseq.data.TokenBlockDataset", "fairseq.data.LanguagePairDataset", "torch.LongTensor().view", "fairseq.data.TokenBlockDataset", "fairseq.data.LanguagePairDataset", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor().view.size", "torch.LongTensor().view.size"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_train.mock_dict", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "d", "=", "mock_dict", "(", ")", "\n", "tokens_1", "=", "torch", ".", "LongTensor", "(", "[", "1", "]", ")", ".", "view", "(", "1", ",", "-", "1", ")", "\n", "tokens_ds1", "=", "TokenBlockDataset", "(", "\n", "tokens_1", ",", "\n", "sizes", "=", "[", "tokens_1", ".", "size", "(", "-", "1", ")", "]", ",", "\n", "block_size", "=", "1", ",", "\n", "pad", "=", "0", ",", "\n", "eos", "=", "1", ",", "\n", "include_targets", "=", "False", ",", "\n", ")", "\n", "self", ".", "dataset_1", "=", "LanguagePairDataset", "(", "\n", "tokens_ds1", ",", "tokens_ds1", ".", "sizes", ",", "d", ",", "shuffle", "=", "False", "\n", ")", "\n", "tokens_2", "=", "torch", ".", "LongTensor", "(", "[", "2", "]", ")", ".", "view", "(", "1", ",", "-", "1", ")", "\n", "tokens_ds2", "=", "TokenBlockDataset", "(", "\n", "tokens_2", ",", "\n", "sizes", "=", "[", "tokens_2", ".", "size", "(", "-", "1", ")", "]", ",", "\n", "block_size", "=", "1", ",", "\n", "pad", "=", "0", ",", "\n", "eos", "=", "1", ",", "\n", "include_targets", "=", "False", ",", "\n", ")", "\n", "self", ".", "dataset_2", "=", "LanguagePairDataset", "(", "\n", "tokens_ds2", ",", "tokens_ds2", ".", "sizes", ",", "d", ",", "shuffle", "=", "False", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_multi_corpus_sampled_dataset.TestMultiCorpusSampledDataset._test_sample_helper": [[44, 75], ["numpy.random.seed", "fairseq.data.multi_corpus_sampled_dataset.MultiCorpusSampledDataset.ordered_indices", "range", "test_multi_corpus_sampled_dataset.TestMultiCorpusSampledDataset.assertLess", "fairseq.data.multi_corpus_sampled_dataset.MultiCorpusSampledDataset", "fairseq.data.multi_corpus_sampled_dataset.MultiCorpusSampledDataset", "abs", "collections.OrderedDict", "collections.OrderedDict", "fairseq.data.multi_corpus_sampled_dataset.MultiCorpusSampledDataset.collater"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.ordered_indices", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.collater"], ["", "def", "_test_sample_helper", "(", "\n", "self", ",", "\n", "expected_sample_from_first_ds_percentage", ",", "\n", "num_samples", "=", "1000", ",", "\n", "sampling_func", "=", "None", ",", "\n", ")", ":", "\n", "# To make sure test is not flaky", "\n", "        ", "np", ".", "random", ".", "seed", "(", "0", ")", "\n", "if", "sampling_func", "is", "None", ":", "\n", "            ", "m", "=", "MultiCorpusSampledDataset", "(", "\n", "OrderedDict", "(", "{", "0", ":", "self", ".", "dataset_1", ",", "1", ":", "self", ".", "dataset_2", "}", ")", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "m", "=", "MultiCorpusSampledDataset", "(", "\n", "OrderedDict", "(", "{", "0", ":", "self", ".", "dataset_1", ",", "1", ":", "self", ".", "dataset_2", "}", ")", ",", "\n", "sampling_func", "=", "sampling_func", ",", "\n", ")", "\n", "", "m", ".", "ordered_indices", "(", ")", "\n", "count_sample_from_first_dataset", "=", "0", "\n", "for", "_", "in", "range", "(", "num_samples", ")", ":", "\n", "            ", "if", "m", ".", "collater", "(", "[", "m", "[", "0", "]", ",", "m", "[", "1", "]", "]", ")", "[", "\"net_input\"", "]", "[", "\"src_tokens\"", "]", "[", "0", "]", "==", "1", ":", "\n", "                ", "count_sample_from_first_dataset", "+=", "1", "\n", "", "", "sample_from_first_ds_percentage", "=", "(", "\n", "1.0", "*", "count_sample_from_first_dataset", "/", "num_samples", "\n", ")", "\n", "self", ".", "assertLess", "(", "\n", "abs", "(", "\n", "sample_from_first_ds_percentage", "\n", "-", "expected_sample_from_first_ds_percentage", "\n", ")", ",", "\n", "0.01", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_multi_corpus_sampled_dataset.TestMultiCorpusSampledDataset.test_multi_corpus_sampled_dataset_uniform_sample": [[77, 79], ["test_multi_corpus_sampled_dataset.TestMultiCorpusSampledDataset._test_sample_helper"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_multi_corpus_sampled_dataset.TestMultiCorpusSampledDataset._test_sample_helper"], ["", "def", "test_multi_corpus_sampled_dataset_uniform_sample", "(", "self", ")", ":", "\n", "        ", "self", ".", "_test_sample_helper", "(", "expected_sample_from_first_ds_percentage", "=", "0.5", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_multi_corpus_sampled_dataset.TestMultiCorpusSampledDataset.test_multi_corpus_sampled_dataset_weighted_sample": [[80, 95], ["test_multi_corpus_sampled_dataset.TestMultiCorpusSampledDataset._test_sample_helper", "numpy.random.random", "enumerate", "test_multi_corpus_sampled_dataset.TestMultiCorpusSampledDataset.test_multi_corpus_sampled_dataset_weighted_sample.naive_weighted_sample"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_multi_corpus_sampled_dataset.TestMultiCorpusSampledDataset._test_sample_helper"], ["", "def", "test_multi_corpus_sampled_dataset_weighted_sample", "(", "self", ")", ":", "\n", "        ", "def", "naive_weighted_sample", "(", "weights", ")", ":", "\n", "            ", "def", "f", "(", "l", ")", ":", "\n", "                ", "v", "=", "np", ".", "random", ".", "random", "(", ")", "\n", "agg", "=", "0", "\n", "for", "i", ",", "weight", "in", "enumerate", "(", "weights", ")", ":", "\n", "                    ", "agg", "+=", "weight", "\n", "if", "agg", ">", "v", ":", "\n", "                        ", "return", "i", "\n", "\n", "", "", "", "return", "f", "\n", "\n", "", "self", ".", "_test_sample_helper", "(", "\n", "expected_sample_from_first_ds_percentage", "=", "0.9", ",", "\n", "sampling_func", "=", "naive_weighted_sample", "(", "weights", "=", "[", "0.9", ",", "0.1", "]", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_iterators.TestIterators.test_counting_iterator": [[13, 24], ["list", "fairseq.data.iterators.CountingIterator", "test_iterators.TestIterators.assertTrue", "test_iterators.TestIterators.assertEqual", "test_iterators.TestIterators.assertEqual", "fairseq.data.iterators.CountingIterator.skip", "test_iterators.TestIterators.assertEqual", "fairseq.data.iterators.CountingIterator.skip", "test_iterators.TestIterators.assertEqual", "test_iterators.TestIterators.assertFalse", "range", "fairseq.data.iterators.CountingIterator.has_next", "next", "next", "next", "next", "fairseq.data.iterators.CountingIterator.has_next"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.CountingIterator.skip", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.CountingIterator.skip", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.CountingIterator.has_next", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.CountingIterator.has_next"], ["    ", "def", "test_counting_iterator", "(", "self", ")", ":", "\n", "        ", "x", "=", "list", "(", "range", "(", "10", ")", ")", "\n", "itr", "=", "iterators", ".", "CountingIterator", "(", "x", ")", "\n", "self", ".", "assertTrue", "(", "itr", ".", "has_next", "(", ")", ")", "\n", "self", ".", "assertEqual", "(", "next", "(", "itr", ")", ",", "0", ")", "\n", "self", ".", "assertEqual", "(", "next", "(", "itr", ")", ",", "1", ")", "\n", "itr", ".", "skip", "(", "3", ")", "\n", "self", ".", "assertEqual", "(", "next", "(", "itr", ")", ",", "5", ")", "\n", "itr", ".", "skip", "(", "3", ")", "\n", "self", ".", "assertEqual", "(", "next", "(", "itr", ")", ",", "9", ")", "\n", "self", ".", "assertFalse", "(", "itr", ".", "has_next", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_dictionary.TestDictionary.test_finalize": [[16, 67], ["list", "list", "fairseq.data.Dictionary", "test_dictionary.TestDictionary.test_finalize.get_ids"], "methods", ["None"], ["    ", "def", "test_finalize", "(", "self", ")", ":", "\n", "        ", "txt", "=", "[", "\n", "'A B C D'", ",", "\n", "'B C D'", ",", "\n", "'C D'", ",", "\n", "'D'", ",", "\n", "]", "\n", "ref_ids1", "=", "list", "(", "map", "(", "torch", ".", "IntTensor", ",", "[", "\n", "[", "4", ",", "5", ",", "6", ",", "7", ",", "2", "]", ",", "\n", "[", "5", ",", "6", ",", "7", ",", "2", "]", ",", "\n", "[", "6", ",", "7", ",", "2", "]", ",", "\n", "[", "7", ",", "2", "]", ",", "\n", "]", ")", ")", "\n", "ref_ids2", "=", "list", "(", "map", "(", "torch", ".", "IntTensor", ",", "[", "\n", "[", "7", ",", "6", ",", "5", ",", "4", ",", "2", "]", ",", "\n", "[", "6", ",", "5", ",", "4", ",", "2", "]", ",", "\n", "[", "5", ",", "4", ",", "2", "]", ",", "\n", "[", "4", ",", "2", "]", ",", "\n", "]", ")", ")", "\n", "\n", "# build dictionary", "\n", "d", "=", "Dictionary", "(", ")", "\n", "for", "line", "in", "txt", ":", "\n", "            ", "d", ".", "encode_line", "(", "line", ",", "add_if_not_exist", "=", "True", ")", "\n", "\n", "", "def", "get_ids", "(", "dictionary", ")", ":", "\n", "            ", "ids", "=", "[", "]", "\n", "for", "line", "in", "txt", ":", "\n", "                ", "ids", ".", "append", "(", "dictionary", ".", "encode_line", "(", "line", ",", "add_if_not_exist", "=", "False", ")", ")", "\n", "", "return", "ids", "\n", "\n", "", "def", "assertMatch", "(", "ids", ",", "ref_ids", ")", ":", "\n", "            ", "for", "toks", ",", "ref_toks", "in", "zip", "(", "ids", ",", "ref_ids", ")", ":", "\n", "                ", "self", ".", "assertEqual", "(", "toks", ".", "size", "(", ")", ",", "ref_toks", ".", "size", "(", ")", ")", "\n", "self", ".", "assertEqual", "(", "0", ",", "(", "toks", "!=", "ref_toks", ")", ".", "sum", "(", ")", ".", "item", "(", ")", ")", "\n", "\n", "", "", "ids", "=", "get_ids", "(", "d", ")", "\n", "assertMatch", "(", "ids", ",", "ref_ids1", ")", "\n", "\n", "# check finalized dictionary", "\n", "d", ".", "finalize", "(", ")", "\n", "finalized_ids", "=", "get_ids", "(", "d", ")", "\n", "assertMatch", "(", "finalized_ids", ",", "ref_ids2", ")", "\n", "\n", "# write to disk and reload", "\n", "with", "tempfile", ".", "NamedTemporaryFile", "(", "mode", "=", "'w'", ")", "as", "tmp_dict", ":", "\n", "            ", "d", ".", "save", "(", "tmp_dict", ".", "name", ")", "\n", "d", "=", "Dictionary", ".", "load", "(", "tmp_dict", ".", "name", ")", "\n", "reload_ids", "=", "get_ids", "(", "d", ")", "\n", "assertMatch", "(", "reload_ids", ",", "ref_ids2", ")", "\n", "assertMatch", "(", "finalized_ids", ",", "reload_ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising._get_test_data_with_bpe_cont_marker": [[22, 56], ["fairseq.data.Dictionary", "fairseq.data.Dictionary.add_symbol", "fairseq.data.Dictionary.add_symbol", "fairseq.data.Dictionary.add_symbol", "fairseq.data.Dictionary.add_symbol", "fairseq.data.Dictionary.add_symbol", "fairseq.data.Dictionary.add_symbol", "fairseq.data.Dictionary.add_symbol", "fairseq.data.Dictionary.add_symbol", "fairseq.data.Dictionary.add_symbol", "fairseq.data.Dictionary.add_symbol", "test_noising.TestDataNoising._convert_src_tokens_to_tensor"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising._convert_src_tokens_to_tensor"], ["    ", "def", "_get_test_data_with_bpe_cont_marker", "(", "self", ",", "append_eos", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            append_eos: if True, each input sentence in the source tokens tensor\n                will have an EOS appended to the end.\n\n        Returns:\n            vocabs: BPE vocab with continuation markers as suffixes to denote\n                non-end of word tokens. This is the standard BPE format used in\n                fairseq's preprocessing.\n            x: input tensor containing numberized source tokens, with EOS at the\n                end if append_eos is true\n            src_lengths: and source lengths.\n        \"\"\"", "\n", "vocab", "=", "Dictionary", "(", ")", "\n", "vocab", ".", "add_symbol", "(", "\"he@@\"", ")", "\n", "vocab", ".", "add_symbol", "(", "\"llo\"", ")", "\n", "vocab", ".", "add_symbol", "(", "\"how\"", ")", "\n", "vocab", ".", "add_symbol", "(", "\"are\"", ")", "\n", "vocab", ".", "add_symbol", "(", "\"y@@\"", ")", "\n", "vocab", ".", "add_symbol", "(", "\"ou\"", ")", "\n", "vocab", ".", "add_symbol", "(", "\"n@@\"", ")", "\n", "vocab", ".", "add_symbol", "(", "\"ew\"", ")", "\n", "vocab", ".", "add_symbol", "(", "\"or@@\"", ")", "\n", "vocab", ".", "add_symbol", "(", "\"k\"", ")", "\n", "\n", "src_tokens", "=", "[", "\n", "[", "\"he@@\"", ",", "\"llo\"", ",", "\"n@@\"", ",", "\"ew\"", ",", "\"y@@\"", ",", "\"or@@\"", ",", "\"k\"", "]", ",", "\n", "[", "\"how\"", ",", "\"are\"", ",", "\"y@@\"", ",", "\"ou\"", "]", ",", "\n", "]", "\n", "x", ",", "src_lengths", "=", "x", ",", "src_lengths", "=", "self", ".", "_convert_src_tokens_to_tensor", "(", "\n", "vocab", "=", "vocab", ",", "src_tokens", "=", "src_tokens", ",", "append_eos", "=", "append_eos", "\n", ")", "\n", "return", "vocab", ",", "x", ",", "src_lengths", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising._get_test_data_with_bpe_end_marker": [[57, 92], ["fairseq.data.Dictionary", "fairseq.data.Dictionary.add_symbol", "fairseq.data.Dictionary.add_symbol", "fairseq.data.Dictionary.add_symbol", "fairseq.data.Dictionary.add_symbol", "fairseq.data.Dictionary.add_symbol", "fairseq.data.Dictionary.add_symbol", "fairseq.data.Dictionary.add_symbol", "fairseq.data.Dictionary.add_symbol", "fairseq.data.Dictionary.add_symbol", "fairseq.data.Dictionary.add_symbol", "test_noising.TestDataNoising._convert_src_tokens_to_tensor"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising._convert_src_tokens_to_tensor"], ["", "def", "_get_test_data_with_bpe_end_marker", "(", "self", ",", "append_eos", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            append_eos: if True, each input sentence in the source tokens tensor\n                will have an EOS appended to the end.\n\n        Returns:\n            vocabs: BPE vocab with end-of-word markers as suffixes to denote\n                tokens at the end of a word. This is an alternative to fairseq's\n                standard preprocessing framework and is not generally supported\n                within fairseq.\n            x: input tensor containing numberized source tokens, with EOS at the\n                end if append_eos is true\n            src_lengths: and source lengths.\n        \"\"\"", "\n", "vocab", "=", "Dictionary", "(", ")", "\n", "vocab", ".", "add_symbol", "(", "\"he\"", ")", "\n", "vocab", ".", "add_symbol", "(", "\"llo_EOW\"", ")", "\n", "vocab", ".", "add_symbol", "(", "\"how_EOW\"", ")", "\n", "vocab", ".", "add_symbol", "(", "\"are_EOW\"", ")", "\n", "vocab", ".", "add_symbol", "(", "\"y\"", ")", "\n", "vocab", ".", "add_symbol", "(", "\"ou_EOW\"", ")", "\n", "vocab", ".", "add_symbol", "(", "\"n\"", ")", "\n", "vocab", ".", "add_symbol", "(", "\"ew_EOW\"", ")", "\n", "vocab", ".", "add_symbol", "(", "\"or\"", ")", "\n", "vocab", ".", "add_symbol", "(", "\"k_EOW\"", ")", "\n", "\n", "src_tokens", "=", "[", "\n", "[", "\"he\"", ",", "\"llo_EOW\"", ",", "\"n\"", ",", "\"ew_EOW\"", ",", "\"y\"", ",", "\"or\"", ",", "\"k_EOW\"", "]", ",", "\n", "[", "\"how_EOW\"", ",", "\"are_EOW\"", ",", "\"y\"", ",", "\"ou_EOW\"", "]", ",", "\n", "]", "\n", "x", ",", "src_lengths", "=", "x", ",", "src_lengths", "=", "self", ".", "_convert_src_tokens_to_tensor", "(", "\n", "vocab", "=", "vocab", ",", "src_tokens", "=", "src_tokens", ",", "append_eos", "=", "append_eos", "\n", ")", "\n", "return", "vocab", ",", "x", ",", "src_lengths", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising._get_test_data_with_word_vocab": [[93, 121], ["fairseq.data.Dictionary", "fairseq.data.Dictionary.add_symbol", "fairseq.data.Dictionary.add_symbol", "fairseq.data.Dictionary.add_symbol", "fairseq.data.Dictionary.add_symbol", "fairseq.data.Dictionary.add_symbol", "fairseq.data.Dictionary.add_symbol", "test_noising.TestDataNoising._convert_src_tokens_to_tensor"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising._convert_src_tokens_to_tensor"], ["", "def", "_get_test_data_with_word_vocab", "(", "self", ",", "append_eos", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            append_eos: if True, each input sentence in the source tokens tensor\n                will have an EOS appended to the end.\n\n        Returns:\n            vocabs: word vocab\n            x: input tensor containing numberized source tokens, with EOS at the\n                end if append_eos is true\n            src_lengths: and source lengths.\n        \"\"\"", "\n", "vocab", "=", "Dictionary", "(", ")", "\n", "\n", "vocab", ".", "add_symbol", "(", "\"hello\"", ")", "\n", "vocab", ".", "add_symbol", "(", "\"how\"", ")", "\n", "vocab", ".", "add_symbol", "(", "\"are\"", ")", "\n", "vocab", ".", "add_symbol", "(", "\"you\"", ")", "\n", "vocab", ".", "add_symbol", "(", "\"new\"", ")", "\n", "vocab", ".", "add_symbol", "(", "\"york\"", ")", "\n", "src_tokens", "=", "[", "\n", "[", "\"hello\"", ",", "\"new\"", ",", "\"york\"", ",", "\"you\"", "]", ",", "\n", "[", "\"how\"", ",", "\"are\"", ",", "\"you\"", ",", "\"new\"", ",", "\"york\"", "]", ",", "\n", "]", "\n", "x", ",", "src_lengths", "=", "self", ".", "_convert_src_tokens_to_tensor", "(", "\n", "vocab", "=", "vocab", ",", "src_tokens", "=", "src_tokens", ",", "append_eos", "=", "append_eos", "\n", ")", "\n", "return", "vocab", ",", "x", ",", "src_lengths", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising._convert_src_tokens_to_tensor": [[122, 139], ["torch.LongTensor().fill_", "range", "x.transpose.transpose.transpose", "len", "vocab.pad", "len", "range", "torch.LongTensor", "torch.LongTensor", "len", "vocab.index", "vocab.eos", "len", "max"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.index", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos"], ["", "def", "_convert_src_tokens_to_tensor", "(", "\n", "self", ",", "vocab", ":", "Dictionary", ",", "src_tokens", ":", "List", "[", "List", "[", "str", "]", "]", ",", "append_eos", ":", "bool", "\n", ")", ":", "\n", "        ", "src_len", "=", "[", "len", "(", "x", ")", "for", "x", "in", "src_tokens", "]", "\n", "# If we have to append EOS, we include EOS in counting src length", "\n", "if", "append_eos", ":", "\n", "            ", "src_len", "=", "[", "length", "+", "1", "for", "length", "in", "src_len", "]", "\n", "\n", "", "x", "=", "torch", ".", "LongTensor", "(", "len", "(", "src_tokens", ")", ",", "max", "(", "src_len", ")", ")", ".", "fill_", "(", "vocab", ".", "pad", "(", ")", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "src_tokens", ")", ")", ":", "\n", "            ", "for", "j", "in", "range", "(", "len", "(", "src_tokens", "[", "i", "]", ")", ")", ":", "\n", "                ", "x", "[", "i", "]", "[", "j", "]", "=", "vocab", ".", "index", "(", "src_tokens", "[", "i", "]", "[", "j", "]", ")", "\n", "", "if", "append_eos", ":", "\n", "                ", "x", "[", "i", "]", "[", "j", "+", "1", "]", "=", "vocab", ".", "eos", "(", ")", "\n", "\n", "", "", "x", "=", "x", ".", "transpose", "(", "1", ",", "0", ")", "\n", "return", "x", ",", "torch", ".", "LongTensor", "(", "src_len", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising.assert_eos_at_end": [[140, 150], ["range", "len", "test_noising.TestDataNoising.assertEqual"], "methods", ["None"], ["", "def", "assert_eos_at_end", "(", "self", ",", "x", ",", "x_len", ",", "eos", ")", ":", "\n", "        ", "\"\"\"Asserts last token of every sentence in x is EOS \"\"\"", "\n", "for", "i", "in", "range", "(", "len", "(", "x_len", ")", ")", ":", "\n", "            ", "self", ".", "assertEqual", "(", "\n", "x", "[", "x_len", "[", "i", "]", "-", "1", "]", "[", "i", "]", ",", "\n", "eos", ",", "\n", "(", "\n", "\"Expected eos (token id {eos}) at the end of sentence {i} \"", "\n", "\"but got {other} instead\"", "\n", ")", ".", "format", "(", "i", "=", "i", ",", "eos", "=", "eos", ",", "other", "=", "x", "[", "i", "]", "[", "-", "1", "]", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising.assert_word_dropout_correct": [[152, 158], ["test_noising.TestDataNoising.assertEqual", "range", "test_noising.TestDataNoising.assertEqual"], "methods", ["None"], ["", "", "def", "assert_word_dropout_correct", "(", "self", ",", "x", ",", "x_noised", ",", "x_len", ",", "l_noised", ")", ":", "\n", "# Expect only the first word (2 bpe tokens) of the first example", "\n", "# was dropped out", "\n", "        ", "self", ".", "assertEqual", "(", "x_len", "[", "0", "]", "-", "2", ",", "l_noised", "[", "0", "]", ")", "\n", "for", "i", "in", "range", "(", "l_noised", "[", "0", "]", ")", ":", "\n", "            ", "self", ".", "assertEqual", "(", "x_noised", "[", "i", "]", "[", "0", "]", ",", "x", "[", "i", "+", "2", "]", "[", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising.test_word_dropout_with_eos": [[159, 169], ["test_noising.TestDataNoising._get_test_data_with_bpe_cont_marker", "fairseq.data.data_utils.numpy_seed", "fairseq.data.noising.WordDropout", "fairseq.data.noising.WordDropout.noising", "test_noising.TestDataNoising.assert_word_dropout_correct", "test_noising.TestDataNoising.assert_eos_at_end", "vocab.eos"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising._get_test_data_with_bpe_cont_marker", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.data_utils.numpy_seed", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.noising.UnsupervisedMTNoising.noising", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising.assert_word_dropout_correct", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising.assert_eos_at_end", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos"], ["", "", "def", "test_word_dropout_with_eos", "(", "self", ")", ":", "\n", "        ", "vocab", ",", "x", ",", "x_len", "=", "self", ".", "_get_test_data_with_bpe_cont_marker", "(", "append_eos", "=", "True", ")", "\n", "\n", "with", "data_utils", ".", "numpy_seed", "(", "1234", ")", ":", "\n", "            ", "noising_gen", "=", "noising", ".", "WordDropout", "(", "vocab", ")", "\n", "x_noised", ",", "l_noised", "=", "noising_gen", ".", "noising", "(", "x", ",", "x_len", ",", "0.2", ")", "\n", "self", ".", "assert_word_dropout_correct", "(", "\n", "x", "=", "x", ",", "x_noised", "=", "x_noised", ",", "x_len", "=", "x_len", ",", "l_noised", "=", "l_noised", "\n", ")", "\n", "self", ".", "assert_eos_at_end", "(", "x", "=", "x_noised", ",", "x_len", "=", "l_noised", ",", "eos", "=", "vocab", ".", "eos", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising.assert_word_blanking_correct": [[170, 179], ["test_noising.TestDataNoising.assertEqual", "range", "test_noising.TestDataNoising.assertEqual", "test_noising.TestDataNoising.assertEqual"], "methods", ["None"], ["", "", "def", "assert_word_blanking_correct", "(", "self", ",", "x", ",", "x_noised", ",", "x_len", ",", "l_noised", ",", "unk", ")", ":", "\n", "# Expect only the first word (2 bpe tokens) of the first example", "\n", "# was blanked out", "\n", "        ", "self", ".", "assertEqual", "(", "x_len", "[", "0", "]", ",", "l_noised", "[", "0", "]", ")", "\n", "for", "i", "in", "range", "(", "l_noised", "[", "0", "]", ")", ":", "\n", "            ", "if", "i", "<", "2", ":", "\n", "                ", "self", ".", "assertEqual", "(", "x_noised", "[", "i", "]", "[", "0", "]", ",", "unk", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "assertEqual", "(", "x_noised", "[", "i", "]", "[", "0", "]", ",", "x", "[", "i", "]", "[", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising.test_word_blank_with_eos": [[180, 190], ["test_noising.TestDataNoising._get_test_data_with_bpe_cont_marker", "fairseq.data.data_utils.numpy_seed", "fairseq.data.noising.WordDropout", "fairseq.data.noising.WordDropout.noising", "test_noising.TestDataNoising.assert_word_blanking_correct", "test_noising.TestDataNoising.assert_eos_at_end", "vocab.unk", "vocab.unk", "vocab.eos"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising._get_test_data_with_bpe_cont_marker", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.data_utils.numpy_seed", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.noising.UnsupervisedMTNoising.noising", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising.assert_word_blanking_correct", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising.assert_eos_at_end", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.unk", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.unk", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos"], ["", "", "", "def", "test_word_blank_with_eos", "(", "self", ")", ":", "\n", "        ", "vocab", ",", "x", ",", "x_len", "=", "self", ".", "_get_test_data_with_bpe_cont_marker", "(", "append_eos", "=", "True", ")", "\n", "\n", "with", "data_utils", ".", "numpy_seed", "(", "1234", ")", ":", "\n", "            ", "noising_gen", "=", "noising", ".", "WordDropout", "(", "vocab", ")", "\n", "x_noised", ",", "l_noised", "=", "noising_gen", ".", "noising", "(", "x", ",", "x_len", ",", "0.2", ",", "vocab", ".", "unk", "(", ")", ")", "\n", "self", ".", "assert_word_blanking_correct", "(", "\n", "x", "=", "x", ",", "x_noised", "=", "x_noised", ",", "x_len", "=", "x_len", ",", "l_noised", "=", "l_noised", ",", "unk", "=", "vocab", ".", "unk", "(", ")", "\n", ")", "\n", "self", ".", "assert_eos_at_end", "(", "x", "=", "x_noised", ",", "x_len", "=", "l_noised", ",", "eos", "=", "vocab", ".", "eos", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising.generate_unchanged_shuffle_map": [[191, 193], ["range"], "methods", ["None"], ["", "", "def", "generate_unchanged_shuffle_map", "(", "self", ",", "length", ")", ":", "\n", "        ", "return", "{", "i", ":", "i", "for", "i", "in", "range", "(", "length", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising.assert_word_shuffle_matches_expected": [[194, 245], ["range", "zip", "fairseq.data.data_utils.numpy_seed", "fairseq.data.noising.WordShuffle", "fairseq.data.noising.WordShuffle.noising", "len", "shuffle_map.items", "test_noising.TestDataNoising.assertEqual", "test_noising.TestDataNoising.assert_eos_at_end", "test_noising.TestDataNoising.assertEqual", "vocab.eos"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.data_utils.numpy_seed", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.noising.UnsupervisedMTNoising.noising", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising.assert_eos_at_end", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos"], ["", "def", "assert_word_shuffle_matches_expected", "(", "\n", "self", ",", "\n", "x", ",", "\n", "x_len", ",", "\n", "max_shuffle_distance", ":", "int", ",", "\n", "vocab", ":", "Dictionary", ",", "\n", "expected_shufle_maps", ":", "List", "[", "Dict", "[", "int", ",", "int", "]", "]", ",", "\n", "expect_eos_at_end", ":", "bool", ",", "\n", "bpe_end_marker", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        This verifies that with a given x, x_len, max_shuffle_distance, and\n        vocab, we get the expected shuffle result.\n\n        Args:\n            x: Tensor of shape (T x B) = (sequence_length, batch_size)\n            x_len: Tensor of length B = batch_size\n            max_shuffle_distance: arg to pass to noising\n            expected_shuffle_maps: List[mapping] where mapping is a\n                Dict[old_index, new_index], mapping x's elements from their\n                old positions in x to their new positions in x.\n            expect_eos_at_end: if True, check the output to make sure there is\n                an EOS at the end.\n            bpe_end_marker: str denoting the BPE end token. If this is not None, we\n                set the BPE cont token to None in the noising classes.\n        \"\"\"", "\n", "bpe_cont_marker", "=", "None", "\n", "if", "bpe_end_marker", "is", "None", ":", "\n", "            ", "bpe_cont_marker", "=", "\"@@\"", "\n", "\n", "", "with", "data_utils", ".", "numpy_seed", "(", "1234", ")", ":", "\n", "            ", "word_shuffle", "=", "noising", ".", "WordShuffle", "(", "\n", "vocab", ",", "bpe_cont_marker", "=", "bpe_cont_marker", ",", "bpe_end_marker", "=", "bpe_end_marker", "\n", ")", "\n", "x_noised", ",", "l_noised", "=", "word_shuffle", ".", "noising", "(", "\n", "x", ",", "x_len", ",", "max_shuffle_distance", "=", "max_shuffle_distance", "\n", ")", "\n", "\n", "# For every example, we have a different expected shuffle map. We check", "\n", "# that each example is shuffled as expected according to each", "\n", "# corresponding shuffle map.", "\n", "", "for", "i", "in", "range", "(", "len", "(", "expected_shufle_maps", ")", ")", ":", "\n", "            ", "shuffle_map", "=", "expected_shufle_maps", "[", "i", "]", "\n", "for", "k", ",", "v", "in", "shuffle_map", ".", "items", "(", ")", ":", "\n", "                ", "self", ".", "assertEqual", "(", "x", "[", "k", "]", "[", "i", "]", ",", "x_noised", "[", "v", "]", "[", "i", "]", ")", "\n", "\n", "# Shuffling should not affect the length of each example", "\n", "", "", "for", "pre_shuffle_length", ",", "post_shuffle_length", "in", "zip", "(", "x_len", ",", "l_noised", ")", ":", "\n", "            ", "self", ".", "assertEqual", "(", "pre_shuffle_length", ",", "post_shuffle_length", ")", "\n", "", "if", "expect_eos_at_end", ":", "\n", "            ", "self", ".", "assert_eos_at_end", "(", "x", "=", "x_noised", ",", "x_len", "=", "l_noised", ",", "eos", "=", "vocab", ".", "eos", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising.test_word_shuffle_with_eos": [[246, 275], ["test_noising.TestDataNoising._get_test_data_with_bpe_cont_marker", "test_noising.TestDataNoising.assert_word_shuffle_matches_expected", "test_noising.TestDataNoising.assert_word_shuffle_matches_expected", "test_noising.TestDataNoising.generate_unchanged_shuffle_map", "test_noising.TestDataNoising.generate_unchanged_shuffle_map"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising._get_test_data_with_bpe_cont_marker", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising.assert_word_shuffle_matches_expected", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising.assert_word_shuffle_matches_expected", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising.generate_unchanged_shuffle_map", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising.generate_unchanged_shuffle_map"], ["", "", "def", "test_word_shuffle_with_eos", "(", "self", ")", ":", "\n", "        ", "vocab", ",", "x", ",", "x_len", "=", "self", ".", "_get_test_data_with_bpe_cont_marker", "(", "append_eos", "=", "True", ")", "\n", "\n", "# Assert word shuffle with max shuffle distance 0 causes input to be", "\n", "# unchanged", "\n", "self", ".", "assert_word_shuffle_matches_expected", "(", "\n", "x", "=", "x", ",", "\n", "x_len", "=", "x_len", ",", "\n", "max_shuffle_distance", "=", "0", ",", "\n", "vocab", "=", "vocab", ",", "\n", "expected_shufle_maps", "=", "[", "\n", "self", ".", "generate_unchanged_shuffle_map", "(", "example_len", ")", "\n", "for", "example_len", "in", "x_len", "\n", "]", ",", "\n", "expect_eos_at_end", "=", "True", ",", "\n", ")", "\n", "\n", "# Assert word shuffle with max shuffle distance 3 matches our expected", "\n", "# shuffle order", "\n", "self", ".", "assert_word_shuffle_matches_expected", "(", "\n", "x", "=", "x", ",", "\n", "x_len", "=", "x_len", ",", "\n", "vocab", "=", "vocab", ",", "\n", "max_shuffle_distance", "=", "3", ",", "\n", "expected_shufle_maps", "=", "[", "\n", "self", ".", "generate_unchanged_shuffle_map", "(", "x_len", "[", "0", "]", ")", ",", "\n", "{", "0", ":", "0", ",", "1", ":", "3", ",", "2", ":", "1", ",", "3", ":", "2", "}", ",", "\n", "]", ",", "\n", "expect_eos_at_end", "=", "True", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising.test_word_shuffle_with_eos_nonbpe": [[277, 307], ["test_noising.TestDataNoising._get_test_data_with_word_vocab", "test_noising.TestDataNoising.assert_word_shuffle_matches_expected", "test_noising.TestDataNoising.assert_word_shuffle_matches_expected", "test_noising.TestDataNoising.generate_unchanged_shuffle_map"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising._get_test_data_with_word_vocab", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising.assert_word_shuffle_matches_expected", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising.assert_word_shuffle_matches_expected", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising.generate_unchanged_shuffle_map"], ["", "def", "test_word_shuffle_with_eos_nonbpe", "(", "self", ")", ":", "\n", "        ", "\"\"\"The purpose of this is to test shuffling logic with word vocabs\"\"\"", "\n", "vocab", ",", "x", ",", "x_len", "=", "self", ".", "_get_test_data_with_word_vocab", "(", "append_eos", "=", "True", ")", "\n", "\n", "# Assert word shuffle with max shuffle distance 0 causes input to be", "\n", "# unchanged", "\n", "self", ".", "assert_word_shuffle_matches_expected", "(", "\n", "x", "=", "x", ",", "\n", "x_len", "=", "x_len", ",", "\n", "max_shuffle_distance", "=", "0", ",", "\n", "vocab", "=", "vocab", ",", "\n", "expected_shufle_maps", "=", "[", "\n", "self", ".", "generate_unchanged_shuffle_map", "(", "example_len", ")", "\n", "for", "example_len", "in", "x_len", "\n", "]", ",", "\n", "expect_eos_at_end", "=", "True", ",", "\n", ")", "\n", "\n", "# Assert word shuffle with max shuffle distance 3 matches our expected", "\n", "# shuffle order", "\n", "self", ".", "assert_word_shuffle_matches_expected", "(", "\n", "x", "=", "x", ",", "\n", "x_len", "=", "x_len", ",", "\n", "vocab", "=", "vocab", ",", "\n", "max_shuffle_distance", "=", "3", ",", "\n", "expected_shufle_maps", "=", "[", "\n", "{", "0", ":", "0", ",", "1", ":", "1", ",", "2", ":", "3", ",", "3", ":", "2", "}", ",", "\n", "{", "0", ":", "0", ",", "1", ":", "2", ",", "2", ":", "1", ",", "3", ":", "3", ",", "4", ":", "4", "}", ",", "\n", "]", ",", "\n", "expect_eos_at_end", "=", "True", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising.test_word_shuffle_without_eos": [[309, 339], ["test_noising.TestDataNoising._get_test_data_with_bpe_cont_marker", "test_noising.TestDataNoising.assert_word_shuffle_matches_expected", "test_noising.TestDataNoising.assert_word_shuffle_matches_expected", "test_noising.TestDataNoising.generate_unchanged_shuffle_map", "test_noising.TestDataNoising.generate_unchanged_shuffle_map"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising._get_test_data_with_bpe_cont_marker", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising.assert_word_shuffle_matches_expected", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising.assert_word_shuffle_matches_expected", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising.generate_unchanged_shuffle_map", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising.generate_unchanged_shuffle_map"], ["", "def", "test_word_shuffle_without_eos", "(", "self", ")", ":", "\n", "        ", "\"\"\"Same result as word shuffle with eos except no EOS at end\"\"\"", "\n", "vocab", ",", "x", ",", "x_len", "=", "self", ".", "_get_test_data_with_bpe_cont_marker", "(", "append_eos", "=", "False", ")", "\n", "\n", "# Assert word shuffle with max shuffle distance 0 causes input to be", "\n", "# unchanged", "\n", "self", ".", "assert_word_shuffle_matches_expected", "(", "\n", "x", "=", "x", ",", "\n", "x_len", "=", "x_len", ",", "\n", "max_shuffle_distance", "=", "0", ",", "\n", "vocab", "=", "vocab", ",", "\n", "expected_shufle_maps", "=", "[", "\n", "self", ".", "generate_unchanged_shuffle_map", "(", "example_len", ")", "\n", "for", "example_len", "in", "x_len", "\n", "]", ",", "\n", "expect_eos_at_end", "=", "False", ",", "\n", ")", "\n", "\n", "# Assert word shuffle with max shuffle distance 3 matches our expected", "\n", "# shuffle order", "\n", "self", ".", "assert_word_shuffle_matches_expected", "(", "\n", "x", "=", "x", ",", "\n", "x_len", "=", "x_len", ",", "\n", "vocab", "=", "vocab", ",", "\n", "max_shuffle_distance", "=", "3", ",", "\n", "expected_shufle_maps", "=", "[", "\n", "self", ".", "generate_unchanged_shuffle_map", "(", "x_len", "[", "0", "]", ")", ",", "\n", "{", "0", ":", "0", ",", "1", ":", "3", ",", "2", ":", "1", ",", "3", ":", "2", "}", ",", "\n", "]", ",", "\n", "expect_eos_at_end", "=", "False", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising.test_word_shuffle_without_eos_with_bpe_end_marker": [[341, 373], ["test_noising.TestDataNoising._get_test_data_with_bpe_end_marker", "test_noising.TestDataNoising.assert_word_shuffle_matches_expected", "test_noising.TestDataNoising.assert_word_shuffle_matches_expected", "test_noising.TestDataNoising.generate_unchanged_shuffle_map", "test_noising.TestDataNoising.generate_unchanged_shuffle_map"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising._get_test_data_with_bpe_end_marker", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising.assert_word_shuffle_matches_expected", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising.assert_word_shuffle_matches_expected", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising.generate_unchanged_shuffle_map", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising.generate_unchanged_shuffle_map"], ["", "def", "test_word_shuffle_without_eos_with_bpe_end_marker", "(", "self", ")", ":", "\n", "        ", "\"\"\"Same result as word shuffle without eos except using BPE end token\"\"\"", "\n", "vocab", ",", "x", ",", "x_len", "=", "self", ".", "_get_test_data_with_bpe_end_marker", "(", "append_eos", "=", "False", ")", "\n", "\n", "# Assert word shuffle with max shuffle distance 0 causes input to be", "\n", "# unchanged", "\n", "self", ".", "assert_word_shuffle_matches_expected", "(", "\n", "x", "=", "x", ",", "\n", "x_len", "=", "x_len", ",", "\n", "max_shuffle_distance", "=", "0", ",", "\n", "vocab", "=", "vocab", ",", "\n", "expected_shufle_maps", "=", "[", "\n", "self", ".", "generate_unchanged_shuffle_map", "(", "example_len", ")", "\n", "for", "example_len", "in", "x_len", "\n", "]", ",", "\n", "expect_eos_at_end", "=", "False", ",", "\n", "bpe_end_marker", "=", "\"_EOW\"", ",", "\n", ")", "\n", "\n", "# Assert word shuffle with max shuffle distance 3 matches our expected", "\n", "# shuffle order", "\n", "self", ".", "assert_word_shuffle_matches_expected", "(", "\n", "x", "=", "x", ",", "\n", "x_len", "=", "x_len", ",", "\n", "vocab", "=", "vocab", ",", "\n", "max_shuffle_distance", "=", "3", ",", "\n", "expected_shufle_maps", "=", "[", "\n", "self", ".", "generate_unchanged_shuffle_map", "(", "x_len", "[", "0", "]", ")", ",", "\n", "{", "0", ":", "0", ",", "1", ":", "3", ",", "2", ":", "1", ",", "3", ":", "2", "}", ",", "\n", "]", ",", "\n", "expect_eos_at_end", "=", "False", ",", "\n", "bpe_end_marker", "=", "\"_EOW\"", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising.assert_no_eos_at_end": [[375, 383], ["range", "len", "test_noising.TestDataNoising.assertNotEqual"], "methods", ["None"], ["", "def", "assert_no_eos_at_end", "(", "self", ",", "x", ",", "x_len", ",", "eos", ")", ":", "\n", "        ", "\"\"\"Asserts that the last token of each sentence in x is not EOS \"\"\"", "\n", "for", "i", "in", "range", "(", "len", "(", "x_len", ")", ")", ":", "\n", "            ", "self", ".", "assertNotEqual", "(", "\n", "x", "[", "x_len", "[", "i", "]", "-", "1", "]", "[", "i", "]", ",", "\n", "eos", ",", "\n", "\"Expected no eos (token id {eos}) at the end of sentence {i}.\"", ".", "format", "(", "\n", "eos", "=", "eos", ",", "i", "=", "i", "\n", ")", ",", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising.test_word_dropout_without_eos": [[386, 397], ["test_noising.TestDataNoising._get_test_data_with_bpe_cont_marker", "fairseq.data.data_utils.numpy_seed", "fairseq.data.noising.WordDropout", "fairseq.data.noising.WordDropout.noising", "test_noising.TestDataNoising.assert_word_dropout_correct", "test_noising.TestDataNoising.assert_no_eos_at_end", "vocab.eos"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising._get_test_data_with_bpe_cont_marker", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.data_utils.numpy_seed", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.noising.UnsupervisedMTNoising.noising", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising.assert_word_dropout_correct", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising.assert_no_eos_at_end", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos"], ["", "", "def", "test_word_dropout_without_eos", "(", "self", ")", ":", "\n", "        ", "\"\"\"Same result as word dropout with eos except no EOS at end\"\"\"", "\n", "vocab", ",", "x", ",", "x_len", "=", "self", ".", "_get_test_data_with_bpe_cont_marker", "(", "append_eos", "=", "False", ")", "\n", "\n", "with", "data_utils", ".", "numpy_seed", "(", "1234", ")", ":", "\n", "            ", "noising_gen", "=", "noising", ".", "WordDropout", "(", "vocab", ")", "\n", "x_noised", ",", "l_noised", "=", "noising_gen", ".", "noising", "(", "x", ",", "x_len", ",", "0.2", ")", "\n", "self", ".", "assert_word_dropout_correct", "(", "\n", "x", "=", "x", ",", "x_noised", "=", "x_noised", ",", "x_len", "=", "x_len", ",", "l_noised", "=", "l_noised", "\n", ")", "\n", "self", ".", "assert_no_eos_at_end", "(", "x", "=", "x_noised", ",", "x_len", "=", "l_noised", ",", "eos", "=", "vocab", ".", "eos", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising.test_word_blank_without_eos": [[398, 409], ["test_noising.TestDataNoising._get_test_data_with_bpe_cont_marker", "fairseq.data.data_utils.numpy_seed", "fairseq.data.noising.WordDropout", "fairseq.data.noising.WordDropout.noising", "test_noising.TestDataNoising.assert_word_blanking_correct", "test_noising.TestDataNoising.assert_no_eos_at_end", "vocab.unk", "vocab.unk", "vocab.eos"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising._get_test_data_with_bpe_cont_marker", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.data_utils.numpy_seed", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.noising.UnsupervisedMTNoising.noising", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising.assert_word_blanking_correct", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising.assert_no_eos_at_end", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.unk", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.unk", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos"], ["", "", "def", "test_word_blank_without_eos", "(", "self", ")", ":", "\n", "        ", "\"\"\"Same result as word blank with eos except no EOS at end\"\"\"", "\n", "vocab", ",", "x", ",", "x_len", "=", "self", ".", "_get_test_data_with_bpe_cont_marker", "(", "append_eos", "=", "False", ")", "\n", "\n", "with", "data_utils", ".", "numpy_seed", "(", "1234", ")", ":", "\n", "            ", "noising_gen", "=", "noising", ".", "WordDropout", "(", "vocab", ")", "\n", "x_noised", ",", "l_noised", "=", "noising_gen", ".", "noising", "(", "x", ",", "x_len", ",", "0.2", ",", "vocab", ".", "unk", "(", ")", ")", "\n", "self", ".", "assert_word_blanking_correct", "(", "\n", "x", "=", "x", ",", "x_noised", "=", "x_noised", ",", "x_len", "=", "x_len", ",", "l_noised", "=", "l_noised", ",", "unk", "=", "vocab", ".", "unk", "(", ")", "\n", ")", "\n", "self", ".", "assert_no_eos_at_end", "(", "x", "=", "x_noised", ",", "x_len", "=", "l_noised", ",", "eos", "=", "vocab", ".", "eos", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising._get_noising_dataset_batch": [[410, 447], ["tests.TestDataset", "fairseq.data.noising.NoisingDataset", "fairseq.data.LanguagePairDataset", "fairseq.data.TransformEosDataset", "torch.utils.data.DataLoader", "next", "src_dict.eos", "iter"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos"], ["", "", "def", "_get_noising_dataset_batch", "(", "\n", "self", ",", "src_tokens_no_pad", ",", "src_dict", ",", "append_eos_to_tgt", "=", "False", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Constructs a NoisingDataset and the corresponding\n        ``LanguagePairDataset(NoisingDataset(src), src)``. If\n        *append_eos_to_tgt* is True, wrap the source dataset in\n        :class:`TransformEosDataset` to append EOS to the clean source when\n        using it as the target.\n        \"\"\"", "\n", "src_dataset", "=", "test_utils", ".", "TestDataset", "(", "data", "=", "src_tokens_no_pad", ")", "\n", "\n", "noising_dataset", "=", "noising", ".", "NoisingDataset", "(", "\n", "src_dataset", "=", "src_dataset", ",", "\n", "src_dict", "=", "src_dict", ",", "\n", "seed", "=", "1234", ",", "\n", "max_word_shuffle_distance", "=", "3", ",", "\n", "word_dropout_prob", "=", "0.2", ",", "\n", "word_blanking_prob", "=", "0.2", ",", "\n", "noising_class", "=", "noising", ".", "UnsupervisedMTNoising", ",", "\n", ")", "\n", "tgt", "=", "src_dataset", "\n", "language_pair_dataset", "=", "LanguagePairDataset", "(", "\n", "src", "=", "noising_dataset", ",", "tgt", "=", "tgt", ",", "src_sizes", "=", "None", ",", "src_dict", "=", "src_dict", "\n", ")", "\n", "language_pair_dataset", "=", "TransformEosDataset", "(", "\n", "language_pair_dataset", ",", "src_dict", ".", "eos", "(", ")", ",", "\n", "append_eos_to_tgt", "=", "append_eos_to_tgt", ",", "\n", ")", "\n", "\n", "dataloader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "dataset", "=", "language_pair_dataset", ",", "\n", "batch_size", "=", "2", ",", "\n", "collate_fn", "=", "language_pair_dataset", ".", "collater", ",", "\n", ")", "\n", "denoising_batch_result", "=", "next", "(", "iter", "(", "dataloader", ")", ")", "\n", "return", "denoising_batch_result", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising.test_noising_dataset_with_eos": [[448, 479], ["test_noising.TestDataNoising._get_test_data_with_bpe_cont_marker", "torch.t", "test_noising.TestDataNoising._get_noising_dataset_batch", "torch.LongTensor", "torch.LongTensor", "test_noising.TestDataNoising.assertTensorEqual", "test_noising.TestDataNoising.assertTensorEqual", "src_tokens_no_pad.append", "src_dict.eos", "src_dict.pad", "fairseq.utils.strip_pad", "src_dict.pad"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising._get_test_data_with_bpe_cont_marker", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising._get_noising_dataset_batch", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.test_collaters.TestSeq2SeqCollator.assertTensorEqual", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.test_collaters.TestSeq2SeqCollator.assertTensorEqual", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.strip_pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad"], ["", "def", "test_noising_dataset_with_eos", "(", "self", ")", ":", "\n", "        ", "src_dict", ",", "src_tokens", ",", "_", "=", "self", ".", "_get_test_data_with_bpe_cont_marker", "(", "\n", "append_eos", "=", "True", "\n", ")", "\n", "\n", "# Format data for src_dataset", "\n", "src_tokens", "=", "torch", ".", "t", "(", "src_tokens", ")", "\n", "src_tokens_no_pad", "=", "[", "]", "\n", "for", "src_sentence", "in", "src_tokens", ":", "\n", "            ", "src_tokens_no_pad", ".", "append", "(", "\n", "utils", ".", "strip_pad", "(", "tensor", "=", "src_sentence", ",", "pad", "=", "src_dict", ".", "pad", "(", ")", ")", "\n", ")", "\n", "", "denoising_batch_result", "=", "self", ".", "_get_noising_dataset_batch", "(", "\n", "src_tokens_no_pad", "=", "src_tokens_no_pad", ",", "src_dict", "=", "src_dict", "\n", ")", "\n", "\n", "eos", ",", "pad", "=", "src_dict", ".", "eos", "(", ")", ",", "src_dict", ".", "pad", "(", ")", "\n", "\n", "# Generated noisy source as source", "\n", "expected_src", "=", "torch", ".", "LongTensor", "(", "\n", "[", "[", "4", ",", "5", ",", "10", ",", "11", ",", "8", ",", "12", ",", "13", ",", "eos", "]", ",", "[", "pad", ",", "pad", ",", "pad", ",", "6", ",", "8", ",", "9", ",", "7", ",", "eos", "]", "]", "\n", ")", "\n", "# Original clean source as target (right-padded)", "\n", "expected_tgt", "=", "torch", ".", "LongTensor", "(", "\n", "[", "[", "4", ",", "5", ",", "10", ",", "11", ",", "8", ",", "12", ",", "13", ",", "eos", "]", ",", "[", "6", ",", "7", ",", "8", ",", "9", ",", "eos", ",", "pad", ",", "pad", ",", "pad", "]", "]", "\n", ")", "\n", "generated_src", "=", "denoising_batch_result", "[", "\"net_input\"", "]", "[", "\"src_tokens\"", "]", "\n", "tgt_tokens", "=", "denoising_batch_result", "[", "\"target\"", "]", "\n", "\n", "self", ".", "assertTensorEqual", "(", "expected_src", ",", "generated_src", ")", "\n", "self", ".", "assertTensorEqual", "(", "expected_tgt", ",", "tgt_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising.test_noising_dataset_without_eos": [[480, 519], ["test_noising.TestDataNoising._get_test_data_with_bpe_cont_marker", "torch.t", "test_noising.TestDataNoising._get_noising_dataset_batch", "torch.LongTensor", "torch.LongTensor", "test_noising.TestDataNoising.assertTensorEqual", "test_noising.TestDataNoising.assertTensorEqual", "src_tokens_no_pad.append", "src_dict.eos", "src_dict.pad", "fairseq.utils.strip_pad", "src_dict.pad"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising._get_test_data_with_bpe_cont_marker", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising._get_noising_dataset_batch", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.test_collaters.TestSeq2SeqCollator.assertTensorEqual", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.test_collaters.TestSeq2SeqCollator.assertTensorEqual", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.strip_pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad"], ["", "def", "test_noising_dataset_without_eos", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Similar to test noising dataset with eos except that we have to set\n        *append_eos_to_tgt* to ``True``.\n        \"\"\"", "\n", "\n", "src_dict", ",", "src_tokens", ",", "_", "=", "self", ".", "_get_test_data_with_bpe_cont_marker", "(", "\n", "append_eos", "=", "False", "\n", ")", "\n", "\n", "# Format data for src_dataset", "\n", "src_tokens", "=", "torch", ".", "t", "(", "src_tokens", ")", "\n", "src_tokens_no_pad", "=", "[", "]", "\n", "for", "src_sentence", "in", "src_tokens", ":", "\n", "            ", "src_tokens_no_pad", ".", "append", "(", "\n", "utils", ".", "strip_pad", "(", "tensor", "=", "src_sentence", ",", "pad", "=", "src_dict", ".", "pad", "(", ")", ")", "\n", ")", "\n", "", "denoising_batch_result", "=", "self", ".", "_get_noising_dataset_batch", "(", "\n", "src_tokens_no_pad", "=", "src_tokens_no_pad", ",", "\n", "src_dict", "=", "src_dict", ",", "\n", "append_eos_to_tgt", "=", "True", ",", "\n", ")", "\n", "\n", "eos", ",", "pad", "=", "src_dict", ".", "eos", "(", ")", ",", "src_dict", ".", "pad", "(", ")", "\n", "\n", "# Generated noisy source as source", "\n", "expected_src", "=", "torch", ".", "LongTensor", "(", "\n", "[", "[", "4", ",", "5", ",", "10", ",", "11", ",", "8", ",", "12", ",", "13", "]", ",", "[", "pad", ",", "pad", ",", "pad", ",", "6", ",", "8", ",", "9", ",", "7", "]", "]", "\n", ")", "\n", "# Original clean source as target (right-padded)", "\n", "expected_tgt", "=", "torch", ".", "LongTensor", "(", "\n", "[", "[", "4", ",", "5", ",", "10", ",", "11", ",", "8", ",", "12", ",", "13", ",", "eos", "]", ",", "[", "6", ",", "7", ",", "8", ",", "9", ",", "eos", ",", "pad", ",", "pad", ",", "pad", "]", "]", "\n", ")", "\n", "\n", "generated_src", "=", "denoising_batch_result", "[", "\"net_input\"", "]", "[", "\"src_tokens\"", "]", "\n", "tgt_tokens", "=", "denoising_batch_result", "[", "\"target\"", "]", "\n", "\n", "self", ".", "assertTensorEqual", "(", "expected_src", ",", "generated_src", ")", "\n", "self", ".", "assertTensorEqual", "(", "expected_tgt", ",", "tgt_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_noising.TestDataNoising.assertTensorEqual": [[520, 523], ["test_noising.TestDataNoising.assertEqual", "test_noising.TestDataNoising.assertEqual", "t1.size", "t2.size", "t1.ne().long().sum", "t1.ne().long", "t1.ne"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "assertTensorEqual", "(", "self", ",", "t1", ",", "t2", ")", ":", "\n", "        ", "self", ".", "assertEqual", "(", "t1", ".", "size", "(", ")", ",", "t2", ".", "size", "(", ")", ",", "\"size mismatch\"", ")", "\n", "self", ".", "assertEqual", "(", "t1", ".", "ne", "(", "t2", ")", ".", "long", "(", ")", ".", "sum", "(", ")", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_convtbc.TestConvTBC.test_convtbc": [[14, 41], ["fairseq.modules.ConvTBC", "torch.Conv1d", "torch.Conv1d", "fairseq.modules.ConvTBC.weight.data.copy_", "fairseq.modules.ConvTBC.bias.data.copy_", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn.data.transpose().transpose", "torch.randn.data.transpose().transpose", "fairseq.modules.ConvTBC.", "torch.Conv1d.", "test_convtbc.TestConvTBC.assertAlmostEqual", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn.transpose().transpose().contiguous", "torch.randn.transpose().transpose().contiguous", "fairseq.modules.ConvTBC.backward", "nn.Conv1d.backward", "test_convtbc.TestConvTBC.assertAlmostEqual", "test_convtbc.TestConvTBC.assertAlmostEqual", "test_convtbc.TestConvTBC.assertAlmostEqual", "torch.Conv1d.weight.data.transpose", "fairseq.modules.ConvTBC.data.transpose().transpose", "fairseq.modules.ConvTBC.size", "fairseq.modules.ConvTBC.weight.grad.data.transpose", "torch.randn.grad.data.transpose().transpose", "torch.randn.grad.data.transpose().transpose", "torch.randn.data.transpose", "torch.randn.data.transpose", "torch.randn.transpose().transpose", "torch.randn.transpose().transpose", "fairseq.modules.ConvTBC.data.transpose", "torch.randn.grad.data.transpose", "torch.randn.grad.data.transpose", "torch.randn.transpose", "torch.randn.transpose"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv.ConvTBC", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertAlmostEqual", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.scalar_bias.ScalarBias.backward", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.scalar_bias.ScalarBias.backward", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertAlmostEqual", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertAlmostEqual", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertAlmostEqual", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["    ", "def", "test_convtbc", "(", "self", ")", ":", "\n", "# ksz, in_channels, out_channels", "\n", "        ", "conv_tbc", "=", "ConvTBC", "(", "4", ",", "5", ",", "kernel_size", "=", "3", ",", "padding", "=", "1", ")", "\n", "# out_channels, in_channels, ksz", "\n", "conv1d", "=", "nn", ".", "Conv1d", "(", "4", ",", "5", ",", "kernel_size", "=", "3", ",", "padding", "=", "1", ")", "\n", "\n", "conv_tbc", ".", "weight", ".", "data", ".", "copy_", "(", "conv1d", ".", "weight", ".", "data", ".", "transpose", "(", "0", ",", "2", ")", ")", "\n", "conv_tbc", ".", "bias", ".", "data", ".", "copy_", "(", "conv1d", ".", "bias", ".", "data", ")", "\n", "\n", "input_tbc", "=", "torch", ".", "randn", "(", "7", ",", "2", ",", "4", ",", "requires_grad", "=", "True", ")", "\n", "input1d", "=", "input_tbc", ".", "data", ".", "transpose", "(", "0", ",", "1", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "input1d", ".", "requires_grad", "=", "True", "\n", "\n", "output_tbc", "=", "conv_tbc", "(", "input_tbc", ")", "\n", "output1d", "=", "conv1d", "(", "input1d", ")", "\n", "\n", "self", ".", "assertAlmostEqual", "(", "output_tbc", ".", "data", ".", "transpose", "(", "0", ",", "1", ")", ".", "transpose", "(", "1", ",", "2", ")", ",", "output1d", ".", "data", ")", "\n", "\n", "grad_tbc", "=", "torch", ".", "randn", "(", "output_tbc", ".", "size", "(", ")", ")", "\n", "grad1d", "=", "grad_tbc", ".", "transpose", "(", "0", ",", "1", ")", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", "\n", "\n", "output_tbc", ".", "backward", "(", "grad_tbc", ")", "\n", "output1d", ".", "backward", "(", "grad1d", ")", "\n", "\n", "self", ".", "assertAlmostEqual", "(", "conv_tbc", ".", "weight", ".", "grad", ".", "data", ".", "transpose", "(", "0", ",", "2", ")", ",", "conv1d", ".", "weight", ".", "grad", ".", "data", ")", "\n", "self", ".", "assertAlmostEqual", "(", "conv_tbc", ".", "bias", ".", "grad", ".", "data", ",", "conv1d", ".", "bias", ".", "grad", ".", "data", ")", "\n", "self", ".", "assertAlmostEqual", "(", "input_tbc", ".", "grad", ".", "data", ".", "transpose", "(", "0", ",", "1", ")", ".", "transpose", "(", "1", ",", "2", ")", ",", "input1d", ".", "grad", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_convtbc.TestConvTBC.assertAlmostEqual": [[42, 45], ["test_convtbc.TestConvTBC.assertEqual", "test_convtbc.TestConvTBC.assertLess", "t1.size", "t2.size"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "assertAlmostEqual", "(", "self", ",", "t1", ",", "t2", ")", ":", "\n", "        ", "self", ".", "assertEqual", "(", "t1", ".", "size", "(", ")", ",", "t2", ".", "size", "(", ")", ",", "\"size mismatch\"", ")", "\n", "self", ".", "assertLess", "(", "(", "t1", "-", "t2", ")", ".", "abs", "(", ")", ".", "max", "(", ")", ",", "1e-4", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_scorer.TestSequenceScorer.test_sequence_scorer": [[18, 92], ["tests.dummy_dictionary", "test_sequence_scorer.TestSequenceScorer.assertEqual", "test_sequence_scorer.TestSequenceScorer.assertEqual", "test_sequence_scorer.TestSequenceScorer.assertEqual", "tests.dummy_dictionary.eos", "tests.dummy_dataloader", "argparse.Namespace", "tests.TestTranslationTask.setup_task", "tests.TestTranslationTask.setup_task.build_model", "fairseq.sequence_scorer.SequenceScorer", "tests.dummy_dictionary.pad", "tests.dummy_dictionary.eos", "tests.dummy_dictionary.unk", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "tests.TestTranslationTask.setup_task.inference_step", "zip", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "sample[].tolist", "test_sequence_scorer.TestSequenceScorer.assertHypoTokens", "test_sequence_scorer.TestSequenceScorer.assertHypoScore"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.utils.dummy_dictionary", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.utils.dummy_dataloader", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.sentence_prediction.SentencePredictionTask.setup_task", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv_lm.LightConvLanguageModel.build_model", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.unk", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.translation_moe.TranslationMoETask.inference_step", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoTokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoScore"], ["    ", "def", "test_sequence_scorer", "(", "self", ")", ":", "\n", "# construct dummy dictionary", "\n", "        ", "d", "=", "test_utils", ".", "dummy_dictionary", "(", "vocab_size", "=", "2", ")", "\n", "self", ".", "assertEqual", "(", "d", ".", "pad", "(", ")", ",", "1", ")", "\n", "self", ".", "assertEqual", "(", "d", ".", "eos", "(", ")", ",", "2", ")", "\n", "self", ".", "assertEqual", "(", "d", ".", "unk", "(", ")", ",", "3", ")", "\n", "eos", "=", "d", ".", "eos", "(", ")", "\n", "w1", "=", "4", "\n", "w2", "=", "5", "\n", "\n", "# construct dataloader", "\n", "data", "=", "[", "\n", "{", "\n", "'source'", ":", "torch", ".", "LongTensor", "(", "[", "w1", ",", "w2", ",", "eos", "]", ")", ",", "\n", "'target'", ":", "torch", ".", "LongTensor", "(", "[", "w1", ",", "w2", ",", "w1", ",", "eos", "]", ")", ",", "\n", "}", ",", "\n", "{", "\n", "'source'", ":", "torch", ".", "LongTensor", "(", "[", "w2", ",", "eos", "]", ")", ",", "\n", "'target'", ":", "torch", ".", "LongTensor", "(", "[", "w2", ",", "w1", ",", "eos", "]", ")", ",", "\n", "}", ",", "\n", "{", "\n", "'source'", ":", "torch", ".", "LongTensor", "(", "[", "w2", ",", "eos", "]", ")", ",", "\n", "'target'", ":", "torch", ".", "LongTensor", "(", "[", "w2", ",", "eos", "]", ")", ",", "\n", "}", ",", "\n", "]", "\n", "data_itr", "=", "test_utils", ".", "dummy_dataloader", "(", "data", ")", "\n", "\n", "# specify expected output probabilities", "\n", "args", "=", "argparse", ".", "Namespace", "(", ")", "\n", "unk", "=", "0.", "\n", "args", ".", "beam_probs", "=", "[", "\n", "# step 0:", "\n", "torch", ".", "FloatTensor", "(", "[", "\n", "# eos      w1   w2", "\n", "[", "0.0", ",", "unk", ",", "0.6", ",", "0.4", "]", ",", "# sentence 1", "\n", "[", "0.0", ",", "unk", ",", "0.4", ",", "0.6", "]", ",", "# sentence 2", "\n", "[", "0.0", ",", "unk", ",", "0.7", ",", "0.3", "]", ",", "# sentence 3", "\n", "]", ")", ",", "\n", "# step 1:", "\n", "torch", ".", "FloatTensor", "(", "[", "\n", "# eos      w1   w2", "\n", "[", "0.0", ",", "unk", ",", "0.2", ",", "0.7", "]", ",", "# sentence 1", "\n", "[", "0.0", ",", "unk", ",", "0.8", ",", "0.2", "]", ",", "# sentence 2", "\n", "[", "0.7", ",", "unk", ",", "0.1", ",", "0.2", "]", ",", "# sentence 3", "\n", "]", ")", ",", "\n", "# step 2:", "\n", "torch", ".", "FloatTensor", "(", "[", "\n", "# eos       w1    w2", "\n", "[", "0.10", ",", "unk", ",", "0.50", ",", "0.4", "]", ",", "# sentence 1", "\n", "[", "0.15", ",", "unk", ",", "0.15", ",", "0.7", "]", ",", "# sentence 2", "\n", "[", "0.00", ",", "unk", ",", "0.00", ",", "0.0", "]", ",", "# sentence 3", "\n", "]", ")", ",", "\n", "# step 3:", "\n", "torch", ".", "FloatTensor", "(", "[", "\n", "# eos      w1    w2", "\n", "[", "0.9", ",", "unk", ",", "0.05", ",", "0.05", "]", ",", "# sentence 1", "\n", "[", "0.0", ",", "unk", ",", "0.00", ",", "0.0", "]", ",", "# sentence 2", "\n", "[", "0.0", ",", "unk", ",", "0.00", ",", "0.0", "]", ",", "# sentence 3", "\n", "]", ")", ",", "\n", "]", "\n", "expected_scores", "=", "[", "\n", "[", "0.6", ",", "0.7", ",", "0.5", ",", "0.9", "]", ",", "# sentence 1", "\n", "[", "0.6", ",", "0.8", ",", "0.15", "]", ",", "# sentence 2", "\n", "[", "0.3", ",", "0.7", "]", ",", "# sentence 3", "\n", "]", "\n", "\n", "task", "=", "test_utils", ".", "TestTranslationTask", ".", "setup_task", "(", "args", ",", "d", ",", "d", ")", "\n", "model", "=", "task", ".", "build_model", "(", "args", ")", "\n", "scorer", "=", "SequenceScorer", "(", "task", ".", "target_dictionary", ")", "\n", "for", "sample", "in", "data_itr", ":", "\n", "            ", "hypos", "=", "task", ".", "inference_step", "(", "scorer", ",", "[", "model", "]", ",", "sample", ")", "\n", "for", "id", ",", "hypos_id", "in", "zip", "(", "sample", "[", "'id'", "]", ".", "tolist", "(", ")", ",", "hypos", ")", ":", "\n", "                ", "self", ".", "assertHypoTokens", "(", "hypos_id", "[", "0", "]", ",", "data", "[", "id", "]", "[", "'target'", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos_id", "[", "0", "]", ",", "expected_scores", "[", "id", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_scorer.TestSequenceScorer.assertHypoTokens": [[93, 95], ["test_sequence_scorer.TestSequenceScorer.assertTensorEqual", "torch.LongTensor"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.test_collaters.TestSeq2SeqCollator.assertTensorEqual"], ["", "", "", "def", "assertHypoTokens", "(", "self", ",", "hypo", ",", "tokens", ")", ":", "\n", "        ", "self", ".", "assertTensorEqual", "(", "hypo", "[", "'tokens'", "]", ",", "torch", ".", "LongTensor", "(", "tokens", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_scorer.TestSequenceScorer.assertHypoScore": [[96, 104], ["torch.FloatTensor().log", "test_sequence_scorer.TestSequenceScorer.assertAlmostEqual", "test_sequence_scorer.TestSequenceScorer.assertEqual", "torch.FloatTensor().log.sum", "test_sequence_scorer.TestSequenceScorer.assertLess", "torch.FloatTensor().log.numel", "hypo[].numel", "abs", "torch.FloatTensor", "torch.FloatTensor().log.numel"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertAlmostEqual"], ["", "def", "assertHypoScore", "(", "self", ",", "hypo", ",", "pos_probs", ",", "normalized", "=", "True", ",", "lenpen", "=", "1.", ")", ":", "\n", "        ", "pos_scores", "=", "torch", ".", "FloatTensor", "(", "pos_probs", ")", ".", "log", "(", ")", "\n", "self", ".", "assertAlmostEqual", "(", "hypo", "[", "'positional_scores'", "]", ",", "pos_scores", ")", "\n", "self", ".", "assertEqual", "(", "pos_scores", ".", "numel", "(", ")", ",", "hypo", "[", "'tokens'", "]", ".", "numel", "(", ")", ")", "\n", "score", "=", "pos_scores", ".", "sum", "(", ")", "\n", "if", "normalized", ":", "\n", "            ", "score", "/=", "pos_scores", ".", "numel", "(", ")", "**", "lenpen", "\n", "", "self", ".", "assertLess", "(", "abs", "(", "score", "-", "hypo", "[", "'score'", "]", ")", ",", "1e-6", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_scorer.TestSequenceScorer.assertAlmostEqual": [[105, 108], ["test_sequence_scorer.TestSequenceScorer.assertEqual", "test_sequence_scorer.TestSequenceScorer.assertLess", "t1.size", "t2.size"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "assertAlmostEqual", "(", "self", ",", "t1", ",", "t2", ")", ":", "\n", "        ", "self", ".", "assertEqual", "(", "t1", ".", "size", "(", ")", ",", "t2", ".", "size", "(", ")", ",", "\"size mismatch\"", ")", "\n", "self", ".", "assertLess", "(", "(", "t1", "-", "t2", ")", ".", "abs", "(", ")", ".", "max", "(", ")", ",", "1e-4", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_scorer.TestSequenceScorer.assertTensorEqual": [[109, 112], ["test_sequence_scorer.TestSequenceScorer.assertEqual", "test_sequence_scorer.TestSequenceScorer.assertEqual", "t1.size", "t2.size", "t1.ne().long().sum", "t1.ne().long", "t1.ne"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "assertTensorEqual", "(", "self", ",", "t1", ",", "t2", ")", ":", "\n", "        ", "self", ".", "assertEqual", "(", "t1", ".", "size", "(", ")", ",", "t2", ".", "size", "(", ")", ",", "\"size mismatch\"", ")", "\n", "self", ".", "assertEqual", "(", "t1", ".", "ne", "(", "t2", ")", ".", "long", "(", ")", ".", "sum", "(", ")", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_backtranslation_dataset.TestBacktranslationDataset.setUp": [[22, 31], ["tests.sequence_generator_setup", "tests.TestDataset", "torch.cuda.is_available"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.utils.sequence_generator_setup"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "self", ".", "tgt_dict", ",", "self", ".", "w1", ",", "self", ".", "w2", ",", "self", ".", "src_tokens", ",", "self", ".", "src_lengths", ",", "self", ".", "model", "=", "(", "\n", "test_utils", ".", "sequence_generator_setup", "(", ")", "\n", ")", "\n", "\n", "dummy_src_samples", "=", "self", ".", "src_tokens", "\n", "\n", "self", ".", "tgt_dataset", "=", "test_utils", ".", "TestDataset", "(", "data", "=", "dummy_src_samples", ")", "\n", "self", ".", "cuda", "=", "torch", ".", "cuda", ".", "is_available", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_backtranslation_dataset.TestBacktranslationDataset._backtranslation_dataset_helper": [[32, 94], ["fairseq.data.LanguagePairDataset", "fairseq.sequence_generator.SequenceGenerator", "fairseq.data.BacktranslationDataset", "torch.utils.data.DataLoader", "next", "torch.LongTensor", "torch.LongTensor", "test_backtranslation_dataset.TestBacktranslationDataset.assertTensorEqual", "test_backtranslation_dataset.TestBacktranslationDataset.assertTensorEqual", "iter", "test_backtranslation_dataset.TestBacktranslationDataset.tgt_dict.eos", "test_backtranslation_dataset.TestBacktranslationDataset.tgt_dict.pad", "fairseq.data.TransformEosDataset", "fairseq.sequence_generator.SequenceGenerator.generate", "fairseq.data.TransformEosDataset", "test_backtranslation_dataset.TestBacktranslationDataset.tgt_dict.eos", "test_backtranslation_dataset.TestBacktranslationDataset.tgt_dict.eos"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.test_collaters.TestSeq2SeqCollator.assertTensorEqual", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.test_collaters.TestSeq2SeqCollator.assertTensorEqual", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.sequence_generator.SequenceGenerator.generate", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos"], ["", "def", "_backtranslation_dataset_helper", "(", "\n", "self", ",", "remove_eos_from_input_src", ",", "remove_eos_from_output_src", ",", "\n", ")", ":", "\n", "        ", "tgt_dataset", "=", "LanguagePairDataset", "(", "\n", "src", "=", "self", ".", "tgt_dataset", ",", "\n", "src_sizes", "=", "self", ".", "tgt_dataset", ".", "sizes", ",", "\n", "src_dict", "=", "self", ".", "tgt_dict", ",", "\n", "tgt", "=", "None", ",", "\n", "tgt_sizes", "=", "None", ",", "\n", "tgt_dict", "=", "None", ",", "\n", ")", "\n", "\n", "generator", "=", "SequenceGenerator", "(", "\n", "tgt_dict", "=", "self", ".", "tgt_dict", ",", "\n", "max_len_a", "=", "0", ",", "\n", "max_len_b", "=", "200", ",", "\n", "beam_size", "=", "2", ",", "\n", "unk_penalty", "=", "0", ",", "\n", "sampling", "=", "False", ",", "\n", ")", "\n", "\n", "backtranslation_dataset", "=", "BacktranslationDataset", "(", "\n", "tgt_dataset", "=", "TransformEosDataset", "(", "\n", "dataset", "=", "tgt_dataset", ",", "\n", "eos", "=", "self", ".", "tgt_dict", ".", "eos", "(", ")", ",", "\n", "# remove eos from the input src", "\n", "remove_eos_from_src", "=", "remove_eos_from_input_src", ",", "\n", ")", ",", "\n", "src_dict", "=", "self", ".", "tgt_dict", ",", "\n", "backtranslation_fn", "=", "(", "\n", "lambda", "sample", ":", "generator", ".", "generate", "(", "[", "self", ".", "model", "]", ",", "sample", ")", "\n", ")", ",", "\n", "output_collater", "=", "TransformEosDataset", "(", "\n", "dataset", "=", "tgt_dataset", ",", "\n", "eos", "=", "self", ".", "tgt_dict", ".", "eos", "(", ")", ",", "\n", "# if we remove eos from the input src, then we need to add it", "\n", "# back to the output tgt", "\n", "append_eos_to_tgt", "=", "remove_eos_from_input_src", ",", "\n", "remove_eos_from_src", "=", "remove_eos_from_output_src", ",", "\n", ")", ".", "collater", ",", "\n", "cuda", "=", "self", ".", "cuda", ",", "\n", ")", "\n", "dataloader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "backtranslation_dataset", ",", "\n", "batch_size", "=", "2", ",", "\n", "collate_fn", "=", "backtranslation_dataset", ".", "collater", ",", "\n", ")", "\n", "backtranslation_batch_result", "=", "next", "(", "iter", "(", "dataloader", ")", ")", "\n", "\n", "eos", ",", "pad", ",", "w1", ",", "w2", "=", "self", ".", "tgt_dict", ".", "eos", "(", ")", ",", "self", ".", "tgt_dict", ".", "pad", "(", ")", ",", "self", ".", "w1", ",", "self", ".", "w2", "\n", "\n", "# Note that we sort by src_lengths and add left padding, so actually", "\n", "# ids will look like: [1, 0]", "\n", "expected_src", "=", "torch", ".", "LongTensor", "(", "[", "[", "w1", ",", "w2", ",", "w1", ",", "eos", "]", ",", "[", "pad", ",", "pad", ",", "w1", ",", "eos", "]", "]", ")", "\n", "if", "remove_eos_from_output_src", ":", "\n", "            ", "expected_src", "=", "expected_src", "[", ":", ",", ":", "-", "1", "]", "\n", "", "expected_tgt", "=", "torch", ".", "LongTensor", "(", "[", "[", "w1", ",", "w2", ",", "eos", "]", ",", "[", "w1", ",", "w2", ",", "eos", "]", "]", ")", "\n", "generated_src", "=", "backtranslation_batch_result", "[", "\"net_input\"", "]", "[", "\"src_tokens\"", "]", "\n", "tgt_tokens", "=", "backtranslation_batch_result", "[", "\"target\"", "]", "\n", "\n", "self", ".", "assertTensorEqual", "(", "expected_src", ",", "generated_src", ")", "\n", "self", ".", "assertTensorEqual", "(", "expected_tgt", ",", "tgt_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_backtranslation_dataset.TestBacktranslationDataset.test_backtranslation_dataset_no_eos_in_output_src": [[95, 98], ["test_backtranslation_dataset.TestBacktranslationDataset._backtranslation_dataset_helper"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_backtranslation_dataset.TestBacktranslationDataset._backtranslation_dataset_helper"], ["", "def", "test_backtranslation_dataset_no_eos_in_output_src", "(", "self", ")", ":", "\n", "        ", "self", ".", "_backtranslation_dataset_helper", "(", "\n", "remove_eos_from_input_src", "=", "False", ",", "remove_eos_from_output_src", "=", "True", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_backtranslation_dataset.TestBacktranslationDataset.test_backtranslation_dataset_with_eos_in_output_src": [[100, 103], ["test_backtranslation_dataset.TestBacktranslationDataset._backtranslation_dataset_helper"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_backtranslation_dataset.TestBacktranslationDataset._backtranslation_dataset_helper"], ["", "def", "test_backtranslation_dataset_with_eos_in_output_src", "(", "self", ")", ":", "\n", "        ", "self", ".", "_backtranslation_dataset_helper", "(", "\n", "remove_eos_from_input_src", "=", "False", ",", "remove_eos_from_output_src", "=", "False", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_backtranslation_dataset.TestBacktranslationDataset.test_backtranslation_dataset_no_eos_in_input_src": [[105, 108], ["test_backtranslation_dataset.TestBacktranslationDataset._backtranslation_dataset_helper"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_backtranslation_dataset.TestBacktranslationDataset._backtranslation_dataset_helper"], ["", "def", "test_backtranslation_dataset_no_eos_in_input_src", "(", "self", ")", ":", "\n", "        ", "self", ".", "_backtranslation_dataset_helper", "(", "\n", "remove_eos_from_input_src", "=", "True", ",", "remove_eos_from_output_src", "=", "False", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_backtranslation_dataset.TestBacktranslationDataset.assertTensorEqual": [[110, 113], ["test_backtranslation_dataset.TestBacktranslationDataset.assertEqual", "test_backtranslation_dataset.TestBacktranslationDataset.assertEqual", "t1.size", "t2.size", "t1.ne().long().sum", "t1.ne().long", "t1.ne"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "assertTensorEqual", "(", "self", ",", "t1", ",", "t2", ")", ":", "\n", "        ", "self", ".", "assertEqual", "(", "t1", ".", "size", "(", ")", ",", "t2", ".", "size", "(", ")", ",", "\"size mismatch\"", ")", "\n", "self", ".", "assertEqual", "(", "t1", ".", "ne", "(", "t2", ")", ".", "long", "(", ")", ".", "sum", "(", ")", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_utils.TestUtils.test_convert_padding_direction": [[15, 42], ["torch.LongTensor", "torch.LongTensor", "test_utils.TestUtils.assertAlmostEqual", "test_utils.TestUtils.assertAlmostEqual", "fairseq.utils.convert_padding_direction", "fairseq.utils.convert_padding_direction"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertAlmostEqual", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertAlmostEqual", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.convert_padding_direction", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.convert_padding_direction"], ["    ", "def", "test_convert_padding_direction", "(", "self", ")", ":", "\n", "        ", "pad", "=", "1", "\n", "left_pad", "=", "torch", ".", "LongTensor", "(", "[", "\n", "[", "2", ",", "3", ",", "4", ",", "5", ",", "6", "]", ",", "\n", "[", "1", ",", "7", ",", "8", ",", "9", ",", "10", "]", ",", "\n", "[", "1", ",", "1", ",", "1", ",", "11", ",", "12", "]", ",", "\n", "]", ")", "\n", "right_pad", "=", "torch", ".", "LongTensor", "(", "[", "\n", "[", "2", ",", "3", ",", "4", ",", "5", ",", "6", "]", ",", "\n", "[", "7", ",", "8", ",", "9", ",", "10", ",", "1", "]", ",", "\n", "[", "11", ",", "12", ",", "1", ",", "1", ",", "1", "]", ",", "\n", "]", ")", "\n", "\n", "self", ".", "assertAlmostEqual", "(", "\n", "right_pad", ",", "\n", "utils", ".", "convert_padding_direction", "(", "\n", "left_pad", ",", "\n", "pad", ",", "\n", "left_to_right", "=", "True", ",", "\n", ")", ",", "\n", ")", "\n", "self", ".", "assertAlmostEqual", "(", "\n", "left_pad", ",", "\n", "utils", ".", "convert_padding_direction", "(", "\n", "right_pad", ",", "\n", "pad", ",", "\n", "right_to_left", "=", "True", ",", "\n", ")", ",", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_utils.TestUtils.test_make_positions": [[45, 75], ["torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "test_utils.TestUtils.assertAlmostEqual", "test_utils.TestUtils.assertAlmostEqual", "fairseq.utils.make_positions", "fairseq.utils.make_positions"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertAlmostEqual", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertAlmostEqual", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.make_positions", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.make_positions"], ["", "def", "test_make_positions", "(", "self", ")", ":", "\n", "        ", "pad", "=", "1", "\n", "left_pad_input", "=", "torch", ".", "LongTensor", "(", "[", "\n", "[", "9", ",", "9", ",", "9", ",", "9", ",", "9", "]", ",", "\n", "[", "1", ",", "9", ",", "9", ",", "9", ",", "9", "]", ",", "\n", "[", "1", ",", "1", ",", "1", ",", "9", ",", "9", "]", ",", "\n", "]", ")", "\n", "left_pad_output", "=", "torch", ".", "LongTensor", "(", "[", "\n", "[", "2", ",", "3", ",", "4", ",", "5", ",", "6", "]", ",", "\n", "[", "1", ",", "2", ",", "3", ",", "4", ",", "5", "]", ",", "\n", "[", "1", ",", "1", ",", "1", ",", "2", ",", "3", "]", ",", "\n", "]", ")", "\n", "right_pad_input", "=", "torch", ".", "LongTensor", "(", "[", "\n", "[", "9", ",", "9", ",", "9", ",", "9", ",", "9", "]", ",", "\n", "[", "9", ",", "9", ",", "9", ",", "9", ",", "1", "]", ",", "\n", "[", "9", ",", "9", ",", "1", ",", "1", ",", "1", "]", ",", "\n", "]", ")", "\n", "right_pad_output", "=", "torch", ".", "LongTensor", "(", "[", "\n", "[", "2", ",", "3", ",", "4", ",", "5", ",", "6", "]", ",", "\n", "[", "2", ",", "3", ",", "4", ",", "5", ",", "1", "]", ",", "\n", "[", "2", ",", "3", ",", "1", ",", "1", ",", "1", "]", ",", "\n", "]", ")", "\n", "\n", "self", ".", "assertAlmostEqual", "(", "\n", "left_pad_output", ",", "\n", "utils", ".", "make_positions", "(", "left_pad_input", ",", "pad", ")", ",", "\n", ")", "\n", "self", ".", "assertAlmostEqual", "(", "\n", "right_pad_output", ",", "\n", "utils", ".", "make_positions", "(", "right_pad_input", ",", "pad", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_utils.TestUtils.assertAlmostEqual": [[77, 80], ["test_utils.TestUtils.assertEqual", "test_utils.TestUtils.assertLess", "t1.size", "t2.size", "fairseq.utils.item"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item"], ["", "def", "assertAlmostEqual", "(", "self", ",", "t1", ",", "t2", ")", ":", "\n", "        ", "self", ".", "assertEqual", "(", "t1", ".", "size", "(", ")", ",", "t2", ".", "size", "(", ")", ",", "\"size mismatch\"", ")", "\n", "self", ".", "assertLess", "(", "utils", ".", "item", "(", "(", "t1", "-", "t2", ")", ".", "abs", "(", ")", ".", "max", "(", ")", ")", ",", "1e-4", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_train.TestLoadCheckpoint.setUp": [[54, 68], ["unittest.mock.MagicMock", "unittest.mock.MagicMock", "unittest.mock.MagicMock", "unittest.mock.MagicMock", "unittest.mock.MagicMock", "unittest.mock.patch", "p.start", "test_train.TestLoadCheckpoint.patches.items"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.meters.StopwatchMeter.start"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "self", ".", "args_mock", "=", "MagicMock", "(", ")", "\n", "self", ".", "args_mock", ".", "optimizer_overrides", "=", "'{}'", "\n", "self", ".", "args_mock", ".", "reset_dataloader", "=", "False", "\n", "self", ".", "args_mock", ".", "reset_meters", "=", "False", "\n", "self", ".", "args_mock", ".", "reset_optimizer", "=", "False", "\n", "self", ".", "patches", "=", "{", "\n", "'os.makedirs'", ":", "MagicMock", "(", ")", ",", "\n", "'os.path.join'", ":", "MagicMock", "(", ")", ",", "\n", "'os.path.isfile'", ":", "MagicMock", "(", "return_value", "=", "True", ")", ",", "\n", "'os.path.isabs'", ":", "MagicMock", "(", "return_value", "=", "False", ")", ",", "\n", "}", "\n", "self", ".", "applied_patches", "=", "[", "patch", "(", "p", ",", "d", ")", "for", "p", ",", "d", "in", "self", ".", "patches", ".", "items", "(", ")", "]", "\n", "[", "p", ".", "start", "(", ")", "for", "p", "in", "self", ".", "applied_patches", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_train.TestLoadCheckpoint.test_load_partial_checkpoint": [[69, 98], ["contextlib.redirect_stdout", "test_train.get_trainer_and_epoch_itr", "unittest.mock.MagicMock", "fairseq.checkpoint_utils.load_checkpoint", "test_train.TestLoadCheckpoint.assertEqual", "test_train.TestLoadCheckpoint.assertEqual", "epoch_itr.next_epoch_itr", "test_train.TestLoadCheckpoint.assertEqual", "test_train.TestLoadCheckpoint.assertEqual", "test_train.TestLoadCheckpoint.assertEqual", "test_train.TestLoadCheckpoint.assertEqual", "range", "test_train.TestLoadCheckpoint.assertEqual", "test_train.TestLoadCheckpoint.assertTrue", "next", "test_train.TestLoadCheckpoint.assertFalse", "epoch_itr.next_epoch_itr", "test_train.TestLoadCheckpoint.assertTrue", "test_train.TestLoadCheckpoint.assertEqual", "test_train.TestLoadCheckpoint.assertEqual", "io.StringIO", "[].item", "next", "epoch_itr.next_epoch_itr.has_next", "epoch_itr.next_epoch_itr.has_next", "epoch_itr.next_epoch_itr.has_next", "next"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_train.get_trainer_and_epoch_itr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.checkpoint_utils.load_checkpoint", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.EpochBatchIterator.next_epoch_itr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.EpochBatchIterator.next_epoch_itr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.CountingIterator.has_next", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.CountingIterator.has_next", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.CountingIterator.has_next"], ["", "def", "test_load_partial_checkpoint", "(", "self", ")", ":", "\n", "\n", "        ", "with", "contextlib", ".", "redirect_stdout", "(", "StringIO", "(", ")", ")", ":", "\n", "            ", "trainer", ",", "epoch_itr", "=", "get_trainer_and_epoch_itr", "(", "2", ",", "150", ",", "200", ",", "50", ")", "\n", "trainer", ".", "get_train_iterator", "=", "MagicMock", "(", "return_value", "=", "epoch_itr", ")", "\n", "\n", "_", ",", "epoch_itr", "=", "checkpoint_utils", ".", "load_checkpoint", "(", "self", ".", "args_mock", ",", "trainer", ")", "\n", "\n", "self", ".", "assertEqual", "(", "epoch_itr", ".", "epoch", ",", "2", ")", "\n", "self", ".", "assertEqual", "(", "epoch_itr", ".", "iterations_in_epoch", ",", "50", ")", "\n", "\n", "itr", "=", "epoch_itr", ".", "next_epoch_itr", "(", "shuffle", "=", "False", ")", "\n", "self", ".", "assertEqual", "(", "epoch_itr", ".", "epoch", ",", "2", ")", "\n", "self", ".", "assertEqual", "(", "epoch_itr", ".", "iterations_in_epoch", ",", "50", ")", "\n", "\n", "self", ".", "assertEqual", "(", "next", "(", "itr", ")", "[", "'net_input'", "]", "[", "'src_tokens'", "]", "[", "0", "]", ".", "item", "(", ")", ",", "50", ")", "\n", "self", ".", "assertEqual", "(", "epoch_itr", ".", "iterations_in_epoch", ",", "51", ")", "\n", "\n", "for", "_", "in", "range", "(", "150", "-", "52", ")", ":", "\n", "                ", "next", "(", "itr", ")", "\n", "", "self", ".", "assertEqual", "(", "epoch_itr", ".", "iterations_in_epoch", ",", "149", ")", "\n", "self", ".", "assertTrue", "(", "itr", ".", "has_next", "(", ")", ")", "\n", "next", "(", "itr", ")", "\n", "self", ".", "assertFalse", "(", "itr", ".", "has_next", "(", ")", ")", "\n", "\n", "itr", "=", "epoch_itr", ".", "next_epoch_itr", "(", "shuffle", "=", "False", ")", "\n", "self", ".", "assertTrue", "(", "itr", ".", "has_next", "(", ")", ")", "\n", "self", ".", "assertEqual", "(", "epoch_itr", ".", "epoch", ",", "3", ")", "\n", "self", ".", "assertEqual", "(", "epoch_itr", ".", "iterations_in_epoch", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_train.TestLoadCheckpoint.test_load_full_checkpoint": [[99, 110], ["contextlib.redirect_stdout", "test_train.get_trainer_and_epoch_itr", "unittest.mock.MagicMock", "fairseq.checkpoint_utils.load_checkpoint", "epoch_itr.next_epoch_itr", "test_train.TestLoadCheckpoint.assertEqual", "test_train.TestLoadCheckpoint.assertEqual", "test_train.TestLoadCheckpoint.assertEqual", "io.StringIO", "[].item", "next"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_train.get_trainer_and_epoch_itr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.checkpoint_utils.load_checkpoint", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.EpochBatchIterator.next_epoch_itr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item"], ["", "", "def", "test_load_full_checkpoint", "(", "self", ")", ":", "\n", "        ", "with", "contextlib", ".", "redirect_stdout", "(", "StringIO", "(", ")", ")", ":", "\n", "            ", "trainer", ",", "epoch_itr", "=", "get_trainer_and_epoch_itr", "(", "2", ",", "150", ",", "300", ",", "150", ")", "\n", "trainer", ".", "get_train_iterator", "=", "MagicMock", "(", "return_value", "=", "epoch_itr", ")", "\n", "\n", "_", ",", "epoch_itr", "=", "checkpoint_utils", ".", "load_checkpoint", "(", "self", ".", "args_mock", ",", "trainer", ")", "\n", "itr", "=", "epoch_itr", ".", "next_epoch_itr", "(", "shuffle", "=", "False", ")", "\n", "\n", "self", ".", "assertEqual", "(", "epoch_itr", ".", "epoch", ",", "3", ")", "\n", "self", ".", "assertEqual", "(", "epoch_itr", ".", "iterations_in_epoch", ",", "0", ")", "\n", "self", ".", "assertEqual", "(", "next", "(", "itr", ")", "[", "'net_input'", "]", "[", "'src_tokens'", "]", "[", "0", "]", ".", "item", "(", ")", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_train.TestLoadCheckpoint.test_load_no_checkpoint": [[111, 123], ["contextlib.redirect_stdout", "test_train.get_trainer_and_epoch_itr", "unittest.mock.MagicMock", "fairseq.checkpoint_utils.load_checkpoint", "epoch_itr.next_epoch_itr", "test_train.TestLoadCheckpoint.assertEqual", "test_train.TestLoadCheckpoint.assertEqual", "test_train.TestLoadCheckpoint.assertEqual", "io.StringIO", "[].item", "next"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_train.get_trainer_and_epoch_itr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.checkpoint_utils.load_checkpoint", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.EpochBatchIterator.next_epoch_itr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item"], ["", "", "def", "test_load_no_checkpoint", "(", "self", ")", ":", "\n", "        ", "with", "contextlib", ".", "redirect_stdout", "(", "StringIO", "(", ")", ")", ":", "\n", "            ", "trainer", ",", "epoch_itr", "=", "get_trainer_and_epoch_itr", "(", "0", ",", "150", ",", "0", ",", "0", ")", "\n", "trainer", ".", "get_train_iterator", "=", "MagicMock", "(", "return_value", "=", "epoch_itr", ")", "\n", "self", ".", "patches", "[", "'os.path.isfile'", "]", ".", "return_value", "=", "False", "\n", "\n", "_", ",", "epoch_itr", "=", "checkpoint_utils", ".", "load_checkpoint", "(", "self", ".", "args_mock", ",", "trainer", ")", "\n", "itr", "=", "epoch_itr", ".", "next_epoch_itr", "(", "shuffle", "=", "False", ")", "\n", "\n", "self", ".", "assertEqual", "(", "epoch_itr", ".", "epoch", ",", "1", ")", "\n", "self", ".", "assertEqual", "(", "epoch_itr", ".", "iterations_in_epoch", ",", "0", ")", "\n", "self", ".", "assertEqual", "(", "next", "(", "itr", ")", "[", "'net_input'", "]", "[", "'src_tokens'", "]", "[", "0", "]", ".", "item", "(", ")", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_train.TestLoadCheckpoint.tearDown": [[124, 126], ["unittest.mock.patch.stopall"], "methods", ["None"], ["", "", "def", "tearDown", "(", "self", ")", ":", "\n", "        ", "patch", ".", "stopall", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_train.mock_trainer": [[16, 27], ["unittest.mock.MagicMock"], "function", ["None"], ["def", "mock_trainer", "(", "epoch", ",", "num_updates", ",", "iterations_in_epoch", ")", ":", "\n", "    ", "trainer", "=", "MagicMock", "(", ")", "\n", "trainer", ".", "load_checkpoint", ".", "return_value", "=", "{", "\n", "'train_iterator'", ":", "{", "\n", "'epoch'", ":", "epoch", ",", "\n", "'iterations_in_epoch'", ":", "iterations_in_epoch", ",", "\n", "'shuffle'", ":", "False", ",", "\n", "}", ",", "\n", "}", "\n", "trainer", ".", "get_num_updates", ".", "return_value", "=", "num_updates", "\n", "return", "trainer", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_train.mock_dict": [[29, 35], ["unittest.mock.MagicMock"], "function", ["None"], ["", "def", "mock_dict", "(", ")", ":", "\n", "    ", "d", "=", "MagicMock", "(", ")", "\n", "d", ".", "pad", ".", "return_value", "=", "1", "\n", "d", ".", "eos", ".", "return_value", "=", "2", "\n", "d", ".", "unk", ".", "return_value", "=", "3", "\n", "return", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_train.get_trainer_and_epoch_itr": [[37, 50], ["torch.LongTensor().view", "fairseq.data.TokenBlockDataset", "test_train.mock_trainer", "fairseq.data.LanguagePairDataset", "fairseq.data.EpochBatchIterator", "test_train.mock_dict", "torch.LongTensor", "list", "torch.LongTensor().view.size", "range", "range"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_train.mock_trainer", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_train.mock_dict", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "get_trainer_and_epoch_itr", "(", "epoch", ",", "epoch_size", ",", "num_updates", ",", "iterations_in_epoch", ")", ":", "\n", "    ", "tokens", "=", "torch", ".", "LongTensor", "(", "list", "(", "range", "(", "epoch_size", ")", ")", ")", ".", "view", "(", "1", ",", "-", "1", ")", "\n", "tokens_ds", "=", "data", ".", "TokenBlockDataset", "(", "\n", "tokens", ",", "sizes", "=", "[", "tokens", ".", "size", "(", "-", "1", ")", "]", ",", "block_size", "=", "1", ",", "pad", "=", "0", ",", "eos", "=", "1", ",", "include_targets", "=", "False", ",", "\n", ")", "\n", "trainer", "=", "mock_trainer", "(", "epoch", ",", "num_updates", ",", "iterations_in_epoch", ")", "\n", "dataset", "=", "data", ".", "LanguagePairDataset", "(", "tokens_ds", ",", "tokens_ds", ".", "sizes", ",", "mock_dict", "(", ")", ",", "shuffle", "=", "False", ")", "\n", "epoch_itr", "=", "data", ".", "EpochBatchIterator", "(", "\n", "dataset", "=", "dataset", ",", "\n", "collate_fn", "=", "dataset", ".", "collater", ",", "\n", "batch_sampler", "=", "[", "[", "i", "]", "for", "i", "in", "range", "(", "epoch_size", ")", "]", ",", "\n", ")", "\n", "return", "trainer", ",", "epoch_itr", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.TestTranslation.test_fconv": [[27, 34], ["contextlib.redirect_stdout", "io.StringIO", "tempfile.TemporaryDirectory", "test_binaries.create_dummy_data", "test_binaries.preprocess_translation_data", "test_binaries.train_translation_model", "test_binaries.generate_main"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.create_dummy_data", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.preprocess_translation_data", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.train_translation_model", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.generate_main"], ["    ", "def", "test_fconv", "(", "self", ")", ":", "\n", "        ", "with", "contextlib", ".", "redirect_stdout", "(", "StringIO", "(", ")", ")", ":", "\n", "            ", "with", "tempfile", ".", "TemporaryDirectory", "(", "'test_fconv'", ")", "as", "data_dir", ":", "\n", "                ", "create_dummy_data", "(", "data_dir", ")", "\n", "preprocess_translation_data", "(", "data_dir", ")", "\n", "train_translation_model", "(", "data_dir", ",", "'fconv_iwslt_de_en'", ")", "\n", "generate_main", "(", "data_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.TestTranslation.test_raw": [[35, 42], ["contextlib.redirect_stdout", "io.StringIO", "tempfile.TemporaryDirectory", "test_binaries.create_dummy_data", "test_binaries.preprocess_translation_data", "test_binaries.train_translation_model", "test_binaries.generate_main"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.create_dummy_data", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.preprocess_translation_data", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.train_translation_model", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.generate_main"], ["", "", "", "def", "test_raw", "(", "self", ")", ":", "\n", "        ", "with", "contextlib", ".", "redirect_stdout", "(", "StringIO", "(", ")", ")", ":", "\n", "            ", "with", "tempfile", ".", "TemporaryDirectory", "(", "'test_fconv_raw'", ")", "as", "data_dir", ":", "\n", "                ", "create_dummy_data", "(", "data_dir", ")", "\n", "preprocess_translation_data", "(", "data_dir", ",", "[", "'--dataset-impl'", ",", "'raw'", "]", ")", "\n", "train_translation_model", "(", "data_dir", ",", "'fconv_iwslt_de_en'", ",", "[", "'--dataset-impl'", ",", "'raw'", "]", ")", "\n", "generate_main", "(", "data_dir", ",", "[", "'--dataset-impl'", ",", "'raw'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.TestTranslation.test_fp16": [[43, 50], ["contextlib.redirect_stdout", "io.StringIO", "tempfile.TemporaryDirectory", "test_binaries.create_dummy_data", "test_binaries.preprocess_translation_data", "test_binaries.train_translation_model", "test_binaries.generate_main"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.create_dummy_data", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.preprocess_translation_data", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.train_translation_model", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.generate_main"], ["", "", "", "def", "test_fp16", "(", "self", ")", ":", "\n", "        ", "with", "contextlib", ".", "redirect_stdout", "(", "StringIO", "(", ")", ")", ":", "\n", "            ", "with", "tempfile", ".", "TemporaryDirectory", "(", "'test_fp16'", ")", "as", "data_dir", ":", "\n", "                ", "create_dummy_data", "(", "data_dir", ")", "\n", "preprocess_translation_data", "(", "data_dir", ")", "\n", "train_translation_model", "(", "data_dir", ",", "'fconv_iwslt_de_en'", ",", "[", "'--fp16'", "]", ")", "\n", "generate_main", "(", "data_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.TestTranslation.test_memory_efficient_fp16": [[51, 58], ["contextlib.redirect_stdout", "io.StringIO", "tempfile.TemporaryDirectory", "test_binaries.create_dummy_data", "test_binaries.preprocess_translation_data", "test_binaries.train_translation_model", "test_binaries.generate_main"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.create_dummy_data", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.preprocess_translation_data", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.train_translation_model", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.generate_main"], ["", "", "", "def", "test_memory_efficient_fp16", "(", "self", ")", ":", "\n", "        ", "with", "contextlib", ".", "redirect_stdout", "(", "StringIO", "(", ")", ")", ":", "\n", "            ", "with", "tempfile", ".", "TemporaryDirectory", "(", "'test_memory_efficient_fp16'", ")", "as", "data_dir", ":", "\n", "                ", "create_dummy_data", "(", "data_dir", ")", "\n", "preprocess_translation_data", "(", "data_dir", ")", "\n", "train_translation_model", "(", "data_dir", ",", "'fconv_iwslt_de_en'", ",", "[", "'--memory-efficient-fp16'", "]", ")", "\n", "generate_main", "(", "data_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.TestTranslation.test_update_freq": [[59, 66], ["contextlib.redirect_stdout", "io.StringIO", "tempfile.TemporaryDirectory", "test_binaries.create_dummy_data", "test_binaries.preprocess_translation_data", "test_binaries.train_translation_model", "test_binaries.generate_main"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.create_dummy_data", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.preprocess_translation_data", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.train_translation_model", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.generate_main"], ["", "", "", "def", "test_update_freq", "(", "self", ")", ":", "\n", "        ", "with", "contextlib", ".", "redirect_stdout", "(", "StringIO", "(", ")", ")", ":", "\n", "            ", "with", "tempfile", ".", "TemporaryDirectory", "(", "'test_update_freq'", ")", "as", "data_dir", ":", "\n", "                ", "create_dummy_data", "(", "data_dir", ")", "\n", "preprocess_translation_data", "(", "data_dir", ")", "\n", "train_translation_model", "(", "data_dir", ",", "'fconv_iwslt_de_en'", ",", "[", "'--update-freq'", ",", "'3'", "]", ")", "\n", "generate_main", "(", "data_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.TestTranslation.test_max_positions": [[67, 86], ["contextlib.redirect_stdout", "io.StringIO", "tempfile.TemporaryDirectory", "test_binaries.create_dummy_data", "test_binaries.preprocess_translation_data", "test_binaries.TestTranslation.assertTrue", "test_binaries.train_translation_model", "test_binaries.generate_main", "test_binaries.TestTranslation.assertRaises", "test_binaries.train_translation_model", "test_binaries.TestTranslation.assertRaises", "test_binaries.generate_main", "str"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.create_dummy_data", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.preprocess_translation_data", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.train_translation_model", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.generate_main", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.train_translation_model", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.generate_main"], ["", "", "", "def", "test_max_positions", "(", "self", ")", ":", "\n", "        ", "with", "contextlib", ".", "redirect_stdout", "(", "StringIO", "(", ")", ")", ":", "\n", "            ", "with", "tempfile", ".", "TemporaryDirectory", "(", "'test_max_positions'", ")", "as", "data_dir", ":", "\n", "                ", "create_dummy_data", "(", "data_dir", ")", "\n", "preprocess_translation_data", "(", "data_dir", ")", "\n", "with", "self", ".", "assertRaises", "(", "Exception", ")", "as", "context", ":", "\n", "                    ", "train_translation_model", "(", "\n", "data_dir", ",", "'fconv_iwslt_de_en'", ",", "[", "'--max-target-positions'", ",", "'5'", "]", ",", "\n", ")", "\n", "", "self", ".", "assertTrue", "(", "\n", "'skip this example with --skip-invalid-size-inputs-valid-test'", "in", "str", "(", "context", ".", "exception", ")", "\n", ")", "\n", "train_translation_model", "(", "\n", "data_dir", ",", "'fconv_iwslt_de_en'", ",", "\n", "[", "'--max-target-positions'", ",", "'5'", ",", "'--skip-invalid-size-inputs-valid-test'", "]", ",", "\n", ")", "\n", "with", "self", ".", "assertRaises", "(", "Exception", ")", "as", "context", ":", "\n", "                    ", "generate_main", "(", "data_dir", ")", "\n", "", "generate_main", "(", "data_dir", ",", "[", "'--skip-invalid-size-inputs-valid-test'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.TestTranslation.test_generation": [[87, 112], ["contextlib.redirect_stdout", "io.StringIO", "tempfile.TemporaryDirectory", "test_binaries.create_dummy_data", "test_binaries.preprocess_translation_data", "test_binaries.train_translation_model", "test_binaries.generate_main", "test_binaries.generate_main", "test_binaries.generate_main", "test_binaries.generate_main"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.create_dummy_data", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.preprocess_translation_data", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.train_translation_model", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.generate_main", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.generate_main", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.generate_main", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.generate_main"], ["", "", "", "def", "test_generation", "(", "self", ")", ":", "\n", "        ", "with", "contextlib", ".", "redirect_stdout", "(", "StringIO", "(", ")", ")", ":", "\n", "            ", "with", "tempfile", ".", "TemporaryDirectory", "(", "'test_sampling'", ")", "as", "data_dir", ":", "\n", "                ", "create_dummy_data", "(", "data_dir", ")", "\n", "preprocess_translation_data", "(", "data_dir", ")", "\n", "train_translation_model", "(", "data_dir", ",", "'fconv_iwslt_de_en'", ")", "\n", "generate_main", "(", "data_dir", ",", "[", "\n", "'--sampling'", ",", "\n", "'--temperature'", ",", "'2'", ",", "\n", "'--beam'", ",", "'2'", ",", "\n", "'--nbest'", ",", "'2'", ",", "\n", "]", ")", "\n", "generate_main", "(", "data_dir", ",", "[", "\n", "'--sampling'", ",", "\n", "'--sampling-topk'", ",", "'3'", ",", "\n", "'--beam'", ",", "'2'", ",", "\n", "'--nbest'", ",", "'2'", ",", "\n", "]", ")", "\n", "generate_main", "(", "data_dir", ",", "[", "\n", "'--sampling'", ",", "\n", "'--sampling-topp'", ",", "'0.2'", ",", "\n", "'--beam'", ",", "'2'", ",", "\n", "'--nbest'", ",", "'2'", ",", "\n", "]", ")", "\n", "generate_main", "(", "data_dir", ",", "[", "'--prefix-size'", ",", "'2'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.TestTranslation.test_lstm": [[113, 126], ["contextlib.redirect_stdout", "io.StringIO", "tempfile.TemporaryDirectory", "test_binaries.create_dummy_data", "test_binaries.preprocess_translation_data", "test_binaries.train_translation_model", "test_binaries.generate_main"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.create_dummy_data", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.preprocess_translation_data", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.train_translation_model", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.generate_main"], ["", "", "", "def", "test_lstm", "(", "self", ")", ":", "\n", "        ", "with", "contextlib", ".", "redirect_stdout", "(", "StringIO", "(", ")", ")", ":", "\n", "            ", "with", "tempfile", ".", "TemporaryDirectory", "(", "'test_lstm'", ")", "as", "data_dir", ":", "\n", "                ", "create_dummy_data", "(", "data_dir", ")", "\n", "preprocess_translation_data", "(", "data_dir", ")", "\n", "train_translation_model", "(", "data_dir", ",", "'lstm_wiseman_iwslt_de_en'", ",", "[", "\n", "'--encoder-layers'", ",", "'2'", ",", "\n", "'--decoder-layers'", ",", "'2'", ",", "\n", "'--encoder-embed-dim'", ",", "'8'", ",", "\n", "'--decoder-embed-dim'", ",", "'8'", ",", "\n", "'--decoder-out-embed-dim'", ",", "'8'", ",", "\n", "]", ")", "\n", "generate_main", "(", "data_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.TestTranslation.test_lstm_bidirectional": [[127, 142], ["contextlib.redirect_stdout", "io.StringIO", "tempfile.TemporaryDirectory", "test_binaries.create_dummy_data", "test_binaries.preprocess_translation_data", "test_binaries.train_translation_model", "test_binaries.generate_main"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.create_dummy_data", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.preprocess_translation_data", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.train_translation_model", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.generate_main"], ["", "", "", "def", "test_lstm_bidirectional", "(", "self", ")", ":", "\n", "        ", "with", "contextlib", ".", "redirect_stdout", "(", "StringIO", "(", ")", ")", ":", "\n", "            ", "with", "tempfile", ".", "TemporaryDirectory", "(", "'test_lstm_bidirectional'", ")", "as", "data_dir", ":", "\n", "                ", "create_dummy_data", "(", "data_dir", ")", "\n", "preprocess_translation_data", "(", "data_dir", ")", "\n", "train_translation_model", "(", "data_dir", ",", "'lstm'", ",", "[", "\n", "'--encoder-layers'", ",", "'2'", ",", "\n", "'--encoder-bidirectional'", ",", "\n", "'--encoder-hidden-size'", ",", "'16'", ",", "\n", "'--encoder-embed-dim'", ",", "'8'", ",", "\n", "'--decoder-embed-dim'", ",", "'8'", ",", "\n", "'--decoder-out-embed-dim'", ",", "'8'", ",", "\n", "'--decoder-layers'", ",", "'2'", ",", "\n", "]", ")", "\n", "generate_main", "(", "data_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.TestTranslation.test_transformer": [[143, 155], ["contextlib.redirect_stdout", "io.StringIO", "tempfile.TemporaryDirectory", "test_binaries.create_dummy_data", "test_binaries.preprocess_translation_data", "test_binaries.train_translation_model", "test_binaries.generate_main"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.create_dummy_data", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.preprocess_translation_data", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.train_translation_model", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.generate_main"], ["", "", "", "def", "test_transformer", "(", "self", ")", ":", "\n", "        ", "with", "contextlib", ".", "redirect_stdout", "(", "StringIO", "(", ")", ")", ":", "\n", "            ", "with", "tempfile", ".", "TemporaryDirectory", "(", "'test_transformer'", ")", "as", "data_dir", ":", "\n", "                ", "create_dummy_data", "(", "data_dir", ")", "\n", "preprocess_translation_data", "(", "data_dir", ")", "\n", "train_translation_model", "(", "data_dir", ",", "'transformer_iwslt_de_en'", ",", "[", "\n", "'--encoder-layers'", ",", "'2'", ",", "\n", "'--decoder-layers'", ",", "'2'", ",", "\n", "'--encoder-embed-dim'", ",", "'8'", ",", "\n", "'--decoder-embed-dim'", ",", "'8'", ",", "\n", "]", ")", "\n", "generate_main", "(", "data_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.TestTranslation.test_lightconv": [[156, 168], ["contextlib.redirect_stdout", "io.StringIO", "tempfile.TemporaryDirectory", "test_binaries.create_dummy_data", "test_binaries.preprocess_translation_data", "test_binaries.train_translation_model", "test_binaries.generate_main"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.create_dummy_data", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.preprocess_translation_data", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.train_translation_model", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.generate_main"], ["", "", "", "def", "test_lightconv", "(", "self", ")", ":", "\n", "        ", "with", "contextlib", ".", "redirect_stdout", "(", "StringIO", "(", ")", ")", ":", "\n", "            ", "with", "tempfile", ".", "TemporaryDirectory", "(", "'test_lightconv'", ")", "as", "data_dir", ":", "\n", "                ", "create_dummy_data", "(", "data_dir", ")", "\n", "preprocess_translation_data", "(", "data_dir", ")", "\n", "train_translation_model", "(", "data_dir", ",", "'lightconv_iwslt_de_en'", ",", "[", "\n", "'--encoder-conv-type'", ",", "'lightweight'", ",", "\n", "'--decoder-conv-type'", ",", "'lightweight'", ",", "\n", "'--encoder-embed-dim'", ",", "'8'", ",", "\n", "'--decoder-embed-dim'", ",", "'8'", ",", "\n", "]", ")", "\n", "generate_main", "(", "data_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.TestTranslation.test_dynamicconv": [[169, 181], ["contextlib.redirect_stdout", "io.StringIO", "tempfile.TemporaryDirectory", "test_binaries.create_dummy_data", "test_binaries.preprocess_translation_data", "test_binaries.train_translation_model", "test_binaries.generate_main"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.create_dummy_data", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.preprocess_translation_data", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.train_translation_model", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.generate_main"], ["", "", "", "def", "test_dynamicconv", "(", "self", ")", ":", "\n", "        ", "with", "contextlib", ".", "redirect_stdout", "(", "StringIO", "(", ")", ")", ":", "\n", "            ", "with", "tempfile", ".", "TemporaryDirectory", "(", "'test_dynamicconv'", ")", "as", "data_dir", ":", "\n", "                ", "create_dummy_data", "(", "data_dir", ")", "\n", "preprocess_translation_data", "(", "data_dir", ")", "\n", "train_translation_model", "(", "data_dir", ",", "'lightconv_iwslt_de_en'", ",", "[", "\n", "'--encoder-conv-type'", ",", "'dynamic'", ",", "\n", "'--decoder-conv-type'", ",", "'dynamic'", ",", "\n", "'--encoder-embed-dim'", ",", "'8'", ",", "\n", "'--decoder-embed-dim'", ",", "'8'", ",", "\n", "]", ")", "\n", "generate_main", "(", "data_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.TestTranslation.test_mixture_of_experts": [[182, 203], ["contextlib.redirect_stdout", "io.StringIO", "tempfile.TemporaryDirectory", "test_binaries.create_dummy_data", "test_binaries.preprocess_translation_data", "test_binaries.train_translation_model", "test_binaries.generate_main"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.create_dummy_data", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.preprocess_translation_data", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.train_translation_model", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.generate_main"], ["", "", "", "def", "test_mixture_of_experts", "(", "self", ")", ":", "\n", "        ", "with", "contextlib", ".", "redirect_stdout", "(", "StringIO", "(", ")", ")", ":", "\n", "            ", "with", "tempfile", ".", "TemporaryDirectory", "(", "'test_moe'", ")", "as", "data_dir", ":", "\n", "                ", "create_dummy_data", "(", "data_dir", ")", "\n", "preprocess_translation_data", "(", "data_dir", ")", "\n", "train_translation_model", "(", "data_dir", ",", "'transformer_iwslt_de_en'", ",", "[", "\n", "'--task'", ",", "'translation_moe'", ",", "\n", "'--method'", ",", "'hMoElp'", ",", "\n", "'--mean-pool-gating-network'", ",", "\n", "'--num-experts'", ",", "'3'", ",", "\n", "'--encoder-layers'", ",", "'2'", ",", "\n", "'--decoder-layers'", ",", "'2'", ",", "\n", "'--encoder-embed-dim'", ",", "'8'", ",", "\n", "'--decoder-embed-dim'", ",", "'8'", ",", "\n", "]", ")", "\n", "generate_main", "(", "data_dir", ",", "[", "\n", "'--task'", ",", "'translation_moe'", ",", "\n", "'--method'", ",", "'hMoElp'", ",", "\n", "'--mean-pool-gating-network'", ",", "\n", "'--num-experts'", ",", "'3'", ",", "\n", "'--gen-expert'", ",", "'0'", "\n", "]", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.TestStories.test_fconv_self_att_wp": [[208, 237], ["contextlib.redirect_stdout", "io.StringIO", "tempfile.TemporaryDirectory", "test_binaries.create_dummy_data", "test_binaries.preprocess_translation_data", "test_binaries.train_translation_model", "test_binaries.generate_main", "os.rename", "config.extend", "test_binaries.train_translation_model", "os.path.join", "os.path.join", "os.path.join", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.create_dummy_data", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.preprocess_translation_data", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.train_translation_model", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.generate_main", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.train_translation_model"], ["    ", "def", "test_fconv_self_att_wp", "(", "self", ")", ":", "\n", "        ", "with", "contextlib", ".", "redirect_stdout", "(", "StringIO", "(", ")", ")", ":", "\n", "            ", "with", "tempfile", ".", "TemporaryDirectory", "(", "'test_fconv_self_att_wp'", ")", "as", "data_dir", ":", "\n", "                ", "create_dummy_data", "(", "data_dir", ")", "\n", "preprocess_translation_data", "(", "data_dir", ")", "\n", "config", "=", "[", "\n", "'--encoder-layers'", ",", "'[(128, 3)] * 2'", ",", "\n", "'--decoder-layers'", ",", "'[(128, 3)] * 2'", ",", "\n", "'--decoder-attention'", ",", "'True'", ",", "\n", "'--encoder-attention'", ",", "'False'", ",", "\n", "'--gated-attention'", ",", "'True'", ",", "\n", "'--self-attention'", ",", "'True'", ",", "\n", "'--project-input'", ",", "'True'", ",", "\n", "'--encoder-embed-dim'", ",", "'8'", ",", "\n", "'--decoder-embed-dim'", ",", "'8'", ",", "\n", "'--decoder-out-embed-dim'", ",", "'8'", ",", "\n", "'--multihead-self-attention-nheads'", ",", "'2'", "\n", "]", "\n", "train_translation_model", "(", "data_dir", ",", "'fconv_self_att_wp'", ",", "config", ")", "\n", "generate_main", "(", "data_dir", ")", "\n", "\n", "# fusion model", "\n", "os", ".", "rename", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'checkpoint_last.pt'", ")", ",", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'pretrained.pt'", ")", ")", "\n", "config", ".", "extend", "(", "[", "\n", "'--pretrained'", ",", "'True'", ",", "\n", "'--pretrained-checkpoint'", ",", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'pretrained.pt'", ")", ",", "\n", "'--save-dir'", ",", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'fusion_model'", ")", ",", "\n", "]", ")", "\n", "train_translation_model", "(", "data_dir", ",", "'fconv_self_att_wp'", ",", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.TestLanguageModeling.test_fconv_lm": [[241, 253], ["contextlib.redirect_stdout", "io.StringIO", "tempfile.TemporaryDirectory", "test_binaries.create_dummy_data", "test_binaries.preprocess_lm_data", "test_binaries.train_language_model", "test_binaries.eval_lm_main"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.create_dummy_data", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.preprocess_lm_data", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.train_language_model", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.eval_lm_main"], ["    ", "def", "test_fconv_lm", "(", "self", ")", ":", "\n", "        ", "with", "contextlib", ".", "redirect_stdout", "(", "StringIO", "(", ")", ")", ":", "\n", "            ", "with", "tempfile", ".", "TemporaryDirectory", "(", "'test_fconv_lm'", ")", "as", "data_dir", ":", "\n", "                ", "create_dummy_data", "(", "data_dir", ")", "\n", "preprocess_lm_data", "(", "data_dir", ")", "\n", "train_language_model", "(", "data_dir", ",", "'fconv_lm'", ",", "[", "\n", "'--decoder-layers'", ",", "'[(850, 3)] * 2 + [(1024,4)]'", ",", "\n", "'--decoder-embed-dim'", ",", "'280'", ",", "\n", "'--optimizer'", ",", "'nag'", ",", "\n", "'--lr'", ",", "'0.1'", ",", "\n", "]", ")", "\n", "eval_lm_main", "(", "data_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.TestLanguageModeling.test_transformer_lm": [[254, 261], ["contextlib.redirect_stdout", "io.StringIO", "tempfile.TemporaryDirectory", "test_binaries.create_dummy_data", "test_binaries.preprocess_lm_data", "test_binaries.train_language_model", "test_binaries.eval_lm_main"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.create_dummy_data", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.preprocess_lm_data", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.train_language_model", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.eval_lm_main"], ["", "", "", "def", "test_transformer_lm", "(", "self", ")", ":", "\n", "        ", "with", "contextlib", ".", "redirect_stdout", "(", "StringIO", "(", ")", ")", ":", "\n", "            ", "with", "tempfile", ".", "TemporaryDirectory", "(", "'test_transformer_lm'", ")", "as", "data_dir", ":", "\n", "                ", "create_dummy_data", "(", "data_dir", ")", "\n", "preprocess_lm_data", "(", "data_dir", ")", "\n", "train_language_model", "(", "data_dir", ",", "'transformer_lm'", ",", "[", "'--add-bos-token'", "]", ")", "\n", "eval_lm_main", "(", "data_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.TestMaskedLanguageModel.test_legacy_masked_lm": [[265, 271], ["contextlib.redirect_stdout", "io.StringIO", "tempfile.TemporaryDirectory", "test_binaries.create_dummy_data", "test_binaries.preprocess_lm_data", "test_binaries.train_legacy_masked_language_model"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.create_dummy_data", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.preprocess_lm_data", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.train_legacy_masked_language_model"], ["    ", "def", "test_legacy_masked_lm", "(", "self", ")", ":", "\n", "        ", "with", "contextlib", ".", "redirect_stdout", "(", "StringIO", "(", ")", ")", ":", "\n", "            ", "with", "tempfile", ".", "TemporaryDirectory", "(", "\"test_legacy_mlm\"", ")", "as", "data_dir", ":", "\n", "                ", "create_dummy_data", "(", "data_dir", ")", "\n", "preprocess_lm_data", "(", "data_dir", ")", "\n", "train_legacy_masked_language_model", "(", "data_dir", ",", "\"masked_lm\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.TestMaskedLanguageModel._test_pretrained_masked_lm_for_translation": [[272, 323], ["contextlib.redirect_stdout", "io.StringIO", "tempfile.TemporaryDirectory", "test_binaries.create_dummy_data", "test_binaries.preprocess_lm_data", "test_binaries.train_legacy_masked_language_model", "tempfile.TemporaryDirectory", "test_binaries.create_dummy_data", "test_binaries.preprocess_translation_data", "test_binaries.train_translation_model"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.create_dummy_data", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.preprocess_lm_data", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.train_legacy_masked_language_model", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.create_dummy_data", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.preprocess_translation_data", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.train_translation_model"], ["", "", "", "def", "_test_pretrained_masked_lm_for_translation", "(", "self", ",", "learned_pos_emb", ",", "encoder_only", ")", ":", "\n", "        ", "with", "contextlib", ".", "redirect_stdout", "(", "StringIO", "(", ")", ")", ":", "\n", "            ", "with", "tempfile", ".", "TemporaryDirectory", "(", "\"test_mlm\"", ")", "as", "data_dir", ":", "\n", "                ", "create_dummy_data", "(", "data_dir", ")", "\n", "preprocess_lm_data", "(", "data_dir", ")", "\n", "train_legacy_masked_language_model", "(", "\n", "data_dir", ",", "\n", "arch", "=", "\"masked_lm\"", ",", "\n", "extra_args", "=", "(", "'--encoder-learned-pos'", ",", ")", "if", "learned_pos_emb", "else", "(", ")", "\n", ")", "\n", "with", "tempfile", ".", "TemporaryDirectory", "(", "\n", "\"test_mlm_translation\"", "\n", ")", "as", "translation_dir", ":", "\n", "                    ", "create_dummy_data", "(", "translation_dir", ")", "\n", "preprocess_translation_data", "(", "\n", "translation_dir", ",", "extra_flags", "=", "[", "\"--joined-dictionary\"", "]", "\n", ")", "\n", "# Train transformer with data_dir/checkpoint_last.pt", "\n", "train_translation_model", "(", "\n", "translation_dir", ",", "\n", "arch", "=", "\"transformer_from_pretrained_xlm\"", ",", "\n", "extra_flags", "=", "[", "\n", "\"--decoder-layers\"", ",", "\n", "\"1\"", ",", "\n", "\"--decoder-embed-dim\"", ",", "\n", "\"32\"", ",", "\n", "\"--decoder-attention-heads\"", ",", "\n", "\"1\"", ",", "\n", "\"--decoder-ffn-embed-dim\"", ",", "\n", "\"32\"", ",", "\n", "\"--encoder-layers\"", ",", "\n", "\"1\"", ",", "\n", "\"--encoder-embed-dim\"", ",", "\n", "\"32\"", ",", "\n", "\"--encoder-attention-heads\"", ",", "\n", "\"1\"", ",", "\n", "\"--encoder-ffn-embed-dim\"", ",", "\n", "\"32\"", ",", "\n", "\"--pretrained-xlm-checkpoint\"", ",", "\n", "\"{}/checkpoint_last.pt\"", ".", "format", "(", "data_dir", ")", ",", "\n", "\"--activation-fn\"", ",", "\n", "\"gelu\"", ",", "\n", "\"--max-source-positions\"", ",", "\n", "\"500\"", ",", "\n", "\"--max-target-positions\"", ",", "\n", "\"500\"", ",", "\n", "]", "+", "(", "\n", "[", "\"--encoder-learned-pos\"", ",", "\"--decoder-learned-pos\"", "]", "\n", "if", "learned_pos_emb", "else", "[", "]", "\n", ")", "+", "(", "[", "'--init-encoder-only'", "]", "if", "encoder_only", "else", "[", "]", ")", ",", "\n", "task", "=", "\"translation_from_pretrained_xlm\"", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.TestMaskedLanguageModel.test_pretrained_masked_lm_for_translation_learned_pos_emb": [[325, 327], ["test_binaries.TestMaskedLanguageModel._test_pretrained_masked_lm_for_translation"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.TestMaskedLanguageModel._test_pretrained_masked_lm_for_translation"], ["", "", "", "", "def", "test_pretrained_masked_lm_for_translation_learned_pos_emb", "(", "self", ")", ":", "\n", "        ", "self", ".", "_test_pretrained_masked_lm_for_translation", "(", "True", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.TestMaskedLanguageModel.test_pretrained_masked_lm_for_translation_sinusoidal_pos_emb": [[328, 330], ["test_binaries.TestMaskedLanguageModel._test_pretrained_masked_lm_for_translation"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.TestMaskedLanguageModel._test_pretrained_masked_lm_for_translation"], ["", "def", "test_pretrained_masked_lm_for_translation_sinusoidal_pos_emb", "(", "self", ")", ":", "\n", "        ", "self", ".", "_test_pretrained_masked_lm_for_translation", "(", "False", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.TestMaskedLanguageModel.test_pretrained_masked_lm_for_translation_encoder_only": [[331, 333], ["test_binaries.TestMaskedLanguageModel._test_pretrained_masked_lm_for_translation"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.TestMaskedLanguageModel._test_pretrained_masked_lm_for_translation"], ["", "def", "test_pretrained_masked_lm_for_translation_encoder_only", "(", "self", ")", ":", "\n", "        ", "self", ".", "_test_pretrained_masked_lm_for_translation", "(", "True", ",", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.TestCommonOptions.test_optimizers": [[400, 419], ["contextlib.redirect_stdout", "io.StringIO", "tempfile.TemporaryDirectory", "test_binaries.create_dummy_data", "test_binaries.preprocess_translation_data", "os.path.join", "os.path.exists", "test_binaries.train_translation_model", "test_binaries.generate_main", "os.remove"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.create_dummy_data", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.preprocess_translation_data", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.train_translation_model", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.generate_main"], ["    ", "def", "test_optimizers", "(", "self", ")", ":", "\n", "        ", "with", "contextlib", ".", "redirect_stdout", "(", "StringIO", "(", ")", ")", ":", "\n", "            ", "with", "tempfile", ".", "TemporaryDirectory", "(", "'test_optimizers'", ")", "as", "data_dir", ":", "\n", "# Use just a bit of data and tiny model to keep this test runtime reasonable", "\n", "                ", "create_dummy_data", "(", "data_dir", ",", "num_examples", "=", "10", ",", "maxlen", "=", "5", ")", "\n", "preprocess_translation_data", "(", "data_dir", ")", "\n", "optimizers", "=", "[", "'adafactor'", ",", "'adam'", ",", "'nag'", ",", "'adagrad'", ",", "'sgd'", ",", "'adadelta'", "]", "\n", "last_checkpoint", "=", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'checkpoint_last.pt'", ")", "\n", "for", "optimizer", "in", "optimizers", ":", "\n", "                    ", "if", "os", ".", "path", ".", "exists", "(", "last_checkpoint", ")", ":", "\n", "                        ", "os", ".", "remove", "(", "last_checkpoint", ")", "\n", "", "train_translation_model", "(", "data_dir", ",", "'lstm'", ",", "[", "\n", "'--required-batch-size-multiple'", ",", "'1'", ",", "\n", "'--encoder-layers'", ",", "'1'", ",", "\n", "'--encoder-hidden-size'", ",", "'32'", ",", "\n", "'--decoder-layers'", ",", "'1'", ",", "\n", "'--optimizer'", ",", "optimizer", ",", "\n", "]", ")", "\n", "generate_main", "(", "data_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.train_legacy_masked_language_model": [[335, 396], ["fairseq.options.get_training_parser", "fairseq.options.parse_args_and_arch", "train.main", "list"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.get_training_parser", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.parse_args_and_arch", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.main"], ["", "", "def", "train_legacy_masked_language_model", "(", "data_dir", ",", "arch", ",", "extra_args", "=", "(", ")", ")", ":", "\n", "    ", "train_parser", "=", "options", ".", "get_training_parser", "(", ")", "\n", "# TODO: langs should be in and out right?", "\n", "train_args", "=", "options", ".", "parse_args_and_arch", "(", "\n", "train_parser", ",", "\n", "[", "\n", "\"--task\"", ",", "\n", "\"cross_lingual_lm\"", ",", "\n", "data_dir", ",", "\n", "\"--arch\"", ",", "\n", "arch", ",", "\n", "# Optimizer args", "\n", "\"--optimizer\"", ",", "\n", "\"adam\"", ",", "\n", "\"--lr-scheduler\"", ",", "\n", "\"reduce_lr_on_plateau\"", ",", "\n", "\"--lr-shrink\"", ",", "\n", "\"0.5\"", ",", "\n", "\"--lr\"", ",", "\n", "\"0.0001\"", ",", "\n", "\"--min-lr\"", ",", "\n", "\"1e-09\"", ",", "\n", "# dropout, attention args", "\n", "\"--dropout\"", ",", "\n", "\"0.1\"", ",", "\n", "\"--attention-dropout\"", ",", "\n", "\"0.1\"", ",", "\n", "# MLM args", "\n", "\"--criterion\"", ",", "\n", "\"legacy_masked_lm_loss\"", ",", "\n", "\"--masked-lm-only\"", ",", "\n", "\"--monolingual-langs\"", ",", "\n", "\"in,out\"", ",", "\n", "\"--num-segment\"", ",", "\n", "\"5\"", ",", "\n", "# Transformer args: use a small transformer model for fast training", "\n", "\"--encoder-layers\"", ",", "\n", "\"1\"", ",", "\n", "\"--encoder-embed-dim\"", ",", "\n", "\"32\"", ",", "\n", "\"--encoder-attention-heads\"", ",", "\n", "\"1\"", ",", "\n", "\"--encoder-ffn-embed-dim\"", ",", "\n", "\"32\"", ",", "\n", "# Other training args", "\n", "\"--max-tokens\"", ",", "\n", "\"500\"", ",", "\n", "\"--tokens-per-sample\"", ",", "\n", "\"500\"", ",", "\n", "\"--save-dir\"", ",", "\n", "data_dir", ",", "\n", "\"--max-epoch\"", ",", "\n", "\"1\"", ",", "\n", "\"--no-progress-bar\"", ",", "\n", "\"--distributed-world-size\"", ",", "\n", "\"1\"", ",", "\n", "\"--dataset-impl\"", ",", "\n", "\"raw\"", ",", "\n", "]", "+", "list", "(", "extra_args", ")", ",", "\n", ")", "\n", "train", ".", "main", "(", "train_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.create_dummy_data": [[421, 440], ["test_binaries.create_dummy_data._create_dummy_data"], "function", ["None"], ["", "", "", "", "", "def", "create_dummy_data", "(", "data_dir", ",", "num_examples", "=", "1000", ",", "maxlen", "=", "20", ")", ":", "\n", "\n", "    ", "def", "_create_dummy_data", "(", "filename", ")", ":", "\n", "        ", "data", "=", "torch", ".", "rand", "(", "num_examples", "*", "maxlen", ")", "\n", "data", "=", "97", "+", "torch", ".", "floor", "(", "26", "*", "data", ")", ".", "int", "(", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "filename", ")", ",", "'w'", ")", "as", "h", ":", "\n", "            ", "offset", "=", "0", "\n", "for", "_", "in", "range", "(", "num_examples", ")", ":", "\n", "                ", "ex_len", "=", "random", ".", "randint", "(", "1", ",", "maxlen", ")", "\n", "ex_str", "=", "' '", ".", "join", "(", "map", "(", "chr", ",", "data", "[", "offset", ":", "offset", "+", "ex_len", "]", ")", ")", "\n", "print", "(", "ex_str", ",", "file", "=", "h", ")", "\n", "offset", "+=", "ex_len", "\n", "\n", "", "", "", "_create_dummy_data", "(", "'train.in'", ")", "\n", "_create_dummy_data", "(", "'train.out'", ")", "\n", "_create_dummy_data", "(", "'valid.in'", ")", "\n", "_create_dummy_data", "(", "'valid.out'", ")", "\n", "_create_dummy_data", "(", "'test.in'", ")", "\n", "_create_dummy_data", "(", "'test.out'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.preprocess_translation_data": [[442, 457], ["fairseq.options.get_preprocessing_parser", "options.get_preprocessing_parser.parse_args", "preprocess.main", "os.path.join", "os.path.join", "os.path.join"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.get_preprocessing_parser", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.main"], ["", "def", "preprocess_translation_data", "(", "data_dir", ",", "extra_flags", "=", "None", ")", ":", "\n", "    ", "preprocess_parser", "=", "options", ".", "get_preprocessing_parser", "(", ")", "\n", "preprocess_args", "=", "preprocess_parser", ".", "parse_args", "(", "\n", "[", "\n", "'--source-lang'", ",", "'in'", ",", "\n", "'--target-lang'", ",", "'out'", ",", "\n", "'--trainpref'", ",", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'train'", ")", ",", "\n", "'--validpref'", ",", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'valid'", ")", ",", "\n", "'--testpref'", ",", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'test'", ")", ",", "\n", "'--thresholdtgt'", ",", "'0'", ",", "\n", "'--thresholdsrc'", ",", "'0'", ",", "\n", "'--destdir'", ",", "data_dir", ",", "\n", "]", "+", "(", "extra_flags", "or", "[", "]", ")", ",", "\n", ")", "\n", "preprocess", ".", "main", "(", "preprocess_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.train_translation_model": [[459, 478], ["fairseq.options.get_training_parser", "fairseq.options.parse_args_and_arch", "train.main"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.get_training_parser", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.parse_args_and_arch", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.main"], ["", "def", "train_translation_model", "(", "data_dir", ",", "arch", ",", "extra_flags", "=", "None", ",", "task", "=", "'translation'", ")", ":", "\n", "    ", "train_parser", "=", "options", ".", "get_training_parser", "(", ")", "\n", "train_args", "=", "options", ".", "parse_args_and_arch", "(", "\n", "train_parser", ",", "\n", "[", "\n", "'--task'", ",", "task", ",", "\n", "data_dir", ",", "\n", "'--save-dir'", ",", "data_dir", ",", "\n", "'--arch'", ",", "arch", ",", "\n", "'--lr'", ",", "'0.05'", ",", "\n", "'--max-tokens'", ",", "'500'", ",", "\n", "'--max-epoch'", ",", "'1'", ",", "\n", "'--no-progress-bar'", ",", "\n", "'--distributed-world-size'", ",", "'1'", ",", "\n", "'--source-lang'", ",", "'in'", ",", "\n", "'--target-lang'", ",", "'out'", ",", "\n", "]", "+", "(", "extra_flags", "or", "[", "]", ")", ",", "\n", ")", "\n", "train", ".", "main", "(", "train_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.generate_main": [[480, 507], ["fairseq.options.get_generation_parser", "fairseq.options.parse_args_and_arch", "generate.main", "io.StringIO", "interactive.main", "os.path.join"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.get_generation_parser", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.parse_args_and_arch", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.main", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.main"], ["", "def", "generate_main", "(", "data_dir", ",", "extra_flags", "=", "None", ")", ":", "\n", "    ", "generate_parser", "=", "options", ".", "get_generation_parser", "(", ")", "\n", "generate_args", "=", "options", ".", "parse_args_and_arch", "(", "\n", "generate_parser", ",", "\n", "[", "\n", "data_dir", ",", "\n", "'--path'", ",", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'checkpoint_last.pt'", ")", ",", "\n", "'--beam'", ",", "'3'", ",", "\n", "'--batch-size'", ",", "'64'", ",", "\n", "'--max-len-b'", ",", "'5'", ",", "\n", "'--gen-subset'", ",", "'valid'", ",", "\n", "'--no-progress-bar'", ",", "\n", "'--print-alignment'", ",", "\n", "]", "+", "(", "extra_flags", "or", "[", "]", ")", ",", "\n", ")", "\n", "\n", "# evaluate model in batch mode", "\n", "generate", ".", "main", "(", "generate_args", ")", "\n", "\n", "# evaluate model interactively", "\n", "generate_args", ".", "buffer_size", "=", "0", "\n", "generate_args", ".", "input", "=", "'-'", "\n", "generate_args", ".", "max_sentences", "=", "None", "\n", "orig_stdin", "=", "sys", ".", "stdin", "\n", "sys", ".", "stdin", "=", "StringIO", "(", "'h e l l o\\n'", ")", "\n", "interactive", ".", "main", "(", "generate_args", ")", "\n", "sys", ".", "stdin", "=", "orig_stdin", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.preprocess_lm_data": [[509, 519], ["fairseq.options.get_preprocessing_parser", "options.get_preprocessing_parser.parse_args", "preprocess.main", "os.path.join", "os.path.join", "os.path.join"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.get_preprocessing_parser", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.main"], ["", "def", "preprocess_lm_data", "(", "data_dir", ")", ":", "\n", "    ", "preprocess_parser", "=", "options", ".", "get_preprocessing_parser", "(", ")", "\n", "preprocess_args", "=", "preprocess_parser", ".", "parse_args", "(", "[", "\n", "'--only-source'", ",", "\n", "'--trainpref'", ",", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'train.out'", ")", ",", "\n", "'--validpref'", ",", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'valid.out'", ")", ",", "\n", "'--testpref'", ",", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'test.out'", ")", ",", "\n", "'--destdir'", ",", "data_dir", ",", "\n", "]", ")", "\n", "preprocess", ".", "main", "(", "preprocess_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.train_language_model": [[521, 543], ["fairseq.options.get_training_parser", "fairseq.options.parse_args_and_arch", "train.main"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.get_training_parser", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.parse_args_and_arch", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.main"], ["", "def", "train_language_model", "(", "data_dir", ",", "arch", ",", "extra_flags", "=", "None", ")", ":", "\n", "    ", "train_parser", "=", "options", ".", "get_training_parser", "(", ")", "\n", "train_args", "=", "options", ".", "parse_args_and_arch", "(", "\n", "train_parser", ",", "\n", "[", "\n", "'--task'", ",", "'language_modeling'", ",", "\n", "data_dir", ",", "\n", "'--arch'", ",", "arch", ",", "\n", "'--optimizer'", ",", "'adam'", ",", "\n", "'--lr'", ",", "'0.0001'", ",", "\n", "'--criterion'", ",", "'adaptive_loss'", ",", "\n", "'--adaptive-softmax-cutoff'", ",", "'5,10,15'", ",", "\n", "'--max-tokens'", ",", "'500'", ",", "\n", "'--tokens-per-sample'", ",", "'500'", ",", "\n", "'--save-dir'", ",", "data_dir", ",", "\n", "'--max-epoch'", ",", "'1'", ",", "\n", "'--no-progress-bar'", ",", "\n", "'--distributed-world-size'", ",", "'1'", ",", "\n", "'--ddp-backend'", ",", "'no_c10d'", ",", "\n", "]", "+", "(", "extra_flags", "or", "[", "]", ")", ",", "\n", ")", "\n", "train", ".", "main", "(", "train_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_binaries.eval_lm_main": [[545, 556], ["fairseq.options.get_eval_lm_parser", "fairseq.options.parse_args_and_arch", "eval_lm.main", "os.path.join"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.get_eval_lm_parser", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.parse_args_and_arch", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.main"], ["", "def", "eval_lm_main", "(", "data_dir", ")", ":", "\n", "    ", "eval_lm_parser", "=", "options", ".", "get_eval_lm_parser", "(", ")", "\n", "eval_lm_args", "=", "options", ".", "parse_args_and_arch", "(", "\n", "eval_lm_parser", ",", "\n", "[", "\n", "data_dir", ",", "\n", "'--path'", ",", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'checkpoint_last.pt'", ")", ",", "\n", "'--no-progress-bar'", ",", "\n", "]", ",", "\n", ")", "\n", "eval_lm", ".", "main", "(", "eval_lm_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoTokens": [[18, 20], ["test_sequence_generator.TestSequenceGeneratorBase.assertTensorEqual", "torch.LongTensor"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.test_collaters.TestSeq2SeqCollator.assertTensorEqual"], ["    ", "def", "assertHypoTokens", "(", "self", ",", "hypo", ",", "tokens", ")", ":", "\n", "        ", "self", ".", "assertTensorEqual", "(", "hypo", "[", "'tokens'", "]", ",", "torch", ".", "LongTensor", "(", "tokens", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoScore": [[21, 29], ["torch.FloatTensor().log", "test_sequence_generator.TestSequenceGeneratorBase.assertAlmostEqual", "test_sequence_generator.TestSequenceGeneratorBase.assertEqual", "torch.FloatTensor().log.sum", "test_sequence_generator.TestSequenceGeneratorBase.assertLess", "torch.FloatTensor().log.numel", "hypo[].numel", "abs", "torch.FloatTensor", "torch.FloatTensor().log.numel"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertAlmostEqual"], ["", "def", "assertHypoScore", "(", "self", ",", "hypo", ",", "pos_probs", ",", "normalized", "=", "True", ",", "lenpen", "=", "1.", ")", ":", "\n", "        ", "pos_scores", "=", "torch", ".", "FloatTensor", "(", "pos_probs", ")", ".", "log", "(", ")", "\n", "self", ".", "assertAlmostEqual", "(", "hypo", "[", "'positional_scores'", "]", ",", "pos_scores", ")", "\n", "self", ".", "assertEqual", "(", "pos_scores", ".", "numel", "(", ")", ",", "hypo", "[", "'tokens'", "]", ".", "numel", "(", ")", ")", "\n", "score", "=", "pos_scores", ".", "sum", "(", ")", "\n", "if", "normalized", ":", "\n", "            ", "score", "/=", "pos_scores", ".", "numel", "(", ")", "**", "lenpen", "\n", "", "self", ".", "assertLess", "(", "abs", "(", "score", "-", "hypo", "[", "'score'", "]", ")", ",", "1e-6", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertAlmostEqual": [[30, 33], ["test_sequence_generator.TestSequenceGeneratorBase.assertEqual", "test_sequence_generator.TestSequenceGeneratorBase.assertLess", "t1.size", "t2.size"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "assertAlmostEqual", "(", "self", ",", "t1", ",", "t2", ")", ":", "\n", "        ", "self", ".", "assertEqual", "(", "t1", ".", "size", "(", ")", ",", "t2", ".", "size", "(", ")", ",", "\"size mismatch\"", ")", "\n", "self", ".", "assertLess", "(", "(", "t1", "-", "t2", ")", ".", "abs", "(", ")", ".", "max", "(", ")", ",", "1e-4", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertTensorEqual": [[34, 37], ["test_sequence_generator.TestSequenceGeneratorBase.assertEqual", "test_sequence_generator.TestSequenceGeneratorBase.assertEqual", "t1.size", "t2.size", "t1.ne().long().sum", "t1.ne().long", "t1.ne"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "assertTensorEqual", "(", "self", ",", "t1", ",", "t2", ")", ":", "\n", "        ", "self", ".", "assertEqual", "(", "t1", ".", "size", "(", ")", ",", "t2", ".", "size", "(", ")", ",", "\"size mismatch\"", ")", "\n", "self", ".", "assertEqual", "(", "t1", ".", "ne", "(", "t2", ")", ".", "long", "(", ")", ".", "sum", "(", ")", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGenerator.setUp": [[41, 48], ["tests.sequence_generator_setup"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.utils.sequence_generator_setup"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "self", ".", "tgt_dict", ",", "self", ".", "w1", ",", "self", ".", "w2", ",", "src_tokens", ",", "src_lengths", ",", "self", ".", "model", "=", "(", "\n", "test_utils", ".", "sequence_generator_setup", "(", ")", "\n", ")", "\n", "self", ".", "sample", "=", "{", "\n", "'net_input'", ":", "{", "\n", "'src_tokens'", ":", "src_tokens", ",", "'src_lengths'", ":", "src_lengths", ",", "\n", "}", ",", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGenerator.test_with_normalization": [[51, 67], ["fairseq.sequence_generator.SequenceGenerator", "fairseq.sequence_generator.SequenceGenerator.generate", "test_sequence_generator.TestSequenceGenerator.assertHypoTokens", "test_sequence_generator.TestSequenceGenerator.assertHypoScore", "test_sequence_generator.TestSequenceGenerator.assertHypoTokens", "test_sequence_generator.TestSequenceGenerator.assertHypoScore", "test_sequence_generator.TestSequenceGenerator.assertHypoTokens", "test_sequence_generator.TestSequenceGenerator.assertHypoScore", "test_sequence_generator.TestSequenceGenerator.assertHypoTokens", "test_sequence_generator.TestSequenceGenerator.assertHypoScore", "test_sequence_generator.TestSequenceGenerator.tgt_dict.eos"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.sequence_generator.SequenceGenerator.generate", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoTokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoScore", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoTokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoScore", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoTokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoScore", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoTokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoScore", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos"], ["", "def", "test_with_normalization", "(", "self", ")", ":", "\n", "        ", "generator", "=", "SequenceGenerator", "(", "self", ".", "tgt_dict", ",", "beam_size", "=", "2", ")", "\n", "hypos", "=", "generator", ".", "generate", "(", "[", "self", ".", "model", "]", ",", "self", ".", "sample", ")", "\n", "eos", ",", "w1", ",", "w2", "=", "self", ".", "tgt_dict", ".", "eos", "(", ")", ",", "self", ".", "w1", ",", "self", ".", "w2", "\n", "# sentence 1, beam 1", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "0", "]", "[", "0", "]", ",", "[", "w1", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "0", "]", "[", "0", "]", ",", "[", "0.9", ",", "1.0", "]", ")", "\n", "# sentence 1, beam 2", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "0", "]", "[", "1", "]", ",", "[", "w2", ",", "w1", ",", "w2", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "0", "]", "[", "1", "]", ",", "[", "0.1", ",", "0.9", ",", "0.9", ",", "1.0", "]", ")", "\n", "# sentence 2, beam 1", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "1", "]", "[", "0", "]", ",", "[", "w1", ",", "w2", ",", "w1", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "1", "]", "[", "0", "]", ",", "[", "0.7", ",", "0.4", ",", "0.4", ",", "1.0", "]", ")", "\n", "# sentence 2, beam 2", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "1", "]", "[", "1", "]", ",", "[", "w1", ",", "w2", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "1", "]", "[", "1", "]", ",", "[", "0.7", ",", "0.4", ",", "0.6", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGenerator.test_without_normalization": [[68, 86], ["fairseq.sequence_generator.SequenceGenerator", "fairseq.sequence_generator.SequenceGenerator.generate", "test_sequence_generator.TestSequenceGenerator.assertHypoTokens", "test_sequence_generator.TestSequenceGenerator.assertHypoScore", "test_sequence_generator.TestSequenceGenerator.assertHypoTokens", "test_sequence_generator.TestSequenceGenerator.assertHypoScore", "test_sequence_generator.TestSequenceGenerator.assertHypoTokens", "test_sequence_generator.TestSequenceGenerator.assertHypoScore", "test_sequence_generator.TestSequenceGenerator.assertHypoTokens", "test_sequence_generator.TestSequenceGenerator.assertHypoScore", "test_sequence_generator.TestSequenceGenerator.tgt_dict.eos"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.sequence_generator.SequenceGenerator.generate", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoTokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoScore", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoTokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoScore", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoTokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoScore", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoTokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoScore", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos"], ["", "def", "test_without_normalization", "(", "self", ")", ":", "\n", "# Sentence 1: unchanged from the normalized case", "\n", "# Sentence 2: beams swap order", "\n", "        ", "generator", "=", "SequenceGenerator", "(", "self", ".", "tgt_dict", ",", "beam_size", "=", "2", ",", "normalize_scores", "=", "False", ")", "\n", "hypos", "=", "generator", ".", "generate", "(", "[", "self", ".", "model", "]", ",", "self", ".", "sample", ")", "\n", "eos", ",", "w1", ",", "w2", "=", "self", ".", "tgt_dict", ".", "eos", "(", ")", ",", "self", ".", "w1", ",", "self", ".", "w2", "\n", "# sentence 1, beam 1", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "0", "]", "[", "0", "]", ",", "[", "w1", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "0", "]", "[", "0", "]", ",", "[", "0.9", ",", "1.0", "]", ",", "normalized", "=", "False", ")", "\n", "# sentence 1, beam 2", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "0", "]", "[", "1", "]", ",", "[", "w2", ",", "w1", ",", "w2", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "0", "]", "[", "1", "]", ",", "[", "0.1", ",", "0.9", ",", "0.9", ",", "1.0", "]", ",", "normalized", "=", "False", ")", "\n", "# sentence 2, beam 1", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "1", "]", "[", "0", "]", ",", "[", "w1", ",", "w2", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "1", "]", "[", "0", "]", ",", "[", "0.7", ",", "0.4", ",", "0.6", "]", ",", "normalized", "=", "False", ")", "\n", "# sentence 2, beam 2", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "1", "]", "[", "1", "]", ",", "[", "w1", ",", "w2", ",", "w1", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "1", "]", "[", "1", "]", ",", "[", "0.7", ",", "0.4", ",", "0.4", ",", "1.0", "]", ",", "normalized", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGenerator.test_with_lenpen_favoring_short_hypos": [[87, 104], ["fairseq.sequence_generator.SequenceGenerator", "fairseq.sequence_generator.SequenceGenerator.generate", "test_sequence_generator.TestSequenceGenerator.assertHypoTokens", "test_sequence_generator.TestSequenceGenerator.assertHypoScore", "test_sequence_generator.TestSequenceGenerator.assertHypoTokens", "test_sequence_generator.TestSequenceGenerator.assertHypoScore", "test_sequence_generator.TestSequenceGenerator.assertHypoTokens", "test_sequence_generator.TestSequenceGenerator.assertHypoScore", "test_sequence_generator.TestSequenceGenerator.assertHypoTokens", "test_sequence_generator.TestSequenceGenerator.assertHypoScore", "test_sequence_generator.TestSequenceGenerator.tgt_dict.eos"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.sequence_generator.SequenceGenerator.generate", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoTokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoScore", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoTokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoScore", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoTokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoScore", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoTokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoScore", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos"], ["", "def", "test_with_lenpen_favoring_short_hypos", "(", "self", ")", ":", "\n", "        ", "lenpen", "=", "0.6", "\n", "generator", "=", "SequenceGenerator", "(", "self", ".", "tgt_dict", ",", "beam_size", "=", "2", ",", "len_penalty", "=", "lenpen", ")", "\n", "hypos", "=", "generator", ".", "generate", "(", "[", "self", ".", "model", "]", ",", "self", ".", "sample", ")", "\n", "eos", ",", "w1", ",", "w2", "=", "self", ".", "tgt_dict", ".", "eos", "(", ")", ",", "self", ".", "w1", ",", "self", ".", "w2", "\n", "# sentence 1, beam 1", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "0", "]", "[", "0", "]", ",", "[", "w1", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "0", "]", "[", "0", "]", ",", "[", "0.9", ",", "1.0", "]", ",", "lenpen", "=", "lenpen", ")", "\n", "# sentence 1, beam 2", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "0", "]", "[", "1", "]", ",", "[", "w2", ",", "w1", ",", "w2", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "0", "]", "[", "1", "]", ",", "[", "0.1", ",", "0.9", ",", "0.9", ",", "1.0", "]", ",", "lenpen", "=", "lenpen", ")", "\n", "# sentence 2, beam 1", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "1", "]", "[", "0", "]", ",", "[", "w1", ",", "w2", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "1", "]", "[", "0", "]", ",", "[", "0.7", ",", "0.4", ",", "0.6", "]", ",", "lenpen", "=", "lenpen", ")", "\n", "# sentence 2, beam 2", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "1", "]", "[", "1", "]", ",", "[", "w1", ",", "w2", ",", "w1", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "1", "]", "[", "1", "]", ",", "[", "0.7", ",", "0.4", ",", "0.4", ",", "1.0", "]", ",", "lenpen", "=", "lenpen", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGenerator.test_with_lenpen_favoring_long_hypos": [[105, 122], ["fairseq.sequence_generator.SequenceGenerator", "fairseq.sequence_generator.SequenceGenerator.generate", "test_sequence_generator.TestSequenceGenerator.assertHypoTokens", "test_sequence_generator.TestSequenceGenerator.assertHypoScore", "test_sequence_generator.TestSequenceGenerator.assertHypoTokens", "test_sequence_generator.TestSequenceGenerator.assertHypoScore", "test_sequence_generator.TestSequenceGenerator.assertHypoTokens", "test_sequence_generator.TestSequenceGenerator.assertHypoScore", "test_sequence_generator.TestSequenceGenerator.assertHypoTokens", "test_sequence_generator.TestSequenceGenerator.assertHypoScore", "test_sequence_generator.TestSequenceGenerator.tgt_dict.eos"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.sequence_generator.SequenceGenerator.generate", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoTokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoScore", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoTokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoScore", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoTokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoScore", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoTokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoScore", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos"], ["", "def", "test_with_lenpen_favoring_long_hypos", "(", "self", ")", ":", "\n", "        ", "lenpen", "=", "5.0", "\n", "generator", "=", "SequenceGenerator", "(", "self", ".", "tgt_dict", ",", "beam_size", "=", "2", ",", "len_penalty", "=", "lenpen", ")", "\n", "hypos", "=", "generator", ".", "generate", "(", "[", "self", ".", "model", "]", ",", "self", ".", "sample", ")", "\n", "eos", ",", "w1", ",", "w2", "=", "self", ".", "tgt_dict", ".", "eos", "(", ")", ",", "self", ".", "w1", ",", "self", ".", "w2", "\n", "# sentence 1, beam 1", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "0", "]", "[", "0", "]", ",", "[", "w2", ",", "w1", ",", "w2", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "0", "]", "[", "0", "]", ",", "[", "0.1", ",", "0.9", ",", "0.9", ",", "1.0", "]", ",", "lenpen", "=", "lenpen", ")", "\n", "# sentence 1, beam 2", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "0", "]", "[", "1", "]", ",", "[", "w1", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "0", "]", "[", "1", "]", ",", "[", "0.9", ",", "1.0", "]", ",", "lenpen", "=", "lenpen", ")", "\n", "# sentence 2, beam 1", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "1", "]", "[", "0", "]", ",", "[", "w1", ",", "w2", ",", "w1", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "1", "]", "[", "0", "]", ",", "[", "0.7", ",", "0.4", ",", "0.4", ",", "1.0", "]", ",", "lenpen", "=", "lenpen", ")", "\n", "# sentence 2, beam 2", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "1", "]", "[", "1", "]", ",", "[", "w1", ",", "w2", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "1", "]", "[", "1", "]", ",", "[", "0.7", ",", "0.4", ",", "0.6", "]", ",", "lenpen", "=", "lenpen", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGenerator.test_maxlen": [[123, 139], ["fairseq.sequence_generator.SequenceGenerator", "fairseq.sequence_generator.SequenceGenerator.generate", "test_sequence_generator.TestSequenceGenerator.assertHypoTokens", "test_sequence_generator.TestSequenceGenerator.assertHypoScore", "test_sequence_generator.TestSequenceGenerator.assertHypoTokens", "test_sequence_generator.TestSequenceGenerator.assertHypoScore", "test_sequence_generator.TestSequenceGenerator.assertHypoTokens", "test_sequence_generator.TestSequenceGenerator.assertHypoScore", "test_sequence_generator.TestSequenceGenerator.assertHypoTokens", "test_sequence_generator.TestSequenceGenerator.assertHypoScore", "test_sequence_generator.TestSequenceGenerator.tgt_dict.eos"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.sequence_generator.SequenceGenerator.generate", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoTokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoScore", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoTokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoScore", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoTokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoScore", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoTokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoScore", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos"], ["", "def", "test_maxlen", "(", "self", ")", ":", "\n", "        ", "generator", "=", "SequenceGenerator", "(", "self", ".", "tgt_dict", ",", "beam_size", "=", "2", ",", "max_len_b", "=", "2", ")", "\n", "hypos", "=", "generator", ".", "generate", "(", "[", "self", ".", "model", "]", ",", "self", ".", "sample", ")", "\n", "eos", ",", "w1", ",", "w2", "=", "self", ".", "tgt_dict", ".", "eos", "(", ")", ",", "self", ".", "w1", ",", "self", ".", "w2", "\n", "# sentence 1, beam 1", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "0", "]", "[", "0", "]", ",", "[", "w1", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "0", "]", "[", "0", "]", ",", "[", "0.9", ",", "1.0", "]", ")", "\n", "# sentence 1, beam 2", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "0", "]", "[", "1", "]", ",", "[", "w2", ",", "w2", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "0", "]", "[", "1", "]", ",", "[", "0.1", ",", "0.1", ",", "0.6", "]", ")", "\n", "# sentence 2, beam 1", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "1", "]", "[", "0", "]", ",", "[", "w1", ",", "w2", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "1", "]", "[", "0", "]", ",", "[", "0.7", ",", "0.4", ",", "0.6", "]", ")", "\n", "# sentence 2, beam 2", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "1", "]", "[", "1", "]", ",", "[", "w2", ",", "w2", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "1", "]", "[", "1", "]", ",", "[", "0.3", ",", "0.9", ",", "0.01", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestDiverseBeamSearch.setUp": [[143, 198], ["tests.dummy_dictionary", "test_sequence_generator.TestDiverseBeamSearch.assertEqual", "test_sequence_generator.TestDiverseBeamSearch.assertEqual", "test_sequence_generator.TestDiverseBeamSearch.assertEqual", "tests.dummy_dictionary.eos", "torch.LongTensor", "torch.LongTensor", "argparse.Namespace", "tests.TestTranslationTask.setup_task", "tests.TestTranslationTask.setup_task.build_model", "tests.dummy_dictionary.pad", "tests.dummy_dictionary.eos", "tests.dummy_dictionary.unk", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.utils.dummy_dictionary", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.sentence_prediction.SentencePredictionTask.setup_task", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv_lm.LightConvLanguageModel.build_model", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.unk"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "# construct dummy dictionary", "\n", "        ", "d", "=", "test_utils", ".", "dummy_dictionary", "(", "vocab_size", "=", "2", ")", "\n", "self", ".", "assertEqual", "(", "d", ".", "pad", "(", ")", ",", "1", ")", "\n", "self", ".", "assertEqual", "(", "d", ".", "eos", "(", ")", ",", "2", ")", "\n", "self", ".", "assertEqual", "(", "d", ".", "unk", "(", ")", ",", "3", ")", "\n", "self", ".", "eos", "=", "d", ".", "eos", "(", ")", "\n", "self", ".", "w1", "=", "4", "\n", "self", ".", "w2", "=", "5", "\n", "\n", "# construct source data", "\n", "self", ".", "src_tokens", "=", "torch", ".", "LongTensor", "(", "[", "\n", "[", "self", ".", "w1", ",", "self", ".", "w2", ",", "self", ".", "eos", "]", ",", "\n", "[", "self", ".", "w1", ",", "self", ".", "w2", ",", "self", ".", "eos", "]", ",", "\n", "]", ")", "\n", "self", ".", "src_lengths", "=", "torch", ".", "LongTensor", "(", "[", "2", ",", "2", "]", ")", "\n", "\n", "args", "=", "argparse", ".", "Namespace", "(", ")", "\n", "unk", "=", "0.", "\n", "args", ".", "beam_probs", "=", "[", "\n", "# step 0:", "\n", "torch", ".", "FloatTensor", "(", "[", "\n", "# eos      w1   w2", "\n", "# sentence 1:", "\n", "[", "0.0", ",", "unk", ",", "0.9", ",", "0.1", "]", ",", "# beam 1", "\n", "[", "0.0", ",", "unk", ",", "0.9", ",", "0.1", "]", ",", "# beam 2", "\n", "# sentence 2:", "\n", "[", "0.0", ",", "unk", ",", "0.7", ",", "0.3", "]", ",", "\n", "[", "0.0", ",", "unk", ",", "0.7", ",", "0.3", "]", ",", "\n", "]", ")", ",", "\n", "# step 1:", "\n", "torch", ".", "FloatTensor", "(", "[", "\n", "# eos      w1   w2", "\n", "# sentence 1:", "\n", "[", "0.0", ",", "unk", ",", "0.6", ",", "0.4", "]", ",", "\n", "[", "0.0", ",", "unk", ",", "0.6", ",", "0.4", "]", ",", "\n", "# sentence 2:", "\n", "[", "0.25", ",", "unk", ",", "0.35", ",", "0.4", "]", ",", "\n", "[", "0.25", ",", "unk", ",", "0.35", ",", "0.4", "]", ",", "\n", "]", ")", ",", "\n", "# step 2:", "\n", "torch", ".", "FloatTensor", "(", "[", "\n", "# eos      w1   w2", "\n", "# sentence 1:", "\n", "[", "1.0", ",", "unk", ",", "0.0", ",", "0.0", "]", ",", "\n", "[", "1.0", ",", "unk", ",", "0.0", ",", "0.0", "]", ",", "\n", "# sentence 2:", "\n", "[", "0.9", ",", "unk", ",", "0.1", ",", "0.0", "]", ",", "\n", "[", "0.9", ",", "unk", ",", "0.1", ",", "0.0", "]", ",", "\n", "]", ")", ",", "\n", "]", "\n", "\n", "task", "=", "test_utils", ".", "TestTranslationTask", ".", "setup_task", "(", "args", ",", "d", ",", "d", ")", "\n", "self", ".", "model", "=", "task", ".", "build_model", "(", "args", ")", "\n", "self", ".", "tgt_dict", "=", "task", ".", "target_dictionary", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestDiverseBeamSearch.test_diverse_beam_search": [[199, 218], ["fairseq.sequence_generator.SequenceGenerator", "fairseq.sequence_generator.SequenceGenerator.generate", "test_sequence_generator.TestDiverseBeamSearch.assertHypoTokens", "test_sequence_generator.TestDiverseBeamSearch.assertHypoScore", "test_sequence_generator.TestDiverseBeamSearch.assertHypoTokens", "test_sequence_generator.TestDiverseBeamSearch.assertHypoScore", "test_sequence_generator.TestDiverseBeamSearch.assertHypoTokens", "test_sequence_generator.TestDiverseBeamSearch.assertHypoScore", "test_sequence_generator.TestDiverseBeamSearch.assertHypoTokens", "test_sequence_generator.TestDiverseBeamSearch.assertHypoScore"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.sequence_generator.SequenceGenerator.generate", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoTokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoScore", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoTokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoScore", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoTokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoScore", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoTokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoScore"], ["", "def", "test_diverse_beam_search", "(", "self", ")", ":", "\n", "        ", "generator", "=", "SequenceGenerator", "(", "\n", "self", ".", "tgt_dict", ",", "beam_size", "=", "2", ",", "diverse_beam_groups", "=", "2", ",", "diverse_beam_strength", "=", "0.", ",", "\n", ")", "\n", "sample", "=", "{", "'net_input'", ":", "{", "'src_tokens'", ":", "self", ".", "src_tokens", ",", "'src_lengths'", ":", "self", ".", "src_lengths", "}", "}", "\n", "hypos", "=", "generator", ".", "generate", "(", "[", "self", ".", "model", "]", ",", "sample", ")", "\n", "eos", ",", "w1", ",", "w2", "=", "self", ".", "eos", ",", "self", ".", "w1", ",", "self", ".", "w2", "\n", "# sentence 1, beam 1", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "0", "]", "[", "0", "]", ",", "[", "w1", ",", "w1", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "0", "]", "[", "0", "]", ",", "[", "0.9", ",", "0.6", ",", "1.0", "]", ")", "\n", "# sentence 1, beam 2", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "0", "]", "[", "1", "]", ",", "[", "w1", ",", "w1", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "0", "]", "[", "1", "]", ",", "[", "0.9", ",", "0.6", ",", "1.0", "]", ")", "\n", "# sentence 2, beam 1", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "1", "]", "[", "0", "]", ",", "[", "w1", ",", "w2", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "1", "]", "[", "0", "]", ",", "[", "0.7", ",", "0.4", ",", "0.9", "]", ")", "\n", "# sentence 2, beam 2", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "1", "]", "[", "1", "]", ",", "[", "w1", ",", "w2", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "1", "]", "[", "1", "]", ",", "[", "0.7", ",", "0.4", ",", "0.9", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestTopPSamplingSearch.setUp": [[222, 280], ["tests.dummy_dictionary", "test_sequence_generator.TestTopPSamplingSearch.assertEqual", "test_sequence_generator.TestTopPSamplingSearch.assertEqual", "test_sequence_generator.TestTopPSamplingSearch.assertEqual", "tests.dummy_dictionary.eos", "torch.LongTensor", "torch.LongTensor", "argparse.Namespace", "tests.TestTranslationTask.setup_task", "tests.TestTranslationTask.setup_task.build_model", "tests.dummy_dictionary.pad", "tests.dummy_dictionary.eos", "tests.dummy_dictionary.unk", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.utils.dummy_dictionary", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.sentence_prediction.SentencePredictionTask.setup_task", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv_lm.LightConvLanguageModel.build_model", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.unk"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "# construct dummy dictionary", "\n", "        ", "d", "=", "test_utils", ".", "dummy_dictionary", "(", "vocab_size", "=", "2", ")", "\n", "self", ".", "assertEqual", "(", "d", ".", "pad", "(", ")", ",", "1", ")", "\n", "self", ".", "assertEqual", "(", "d", ".", "eos", "(", ")", ",", "2", ")", "\n", "self", ".", "assertEqual", "(", "d", ".", "unk", "(", ")", ",", "3", ")", "\n", "self", ".", "eos", "=", "d", ".", "eos", "(", ")", "\n", "self", ".", "w1", "=", "4", "\n", "self", ".", "w2", "=", "5", "\n", "\n", "# construct source data", "\n", "self", ".", "src_tokens", "=", "torch", ".", "LongTensor", "(", "[", "\n", "[", "self", ".", "w1", ",", "self", ".", "w2", ",", "self", ".", "eos", "]", ",", "\n", "[", "self", ".", "w1", ",", "self", ".", "w2", ",", "self", ".", "eos", "]", ",", "\n", "]", ")", "\n", "self", ".", "src_lengths", "=", "torch", ".", "LongTensor", "(", "[", "2", ",", "2", "]", ")", "\n", "\n", "args", "=", "argparse", ".", "Namespace", "(", ")", "\n", "unk", "=", "0.", "\n", "# The minimal probability of top 2 tokens.", "\n", "self", ".", "min_top2_prob", "=", "0.75", "\n", "# The minimal probability of the top 1 token.", "\n", "self", ".", "min_top1_prob", "=", "0.4", "\n", "\n", "w1_prob", "=", "self", ".", "min_top1_prob", "\n", "w2_prob", "=", "self", ".", "min_top2_prob", "-", "self", ".", "min_top1_prob", "\n", "eos_prob", "=", "1", "-", "self", ".", "min_top2_prob", "\n", "\n", "args", ".", "beam_probs", "=", "[", "\n", "# step 0:", "\n", "torch", ".", "FloatTensor", "(", "[", "\n", "# eos      w1   w2", "\n", "[", "0.0", ",", "unk", ",", "1.0", ",", "0.0", "]", ",", "\n", "[", "0.0", ",", "unk", ",", "1.0", ",", "0.0", "]", ",", "\n", "[", "0.0", ",", "unk", ",", "1.0", ",", "0.0", "]", ",", "\n", "[", "0.0", ",", "unk", ",", "1.0", ",", "0.0", "]", ",", "\n", "]", ")", ",", "\n", "# step 1:", "\n", "torch", ".", "FloatTensor", "(", "[", "\n", "# eos           w1       w2", "\n", "[", "eos_prob", ",", "unk", ",", "w1_prob", ",", "w2_prob", "]", ",", "\n", "[", "eos_prob", ",", "unk", ",", "w1_prob", ",", "w2_prob", "]", ",", "\n", "[", "eos_prob", ",", "unk", ",", "w1_prob", ",", "w2_prob", "]", ",", "\n", "[", "eos_prob", ",", "unk", ",", "w1_prob", ",", "w2_prob", "]", ",", "\n", "]", ")", ",", "\n", "# step 2:", "\n", "torch", ".", "FloatTensor", "(", "[", "\n", "# eos      w1   w2", "\n", "[", "1.0", ",", "unk", ",", "0.0", ",", "0.0", "]", ",", "\n", "[", "1.0", ",", "unk", ",", "0.0", ",", "0.0", "]", ",", "\n", "[", "1.0", ",", "unk", ",", "0.0", ",", "0.0", "]", ",", "\n", "[", "1.0", ",", "unk", ",", "0.0", ",", "0.0", "]", ",", "\n", "]", ")", ",", "\n", "]", "\n", "\n", "task", "=", "test_utils", ".", "TestTranslationTask", ".", "setup_task", "(", "args", ",", "d", ",", "d", ")", "\n", "self", ".", "model", "=", "task", ".", "build_model", "(", "args", ")", "\n", "self", ".", "tgt_dict", "=", "task", ".", "target_dictionary", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestTopPSamplingSearch.test_topp_sampling_search_low_prob": [[281, 309], ["fairseq.sequence_generator.SequenceGenerator", "fairseq.sequence_generator.SequenceGenerator.generate", "test_sequence_generator.TestTopPSamplingSearch.assertHypoTokens", "test_sequence_generator.TestTopPSamplingSearch.assertHypoScore", "test_sequence_generator.TestTopPSamplingSearch.assertHypoTokens", "test_sequence_generator.TestTopPSamplingSearch.assertHypoScore", "test_sequence_generator.TestTopPSamplingSearch.assertHypoTokens", "test_sequence_generator.TestTopPSamplingSearch.assertHypoScore", "test_sequence_generator.TestTopPSamplingSearch.assertHypoTokens", "test_sequence_generator.TestTopPSamplingSearch.assertHypoScore"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.sequence_generator.SequenceGenerator.generate", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoTokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoScore", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoTokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoScore", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoTokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoScore", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoTokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestSequenceGeneratorBase.assertHypoScore"], ["", "def", "test_topp_sampling_search_low_prob", "(", "self", ")", ":", "\n", "# Given a prob low enough to top-P sampling, we expect only the top", "\n", "# 1 token to be sampled, which always results in the same output.", "\n", "        ", "low_sampling_topp", "=", "self", ".", "min_top1_prob", "/", "2.0", "\n", "generator", "=", "SequenceGenerator", "(", "\n", "self", ".", "tgt_dict", ",", "beam_size", "=", "2", ",", "sampling", "=", "True", ",", "\n", "sampling_topp", "=", "low_sampling_topp", "\n", ")", "\n", "sample", "=", "{", "\n", "'net_input'", ":", "{", "\n", "'src_tokens'", ":", "self", ".", "src_tokens", ",", "\n", "'src_lengths'", ":", "self", ".", "src_lengths", "\n", "}", "\n", "}", "\n", "hypos", "=", "generator", ".", "generate", "(", "[", "self", ".", "model", "]", ",", "sample", ")", "\n", "eos", ",", "w1", "=", "self", ".", "eos", ",", "self", ".", "w1", "\n", "# sentence 1, beam 1", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "0", "]", "[", "0", "]", ",", "[", "w1", ",", "w1", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "0", "]", "[", "0", "]", ",", "[", "1.0", ",", "0.4", ",", "1.0", "]", ")", "\n", "# sentence 1, beam 2", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "0", "]", "[", "1", "]", ",", "[", "w1", ",", "w1", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "0", "]", "[", "1", "]", ",", "[", "1.0", ",", "0.4", ",", "1.0", "]", ")", "\n", "# sentence 2, beam 1", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "1", "]", "[", "0", "]", ",", "[", "w1", ",", "w1", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "1", "]", "[", "0", "]", ",", "[", "1.0", ",", "0.4", ",", "1.0", "]", ")", "\n", "# sentence 2, beam 2", "\n", "self", ".", "assertHypoTokens", "(", "hypos", "[", "1", "]", "[", "1", "]", ",", "[", "w1", ",", "w1", ",", "eos", "]", ")", "\n", "self", ".", "assertHypoScore", "(", "hypos", "[", "1", "]", "[", "1", "]", ",", "[", "1.0", ",", "0.4", ",", "1.0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestTopPSamplingSearch.test_topp_sampling_search_high_prob": [[310, 349], ["fairseq.sequence_generator.SequenceGenerator", "fairseq.sequence_generator.SequenceGenerator.generate", "test_sequence_generator.TestTopPSamplingSearch.assertTrue", "test_sequence_generator.TestTopPSamplingSearch.assertTrue", "test_sequence_generator.TestTopPSamplingSearch.assertTrue", "test_sequence_generator.TestTopPSamplingSearch.assertTrue", "test_sequence_generator.TestTopPSamplingSearch.assertTrue", "test_sequence_generator.TestTopPSamplingSearch.assertTrue", "test_sequence_generator.TestTopPSamplingSearch.assertTrue", "test_sequence_generator.TestTopPSamplingSearch.assertTrue", "test_sequence_generator.TestTopPSamplingSearch.hypoTokens", "test_sequence_generator.TestTopPSamplingSearch.hypoTokens", "test_sequence_generator.TestTopPSamplingSearch.hypoScore", "test_sequence_generator.TestTopPSamplingSearch.hypoScore", "test_sequence_generator.TestTopPSamplingSearch.hypoTokens", "test_sequence_generator.TestTopPSamplingSearch.hypoTokens", "test_sequence_generator.TestTopPSamplingSearch.hypoScore", "test_sequence_generator.TestTopPSamplingSearch.hypoScore", "test_sequence_generator.TestTopPSamplingSearch.hypoTokens", "test_sequence_generator.TestTopPSamplingSearch.hypoTokens", "test_sequence_generator.TestTopPSamplingSearch.hypoScore", "test_sequence_generator.TestTopPSamplingSearch.hypoScore", "test_sequence_generator.TestTopPSamplingSearch.hypoTokens", "test_sequence_generator.TestTopPSamplingSearch.hypoTokens", "test_sequence_generator.TestTopPSamplingSearch.hypoScore", "test_sequence_generator.TestTopPSamplingSearch.hypoScore"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.sequence_generator.SequenceGenerator.generate", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestTopPSamplingSearch.hypoTokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestTopPSamplingSearch.hypoTokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestTopPSamplingSearch.hypoScore", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestTopPSamplingSearch.hypoScore", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestTopPSamplingSearch.hypoTokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestTopPSamplingSearch.hypoTokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestTopPSamplingSearch.hypoScore", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestTopPSamplingSearch.hypoScore", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestTopPSamplingSearch.hypoTokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestTopPSamplingSearch.hypoTokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestTopPSamplingSearch.hypoScore", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestTopPSamplingSearch.hypoScore", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestTopPSamplingSearch.hypoTokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestTopPSamplingSearch.hypoTokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestTopPSamplingSearch.hypoScore", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestTopPSamplingSearch.hypoScore"], ["", "def", "test_topp_sampling_search_high_prob", "(", "self", ")", ":", "\n", "# Given a prob high enough to top-P sampling, any of the top 2", "\n", "# tokens could be sampled. This can cause different outputs.", "\n", "        ", "high_sampling_topp", "=", "(", "self", ".", "min_top1_prob", "+", "self", ".", "min_top2_prob", ")", "/", "2.0", "\n", "generator", "=", "SequenceGenerator", "(", "\n", "self", ".", "tgt_dict", ",", "beam_size", "=", "2", ",", "sampling", "=", "True", ",", "\n", "sampling_topp", "=", "high_sampling_topp", "\n", ")", "\n", "sample", "=", "{", "\n", "'net_input'", ":", "{", "\n", "'src_tokens'", ":", "self", ".", "src_tokens", ",", "\n", "'src_lengths'", ":", "self", ".", "src_lengths", "\n", "}", "\n", "}", "\n", "hypos", "=", "generator", ".", "generate", "(", "[", "self", ".", "model", "]", ",", "sample", ")", "\n", "eos", ",", "w1", ",", "w2", "=", "self", ".", "eos", ",", "self", ".", "w1", ",", "self", ".", "w2", "\n", "# sentence 1, beam 1", "\n", "self", ".", "assertTrue", "(", "self", ".", "hypoTokens", "(", "hypos", "[", "0", "]", "[", "0", "]", ",", "[", "w1", ",", "w1", ",", "eos", "]", ")", "or", "\n", "self", ".", "hypoTokens", "(", "hypos", "[", "0", "]", "[", "0", "]", ",", "[", "w1", ",", "w2", ",", "eos", "]", ")", ")", "\n", "self", ".", "assertTrue", "(", "self", ".", "hypoScore", "(", "hypos", "[", "0", "]", "[", "0", "]", ",", "[", "1.0", ",", "0.4", ",", "1.0", "]", ")", "or", "\n", "self", ".", "hypoScore", "(", "hypos", "[", "0", "]", "[", "0", "]", ",", "[", "1.0", ",", "0.35", ",", "1.0", "]", ")", ")", "\n", "\n", "# sentence 1, beam 2", "\n", "self", ".", "assertTrue", "(", "self", ".", "hypoTokens", "(", "hypos", "[", "0", "]", "[", "1", "]", ",", "[", "w1", ",", "w1", ",", "eos", "]", ")", "or", "\n", "self", ".", "hypoTokens", "(", "hypos", "[", "0", "]", "[", "1", "]", ",", "[", "w1", ",", "w2", ",", "eos", "]", ")", ")", "\n", "self", ".", "assertTrue", "(", "self", ".", "hypoScore", "(", "hypos", "[", "0", "]", "[", "1", "]", ",", "[", "1.0", ",", "0.4", ",", "1.0", "]", ")", "or", "\n", "self", ".", "hypoScore", "(", "hypos", "[", "0", "]", "[", "1", "]", ",", "[", "1.0", ",", "0.35", ",", "1.0", "]", ")", ")", "\n", "\n", "# sentence 2, beam 1", "\n", "self", ".", "assertTrue", "(", "self", ".", "hypoTokens", "(", "hypos", "[", "1", "]", "[", "0", "]", ",", "[", "w1", ",", "w1", ",", "eos", "]", ")", "or", "\n", "self", ".", "hypoTokens", "(", "hypos", "[", "1", "]", "[", "0", "]", ",", "[", "w1", ",", "w2", ",", "eos", "]", ")", ")", "\n", "self", ".", "assertTrue", "(", "self", ".", "hypoScore", "(", "hypos", "[", "1", "]", "[", "0", "]", ",", "[", "1.0", ",", "0.4", ",", "1.0", "]", ")", "or", "\n", "self", ".", "hypoScore", "(", "hypos", "[", "1", "]", "[", "0", "]", ",", "[", "1.0", ",", "0.35", ",", "1.0", "]", ")", ")", "\n", "\n", "# sentence 2, beam 2", "\n", "self", ".", "assertTrue", "(", "self", ".", "hypoTokens", "(", "hypos", "[", "1", "]", "[", "1", "]", ",", "[", "w1", ",", "w1", ",", "eos", "]", ")", "or", "\n", "self", ".", "hypoTokens", "(", "hypos", "[", "1", "]", "[", "1", "]", ",", "[", "w1", ",", "w2", ",", "eos", "]", ")", ")", "\n", "self", ".", "assertTrue", "(", "self", ".", "hypoScore", "(", "hypos", "[", "1", "]", "[", "1", "]", ",", "[", "1.0", ",", "0.4", ",", "1.0", "]", ")", "or", "\n", "self", ".", "hypoScore", "(", "hypos", "[", "1", "]", "[", "1", "]", ",", "[", "1.0", ",", "0.35", ",", "1.0", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestTopPSamplingSearch.hypoTokens": [[350, 352], ["test_sequence_generator.TestTopPSamplingSearch.tensorEqual", "torch.LongTensor"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestTopPSamplingSearch.tensorEqual"], ["", "def", "hypoTokens", "(", "self", ",", "hypo", ",", "tokens", ")", ":", "\n", "        ", "return", "self", ".", "tensorEqual", "(", "hypo", "[", "'tokens'", "]", ",", "torch", ".", "LongTensor", "(", "tokens", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestTopPSamplingSearch.hypoScore": [[353, 363], ["torch.FloatTensor().log", "torch.FloatTensor().log.sum", "test_sequence_generator.TestTopPSamplingSearch.almostEqual", "torch.FloatTensor().log.numel", "hypo[].numel", "abs", "torch.FloatTensor", "torch.FloatTensor().log.numel"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestTopPSamplingSearch.almostEqual"], ["", "def", "hypoScore", "(", "self", ",", "hypo", ",", "pos_probs", ",", "normalized", "=", "True", ",", "lenpen", "=", "1.", ")", ":", "\n", "        ", "pos_scores", "=", "torch", ".", "FloatTensor", "(", "pos_probs", ")", ".", "log", "(", ")", "\n", "if", "not", "self", ".", "almostEqual", "(", "hypo", "[", "'positional_scores'", "]", ",", "pos_scores", ")", ":", "\n", "            ", "return", "False", "\n", "", "if", "pos_scores", ".", "numel", "(", ")", "!=", "hypo", "[", "'tokens'", "]", ".", "numel", "(", ")", ":", "\n", "            ", "return", "False", "\n", "", "score", "=", "pos_scores", ".", "sum", "(", ")", "\n", "if", "normalized", ":", "\n", "            ", "score", "/=", "pos_scores", ".", "numel", "(", ")", "**", "lenpen", "\n", "", "return", "abs", "(", "score", "-", "hypo", "[", "'score'", "]", ")", "<", "1e-6", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestTopPSamplingSearch.almostEqual": [[364, 366], ["t1.size", "t2.size"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "almostEqual", "(", "self", ",", "t1", ",", "t2", ")", ":", "\n", "        ", "return", "t1", ".", "size", "(", ")", "==", "t2", ".", "size", "(", ")", "and", "(", "t1", "-", "t2", ")", ".", "abs", "(", ")", ".", "max", "(", ")", "<", "1e-4", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_sequence_generator.TestTopPSamplingSearch.tensorEqual": [[367, 369], ["t1.size", "t2.size", "t1.ne().long().sum", "t1.ne().long", "t1.ne"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "tensorEqual", "(", "self", ",", "t1", ",", "t2", ")", ":", "\n", "        ", "return", "t1", ".", "size", "(", ")", "==", "t2", ".", "size", "(", ")", "and", "t1", ".", "ne", "(", "t2", ")", ".", "long", "(", ")", ".", "sum", "(", ")", "==", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_average_checkpoints.ModelWithSharedParameter.__init__": [[22, 32], ["torch.nn.Module.__init__", "torch.nn.Embedding", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.ReLU"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.Embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "ModelWithSharedParameter", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "embedding", "=", "nn", ".", "Embedding", "(", "1000", ",", "200", ")", "\n", "self", ".", "FC1", "=", "nn", ".", "Linear", "(", "200", ",", "200", ")", "\n", "self", ".", "FC2", "=", "nn", ".", "Linear", "(", "200", ",", "200", ")", "\n", "# tie weight in FC2 to FC1", "\n", "self", ".", "FC2", ".", "weight", "=", "nn", ".", "Parameter", "(", "self", ".", "FC1", ".", "weight", ")", "\n", "self", ".", "FC2", ".", "bias", "=", "nn", ".", "Parameter", "(", "self", ".", "FC1", ".", "bias", ")", "\n", "\n", "self", ".", "relu", "=", "nn", ".", "ReLU", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_average_checkpoints.ModelWithSharedParameter.forward": [[33, 35], ["test_average_checkpoints.ModelWithSharedParameter.FC2", "test_average_checkpoints.ModelWithSharedParameter.FC1", "test_average_checkpoints.ModelWithSharedParameter.ReLU", "test_average_checkpoints.ModelWithSharedParameter.FC1"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "return", "self", ".", "FC2", "(", "self", ".", "ReLU", "(", "self", ".", "FC1", "(", "input", ")", ")", ")", "+", "self", ".", "FC1", "(", "input", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_average_checkpoints.TestAverageCheckpoints.test_average_checkpoints": [[38, 86], ["collections.OrderedDict", "collections.OrderedDict", "collections.OrderedDict", "tempfile.mkstemp", "tempfile.mkstemp", "torch.save", "torch.save", "os.close", "os.remove", "os.close", "os.remove", "zip", "collections.OrderedDict", "collections.OrderedDict", "scripts.average_checkpoints.average_checkpoints", "collections.OrderedDict.items", "output.items", "test_average_checkpoints.TestAverageCheckpoints.assertEqual", "numpy.testing.assert_allclose", "v_expected.numpy", "v_out.numpy", "torch.DoubleTensor", "torch.FloatTensor", "torch.IntTensor", "torch.DoubleTensor", "torch.FloatTensor", "torch.IntTensor", "torch.DoubleTensor", "torch.FloatTensor", "torch.IntTensor", "collections.OrderedDict.keys", "output.keys"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.save", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.save", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.GoldMiner.close", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.GoldMiner.close", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.average_checkpoints.average_checkpoints"], ["    ", "def", "test_average_checkpoints", "(", "self", ")", ":", "\n", "        ", "params_0", "=", "collections", ".", "OrderedDict", "(", "\n", "[", "\n", "(", "'a'", ",", "torch", ".", "DoubleTensor", "(", "[", "100.0", "]", ")", ")", ",", "\n", "(", "'b'", ",", "torch", ".", "FloatTensor", "(", "[", "[", "1.0", ",", "2.0", ",", "3.0", "]", ",", "[", "4.0", ",", "5.0", ",", "6.0", "]", "]", ")", ")", ",", "\n", "(", "'c'", ",", "torch", ".", "IntTensor", "(", "[", "7", ",", "8", ",", "9", "]", ")", ")", ",", "\n", "]", "\n", ")", "\n", "params_1", "=", "collections", ".", "OrderedDict", "(", "\n", "[", "\n", "(", "'a'", ",", "torch", ".", "DoubleTensor", "(", "[", "1.0", "]", ")", ")", ",", "\n", "(", "'b'", ",", "torch", ".", "FloatTensor", "(", "[", "[", "1.0", ",", "1.0", ",", "1.0", "]", ",", "[", "1.0", ",", "1.0", ",", "1.0", "]", "]", ")", ")", ",", "\n", "(", "'c'", ",", "torch", ".", "IntTensor", "(", "[", "2", ",", "2", ",", "2", "]", ")", ")", ",", "\n", "]", "\n", ")", "\n", "params_avg", "=", "collections", ".", "OrderedDict", "(", "\n", "[", "\n", "(", "'a'", ",", "torch", ".", "DoubleTensor", "(", "[", "50.5", "]", ")", ")", ",", "\n", "(", "'b'", ",", "torch", ".", "FloatTensor", "(", "[", "[", "1.0", ",", "1.5", ",", "2.0", "]", ",", "[", "2.5", ",", "3.0", ",", "3.5", "]", "]", ")", ")", ",", "\n", "# We expect truncation for integer division", "\n", "(", "'c'", ",", "torch", ".", "IntTensor", "(", "[", "4", ",", "5", ",", "5", "]", ")", ")", ",", "\n", "]", "\n", ")", "\n", "\n", "fd_0", ",", "path_0", "=", "tempfile", ".", "mkstemp", "(", ")", "\n", "fd_1", ",", "path_1", "=", "tempfile", ".", "mkstemp", "(", ")", "\n", "torch", ".", "save", "(", "collections", ".", "OrderedDict", "(", "[", "(", "'model'", ",", "params_0", ")", "]", ")", ",", "path_0", ")", "\n", "torch", ".", "save", "(", "collections", ".", "OrderedDict", "(", "[", "(", "'model'", ",", "params_1", ")", "]", ")", ",", "path_1", ")", "\n", "\n", "output", "=", "average_checkpoints", "(", "[", "path_0", ",", "path_1", "]", ")", "[", "'model'", "]", "\n", "\n", "os", ".", "close", "(", "fd_0", ")", "\n", "os", ".", "remove", "(", "path_0", ")", "\n", "os", ".", "close", "(", "fd_1", ")", "\n", "os", ".", "remove", "(", "path_1", ")", "\n", "\n", "for", "(", "k_expected", ",", "v_expected", ")", ",", "(", "k_out", ",", "v_out", ")", "in", "zip", "(", "\n", "params_avg", ".", "items", "(", ")", ",", "output", ".", "items", "(", ")", ")", ":", "\n", "            ", "self", ".", "assertEqual", "(", "\n", "k_expected", ",", "k_out", ",", "'Key mismatch - expected {} but found {}. '", "\n", "'(Expected list of keys: {} vs actual list of keys: {})'", ".", "format", "(", "\n", "k_expected", ",", "k_out", ",", "params_avg", ".", "keys", "(", ")", ",", "output", ".", "keys", "(", ")", "\n", ")", "\n", ")", "\n", "np", ".", "testing", ".", "assert_allclose", "(", "\n", "v_expected", ".", "numpy", "(", ")", ",", "\n", "v_out", ".", "numpy", "(", ")", ",", "\n", "err_msg", "=", "'Tensor value mismatch for key {}'", ".", "format", "(", "k_expected", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_average_checkpoints.TestAverageCheckpoints.test_average_checkpoints_with_shared_parameters": [[88, 141], ["tempfile.mkdtemp", "os.path.join", "test_average_checkpoints.TestAverageCheckpoints.test_average_checkpoints_with_shared_parameters._construct_model_with_shared_parameters"], "methods", ["None"], ["", "", "def", "test_average_checkpoints_with_shared_parameters", "(", "self", ")", ":", "\n", "\n", "        ", "def", "_construct_model_with_shared_parameters", "(", "path", ",", "value", ")", ":", "\n", "            ", "m", "=", "ModelWithSharedParameter", "(", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "FC1", ".", "weight", ",", "value", ")", "\n", "torch", ".", "save", "(", "\n", "{", "'model'", ":", "m", ".", "state_dict", "(", ")", "}", ",", "\n", "path", "\n", ")", "\n", "return", "m", "\n", "\n", "", "tmpdir", "=", "tempfile", ".", "mkdtemp", "(", ")", "\n", "paths", "=", "[", "]", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "tmpdir", ",", "\"m1.pt\"", ")", "\n", "m1", "=", "_construct_model_with_shared_parameters", "(", "path", ",", "1.0", ")", "\n", "paths", ".", "append", "(", "path", ")", "\n", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "tmpdir", ",", "\"m2.pt\"", ")", "\n", "m2", "=", "_construct_model_with_shared_parameters", "(", "path", ",", "2.0", ")", "\n", "paths", ".", "append", "(", "path", ")", "\n", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "tmpdir", ",", "\"m3.pt\"", ")", "\n", "m3", "=", "_construct_model_with_shared_parameters", "(", "path", ",", "3.0", ")", "\n", "paths", ".", "append", "(", "path", ")", "\n", "\n", "new_model", "=", "average_checkpoints", "(", "paths", ")", "\n", "self", ".", "assertTrue", "(", "\n", "torch", ".", "equal", "(", "\n", "new_model", "[", "'model'", "]", "[", "'embedding.weight'", "]", ",", "\n", "(", "m1", ".", "embedding", ".", "weight", "+", "\n", "m2", ".", "embedding", ".", "weight", "+", "\n", "m3", ".", "embedding", ".", "weight", ")", "/", "3.0", "\n", ")", "\n", ")", "\n", "\n", "self", ".", "assertTrue", "(", "\n", "torch", ".", "equal", "(", "\n", "new_model", "[", "'model'", "]", "[", "'FC1.weight'", "]", ",", "\n", "(", "m1", ".", "FC1", ".", "weight", "+", "\n", "m2", ".", "FC1", ".", "weight", "+", "\n", "m3", ".", "FC1", ".", "weight", ")", "/", "3.0", "\n", ")", "\n", ")", "\n", "\n", "self", ".", "assertTrue", "(", "\n", "torch", ".", "equal", "(", "\n", "new_model", "[", "'model'", "]", "[", "'FC2.weight'", "]", ",", "\n", "(", "m1", ".", "FC2", ".", "weight", "+", "\n", "m2", ".", "FC2", ".", "weight", "+", "\n", "m3", ".", "FC2", ".", "weight", ")", "/", "3.0", "\n", ")", "\n", ")", "\n", "shutil", ".", "rmtree", "(", "tmpdir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_token_block_dataset.TestTokenBlockDataset._build_dataset": [[17, 21], ["tests.TestDataset", "fairseq.data.TokenBlockDataset", "len"], "methods", ["None"], ["    ", "def", "_build_dataset", "(", "self", ",", "data", ",", "**", "kwargs", ")", ":", "\n", "        ", "sizes", "=", "[", "len", "(", "x", ")", "for", "x", "in", "data", "]", "\n", "underlying_ds", "=", "test_utils", ".", "TestDataset", "(", "data", ")", "\n", "return", "TokenBlockDataset", "(", "underlying_ds", ",", "sizes", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_token_block_dataset.TestTokenBlockDataset.test_eos_break_mode": [[22, 42], ["test_token_block_dataset.TestTokenBlockDataset._build_dataset", "test_token_block_dataset.TestTokenBlockDataset.assertEqual", "test_token_block_dataset.TestTokenBlockDataset.assertEqual", "test_token_block_dataset.TestTokenBlockDataset.assertEqual", "test_token_block_dataset.TestTokenBlockDataset._build_dataset", "test_token_block_dataset.TestTokenBlockDataset.assertEqual", "test_token_block_dataset.TestTokenBlockDataset.assertEqual", "test_token_block_dataset.TestTokenBlockDataset.assertEqual", "torch.tensor", "torch.tensor", "torch.tensor", "ds[].tolist", "ds[].tolist", "ds[].tolist", "torch.tensor", "torch.tensor", "torch.tensor", "ds[].tolist", "ds[].tolist", "ds[].tolist"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_token_block_dataset.TestTokenBlockDataset._build_dataset", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_token_block_dataset.TestTokenBlockDataset._build_dataset"], ["", "def", "test_eos_break_mode", "(", "self", ")", ":", "\n", "        ", "data", "=", "[", "\n", "torch", ".", "tensor", "(", "[", "5", ",", "4", ",", "3", ",", "2", ",", "1", "]", ",", "dtype", "=", "torch", ".", "long", ")", ",", "\n", "torch", ".", "tensor", "(", "[", "1", "]", ",", "dtype", "=", "torch", ".", "long", ")", ",", "\n", "torch", ".", "tensor", "(", "[", "8", ",", "7", ",", "6", ",", "1", "]", ",", "dtype", "=", "torch", ".", "long", ")", ",", "\n", "]", "\n", "ds", "=", "self", ".", "_build_dataset", "(", "data", ",", "block_size", "=", "None", ",", "pad", "=", "0", ",", "eos", "=", "1", ",", "break_mode", "=", "'eos'", ")", "\n", "self", ".", "assertEqual", "(", "ds", "[", "0", "]", ".", "tolist", "(", ")", ",", "[", "5", ",", "4", ",", "3", ",", "2", ",", "1", "]", ")", "\n", "self", ".", "assertEqual", "(", "ds", "[", "1", "]", ".", "tolist", "(", ")", ",", "[", "1", "]", ")", "\n", "self", ".", "assertEqual", "(", "ds", "[", "2", "]", ".", "tolist", "(", ")", ",", "[", "8", ",", "7", ",", "6", ",", "1", "]", ")", "\n", "\n", "data", "=", "[", "\n", "torch", ".", "tensor", "(", "[", "5", ",", "4", ",", "3", ",", "2", ",", "1", "]", ",", "dtype", "=", "torch", ".", "long", ")", ",", "\n", "torch", ".", "tensor", "(", "[", "8", ",", "7", ",", "6", ",", "1", "]", ",", "dtype", "=", "torch", ".", "long", ")", ",", "\n", "torch", ".", "tensor", "(", "[", "1", "]", ",", "dtype", "=", "torch", ".", "long", ")", ",", "\n", "]", "\n", "ds", "=", "self", ".", "_build_dataset", "(", "data", ",", "block_size", "=", "None", ",", "pad", "=", "0", ",", "eos", "=", "1", ",", "break_mode", "=", "'eos'", ")", "\n", "self", ".", "assertEqual", "(", "ds", "[", "0", "]", ".", "tolist", "(", ")", ",", "[", "5", ",", "4", ",", "3", ",", "2", ",", "1", "]", ")", "\n", "self", ".", "assertEqual", "(", "ds", "[", "1", "]", ".", "tolist", "(", ")", ",", "[", "8", ",", "7", ",", "6", ",", "1", "]", ")", "\n", "self", ".", "assertEqual", "(", "ds", "[", "2", "]", ".", "tolist", "(", ")", ",", "[", "1", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_token_block_dataset.TestTokenBlockDataset.test_block_break_mode": [[43, 54], ["test_token_block_dataset.TestTokenBlockDataset._build_dataset", "test_token_block_dataset.TestTokenBlockDataset.assertEqual", "test_token_block_dataset.TestTokenBlockDataset.assertEqual", "test_token_block_dataset.TestTokenBlockDataset.assertEqual", "test_token_block_dataset.TestTokenBlockDataset.assertEqual", "torch.tensor", "torch.tensor", "torch.tensor", "ds[].tolist", "ds[].tolist", "ds[].tolist", "ds[].tolist"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_token_block_dataset.TestTokenBlockDataset._build_dataset"], ["", "def", "test_block_break_mode", "(", "self", ")", ":", "\n", "        ", "data", "=", "[", "\n", "torch", ".", "tensor", "(", "[", "5", ",", "4", ",", "3", ",", "2", ",", "1", "]", ",", "dtype", "=", "torch", ".", "long", ")", ",", "\n", "torch", ".", "tensor", "(", "[", "8", ",", "7", ",", "6", ",", "1", "]", ",", "dtype", "=", "torch", ".", "long", ")", ",", "\n", "torch", ".", "tensor", "(", "[", "9", ",", "1", "]", ",", "dtype", "=", "torch", ".", "long", ")", ",", "\n", "]", "\n", "ds", "=", "self", ".", "_build_dataset", "(", "data", ",", "block_size", "=", "3", ",", "pad", "=", "0", ",", "eos", "=", "1", ",", "break_mode", "=", "'none'", ")", "\n", "self", ".", "assertEqual", "(", "ds", "[", "0", "]", ".", "tolist", "(", ")", ",", "[", "5", ",", "4", ",", "3", "]", ")", "\n", "self", ".", "assertEqual", "(", "ds", "[", "1", "]", ".", "tolist", "(", ")", ",", "[", "2", ",", "1", ",", "8", "]", ")", "\n", "self", ".", "assertEqual", "(", "ds", "[", "2", "]", ".", "tolist", "(", ")", ",", "[", "7", ",", "6", ",", "1", "]", ")", "\n", "self", ".", "assertEqual", "(", "ds", "[", "3", "]", ".", "tolist", "(", ")", ",", "[", "9", ",", "1", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_token_block_dataset.TestTokenBlockDataset.test_complete_break_mode": [[55, 75], ["test_token_block_dataset.TestTokenBlockDataset._build_dataset", "test_token_block_dataset.TestTokenBlockDataset.assertEqual", "test_token_block_dataset.TestTokenBlockDataset.assertEqual", "test_token_block_dataset.TestTokenBlockDataset._build_dataset", "test_token_block_dataset.TestTokenBlockDataset.assertEqual", "test_token_block_dataset.TestTokenBlockDataset.assertEqual", "test_token_block_dataset.TestTokenBlockDataset.assertEqual", "torch.tensor", "torch.tensor", "torch.tensor", "ds[].tolist", "ds[].tolist", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "ds[].tolist", "ds[].tolist", "ds[].tolist"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_token_block_dataset.TestTokenBlockDataset._build_dataset", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.test_token_block_dataset.TestTokenBlockDataset._build_dataset"], ["", "def", "test_complete_break_mode", "(", "self", ")", ":", "\n", "        ", "data", "=", "[", "\n", "torch", ".", "tensor", "(", "[", "5", ",", "4", ",", "3", ",", "2", ",", "1", "]", ",", "dtype", "=", "torch", ".", "long", ")", ",", "\n", "torch", ".", "tensor", "(", "[", "8", ",", "7", ",", "6", ",", "1", "]", ",", "dtype", "=", "torch", ".", "long", ")", ",", "\n", "torch", ".", "tensor", "(", "[", "9", ",", "1", "]", ",", "dtype", "=", "torch", ".", "long", ")", ",", "\n", "]", "\n", "ds", "=", "self", ".", "_build_dataset", "(", "data", ",", "block_size", "=", "6", ",", "pad", "=", "0", ",", "eos", "=", "1", ",", "break_mode", "=", "'complete'", ")", "\n", "self", ".", "assertEqual", "(", "ds", "[", "0", "]", ".", "tolist", "(", ")", ",", "[", "5", ",", "4", ",", "3", ",", "2", ",", "1", "]", ")", "\n", "self", ".", "assertEqual", "(", "ds", "[", "1", "]", ".", "tolist", "(", ")", ",", "[", "8", ",", "7", ",", "6", ",", "1", ",", "9", ",", "1", "]", ")", "\n", "\n", "data", "=", "[", "\n", "torch", ".", "tensor", "(", "[", "4", ",", "3", ",", "2", ",", "1", "]", ",", "dtype", "=", "torch", ".", "long", ")", ",", "\n", "torch", ".", "tensor", "(", "[", "5", ",", "1", "]", ",", "dtype", "=", "torch", ".", "long", ")", ",", "\n", "torch", ".", "tensor", "(", "[", "1", "]", ",", "dtype", "=", "torch", ".", "long", ")", ",", "\n", "torch", ".", "tensor", "(", "[", "6", ",", "1", "]", ",", "dtype", "=", "torch", ".", "long", ")", ",", "\n", "]", "\n", "ds", "=", "self", ".", "_build_dataset", "(", "data", ",", "block_size", "=", "3", ",", "pad", "=", "0", ",", "eos", "=", "1", ",", "break_mode", "=", "'complete'", ")", "\n", "self", ".", "assertEqual", "(", "ds", "[", "0", "]", ".", "tolist", "(", ")", ",", "[", "4", ",", "3", ",", "2", ",", "1", "]", ")", "\n", "self", ".", "assertEqual", "(", "ds", "[", "1", "]", ".", "tolist", "(", ")", ",", "[", "5", ",", "1", ",", "1", "]", ")", "\n", "self", ".", "assertEqual", "(", "ds", "[", "2", "]", ".", "tolist", "(", ")", ",", "[", "6", ",", "1", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.utils.TestDataset.__init__": [[119, 123], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["    ", "def", "__init__", "(", "self", ",", "data", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "data", "=", "data", "\n", "self", ".", "sizes", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.utils.TestDataset.__getitem__": [[124, 126], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "data", "[", "index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.utils.TestDataset.__len__": [[127, 129], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.utils.TestTranslationTask.__init__": [[133, 138], ["fairseq.tasks.FairseqTask.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "src_dict", ",", "tgt_dict", ",", "model", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ")", "\n", "self", ".", "src_dict", "=", "src_dict", "\n", "self", ".", "tgt_dict", "=", "tgt_dict", "\n", "self", ".", "model", "=", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.utils.TestTranslationTask.setup_task": [[139, 142], ["cls"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.masked_lm_dictionary.BertDictionary.cls"], ["", "@", "classmethod", "\n", "def", "setup_task", "(", "cls", ",", "args", ",", "src_dict", "=", "None", ",", "tgt_dict", "=", "None", ",", "model", "=", "None", ")", ":", "\n", "        ", "return", "cls", "(", "args", ",", "src_dict", ",", "tgt_dict", ",", "model", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.utils.TestTranslationTask.build_model": [[143, 145], ["fairseq.utils.TestModel.build_model"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv_lm.LightConvLanguageModel.build_model"], ["", "def", "build_model", "(", "self", ",", "args", ")", ":", "\n", "        ", "return", "TestModel", ".", "build_model", "(", "args", ",", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.utils.TestTranslationTask.source_dictionary": [[146, 149], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "source_dictionary", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "src_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.utils.TestTranslationTask.target_dictionary": [[150, 153], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "target_dictionary", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "tgt_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.utils.TestModel.__init__": [[156, 158], ["fairseq.models.FairseqEncoderDecoderModel.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["    ", "def", "__init__", "(", "self", ",", "encoder", ",", "decoder", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "encoder", ",", "decoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.utils.TestModel.build_model": [[159, 164], ["fairseq.utils.TestEncoder", "fairseq.utils.TestIncrementalDecoder", "cls"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.masked_lm_dictionary.BertDictionary.cls"], ["", "@", "classmethod", "\n", "def", "build_model", "(", "cls", ",", "args", ",", "task", ")", ":", "\n", "        ", "encoder", "=", "TestEncoder", "(", "args", ",", "task", ".", "source_dictionary", ")", "\n", "decoder", "=", "TestIncrementalDecoder", "(", "args", ",", "task", ".", "target_dictionary", ")", "\n", "return", "cls", "(", "encoder", ",", "decoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.utils.TestEncoder.__init__": [[167, 170], ["fairseq.models.FairseqEncoder.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "dictionary", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ")", "\n", "self", ".", "args", "=", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.utils.TestEncoder.forward": [[171, 173], ["None"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "src_tokens", ",", "src_lengths", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "src_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.utils.TestEncoder.reorder_encoder_out": [[174, 176], ["encoder_out.index_select"], "methods", ["None"], ["", "def", "reorder_encoder_out", "(", "self", ",", "encoder_out", ",", "new_order", ")", ":", "\n", "        ", "return", "encoder_out", ".", "index_select", "(", "0", ",", "new_order", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.utils.TestIncrementalDecoder.__init__": [[179, 184], ["fairseq.models.FairseqIncrementalDecoder.__init__", "getattr", "hasattr", "hasattr"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "dictionary", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ")", "\n", "assert", "hasattr", "(", "args", ",", "'beam_probs'", ")", "or", "hasattr", "(", "args", ",", "'probs'", ")", "\n", "args", ".", "max_decoder_positions", "=", "getattr", "(", "args", ",", "'max_decoder_positions'", ",", "100", ")", "\n", "self", ".", "args", "=", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.utils.TestIncrementalDecoder.forward": [[185, 224], ["prev_output_tokens.size", "len", "encoder_out.size", "prev_output_tokens.size", "hasattr", "torch.rand", "fairseq.utils.get_incremental_state", "fairseq.utils.set_incremental_state", "list", "fairseq.utils.TestIncrementalDecoder.args.probs.index_select", "torch.FloatTensor().zero_", "enumerate", "torch.FloatTensor().zero_.to", "torch.rand.to", "range", "fairseq.utils.TestIncrementalDecoder.args.probs.dim", "torch.LongTensor", "torch.FloatTensor", "len", "len", "fairseq.utils.TestIncrementalDecoder.dictionary.eos", "fairseq.utils.TestIncrementalDecoder.dictionary.eos"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.get_incremental_state", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.set_incremental_state", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos"], ["", "def", "forward", "(", "self", ",", "prev_output_tokens", ",", "encoder_out", "=", "None", ",", "incremental_state", "=", "None", ")", ":", "\n", "        ", "if", "incremental_state", "is", "not", "None", ":", "\n", "            ", "prev_output_tokens", "=", "prev_output_tokens", "[", ":", ",", "-", "1", ":", "]", "\n", "", "bbsz", "=", "prev_output_tokens", ".", "size", "(", "0", ")", "\n", "vocab", "=", "len", "(", "self", ".", "dictionary", ")", "\n", "src_len", "=", "encoder_out", ".", "size", "(", "1", ")", "\n", "tgt_len", "=", "prev_output_tokens", ".", "size", "(", "1", ")", "\n", "\n", "# determine number of steps", "\n", "if", "incremental_state", "is", "not", "None", ":", "\n", "# cache step number", "\n", "            ", "step", "=", "utils", ".", "get_incremental_state", "(", "self", ",", "incremental_state", ",", "'step'", ")", "\n", "if", "step", "is", "None", ":", "\n", "                ", "step", "=", "0", "\n", "", "utils", ".", "set_incremental_state", "(", "self", ",", "incremental_state", ",", "'step'", ",", "step", "+", "1", ")", "\n", "steps", "=", "[", "step", "]", "\n", "", "else", ":", "\n", "            ", "steps", "=", "list", "(", "range", "(", "tgt_len", ")", ")", "\n", "\n", "# define output in terms of raw probs", "\n", "", "if", "hasattr", "(", "self", ".", "args", ",", "'probs'", ")", ":", "\n", "            ", "assert", "self", ".", "args", ".", "probs", ".", "dim", "(", ")", "==", "3", ",", "'expected probs to have size bsz*steps*vocab'", "\n", "probs", "=", "self", ".", "args", ".", "probs", ".", "index_select", "(", "1", ",", "torch", ".", "LongTensor", "(", "steps", ")", ")", "\n", "", "else", ":", "\n", "            ", "probs", "=", "torch", ".", "FloatTensor", "(", "bbsz", ",", "len", "(", "steps", ")", ",", "vocab", ")", ".", "zero_", "(", ")", "\n", "for", "i", ",", "step", "in", "enumerate", "(", "steps", ")", ":", "\n", "# args.beam_probs gives the probability for every vocab element,", "\n", "# starting with eos, then unknown, and then the rest of the vocab", "\n", "                ", "if", "step", "<", "len", "(", "self", ".", "args", ".", "beam_probs", ")", ":", "\n", "                    ", "probs", "[", ":", ",", "i", ",", "self", ".", "dictionary", ".", "eos", "(", ")", ":", "]", "=", "self", ".", "args", ".", "beam_probs", "[", "step", "]", "\n", "", "else", ":", "\n", "                    ", "probs", "[", ":", ",", "i", ",", "self", ".", "dictionary", ".", "eos", "(", ")", "]", "=", "1.0", "\n", "\n", "# random attention", "\n", "", "", "", "attn", "=", "torch", ".", "rand", "(", "bbsz", ",", "tgt_len", ",", "src_len", ")", "\n", "\n", "dev", "=", "prev_output_tokens", ".", "device", "\n", "return", "probs", ".", "to", "(", "dev", ")", ",", "attn", ".", "to", "(", "dev", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.utils.TestIncrementalDecoder.get_normalized_probs": [[225, 232], ["probs.log"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.log"], ["", "def", "get_normalized_probs", "(", "self", ",", "net_output", ",", "log_probs", ",", "_", ")", ":", "\n", "# the decoder returns probabilities directly", "\n", "        ", "probs", "=", "net_output", "[", "0", "]", "\n", "if", "log_probs", ":", "\n", "            ", "return", "probs", ".", "log", "(", ")", "\n", "", "else", ":", "\n", "            ", "return", "probs", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.utils.TestIncrementalDecoder.max_positions": [[233, 235], ["None"], "methods", ["None"], ["", "", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "args", ".", "max_decoder_positions", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.utils.dummy_dictionary": [[20, 27], ["fairseq.data.Dictionary", "range", "fairseq.data.Dictionary.finalize", "fairseq.data.Dictionary.add_symbol", "str"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDatasetBuilder.finalize", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol"], ["def", "dummy_dictionary", "(", "vocab_size", ",", "prefix", "=", "'token_'", ")", ":", "\n", "    ", "d", "=", "Dictionary", "(", ")", "\n", "for", "i", "in", "range", "(", "vocab_size", ")", ":", "\n", "        ", "token", "=", "prefix", "+", "str", "(", "i", ")", "\n", "d", ".", "add_symbol", "(", "token", ")", "\n", "", "d", ".", "finalize", "(", "padding_factor", "=", "1", ")", "# don't add extra padding symbols", "\n", "return", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.utils.dummy_dataloader": [[29, 51], ["enumerate", "utils.TestDataset", "torch.utils.data.DataLoader", "iter", "len", "fairseq.data.language_pair_dataset.collate"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.language_pair_dataset.collate"], ["", "def", "dummy_dataloader", "(", "\n", "samples", ",", "\n", "padding_idx", "=", "1", ",", "\n", "eos_idx", "=", "2", ",", "\n", "batch_size", "=", "None", ",", "\n", ")", ":", "\n", "    ", "if", "batch_size", "is", "None", ":", "\n", "        ", "batch_size", "=", "len", "(", "samples", ")", "\n", "\n", "# add any missing data to samples", "\n", "", "for", "i", ",", "sample", "in", "enumerate", "(", "samples", ")", ":", "\n", "        ", "if", "'id'", "not", "in", "sample", ":", "\n", "            ", "sample", "[", "'id'", "]", "=", "i", "\n", "\n", "# create dataloader", "\n", "", "", "dataset", "=", "TestDataset", "(", "samples", ")", "\n", "dataloader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "dataset", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "collate_fn", "=", "(", "lambda", "samples", ":", "collate", "(", "samples", ",", "padding_idx", ",", "eos_idx", ")", ")", ",", "\n", ")", "\n", "return", "iter", "(", "dataloader", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.utils.sequence_generator_setup": [[53, 115], ["utils.dummy_dictionary", "dummy_dictionary.eos", "torch.LongTensor", "torch.LongTensor", "argparse.Namespace", "utils.TestTranslationTask.setup_task", "TestTranslationTask.setup_task.build_model", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tests.utils.dummy_dictionary", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.sentence_prediction.SentencePredictionTask.setup_task", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv_lm.LightConvLanguageModel.build_model"], ["", "def", "sequence_generator_setup", "(", ")", ":", "\n", "# construct dummy dictionary", "\n", "    ", "d", "=", "dummy_dictionary", "(", "vocab_size", "=", "2", ")", "\n", "\n", "eos", "=", "d", ".", "eos", "(", ")", "\n", "w1", "=", "4", "\n", "w2", "=", "5", "\n", "\n", "# construct source data", "\n", "src_tokens", "=", "torch", ".", "LongTensor", "(", "[", "[", "w1", ",", "w2", ",", "eos", "]", ",", "[", "w1", ",", "w2", ",", "eos", "]", "]", ")", "\n", "src_lengths", "=", "torch", ".", "LongTensor", "(", "[", "2", ",", "2", "]", ")", "\n", "\n", "args", "=", "argparse", ".", "Namespace", "(", ")", "\n", "unk", "=", "0.", "\n", "args", ".", "beam_probs", "=", "[", "\n", "# step 0:", "\n", "torch", ".", "FloatTensor", "(", "[", "\n", "# eos      w1   w2", "\n", "# sentence 1:", "\n", "[", "0.0", ",", "unk", ",", "0.9", ",", "0.1", "]", ",", "# beam 1", "\n", "[", "0.0", ",", "unk", ",", "0.9", ",", "0.1", "]", ",", "# beam 2", "\n", "# sentence 2:", "\n", "[", "0.0", ",", "unk", ",", "0.7", ",", "0.3", "]", ",", "\n", "[", "0.0", ",", "unk", ",", "0.7", ",", "0.3", "]", ",", "\n", "]", ")", ",", "\n", "# step 1:", "\n", "torch", ".", "FloatTensor", "(", "[", "\n", "# eos      w1   w2       prefix", "\n", "# sentence 1:", "\n", "[", "1.0", ",", "unk", ",", "0.0", ",", "0.0", "]", ",", "# w1: 0.9  (emit: w1 <eos>: 0.9*1.0)", "\n", "[", "0.0", ",", "unk", ",", "0.9", ",", "0.1", "]", ",", "# w2: 0.1", "\n", "# sentence 2:", "\n", "[", "0.25", ",", "unk", ",", "0.35", ",", "0.4", "]", ",", "# w1: 0.7  (don't emit: w1 <eos>: 0.7*0.25)", "\n", "[", "0.00", ",", "unk", ",", "0.10", ",", "0.9", "]", ",", "# w2: 0.3", "\n", "]", ")", ",", "\n", "# step 2:", "\n", "torch", ".", "FloatTensor", "(", "[", "\n", "# eos      w1   w2       prefix", "\n", "# sentence 1:", "\n", "[", "0.0", ",", "unk", ",", "0.1", ",", "0.9", "]", ",", "# w2 w1: 0.1*0.9", "\n", "[", "0.6", ",", "unk", ",", "0.2", ",", "0.2", "]", ",", "# w2 w2: 0.1*0.1  (emit: w2 w2 <eos>: 0.1*0.1*0.6)", "\n", "# sentence 2:", "\n", "[", "0.60", ",", "unk", ",", "0.4", ",", "0.00", "]", ",", "# w1 w2: 0.7*0.4  (emit: w1 w2 <eos>: 0.7*0.4*0.6)", "\n", "[", "0.01", ",", "unk", ",", "0.0", ",", "0.99", "]", ",", "# w2 w2: 0.3*0.9", "\n", "]", ")", ",", "\n", "# step 3:", "\n", "torch", ".", "FloatTensor", "(", "[", "\n", "# eos      w1   w2       prefix", "\n", "# sentence 1:", "\n", "[", "1.0", ",", "unk", ",", "0.0", ",", "0.0", "]", ",", "# w2 w1 w2: 0.1*0.9*0.9  (emit: w2 w1 w2 <eos>: 0.1*0.9*0.9*1.0)", "\n", "[", "1.0", ",", "unk", ",", "0.0", ",", "0.0", "]", ",", "# w2 w1 w1: 0.1*0.9*0.1  (emit: w2 w1 w1 <eos>: 0.1*0.9*0.1*1.0)", "\n", "# sentence 2:", "\n", "[", "0.1", ",", "unk", ",", "0.5", ",", "0.4", "]", ",", "# w2 w2 w2: 0.3*0.9*0.99  (emit: w2 w2 w2 <eos>: 0.3*0.9*0.99*0.1)", "\n", "[", "1.0", ",", "unk", ",", "0.0", ",", "0.0", "]", ",", "# w1 w2 w1: 0.7*0.4*0.4  (emit: w1 w2 w1 <eos>: 0.7*0.4*0.4*1.0)", "\n", "]", ")", ",", "\n", "]", "\n", "\n", "task", "=", "TestTranslationTask", ".", "setup_task", "(", "args", ",", "d", ",", "d", ")", "\n", "model", "=", "task", ".", "build_model", "(", "args", ")", "\n", "tgt_dict", "=", "task", ".", "target_dictionary", "\n", "\n", "return", "tgt_dict", ",", "w1", ",", "w2", ",", "src_tokens", ",", "src_lengths", ",", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq-stack-transformer.generate.main": [[19, 203], ["fairseq.utils.import_user_module", "print", "fairseq.tasks.setup_task", "tasks.setup_task.load_dataset", "print", "fairseq.checkpoint_utils.load_model_ensemble", "fairseq.utils.load_align_dict", "tasks.setup_task.get_batch_iterator().next_epoch_itr", "fairseq.meters.StopwatchMeter", "tasks.setup_task.build_generator", "transition_amr_parser.stack_transformer.data_utils.Examples", "transition_amr_parser.stack_transformer.data_utils.Examples.save", "print", "torch.cuda.is_available", "getattr", "args.path.split", "model.make_generation_fast_", "fairseq.bleu.SacrebleuScorer", "fairseq.bleu.Scorer", "fairseq.progress_bar.build_progress_bar", "fairseq.meters.TimeMeter", "print", "eval", "model.half", "model.cuda", "tasks.setup_task.get_batch_iterator", "tgt_dict.pad", "tgt_dict.eos", "tgt_dict.unk", "fairseq.meters.StopwatchMeter.start", "tasks.setup_task.inference_step", "sum", "fairseq.meters.StopwatchMeter.stop", "enumerate", "fairseq.meters.TimeMeter.update", "t.log", "fairseq.utils.move_to_cuda", "Exception", "sample[].tolist", "fairseq.utils.strip_pad", "enumerate", "bleu.Scorer.result_string", "tasks.setup_task.dataset", "len", "tgt_dict.pad", "fairseq.utils.strip_pad().int().cpu", "tasks.setup_task.dataset().src.get_original_text", "tasks.setup_task.dataset().tgt.get_original_text", "fairseq.utils.post_process_prediction", "transition_amr_parser.stack_transformer.data_utils.Examples.append", "round", "getattr.string", "tgt_dict.string", "print", "print", "print", "print", "hasattr", "fairseq.utils.strip_pad().int", "hypo[].int().cpu", "print", "tgt_dict.encode_line", "bleu.Scorer.add_string", "bleu.Scorer.add", "tasks.setup_task.dataset", "tasks.setup_task.dataset", "hypo[].int().cpu", "fairseq.utils.strip_pad", "hypo[].int", "map", "tgt_dict.pad", "hypo[].int", "hypo[].tolist", "map", "str", "fairseq.utils.item"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.import_user_module", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.sentence_prediction.SentencePredictionTask.setup_task", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.sentence_prediction.SentencePredictionTask.load_dataset", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.checkpoint_utils.load_model_ensemble", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.load_align_dict", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.EpochBatchIterator.next_epoch_itr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.fairseq_task.FairseqTask.build_generator", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.save", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.TransformerDecoderLayer.make_generation_fast_", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.build_progress_bar", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.fairseq_task.FairseqTask.get_batch_iterator", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.unk", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.meters.StopwatchMeter.start", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.translation_moe.TranslationMoETask.inference_step", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.meters.StopwatchMeter.stop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.move_to_cuda", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.strip_pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.bleu.Scorer.result_string", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.fairseq_task.FairseqTask.dataset", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.IndexedRawTextDataset.get_original_text", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.IndexedRawTextDataset.get_original_text", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.post_process_prediction", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.hub_utils.GeneratorHubInterface.string", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.hub_utils.GeneratorHubInterface.string", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.encode_line", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.bleu.SacrebleuScorer.add_string", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.add", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.fairseq_task.FairseqTask.dataset", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.fairseq_task.FairseqTask.dataset", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.strip_pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item"], ["def", "main", "(", "args", ")", ":", "\n", "    ", "assert", "args", ".", "path", "is", "not", "None", ",", "'--path required for generation!'", "\n", "assert", "not", "args", ".", "sampling", "or", "args", ".", "nbest", "==", "args", ".", "beam", ",", "'--sampling requires --nbest to be equal to --beam'", "\n", "assert", "args", ".", "replace_unk", "is", "None", "or", "args", ".", "raw_text", ",", "'--replace-unk requires a raw text dataset (--raw-text)'", "\n", "\n", "utils", ".", "import_user_module", "(", "args", ")", "\n", "\n", "if", "args", ".", "max_tokens", "is", "None", "and", "args", ".", "max_sentences", "is", "None", ":", "\n", "        ", "args", ".", "max_tokens", "=", "12000", "\n", "", "print", "(", "args", ")", "\n", "\n", "use_cuda", "=", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "cpu", "\n", "\n", "# Load dataset splits", "\n", "task", "=", "tasks", ".", "setup_task", "(", "args", ")", "\n", "# Note: states are not needed since they will be provided by the state", "\n", "# machine", "\n", "task", ".", "load_dataset", "(", "args", ".", "gen_subset", ",", "state_machine", "=", "False", ")", "\n", "\n", "# Set dictionaries", "\n", "try", ":", "\n", "        ", "src_dict", "=", "getattr", "(", "task", ",", "'source_dictionary'", ",", "None", ")", "\n", "", "except", "NotImplementedError", ":", "\n", "        ", "src_dict", "=", "None", "\n", "", "tgt_dict", "=", "task", ".", "target_dictionary", "\n", "\n", "# Load ensemble", "\n", "print", "(", "'| loading model(s) from {}'", ".", "format", "(", "args", ".", "path", ")", ")", "\n", "models", ",", "_model_args", "=", "checkpoint_utils", ".", "load_model_ensemble", "(", "\n", "args", ".", "path", ".", "split", "(", "':'", ")", ",", "\n", "arg_overrides", "=", "eval", "(", "args", ".", "model_overrides", ")", ",", "\n", "task", "=", "task", ",", "\n", ")", "\n", "\n", "# Optimize ensemble for generation", "\n", "for", "model", "in", "models", ":", "\n", "        ", "model", ".", "make_generation_fast_", "(", "\n", "beamable_mm_beam_size", "=", "None", "if", "args", ".", "no_beamable_mm", "else", "args", ".", "beam", ",", "\n", "need_attn", "=", "args", ".", "print_alignment", ",", "\n", ")", "\n", "if", "args", ".", "fp16", ":", "\n", "            ", "model", ".", "half", "(", ")", "\n", "", "if", "use_cuda", ":", "\n", "            ", "model", ".", "cuda", "(", ")", "\n", "\n", "# Load alignment dictionary for unknown word replacement", "\n", "# (None if no unknown word replacement, empty if no path to align", "\n", "# dictionary)", "\n", "", "", "align_dict", "=", "utils", ".", "load_align_dict", "(", "args", ".", "replace_unk", ")", "\n", "\n", "# Load dataset (possibly sharded)", "\n", "itr", "=", "task", ".", "get_batch_iterator", "(", "\n", "dataset", "=", "task", ".", "dataset", "(", "args", ".", "gen_subset", ")", ",", "\n", "max_tokens", "=", "args", ".", "max_tokens", ",", "\n", "max_sentences", "=", "args", ".", "max_sentences", ",", "\n", "max_positions", "=", "None", ",", "\n", "ignore_invalid_inputs", "=", "args", ".", "skip_invalid_size_inputs_valid_test", ",", "\n", "required_batch_size_multiple", "=", "args", ".", "required_batch_size_multiple", ",", "\n", "num_shards", "=", "args", ".", "num_shards", ",", "\n", "shard_id", "=", "args", ".", "shard_id", ",", "\n", "num_workers", "=", "args", ".", "num_workers", ",", "\n", "large_sent_first", "=", "False", "\n", ")", ".", "next_epoch_itr", "(", "shuffle", "=", "False", ")", "\n", "\n", "# Initialize generator", "\n", "gen_timer", "=", "StopwatchMeter", "(", ")", "\n", "generator", "=", "task", ".", "build_generator", "(", "args", ")", "\n", "\n", "# Generate and compute BLEU score", "\n", "if", "args", ".", "sacrebleu", ":", "\n", "        ", "scorer", "=", "bleu", ".", "SacrebleuScorer", "(", ")", "\n", "", "else", ":", "\n", "        ", "scorer", "=", "bleu", ".", "Scorer", "(", "tgt_dict", ".", "pad", "(", ")", ",", "tgt_dict", ".", "eos", "(", ")", ",", "tgt_dict", ".", "unk", "(", ")", ")", "\n", "", "num_sentences", "=", "0", "\n", "has_target", "=", "True", "\n", "\n", "examples", "=", "Examples", "(", "args", ".", "path", ",", "args", ".", "results_path", ",", "args", ".", "gen_subset", ",", "args", ".", "nbest", ")", "\n", "\n", "with", "progress_bar", ".", "build_progress_bar", "(", "args", ",", "itr", ")", "as", "t", ":", "\n", "        ", "wps_meter", "=", "TimeMeter", "(", ")", "\n", "for", "sample", "in", "t", ":", "\n", "            ", "sample", "=", "utils", ".", "move_to_cuda", "(", "sample", ")", "if", "use_cuda", "else", "sample", "\n", "if", "'net_input'", "not", "in", "sample", ":", "\n", "                ", "raise", "Exception", "(", "\"Did not expect empty sample\"", ")", "\n", "continue", "\n", "\n", "", "prefix_tokens", "=", "None", "\n", "if", "args", ".", "prefix_size", ">", "0", ":", "\n", "                ", "prefix_tokens", "=", "sample", "[", "'target'", "]", "[", ":", ",", ":", "args", ".", "prefix_size", "]", "\n", "\n", "", "gen_timer", ".", "start", "(", ")", "\n", "hypos", "=", "task", ".", "inference_step", "(", "generator", ",", "models", ",", "sample", ",", "prefix_tokens", ")", "\n", "num_generated_tokens", "=", "sum", "(", "len", "(", "h", "[", "0", "]", "[", "'tokens'", "]", ")", "for", "h", "in", "hypos", ")", "\n", "gen_timer", ".", "stop", "(", "num_generated_tokens", ")", "\n", "\n", "for", "i", ",", "sample_id", "in", "enumerate", "(", "sample", "[", "'id'", "]", ".", "tolist", "(", ")", ")", ":", "\n", "                ", "has_target", "=", "sample", "[", "'target'", "]", "is", "not", "None", "\n", "\n", "# Remove padding", "\n", "src_tokens", "=", "utils", ".", "strip_pad", "(", "sample", "[", "'net_input'", "]", "[", "'src_tokens'", "]", "[", "i", ",", ":", "]", ",", "tgt_dict", ".", "pad", "(", ")", ")", "\n", "target_tokens", "=", "None", "\n", "if", "has_target", ":", "\n", "                    ", "target_tokens", "=", "utils", ".", "strip_pad", "(", "sample", "[", "'target'", "]", "[", "i", ",", ":", "]", ",", "tgt_dict", ".", "pad", "(", ")", ")", ".", "int", "(", ")", ".", "cpu", "(", ")", "\n", "\n", "# Either retrieve the original sentences or regenerate them from tokens.", "\n", "", "if", "align_dict", "is", "not", "None", ":", "\n", "                    ", "src_str", "=", "task", ".", "dataset", "(", "args", ".", "gen_subset", ")", ".", "src", ".", "get_original_text", "(", "sample_id", ")", "\n", "target_str", "=", "task", ".", "dataset", "(", "args", ".", "gen_subset", ")", ".", "tgt", ".", "get_original_text", "(", "sample_id", ")", "\n", "", "else", ":", "\n", "                    ", "if", "src_dict", "is", "not", "None", ":", "\n", "                        ", "src_str", "=", "src_dict", ".", "string", "(", "src_tokens", ",", "args", ".", "remove_bpe", ")", "\n", "", "else", ":", "\n", "                        ", "src_str", "=", "\"\"", "\n", "", "if", "has_target", ":", "\n", "                        ", "target_str", "=", "tgt_dict", ".", "string", "(", "target_tokens", ",", "args", ".", "remove_bpe", ",", "escape_unk", "=", "True", ")", "\n", "\n", "", "", "if", "not", "args", ".", "quiet", ":", "\n", "                    ", "if", "src_dict", "is", "not", "None", ":", "\n", "                        ", "print", "(", "'S-{}\\t{}'", ".", "format", "(", "sample_id", ",", "src_str", ")", ")", "\n", "", "if", "has_target", ":", "\n", "                        ", "print", "(", "'T-{}\\t{}'", ".", "format", "(", "sample_id", ",", "target_str", ")", ")", "\n", "\n", "# Process top predictions", "\n", "", "", "for", "j", ",", "hypo", "in", "enumerate", "(", "hypos", "[", "i", "]", "[", ":", "args", ".", "nbest", "]", ")", ":", "\n", "                    ", "hypo_tokens", ",", "hypo_str", ",", "alignment", "=", "utils", ".", "post_process_prediction", "(", "\n", "hypo_tokens", "=", "hypo", "[", "'tokens'", "]", ".", "int", "(", ")", ".", "cpu", "(", ")", ",", "\n", "src_str", "=", "src_str", ",", "\n", "alignment", "=", "hypo", "[", "'alignment'", "]", ".", "int", "(", ")", ".", "cpu", "(", ")", "if", "hypo", "[", "'alignment'", "]", "is", "not", "None", "else", "None", ",", "\n", "align_dict", "=", "align_dict", ",", "\n", "tgt_dict", "=", "tgt_dict", ",", "\n", "remove_bpe", "=", "args", ".", "remove_bpe", ",", "\n", "# FIXME: AMR specific", "\n", "split_token", "=", "\"\\t\"", ",", "\n", "line_tokenizer", "=", "tab_tokenize", ",", "\n", ")", "\n", "\n", "# update the list of examples", "\n", "examples", ".", "append", "(", "{", "\n", "'hypothesis'", ":", "hypo_str", ",", "\n", "'reference'", ":", "target_str", ",", "\n", "'src_str'", ":", "src_str", ",", "\n", "'sample_id'", ":", "sample_id", "\n", "}", ")", "\n", "\n", "if", "not", "args", ".", "quiet", ":", "\n", "                        ", "print", "(", "'H-{}\\t{}\\t{}'", ".", "format", "(", "sample_id", ",", "hypo_str", ",", "hypo", "[", "'score'", "]", ")", ")", "\n", "print", "(", "'P-{}\\t{}'", ".", "format", "(", "\n", "sample_id", ",", "\n", "' '", ".", "join", "(", "map", "(", "\n", "lambda", "x", ":", "'{:.4f}'", ".", "format", "(", "x", ")", ",", "\n", "hypo", "[", "'positional_scores'", "]", ".", "tolist", "(", ")", ",", "\n", ")", ")", "\n", ")", ")", "\n", "\n", "if", "args", ".", "print_alignment", ":", "\n", "                            ", "print", "(", "'A-{}\\t{}'", ".", "format", "(", "\n", "sample_id", ",", "\n", "' '", ".", "join", "(", "map", "(", "lambda", "x", ":", "str", "(", "utils", ".", "item", "(", "x", ")", ")", ",", "alignment", ")", ")", "\n", ")", ")", "\n", "\n", "# Score only the top hypothesis", "\n", "", "", "if", "has_target", "and", "j", "==", "0", ":", "\n", "                        ", "if", "align_dict", "is", "not", "None", "or", "args", ".", "remove_bpe", "is", "not", "None", ":", "\n", "# Convert back to tokens for evaluation with unk replacement and/or without BPE", "\n", "                            ", "target_tokens", "=", "tgt_dict", ".", "encode_line", "(", "target_str", ",", "add_if_not_exist", "=", "True", ")", "\n", "", "if", "hasattr", "(", "scorer", ",", "'add_string'", ")", ":", "\n", "                            ", "scorer", ".", "add_string", "(", "target_str", ",", "hypo_str", ")", "\n", "", "else", ":", "\n", "                            ", "scorer", ".", "add", "(", "target_tokens", ",", "hypo_tokens", ")", "\n", "\n", "", "", "", "", "wps_meter", ".", "update", "(", "num_generated_tokens", ")", "\n", "t", ".", "log", "(", "{", "'wps'", ":", "round", "(", "wps_meter", ".", "avg", ")", "}", ")", "\n", "num_sentences", "+=", "sample", "[", "'nsentences'", "]", "\n", "\n", "# Save examples to files", "\n", "", "", "examples", ".", "save", "(", ")", "\n", "\n", "print", "(", "'| Translated {} sentences ({} tokens) in {:.1f}s ({:.2f} sentences/s, {:.2f} tokens/s)'", ".", "format", "(", "\n", "num_sentences", ",", "gen_timer", ".", "n", ",", "gen_timer", ".", "sum", ",", "num_sentences", "/", "gen_timer", ".", "sum", ",", "1.", "/", "gen_timer", ".", "avg", ")", ")", "\n", "if", "has_target", ":", "\n", "        ", "print", "(", "'| Generate {} with beam={}: {}'", ".", "format", "(", "args", ".", "gen_subset", ",", "args", ".", "beam", ",", "scorer", ".", "result_string", "(", ")", ")", ")", "\n", "", "return", "scorer", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq-stack-transformer.generate.cli_main": [[205, 209], ["fairseq.options.get_generation_parser", "fairseq.options.parse_args_and_arch", "generate.main"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.get_generation_parser", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.parse_args_and_arch", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.main"], ["", "def", "cli_main", "(", ")", ":", "\n", "    ", "parser", "=", "options", ".", "get_generation_parser", "(", ")", "\n", "args", "=", "options", ".", "parse_args_and_arch", "(", "parser", ")", "\n", "main", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq-stack-transformer.interactive.buffered_read": [[23, 34], ["fileinput.input", "len", "buffer.append", "fileinput.hook_encoded", "src_str.strip", "len"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["def", "buffered_read", "(", "input", ",", "buffer_size", ")", ":", "\n", "    ", "buffer", "=", "[", "]", "\n", "with", "fileinput", ".", "input", "(", "files", "=", "[", "input", "]", ",", "openhook", "=", "fileinput", ".", "hook_encoded", "(", "\"utf-8\"", ")", ")", "as", "h", ":", "\n", "        ", "for", "src_str", "in", "h", ":", "\n", "            ", "buffer", ".", "append", "(", "src_str", ".", "strip", "(", ")", ")", "\n", "if", "len", "(", "buffer", ")", ">=", "buffer_size", ":", "\n", "                ", "yield", "buffer", "\n", "buffer", "=", "[", "]", "\n", "\n", "", "", "", "if", "len", "(", "buffer", ")", ">", "0", ":", "\n", "        ", "yield", "buffer", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq-stack-transformer.interactive.make_batches": [[36, 54], ["torch.LongTensor", "task.get_batch_iterator().next_epoch_itr", "task.source_dictionary.encode_line().long", "t.numel", "task.get_batch_iterator", "Batch", "task.source_dictionary.encode_line", "interactive.main.encode_fn", "task.build_dataset_for_inference"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.EpochBatchIterator.next_epoch_itr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.fairseq_task.FairseqTask.get_batch_iterator", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.encode_line", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.masked_lm.MaskedLMTask.build_dataset_for_inference"], ["", "", "def", "make_batches", "(", "lines", ",", "args", ",", "task", ",", "max_positions", ",", "encode_fn", ")", ":", "\n", "    ", "tokens", "=", "[", "\n", "task", ".", "source_dictionary", ".", "encode_line", "(", "\n", "encode_fn", "(", "src_str", ")", ",", "add_if_not_exist", "=", "False", "\n", ")", ".", "long", "(", ")", "\n", "for", "src_str", "in", "lines", "\n", "]", "\n", "lengths", "=", "torch", ".", "LongTensor", "(", "[", "t", ".", "numel", "(", ")", "for", "t", "in", "tokens", "]", ")", "\n", "itr", "=", "task", ".", "get_batch_iterator", "(", "\n", "dataset", "=", "task", ".", "build_dataset_for_inference", "(", "tokens", ",", "lengths", ")", ",", "\n", "max_tokens", "=", "args", ".", "max_tokens", ",", "\n", "max_sentences", "=", "args", ".", "max_sentences", ",", "\n", "max_positions", "=", "max_positions", ",", "\n", ")", ".", "next_epoch_itr", "(", "shuffle", "=", "False", ")", "\n", "for", "batch", "in", "itr", ":", "\n", "        ", "yield", "Batch", "(", "\n", "ids", "=", "batch", "[", "'id'", "]", ",", "\n", "src_tokens", "=", "batch", "[", "'net_input'", "]", "[", "'src_tokens'", "]", ",", "src_lengths", "=", "batch", "[", "'net_input'", "]", "[", "'src_lengths'", "]", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq-stack-transformer.interactive.main": [[57, 184], ["fairseq.utils.import_user_module", "print", "fairseq.tasks.setup_task", "print", "fairseq.checkpoint_utils.load_model_ensemble", "tasks.setup_task.build_generator", "fairseq.data.encoders.build_tokenizer", "fairseq.data.encoders.build_bpe", "fairseq.utils.load_align_dict", "fairseq.utils.resolve_max_positions", "print", "interactive.buffered_read", "torch.cuda.is_available", "args.path.split", "model.make_generation_fast_", "tasks.setup_task.max_positions", "print", "interactive.make_batches", "sorted", "len", "eval", "model.half", "model.cuda", "encoders.build_tokenizer.encode", "encoders.build_bpe.encode", "encoders.build_bpe.decode", "encoders.build_tokenizer.decode", "tasks.setup_task.inference_step", "enumerate", "model.max_positions", "src_tokens.cuda.cuda", "src_lengths.cuda.cuda", "zip", "fairseq.utils.strip_pad", "results.append", "src_dict.string", "print", "fairseq.utils.post_process_prediction", "interactive.main.decode_fn"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.import_user_module", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.sentence_prediction.SentencePredictionTask.setup_task", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.checkpoint_utils.load_model_ensemble", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.fairseq_task.FairseqTask.build_generator", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.load_align_dict", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.resolve_max_positions", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq-stack-transformer.interactive.buffered_read", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.TransformerDecoderLayer.make_generation_fast_", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.max_positions", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq-stack-transformer.interactive.make_batches", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.encode", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.encode", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.decode", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.decode", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.translation_moe.TranslationMoETask.inference_step", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.max_positions", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.strip_pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.hub_utils.GeneratorHubInterface.string", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.post_process_prediction"], ["", "", "def", "main", "(", "args", ")", ":", "\n", "    ", "utils", ".", "import_user_module", "(", "args", ")", "\n", "\n", "if", "args", ".", "buffer_size", "<", "1", ":", "\n", "        ", "args", ".", "buffer_size", "=", "1", "\n", "", "if", "args", ".", "max_tokens", "is", "None", "and", "args", ".", "max_sentences", "is", "None", ":", "\n", "        ", "args", ".", "max_sentences", "=", "1", "\n", "\n", "", "assert", "not", "args", ".", "sampling", "or", "args", ".", "nbest", "==", "args", ".", "beam", ",", "'--sampling requires --nbest to be equal to --beam'", "\n", "assert", "not", "args", ".", "max_sentences", "or", "args", ".", "max_sentences", "<=", "args", ".", "buffer_size", ",", "'--max-sentences/--batch-size cannot be larger than --buffer-size'", "\n", "\n", "print", "(", "args", ")", "\n", "\n", "use_cuda", "=", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "cpu", "\n", "\n", "# Setup task, e.g., translation", "\n", "task", "=", "tasks", ".", "setup_task", "(", "args", ")", "\n", "\n", "# Load ensemble", "\n", "print", "(", "'| loading model(s) from {}'", ".", "format", "(", "args", ".", "path", ")", ")", "\n", "models", ",", "_model_args", "=", "checkpoint_utils", ".", "load_model_ensemble", "(", "\n", "args", ".", "path", ".", "split", "(", "':'", ")", ",", "\n", "arg_overrides", "=", "eval", "(", "args", ".", "model_overrides", ")", ",", "\n", "task", "=", "task", ",", "\n", ")", "\n", "\n", "# Set dictionaries", "\n", "src_dict", "=", "task", ".", "source_dictionary", "\n", "tgt_dict", "=", "task", ".", "target_dictionary", "\n", "\n", "# Optimize ensemble for generation", "\n", "for", "model", "in", "models", ":", "\n", "        ", "model", ".", "make_generation_fast_", "(", "\n", "beamable_mm_beam_size", "=", "None", "if", "args", ".", "no_beamable_mm", "else", "args", ".", "beam", ",", "\n", "need_attn", "=", "args", ".", "print_alignment", ",", "\n", ")", "\n", "if", "args", ".", "fp16", ":", "\n", "            ", "model", ".", "half", "(", ")", "\n", "", "if", "use_cuda", ":", "\n", "            ", "model", ".", "cuda", "(", ")", "\n", "\n", "# Initialize generator", "\n", "", "", "generator", "=", "task", ".", "build_generator", "(", "args", ")", "\n", "\n", "# Handle tokenization and BPE", "\n", "tokenizer", "=", "encoders", ".", "build_tokenizer", "(", "args", ")", "\n", "bpe", "=", "encoders", ".", "build_bpe", "(", "args", ")", "\n", "\n", "def", "encode_fn", "(", "x", ")", ":", "\n", "        ", "if", "tokenizer", "is", "not", "None", ":", "\n", "            ", "x", "=", "tokenizer", ".", "encode", "(", "x", ")", "\n", "", "if", "bpe", "is", "not", "None", ":", "\n", "            ", "x", "=", "bpe", ".", "encode", "(", "x", ")", "\n", "", "return", "x", "\n", "\n", "", "def", "decode_fn", "(", "x", ")", ":", "\n", "        ", "if", "bpe", "is", "not", "None", ":", "\n", "            ", "x", "=", "bpe", ".", "decode", "(", "x", ")", "\n", "", "if", "tokenizer", "is", "not", "None", ":", "\n", "            ", "x", "=", "tokenizer", ".", "decode", "(", "x", ")", "\n", "", "return", "x", "\n", "\n", "# Load alignment dictionary for unknown word replacement", "\n", "# (None if no unknown word replacement, empty if no path to align dictionary)", "\n", "", "align_dict", "=", "utils", ".", "load_align_dict", "(", "args", ".", "replace_unk", ")", "\n", "\n", "max_positions", "=", "utils", ".", "resolve_max_positions", "(", "\n", "task", ".", "max_positions", "(", ")", ",", "\n", "*", "[", "model", ".", "max_positions", "(", ")", "for", "model", "in", "models", "]", "\n", ")", "\n", "\n", "if", "args", ".", "buffer_size", ">", "1", ":", "\n", "        ", "print", "(", "'| Sentence buffer size:'", ",", "args", ".", "buffer_size", ")", "\n", "", "print", "(", "'| Type the input sentence and press return:'", ")", "\n", "start_id", "=", "0", "\n", "for", "inputs", "in", "buffered_read", "(", "args", ".", "input", ",", "args", ".", "buffer_size", ")", ":", "\n", "        ", "results", "=", "[", "]", "\n", "for", "batch", "in", "make_batches", "(", "inputs", ",", "args", ",", "task", ",", "max_positions", ",", "encode_fn", ")", ":", "\n", "            ", "src_tokens", "=", "batch", ".", "src_tokens", "\n", "src_lengths", "=", "batch", ".", "src_lengths", "\n", "if", "use_cuda", ":", "\n", "                ", "src_tokens", "=", "src_tokens", ".", "cuda", "(", ")", "\n", "src_lengths", "=", "src_lengths", ".", "cuda", "(", ")", "\n", "\n", "", "sample", "=", "{", "\n", "'net_input'", ":", "{", "\n", "'src_tokens'", ":", "src_tokens", ",", "\n", "'src_lengths'", ":", "src_lengths", ",", "\n", "}", ",", "\n", "}", "\n", "translations", "=", "task", ".", "inference_step", "(", "generator", ",", "models", ",", "sample", ")", "\n", "for", "i", ",", "(", "id", ",", "hypos", ")", "in", "enumerate", "(", "zip", "(", "batch", ".", "ids", ".", "tolist", "(", ")", ",", "translations", ")", ")", ":", "\n", "                ", "src_tokens_i", "=", "utils", ".", "strip_pad", "(", "src_tokens", "[", "i", "]", ",", "tgt_dict", ".", "pad", "(", ")", ")", "\n", "results", ".", "append", "(", "(", "start_id", "+", "id", ",", "src_tokens_i", ",", "hypos", ")", ")", "\n", "\n", "# sort output to match input order", "\n", "", "", "for", "id", ",", "src_tokens", ",", "hypos", "in", "sorted", "(", "results", ",", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ")", ":", "\n", "            ", "if", "src_dict", "is", "not", "None", ":", "\n", "                ", "src_str", "=", "src_dict", ".", "string", "(", "src_tokens", ",", "args", ".", "remove_bpe", ")", "\n", "print", "(", "'S-{}\\t{}'", ".", "format", "(", "id", ",", "src_str", ")", ")", "\n", "\n", "# Process top predictions", "\n", "", "for", "hypo", "in", "hypos", "[", ":", "min", "(", "len", "(", "hypos", ")", ",", "args", ".", "nbest", ")", "]", ":", "\n", "                ", "hypo_tokens", ",", "hypo_str", ",", "alignment", "=", "utils", ".", "post_process_prediction", "(", "\n", "hypo_tokens", "=", "hypo", "[", "'tokens'", "]", ".", "int", "(", ")", ".", "cpu", "(", ")", ",", "\n", "src_str", "=", "src_str", ",", "\n", "alignment", "=", "hypo", "[", "'alignment'", "]", ".", "int", "(", ")", ".", "cpu", "(", ")", "if", "hypo", "[", "'alignment'", "]", "is", "not", "None", "else", "None", ",", "\n", "align_dict", "=", "align_dict", ",", "\n", "tgt_dict", "=", "tgt_dict", ",", "\n", "remove_bpe", "=", "args", ".", "remove_bpe", ",", "\n", ")", "\n", "hypo_str", "=", "decode_fn", "(", "hypo_str", ")", "\n", "print", "(", "'H-{}\\t{}\\t{}'", ".", "format", "(", "id", ",", "hypo", "[", "'score'", "]", ",", "hypo_str", ")", ")", "\n", "print", "(", "'P-{}\\t{}'", ".", "format", "(", "\n", "id", ",", "\n", "' '", ".", "join", "(", "map", "(", "lambda", "x", ":", "'{:.4f}'", ".", "format", "(", "x", ")", ",", "hypo", "[", "'positional_scores'", "]", ".", "tolist", "(", ")", ")", ")", "\n", ")", ")", "\n", "if", "args", ".", "print_alignment", ":", "\n", "                    ", "print", "(", "'A-{}\\t{}'", ".", "format", "(", "\n", "id", ",", "\n", "' '", ".", "join", "(", "map", "(", "lambda", "x", ":", "str", "(", "utils", ".", "item", "(", "x", ")", ")", ",", "alignment", ")", ")", "\n", ")", ")", "\n", "\n", "# update running id counter", "\n", "", "", "", "start_id", "+=", "len", "(", "inputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq-stack-transformer.interactive.cli_main": [[186, 190], ["fairseq.options.get_generation_parser", "fairseq.options.parse_args_and_arch", "interactive.main"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.get_generation_parser", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.parse_args_and_arch", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.main"], ["", "", "def", "cli_main", "(", ")", ":", "\n", "    ", "parser", "=", "options", ".", "get_generation_parser", "(", "interactive", "=", "True", ")", "\n", "args", "=", "options", ".", "parse_args_and_arch", "(", "parser", ")", "\n", "main", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq-stack-transformer.train.main": [[22, 106], ["fairseq.utils.import_user_module", "torch.manual_seed", "fairseq.distributed_utils.is_master", "print", "fairseq.tasks.setup_task", "args.valid_subset.split", "tasks.setup_task.build_model", "tasks.setup_task.build_criterion", "print", "print", "print", "fairseq.trainer.Trainer", "print", "print", "fairseq.checkpoint_utils.load_checkpoint", "fairseq.trainer.Trainer.get_lr", "fairseq.meters.StopwatchMeter", "fairseq.meters.StopwatchMeter.start", "args.valid_subset.split", "fairseq.meters.StopwatchMeter.stop", "print", "torch.cuda.is_available", "torch.cuda.set_device", "fairseq.distributed_utils.distributed_init", "fairseq.checkpoint_utils.verify_checkpoint_directory", "tasks.setup_task.load_dataset", "train.train", "fairseq.trainer.Trainer.lr_step", "sum", "sum", "fairseq.trainer.Trainer.get_num_updates", "train.validate", "fairseq.checkpoint_utils.save_checkpoint", "getattr", "fairseq.trainer.Trainer.get_train_iterator", "p.numel", "p.numel", "task.build_model.parameters", "task.build_model.parameters"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.import_user_module", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.distributed_utils.is_master", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.sentence_prediction.SentencePredictionTask.setup_task", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv_lm.LightConvLanguageModel.build_model", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.composite_loss.CompositeLoss.build_criterion", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.checkpoint_utils.load_checkpoint", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.get_lr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.meters.StopwatchMeter.start", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.meters.StopwatchMeter.stop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.distributed_utils.distributed_init", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.checkpoint_utils.verify_checkpoint_directory", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.sentence_prediction.SentencePredictionTask.load_dataset", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq-stack-transformer.train.train", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.lr_step", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.bmuf.FairseqBMUF.get_num_updates", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq-stack-transformer.train.validate", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.checkpoint_utils.save_checkpoint", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.get_train_iterator"], ["def", "main", "(", "args", ",", "init_distributed", "=", "False", ")", ":", "\n", "    ", "utils", ".", "import_user_module", "(", "args", ")", "\n", "\n", "assert", "args", ".", "max_tokens", "is", "not", "None", "or", "args", ".", "max_sentences", "is", "not", "None", ",", "'Must specify batch size either with --max-tokens or --max-sentences'", "\n", "\n", "# Initialize CUDA and distributed training", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "cpu", ":", "\n", "        ", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "device_id", ")", "\n", "", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "if", "init_distributed", ":", "\n", "        ", "args", ".", "distributed_rank", "=", "distributed_utils", ".", "distributed_init", "(", "args", ")", "\n", "\n", "", "if", "distributed_utils", ".", "is_master", "(", "args", ")", ":", "\n", "        ", "checkpoint_utils", ".", "verify_checkpoint_directory", "(", "args", ".", "save_dir", ")", "\n", "\n", "# Print args", "\n", "", "print", "(", "args", ")", "\n", "\n", "# Setup task, e.g., translation, language modeling, etc.", "\n", "task", "=", "tasks", ".", "setup_task", "(", "args", ")", "\n", "\n", "# Load valid dataset (we load training data below, based on the latest checkpoint)", "\n", "for", "valid_sub_split", "in", "args", ".", "valid_subset", ".", "split", "(", "','", ")", ":", "\n", "        ", "task", ".", "load_dataset", "(", "valid_sub_split", ",", "combine", "=", "False", ",", "epoch", "=", "0", ")", "\n", "\n", "# Build model and criterion", "\n", "", "model", "=", "task", ".", "build_model", "(", "args", ")", "\n", "criterion", "=", "task", ".", "build_criterion", "(", "args", ")", "\n", "print", "(", "model", ")", "\n", "print", "(", "'| model {}, criterion {}'", ".", "format", "(", "args", ".", "arch", ",", "criterion", ".", "__class__", ".", "__name__", ")", ")", "\n", "print", "(", "'| num. model params: {} (num. trained: {})'", ".", "format", "(", "\n", "sum", "(", "p", ".", "numel", "(", ")", "for", "p", "in", "model", ".", "parameters", "(", ")", ")", ",", "\n", "sum", "(", "p", ".", "numel", "(", ")", "for", "p", "in", "model", ".", "parameters", "(", ")", "if", "p", ".", "requires_grad", ")", ",", "\n", ")", ")", "\n", "\n", "# Build trainer", "\n", "trainer", "=", "Trainer", "(", "args", ",", "task", ",", "model", ",", "criterion", ")", "\n", "print", "(", "'| training on {} GPUs'", ".", "format", "(", "args", ".", "distributed_world_size", ")", ")", "\n", "print", "(", "'| max tokens per GPU = {} and max sentences per GPU = {}'", ".", "format", "(", "\n", "args", ".", "max_tokens", ",", "\n", "args", ".", "max_sentences", ",", "\n", ")", ")", "\n", "\n", "# Load the latest checkpoint if one is available and restore the", "\n", "# corresponding train iterator", "\n", "extra_state", ",", "epoch_itr", "=", "checkpoint_utils", ".", "load_checkpoint", "(", "args", ",", "trainer", ")", "\n", "\n", "# Train until the learning rate gets too small", "\n", "max_epoch", "=", "args", ".", "max_epoch", "or", "math", ".", "inf", "\n", "max_update", "=", "args", ".", "max_update", "or", "math", ".", "inf", "\n", "lr", "=", "trainer", ".", "get_lr", "(", ")", "\n", "train_meter", "=", "StopwatchMeter", "(", ")", "\n", "train_meter", ".", "start", "(", ")", "\n", "valid_losses", "=", "[", "None", "]", "\n", "valid_subsets", "=", "args", ".", "valid_subset", ".", "split", "(", "','", ")", "\n", "while", "lr", ">", "args", ".", "min_lr", "and", "epoch_itr", ".", "epoch", "<", "max_epoch", "and", "trainer", ".", "get_num_updates", "(", ")", "<", "max_update", ":", "\n", "# train for one epoch", "\n", "        ", "train", "(", "args", ",", "trainer", ",", "task", ",", "epoch_itr", ")", "\n", "\n", "if", "(", "\n", "not", "args", ".", "disable_validation", "\n", "and", "epoch_itr", ".", "epoch", "%", "args", ".", "validate_interval", "==", "0", "\n", "and", "epoch_itr", ".", "epoch", ">", "args", ".", "burnthrough", "\n", ")", ":", "\n", "            ", "valid_losses", "=", "validate", "(", "args", ",", "trainer", ",", "task", ",", "epoch_itr", ",", "valid_subsets", ")", "\n", "", "else", ":", "\n", "            ", "valid_losses", "=", "[", "None", "]", "\n", "\n", "# only use first validation loss to update the learning rate", "\n", "", "lr", "=", "trainer", ".", "lr_step", "(", "epoch_itr", ".", "epoch", ",", "valid_losses", "[", "0", "]", ")", "\n", "\n", "# save checkpoint", "\n", "if", "(", "\n", "epoch_itr", ".", "epoch", "%", "args", ".", "save_interval", "==", "0", "\n", "and", "epoch_itr", ".", "epoch", ">", "args", ".", "burnthrough", "\n", ")", ":", "\n", "            ", "checkpoint_utils", ".", "save_checkpoint", "(", "args", ",", "trainer", ",", "epoch_itr", ",", "valid_losses", "[", "0", "]", ")", "\n", "\n", "", "if", "':'", "in", "getattr", "(", "args", ",", "'data'", ",", "''", ")", ":", "\n", "# sharded data: get train iterator for next epoch", "\n", "            ", "epoch_itr", "=", "trainer", ".", "get_train_iterator", "(", "epoch_itr", ".", "epoch", ")", "\n", "", "", "train_meter", ".", "stop", "(", ")", "\n", "print", "(", "'| done training in {:.1f} seconds'", ".", "format", "(", "train_meter", ".", "sum", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq-stack-transformer.train.train": [[108, 184], ["epoch_itr.next_epoch_itr", "fairseq.data.iterators.GroupedIterator", "fairseq.progress_bar.build_progress_bar", "collections.defaultdict", "args.valid_subset.split", "enumerate", "train.get_training_stats", "collections.defaultdict.items", "progress_bar.build_progress_bar.print", "trainer.train_step", "train.get_training_stats", "trainer.train_step.items", "progress_bar.build_progress_bar.log", "trainer.get_num_updates", "trainer.get_meter", "len", "fairseq.meters.AverageMeter", "trainer.get_meter().reset", "train.validate", "fairseq.checkpoint_utils.save_checkpoint", "trainer.get_meter.reset", "extra_meters[].update", "extra_meters[].update", "trainer.get_meter"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.EpochBatchIterator.next_epoch_itr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.build_progress_bar", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq-stack-transformer.train.get_training_stats", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.train_step", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq-stack-transformer.train.get_training_stats", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.bmuf.FairseqBMUF.get_num_updates", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.reset", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq-stack-transformer.train.validate", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.checkpoint_utils.save_checkpoint", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.reset", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.get_meter"], ["", "def", "train", "(", "args", ",", "trainer", ",", "task", ",", "epoch_itr", ")", ":", "\n", "    ", "\"\"\"Train the model for one epoch.\"\"\"", "\n", "# Update parameters every N batches", "\n", "update_freq", "=", "args", ".", "update_freq", "[", "epoch_itr", ".", "epoch", "-", "1", "]", "if", "epoch_itr", ".", "epoch", "<=", "len", "(", "args", ".", "update_freq", ")", "else", "args", ".", "update_freq", "[", "-", "1", "]", "\n", "\n", "# Initialize data iterator", "\n", "itr", "=", "epoch_itr", ".", "next_epoch_itr", "(", "\n", "fix_batches_to_gpus", "=", "args", ".", "fix_batches_to_gpus", ",", "\n", "shuffle", "=", "(", "epoch_itr", ".", "epoch", ">=", "args", ".", "curriculum", ")", ",", "\n", ")", "\n", "itr", "=", "iterators", ".", "GroupedIterator", "(", "itr", ",", "update_freq", ")", "\n", "progress", "=", "progress_bar", ".", "build_progress_bar", "(", "\n", "args", ",", "itr", ",", "epoch_itr", ".", "epoch", ",", "no_progress_bar", "=", "'simple'", ",", "\n", ")", "\n", "\n", "#from fairseq.debug_tools import add_hook_to_model, break_on_nan", "\n", "#add_hook_to_model(trainer.model, break_on_nan)", "\n", "#from fairseq.debug_tools import check_data_iterator ", "\n", "#check_data_iterator(itr, source_dict=task.source_dictionary, target_dict=task.tgt_dict)", "\n", "\n", "extra_meters", "=", "collections", ".", "defaultdict", "(", "lambda", ":", "AverageMeter", "(", ")", ")", "\n", "valid_subsets", "=", "args", ".", "valid_subset", ".", "split", "(", "','", ")", "\n", "max_update", "=", "args", ".", "max_update", "or", "math", ".", "inf", "\n", "for", "i", ",", "samples", "in", "enumerate", "(", "progress", ",", "start", "=", "epoch_itr", ".", "iterations_in_epoch", ")", ":", "\n", "\n", "# sanity check batch", "\n", "# from fairseq.debug_tools import sanity_check_collated_batch", "\n", "# sanity_check_collated_batch(samples[0], task.datasets['train'])", "\n", "\n", "        ", "log_output", "=", "trainer", ".", "train_step", "(", "samples", ")", "\n", "if", "log_output", "is", "None", ":", "\n", "            ", "continue", "\n", "\n", "# log mid-epoch stats", "\n", "", "stats", "=", "get_training_stats", "(", "trainer", ")", "\n", "for", "k", ",", "v", "in", "log_output", ".", "items", "(", ")", ":", "\n", "            ", "if", "k", "in", "[", "'loss'", ",", "'nll_loss'", ",", "'ntokens'", ",", "'nsentences'", ",", "'sample_size'", "]", ":", "\n", "                ", "continue", "# these are already logged above", "\n", "", "if", "'loss'", "in", "k", "or", "k", "==", "'accuracy'", ":", "\n", "                ", "extra_meters", "[", "k", "]", ".", "update", "(", "v", ",", "log_output", "[", "'sample_size'", "]", ")", "\n", "", "else", ":", "\n", "                ", "extra_meters", "[", "k", "]", ".", "update", "(", "v", ")", "\n", "", "stats", "[", "k", "]", "=", "extra_meters", "[", "k", "]", ".", "avg", "\n", "", "progress", ".", "log", "(", "stats", ",", "tag", "=", "'train'", ",", "step", "=", "stats", "[", "'num_updates'", "]", ")", "\n", "\n", "# ignore the first mini-batch in words-per-second calculation", "\n", "if", "i", "==", "0", ":", "\n", "            ", "trainer", ".", "get_meter", "(", "'wps'", ")", ".", "reset", "(", ")", "\n", "\n", "", "num_updates", "=", "trainer", ".", "get_num_updates", "(", ")", "\n", "if", "(", "\n", "not", "args", ".", "disable_validation", "\n", "and", "args", ".", "save_interval_updates", ">", "0", "\n", "and", "num_updates", "%", "args", ".", "save_interval_updates", "==", "0", "\n", "and", "num_updates", ">", "0", "\n", ")", ":", "\n", "            ", "valid_losses", "=", "validate", "(", "args", ",", "trainer", ",", "task", ",", "epoch_itr", ",", "valid_subsets", ")", "\n", "checkpoint_utils", ".", "save_checkpoint", "(", "args", ",", "trainer", ",", "epoch_itr", ",", "valid_losses", "[", "0", "]", ")", "\n", "\n", "", "if", "num_updates", ">=", "max_update", ":", "\n", "            ", "break", "\n", "\n", "# log end-of-epoch stats", "\n", "", "", "stats", "=", "get_training_stats", "(", "trainer", ")", "\n", "for", "k", ",", "meter", "in", "extra_meters", ".", "items", "(", ")", ":", "\n", "        ", "stats", "[", "k", "]", "=", "meter", ".", "avg", "\n", "", "progress", ".", "print", "(", "stats", ",", "tag", "=", "'train'", ",", "step", "=", "stats", "[", "'num_updates'", "]", ")", "\n", "\n", "# reset training meters", "\n", "for", "k", "in", "[", "\n", "'train_loss'", ",", "'train_nll_loss'", ",", "'wps'", ",", "'ups'", ",", "'wpb'", ",", "'bsz'", ",", "'gnorm'", ",", "'clip'", ",", "\n", "]", ":", "\n", "        ", "meter", "=", "trainer", ".", "get_meter", "(", "k", ")", "\n", "if", "meter", "is", "not", "None", ":", "\n", "            ", "meter", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq-stack-transformer.train.get_training_stats": [[186, 209], ["collections.OrderedDict", "trainer.get_meter", "fairseq.utils.get_perplexity", "trainer.get_meter", "trainer.get_meter", "trainer.get_meter", "trainer.get_meter", "trainer.get_num_updates", "trainer.get_lr", "trainer.get_meter", "trainer.get_meter", "trainer.get_meter", "round", "trainer.get_meter", "trainer.get_meter", "trainer.get_meter", "trainer.get_meter", "trainer.get_meter", "trainer.get_meter", "trainer.get_meter"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.get_perplexity", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.bmuf.FairseqBMUF.get_num_updates", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.get_lr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.get_meter"], ["", "", "", "def", "get_training_stats", "(", "trainer", ")", ":", "\n", "    ", "stats", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "stats", "[", "'loss'", "]", "=", "trainer", ".", "get_meter", "(", "'train_loss'", ")", "\n", "if", "trainer", ".", "get_meter", "(", "'train_nll_loss'", ")", ".", "count", ">", "0", ":", "\n", "        ", "nll_loss", "=", "trainer", ".", "get_meter", "(", "'train_nll_loss'", ")", "\n", "stats", "[", "'nll_loss'", "]", "=", "nll_loss", "\n", "", "else", ":", "\n", "        ", "nll_loss", "=", "trainer", ".", "get_meter", "(", "'train_loss'", ")", "\n", "", "stats", "[", "'ppl'", "]", "=", "utils", ".", "get_perplexity", "(", "nll_loss", ".", "avg", ")", "\n", "stats", "[", "'wps'", "]", "=", "trainer", ".", "get_meter", "(", "'wps'", ")", "\n", "stats", "[", "'ups'", "]", "=", "trainer", ".", "get_meter", "(", "'ups'", ")", "\n", "stats", "[", "'wpb'", "]", "=", "trainer", ".", "get_meter", "(", "'wpb'", ")", "\n", "stats", "[", "'bsz'", "]", "=", "trainer", ".", "get_meter", "(", "'bsz'", ")", "\n", "stats", "[", "'num_updates'", "]", "=", "trainer", ".", "get_num_updates", "(", ")", "\n", "stats", "[", "'lr'", "]", "=", "trainer", ".", "get_lr", "(", ")", "\n", "stats", "[", "'gnorm'", "]", "=", "trainer", ".", "get_meter", "(", "'gnorm'", ")", "\n", "stats", "[", "'clip'", "]", "=", "trainer", ".", "get_meter", "(", "'clip'", ")", "\n", "stats", "[", "'oom'", "]", "=", "trainer", ".", "get_meter", "(", "'oom'", ")", "\n", "if", "trainer", ".", "get_meter", "(", "'loss_scale'", ")", "is", "not", "None", ":", "\n", "        ", "stats", "[", "'loss_scale'", "]", "=", "trainer", ".", "get_meter", "(", "'loss_scale'", ")", "\n", "", "stats", "[", "'wall'", "]", "=", "round", "(", "trainer", ".", "get_meter", "(", "'wall'", ")", ".", "elapsed_time", ")", "\n", "stats", "[", "'train_wall'", "]", "=", "trainer", ".", "get_meter", "(", "'train_wall'", ")", "\n", "return", "stats", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq-stack-transformer.train.validate": [[211, 264], ["task.get_batch_iterator().next_epoch_itr", "fairseq.progress_bar.build_progress_bar", "collections.defaultdict", "train.get_valid_stats", "collections.defaultdict.items", "progress_bar.build_progress_bar.print", "valid_losses.append", "trainer.get_meter", "trainer.valid_step", "trainer.valid_step.items", "task.get_batch_iterator", "trainer.get_meter.reset", "fairseq.meters.AverageMeter", "extra_meters[].update", "trainer.get_num_updates", "task.dataset", "fairseq.utils.resolve_max_positions", "task.max_positions", "trainer.get_model().max_positions", "trainer.get_model"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.EpochBatchIterator.next_epoch_itr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.build_progress_bar", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq-stack-transformer.train.get_valid_stats", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.valid_step", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.fairseq_task.FairseqTask.get_batch_iterator", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.reset", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.bmuf.FairseqBMUF.get_num_updates", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.fairseq_task.FairseqTask.dataset", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.resolve_max_positions", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.max_positions", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.max_positions", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.get_model"], ["", "def", "validate", "(", "args", ",", "trainer", ",", "task", ",", "epoch_itr", ",", "subsets", ")", ":", "\n", "    ", "\"\"\"Evaluate the model on the validation set(s) and return the losses.\"\"\"", "\n", "valid_losses", "=", "[", "]", "\n", "for", "subset", "in", "subsets", ":", "\n", "# Initialize data iterator", "\n", "        ", "itr", "=", "task", ".", "get_batch_iterator", "(", "\n", "dataset", "=", "task", ".", "dataset", "(", "subset", ")", ",", "\n", "max_tokens", "=", "args", ".", "max_tokens_valid", ",", "\n", "max_sentences", "=", "args", ".", "max_sentences_valid", ",", "\n", "max_positions", "=", "utils", ".", "resolve_max_positions", "(", "\n", "task", ".", "max_positions", "(", ")", ",", "\n", "trainer", ".", "get_model", "(", ")", ".", "max_positions", "(", ")", ",", "\n", ")", ",", "\n", "ignore_invalid_inputs", "=", "args", ".", "skip_invalid_size_inputs_valid_test", ",", "\n", "required_batch_size_multiple", "=", "args", ".", "required_batch_size_multiple", ",", "\n", "seed", "=", "args", ".", "seed", ",", "\n", "num_shards", "=", "args", ".", "distributed_world_size", ",", "\n", "shard_id", "=", "args", ".", "distributed_rank", ",", "\n", "num_workers", "=", "args", ".", "num_workers", ",", "\n", ")", ".", "next_epoch_itr", "(", "shuffle", "=", "False", ")", "\n", "progress", "=", "progress_bar", ".", "build_progress_bar", "(", "\n", "args", ",", "itr", ",", "epoch_itr", ".", "epoch", ",", "\n", "prefix", "=", "'valid on \\'{}\\' subset'", ".", "format", "(", "subset", ")", ",", "\n", "no_progress_bar", "=", "'simple'", "\n", ")", "\n", "\n", "# reset validation loss meters", "\n", "for", "k", "in", "[", "'valid_loss'", ",", "'valid_nll_loss'", "]", ":", "\n", "            ", "meter", "=", "trainer", ".", "get_meter", "(", "k", ")", "\n", "if", "meter", "is", "not", "None", ":", "\n", "                ", "meter", ".", "reset", "(", ")", "\n", "", "", "extra_meters", "=", "collections", ".", "defaultdict", "(", "lambda", ":", "AverageMeter", "(", ")", ")", "\n", "\n", "for", "sample", "in", "progress", ":", "\n", "            ", "log_output", "=", "trainer", ".", "valid_step", "(", "sample", ")", "\n", "\n", "for", "k", ",", "v", "in", "log_output", ".", "items", "(", ")", ":", "\n", "                ", "if", "k", "in", "[", "'loss'", ",", "'nll_loss'", ",", "'ntokens'", ",", "'nsentences'", ",", "'sample_size'", "]", ":", "\n", "                    ", "continue", "\n", "", "extra_meters", "[", "k", "]", ".", "update", "(", "v", ")", "\n", "\n", "# log validation stats", "\n", "", "", "stats", "=", "get_valid_stats", "(", "trainer", ",", "args", ",", "extra_meters", ")", "\n", "for", "k", ",", "meter", "in", "extra_meters", ".", "items", "(", ")", ":", "\n", "            ", "stats", "[", "k", "]", "=", "meter", ".", "avg", "\n", "", "progress", ".", "print", "(", "stats", ",", "tag", "=", "subset", ",", "step", "=", "trainer", ".", "get_num_updates", "(", ")", ")", "\n", "\n", "valid_losses", ".", "append", "(", "\n", "stats", "[", "args", ".", "best_checkpoint_metric", "]", ".", "avg", "\n", "if", "args", ".", "best_checkpoint_metric", "==", "'loss'", "\n", "else", "stats", "[", "args", ".", "best_checkpoint_metric", "]", "\n", ")", "\n", "", "return", "valid_losses", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq-stack-transformer.train.get_valid_stats": [[266, 295], ["collections.OrderedDict", "trainer.get_meter", "fairseq.utils.get_perplexity", "trainer.get_num_updates", "hasattr", "trainer.get_meter", "best_function", "trainer.get_meter", "ValueError"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.get_perplexity", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.bmuf.FairseqBMUF.get_num_updates", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.get_meter", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.get_meter"], ["", "def", "get_valid_stats", "(", "trainer", ",", "args", ",", "extra_meters", "=", "None", ")", ":", "\n", "    ", "stats", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "stats", "[", "'loss'", "]", "=", "trainer", ".", "get_meter", "(", "'valid_loss'", ")", "\n", "if", "trainer", ".", "get_meter", "(", "'valid_nll_loss'", ")", ".", "count", ">", "0", ":", "\n", "        ", "nll_loss", "=", "trainer", ".", "get_meter", "(", "'valid_nll_loss'", ")", "\n", "stats", "[", "'nll_loss'", "]", "=", "nll_loss", "\n", "", "else", ":", "\n", "        ", "nll_loss", "=", "stats", "[", "'loss'", "]", "\n", "", "stats", "[", "'ppl'", "]", "=", "utils", ".", "get_perplexity", "(", "nll_loss", ".", "avg", ")", "\n", "stats", "[", "'num_updates'", "]", "=", "trainer", ".", "get_num_updates", "(", ")", "\n", "if", "hasattr", "(", "checkpoint_utils", ".", "save_checkpoint", ",", "'best'", ")", ":", "\n", "        ", "key", "=", "'best_{0}'", ".", "format", "(", "args", ".", "best_checkpoint_metric", ")", "\n", "best_function", "=", "max", "if", "args", ".", "maximize_best_checkpoint_metric", "else", "min", "\n", "\n", "current_metric", "=", "None", "\n", "if", "args", ".", "best_checkpoint_metric", "==", "'loss'", ":", "\n", "            ", "current_metric", "=", "stats", "[", "'loss'", "]", ".", "avg", "\n", "", "elif", "args", ".", "best_checkpoint_metric", "in", "extra_meters", ":", "\n", "            ", "current_metric", "=", "extra_meters", "[", "args", ".", "best_checkpoint_metric", "]", ".", "avg", "\n", "", "elif", "args", ".", "best_checkpoint_metric", "in", "stats", ":", "\n", "            ", "current_metric", "=", "stats", "[", "args", ".", "best_checkpoint_metric", "]", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"best_checkpoint_metric not found in logs\"", ")", "\n", "\n", "", "stats", "[", "key", "]", "=", "best_function", "(", "\n", "checkpoint_utils", ".", "save_checkpoint", ".", "best", ",", "\n", "current_metric", ",", "\n", ")", "\n", "", "return", "stats", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq-stack-transformer.train.distributed_main": [[297, 302], ["train.main"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.main"], ["", "def", "distributed_main", "(", "i", ",", "args", ",", "start_rank", "=", "0", ")", ":", "\n", "    ", "args", ".", "device_id", "=", "i", "\n", "if", "args", ".", "distributed_rank", "is", "None", ":", "# torch.multiprocessing.spawn", "\n", "        ", "args", ".", "distributed_rank", "=", "start_rank", "+", "i", "\n", "", "main", "(", "args", ",", "init_distributed", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq-stack-transformer.train.cli_main": [[304, 339], ["fairseq.options.get_training_parser", "fairseq.options.parse_args_and_arch", "fairseq.distributed_utils.infer_init_method", "torch.multiprocessing.spawn", "train.distributed_main", "random.randint", "torch.multiprocessing.spawn", "train.main", "torch.cuda.device_count", "torch.cuda.device_count", "print", "torch.cuda.device_count", "max"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.get_training_parser", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.parse_args_and_arch", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.distributed_utils.infer_init_method", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq-stack-transformer.train.distributed_main", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.main", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["", "def", "cli_main", "(", ")", ":", "\n", "    ", "parser", "=", "options", ".", "get_training_parser", "(", ")", "\n", "args", "=", "options", ".", "parse_args_and_arch", "(", "parser", ")", "\n", "\n", "if", "args", ".", "distributed_init_method", "is", "None", ":", "\n", "        ", "distributed_utils", ".", "infer_init_method", "(", "args", ")", "\n", "\n", "", "if", "args", ".", "distributed_init_method", "is", "not", "None", ":", "\n", "# distributed training", "\n", "        ", "if", "torch", ".", "cuda", ".", "device_count", "(", ")", ">", "1", "and", "not", "args", ".", "distributed_no_spawn", ":", "\n", "            ", "start_rank", "=", "args", ".", "distributed_rank", "\n", "args", ".", "distributed_rank", "=", "None", "# assign automatically", "\n", "torch", ".", "multiprocessing", ".", "spawn", "(", "\n", "fn", "=", "distributed_main", ",", "\n", "args", "=", "(", "args", ",", "start_rank", ")", ",", "\n", "nprocs", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "distributed_main", "(", "args", ".", "device_id", ",", "args", ")", "\n", "", "", "elif", "args", ".", "distributed_world_size", ">", "1", ":", "\n", "# fallback for single node with multiple GPUs", "\n", "        ", "assert", "args", ".", "distributed_world_size", "<=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "port", "=", "random", ".", "randint", "(", "10000", ",", "20000", ")", "\n", "args", ".", "distributed_init_method", "=", "'tcp://localhost:{port}'", ".", "format", "(", "port", "=", "port", ")", "\n", "args", ".", "distributed_rank", "=", "None", "# set based on device id", "\n", "if", "max", "(", "args", ".", "update_freq", ")", ">", "1", "and", "args", ".", "ddp_backend", "!=", "'no_c10d'", ":", "\n", "            ", "print", "(", "'| NOTE: you may get better performance with: --ddp-backend=no_c10d'", ")", "\n", "", "torch", ".", "multiprocessing", ".", "spawn", "(", "\n", "fn", "=", "distributed_main", ",", "\n", "args", "=", "(", "args", ",", ")", ",", "\n", "nprocs", "=", "args", ".", "distributed_world_size", ",", "\n", ")", "\n", "", "else", ":", "\n", "# single GPU training", "\n", "        ", "main", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq-stack-transformer.preprocess.main": [[26, 261], ["fairseq.utils.import_user_module", "print", "os.makedirs", "fairseq.tasks.get_task", "build_dictionary.save", "preprocess.main.make_all"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.import_user_module", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.__init__.get_task", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.save"], ["def", "main", "(", "args", ")", ":", "\n", "    ", "utils", ".", "import_user_module", "(", "args", ")", "\n", "\n", "print", "(", "args", ")", "\n", "\n", "os", ".", "makedirs", "(", "args", ".", "destdir", ",", "exist_ok", "=", "True", ")", "\n", "target", "=", "not", "args", ".", "only_source", "\n", "\n", "task", "=", "tasks", ".", "get_task", "(", "args", ".", "task", ")", "\n", "\n", "\n", "\n", "# Determine tokenizer", "\n", "if", "args", ".", "tokenize_by_whitespace", ":", "\n", "# for the rest normal tokenization", "\n", "        ", "tokenize", "=", "tokenize_line", "\n", "", "else", ":", "\n", "# for AMR we expect tokenization by tab", "\n", "        ", "tokenize", "=", "tab_tokenize", "\n", "\n", "", "def", "train_path", "(", "lang", ")", ":", "\n", "        ", "return", "\"{}{}\"", ".", "format", "(", "args", ".", "trainpref", ",", "(", "\".\"", "+", "lang", ")", "if", "lang", "else", "\"\"", ")", "\n", "\n", "", "def", "valid_path", "(", "lang", ")", ":", "\n", "        ", "return", "\"{}{}\"", ".", "format", "(", "args", ".", "validpref", ",", "(", "\".\"", "+", "lang", ")", "if", "lang", "else", "\"\"", ")", "\n", "\n", "", "def", "file_name", "(", "prefix", ",", "lang", ")", ":", "\n", "        ", "fname", "=", "prefix", "\n", "if", "lang", "is", "not", "None", ":", "\n", "            ", "fname", "+=", "\".{lang}\"", ".", "format", "(", "lang", "=", "lang", ")", "\n", "", "return", "fname", "\n", "\n", "", "def", "dest_path", "(", "prefix", ",", "lang", ")", ":", "\n", "        ", "return", "os", ".", "path", ".", "join", "(", "args", ".", "destdir", ",", "file_name", "(", "prefix", ",", "lang", ")", ")", "\n", "\n", "", "def", "dict_path", "(", "lang", ")", ":", "\n", "        ", "return", "dest_path", "(", "\"dict\"", ",", "lang", ")", "+", "\".txt\"", "\n", "\n", "", "def", "build_dictionary", "(", "filenames", ",", "src", "=", "False", ",", "tgt", "=", "False", ",", "tokenize", "=", "tokenize", ")", ":", "\n", "        ", "assert", "src", "^", "tgt", "\n", "return", "task", ".", "build_dictionary", "(", "\n", "filenames", ",", "\n", "workers", "=", "args", ".", "workers", ",", "\n", "threshold", "=", "args", ".", "thresholdsrc", "if", "src", "else", "args", ".", "thresholdtgt", ",", "\n", "nwords", "=", "args", ".", "nwordssrc", "if", "src", "else", "args", ".", "nwordstgt", ",", "\n", "padding_factor", "=", "args", ".", "padding_factor", ",", "\n", "tokenize", "=", "tokenize", "\n", ")", "\n", "\n", "", "if", "not", "args", ".", "srcdict", "and", "os", ".", "path", ".", "exists", "(", "dict_path", "(", "args", ".", "source_lang", ")", ")", ":", "\n", "        ", "raise", "FileExistsError", "(", "dict_path", "(", "args", ".", "source_lang", ")", ")", "\n", "", "if", "target", "and", "not", "args", ".", "tgtdict", "and", "os", ".", "path", ".", "exists", "(", "dict_path", "(", "args", ".", "target_lang", ")", ")", ":", "\n", "        ", "raise", "FileExistsError", "(", "dict_path", "(", "args", ".", "target_lang", ")", ")", "\n", "\n", "", "if", "args", ".", "joined_dictionary", ":", "\n", "        ", "assert", "not", "args", ".", "srcdict", "or", "not", "args", ".", "tgtdict", ",", "\"cannot use both --srcdict and --tgtdict with --joined-dictionary\"", "\n", "\n", "if", "args", ".", "srcdict", ":", "\n", "            ", "src_dict", "=", "task", ".", "load_dictionary", "(", "args", ".", "srcdict", ")", "\n", "", "elif", "args", ".", "tgtdict", ":", "\n", "            ", "src_dict", "=", "task", ".", "load_dictionary", "(", "args", ".", "tgtdict", ")", "\n", "", "else", ":", "\n", "            ", "assert", "args", ".", "trainpref", ",", "\"--trainpref must be set if --srcdict is not specified\"", "\n", "src_dict", "=", "build_dictionary", "(", "\n", "{", "train_path", "(", "lang", ")", "for", "lang", "in", "[", "args", ".", "source_lang", ",", "args", ".", "target_lang", "]", "}", ",", "src", "=", "True", ",", "tokenize", "=", "tokenize", "\n", ")", "\n", "", "tgt_dict", "=", "src_dict", "\n", "", "else", ":", "\n", "        ", "if", "args", ".", "srcdict", ":", "\n", "            ", "src_dict", "=", "task", ".", "load_dictionary", "(", "args", ".", "srcdict", ")", "\n", "", "else", ":", "\n", "            ", "assert", "args", ".", "trainpref", ",", "\"--trainpref must be set if --srcdict is not specified\"", "\n", "# FIXME: Hacky way to avoid <unks>, add dev", "\n", "# As long as we use only BERT for input this should be fine", "\n", "dict_paths", "=", "[", "train_path", "(", "args", ".", "source_lang", ")", ",", "valid_path", "(", "args", ".", "source_lang", ")", "]", "\n", "src_dict", "=", "build_dictionary", "(", "dict_paths", ",", "src", "=", "True", ",", "tokenize", "=", "tokenize", ")", "\n", "\n", "", "if", "target", ":", "\n", "            ", "if", "args", ".", "tgtdict", ":", "\n", "                ", "tgt_dict", "=", "task", ".", "load_dictionary", "(", "args", ".", "tgtdict", ")", "\n", "", "else", ":", "\n", "                ", "assert", "args", ".", "trainpref", ",", "\"--trainpref must be set if --tgtdict is not specified\"", "\n", "# Hacky way to avoid <unks>, add dev", "\n", "# dict_paths = [train_path(args.target_lang), valid_path(args.target_lang)]", "\n", "dict_paths", "=", "[", "train_path", "(", "args", ".", "target_lang", ")", "]", "\n", "tgt_dict", "=", "build_dictionary", "(", "dict_paths", ",", "tgt", "=", "True", ",", "tokenize", "=", "tokenize", ")", "\n", "", "", "else", ":", "\n", "            ", "tgt_dict", "=", "None", "\n", "\n", "", "", "src_dict", ".", "save", "(", "dict_path", "(", "args", ".", "source_lang", ")", ")", "\n", "if", "target", "and", "tgt_dict", "is", "not", "None", ":", "\n", "        ", "tgt_dict", ".", "save", "(", "dict_path", "(", "args", ".", "target_lang", ")", ")", "\n", "\n", "", "def", "make_binary_dataset", "(", "vocab", ",", "input_prefix", ",", "output_prefix", ",", "lang", ",", "num_workers", ")", ":", "\n", "        ", "print", "(", "\"| [{}] Dictionary: {} types\"", ".", "format", "(", "lang", ",", "len", "(", "vocab", ")", "-", "1", ")", ")", "\n", "n_seq_tok", "=", "[", "0", ",", "0", "]", "\n", "replaced", "=", "Counter", "(", ")", "\n", "\n", "def", "merge_result", "(", "worker_result", ")", ":", "\n", "            ", "replaced", ".", "update", "(", "worker_result", "[", "\"replaced\"", "]", ")", "\n", "n_seq_tok", "[", "0", "]", "+=", "worker_result", "[", "\"nseq\"", "]", "\n", "n_seq_tok", "[", "1", "]", "+=", "worker_result", "[", "\"ntok\"", "]", "\n", "\n", "", "input_file", "=", "\"{}{}\"", ".", "format", "(", "\n", "input_prefix", ",", "(", "\".\"", "+", "lang", ")", "if", "lang", "is", "not", "None", "else", "\"\"", "\n", ")", "\n", "offsets", "=", "Binarizer", ".", "find_offsets", "(", "input_file", ",", "num_workers", ")", "\n", "pool", "=", "None", "\n", "if", "num_workers", ">", "1", ":", "\n", "            ", "raise", "Exception", "(", "\"to set tokenize, we can not allow num_workers > 1\"", ")", "\n", "pool", "=", "Pool", "(", "processes", "=", "num_workers", "-", "1", ")", "\n", "for", "worker_id", "in", "range", "(", "1", ",", "num_workers", ")", ":", "\n", "                ", "prefix", "=", "\"{}{}\"", ".", "format", "(", "output_prefix", ",", "worker_id", ")", "\n", "pool", ".", "apply_async", "(", "\n", "binarize", ",", "\n", "(", "\n", "args", ",", "\n", "input_file", ",", "\n", "vocab", ",", "\n", "prefix", ",", "\n", "lang", ",", "\n", "offsets", "[", "worker_id", "]", ",", "\n", "offsets", "[", "worker_id", "+", "1", "]", ",", "\n", "tokenize", "# this may fail", "\n", ")", ",", "\n", "callback", "=", "merge_result", "\n", ")", "\n", "", "pool", ".", "close", "(", ")", "\n", "\n", "", "ds", "=", "indexed_dataset", ".", "make_builder", "(", "dataset_dest_file", "(", "args", ",", "output_prefix", ",", "lang", ",", "\"bin\"", ")", ",", "\n", "impl", "=", "args", ".", "dataset_impl", ",", "vocab_size", "=", "len", "(", "vocab", ")", ")", "\n", "merge_result", "(", "\n", "Binarizer", ".", "binarize", "(", "\n", "input_file", ",", "vocab", ",", "lambda", "t", ":", "ds", ".", "add_item", "(", "t", ")", ",", "\n", "offset", "=", "0", ",", "end", "=", "offsets", "[", "1", "]", ",", "\n", "append_eos", "=", "False", ",", "\n", "tokenize", "=", "tokenize", "\n", ")", "\n", ")", "\n", "if", "num_workers", ">", "1", ":", "\n", "            ", "pool", ".", "join", "(", ")", "\n", "for", "worker_id", "in", "range", "(", "1", ",", "num_workers", ")", ":", "\n", "                ", "prefix", "=", "\"{}{}\"", ".", "format", "(", "output_prefix", ",", "worker_id", ")", "\n", "temp_file_path", "=", "dataset_dest_prefix", "(", "args", ",", "prefix", ",", "lang", ")", "\n", "ds", ".", "merge_file_", "(", "temp_file_path", ")", "\n", "os", ".", "remove", "(", "indexed_dataset", ".", "data_file_path", "(", "temp_file_path", ")", ")", "\n", "os", ".", "remove", "(", "indexed_dataset", ".", "index_file_path", "(", "temp_file_path", ")", ")", "\n", "\n", "", "", "ds", ".", "finalize", "(", "dataset_dest_file", "(", "args", ",", "output_prefix", ",", "lang", ",", "\"idx\"", ")", ")", "\n", "\n", "print", "(", "\n", "\"| [{}] {}: {} sents, {} tokens, {:.3}% replaced by {}\"", ".", "format", "(", "\n", "lang", ",", "\n", "input_file", ",", "\n", "n_seq_tok", "[", "0", "]", ",", "\n", "n_seq_tok", "[", "1", "]", ",", "\n", "100", "*", "sum", "(", "replaced", ".", "values", "(", ")", ")", "/", "n_seq_tok", "[", "1", "]", ",", "\n", "vocab", ".", "unk_word", ",", "\n", ")", "\n", ")", "\n", "\n", "", "def", "make_dataset", "(", "vocab", ",", "input_prefix", ",", "output_prefix", ",", "lang", ",", "num_workers", "=", "1", ")", ":", "\n", "        ", "if", "args", ".", "dataset_impl", "==", "\"raw\"", ":", "\n", "# Copy original text file to destination folder", "\n", "            ", "output_text_file", "=", "dest_path", "(", "\n", "output_prefix", "+", "\".{}-{}\"", ".", "format", "(", "args", ".", "source_lang", ",", "args", ".", "target_lang", ")", ",", "\n", "lang", ",", "\n", ")", "\n", "shutil", ".", "copyfile", "(", "file_name", "(", "input_prefix", ",", "lang", ")", ",", "output_text_file", ")", "\n", "", "else", ":", "\n", "            ", "make_binary_dataset", "(", "vocab", ",", "input_prefix", ",", "output_prefix", ",", "lang", ",", "num_workers", ")", "\n", "\n", "", "", "def", "make_all", "(", "lang", ",", "vocab", ")", ":", "\n", "        ", "if", "args", ".", "trainpref", ":", "\n", "            ", "make_dataset", "(", "vocab", ",", "args", ".", "trainpref", ",", "\"train\"", ",", "lang", ",", "num_workers", "=", "args", ".", "workers", ")", "\n", "", "if", "args", ".", "validpref", ":", "\n", "            ", "for", "k", ",", "validpref", "in", "enumerate", "(", "args", ".", "validpref", ".", "split", "(", "\",\"", ")", ")", ":", "\n", "                ", "outprefix", "=", "\"valid{}\"", ".", "format", "(", "k", ")", "if", "k", ">", "0", "else", "\"valid\"", "\n", "make_dataset", "(", "vocab", ",", "validpref", ",", "outprefix", ",", "lang", ",", "num_workers", "=", "args", ".", "workers", ")", "\n", "", "", "if", "args", ".", "testpref", ":", "\n", "            ", "for", "k", ",", "testpref", "in", "enumerate", "(", "args", ".", "testpref", ".", "split", "(", "\",\"", ")", ")", ":", "\n", "                ", "outprefix", "=", "\"test{}\"", ".", "format", "(", "k", ")", "if", "k", ">", "0", "else", "\"test\"", "\n", "make_dataset", "(", "vocab", ",", "testpref", ",", "outprefix", ",", "lang", ",", "num_workers", "=", "args", ".", "workers", ")", "\n", "\n", "", "", "", "make_all", "(", "args", ".", "source_lang", ",", "src_dict", ")", "\n", "if", "target", ":", "\n", "        ", "make_all", "(", "args", ".", "target_lang", ",", "tgt_dict", ")", "\n", "\n", "# Make preprocessing data for the state machine", "\n", "", "make_state_machine", "(", "args", ",", "src_dict", ",", "tgt_dict", ",", "tokenize", "=", "tokenize", ")", "\n", "\n", "print", "(", "\"| Wrote preprocessed data to {}\"", ".", "format", "(", "args", ".", "destdir", ")", ")", "\n", "\n", "if", "args", ".", "alignfile", ":", "\n", "        ", "assert", "args", ".", "trainpref", ",", "\"--trainpref must be set if --alignfile is specified\"", "\n", "src_file_name", "=", "train_path", "(", "args", ".", "source_lang", ")", "\n", "tgt_file_name", "=", "train_path", "(", "args", ".", "target_lang", ")", "\n", "freq_map", "=", "{", "}", "\n", "with", "open", "(", "args", ".", "alignfile", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "align_file", ":", "\n", "            ", "with", "open", "(", "src_file_name", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "src_file", ":", "\n", "                ", "with", "open", "(", "tgt_file_name", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "tgt_file", ":", "\n", "                    ", "for", "a", ",", "s", ",", "t", "in", "zip_longest", "(", "align_file", ",", "src_file", ",", "tgt_file", ")", ":", "\n", "                        ", "si", "=", "src_dict", ".", "encode_line", "(", "s", ",", "add_if_not_exist", "=", "False", ")", "\n", "ti", "=", "tgt_dict", ".", "encode_line", "(", "t", ",", "add_if_not_exist", "=", "False", ")", "\n", "ai", "=", "list", "(", "map", "(", "lambda", "x", ":", "tuple", "(", "x", ".", "split", "(", "\"-\"", ")", ")", ",", "a", ".", "split", "(", ")", ")", ")", "\n", "for", "sai", ",", "tai", "in", "ai", ":", "\n", "                            ", "srcidx", "=", "si", "[", "int", "(", "sai", ")", "]", "\n", "tgtidx", "=", "ti", "[", "int", "(", "tai", ")", "]", "\n", "if", "srcidx", "!=", "src_dict", ".", "unk", "(", ")", "and", "tgtidx", "!=", "tgt_dict", ".", "unk", "(", ")", ":", "\n", "                                ", "assert", "srcidx", "!=", "src_dict", ".", "pad", "(", ")", "\n", "assert", "srcidx", "!=", "src_dict", ".", "eos", "(", ")", "\n", "assert", "tgtidx", "!=", "tgt_dict", ".", "pad", "(", ")", "\n", "assert", "tgtidx", "!=", "tgt_dict", ".", "eos", "(", ")", "\n", "\n", "if", "srcidx", "not", "in", "freq_map", ":", "\n", "                                    ", "freq_map", "[", "srcidx", "]", "=", "{", "}", "\n", "", "if", "tgtidx", "not", "in", "freq_map", "[", "srcidx", "]", ":", "\n", "                                    ", "freq_map", "[", "srcidx", "]", "[", "tgtidx", "]", "=", "1", "\n", "", "else", ":", "\n", "                                    ", "freq_map", "[", "srcidx", "]", "[", "tgtidx", "]", "+=", "1", "\n", "\n", "", "", "", "", "", "", "", "align_dict", "=", "{", "}", "\n", "for", "srcidx", "in", "freq_map", ".", "keys", "(", ")", ":", "\n", "            ", "align_dict", "[", "srcidx", "]", "=", "max", "(", "freq_map", "[", "srcidx", "]", ",", "key", "=", "freq_map", "[", "srcidx", "]", ".", "get", ")", "\n", "\n", "", "with", "open", "(", "\n", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "destdir", ",", "\n", "\"alignment.{}-{}.txt\"", ".", "format", "(", "args", ".", "source_lang", ",", "args", ".", "target_lang", ")", ",", "\n", ")", ",", "\n", "\"w\"", ",", "encoding", "=", "'utf-8'", "\n", ")", "as", "f", ":", "\n", "            ", "for", "k", ",", "v", "in", "align_dict", ".", "items", "(", ")", ":", "\n", "                ", "print", "(", "\"{} {}\"", ".", "format", "(", "src_dict", "[", "k", "]", ",", "tgt_dict", "[", "v", "]", ")", ",", "file", "=", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq-stack-transformer.preprocess.binarize": [[266, 277], ["fairseq.data.indexed_dataset.make_builder", "fairseq.binarizer.Binarizer.binarize", "indexed_dataset.make_builder.finalize", "preprocess.dataset_dest_file", "indexed_dataset.make_builder.add_item", "preprocess.dataset_dest_file", "len"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.make_builder", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.binarizer.Binarizer.binarize", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDatasetBuilder.finalize", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.preprocess.dataset_dest_file", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDatasetBuilder.add_item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.preprocess.dataset_dest_file"], ["", "", "", "", "def", "binarize", "(", "args", ",", "filename", ",", "vocab", ",", "output_prefix", ",", "lang", ",", "offset", ",", "end", ",", "append_eos", "=", "True", ",", "tokenize", "=", "tokenize_line", ")", ":", "\n", "    ", "ds", "=", "indexed_dataset", ".", "make_builder", "(", "dataset_dest_file", "(", "args", ",", "output_prefix", ",", "lang", ",", "\"bin\"", ")", ",", "\n", "impl", "=", "args", ".", "dataset_impl", ",", "vocab_size", "=", "len", "(", "vocab", ")", ")", "\n", "\n", "def", "consumer", "(", "tensor", ")", ":", "\n", "        ", "ds", ".", "add_item", "(", "tensor", ")", "\n", "\n", "", "res", "=", "Binarizer", ".", "binarize", "(", "filename", ",", "vocab", ",", "consumer", ",", "append_eos", "=", "append_eos", ",", "\n", "offset", "=", "offset", ",", "end", "=", "end", ",", "tokenize", "=", "tokenize", ")", "\n", "ds", ".", "finalize", "(", "dataset_dest_file", "(", "args", ",", "output_prefix", ",", "lang", ",", "\"idx\"", ")", ")", "\n", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq-stack-transformer.preprocess.dataset_dest_prefix": [[279, 285], ["None"], "function", ["None"], ["", "def", "dataset_dest_prefix", "(", "args", ",", "output_prefix", ",", "lang", ")", ":", "\n", "    ", "base", "=", "\"{}/{}\"", ".", "format", "(", "args", ".", "destdir", ",", "output_prefix", ")", "\n", "lang_part", "=", "(", "\n", "\".{}-{}.{}\"", ".", "format", "(", "args", ".", "source_lang", ",", "args", ".", "target_lang", ",", "lang", ")", "if", "lang", "is", "not", "None", "else", "\"\"", "\n", ")", "\n", "return", "\"{}{}\"", ".", "format", "(", "base", ",", "lang_part", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq-stack-transformer.preprocess.dataset_dest_file": [[287, 290], ["preprocess.dataset_dest_prefix"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.preprocess.dataset_dest_prefix"], ["", "def", "dataset_dest_file", "(", "args", ",", "output_prefix", ",", "lang", ",", "extension", ")", ":", "\n", "    ", "base", "=", "dataset_dest_prefix", "(", "args", ",", "output_prefix", ",", "lang", ")", "\n", "return", "\"{}.{}\"", ".", "format", "(", "base", ",", "extension", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq-stack-transformer.preprocess.get_offsets": [[292, 294], ["fairseq.binarizer.Binarizer.find_offsets"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.binarizer.Binarizer.find_offsets"], ["", "def", "get_offsets", "(", "input_file", ",", "num_workers", ")", ":", "\n", "    ", "return", "Binarizer", ".", "find_offsets", "(", "input_file", ",", "num_workers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq-stack-transformer.preprocess.cli_main": [[296, 300], ["fairseq.options.get_preprocessing_parser", "options.get_preprocessing_parser.parse_args", "preprocess.main"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.get_preprocessing_parser", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.main"], ["", "def", "cli_main", "(", ")", ":", "\n", "    ", "parser", "=", "options", ".", "get_preprocessing_parser", "(", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "main", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq-stack-transformer.score.get_parser": [[18, 33], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument"], "function", ["None"], ["def", "get_parser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Command-line script for BLEU scoring.'", ")", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'-s'", ",", "'--sys'", ",", "default", "=", "'-'", ",", "help", "=", "'system output'", ")", "\n", "parser", ".", "add_argument", "(", "'-r'", ",", "'--ref'", ",", "required", "=", "True", ",", "help", "=", "'references'", ")", "\n", "parser", ".", "add_argument", "(", "'-o'", ",", "'--order'", ",", "default", "=", "4", ",", "metavar", "=", "'N'", ",", "\n", "type", "=", "int", ",", "help", "=", "'consider ngrams up to this order'", ")", "\n", "parser", ".", "add_argument", "(", "'--ignore-case'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'case-insensitive scoring'", ")", "\n", "parser", ".", "add_argument", "(", "'--sacrebleu'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'score with sacrebleu'", ")", "\n", "parser", ".", "add_argument", "(", "'--sentence-bleu'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'report sentence-level BLEUs (i.e., with +1 smoothing)'", ")", "\n", "# fmt: on", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq-stack-transformer.score.main": [[35, 85], ["score.get_parser", "get_parser.parse_args", "print", "os.path.exists", "fairseq.data.dictionary.Dictionary", "os.path.exists", "fd.readlines", "score.main.score"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.get_parser", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.readlines", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.bleu.Scorer.score"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "get_parser", "(", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "print", "(", "args", ")", "\n", "\n", "assert", "args", ".", "sys", "==", "'-'", "or", "os", ".", "path", ".", "exists", "(", "args", ".", "sys", ")", ",", "\"System output file {} does not exist\"", ".", "format", "(", "args", ".", "sys", ")", "\n", "assert", "os", ".", "path", ".", "exists", "(", "args", ".", "ref", ")", ",", "\"Reference file {} does not exist\"", ".", "format", "(", "args", ".", "ref", ")", "\n", "\n", "dict", "=", "dictionary", ".", "Dictionary", "(", ")", "\n", "\n", "def", "readlines", "(", "fd", ")", ":", "\n", "        ", "for", "line", "in", "fd", ".", "readlines", "(", ")", ":", "\n", "            ", "if", "args", ".", "ignore_case", ":", "\n", "                ", "yield", "line", ".", "lower", "(", ")", "\n", "", "else", ":", "\n", "                ", "yield", "line", "\n", "\n", "", "", "", "if", "args", ".", "sacrebleu", ":", "\n", "        ", "import", "sacrebleu", "\n", "\n", "def", "score", "(", "fdsys", ")", ":", "\n", "            ", "with", "open", "(", "args", ".", "ref", ")", "as", "fdref", ":", "\n", "                ", "print", "(", "sacrebleu", ".", "corpus_bleu", "(", "fdsys", ",", "[", "fdref", "]", ")", ")", "\n", "", "", "", "elif", "args", ".", "sentence_bleu", ":", "\n", "        ", "def", "score", "(", "fdsys", ")", ":", "\n", "            ", "with", "open", "(", "args", ".", "ref", ")", "as", "fdref", ":", "\n", "                ", "scorer", "=", "bleu", ".", "Scorer", "(", "dict", ".", "pad", "(", ")", ",", "dict", ".", "eos", "(", ")", ",", "dict", ".", "unk", "(", ")", ")", "\n", "for", "i", ",", "(", "sys_tok", ",", "ref_tok", ")", "in", "enumerate", "(", "zip", "(", "readlines", "(", "fdsys", ")", ",", "readlines", "(", "fdref", ")", ")", ")", ":", "\n", "                    ", "scorer", ".", "reset", "(", "one_init", "=", "True", ")", "\n", "sys_tok", "=", "dict", ".", "encode_line", "(", "sys_tok", ")", "\n", "ref_tok", "=", "dict", ".", "encode_line", "(", "ref_tok", ")", "\n", "scorer", ".", "add", "(", "ref_tok", ",", "sys_tok", ")", "\n", "print", "(", "i", ",", "scorer", ".", "result_string", "(", "args", ".", "order", ")", ")", "\n", "", "", "", "", "else", ":", "\n", "        ", "def", "score", "(", "fdsys", ")", ":", "\n", "            ", "with", "open", "(", "args", ".", "ref", ")", "as", "fdref", ":", "\n", "                ", "scorer", "=", "bleu", ".", "Scorer", "(", "dict", ".", "pad", "(", ")", ",", "dict", ".", "eos", "(", ")", ",", "dict", ".", "unk", "(", ")", ")", "\n", "for", "sys_tok", ",", "ref_tok", "in", "zip", "(", "readlines", "(", "fdsys", ")", ",", "readlines", "(", "fdref", ")", ")", ":", "\n", "                    ", "sys_tok", "=", "dict", ".", "encode_line", "(", "sys_tok", ")", "\n", "ref_tok", "=", "dict", ".", "encode_line", "(", "ref_tok", ")", "\n", "scorer", ".", "add", "(", "ref_tok", ",", "sys_tok", ")", "\n", "", "print", "(", "scorer", ".", "result_string", "(", "args", ".", "order", ")", ")", "\n", "\n", "", "", "", "if", "args", ".", "sys", "==", "'-'", ":", "\n", "        ", "score", "(", "sys", ".", "stdin", ")", "\n", "", "else", ":", "\n", "        ", "with", "open", "(", "args", ".", "sys", ",", "'r'", ")", "as", "f", ":", "\n", "            ", "score", "(", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq-stack-transformer.eval_lm.WordStat.__init__": [[21, 28], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "word", ",", "is_bpe", ")", ":", "\n", "        ", "self", ".", "word", "=", "word", "\n", "self", ".", "is_bpe", "=", "is_bpe", "\n", "self", ".", "log_prob", "=", "0", "\n", "self", ".", "next_word_prob", "=", "0", "\n", "self", ".", "count", "=", "0", "\n", "self", ".", "missing_next_words", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq-stack-transformer.eval_lm.WordStat.add": [[29, 40], ["None"], "methods", ["None"], ["", "def", "add", "(", "self", ",", "log_prob", ",", "next_word_prob", ")", ":", "\n", "        ", "\"\"\" increments counters for the sum of log probs of current word and next\n            word (given context ending at current word). Since the next word might be at the end of the example,\n            or it might be not counted because it is not an ending subword unit,\n            also keeps track of how many of those we have seen \"\"\"", "\n", "if", "next_word_prob", "is", "not", "None", ":", "\n", "            ", "self", ".", "next_word_prob", "+=", "next_word_prob", "\n", "", "else", ":", "\n", "            ", "self", ".", "missing_next_words", "+=", "1", "\n", "", "self", ".", "log_prob", "+=", "log_prob", "\n", "self", ".", "count", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq-stack-transformer.eval_lm.WordStat.__str__": [[41, 44], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "'{}\\t{}\\t{}\\t{}\\t{}\\t{}'", ".", "format", "(", "self", ".", "word", ",", "self", ".", "count", ",", "self", ".", "log_prob", ",", "self", ".", "is_bpe", ",", "\n", "self", ".", "next_word_prob", ",", "self", ".", "count", "-", "self", ".", "missing_next_words", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq-stack-transformer.eval_lm.main": [[46, 214], ["fairseq.utils.import_user_module", "print", "fairseq.tasks.setup_task", "print", "fairseq.checkpoint_utils.load_model_ensemble", "vars().keys", "fairseq.tasks.setup_task", "tasks.setup_task.load_dataset", "tasks.setup_task.dataset", "print", "print", "tasks.setup_task.get_batch_iterator().next_epoch_itr", "fairseq.meters.StopwatchMeter", "fairseq.sequence_scorer.SequenceScorer", "dict", "print", "print", "torch.cuda.is_available", "parsed_args.path.split", "fairseq.data.LMContextWindowDataset", "model.make_generation_fast_", "len", "len", "fairseq.progress_bar.build_progress_bar", "fairseq.meters.TimeMeter", "sorted", "eval", "vars", "setattr", "len", "model.half", "model.cuda", "sum", "tasks.setup_task.get_batch_iterator", "args.remove_bpe.rstrip", "set", "fairseq.meters.StopwatchMeter.start", "fairseq.sequence_scorer.SequenceScorer.generate", "fairseq.meters.StopwatchMeter.stop", "fairseq.meters.TimeMeter.update", "t.log", "numpy.exp", "dict.values", "print", "getattr", "tasks.setup_task.source_dictionary.pad", "fairseq.utils.move_to_cuda", "tokens.numel", "hypo[].float", "inf_scores.any", "hypo[].float.sum().cpu", "p.numel", "fairseq.utils.resolve_max_positions", "range", "hypo[].float.eq", "hypo[].float.eq", "print", "hypo[].float.numel", "range", "round", "models[].parameters", "range", "tasks.setup_task.source_dictionary[].endswith", "[].item", "tasks.setup_task.target_dictionary.bos", "float", "float", "tasks.setup_task.target_dictionary.string", "hypo[].float.sum", "len", "tokens[].item", "print", "len", "tokens[].item", "word_prob.append", "dict.setdefault().add", "model.max_positions", "len", "pos_scores[].item", "inf_scores.nonzero", "pos_scores[].item", "pos_scores[].item", "dict.setdefault", "eval_lm.WordStat"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.import_user_module", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.sentence_prediction.SentencePredictionTask.setup_task", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.checkpoint_utils.load_model_ensemble", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.sentence_prediction.SentencePredictionTask.setup_task", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.sentence_prediction.SentencePredictionTask.load_dataset", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.fairseq_task.FairseqTask.dataset", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.EpochBatchIterator.next_epoch_itr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.TransformerDecoderLayer.make_generation_fast_", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.build_progress_bar", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.fairseq_task.FairseqTask.get_batch_iterator", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.meters.StopwatchMeter.start", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.sequence_generator.SequenceGenerator.generate", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.meters.StopwatchMeter.stop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.move_to_cuda", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.resolve_max_positions", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.bos", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.hub_utils.GeneratorHubInterface.string", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.add", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.max_positions", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item"], ["", "", "def", "main", "(", "parsed_args", ")", ":", "\n", "    ", "assert", "parsed_args", ".", "path", "is", "not", "None", ",", "'--path required for evaluation!'", "\n", "\n", "utils", ".", "import_user_module", "(", "parsed_args", ")", "\n", "\n", "print", "(", "parsed_args", ")", "\n", "\n", "use_cuda", "=", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "parsed_args", ".", "cpu", "\n", "\n", "task", "=", "tasks", ".", "setup_task", "(", "parsed_args", ")", "\n", "\n", "# Load ensemble", "\n", "print", "(", "'| loading model(s) from {}'", ".", "format", "(", "parsed_args", ".", "path", ")", ")", "\n", "models", ",", "args", "=", "checkpoint_utils", ".", "load_model_ensemble", "(", "\n", "parsed_args", ".", "path", ".", "split", "(", "':'", ")", ",", "\n", "arg_overrides", "=", "eval", "(", "parsed_args", ".", "model_overrides", ")", ",", "\n", "task", "=", "task", ",", "\n", ")", "\n", "\n", "for", "arg", "in", "vars", "(", "parsed_args", ")", ".", "keys", "(", ")", ":", "\n", "        ", "if", "arg", "not", "in", "{", "\n", "'self_target'", ",", "'future_target'", ",", "'past_target'", ",", "'tokens_per_sample'", ",", "\n", "'output_size_dictionary'", ",", "'add_bos_token'", ",", "\n", "}", ":", "\n", "            ", "setattr", "(", "args", ",", "arg", ",", "getattr", "(", "parsed_args", ",", "arg", ")", ")", "\n", "\n", "# reduce tokens per sample by the required context window size", "\n", "", "", "args", ".", "tokens_per_sample", "-=", "args", ".", "context_window", "\n", "task", "=", "tasks", ".", "setup_task", "(", "args", ")", "\n", "\n", "# Load dataset splits", "\n", "task", ".", "load_dataset", "(", "args", ".", "gen_subset", ")", "\n", "dataset", "=", "task", ".", "dataset", "(", "args", ".", "gen_subset", ")", "\n", "if", "args", ".", "context_window", ">", "0", ":", "\n", "        ", "dataset", "=", "LMContextWindowDataset", "(", "\n", "dataset", "=", "dataset", ",", "\n", "tokens_per_sample", "=", "args", ".", "tokens_per_sample", ",", "\n", "context_window", "=", "args", ".", "context_window", ",", "\n", "pad_idx", "=", "task", ".", "source_dictionary", ".", "pad", "(", ")", ",", "\n", ")", "\n", "", "print", "(", "'| {} {} {} examples'", ".", "format", "(", "args", ".", "data", ",", "args", ".", "gen_subset", ",", "len", "(", "dataset", ")", ")", ")", "\n", "\n", "# Optimize ensemble for generation and set the source and dest dicts on the model (required by scorer)", "\n", "for", "model", "in", "models", ":", "\n", "        ", "model", ".", "make_generation_fast_", "(", ")", "\n", "if", "args", ".", "fp16", ":", "\n", "            ", "model", ".", "half", "(", ")", "\n", "", "if", "use_cuda", ":", "\n", "            ", "model", ".", "cuda", "(", ")", "\n", "\n", "", "", "assert", "len", "(", "models", ")", ">", "0", "\n", "\n", "print", "(", "'num. model params: {}'", ".", "format", "(", "sum", "(", "p", ".", "numel", "(", ")", "for", "p", "in", "models", "[", "0", "]", ".", "parameters", "(", ")", ")", ")", ")", "\n", "\n", "itr", "=", "task", ".", "get_batch_iterator", "(", "\n", "dataset", "=", "dataset", ",", "\n", "max_tokens", "=", "args", ".", "max_tokens", "or", "36000", ",", "\n", "max_sentences", "=", "args", ".", "max_sentences", ",", "\n", "max_positions", "=", "utils", ".", "resolve_max_positions", "(", "*", "[", "\n", "model", ".", "max_positions", "(", ")", "for", "model", "in", "models", "\n", "]", ")", ",", "\n", "ignore_invalid_inputs", "=", "True", ",", "\n", "num_shards", "=", "args", ".", "num_shards", ",", "\n", "shard_id", "=", "args", ".", "shard_id", ",", "\n", "num_workers", "=", "args", ".", "num_workers", ",", "\n", ")", ".", "next_epoch_itr", "(", "shuffle", "=", "False", ")", "\n", "\n", "gen_timer", "=", "StopwatchMeter", "(", ")", "\n", "scorer", "=", "SequenceScorer", "(", "task", ".", "target_dictionary", ",", "args", ".", "softmax_batch", ")", "\n", "\n", "score_sum", "=", "0.", "\n", "count", "=", "0", "\n", "\n", "if", "args", ".", "remove_bpe", "is", "not", "None", ":", "\n", "        ", "if", "args", ".", "remove_bpe", "==", "'sentencepiece'", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "", "else", ":", "\n", "            ", "bpe_cont", "=", "args", ".", "remove_bpe", ".", "rstrip", "(", ")", "\n", "bpe_toks", "=", "set", "(", "\n", "i", "\n", "for", "i", "in", "range", "(", "len", "(", "task", ".", "source_dictionary", ")", ")", "\n", "if", "task", ".", "source_dictionary", "[", "i", "]", ".", "endswith", "(", "bpe_cont", ")", "\n", ")", "\n", "", "bpe_len", "=", "len", "(", "bpe_cont", ")", "\n", "", "else", ":", "\n", "        ", "bpe_toks", "=", "None", "\n", "bpe_len", "=", "0", "\n", "\n", "", "word_stats", "=", "dict", "(", ")", "\n", "\n", "with", "progress_bar", ".", "build_progress_bar", "(", "args", ",", "itr", ")", "as", "t", ":", "\n", "        ", "wps_meter", "=", "TimeMeter", "(", ")", "\n", "\n", "for", "sample", "in", "t", ":", "\n", "            ", "if", "'net_input'", "not", "in", "sample", ":", "\n", "                ", "continue", "\n", "\n", "", "sample", "=", "utils", ".", "move_to_cuda", "(", "sample", ")", "if", "use_cuda", "else", "sample", "\n", "\n", "gen_timer", ".", "start", "(", ")", "\n", "hypos", "=", "scorer", ".", "generate", "(", "models", ",", "sample", ")", "\n", "gen_timer", ".", "stop", "(", "sample", "[", "'ntokens'", "]", ")", "\n", "\n", "for", "hypos_i", "in", "hypos", ":", "\n", "                ", "hypo", "=", "hypos_i", "[", "0", "]", "\n", "\n", "tokens", "=", "hypo", "[", "'tokens'", "]", "\n", "tgt_len", "=", "tokens", ".", "numel", "(", ")", "\n", "pos_scores", "=", "hypo", "[", "'positional_scores'", "]", ".", "float", "(", ")", "\n", "\n", "if", "args", ".", "add_bos_token", ":", "\n", "                    ", "assert", "hypo", "[", "'tokens'", "]", "[", "0", "]", ".", "item", "(", ")", "==", "task", ".", "target_dictionary", ".", "bos", "(", ")", "\n", "tokens", "=", "tokens", "[", "1", ":", "]", "\n", "pos_scores", "=", "pos_scores", "[", "1", ":", "]", "\n", "\n", "", "skipped_toks", "=", "0", "\n", "if", "bpe_toks", "is", "not", "None", ":", "\n", "                    ", "for", "i", "in", "range", "(", "tgt_len", "-", "1", ")", ":", "\n", "                        ", "if", "tokens", "[", "i", "]", ".", "item", "(", ")", "in", "bpe_toks", ":", "\n", "                            ", "skipped_toks", "+=", "1", "\n", "pos_scores", "[", "i", "+", "1", "]", "+=", "pos_scores", "[", "i", "]", "\n", "pos_scores", "[", "i", "]", "=", "0", "\n", "\n", "", "", "", "inf_scores", "=", "pos_scores", ".", "eq", "(", "float", "(", "'inf'", ")", ")", "|", "pos_scores", ".", "eq", "(", "float", "(", "'-inf'", ")", ")", "\n", "if", "inf_scores", ".", "any", "(", ")", ":", "\n", "                    ", "print", "(", "'| Skipping tokens with inf scores:'", ",", "\n", "task", ".", "target_dictionary", ".", "string", "(", "tokens", "[", "inf_scores", ".", "nonzero", "(", ")", "]", ")", ")", "\n", "pos_scores", "=", "pos_scores", "[", "(", "~", "inf_scores", ")", ".", "nonzero", "(", ")", "]", "\n", "", "score_sum", "+=", "pos_scores", ".", "sum", "(", ")", ".", "cpu", "(", ")", "\n", "count", "+=", "pos_scores", ".", "numel", "(", ")", "-", "skipped_toks", "\n", "\n", "if", "args", ".", "output_word_probs", "or", "args", ".", "output_word_stats", ":", "\n", "                    ", "w", "=", "''", "\n", "word_prob", "=", "[", "]", "\n", "is_bpe", "=", "False", "\n", "for", "i", "in", "range", "(", "len", "(", "tokens", ")", ")", ":", "\n", "                        ", "w_ind", "=", "tokens", "[", "i", "]", ".", "item", "(", ")", "\n", "w", "+=", "task", ".", "source_dictionary", "[", "w_ind", "]", "\n", "if", "bpe_toks", "is", "not", "None", "and", "w_ind", "in", "bpe_toks", ":", "\n", "                            ", "w", "=", "w", "[", ":", "-", "bpe_len", "]", "\n", "is_bpe", "=", "True", "\n", "", "else", ":", "\n", "                            ", "word_prob", ".", "append", "(", "(", "w", ",", "pos_scores", "[", "i", "]", ".", "item", "(", ")", ")", ")", "\n", "\n", "next_prob", "=", "None", "\n", "ind", "=", "i", "+", "1", "\n", "while", "ind", "<", "len", "(", "tokens", ")", ":", "\n", "                                ", "if", "pos_scores", "[", "ind", "]", ".", "item", "(", ")", "!=", "0", ":", "\n", "                                    ", "next_prob", "=", "pos_scores", "[", "ind", "]", "\n", "break", "\n", "", "ind", "+=", "1", "\n", "\n", "", "word_stats", ".", "setdefault", "(", "w", ",", "WordStat", "(", "w", ",", "is_bpe", ")", ")", ".", "add", "(", "pos_scores", "[", "i", "]", ".", "item", "(", ")", ",", "next_prob", ")", "\n", "is_bpe", "=", "False", "\n", "w", "=", "''", "\n", "", "", "if", "args", ".", "output_word_probs", ":", "\n", "                        ", "print", "(", "'\\t'", ".", "join", "(", "'{} [{:2f}]'", ".", "format", "(", "x", "[", "0", "]", ",", "x", "[", "1", "]", ")", "for", "x", "in", "word_prob", ")", ")", "\n", "\n", "", "", "", "wps_meter", ".", "update", "(", "sample", "[", "'ntokens'", "]", ")", "\n", "t", ".", "log", "(", "{", "'wps'", ":", "round", "(", "wps_meter", ".", "avg", ")", "}", ")", "\n", "\n", "", "", "avg_nll_loss", "=", "-", "score_sum", "/", "count", "\n", "print", "(", "'| Evaluated {} tokens in {:.1f}s ({:.2f} tokens/s)'", ".", "format", "(", "gen_timer", ".", "n", ",", "gen_timer", ".", "sum", ",", "1.", "/", "gen_timer", ".", "avg", ")", ")", "\n", "print", "(", "'| Loss: {:.4f}, Perplexity: {:.2f}'", ".", "format", "(", "avg_nll_loss", ",", "np", ".", "exp", "(", "avg_nll_loss", ")", ")", ")", "\n", "\n", "if", "args", ".", "output_word_stats", ":", "\n", "        ", "for", "ws", "in", "sorted", "(", "word_stats", ".", "values", "(", ")", ",", "key", "=", "lambda", "x", ":", "x", ".", "count", ",", "reverse", "=", "True", ")", ":", "\n", "            ", "print", "(", "ws", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq-stack-transformer.eval_lm.cli_main": [[216, 220], ["fairseq.options.get_eval_lm_parser", "fairseq.options.parse_args_and_arch", "eval_lm.main"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.get_eval_lm_parser", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.parse_args_and_arch", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.main"], ["", "", "", "def", "cli_main", "(", ")", ":", "\n", "    ", "parser", "=", "options", ".", "get_eval_lm_parser", "(", ")", "\n", "args", "=", "options", ".", "parse_args_and_arch", "(", "parser", ")", "\n", "main", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.translation_moe.score.main": [[23, 45], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "score.load_sys", "print", "score.load_ref", "score.merge", "score.multi_ref", "score.intra_ref", "score.pairwise"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.translation_moe.score.load_sys", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.translation_moe.score.load_ref", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.translation_moe.score.merge", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.translation_moe.score.multi_ref", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.translation_moe.score.intra_ref", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.translation_moe.score.pairwise"], ["parser", ".", "add_argument", "(", "'-o'", ",", "'--order'", ",", "default", "=", "4", ",", "metavar", "=", "'N'", ",", "\n", "type", "=", "int", ",", "help", "=", "'consider ngrams up to this order'", ")", "\n", "parser", ".", "add_argument", "(", "'--ignore-case'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'case-insensitive scoring'", ")", "\n", "parser", ".", "add_argument", "(", "'--sacrebleu'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'score with sacrebleu'", ")", "\n", "parser", ".", "add_argument", "(", "'--sentence-bleu'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'report sentence-level BLEUs (i.e., with +1 smoothing)'", ")", "\n", "# fmt: on", "\n", "return", "parser", "\n", "\n", "\n", "", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "get_parser", "(", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "print", "(", "args", ")", "\n", "\n", "assert", "args", ".", "sys", "==", "'-'", "or", "os", ".", "path", ".", "exists", "(", "args", ".", "sys", ")", ",", "\"System output file {} does not exist\"", ".", "format", "(", "args", ".", "sys", ")", "\n", "assert", "os", ".", "path", ".", "exists", "(", "args", ".", "ref", ")", ",", "\"Reference file {} does not exist\"", ".", "format", "(", "args", ".", "ref", ")", "\n", "\n", "dict", "=", "dictionary", ".", "Dictionary", "(", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.translation_moe.score.dictolist": [[47, 50], ["sorted", "d.items"], "function", ["None"], ["def", "readlines", "(", "fd", ")", ":", "\n", "        ", "for", "line", "in", "fd", ".", "readlines", "(", ")", ":", "\n", "            ", "if", "args", ".", "ignore_case", ":", "\n", "                ", "yield", "line", ".", "lower", "(", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.translation_moe.score.load_sys": [[52, 71], ["score.dictolist", "score.dictolist", "score.dictolist", "score.dictolist", "open", "line.rstrip.rstrip", "line.rstrip.startswith", "int", "line.rstrip.startswith", "line.rstrip.startswith", "line.rstrip.startswith", "hypos[].append", "log_probs[].append", "line.rstrip.split", "line.rstrip.split", "float", "line.rstrip.find", "line.rstrip.split", "line.rstrip.find", "line.rstrip.split"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.translation_moe.score.dictolist", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.translation_moe.score.dictolist", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.translation_moe.score.dictolist", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.translation_moe.score.dictolist", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["                ", "yield", "line", "\n", "\n", "", "", "", "if", "args", ".", "sacrebleu", ":", "\n", "        ", "import", "sacrebleu", "\n", "\n", "def", "score", "(", "fdsys", ")", ":", "\n", "            ", "with", "open", "(", "args", ".", "ref", ")", "as", "fdref", ":", "\n", "                ", "print", "(", "sacrebleu", ".", "corpus_bleu", "(", "fdsys", ",", "[", "fdref", "]", ")", ")", "\n", "", "", "", "elif", "args", ".", "sentence_bleu", ":", "\n", "        ", "def", "score", "(", "fdsys", ")", ":", "\n", "            ", "with", "open", "(", "args", ".", "ref", ")", "as", "fdref", ":", "\n", "                ", "scorer", "=", "bleu", ".", "Scorer", "(", "dict", ".", "pad", "(", ")", ",", "dict", ".", "eos", "(", ")", ",", "dict", ".", "unk", "(", ")", ")", "\n", "for", "i", ",", "(", "sys_tok", ",", "ref_tok", ")", "in", "enumerate", "(", "zip", "(", "readlines", "(", "fdsys", ")", ",", "readlines", "(", "fdref", ")", ")", ")", ":", "\n", "                    ", "scorer", ".", "reset", "(", "one_init", "=", "True", ")", "\n", "sys_tok", "=", "dict", ".", "encode_line", "(", "sys_tok", ")", "\n", "ref_tok", "=", "dict", ".", "encode_line", "(", "ref_tok", ")", "\n", "scorer", ".", "add", "(", "ref_tok", ",", "sys_tok", ")", "\n", "print", "(", "i", ",", "scorer", ".", "result_string", "(", "args", ".", "order", ")", ")", "\n", "", "", "", "", "else", ":", "\n", "        ", "def", "score", "(", "fdsys", ")", ":", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.translation_moe.score.load_ref": [[73, 92], ["open", "f.readlines", "len", "lines[].startswith", "src.append", "lines[].startswith", "[].rstrip", "tgt.append", "refs.append", "[].rstrip", "lines[].startswith", "a.append", "len", "[].rstrip", "lines[].split", "lines[].split", "lines[].split"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.readlines", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["                ", "scorer", "=", "bleu", ".", "Scorer", "(", "dict", ".", "pad", "(", ")", ",", "dict", ".", "eos", "(", ")", ",", "dict", ".", "unk", "(", ")", ")", "\n", "for", "sys_tok", ",", "ref_tok", "in", "zip", "(", "readlines", "(", "fdsys", ")", ",", "readlines", "(", "fdref", ")", ")", ":", "\n", "                    ", "sys_tok", "=", "dict", ".", "encode_line", "(", "sys_tok", ")", "\n", "ref_tok", "=", "dict", ".", "encode_line", "(", "ref_tok", ")", "\n", "scorer", ".", "add", "(", "ref_tok", ",", "sys_tok", ")", "\n", "", "print", "(", "scorer", ".", "result_string", "(", "args", ".", "order", ")", ")", "\n", "\n", "", "", "", "if", "args", ".", "sys", "==", "'-'", ":", "\n", "        ", "score", "(", "sys", ".", "stdin", ")", "\n", "", "else", ":", "\n", "        ", "with", "open", "(", "args", ".", "sys", ",", "'r'", ")", "as", "f", ":", "\n", "            ", "score", "(", "f", ")", "\n", "\n", "\n", "", "", "", "if", "__name__", "==", "'__main__'", ":", "\n", "    ", "main", "(", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.translation_moe.score.merge": [[94, 103], ["open", "zip", "f.write", "f.write", "f.write", "zip", "f.write", "f.write", "h.strip"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.translation_moe.score.corpus_bleu": [[105, 108], ["sacrebleu.corpus_bleu"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.translation_moe.score.corpus_bleu"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.translation_moe.score.sentence_bleu": [[110, 121], ["sacrebleu.corpus_bleu", "range", "sacrebleu.compute_bleu"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.translation_moe.score.corpus_bleu"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.translation_moe.score.pairwise": [[123, 132], ["sacrebleu.corpus_bleu", "range", "len", "range", "len", "_ref.append", "_hypo.append"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.translation_moe.score.corpus_bleu", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.translation_moe.score.multi_ref": [[134, 170], ["zip", "print", "list", "list", "len", "len", "range", "print", "len", "len", "set", "len", "zip", "zip", "loo_bleus.append", "numpy.argmax", "_ref.append", "_hypo.append", "set.add", "range", "range", "len", "sacrebleu.corpus_bleu", "numpy.mean", "score.sentence_bleu", "random.choice", "len", "len", "range", "range", "len"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.argmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.add", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.translation_moe.score.corpus_bleu", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.mean", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.translation_moe.score.sentence_bleu"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.translation_moe.score.intra_ref": [[172, 186], ["print", "list", "len", "enumerate", "list", "sacrebleu.corpus_bleu", "print", "zip", "list.append", "range", "itertools.chain.from_iterable", "score.pairwise", "range", "concat_rest[].extend"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.translation_moe.score.corpus_bleu", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.translation_moe.score.pairwise"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.multiprocessing_bpe_encoder.MultiprocessingEncoder.__init__": [[91, 93], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "self", ".", "args", "=", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.multiprocessing_bpe_encoder.MultiprocessingEncoder.initializer": [[94, 97], ["fairseq.data.encoders.gpt2_bpe.get_encoder"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.gpt2_bpe_utils.get_encoder"], ["", "def", "initializer", "(", "self", ")", ":", "\n", "        ", "global", "bpe", "\n", "bpe", "=", "get_encoder", "(", "self", ".", "args", ".", "encoder_json", ",", "self", ".", "args", ".", "vocab_bpe", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.multiprocessing_bpe_encoder.MultiprocessingEncoder.encode": [[98, 102], ["bpe.encode", "list", "map"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.encode"], ["", "def", "encode", "(", "self", ",", "line", ")", ":", "\n", "        ", "global", "bpe", "\n", "ids", "=", "bpe", ".", "encode", "(", "line", ")", "\n", "return", "list", "(", "map", "(", "str", ",", "ids", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.multiprocessing_bpe_encoder.MultiprocessingEncoder.decode": [[103, 106], ["bpe.decode"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.decode"], ["", "def", "decode", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "global", "bpe", "\n", "return", "bpe", ".", "decode", "(", "tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.multiprocessing_bpe_encoder.MultiprocessingEncoder.encode_lines": [[107, 119], ["line.strip.strip.strip", "multiprocessing_bpe_encoder.MultiprocessingEncoder.encode", "enc_lines.append", "len"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.encode", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "encode_lines", "(", "self", ",", "lines", ")", ":", "\n", "        ", "\"\"\"\n        Encode a set of lines. All lines will be encoded together.\n        \"\"\"", "\n", "enc_lines", "=", "[", "]", "\n", "for", "line", "in", "lines", ":", "\n", "            ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "len", "(", "line", ")", "==", "0", "and", "not", "self", ".", "args", ".", "keep_empty", ":", "\n", "                ", "return", "[", "\"EMPTY\"", ",", "None", "]", "\n", "", "tokens", "=", "self", ".", "encode", "(", "line", ")", "\n", "enc_lines", ".", "append", "(", "\" \"", ".", "join", "(", "tokens", ")", ")", "\n", "", "return", "[", "\"PASS\"", ",", "enc_lines", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.multiprocessing_bpe_encoder.MultiprocessingEncoder.decode_lines": [[120, 126], ["map", "dec_lines.append", "line.strip().split", "multiprocessing_bpe_encoder.MultiprocessingEncoder.decode", "line.strip"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.decode"], ["", "def", "decode_lines", "(", "self", ",", "lines", ")", ":", "\n", "        ", "dec_lines", "=", "[", "]", "\n", "for", "line", "in", "lines", ":", "\n", "            ", "tokens", "=", "map", "(", "int", ",", "line", ".", "strip", "(", ")", ".", "split", "(", ")", ")", "\n", "dec_lines", ".", "append", "(", "self", ".", "decode", "(", "tokens", ")", ")", "\n", "", "return", "[", "\"PASS\"", ",", "dec_lines", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.multiprocessing_bpe_encoder.main": [[18, 87], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "len", "len", "contextlib.ExitStack", "multiprocessing_bpe_encoder.MultiprocessingEncoder", "multiprocessing.Pool", "multiprocessing.Pool.imap", "collections.Counter", "enumerate", "collections.Counter.most_common", "zip", "print", "stack.enter_context", "stack.enter_context", "zip", "print", "open", "open", "print"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["def", "main", "(", ")", ":", "\n", "    ", "\"\"\"\n    Helper script to encode raw text with the GPT-2 BPE using multiple processes.\n\n    The encoder.json and vocab.bpe files can be obtained here:\n    - https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json\n    - https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe\n    \"\"\"", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--encoder-json\"", ",", "\n", "help", "=", "'path to encoder.json'", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--vocab-bpe\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "'path to vocab.bpe'", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--inputs\"", ",", "\n", "nargs", "=", "\"+\"", ",", "\n", "default", "=", "[", "'-'", "]", ",", "\n", "help", "=", "\"input files to filter/encode\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--outputs\"", ",", "\n", "nargs", "=", "\"+\"", ",", "\n", "default", "=", "[", "'-'", "]", ",", "\n", "help", "=", "\"path to save encoded outputs\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--keep-empty\"", ",", "\n", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"keep empty lines\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\"--workers\"", ",", "type", "=", "int", ",", "default", "=", "20", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "assert", "len", "(", "args", ".", "inputs", ")", "==", "len", "(", "args", ".", "outputs", ")", ",", "\"number of input and output paths should match\"", "\n", "\n", "with", "contextlib", ".", "ExitStack", "(", ")", "as", "stack", ":", "\n", "        ", "inputs", "=", "[", "\n", "stack", ".", "enter_context", "(", "open", "(", "input", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", ")", "\n", "if", "input", "!=", "\"-\"", "else", "sys", ".", "stdin", "\n", "for", "input", "in", "args", ".", "inputs", "\n", "]", "\n", "outputs", "=", "[", "\n", "stack", ".", "enter_context", "(", "open", "(", "output", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", ")", "\n", "if", "output", "!=", "\"-\"", "else", "sys", ".", "stdout", "\n", "for", "output", "in", "args", ".", "outputs", "\n", "]", "\n", "\n", "encoder", "=", "MultiprocessingEncoder", "(", "args", ")", "\n", "pool", "=", "Pool", "(", "args", ".", "workers", ",", "initializer", "=", "encoder", ".", "initializer", ")", "\n", "encoded_lines", "=", "pool", ".", "imap", "(", "encoder", ".", "encode_lines", ",", "zip", "(", "*", "inputs", ")", ",", "100", ")", "\n", "\n", "stats", "=", "Counter", "(", ")", "\n", "for", "i", ",", "(", "filt", ",", "enc_lines", ")", "in", "enumerate", "(", "encoded_lines", ",", "start", "=", "1", ")", ":", "\n", "            ", "if", "filt", "==", "\"PASS\"", ":", "\n", "                ", "for", "enc_line", ",", "output_h", "in", "zip", "(", "enc_lines", ",", "outputs", ")", ":", "\n", "                    ", "print", "(", "enc_line", ",", "file", "=", "output_h", ")", "\n", "", "", "else", ":", "\n", "                ", "stats", "[", "\"num_filtered_\"", "+", "filt", "]", "+=", "1", "\n", "", "if", "i", "%", "10000", "==", "0", ":", "\n", "                ", "print", "(", "\"processed {} lines\"", ".", "format", "(", "i", ")", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "\n", "", "", "for", "k", ",", "v", "in", "stats", ".", "most_common", "(", ")", ":", "\n", "            ", "print", "(", "\"[{}] filtered {} lines\"", ".", "format", "(", "k", ",", "v", ")", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.preprocess_RACE.InputExample.__init__": [[15, 19], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "paragraph", ",", "qa_list", ",", "label", ")", ":", "\n", "        ", "self", ".", "paragraph", "=", "paragraph", "\n", "self", ".", "qa_list", "=", "qa_list", "\n", "self", ".", "label", "=", "label", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.preprocess_RACE.get_examples": [[21, 58], ["set_type.split", "len", "os.path.join", "os.listdir", "os.path.join", "open", "json.load", "cur_data[].replace", "re.sub", "range", "len", "range", "examples.append", "ord", "ord", "re.sub", "qa_list.append", "preprocess_RACE.InputExample", "question.replace"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "", "def", "get_examples", "(", "data_dir", ",", "set_type", ")", ":", "\n", "    ", "\"\"\"\n    Extract paragraph and question-answer list from each json file\n    \"\"\"", "\n", "examples", "=", "[", "]", "\n", "\n", "levels", "=", "[", "\"middle\"", ",", "\"high\"", "]", "\n", "set_type_c", "=", "set_type", ".", "split", "(", "'-'", ")", "\n", "if", "len", "(", "set_type_c", ")", "==", "2", ":", "\n", "        ", "levels", "=", "[", "set_type_c", "[", "1", "]", "]", "\n", "set_type", "=", "set_type_c", "[", "0", "]", "\n", "", "for", "level", "in", "levels", ":", "\n", "        ", "cur_dir", "=", "os", ".", "path", ".", "join", "(", "data_dir", ",", "set_type", ",", "level", ")", "\n", "for", "filename", "in", "os", ".", "listdir", "(", "cur_dir", ")", ":", "\n", "            ", "cur_path", "=", "os", ".", "path", ".", "join", "(", "cur_dir", ",", "filename", ")", "\n", "with", "open", "(", "cur_path", ",", "'r'", ")", "as", "f", ":", "\n", "                ", "cur_data", "=", "json", ".", "load", "(", "f", ")", "\n", "answers", "=", "cur_data", "[", "\"answers\"", "]", "\n", "options", "=", "cur_data", "[", "\"options\"", "]", "\n", "questions", "=", "cur_data", "[", "\"questions\"", "]", "\n", "context", "=", "cur_data", "[", "\"article\"", "]", ".", "replace", "(", "\"\\n\"", ",", "\" \"", ")", "\n", "context", "=", "re", ".", "sub", "(", "r'\\s+'", ",", "' '", ",", "context", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "answers", ")", ")", ":", "\n", "                    ", "label", "=", "ord", "(", "answers", "[", "i", "]", ")", "-", "ord", "(", "\"A\"", ")", "\n", "qa_list", "=", "[", "]", "\n", "question", "=", "questions", "[", "i", "]", "\n", "for", "j", "in", "range", "(", "4", ")", ":", "\n", "                        ", "option", "=", "options", "[", "i", "]", "[", "j", "]", "\n", "if", "\"_\"", "in", "question", ":", "\n", "                            ", "qa_cat", "=", "question", ".", "replace", "(", "\"_\"", ",", "option", ")", "\n", "", "else", ":", "\n", "                            ", "qa_cat", "=", "\" \"", ".", "join", "(", "[", "question", ",", "option", "]", ")", "\n", "", "qa_cat", "=", "re", ".", "sub", "(", "r'\\s+'", ",", "' '", ",", "qa_cat", ")", "\n", "qa_list", ".", "append", "(", "qa_cat", ")", "\n", "", "examples", ".", "append", "(", "InputExample", "(", "context", ",", "qa_list", ",", "label", ")", ")", "\n", "\n", "", "", "", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.preprocess_RACE.main": [[60, 96], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "os.path.exists", "os.makedirs", "preprocess_RACE.get_examples", "os.path.join", "os.path.join", "open", "open", "open.close", "open.close", "os.path.join", "open", "open.write", "range", "open.write", "f.close", "range", "qa_files[].write", "str", "str"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.preprocess_RACE.get_examples", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.GoldMiner.close", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.GoldMiner.close", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.GoldMiner.close", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write"], ["", "def", "main", "(", ")", ":", "\n", "    ", "\"\"\"\n    Helper script to extract paragraphs questions and answers from RACE datasets.\n    \"\"\"", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--input-dir\"", ",", "\n", "help", "=", "'input directory for downloaded RACE dataset'", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--output-dir\"", ",", "\n", "help", "=", "'output directory for extracted data'", ",", "\n", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "args", ".", "output_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "args", ".", "output_dir", ",", "exist_ok", "=", "True", ")", "\n", "\n", "", "for", "set_type", "in", "[", "\"train\"", ",", "\"dev\"", ",", "\"test-middle\"", ",", "\"test-high\"", "]", ":", "\n", "        ", "examples", "=", "get_examples", "(", "args", ".", "input_dir", ",", "set_type", ")", "\n", "qa_file_paths", "=", "[", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "set_type", "+", "\".input\"", "+", "str", "(", "i", "+", "1", ")", ")", "for", "i", "in", "range", "(", "4", ")", "]", "\n", "qa_files", "=", "[", "open", "(", "qa_file_path", ",", "'w'", ")", "for", "qa_file_path", "in", "qa_file_paths", "]", "\n", "outf_context_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "set_type", "+", "\".input0\"", ")", "\n", "outf_label_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "set_type", "+", "\".label\"", ")", "\n", "outf_context", "=", "open", "(", "outf_context_path", ",", "'w'", ")", "\n", "outf_label", "=", "open", "(", "outf_label_path", ",", "'w'", ")", "\n", "for", "example", "in", "examples", ":", "\n", "            ", "outf_context", ".", "write", "(", "example", ".", "paragraph", "+", "'\\n'", ")", "\n", "for", "i", "in", "range", "(", "4", ")", ":", "\n", "                ", "qa_files", "[", "i", "]", ".", "write", "(", "example", ".", "qa_list", "[", "i", "]", "+", "'\\n'", ")", "\n", "", "outf_label", ".", "write", "(", "str", "(", "example", ".", "label", ")", "+", "'\\n'", ")", "\n", "\n", "", "for", "f", "in", "qa_files", ":", "\n", "            ", "f", ".", "close", "(", ")", "\n", "", "outf_label", ".", "close", "(", ")", "\n", "outf_context", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.hub_interface.RobertaHubInterface.__init__": [[21, 31], ["torch.Module.__init__", "fairseq.data.encoders.build_bpe", "hub_interface.RobertaHubInterface.register_buffer", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["def", "__init__", "(", "self", ",", "args", ",", "task", ",", "model", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "args", "=", "args", "\n", "self", ".", "task", "=", "task", "\n", "self", ".", "model", "=", "model", "\n", "\n", "self", ".", "bpe", "=", "encoders", ".", "build_bpe", "(", "args", ")", "\n", "\n", "# this is useful for determining the device", "\n", "self", ".", "register_buffer", "(", "'_float_tensor'", ",", "torch", ".", "tensor", "(", "[", "0", "]", ",", "dtype", "=", "torch", ".", "float", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.hub_interface.RobertaHubInterface.device": [[32, 35], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "device", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_float_tensor", ".", "device", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.hub_interface.RobertaHubInterface.encode": [[36, 63], ["hub_interface.RobertaHubInterface.task.source_dictionary.encode_line", "hub_interface.RobertaHubInterface.long", "hub_interface.RobertaHubInterface.bpe.encode", "hub_interface.RobertaHubInterface.bpe.encode"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.encode_line", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.encode", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.encode"], ["", "def", "encode", "(", "self", ",", "sentence", ":", "str", ",", "*", "addl_sentences", ",", "no_separator", "=", "False", ")", "->", "torch", ".", "LongTensor", ":", "\n", "        ", "\"\"\"\n        BPE-encode a sentence (or multiple sentences).\n\n        Every sequence begins with a beginning-of-sentence (`<s>`) symbol.\n        Every sentence ends with an end-of-sentence (`</s>`) and we use an\n        extra end-of-sentence (`</s>`) as a separator.\n\n        Example (single sentence): `<s> a b c </s>`\n        Example (sentence pair): `<s> d e f </s> </s> 1 2 3 </s>`\n\n        The BPE encoding follows GPT-2. One subtle detail is that the GPT-2 BPE\n        requires leading spaces. For example::\n\n            >>> roberta.encode('Hello world').tolist()\n            [0, 31414, 232, 2]\n            >>> roberta.encode(' world').tolist()\n            [0, 232, 2]\n            >>> roberta.encode('world').tolist()\n            [0, 8331, 2]\n        \"\"\"", "\n", "bpe_sentence", "=", "'<s> '", "+", "self", ".", "bpe", ".", "encode", "(", "sentence", ")", "+", "' </s>'", "\n", "for", "s", "in", "addl_sentences", ":", "\n", "            ", "bpe_sentence", "+=", "(", "' </s>'", "if", "not", "no_separator", "else", "''", ")", "\n", "bpe_sentence", "+=", "' '", "+", "self", ".", "bpe", ".", "encode", "(", "s", ")", "+", "' </s>'", "\n", "", "tokens", "=", "self", ".", "task", ".", "source_dictionary", ".", "encode_line", "(", "bpe_sentence", ",", "append_eos", "=", "False", ")", "\n", "return", "tokens", ".", "long", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.hub_interface.RobertaHubInterface.decode": [[64, 76], ["tokens.numpy.numpy.numpy", "numpy.split", "tokens.numpy.numpy.dim", "hub_interface.RobertaHubInterface.task.source_dictionary.bos", "hub_interface.RobertaHubInterface.task.source_dictionary.eos", "hub_interface.RobertaHubInterface.bpe.decode", "len", "hub_interface.RobertaHubInterface.task.source_dictionary.string", "doc_mask.nonzero"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.bos", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.decode", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.hub_utils.GeneratorHubInterface.string"], ["", "def", "decode", "(", "self", ",", "tokens", ":", "torch", ".", "LongTensor", ")", ":", "\n", "        ", "assert", "tokens", ".", "dim", "(", ")", "==", "1", "\n", "tokens", "=", "tokens", ".", "numpy", "(", ")", "\n", "if", "tokens", "[", "0", "]", "==", "self", ".", "task", ".", "source_dictionary", ".", "bos", "(", ")", ":", "\n", "            ", "tokens", "=", "tokens", "[", "1", ":", "]", "# remove <s>", "\n", "", "eos_mask", "=", "(", "tokens", "==", "self", ".", "task", ".", "source_dictionary", ".", "eos", "(", ")", ")", "\n", "doc_mask", "=", "eos_mask", "[", "1", ":", "]", "&", "eos_mask", "[", ":", "-", "1", "]", "\n", "sentences", "=", "np", ".", "split", "(", "tokens", ",", "doc_mask", ".", "nonzero", "(", ")", "[", "0", "]", "+", "1", ")", "\n", "sentences", "=", "[", "self", ".", "bpe", ".", "decode", "(", "self", ".", "task", ".", "source_dictionary", ".", "string", "(", "s", ")", ")", "for", "s", "in", "sentences", "]", "\n", "if", "len", "(", "sentences", ")", "==", "1", ":", "\n", "            ", "return", "sentences", "[", "0", "]", "\n", "", "return", "sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.hub_interface.RobertaHubInterface.extract_features": [[77, 95], ["hub_interface.RobertaHubInterface.model", "tokens.unsqueeze.unsqueeze.dim", "tokens.unsqueeze.unsqueeze.unsqueeze", "tokens.unsqueeze.unsqueeze.size", "hub_interface.RobertaHubInterface.model.max_positions", "ValueError", "tokens.unsqueeze.unsqueeze.to", "inner_state.transpose", "tokens.unsqueeze.unsqueeze.size", "hub_interface.RobertaHubInterface.model.max_positions"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.model", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.max_positions", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.max_positions"], ["", "def", "extract_features", "(", "self", ",", "tokens", ":", "torch", ".", "LongTensor", ",", "return_all_hiddens", ":", "bool", "=", "False", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "if", "tokens", ".", "dim", "(", ")", "==", "1", ":", "\n", "            ", "tokens", "=", "tokens", ".", "unsqueeze", "(", "0", ")", "\n", "", "if", "tokens", ".", "size", "(", "-", "1", ")", ">", "self", ".", "model", ".", "max_positions", "(", ")", ":", "\n", "            ", "raise", "ValueError", "(", "'tokens exceeds maximum length: {} > {}'", ".", "format", "(", "\n", "tokens", ".", "size", "(", "-", "1", ")", ",", "self", ".", "model", ".", "max_positions", "(", ")", "\n", ")", ")", "\n", "", "features", ",", "extra", "=", "self", ".", "model", "(", "\n", "tokens", ".", "to", "(", "device", "=", "self", ".", "device", ")", ",", "\n", "features_only", "=", "True", ",", "\n", "return_all_hiddens", "=", "return_all_hiddens", ",", "\n", ")", "\n", "if", "return_all_hiddens", ":", "\n", "# convert from T x B x C -> B x T x C", "\n", "            ", "inner_states", "=", "extra", "[", "'inner_states'", "]", "\n", "return", "[", "inner_state", ".", "transpose", "(", "0", ",", "1", ")", "for", "inner_state", "in", "inner_states", "]", "\n", "", "else", ":", "\n", "            ", "return", "features", "# just the last layer's features", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.hub_interface.RobertaHubInterface.register_classification_head": [[96, 101], ["hub_interface.RobertaHubInterface.model.register_classification_head"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.model.RobertaModel.register_classification_head"], ["", "", "def", "register_classification_head", "(", "\n", "self", ",", "name", ":", "str", ",", "num_classes", ":", "int", "=", "None", ",", "embedding_size", ":", "int", "=", "None", ",", "**", "kwargs", "\n", ")", ":", "\n", "        ", "self", ".", "model", ".", "register_classification_head", "(", "\n", "name", ",", "num_classes", "=", "num_classes", ",", "embedding_size", "=", "embedding_size", ",", "**", "kwargs", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.hub_interface.RobertaHubInterface.predict": [[103, 109], ["hub_interface.RobertaHubInterface.extract_features", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.extract_features", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.log_softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.log_softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.log_softmax"], ["", "def", "predict", "(", "self", ",", "head", ":", "str", ",", "tokens", ":", "torch", ".", "LongTensor", ",", "return_logits", ":", "bool", "=", "False", ")", ":", "\n", "        ", "features", "=", "self", ".", "extract_features", "(", "tokens", ")", "\n", "logits", "=", "self", ".", "model", ".", "classification_heads", "[", "head", "]", "(", "features", ")", "\n", "if", "return_logits", ":", "\n", "            ", "return", "logits", "\n", "", "return", "F", ".", "log_softmax", "(", "logits", ",", "dim", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.hub_interface.RobertaHubInterface.extract_features_aligned_to_words": [[110, 138], ["alignment_utils.spacy_nlp", "alignment_utils.spacy_tokenizer", "hub_interface.RobertaHubInterface.encode", "alignment_utils.spacy_tokenizer.", "alignment_utils.align_bpe_to_words", "hub_interface.RobertaHubInterface.extract_features", "features.squeeze.squeeze.squeeze", "alignment_utils.align_features_to_words", "Doc", "len", "alignment_utils.align_features_to_words.size", "alignment_utils.spacy_tokenizer.", "x.endswith"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.alignment_utils.spacy_nlp", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.alignment_utils.spacy_tokenizer", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.encode", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.alignment_utils.align_bpe_to_words", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.extract_features", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.roberta_utils.align_features_to_words", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "extract_features_aligned_to_words", "(", "self", ",", "sentence", ":", "str", ",", "return_all_hiddens", ":", "bool", "=", "False", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"Extract RoBERTa features, aligned to spaCy's word-level tokenizer.\"\"\"", "\n", "from", "fairseq", ".", "models", ".", "roberta", "import", "alignment_utils", "\n", "from", "spacy", ".", "tokens", "import", "Doc", "\n", "\n", "nlp", "=", "alignment_utils", ".", "spacy_nlp", "(", ")", "\n", "tokenizer", "=", "alignment_utils", ".", "spacy_tokenizer", "(", ")", "\n", "\n", "# tokenize both with GPT-2 BPE and spaCy", "\n", "bpe_toks", "=", "self", ".", "encode", "(", "sentence", ")", "\n", "spacy_toks", "=", "tokenizer", "(", "sentence", ")", "\n", "spacy_toks_ws", "=", "[", "t", ".", "text_with_ws", "for", "t", "in", "tokenizer", "(", "sentence", ")", "]", "\n", "alignment", "=", "alignment_utils", ".", "align_bpe_to_words", "(", "self", ",", "bpe_toks", ",", "spacy_toks_ws", ")", "\n", "\n", "# extract features and align them", "\n", "features", "=", "self", ".", "extract_features", "(", "bpe_toks", ",", "return_all_hiddens", "=", "return_all_hiddens", ")", "\n", "features", "=", "features", ".", "squeeze", "(", "0", ")", "\n", "aligned_feats", "=", "alignment_utils", ".", "align_features_to_words", "(", "self", ",", "features", ",", "alignment", ")", "\n", "\n", "# wrap in spaCy Doc", "\n", "doc", "=", "Doc", "(", "\n", "nlp", ".", "vocab", ",", "\n", "words", "=", "[", "'<s>'", "]", "+", "[", "x", ".", "text", "for", "x", "in", "spacy_toks", "]", "+", "[", "'</s>'", "]", ",", "\n", "spaces", "=", "[", "True", "]", "+", "[", "x", ".", "endswith", "(", "' '", ")", "for", "x", "in", "spacy_toks_ws", "[", ":", "-", "1", "]", "]", "+", "[", "True", ",", "False", "]", ",", "\n", ")", "\n", "assert", "len", "(", "doc", ")", "==", "aligned_feats", ".", "size", "(", "0", ")", "\n", "doc", ".", "user_token_hooks", "[", "'vector'", "]", "=", "lambda", "token", ":", "aligned_feats", "[", "token", ".", "i", "]", "\n", "return", "doc", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.hub_interface.RobertaHubInterface.fill_mask": [[139, 184], ["masked_input.split", "hub_interface.RobertaHubInterface.task.source_dictionary.encode_line", "features[].squeeze", "features[].squeeze.softmax", "features[].squeeze.softmax.topk", "hub_interface.RobertaHubInterface.task.source_dictionary.string", "enumerate", "tokens.unsqueeze.unsqueeze.dim", "tokens.unsqueeze.unsqueeze.unsqueeze", "fairseq.utils.eval", "hub_interface.RobertaHubInterface.model", "hub_interface.RobertaHubInterface.split", "hub_interface.RobertaHubInterface.bpe.decode", "masked_input.count", "tokens.unsqueeze.unsqueeze.long().to", "topk_filled_outputs.append", "topk_filled_outputs.append", "hub_interface.RobertaHubInterface.bpe.encode", "tokens.unsqueeze.unsqueeze.long", "masked_input.replace", "values[].item", "masked_input.replace", "values[].item", "text_span.rstrip"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.encode_line", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.hub_utils.GeneratorHubInterface.string", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.model", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.decode", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.encode", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item"], ["", "def", "fill_mask", "(", "self", ",", "masked_input", ":", "str", ",", "topk", ":", "int", "=", "5", ")", ":", "\n", "        ", "masked_token", "=", "'<mask>'", "\n", "assert", "masked_token", "in", "masked_input", "and", "masked_input", ".", "count", "(", "masked_token", ")", "==", "1", ",", "\"Please add one {0} token for the input, eg: 'He is a {0} guy'\"", ".", "format", "(", "masked_token", ")", "\n", "\n", "text_spans", "=", "masked_input", ".", "split", "(", "masked_token", ")", "\n", "text_spans_bpe", "=", "(", "' {0} '", ".", "format", "(", "masked_token", ")", ")", ".", "join", "(", "\n", "[", "self", ".", "bpe", ".", "encode", "(", "text_span", ".", "rstrip", "(", ")", ")", "for", "text_span", "in", "text_spans", "]", "\n", ")", ".", "strip", "(", ")", "\n", "tokens", "=", "self", ".", "task", ".", "source_dictionary", ".", "encode_line", "(", "\n", "'<s> '", "+", "text_spans_bpe", ",", "\n", "append_eos", "=", "True", ",", "\n", ")", "\n", "\n", "masked_index", "=", "(", "tokens", "==", "self", ".", "task", ".", "mask_idx", ")", ".", "nonzero", "(", ")", "\n", "if", "tokens", ".", "dim", "(", ")", "==", "1", ":", "\n", "            ", "tokens", "=", "tokens", ".", "unsqueeze", "(", "0", ")", "\n", "\n", "", "with", "utils", ".", "eval", "(", "self", ".", "model", ")", ":", "\n", "            ", "features", ",", "extra", "=", "self", ".", "model", "(", "\n", "tokens", ".", "long", "(", ")", ".", "to", "(", "device", "=", "self", ".", "device", ")", ",", "\n", "features_only", "=", "False", ",", "\n", "return_all_hiddens", "=", "False", ",", "\n", ")", "\n", "", "logits", "=", "features", "[", "0", ",", "masked_index", ",", ":", "]", ".", "squeeze", "(", ")", "\n", "prob", "=", "logits", ".", "softmax", "(", "dim", "=", "0", ")", "\n", "values", ",", "index", "=", "prob", ".", "topk", "(", "k", "=", "topk", ",", "dim", "=", "0", ")", "\n", "topk_predicted_token_bpe", "=", "self", ".", "task", ".", "source_dictionary", ".", "string", "(", "index", ")", "\n", "\n", "topk_filled_outputs", "=", "[", "]", "\n", "for", "index", ",", "predicted_token_bpe", "in", "enumerate", "(", "topk_predicted_token_bpe", ".", "split", "(", "' '", ")", ")", ":", "\n", "            ", "predicted_token", "=", "self", ".", "bpe", ".", "decode", "(", "predicted_token_bpe", ")", "\n", "if", "\" {0}\"", ".", "format", "(", "masked_token", ")", "in", "masked_input", ":", "\n", "                ", "topk_filled_outputs", ".", "append", "(", "(", "\n", "masked_input", ".", "replace", "(", "\n", "' {0}'", ".", "format", "(", "masked_token", ")", ",", "predicted_token", "\n", ")", ",", "\n", "values", "[", "index", "]", ".", "item", "(", ")", ",", "\n", ")", ")", "\n", "", "else", ":", "\n", "                ", "topk_filled_outputs", ".", "append", "(", "(", "\n", "masked_input", ".", "replace", "(", "masked_token", ",", "predicted_token", ")", ",", "\n", "values", "[", "index", "]", ".", "item", "(", ")", ",", "\n", ")", ")", "\n", "", "", "return", "topk_filled_outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.hub_interface.RobertaHubInterface.disambiguate_pronoun": [[185, 199], ["hasattr", "fairseq.utils.eval", "hub_interface.RobertaHubInterface.task.disambiguate_pronoun"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.wsc.wsc_task.WSCTask.disambiguate_pronoun"], ["", "def", "disambiguate_pronoun", "(", "self", ",", "sentence", ":", "str", ")", "->", "bool", ":", "\n", "        ", "\"\"\"\n        Usage::\n\n            >>> disambiguate_pronoun('The _trophy_ would not fit in the brown suitcase because [it] was too big.')\n            True\n\n            >>> disambiguate_pronoun('The trophy would not fit in the brown suitcase because [it] was too big.')\n            'The trophy'\n        \"\"\"", "\n", "assert", "hasattr", "(", "self", ".", "task", ",", "'disambiguate_pronoun'", ")", ",", "'roberta.disambiguate_pronoun() requires a model trained with the WSC task.'", "\n", "with", "utils", ".", "eval", "(", "self", ".", "model", ")", ":", "\n", "            ", "return", "self", ".", "task", ".", "disambiguate_pronoun", "(", "self", ".", "model", ",", "sentence", ",", "use_cuda", "=", "self", ".", "device", ".", "type", "==", "'cuda'", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.model_gottbert.GottbertModel.hub_models": [[18, 22], ["None"], "methods", ["None"], ["    ", "@", "classmethod", "\n", "def", "hub_models", "(", "cls", ")", ":", "\n", "        ", "return", "{", "\n", "'gottbert-base'", ":", "'https://dl.gottbert.de/fairseq/models/gottbert-base.tar.gz'", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.model_gottbert.GottbertModel.from_pretrained": [[24, 50], ["hub_utils.from_pretrained", "hub_interface.RobertaHubInterface", "cls.hub_models"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.model_gottbert.GottbertModel.from_pretrained", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.model_gottbert.GottbertModel.hub_models"], ["", "@", "classmethod", "\n", "def", "from_pretrained", "(", "cls", ",", "\n", "model_name_or_path", ",", "\n", "checkpoint_file", "=", "'model.pt'", ",", "\n", "data_name_or_path", "=", "'.'", ",", "\n", "bpe", "=", "'hf_byte_bpe'", ",", "\n", "bpe_vocab", "=", "'vocab.json'", ",", "\n", "bpe_merges", "=", "'merges.txt'", ",", "\n", "bpe_add_prefix_space", "=", "False", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "from", "fairseq", "import", "hub_utils", "\n", "\n", "x", "=", "hub_utils", ".", "from_pretrained", "(", "\n", "model_name_or_path", ",", "\n", "checkpoint_file", ",", "\n", "data_name_or_path", ",", "\n", "archive_map", "=", "cls", ".", "hub_models", "(", ")", ",", "\n", "bpe", "=", "bpe", ",", "\n", "load_checkpoint_heads", "=", "True", ",", "\n", "bpe_vocab", "=", "bpe_vocab", ",", "\n", "bpe_merges", "=", "bpe_merges", ",", "\n", "bpe_add_prefix_space", "=", "bpe_add_prefix_space", ",", "\n", "**", "kwargs", ",", "\n", ")", "\n", "return", "RobertaHubInterface", "(", "x", "[", "'args'", "]", ",", "x", "[", "'task'", "]", ",", "x", "[", "'models'", "]", "[", "0", "]", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.model.RobertaModel.hub_models": [[32, 39], ["None"], "methods", ["None"], ["    ", "@", "classmethod", "\n", "def", "hub_models", "(", "cls", ")", ":", "\n", "        ", "return", "{", "\n", "'roberta.base'", ":", "'http://dl.fbaipublicfiles.com/fairseq/models/roberta.base.tar.gz'", ",", "\n", "'roberta.large'", ":", "'http://dl.fbaipublicfiles.com/fairseq/models/roberta.large.tar.gz'", ",", "\n", "'roberta.large.mnli'", ":", "'http://dl.fbaipublicfiles.com/fairseq/models/roberta.large.mnli.tar.gz'", ",", "\n", "'roberta.large.wsc'", ":", "'http://dl.fbaipublicfiles.com/fairseq/models/roberta.large.wsc.tar.gz'", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.model.RobertaModel.__init__": [[41, 49], ["fairseq.models.FairseqLanguageModel.__init__", "model.RobertaModel.apply", "torch.ModuleDict", "torch.ModuleDict", "torch.ModuleDict"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["", "def", "__init__", "(", "self", ",", "args", ",", "encoder", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "encoder", ")", "\n", "self", ".", "args", "=", "args", "\n", "\n", "# We follow BERT's random weight initialization", "\n", "self", ".", "apply", "(", "init_bert_params", ")", "\n", "\n", "self", ".", "classification_heads", "=", "nn", ".", "ModuleDict", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.model.RobertaModel.add_args": [[50, 81], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "fairseq.utils.get_available_activation_fns", "fairseq.utils.get_available_activation_fns"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.get_available_activation_fns", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.get_available_activation_fns"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add model-specific arguments to the parser.\"\"\"", "\n", "parser", ".", "add_argument", "(", "'--encoder-layers'", ",", "type", "=", "int", ",", "metavar", "=", "'L'", ",", "\n", "help", "=", "'num encoder layers'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'H'", ",", "\n", "help", "=", "'encoder embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-ffn-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'F'", ",", "\n", "help", "=", "'encoder embedding dimension for FFN'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-attention-heads'", ",", "type", "=", "int", ",", "metavar", "=", "'A'", ",", "\n", "help", "=", "'num encoder attention heads'", ")", "\n", "parser", ".", "add_argument", "(", "'--activation-fn'", ",", "\n", "choices", "=", "utils", ".", "get_available_activation_fns", "(", ")", ",", "\n", "help", "=", "'activation function to use'", ")", "\n", "parser", ".", "add_argument", "(", "'--pooler-activation-fn'", ",", "\n", "choices", "=", "utils", ".", "get_available_activation_fns", "(", ")", ",", "\n", "help", "=", "'activation function to use for pooler layer'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-normalize-before'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'apply layernorm before each encoder block'", ")", "\n", "parser", ".", "add_argument", "(", "'--dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability'", ")", "\n", "parser", ".", "add_argument", "(", "'--attention-dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability for attention weights'", ")", "\n", "parser", ".", "add_argument", "(", "'--activation-dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability after activation in FFN'", ")", "\n", "parser", ".", "add_argument", "(", "'--pooler-dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability in the masked_lm pooler layers'", ")", "\n", "parser", ".", "add_argument", "(", "'--max-positions'", ",", "type", "=", "int", ",", "\n", "help", "=", "'number of positional embeddings to learn'", ")", "\n", "parser", ".", "add_argument", "(", "'--load-checkpoint-heads'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'(re-)register and load heads when loading checkpoints'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.model.RobertaModel.build_model": [[82, 94], ["model.base_architecture", "model.RobertaEncoder", "cls", "hasattr"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.base_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.masked_lm_dictionary.BertDictionary.cls"], ["", "@", "classmethod", "\n", "def", "build_model", "(", "cls", ",", "args", ",", "task", ")", ":", "\n", "        ", "\"\"\"Build a new model instance.\"\"\"", "\n", "\n", "# make sure all arguments are present", "\n", "base_architecture", "(", "args", ")", "\n", "\n", "if", "not", "hasattr", "(", "args", ",", "'max_positions'", ")", ":", "\n", "            ", "args", ".", "max_positions", "=", "args", ".", "tokens_per_sample", "\n", "\n", "", "encoder", "=", "RobertaEncoder", "(", "args", ",", "task", ".", "source_dictionary", ")", "\n", "return", "cls", "(", "args", ",", "encoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.model.RobertaModel.forward": [[95, 104], ["model.RobertaModel.decoder"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.FairseqMultiModel.decoder"], ["", "def", "forward", "(", "self", ",", "src_tokens", ",", "features_only", "=", "False", ",", "return_all_hiddens", "=", "False", ",", "classification_head_name", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "if", "classification_head_name", "is", "not", "None", ":", "\n", "            ", "features_only", "=", "True", "\n", "\n", "", "x", ",", "extra", "=", "self", ".", "decoder", "(", "src_tokens", ",", "features_only", ",", "return_all_hiddens", ",", "**", "kwargs", ")", "\n", "\n", "if", "classification_head_name", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "classification_heads", "[", "classification_head_name", "]", "(", "x", ")", "\n", "", "return", "x", ",", "extra", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.model.RobertaModel.register_classification_head": [[105, 123], ["model.RobertaClassificationHead", "print"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["", "def", "register_classification_head", "(", "self", ",", "name", ",", "num_classes", "=", "None", ",", "inner_dim", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Register a classification head.\"\"\"", "\n", "if", "name", "in", "self", ".", "classification_heads", ":", "\n", "            ", "prev_num_classes", "=", "self", ".", "classification_heads", "[", "name", "]", ".", "out_proj", ".", "out_features", "\n", "prev_inner_dim", "=", "self", ".", "classification_heads", "[", "name", "]", ".", "dense", ".", "out_features", "\n", "if", "num_classes", "!=", "prev_num_classes", "or", "inner_dim", "!=", "prev_inner_dim", ":", "\n", "                ", "print", "(", "\n", "'WARNING: re-registering head \"{}\" with num_classes {} (prev: {}) '", "\n", "'and inner_dim {} (prev: {})'", ".", "format", "(", "\n", "name", ",", "num_classes", ",", "prev_num_classes", ",", "inner_dim", ",", "prev_inner_dim", "\n", ")", "\n", ")", "\n", "", "", "self", ".", "classification_heads", "[", "name", "]", "=", "RobertaClassificationHead", "(", "\n", "self", ".", "args", ".", "encoder_embed_dim", ",", "\n", "inner_dim", "or", "self", ".", "args", ".", "encoder_embed_dim", ",", "\n", "num_classes", ",", "\n", "self", ".", "args", ".", "pooler_activation_fn", ",", "\n", "self", ".", "args", ".", "pooler_dropout", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.model.RobertaModel.supported_targets": [[125, 128], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "supported_targets", "(", "self", ")", ":", "\n", "        ", "return", "{", "'self'", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.model.RobertaModel.from_pretrained": [[129, 142], ["hub_utils.from_pretrained", "hub_interface.RobertaHubInterface", "cls.hub_models"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.model_gottbert.GottbertModel.from_pretrained", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.model_gottbert.GottbertModel.hub_models"], ["", "@", "classmethod", "\n", "def", "from_pretrained", "(", "cls", ",", "model_name_or_path", ",", "checkpoint_file", "=", "'model.pt'", ",", "data_name_or_path", "=", "'.'", ",", "**", "kwargs", ")", ":", "\n", "        ", "from", "fairseq", "import", "hub_utils", "\n", "x", "=", "hub_utils", ".", "from_pretrained", "(", "\n", "model_name_or_path", ",", "\n", "checkpoint_file", ",", "\n", "data_name_or_path", ",", "\n", "archive_map", "=", "cls", ".", "hub_models", "(", ")", ",", "\n", "bpe", "=", "'gpt2'", ",", "\n", "load_checkpoint_heads", "=", "True", ",", "\n", "**", "kwargs", ",", "\n", ")", "\n", "return", "RobertaHubInterface", "(", "x", "[", "'args'", "]", ",", "x", "[", "'task'", "]", ",", "x", "[", "'models'", "]", "[", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.model.RobertaModel.upgrade_state_dict_named": [[143, 188], ["state_dict.keys", "hasattr", "model.RobertaModel.classification_heads.keys", "state_dict[].size", "state_dict[].size", "getattr", "model.RobertaModel.classification_heads.state_dict", "model.RobertaModel.items", "hasattr", "k.startswith", "k[].split", "model.RobertaModel.register_classification_head", "print", "keys_to_delete.append", "print", "print", "keys_to_delete.append", "len"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.state_dict", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.model.RobertaModel.register_classification_head", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "upgrade_state_dict_named", "(", "self", ",", "state_dict", ",", "name", ")", ":", "\n", "        ", "prefix", "=", "name", "+", "'.'", "if", "name", "!=", "''", "else", "''", "\n", "current_head_names", "=", "[", "]", "if", "not", "hasattr", "(", "self", ",", "'classification_heads'", ")", "else", "self", ".", "classification_heads", ".", "keys", "(", ")", "\n", "\n", "# Handle new classification heads present in the state dict.", "\n", "keys_to_delete", "=", "[", "]", "\n", "for", "k", "in", "state_dict", ".", "keys", "(", ")", ":", "\n", "            ", "if", "not", "k", ".", "startswith", "(", "prefix", "+", "'classification_heads.'", ")", ":", "\n", "                ", "continue", "\n", "\n", "", "head_name", "=", "k", "[", "len", "(", "prefix", "+", "'classification_heads.'", ")", ":", "]", ".", "split", "(", "'.'", ")", "[", "0", "]", "\n", "num_classes", "=", "state_dict", "[", "prefix", "+", "'classification_heads.'", "+", "head_name", "+", "'.out_proj.weight'", "]", ".", "size", "(", "0", ")", "\n", "inner_dim", "=", "state_dict", "[", "prefix", "+", "'classification_heads.'", "+", "head_name", "+", "'.dense.weight'", "]", ".", "size", "(", "0", ")", "\n", "\n", "if", "getattr", "(", "self", ".", "args", ",", "'load_checkpoint_heads'", ",", "False", ")", ":", "\n", "                ", "if", "head_name", "not", "in", "current_head_names", ":", "\n", "                    ", "self", ".", "register_classification_head", "(", "head_name", ",", "num_classes", ",", "inner_dim", ")", "\n", "", "", "else", ":", "\n", "                ", "if", "head_name", "not", "in", "current_head_names", ":", "\n", "                    ", "print", "(", "\n", "'WARNING: deleting classification head ({}) from checkpoint '", "\n", "'not present in current model: {}'", ".", "format", "(", "head_name", ",", "k", ")", "\n", ")", "\n", "keys_to_delete", ".", "append", "(", "k", ")", "\n", "", "elif", "(", "\n", "num_classes", "!=", "self", ".", "classification_heads", "[", "head_name", "]", ".", "out_proj", ".", "out_features", "\n", "or", "inner_dim", "!=", "self", ".", "classification_heads", "[", "head_name", "]", ".", "dense", ".", "out_features", "\n", ")", ":", "\n", "                    ", "print", "(", "\n", "'WARNING: deleting classification head ({}) from checkpoint '", "\n", "'with different dimensions than current model: {}'", ".", "format", "(", "head_name", ",", "k", ")", "\n", ")", "\n", "keys_to_delete", ".", "append", "(", "k", ")", "\n", "", "", "", "for", "k", "in", "keys_to_delete", ":", "\n", "            ", "del", "state_dict", "[", "k", "]", "\n", "\n", "# Copy any newly-added classification heads into the state dict", "\n", "# with their current weights.", "\n", "", "if", "hasattr", "(", "self", ",", "'classification_heads'", ")", ":", "\n", "            ", "cur_state", "=", "self", ".", "classification_heads", ".", "state_dict", "(", ")", "\n", "for", "k", ",", "v", "in", "cur_state", ".", "items", "(", ")", ":", "\n", "                ", "if", "prefix", "+", "'classification_heads.'", "+", "k", "not", "in", "state_dict", ":", "\n", "                    ", "print", "(", "'Overwriting'", ",", "prefix", "+", "'classification_heads.'", "+", "k", ")", "\n", "state_dict", "[", "prefix", "+", "'classification_heads.'", "+", "k", "]", "=", "v", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.model.RobertaLMHead.__init__": [[193, 203], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "fairseq.utils.get_activation_fn", "fairseq.modules.LayerNorm", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.get_activation_fn", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.layer_norm.LayerNorm", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear"], ["def", "__init__", "(", "self", ",", "embed_dim", ",", "output_dim", ",", "activation_fn", ",", "weight", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "embed_dim", ")", "\n", "self", ".", "activation_fn", "=", "utils", ".", "get_activation_fn", "(", "activation_fn", ")", "\n", "self", ".", "layer_norm", "=", "LayerNorm", "(", "embed_dim", ")", "\n", "\n", "if", "weight", "is", "None", ":", "\n", "            ", "weight", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "output_dim", ",", "bias", "=", "False", ")", ".", "weight", "\n", "", "self", ".", "weight", "=", "weight", "\n", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "output_dim", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.model.RobertaLMHead.forward": [[204, 213], ["model.RobertaLMHead.dense", "model.RobertaLMHead.activation_fn", "model.RobertaLMHead.layer_norm", "torch.linear", "torch.linear", "torch.linear"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "features", ",", "**", "kwargs", ")", ":", "\n", "        ", "x", "=", "self", ".", "dense", "(", "features", ")", "\n", "x", "=", "self", ".", "activation_fn", "(", "x", ")", "\n", "x", "=", "self", ".", "layer_norm", "(", "x", ")", "\n", "\n", "# project back to size of vocabulary with bias", "\n", "x", "=", "F", ".", "linear", "(", "x", ",", "self", ".", "weight", ")", "+", "self", ".", "bias", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.model.RobertaClassificationHead.__init__": [[218, 224], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "fairseq.utils.get_activation_fn", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.get_activation_fn", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear"], ["def", "__init__", "(", "self", ",", "input_dim", ",", "inner_dim", ",", "num_classes", ",", "activation_fn", ",", "pooler_dropout", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "input_dim", ",", "inner_dim", ")", "\n", "self", ".", "activation_fn", "=", "utils", ".", "get_activation_fn", "(", "activation_fn", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "p", "=", "pooler_dropout", ")", "\n", "self", ".", "out_proj", "=", "nn", ".", "Linear", "(", "inner_dim", ",", "num_classes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.model.RobertaClassificationHead.forward": [[225, 233], ["model.RobertaClassificationHead.dropout", "model.RobertaClassificationHead.dense", "model.RobertaClassificationHead.activation_fn", "model.RobertaClassificationHead.dropout", "model.RobertaClassificationHead.out_proj"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "features", ",", "**", "kwargs", ")", ":", "\n", "        ", "x", "=", "features", "[", ":", ",", "0", ",", ":", "]", "# take <s> token (equiv. to [CLS])", "\n", "x", "=", "self", ".", "dropout", "(", "x", ")", "\n", "x", "=", "self", ".", "dense", "(", "x", ")", "\n", "x", "=", "self", ".", "activation_fn", "(", "x", ")", "\n", "x", "=", "self", ".", "dropout", "(", "x", ")", "\n", "x", "=", "self", ".", "out_proj", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.model.RobertaEncoder.__init__": [[242, 266], ["fairseq.models.FairseqDecoder.__init__", "fairseq.modules.TransformerSentenceEncoder", "model.RobertaLMHead", "dictionary.pad", "len", "len"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad"], ["def", "__init__", "(", "self", ",", "args", ",", "dictionary", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ")", "\n", "self", ".", "args", "=", "args", "\n", "self", ".", "sentence_encoder", "=", "TransformerSentenceEncoder", "(", "\n", "padding_idx", "=", "dictionary", ".", "pad", "(", ")", ",", "\n", "vocab_size", "=", "len", "(", "dictionary", ")", ",", "\n", "num_encoder_layers", "=", "args", ".", "encoder_layers", ",", "\n", "embedding_dim", "=", "args", ".", "encoder_embed_dim", ",", "\n", "ffn_embedding_dim", "=", "args", ".", "encoder_ffn_embed_dim", ",", "\n", "num_attention_heads", "=", "args", ".", "encoder_attention_heads", ",", "\n", "dropout", "=", "args", ".", "dropout", ",", "\n", "attention_dropout", "=", "args", ".", "attention_dropout", ",", "\n", "activation_dropout", "=", "args", ".", "activation_dropout", ",", "\n", "max_seq_len", "=", "args", ".", "max_positions", ",", "\n", "num_segments", "=", "0", ",", "\n", "encoder_normalize_before", "=", "True", ",", "\n", "apply_bert_init", "=", "True", ",", "\n", "activation_fn", "=", "args", ".", "activation_fn", ",", "\n", ")", "\n", "self", ".", "lm_head", "=", "RobertaLMHead", "(", "\n", "embed_dim", "=", "args", ".", "encoder_embed_dim", ",", "\n", "output_dim", "=", "len", "(", "dictionary", ")", ",", "\n", "activation_fn", "=", "args", ".", "activation_fn", ",", "\n", "weight", "=", "self", ".", "sentence_encoder", ".", "embed_tokens", ".", "weight", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.model.RobertaEncoder.forward": [[268, 288], ["model.RobertaEncoder.extract_features", "model.RobertaEncoder.output_layer"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.extract_features", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.TransformerDecoder.output_layer"], ["", "def", "forward", "(", "self", ",", "src_tokens", ",", "features_only", "=", "False", ",", "return_all_hiddens", "=", "False", ",", "**", "unused", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            src_tokens (LongTensor): input tokens of shape `(batch, src_len)`\n            features_only (bool, optional): skip LM head and just return\n                features. If True, the output will be of shape\n                `(batch, src_len, embed_dim)`.\n            return_all_hiddens (bool, optional): also return all of the\n                intermediate hidden states (default: False).\n\n        Returns:\n            tuple:\n                - the LM output of shape `(batch, src_len, vocab)`\n                - a dictionary of additional data, where 'inner_states'\n                  is a list of hidden states.\n        \"\"\"", "\n", "x", ",", "extra", "=", "self", ".", "extract_features", "(", "src_tokens", ",", "return_all_hiddens", ")", "\n", "if", "not", "features_only", ":", "\n", "            ", "x", "=", "self", ".", "output_layer", "(", "x", ")", "\n", "", "return", "x", ",", "extra", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.model.RobertaEncoder.extract_features": [[289, 295], ["model.RobertaEncoder.sentence_encoder"], "methods", ["None"], ["", "def", "extract_features", "(", "self", ",", "src_tokens", ",", "return_all_hiddens", "=", "False", ",", "**", "unused", ")", ":", "\n", "        ", "inner_states", ",", "_", "=", "self", ".", "sentence_encoder", "(", "\n", "src_tokens", ",", "last_state_only", "=", "not", "return_all_hiddens", ",", "\n", ")", "\n", "features", "=", "inner_states", "[", "-", "1", "]", "\n", "return", "features", ",", "{", "'inner_states'", ":", "inner_states", "if", "return_all_hiddens", "else", "None", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.model.RobertaEncoder.output_layer": [[296, 298], ["model.RobertaEncoder.lm_head"], "methods", ["None"], ["", "def", "output_layer", "(", "self", ",", "features", ",", "**", "unused", ")", ":", "\n", "        ", "return", "self", ".", "lm_head", "(", "features", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.model.RobertaEncoder.max_positions": [[299, 302], ["None"], "methods", ["None"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum output length supported by the encoder.\"\"\"", "\n", "return", "self", ".", "args", ".", "max_positions", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.model.base_architecture": [[304, 318], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture"], ["", "", "@", "register_model_architecture", "(", "'roberta'", ",", "'roberta'", ")", "\n", "def", "base_architecture", "(", "args", ")", ":", "\n", "    ", "args", ".", "encoder_layers", "=", "getattr", "(", "args", ",", "'encoder_layers'", ",", "12", ")", "\n", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "768", ")", "\n", "args", ".", "encoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_ffn_embed_dim'", ",", "3072", ")", "\n", "args", ".", "encoder_attention_heads", "=", "getattr", "(", "args", ",", "'encoder_attention_heads'", ",", "12", ")", "\n", "\n", "args", ".", "activation_fn", "=", "getattr", "(", "args", ",", "'activation_fn'", ",", "'gelu'", ")", "\n", "args", ".", "pooler_activation_fn", "=", "getattr", "(", "args", ",", "'pooler_activation_fn'", ",", "'tanh'", ")", "\n", "\n", "args", ".", "dropout", "=", "getattr", "(", "args", ",", "'dropout'", ",", "0.1", ")", "\n", "args", ".", "attention_dropout", "=", "getattr", "(", "args", ",", "'attention_dropout'", ",", "0.1", ")", "\n", "args", ".", "activation_dropout", "=", "getattr", "(", "args", ",", "'activation_dropout'", ",", "0.0", ")", "\n", "args", ".", "pooler_dropout", "=", "getattr", "(", "args", ",", "'pooler_dropout'", ",", "0.0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.model.roberta_base_architecture": [[320, 323], ["fairseq.models.register_model_architecture", "model.base_architecture"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.base_architecture"], ["", "@", "register_model_architecture", "(", "'roberta'", ",", "'roberta_base'", ")", "\n", "def", "roberta_base_architecture", "(", "args", ")", ":", "\n", "    ", "base_architecture", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.model.roberta_large_architecture": [[325, 332], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "model.base_architecture"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.base_architecture"], ["", "@", "register_model_architecture", "(", "'roberta'", ",", "'roberta_large'", ")", "\n", "def", "roberta_large_architecture", "(", "args", ")", ":", "\n", "    ", "args", ".", "encoder_layers", "=", "getattr", "(", "args", ",", "'encoder_layers'", ",", "24", ")", "\n", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "1024", ")", "\n", "args", ".", "encoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_ffn_embed_dim'", ",", "4096", ")", "\n", "args", ".", "encoder_attention_heads", "=", "getattr", "(", "args", ",", "'encoder_attention_heads'", ",", "16", ")", "\n", "base_architecture", "(", "args", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.alignment_utils.align_bpe_to_words": [[12, 67], ["filter", "next", "bpe_tokens.dim", "text.strip", "roberta.task.source_dictionary.string", "alignment_utils.align_bpe_to_words.clean"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.hub_utils.GeneratorHubInterface.string"], ["def", "align_bpe_to_words", "(", "roberta", ",", "bpe_tokens", ":", "torch", ".", "LongTensor", ",", "other_tokens", ":", "List", "[", "str", "]", ")", ":", "\n", "    ", "\"\"\"\n    Helper to align GPT-2 BPE to other tokenization formats (e.g., spaCy).\n\n    Args:\n        roberta (RobertaHubInterface): RoBERTa instance\n        bpe_tokens (torch.LongTensor): GPT-2 BPE tokens of shape `(T_bpe)`\n        other_tokens (List[str]): other tokens of shape `(T_words)`\n\n    Returns:\n        List[str]: mapping from *other_tokens* to corresponding *bpe_tokens*.\n    \"\"\"", "\n", "assert", "bpe_tokens", ".", "dim", "(", ")", "==", "1", "\n", "\n", "def", "clean", "(", "text", ")", ":", "\n", "        ", "return", "text", ".", "strip", "(", ")", "\n", "\n", "# remove whitespaces to simplify alignment", "\n", "", "bpe_tokens", "=", "[", "roberta", ".", "task", ".", "source_dictionary", ".", "string", "(", "[", "x", "]", ")", "for", "x", "in", "bpe_tokens", "]", "\n", "bpe_tokens", "=", "[", "clean", "(", "roberta", ".", "bpe", ".", "decode", "(", "x", ")", "if", "x", "not", "in", "{", "'<s>'", ",", "''", "}", "else", "x", ")", "for", "x", "in", "bpe_tokens", "]", "\n", "other_tokens", "=", "[", "clean", "(", "str", "(", "o", ")", ")", "for", "o", "in", "other_tokens", "]", "\n", "\n", "# strip leading <s>", "\n", "assert", "bpe_tokens", "[", "0", "]", "==", "'<s>'", "\n", "bpe_tokens", "=", "bpe_tokens", "[", "1", ":", "]", "\n", "assert", "''", ".", "join", "(", "bpe_tokens", ")", "==", "''", ".", "join", "(", "other_tokens", ")", "\n", "\n", "# create alignment from every word to a list of BPE tokens", "\n", "alignment", "=", "[", "]", "\n", "bpe_toks", "=", "filter", "(", "lambda", "item", ":", "item", "[", "1", "]", "!=", "''", ",", "enumerate", "(", "bpe_tokens", ",", "start", "=", "1", ")", ")", "\n", "j", ",", "bpe_tok", "=", "next", "(", "bpe_toks", ")", "\n", "for", "other_tok", "in", "other_tokens", ":", "\n", "        ", "bpe_indices", "=", "[", "]", "\n", "while", "True", ":", "\n", "            ", "if", "other_tok", ".", "startswith", "(", "bpe_tok", ")", ":", "\n", "                ", "bpe_indices", ".", "append", "(", "j", ")", "\n", "other_tok", "=", "other_tok", "[", "len", "(", "bpe_tok", ")", ":", "]", "\n", "try", ":", "\n", "                    ", "j", ",", "bpe_tok", "=", "next", "(", "bpe_toks", ")", "\n", "", "except", "StopIteration", ":", "\n", "                    ", "j", ",", "bpe_tok", "=", "None", ",", "None", "\n", "", "", "elif", "bpe_tok", ".", "startswith", "(", "other_tok", ")", ":", "\n", "# other_tok spans multiple BPE tokens", "\n", "                ", "bpe_indices", ".", "append", "(", "j", ")", "\n", "bpe_tok", "=", "bpe_tok", "[", "len", "(", "other_tok", ")", ":", "]", "\n", "other_tok", "=", "''", "\n", "", "else", ":", "\n", "                ", "raise", "Exception", "(", "'Cannot align \"{}\" and \"{}\"'", ".", "format", "(", "other_tok", ",", "bpe_tok", ")", ")", "\n", "", "if", "other_tok", "==", "''", ":", "\n", "                ", "break", "\n", "", "", "assert", "len", "(", "bpe_indices", ")", ">", "0", "\n", "alignment", ".", "append", "(", "bpe_indices", ")", "\n", "", "assert", "len", "(", "alignment", ")", "==", "len", "(", "other_tokens", ")", "\n", "\n", "return", "alignment", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.alignment_utils.align_features_to_words": [[69, 96], ["collections.Counter", "features.new", "range", "torch.stack", "torch.all", "features.dim", "features.new.unsqueeze", "torch.stack.append", "max", "len", "torch.stack.append", "collections.Counter.get", "weighted_features[].sum", "torch.abs", "range", "len", "torch.stack.sum", "features.sum"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "align_features_to_words", "(", "roberta", ",", "features", ",", "alignment", ")", ":", "\n", "    ", "\"\"\"\n    Align given features to words.\n\n    Args:\n        roberta (RobertaHubInterface): RoBERTa instance\n        features (torch.Tensor): features to align of shape `(T_bpe x C)`\n        alignment: alignment between BPE tokens and words returned by\n            func:`align_bpe_to_words`.\n    \"\"\"", "\n", "assert", "features", ".", "dim", "(", ")", "==", "2", "\n", "\n", "bpe_counts", "=", "Counter", "(", "j", "for", "bpe_indices", "in", "alignment", "for", "j", "in", "bpe_indices", ")", "\n", "assert", "bpe_counts", "[", "0", "]", "==", "0", "# <s> shouldn't be aligned", "\n", "denom", "=", "features", ".", "new", "(", "[", "bpe_counts", ".", "get", "(", "j", ",", "1", ")", "for", "j", "in", "range", "(", "len", "(", "features", ")", ")", "]", ")", "\n", "weighted_features", "=", "features", "/", "denom", ".", "unsqueeze", "(", "-", "1", ")", "\n", "\n", "output", "=", "[", "weighted_features", "[", "0", "]", "]", "\n", "largest_j", "=", "-", "1", "\n", "for", "bpe_indices", "in", "alignment", ":", "\n", "        ", "output", ".", "append", "(", "weighted_features", "[", "bpe_indices", "]", ".", "sum", "(", "dim", "=", "0", ")", ")", "\n", "largest_j", "=", "max", "(", "largest_j", ",", "*", "bpe_indices", ")", "\n", "", "for", "j", "in", "range", "(", "largest_j", "+", "1", ",", "len", "(", "features", ")", ")", ":", "\n", "        ", "output", ".", "append", "(", "weighted_features", "[", "j", "]", ")", "\n", "", "output", "=", "torch", ".", "stack", "(", "output", ")", "\n", "assert", "torch", ".", "all", "(", "torch", ".", "abs", "(", "output", ".", "sum", "(", "dim", "=", "0", ")", "-", "features", ".", "sum", "(", "dim", "=", "0", ")", ")", "<", "1e-4", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.alignment_utils.spacy_nlp": [[98, 106], ["getattr", "English", "ImportError"], "function", ["None"], ["", "def", "spacy_nlp", "(", ")", ":", "\n", "    ", "if", "getattr", "(", "spacy_nlp", ",", "'_nlp'", ",", "None", ")", "is", "None", ":", "\n", "        ", "try", ":", "\n", "            ", "from", "spacy", ".", "lang", ".", "en", "import", "English", "\n", "spacy_nlp", ".", "_nlp", "=", "English", "(", ")", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "'Please install spacy with: pip install spacy'", ")", "\n", "", "", "return", "spacy_nlp", ".", "_nlp", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.alignment_utils.spacy_tokenizer": [[108, 116], ["getattr", "alignment_utils.spacy_nlp", "spacy_nlp.Defaults.create_tokenizer", "ImportError"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.alignment_utils.spacy_nlp"], ["", "def", "spacy_tokenizer", "(", ")", ":", "\n", "    ", "if", "getattr", "(", "spacy_tokenizer", ",", "'_tokenizer'", ",", "None", ")", "is", "None", ":", "\n", "        ", "try", ":", "\n", "            ", "nlp", "=", "spacy_nlp", "(", ")", "\n", "spacy_tokenizer", ".", "_tokenizer", "=", "nlp", ".", "Defaults", ".", "create_tokenizer", "(", "nlp", ")", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "'Please install spacy with: pip install spacy'", ")", "\n", "", "", "return", "spacy_tokenizer", ".", "_tokenizer", "\n", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.commonsense_qa.commonsense_qa_task.CommonsenseQATask.add_args": [[32, 40], ["parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add task-specific arguments to the parser.\"\"\"", "\n", "parser", ".", "add_argument", "(", "'data'", ",", "metavar", "=", "'DIR'", ",", "\n", "help", "=", "'path to data directory; we load <split>.jsonl'", ")", "\n", "parser", ".", "add_argument", "(", "'--init-token'", ",", "type", "=", "int", ",", "default", "=", "None", ",", "\n", "help", "=", "'add token at the beginning of each batch item'", ")", "\n", "parser", ".", "add_argument", "(", "'--num-classes'", ",", "type", "=", "int", ",", "default", "=", "5", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.commonsense_qa.commonsense_qa_task.CommonsenseQATask.__init__": [[41, 47], ["fairseq.tasks.FairseqTask.__init__", "vocab.add_symbol", "fairseq.data.encoders.build_bpe"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol"], ["", "def", "__init__", "(", "self", ",", "args", ",", "vocab", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ")", "\n", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "mask", "=", "vocab", ".", "add_symbol", "(", "'<mask>'", ")", "\n", "\n", "self", ".", "bpe", "=", "encoders", ".", "build_bpe", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.commonsense_qa.commonsense_qa_task.CommonsenseQATask.load_dictionary": [[48, 58], ["fairseq.data.Dictionary.load", "fairseq.data.Dictionary.load.add_symbol"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol"], ["", "@", "classmethod", "\n", "def", "load_dictionary", "(", "cls", ",", "filename", ")", ":", "\n", "        ", "\"\"\"Load the dictionary from the filename\n\n        Args:\n            filename (str): the filename\n        \"\"\"", "\n", "dictionary", "=", "Dictionary", ".", "load", "(", "filename", ")", "\n", "dictionary", ".", "add_symbol", "(", "'<mask>'", ")", "\n", "return", "dictionary", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.commonsense_qa.commonsense_qa_task.CommonsenseQATask.setup_task": [[59, 68], ["cls.load_dictionary", "print", "cls", "os.path.join", "len"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.sentence_prediction.SentencePredictionTask.load_dictionary", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.masked_lm_dictionary.BertDictionary.cls"], ["", "@", "classmethod", "\n", "def", "setup_task", "(", "cls", ",", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "assert", "args", ".", "criterion", "==", "'sentence_ranking'", ",", "'Must set --criterion=sentence_ranking'", "\n", "\n", "# load data and label dictionaries", "\n", "vocab", "=", "cls", ".", "load_dictionary", "(", "os", ".", "path", ".", "join", "(", "args", ".", "data", ",", "'dict.txt'", ")", ")", "\n", "print", "(", "'| dictionary: {} types'", ".", "format", "(", "len", "(", "vocab", ")", ")", ")", "\n", "\n", "return", "cls", "(", "args", ",", "vocab", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.commonsense_qa.commonsense_qa_task.CommonsenseQATask.load_dataset": [[69, 156], ["all", "range", "range", "fairseq.data.NestedDictionaryDataset", "print", "commonsense_qa_task.CommonsenseQATask.vocab.encode_line().long", "os.path.join", "os.path.exists", "FileNotFoundError", "open", "len", "len", "numpy.array", "fairseq.data.ListDataset", "fairseq.data.ListDataset", "fairseq.data.IdDataset", "fairseq.data.NumSamplesDataset", "fairseq.data.NumelDataset", "fairseq.data.SortDataset.update", "len", "fairseq.data.SortDataset.update", "fairseq.data.data_utils.numpy_seed", "fairseq.data.SortDataset", "commonsense_qa_task.CommonsenseQATask.bpe.encode", "torch.cat", "range", "range", "json.loads", "commonsense_qa_task.CommonsenseQATask.load_dataset.binarize"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.data_utils.numpy_seed", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.encode", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.binarizer.Binarizer.binarize"], ["", "def", "load_dataset", "(", "self", ",", "split", ",", "epoch", "=", "0", ",", "combine", "=", "False", ",", "data_path", "=", "None", ",", "return_only", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Load a given dataset split.\n\n        Args:\n            split (str): name of the split (e.g., train, valid, test)\n        \"\"\"", "\n", "\n", "def", "binarize", "(", "s", ",", "append_bos", "=", "False", ")", ":", "\n", "            ", "if", "self", ".", "bpe", "is", "not", "None", ":", "\n", "                ", "s", "=", "self", ".", "bpe", ".", "encode", "(", "s", ")", "\n", "", "tokens", "=", "self", ".", "vocab", ".", "encode_line", "(", "\n", "s", ",", "append_eos", "=", "True", ",", "add_if_not_exist", "=", "False", ",", "\n", ")", ".", "long", "(", ")", "\n", "if", "append_bos", "and", "self", ".", "args", ".", "init_token", "is", "not", "None", ":", "\n", "                ", "tokens", "=", "torch", ".", "cat", "(", "[", "tokens", ".", "new", "(", "[", "self", ".", "args", ".", "init_token", "]", ")", ",", "tokens", "]", ")", "\n", "", "return", "tokens", "\n", "\n", "", "if", "data_path", "is", "None", ":", "\n", "            ", "data_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "args", ".", "data", ",", "split", "+", "'.jsonl'", ")", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "data_path", ")", ":", "\n", "            ", "raise", "FileNotFoundError", "(", "'Cannot find data: {}'", ".", "format", "(", "data_path", ")", ")", "\n", "\n", "", "src_tokens", "=", "[", "[", "]", "for", "i", "in", "range", "(", "self", ".", "args", ".", "num_classes", ")", "]", "\n", "src_lengths", "=", "[", "[", "]", "for", "i", "in", "range", "(", "self", ".", "args", ".", "num_classes", ")", "]", "\n", "labels", "=", "[", "]", "\n", "\n", "with", "open", "(", "data_path", ")", "as", "h", ":", "\n", "            ", "for", "line", "in", "h", ":", "\n", "                ", "example", "=", "json", ".", "loads", "(", "line", ".", "strip", "(", ")", ")", "\n", "if", "'answerKey'", "in", "example", ":", "\n", "                    ", "label", "=", "ord", "(", "example", "[", "'answerKey'", "]", ")", "-", "ord", "(", "'A'", ")", "\n", "labels", ".", "append", "(", "label", ")", "\n", "", "question", "=", "example", "[", "'question'", "]", "[", "'stem'", "]", "\n", "assert", "len", "(", "example", "[", "'question'", "]", "[", "'choices'", "]", ")", "==", "self", ".", "args", ".", "num_classes", "\n", "# format: `<s> Q: Where would I not want a fox? </s> A: hen house </s>`", "\n", "question", "=", "'Q: '", "+", "question", "\n", "question_toks", "=", "binarize", "(", "question", ",", "append_bos", "=", "True", ")", "\n", "for", "i", ",", "choice", "in", "enumerate", "(", "example", "[", "'question'", "]", "[", "'choices'", "]", ")", ":", "\n", "                    ", "src", "=", "'A: '", "+", "choice", "[", "'text'", "]", "\n", "src_bin", "=", "torch", ".", "cat", "(", "[", "question_toks", ",", "binarize", "(", "src", ")", "]", ")", "\n", "src_tokens", "[", "i", "]", ".", "append", "(", "src_bin", ")", "\n", "src_lengths", "[", "i", "]", ".", "append", "(", "len", "(", "src_bin", ")", ")", "\n", "", "", "", "assert", "all", "(", "len", "(", "src_tokens", "[", "0", "]", ")", "==", "len", "(", "src_tokens", "[", "i", "]", ")", "for", "i", "in", "range", "(", "self", ".", "args", ".", "num_classes", ")", ")", "\n", "assert", "len", "(", "src_tokens", "[", "0", "]", ")", "==", "len", "(", "src_lengths", "[", "0", "]", ")", "\n", "assert", "len", "(", "labels", ")", "==", "0", "or", "len", "(", "labels", ")", "==", "len", "(", "src_tokens", "[", "0", "]", ")", "\n", "\n", "for", "i", "in", "range", "(", "self", ".", "args", ".", "num_classes", ")", ":", "\n", "            ", "src_lengths", "[", "i", "]", "=", "np", ".", "array", "(", "src_lengths", "[", "i", "]", ")", "\n", "src_tokens", "[", "i", "]", "=", "ListDataset", "(", "src_tokens", "[", "i", "]", ",", "src_lengths", "[", "i", "]", ")", "\n", "src_lengths", "[", "i", "]", "=", "ListDataset", "(", "src_lengths", "[", "i", "]", ")", "\n", "\n", "", "dataset", "=", "{", "\n", "'id'", ":", "IdDataset", "(", ")", ",", "\n", "'nsentences'", ":", "NumSamplesDataset", "(", ")", ",", "\n", "'ntokens'", ":", "NumelDataset", "(", "src_tokens", "[", "0", "]", ",", "reduce", "=", "True", ")", ",", "\n", "}", "\n", "\n", "for", "i", "in", "range", "(", "self", ".", "args", ".", "num_classes", ")", ":", "\n", "            ", "dataset", ".", "update", "(", "{", "\n", "'net_input{}'", ".", "format", "(", "i", "+", "1", ")", ":", "{", "\n", "'src_tokens'", ":", "RightPadDataset", "(", "\n", "src_tokens", "[", "i", "]", ",", "\n", "pad_idx", "=", "self", ".", "source_dictionary", ".", "pad", "(", ")", ",", "\n", ")", ",", "\n", "'src_lengths'", ":", "src_lengths", "[", "i", "]", ",", "\n", "}", "\n", "}", ")", "\n", "\n", "", "if", "len", "(", "labels", ")", ">", "0", ":", "\n", "            ", "dataset", ".", "update", "(", "{", "'target'", ":", "RawLabelDataset", "(", "labels", ")", "}", ")", "\n", "\n", "", "dataset", "=", "NestedDictionaryDataset", "(", "\n", "dataset", ",", "\n", "sizes", "=", "[", "np", ".", "maximum", ".", "reduce", "(", "[", "src_token", ".", "sizes", "for", "src_token", "in", "src_tokens", "]", ")", "]", ",", "\n", ")", "\n", "\n", "with", "data_utils", ".", "numpy_seed", "(", "self", ".", "args", ".", "seed", ")", ":", "\n", "            ", "dataset", "=", "SortDataset", "(", "\n", "dataset", ",", "\n", "# shuffle", "\n", "sort_order", "=", "[", "np", ".", "random", ".", "permutation", "(", "len", "(", "dataset", ")", ")", "]", ",", "\n", ")", "\n", "\n", "", "print", "(", "'| Loaded {} with {} samples'", ".", "format", "(", "split", ",", "len", "(", "dataset", ")", ")", ")", "\n", "\n", "self", ".", "datasets", "[", "split", "]", "=", "dataset", "\n", "return", "self", ".", "datasets", "[", "split", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.commonsense_qa.commonsense_qa_task.CommonsenseQATask.build_model": [[157, 167], ["models.build_model", "models.build_model.register_classification_head"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv_lm.LightConvLanguageModel.build_model", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.model.RobertaModel.register_classification_head"], ["", "def", "build_model", "(", "self", ",", "args", ")", ":", "\n", "        ", "from", "fairseq", "import", "models", "\n", "model", "=", "models", ".", "build_model", "(", "args", ",", "self", ")", "\n", "\n", "model", ".", "register_classification_head", "(", "\n", "'sentence_classification_head'", ",", "\n", "num_classes", "=", "1", ",", "\n", ")", "\n", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.commonsense_qa.commonsense_qa_task.CommonsenseQATask.source_dictionary": [[168, 171], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "source_dictionary", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.commonsense_qa.commonsense_qa_task.CommonsenseQATask.target_dictionary": [[172, 175], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "target_dictionary", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "vocab", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.wsc.wsc_task.WSCTask.add_args": [[35, 42], ["parser.add_argument", "parser.add_argument"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add task-specific arguments to the parser.\"\"\"", "\n", "parser", ".", "add_argument", "(", "'data'", ",", "metavar", "=", "'DIR'", ",", "\n", "help", "=", "'path to data directory; we load <split>.jsonl'", ")", "\n", "parser", ".", "add_argument", "(", "'--init-token'", ",", "type", "=", "int", ",", "default", "=", "None", ",", "\n", "help", "=", "'add token at the beginning of each batch item'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.wsc.wsc_task.WSCTask.__init__": [[43, 58], ["fairseq.tasks.FairseqTask.__init__", "vocab.add_symbol", "fairseq.data.encoders.build_bpe", "fairseq.data.encoders.build_tokenizer"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol"], ["", "def", "__init__", "(", "self", ",", "args", ",", "vocab", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ")", "\n", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "mask", "=", "vocab", ".", "add_symbol", "(", "'<mask>'", ")", "\n", "\n", "self", ".", "bpe", "=", "encoders", ".", "build_bpe", "(", "args", ")", "\n", "self", ".", "tokenizer", "=", "encoders", ".", "build_tokenizer", "(", "args", ")", "\n", "\n", "# hack to handle GPT-2 BPE, which includes leading spaces", "\n", "if", "args", ".", "bpe", "==", "'gpt2'", ":", "\n", "            ", "self", ".", "leading_space", "=", "True", "\n", "self", ".", "trailing_space", "=", "False", "\n", "", "else", ":", "\n", "            ", "self", ".", "leading_space", "=", "False", "\n", "self", ".", "trailing_space", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.wsc.wsc_task.WSCTask.load_dictionary": [[59, 69], ["fairseq.data.Dictionary.load", "fairseq.data.Dictionary.load.add_symbol"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol"], ["", "", "@", "classmethod", "\n", "def", "load_dictionary", "(", "cls", ",", "filename", ")", ":", "\n", "        ", "\"\"\"Load the dictionary from the filename\n\n        Args:\n            filename (str): the filename\n        \"\"\"", "\n", "dictionary", "=", "Dictionary", ".", "load", "(", "filename", ")", "\n", "dictionary", ".", "add_symbol", "(", "'<mask>'", ")", "\n", "return", "dictionary", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.wsc.wsc_task.WSCTask.setup_task": [[70, 79], ["cls.load_dictionary", "print", "cls", "os.path.join", "len"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.sentence_prediction.SentencePredictionTask.load_dictionary", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.masked_lm_dictionary.BertDictionary.cls"], ["", "@", "classmethod", "\n", "def", "setup_task", "(", "cls", ",", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "assert", "args", ".", "criterion", "==", "'wsc'", ",", "'Must set --criterion=wsc'", "\n", "\n", "# load data and label dictionaries", "\n", "vocab", "=", "cls", ".", "load_dictionary", "(", "os", ".", "path", ".", "join", "(", "args", ".", "data", ",", "'dict.txt'", ")", ")", "\n", "print", "(", "'| dictionary: {} types'", ".", "format", "(", "len", "(", "vocab", ")", ")", ")", "\n", "\n", "return", "cls", "(", "args", ",", "vocab", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.wsc.wsc_task.WSCTask.load_dataset": [[80, 206], ["wsc_utils.jsonl_iterator", "numpy.array", "fairseq.data.ListDataset", "fairseq.data.ListDataset", "numpy.array", "fairseq.data.ListDataset", "fairseq.data.ListDataset", "fairseq.data.ListDataset", "fairseq.data.NestedDictionaryDataset", "fairseq.data.SortDataset", "wsc_task.WSCTask.vocab.encode_line().long", "os.path.join", "os.path.exists", "FileNotFoundError", "wsc_utils.filter_noun_chunks", "fairseq.data.ListDataset.append", "fairseq.data.ListDataset.append", "numpy.array.append", "fairseq.data.data_utils.collate_tokens", "fairseq.data.data_utils.collate_tokens", "fairseq.data.ListDataset.append", "fairseq.data.ListDataset.append", "numpy.array.append", "fairseq.data.ListDataset.append", "fairseq.data.IdDataset", "fairseq.data.NumSamplesDataset", "fairseq.data.NumelDataset", "fairseq.data.data_utils.numpy_seed", "numpy.random.permutation", "wsc_task.WSCTask.tokenizer.encode", "wsc_task.WSCTask.bpe.encode", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "sentence[].text_with_ws.endswith", "pronoun_span.text_with_ws.endswith", "wsc_utils.extended_noun_chunks", "wsc_task.WSCTask.load_dataset.binarize"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.wsc.wsc_utils.jsonl_iterator", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.wsc.wsc_utils.filter_noun_chunks", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.data_utils.collate_tokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.data_utils.collate_tokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.data_utils.numpy_seed", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.encode", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.encode", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.wsc.wsc_utils.extended_noun_chunks", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.binarizer.Binarizer.binarize"], ["", "def", "load_dataset", "(", "self", ",", "split", ",", "epoch", "=", "0", ",", "combine", "=", "False", ",", "data_path", "=", "None", ",", "return_only", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Load a given dataset split.\n\n        Args:\n            split (str): name of the split (e.g., train, valid, test)\n        \"\"\"", "\n", "\n", "def", "binarize", "(", "s", ":", "str", ",", "append_eos", ":", "bool", "=", "False", ")", ":", "\n", "            ", "if", "self", ".", "tokenizer", "is", "not", "None", ":", "\n", "                ", "s", "=", "self", ".", "tokenizer", ".", "encode", "(", "s", ")", "\n", "", "if", "self", ".", "bpe", "is", "not", "None", ":", "\n", "                ", "s", "=", "self", ".", "bpe", ".", "encode", "(", "s", ")", "\n", "", "tokens", "=", "self", ".", "vocab", ".", "encode_line", "(", "\n", "s", ",", "append_eos", "=", "append_eos", ",", "add_if_not_exist", "=", "False", ",", "\n", ")", ".", "long", "(", ")", "\n", "if", "self", ".", "args", ".", "init_token", "is", "not", "None", ":", "\n", "                ", "tokens", "=", "torch", ".", "cat", "(", "[", "tokens", ".", "new", "(", "[", "self", ".", "args", ".", "init_token", "]", ")", ",", "tokens", "]", ")", "\n", "", "return", "tokens", "\n", "\n", "", "if", "data_path", "is", "None", ":", "\n", "            ", "data_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "args", ".", "data", ",", "split", "+", "'.jsonl'", ")", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "data_path", ")", ":", "\n", "            ", "raise", "FileNotFoundError", "(", "'Cannot find data: {}'", ".", "format", "(", "data_path", ")", ")", "\n", "\n", "", "query_tokens", "=", "[", "]", "\n", "query_masks", "=", "[", "]", "\n", "query_lengths", "=", "[", "]", "\n", "candidate_tokens", "=", "[", "]", "\n", "candidate_masks", "=", "[", "]", "\n", "candidate_lengths", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "\n", "for", "sentence", ",", "pronoun_span", ",", "query", ",", "label", "in", "wsc_utils", ".", "jsonl_iterator", "(", "data_path", ")", ":", "\n", "            ", "prefix", "=", "sentence", "[", ":", "pronoun_span", ".", "start", "]", ".", "text", "\n", "suffix", "=", "sentence", "[", "pronoun_span", ".", "end", ":", "]", ".", "text_with_ws", "\n", "\n", "# spaCy spans include trailing spaces, but we need to know about", "\n", "# leading spaces for the GPT-2 BPE", "\n", "leading_space", "=", "' '", "if", "sentence", "[", ":", "pronoun_span", ".", "start", "]", ".", "text_with_ws", ".", "endswith", "(", "' '", ")", "else", "''", "\n", "trailing_space", "=", "' '", "if", "pronoun_span", ".", "text_with_ws", ".", "endswith", "(", "' '", ")", "else", "''", "\n", "\n", "# get noun phrases, excluding pronouns and anything overlapping with the query", "\n", "cand_spans", "=", "wsc_utils", ".", "filter_noun_chunks", "(", "\n", "wsc_utils", ".", "extended_noun_chunks", "(", "sentence", ")", ",", "\n", "exclude_pronouns", "=", "True", ",", "\n", "exclude_query", "=", "query", ",", "\n", "exact_match", "=", "False", ",", "\n", ")", "\n", "\n", "def", "binarize_with_mask", "(", "txt", ")", ":", "\n", "                ", "toks", "=", "binarize", "(", "\n", "prefix", "+", "leading_space", "+", "txt", "+", "trailing_space", "+", "suffix", ",", "\n", "append_eos", "=", "True", ",", "\n", ")", "\n", "mask", "=", "torch", ".", "zeros_like", "(", "toks", ",", "dtype", "=", "torch", ".", "uint8", ")", "\n", "mask_start", "=", "len", "(", "binarize", "(", "prefix", ")", ")", "\n", "mask_size", "=", "len", "(", "binarize", "(", "leading_space", "+", "txt", ")", ")", "\n", "mask", "[", "mask_start", ":", "mask_start", "+", "mask_size", "]", "=", "1", "\n", "return", "toks", ",", "mask", "\n", "\n", "", "if", "query", "is", "not", "None", ":", "\n", "                ", "query_toks", ",", "query_mask", "=", "binarize_with_mask", "(", "query", ")", "\n", "query_len", "=", "len", "(", "query_toks", ")", "\n", "", "else", ":", "\n", "                ", "query_toks", ",", "query_mask", ",", "query_len", "=", "None", ",", "None", ",", "0", "\n", "\n", "", "query_tokens", ".", "append", "(", "query_toks", ")", "\n", "query_masks", ".", "append", "(", "query_mask", ")", "\n", "query_lengths", ".", "append", "(", "query_len", ")", "\n", "\n", "cand_toks", ",", "cand_masks", "=", "[", "]", ",", "[", "]", "\n", "for", "cand_span", "in", "cand_spans", ":", "\n", "                ", "toks", ",", "mask", "=", "binarize_with_mask", "(", "cand_span", ".", "text", ")", "\n", "cand_toks", ".", "append", "(", "toks", ")", "\n", "cand_masks", ".", "append", "(", "mask", ")", "\n", "\n", "# collate candidates", "\n", "", "cand_toks", "=", "data_utils", ".", "collate_tokens", "(", "cand_toks", ",", "pad_idx", "=", "self", ".", "vocab", ".", "pad", "(", ")", ")", "\n", "cand_masks", "=", "data_utils", ".", "collate_tokens", "(", "cand_masks", ",", "pad_idx", "=", "0", ")", "\n", "assert", "cand_toks", ".", "size", "(", ")", "==", "cand_masks", ".", "size", "(", ")", "\n", "\n", "candidate_tokens", ".", "append", "(", "cand_toks", ")", "\n", "candidate_masks", ".", "append", "(", "cand_masks", ")", "\n", "candidate_lengths", ".", "append", "(", "cand_toks", ".", "size", "(", "1", ")", ")", "\n", "\n", "labels", ".", "append", "(", "label", ")", "\n", "\n", "", "query_lengths", "=", "np", ".", "array", "(", "query_lengths", ")", "\n", "query_tokens", "=", "ListDataset", "(", "query_tokens", ",", "query_lengths", ")", "\n", "query_masks", "=", "ListDataset", "(", "query_masks", ",", "query_lengths", ")", "\n", "\n", "candidate_lengths", "=", "np", ".", "array", "(", "candidate_lengths", ")", "\n", "candidate_tokens", "=", "ListDataset", "(", "candidate_tokens", ",", "candidate_lengths", ")", "\n", "candidate_masks", "=", "ListDataset", "(", "candidate_masks", ",", "candidate_lengths", ")", "\n", "\n", "labels", "=", "ListDataset", "(", "labels", ",", "[", "1", "]", "*", "len", "(", "labels", ")", ")", "\n", "\n", "dataset", "=", "{", "\n", "'id'", ":", "IdDataset", "(", ")", ",", "\n", "'query_tokens'", ":", "query_tokens", ",", "\n", "'query_masks'", ":", "query_masks", ",", "\n", "'candidate_tokens'", ":", "candidate_tokens", ",", "\n", "'candidate_masks'", ":", "candidate_masks", ",", "\n", "'labels'", ":", "labels", ",", "\n", "'nsentences'", ":", "NumSamplesDataset", "(", ")", ",", "\n", "'ntokens'", ":", "NumelDataset", "(", "query_tokens", ",", "reduce", "=", "True", ")", ",", "\n", "}", "\n", "\n", "nested_dataset", "=", "NestedDictionaryDataset", "(", "\n", "dataset", ",", "\n", "sizes", "=", "[", "query_lengths", "]", ",", "\n", ")", "\n", "\n", "with", "data_utils", ".", "numpy_seed", "(", "self", ".", "args", ".", "seed", ")", ":", "\n", "            ", "shuffle", "=", "np", ".", "random", ".", "permutation", "(", "len", "(", "query_tokens", ")", ")", "\n", "", "dataset", "=", "SortDataset", "(", "\n", "nested_dataset", ",", "\n", "# shuffle", "\n", "sort_order", "=", "[", "shuffle", "]", ",", "\n", ")", "\n", "\n", "if", "return_only", ":", "\n", "            ", "return", "dataset", "\n", "\n", "", "self", ".", "datasets", "[", "split", "]", "=", "dataset", "\n", "return", "self", ".", "datasets", "[", "split", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.wsc.wsc_task.WSCTask.build_dataset_for_inference": [[207, 216], ["tempfile.NamedTemporaryFile", "h.write", "wsc_task.WSCTask.load_dataset", "json.dumps"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.sentence_prediction.SentencePredictionTask.load_dataset"], ["", "def", "build_dataset_for_inference", "(", "self", ",", "sample_json", ")", ":", "\n", "        ", "with", "tempfile", ".", "NamedTemporaryFile", "(", "buffering", "=", "0", ")", "as", "h", ":", "\n", "            ", "h", ".", "write", "(", "(", "json", ".", "dumps", "(", "sample_json", ")", "+", "'\\n'", ")", ".", "encode", "(", "'utf-8'", ")", ")", "\n", "dataset", "=", "self", ".", "load_dataset", "(", "\n", "'disambiguate_pronoun'", ",", "\n", "data_path", "=", "h", ".", "name", ",", "\n", "return_only", "=", "True", ",", "\n", ")", "\n", "", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.wsc.wsc_task.WSCTask.disambiguate_pronoun": [[217, 253], ["wsc_utils.convert_sentence_to_json", "wsc_task.WSCTask.build_dataset_for_inference", "wsc_task.WSCTask.collater", "wsc_task.WSCTask.disambiguate_pronoun.get_lprobs"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.wsc.wsc_utils.convert_sentence_to_json", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.masked_lm.MaskedLMTask.build_dataset_for_inference", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.collater"], ["", "def", "disambiguate_pronoun", "(", "self", ",", "model", ",", "sentence", ",", "use_cuda", "=", "False", ")", ":", "\n", "        ", "sample_json", "=", "wsc_utils", ".", "convert_sentence_to_json", "(", "sentence", ")", "\n", "dataset", "=", "self", ".", "build_dataset_for_inference", "(", "sample_json", ")", "\n", "sample", "=", "dataset", ".", "collater", "(", "[", "dataset", "[", "0", "]", "]", ")", "\n", "if", "use_cuda", ":", "\n", "            ", "sample", "=", "utils", ".", "move_to_cuda", "(", "sample", ")", "\n", "\n", "", "def", "get_masked_input", "(", "tokens", ",", "mask", ")", ":", "\n", "            ", "masked_tokens", "=", "tokens", ".", "clone", "(", ")", "\n", "masked_tokens", "[", "mask", "]", "=", "self", ".", "mask", "\n", "return", "masked_tokens", "\n", "\n", "", "def", "get_lprobs", "(", "tokens", ",", "mask", ")", ":", "\n", "            ", "logits", ",", "_", "=", "model", "(", "src_tokens", "=", "get_masked_input", "(", "tokens", ",", "mask", ")", ")", "\n", "lprobs", "=", "F", ".", "log_softmax", "(", "logits", ",", "dim", "=", "-", "1", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "scores", "=", "lprobs", ".", "gather", "(", "2", ",", "tokens", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "mask", "=", "mask", ".", "type_as", "(", "scores", ")", "\n", "scores", "=", "(", "scores", "*", "mask", ")", ".", "sum", "(", "dim", "=", "-", "1", ")", "/", "mask", ".", "sum", "(", "dim", "=", "-", "1", ")", "\n", "return", "scores", "\n", "\n", "", "cand_lprobs", "=", "get_lprobs", "(", "\n", "sample", "[", "'candidate_tokens'", "]", "[", "0", "]", ",", "\n", "sample", "[", "'candidate_masks'", "]", "[", "0", "]", ",", "\n", ")", "\n", "if", "sample", "[", "'query_tokens'", "]", "[", "0", "]", "is", "not", "None", ":", "\n", "            ", "query_lprobs", "=", "get_lprobs", "(", "\n", "sample", "[", "'query_tokens'", "]", "[", "0", "]", ".", "unsqueeze", "(", "0", ")", ",", "\n", "sample", "[", "'query_masks'", "]", "[", "0", "]", ".", "unsqueeze", "(", "0", ")", ",", "\n", ")", "\n", "return", "(", "query_lprobs", ">=", "cand_lprobs", ")", ".", "all", "(", ")", ".", "item", "(", ")", "==", "1", "\n", "", "else", ":", "\n", "            ", "best_idx", "=", "cand_lprobs", ".", "argmax", "(", ")", ".", "item", "(", ")", "\n", "full_cand", "=", "sample", "[", "'candidate_tokens'", "]", "[", "0", "]", "[", "best_idx", "]", "\n", "mask", "=", "sample", "[", "'candidate_masks'", "]", "[", "0", "]", "[", "best_idx", "]", "\n", "toks", "=", "full_cand", "[", "mask", "]", "\n", "return", "self", ".", "bpe", ".", "decode", "(", "self", ".", "source_dictionary", ".", "string", "(", "toks", ")", ")", ".", "strip", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.wsc.wsc_task.WSCTask.source_dictionary": [[254, 257], ["None"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "source_dictionary", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.wsc.wsc_task.WSCTask.target_dictionary": [[258, 261], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "target_dictionary", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "vocab", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.wsc.wsc_utils.convert_sentence_to_json": [[10, 32], ["sentence.replace().replace().replace.split", "rest.split", "len", "sentence.replace().replace().replace.replace().replace().replace", "sentence.replace().replace().replace.split", "rest.split", "len", "prefix.rstrip().split", "prefix.rstrip().split", "sentence.replace().replace().replace.replace().replace", "prefix.rstrip", "prefix.rstrip", "sentence.replace().replace().replace.replace"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["def", "convert_sentence_to_json", "(", "sentence", ")", ":", "\n", "    ", "if", "'_'", "in", "sentence", ":", "\n", "        ", "prefix", ",", "rest", "=", "sentence", ".", "split", "(", "'_'", ",", "1", ")", "\n", "query", ",", "rest", "=", "rest", ".", "split", "(", "'_'", ",", "1", ")", "\n", "query_index", "=", "len", "(", "prefix", ".", "rstrip", "(", ")", ".", "split", "(", "' '", ")", ")", "\n", "", "else", ":", "\n", "        ", "query", ",", "query_index", "=", "None", ",", "None", "\n", "\n", "", "prefix", ",", "rest", "=", "sentence", ".", "split", "(", "'['", ",", "1", ")", "\n", "pronoun", ",", "rest", "=", "rest", ".", "split", "(", "']'", ",", "1", ")", "\n", "pronoun_index", "=", "len", "(", "prefix", ".", "rstrip", "(", ")", ".", "split", "(", "' '", ")", ")", "\n", "\n", "sentence", "=", "sentence", ".", "replace", "(", "'_'", ",", "''", ")", ".", "replace", "(", "'['", ",", "''", ")", ".", "replace", "(", "']'", ",", "''", ")", "\n", "\n", "return", "{", "\n", "'idx'", ":", "0", ",", "\n", "'text'", ":", "sentence", ",", "\n", "'target'", ":", "{", "\n", "'span1_index'", ":", "query_index", ",", "\n", "'span1_text'", ":", "query", ",", "\n", "'span2_index'", ":", "pronoun_index", ",", "\n", "'span2_text'", ":", "pronoun", ",", "\n", "}", ",", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.wsc.wsc_utils.extended_noun_chunks": [[36, 50], ["enumerate", "noun_chunks.add", "sorted", "noun_chunks.add", "len"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.add", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.add"], ["", "def", "extended_noun_chunks", "(", "sentence", ")", ":", "\n", "    ", "noun_chunks", "=", "{", "(", "np", ".", "start", ",", "np", ".", "end", ")", "for", "np", "in", "sentence", ".", "noun_chunks", "}", "\n", "np_start", ",", "cur_np", "=", "0", ",", "'NONE'", "\n", "for", "i", ",", "token", "in", "enumerate", "(", "sentence", ")", ":", "\n", "        ", "np_type", "=", "token", ".", "pos_", "if", "token", ".", "pos_", "in", "{", "'NOUN'", ",", "'PROPN'", "}", "else", "'NONE'", "\n", "if", "np_type", "!=", "cur_np", ":", "\n", "            ", "if", "cur_np", "!=", "'NONE'", ":", "\n", "                ", "noun_chunks", ".", "add", "(", "(", "np_start", ",", "i", ")", ")", "\n", "", "if", "np_type", "!=", "'NONE'", ":", "\n", "                ", "np_start", "=", "i", "\n", "", "cur_np", "=", "np_type", "\n", "", "", "if", "cur_np", "!=", "'NONE'", ":", "\n", "        ", "noun_chunks", ".", "add", "(", "(", "np_start", ",", "len", "(", "sentence", ")", ")", ")", "\n", "", "return", "[", "sentence", "[", "s", ":", "e", "]", "for", "(", "s", ",", "e", ")", "in", "sorted", "(", "noun_chunks", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.wsc.wsc_utils.find_token": [[52, 59], ["None"], "function", ["None"], ["", "def", "find_token", "(", "sentence", ",", "start_pos", ")", ":", "\n", "    ", "found_tok", "=", "None", "\n", "for", "tok", "in", "sentence", ":", "\n", "        ", "if", "tok", ".", "idx", "==", "start_pos", ":", "\n", "            ", "found_tok", "=", "tok", "\n", "break", "\n", "", "", "return", "found_tok", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.wsc.wsc_utils.find_span": [[61, 74], ["search_text.lower.lower", "sentence[].text.lower", "sentence[].text.lower.startswith", "len", "len"], "function", ["None"], ["", "def", "find_span", "(", "sentence", ",", "search_text", ",", "start", "=", "0", ")", ":", "\n", "    ", "search_text", "=", "search_text", ".", "lower", "(", ")", "\n", "for", "tok", "in", "sentence", "[", "start", ":", "]", ":", "\n", "        ", "remainder", "=", "sentence", "[", "tok", ".", "i", ":", "]", ".", "text", ".", "lower", "(", ")", "\n", "if", "remainder", ".", "startswith", "(", "search_text", ")", ":", "\n", "            ", "len_to_consume", "=", "len", "(", "search_text", ")", "\n", "start_idx", "=", "tok", ".", "idx", "\n", "for", "next_tok", "in", "sentence", "[", "tok", ".", "i", ":", "]", ":", "\n", "                ", "end_idx", "=", "next_tok", ".", "idx", "+", "len", "(", "next_tok", ".", "text", ")", "\n", "if", "end_idx", "-", "start_idx", "==", "len_to_consume", ":", "\n", "                    ", "span", "=", "sentence", "[", "tok", ".", "i", ":", "next_tok", ".", "i", "+", "1", "]", "\n", "return", "span", "\n", "", "", "", "", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.wsc.wsc_utils.get_detokenizer": [[76, 81], ["functools.lru_cache", "MosesDetokenizer"], "function", ["None"], ["", "@", "lru_cache", "(", "maxsize", "=", "1", ")", "\n", "def", "get_detokenizer", "(", ")", ":", "\n", "    ", "from", "sacremoses", "import", "MosesDetokenizer", "\n", "detok", "=", "MosesDetokenizer", "(", "lang", "=", "'en'", ")", "\n", "return", "detok", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.wsc.wsc_utils.get_spacy_nlp": [[83, 88], ["functools.lru_cache", "en_core_web_lg.load"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load"], ["", "@", "lru_cache", "(", "maxsize", "=", "1", ")", "\n", "def", "get_spacy_nlp", "(", ")", ":", "\n", "    ", "import", "en_core_web_lg", "\n", "nlp", "=", "en_core_web_lg", ".", "load", "(", ")", "\n", "return", "nlp", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.wsc.wsc_utils.jsonl_iterator": [[90, 191], ["wsc_utils.get_detokenizer", "wsc_utils.get_spacy_nlp", "open", "json.loads", "sample[].split", "wsc_utils.jsonl_iterator.strip_pronoun"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.wsc.wsc_utils.get_detokenizer", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.wsc.wsc_utils.get_spacy_nlp", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["", "def", "jsonl_iterator", "(", "input_fname", ",", "positive_only", "=", "False", ",", "ngram_order", "=", "3", ",", "eval", "=", "False", ")", ":", "\n", "    ", "detok", "=", "get_detokenizer", "(", ")", "\n", "nlp", "=", "get_spacy_nlp", "(", ")", "\n", "\n", "with", "open", "(", "input_fname", ")", "as", "fin", ":", "\n", "        ", "for", "line", "in", "fin", ":", "\n", "            ", "sample", "=", "json", ".", "loads", "(", "line", ".", "strip", "(", ")", ")", "\n", "\n", "if", "positive_only", "and", "'label'", "in", "sample", "and", "not", "sample", "[", "'label'", "]", ":", "\n", "# only consider examples where the query is correct", "\n", "                ", "continue", "\n", "\n", "", "target", "=", "sample", "[", "'target'", "]", "\n", "\n", "# clean up the query", "\n", "query", "=", "target", "[", "'span1_text'", "]", "\n", "if", "query", "is", "not", "None", ":", "\n", "                ", "if", "'\\n'", "in", "query", ":", "\n", "                    ", "continue", "\n", "", "if", "query", ".", "endswith", "(", "'.'", ")", "or", "query", ".", "endswith", "(", "','", ")", ":", "\n", "                    ", "query", "=", "query", "[", ":", "-", "1", "]", "\n", "\n", "# split tokens", "\n", "", "", "tokens", "=", "sample", "[", "'text'", "]", ".", "split", "(", "' '", ")", "\n", "\n", "def", "strip_pronoun", "(", "x", ")", ":", "\n", "                ", "return", "x", ".", "rstrip", "(", "'.,\"'", ")", "\n", "\n", "# find the pronoun", "\n", "", "pronoun_idx", "=", "target", "[", "'span2_index'", "]", "\n", "pronoun", "=", "strip_pronoun", "(", "target", "[", "'span2_text'", "]", ")", "\n", "if", "strip_pronoun", "(", "tokens", "[", "pronoun_idx", "]", ")", "!=", "pronoun", ":", "\n", "# hack: sometimes the index is misaligned", "\n", "                ", "if", "strip_pronoun", "(", "tokens", "[", "pronoun_idx", "+", "1", "]", ")", "==", "pronoun", ":", "\n", "                    ", "pronoun_idx", "+=", "1", "\n", "", "else", ":", "\n", "                    ", "raise", "Exception", "(", "'Misaligned pronoun!'", ")", "\n", "", "", "assert", "strip_pronoun", "(", "tokens", "[", "pronoun_idx", "]", ")", "==", "pronoun", "\n", "\n", "# split tokens before and after the pronoun", "\n", "before", "=", "tokens", "[", ":", "pronoun_idx", "]", "\n", "after", "=", "tokens", "[", "pronoun_idx", "+", "1", ":", "]", "\n", "\n", "# the GPT BPE attaches leading spaces to tokens, so we keep track", "\n", "# of whether we need spaces before or after the pronoun", "\n", "leading_space", "=", "' '", "if", "pronoun_idx", ">", "0", "else", "''", "\n", "trailing_space", "=", "' '", "if", "len", "(", "after", ")", ">", "0", "else", "''", "\n", "\n", "# detokenize", "\n", "before", "=", "detok", ".", "detokenize", "(", "before", ",", "return_str", "=", "True", ")", "\n", "pronoun", "=", "detok", ".", "detokenize", "(", "[", "pronoun", "]", ",", "return_str", "=", "True", ")", "\n", "after", "=", "detok", ".", "detokenize", "(", "after", ",", "return_str", "=", "True", ")", "\n", "\n", "# hack: when the pronoun ends in a period (or comma), move the", "\n", "# punctuation to the \"after\" part", "\n", "if", "pronoun", ".", "endswith", "(", "'.'", ")", "or", "pronoun", ".", "endswith", "(", "','", ")", ":", "\n", "                ", "after", "=", "pronoun", "[", "-", "1", "]", "+", "trailing_space", "+", "after", "\n", "pronoun", "=", "pronoun", "[", ":", "-", "1", "]", "\n", "\n", "# hack: when the \"after\" part begins with a comma or period, remove", "\n", "# the trailing space", "\n", "", "if", "after", ".", "startswith", "(", "'.'", ")", "or", "after", ".", "startswith", "(", "','", ")", ":", "\n", "                ", "trailing_space", "=", "''", "\n", "\n", "# parse sentence with spacy", "\n", "", "sentence", "=", "nlp", "(", "before", "+", "leading_space", "+", "pronoun", "+", "trailing_space", "+", "after", ")", "\n", "\n", "# find pronoun span", "\n", "start", "=", "len", "(", "before", "+", "leading_space", ")", "\n", "first_pronoun_tok", "=", "find_token", "(", "sentence", ",", "start_pos", "=", "start", ")", "\n", "pronoun_span", "=", "find_span", "(", "sentence", ",", "pronoun", ",", "start", "=", "first_pronoun_tok", ".", "i", ")", "\n", "assert", "pronoun_span", ".", "text", "==", "pronoun", "\n", "\n", "if", "eval", ":", "\n", "# convert to format where pronoun is surrounded by \"[]\" and", "\n", "# query is surrounded by \"_\"", "\n", "                ", "query_span", "=", "find_span", "(", "sentence", ",", "query", ")", "\n", "query_with_ws", "=", "'_{}_{}'", ".", "format", "(", "\n", "query_span", ".", "text", ",", "\n", "(", "' '", "if", "query_span", ".", "text_with_ws", ".", "endswith", "(", "' '", ")", "else", "''", ")", "\n", ")", "\n", "pronoun_with_ws", "=", "'[{}]{}'", ".", "format", "(", "\n", "pronoun_span", ".", "text", ",", "\n", "(", "' '", "if", "pronoun_span", ".", "text_with_ws", ".", "endswith", "(", "' '", ")", "else", "''", ")", "\n", ")", "\n", "if", "query_span", ".", "start", "<", "pronoun_span", ".", "start", ":", "\n", "                    ", "first", "=", "(", "query_span", ",", "query_with_ws", ")", "\n", "second", "=", "(", "pronoun_span", ",", "pronoun_with_ws", ")", "\n", "", "else", ":", "\n", "                    ", "first", "=", "(", "pronoun_span", ",", "pronoun_with_ws", ")", "\n", "second", "=", "(", "query_span", ",", "query_with_ws", ")", "\n", "", "sentence", "=", "(", "\n", "sentence", "[", ":", "first", "[", "0", "]", ".", "start", "]", ".", "text_with_ws", "\n", "+", "first", "[", "1", "]", "\n", "+", "sentence", "[", "first", "[", "0", "]", ".", "end", ":", "second", "[", "0", "]", ".", "start", "]", ".", "text_with_ws", "\n", "+", "second", "[", "1", "]", "\n", "+", "sentence", "[", "second", "[", "0", "]", ".", "end", ":", "]", ".", "text", "\n", ")", "\n", "yield", "sentence", ",", "sample", ".", "get", "(", "'label'", ",", "None", ")", "\n", "", "else", ":", "\n", "                ", "yield", "sentence", ",", "pronoun_span", ",", "query", ",", "sample", ".", "get", "(", "'label'", ",", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.wsc.wsc_utils.filter_noun_chunks": [[193, 220], ["exclude_query.lower", "chunk.text.lower", "filtered_chunks.append", "all"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "", "", "", "def", "filter_noun_chunks", "(", "chunks", ",", "exclude_pronouns", "=", "False", ",", "exclude_query", "=", "None", ",", "exact_match", "=", "False", ")", ":", "\n", "    ", "if", "exclude_pronouns", ":", "\n", "        ", "chunks", "=", "[", "\n", "np", "for", "np", "in", "chunks", "if", "(", "\n", "np", ".", "lemma_", "!=", "'-PRON-'", "\n", "and", "not", "all", "(", "tok", ".", "pos_", "==", "'PRON'", "for", "tok", "in", "np", ")", "\n", ")", "\n", "]", "\n", "\n", "", "if", "exclude_query", "is", "not", "None", ":", "\n", "        ", "excl_txt", "=", "[", "exclude_query", ".", "lower", "(", ")", "]", "\n", "filtered_chunks", "=", "[", "]", "\n", "for", "chunk", "in", "chunks", ":", "\n", "            ", "lower_chunk", "=", "chunk", ".", "text", ".", "lower", "(", ")", "\n", "found", "=", "False", "\n", "for", "excl", "in", "excl_txt", ":", "\n", "                ", "if", "(", "\n", "(", "not", "exact_match", "and", "(", "lower_chunk", "in", "excl", "or", "excl", "in", "lower_chunk", ")", ")", "\n", "or", "lower_chunk", "==", "excl", "\n", ")", ":", "\n", "                    ", "found", "=", "True", "\n", "break", "\n", "", "", "if", "not", "found", ":", "\n", "                ", "filtered_chunks", ".", "append", "(", "chunk", ")", "\n", "", "", "chunks", "=", "filtered_chunks", "\n", "\n", "", "return", "chunks", "\n", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.wsc.wsc_criterion.WSCCriterion.__init__": [[19, 27], ["fairseq.criterions.FairseqCriterion.__init__", "fairseq.data.encoders.build_bpe", "fairseq.data.encoders.build_tokenizer", "open"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "task", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "task", ")", "\n", "if", "self", ".", "args", ".", "save_predictions", "is", "not", "None", ":", "\n", "            ", "self", ".", "prediction_h", "=", "open", "(", "self", ".", "args", ".", "save_predictions", ",", "'w'", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "prediction_h", "=", "None", "\n", "", "self", ".", "bpe", "=", "encoders", ".", "build_bpe", "(", "args", ")", "\n", "self", ".", "tokenizer", "=", "encoders", ".", "build_tokenizer", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.wsc.wsc_criterion.WSCCriterion.__del__": [[28, 31], ["wsc_criterion.WSCCriterion.prediction_h.close"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.GoldMiner.close"], ["", "def", "__del__", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "prediction_h", "is", "not", "None", ":", "\n", "            ", "self", ".", "prediction_h", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.wsc.wsc_criterion.WSCCriterion.add_args": [[32, 41], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add criterion-specific arguments to the parser.\"\"\"", "\n", "parser", ".", "add_argument", "(", "'--wsc-margin-alpha'", ",", "type", "=", "float", ",", "metavar", "=", "'A'", ",", "default", "=", "1.0", ")", "\n", "parser", ".", "add_argument", "(", "'--wsc-margin-beta'", ",", "type", "=", "float", ",", "metavar", "=", "'B'", ",", "default", "=", "0.0", ")", "\n", "parser", ".", "add_argument", "(", "'--wsc-cross-entropy'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'use cross entropy formulation instead of margin loss'", ")", "\n", "parser", ".", "add_argument", "(", "'--save-predictions'", ",", "metavar", "=", "'FILE'", ",", "\n", "help", "=", "'file to save predictions to'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.wsc.wsc_criterion.WSCCriterion.forward": [[42, 110], ["enumerate", "tokens.clone", "model", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax.gather().squeeze", "mask.type_as.type_as.type_as", "wsc_criterion.WSCCriterion.forward.get_lprobs"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.model", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.log_softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.log_softmax"], ["", "def", "forward", "(", "self", ",", "model", ",", "sample", ",", "reduce", "=", "True", ")", ":", "\n", "\n", "        ", "def", "get_masked_input", "(", "tokens", ",", "mask", ")", ":", "\n", "            ", "masked_tokens", "=", "tokens", ".", "clone", "(", ")", "\n", "masked_tokens", "[", "mask", "]", "=", "self", ".", "task", ".", "mask", "\n", "return", "masked_tokens", "\n", "\n", "", "def", "get_lprobs", "(", "tokens", ",", "mask", ")", ":", "\n", "            ", "logits", ",", "_", "=", "model", "(", "src_tokens", "=", "get_masked_input", "(", "tokens", ",", "mask", ")", ")", "\n", "lprobs", "=", "F", ".", "log_softmax", "(", "logits", ",", "dim", "=", "-", "1", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "scores", "=", "lprobs", ".", "gather", "(", "2", ",", "tokens", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "mask", "=", "mask", ".", "type_as", "(", "scores", ")", "\n", "scores", "=", "(", "scores", "*", "mask", ")", ".", "sum", "(", "dim", "=", "-", "1", ")", "/", "mask", ".", "sum", "(", "dim", "=", "-", "1", ")", "\n", "return", "scores", "\n", "\n", "# compute loss and accuracy", "\n", "", "loss", ",", "nloss", "=", "0.", ",", "0", "\n", "ncorrect", ",", "nqueries", "=", "0", ",", "0", "\n", "for", "i", ",", "label", "in", "enumerate", "(", "sample", "[", "'labels'", "]", ")", ":", "\n", "            ", "query_lprobs", "=", "get_lprobs", "(", "\n", "sample", "[", "'query_tokens'", "]", "[", "i", "]", ".", "unsqueeze", "(", "0", ")", ",", "\n", "sample", "[", "'query_masks'", "]", "[", "i", "]", ".", "unsqueeze", "(", "0", ")", ",", "\n", ")", "\n", "cand_lprobs", "=", "get_lprobs", "(", "\n", "sample", "[", "'candidate_tokens'", "]", "[", "i", "]", ",", "\n", "sample", "[", "'candidate_masks'", "]", "[", "i", "]", ",", "\n", ")", "\n", "\n", "pred", "=", "(", "query_lprobs", ">=", "cand_lprobs", ")", ".", "all", "(", ")", ".", "item", "(", ")", "\n", "\n", "if", "label", "is", "not", "None", ":", "\n", "                ", "label", "=", "1", "if", "label", "else", "0", "\n", "ncorrect", "+=", "1", "if", "pred", "==", "label", "else", "0", "\n", "nqueries", "+=", "1", "\n", "\n", "", "if", "label", ":", "\n", "# only compute a loss for positive instances", "\n", "                ", "nloss", "+=", "1", "\n", "if", "self", ".", "args", ".", "wsc_cross_entropy", ":", "\n", "                    ", "loss", "+=", "F", ".", "cross_entropy", "(", "\n", "torch", ".", "cat", "(", "[", "query_lprobs", ",", "cand_lprobs", "]", ")", ".", "unsqueeze", "(", "0", ")", ",", "\n", "query_lprobs", ".", "new", "(", "[", "0", "]", ")", ".", "long", "(", ")", ",", "\n", ")", "\n", "", "else", ":", "\n", "                    ", "loss", "+=", "(", "\n", "-", "query_lprobs", "\n", "+", "self", ".", "args", ".", "wsc_margin_alpha", "*", "(", "\n", "cand_lprobs", "-", "query_lprobs", "+", "self", ".", "args", ".", "wsc_margin_beta", "\n", ")", ".", "clamp", "(", "min", "=", "0", ")", "\n", ")", ".", "sum", "(", ")", "\n", "\n", "", "", "id", "=", "sample", "[", "'id'", "]", "[", "i", "]", ".", "item", "(", ")", "\n", "if", "self", ".", "prediction_h", "is", "not", "None", ":", "\n", "                ", "print", "(", "'{}\\t{}\\t{}'", ".", "format", "(", "id", ",", "pred", ",", "label", ")", ",", "file", "=", "self", ".", "prediction_h", ")", "\n", "\n", "", "", "if", "nloss", "==", "0", ":", "\n", "            ", "loss", "=", "torch", ".", "tensor", "(", "0.0", ",", "requires_grad", "=", "True", ")", "\n", "\n", "", "sample_size", "=", "nqueries", "if", "nqueries", ">", "0", "else", "1", "\n", "logging_output", "=", "{", "\n", "'loss'", ":", "utils", ".", "item", "(", "loss", ".", "data", ")", "if", "reduce", "else", "loss", ".", "data", ",", "\n", "'ntokens'", ":", "sample", "[", "'ntokens'", "]", ",", "\n", "'nsentences'", ":", "sample", "[", "'nsentences'", "]", ",", "\n", "'sample_size'", ":", "sample_size", ",", "\n", "'ncorrect'", ":", "ncorrect", ",", "\n", "'nqueries'", ":", "nqueries", ",", "\n", "}", "\n", "return", "loss", ",", "sample_size", ",", "logging_output", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.wsc.wsc_criterion.WSCCriterion.aggregate_logging_outputs": [[111, 132], ["sum", "sum", "sum", "sum", "sum", "sum", "log.get", "log.get", "log.get", "log.get", "math.log", "log.get", "log.get", "float"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.log"], ["", "@", "staticmethod", "\n", "def", "aggregate_logging_outputs", "(", "logging_outputs", ")", ":", "\n", "        ", "\"\"\"Aggregate logging outputs from data parallel training.\"\"\"", "\n", "loss_sum", "=", "sum", "(", "log", ".", "get", "(", "'loss'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "ntokens", "=", "sum", "(", "log", ".", "get", "(", "'ntokens'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "nsentences", "=", "sum", "(", "log", ".", "get", "(", "'nsentences'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "sample_size", "=", "sum", "(", "log", ".", "get", "(", "'sample_size'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "\n", "agg_output", "=", "{", "\n", "'loss'", ":", "loss_sum", "/", "sample_size", "/", "math", ".", "log", "(", "2", ")", ",", "\n", "'ntokens'", ":", "ntokens", ",", "\n", "'nsentences'", ":", "nsentences", ",", "\n", "'sample_size'", ":", "sample_size", ",", "\n", "}", "\n", "\n", "ncorrect", "=", "sum", "(", "log", ".", "get", "(", "'ncorrect'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "nqueries", "=", "sum", "(", "log", ".", "get", "(", "'nqueries'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "if", "nqueries", ">", "0", ":", "\n", "            ", "agg_output", "[", "'accuracy'", "]", "=", "ncorrect", "/", "float", "(", "nqueries", ")", "\n", "\n", "", "return", "agg_output", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.infer.add_asr_eval_argument": [[25, 48], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "function", ["None"], ["def", "add_asr_eval_argument", "(", "parser", ")", ":", "\n", "    ", "parser", ".", "add_argument", "(", "\"--ctc\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"decode a ctc model\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--rnnt\"", ",", "default", "=", "False", ",", "help", "=", "\"decode a rnnt model\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--kspmodel\"", ",", "default", "=", "None", ",", "help", "=", "\"sentence piece model\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--wfstlm\"", ",", "default", "=", "None", ",", "help", "=", "\"wfstlm on dictonary output units\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--rnnt_decoding_type\"", ",", "\n", "default", "=", "\"greedy\"", ",", "\n", "help", "=", "\"wfstlm on dictonary\\\noutput units\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--lm_weight\"", ",", "\n", "default", "=", "0.2", ",", "\n", "help", "=", "\"weight for wfstlm while interpolating\\\nwith neural score\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--rnnt_len_penalty\"", ",", "default", "=", "-", "0.5", ",", "help", "=", "\"rnnt length penalty on word level\"", "\n", ")", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.infer.check_args": [[50, 59], ["None"], "function", ["None"], ["", "def", "check_args", "(", "args", ")", ":", "\n", "    ", "assert", "args", ".", "path", "is", "not", "None", ",", "\"--path required for generation!\"", "\n", "assert", "args", ".", "results_path", "is", "not", "None", ",", "\"--results_path required for generation!\"", "\n", "assert", "(", "\n", "not", "args", ".", "sampling", "or", "args", ".", "nbest", "==", "args", ".", "beam", "\n", ")", ",", "\"--sampling requires --nbest to be equal to --beam\"", "\n", "assert", "(", "\n", "args", ".", "replace_unk", "is", "None", "or", "args", ".", "raw_text", "\n", ")", ",", "\"--replace-unk requires a raw text dataset (--raw-text)\"", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.infer.get_dataset_itr": [[61, 73], ["task.get_batch_iterator().next_epoch_itr", "task.get_batch_iterator", "task.dataset"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.EpochBatchIterator.next_epoch_itr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.fairseq_task.FairseqTask.get_batch_iterator", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.fairseq_task.FairseqTask.dataset"], ["", "def", "get_dataset_itr", "(", "args", ",", "task", ")", ":", "\n", "    ", "return", "task", ".", "get_batch_iterator", "(", "\n", "dataset", "=", "task", ".", "dataset", "(", "args", ".", "gen_subset", ")", ",", "\n", "max_tokens", "=", "args", ".", "max_tokens", ",", "\n", "max_sentences", "=", "args", ".", "max_sentences", ",", "\n", "max_positions", "=", "(", "1000000.0", ",", "1000000.0", ")", ",", "\n", "ignore_invalid_inputs", "=", "args", ".", "skip_invalid_size_inputs_valid_test", ",", "\n", "required_batch_size_multiple", "=", "args", ".", "required_batch_size_multiple", ",", "\n", "num_shards", "=", "args", ".", "num_shards", ",", "\n", "shard_id", "=", "args", ".", "shard_id", ",", "\n", "num_workers", "=", "args", ".", "num_workers", ",", "\n", ")", ".", "next_epoch_itr", "(", "shuffle", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.infer.process_predictions": [[75, 103], ["tgt_dict.string", "sp.DecodePieces", "print", "print", "tgt_dict.string", "sp.DecodePieces", "print", "print", "min", "hypo[].int().cpu", "tgt_dict.string.split", "tgt_dict.string.split", "logger.debug", "logger.debug", "logger.debug", "len", "hypo[].int"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.hub_utils.GeneratorHubInterface.string", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.hub_utils.GeneratorHubInterface.string", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["", "def", "process_predictions", "(", "args", ",", "hypos", ",", "sp", ",", "tgt_dict", ",", "target_tokens", ",", "res_files", ",", "speaker", ",", "id", ")", ":", "\n", "    ", "for", "hypo", "in", "hypos", "[", ":", "min", "(", "len", "(", "hypos", ")", ",", "args", ".", "nbest", ")", "]", ":", "\n", "        ", "hyp_pieces", "=", "tgt_dict", ".", "string", "(", "hypo", "[", "\"tokens\"", "]", ".", "int", "(", ")", ".", "cpu", "(", ")", ")", "\n", "hyp_words", "=", "sp", ".", "DecodePieces", "(", "hyp_pieces", ".", "split", "(", ")", ")", "\n", "print", "(", "\n", "\"{} ({}-{})\"", ".", "format", "(", "hyp_pieces", ",", "speaker", ",", "id", ")", ",", "\n", "file", "=", "res_files", "[", "\"hypo.units\"", "]", ",", "\n", ")", "\n", "print", "(", "\n", "\"{} ({}-{})\"", ".", "format", "(", "hyp_words", ",", "speaker", ",", "id", ")", ",", "\n", "file", "=", "res_files", "[", "\"hypo.words\"", "]", ",", "\n", ")", "\n", "\n", "tgt_pieces", "=", "tgt_dict", ".", "string", "(", "target_tokens", ")", "\n", "tgt_words", "=", "sp", ".", "DecodePieces", "(", "tgt_pieces", ".", "split", "(", ")", ")", "\n", "print", "(", "\n", "\"{} ({}-{})\"", ".", "format", "(", "tgt_pieces", ",", "speaker", ",", "id", ")", ",", "\n", "file", "=", "res_files", "[", "\"ref.units\"", "]", ",", "\n", ")", "\n", "print", "(", "\n", "\"{} ({}-{})\"", ".", "format", "(", "tgt_words", ",", "speaker", ",", "id", ")", ",", "\n", "file", "=", "res_files", "[", "\"ref.words\"", "]", ",", "\n", ")", "\n", "# only score top hypothesis", "\n", "if", "not", "args", ".", "quiet", ":", "\n", "            ", "logger", ".", "debug", "(", "\"HYPO:\"", "+", "hyp_words", ")", "\n", "logger", ".", "debug", "(", "\"TARGET:\"", "+", "tgt_words", ")", "\n", "logger", ".", "debug", "(", "\"___________________\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.infer.prepare_result_files": [[105, 120], ["os.path.join", "open", "infer.prepare_result_files.get_res_file"], "function", ["None"], ["", "", "", "def", "prepare_result_files", "(", "args", ")", ":", "\n", "    ", "def", "get_res_file", "(", "file_prefix", ")", ":", "\n", "        ", "path", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "results_path", ",", "\n", "\"{}-{}-{}.txt\"", ".", "format", "(", "\n", "file_prefix", ",", "os", ".", "path", ".", "basename", "(", "args", ".", "path", ")", ",", "args", ".", "gen_subset", "\n", ")", ",", "\n", ")", "\n", "return", "open", "(", "path", ",", "\"w\"", ",", "buffering", "=", "1", ")", "\n", "\n", "", "return", "{", "\n", "\"hypo.words\"", ":", "get_res_file", "(", "\"hypo.word\"", ")", ",", "\n", "\"hypo.units\"", ":", "get_res_file", "(", "\"hypo.units\"", ")", ",", "\n", "\"ref.words\"", ":", "get_res_file", "(", "\"ref.word\"", ")", ",", "\n", "\"ref.units\"", ":", "get_res_file", "(", "\"ref.units\"", ")", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.infer.optimize_models": [[123, 135], ["model.make_generation_fast_", "model.half", "model.cuda"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.TransformerDecoderLayer.make_generation_fast_"], ["", "def", "optimize_models", "(", "args", ",", "use_cuda", ",", "models", ")", ":", "\n", "    ", "\"\"\"Optimize ensemble for generation\n    \"\"\"", "\n", "for", "model", "in", "models", ":", "\n", "        ", "model", ".", "make_generation_fast_", "(", "\n", "beamable_mm_beam_size", "=", "None", "if", "args", ".", "no_beamable_mm", "else", "args", ".", "beam", ",", "\n", "need_attn", "=", "args", ".", "print_alignment", ",", "\n", ")", "\n", "if", "args", ".", "fp16", ":", "\n", "            ", "model", ".", "half", "(", ")", "\n", "", "if", "use_cuda", ":", "\n", "            ", "model", ".", "cuda", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.infer.main": [[137, 233], ["infer.check_args", "fairseq.utils.import_user_module", "logger.info", "fairseq.tasks.setup_task", "tasks.setup_task.load_dataset", "logger.info", "logger.info", "fairseq.utils.load_ensemble_for_inference", "infer.optimize_models", "infer.get_dataset_itr", "fairseq.meters.StopwatchMeter", "tasks.setup_task.build_generator", "sentencepiece.SentencePieceProcessor", "spm.SentencePieceProcessor.Load", "infer.prepare_result_files", "logger.info", "logger.info", "torch.cuda.is_available", "tgt_dict.add_symbol", "args.path.split", "os.path.exists", "os.makedirs", "os.path.join", "fairseq.progress_bar.build_progress_bar", "fairseq.meters.TimeMeter", "len", "logger.info", "logger.info", "eval", "fairseq.meters.StopwatchMeter.start", "tasks.setup_task.inference_step", "sum", "fairseq.meters.StopwatchMeter.stop", "enumerate", "fairseq.meters.TimeMeter.update", "t.log", "tasks.setup_task.dataset", "fairseq.utils.move_to_cuda", "sample[].tolist", "fairseq.utils.strip_pad().int().cpu", "infer.process_predictions", "len", "round", "tasks.setup_task.dataset", "int", "tasks.setup_task.dataset", "int", "fairseq.utils.strip_pad().int", "fairseq.utils.strip_pad", "tgt_dict.pad"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.infer.check_args", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.import_user_module", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.sentence_prediction.SentencePredictionTask.setup_task", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.sentence_prediction.SentencePredictionTask.load_dataset", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.load_ensemble_for_inference", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.infer.optimize_models", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.infer.get_dataset_itr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.fairseq_task.FairseqTask.build_generator", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.infer.prepare_result_files", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.build_progress_bar", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.meters.StopwatchMeter.start", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.translation_moe.TranslationMoETask.inference_step", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.meters.StopwatchMeter.stop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.fairseq_task.FairseqTask.dataset", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.move_to_cuda", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.infer.process_predictions", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.fairseq_task.FairseqTask.dataset", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.fairseq_task.FairseqTask.dataset", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.strip_pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad"], ["", "", "", "def", "main", "(", "args", ")", ":", "\n", "    ", "check_args", "(", "args", ")", "\n", "import_user_module", "(", "args", ")", "\n", "\n", "if", "args", ".", "max_tokens", "is", "None", "and", "args", ".", "max_sentences", "is", "None", ":", "\n", "        ", "args", ".", "max_tokens", "=", "30000", "\n", "", "logger", ".", "info", "(", "args", ")", "\n", "\n", "use_cuda", "=", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "cpu", "\n", "\n", "# Load dataset splits", "\n", "task", "=", "tasks", ".", "setup_task", "(", "args", ")", "\n", "task", ".", "load_dataset", "(", "args", ".", "gen_subset", ")", "\n", "logger", ".", "info", "(", "\n", "\"| {} {} {} examples\"", ".", "format", "(", "\n", "args", ".", "data", ",", "args", ".", "gen_subset", ",", "len", "(", "task", ".", "dataset", "(", "args", ".", "gen_subset", ")", ")", "\n", ")", "\n", ")", "\n", "\n", "# Set dictionary", "\n", "tgt_dict", "=", "task", ".", "target_dictionary", "\n", "\n", "if", "args", ".", "ctc", "or", "args", ".", "rnnt", ":", "\n", "        ", "tgt_dict", ".", "add_symbol", "(", "\"<ctc_blank>\"", ")", "\n", "if", "args", ".", "ctc", ":", "\n", "            ", "logger", ".", "info", "(", "\"| decoding a ctc model\"", ")", "\n", "", "if", "args", ".", "rnnt", ":", "\n", "            ", "logger", ".", "info", "(", "\"| decoding a rnnt model\"", ")", "\n", "\n", "# Load ensemble", "\n", "", "", "logger", ".", "info", "(", "\"| loading model(s) from {}\"", ".", "format", "(", "args", ".", "path", ")", ")", "\n", "models", ",", "_model_args", "=", "utils", ".", "load_ensemble_for_inference", "(", "\n", "args", ".", "path", ".", "split", "(", "\":\"", ")", ",", "\n", "task", ",", "\n", "model_arg_overrides", "=", "eval", "(", "args", ".", "model_overrides", ")", ",", "# noqa", "\n", ")", "\n", "optimize_models", "(", "args", ",", "use_cuda", ",", "models", ")", "\n", "\n", "# Load dataset (possibly sharded)", "\n", "itr", "=", "get_dataset_itr", "(", "args", ",", "task", ")", "\n", "\n", "# Initialize generator", "\n", "gen_timer", "=", "StopwatchMeter", "(", ")", "\n", "generator", "=", "task", ".", "build_generator", "(", "args", ")", "\n", "\n", "num_sentences", "=", "0", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "args", ".", "results_path", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "args", ".", "results_path", ")", "\n", "\n", "", "sp", "=", "spm", ".", "SentencePieceProcessor", "(", ")", "\n", "sp", ".", "Load", "(", "os", ".", "path", ".", "join", "(", "args", ".", "data", ",", "'spm.model'", ")", ")", "\n", "\n", "res_files", "=", "prepare_result_files", "(", "args", ")", "\n", "with", "progress_bar", ".", "build_progress_bar", "(", "args", ",", "itr", ")", "as", "t", ":", "\n", "        ", "wps_meter", "=", "TimeMeter", "(", ")", "\n", "for", "sample", "in", "t", ":", "\n", "            ", "sample", "=", "utils", ".", "move_to_cuda", "(", "sample", ")", "if", "use_cuda", "else", "sample", "\n", "if", "\"net_input\"", "not", "in", "sample", ":", "\n", "                ", "continue", "\n", "\n", "", "prefix_tokens", "=", "None", "\n", "if", "args", ".", "prefix_size", ">", "0", ":", "\n", "                ", "prefix_tokens", "=", "sample", "[", "\"target\"", "]", "[", ":", ",", ":", "args", ".", "prefix_size", "]", "\n", "\n", "", "gen_timer", ".", "start", "(", ")", "\n", "hypos", "=", "task", ".", "inference_step", "(", "generator", ",", "models", ",", "sample", ",", "prefix_tokens", ")", "\n", "num_generated_tokens", "=", "sum", "(", "len", "(", "h", "[", "0", "]", "[", "\"tokens\"", "]", ")", "for", "h", "in", "hypos", ")", "\n", "gen_timer", ".", "stop", "(", "num_generated_tokens", ")", "\n", "\n", "for", "i", ",", "sample_id", "in", "enumerate", "(", "sample", "[", "'id'", "]", ".", "tolist", "(", ")", ")", ":", "\n", "                ", "speaker", "=", "task", ".", "dataset", "(", "args", ".", "gen_subset", ")", ".", "speakers", "[", "int", "(", "sample_id", ")", "]", "\n", "id", "=", "task", ".", "dataset", "(", "args", ".", "gen_subset", ")", ".", "ids", "[", "int", "(", "sample_id", ")", "]", "\n", "target_tokens", "=", "(", "\n", "utils", ".", "strip_pad", "(", "sample", "[", "\"target\"", "]", "[", "i", ",", ":", "]", ",", "tgt_dict", ".", "pad", "(", ")", ")", ".", "int", "(", ")", ".", "cpu", "(", ")", "\n", ")", "\n", "# Process top predictions", "\n", "process_predictions", "(", "\n", "args", ",", "hypos", "[", "i", "]", ",", "sp", ",", "tgt_dict", ",", "target_tokens", ",", "res_files", ",", "speaker", ",", "id", "\n", ")", "\n", "\n", "", "wps_meter", ".", "update", "(", "num_generated_tokens", ")", "\n", "t", ".", "log", "(", "{", "\"wps\"", ":", "round", "(", "wps_meter", ".", "avg", ")", "}", ")", "\n", "num_sentences", "+=", "sample", "[", "\"nsentences\"", "]", "\n", "\n", "", "", "logger", ".", "info", "(", "\n", "\"| Processed {} sentences ({} tokens) in {:.1f}s ({:.2f}\"", "\n", "\"sentences/s, {:.2f} tokens/s)\"", ".", "format", "(", "\n", "num_sentences", ",", "\n", "gen_timer", ".", "n", ",", "\n", "gen_timer", ".", "sum", ",", "\n", "num_sentences", "/", "gen_timer", ".", "sum", ",", "\n", "1.0", "/", "gen_timer", ".", "avg", ",", "\n", ")", "\n", ")", "\n", "logger", ".", "info", "(", "\"| Generate {} with beam={}\"", ".", "format", "(", "args", ".", "gen_subset", ",", "args", ".", "beam", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.infer.cli_main": [[235, 240], ["fairseq.options.get_generation_parser", "infer.add_asr_eval_argument", "fairseq.options.parse_args_and_arch", "infer.main"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.get_generation_parser", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.infer.add_asr_eval_argument", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.parse_args_and_arch", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.main"], ["", "def", "cli_main", "(", ")", ":", "\n", "    ", "parser", "=", "options", ".", "get_generation_parser", "(", ")", "\n", "parser", "=", "add_asr_eval_argument", "(", "parser", ")", "\n", "args", "=", "options", ".", "parse_args_and_arch", "(", "parser", ")", "\n", "main", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.test_collaters.TestSeq2SeqCollator.test_collate": [[17, 53], ["examples.speech_recognition.data.collaters.Seq2SeqCollater", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "examples.speech_recognition.data.collaters.Seq2SeqCollater.collate", "test_collaters.TestSeq2SeqCollator.assertTensorEqual", "test_collaters.TestSeq2SeqCollator.assertEqual", "test_collaters.TestSeq2SeqCollator.assertTensorEqual", "test_collaters.TestSeq2SeqCollator.assertTensorEqual", "test_collaters.TestSeq2SeqCollator.assertTensorEqual", "test_collaters.TestSeq2SeqCollator.assertTensorEqual", "test_collaters.TestSeq2SeqCollator.assertEqual", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.language_pair_dataset.collate", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.test_collaters.TestSeq2SeqCollator.assertTensorEqual", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.test_collaters.TestSeq2SeqCollator.assertTensorEqual", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.test_collaters.TestSeq2SeqCollator.assertTensorEqual", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.test_collaters.TestSeq2SeqCollator.assertTensorEqual", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.test_collaters.TestSeq2SeqCollator.assertTensorEqual"], ["    ", "def", "test_collate", "(", "self", ")", ":", "\n", "\n", "        ", "eos_idx", "=", "1", "\n", "pad_idx", "=", "0", "\n", "collater", "=", "Seq2SeqCollater", "(", "\n", "feature_index", "=", "0", ",", "label_index", "=", "1", ",", "pad_index", "=", "pad_idx", ",", "eos_index", "=", "eos_idx", "\n", ")", "\n", "\n", "# 2 frames in the first sample and 3 frames in the second one", "\n", "frames1", "=", "np", ".", "array", "(", "[", "[", "7", ",", "8", "]", ",", "[", "9", ",", "10", "]", "]", ")", "\n", "frames2", "=", "np", ".", "array", "(", "[", "[", "1", ",", "2", "]", ",", "[", "3", ",", "4", "]", ",", "[", "5", ",", "6", "]", "]", ")", "\n", "target1", "=", "np", ".", "array", "(", "[", "4", ",", "2", ",", "3", ",", "eos_idx", "]", ")", "\n", "target2", "=", "np", ".", "array", "(", "[", "3", ",", "2", ",", "eos_idx", "]", ")", "\n", "sample1", "=", "{", "\"id\"", ":", "0", ",", "\"data\"", ":", "[", "frames1", ",", "target1", "]", "}", "\n", "sample2", "=", "{", "\"id\"", ":", "1", ",", "\"data\"", ":", "[", "frames2", ",", "target2", "]", "}", "\n", "batch", "=", "collater", ".", "collate", "(", "[", "sample1", ",", "sample2", "]", ")", "\n", "\n", "# collate sort inputs by frame's length before creating the batch", "\n", "self", ".", "assertTensorEqual", "(", "batch", "[", "\"id\"", "]", ",", "torch", ".", "tensor", "(", "[", "1", ",", "0", "]", ")", ")", "\n", "self", ".", "assertEqual", "(", "batch", "[", "\"ntokens\"", "]", ",", "7", ")", "\n", "self", ".", "assertTensorEqual", "(", "\n", "batch", "[", "\"net_input\"", "]", "[", "\"src_tokens\"", "]", ",", "\n", "torch", ".", "tensor", "(", "\n", "[", "[", "[", "1", ",", "2", "]", ",", "[", "3", ",", "4", "]", ",", "[", "5", ",", "6", "]", "]", ",", "[", "[", "7", ",", "8", "]", ",", "[", "9", ",", "10", "]", ",", "[", "pad_idx", ",", "pad_idx", "]", "]", "]", "\n", ")", ",", "\n", ")", "\n", "self", ".", "assertTensorEqual", "(", "\n", "batch", "[", "\"net_input\"", "]", "[", "\"prev_output_tokens\"", "]", ",", "\n", "torch", ".", "tensor", "(", "[", "[", "eos_idx", ",", "3", ",", "2", ",", "pad_idx", "]", ",", "[", "eos_idx", ",", "4", ",", "2", ",", "3", "]", "]", ")", ",", "\n", ")", "\n", "self", ".", "assertTensorEqual", "(", "batch", "[", "\"net_input\"", "]", "[", "\"src_lengths\"", "]", ",", "torch", ".", "tensor", "(", "[", "3", ",", "2", "]", ")", ")", "\n", "self", ".", "assertTensorEqual", "(", "\n", "batch", "[", "\"target\"", "]", ",", "\n", "torch", ".", "tensor", "(", "[", "[", "3", ",", "2", ",", "eos_idx", ",", "pad_idx", "]", ",", "[", "4", ",", "2", ",", "3", ",", "eos_idx", "]", "]", ")", ",", "\n", ")", "\n", "self", ".", "assertEqual", "(", "batch", "[", "\"nsentences\"", "]", ",", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.test_collaters.TestSeq2SeqCollator.assertTensorEqual": [[54, 57], ["test_collaters.TestSeq2SeqCollator.assertEqual", "test_collaters.TestSeq2SeqCollator.assertEqual", "t1.size", "t2.size", "t1.ne().long().sum", "t1.ne().long", "t1.ne"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "assertTensorEqual", "(", "self", ",", "t1", ",", "t2", ")", ":", "\n", "        ", "self", ".", "assertEqual", "(", "t1", ".", "size", "(", ")", ",", "t2", ".", "size", "(", ")", ",", "\"size mismatch\"", ")", "\n", "self", ".", "assertEqual", "(", "t1", ".", "ne", "(", "t2", ")", ".", "long", "(", ")", ".", "sum", "(", ")", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.test_cross_entropy.CrossEntropyWithAccCriterionTest.setUp": [[14, 17], ["super().setUp"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.test_vggtransformer.TransformerDecoderTest.setUp"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "self", ".", "criterion_cls", "=", "CrossEntropyWithAccCriterion", "\n", "super", "(", ")", ".", "setUp", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.test_cross_entropy.CrossEntropyWithAccCriterionTest.test_cross_entropy_all_correct": [[18, 27], ["test_cross_entropy.CrossEntropyWithAccCriterionTest.get_test_sample", "test_cross_entropy.CrossEntropyWithAccCriterionTest.criterion"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.CrossEntropyCriterionTestBase.get_test_sample"], ["", "def", "test_cross_entropy_all_correct", "(", "self", ")", ":", "\n", "        ", "sample", "=", "self", ".", "get_test_sample", "(", "correct", "=", "True", ",", "soft_target", "=", "False", ",", "aggregate", "=", "False", ")", "\n", "loss", ",", "sample_size", ",", "logging_output", "=", "self", ".", "criterion", "(", "\n", "self", ".", "model", ",", "sample", ",", "\"sum\"", ",", "log_probs", "=", "True", "\n", ")", "\n", "assert", "logging_output", "[", "\"correct\"", "]", "==", "20", "\n", "assert", "logging_output", "[", "\"total\"", "]", "==", "20", "\n", "assert", "logging_output", "[", "\"sample_size\"", "]", "==", "20", "\n", "assert", "logging_output", "[", "\"ntokens\"", "]", "==", "20", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.test_cross_entropy.CrossEntropyWithAccCriterionTest.test_cross_entropy_all_wrong": [[28, 37], ["test_cross_entropy.CrossEntropyWithAccCriterionTest.get_test_sample", "test_cross_entropy.CrossEntropyWithAccCriterionTest.criterion"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.CrossEntropyCriterionTestBase.get_test_sample"], ["", "def", "test_cross_entropy_all_wrong", "(", "self", ")", ":", "\n", "        ", "sample", "=", "self", ".", "get_test_sample", "(", "correct", "=", "False", ",", "soft_target", "=", "False", ",", "aggregate", "=", "False", ")", "\n", "loss", ",", "sample_size", ",", "logging_output", "=", "self", ".", "criterion", "(", "\n", "self", ".", "model", ",", "sample", ",", "\"sum\"", ",", "log_probs", "=", "True", "\n", ")", "\n", "assert", "logging_output", "[", "\"correct\"", "]", "==", "0", "\n", "assert", "logging_output", "[", "\"total\"", "]", "==", "20", "\n", "assert", "logging_output", "[", "\"sample_size\"", "]", "==", "20", "\n", "assert", "logging_output", "[", "\"ntokens\"", "]", "==", "20", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.DummyTask.__init__": [[41, 47], ["fairseq.tasks.fairseq_task.FairseqTask.__init__", "asr_test_base.get_dummy_dictionary", "getattr", "asr_test_base.DummyTask.dictionary.add_symbol"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.get_dummy_dictionary", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ")", "\n", "self", ".", "dictionary", "=", "get_dummy_dictionary", "(", ")", "\n", "if", "getattr", "(", "self", ".", "args", ",", "\"ctc\"", ",", "False", ")", ":", "\n", "            ", "self", ".", "dictionary", ".", "add_symbol", "(", "\"<ctc_blank>\"", ")", "\n", "", "self", ".", "tgt_dict", "=", "self", ".", "dictionary", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.DummyTask.target_dictionary": [[48, 51], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "target_dictionary", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "dictionary", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.TestBaseFairseqModelBase.setUpClass": [[232, 237], ["super().setUpClass", "unittest.SkipTest"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.CrossEntropyCriterionTestBase.setUpClass"], ["@", "classmethod", "\n", "def", "setUpClass", "(", "cls", ")", ":", "\n", "        ", "if", "cls", "is", "TestBaseFairseqModelBase", ":", "\n", "            ", "raise", "unittest", ".", "SkipTest", "(", "\"Skipping test case in base\"", ")", "\n", "", "super", "(", ")", ".", "setUpClass", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.TestBaseFairseqModelBase.setUpModel": [[238, 241], ["asr_test_base.TestBaseFairseqModelBase.assertTrue", "isinstance"], "methods", ["None"], ["", "def", "setUpModel", "(", "self", ",", "model", ")", ":", "\n", "        ", "self", ".", "assertTrue", "(", "isinstance", "(", "model", ",", "BaseFairseqModel", ")", ")", "\n", "self", ".", "model", "=", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.TestBaseFairseqModelBase.setupInput": [[242, 244], ["None"], "methods", ["None"], ["", "def", "setupInput", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.TestBaseFairseqModelBase.setUp": [[245, 249], ["None"], "methods", ["None"], ["", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "self", ".", "model", "=", "None", "\n", "self", ".", "forward_input", "=", "None", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.TestFairseqEncoderDecoderModelBase.setUpClass": [[257, 262], ["asr_test_base.TestBaseFairseqModelBase.setUpClass", "unittest.SkipTest"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.CrossEntropyCriterionTestBase.setUpClass"], ["@", "classmethod", "\n", "def", "setUpClass", "(", "cls", ")", ":", "\n", "        ", "if", "cls", "is", "TestFairseqEncoderDecoderModelBase", ":", "\n", "            ", "raise", "unittest", ".", "SkipTest", "(", "\"Skipping test case in base\"", ")", "\n", "", "super", "(", ")", ".", "setUpClass", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.TestFairseqEncoderDecoderModelBase.setUpModel": [[263, 278], ["asr_test_base.TestFairseqEncoderDecoderModelBase.assertTrue", "asr_test_base.get_dummy_task_and_parser", "model_cls.add_args", "parser.parse_args", "model_cls.build_model", "issubclass", "args_setter"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.get_dummy_task_and_parser", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.add_args", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv_lm.LightConvLanguageModel.build_model"], ["", "def", "setUpModel", "(", "self", ",", "model_cls", ",", "extra_args_setters", "=", "None", ")", ":", "\n", "        ", "self", ".", "assertTrue", "(", "\n", "issubclass", "(", "model_cls", ",", "(", "FairseqEncoderDecoderModel", ",", "FairseqModel", ")", ")", ",", "\n", "msg", "=", "\"This class only tests for FairseqModel subclasses\"", ",", "\n", ")", "\n", "\n", "task", ",", "parser", "=", "get_dummy_task_and_parser", "(", ")", "\n", "model_cls", ".", "add_args", "(", "parser", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", "[", "]", ")", "\n", "if", "extra_args_setters", "is", "not", "None", ":", "\n", "            ", "for", "args_setter", "in", "extra_args_setters", ":", "\n", "                ", "args_setter", "(", "args", ")", "\n", "", "", "model", "=", "model_cls", ".", "build_model", "(", "args", ",", "task", ")", "\n", "self", ".", "model", "=", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.TestFairseqEncoderDecoderModelBase.setUpInput": [[279, 281], ["asr_test_base.get_dummy_input"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.get_dummy_input"], ["", "def", "setUpInput", "(", "self", ",", "input", "=", "None", ")", ":", "\n", "        ", "self", ".", "forward_input", "=", "get_dummy_input", "(", ")", "if", "input", "is", "None", "else", "input", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.TestFairseqEncoderDecoderModelBase.setUp": [[282, 284], ["asr_test_base.TestBaseFairseqModelBase.setUp"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.test_vggtransformer.TransformerDecoderTest.setUp"], ["", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "setUp", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.TestFairseqEncoderDecoderModelBase.test_forward": [[285, 294], ["asr_test_base.TestFairseqEncoderDecoderModelBase.model.forward", "asr_test_base.check_decoder_output", "asr_test_base.TestFairseqEncoderDecoderModelBase.assertTrue"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.model.AMRModel.forward", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.check_decoder_output"], ["", "def", "test_forward", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "model", "and", "self", ".", "forward_input", ":", "\n", "            ", "forward_output", "=", "self", ".", "model", ".", "forward", "(", "**", "self", ".", "forward_input", ")", "\n", "# for FairseqEncoderDecoderModel, forward returns a tuple of two", "\n", "# elements, the first one is a Torch.Tensor", "\n", "succ", ",", "msg", "=", "check_decoder_output", "(", "forward_output", ")", "\n", "if", "not", "succ", ":", "\n", "                ", "self", ".", "assertTrue", "(", "succ", ",", "msg", "=", "msg", ")", "\n", "", "self", ".", "forward_output", "=", "forward_output", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.TestFairseqEncoderDecoderModelBase.test_get_normalized_probs": [[295, 312], ["asr_test_base.TestFairseqEncoderDecoderModelBase.model.forward", "asr_test_base.TestFairseqEncoderDecoderModelBase.model.get_normalized_probs", "asr_test_base.TestFairseqEncoderDecoderModelBase.model.get_normalized_probs", "asr_test_base.TestFairseqEncoderDecoderModelBase.assertTrue", "asr_test_base.TestFairseqEncoderDecoderModelBase.assertTrue", "asr_test_base.TestFairseqEncoderDecoderModelBase.assertTrue", "asr_test_base.TestFairseqEncoderDecoderModelBase.assertTrue", "hasattr", "hasattr", "torch.is_tensor", "torch.is_tensor"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.model.AMRModel.forward", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_decoder.FairseqDecoder.get_normalized_probs", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_decoder.FairseqDecoder.get_normalized_probs"], ["", "", "def", "test_get_normalized_probs", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "model", "and", "self", ".", "forward_input", ":", "\n", "            ", "forward_output", "=", "self", ".", "model", ".", "forward", "(", "**", "self", ".", "forward_input", ")", "\n", "logprob", "=", "self", ".", "model", ".", "get_normalized_probs", "(", "forward_output", ",", "log_probs", "=", "True", ")", "\n", "prob", "=", "self", ".", "model", ".", "get_normalized_probs", "(", "forward_output", ",", "log_probs", "=", "False", ")", "\n", "\n", "# in order for different models/criterion to play with each other", "\n", "# we need to know whether the logprob or prob output is batch_first", "\n", "# or not. We assume an additional attribute will be attached to logprob", "\n", "# or prob. If you find your code failed here, simply override", "\n", "# FairseqModel.get_normalized_probs, see example at", "\n", "# https://fburl.com/batch_first_example", "\n", "self", ".", "assertTrue", "(", "hasattr", "(", "logprob", ",", "\"batch_first\"", ")", ")", "\n", "self", ".", "assertTrue", "(", "hasattr", "(", "prob", ",", "\"batch_first\"", ")", ")", "\n", "\n", "self", ".", "assertTrue", "(", "torch", ".", "is_tensor", "(", "logprob", ")", ")", "\n", "self", ".", "assertTrue", "(", "torch", ".", "is_tensor", "(", "prob", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.TestFairseqEncoderModelBase.setUpClass": [[319, 324], ["asr_test_base.TestBaseFairseqModelBase.setUpClass", "unittest.SkipTest"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.CrossEntropyCriterionTestBase.setUpClass"], ["@", "classmethod", "\n", "def", "setUpClass", "(", "cls", ")", ":", "\n", "        ", "if", "cls", "is", "TestFairseqEncoderModelBase", ":", "\n", "            ", "raise", "unittest", ".", "SkipTest", "(", "\"Skipping test case in base\"", ")", "\n", "", "super", "(", ")", ".", "setUpClass", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.TestFairseqEncoderModelBase.setUpModel": [[325, 339], ["asr_test_base.TestFairseqEncoderModelBase.assertTrue", "asr_test_base.get_dummy_task_and_parser", "model_cls.add_args", "parser.parse_args", "model_cls.build_model", "issubclass", "args_setter"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.get_dummy_task_and_parser", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.add_args", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv_lm.LightConvLanguageModel.build_model"], ["", "def", "setUpModel", "(", "self", ",", "model_cls", ",", "extra_args_setters", "=", "None", ")", ":", "\n", "        ", "self", ".", "assertTrue", "(", "\n", "issubclass", "(", "model_cls", ",", "FairseqEncoderModel", ")", ",", "\n", "msg", "=", "\"This class is only used for testing FairseqEncoderModel\"", ",", "\n", ")", "\n", "task", ",", "parser", "=", "get_dummy_task_and_parser", "(", ")", "\n", "model_cls", ".", "add_args", "(", "parser", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", "[", "]", ")", "\n", "if", "extra_args_setters", "is", "not", "None", ":", "\n", "            ", "for", "args_setter", "in", "extra_args_setters", ":", "\n", "                ", "args_setter", "(", "args", ")", "\n", "\n", "", "", "model", "=", "model_cls", ".", "build_model", "(", "args", ",", "task", ")", "\n", "self", ".", "model", "=", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.TestFairseqEncoderModelBase.setUpInput": [[340, 345], ["asr_test_base.TestFairseqEncoderModelBase.forward_input.pop", "asr_test_base.get_dummy_input"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.get_dummy_input"], ["", "def", "setUpInput", "(", "self", ",", "input", "=", "None", ")", ":", "\n", "        ", "self", ".", "forward_input", "=", "get_dummy_input", "(", ")", "if", "input", "is", "None", "else", "input", "\n", "# get_dummy_input() is originally for s2s, here we delete extra dict", "\n", "# items, so it can be used for EncoderModel / Encoder as well", "\n", "self", ".", "forward_input", ".", "pop", "(", "\"prev_output_tokens\"", ",", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.TestFairseqEncoderModelBase.setUp": [[346, 348], ["asr_test_base.TestBaseFairseqModelBase.setUp"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.test_vggtransformer.TransformerDecoderTest.setUp"], ["", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "setUp", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.TestFairseqEncoderModelBase.test_forward": [[349, 362], ["asr_test_base.TestFairseqEncoderModelBase.forward_input[].size", "asr_test_base.TestFairseqEncoderModelBase.model.forward", "asr_test_base.check_encoder_output", "asr_test_base.TestFairseqEncoderModelBase.assertTrue"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.model.AMRModel.forward", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.check_encoder_output"], ["", "def", "test_forward", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "forward_input", "and", "self", ".", "model", ":", "\n", "            ", "bsz", "=", "self", ".", "forward_input", "[", "\"src_tokens\"", "]", ".", "size", "(", "0", ")", "\n", "forward_output", "=", "self", ".", "model", ".", "forward", "(", "**", "self", ".", "forward_input", ")", "\n", "\n", "# we expect forward_output to be a dict with the following", "\n", "# key/value pairs:", "\n", "# - encoder_out: a Torch.Tensor", "\n", "# - encoder_padding_mask: a binary Torch.Tensor", "\n", "succ", ",", "msg", "=", "check_encoder_output", "(", "forward_output", ",", "batch_size", "=", "bsz", ")", "\n", "if", "not", "succ", ":", "\n", "                ", "self", ".", "assertTrue", "(", "succ", ",", "msg", "=", "msg", ")", "\n", "", "self", ".", "forward_output", "=", "forward_output", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.TestFairseqEncoderModelBase.test_get_normalized_probs": [[363, 380], ["asr_test_base.TestFairseqEncoderModelBase.model.forward", "asr_test_base.TestFairseqEncoderModelBase.model.get_normalized_probs", "asr_test_base.TestFairseqEncoderModelBase.model.get_normalized_probs", "asr_test_base.TestFairseqEncoderModelBase.assertTrue", "asr_test_base.TestFairseqEncoderModelBase.assertTrue", "asr_test_base.TestFairseqEncoderModelBase.assertTrue", "asr_test_base.TestFairseqEncoderModelBase.assertTrue", "hasattr", "hasattr", "torch.is_tensor", "torch.is_tensor"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.model.AMRModel.forward", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_decoder.FairseqDecoder.get_normalized_probs", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_decoder.FairseqDecoder.get_normalized_probs"], ["", "", "def", "test_get_normalized_probs", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "model", "and", "self", ".", "forward_input", ":", "\n", "            ", "forward_output", "=", "self", ".", "model", ".", "forward", "(", "**", "self", ".", "forward_input", ")", "\n", "logprob", "=", "self", ".", "model", ".", "get_normalized_probs", "(", "forward_output", ",", "log_probs", "=", "True", ")", "\n", "prob", "=", "self", ".", "model", ".", "get_normalized_probs", "(", "forward_output", ",", "log_probs", "=", "False", ")", "\n", "\n", "# in order for different models/criterion to play with each other", "\n", "# we need to know whether the logprob or prob output is batch_first", "\n", "# or not. We assume an additional attribute will be attached to logprob", "\n", "# or prob. If you find your code failed here, simply override", "\n", "# FairseqModel.get_normalized_probs, see example at", "\n", "# https://fburl.com/batch_first_example", "\n", "self", ".", "assertTrue", "(", "hasattr", "(", "logprob", ",", "\"batch_first\"", ")", ")", "\n", "self", ".", "assertTrue", "(", "hasattr", "(", "prob", ",", "\"batch_first\"", ")", ")", "\n", "\n", "self", ".", "assertTrue", "(", "torch", ".", "is_tensor", "(", "logprob", ")", ")", "\n", "self", ".", "assertTrue", "(", "torch", ".", "is_tensor", "(", "prob", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.TestFairseqEncoderBase.setUpClass": [[387, 392], ["super().setUpClass", "unittest.SkipTest"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.CrossEntropyCriterionTestBase.setUpClass"], ["@", "classmethod", "\n", "def", "setUpClass", "(", "cls", ")", ":", "\n", "        ", "if", "cls", "is", "TestFairseqEncoderBase", ":", "\n", "            ", "raise", "unittest", ".", "SkipTest", "(", "\"Skipping test case in base\"", ")", "\n", "", "super", "(", ")", ".", "setUpClass", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.TestFairseqEncoderBase.setUpEncoder": [[393, 399], ["asr_test_base.TestFairseqEncoderBase.assertTrue", "isinstance"], "methods", ["None"], ["", "def", "setUpEncoder", "(", "self", ",", "encoder", ")", ":", "\n", "        ", "self", ".", "assertTrue", "(", "\n", "isinstance", "(", "encoder", ",", "FairseqEncoder", ")", ",", "\n", "msg", "=", "\"This class is only used for test FairseqEncoder\"", ",", "\n", ")", "\n", "self", ".", "encoder", "=", "encoder", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.TestFairseqEncoderBase.setUpInput": [[400, 405], ["asr_test_base.TestFairseqEncoderBase.forward_input.pop", "asr_test_base.get_dummy_input"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.get_dummy_input"], ["", "def", "setUpInput", "(", "self", ",", "input", "=", "None", ")", ":", "\n", "        ", "self", ".", "forward_input", "=", "get_dummy_input", "(", ")", "if", "input", "is", "None", "else", "input", "\n", "# get_dummy_input() is originally for s2s, here we delete extra dict", "\n", "# items, so it can be used for EncoderModel / Encoder as well", "\n", "self", ".", "forward_input", ".", "pop", "(", "\"prev_output_tokens\"", ",", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.TestFairseqEncoderBase.setUp": [[406, 409], ["None"], "methods", ["None"], ["", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "self", ".", "encoder", "=", "None", "\n", "self", ".", "forward_input", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.TestFairseqEncoderBase.test_forward": [[410, 419], ["asr_test_base.TestFairseqEncoderBase.forward_input[].size", "asr_test_base.TestFairseqEncoderBase.encoder.forward", "asr_test_base.check_encoder_output", "asr_test_base.TestFairseqEncoderBase.assertTrue"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.model.AMRModel.forward", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.check_encoder_output"], ["", "def", "test_forward", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "encoder", "and", "self", ".", "forward_input", ":", "\n", "            ", "bsz", "=", "self", ".", "forward_input", "[", "\"src_tokens\"", "]", ".", "size", "(", "0", ")", "\n", "\n", "forward_output", "=", "self", ".", "encoder", ".", "forward", "(", "**", "self", ".", "forward_input", ")", "\n", "succ", ",", "msg", "=", "check_encoder_output", "(", "forward_output", ",", "batch_size", "=", "bsz", ")", "\n", "if", "not", "succ", ":", "\n", "                ", "self", ".", "assertTrue", "(", "succ", ",", "msg", "=", "msg", ")", "\n", "", "self", ".", "forward_output", "=", "forward_output", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.TestFairseqDecoderBase.setUpClass": [[426, 431], ["super().setUpClass", "unittest.SkipTest"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.CrossEntropyCriterionTestBase.setUpClass"], ["@", "classmethod", "\n", "def", "setUpClass", "(", "cls", ")", ":", "\n", "        ", "if", "cls", "is", "TestFairseqDecoderBase", ":", "\n", "            ", "raise", "unittest", ".", "SkipTest", "(", "\"Skipping test case in base\"", ")", "\n", "", "super", "(", ")", ".", "setUpClass", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.TestFairseqDecoderBase.setUpDecoder": [[432, 438], ["asr_test_base.TestFairseqDecoderBase.assertTrue", "isinstance"], "methods", ["None"], ["", "def", "setUpDecoder", "(", "self", ",", "decoder", ")", ":", "\n", "        ", "self", ".", "assertTrue", "(", "\n", "isinstance", "(", "decoder", ",", "FairseqDecoder", ")", ",", "\n", "msg", "=", "\"This class is only used for test FairseqDecoder\"", ",", "\n", ")", "\n", "self", ".", "decoder", "=", "decoder", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.TestFairseqDecoderBase.setUpInput": [[439, 441], ["asr_test_base.get_dummy_encoder_output"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.get_dummy_encoder_output"], ["", "def", "setUpInput", "(", "self", ",", "input", "=", "None", ")", ":", "\n", "        ", "self", ".", "forward_input", "=", "get_dummy_encoder_output", "(", ")", "if", "input", "is", "None", "else", "input", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.TestFairseqDecoderBase.setUpPrevOutputTokens": [[442, 448], ["asr_test_base.get_dummy_input"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.get_dummy_input"], ["", "def", "setUpPrevOutputTokens", "(", "self", ",", "tokens", "=", "None", ")", ":", "\n", "        ", "if", "tokens", "is", "None", ":", "\n", "            ", "self", ".", "encoder_input", "=", "get_dummy_input", "(", ")", "\n", "self", ".", "prev_output_tokens", "=", "self", ".", "encoder_input", "[", "\"prev_output_tokens\"", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "prev_output_tokens", "=", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.TestFairseqDecoderBase.setUp": [[449, 453], ["None"], "methods", ["None"], ["", "", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "self", ".", "decoder", "=", "None", "\n", "self", ".", "forward_input", "=", "None", "\n", "self", ".", "prev_output_tokens", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.TestFairseqDecoderBase.test_forward": [[454, 468], ["asr_test_base.TestFairseqDecoderBase.decoder.forward", "asr_test_base.check_decoder_output", "asr_test_base.TestFairseqDecoderBase.assertTrue"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.model.AMRModel.forward", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.check_decoder_output"], ["", "def", "test_forward", "(", "self", ")", ":", "\n", "        ", "if", "(", "\n", "self", ".", "decoder", "is", "not", "None", "\n", "and", "self", ".", "forward_input", "is", "not", "None", "\n", "and", "self", ".", "prev_output_tokens", "is", "not", "None", "\n", ")", ":", "\n", "            ", "forward_output", "=", "self", ".", "decoder", ".", "forward", "(", "\n", "prev_output_tokens", "=", "self", ".", "prev_output_tokens", ",", "\n", "encoder_out", "=", "self", ".", "forward_input", ",", "\n", ")", "\n", "succ", ",", "msg", "=", "check_decoder_output", "(", "forward_output", ")", "\n", "if", "not", "succ", ":", "\n", "                ", "self", ".", "assertTrue", "(", "succ", ",", "msg", "=", "msg", ")", "\n", "", "self", ".", "forward_input", "=", "forward_output", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.DummyEncoderModel.__init__": [[471, 473], ["fairseq.models.FairseqEncoderModel.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["    ", "def", "__init__", "(", "self", ",", "encoder", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "encoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.DummyEncoderModel.build_model": [[474, 477], ["cls", "asr_test_base.DummyEncoder"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.masked_lm_dictionary.BertDictionary.cls"], ["", "@", "classmethod", "\n", "def", "build_model", "(", "cls", ",", "args", ",", "task", ")", ":", "\n", "        ", "return", "cls", "(", "DummyEncoder", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.DummyEncoderModel.get_logits": [[478, 483], ["torch.log", "torch.div"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.log"], ["", "def", "get_logits", "(", "self", ",", "net_output", ")", ":", "\n", "# Inverse of sigmoid to use with BinaryCrossEntropyWithLogitsCriterion as", "\n", "# F.binary_cross_entropy_with_logits combines sigmoid and CE", "\n", "        ", "return", "torch", ".", "log", "(", "\n", "torch", ".", "div", "(", "net_output", "[", "\"encoder_out\"", "]", ",", "1", "-", "net_output", "[", "\"encoder_out\"", "]", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.DummyEncoder.__init__": [[487, 489], ["fairseq.models.FairseqEncoder.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.DummyEncoder.forward": [[490, 493], ["examples.speech_recognition.data.data_utils.lengths_to_encoder_padding_mask"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.data_utils.lengths_to_encoder_padding_mask"], ["", "def", "forward", "(", "self", ",", "src_tokens", ",", "src_lengths", ")", ":", "\n", "        ", "mask", ",", "max_len", "=", "lengths_to_encoder_padding_mask", "(", "src_lengths", ")", "\n", "return", "{", "\"encoder_out\"", ":", "src_tokens", ",", "\"encoder_padding_mask\"", ":", "mask", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.CrossEntropyCriterionTestBase.setUpClass": [[496, 501], ["super().setUpClass", "unittest.SkipTest"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.CrossEntropyCriterionTestBase.setUpClass"], ["    ", "@", "classmethod", "\n", "def", "setUpClass", "(", "cls", ")", ":", "\n", "        ", "if", "cls", "is", "CrossEntropyCriterionTestBase", ":", "\n", "            ", "raise", "unittest", ".", "SkipTest", "(", "\"Skipping base class test case\"", ")", "\n", "", "super", "(", ")", ".", "setUpClass", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.CrossEntropyCriterionTestBase.setUpArgs": [[502, 507], ["argparse.Namespace"], "methods", ["None"], ["", "def", "setUpArgs", "(", "self", ")", ":", "\n", "        ", "args", "=", "argparse", ".", "Namespace", "(", ")", "\n", "args", ".", "sentence_avg", "=", "False", "\n", "args", ".", "threshold", "=", "0.1", "# to use with BinaryCrossEntropyWithLogitsCriterion", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.CrossEntropyCriterionTestBase.setUp": [[508, 512], ["asr_test_base.CrossEntropyCriterionTestBase.setUpArgs", "asr_test_base.DummyEncoderModel", "asr_test_base.CrossEntropyCriterionTestBase.criterion_cls", "asr_test_base.DummyEncoder", "asr_test_base.DummyTask"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.CrossEntropyCriterionTestBase.setUpArgs"], ["", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "args", "=", "self", ".", "setUpArgs", "(", ")", "\n", "self", ".", "model", "=", "DummyEncoderModel", "(", "encoder", "=", "DummyEncoder", "(", ")", ")", "\n", "self", ".", "criterion", "=", "self", ".", "criterion_cls", "(", "args", "=", "args", ",", "task", "=", "DummyTask", "(", "args", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.CrossEntropyCriterionTestBase.get_src_tokens": [[513, 531], ["torch.zeros", "range", "torch.zeros", "range", "range"], "methods", ["None"], ["", "def", "get_src_tokens", "(", "self", ",", "correct_prediction", ",", "aggregate", ")", ":", "\n", "        ", "\"\"\"\n            correct_prediction: True if the net_output (src_tokens) should\n            predict the correct target\n            aggregate: True if the criterion expects net_output (src_tokens)\n            aggregated across time axis\n        \"\"\"", "\n", "predicted_idx", "=", "0", "if", "correct_prediction", "else", "1", "\n", "if", "aggregate", ":", "\n", "            ", "src_tokens", "=", "torch", ".", "zeros", "(", "(", "2", ",", "2", ")", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "for", "b", "in", "range", "(", "2", ")", ":", "\n", "                ", "src_tokens", "[", "b", "]", "[", "predicted_idx", "]", "=", "1.0", "\n", "", "", "else", ":", "\n", "            ", "src_tokens", "=", "torch", ".", "zeros", "(", "(", "2", ",", "10", ",", "2", ")", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "for", "b", "in", "range", "(", "2", ")", ":", "\n", "                ", "for", "t", "in", "range", "(", "10", ")", ":", "\n", "                    ", "src_tokens", "[", "b", "]", "[", "t", "]", "[", "predicted_idx", "]", "=", "1.0", "\n", "", "", "", "return", "src_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.CrossEntropyCriterionTestBase.get_target": [[532, 540], ["torch.zeros", "range", "torch.zeros"], "methods", ["None"], ["", "def", "get_target", "(", "self", ",", "soft_target", ")", ":", "\n", "        ", "if", "soft_target", ":", "\n", "            ", "target", "=", "torch", ".", "zeros", "(", "(", "2", ",", "2", ")", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "for", "b", "in", "range", "(", "2", ")", ":", "\n", "                ", "target", "[", "b", "]", "[", "0", "]", "=", "1.0", "\n", "", "", "else", ":", "\n", "            ", "target", "=", "torch", ".", "zeros", "(", "(", "2", ",", "10", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "", "return", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.CrossEntropyCriterionTestBase.get_test_sample": [[541, 549], ["asr_test_base.CrossEntropyCriterionTestBase.get_src_tokens", "asr_test_base.CrossEntropyCriterionTestBase.get_target", "asr_test_base.CrossEntropyCriterionTestBase.size", "torch.tensor", "asr_test_base.CrossEntropyCriterionTestBase.size", "asr_test_base.CrossEntropyCriterionTestBase.size"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.CrossEntropyCriterionTestBase.get_src_tokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.CrossEntropyCriterionTestBase.get_target", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "get_test_sample", "(", "self", ",", "correct", ",", "soft_target", ",", "aggregate", ")", ":", "\n", "        ", "src_tokens", "=", "self", ".", "get_src_tokens", "(", "correct", ",", "aggregate", ")", "\n", "target", "=", "self", ".", "get_target", "(", "soft_target", ")", "\n", "L", "=", "src_tokens", ".", "size", "(", "1", ")", "\n", "return", "{", "\n", "\"net_input\"", ":", "{", "\"src_tokens\"", ":", "src_tokens", ",", "\"src_lengths\"", ":", "torch", ".", "tensor", "(", "[", "L", "]", ")", "}", ",", "\n", "\"target\"", ":", "target", ",", "\n", "\"ntokens\"", ":", "src_tokens", ".", "size", "(", "0", ")", "*", "src_tokens", ".", "size", "(", "1", ")", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.get_dummy_dictionary": [[32, 38], ["fairseq.data.dictionary.Dictionary", "enumerate", "range", "fairseq.data.dictionary.Dictionary.add_symbol"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol"], ["def", "get_dummy_dictionary", "(", "vocab_size", "=", "DEFAULT_TEST_VOCAB_SIZE", ")", ":", "\n", "    ", "dummy_dict", "=", "Dictionary", "(", ")", "\n", "# add dummy symbol to satisfy vocab size", "\n", "for", "id", ",", "_", "in", "enumerate", "(", "range", "(", "vocab_size", ")", ")", ":", "\n", "        ", "dummy_dict", ".", "add_symbol", "(", "\"{}\"", ".", "format", "(", "id", ")", ",", "1000", ")", "\n", "", "return", "dummy_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.get_dummy_task_and_parser": [[53, 68], ["argparse.ArgumentParser", "DummyTask.add_args", "argparse.ArgumentParser.parse_args", "DummyTask.setup_task"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.add_args", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.sentence_prediction.SentencePredictionTask.setup_task"], ["", "", "def", "get_dummy_task_and_parser", "(", ")", ":", "\n", "    ", "\"\"\"\n    to build a fariseq model, we need some dummy parse and task. This function\n    is used to create dummy task and parser to faciliate model/criterion test\n\n    Note: we use FbSpeechRecognitionTask as the dummy task. You may want\n    to use other task by providing another function\n    \"\"\"", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "description", "=", "\"test_dummy_s2s_task\"", ",", "argument_default", "=", "argparse", ".", "SUPPRESS", "\n", ")", "\n", "DummyTask", ".", "add_args", "(", "parser", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", "[", "]", ")", "\n", "task", "=", "DummyTask", ".", "setup_task", "(", "args", ")", "\n", "return", "task", ",", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.get_dummy_input": [[70, 102], ["torch.randn", "torch.from_numpy", "range", "fairseq.data.data_utils.collate_tokens", "torch.from_numpy.sort", "torch.randn.index_select", "numpy.random.randint().astype", "numpy.random.randint", "numpy.random.randint", "fairseq_data_utils.collate_tokens.append", "torch.from_numpy", "numpy.random.randint", "src_lengths[].item"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.data_utils.collate_tokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item"], ["", "def", "get_dummy_input", "(", "T", "=", "100", ",", "D", "=", "80", ",", "B", "=", "5", ",", "K", "=", "100", ")", ":", "\n", "    ", "forward_input", "=", "{", "}", "\n", "# T max sequence length", "\n", "# D feature vector dimension", "\n", "# B batch size", "\n", "# K target dimension size", "\n", "feature", "=", "torch", ".", "randn", "(", "B", ",", "T", ",", "D", ")", "\n", "# this (B, T, D) layout is just a convention, you can override it by", "\n", "# write your own _prepare_forward_input function", "\n", "src_lengths", "=", "torch", ".", "from_numpy", "(", "\n", "np", ".", "random", ".", "randint", "(", "low", "=", "1", ",", "high", "=", "T", ",", "size", "=", "B", ")", ".", "astype", "(", "np", ".", "int64", ")", "\n", ")", "\n", "src_lengths", "[", "0", "]", "=", "T", "# make sure the maximum length matches", "\n", "prev_output_tokens", "=", "[", "]", "\n", "for", "b", "in", "range", "(", "B", ")", ":", "\n", "        ", "token_length", "=", "np", ".", "random", ".", "randint", "(", "low", "=", "1", ",", "high", "=", "src_lengths", "[", "b", "]", ".", "item", "(", ")", "+", "1", ")", "\n", "tokens", "=", "np", ".", "random", ".", "randint", "(", "low", "=", "0", ",", "high", "=", "K", ",", "size", "=", "token_length", ")", "\n", "prev_output_tokens", ".", "append", "(", "torch", ".", "from_numpy", "(", "tokens", ")", ")", "\n", "\n", "", "prev_output_tokens", "=", "fairseq_data_utils", ".", "collate_tokens", "(", "\n", "prev_output_tokens", ",", "\n", "pad_idx", "=", "1", ",", "\n", "eos_idx", "=", "2", ",", "\n", "left_pad", "=", "False", ",", "\n", "move_eos_to_beginning", "=", "False", ",", "\n", ")", "\n", "src_lengths", ",", "sorted_order", "=", "src_lengths", ".", "sort", "(", "descending", "=", "True", ")", "\n", "forward_input", "[", "\"src_tokens\"", "]", "=", "feature", ".", "index_select", "(", "0", ",", "sorted_order", ")", "\n", "forward_input", "[", "\"src_lengths\"", "]", "=", "src_lengths", "\n", "forward_input", "[", "\"prev_output_tokens\"", "]", "=", "prev_output_tokens", "\n", "\n", "return", "forward_input", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.get_dummy_encoder_output": [[104, 124], ["torch.from_numpy", "torch.from_numpy", "encoder_out[].t_", "numpy.random.randn().astype", "numpy.random.randint", "torch.arange().view().expand", "torch.from_numpy.view().expand", "numpy.random.randn", "torch.arange().view", "torch.from_numpy.view", "torch.arange"], "function", ["None"], ["", "def", "get_dummy_encoder_output", "(", "encoder_out_shape", "=", "(", "100", ",", "80", ",", "5", ")", ")", ":", "\n", "    ", "\"\"\"\n    This only provides an example to generate dummy encoder output\n    \"\"\"", "\n", "(", "T", ",", "B", ",", "D", ")", "=", "encoder_out_shape", "\n", "encoder_out", "=", "{", "}", "\n", "\n", "encoder_out", "[", "\"encoder_out\"", "]", "=", "torch", ".", "from_numpy", "(", "\n", "np", ".", "random", ".", "randn", "(", "*", "encoder_out_shape", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", ")", "\n", "seq_lengths", "=", "torch", ".", "from_numpy", "(", "np", ".", "random", ".", "randint", "(", "low", "=", "1", ",", "high", "=", "T", ",", "size", "=", "B", ")", ")", "\n", "# some dummy mask", "\n", "encoder_out", "[", "\"encoder_padding_mask\"", "]", "=", "torch", ".", "arange", "(", "T", ")", ".", "view", "(", "1", ",", "T", ")", ".", "expand", "(", "\n", "B", ",", "-", "1", "\n", ")", ">=", "seq_lengths", ".", "view", "(", "B", ",", "1", ")", ".", "expand", "(", "-", "1", ",", "T", ")", "\n", "encoder_out", "[", "\"encoder_padding_mask\"", "]", ".", "t_", "(", ")", "\n", "\n", "# encoer_padding_mask is (T, B) tensor, with (t, b)-th element indicate", "\n", "# whether encoder_out[t, b] is valid (=0) or not (=1)", "\n", "return", "encoder_out", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base._current_postion_info": [[126, 132], ["inspect.currentframe", "os.path.basename", "inspect.getframeinfo"], "function", ["None"], ["", "def", "_current_postion_info", "(", ")", ":", "\n", "    ", "cf", "=", "currentframe", "(", ")", "\n", "frameinfo", "=", "\" (at {}:{})\"", ".", "format", "(", "\n", "os", ".", "path", ".", "basename", "(", "getframeinfo", "(", "cf", ")", ".", "filename", ")", ",", "cf", ".", "f_back", ".", "f_lineno", "\n", ")", "\n", "return", "frameinfo", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.check_encoder_output": [[134, 197], ["isinstance", "isinstance", "asr_test_base._current_postion_info", "asr_test_base._current_postion_info", "asr_test_base._current_postion_info", "asr_test_base._current_postion_info", "asr_test_base._current_postion_info", "isinstance", "mask.dim", "asr_test_base._current_postion_info", "asr_test_base._current_postion_info", "asr_test_base._current_postion_info", "mask.size", "asr_test_base._current_postion_info"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base._current_postion_info", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base._current_postion_info", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base._current_postion_info", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base._current_postion_info", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base._current_postion_info", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base._current_postion_info", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base._current_postion_info", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base._current_postion_info", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base._current_postion_info"], ["", "def", "check_encoder_output", "(", "encoder_output", ",", "batch_size", "=", "None", ")", ":", "\n", "    ", "\"\"\"we expect encoder_output to be a dict with the following\n    key/value pairs:\n    - encoder_out: a Torch.Tensor\n    - encoder_padding_mask: a binary Torch.Tensor\n    \"\"\"", "\n", "if", "not", "isinstance", "(", "encoder_output", ",", "dict", ")", ":", "\n", "        ", "msg", "=", "(", "\n", "\"FairseqEncoderModel.forward(...) must be a dict\"", "+", "_current_postion_info", "(", ")", "\n", ")", "\n", "return", "False", ",", "msg", "\n", "\n", "", "if", "\"encoder_out\"", "not", "in", "encoder_output", ":", "\n", "        ", "msg", "=", "(", "\n", "\"FairseqEncoderModel.forward(...) must contain encoder_out\"", "\n", "+", "_current_postion_info", "(", ")", "\n", ")", "\n", "return", "False", ",", "msg", "\n", "\n", "", "if", "\"encoder_padding_mask\"", "not", "in", "encoder_output", ":", "\n", "        ", "msg", "=", "(", "\n", "\"FairseqEncoderModel.forward(...) must contain encoder_padding_mask\"", "\n", "+", "_current_postion_info", "(", ")", "\n", ")", "\n", "return", "False", ",", "msg", "\n", "\n", "", "if", "not", "isinstance", "(", "encoder_output", "[", "\"encoder_out\"", "]", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "msg", "=", "\"encoder_out must be a torch.Tensor\"", "+", "_current_postion_info", "(", ")", "\n", "return", "False", ",", "msg", "\n", "\n", "", "if", "encoder_output", "[", "\"encoder_out\"", "]", ".", "dtype", "!=", "torch", ".", "float32", ":", "\n", "        ", "msg", "=", "\"encoder_out must have float32 dtype\"", "+", "_current_postion_info", "(", ")", "\n", "return", "False", ",", "msg", "\n", "\n", "", "mask", "=", "encoder_output", "[", "\"encoder_padding_mask\"", "]", "\n", "if", "mask", "is", "not", "None", ":", "\n", "        ", "if", "not", "isinstance", "(", "mask", ",", "torch", ".", "Tensor", ")", ":", "\n", "            ", "msg", "=", "(", "\n", "\"encoder_padding_mask must be a torch.Tensor\"", "+", "_current_postion_info", "(", ")", "\n", ")", "\n", "return", "False", ",", "msg", "\n", "", "if", "mask", ".", "dtype", "!=", "torch", ".", "uint8", ":", "\n", "            ", "msg", "=", "(", "\n", "\"encoder_padding_mask must have dtype of uint8\"", "\n", "+", "_current_postion_info", "(", ")", "\n", ")", "\n", "return", "False", ",", "msg", "\n", "\n", "", "if", "mask", ".", "dim", "(", ")", "!=", "2", ":", "\n", "            ", "msg", "=", "(", "\n", "\"we expect encoder_padding_mask to be a 2-d tensor, in shape (T, B)\"", "\n", "+", "_current_postion_info", "(", ")", "\n", ")", "\n", "return", "False", ",", "msg", "\n", "\n", "", "if", "batch_size", "is", "not", "None", "and", "mask", ".", "size", "(", "1", ")", "!=", "batch_size", ":", "\n", "            ", "msg", "=", "(", "\n", "\"we expect encoder_padding_mask to be a 2-d tensor, with size(1)\"", "\n", "+", "\" being the batch size\"", "\n", "+", "_current_postion_info", "(", ")", "\n", ")", "\n", "return", "False", ",", "msg", "\n", "", "", "return", "True", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.check_decoder_output": [[199, 219], ["isinstance", "len", "isinstance", "asr_test_base._current_postion_info", "asr_test_base._current_postion_info", "asr_test_base._current_postion_info"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base._current_postion_info", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base._current_postion_info", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base._current_postion_info"], ["", "def", "check_decoder_output", "(", "decoder_output", ")", ":", "\n", "    ", "\"\"\"we expect output from a decoder is a tuple with the following constraint:\n    - the first element is a torch.Tensor\n    - the second element can be anything (reserved for future use)\n    \"\"\"", "\n", "if", "not", "isinstance", "(", "decoder_output", ",", "tuple", ")", ":", "\n", "        ", "msg", "=", "\"FariseqDecoder output must be a tuple\"", "+", "_current_postion_info", "(", ")", "\n", "return", "False", ",", "msg", "\n", "\n", "", "if", "len", "(", "decoder_output", ")", "!=", "2", ":", "\n", "        ", "msg", "=", "\"FairseqDecoder output must be 2-elem tuple\"", "+", "_current_postion_info", "(", ")", "\n", "return", "False", ",", "msg", "\n", "\n", "", "if", "not", "isinstance", "(", "decoder_output", "[", "0", "]", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "msg", "=", "(", "\n", "\"FariseqDecoder output[0] must be a torch.Tensor\"", "+", "_current_postion_info", "(", ")", "\n", ")", "\n", "return", "False", ",", "msg", "\n", "\n", "", "return", "True", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.test_vggtransformer.VGGTransformerModelTest_mid.setUp": [[26, 42], ["super().setUp", "test_vggtransformer.VGGTransformerModelTest_mid.setUpModel", "test_vggtransformer.VGGTransformerModelTest_mid.setUpInput", "asr_test_base.get_dummy_input"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.test_vggtransformer.TransformerDecoderTest.setUp", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.TestFairseqEncoderModelBase.setUpModel", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.TestFairseqDecoderBase.setUpInput", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.get_dummy_input"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "def", "override_config", "(", "args", ")", ":", "\n", "            ", "\"\"\"\n            vggtrasformer_1 use 14 layers of transformer,\n            for testing purpose, it is too expensive. For fast turn-around\n            test, reduce the number of layers to 3.\n            \"\"\"", "\n", "args", ".", "transformer_enc_config", "=", "(", "\n", "\"((1024, 16, 4096, True, 0.15, 0.15, 0.15),) * 3\"", "\n", ")", "\n", "\n", "", "super", "(", ")", ".", "setUp", "(", ")", "\n", "extra_args_setter", "=", "[", "vggtransformer_1", ",", "override_config", "]", "\n", "\n", "self", ".", "setUpModel", "(", "VGGTransformerModel", ",", "extra_args_setter", ")", "\n", "self", ".", "setUpInput", "(", "get_dummy_input", "(", "T", "=", "50", ",", "D", "=", "80", ",", "B", "=", "5", ",", "K", "=", "DEFAULT_TEST_VOCAB_SIZE", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.test_vggtransformer.VGGTransformerModelTest_big.setUp": [[45, 61], ["super().setUp", "test_vggtransformer.VGGTransformerModelTest_big.setUpModel", "test_vggtransformer.VGGTransformerModelTest_big.setUpInput", "asr_test_base.get_dummy_input"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.test_vggtransformer.TransformerDecoderTest.setUp", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.TestFairseqEncoderModelBase.setUpModel", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.TestFairseqDecoderBase.setUpInput", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.get_dummy_input"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "def", "override_config", "(", "args", ")", ":", "\n", "            ", "\"\"\"\n            vggtrasformer_2 use 16 layers of transformer,\n            for testing purpose, it is too expensive. For fast turn-around\n            test, reduce the number of layers to 3.\n            \"\"\"", "\n", "args", ".", "transformer_enc_config", "=", "(", "\n", "\"((1024, 16, 4096, True, 0.15, 0.15, 0.15),) * 3\"", "\n", ")", "\n", "\n", "", "super", "(", ")", ".", "setUp", "(", ")", "\n", "extra_args_setter", "=", "[", "vggtransformer_2", ",", "override_config", "]", "\n", "\n", "self", ".", "setUpModel", "(", "VGGTransformerModel", ",", "extra_args_setter", ")", "\n", "self", ".", "setUpInput", "(", "get_dummy_input", "(", "T", "=", "50", ",", "D", "=", "80", ",", "B", "=", "5", ",", "K", "=", "DEFAULT_TEST_VOCAB_SIZE", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.test_vggtransformer.VGGTransformerModelTest_base.setUp": [[64, 80], ["super().setUp", "test_vggtransformer.VGGTransformerModelTest_base.setUpModel", "test_vggtransformer.VGGTransformerModelTest_base.setUpInput", "asr_test_base.get_dummy_input"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.test_vggtransformer.TransformerDecoderTest.setUp", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.TestFairseqEncoderModelBase.setUpModel", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.TestFairseqDecoderBase.setUpInput", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.get_dummy_input"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "def", "override_config", "(", "args", ")", ":", "\n", "            ", "\"\"\"\n            vggtrasformer_base use 12 layers of transformer,\n            for testing purpose, it is too expensive. For fast turn-around\n            test, reduce the number of layers to 3.\n            \"\"\"", "\n", "args", ".", "transformer_enc_config", "=", "(", "\n", "\"((512, 8, 2048, True, 0.15, 0.15, 0.15),) * 3\"", "\n", ")", "\n", "\n", "", "super", "(", ")", ".", "setUp", "(", ")", "\n", "extra_args_setter", "=", "[", "vggtransformer_base", ",", "override_config", "]", "\n", "\n", "self", ".", "setUpModel", "(", "VGGTransformerModel", ",", "extra_args_setter", ")", "\n", "self", ".", "setUpInput", "(", "get_dummy_input", "(", "T", "=", "50", ",", "D", "=", "80", ",", "B", "=", "5", ",", "K", "=", "DEFAULT_TEST_VOCAB_SIZE", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.test_vggtransformer.VGGTransformerEncoderTest.setUp": [[83, 87], ["super().setUp", "test_vggtransformer.VGGTransformerEncoderTest.setUpInput", "asr_test_base.get_dummy_input"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.test_vggtransformer.TransformerDecoderTest.setUp", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.TestFairseqDecoderBase.setUpInput", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.get_dummy_input"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "setUp", "(", ")", "\n", "\n", "self", ".", "setUpInput", "(", "get_dummy_input", "(", "T", "=", "50", ",", "D", "=", "80", ",", "B", "=", "5", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.test_vggtransformer.VGGTransformerEncoderTest.test_forward": [[88, 121], ["print", "test_vggtransformer.VGGTransformerEncoderTest.setUpEncoder", "super().test_forward", "print", "test_vggtransformer.VGGTransformerEncoderTest.setUpEncoder", "super().test_forward", "print", "test_vggtransformer.VGGTransformerEncoderTest.setUpEncoder", "super().test_forward", "print", "test_vggtransformer.VGGTransformerEncoderTest.setUpEncoder", "super().test_forward", "print", "test_vggtransformer.VGGTransformerEncoderTest.setUpEncoder", "examples.speech_recognition.models.vggtransformer.VGGTransformerEncoder", "examples.speech_recognition.models.vggtransformer.VGGTransformerEncoder", "examples.speech_recognition.models.vggtransformer.VGGTransformerEncoder", "examples.speech_recognition.models.vggtransformer.VGGTransformerEncoder", "examples.speech_recognition.models.vggtransformer.VGGTransformerEncoder"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.TestFairseqEncoderBase.setUpEncoder", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.test_vggtransformer.VGGTransformerEncoderTest.test_forward", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.TestFairseqEncoderBase.setUpEncoder", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.test_vggtransformer.VGGTransformerEncoderTest.test_forward", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.TestFairseqEncoderBase.setUpEncoder", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.test_vggtransformer.VGGTransformerEncoderTest.test_forward", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.TestFairseqEncoderBase.setUpEncoder", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.test_vggtransformer.VGGTransformerEncoderTest.test_forward", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.TestFairseqEncoderBase.setUpEncoder"], ["", "def", "test_forward", "(", "self", ")", ":", "\n", "        ", "print", "(", "\"1. test standard vggtransformer\"", ")", "\n", "self", ".", "setUpEncoder", "(", "VGGTransformerEncoder", "(", "input_feat_per_channel", "=", "80", ")", ")", "\n", "super", "(", ")", ".", "test_forward", "(", ")", "\n", "print", "(", "\"2. test vggtransformer with limited right context\"", ")", "\n", "self", ".", "setUpEncoder", "(", "\n", "VGGTransformerEncoder", "(", "\n", "input_feat_per_channel", "=", "80", ",", "transformer_context", "=", "(", "-", "1", ",", "5", ")", "\n", ")", "\n", ")", "\n", "super", "(", ")", ".", "test_forward", "(", ")", "\n", "print", "(", "\"3. test vggtransformer with limited left context\"", ")", "\n", "self", ".", "setUpEncoder", "(", "\n", "VGGTransformerEncoder", "(", "\n", "input_feat_per_channel", "=", "80", ",", "transformer_context", "=", "(", "5", ",", "-", "1", ")", "\n", ")", "\n", ")", "\n", "super", "(", ")", ".", "test_forward", "(", ")", "\n", "print", "(", "\"4. test vggtransformer with limited right context and sampling\"", ")", "\n", "self", ".", "setUpEncoder", "(", "\n", "VGGTransformerEncoder", "(", "\n", "input_feat_per_channel", "=", "80", ",", "\n", "transformer_context", "=", "(", "-", "1", ",", "12", ")", ",", "\n", "transformer_sampling", "=", "(", "2", ",", "2", ")", ",", "\n", ")", "\n", ")", "\n", "super", "(", ")", ".", "test_forward", "(", ")", "\n", "print", "(", "\"5. test vggtransformer with windowed context and sampling\"", ")", "\n", "self", ".", "setUpEncoder", "(", "\n", "VGGTransformerEncoder", "(", "\n", "input_feat_per_channel", "=", "80", ",", "\n", "transformer_context", "=", "(", "12", ",", "12", ")", ",", "\n", "transformer_sampling", "=", "(", "2", ",", "2", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.test_vggtransformer.TransformerDecoderTest.setUp": [[126, 136], ["super().setUp", "asr_test_base.get_dummy_dictionary", "examples.speech_recognition.models.vggtransformer.TransformerDecoder", "asr_test_base.get_dummy_encoder_output", "test_vggtransformer.TransformerDecoderTest.setUpDecoder", "test_vggtransformer.TransformerDecoderTest.setUpInput", "test_vggtransformer.TransformerDecoderTest.setUpPrevOutputTokens"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.test_vggtransformer.TransformerDecoderTest.setUp", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.get_dummy_dictionary", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.get_dummy_encoder_output", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.TestFairseqDecoderBase.setUpDecoder", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.TestFairseqDecoderBase.setUpInput", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.asr_test_base.TestFairseqDecoderBase.setUpPrevOutputTokens"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "setUp", "(", ")", "\n", "\n", "dict", "=", "get_dummy_dictionary", "(", "vocab_size", "=", "DEFAULT_TEST_VOCAB_SIZE", ")", "\n", "decoder", "=", "TransformerDecoder", "(", "dict", ")", "\n", "dummy_encoder_output", "=", "get_dummy_encoder_output", "(", "encoder_out_shape", "=", "(", "50", ",", "5", ",", "256", ")", ")", "\n", "\n", "self", ".", "setUpDecoder", "(", "decoder", ")", "\n", "self", ".", "setUpInput", "(", "dummy_encoder_output", ")", "\n", "self", ".", "setUpPrevOutputTokens", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.datasets.asr_prep_json.process_sample": [[24, 37], ["torchaudio.info", "int", "tgt_dict.encode_line", "sp.EncodeAsPieces", "map", "t.tolist"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.encode_line"], ["def", "process_sample", "(", "aud_path", ",", "lable", ",", "utt_id", ",", "sp", ",", "tgt_dict", ")", ":", "\n", "    ", "input", "=", "{", "}", "\n", "output", "=", "{", "}", "\n", "si", ",", "ei", "=", "torchaudio", ".", "info", "(", "aud_path", ")", "\n", "input", "[", "\"length_ms\"", "]", "=", "int", "(", "si", ".", "length", "/", "si", ".", "channels", "/", "si", ".", "rate", "/", "MILLISECONDS_TO_SECONDS", ")", "\n", "input", "[", "\"path\"", "]", "=", "aud_path", "\n", "\n", "token", "=", "\" \"", ".", "join", "(", "sp", ".", "EncodeAsPieces", "(", "lable", ")", ")", "\n", "ids", "=", "tgt_dict", ".", "encode_line", "(", "token", ",", "append_eos", "=", "False", ")", "\n", "output", "[", "\"text\"", "]", "=", "lable", "\n", "output", "[", "\"token\"", "]", "=", "token", "\n", "output", "[", "\"tokenid\"", "]", "=", "', '", ".", "join", "(", "map", "(", "str", ",", "[", "t", ".", "tolist", "(", ")", "for", "t", "in", "ids", "]", ")", ")", "\n", "return", "{", "utt_id", ":", "{", "\"input\"", ":", "input", ",", "\"output\"", ":", "output", "}", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.datasets.asr_prep_json.main": [[39, 93], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "sentencepiece.SentencePieceProcessor", "spm.SentencePieceProcessor.Load", "fairseq.data.Dictionary.load", "collections.namedtuple", "itertools.chain.from_iterable", "multiprocessing.cpu_count", "json.dump", "line.split", "len", "Exception", "concurrent.futures.ThreadPoolExecutor", "concurrent.futures.as_completed", "argparse.FileType", "argparse.FileType", "argparse.FileType", "argparse.FileType", "os.walk", "f.endswith", "executor.submit", "samples.append", "future.result", "utts.update", "len", "Exception", "os.path.splitext", "collections.namedtuple.", "print", "os.path.splitext", "os.path.join"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.retyper.Stack.dump", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--audio-dirs\"", ",", "nargs", "=", "\"+\"", ",", "default", "=", "[", "'-'", "]", ",", "required", "=", "True", ",", "\n", "help", "=", "\"input directories with audio files\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--labels\"", ",", "required", "=", "True", ",", "\n", "help", "=", "\"aggregated input labels with format <ID LABEL> per line\"", ",", "\n", "type", "=", "argparse", ".", "FileType", "(", "'r'", ",", "encoding", "=", "'UTF-8'", ")", ")", "\n", "parser", ".", "add_argument", "(", "\"--spm-model\"", ",", "required", "=", "True", ",", "\n", "help", "=", "\"sentencepiece model to use for encoding\"", ",", "\n", "type", "=", "argparse", ".", "FileType", "(", "'r'", ",", "encoding", "=", "'UTF-8'", ")", ")", "\n", "parser", ".", "add_argument", "(", "\"--dictionary\"", ",", "required", "=", "True", ",", "\n", "help", "=", "\"file to load fairseq dictionary from\"", ",", "\n", "type", "=", "argparse", ".", "FileType", "(", "'r'", ",", "encoding", "=", "'UTF-8'", ")", ")", "\n", "parser", ".", "add_argument", "(", "\"--audio-format\"", ",", "choices", "=", "[", "\"flac\"", ",", "\"wav\"", "]", ",", "default", "=", "\"wav\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output\"", ",", "required", "=", "True", ",", "type", "=", "argparse", ".", "FileType", "(", "'w'", ")", ",", "\n", "help", "=", "\"path to save json output\"", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "sp", "=", "spm", ".", "SentencePieceProcessor", "(", ")", "\n", "sp", ".", "Load", "(", "args", ".", "spm_model", ".", "name", ")", "\n", "\n", "tgt_dict", "=", "Dictionary", ".", "load", "(", "args", ".", "dictionary", ")", "\n", "\n", "labels", "=", "{", "}", "\n", "for", "line", "in", "args", ".", "labels", ":", "\n", "        ", "(", "utt_id", ",", "label", ")", "=", "line", ".", "split", "(", "\" \"", ",", "1", ")", "\n", "labels", "[", "utt_id", "]", "=", "label", "\n", "", "if", "len", "(", "labels", ")", "==", "0", ":", "\n", "        ", "raise", "Exception", "(", "'No labels found in '", ",", "args", ".", "labels_path", ")", "\n", "\n", "", "Sample", "=", "namedtuple", "(", "'Sample'", ",", "'aud_path utt_id'", ")", "\n", "samples", "=", "[", "]", "\n", "for", "path", ",", "_", ",", "files", "in", "chain", ".", "from_iterable", "(", "os", ".", "walk", "(", "path", ")", "for", "path", "in", "args", ".", "audio_dirs", ")", ":", "\n", "        ", "for", "f", "in", "files", ":", "\n", "            ", "if", "f", ".", "endswith", "(", "args", ".", "audio_format", ")", ":", "\n", "                ", "if", "len", "(", "os", ".", "path", ".", "splitext", "(", "f", ")", ")", "!=", "2", ":", "\n", "                    ", "raise", "Exception", "(", "'Expect <utt_id.extension> file name. Got: '", ",", "f", ")", "\n", "", "utt_id", "=", "os", ".", "path", ".", "splitext", "(", "f", ")", "[", "0", "]", "\n", "if", "utt_id", "not", "in", "labels", ":", "\n", "                    ", "continue", "\n", "", "samples", ".", "append", "(", "Sample", "(", "os", ".", "path", ".", "join", "(", "path", ",", "f", ")", ",", "utt_id", ")", ")", "\n", "\n", "", "", "", "utts", "=", "{", "}", "\n", "num_cpu", "=", "multiprocessing", ".", "cpu_count", "(", ")", "\n", "with", "concurrent", ".", "futures", ".", "ThreadPoolExecutor", "(", "max_workers", "=", "num_cpu", ")", "as", "executor", ":", "\n", "        ", "future_to_sample", "=", "{", "executor", ".", "submit", "(", "process_sample", ",", "s", ".", "aud_path", ",", "labels", "[", "s", ".", "utt_id", "]", ",", "s", ".", "utt_id", ",", "sp", ",", "tgt_dict", ")", ":", "s", "for", "s", "in", "samples", "}", "\n", "for", "future", "in", "concurrent", ".", "futures", ".", "as_completed", "(", "future_to_sample", ")", ":", "\n", "            ", "try", ":", "\n", "                ", "data", "=", "future", ".", "result", "(", ")", "\n", "", "except", "Exception", "as", "exc", ":", "\n", "                ", "print", "(", "'generated an exception: '", ",", "exc", ")", "\n", "", "else", ":", "\n", "                ", "utts", ".", "update", "(", "data", ")", "\n", "", "", "", "json", ".", "dump", "(", "{", "\"utts\"", ":", "utts", "}", ",", "args", ".", "output", ",", "indent", "=", "4", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.speech_recognition.SpeechRecognitionTask.add_args": [[76, 80], ["parser.add_argument"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add task-specific arguments to the parser.\"\"\"", "\n", "parser", ".", "add_argument", "(", "\"data\"", ",", "help", "=", "\"path to data directory\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.speech_recognition.SpeechRecognitionTask.__init__": [[81, 84], ["fairseq.tasks.FairseqTask.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["", "def", "__init__", "(", "self", ",", "args", ",", "tgt_dict", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ")", "\n", "self", ".", "tgt_dict", "=", "tgt_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.speech_recognition.SpeechRecognitionTask.setup_task": [[85, 95], ["os.path.join", "fairseq.data.Dictionary.load", "print", "cls", "os.path.isfile", "FileNotFoundError", "len"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.masked_lm_dictionary.BertDictionary.cls"], ["", "@", "classmethod", "\n", "def", "setup_task", "(", "cls", ",", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Setup the task (e.g., load dictionaries).\"\"\"", "\n", "dict_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data", ",", "\"dict.txt\"", ")", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "dict_path", ")", ":", "\n", "            ", "raise", "FileNotFoundError", "(", "\"Dict not found: {}\"", ".", "format", "(", "dict_path", ")", ")", "\n", "", "tgt_dict", "=", "Dictionary", ".", "load", "(", "dict_path", ")", "\n", "\n", "print", "(", "\"| dictionary: {} types\"", ".", "format", "(", "len", "(", "tgt_dict", ")", ")", ")", "\n", "return", "cls", "(", "args", ",", "tgt_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.speech_recognition.SpeechRecognitionTask.load_dataset": [[96, 105], ["os.path.join", "speech_recognition.get_asr_dataset_from_json"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.speech_recognition.get_asr_dataset_from_json"], ["", "def", "load_dataset", "(", "self", ",", "split", ",", "combine", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Load a given dataset split.\n\n        Args:\n            split (str): name of the split (e.g., train, valid, test)\n        \"\"\"", "\n", "data_json_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "args", ".", "data", ",", "\"{}.json\"", ".", "format", "(", "split", ")", ")", "\n", "self", ".", "datasets", "[", "split", "]", "=", "get_asr_dataset_from_json", "(", "\n", "data_json_path", ",", "self", ".", "tgt_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.speech_recognition.SpeechRecognitionTask.target_dictionary": [[106, 111], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "target_dictionary", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the :class:`~fairseq.data.Dictionary` for the language\n        model.\"\"\"", "\n", "return", "self", ".", "tgt_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.speech_recognition.SpeechRecognitionTask.source_dictionary": [[112, 117], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "source_dictionary", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the source :class:`~fairseq.data.Dictionary` (if applicable\n        for this task).\"\"\"", "\n", "return", "None", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.speech_recognition.get_asr_dataset_from_json": [[16, 67], ["os.path.isfile", "FileNotFoundError", "open", "sorted", "examples.speech_recognition.data.AsrDataset", "json.load", "len", "data_samples.items", "re.search", "speakers.append", "torch.LongTensor", "torch.cat", "int", "re.search.group", "int", "torch.LongTensor", "re.search.group", "[].split", "tgt_dict.eos"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos"], ["def", "get_asr_dataset_from_json", "(", "data_json_path", ",", "tgt_dict", ")", ":", "\n", "    ", "\"\"\"\n    Parse data json and create dataset.\n    See scripts/asr_prep_json.py which pack json from raw files\n\n    Json example:\n    {\n    \"utts\": {\n        \"4771-29403-0025\": {\n            \"input\": {\n                \"length_ms\": 170,\n                \"path\": \"/tmp/file1.flac\"\n            },\n            \"output\": {\n                \"text\": \"HELLO \\n\",\n                \"token\": \"HE LLO\",\n                \"tokenid\": \"4815, 861\"\n            }\n        },\n        \"1564-142299-0096\": {\n            ...\n        }\n    }\n    \"\"\"", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "data_json_path", ")", ":", "\n", "        ", "raise", "FileNotFoundError", "(", "\"Dataset not found: {}\"", ".", "format", "(", "data_json_path", ")", ")", "\n", "", "with", "open", "(", "data_json_path", ",", "\"rb\"", ")", "as", "f", ":", "\n", "        ", "data_samples", "=", "json", ".", "load", "(", "f", ")", "[", "\"utts\"", "]", "\n", "assert", "len", "(", "data_samples", ")", "!=", "0", "\n", "sorted_samples", "=", "sorted", "(", "\n", "data_samples", ".", "items", "(", ")", ",", "\n", "key", "=", "lambda", "sample", ":", "int", "(", "sample", "[", "1", "]", "[", "\"input\"", "]", "[", "\"length_ms\"", "]", ")", ",", "\n", "reverse", "=", "True", ",", "\n", ")", "\n", "aud_paths", "=", "[", "s", "[", "1", "]", "[", "\"input\"", "]", "[", "\"path\"", "]", "for", "s", "in", "sorted_samples", "]", "\n", "ids", "=", "[", "s", "[", "0", "]", "for", "s", "in", "sorted_samples", "]", "\n", "speakers", "=", "[", "]", "\n", "for", "s", "in", "sorted_samples", ":", "\n", "            ", "m", "=", "re", ".", "search", "(", "\"(.+?)-(.+?)-(.+?)\"", ",", "s", "[", "0", "]", ")", "\n", "speakers", ".", "append", "(", "m", ".", "group", "(", "1", ")", "+", "\"_\"", "+", "m", ".", "group", "(", "2", ")", ")", "\n", "", "frame_sizes", "=", "[", "s", "[", "1", "]", "[", "\"input\"", "]", "[", "\"length_ms\"", "]", "for", "s", "in", "sorted_samples", "]", "\n", "tgt", "=", "[", "\n", "torch", ".", "LongTensor", "(", "\n", "[", "int", "(", "i", ")", "for", "i", "in", "s", "[", "1", "]", "[", "\"output\"", "]", "[", "\"tokenid\"", "]", ".", "split", "(", "\", \"", ")", "]", "\n", ")", "\n", "for", "s", "in", "sorted_samples", "\n", "]", "\n", "# append eos", "\n", "tgt", "=", "[", "torch", ".", "cat", "(", "[", "t", ",", "torch", ".", "LongTensor", "(", "[", "tgt_dict", ".", "eos", "(", ")", "]", ")", "]", ")", "for", "t", "in", "tgt", "]", "\n", "return", "AsrDataset", "(", "\n", "aud_paths", ",", "frame_sizes", ",", "tgt", ",", "tgt_dict", ",", "ids", ",", "speakers", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.semisupervised_translation.SemisupervisedTranslationTask.add_args": [[79, 113], ["multilingual_translation.MultilingualTranslationTask.add_args", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.add_args"], ["@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add task-specific arguments to the parser.\"\"\"", "\n", "# fmt: off", "\n", "MultilingualTranslationTask", ".", "add_args", "(", "parser", ")", "\n", "parser", ".", "add_argument", "(", "'--lambda-parallel-config'", ",", "default", "=", "\"1.0\"", ",", "type", "=", "str", ",", "metavar", "=", "'CONFIG'", ",", "\n", "help", "=", "'cross-entropy reconstruction coefficient (parallel data). '", "\n", "'use fixed weight during training if set to floating point number. '", "\n", "'use piecewise linear function over number of updates to schedule the '", "\n", "'weight with the format: w0:step0,w1:step1,...'", ")", "\n", "parser", ".", "add_argument", "(", "'--lambda-denoising-config'", ",", "default", "=", "\"0.0\"", ",", "type", "=", "str", ",", "metavar", "=", "'CONFIG'", ",", "\n", "help", "=", "'Cross-entropy reconstruction coefficient (denoising autoencoding)'", "\n", "'use fixed weight during training if set to floating point number. '", "\n", "'use piecewise linear function over number of updates to schedule the '", "\n", "'weight with the format: w0:step0,w1:step1,...'", ")", "\n", "parser", ".", "add_argument", "(", "'--lambda-otf-bt-config'", ",", "default", "=", "\"0.0\"", ",", "type", "=", "str", ",", "metavar", "=", "'CONFIG'", ",", "\n", "help", "=", "'cross-entropy reconstruction coefficient (on-the-fly back-translation parallel data)'", "\n", "'use fixed weight during training if set to floating point number. '", "\n", "'use piecewise linear function over number of updates to schedule the '", "\n", "'weight with the format: w0:step0,w1:step1,...'", ")", "\n", "parser", ".", "add_argument", "(", "'--bt-max-len-a'", ",", "default", "=", "1.1", ",", "type", "=", "float", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'generate back-translated sequences of maximum length ax + b, where x is the '", "\n", "'source length'", ")", "\n", "parser", ".", "add_argument", "(", "'--bt-max-len-b'", ",", "default", "=", "10.0", ",", "type", "=", "float", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'generate back-translated sequences of maximum length ax + b, where x is the '", "\n", "'source length'", ")", "\n", "parser", ".", "add_argument", "(", "'--bt-beam-size'", ",", "default", "=", "1", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'beam size used in beam search of online back-translation'", ")", "\n", "parser", ".", "add_argument", "(", "'--max-word-shuffle-distance'", ",", "default", "=", "3.0", ",", "type", "=", "float", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'maximum word shuffle distance for denoising autoencoding data generation'", ")", "\n", "parser", ".", "add_argument", "(", "'--word-dropout-prob'", ",", "default", "=", "0.1", ",", "type", "=", "float", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'word dropout probability for denoising autoencoding data generation'", ")", "\n", "parser", ".", "add_argument", "(", "'--word-blanking-prob'", ",", "default", "=", "0.2", ",", "type", "=", "float", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'word blanking probability for denoising autoencoding data generation'", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.semisupervised_translation.SemisupervisedTranslationTask.__init__": [[115, 128], ["multilingual_translation.MultilingualTranslationTask.__init__", "semisupervised_translation.parse_lambda_config", "semisupervised_translation.parse_lambda_config", "semisupervised_translation.parse_lambda_config", "lang_pair.split"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.semisupervised_translation.parse_lambda_config", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.semisupervised_translation.parse_lambda_config", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.semisupervised_translation.parse_lambda_config", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["", "def", "__init__", "(", "self", ",", "args", ",", "dicts", ",", "training", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "dicts", ",", "training", ")", "\n", "self", ".", "lambda_parallel", ",", "self", ".", "lambda_parallel_steps", "=", "parse_lambda_config", "(", "args", ".", "lambda_parallel_config", ")", "\n", "self", ".", "lambda_otf_bt", ",", "self", ".", "lambda_otf_bt_steps", "=", "parse_lambda_config", "(", "args", ".", "lambda_otf_bt_config", ")", "\n", "self", ".", "lambda_denoising", ",", "self", ".", "lambda_denoising_steps", "=", "parse_lambda_config", "(", "args", ".", "lambda_denoising_config", ")", "\n", "if", "(", "self", ".", "lambda_denoising", ">", "0.0", "or", "self", ".", "lambda_denoising_steps", "is", "not", "None", ")", ":", "\n", "            ", "denoising_lang_pairs", "=", "[", "\n", "\"%s-%s\"", "%", "(", "tgt", ",", "tgt", ")", "\n", "for", "tgt", "in", "{", "lang_pair", ".", "split", "(", "'-'", ")", "[", "1", "]", "for", "lang_pair", "in", "args", ".", "lang_pairs", "}", "\n", "]", "\n", "self", ".", "model_lang_pairs", "=", "self", ".", "model_lang_pairs", "+", "denoising_lang_pairs", "\n", "", "self", ".", "backtranslate_datasets", "=", "{", "}", "\n", "self", ".", "backtranslators", "=", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.semisupervised_translation.SemisupervisedTranslationTask.setup_task": [[129, 133], ["multilingual_translation.MultilingualTranslationTask.prepare", "cls"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.multilingual_translation.MultilingualTranslationTask.prepare", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.masked_lm_dictionary.BertDictionary.cls"], ["", "@", "classmethod", "\n", "def", "setup_task", "(", "cls", ",", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "dicts", ",", "training", "=", "MultilingualTranslationTask", ".", "prepare", "(", "args", ",", "**", "kwargs", ")", "\n", "return", "cls", "(", "args", ",", "dicts", ",", "training", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.semisupervised_translation.SemisupervisedTranslationTask.load_dataset": [[134, 295], ["semisupervised_translation.SemisupervisedTranslationTask.args.data.split", "fairseq.data.RoundRobinZipDatasets", "len", "split.startswith", "split.startswith", "lang_pair.split", "semisupervised_translation.SemisupervisedTranslationTask.alter_dataset_langtok", "collections.OrderedDict", "os.path.join", "os.path.join", "fairseq.data.IndexedRawTextDataset.exists", "fairseq.data.IndexedRawTextDataset", "fairseq.data.IndexedDataset.exists", "split.startswith", "lang_pair.split", "semisupervised_translation.SemisupervisedTranslationTask.load_dataset.split_exists"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.multilingual_translation.MultilingualTranslationTask.alter_dataset_langtok", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["", "def", "load_dataset", "(", "self", ",", "split", ",", "epoch", "=", "0", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Load a dataset split.\"\"\"", "\n", "\n", "paths", "=", "self", ".", "args", ".", "data", ".", "split", "(", "':'", ")", "\n", "assert", "len", "(", "paths", ")", ">", "0", "\n", "data_path", "=", "paths", "[", "epoch", "%", "len", "(", "paths", ")", "]", "\n", "\n", "def", "split_exists", "(", "split", ",", "src", ",", "tgt", ",", "lang", ")", ":", "\n", "            ", "if", "src", "is", "not", "None", ":", "\n", "                ", "filename", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "'{}.{}-{}.{}'", ".", "format", "(", "split", ",", "src", ",", "tgt", ",", "lang", ")", ")", "\n", "", "else", ":", "\n", "                ", "filename", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "'{}.{}-None.{}'", ".", "format", "(", "split", ",", "src", ",", "tgt", ")", ")", "\n", "", "if", "self", ".", "args", ".", "raw_text", "and", "IndexedRawTextDataset", ".", "exists", "(", "filename", ")", ":", "\n", "                ", "return", "True", "\n", "", "elif", "not", "self", ".", "args", ".", "raw_text", "and", "IndexedDataset", ".", "exists", "(", "filename", ")", ":", "\n", "                ", "return", "True", "\n", "", "return", "False", "\n", "\n", "", "def", "indexed_dataset", "(", "path", ",", "dictionary", ")", ":", "\n", "            ", "if", "self", ".", "args", ".", "raw_text", ":", "\n", "                ", "return", "IndexedRawTextDataset", "(", "path", ",", "dictionary", ")", "\n", "", "elif", "IndexedDataset", ".", "exists", "(", "path", ")", ":", "\n", "                ", "if", "self", ".", "args", ".", "lazy_load", ":", "\n", "                    ", "return", "IndexedDataset", "(", "path", ",", "fix_lua_indexing", "=", "True", ")", "\n", "", "else", ":", "\n", "                    ", "return", "IndexedCachedDataset", "(", "path", ",", "fix_lua_indexing", "=", "True", ")", "\n", "", "", "return", "None", "\n", "\n", "# load parallel datasets", "\n", "", "src_datasets", ",", "tgt_datasets", "=", "{", "}", ",", "{", "}", "\n", "if", "(", "self", ".", "lambda_parallel", ">", "0.0", "or", "self", ".", "lambda_parallel_steps", "is", "not", "None", "or", "not", "split", ".", "startswith", "(", "\"train\"", ")", ")", ":", "\n", "            ", "for", "lang_pair", "in", "self", ".", "lang_pairs", ":", "\n", "                ", "src", ",", "tgt", "=", "lang_pair", ".", "split", "(", "'-'", ")", "\n", "if", "split_exists", "(", "split", ",", "src", ",", "tgt", ",", "src", ")", ":", "\n", "                    ", "prefix", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "'{}.{}-{}.'", ".", "format", "(", "split", ",", "src", ",", "tgt", ")", ")", "\n", "", "elif", "split_exists", "(", "split", ",", "tgt", ",", "src", ",", "src", ")", ":", "\n", "                    ", "prefix", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "'{}.{}-{}.'", ".", "format", "(", "split", ",", "tgt", ",", "src", ")", ")", "\n", "", "else", ":", "\n", "                    ", "continue", "\n", "", "src_datasets", "[", "lang_pair", "]", "=", "indexed_dataset", "(", "prefix", "+", "src", ",", "self", ".", "dicts", "[", "src", "]", ")", "\n", "tgt_datasets", "[", "lang_pair", "]", "=", "indexed_dataset", "(", "prefix", "+", "tgt", ",", "self", ".", "dicts", "[", "tgt", "]", ")", "\n", "print", "(", "'| parallel-{} {} {} examples'", ".", "format", "(", "data_path", ",", "split", ",", "len", "(", "src_datasets", "[", "lang_pair", "]", ")", ")", ")", "\n", "", "if", "len", "(", "src_datasets", ")", "==", "0", ":", "\n", "                ", "raise", "FileNotFoundError", "(", "'Dataset not found: {} ({})'", ".", "format", "(", "split", ",", "data_path", ")", ")", "\n", "\n", "# back translation datasets", "\n", "", "", "backtranslate_datasets", "=", "{", "}", "\n", "if", "(", "self", ".", "lambda_otf_bt", ">", "0.0", "or", "self", ".", "lambda_otf_bt_steps", "is", "not", "None", ")", "and", "split", ".", "startswith", "(", "\"train\"", ")", ":", "\n", "            ", "for", "lang_pair", "in", "self", ".", "lang_pairs", ":", "\n", "                ", "src", ",", "tgt", "=", "lang_pair", ".", "split", "(", "'-'", ")", "\n", "if", "not", "split_exists", "(", "split", ",", "tgt", ",", "None", ",", "tgt", ")", ":", "\n", "                    ", "raise", "FileNotFoundError", "(", "'Dataset not found: backtranslation {} ({})'", ".", "format", "(", "split", ",", "data_path", ")", ")", "\n", "", "filename", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "'{}.{}-None.{}'", ".", "format", "(", "split", ",", "tgt", ",", "tgt", ")", ")", "\n", "dataset", "=", "indexed_dataset", "(", "filename", ",", "self", ".", "dicts", "[", "tgt", "]", ")", "\n", "lang_pair_dataset_tgt", "=", "LanguagePairDataset", "(", "\n", "dataset", ",", "\n", "dataset", ".", "sizes", ",", "\n", "self", ".", "dicts", "[", "tgt", "]", ",", "\n", "left_pad_source", "=", "self", ".", "args", ".", "left_pad_source", ",", "\n", "left_pad_target", "=", "self", ".", "args", ".", "left_pad_target", ",", "\n", ")", "\n", "lang_pair_dataset", "=", "LanguagePairDataset", "(", "\n", "dataset", ",", "\n", "dataset", ".", "sizes", ",", "\n", "src_dict", "=", "self", ".", "dicts", "[", "src", "]", ",", "\n", "tgt", "=", "dataset", ",", "\n", "tgt_sizes", "=", "dataset", ".", "sizes", ",", "\n", "tgt_dict", "=", "self", ".", "dicts", "[", "tgt", "]", ",", "\n", "left_pad_source", "=", "self", ".", "args", ".", "left_pad_source", ",", "\n", "left_pad_target", "=", "self", ".", "args", ".", "left_pad_target", ",", "\n", ")", "\n", "backtranslate_datasets", "[", "lang_pair", "]", "=", "BacktranslationDataset", "(", "\n", "tgt_dataset", "=", "self", ".", "alter_dataset_langtok", "(", "\n", "lang_pair_dataset_tgt", ",", "\n", "src_eos", "=", "self", ".", "dicts", "[", "tgt", "]", ".", "eos", "(", ")", ",", "\n", "src_lang", "=", "tgt", ",", "\n", "tgt_lang", "=", "src", ",", "\n", ")", ",", "\n", "backtranslation_fn", "=", "self", ".", "backtranslators", "[", "lang_pair", "]", ",", "\n", "src_dict", "=", "self", ".", "dicts", "[", "src", "]", ",", "tgt_dict", "=", "self", ".", "dicts", "[", "tgt", "]", ",", "\n", "output_collater", "=", "self", ".", "alter_dataset_langtok", "(", "\n", "lang_pair_dataset", "=", "lang_pair_dataset", ",", "\n", "src_eos", "=", "self", ".", "dicts", "[", "src", "]", ".", "eos", "(", ")", ",", "\n", "src_lang", "=", "src", ",", "\n", "tgt_eos", "=", "self", ".", "dicts", "[", "tgt", "]", ".", "eos", "(", ")", ",", "\n", "tgt_lang", "=", "tgt", ",", "\n", ")", ".", "collater", ",", "\n", ")", "\n", "print", "(", "'| backtranslate-{}: {} {} {} examples'", ".", "format", "(", "\n", "tgt", ",", "data_path", ",", "split", ",", "len", "(", "backtranslate_datasets", "[", "lang_pair", "]", ")", ",", "\n", ")", ")", "\n", "self", ".", "backtranslate_datasets", "[", "lang_pair", "]", "=", "backtranslate_datasets", "[", "lang_pair", "]", "\n", "\n", "# denoising autoencoder", "\n", "", "", "noising_datasets", "=", "{", "}", "\n", "if", "(", "self", ".", "lambda_denoising", ">", "0.0", "or", "self", ".", "lambda_denoising_steps", "is", "not", "None", ")", "and", "split", ".", "startswith", "(", "\"train\"", ")", ":", "\n", "            ", "for", "lang_pair", "in", "self", ".", "lang_pairs", ":", "\n", "                ", "_", ",", "tgt", "=", "lang_pair", ".", "split", "(", "'-'", ")", "\n", "if", "not", "split_exists", "(", "split", ",", "tgt", ",", "None", ",", "tgt", ")", ":", "\n", "                    ", "continue", "\n", "", "filename", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "'{}.{}-None.{}'", ".", "format", "(", "split", ",", "tgt", ",", "tgt", ")", ")", "\n", "tgt_dataset1", "=", "indexed_dataset", "(", "filename", ",", "self", ".", "dicts", "[", "tgt", "]", ")", "\n", "tgt_dataset2", "=", "indexed_dataset", "(", "filename", ",", "self", ".", "dicts", "[", "tgt", "]", ")", "\n", "noising_dataset", "=", "NoisingDataset", "(", "\n", "tgt_dataset1", ",", "\n", "self", ".", "dicts", "[", "tgt", "]", ",", "\n", "seed", "=", "1", ",", "\n", "max_word_shuffle_distance", "=", "self", ".", "args", ".", "max_word_shuffle_distance", ",", "\n", "word_dropout_prob", "=", "self", ".", "args", ".", "word_dropout_prob", ",", "\n", "word_blanking_prob", "=", "self", ".", "args", ".", "word_blanking_prob", ",", "\n", ")", "\n", "noising_datasets", "[", "lang_pair", "]", "=", "self", ".", "alter_dataset_langtok", "(", "\n", "LanguagePairDataset", "(", "\n", "noising_dataset", ",", "\n", "tgt_dataset1", ".", "sizes", ",", "\n", "self", ".", "dicts", "[", "tgt", "]", ",", "\n", "tgt_dataset2", ",", "\n", "tgt_dataset2", ".", "sizes", ",", "\n", "self", ".", "dicts", "[", "tgt", "]", ",", "\n", "left_pad_source", "=", "self", ".", "args", ".", "left_pad_source", ",", "\n", "left_pad_target", "=", "self", ".", "args", ".", "left_pad_target", ",", "\n", ")", ",", "\n", "src_eos", "=", "self", ".", "dicts", "[", "tgt", "]", ".", "eos", "(", ")", ",", "\n", "src_lang", "=", "tgt", ",", "\n", "tgt_eos", "=", "self", ".", "dicts", "[", "tgt", "]", ".", "eos", "(", ")", ",", "\n", "tgt_lang", "=", "tgt", ",", "\n", ")", "\n", "print", "(", "'| denoising-{}: {} {} {} examples'", ".", "format", "(", "\n", "tgt", ",", "data_path", ",", "split", ",", "len", "(", "noising_datasets", "[", "lang_pair", "]", ")", ",", "\n", ")", ")", "\n", "\n", "", "", "def", "language_pair_dataset", "(", "lang_pair", ")", ":", "\n", "            ", "src", ",", "tgt", "=", "lang_pair", ".", "split", "(", "'-'", ")", "\n", "src_dataset", ",", "tgt_dataset", "=", "src_datasets", "[", "lang_pair", "]", ",", "tgt_datasets", "[", "lang_pair", "]", "\n", "return", "self", ".", "alter_dataset_langtok", "(", "\n", "LanguagePairDataset", "(", "\n", "src_dataset", ",", "src_dataset", ".", "sizes", ",", "self", ".", "dicts", "[", "src", "]", ",", "\n", "tgt_dataset", ",", "tgt_dataset", ".", "sizes", ",", "self", ".", "dicts", "[", "tgt", "]", ",", "\n", "left_pad_source", "=", "self", ".", "args", ".", "left_pad_source", ",", "\n", "left_pad_target", "=", "self", ".", "args", ".", "left_pad_target", ",", "\n", "max_source_positions", "=", "self", ".", "args", ".", "max_source_positions", ",", "\n", "max_target_positions", "=", "self", ".", "args", ".", "max_target_positions", ",", "\n", ")", ",", "\n", "self", ".", "dicts", "[", "src", "]", ".", "eos", "(", ")", ",", "\n", "src", ",", "\n", "self", ".", "dicts", "[", "tgt", "]", ".", "eos", "(", ")", ",", "\n", "tgt", ",", "\n", ")", "\n", "\n", "", "self", ".", "datasets", "[", "split", "]", "=", "RoundRobinZipDatasets", "(", "\n", "OrderedDict", "(", "[", "\n", "(", "lang_pair", ",", "language_pair_dataset", "(", "lang_pair", ")", ")", "\n", "for", "lang_pair", "in", "src_datasets", ".", "keys", "(", ")", "\n", "]", "+", "[", "\n", "(", "_get_bt_dataset_key", "(", "lang_pair", ")", ",", "dataset", ")", "\n", "for", "lang_pair", ",", "dataset", "in", "backtranslate_datasets", ".", "items", "(", ")", "\n", "]", "+", "[", "\n", "(", "_get_denoising_dataset_key", "(", "lang_pair", ")", ",", "dataset", ")", "\n", "for", "lang_pair", ",", "dataset", "in", "noising_datasets", ".", "items", "(", ")", "\n", "]", ")", ",", "\n", "eval_key", "=", "None", "if", "self", ".", "training", "else", "\"%s-%s\"", "%", "(", "self", ".", "args", ".", "source_lang", ",", "self", ".", "args", ".", "target_lang", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.semisupervised_translation.SemisupervisedTranslationTask.build_model": [[297, 330], ["models.build_model", "isinstance", "ValueError", "lang_pair.split", "fairseq.sequence_generator.SequenceGenerator", "semisupervised_translation.SemisupervisedTranslationTask.get_decoder_langtok", "sequence_generator.generate"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv_lm.LightConvLanguageModel.build_model", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.multilingual_translation.MultilingualTranslationTask.get_decoder_langtok", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.sequence_generator.SequenceGenerator.generate"], ["", "def", "build_model", "(", "self", ",", "args", ")", ":", "\n", "        ", "from", "fairseq", "import", "models", "\n", "model", "=", "models", ".", "build_model", "(", "args", ",", "self", ")", "\n", "if", "not", "isinstance", "(", "model", ",", "FairseqMultiModel", ")", ":", "\n", "            ", "raise", "ValueError", "(", "'SemisupervisedTranslationTask requires a FairseqMultiModel architecture'", ")", "\n", "\n", "# create SequenceGenerator for each model that has backtranslation dependency on it", "\n", "", "self", ".", "sequence_generators", "=", "{", "}", "\n", "if", "(", "self", ".", "lambda_otf_bt", ">", "0.0", "or", "self", ".", "lambda_otf_bt_steps", "is", "not", "None", ")", "and", "self", ".", "training", ":", "\n", "            ", "for", "lang_pair", "in", "self", ".", "lang_pairs", ":", "\n", "                ", "src", ",", "tgt", "=", "lang_pair", ".", "split", "(", "'-'", ")", "\n", "key", "=", "'{}-{}'", ".", "format", "(", "tgt", ",", "src", ")", "\n", "self", ".", "sequence_generators", "[", "key", "]", "=", "SequenceGenerator", "(", "\n", "tgt_dict", "=", "self", ".", "dicts", "[", "src", "]", ",", "\n", "beam_size", "=", "args", ".", "bt_beam_size", ",", "\n", "max_len_a", "=", "args", ".", "bt_max_len_a", ",", "\n", "max_len_b", "=", "args", ".", "bt_max_len_b", ",", "\n", ")", "\n", "decoder_lang_tok_idx", "=", "self", ".", "get_decoder_langtok", "(", "src", ")", "\n", "\n", "def", "backtranslate_fn", "(", "\n", "sample", ",", "model", "=", "model", ".", "models", "[", "key", "]", ",", "\n", "bos_token", "=", "decoder_lang_tok_idx", ",", "\n", "sequence_generator", "=", "self", ".", "sequence_generators", "[", "key", "]", ",", "\n", ")", ":", "\n", "                    ", "return", "sequence_generator", ".", "generate", "(", "\n", "[", "model", "]", ",", "\n", "sample", ",", "\n", "bos_token", "=", "bos_token", ",", "\n", ")", "\n", "", "self", ".", "backtranslators", "[", "lang_pair", "]", "=", "backtranslate_fn", "\n", "\n", "", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.semisupervised_translation.SemisupervisedTranslationTask.train_step": [[331, 366], ["model.train", "criterion", "optimizer.backward", "loss.detach().item", "semisupervised_translation.SemisupervisedTranslationTask.train_step.forward_backward"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq-stack-transformer.train.train", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.scalar_bias.ScalarBias.backward", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item"], ["", "def", "train_step", "(", "self", ",", "sample", ",", "model", ",", "criterion", ",", "optimizer", ",", "ignore_grad", "=", "False", ")", ":", "\n", "        ", "model", ".", "train", "(", ")", "\n", "agg_loss", ",", "agg_sample_size", ",", "agg_logging_output", "=", "0.", ",", "0.", ",", "{", "}", "\n", "\n", "def", "forward_backward", "(", "model", ",", "samples", ",", "logging_output_key", ",", "weight", ")", ":", "\n", "            ", "nonlocal", "agg_loss", ",", "agg_sample_size", ",", "agg_logging_output", "\n", "if", "samples", "is", "None", "or", "len", "(", "samples", ")", "==", "0", ":", "\n", "                ", "return", "\n", "", "loss", ",", "sample_size", ",", "logging_output", "=", "criterion", "(", "model", ",", "samples", ")", "\n", "if", "ignore_grad", ":", "\n", "                ", "loss", "*=", "0", "\n", "", "else", ":", "\n", "                ", "loss", "*=", "weight", "\n", "", "optimizer", ".", "backward", "(", "loss", ")", "\n", "agg_loss", "+=", "loss", ".", "detach", "(", ")", ".", "item", "(", ")", "\n", "# TODO make summing of the sample sizes configurable", "\n", "agg_sample_size", "+=", "sample_size", "\n", "agg_logging_output", "[", "logging_output_key", "]", "=", "logging_output", "\n", "\n", "", "if", "self", ".", "lambda_parallel", ">", "0.0", ":", "\n", "            ", "for", "lang_pair", "in", "self", ".", "lang_pairs", ":", "\n", "                ", "forward_backward", "(", "model", ".", "models", "[", "lang_pair", "]", ",", "sample", "[", "lang_pair", "]", ",", "lang_pair", ",", "self", ".", "lambda_parallel", ")", "\n", "\n", "", "", "if", "self", ".", "lambda_otf_bt", ">", "0.0", ":", "\n", "            ", "for", "lang_pair", "in", "self", ".", "lang_pairs", ":", "\n", "                ", "sample_key", "=", "_get_bt_dataset_key", "(", "lang_pair", ")", "\n", "forward_backward", "(", "model", ".", "models", "[", "lang_pair", "]", ",", "sample", "[", "sample_key", "]", ",", "sample_key", ",", "self", ".", "lambda_otf_bt", ")", "\n", "\n", "", "", "if", "self", ".", "lambda_denoising", ">", "0.0", ":", "\n", "            ", "for", "lang_pair", "in", "self", ".", "lang_pairs", ":", "\n", "                ", "_", ",", "tgt", "=", "lang_pair", ".", "split", "(", "'-'", ")", "\n", "sample_key", "=", "_get_denoising_dataset_key", "(", "lang_pair", ")", "\n", "forward_backward", "(", "model", ".", "models", "[", "'{0}-{0}'", ".", "format", "(", "tgt", ")", "]", ",", "sample", "[", "sample_key", "]", ",", "sample_key", ",", "self", ".", "lambda_denoising", ")", "\n", "\n", "", "", "return", "agg_loss", ",", "agg_sample_size", ",", "agg_logging_output", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.semisupervised_translation.SemisupervisedTranslationTask.update_step": [[367, 388], ["semisupervised_translation.SemisupervisedTranslationTask.update_step.lambda_step_func"], "methods", ["None"], ["", "def", "update_step", "(", "self", ",", "num_updates", ")", ":", "\n", "        ", "def", "lambda_step_func", "(", "config", ",", "n_iter", ")", ":", "\n", "            ", "\"\"\"\n            Update a lambda value according to its schedule configuration.\n            \"\"\"", "\n", "ranges", "=", "[", "i", "for", "i", "in", "range", "(", "len", "(", "config", ")", "-", "1", ")", "if", "config", "[", "i", "]", "[", "0", "]", "<=", "n_iter", "<", "config", "[", "i", "+", "1", "]", "[", "0", "]", "]", "\n", "if", "len", "(", "ranges", ")", "==", "0", ":", "\n", "                ", "assert", "n_iter", ">=", "config", "[", "-", "1", "]", "[", "0", "]", "\n", "return", "config", "[", "-", "1", "]", "[", "1", "]", "\n", "", "assert", "len", "(", "ranges", ")", "==", "1", "\n", "i", "=", "ranges", "[", "0", "]", "\n", "x_a", ",", "y_a", "=", "config", "[", "i", "]", "\n", "x_b", ",", "y_b", "=", "config", "[", "i", "+", "1", "]", "\n", "return", "y_a", "+", "(", "n_iter", "-", "x_a", ")", "*", "float", "(", "y_b", "-", "y_a", ")", "/", "float", "(", "x_b", "-", "x_a", ")", "\n", "\n", "", "if", "self", ".", "lambda_parallel_steps", "is", "not", "None", ":", "\n", "            ", "self", ".", "lambda_parallel", "=", "lambda_step_func", "(", "self", ".", "lambda_parallel_steps", ",", "num_updates", ")", "\n", "", "if", "self", ".", "lambda_denoising_steps", "is", "not", "None", ":", "\n", "            ", "self", ".", "lambda_denoising", "=", "lambda_step_func", "(", "self", ".", "lambda_denoising_steps", ",", "num_updates", ")", "\n", "", "if", "self", ".", "lambda_otf_bt_steps", "is", "not", "None", ":", "\n", "            ", "self", ".", "lambda_otf_bt", "=", "lambda_step_func", "(", "self", ".", "lambda_otf_bt_steps", ",", "num_updates", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.semisupervised_translation.SemisupervisedTranslationTask.aggregate_logging_outputs": [[389, 405], ["set", "logging_output_keys.intersection.intersection.intersection", "super().aggregate_logging_outputs", "semisupervised_translation._get_denoising_dataset_key", "semisupervised_translation._get_bt_dataset_key"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.binary_cross_entropy.BinaryCrossEntropyCriterion.aggregate_logging_outputs", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.semisupervised_translation._get_denoising_dataset_key", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.semisupervised_translation._get_bt_dataset_key"], ["", "", "def", "aggregate_logging_outputs", "(", "self", ",", "logging_outputs", ",", "criterion", ")", ":", "\n", "# aggregate logging outputs for each language pair", "\n", "        ", "logging_output_keys", "=", "{", "\n", "key", "\n", "for", "logging_output", "in", "logging_outputs", "\n", "for", "key", "in", "logging_output", "\n", "}", "\n", "lang_pair_keys", "=", "set", "(", "self", ".", "lang_pairs", "+", "[", "\n", "_get_bt_dataset_key", "(", "lang_pair", ")", "\n", "for", "lang_pair", "in", "self", ".", "lang_pairs", "\n", "]", "+", "[", "\n", "_get_denoising_dataset_key", "(", "lang_pair", ")", "\n", "for", "lang_pair", "in", "self", ".", "lang_pairs", "\n", "]", ")", "\n", "logging_output_keys", "=", "logging_output_keys", ".", "intersection", "(", "lang_pair_keys", ")", "\n", "return", "super", "(", ")", ".", "aggregate_logging_outputs", "(", "logging_outputs", ",", "criterion", ",", "logging_output_keys", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.semisupervised_translation._get_bt_dataset_key": [[26, 28], ["None"], "function", ["None"], ["def", "_get_bt_dataset_key", "(", "lang_pair", ")", ":", "\n", "    ", "return", "\"bt:\"", "+", "lang_pair", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.semisupervised_translation._get_denoising_dataset_key": [[30, 32], ["None"], "function", ["None"], ["", "def", "_get_denoising_dataset_key", "(", "lang_pair", ")", ":", "\n", "    ", "return", "\"denoising:\"", "+", "lang_pair", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.semisupervised_translation.parse_lambda_config": [[35, 53], ["x.split", "len", "all", "all", "all", "float", "s.split", "float", "k.isdigit", "len", "int", "int", "range", "int", "float", "len"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["", "def", "parse_lambda_config", "(", "x", ")", ":", "\n", "    ", "\"\"\"\n    Parse the configuration of lambda coefficient (for scheduling).\n    x = \"3\"                  # lambda will be a constant equal to x\n    x = \"0:1,1000:0\"         # lambda will start from 1 and linearly decrease\n                             # to 0 during the first 1000 iterations\n    x = \"0:0,1000:0,2000:1\"  # lambda will be equal to 0 for the first 1000\n                             # iterations, then will linearly increase to 1 until iteration 2000\n    \"\"\"", "\n", "split", "=", "x", ".", "split", "(", "','", ")", "\n", "if", "len", "(", "split", ")", "==", "1", ":", "\n", "        ", "return", "float", "(", "x", ")", ",", "None", "\n", "", "else", ":", "\n", "        ", "split", "=", "[", "s", ".", "split", "(", "':'", ")", "for", "s", "in", "split", "]", "\n", "assert", "all", "(", "len", "(", "s", ")", "==", "2", "for", "s", "in", "split", ")", "\n", "assert", "all", "(", "k", ".", "isdigit", "(", ")", "for", "k", ",", "_", "in", "split", ")", "\n", "assert", "all", "(", "int", "(", "split", "[", "i", "]", "[", "0", "]", ")", "<", "int", "(", "split", "[", "i", "+", "1", "]", "[", "0", "]", ")", "for", "i", "in", "range", "(", "len", "(", "split", ")", "-", "1", ")", ")", "\n", "return", "float", "(", "split", "[", "0", "]", "[", "1", "]", ")", ",", "[", "(", "int", "(", "k", ")", ",", "float", "(", "v", ")", ")", "for", "k", ",", "v", "in", "split", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.multilingual_translation.MultilingualTranslationTask.add_args": [[63, 94], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add task-specific arguments to the parser.\"\"\"", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'data'", ",", "metavar", "=", "'DIR'", ",", "help", "=", "'path to data directory'", ")", "\n", "parser", ".", "add_argument", "(", "'--lang-pairs'", ",", "default", "=", "None", ",", "metavar", "=", "'PAIRS'", ",", "\n", "help", "=", "'comma-separated list of language pairs (in training order): en-de,en-fr,de-fr'", ")", "\n", "parser", ".", "add_argument", "(", "'-s'", ",", "'--source-lang'", ",", "default", "=", "None", ",", "metavar", "=", "'SRC'", ",", "\n", "help", "=", "'source language (only needed for inference)'", ")", "\n", "parser", ".", "add_argument", "(", "'-t'", ",", "'--target-lang'", ",", "default", "=", "None", ",", "metavar", "=", "'TARGET'", ",", "\n", "help", "=", "'target language (only needed for inference)'", ")", "\n", "parser", ".", "add_argument", "(", "'--lazy-load'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'load the dataset lazily'", ")", "\n", "parser", ".", "add_argument", "(", "'--raw-text'", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'load raw text dataset'", ")", "\n", "parser", ".", "add_argument", "(", "'--left-pad-source'", ",", "default", "=", "'True'", ",", "type", "=", "str", ",", "metavar", "=", "'BOOL'", ",", "\n", "help", "=", "'pad the source on the left (default: True)'", ")", "\n", "parser", ".", "add_argument", "(", "'--left-pad-target'", ",", "default", "=", "'False'", ",", "type", "=", "str", ",", "metavar", "=", "'BOOL'", ",", "\n", "help", "=", "'pad the target on the left (default: False)'", ")", "\n", "parser", ".", "add_argument", "(", "'--max-source-positions'", ",", "default", "=", "1024", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'max number of tokens in the source sequence'", ")", "\n", "parser", ".", "add_argument", "(", "'--max-target-positions'", ",", "default", "=", "1024", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'max number of tokens in the target sequence'", ")", "\n", "parser", ".", "add_argument", "(", "'--upsample-primary'", ",", "default", "=", "1", ",", "type", "=", "int", ",", "\n", "help", "=", "'amount to upsample primary dataset'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-langtok'", ",", "default", "=", "None", ",", "type", "=", "str", ",", "choices", "=", "[", "'src'", ",", "'tgt'", "]", ",", "\n", "metavar", "=", "'SRCTGT'", ",", "\n", "help", "=", "'replace beginning-of-sentence in source sentence with source or target '", "\n", "'language token. (src/tgt)'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-langtok'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'replace beginning-of-sentence in target sentence with target language token'", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.multilingual_translation.MultilingualTranslationTask.__init__": [[96, 116], ["FairseqTask.__init__", "list", "args.lang_pairs[].split", "dicts.keys"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["", "def", "__init__", "(", "self", ",", "args", ",", "dicts", ",", "training", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ")", "\n", "self", ".", "dicts", "=", "dicts", "\n", "self", ".", "training", "=", "training", "\n", "if", "training", ":", "\n", "            ", "self", ".", "lang_pairs", "=", "args", ".", "lang_pairs", "\n", "args", ".", "source_lang", ",", "args", ".", "target_lang", "=", "args", ".", "lang_pairs", "[", "0", "]", ".", "split", "(", "'-'", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "lang_pairs", "=", "[", "'{}-{}'", ".", "format", "(", "args", ".", "source_lang", ",", "args", ".", "target_lang", ")", "]", "\n", "# eval_lang_pairs for multilingual translation is usually all of the", "\n", "# lang_pairs. However for other multitask settings or when we want to", "\n", "# optimize for certain languages we want to use a different subset. Thus", "\n", "# the eval_lang_pairs class variable is provided for classes that extend", "\n", "# this class.", "\n", "", "self", ".", "eval_lang_pairs", "=", "self", ".", "lang_pairs", "\n", "# model_lang_pairs will be used to build encoder-decoder model pairs in", "\n", "# models.build_model(). This allows multitask type of sub-class can", "\n", "# build models other than the input lang_pairs", "\n", "self", ".", "model_lang_pairs", "=", "self", ".", "lang_pairs", "\n", "self", ".", "langs", "=", "list", "(", "dicts", ".", "keys", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.multilingual_translation.MultilingualTranslationTask.setup_task": [[117, 121], ["cls.prepare", "cls"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.multilingual_translation.MultilingualTranslationTask.prepare", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.masked_lm_dictionary.BertDictionary.cls"], ["", "@", "classmethod", "\n", "def", "setup_task", "(", "cls", ",", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "dicts", ",", "training", "=", "cls", ".", "prepare", "(", "args", ",", "**", "kwargs", ")", "\n", "return", "cls", "(", "args", ",", "dicts", ",", "training", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.multilingual_translation.MultilingualTranslationTask.prepare": [[122, 157], ["fairseq.options.eval_bool", "fairseq.options.eval_bool", "getattr", "args.lang_pairs.split", "sorted", "collections.OrderedDict", "fairseq.utils.deprecation_warning", "getattr", "ValueError", "list", "args.data.split", "fairseq.data.Dictionary.load", "print", "fairseq.utils.deprecation_warning", "len", "os.path.join", "len", "dicts[].pad", "dicts[].pad", "dicts[].eos", "dicts[].eos", "dicts[].unk", "dicts[].unk", "dicts[].add_symbol", "len", "lang_pair.split", "multilingual_translation._lang_token"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.eval_bool", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.eval_bool", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.deprecation_warning", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.deprecation_warning", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.unk", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.unk", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.multilingual_translation._lang_token"], ["", "@", "classmethod", "\n", "def", "prepare", "(", "cls", ",", "args", ",", "**", "kargs", ")", ":", "\n", "        ", "args", ".", "left_pad_source", "=", "options", ".", "eval_bool", "(", "args", ".", "left_pad_source", ")", "\n", "args", ".", "left_pad_target", "=", "options", ".", "eval_bool", "(", "args", ".", "left_pad_target", ")", "\n", "if", "getattr", "(", "args", ",", "'raw_text'", ",", "False", ")", ":", "\n", "            ", "utils", ".", "deprecation_warning", "(", "'--raw-text is deprecated, please use --dataset-impl=raw'", ")", "\n", "args", ".", "dataset_impl", "=", "'raw'", "\n", "", "elif", "getattr", "(", "args", ",", "'lazy_load'", ",", "False", ")", ":", "\n", "            ", "utils", ".", "deprecation_warning", "(", "'--lazy-load is deprecated, please use --dataset-impl=lazy'", ")", "\n", "args", ".", "dataset_impl", "=", "'lazy'", "\n", "\n", "", "if", "args", ".", "lang_pairs", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "'--lang-pairs is required. List all the language pairs in the training objective.'", ")", "\n", "", "args", ".", "lang_pairs", "=", "args", ".", "lang_pairs", ".", "split", "(", "','", ")", "\n", "sorted_langs", "=", "sorted", "(", "list", "(", "{", "x", "for", "lang_pair", "in", "args", ".", "lang_pairs", "for", "x", "in", "lang_pair", ".", "split", "(", "'-'", ")", "}", ")", ")", "\n", "if", "args", ".", "source_lang", "is", "not", "None", "or", "args", ".", "target_lang", "is", "not", "None", ":", "\n", "            ", "training", "=", "False", "\n", "", "else", ":", "\n", "            ", "training", "=", "True", "\n", "\n", "# load dictionaries", "\n", "", "dicts", "=", "OrderedDict", "(", ")", "\n", "for", "lang", "in", "sorted_langs", ":", "\n", "            ", "paths", "=", "args", ".", "data", ".", "split", "(", "':'", ")", "\n", "assert", "len", "(", "paths", ")", ">", "0", "\n", "dicts", "[", "lang", "]", "=", "Dictionary", ".", "load", "(", "os", ".", "path", ".", "join", "(", "paths", "[", "0", "]", ",", "'dict.{}.txt'", ".", "format", "(", "lang", ")", ")", ")", "\n", "if", "len", "(", "dicts", ")", ">", "0", ":", "\n", "                ", "assert", "dicts", "[", "lang", "]", ".", "pad", "(", ")", "==", "dicts", "[", "sorted_langs", "[", "0", "]", "]", ".", "pad", "(", ")", "\n", "assert", "dicts", "[", "lang", "]", ".", "eos", "(", ")", "==", "dicts", "[", "sorted_langs", "[", "0", "]", "]", ".", "eos", "(", ")", "\n", "assert", "dicts", "[", "lang", "]", ".", "unk", "(", ")", "==", "dicts", "[", "sorted_langs", "[", "0", "]", "]", ".", "unk", "(", ")", "\n", "", "if", "args", ".", "encoder_langtok", "is", "not", "None", "or", "args", ".", "decoder_langtok", ":", "\n", "                ", "for", "lang_to_add", "in", "sorted_langs", ":", "\n", "                    ", "dicts", "[", "lang", "]", ".", "add_symbol", "(", "_lang_token", "(", "lang_to_add", ")", ")", "\n", "", "", "print", "(", "'| [{}] dictionary: {} types'", ".", "format", "(", "lang", ",", "len", "(", "dicts", "[", "lang", "]", ")", ")", ")", "\n", "", "return", "dicts", ",", "training", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.multilingual_translation.MultilingualTranslationTask.get_encoder_langtok": [[158, 165], ["multilingual_translation.MultilingualTranslationTask.dicts[].eos", "multilingual_translation._lang_token_index", "multilingual_translation._lang_token_index"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.multilingual_translation._lang_token_index", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.multilingual_translation._lang_token_index"], ["", "def", "get_encoder_langtok", "(", "self", ",", "src_lang", ",", "tgt_lang", ")", ":", "\n", "        ", "if", "self", ".", "args", ".", "encoder_langtok", "is", "None", ":", "\n", "            ", "return", "self", ".", "dicts", "[", "src_lang", "]", ".", "eos", "(", ")", "\n", "", "if", "self", ".", "args", ".", "encoder_langtok", "==", "'src'", ":", "\n", "            ", "return", "_lang_token_index", "(", "self", ".", "dicts", "[", "src_lang", "]", ",", "src_lang", ")", "\n", "", "else", ":", "\n", "            ", "return", "_lang_token_index", "(", "self", ".", "dicts", "[", "src_lang", "]", ",", "tgt_lang", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.multilingual_translation.MultilingualTranslationTask.get_decoder_langtok": [[166, 170], ["multilingual_translation._lang_token_index", "multilingual_translation.MultilingualTranslationTask.dicts[].eos"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.multilingual_translation._lang_token_index", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos"], ["", "", "def", "get_decoder_langtok", "(", "self", ",", "tgt_lang", ")", ":", "\n", "        ", "if", "not", "self", ".", "args", ".", "decoder_langtok", ":", "\n", "            ", "return", "self", ".", "dicts", "[", "tgt_lang", "]", ".", "eos", "(", ")", "\n", "", "return", "_lang_token_index", "(", "self", ".", "dicts", "[", "tgt_lang", "]", ",", "tgt_lang", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.multilingual_translation.MultilingualTranslationTask.alter_dataset_langtok": [[171, 195], ["fairseq.data.TransformEosLangPairDataset", "multilingual_translation.MultilingualTranslationTask.get_encoder_langtok", "multilingual_translation.MultilingualTranslationTask.get_decoder_langtok"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.multilingual_translation.MultilingualTranslationTask.get_encoder_langtok", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.multilingual_translation.MultilingualTranslationTask.get_decoder_langtok"], ["", "def", "alter_dataset_langtok", "(", "self", ",", "lang_pair_dataset", ",", "\n", "src_eos", "=", "None", ",", "src_lang", "=", "None", ",", "tgt_eos", "=", "None", ",", "tgt_lang", "=", "None", ")", ":", "\n", "        ", "if", "self", ".", "args", ".", "encoder_langtok", "is", "None", "and", "not", "self", ".", "args", ".", "decoder_langtok", ":", "\n", "            ", "return", "lang_pair_dataset", "\n", "\n", "", "new_src_eos", "=", "None", "\n", "if", "self", ".", "args", ".", "encoder_langtok", "is", "not", "None", "and", "src_eos", "is", "not", "None", "and", "src_lang", "is", "not", "None", "and", "tgt_lang", "is", "not", "None", ":", "\n", "            ", "new_src_eos", "=", "self", ".", "get_encoder_langtok", "(", "src_lang", ",", "tgt_lang", ")", "\n", "", "else", ":", "\n", "            ", "src_eos", "=", "None", "\n", "\n", "", "new_tgt_bos", "=", "None", "\n", "if", "self", ".", "args", ".", "decoder_langtok", "and", "tgt_eos", "is", "not", "None", "and", "tgt_lang", "is", "not", "None", ":", "\n", "            ", "new_tgt_bos", "=", "self", ".", "get_decoder_langtok", "(", "tgt_lang", ")", "\n", "", "else", ":", "\n", "            ", "tgt_eos", "=", "None", "\n", "\n", "", "return", "TransformEosLangPairDataset", "(", "\n", "lang_pair_dataset", ",", "\n", "src_eos", "=", "src_eos", ",", "\n", "new_src_eos", "=", "new_src_eos", ",", "\n", "tgt_bos", "=", "tgt_eos", ",", "\n", "new_tgt_bos", "=", "new_tgt_bos", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.multilingual_translation.MultilingualTranslationTask.load_dataset": [[197, 228], ["multilingual_translation.MultilingualTranslationTask.args.data.split", "fairseq.data.RoundRobinZipDatasets", "len", "lang_pair.split", "fairseq.tasks.translation.load_langpair_dataset", "multilingual_translation.MultilingualTranslationTask.alter_dataset_langtok", "collections.OrderedDict", "len", "multilingual_translation.MultilingualTranslationTask.dicts[].eos", "multilingual_translation.MultilingualTranslationTask.load_dataset.language_pair_dataset"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.translation.load_langpair_dataset", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.multilingual_translation.MultilingualTranslationTask.alter_dataset_langtok", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos"], ["", "def", "load_dataset", "(", "self", ",", "split", ",", "epoch", "=", "0", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Load a dataset split.\"\"\"", "\n", "\n", "paths", "=", "self", ".", "args", ".", "data", ".", "split", "(", "':'", ")", "\n", "assert", "len", "(", "paths", ")", ">", "0", "\n", "data_path", "=", "paths", "[", "epoch", "%", "len", "(", "paths", ")", "]", "\n", "\n", "def", "language_pair_dataset", "(", "lang_pair", ")", ":", "\n", "            ", "src", ",", "tgt", "=", "lang_pair", ".", "split", "(", "'-'", ")", "\n", "langpair_dataset", "=", "load_langpair_dataset", "(", "\n", "data_path", ",", "split", ",", "src", ",", "self", ".", "dicts", "[", "src", "]", ",", "tgt", ",", "self", ".", "dicts", "[", "tgt", "]", ",", "\n", "combine", "=", "True", ",", "dataset_impl", "=", "self", ".", "args", ".", "dataset_impl", ",", "\n", "upsample_primary", "=", "self", ".", "args", ".", "upsample_primary", ",", "\n", "left_pad_source", "=", "self", ".", "args", ".", "left_pad_source", ",", "\n", "left_pad_target", "=", "self", ".", "args", ".", "left_pad_target", ",", "\n", "max_source_positions", "=", "self", ".", "args", ".", "max_source_positions", ",", "\n", "max_target_positions", "=", "self", ".", "args", ".", "max_target_positions", ",", "\n", ")", "\n", "return", "self", ".", "alter_dataset_langtok", "(", "\n", "langpair_dataset", ",", "\n", "src_eos", "=", "self", ".", "dicts", "[", "tgt", "]", ".", "eos", "(", ")", ",", "\n", "src_lang", "=", "src", ",", "\n", "tgt_lang", "=", "tgt", ",", "\n", ")", "\n", "\n", "", "self", ".", "datasets", "[", "split", "]", "=", "RoundRobinZipDatasets", "(", "\n", "OrderedDict", "(", "[", "\n", "(", "lang_pair", ",", "language_pair_dataset", "(", "lang_pair", ")", ")", "\n", "for", "lang_pair", "in", "self", ".", "lang_pairs", "\n", "]", ")", ",", "\n", "eval_key", "=", "None", "if", "self", ".", "training", "else", "\"%s-%s\"", "%", "(", "self", ".", "args", ".", "source_lang", ",", "self", ".", "args", ".", "target_lang", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.multilingual_translation.MultilingualTranslationTask.build_dataset_for_inference": [[230, 246], ["fairseq.data.RoundRobinZipDatasets", "collections.OrderedDict", "multilingual_translation.MultilingualTranslationTask.alter_dataset_langtok", "fairseq.data.LanguagePairDataset", "multilingual_translation.MultilingualTranslationTask.source_dictionary.eos"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.multilingual_translation.MultilingualTranslationTask.alter_dataset_langtok", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos"], ["", "def", "build_dataset_for_inference", "(", "self", ",", "src_tokens", ",", "src_lengths", ")", ":", "\n", "        ", "lang_pair", "=", "\"%s-%s\"", "%", "(", "self", ".", "args", ".", "source_lang", ",", "self", ".", "args", ".", "target_lang", ")", "\n", "return", "RoundRobinZipDatasets", "(", "\n", "OrderedDict", "(", "[", "(", "\n", "lang_pair", ",", "\n", "self", ".", "alter_dataset_langtok", "(", "\n", "LanguagePairDataset", "(", "\n", "src_tokens", ",", "src_lengths", ",", "\n", "self", ".", "source_dictionary", "\n", ")", ",", "\n", "src_eos", "=", "self", ".", "source_dictionary", ".", "eos", "(", ")", ",", "\n", "src_lang", "=", "self", ".", "args", ".", "source_lang", ",", "\n", "tgt_lang", "=", "self", ".", "args", ".", "target_lang", ",", "\n", ")", ",", "\n", ")", "]", ")", ",", "\n", "eval_key", "=", "lang_pair", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.multilingual_translation.MultilingualTranslationTask.build_model": [[248, 269], ["multilingual_translation.MultilingualTranslationTask.build_model.check_args"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.speech_recognition.infer.check_args"], ["", "def", "build_model", "(", "self", ",", "args", ")", ":", "\n", "        ", "def", "check_args", "(", ")", ":", "\n", "            ", "messages", "=", "[", "]", "\n", "if", "len", "(", "set", "(", "self", ".", "args", ".", "lang_pairs", ")", ".", "symmetric_difference", "(", "args", ".", "lang_pairs", ")", ")", "!=", "0", ":", "\n", "                ", "messages", ".", "append", "(", "'--lang-pairs should include all the language pairs {}.'", ".", "format", "(", "args", ".", "lang_pairs", ")", ")", "\n", "", "if", "self", ".", "args", ".", "encoder_langtok", "!=", "args", ".", "encoder_langtok", ":", "\n", "                ", "messages", ".", "append", "(", "'--encoder-langtok should be {}.'", ".", "format", "(", "args", ".", "encoder_langtok", ")", ")", "\n", "", "if", "self", ".", "args", ".", "decoder_langtok", "!=", "args", ".", "decoder_langtok", ":", "\n", "                ", "messages", ".", "append", "(", "'--decoder-langtok should {} be set.'", ".", "format", "(", "\"\"", "if", "args", ".", "decoder_langtok", "else", "\"not\"", ")", ")", "\n", "\n", "", "if", "len", "(", "messages", ")", ">", "0", ":", "\n", "                ", "raise", "ValueError", "(", "' '", ".", "join", "(", "messages", ")", ")", "\n", "\n", "# Check if task args are consistant with model args", "\n", "", "", "check_args", "(", ")", "\n", "\n", "from", "fairseq", "import", "models", "\n", "model", "=", "models", ".", "build_model", "(", "args", ",", "self", ")", "\n", "if", "not", "isinstance", "(", "model", ",", "FairseqMultiModel", ")", ":", "\n", "            ", "raise", "ValueError", "(", "'MultilingualTranslationTask requires a FairseqMultiModel architecture'", ")", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.multilingual_translation.MultilingualTranslationTask.train_step": [[270, 285], ["model.train", "criterion", "optimizer.backward", "loss.detach().item", "len", "loss.detach"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq-stack-transformer.train.train", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.scalar_bias.ScalarBias.backward", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item"], ["", "def", "train_step", "(", "self", ",", "sample", ",", "model", ",", "criterion", ",", "optimizer", ",", "ignore_grad", "=", "False", ")", ":", "\n", "        ", "model", ".", "train", "(", ")", "\n", "agg_loss", ",", "agg_sample_size", ",", "agg_logging_output", "=", "0.", ",", "0.", ",", "{", "}", "\n", "for", "lang_pair", "in", "self", ".", "model_lang_pairs", ":", "\n", "            ", "if", "sample", "[", "lang_pair", "]", "is", "None", "or", "len", "(", "sample", "[", "lang_pair", "]", ")", "==", "0", ":", "\n", "                ", "continue", "\n", "", "loss", ",", "sample_size", ",", "logging_output", "=", "criterion", "(", "model", ".", "models", "[", "lang_pair", "]", ",", "sample", "[", "lang_pair", "]", ")", "\n", "if", "ignore_grad", ":", "\n", "                ", "loss", "*=", "0", "\n", "", "optimizer", ".", "backward", "(", "loss", ")", "\n", "agg_loss", "+=", "loss", ".", "detach", "(", ")", ".", "item", "(", ")", "\n", "# TODO make summing of the sample sizes configurable", "\n", "agg_sample_size", "+=", "sample_size", "\n", "agg_logging_output", "[", "lang_pair", "]", "=", "logging_output", "\n", "", "return", "agg_loss", ",", "agg_sample_size", ",", "agg_logging_output", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.multilingual_translation.MultilingualTranslationTask.valid_step": [[286, 299], ["model.eval", "torch.no_grad", "criterion", "loss.data.item", "len"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item"], ["", "def", "valid_step", "(", "self", ",", "sample", ",", "model", ",", "criterion", ")", ":", "\n", "        ", "model", ".", "eval", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "agg_loss", ",", "agg_sample_size", ",", "agg_logging_output", "=", "0.", ",", "0.", ",", "{", "}", "\n", "for", "lang_pair", "in", "self", ".", "eval_lang_pairs", ":", "\n", "                ", "if", "lang_pair", "not", "in", "sample", "or", "sample", "[", "lang_pair", "]", "is", "None", "or", "len", "(", "sample", "[", "lang_pair", "]", ")", "==", "0", ":", "\n", "                    ", "continue", "\n", "", "loss", ",", "sample_size", ",", "logging_output", "=", "criterion", "(", "model", ".", "models", "[", "lang_pair", "]", ",", "sample", "[", "lang_pair", "]", ")", "\n", "agg_loss", "+=", "loss", ".", "data", ".", "item", "(", ")", "\n", "# TODO make summing of the sample sizes configurable", "\n", "agg_sample_size", "+=", "sample_size", "\n", "agg_logging_output", "[", "lang_pair", "]", "=", "logging_output", "\n", "", "", "return", "agg_loss", ",", "agg_sample_size", ",", "agg_logging_output", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.multilingual_translation.MultilingualTranslationTask.inference_step": [[300, 308], ["torch.no_grad", "generator.generate", "multilingual_translation._lang_token_index", "multilingual_translation.MultilingualTranslationTask.target_dictionary.eos"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.sequence_generator.SequenceGenerator.generate", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.multilingual_translation._lang_token_index", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos"], ["", "def", "inference_step", "(", "self", ",", "generator", ",", "models", ",", "sample", ",", "prefix_tokens", "=", "None", ")", ":", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "return", "generator", ".", "generate", "(", "\n", "models", ",", "\n", "sample", ",", "\n", "prefix_tokens", "=", "prefix_tokens", ",", "\n", "bos_token", "=", "_lang_token_index", "(", "self", ".", "target_dictionary", ",", "self", ".", "args", ".", "target_lang", ")", "\n", "if", "self", ".", "args", ".", "decoder_langtok", "else", "self", ".", "target_dictionary", ".", "eos", "(", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.multilingual_translation.MultilingualTranslationTask.init_logging_output": [[310, 320], ["sum", "sum", "sample_lang.get", "sample.values", "sample_lang[].size", "sample.values"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "", "def", "init_logging_output", "(", "self", ",", "sample", ")", ":", "\n", "        ", "return", "{", "\n", "'ntokens'", ":", "sum", "(", "\n", "sample_lang", ".", "get", "(", "'ntokens'", ",", "0", ")", "\n", "for", "sample_lang", "in", "sample", ".", "values", "(", ")", "\n", ")", "if", "sample", "is", "not", "None", "else", "0", ",", "\n", "'nsentences'", ":", "sum", "(", "\n", "sample_lang", "[", "'target'", "]", ".", "size", "(", "0", ")", "if", "'target'", "in", "sample_lang", "else", "0", "\n", "for", "sample_lang", "in", "sample", ".", "values", "(", ")", "\n", ")", "if", "sample", "is", "not", "None", "else", "0", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.multilingual_translation.MultilingualTranslationTask.grad_denom": [[322, 324], ["criterion.__class__.grad_denom"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.fairseq_criterion.FairseqCriterion.grad_denom"], ["", "def", "grad_denom", "(", "self", ",", "sample_sizes", ",", "criterion", ")", ":", "\n", "        ", "return", "criterion", ".", "__class__", ".", "grad_denom", "(", "sample_sizes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.multilingual_translation.MultilingualTranslationTask.aggregate_logging_outputs": [[325, 351], ["multilingual_translation.MultilingualTranslationTask.aggregate_logging_outputs.sum_over_languages"], "methods", ["None"], ["", "def", "aggregate_logging_outputs", "(", "self", ",", "logging_outputs", ",", "criterion", ",", "logging_output_keys", "=", "None", ")", ":", "\n", "        ", "logging_output_keys", "=", "logging_output_keys", "or", "self", ".", "eval_lang_pairs", "\n", "# aggregate logging outputs for each language pair", "\n", "agg_logging_outputs", "=", "{", "\n", "key", ":", "criterion", ".", "__class__", ".", "aggregate_logging_outputs", "(", "[", "\n", "logging_output", ".", "get", "(", "key", ",", "{", "}", ")", "for", "logging_output", "in", "logging_outputs", "\n", "]", ")", "\n", "for", "key", "in", "logging_output_keys", "\n", "}", "\n", "\n", "def", "sum_over_languages", "(", "key", ")", ":", "\n", "            ", "return", "sum", "(", "logging_output", "[", "key", "]", "for", "logging_output", "in", "agg_logging_outputs", ".", "values", "(", ")", ")", "\n", "\n", "# flatten logging outputs", "\n", "", "flat_logging_output", "=", "{", "\n", "'{}:{}'", ".", "format", "(", "lang_pair", ",", "k", ")", ":", "v", "\n", "for", "lang_pair", ",", "agg_logging_output", "in", "agg_logging_outputs", ".", "items", "(", ")", "\n", "for", "k", ",", "v", "in", "agg_logging_output", ".", "items", "(", ")", "\n", "}", "\n", "flat_logging_output", "[", "'loss'", "]", "=", "sum_over_languages", "(", "'loss'", ")", "\n", "if", "any", "(", "'nll_loss'", "in", "logging_output", "for", "logging_output", "in", "agg_logging_outputs", ".", "values", "(", ")", ")", ":", "\n", "            ", "flat_logging_output", "[", "'nll_loss'", "]", "=", "sum_over_languages", "(", "'nll_loss'", ")", "\n", "", "flat_logging_output", "[", "'sample_size'", "]", "=", "sum_over_languages", "(", "'sample_size'", ")", "\n", "flat_logging_output", "[", "'nsentences'", "]", "=", "sum_over_languages", "(", "'nsentences'", ")", "\n", "flat_logging_output", "[", "'ntokens'", "]", "=", "sum_over_languages", "(", "'ntokens'", ")", "\n", "return", "flat_logging_output", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.multilingual_translation.MultilingualTranslationTask.source_dictionary": [[352, 355], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "source_dictionary", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "dicts", "[", "self", ".", "args", ".", "source_lang", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.multilingual_translation.MultilingualTranslationTask.target_dictionary": [[356, 359], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "target_dictionary", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "dicts", "[", "self", ".", "args", ".", "target_lang", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.multilingual_translation.MultilingualTranslationTask.max_positions": [[360, 369], ["collections.OrderedDict", "len", "multilingual_translation.MultilingualTranslationTask.datasets.values", "multilingual_translation.MultilingualTranslationTask.datasets.keys", "multilingual_translation.MultilingualTranslationTask.datasets[].datasets.keys"], "methods", ["None"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the max sentence length allowed by the task.\"\"\"", "\n", "if", "len", "(", "self", ".", "datasets", ".", "values", "(", ")", ")", "==", "0", ":", "\n", "            ", "return", "{", "'%s-%s'", "%", "(", "self", ".", "args", ".", "source_lang", ",", "self", ".", "args", ".", "target_lang", ")", ":", "\n", "(", "self", ".", "args", ".", "max_source_positions", ",", "self", ".", "args", ".", "max_target_positions", ")", "}", "\n", "", "return", "OrderedDict", "(", "[", "\n", "(", "key", ",", "(", "self", ".", "args", ".", "max_source_positions", ",", "self", ".", "args", ".", "max_target_positions", ")", ")", "\n", "for", "split", "in", "self", ".", "datasets", ".", "keys", "(", ")", "\n", "for", "key", "in", "self", ".", "datasets", "[", "split", "]", ".", "datasets", ".", "keys", "(", ")", "\n", "]", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.multilingual_translation._lang_token": [[25, 27], ["None"], "function", ["None"], ["def", "_lang_token", "(", "lang", ":", "str", ")", ":", "\n", "    ", "return", "'__{}__'", ".", "format", "(", "lang", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.multilingual_translation._lang_token_index": [[29, 35], ["dic.index", "multilingual_translation._lang_token"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.index", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.multilingual_translation._lang_token"], ["", "def", "_lang_token_index", "(", "dic", ":", "Dictionary", ",", "lang", ":", "str", ")", ":", "\n", "    ", "\"\"\"Return language token index.\"\"\"", "\n", "idx", "=", "dic", ".", "index", "(", "_lang_token", "(", "lang", ")", ")", "\n", "assert", "idx", "!=", "dic", ".", "unk_index", ",", "'cannot find language token for lang {}'", ".", "format", "(", "lang", ")", "\n", "return", "idx", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.legacy_masked_lm.LegacyMaskedLMTask.add_args": [[33, 43], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add task-specific arguments to the parser.\"\"\"", "\n", "parser", ".", "add_argument", "(", "'data'", ",", "help", "=", "'colon separated path to data directories list, \\\n                            will be iterated upon during epochs in round-robin manner'", ")", "\n", "parser", ".", "add_argument", "(", "'--tokens-per-sample'", ",", "default", "=", "512", ",", "type", "=", "int", ",", "\n", "help", "=", "'max number of total tokens over all segments'", "\n", "' per sample for BERT dataset'", ")", "\n", "parser", ".", "add_argument", "(", "'--break-mode'", ",", "default", "=", "\"doc\"", ",", "type", "=", "str", ",", "help", "=", "'mode for breaking sentence'", ")", "\n", "parser", ".", "add_argument", "(", "'--shuffle-dataset'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.legacy_masked_lm.LegacyMaskedLMTask.__init__": [[44, 48], ["FairseqTask.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["", "def", "__init__", "(", "self", ",", "args", ",", "dictionary", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ")", "\n", "self", ".", "dictionary", "=", "dictionary", "\n", "self", ".", "seed", "=", "args", ".", "seed", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.legacy_masked_lm.LegacyMaskedLMTask.load_dictionary": [[49, 52], ["fairseq.data.legacy.masked_lm_dictionary.BertDictionary.load"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load"], ["", "@", "classmethod", "\n", "def", "load_dictionary", "(", "cls", ",", "filename", ")", ":", "\n", "        ", "return", "BertDictionary", ".", "load", "(", "filename", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.legacy_masked_lm.LegacyMaskedLMTask.build_dictionary": [[53, 60], ["fairseq.data.legacy.masked_lm_dictionary.BertDictionary", "fairseq.data.legacy.masked_lm_dictionary.BertDictionary.finalize", "fairseq.data.Dictionary.add_file_to_dictionary"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDatasetBuilder.finalize", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_file_to_dictionary"], ["", "@", "classmethod", "\n", "def", "build_dictionary", "(", "cls", ",", "filenames", ",", "workers", "=", "1", ",", "threshold", "=", "-", "1", ",", "nwords", "=", "-", "1", ",", "padding_factor", "=", "8", ")", ":", "\n", "        ", "d", "=", "BertDictionary", "(", ")", "\n", "for", "filename", "in", "filenames", ":", "\n", "            ", "Dictionary", ".", "add_file_to_dictionary", "(", "filename", ",", "d", ",", "tokenizer", ".", "tokenize_line", ",", "workers", ")", "\n", "", "d", ".", "finalize", "(", "threshold", "=", "threshold", ",", "nwords", "=", "nwords", ",", "padding_factor", "=", "padding_factor", ")", "\n", "return", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.legacy_masked_lm.LegacyMaskedLMTask.target_dictionary": [[61, 64], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "target_dictionary", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "dictionary", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.legacy_masked_lm.LegacyMaskedLMTask.setup_task": [[65, 75], ["args.data.split", "fairseq.data.legacy.masked_lm_dictionary.BertDictionary.load", "print", "cls", "len", "os.path.join", "len"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.masked_lm_dictionary.BertDictionary.cls"], ["", "@", "classmethod", "\n", "def", "setup_task", "(", "cls", ",", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Setup the task.\n        \"\"\"", "\n", "paths", "=", "args", ".", "data", ".", "split", "(", "':'", ")", "\n", "assert", "len", "(", "paths", ")", ">", "0", "\n", "dictionary", "=", "BertDictionary", ".", "load", "(", "os", ".", "path", ".", "join", "(", "paths", "[", "0", "]", ",", "'dict.txt'", ")", ")", "\n", "print", "(", "'| dictionary: {} types'", ".", "format", "(", "len", "(", "dictionary", ")", ")", ")", "\n", "\n", "return", "cls", "(", "args", ",", "dictionary", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.legacy_masked_lm.LegacyMaskedLMTask.load_dataset": [[76, 138], ["legacy_masked_lm.LegacyMaskedLMTask.args.data.split", "print", "itertools.count", "fairseq.data.legacy.masked_lm_dataset.MaskedLMDataset", "len", "os.path.join", "fairseq.data.indexed_dataset.make_dataset", "print", "len", "fairseq.data.ConcatDataset", "numpy.concatenate", "fairseq.data.data_utils.numpy_seed", "loaded_datasets.append", "legacy_masked_lm.LegacyMaskedLMTask.dictionary.pad", "legacy_masked_lm.LegacyMaskedLMTask.dictionary.mask", "legacy_masked_lm.LegacyMaskedLMTask.dictionary.cls", "legacy_masked_lm.LegacyMaskedLMTask.dictionary.sep", "len", "str", "FileNotFoundError", "fairseq.data.legacy.block_pair_dataset.BlockPairDataset", "len"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.make_dataset", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.data_utils.numpy_seed", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.masked_lm_dictionary.MaskedLMDictionary.mask", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.masked_lm_dictionary.BertDictionary.cls", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.masked_lm_dictionary.BertDictionary.sep"], ["", "def", "load_dataset", "(", "self", ",", "split", ",", "epoch", "=", "0", ",", "combine", "=", "False", ")", ":", "\n", "        ", "\"\"\"Load a given dataset split.\n        Args:\n            split (str): name of the split (e.g., train, valid, test)\n        \"\"\"", "\n", "loaded_datasets", "=", "[", "]", "\n", "\n", "paths", "=", "self", ".", "args", ".", "data", ".", "split", "(", "':'", ")", "\n", "assert", "len", "(", "paths", ")", ">", "0", "\n", "data_path", "=", "paths", "[", "epoch", "%", "len", "(", "paths", ")", "]", "\n", "print", "(", "\"| data_path\"", ",", "data_path", ")", "\n", "\n", "for", "k", "in", "itertools", ".", "count", "(", ")", ":", "\n", "            ", "split_k", "=", "split", "+", "(", "str", "(", "k", ")", "if", "k", ">", "0", "else", "''", ")", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "split_k", ")", "\n", "ds", "=", "indexed_dataset", ".", "make_dataset", "(", "\n", "path", ",", "\n", "impl", "=", "self", ".", "args", ".", "dataset_impl", ",", "\n", "fix_lua_indexing", "=", "True", ",", "\n", "dictionary", "=", "self", ".", "dictionary", ",", "\n", ")", "\n", "\n", "if", "ds", "is", "None", ":", "\n", "                ", "if", "k", ">", "0", ":", "\n", "                    ", "break", "\n", "", "else", ":", "\n", "                    ", "raise", "FileNotFoundError", "(", "'Dataset not found: {} ({})'", ".", "format", "(", "split", ",", "data_path", ")", ")", "\n", "\n", "", "", "with", "data_utils", ".", "numpy_seed", "(", "self", ".", "seed", "+", "k", ")", ":", "\n", "                ", "loaded_datasets", ".", "append", "(", "\n", "BlockPairDataset", "(", "\n", "ds", ",", "\n", "self", ".", "dictionary", ",", "\n", "ds", ".", "sizes", ",", "\n", "self", ".", "args", ".", "tokens_per_sample", ",", "\n", "break_mode", "=", "self", ".", "args", ".", "break_mode", ",", "\n", "doc_break_size", "=", "1", ",", "\n", ")", "\n", ")", "\n", "\n", "", "print", "(", "'| {} {} {} examples'", ".", "format", "(", "data_path", ",", "split_k", ",", "len", "(", "loaded_datasets", "[", "-", "1", "]", ")", ")", ")", "\n", "\n", "if", "not", "combine", ":", "\n", "                ", "break", "\n", "\n", "", "", "if", "len", "(", "loaded_datasets", ")", "==", "1", ":", "\n", "            ", "dataset", "=", "loaded_datasets", "[", "0", "]", "\n", "sizes", "=", "dataset", ".", "sizes", "\n", "", "else", ":", "\n", "            ", "dataset", "=", "ConcatDataset", "(", "loaded_datasets", ")", "\n", "sizes", "=", "np", ".", "concatenate", "(", "[", "ds", ".", "sizes", "for", "ds", "in", "loaded_datasets", "]", ")", "\n", "\n", "", "self", ".", "datasets", "[", "split", "]", "=", "MaskedLMDataset", "(", "\n", "dataset", "=", "dataset", ",", "\n", "sizes", "=", "sizes", ",", "\n", "vocab", "=", "self", ".", "dictionary", ",", "\n", "pad_idx", "=", "self", ".", "dictionary", ".", "pad", "(", ")", ",", "\n", "mask_idx", "=", "self", ".", "dictionary", ".", "mask", "(", ")", ",", "\n", "classif_token_idx", "=", "self", ".", "dictionary", ".", "cls", "(", ")", ",", "\n", "sep_token_idx", "=", "self", ".", "dictionary", ".", "sep", "(", ")", ",", "\n", "shuffle", "=", "self", ".", "args", ".", "shuffle_dataset", ",", "\n", "seed", "=", "self", ".", "seed", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.cross_lingual_lm.CrossLingualLMTask.add_args": [[38, 55], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add task-specific arguments to the parser.\"\"\"", "\n", "parser", ".", "add_argument", "(", "'data'", ",", "help", "=", "'colon separated path to data directories list, \\\n                            will be iterated upon during epochs in round-robin manner'", ")", "\n", "parser", ".", "add_argument", "(", "'--tokens-per-sample'", ",", "default", "=", "512", ",", "type", "=", "int", ",", "\n", "help", "=", "'max number of total tokens over all segments'", "\n", "' per sample'", ")", "\n", "parser", ".", "add_argument", "(", "'--monolingual-langs'", ",", "default", "=", "'en'", ",", "type", "=", "str", ",", "\n", "help", "=", "'comma separated list of languages for which we'", "\n", "' want to train XLM on'", ")", "\n", "parser", ".", "add_argument", "(", "'--raw-text'", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'load raw text dataset'", ")", "\n", "parser", ".", "add_argument", "(", "'--lazy-load'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'load the dataset lazily'", ")", "\n", "parser", ".", "add_argument", "(", "'--shuffle'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'shuffle each monolingual dataset while'", "\n", "' training'", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.cross_lingual_lm.CrossLingualLMTask.__init__": [[57, 63], ["FairseqTask.__init__", "cross_lingual_lm.CrossLingualLMTask._lang_to_id"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.cross_lingual_lm.CrossLingualLMTask._lang_to_id"], ["", "def", "__init__", "(", "self", ",", "args", ",", "dictionary", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ")", "\n", "self", ".", "dictionary", "=", "dictionary", "\n", "self", ".", "seed", "=", "args", ".", "seed", "\n", "self", ".", "distributed_world_size", "=", "args", ".", "distributed_world_size", "\n", "self", ".", "langs2id", "=", "self", ".", "_lang_to_id", "(", "args", ".", "monolingual_langs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.cross_lingual_lm.CrossLingualLMTask._lang_to_id": [[64, 77], ["enumerate", "l.strip", "languages.split"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["", "def", "_lang_to_id", "(", "\n", "self", ",", "\n", "languages", ":", "str", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Build a map from languages to ids. These ids are used as segment labels\n        for cross-lingual LM training.\n        \"\"\"", "\n", "lang2id", "=", "{", "}", "\n", "langs", "=", "[", "l", ".", "strip", "(", ")", "for", "l", "in", "languages", ".", "split", "(", "','", ")", "]", "\n", "for", "id", ",", "lang", "in", "enumerate", "(", "langs", ")", ":", "\n", "            ", "lang2id", "[", "lang", "]", "=", "id", "\n", "", "return", "lang2id", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.cross_lingual_lm.CrossLingualLMTask.load_dictionary": [[78, 81], ["fairseq.data.legacy.masked_lm_dictionary.MaskedLMDictionary.load"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load"], ["", "@", "classmethod", "\n", "def", "load_dictionary", "(", "cls", ",", "filename", ")", ":", "\n", "        ", "return", "MaskedLMDictionary", ".", "load", "(", "filename", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.cross_lingual_lm.CrossLingualLMTask.build_dictionary": [[82, 89], ["fairseq.data.legacy.masked_lm_dictionary.MaskedLMDictionary", "fairseq.data.legacy.masked_lm_dictionary.MaskedLMDictionary.finalize", "fairseq.data.Dictionary.add_file_to_dictionary"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDatasetBuilder.finalize", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_file_to_dictionary"], ["", "@", "classmethod", "\n", "def", "build_dictionary", "(", "cls", ",", "filenames", ",", "workers", "=", "1", ",", "threshold", "=", "-", "1", ",", "nwords", "=", "-", "1", ",", "padding_factor", "=", "8", ")", ":", "\n", "        ", "d", "=", "MaskedLMDictionary", "(", ")", "\n", "for", "filename", "in", "filenames", ":", "\n", "            ", "Dictionary", ".", "add_file_to_dictionary", "(", "filename", ",", "d", ",", "tokenizer", ".", "tokenize_line", ",", "workers", ")", "\n", "", "d", ".", "finalize", "(", "threshold", "=", "threshold", ",", "nwords", "=", "nwords", ",", "padding_factor", "=", "padding_factor", ")", "\n", "return", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.cross_lingual_lm.CrossLingualLMTask.target_dictionary": [[90, 93], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "target_dictionary", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "dictionary", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.cross_lingual_lm.CrossLingualLMTask.setup_task": [[94, 103], ["fairseq.data.legacy.masked_lm_dictionary.MaskedLMDictionary.load", "print", "cls", "os.path.join", "len"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.masked_lm_dictionary.BertDictionary.cls"], ["", "@", "classmethod", "\n", "def", "setup_task", "(", "cls", ",", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Setup the task.\n        \"\"\"", "\n", "dictionary", "=", "MaskedLMDictionary", ".", "load", "(", "os", ".", "path", ".", "join", "(", "args", ".", "data", ",", "'dict.txt'", ")", ")", "\n", "\n", "print", "(", "'| dictionary: {} types'", ".", "format", "(", "len", "(", "dictionary", ")", ")", ")", "\n", "\n", "return", "cls", "(", "args", ",", "dictionary", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.cross_lingual_lm.CrossLingualLMTask._load_single_lang_dataset": [[104, 142], ["cross_lingual_lm.CrossLingualLMTask.args.data.split", "itertools.count", "len", "os.path.join", "fairseq.data.data_utils.load_indexed_dataset", "loaded_datasets.append", "print", "len", "fairseq.data.ConcatDataset", "numpy.concatenate", "fairseq.data.TokenBlockDataset", "len", "str", "FileNotFoundError", "len", "cross_lingual_lm.CrossLingualLMTask.dictionary.pad", "cross_lingual_lm.CrossLingualLMTask.dictionary.eos"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.data_utils.load_indexed_dataset", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos"], ["", "def", "_load_single_lang_dataset", "(", "self", ",", "split", ",", "epoch", ")", ":", "\n", "        ", "loaded_datasets", "=", "[", "]", "\n", "\n", "paths", "=", "self", ".", "args", ".", "data", ".", "split", "(", "':'", ")", "\n", "assert", "len", "(", "paths", ")", ">", "0", "\n", "data_path", "=", "paths", "[", "epoch", "%", "len", "(", "paths", ")", "]", "\n", "\n", "for", "k", "in", "itertools", ".", "count", "(", ")", ":", "\n", "            ", "split_k", "=", "split", "+", "(", "str", "(", "k", ")", "if", "k", ">", "0", "else", "''", ")", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "split_k", ")", "\n", "\n", "ds", "=", "data_utils", ".", "load_indexed_dataset", "(", "path", ",", "self", ".", "dictionary", ",", "self", ".", "args", ".", "dataset_impl", ")", "\n", "if", "ds", "is", "None", ":", "\n", "                ", "if", "k", ">", "0", ":", "\n", "                    ", "break", "\n", "", "else", ":", "\n", "                    ", "raise", "FileNotFoundError", "(", "'Dataset not found: {} ({})'", ".", "format", "(", "split", ",", "data_path", ")", ")", "\n", "\n", "# Since we append each block with the classification_token,", "\n", "# we need to effectively create blocks of length", "\n", "# tokens_per_sample-1", "\n", "", "", "loaded_datasets", ".", "append", "(", "\n", "TokenBlockDataset", "(", "\n", "ds", ",", "ds", ".", "sizes", ",", "self", ".", "args", ".", "tokens_per_sample", "-", "1", ",", "\n", "pad", "=", "self", ".", "dictionary", ".", "pad", "(", ")", ",", "eos", "=", "self", ".", "dictionary", ".", "eos", "(", ")", ",", "\n", ")", "\n", ")", "\n", "\n", "print", "(", "'| {} {} {} examples'", ".", "format", "(", "data_path", ",", "split_k", ",", "len", "(", "loaded_datasets", "[", "-", "1", "]", ")", ")", ")", "\n", "\n", "", "if", "len", "(", "loaded_datasets", ")", "==", "1", ":", "\n", "            ", "dataset", "=", "loaded_datasets", "[", "0", "]", "\n", "sizes", "=", "dataset", ".", "sizes", "\n", "", "else", ":", "\n", "            ", "dataset", "=", "ConcatDataset", "(", "loaded_datasets", ")", "\n", "sizes", "=", "np", ".", "concatenate", "(", "[", "ds", ".", "sizes", "for", "ds", "in", "loaded_datasets", "]", ")", "\n", "\n", "", "return", "dataset", ",", "sizes", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.cross_lingual_lm.CrossLingualLMTask.load_dataset": [[143, 174], ["collections.OrderedDict", "cross_lingual_lm.CrossLingualLMTask.langs2id.keys", "fairseq.data.multi_corpus_sampled_dataset.MultiCorpusSampledDataset", "print", "cross_lingual_lm.CrossLingualLMTask._load_single_lang_dataset", "fairseq.data.legacy.masked_lm_dataset.MaskedLMDataset", "len", "cross_lingual_lm.CrossLingualLMTask.dictionary.pad", "cross_lingual_lm.CrossLingualLMTask.dictionary.mask", "cross_lingual_lm.CrossLingualLMTask.dictionary.eos", "cross_lingual_lm.CrossLingualLMTask.dictionary.eos", "getattr", "cross_lingual_lm.CrossLingualLMTask.args.data.split"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.cross_lingual_lm.CrossLingualLMTask._load_single_lang_dataset", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.masked_lm_dictionary.MaskedLMDictionary.mask", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["", "def", "load_dataset", "(", "self", ",", "split", ",", "epoch", "=", "0", ",", "combine", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Load a given dataset split.\n        Args:\n            split (str): name of the split (e.g., train, valid, test)\n        \"\"\"", "\n", "\n", "dataset_map", "=", "OrderedDict", "(", ")", "\n", "\n", "for", "lang", "in", "self", ".", "langs2id", ".", "keys", "(", ")", ":", "\n", "# Datasets are expected to be in \"split.lang\" format (Eg: train.en)", "\n", "            ", "language_split", "=", "'{}.{}'", ".", "format", "(", "split", ",", "lang", ")", "\n", "\n", "block_dataset", ",", "sizes", "=", "self", ".", "_load_single_lang_dataset", "(", "split", "=", "language_split", ",", "epoch", "=", "epoch", ")", "\n", "\n", "dataset_map", "[", "lang", "]", "=", "MaskedLMDataset", "(", "\n", "dataset", "=", "block_dataset", ",", "\n", "sizes", "=", "sizes", ",", "\n", "vocab", "=", "self", ".", "dictionary", ",", "\n", "pad_idx", "=", "self", ".", "dictionary", ".", "pad", "(", ")", ",", "\n", "mask_idx", "=", "self", ".", "dictionary", ".", "mask", "(", ")", ",", "\n", "classif_token_idx", "=", "self", ".", "dictionary", ".", "eos", "(", ")", ",", "\n", "sep_token_idx", "=", "self", ".", "dictionary", ".", "eos", "(", ")", ",", "\n", "shuffle", "=", "getattr", "(", "self", ".", "args", ",", "'shuffle'", ",", "False", ")", ",", "\n", "has_pairs", "=", "False", ",", "\n", "segment_id", "=", "self", ".", "langs2id", "[", "lang", "]", ",", "\n", "seed", "=", "self", ".", "seed", ",", "\n", ")", "\n", "\n", "", "self", ".", "datasets", "[", "split", "]", "=", "MultiCorpusSampledDataset", "(", "dataset_map", ")", "\n", "print", "(", "'| {} {} {} examples'", ".", "format", "(", "\n", "self", ".", "args", ".", "data", ".", "split", "(", "':'", ")", "[", "epoch", "]", ",", "split", ",", "len", "(", "self", ".", "datasets", "[", "split", "]", ")", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.translation.TranslationTask.add_args": [[157, 181], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add task-specific arguments to the parser.\"\"\"", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'data'", ",", "help", "=", "'colon separated path to data directories list, \\\n                            will be iterated upon during epochs in round-robin manner'", ")", "\n", "parser", ".", "add_argument", "(", "'-s'", ",", "'--source-lang'", ",", "default", "=", "None", ",", "metavar", "=", "'SRC'", ",", "\n", "help", "=", "'source language'", ")", "\n", "parser", ".", "add_argument", "(", "'-t'", ",", "'--target-lang'", ",", "default", "=", "None", ",", "metavar", "=", "'TARGET'", ",", "\n", "help", "=", "'target language'", ")", "\n", "parser", ".", "add_argument", "(", "'--lazy-load'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'load the dataset lazily'", ")", "\n", "parser", ".", "add_argument", "(", "'--raw-text'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'load raw text dataset'", ")", "\n", "parser", ".", "add_argument", "(", "'--left-pad-source'", ",", "default", "=", "'True'", ",", "type", "=", "str", ",", "metavar", "=", "'BOOL'", ",", "\n", "help", "=", "'pad the source on the left'", ")", "\n", "parser", ".", "add_argument", "(", "'--left-pad-target'", ",", "default", "=", "'False'", ",", "type", "=", "str", ",", "metavar", "=", "'BOOL'", ",", "\n", "help", "=", "'pad the target on the left'", ")", "\n", "parser", ".", "add_argument", "(", "'--max-source-positions'", ",", "default", "=", "1024", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'max number of tokens in the source sequence'", ")", "\n", "parser", ".", "add_argument", "(", "'--max-target-positions'", ",", "default", "=", "1024", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'max number of tokens in the target sequence'", ")", "\n", "parser", ".", "add_argument", "(", "'--upsample-primary'", ",", "default", "=", "1", ",", "type", "=", "int", ",", "\n", "help", "=", "'amount to upsample primary dataset'", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.translation.TranslationTask.__init__": [[183, 187], ["FairseqTask.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["", "def", "__init__", "(", "self", ",", "args", ",", "src_dict", ",", "tgt_dict", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ")", "\n", "self", ".", "src_dict", "=", "src_dict", "\n", "self", ".", "tgt_dict", "=", "tgt_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.translation.TranslationTask.setup_task": [[188, 222], ["fairseq.options.eval_bool", "fairseq.options.eval_bool", "getattr", "args.data.split", "cls.load_dictionary", "cls.load_dictionary", "print", "print", "cls", "fairseq.utils.deprecation_warning", "getattr", "len", "fairseq.data.data_utils.infer_language_pair", "Exception", "os.path.join", "os.path.join", "cls.load_dictionary.pad", "cls.load_dictionary.pad", "cls.load_dictionary.eos", "cls.load_dictionary.eos", "cls.load_dictionary.unk", "cls.load_dictionary.unk", "fairseq.utils.deprecation_warning", "len", "len"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.eval_bool", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.eval_bool", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.sentence_prediction.SentencePredictionTask.load_dictionary", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.sentence_prediction.SentencePredictionTask.load_dictionary", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.masked_lm_dictionary.BertDictionary.cls", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.deprecation_warning", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.data_utils.infer_language_pair", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.unk", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.unk", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.deprecation_warning"], ["", "@", "classmethod", "\n", "def", "setup_task", "(", "cls", ",", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Setup the task (e.g., load dictionaries).\n\n        Args:\n            args (argparse.Namespace): parsed command-line arguments\n        \"\"\"", "\n", "args", ".", "left_pad_source", "=", "options", ".", "eval_bool", "(", "args", ".", "left_pad_source", ")", "\n", "args", ".", "left_pad_target", "=", "options", ".", "eval_bool", "(", "args", ".", "left_pad_target", ")", "\n", "if", "getattr", "(", "args", ",", "'raw_text'", ",", "False", ")", ":", "\n", "            ", "utils", ".", "deprecation_warning", "(", "'--raw-text is deprecated, please use --dataset-impl=raw'", ")", "\n", "args", ".", "dataset_impl", "=", "'raw'", "\n", "", "elif", "getattr", "(", "args", ",", "'lazy_load'", ",", "False", ")", ":", "\n", "            ", "utils", ".", "deprecation_warning", "(", "'--lazy-load is deprecated, please use --dataset-impl=lazy'", ")", "\n", "args", ".", "dataset_impl", "=", "'lazy'", "\n", "\n", "", "paths", "=", "args", ".", "data", ".", "split", "(", "':'", ")", "\n", "assert", "len", "(", "paths", ")", ">", "0", "\n", "# find language pair automatically", "\n", "if", "args", ".", "source_lang", "is", "None", "or", "args", ".", "target_lang", "is", "None", ":", "\n", "            ", "args", ".", "source_lang", ",", "args", ".", "target_lang", "=", "data_utils", ".", "infer_language_pair", "(", "paths", "[", "0", "]", ")", "\n", "", "if", "args", ".", "source_lang", "is", "None", "or", "args", ".", "target_lang", "is", "None", ":", "\n", "            ", "raise", "Exception", "(", "'Could not infer language pair, please provide it explicitly'", ")", "\n", "\n", "# load dictionaries", "\n", "", "src_dict", "=", "cls", ".", "load_dictionary", "(", "os", ".", "path", ".", "join", "(", "paths", "[", "0", "]", ",", "'dict.{}.txt'", ".", "format", "(", "args", ".", "source_lang", ")", ")", ")", "\n", "tgt_dict", "=", "cls", ".", "load_dictionary", "(", "os", ".", "path", ".", "join", "(", "paths", "[", "0", "]", ",", "'dict.{}.txt'", ".", "format", "(", "args", ".", "target_lang", ")", ")", ")", "\n", "assert", "src_dict", ".", "pad", "(", ")", "==", "tgt_dict", ".", "pad", "(", ")", "\n", "assert", "src_dict", ".", "eos", "(", ")", "==", "tgt_dict", ".", "eos", "(", ")", "\n", "assert", "src_dict", ".", "unk", "(", ")", "==", "tgt_dict", ".", "unk", "(", ")", "\n", "print", "(", "'| [{}] dictionary: {} types'", ".", "format", "(", "args", ".", "source_lang", ",", "len", "(", "src_dict", ")", ")", ")", "\n", "print", "(", "'| [{}] dictionary: {} types'", ".", "format", "(", "args", ".", "target_lang", ",", "len", "(", "tgt_dict", ")", ")", ")", "\n", "\n", "return", "cls", "(", "args", ",", "src_dict", ",", "tgt_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.translation.TranslationTask.load_dataset": [[223, 245], ["translation.TranslationTask.args.data.split", "translation.load_langpair_dataset", "len", "len"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.translation.load_langpair_dataset"], ["", "def", "load_dataset", "(", "self", ",", "split", ",", "epoch", "=", "0", ",", "combine", "=", "False", ",", "state_machine", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Load a given dataset split.\n\n        Args:\n            split (str): name of the split (e.g., train, valid, test)\n        \"\"\"", "\n", "paths", "=", "self", ".", "args", ".", "data", ".", "split", "(", "':'", ")", "\n", "assert", "len", "(", "paths", ")", ">", "0", "\n", "data_path", "=", "paths", "[", "epoch", "%", "len", "(", "paths", ")", "]", "\n", "\n", "# infer langcode", "\n", "src", ",", "tgt", "=", "self", ".", "args", ".", "source_lang", ",", "self", ".", "args", ".", "target_lang", "\n", "\n", "self", ".", "datasets", "[", "split", "]", "=", "load_langpair_dataset", "(", "\n", "data_path", ",", "split", ",", "src", ",", "self", ".", "src_dict", ",", "tgt", ",", "self", ".", "tgt_dict", ",", "\n", "combine", "=", "combine", ",", "dataset_impl", "=", "self", ".", "args", ".", "dataset_impl", ",", "\n", "upsample_primary", "=", "self", ".", "args", ".", "upsample_primary", ",", "\n", "left_pad_source", "=", "self", ".", "args", ".", "left_pad_source", ",", "\n", "left_pad_target", "=", "self", ".", "args", ".", "left_pad_target", ",", "\n", "max_source_positions", "=", "self", ".", "args", ".", "max_source_positions", ",", "\n", "max_target_positions", "=", "self", ".", "args", ".", "max_target_positions", ",", "\n", "state_machine", "=", "state_machine", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.translation.TranslationTask.build_dataset_for_inference": [[247, 249], ["fairseq.data.LanguagePairDataset"], "methods", ["None"], ["", "def", "build_dataset_for_inference", "(", "self", ",", "src_tokens", ",", "src_lengths", ")", ":", "\n", "        ", "return", "LanguagePairDataset", "(", "src_tokens", ",", "src_lengths", ",", "self", ".", "source_dictionary", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.translation.TranslationTask.max_positions": [[250, 253], ["None"], "methods", ["None"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the max sentence length allowed by the task.\"\"\"", "\n", "return", "(", "self", ".", "args", ".", "max_source_positions", ",", "self", ".", "args", ".", "max_target_positions", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.translation.TranslationTask.source_dictionary": [[254, 258], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "source_dictionary", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the source :class:`~fairseq.data.Dictionary`.\"\"\"", "\n", "return", "self", ".", "src_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.translation.TranslationTask.target_dictionary": [[259, 263], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "target_dictionary", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the target :class:`~fairseq.data.Dictionary`.\"\"\"", "\n", "return", "self", ".", "tgt_dict", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.translation.load_langpair_dataset": [[20, 132], ["itertools.count", "fairseq.data.LanguagePairDataset", "os.path.join", "fairseq.data.indexed_dataset.dataset_exists", "translation.load_langpair_dataset.split_exists"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.dataset_exists"], ["def", "load_langpair_dataset", "(", "\n", "data_path", ",", "split", ",", "\n", "src", ",", "src_dict", ",", "\n", "tgt", ",", "tgt_dict", ",", "\n", "combine", ",", "dataset_impl", ",", "upsample_primary", ",", "\n", "left_pad_source", ",", "left_pad_target", ",", "max_source_positions", ",", "max_target_positions", ",", "\n", "state_machine", "=", "True", "\n", ")", ":", "\n", "    ", "def", "split_exists", "(", "split", ",", "src", ",", "tgt", ",", "lang", ",", "data_path", ")", ":", "\n", "        ", "filename", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "'{}.{}-{}.{}'", ".", "format", "(", "split", ",", "src", ",", "tgt", ",", "lang", ")", ")", "\n", "return", "indexed_dataset", ".", "dataset_exists", "(", "filename", ",", "impl", "=", "dataset_impl", ")", "\n", "\n", "", "src_datasets", "=", "[", "]", "\n", "src_fixed_embeddings", "=", "[", "]", "\n", "src_wordpieces", "=", "[", "]", "\n", "src_wp2w", "=", "[", "]", "\n", "\n", "tgt_datasets", "=", "[", "]", "\n", "memory_datasets", "=", "[", "]", "\n", "memory_pos_datasets", "=", "[", "]", "\n", "target_mask_datasets", "=", "[", "]", "\n", "active_logits_datasets", "=", "[", "]", "\n", "\n", "for", "k", "in", "itertools", ".", "count", "(", ")", ":", "\n", "        ", "split_k", "=", "split", "+", "(", "str", "(", "k", ")", "if", "k", ">", "0", "else", "''", ")", "\n", "\n", "# infer langcode", "\n", "if", "split_exists", "(", "split_k", ",", "src", ",", "tgt", ",", "src", ",", "data_path", ")", ":", "\n", "            ", "prefix", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "'{}.{}-{}.'", ".", "format", "(", "split_k", ",", "src", ",", "tgt", ")", ")", "\n", "", "elif", "split_exists", "(", "split_k", ",", "tgt", ",", "src", ",", "src", ",", "data_path", ")", ":", "\n", "            ", "prefix", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "'{}.{}-{}.'", ".", "format", "(", "split_k", ",", "tgt", ",", "src", ")", ")", "\n", "", "else", ":", "\n", "            ", "if", "k", ">", "0", ":", "\n", "                ", "break", "\n", "", "else", ":", "\n", "                ", "raise", "FileNotFoundError", "(", "'Dataset not found: {} ({})'", ".", "format", "(", "split", ",", "data_path", ")", ")", "\n", "\n", "# source", "\n", "", "", "src_file", "=", "(", "prefix", "+", "src", ",", "src_dict", ",", "dataset_impl", ")", "\n", "src_datasets", ".", "append", "(", "data_utils", ".", "load_indexed_dataset", "(", "*", "src_file", ")", ")", "\n", "\n", "# pre-trained embeddings", "\n", "fixed_embeddings_file", "=", "(", "prefix", "+", "'en.bert'", ",", "None", ",", "dataset_impl", ")", "\n", "src_fixed_embeddings", ".", "append", "(", "\n", "data_utils", ".", "load_indexed_dataset", "(", "*", "fixed_embeddings_file", ")", "\n", ")", "\n", "\n", "# wordpieces", "\n", "wordpieces_file", "=", "(", "prefix", "+", "'en.wordpieces'", ",", "None", ",", "dataset_impl", ")", "\n", "src_wordpieces", ".", "append", "(", "data_utils", ".", "load_indexed_dataset", "(", "*", "wordpieces_file", ")", ")", "\n", "\n", "# wordpieces to word map", "\n", "wp2w_file", "=", "(", "prefix", "+", "'en.wp2w'", ",", "None", ",", "dataset_impl", ")", "\n", "src_wp2w", ".", "append", "(", "data_utils", ".", "load_indexed_dataset", "(", "*", "wp2w_file", ")", ")", "\n", "\n", "# actions", "\n", "tgt_file", "=", "prefix", "+", "tgt", ",", "tgt_dict", ",", "dataset_impl", "\n", "tgt_datasets", ".", "append", "(", "data_utils", ".", "load_indexed_dataset", "(", "*", "tgt_file", ")", ")", "\n", "\n", "# state machine states (buffer/stack) and positions", "\n", "memory_file", "=", "prefix", "+", "'memory'", ",", "None", ",", "dataset_impl", "\n", "memory_datasets", ".", "append", "(", "data_utils", ".", "load_indexed_dataset", "(", "*", "memory_file", ")", ")", "\n", "memory_pos_file", "=", "prefix", "+", "'memory_pos'", ",", "None", ",", "dataset_impl", "\n", "memory_pos_datasets", ".", "append", "(", "data_utils", ".", "load_indexed_dataset", "(", "*", "memory_pos_file", ")", ")", "\n", "\n", "# logit masks", "\n", "target_mask_file", "=", "prefix", "+", "'target_masks'", ",", "None", ",", "dataset_impl", "\n", "target_mask_datasets", ".", "append", "(", "data_utils", ".", "load_indexed_dataset", "(", "*", "target_mask_file", ")", ")", "\n", "\n", "# active logits", "\n", "active_logits_file", "=", "prefix", "+", "'active_logits'", ",", "None", ",", "dataset_impl", "\n", "active_logits_datasets", ".", "append", "(", "data_utils", ".", "load_indexed_dataset", "(", "*", "active_logits_file", ")", ")", "\n", "\n", "print", "(", "'| {} {} {}-{} {} examples'", ".", "format", "(", "data_path", ",", "split_k", ",", "src", ",", "tgt", ",", "len", "(", "src_datasets", "[", "-", "1", "]", ")", ")", ")", "\n", "\n", "if", "not", "combine", ":", "\n", "            ", "break", "\n", "\n", "", "", "assert", "len", "(", "src_datasets", ")", "==", "len", "(", "tgt_datasets", ")", "\n", "\n", "if", "len", "(", "src_datasets", ")", "==", "1", ":", "\n", "        ", "src_dataset", ",", "tgt_dataset", "=", "src_datasets", "[", "0", "]", ",", "tgt_datasets", "[", "0", "]", "\n", "src_fixed_embeddings", "=", "src_fixed_embeddings", "[", "0", "]", "\n", "src_wordpieces", "=", "src_wordpieces", "[", "0", "]", "\n", "src_wp2w", "=", "src_wp2w", "[", "0", "]", "\n", "memory_dataset", "=", "memory_datasets", "[", "0", "]", "\n", "memory_pos_dataset", "=", "memory_pos_datasets", "[", "0", "]", "\n", "target_mask_datasets", "=", "target_mask_datasets", "[", "0", "]", "\n", "active_logits_datasets", "=", "active_logits_datasets", "[", "0", "]", "\n", "", "else", ":", "\n", "# not implemented for stack-transformer", "\n", "        ", "raise", "NotImplementedError", "(", ")", "\n", "sample_ratios", "=", "[", "1", "]", "*", "len", "(", "src_datasets", ")", "\n", "sample_ratios", "[", "0", "]", "=", "upsample_primary", "\n", "src_dataset", "=", "ConcatDataset", "(", "src_datasets", ",", "sample_ratios", ")", "\n", "tgt_dataset", "=", "ConcatDataset", "(", "tgt_datasets", ",", "sample_ratios", ")", "\n", "\n", "", "return", "LanguagePairDataset", "(", "\n", "src_dataset", ",", "src_dataset", ".", "sizes", ",", "src_dict", ",", "\n", "src_fixed_embeddings", ",", "src_fixed_embeddings", ".", "sizes", ",", "\n", "src_wordpieces", ",", "src_wordpieces", ".", "sizes", ",", "\n", "src_wp2w", ",", "src_wp2w", ".", "sizes", ",", "\n", "tgt_dataset", ",", "tgt_dataset", ".", "sizes", ",", "tgt_dict", ",", "\n", "memory_dataset", ",", "memory_dataset", ".", "sizes", ",", "\n", "memory_pos_dataset", ",", "memory_pos_dataset", ".", "sizes", ",", "\n", "target_mask_datasets", ",", "target_mask_datasets", ".", "sizes", ",", "\n", "active_logits_datasets", ",", "active_logits_datasets", ".", "sizes", ",", "\n", "left_pad_source", "=", "left_pad_source", ",", "\n", "left_pad_target", "=", "left_pad_target", ",", "\n", "max_source_positions", "=", "max_source_positions", ",", "\n", "max_target_positions", "=", "max_target_positions", ",", "\n", "state_machine", "=", "state_machine", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.fairseq_task.FairseqTask.add_args": [[20, 24], ["None"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add task-specific arguments to the parser.\"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.fairseq_task.FairseqTask.__init__": [[25, 28], ["None"], "methods", ["None"], ["", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "self", ".", "args", "=", "args", "\n", "self", ".", "datasets", "=", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.fairseq_task.FairseqTask.load_dictionary": [[29, 37], ["fairseq.data.Dictionary.load"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load"], ["", "@", "classmethod", "\n", "def", "load_dictionary", "(", "cls", ",", "filename", ")", ":", "\n", "        ", "\"\"\"Load the dictionary from the filename\n\n        Args:\n            filename (str): the filename\n        \"\"\"", "\n", "return", "Dictionary", ".", "load", "(", "filename", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.fairseq_task.FairseqTask.build_dictionary": [[38, 57], ["fairseq.data.Dictionary", "fairseq.data.Dictionary.finalize", "fairseq.data.Dictionary.add_file_to_dictionary"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDatasetBuilder.finalize", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_file_to_dictionary"], ["", "@", "classmethod", "\n", "def", "build_dictionary", "(", "cls", ",", "filenames", ",", "workers", "=", "1", ",", "threshold", "=", "-", "1", ",", "nwords", "=", "-", "1", ",", "padding_factor", "=", "8", ",", "tokenize", "=", "tokenizer", ".", "tokenize_line", ")", ":", "\n", "        ", "\"\"\"Build the dictionary\n\n        Args:\n            filenames (list): list of filenames\n            workers (int): number of concurrent workers\n            threshold (int): defines the minimum word count\n            nwords (int): defines the total number of words in the final dictionary,\n                including special symbols\n            padding_factor (int): can be used to pad the dictionary size to be a\n                multiple of 8, which is important on some hardware (e.g., Nvidia\n                Tensor Cores).\n        \"\"\"", "\n", "d", "=", "Dictionary", "(", ")", "\n", "for", "filename", "in", "filenames", ":", "\n", "            ", "Dictionary", ".", "add_file_to_dictionary", "(", "filename", ",", "d", ",", "tokenize", ",", "workers", ")", "\n", "", "d", ".", "finalize", "(", "threshold", "=", "threshold", ",", "nwords", "=", "nwords", ",", "padding_factor", "=", "padding_factor", ")", "\n", "return", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.fairseq_task.FairseqTask.setup_task": [[58, 66], ["cls"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.masked_lm_dictionary.BertDictionary.cls"], ["", "@", "classmethod", "\n", "def", "setup_task", "(", "cls", ",", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Setup the task (e.g., load dictionaries).\n\n        Args:\n            args (argparse.Namespace): parsed command-line arguments\n        \"\"\"", "\n", "return", "cls", "(", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.fairseq_task.FairseqTask.load_dataset": [[67, 74], ["None"], "methods", ["None"], ["", "def", "load_dataset", "(", "self", ",", "split", ",", "combine", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Load a given dataset split.\n\n        Args:\n            split (str): name of the split (e.g., train, valid, test)\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.fairseq_task.FairseqTask.dataset": [[75, 91], ["KeyError", "isinstance", "TypeError"], "methods", ["None"], ["", "def", "dataset", "(", "self", ",", "split", ")", ":", "\n", "        ", "\"\"\"\n        Return a loaded dataset split.\n\n        Args:\n            split (str): name of the split (e.g., train, valid, test)\n\n        Returns:\n            a :class:`~fairseq.data.FairseqDataset` corresponding to *split*\n        \"\"\"", "\n", "from", "fairseq", ".", "data", "import", "FairseqDataset", "\n", "if", "split", "not", "in", "self", ".", "datasets", ":", "\n", "            ", "raise", "KeyError", "(", "'Dataset not loaded: '", "+", "split", ")", "\n", "", "if", "not", "isinstance", "(", "self", ".", "datasets", "[", "split", "]", ",", "FairseqDataset", ")", ":", "\n", "            ", "raise", "TypeError", "(", "'Datasets are expected to be of type FairseqDataset'", ")", "\n", "", "return", "self", ".", "datasets", "[", "split", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.fairseq_task.FairseqTask.get_batch_iterator": [[92, 170], ["isinstance", "fairseq.data.data_utils.batch_by_size", "fairseq.data.iterators.EpochBatchIterator", "fairseq.data.data_utils.numpy_seed", "dataset.ordered_indices", "numpy.array", "numpy.array", "numpy.setdiff1d", "fairseq.data.data_utils.filter_by_size"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.data_utils.batch_by_size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.data_utils.numpy_seed", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.ordered_indices", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.data_utils.filter_by_size"], ["", "def", "get_batch_iterator", "(", "\n", "self", ",", "dataset", ",", "max_tokens", "=", "None", ",", "max_sentences", "=", "None", ",", "max_positions", "=", "None", ",", "\n", "ignore_invalid_inputs", "=", "False", ",", "required_batch_size_multiple", "=", "1", ",", "\n", "seed", "=", "1", ",", "num_shards", "=", "1", ",", "shard_id", "=", "0", ",", "num_workers", "=", "0", ",", "epoch", "=", "0", ",", "\n", "large_sent_first", "=", "False", ",", "whitelisted_indices", "=", "None", ",", "\n", "blacklisted_indices", "=", "None", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Get an iterator that yields batches of data from the given dataset.\n\n        Args:\n            dataset (~fairseq.data.FairseqDataset): dataset to batch\n            max_tokens (int, optional): max number of tokens in each batch\n                (default: None).\n            max_sentences (int, optional): max number of sentences in each\n                batch (default: None).\n            max_positions (optional): max sentence length supported by the\n                model (default: None).\n            ignore_invalid_inputs (bool, optional): don't raise Exception for\n                sentences that are too long (default: False).\n            required_batch_size_multiple (int, optional): require batch size to\n                be a multiple of N (default: 1).\n            seed (int, optional): seed for random number generator for\n                reproducibility (default: 1).\n            num_shards (int, optional): shard the data iterator into N\n                shards (default: 1).\n            shard_id (int, optional): which shard of the data iterator to\n                return (default: 0).\n            num_workers (int, optional): how many subprocesses to use for data\n                loading. 0 means the data will be loaded in the main process\n                (default: 0).\n            epoch (int, optional): the epoch to start the iterator from\n                (default: 0).\n\n        Returns:\n            ~fairseq.iterators.EpochBatchIterator: a batched iterator over the\n                given dataset split\n        \"\"\"", "\n", "assert", "isinstance", "(", "dataset", ",", "FairseqDataset", ")", "\n", "\n", "# get indices ordered by example size", "\n", "with", "data_utils", ".", "numpy_seed", "(", "seed", ")", ":", "\n", "            ", "indices", "=", "dataset", ".", "ordered_indices", "(", ")", "\n", "# invert order to start by bigger ones", "\n", "if", "large_sent_first", ":", "\n", "                ", "indices", "=", "indices", "[", ":", ":", "-", "1", "]", "\n", "\n", "# set whitelisted indices", "\n", "", "", "if", "whitelisted_indices", "is", "not", "None", ":", "\n", "            ", "indices", "=", "np", ".", "array", "(", "whitelisted_indices", ")", "\n", "\n", "# filter blacklisted indices", "\n", "", "if", "blacklisted_indices", "is", "not", "None", ":", "\n", "            ", "blacklisted_indices", "=", "np", ".", "array", "(", "blacklisted_indices", ")", "\n", "indices", "=", "np", ".", "setdiff1d", "(", "indices", ",", "blacklisted_indices", ")", "\n", "\n", "# filter examples that are too large", "\n", "", "if", "max_positions", "is", "not", "None", ":", "\n", "            ", "indices", "=", "data_utils", ".", "filter_by_size", "(", "\n", "indices", ",", "dataset", ".", "size", ",", "max_positions", ",", "raise_exception", "=", "(", "not", "ignore_invalid_inputs", ")", ",", "\n", ")", "\n", "\n", "# create mini-batches with given size constraints", "\n", "", "batch_sampler", "=", "data_utils", ".", "batch_by_size", "(", "\n", "indices", ",", "dataset", ".", "num_tokens", ",", "max_tokens", "=", "max_tokens", ",", "max_sentences", "=", "max_sentences", ",", "\n", "required_batch_size_multiple", "=", "required_batch_size_multiple", ",", "\n", ")", "\n", "\n", "# return a reusable, sharded iterator", "\n", "return", "iterators", ".", "EpochBatchIterator", "(", "\n", "dataset", "=", "dataset", ",", "\n", "collate_fn", "=", "dataset", ".", "collater", ",", "\n", "batch_sampler", "=", "batch_sampler", ",", "\n", "seed", "=", "seed", ",", "\n", "num_shards", "=", "num_shards", ",", "\n", "shard_id", "=", "shard_id", ",", "\n", "num_workers", "=", "num_workers", ",", "\n", "epoch", "=", "epoch", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.fairseq_task.FairseqTask.build_model": [[172, 185], ["models.build_model"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv_lm.LightConvLanguageModel.build_model"], ["", "def", "build_model", "(", "self", ",", "args", ")", ":", "\n", "        ", "\"\"\"\n        Build the :class:`~fairseq.models.BaseFairseqModel` instance for this\n        task.\n\n        Args:\n            args (argparse.Namespace): parsed command-line arguments\n\n        Returns:\n            a :class:`~fairseq.models.BaseFairseqModel` instance\n        \"\"\"", "\n", "from", "fairseq", "import", "models", "\n", "return", "models", ".", "build_model", "(", "args", ",", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.fairseq_task.FairseqTask.build_criterion": [[186, 199], ["criterions.build_criterion"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.composite_loss.CompositeLoss.build_criterion"], ["", "def", "build_criterion", "(", "self", ",", "args", ")", ":", "\n", "        ", "\"\"\"\n        Build the :class:`~fairseq.criterions.FairseqCriterion` instance for\n        this task.\n\n        Args:\n            args (argparse.Namespace): parsed command-line arguments\n\n        Returns:\n            a :class:`~fairseq.criterions.FairseqCriterion` instance\n        \"\"\"", "\n", "from", "fairseq", "import", "criterions", "\n", "return", "criterions", ".", "build_criterion", "(", "args", ",", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.fairseq_task.FairseqTask.build_generator": [[200, 230], ["getattr", "SequenceScorer", "SequenceGenerator", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "transition_amr_parser.stack_transformer.amr_state_machine.StateMachineBatch", "getattr"], "methods", ["None"], ["", "def", "build_generator", "(", "self", ",", "args", ")", ":", "\n", "        ", "if", "getattr", "(", "args", ",", "'score_reference'", ",", "False", ")", ":", "\n", "            ", "from", "fairseq", ".", "sequence_scorer", "import", "SequenceScorer", "\n", "return", "SequenceScorer", "(", "self", ".", "target_dictionary", ")", "\n", "", "else", ":", "\n", "            ", "from", "fairseq", ".", "sequence_generator", "import", "SequenceGenerator", "\n", "return", "SequenceGenerator", "(", "\n", "self", ".", "target_dictionary", ",", "\n", "beam_size", "=", "getattr", "(", "args", ",", "'beam'", ",", "5", ")", ",", "\n", "max_len_a", "=", "getattr", "(", "args", ",", "'max_len_a'", ",", "0", ")", ",", "\n", "max_len_b", "=", "getattr", "(", "args", ",", "'max_len_b'", ",", "200", ")", ",", "\n", "min_len", "=", "getattr", "(", "args", ",", "'min_len'", ",", "1", ")", ",", "\n", "normalize_scores", "=", "(", "not", "getattr", "(", "args", ",", "'unnormalized'", ",", "False", ")", ")", ",", "\n", "len_penalty", "=", "getattr", "(", "args", ",", "'lenpen'", ",", "1", ")", ",", "\n", "unk_penalty", "=", "getattr", "(", "args", ",", "'unkpen'", ",", "0", ")", ",", "\n", "sampling", "=", "getattr", "(", "args", ",", "'sampling'", ",", "False", ")", ",", "\n", "sampling_topk", "=", "getattr", "(", "args", ",", "'sampling_topk'", ",", "-", "1", ")", ",", "\n", "sampling_topp", "=", "getattr", "(", "args", ",", "'sampling_topp'", ",", "-", "1.0", ")", ",", "\n", "temperature", "=", "getattr", "(", "args", ",", "'temperature'", ",", "1.", ")", ",", "\n", "diverse_beam_groups", "=", "getattr", "(", "args", ",", "'diverse_beam_groups'", ",", "-", "1", ")", ",", "\n", "diverse_beam_strength", "=", "getattr", "(", "args", ",", "'diverse_beam_strength'", ",", "0.5", ")", ",", "\n", "match_source_len", "=", "getattr", "(", "args", ",", "'match_source_len'", ",", "False", ")", ",", "\n", "no_repeat_ngram_size", "=", "getattr", "(", "args", ",", "'no_repeat_ngram_size'", ",", "0", ")", ",", "\n", "# State machine for parsing", "\n", "state_machine", "=", "StateMachineBatch", "(", "\n", "self", ".", "source_dictionary", ",", "\n", "self", ".", "target_dictionary", ",", "\n", "args", ".", "machine_type", ",", "\n", "machine_rules", "=", "args", ".", "machine_rules", ",", "\n", "entity_rules", "=", "args", ".", "entity_rules", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.fairseq_task.FairseqTask.train_step": [[233, 274], ["model.train", "criterion", "optimizer.backward"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq-stack-transformer.train.train", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.scalar_bias.ScalarBias.backward"], ["", "", "def", "train_step", "(", "self", ",", "sample", ",", "model", ",", "criterion", ",", "optimizer", ",", "ignore_grad", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Do forward and backward, and return the loss as computed by *criterion*\n        for the given *model* and *sample*.\n\n        Args:\n            sample (dict): the mini-batch. The format is defined by the\n                :class:`~fairseq.data.FairseqDataset`.\n            model (~fairseq.models.BaseFairseqModel): the model\n            criterion (~fairseq.criterions.FairseqCriterion): the criterion\n            optimizer (~fairseq.optim.FairseqOptimizer): the optimizer\n            ignore_grad (bool): multiply loss by 0 if this is set to True\n\n        Returns:\n            tuple:\n                - the loss\n                - the sample size, which is used as the denominator for the\n                  gradient\n                - logging outputs to display while training\n        \"\"\"", "\n", "model", ".", "train", "(", ")", "\n", "loss", ",", "sample_size", ",", "logging_output", "=", "criterion", "(", "model", ",", "sample", ")", "\n", "#        if torch.isnan(loss):", "\n", "#            import ipdb; ipdb.set_trace(context=30)", "\n", "#            loss, sample_size, logging_output = criterion(model, sample)", "\n", "\n", "if", "ignore_grad", ":", "\n", "            ", "loss", "*=", "0", "\n", "", "optimizer", ".", "backward", "(", "loss", ")", "\n", "\n", "#        # NaN weigths", "\n", "#        nan_weights = {", "\n", "#            name: param  ", "\n", "#            for name, param in model.named_parameters() ", "\n", "#            if torch.isnan(param).any()", "\n", "#        }", "\n", "#        if nan_weights:", "\n", "#            import ipdb; ipdb.set_trace(context=30)", "\n", "#            print()", "\n", "\n", "return", "loss", ",", "sample_size", ",", "logging_output", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.fairseq_task.FairseqTask.valid_step": [[275, 280], ["model.eval", "torch.no_grad", "criterion"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval"], ["", "def", "valid_step", "(", "self", ",", "sample", ",", "model", ",", "criterion", ")", ":", "\n", "        ", "model", ".", "eval", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "loss", ",", "sample_size", ",", "logging_output", "=", "criterion", "(", "model", ",", "sample", ")", "\n", "", "return", "loss", ",", "sample_size", ",", "logging_output", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.fairseq_task.FairseqTask.inference_step": [[281, 284], ["torch.no_grad", "generator.generate"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.sequence_generator.SequenceGenerator.generate"], ["", "def", "inference_step", "(", "self", ",", "generator", ",", "models", ",", "sample", ",", "prefix_tokens", "=", "None", ")", ":", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "return", "generator", ".", "generate", "(", "models", ",", "sample", ",", "prefix_tokens", "=", "prefix_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.fairseq_task.FairseqTask.update_step": [[285, 289], ["None"], "methods", ["None"], ["", "", "def", "update_step", "(", "self", ",", "num_updates", ")", ":", "\n", "        ", "\"\"\"Task level update when number of update increases. This is called after optimization step and\n           learning rate update of each step\"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.fairseq_task.FairseqTask.grad_denom": [[290, 292], ["criterion.__class__.grad_denom"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.fairseq_criterion.FairseqCriterion.grad_denom"], ["", "def", "grad_denom", "(", "self", ",", "sample_sizes", ",", "criterion", ")", ":", "\n", "        ", "return", "criterion", ".", "__class__", ".", "grad_denom", "(", "sample_sizes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.fairseq_task.FairseqTask.aggregate_logging_outputs": [[293, 295], ["criterion.__class__.aggregate_logging_outputs"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.binary_cross_entropy.BinaryCrossEntropyCriterion.aggregate_logging_outputs"], ["", "def", "aggregate_logging_outputs", "(", "self", ",", "logging_outputs", ",", "criterion", ")", ":", "\n", "        ", "return", "criterion", ".", "__class__", ".", "aggregate_logging_outputs", "(", "logging_outputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.fairseq_task.FairseqTask.max_positions": [[296, 299], ["None"], "methods", ["None"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the max input length allowed by the task.\"\"\"", "\n", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.fairseq_task.FairseqTask.source_dictionary": [[300, 305], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "source_dictionary", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the source :class:`~fairseq.data.Dictionary` (if applicable\n        for this task).\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.fairseq_task.FairseqTask.target_dictionary": [[306, 311], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "target_dictionary", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the target :class:`~fairseq.data.Dictionary` (if applicable\n        for this task).\"\"\"", "\n", "raise", "NotImplementedError", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.language_modeling.LanguageModelingTask.add_args": [[52, 82], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add task-specific arguments to the parser.\"\"\"", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'data'", ",", "help", "=", "'path to data directory'", ")", "\n", "parser", ".", "add_argument", "(", "'--sample-break-mode'", ",", "default", "=", "'none'", ",", "\n", "choices", "=", "[", "'none'", ",", "'complete'", ",", "'complete_doc'", ",", "'eos'", "]", ",", "\n", "help", "=", "'If omitted or \"none\", fills each sample with tokens-per-sample '", "\n", "'tokens. If set to \"complete\", splits samples only at the end '", "\n", "'of sentence, but may include multiple sentences per sample. '", "\n", "'\"complete_doc\" is similar but respects doc boundaries. '", "\n", "'If set to \"eos\", includes only one sentence per sample.'", ")", "\n", "parser", ".", "add_argument", "(", "'--tokens-per-sample'", ",", "default", "=", "1024", ",", "type", "=", "int", ",", "\n", "help", "=", "'max number of tokens per sample for LM dataset'", ")", "\n", "parser", ".", "add_argument", "(", "'--lazy-load'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'load the dataset lazily'", ")", "\n", "parser", ".", "add_argument", "(", "'--raw-text'", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'load raw text dataset'", ")", "\n", "parser", ".", "add_argument", "(", "'--output-dictionary-size'", ",", "default", "=", "-", "1", ",", "type", "=", "int", ",", "\n", "help", "=", "'limit the size of output dictionary'", ")", "\n", "parser", ".", "add_argument", "(", "'--self-target'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'include self target'", ")", "\n", "parser", ".", "add_argument", "(", "'--future-target'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'include future target'", ")", "\n", "parser", ".", "add_argument", "(", "'--past-target'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'include past target'", ")", "\n", "parser", ".", "add_argument", "(", "'--add-bos-token'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'prepend beginning of sentence token (<s>)'", ")", "\n", "parser", ".", "add_argument", "(", "'--max-target-positions'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'max number of tokens in the target sequence'", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.language_modeling.LanguageModelingTask.__init__": [[84, 92], ["fairseq.tasks.FairseqTask.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["", "def", "__init__", "(", "self", ",", "args", ",", "dictionary", ",", "output_dictionary", "=", "None", ",", "targets", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ")", "\n", "self", ".", "dictionary", "=", "dictionary", "\n", "self", ".", "output_dictionary", "=", "output_dictionary", "or", "dictionary", "\n", "\n", "if", "targets", "is", "None", ":", "\n", "            ", "targets", "=", "[", "'future'", "]", "\n", "", "self", ".", "targets", "=", "targets", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.language_modeling.LanguageModelingTask.setup_task": [[93, 134], ["getattr", "hasattr", "getattr", "getattr", "getattr", "cls", "fairseq.utils.deprecation_warning", "getattr", "args.data.split", "fairseq.data.Dictionary.load", "print", "targets.append", "targets.append", "targets.append", "len", "fairseq.utils.deprecation_warning", "len", "os.path.join", "fairseq.data.TruncatedDictionary", "len"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.masked_lm_dictionary.BertDictionary.cls", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.deprecation_warning", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.deprecation_warning"], ["", "@", "classmethod", "\n", "def", "setup_task", "(", "cls", ",", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Setup the task (e.g., load dictionaries).\n\n        Args:\n            args (argparse.Namespace): parsed command-line arguments\n        \"\"\"", "\n", "if", "getattr", "(", "args", ",", "'raw_text'", ",", "False", ")", ":", "\n", "            ", "utils", ".", "deprecation_warning", "(", "'--raw-text is deprecated, please use --dataset-impl=raw'", ")", "\n", "args", ".", "dataset_impl", "=", "'raw'", "\n", "", "elif", "getattr", "(", "args", ",", "'lazy_load'", ",", "False", ")", ":", "\n", "            ", "utils", ".", "deprecation_warning", "(", "'--lazy-load is deprecated, please use --dataset-impl=lazy'", ")", "\n", "args", ".", "dataset_impl", "=", "'lazy'", "\n", "\n", "", "dictionary", "=", "None", "\n", "output_dictionary", "=", "None", "\n", "if", "args", ".", "data", ":", "\n", "            ", "paths", "=", "args", ".", "data", ".", "split", "(", "':'", ")", "\n", "assert", "len", "(", "paths", ")", ">", "0", "\n", "dictionary", "=", "Dictionary", ".", "load", "(", "os", ".", "path", ".", "join", "(", "paths", "[", "0", "]", ",", "'dict.txt'", ")", ")", "\n", "print", "(", "'| dictionary: {} types'", ".", "format", "(", "len", "(", "dictionary", ")", ")", ")", "\n", "output_dictionary", "=", "dictionary", "\n", "if", "args", ".", "output_dictionary_size", ">=", "0", ":", "\n", "                ", "output_dictionary", "=", "TruncatedDictionary", "(", "dictionary", ",", "args", ".", "output_dictionary_size", ")", "\n", "\n", "# upgrade old checkpoints", "\n", "", "", "if", "hasattr", "(", "args", ",", "'exclude_self_target'", ")", ":", "\n", "            ", "args", ".", "self_target", "=", "not", "args", ".", "exclude_self_target", "\n", "\n", "", "targets", "=", "[", "]", "\n", "if", "getattr", "(", "args", ",", "'self_target'", ",", "False", ")", ":", "\n", "            ", "targets", ".", "append", "(", "'self'", ")", "\n", "", "if", "getattr", "(", "args", ",", "'future_target'", ",", "False", ")", ":", "\n", "            ", "targets", ".", "append", "(", "'future'", ")", "\n", "", "if", "getattr", "(", "args", ",", "'past_target'", ",", "False", ")", ":", "\n", "            ", "targets", ".", "append", "(", "'past'", ")", "\n", "", "if", "len", "(", "targets", ")", "==", "0", ":", "\n", "# standard language modeling", "\n", "            ", "targets", "=", "[", "'future'", "]", "\n", "\n", "", "return", "cls", "(", "args", ",", "dictionary", ",", "output_dictionary", ",", "targets", "=", "targets", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.language_modeling.LanguageModelingTask.build_model": [[135, 143], ["super().build_model", "ValueError"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv_lm.LightConvLanguageModel.build_model"], ["", "def", "build_model", "(", "self", ",", "args", ")", ":", "\n", "        ", "model", "=", "super", "(", ")", ".", "build_model", "(", "args", ")", "\n", "\n", "for", "target", "in", "self", ".", "targets", ":", "\n", "            ", "if", "target", "not", "in", "model", ".", "supported_targets", ":", "\n", "                ", "raise", "ValueError", "(", "'Unsupported language modeling target: {}'", ".", "format", "(", "target", ")", ")", "\n", "\n", "", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.language_modeling.LanguageModelingTask.load_dataset": [[144, 176], ["language_modeling.LanguageModelingTask.args.data.split", "os.path.join", "fairseq.data.data_utils.load_indexed_dataset", "fairseq.data.TokenBlockDataset", "fairseq.data.MonolingualDataset", "len", "FileNotFoundError", "language_modeling.LanguageModelingTask.dictionary.pad", "language_modeling.LanguageModelingTask.dictionary.eos", "len"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.data_utils.load_indexed_dataset", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos"], ["", "def", "load_dataset", "(", "self", ",", "split", ",", "epoch", "=", "0", ",", "combine", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Load a given dataset split.\n\n        Args:\n            split (str): name of the split (e.g., train, valid, test)\n        \"\"\"", "\n", "paths", "=", "self", ".", "args", ".", "data", ".", "split", "(", "':'", ")", "\n", "assert", "len", "(", "paths", ")", ">", "0", "\n", "data_path", "=", "paths", "[", "epoch", "%", "len", "(", "paths", ")", "]", "\n", "split_path", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "split", ")", "\n", "\n", "dataset", "=", "data_utils", ".", "load_indexed_dataset", "(", "\n", "split_path", ",", "\n", "self", ".", "dictionary", ",", "\n", "self", ".", "args", ".", "dataset_impl", ",", "\n", "combine", "=", "combine", ",", "\n", ")", "\n", "if", "dataset", "is", "None", ":", "\n", "            ", "raise", "FileNotFoundError", "(", "'Dataset not found: {} ({})'", ".", "format", "(", "split", ",", "split_path", ")", ")", "\n", "\n", "", "dataset", "=", "TokenBlockDataset", "(", "\n", "dataset", ",", "dataset", ".", "sizes", ",", "self", ".", "args", ".", "tokens_per_sample", ",", "\n", "pad", "=", "self", ".", "dictionary", ".", "pad", "(", ")", ",", "eos", "=", "self", ".", "dictionary", ".", "eos", "(", ")", ",", "\n", "break_mode", "=", "self", ".", "args", ".", "sample_break_mode", ",", "include_targets", "=", "True", ",", "\n", ")", "\n", "\n", "add_eos_for_other_targets", "=", "self", ".", "args", ".", "sample_break_mode", "is", "not", "None", "and", "self", ".", "args", ".", "sample_break_mode", "!=", "'none'", "\n", "\n", "self", ".", "datasets", "[", "split", "]", "=", "MonolingualDataset", "(", "\n", "dataset", ",", "dataset", ".", "sizes", ",", "self", ".", "dictionary", ",", "self", ".", "output_dictionary", ",", "\n", "add_eos_for_other_targets", "=", "add_eos_for_other_targets", ",", "shuffle", "=", "True", ",", "\n", "targets", "=", "self", ".", "targets", ",", "add_bos_token", "=", "self", ".", "args", ".", "add_bos_token", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.language_modeling.LanguageModelingTask.build_dataset_for_inference": [[178, 201], ["fairseq.data.TransformEosDataset", "fairseq.data.MonolingualDataset", "fairseq.data.TokenBlockDataset", "language_modeling.LanguageModelingTask.source_dictionary.eos", "language_modeling.LanguageModelingTask.source_dictionary.pad", "language_modeling.LanguageModelingTask.source_dictionary.eos"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos"], ["", "def", "build_dataset_for_inference", "(", "self", ",", "src_tokens", ",", "src_lengths", ")", ":", "\n", "        ", "return", "TransformEosDataset", "(", "\n", "MonolingualDataset", "(", "\n", "TokenBlockDataset", "(", "\n", "src_tokens", ",", "\n", "src_lengths", ",", "\n", "block_size", "=", "None", ",", "\n", "pad", "=", "self", ".", "source_dictionary", ".", "pad", "(", ")", ",", "\n", "eos", "=", "self", ".", "source_dictionary", ".", "eos", "(", ")", ",", "\n", "break_mode", "=", "'eos'", ",", "\n", "include_targets", "=", "False", ",", "\n", ")", ",", "\n", "src_lengths", ",", "\n", "self", ".", "source_dictionary", ",", "\n", "self", ".", "target_dictionary", ",", "\n", "add_eos_for_other_targets", "=", "False", ",", "\n", "shuffle", "=", "False", ",", "\n", "add_bos_token", "=", "self", ".", "args", ".", "add_bos_token", ",", "\n", ")", ",", "\n", "eos", "=", "self", ".", "source_dictionary", ".", "eos", "(", ")", ",", "\n", "# remove EOS since this will be used as a prefix for generation", "\n", "remove_eos_from_src", "=", "True", ",", "\n", "has_target", "=", "False", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.language_modeling.LanguageModelingTask.inference_step": [[203, 209], ["torch.no_grad", "generator.generate", "[].nelement"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.sequence_generator.SequenceGenerator.generate"], ["", "def", "inference_step", "(", "self", ",", "generator", ",", "models", ",", "sample", ",", "prefix_tokens", "=", "None", ")", ":", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "if", "prefix_tokens", "is", "None", "and", "sample", "[", "'net_input'", "]", "[", "'src_tokens'", "]", ".", "nelement", "(", ")", ":", "\n", "# note: EOS has already been removed in build_dataset_for_inference", "\n", "                ", "prefix_tokens", "=", "sample", "[", "'net_input'", "]", "[", "'src_tokens'", "]", "\n", "", "return", "generator", ".", "generate", "(", "models", ",", "sample", ",", "prefix_tokens", "=", "prefix_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.language_modeling.LanguageModelingTask.source_dictionary": [[210, 215], ["None"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "source_dictionary", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the :class:`~fairseq.data.Dictionary` for the language\n        model.\"\"\"", "\n", "return", "self", ".", "dictionary", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.language_modeling.LanguageModelingTask.target_dictionary": [[216, 221], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "target_dictionary", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the :class:`~fairseq.data.Dictionary` for the language\n        model.\"\"\"", "\n", "return", "self", ".", "output_dictionary", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.translation_from_pretrained_xlm.TranslationFromPretrainedXLMTask.load_dictionary": [[24, 32], ["fairseq.data.legacy.masked_lm_dictionary.MaskedLMDictionary.load"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load"], ["@", "classmethod", "\n", "def", "load_dictionary", "(", "cls", ",", "filename", ")", ":", "\n", "        ", "\"\"\"Load the masked LM dictionary from the filename\n\n        Args:\n            filename (str): the filename\n        \"\"\"", "\n", "return", "MaskedLMDictionary", ".", "load", "(", "filename", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.masked_lm.MaskedLMTask.add_args": [[32, 57], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add task-specific arguments to the parser.\"\"\"", "\n", "parser", ".", "add_argument", "(", "'data'", ",", "help", "=", "'colon separated path to data directories list, \\\n                            will be iterated upon during epochs in round-robin manner'", ")", "\n", "parser", ".", "add_argument", "(", "'--sample-break-mode'", ",", "default", "=", "'complete'", ",", "\n", "choices", "=", "[", "'none'", ",", "'complete'", ",", "'complete_doc'", ",", "'eos'", "]", ",", "\n", "help", "=", "'If omitted or \"none\", fills each sample with tokens-per-sample '", "\n", "'tokens. If set to \"complete\", splits samples only at the end '", "\n", "'of sentence, but may include multiple sentences per sample. '", "\n", "'\"complete_doc\" is similar but respects doc boundaries. '", "\n", "'If set to \"eos\", includes only one sentence per sample.'", ")", "\n", "parser", ".", "add_argument", "(", "'--tokens-per-sample'", ",", "default", "=", "512", ",", "type", "=", "int", ",", "\n", "help", "=", "'max number of total tokens over all segments '", "\n", "'per sample for BERT dataset'", ")", "\n", "parser", ".", "add_argument", "(", "'--mask-prob'", ",", "default", "=", "0.15", ",", "type", "=", "float", ",", "\n", "help", "=", "'probability of replacing a token with mask'", ")", "\n", "parser", ".", "add_argument", "(", "'--leave-unmasked-prob'", ",", "default", "=", "0.1", ",", "type", "=", "float", ",", "\n", "help", "=", "'probability that a masked token is unmasked'", ")", "\n", "parser", ".", "add_argument", "(", "'--random-token-prob'", ",", "default", "=", "0.1", ",", "type", "=", "float", ",", "\n", "help", "=", "'probability of replacing a token with a random token'", ")", "\n", "parser", ".", "add_argument", "(", "'--freq-weighted-replacement'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'sample random replacement words based on word frequencies'", ")", "\n", "parser", ".", "add_argument", "(", "'--mask-whole-words'", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'mask whole words; you may also want to set --bpe'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.masked_lm.MaskedLMTask.__init__": [[58, 65], ["fairseq.tasks.FairseqTask.__init__", "dictionary.add_symbol"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol"], ["", "def", "__init__", "(", "self", ",", "args", ",", "dictionary", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ")", "\n", "self", ".", "dictionary", "=", "dictionary", "\n", "self", ".", "seed", "=", "args", ".", "seed", "\n", "\n", "# add mask token", "\n", "self", ".", "mask_idx", "=", "dictionary", ".", "add_symbol", "(", "'<mask>'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.masked_lm.MaskedLMTask.setup_task": [[66, 73], ["args.data.split", "fairseq.data.Dictionary.load", "print", "cls", "len", "os.path.join", "len"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.masked_lm_dictionary.BertDictionary.cls"], ["", "@", "classmethod", "\n", "def", "setup_task", "(", "cls", ",", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "paths", "=", "args", ".", "data", ".", "split", "(", "':'", ")", "\n", "assert", "len", "(", "paths", ")", ">", "0", "\n", "dictionary", "=", "Dictionary", ".", "load", "(", "os", ".", "path", ".", "join", "(", "paths", "[", "0", "]", ",", "'dict.txt'", ")", ")", "\n", "print", "(", "'| dictionary: {} types'", ".", "format", "(", "len", "(", "dictionary", ")", ")", ")", "\n", "return", "cls", "(", "args", ",", "dictionary", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.masked_lm.MaskedLMTask.load_dataset": [[74, 172], ["masked_lm.MaskedLMTask.args.data.split", "os.path.join", "fairseq.data.data_utils.load_indexed_dataset", "fairseq.data.TokenBlockDataset", "print", "fairseq.data.PrependTokenDataset", "fairseq.data.MaskTokensDataset.apply_mask", "fairseq.data.SortDataset", "len", "FileNotFoundError", "masked_lm.MaskedLMTask.source_dictionary.bos", "fairseq.data.encoders.build_bpe", "fairseq.data.data_utils.numpy_seed", "numpy.random.permutation", "fairseq.data.NestedDictionaryDataset", "masked_lm.MaskedLMTask.source_dictionary.pad", "masked_lm.MaskedLMTask.source_dictionary.eos", "len", "torch.ByteTensor", "masked_lm.MaskedLMTask.source_dictionary.pad", "len", "len", "tok.startswith", "list", "fairseq.data.IdDataset", "fairseq.data.PadDataset", "fairseq.data.NumSamplesDataset", "fairseq.data.NumelDataset", "fairseq.data.encoders.build_bpe.is_beginning_of_word", "map", "fairseq.data.PadDataset", "fairseq.data.NumelDataset", "range", "masked_lm.MaskedLMTask.source_dictionary.pad", "len", "masked_lm.MaskedLMTask.source_dictionary.pad"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.data_utils.load_indexed_dataset", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.mask_tokens_dataset.MaskTokensDataset.apply_mask", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.bos", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.data_utils.numpy_seed", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.gpt2_bpe.GPT2BPE.is_beginning_of_word", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad"], ["", "def", "load_dataset", "(", "self", ",", "split", ",", "epoch", "=", "0", ",", "combine", "=", "False", ")", ":", "\n", "        ", "\"\"\"Load a given dataset split.\n\n        Args:\n            split (str): name of the split (e.g., train, valid, test)\n        \"\"\"", "\n", "paths", "=", "self", ".", "args", ".", "data", ".", "split", "(", "':'", ")", "\n", "assert", "len", "(", "paths", ")", ">", "0", "\n", "data_path", "=", "paths", "[", "epoch", "%", "len", "(", "paths", ")", "]", "\n", "split_path", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "split", ")", "\n", "\n", "dataset", "=", "data_utils", ".", "load_indexed_dataset", "(", "\n", "split_path", ",", "\n", "self", ".", "source_dictionary", ",", "\n", "self", ".", "args", ".", "dataset_impl", ",", "\n", "combine", "=", "combine", ",", "\n", ")", "\n", "if", "dataset", "is", "None", ":", "\n", "            ", "raise", "FileNotFoundError", "(", "'Dataset not found: {} ({})'", ".", "format", "(", "split", ",", "split_path", ")", ")", "\n", "\n", "# create continuous blocks of tokens", "\n", "", "dataset", "=", "TokenBlockDataset", "(", "\n", "dataset", ",", "\n", "dataset", ".", "sizes", ",", "\n", "self", ".", "args", ".", "tokens_per_sample", "-", "1", ",", "# one less for <s>", "\n", "pad", "=", "self", ".", "source_dictionary", ".", "pad", "(", ")", ",", "\n", "eos", "=", "self", ".", "source_dictionary", ".", "eos", "(", ")", ",", "\n", "break_mode", "=", "self", ".", "args", ".", "sample_break_mode", ",", "\n", ")", "\n", "print", "(", "'| loaded {} batches from: {}'", ".", "format", "(", "len", "(", "dataset", ")", ",", "split_path", ")", ")", "\n", "\n", "# prepend beginning-of-sentence token (<s>, equiv. to [CLS] in BERT)", "\n", "dataset", "=", "PrependTokenDataset", "(", "dataset", ",", "self", ".", "source_dictionary", ".", "bos", "(", ")", ")", "\n", "\n", "# create masked input and targets", "\n", "if", "self", ".", "args", ".", "mask_whole_words", ":", "\n", "            ", "bpe", "=", "encoders", ".", "build_bpe", "(", "self", ".", "args", ")", "\n", "if", "bpe", "is", "not", "None", ":", "\n", "\n", "                ", "def", "is_beginning_of_word", "(", "i", ")", ":", "\n", "                    ", "if", "i", "<", "self", ".", "source_dictionary", ".", "nspecial", ":", "\n", "# special elements are always considered beginnings", "\n", "                        ", "return", "True", "\n", "", "tok", "=", "self", ".", "source_dictionary", "[", "i", "]", "\n", "if", "tok", ".", "startswith", "(", "'madeupword'", ")", ":", "\n", "                        ", "return", "True", "\n", "", "try", ":", "\n", "                        ", "return", "bpe", ".", "is_beginning_of_word", "(", "tok", ")", "\n", "", "except", "ValueError", ":", "\n", "                        ", "return", "True", "\n", "\n", "", "", "mask_whole_words", "=", "torch", ".", "ByteTensor", "(", "list", "(", "\n", "map", "(", "is_beginning_of_word", ",", "range", "(", "len", "(", "self", ".", "source_dictionary", ")", ")", ")", "\n", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "mask_whole_words", "=", "None", "\n", "\n", "", "src_dataset", ",", "tgt_dataset", "=", "MaskTokensDataset", ".", "apply_mask", "(", "\n", "dataset", ",", "\n", "self", ".", "source_dictionary", ",", "\n", "pad_idx", "=", "self", ".", "source_dictionary", ".", "pad", "(", ")", ",", "\n", "mask_idx", "=", "self", ".", "mask_idx", ",", "\n", "seed", "=", "self", ".", "args", ".", "seed", ",", "\n", "mask_prob", "=", "self", ".", "args", ".", "mask_prob", ",", "\n", "leave_unmasked_prob", "=", "self", ".", "args", ".", "leave_unmasked_prob", ",", "\n", "random_token_prob", "=", "self", ".", "args", ".", "random_token_prob", ",", "\n", "freq_weighted_replacement", "=", "self", ".", "args", ".", "freq_weighted_replacement", ",", "\n", "mask_whole_words", "=", "mask_whole_words", ",", "\n", ")", "\n", "\n", "with", "data_utils", ".", "numpy_seed", "(", "self", ".", "args", ".", "seed", "+", "epoch", ")", ":", "\n", "            ", "shuffle", "=", "np", ".", "random", ".", "permutation", "(", "len", "(", "src_dataset", ")", ")", "\n", "\n", "", "self", ".", "datasets", "[", "split", "]", "=", "SortDataset", "(", "\n", "NestedDictionaryDataset", "(", "\n", "{", "\n", "'id'", ":", "IdDataset", "(", ")", ",", "\n", "'net_input'", ":", "{", "\n", "'src_tokens'", ":", "PadDataset", "(", "\n", "src_dataset", ",", "\n", "pad_idx", "=", "self", ".", "source_dictionary", ".", "pad", "(", ")", ",", "\n", "left_pad", "=", "False", ",", "\n", ")", ",", "\n", "'src_lengths'", ":", "NumelDataset", "(", "src_dataset", ",", "reduce", "=", "False", ")", ",", "\n", "}", ",", "\n", "'target'", ":", "PadDataset", "(", "\n", "tgt_dataset", ",", "\n", "pad_idx", "=", "self", ".", "source_dictionary", ".", "pad", "(", ")", ",", "\n", "left_pad", "=", "False", ",", "\n", ")", ",", "\n", "'nsentences'", ":", "NumSamplesDataset", "(", ")", ",", "\n", "'ntokens'", ":", "NumelDataset", "(", "src_dataset", ",", "reduce", "=", "True", ")", ",", "\n", "}", ",", "\n", "sizes", "=", "[", "src_dataset", ".", "sizes", "]", ",", "\n", ")", ",", "\n", "sort_order", "=", "[", "\n", "shuffle", ",", "\n", "src_dataset", ".", "sizes", ",", "\n", "]", ",", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.masked_lm.MaskedLMTask.build_dataset_for_inference": [[175, 202], ["fairseq.data.PadDataset", "fairseq.data.PrependTokenDataset", "fairseq.data.NestedDictionaryDataset", "fairseq.data.TokenBlockDataset", "masked_lm.MaskedLMTask.source_dictionary.bos", "fairseq.data.SortDataset", "masked_lm.MaskedLMTask.source_dictionary.pad", "fairseq.data.IdDataset", "masked_lm.MaskedLMTask.source_dictionary.pad", "masked_lm.MaskedLMTask.source_dictionary.eos", "fairseq.data.NumelDataset"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.bos", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos"], ["", "def", "build_dataset_for_inference", "(", "self", ",", "src_tokens", ",", "src_lengths", ",", "sort", "=", "True", ")", ":", "\n", "        ", "src_dataset", "=", "PadDataset", "(", "\n", "TokenBlockDataset", "(", "\n", "src_tokens", ",", "\n", "src_lengths", ",", "\n", "self", ".", "args", ".", "tokens_per_sample", "-", "1", ",", "# one less for <s>", "\n", "pad", "=", "self", ".", "source_dictionary", ".", "pad", "(", ")", ",", "\n", "eos", "=", "self", ".", "source_dictionary", ".", "eos", "(", ")", ",", "\n", "break_mode", "=", "'eos'", ",", "\n", ")", ",", "\n", "pad_idx", "=", "self", ".", "source_dictionary", ".", "pad", "(", ")", ",", "\n", "left_pad", "=", "False", ",", "\n", ")", "\n", "src_dataset", "=", "PrependTokenDataset", "(", "src_dataset", ",", "self", ".", "source_dictionary", ".", "bos", "(", ")", ")", "\n", "src_dataset", "=", "NestedDictionaryDataset", "(", "\n", "{", "\n", "'id'", ":", "IdDataset", "(", ")", ",", "\n", "'net_input'", ":", "{", "\n", "'src_tokens'", ":", "src_dataset", ",", "\n", "'src_lengths'", ":", "NumelDataset", "(", "src_dataset", ",", "reduce", "=", "False", ")", ",", "\n", "}", ",", "\n", "}", ",", "\n", "sizes", "=", "src_lengths", ",", "\n", ")", "\n", "if", "sort", ":", "\n", "            ", "src_dataset", "=", "SortDataset", "(", "src_dataset", ",", "sort_order", "=", "[", "src_lengths", "]", ")", "\n", "", "return", "src_dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.masked_lm.MaskedLMTask.source_dictionary": [[203, 206], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "source_dictionary", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "dictionary", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.masked_lm.MaskedLMTask.target_dictionary": [[207, 210], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "target_dictionary", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "dictionary", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.audio_pretraining.AudioPretrainingTask.add_args": [[18, 28], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add task-specific arguments to the parser.\"\"\"", "\n", "parser", ".", "add_argument", "(", "'data'", ",", "help", "=", "'path to data directory'", ")", "\n", "parser", ".", "add_argument", "(", "'--sample-rate'", ",", "default", "=", "16000", ",", "type", "=", "int", ",", "\n", "help", "=", "'target sample rate. audio files will be up/down sampled to this rate'", ")", "\n", "parser", ".", "add_argument", "(", "'--max-sample-size'", ",", "default", "=", "None", ",", "type", "=", "int", ",", "\n", "help", "=", "'max sample size to crop to for batching. default = min sample length'", ")", "\n", "parser", ".", "add_argument", "(", "'--min-sample-size'", ",", "default", "=", "None", ",", "type", "=", "int", ",", "\n", "help", "=", "'min sample size to crop to for batching. default = same as --max-sample-size'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.audio_pretraining.AudioPretrainingTask.__init__": [[29, 31], ["FairseqTask.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.audio_pretraining.AudioPretrainingTask.setup_task": [[32, 40], ["cls"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.masked_lm_dictionary.BertDictionary.cls"], ["", "@", "classmethod", "\n", "def", "setup_task", "(", "cls", ",", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Setup the task (e.g., load dictionaries).\n\n        Args:\n            args (argparse.Namespace): parsed command-line arguments\n        \"\"\"", "\n", "return", "cls", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.audio_pretraining.AudioPretrainingTask.load_dataset": [[41, 53], ["os.path.join", "fairseq.data.RawAudioDataset"], "methods", ["None"], ["", "def", "load_dataset", "(", "self", ",", "split", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Load a given dataset split.\n\n        Args:\n            split (str): name of the split (e.g., train, valid, test)\n        \"\"\"", "\n", "\n", "manifest", "=", "os", ".", "path", ".", "join", "(", "self", ".", "args", ".", "data", ",", "'{}.tsv'", ".", "format", "(", "split", ")", ")", "\n", "self", ".", "datasets", "[", "split", "]", "=", "RawAudioDataset", "(", "manifest", ",", "\n", "sample_rate", "=", "self", ".", "args", ".", "sample_rate", ",", "\n", "max_sample_size", "=", "self", ".", "args", ".", "max_sample_size", ",", "\n", "min_sample_size", "=", "self", ".", "args", ".", "min_sample_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.audio_pretraining.AudioPretrainingTask.target_dictionary": [[54, 59], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "target_dictionary", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the :class:`~fairseq.data.Dictionary` for the language\n        model.\"\"\"", "\n", "return", "None", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.sentence_ranking.SentenceRankingTask.add_args": [[37, 53], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add task-specific arguments to the parser.\"\"\"", "\n", "parser", ".", "add_argument", "(", "'data'", ",", "metavar", "=", "'FILE'", ",", "\n", "help", "=", "'file prefix for data'", ")", "\n", "parser", ".", "add_argument", "(", "'--num-classes'", ",", "type", "=", "int", ",", "\n", "help", "=", "'number of sentences to be ranked'", ")", "\n", "parser", ".", "add_argument", "(", "'--init-token'", ",", "type", "=", "int", ",", "\n", "help", "=", "'add token at the beginning of each batch item'", ")", "\n", "parser", ".", "add_argument", "(", "'--separator-token'", ",", "type", "=", "int", ",", "\n", "help", "=", "'add separator token between inputs'", ")", "\n", "parser", ".", "add_argument", "(", "'--no-shuffle'", ",", "action", "=", "'store_true'", ")", "\n", "parser", ".", "add_argument", "(", "'--truncate-sequence'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Truncate sequence to max_positions'", ")", "\n", "parser", ".", "add_argument", "(", "'--max-option-length'", ",", "type", "=", "int", ",", "\n", "help", "=", "'max length for each option'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.sentence_ranking.SentenceRankingTask.__init__": [[54, 57], ["FairseqTask.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["", "def", "__init__", "(", "self", ",", "args", ",", "dictionary", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ")", "\n", "self", ".", "dictionary", "=", "dictionary", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.sentence_ranking.SentenceRankingTask.load_dictionary": [[58, 68], ["fairseq.data.Dictionary.load", "fairseq.data.Dictionary.load.add_symbol"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol"], ["", "@", "classmethod", "\n", "def", "load_dictionary", "(", "cls", ",", "args", ",", "filename", ",", "source", "=", "True", ")", ":", "\n", "        ", "\"\"\"Load the dictionary from the filename\n\n        Args:\n            filename (str): the filename\n        \"\"\"", "\n", "dictionary", "=", "Dictionary", ".", "load", "(", "filename", ")", "\n", "dictionary", ".", "add_symbol", "(", "'<mask>'", ")", "\n", "return", "dictionary", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.sentence_ranking.SentenceRankingTask.setup_task": [[69, 82], ["cls.load_dictionary", "print", "sentence_ranking.SentenceRankingTask", "os.path.join", "len"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.sentence_prediction.SentencePredictionTask.load_dictionary", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["", "@", "classmethod", "\n", "def", "setup_task", "(", "cls", ",", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "assert", "args", ".", "criterion", "==", "'sentence_ranking'", ",", "'Must set --criterion=sentence_ranking'", "\n", "\n", "# load data dictionary", "\n", "data_dict", "=", "cls", ".", "load_dictionary", "(", "\n", "args", ",", "\n", "os", ".", "path", ".", "join", "(", "args", ".", "data", ",", "'input0'", ",", "'dict.txt'", ")", ",", "\n", "source", "=", "True", ",", "\n", ")", "\n", "print", "(", "'| [input] dictionary: {} types'", ".", "format", "(", "len", "(", "data_dict", ")", ")", ")", "\n", "return", "SentenceRankingTask", "(", "args", ",", "data_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.sentence_ranking.SentenceRankingTask.load_dataset": [[83, 172], ["sentence_ranking.SentenceRankingTask.load_dataset.make_dataset"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.make_dataset"], ["", "def", "load_dataset", "(", "self", ",", "split", ",", "combine", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Load a given dataset split (e.g., train, valid, test).\"\"\"", "\n", "\n", "def", "get_path", "(", "type", ",", "split", ")", ":", "\n", "            ", "return", "os", ".", "path", ".", "join", "(", "self", ".", "args", ".", "data", ",", "type", ",", "split", ")", "\n", "\n", "", "def", "make_dataset", "(", "type", ",", "dictionary", ")", ":", "\n", "            ", "split_path", "=", "get_path", "(", "type", ",", "split", ")", "\n", "\n", "dataset", "=", "data_utils", ".", "load_indexed_dataset", "(", "\n", "split_path", ",", "\n", "self", ".", "source_dictionary", ",", "\n", "self", ".", "args", ".", "dataset_impl", ",", "\n", "combine", "=", "combine", ",", "\n", ")", "\n", "return", "dataset", "\n", "\n", "", "input0", "=", "make_dataset", "(", "'input0'", ",", "self", ".", "source_dictionary", ")", "\n", "input_options", "=", "[", "\n", "make_dataset", "(", "\n", "'input{idx}'", ".", "format", "(", "idx", "=", "idx", "+", "1", ")", ",", "\n", "self", ".", "source_dictionary", "\n", ")", "\n", "for", "idx", "in", "range", "(", "self", ".", "args", ".", "num_classes", ")", "\n", "]", "\n", "\n", "if", "self", ".", "args", ".", "separator_token", "is", "not", "None", ":", "\n", "            ", "input0", "=", "PrependTokenDataset", "(", "input0", ",", "self", ".", "args", ".", "separator_token", ")", "\n", "\n", "", "src_tokens", "=", "[", "]", "\n", "for", "input_option", "in", "input_options", ":", "\n", "            ", "if", "self", ".", "args", ".", "init_token", "is", "not", "None", ":", "\n", "                ", "input_option", "=", "PrependTokenDataset", "(", "input_option", ",", "self", ".", "args", ".", "init_token", ")", "\n", "", "if", "self", ".", "args", ".", "max_option_length", "is", "not", "None", ":", "\n", "                ", "input_option", "=", "TruncateDataset", "(", "input_option", ",", "self", ".", "args", ".", "max_option_length", ")", "\n", "", "src_token", "=", "ConcatSentencesDataset", "(", "input_option", ",", "input0", ")", "\n", "if", "self", ".", "args", ".", "truncate_sequence", ":", "\n", "                ", "src_token", "=", "TruncateDataset", "(", "src_token", ",", "self", ".", "args", ".", "max_positions", ")", "\n", "", "src_tokens", ".", "append", "(", "src_token", ")", "\n", "\n", "", "with", "data_utils", ".", "numpy_seed", "(", "self", ".", "args", ".", "seed", ")", ":", "\n", "            ", "shuffle", "=", "np", ".", "random", ".", "permutation", "(", "len", "(", "src_tokens", "[", "0", "]", ")", ")", "\n", "\n", "", "dataset", "=", "{", "\n", "'id'", ":", "IdDataset", "(", ")", ",", "\n", "'nsentences'", ":", "NumSamplesDataset", "(", ")", ",", "\n", "'ntokens'", ":", "NumelDataset", "(", "src_tokens", "[", "0", "]", ",", "reduce", "=", "True", ")", ",", "\n", "}", "\n", "\n", "for", "src_token_idx", "in", "range", "(", "len", "(", "src_tokens", ")", ")", ":", "\n", "            ", "dataset", ".", "update", "(", "\n", "{", "\n", "'net_input{idx}'", ".", "format", "(", "idx", "=", "src_token_idx", "+", "1", ")", ":", "{", "\n", "'src_tokens'", ":", "RightPadDataset", "(", "\n", "src_tokens", "[", "src_token_idx", "]", ",", "\n", "pad_idx", "=", "self", ".", "source_dictionary", ".", "pad", "(", ")", ",", "\n", ")", ",", "\n", "'src_lengths'", ":", "NumelDataset", "(", "src_tokens", "[", "src_token_idx", "]", ",", "reduce", "=", "False", ")", ",", "\n", "}", "\n", "}", "\n", ")", "\n", "\n", "", "label_path", "=", "'{}.label'", ".", "format", "(", "get_path", "(", "'label'", ",", "split", ")", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "label_path", ")", ":", "\n", "            ", "with", "open", "(", "label_path", ")", "as", "h", ":", "\n", "                ", "dataset", ".", "update", "(", "\n", "target", "=", "RawLabelDataset", "(", "[", "\n", "int", "(", "x", ".", "strip", "(", ")", ")", "for", "x", "in", "h", ".", "readlines", "(", ")", "\n", "]", ")", "\n", ")", "\n", "\n", "", "", "nested_dataset", "=", "NestedDictionaryDataset", "(", "\n", "dataset", ",", "\n", "sizes", "=", "[", "np", ".", "maximum", ".", "reduce", "(", "[", "src_token", ".", "sizes", "for", "src_token", "in", "src_tokens", "]", ")", "]", ",", "\n", ")", "\n", "\n", "if", "self", ".", "args", ".", "no_shuffle", ":", "\n", "            ", "dataset", "=", "nested_dataset", "\n", "", "else", ":", "\n", "            ", "dataset", "=", "SortDataset", "(", "\n", "nested_dataset", ",", "\n", "# shuffle", "\n", "sort_order", "=", "[", "shuffle", "]", ",", "\n", ")", "\n", "\n", "", "print", "(", "\"| Loaded {0} with #samples: {1}\"", ".", "format", "(", "split", ",", "len", "(", "dataset", ")", ")", ")", "\n", "\n", "self", ".", "datasets", "[", "split", "]", "=", "dataset", "\n", "return", "self", ".", "datasets", "[", "split", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.sentence_ranking.SentenceRankingTask.build_model": [[173, 183], ["models.build_model", "models.build_model.register_classification_head"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv_lm.LightConvLanguageModel.build_model", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.model.RobertaModel.register_classification_head"], ["", "def", "build_model", "(", "self", ",", "args", ")", ":", "\n", "        ", "from", "fairseq", "import", "models", "\n", "model", "=", "models", ".", "build_model", "(", "args", ",", "self", ")", "\n", "\n", "model", ".", "register_classification_head", "(", "\n", "'sentence_classification_head'", ",", "\n", "num_classes", "=", "1", ",", "\n", ")", "\n", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.sentence_ranking.SentenceRankingTask.max_positions": [[184, 186], ["None"], "methods", ["None"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "args", ".", "max_positions", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.sentence_ranking.SentenceRankingTask.source_dictionary": [[187, 190], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "source_dictionary", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "dictionary", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.sentence_ranking.SentenceRankingTask.target_dictionary": [[191, 194], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "target_dictionary", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "dictionary", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.__init__.setup_task": [[16, 18], ["TASK_REGISTRY[].setup_task"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.sentence_prediction.SentencePredictionTask.setup_task"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.__init__.register_task": [[20, 54], ["TASK_CLASS_NAMES.add", "ValueError", "issubclass", "ValueError", "ValueError"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.add"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.__init__.get_task": [[75, 77], ["None"], "function", ["None"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.sentence_prediction.SentencePredictionTask.add_args": [[39, 54], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add task-specific arguments to the parser.\"\"\"", "\n", "parser", ".", "add_argument", "(", "'data'", ",", "metavar", "=", "'FILE'", ",", "\n", "help", "=", "'file prefix for data'", ")", "\n", "parser", ".", "add_argument", "(", "'--num-classes'", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "'number of classes'", ")", "\n", "parser", ".", "add_argument", "(", "'--init-token'", ",", "type", "=", "int", ",", "default", "=", "None", ",", "\n", "help", "=", "'add token at the beginning of each batch item'", ")", "\n", "parser", ".", "add_argument", "(", "'--separator-token'", ",", "type", "=", "int", ",", "default", "=", "None", ",", "\n", "help", "=", "'add separator token between inputs'", ")", "\n", "parser", ".", "add_argument", "(", "'--regression-target'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'--no-shuffle'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'--truncate-sequence'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "\n", "help", "=", "'Truncate sequence to max_sequence_length'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.sentence_prediction.SentencePredictionTask.__init__": [[55, 59], ["FairseqTask.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["", "def", "__init__", "(", "self", ",", "args", ",", "data_dictionary", ",", "label_dictionary", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ")", "\n", "self", ".", "dictionary", "=", "data_dictionary", "\n", "self", ".", "label_dictionary", "=", "label_dictionary", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.sentence_prediction.SentencePredictionTask.load_dictionary": [[60, 70], ["fairseq.data.Dictionary.load", "fairseq.data.Dictionary.load.add_symbol"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol"], ["", "@", "classmethod", "\n", "def", "load_dictionary", "(", "cls", ",", "args", ",", "filename", ",", "source", "=", "True", ")", ":", "\n", "        ", "\"\"\"Load the dictionary from the filename\n\n        Args:\n            filename (str): the filename\n        \"\"\"", "\n", "dictionary", "=", "Dictionary", ".", "load", "(", "filename", ")", "\n", "dictionary", ".", "add_symbol", "(", "'<mask>'", ")", "\n", "return", "dictionary", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.sentence_prediction.SentencePredictionTask.setup_task": [[71, 97], ["cls.load_dictionary", "print", "sentence_prediction.SentencePredictionTask", "os.path.join", "cls.load_dictionary", "print", "len", "os.path.join", "len"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.sentence_prediction.SentencePredictionTask.load_dictionary", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.sentence_prediction.SentencePredictionTask.load_dictionary", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["", "@", "classmethod", "\n", "def", "setup_task", "(", "cls", ",", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "assert", "args", ".", "num_classes", ">", "0", ",", "'Must set --num-classes'", "\n", "\n", "args", ".", "tokens_per_sample", "=", "args", ".", "max_positions", "\n", "\n", "# load data dictionary", "\n", "data_dict", "=", "cls", ".", "load_dictionary", "(", "\n", "args", ",", "\n", "os", ".", "path", ".", "join", "(", "args", ".", "data", ",", "'input0'", ",", "'dict.txt'", ")", ",", "\n", "source", "=", "True", ",", "\n", ")", "\n", "print", "(", "'| [input] dictionary: {} types'", ".", "format", "(", "len", "(", "data_dict", ")", ")", ")", "\n", "\n", "label_dict", "=", "None", "\n", "if", "not", "args", ".", "regression_target", ":", "\n", "# load label dictionary", "\n", "            ", "label_dict", "=", "cls", ".", "load_dictionary", "(", "\n", "args", ",", "\n", "os", ".", "path", ".", "join", "(", "args", ".", "data", ",", "'label'", ",", "'dict.txt'", ")", ",", "\n", "source", "=", "False", ",", "\n", ")", "\n", "print", "(", "'| [label] dictionary: {} types'", ".", "format", "(", "len", "(", "label_dict", ")", ")", ")", "\n", "", "else", ":", "\n", "            ", "label_dict", "=", "data_dict", "\n", "", "return", "SentencePredictionTask", "(", "args", ",", "data_dict", ",", "label_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.sentence_prediction.SentencePredictionTask.load_dataset": [[98, 187], ["sentence_prediction.SentencePredictionTask.load_dataset.make_dataset"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.make_dataset"], ["", "def", "load_dataset", "(", "self", ",", "split", ",", "combine", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Load a given dataset split (e.g., train, valid, test).\"\"\"", "\n", "def", "get_path", "(", "type", ",", "split", ")", ":", "\n", "            ", "return", "os", ".", "path", ".", "join", "(", "self", ".", "args", ".", "data", ",", "type", ",", "split", ")", "\n", "\n", "", "def", "make_dataset", "(", "type", ",", "dictionary", ")", ":", "\n", "            ", "split_path", "=", "get_path", "(", "type", ",", "split", ")", "\n", "\n", "dataset", "=", "data_utils", ".", "load_indexed_dataset", "(", "\n", "split_path", ",", "\n", "self", ".", "source_dictionary", ",", "\n", "self", ".", "args", ".", "dataset_impl", ",", "\n", "combine", "=", "combine", ",", "\n", ")", "\n", "return", "dataset", "\n", "\n", "", "input0", "=", "make_dataset", "(", "'input0'", ",", "self", ".", "source_dictionary", ")", "\n", "assert", "input0", "is", "not", "None", ",", "'could not find dataset: {}'", ".", "format", "(", "get_path", "(", "type", ",", "split", ")", ")", "\n", "input1", "=", "make_dataset", "(", "'input1'", ",", "self", ".", "source_dictionary", ")", "\n", "\n", "if", "self", ".", "args", ".", "init_token", "is", "not", "None", ":", "\n", "            ", "input0", "=", "PrependTokenDataset", "(", "input0", ",", "self", ".", "args", ".", "init_token", ")", "\n", "\n", "", "if", "input1", "is", "None", ":", "\n", "            ", "src_tokens", "=", "input0", "\n", "", "else", ":", "\n", "            ", "if", "self", ".", "args", ".", "separator_token", "is", "not", "None", ":", "\n", "                ", "input1", "=", "PrependTokenDataset", "(", "input1", ",", "self", ".", "args", ".", "separator_token", ")", "\n", "\n", "", "src_tokens", "=", "ConcatSentencesDataset", "(", "input0", ",", "input1", ")", "\n", "\n", "", "with", "data_utils", ".", "numpy_seed", "(", "self", ".", "args", ".", "seed", ")", ":", "\n", "            ", "shuffle", "=", "np", ".", "random", ".", "permutation", "(", "len", "(", "src_tokens", ")", ")", "\n", "\n", "", "if", "self", ".", "args", ".", "truncate_sequence", ":", "\n", "            ", "src_tokens", "=", "TruncateDataset", "(", "src_tokens", ",", "self", ".", "args", ".", "max_positions", ")", "\n", "\n", "", "dataset", "=", "{", "\n", "'id'", ":", "IdDataset", "(", ")", ",", "\n", "'net_input'", ":", "{", "\n", "'src_tokens'", ":", "RightPadDataset", "(", "\n", "src_tokens", ",", "\n", "pad_idx", "=", "self", ".", "source_dictionary", ".", "pad", "(", ")", ",", "\n", ")", ",", "\n", "'src_lengths'", ":", "NumelDataset", "(", "src_tokens", ",", "reduce", "=", "False", ")", ",", "\n", "}", ",", "\n", "'nsentences'", ":", "NumSamplesDataset", "(", ")", ",", "\n", "'ntokens'", ":", "NumelDataset", "(", "src_tokens", ",", "reduce", "=", "True", ")", ",", "\n", "}", "\n", "\n", "if", "not", "self", ".", "args", ".", "regression_target", ":", "\n", "            ", "label_dataset", "=", "make_dataset", "(", "'label'", ",", "self", ".", "target_dictionary", ")", "\n", "if", "label_dataset", "is", "not", "None", ":", "\n", "                ", "dataset", ".", "update", "(", "\n", "target", "=", "OffsetTokensDataset", "(", "\n", "StripTokenDataset", "(", "\n", "label_dataset", ",", "\n", "id_to_strip", "=", "self", ".", "target_dictionary", ".", "eos", "(", ")", ",", "\n", ")", ",", "\n", "offset", "=", "-", "self", ".", "target_dictionary", ".", "nspecial", ",", "\n", ")", "\n", ")", "\n", "", "", "else", ":", "\n", "            ", "label_path", "=", "\"{0}.label\"", ".", "format", "(", "get_path", "(", "'label'", ",", "split", ")", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "label_path", ")", ":", "\n", "                ", "dataset", ".", "update", "(", "\n", "target", "=", "RawLabelDataset", "(", "[", "\n", "float", "(", "x", ".", "strip", "(", ")", ")", "for", "x", "in", "open", "(", "label_path", ")", ".", "readlines", "(", ")", "\n", "]", ")", "\n", ")", "\n", "\n", "", "", "nested_dataset", "=", "NestedDictionaryDataset", "(", "\n", "dataset", ",", "\n", "sizes", "=", "[", "src_tokens", ".", "sizes", "]", ",", "\n", ")", "\n", "\n", "if", "self", ".", "args", ".", "no_shuffle", ":", "\n", "            ", "dataset", "=", "nested_dataset", "\n", "", "else", ":", "\n", "            ", "dataset", "=", "SortDataset", "(", "\n", "nested_dataset", ",", "\n", "# shuffle", "\n", "sort_order", "=", "[", "shuffle", "]", ",", "\n", ")", "\n", "\n", "", "print", "(", "\"| Loaded {0} with #samples: {1}\"", ".", "format", "(", "split", ",", "len", "(", "dataset", ")", ")", ")", "\n", "\n", "self", ".", "datasets", "[", "split", "]", "=", "dataset", "\n", "return", "self", ".", "datasets", "[", "split", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.sentence_prediction.SentencePredictionTask.build_model": [[188, 198], ["models.build_model", "models.build_model.register_classification_head"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv_lm.LightConvLanguageModel.build_model", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.model.RobertaModel.register_classification_head"], ["", "def", "build_model", "(", "self", ",", "args", ")", ":", "\n", "        ", "from", "fairseq", "import", "models", "\n", "model", "=", "models", ".", "build_model", "(", "args", ",", "self", ")", "\n", "\n", "model", ".", "register_classification_head", "(", "\n", "'sentence_classification_head'", ",", "\n", "num_classes", "=", "self", ".", "args", ".", "num_classes", ",", "\n", ")", "\n", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.sentence_prediction.SentencePredictionTask.max_positions": [[199, 201], ["None"], "methods", ["None"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "args", ".", "max_positions", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.sentence_prediction.SentencePredictionTask.source_dictionary": [[202, 205], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "source_dictionary", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "dictionary", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.sentence_prediction.SentencePredictionTask.target_dictionary": [[206, 209], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "target_dictionary", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "label_dictionary", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.translation_moe.TranslationMoETask.add_args": [[38, 55], ["fairseq.tasks.translation.TranslationTask.add_args", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.add_args"], ["@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add task-specific arguments to the parser.\"\"\"", "\n", "# fmt: off", "\n", "TranslationTask", ".", "add_args", "(", "parser", ")", "\n", "parser", ".", "add_argument", "(", "'--method'", ",", "default", "=", "'hMoEup'", ",", "\n", "choices", "=", "[", "'sMoElp'", ",", "'sMoEup'", ",", "'hMoElp'", ",", "'hMoEup'", "]", ")", "\n", "parser", ".", "add_argument", "(", "'--num-experts'", ",", "default", "=", "3", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'number of experts'", ")", "\n", "parser", ".", "add_argument", "(", "'--mean-pool-gating-network'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'use a simple mean-pooling gating network'", ")", "\n", "parser", ".", "add_argument", "(", "'--mean-pool-gating-network-dropout'", ",", "type", "=", "float", ",", "\n", "help", "=", "'dropout for mean-pooling gating network'", ")", "\n", "parser", ".", "add_argument", "(", "'--mean-pool-gating-network-encoder-dim'", ",", "type", "=", "float", ",", "\n", "help", "=", "'encoder output dim for mean-pooling gating network'", ")", "\n", "parser", ".", "add_argument", "(", "'--gen-expert'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "'which expert to use for generation'", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.translation_moe.TranslationMoETask.__init__": [[57, 82], ["range", "fairseq.tasks.translation.TranslationTask.__init__", "src_dict.add_symbol", "tgt_dict.add_symbol"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol"], ["", "def", "__init__", "(", "self", ",", "args", ",", "src_dict", ",", "tgt_dict", ")", ":", "\n", "        ", "if", "args", ".", "method", "==", "'sMoElp'", ":", "\n", "# soft MoE with learned prior", "\n", "            ", "self", ".", "uniform_prior", "=", "False", "\n", "self", ".", "hard_selection", "=", "False", "\n", "", "elif", "args", ".", "method", "==", "'sMoEup'", ":", "\n", "# soft MoE with uniform prior", "\n", "            ", "self", ".", "uniform_prior", "=", "True", "\n", "self", ".", "hard_selection", "=", "False", "\n", "", "elif", "args", ".", "method", "==", "'hMoElp'", ":", "\n", "# hard MoE with learned prior", "\n", "            ", "self", ".", "uniform_prior", "=", "False", "\n", "self", ".", "hard_selection", "=", "True", "\n", "", "elif", "args", ".", "method", "==", "'hMoEup'", ":", "\n", "# hard MoE with uniform prior", "\n", "            ", "self", ".", "uniform_prior", "=", "True", "\n", "self", ".", "hard_selection", "=", "True", "\n", "\n", "# add indicator tokens for each expert", "\n", "", "for", "i", "in", "range", "(", "args", ".", "num_experts", ")", ":", "\n", "# add to both dictionaries in case we're sharing embeddings", "\n", "            ", "src_dict", ".", "add_symbol", "(", "'<expert_{}>'", ".", "format", "(", "i", ")", ")", "\n", "tgt_dict", ".", "add_symbol", "(", "'<expert_{}>'", ".", "format", "(", "i", ")", ")", "\n", "\n", "", "super", "(", ")", ".", "__init__", "(", "args", ",", "src_dict", ",", "tgt_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.translation_moe.TranslationMoETask.build_model": [[83, 112], ["models.build_model", "hasattr", "getattr", "getattr", "fairseq.modules.MeanPoolGatingNetwork", "ValueError", "getattr", "getattr", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv_lm.LightConvLanguageModel.build_model"], ["", "def", "build_model", "(", "self", ",", "args", ")", ":", "\n", "        ", "from", "fairseq", "import", "models", "\n", "model", "=", "models", ".", "build_model", "(", "args", ",", "self", ")", "\n", "if", "not", "self", ".", "uniform_prior", "and", "not", "hasattr", "(", "model", ",", "'gating_network'", ")", ":", "\n", "            ", "if", "self", ".", "args", ".", "mean_pool_gating_network", ":", "\n", "                ", "if", "getattr", "(", "args", ",", "'mean_pool_gating_network_encoder_dim'", ",", "None", ")", ":", "\n", "                    ", "encoder_dim", "=", "args", ".", "mean_pool_gating_network_encoder_dim", "\n", "", "elif", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "None", ")", ":", "\n", "# assume that encoder_embed_dim is the encoder's output dimension", "\n", "                    ", "encoder_dim", "=", "args", ".", "encoder_embed_dim", "\n", "", "else", ":", "\n", "                    ", "raise", "ValueError", "(", "'Must specify --mean-pool-gating-network-encoder-dim'", ")", "\n", "\n", "", "if", "getattr", "(", "args", ",", "'mean_pool_gating_network_dropout'", ",", "None", ")", ":", "\n", "                    ", "dropout", "=", "args", ".", "mean_pool_gating_network_dropout", "\n", "", "elif", "getattr", "(", "args", ",", "'dropout'", ",", "None", ")", ":", "\n", "                    ", "dropout", "=", "args", ".", "dropout", "\n", "", "else", ":", "\n", "                    ", "raise", "ValueError", "(", "'Must specify --mean-pool-gating-network-dropout'", ")", "\n", "\n", "", "model", ".", "gating_network", "=", "modules", ".", "MeanPoolGatingNetwork", "(", "\n", "encoder_dim", ",", "args", ".", "num_experts", ",", "dropout", ",", "\n", ")", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "'translation_moe task with learned prior requires the model to '", "\n", "'have a gating network; try using --mean-pool-gating-network'", "\n", ")", "\n", "", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.translation_moe.TranslationMoETask.expert_index": [[113, 115], ["translation_moe.TranslationMoETask.tgt_dict.index"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.index"], ["", "def", "expert_index", "(", "self", ",", "i", ")", ":", "\n", "        ", "return", "i", "+", "self", ".", "tgt_dict", ".", "index", "(", "'<expert_0>'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.translation_moe.TranslationMoETask._get_loss": [[116, 179], ["hasattr", "sample[].size", "loss.view.view.sum", "model.decoder", "criterion.compute_loss", "loss.view.view.view", "model.encoder", "fairseq.utils.eval", "translation_moe.TranslationMoETask._get_loss.get_lprob_yz"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.FairseqMultiModel.decoder", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.label_smoothed_cross_entropy.LabelSmoothedCrossEntropyCriterion.compute_loss", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.FairseqMultiModel.encoder", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval"], ["", "def", "_get_loss", "(", "self", ",", "sample", ",", "model", ",", "criterion", ")", ":", "\n", "        ", "assert", "hasattr", "(", "criterion", ",", "'compute_loss'", ")", ",", "'translation_moe task requires the criterion to implement the compute_loss() method'", "\n", "\n", "k", "=", "self", ".", "args", ".", "num_experts", "\n", "bsz", "=", "sample", "[", "'target'", "]", ".", "size", "(", "0", ")", "\n", "\n", "def", "get_lprob_y", "(", "encoder_out", ",", "prev_output_tokens_k", ")", ":", "\n", "            ", "net_output", "=", "model", ".", "decoder", "(", "prev_output_tokens_k", ",", "encoder_out", ")", "\n", "loss", ",", "_", "=", "criterion", ".", "compute_loss", "(", "model", ",", "net_output", ",", "sample", ",", "reduce", "=", "False", ")", "\n", "loss", "=", "loss", ".", "view", "(", "bsz", ",", "-", "1", ")", "\n", "return", "-", "loss", ".", "sum", "(", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "# -> B x 1", "\n", "\n", "", "def", "get_lprob_yz", "(", "winners", "=", "None", ")", ":", "\n", "            ", "encoder_out", "=", "model", ".", "encoder", "(", "sample", "[", "'net_input'", "]", "[", "'src_tokens'", "]", ",", "sample", "[", "'net_input'", "]", "[", "'src_lengths'", "]", ")", "\n", "\n", "if", "winners", "is", "None", ":", "\n", "                ", "lprob_y", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "k", ")", ":", "\n", "                    ", "prev_output_tokens_k", "=", "sample", "[", "'net_input'", "]", "[", "'prev_output_tokens'", "]", ".", "clone", "(", ")", "\n", "assert", "not", "prev_output_tokens_k", ".", "requires_grad", "\n", "prev_output_tokens_k", "[", ":", ",", "0", "]", "=", "self", ".", "expert_index", "(", "i", ")", "\n", "lprob_y", ".", "append", "(", "get_lprob_y", "(", "encoder_out", ",", "prev_output_tokens_k", ")", ")", "\n", "", "lprob_y", "=", "torch", ".", "cat", "(", "lprob_y", ",", "dim", "=", "1", ")", "# -> B x K", "\n", "", "else", ":", "\n", "                ", "prev_output_tokens_k", "=", "sample", "[", "'net_input'", "]", "[", "'prev_output_tokens'", "]", ".", "clone", "(", ")", "\n", "prev_output_tokens_k", "[", ":", ",", "0", "]", "=", "self", ".", "expert_index", "(", "winners", ")", "\n", "lprob_y", "=", "get_lprob_y", "(", "encoder_out", ",", "prev_output_tokens_k", ")", "# -> B", "\n", "\n", "", "if", "self", ".", "uniform_prior", ":", "\n", "                ", "lprob_yz", "=", "lprob_y", "\n", "", "else", ":", "\n", "                ", "lprob_z", "=", "model", ".", "gating_network", "(", "encoder_out", ")", "# B x K", "\n", "if", "winners", "is", "not", "None", ":", "\n", "                    ", "lprob_z", "=", "lprob_z", ".", "gather", "(", "dim", "=", "1", ",", "index", "=", "winners", ".", "unsqueeze", "(", "-", "1", ")", ")", "\n", "", "lprob_yz", "=", "lprob_y", "+", "lprob_z", ".", "type_as", "(", "lprob_y", ")", "# B x K", "\n", "\n", "", "return", "lprob_yz", "\n", "\n", "# compute responsibilities without dropout", "\n", "", "with", "utils", ".", "eval", "(", "model", ")", ":", "# disable dropout", "\n", "            ", "with", "torch", ".", "no_grad", "(", ")", ":", "# disable autograd", "\n", "                ", "lprob_yz", "=", "get_lprob_yz", "(", ")", "# B x K", "\n", "prob_z_xy", "=", "torch", ".", "nn", ".", "functional", ".", "softmax", "(", "lprob_yz", ",", "dim", "=", "1", ")", "\n", "", "", "assert", "not", "prob_z_xy", ".", "requires_grad", "\n", "\n", "# compute loss with dropout", "\n", "if", "self", ".", "hard_selection", ":", "\n", "            ", "winners", "=", "prob_z_xy", ".", "max", "(", "dim", "=", "1", ")", "[", "1", "]", "\n", "loss", "=", "-", "get_lprob_yz", "(", "winners", ")", "\n", "", "else", ":", "\n", "            ", "lprob_yz", "=", "get_lprob_yz", "(", ")", "# B x K", "\n", "loss", "=", "-", "modules", ".", "LogSumExpMoE", ".", "apply", "(", "lprob_yz", ",", "prob_z_xy", ",", "1", ")", "\n", "\n", "", "loss", "=", "loss", ".", "sum", "(", ")", "\n", "sample_size", "=", "sample", "[", "'target'", "]", ".", "size", "(", "0", ")", "if", "self", ".", "args", ".", "sentence_avg", "else", "sample", "[", "'ntokens'", "]", "\n", "logging_output", "=", "{", "\n", "'loss'", ":", "utils", ".", "item", "(", "loss", ".", "data", ")", ",", "\n", "'ntokens'", ":", "sample", "[", "'ntokens'", "]", ",", "\n", "'sample_size'", ":", "sample_size", ",", "\n", "'posterior'", ":", "prob_z_xy", ".", "float", "(", ")", ".", "sum", "(", "dim", "=", "0", ")", ".", "cpu", "(", ")", ",", "\n", "}", "\n", "return", "loss", ",", "sample_size", ",", "logging_output", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.translation_moe.TranslationMoETask.train_step": [[180, 187], ["model.train", "translation_moe.TranslationMoETask._get_loss", "optimizer.backward"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq-stack-transformer.train.train", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.translation_moe.TranslationMoETask._get_loss", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.scalar_bias.ScalarBias.backward"], ["", "def", "train_step", "(", "self", ",", "sample", ",", "model", ",", "criterion", ",", "optimizer", ",", "ignore_grad", "=", "False", ")", ":", "\n", "        ", "model", ".", "train", "(", ")", "\n", "loss", ",", "sample_size", ",", "logging_output", "=", "self", ".", "_get_loss", "(", "sample", ",", "model", ",", "criterion", ")", "\n", "if", "ignore_grad", ":", "\n", "            ", "loss", "*=", "0", "\n", "", "optimizer", ".", "backward", "(", "loss", ")", "\n", "return", "loss", ",", "sample_size", ",", "logging_output", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.translation_moe.TranslationMoETask.valid_step": [[188, 193], ["model.eval", "torch.no_grad", "translation_moe.TranslationMoETask._get_loss"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.translation_moe.TranslationMoETask._get_loss"], ["", "def", "valid_step", "(", "self", ",", "sample", ",", "model", ",", "criterion", ")", ":", "\n", "        ", "model", ".", "eval", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "loss", ",", "sample_size", ",", "logging_output", "=", "self", ".", "_get_loss", "(", "sample", ",", "model", ",", "criterion", ")", "\n", "", "return", "loss", ",", "sample_size", ",", "logging_output", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.translation_moe.TranslationMoETask.inference_step": [[194, 202], ["torch.no_grad", "generator.generate", "translation_moe.TranslationMoETask.expert_index"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.sequence_generator.SequenceGenerator.generate", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.translation_moe.TranslationMoETask.expert_index"], ["", "def", "inference_step", "(", "self", ",", "generator", ",", "models", ",", "sample", ",", "prefix_tokens", "=", "None", ",", "expert", "=", "None", ")", ":", "\n", "        ", "expert", "=", "expert", "or", "self", ".", "args", ".", "gen_expert", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "return", "generator", ".", "generate", "(", "\n", "models", ",", "\n", "sample", ",", "\n", "prefix_tokens", "=", "prefix_tokens", ",", "\n", "bos_token", "=", "self", ".", "expert_index", "(", "expert", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.translation_moe.TranslationMoETask.aggregate_logging_outputs": [[204, 210], ["criterion.__class__.aggregate_logging_outputs", "sum"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.binary_cross_entropy.BinaryCrossEntropyCriterion.aggregate_logging_outputs"], ["", "", "def", "aggregate_logging_outputs", "(", "self", ",", "logging_outputs", ",", "criterion", ")", ":", "\n", "        ", "agg_logging_outputs", "=", "criterion", ".", "__class__", ".", "aggregate_logging_outputs", "(", "logging_outputs", ")", "\n", "agg_logging_outputs", "[", "'posterior'", "]", "=", "sum", "(", "\n", "log", "[", "'posterior'", "]", "for", "log", "in", "logging_outputs", "if", "'posterior'", "in", "log", "\n", ")", "\n", "return", "agg_logging_outputs", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.vggtransformer.VGGTransformerModel.__init__": [[31, 33], ["fairseq.models.FairseqModel.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["def", "__init__", "(", "self", ",", "encoder", ",", "decoder", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "encoder", ",", "decoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.vggtransformer.VGGTransformerModel.add_args": [[34, 115], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add model-specific arguments to the parser.\"\"\"", "\n", "parser", ".", "add_argument", "(", "\n", "\"--input-feat-per-channel\"", ",", "\n", "type", "=", "int", ",", "\n", "metavar", "=", "\"N\"", ",", "\n", "help", "=", "\"encoder input dimension per input channel\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--vggblock-enc-config\"", ",", "\n", "type", "=", "str", ",", "\n", "metavar", "=", "\"EXPR\"", ",", "\n", "help", "=", "\"\"\"\n    an array of tuples each containing the configuration of one vggblock:\n    [(out_channels,\n      conv_kernel_size,\n      pooling_kernel_size,\n      num_conv_layers,\n      use_layer_norm), ...])\n            \"\"\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--transformer-enc-config\"", ",", "\n", "type", "=", "str", ",", "\n", "metavar", "=", "\"EXPR\"", ",", "\n", "help", "=", "\"\"\"\"\n    a tuple containing the configuration of the encoder transformer layers\n    configurations:\n    [(input_dim,\n      num_heads,\n      ffn_dim,\n      normalize_before,\n      dropout,\n      attention_dropout,\n      relu_dropout), ...]')\n            \"\"\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--enc-output-dim\"", ",", "\n", "type", "=", "int", ",", "\n", "metavar", "=", "\"N\"", ",", "\n", "help", "=", "\"\"\"\n    encoder output dimension, can be None. If specified, projecting the\n    transformer output to the specified dimension\"\"\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--in-channels\"", ",", "\n", "type", "=", "int", ",", "\n", "metavar", "=", "\"N\"", ",", "\n", "help", "=", "\"number of encoder input channels\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--tgt-embed-dim\"", ",", "\n", "type", "=", "int", ",", "\n", "metavar", "=", "\"N\"", ",", "\n", "help", "=", "\"embedding dimension of the decoder target tokens\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--transformer-dec-config\"", ",", "\n", "type", "=", "str", ",", "\n", "metavar", "=", "\"EXPR\"", ",", "\n", "help", "=", "\"\"\"\n    a tuple containing the configuration of the decoder transformer layers\n    configurations:\n    [(input_dim,\n      num_heads,\n      ffn_dim,\n      normalize_before,\n      dropout,\n      attention_dropout,\n      relu_dropout), ...]\n            \"\"\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--conv-dec-config\"", ",", "\n", "type", "=", "str", ",", "\n", "metavar", "=", "\"EXPR\"", ",", "\n", "help", "=", "\"\"\"\n    an array of tuples for the decoder 1-D convolution config\n        [(out_channels, conv_kernel_size, use_layer_norm), ...]\"\"\"", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.vggtransformer.VGGTransformerModel.build_encoder": [[117, 125], ["vggtransformer.VGGTransformerEncoder", "eval", "eval"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval"], ["", "@", "classmethod", "\n", "def", "build_encoder", "(", "cls", ",", "args", ",", "task", ")", ":", "\n", "        ", "return", "VGGTransformerEncoder", "(", "\n", "input_feat_per_channel", "=", "args", ".", "input_feat_per_channel", ",", "\n", "vggblock_config", "=", "eval", "(", "args", ".", "vggblock_enc_config", ")", ",", "\n", "transformer_config", "=", "eval", "(", "args", ".", "transformer_enc_config", ")", ",", "\n", "encoder_output_dim", "=", "args", ".", "enc_output_dim", ",", "\n", "in_channels", "=", "args", ".", "in_channels", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.vggtransformer.VGGTransformerModel.build_decoder": [[127, 135], ["vggtransformer.TransformerDecoder", "eval", "eval"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval"], ["", "@", "classmethod", "\n", "def", "build_decoder", "(", "cls", ",", "args", ",", "task", ")", ":", "\n", "        ", "return", "TransformerDecoder", "(", "\n", "dictionary", "=", "task", ".", "target_dictionary", ",", "\n", "embed_dim", "=", "args", ".", "tgt_embed_dim", ",", "\n", "transformer_config", "=", "eval", "(", "args", ".", "transformer_dec_config", ")", ",", "\n", "conv_config", "=", "eval", "(", "args", ".", "conv_dec_config", ")", ",", "\n", "encoder_output_dim", "=", "args", ".", "enc_output_dim", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.vggtransformer.VGGTransformerModel.build_model": [[137, 147], ["vggtransformer.base_architecture", "cls.build_encoder", "cls.build_decoder", "cls"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.base_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.TransformerModel.build_encoder", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.TransformerModel.build_decoder", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.masked_lm_dictionary.BertDictionary.cls"], ["", "@", "classmethod", "\n", "def", "build_model", "(", "cls", ",", "args", ",", "task", ")", ":", "\n", "        ", "\"\"\"Build a new model instance.\"\"\"", "\n", "# make sure that all args are properly defaulted", "\n", "# (in case there are any new ones)", "\n", "base_architecture", "(", "args", ")", "\n", "\n", "encoder", "=", "cls", ".", "build_encoder", "(", "args", ",", "task", ")", "\n", "decoder", "=", "cls", ".", "build_decoder", "(", "args", ",", "task", ")", "\n", "return", "cls", "(", "encoder", ",", "decoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.vggtransformer.VGGTransformerModel.get_normalized_probs": [[148, 153], ["super().get_normalized_probs"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_decoder.FairseqDecoder.get_normalized_probs"], ["", "def", "get_normalized_probs", "(", "self", ",", "net_output", ",", "log_probs", ",", "sample", "=", "None", ")", ":", "\n", "# net_output['encoder_out'] is a (B, T, D) tensor", "\n", "        ", "lprobs", "=", "super", "(", ")", ".", "get_normalized_probs", "(", "net_output", ",", "log_probs", ",", "sample", ")", "\n", "lprobs", ".", "batch_first", "=", "True", "\n", "return", "lprobs", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.vggtransformer.VGGTransformerEncoder.__init__": [[213, 316], ["fairseq.models.FairseqEncoder.__init__", "torch.ModuleList", "torch.ModuleList", "vggtransformer.VGGTransformerEncoder.infer_conv_output_dim", "vggtransformer.VGGTransformerEncoder.validate_transformer_config", "vggtransformer.VGGTransformerEncoder.parse_transformer_context", "vggtransformer.VGGTransformerEncoder.parse_transformer_sampling", "torch.ModuleList", "torch.ModuleList", "vggtransformer.VGGTransformerEncoder.transformer_layers.append", "range", "vggtransformer.VGGTransformerEncoder.transformer_layers.extend", "len", "enumerate", "len", "vggtransformer.VGGTransformerEncoder.transformer_layers.append", "fairseq.modules.TransformerEncoderLayer", "len", "vggtransformer.VGGTransformerEncoder.transformer_layers.append", "isinstance", "ValueError", "vggtransformer.VGGTransformerEncoder.conv_layers.append", "vggtransformer.Linear", "vggtransformer.prepare_transformer_encoder_params", "vggtransformer.VGGTransformerEncoder.transformer_layers.append", "fairseq.modules.TransformerEncoderLayer", "vggtransformer.Linear", "vggtransformer.LayerNorm", "fairseq.modules.VGGBlock", "vggtransformer.Linear", "vggtransformer.prepare_transformer_encoder_params"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.vggblock.infer_conv_output_dim", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.vggtransformer.VGGTransformerEncoder.validate_transformer_config", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.vggtransformer.VGGTransformerEncoder.parse_transformer_context", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.vggtransformer.VGGTransformerEncoder.parse_transformer_sampling", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.vggtransformer.prepare_transformer_encoder_params", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.layer_norm.LayerNorm", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.vggtransformer.prepare_transformer_encoder_params"], ["def", "__init__", "(", "\n", "self", ",", "\n", "input_feat_per_channel", ",", "\n", "vggblock_config", "=", "DEFAULT_ENC_VGGBLOCK_CONFIG", ",", "\n", "transformer_config", "=", "DEFAULT_ENC_TRANSFORMER_CONFIG", ",", "\n", "encoder_output_dim", "=", "512", ",", "\n", "in_channels", "=", "1", ",", "\n", "transformer_context", "=", "None", ",", "\n", "transformer_sampling", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"constructor for VGGTransformerEncoder\n\n        Args:\n            - input_feat_per_channel: feature dim (not including stacked,\n              just base feature)\n            - in_channel: # input channels (e.g., if stack 8 feature vector\n                together, this is 8)\n            - vggblock_config: configuration of vggblock, see comments on\n                DEFAULT_ENC_VGGBLOCK_CONFIG\n            - transformer_config: configuration of transformer layer, see comments\n                on DEFAULT_ENC_TRANSFORMER_CONFIG\n            - encoder_output_dim: final transformer output embedding dimension\n            - transformer_context: (left, right) if set, self-attention will be focused\n              on (t-left, t+right)\n            - transformer_sampling: an iterable of int, must match with\n              len(transformer_config), transformer_sampling[i] indicates sampling\n              factor for i-th transformer layer, after multihead att and feedfoward\n              part\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", "None", ")", "\n", "\n", "self", ".", "num_vggblocks", "=", "0", "\n", "if", "vggblock_config", "is", "not", "None", ":", "\n", "            ", "if", "not", "isinstance", "(", "vggblock_config", ",", "Iterable", ")", ":", "\n", "                ", "raise", "ValueError", "(", "\"vggblock_config is not iterable\"", ")", "\n", "", "self", ".", "num_vggblocks", "=", "len", "(", "vggblock_config", ")", "\n", "\n", "", "self", ".", "conv_layers", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "in_channels", "=", "in_channels", "\n", "self", ".", "input_dim", "=", "input_feat_per_channel", "\n", "\n", "if", "vggblock_config", "is", "not", "None", ":", "\n", "            ", "for", "_", ",", "config", "in", "enumerate", "(", "vggblock_config", ")", ":", "\n", "                ", "(", "\n", "out_channels", ",", "\n", "conv_kernel_size", ",", "\n", "pooling_kernel_size", ",", "\n", "num_conv_layers", ",", "\n", "layer_norm", ",", "\n", ")", "=", "config", "\n", "self", ".", "conv_layers", ".", "append", "(", "\n", "VGGBlock", "(", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n", "conv_kernel_size", ",", "\n", "pooling_kernel_size", ",", "\n", "num_conv_layers", ",", "\n", "input_dim", "=", "input_feat_per_channel", ",", "\n", "layer_norm", "=", "layer_norm", ",", "\n", ")", "\n", ")", "\n", "in_channels", "=", "out_channels", "\n", "input_feat_per_channel", "=", "self", ".", "conv_layers", "[", "-", "1", "]", ".", "output_dim", "\n", "\n", "", "", "transformer_input_dim", "=", "self", ".", "infer_conv_output_dim", "(", "\n", "self", ".", "in_channels", ",", "self", ".", "input_dim", "\n", ")", "\n", "# transformer_input_dim is the output dimension of VGG part", "\n", "\n", "self", ".", "validate_transformer_config", "(", "transformer_config", ")", "\n", "self", ".", "transformer_context", "=", "self", ".", "parse_transformer_context", "(", "transformer_context", ")", "\n", "self", ".", "transformer_sampling", "=", "self", ".", "parse_transformer_sampling", "(", "\n", "transformer_sampling", ",", "len", "(", "transformer_config", ")", "\n", ")", "\n", "\n", "self", ".", "transformer_layers", "=", "nn", ".", "ModuleList", "(", ")", "\n", "\n", "if", "transformer_input_dim", "!=", "transformer_config", "[", "0", "]", "[", "0", "]", ":", "\n", "            ", "self", ".", "transformer_layers", ".", "append", "(", "\n", "Linear", "(", "transformer_input_dim", ",", "transformer_config", "[", "0", "]", "[", "0", "]", ")", "\n", ")", "\n", "", "self", ".", "transformer_layers", ".", "append", "(", "\n", "TransformerEncoderLayer", "(", "\n", "prepare_transformer_encoder_params", "(", "*", "transformer_config", "[", "0", "]", ")", "\n", ")", "\n", ")", "\n", "\n", "for", "i", "in", "range", "(", "1", ",", "len", "(", "transformer_config", ")", ")", ":", "\n", "            ", "if", "transformer_config", "[", "i", "-", "1", "]", "[", "0", "]", "!=", "transformer_config", "[", "i", "]", "[", "0", "]", ":", "\n", "                ", "self", ".", "transformer_layers", ".", "append", "(", "\n", "Linear", "(", "transformer_config", "[", "i", "-", "1", "]", "[", "0", "]", ",", "transformer_config", "[", "i", "]", "[", "0", "]", ")", "\n", ")", "\n", "", "self", ".", "transformer_layers", ".", "append", "(", "\n", "TransformerEncoderLayer", "(", "\n", "prepare_transformer_encoder_params", "(", "*", "transformer_config", "[", "i", "]", ")", "\n", ")", "\n", ")", "\n", "\n", "", "self", ".", "encoder_output_dim", "=", "encoder_output_dim", "\n", "self", ".", "transformer_layers", ".", "extend", "(", "\n", "[", "\n", "Linear", "(", "transformer_config", "[", "-", "1", "]", "[", "0", "]", ",", "encoder_output_dim", ")", ",", "\n", "LayerNorm", "(", "encoder_output_dim", ")", ",", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.vggtransformer.VGGTransformerEncoder.forward": [[319, 378], ["src_tokens.size", "src_tokens.view", "x.contiguous().view.contiguous().view.transpose().contiguous", "range", "x.contiguous().view.contiguous().view.size", "x.contiguous().view.contiguous().view.transpose().transpose", "x.contiguous().view.contiguous().view.contiguous().view", "int", "examples.speech_recognition.data.data_utils.lengths_to_encoder_padding_mask", "vggtransformer.VGGTransformerEncoder.lengths_to_attn_mask", "range", "len", "encoder_padding_mask.any", "len", "isinstance", "x.contiguous().view.contiguous().view.transpose", "x.contiguous().view.contiguous().view.transpose", "x.contiguous().view.contiguous().view.contiguous", "encoder_padding_mask.t", "vggtransformer.VGGTransformerEncoder.slice", "src_lengths.float"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.data_utils.lengths_to_encoder_padding_mask", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.vggtransformer.VGGTransformerEncoder.lengths_to_attn_mask", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.vggtransformer.VGGTransformerEncoder.slice"], ["", "def", "forward", "(", "self", ",", "src_tokens", ",", "src_lengths", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        src_tokens: padded tensor (B, T, C * feat)\n        src_lengths: tensor of original lengths of input utterances (B,)\n        \"\"\"", "\n", "bsz", ",", "max_seq_len", ",", "_", "=", "src_tokens", ".", "size", "(", ")", "\n", "x", "=", "src_tokens", ".", "view", "(", "bsz", ",", "max_seq_len", ",", "self", ".", "in_channels", ",", "self", ".", "input_dim", ")", "\n", "x", "=", "x", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", "\n", "# (B, C, T, feat)", "\n", "\n", "for", "layer_idx", "in", "range", "(", "len", "(", "self", ".", "conv_layers", ")", ")", ":", "\n", "            ", "x", "=", "self", ".", "conv_layers", "[", "layer_idx", "]", "(", "x", ")", "\n", "\n", "", "bsz", ",", "_", ",", "output_seq_len", ",", "_", "=", "x", ".", "size", "(", ")", "\n", "\n", "# (B, C, T, feat) -> (B, T, C, feat) -> (T, B, C, feat) -> (T, B, C * feat)", "\n", "x", "=", "x", ".", "transpose", "(", "1", ",", "2", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "x", "=", "x", ".", "contiguous", "(", ")", ".", "view", "(", "output_seq_len", ",", "bsz", ",", "-", "1", ")", "\n", "\n", "subsampling_factor", "=", "int", "(", "max_seq_len", "*", "1.0", "/", "output_seq_len", "+", "0.5", ")", "\n", "# TODO: shouldn't subsampling_factor determined in advance ?", "\n", "input_lengths", "=", "(", "src_lengths", ".", "float", "(", ")", "/", "subsampling_factor", ")", ".", "ceil", "(", ")", ".", "long", "(", ")", "\n", "\n", "encoder_padding_mask", ",", "_", "=", "lengths_to_encoder_padding_mask", "(", "\n", "input_lengths", ",", "batch_first", "=", "True", "\n", ")", "\n", "if", "not", "encoder_padding_mask", ".", "any", "(", ")", ":", "\n", "            ", "encoder_padding_mask", "=", "None", "\n", "\n", "", "attn_mask", "=", "self", ".", "lengths_to_attn_mask", "(", "input_lengths", ",", "subsampling_factor", ")", "\n", "\n", "transformer_layer_idx", "=", "0", "\n", "\n", "for", "layer_idx", "in", "range", "(", "len", "(", "self", ".", "transformer_layers", ")", ")", ":", "\n", "\n", "            ", "if", "isinstance", "(", "self", ".", "transformer_layers", "[", "layer_idx", "]", ",", "TransformerEncoderLayer", ")", ":", "\n", "                ", "x", "=", "self", ".", "transformer_layers", "[", "layer_idx", "]", "(", "\n", "x", ",", "encoder_padding_mask", ",", "attn_mask", "\n", ")", "\n", "\n", "if", "self", ".", "transformer_sampling", "[", "transformer_layer_idx", "]", "!=", "1", ":", "\n", "                    ", "sampling_factor", "=", "self", ".", "transformer_sampling", "[", "transformer_layer_idx", "]", "\n", "x", ",", "encoder_padding_mask", ",", "attn_mask", "=", "self", ".", "slice", "(", "\n", "x", ",", "encoder_padding_mask", ",", "attn_mask", ",", "sampling_factor", "\n", ")", "\n", "\n", "", "transformer_layer_idx", "+=", "1", "\n", "\n", "", "else", ":", "\n", "                ", "x", "=", "self", ".", "transformer_layers", "[", "layer_idx", "]", "(", "x", ")", "\n", "\n", "# encoder_padding_maks is a (T x B) tensor, its [t, b] elements indicate", "\n", "# whether encoder_output[t, b] is valid or not (valid=0, invalid=1)", "\n", "\n", "", "", "return", "{", "\n", "\"encoder_out\"", ":", "x", ",", "# (T, B, C)", "\n", "\"encoder_padding_mask\"", ":", "encoder_padding_mask", ".", "t", "(", ")", "\n", "if", "encoder_padding_mask", "is", "not", "None", "\n", "else", "None", ",", "\n", "# (B, T) --> (T, B)", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.vggtransformer.VGGTransformerEncoder.infer_conv_output_dim": [[381, 390], ["torch.randn", "torch.randn", "torch.randn", "torch.randn", "enumerate", "x.transpose.transpose.transpose", "x.transpose.transpose.contiguous().view().size", "x.transpose.transpose.size", "x.transpose.transpose.contiguous().view", "x.transpose.transpose.contiguous"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "infer_conv_output_dim", "(", "self", ",", "in_channels", ",", "input_dim", ")", ":", "\n", "        ", "sample_seq_len", "=", "200", "\n", "sample_bsz", "=", "10", "\n", "x", "=", "torch", ".", "randn", "(", "sample_bsz", ",", "in_channels", ",", "sample_seq_len", ",", "input_dim", ")", "\n", "for", "i", ",", "_", "in", "enumerate", "(", "self", ".", "conv_layers", ")", ":", "\n", "            ", "x", "=", "self", ".", "conv_layers", "[", "i", "]", "(", "x", ")", "\n", "", "x", "=", "x", ".", "transpose", "(", "1", ",", "2", ")", "\n", "mb", ",", "seq", "=", "x", ".", "size", "(", ")", "[", ":", "2", "]", "\n", "return", "x", ".", "contiguous", "(", ")", ".", "view", "(", "mb", ",", "seq", ",", "-", "1", ")", ".", "size", "(", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.vggtransformer.VGGTransformerEncoder.validate_transformer_config": [[391, 401], ["ValueError"], "methods", ["None"], ["", "def", "validate_transformer_config", "(", "self", ",", "transformer_config", ")", ":", "\n", "        ", "for", "config", "in", "transformer_config", ":", "\n", "            ", "input_dim", ",", "num_heads", "=", "config", "[", ":", "2", "]", "\n", "if", "input_dim", "%", "num_heads", "!=", "0", ":", "\n", "                ", "msg", "=", "(", "\n", "\"ERROR in transformer config {}:\"", ".", "format", "(", "config", ")", "\n", "+", "\"input dimension {} \"", ".", "format", "(", "input_dim", ")", "\n", "+", "\"not dividable by number of heads\"", ".", "format", "(", "num_heads", ")", "\n", ")", "\n", "raise", "ValueError", "(", "msg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.vggtransformer.VGGTransformerEncoder.parse_transformer_context": [[402, 435], ["isinstance", "ValueError", "len", "ValueError"], "methods", ["None"], ["", "", "", "def", "parse_transformer_context", "(", "self", ",", "transformer_context", ")", ":", "\n", "        ", "\"\"\"\n        transformer_context can be the following:\n        -   None; indicates no context is used, i.e.,\n            transformer can access full context\n        -   a tuple/list of two int; indicates left and right context,\n            any number <0 indicates infinite context\n                * e.g., (5, 6) indicates that for query at x_t, transformer can\n                access [t-5, t+6] (inclusive)\n                * e.g., (-1, 6) indicates that for query at x_t, transformer can\n                access [0, t+6] (inclusive)\n        \"\"\"", "\n", "if", "transformer_context", "is", "None", ":", "\n", "            ", "return", "None", "\n", "\n", "", "if", "not", "isinstance", "(", "transformer_context", ",", "Iterable", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"transformer context must be Iterable if it is not None\"", ")", "\n", "\n", "", "if", "len", "(", "transformer_context", ")", "!=", "2", ":", "\n", "            ", "raise", "ValueError", "(", "\"transformer context must have length 2\"", ")", "\n", "\n", "", "left_context", "=", "transformer_context", "[", "0", "]", "\n", "if", "left_context", "<", "0", ":", "\n", "            ", "left_context", "=", "None", "\n", "\n", "", "right_context", "=", "transformer_context", "[", "1", "]", "\n", "if", "right_context", "<", "0", ":", "\n", "            ", "right_context", "=", "None", "\n", "\n", "", "if", "left_context", "is", "None", "and", "right_context", "is", "None", ":", "\n", "            ", "return", "None", "\n", "\n", "", "return", "(", "left_context", ",", "right_context", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.vggtransformer.VGGTransformerEncoder.parse_transformer_sampling": [[436, 473], ["enumerate", "isinstance", "ValueError", "len", "ValueError", "isinstance", "ValueError", "ValueError"], "methods", ["None"], ["", "def", "parse_transformer_sampling", "(", "self", ",", "transformer_sampling", ",", "num_layers", ")", ":", "\n", "        ", "\"\"\"\n        parsing transformer sampling configuration\n\n        Args:\n            - transformer_sampling, accepted input:\n                * None, indicating no sampling\n                * an Iterable with int (>0) as element\n            - num_layers, expected number of transformer layers, must match with\n              the length of transformer_sampling if it is not None\n\n        Returns:\n            - A tuple with length num_layers\n        \"\"\"", "\n", "if", "transformer_sampling", "is", "None", ":", "\n", "            ", "return", "(", "1", ",", ")", "*", "num_layers", "\n", "\n", "", "if", "not", "isinstance", "(", "transformer_sampling", ",", "Iterable", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"transformer_sampling must be an iterable if it is not None\"", "\n", ")", "\n", "\n", "", "if", "len", "(", "transformer_sampling", ")", "!=", "num_layers", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"transformer_sampling {} does not match with the number \"", "\n", "+", "\"of layers {}\"", ".", "format", "(", "transformer_sampling", ",", "num_layers", ")", "\n", ")", "\n", "\n", "", "for", "layer", ",", "value", "in", "enumerate", "(", "transformer_sampling", ")", ":", "\n", "            ", "if", "not", "isinstance", "(", "value", ",", "int", ")", ":", "\n", "                ", "raise", "ValueError", "(", "\"Invalid value in transformer_sampling: \"", ")", "\n", "", "if", "value", "<", "1", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "\"{} layer's subsampling is {}.\"", ".", "format", "(", "layer", ",", "value", ")", "\n", "+", "\" This is not allowed! \"", "\n", ")", "\n", "", "", "return", "transformer_sampling", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.vggtransformer.VGGTransformerEncoder.slice": [[474, 487], ["None"], "methods", ["None"], ["", "def", "slice", "(", "self", ",", "embedding", ",", "padding_mask", ",", "attn_mask", ",", "sampling_factor", ")", ":", "\n", "        ", "\"\"\"\n        embedding is a (T, B, D) tensor\n        padding_mask is a (B, T) tensor or None\n        attn_mask is a (T, T) tensor or None\n        \"\"\"", "\n", "embedding", "=", "embedding", "[", ":", ":", "sampling_factor", ",", ":", ",", ":", "]", "\n", "if", "padding_mask", "is", "not", "None", ":", "\n", "            ", "padding_mask", "=", "padding_mask", "[", ":", ",", ":", ":", "sampling_factor", "]", "\n", "", "if", "attn_mask", "is", "not", "None", ":", "\n", "            ", "attn_mask", "=", "attn_mask", "[", ":", ":", "sampling_factor", ",", ":", ":", "sampling_factor", "]", "\n", "\n", "", "return", "embedding", ",", "padding_mask", ",", "attn_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.vggtransformer.VGGTransformerEncoder.lengths_to_attn_mask": [[488, 539], ["torch.max().item", "torch.max().item", "torch.max().item", "torch.max().item", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "range", "torch.zeros.to", "torch.zeros.to", "math.ceil", "math.ceil", "torch.max", "torch.max", "torch.max", "torch.max", "max", "min"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item"], ["", "def", "lengths_to_attn_mask", "(", "self", ",", "input_lengths", ",", "subsampling_factor", "=", "1", ")", ":", "\n", "        ", "\"\"\"\n        create attention mask according to sequence lengths and transformer\n        context\n\n        Args:\n            - input_lengths: (B, )-shape Int/Long tensor; input_lengths[b] is\n              the length of b-th sequence\n            - subsampling_factor: int\n                * Note that the left_context and right_context is specified in\n                  the input frame-level while input to transformer may already\n                  go through subsampling (e.g., the use of striding in vggblock)\n                  we use subsampling_factor to scale the left/right context\n\n        Return:\n            - a (T, T) binary tensor or None, where T is max(input_lengths)\n                * if self.transformer_context is None, None\n                * if left_context is None,\n                    * attn_mask[t, t + right_context + 1:] = 1\n                    * others = 0\n                * if right_context is None,\n                    * attn_mask[t, 0:t - left_context] = 1\n                    * others = 0\n                * elsif\n                    * attn_mask[t, t - left_context: t + right_context + 1] = 0\n                    * others = 1\n        \"\"\"", "\n", "if", "self", ".", "transformer_context", "is", "None", ":", "\n", "            ", "return", "None", "\n", "\n", "", "maxT", "=", "torch", ".", "max", "(", "input_lengths", ")", ".", "item", "(", ")", "\n", "attn_mask", "=", "torch", ".", "zeros", "(", "maxT", ",", "maxT", ")", "\n", "\n", "left_context", "=", "self", ".", "transformer_context", "[", "0", "]", "\n", "right_context", "=", "self", ".", "transformer_context", "[", "1", "]", "\n", "if", "left_context", "is", "not", "None", ":", "\n", "            ", "left_context", "=", "math", ".", "ceil", "(", "self", ".", "transformer_context", "[", "0", "]", "/", "subsampling_factor", ")", "\n", "", "if", "right_context", "is", "not", "None", ":", "\n", "            ", "right_context", "=", "math", ".", "ceil", "(", "self", ".", "transformer_context", "[", "1", "]", "/", "subsampling_factor", ")", "\n", "\n", "", "for", "t", "in", "range", "(", "maxT", ")", ":", "\n", "            ", "if", "left_context", "is", "not", "None", ":", "\n", "                ", "st", "=", "0", "\n", "en", "=", "max", "(", "st", ",", "t", "-", "left_context", ")", "\n", "attn_mask", "[", "t", ",", "st", ":", "en", "]", "=", "1", "\n", "", "if", "right_context", "is", "not", "None", ":", "\n", "                ", "st", "=", "t", "+", "right_context", "+", "1", "\n", "st", "=", "min", "(", "st", ",", "maxT", "-", "1", ")", "\n", "attn_mask", "[", "t", ",", "st", ":", "]", "=", "1", "\n", "\n", "", "", "return", "attn_mask", ".", "to", "(", "input_lengths", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.vggtransformer.VGGTransformerEncoder.reorder_encoder_out": [[540, 549], ["encoder_out[].index_select", "encoder_out[].index_select"], "methods", ["None"], ["", "def", "reorder_encoder_out", "(", "self", ",", "encoder_out", ",", "new_order", ")", ":", "\n", "        ", "encoder_out", "[", "\"encoder_out\"", "]", "=", "encoder_out", "[", "\"encoder_out\"", "]", ".", "index_select", "(", "\n", "1", ",", "new_order", "\n", ")", "\n", "if", "encoder_out", "[", "\"encoder_padding_mask\"", "]", "is", "not", "None", ":", "\n", "            ", "encoder_out", "[", "\"encoder_padding_mask\"", "]", "=", "encoder_out", "[", "\n", "\"encoder_padding_mask\"", "\n", "]", ".", "index_select", "(", "1", ",", "new_order", ")", "\n", "", "return", "encoder_out", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.vggtransformer.TransformerDecoder.__init__": [[565, 614], ["fairseq.models.FairseqIncrementalDecoder.__init__", "len", "dictionary.pad", "vggtransformer.Embedding", "torch.ModuleList", "torch.ModuleList", "range", "torch.ModuleList", "torch.ModuleList", "vggtransformer.TransformerDecoder.layers.append", "range", "vggtransformer.Linear", "len", "vggtransformer.TransformerDecoder.conv_layers.append", "vggtransformer.TransformerDecoder.conv_layers.append", "vggtransformer.TransformerDecoder.layers.append", "fairseq.modules.TransformerDecoderLayer", "len", "vggtransformer.TransformerDecoder.layers.append", "vggtransformer.LinearizedConv1d", "vggtransformer.LinearizedConv1d", "vggtransformer.TransformerDecoder.conv_layers.append", "torch.ReLU", "torch.ReLU", "vggtransformer.Linear", "vggtransformer.prepare_transformer_decoder_params", "vggtransformer.TransformerDecoder.layers.append", "fairseq.modules.TransformerDecoderLayer", "torch.LayerNorm", "torch.LayerNorm", "vggtransformer.Linear", "vggtransformer.prepare_transformer_decoder_params"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.Embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv.LinearizedConv1d", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv.LinearizedConv1d", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.vggtransformer.prepare_transformer_decoder_params", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.layer_norm.LayerNorm", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.layer_norm.LayerNorm", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.vggtransformer.prepare_transformer_decoder_params"], ["def", "__init__", "(", "\n", "self", ",", "\n", "dictionary", ",", "\n", "embed_dim", "=", "512", ",", "\n", "transformer_config", "=", "DEFAULT_ENC_TRANSFORMER_CONFIG", ",", "\n", "conv_config", "=", "DEFAULT_DEC_CONV_CONFIG", ",", "\n", "encoder_output_dim", "=", "512", ",", "\n", ")", ":", "\n", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ")", "\n", "vocab_size", "=", "len", "(", "dictionary", ")", "\n", "self", ".", "padding_idx", "=", "dictionary", ".", "pad", "(", ")", "\n", "self", ".", "embed_tokens", "=", "Embedding", "(", "vocab_size", ",", "embed_dim", ",", "self", ".", "padding_idx", ")", "\n", "\n", "self", ".", "conv_layers", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "conv_config", ")", ")", ":", "\n", "            ", "out_channels", ",", "kernel_size", ",", "layer_norm", "=", "conv_config", "[", "i", "]", "\n", "if", "i", "==", "0", ":", "\n", "                ", "conv_layer", "=", "LinearizedConv1d", "(", "\n", "embed_dim", ",", "out_channels", ",", "kernel_size", ",", "padding", "=", "kernel_size", "-", "1", "\n", ")", "\n", "", "else", ":", "\n", "                ", "conv_layer", "=", "LinearizedConv1d", "(", "\n", "conv_config", "[", "i", "-", "1", "]", "[", "0", "]", ",", "\n", "out_channels", ",", "\n", "kernel_size", ",", "\n", "padding", "=", "kernel_size", "-", "1", ",", "\n", ")", "\n", "", "self", ".", "conv_layers", ".", "append", "(", "conv_layer", ")", "\n", "if", "layer_norm", ":", "\n", "                ", "self", ".", "conv_layers", ".", "append", "(", "nn", ".", "LayerNorm", "(", "out_channels", ")", ")", "\n", "", "self", ".", "conv_layers", ".", "append", "(", "nn", ".", "ReLU", "(", ")", ")", "\n", "\n", "", "self", ".", "layers", "=", "nn", ".", "ModuleList", "(", ")", "\n", "if", "conv_config", "[", "-", "1", "]", "[", "0", "]", "!=", "transformer_config", "[", "0", "]", "[", "0", "]", ":", "\n", "            ", "self", ".", "layers", ".", "append", "(", "Linear", "(", "conv_config", "[", "-", "1", "]", "[", "0", "]", ",", "transformer_config", "[", "0", "]", "[", "0", "]", ")", ")", "\n", "", "self", ".", "layers", ".", "append", "(", "TransformerDecoderLayer", "(", "\n", "prepare_transformer_decoder_params", "(", "*", "transformer_config", "[", "0", "]", ")", "\n", ")", ")", "\n", "\n", "for", "i", "in", "range", "(", "1", ",", "len", "(", "transformer_config", ")", ")", ":", "\n", "            ", "if", "transformer_config", "[", "i", "-", "1", "]", "[", "0", "]", "!=", "transformer_config", "[", "i", "]", "[", "0", "]", ":", "\n", "                ", "self", ".", "layers", ".", "append", "(", "\n", "Linear", "(", "transformer_config", "[", "i", "-", "1", "]", "[", "0", "]", ",", "transformer_config", "[", "i", "]", "[", "0", "]", ")", "\n", ")", "\n", "", "self", ".", "layers", ".", "append", "(", "TransformerDecoderLayer", "(", "\n", "prepare_transformer_decoder_params", "(", "*", "transformer_config", "[", "i", "]", ")", "\n", ")", ")", "\n", "", "self", ".", "fc_out", "=", "Linear", "(", "transformer_config", "[", "-", "1", "]", "[", "0", "]", ",", "vocab_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.vggtransformer.TransformerDecoder.forward": [[615, 685], ["vggtransformer.TransformerDecoder.embed_tokens", "vggtransformer.TransformerDecoder._transpose_if_training", "vggtransformer.TransformerDecoder._transpose_if_inference", "layer.transpose", "vggtransformer.TransformerDecoder.fc_out", "isinstance", "isinstance", "layer", "layer", "layer", "layer", "encoder_out[].t", "vggtransformer.TransformerDecoder.buffered_future_mask"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv.FConvDecoder._transpose_if_training", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.vggtransformer.TransformerDecoder._transpose_if_inference", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.TransformerDecoder.buffered_future_mask"], ["", "def", "forward", "(", "self", ",", "prev_output_tokens", ",", "encoder_out", "=", "None", ",", "incremental_state", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            prev_output_tokens (LongTensor): previous decoder outputs of shape\n                `(batch, tgt_len)`, for input feeding/teacher forcing\n            encoder_out (Tensor, optional): output from the encoder, used for\n                encoder-side attention\n            incremental_state (dict): dictionary used for storing state during\n                :ref:`Incremental decoding`\n        Returns:\n            tuple:\n                - the last decoder layer's output of shape `(batch, tgt_len,\n                  vocab)`\n                - the last decoder layer's attention weights of shape `(batch,\n                  tgt_len, src_len)`\n        \"\"\"", "\n", "target_padding_mask", "=", "(", "\n", "(", "prev_output_tokens", "==", "self", ".", "padding_idx", ")", ".", "to", "(", "prev_output_tokens", ".", "device", ")", "\n", "if", "incremental_state", "is", "None", "\n", "else", "None", "\n", ")", "\n", "\n", "if", "incremental_state", "is", "not", "None", ":", "\n", "            ", "prev_output_tokens", "=", "prev_output_tokens", "[", ":", ",", "-", "1", ":", "]", "\n", "\n", "# embed tokens", "\n", "", "x", "=", "self", ".", "embed_tokens", "(", "prev_output_tokens", ")", "\n", "\n", "# B x T x C -> T x B x C", "\n", "x", "=", "self", ".", "_transpose_if_training", "(", "x", ",", "incremental_state", ")", "\n", "\n", "for", "layer", "in", "self", ".", "conv_layers", ":", "\n", "            ", "if", "isinstance", "(", "layer", ",", "LinearizedConvolution", ")", ":", "\n", "                ", "x", "=", "layer", "(", "x", ",", "incremental_state", ")", "\n", "", "else", ":", "\n", "                ", "x", "=", "layer", "(", "x", ")", "\n", "\n", "# B x T x C -> T x B x C", "\n", "", "", "x", "=", "self", ".", "_transpose_if_inference", "(", "x", ",", "incremental_state", ")", "\n", "\n", "# decoder layers", "\n", "for", "layer", "in", "self", ".", "layers", ":", "\n", "            ", "if", "isinstance", "(", "layer", ",", "TransformerDecoderLayer", ")", ":", "\n", "                ", "x", ",", "_", "=", "layer", "(", "\n", "x", ",", "\n", "(", "encoder_out", "[", "\"encoder_out\"", "]", "if", "encoder_out", "is", "not", "None", "else", "None", ")", ",", "\n", "(", "\n", "encoder_out", "[", "\"encoder_padding_mask\"", "]", ".", "t", "(", ")", "\n", "if", "encoder_out", "[", "\"encoder_padding_mask\"", "]", "is", "not", "None", "\n", "else", "None", "\n", ")", ",", "\n", "incremental_state", ",", "\n", "self_attn_mask", "=", "(", "\n", "self", ".", "buffered_future_mask", "(", "x", ")", "\n", "if", "incremental_state", "is", "None", "\n", "else", "None", "\n", ")", ",", "\n", "self_attn_padding_mask", "=", "(", "\n", "target_padding_mask", "if", "incremental_state", "is", "None", "else", "None", "\n", ")", ",", "\n", ")", "\n", "", "else", ":", "\n", "                ", "x", "=", "layer", "(", "x", ")", "\n", "\n", "# T x B x C -> B x T x C", "\n", "", "", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "x", "=", "self", ".", "fc_out", "(", "x", ")", "\n", "\n", "return", "x", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.vggtransformer.TransformerDecoder.buffered_future_mask": [[686, 701], ["tensor.size", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "vggtransformer.TransformerDecoder._future_mask.size", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "hasattr", "fairseq.utils.fill_with_neg_inf", "fairseq.utils.fill_with_neg_inf", "tensor.new", "vggtransformer.TransformerDecoder._future_mask.resize_"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.fill_with_neg_inf", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.fill_with_neg_inf"], ["", "def", "buffered_future_mask", "(", "self", ",", "tensor", ")", ":", "\n", "        ", "dim", "=", "tensor", ".", "size", "(", "0", ")", "\n", "if", "(", "\n", "not", "hasattr", "(", "self", ",", "\"_future_mask\"", ")", "\n", "or", "self", ".", "_future_mask", "is", "None", "\n", "or", "self", ".", "_future_mask", ".", "device", "!=", "tensor", ".", "device", "\n", ")", ":", "\n", "            ", "self", ".", "_future_mask", "=", "torch", ".", "triu", "(", "\n", "utils", ".", "fill_with_neg_inf", "(", "tensor", ".", "new", "(", "dim", ",", "dim", ")", ")", ",", "1", "\n", ")", "\n", "", "if", "self", ".", "_future_mask", ".", "size", "(", "0", ")", "<", "dim", ":", "\n", "            ", "self", ".", "_future_mask", "=", "torch", ".", "triu", "(", "\n", "utils", ".", "fill_with_neg_inf", "(", "self", ".", "_future_mask", ".", "resize_", "(", "dim", ",", "dim", ")", ")", ",", "1", "\n", ")", "\n", "", "return", "self", ".", "_future_mask", "[", ":", "dim", ",", ":", "dim", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.vggtransformer.TransformerDecoder._transpose_if_training": [[702, 706], ["x.transpose.transpose.transpose"], "methods", ["None"], ["", "def", "_transpose_if_training", "(", "self", ",", "x", ",", "incremental_state", ")", ":", "\n", "        ", "if", "incremental_state", "is", "None", ":", "\n", "            ", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.vggtransformer.TransformerDecoder._transpose_if_inference": [[707, 711], ["x.transpose.transpose.transpose"], "methods", ["None"], ["", "def", "_transpose_if_inference", "(", "self", ",", "x", ",", "incremental_state", ")", ":", "\n", "        ", "if", "incremental_state", ":", "\n", "            ", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.vggtransformer.prepare_transformer_encoder_params": [[170, 188], ["argparse.Namespace"], "function", ["None"], ["def", "prepare_transformer_encoder_params", "(", "\n", "input_dim", ",", "\n", "num_heads", ",", "\n", "ffn_dim", ",", "\n", "normalize_before", ",", "\n", "dropout", ",", "\n", "attention_dropout", ",", "\n", "relu_dropout", ",", "\n", ")", ":", "\n", "    ", "args", "=", "argparse", ".", "Namespace", "(", ")", "\n", "args", ".", "encoder_embed_dim", "=", "input_dim", "\n", "args", ".", "encoder_attention_heads", "=", "num_heads", "\n", "args", ".", "attention_dropout", "=", "attention_dropout", "\n", "args", ".", "dropout", "=", "dropout", "\n", "args", ".", "activation_dropout", "=", "relu_dropout", "\n", "args", ".", "encoder_normalize_before", "=", "normalize_before", "\n", "args", ".", "encoder_ffn_embed_dim", "=", "ffn_dim", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.vggtransformer.prepare_transformer_decoder_params": [[190, 208], ["argparse.Namespace"], "function", ["None"], ["", "def", "prepare_transformer_decoder_params", "(", "\n", "input_dim", ",", "\n", "num_heads", ",", "\n", "ffn_dim", ",", "\n", "normalize_before", ",", "\n", "dropout", ",", "\n", "attention_dropout", ",", "\n", "relu_dropout", ",", "\n", ")", ":", "\n", "    ", "args", "=", "argparse", ".", "Namespace", "(", ")", "\n", "args", ".", "decoder_embed_dim", "=", "input_dim", "\n", "args", ".", "decoder_attention_heads", "=", "num_heads", "\n", "args", ".", "attention_dropout", "=", "attention_dropout", "\n", "args", ".", "dropout", "=", "dropout", "\n", "args", ".", "activation_dropout", "=", "relu_dropout", "\n", "args", ".", "decoder_normalize_before", "=", "normalize_before", "\n", "args", ".", "decoder_ffn_embed_dim", "=", "ffn_dim", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.vggtransformer.Embedding": [[713, 718], ["torch.Embedding"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.Embedding"], ["", "", "def", "Embedding", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", ")", ":", "\n", "    ", "m", "=", "nn", ".", "Embedding", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", "=", "padding_idx", ")", "\n", "# nn.init.uniform_(m.weight, -0.1, 0.1)", "\n", "# nn.init.constant_(m.weight[padding_idx], 0)", "\n", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.vggtransformer.Linear": [[720, 727], ["torch.Linear"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear"], ["", "def", "Linear", "(", "in_features", ",", "out_features", ",", "bias", "=", "True", ",", "dropout", "=", "0", ")", ":", "\n", "    ", "\"\"\"Linear layer (input: N x T x C)\"\"\"", "\n", "m", "=", "nn", ".", "Linear", "(", "in_features", ",", "out_features", ",", "bias", "=", "bias", ")", "\n", "# m.weight.data.uniform_(-0.1, 0.1)", "\n", "# if bias:", "\n", "#     m.bias.data.uniform_(-0.1, 0.1)", "\n", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.vggtransformer.LinearizedConv1d": [[729, 736], ["fairseq.modules.LinearizedConvolution", "math.sqrt", "torch.init.normal_", "torch.init.constant_", "torch.utils.weight_norm"], "function", ["None"], ["", "def", "LinearizedConv1d", "(", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "dropout", "=", "0", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Weight-normalized Conv1d layer optimized for decoding\"\"\"", "\n", "m", "=", "LinearizedConvolution", "(", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "**", "kwargs", ")", "\n", "std", "=", "math", ".", "sqrt", "(", "(", "4", "*", "(", "1.0", "-", "dropout", ")", ")", "/", "(", "m", ".", "kernel_size", "[", "0", "]", "*", "in_channels", ")", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "m", ".", "weight", ",", "mean", "=", "0", ",", "std", "=", "std", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0", ")", "\n", "return", "nn", ".", "utils", ".", "weight_norm", "(", "m", ",", "dim", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.vggtransformer.LayerNorm": [[738, 741], ["torch.LayerNorm"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.layer_norm.LayerNorm"], ["", "def", "LayerNorm", "(", "embedding_dim", ")", ":", "\n", "    ", "m", "=", "nn", ".", "LayerNorm", "(", "embedding_dim", ")", "\n", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.vggtransformer.base_architecture": [[744, 760], ["getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr"], "function", ["None"], ["", "def", "base_architecture", "(", "args", ")", ":", "\n", "    ", "args", ".", "input_feat_per_channel", "=", "getattr", "(", "args", ",", "\"input_feat_per_channel\"", ",", "40", ")", "\n", "args", ".", "vggblock_enc_config", "=", "getattr", "(", "\n", "args", ",", "\"vggblock_enc_config\"", ",", "DEFAULT_ENC_VGGBLOCK_CONFIG", "\n", ")", "\n", "args", ".", "transformer_enc_config", "=", "getattr", "(", "\n", "args", ",", "\"transformer_enc_config\"", ",", "DEFAULT_ENC_TRANSFORMER_CONFIG", "\n", ")", "\n", "args", ".", "enc_output_dim", "=", "getattr", "(", "args", ",", "\"enc_output_dim\"", ",", "512", ")", "\n", "args", ".", "in_channels", "=", "getattr", "(", "args", ",", "\"in_channels\"", ",", "1", ")", "\n", "args", ".", "tgt_embed_dim", "=", "getattr", "(", "args", ",", "\"tgt_embed_dim\"", ",", "128", ")", "\n", "args", ".", "transformer_dec_config", "=", "getattr", "(", "\n", "args", ",", "\"transformer_dec_config\"", ",", "DEFAULT_ENC_TRANSFORMER_CONFIG", "\n", ")", "\n", "args", ".", "conv_dec_config", "=", "getattr", "(", "args", ",", "\"conv_dec_config\"", ",", "DEFAULT_DEC_CONV_CONFIG", ")", "\n", "args", ".", "transformer_context", "=", "getattr", "(", "args", ",", "\"transformer_context\"", ",", "\"None\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.vggtransformer.vggtransformer_1": [[762, 780], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture"], ["", "@", "register_model_architecture", "(", "\"asr_vggtransformer\"", ",", "\"vggtransformer_1\"", ")", "\n", "def", "vggtransformer_1", "(", "args", ")", ":", "\n", "    ", "args", ".", "input_feat_per_channel", "=", "getattr", "(", "args", ",", "\"input_feat_per_channel\"", ",", "80", ")", "\n", "args", ".", "vggblock_enc_config", "=", "getattr", "(", "\n", "args", ",", "\"vggblock_enc_config\"", ",", "\"[(64, 3, 2, 2, True), (128, 3, 2, 2, True)]\"", "\n", ")", "\n", "args", ".", "transformer_enc_config", "=", "getattr", "(", "\n", "args", ",", "\n", "\"transformer_enc_config\"", ",", "\n", "\"((1024, 16, 4096, True, 0.15, 0.15, 0.15),) * 14\"", ",", "\n", ")", "\n", "args", ".", "enc_output_dim", "=", "getattr", "(", "args", ",", "\"enc_output_dim\"", ",", "1024", ")", "\n", "args", ".", "tgt_embed_dim", "=", "getattr", "(", "args", ",", "\"tgt_embed_dim\"", ",", "128", ")", "\n", "args", ".", "conv_dec_config", "=", "getattr", "(", "args", ",", "\"conv_dec_config\"", ",", "\"((256, 3, True),) * 4\"", ")", "\n", "args", ".", "transformer_dec_config", "=", "getattr", "(", "\n", "args", ",", "\n", "\"transformer_dec_config\"", ",", "\n", "\"((1024, 16, 4096, True, 0.15, 0.15, 0.15),) * 4\"", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.vggtransformer.vggtransformer_2": [[783, 801], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture"], ["", "@", "register_model_architecture", "(", "\"asr_vggtransformer\"", ",", "\"vggtransformer_2\"", ")", "\n", "def", "vggtransformer_2", "(", "args", ")", ":", "\n", "    ", "args", ".", "input_feat_per_channel", "=", "getattr", "(", "args", ",", "\"input_feat_per_channel\"", ",", "80", ")", "\n", "args", ".", "vggblock_enc_config", "=", "getattr", "(", "\n", "args", ",", "\"vggblock_enc_config\"", ",", "\"[(64, 3, 2, 2, True), (128, 3, 2, 2, True)]\"", "\n", ")", "\n", "args", ".", "transformer_enc_config", "=", "getattr", "(", "\n", "args", ",", "\n", "\"transformer_enc_config\"", ",", "\n", "\"((1024, 16, 4096, True, 0.15, 0.15, 0.15),) * 16\"", ",", "\n", ")", "\n", "args", ".", "enc_output_dim", "=", "getattr", "(", "args", ",", "\"enc_output_dim\"", ",", "1024", ")", "\n", "args", ".", "tgt_embed_dim", "=", "getattr", "(", "args", ",", "\"tgt_embed_dim\"", ",", "512", ")", "\n", "args", ".", "conv_dec_config", "=", "getattr", "(", "args", ",", "\"conv_dec_config\"", ",", "\"((256, 3, True),) * 4\"", ")", "\n", "args", ".", "transformer_dec_config", "=", "getattr", "(", "\n", "args", ",", "\n", "\"transformer_dec_config\"", ",", "\n", "\"((1024, 16, 4096, True, 0.15, 0.15, 0.15),) * 6\"", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.vggtransformer.vggtransformer_base": [[804, 819], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture"], ["", "@", "register_model_architecture", "(", "\"asr_vggtransformer\"", ",", "\"vggtransformer_base\"", ")", "\n", "def", "vggtransformer_base", "(", "args", ")", ":", "\n", "    ", "args", ".", "input_feat_per_channel", "=", "getattr", "(", "args", ",", "\"input_feat_per_channel\"", ",", "80", ")", "\n", "args", ".", "vggblock_enc_config", "=", "getattr", "(", "\n", "args", ",", "\"vggblock_enc_config\"", ",", "\"[(64, 3, 2, 2, True), (128, 3, 2, 2, True)]\"", "\n", ")", "\n", "args", ".", "transformer_enc_config", "=", "getattr", "(", "\n", "args", ",", "\"transformer_enc_config\"", ",", "\"((512, 8, 2048, True, 0.15, 0.15, 0.15),) * 12\"", "\n", ")", "\n", "\n", "args", ".", "enc_output_dim", "=", "getattr", "(", "args", ",", "\"enc_output_dim\"", ",", "512", ")", "\n", "args", ".", "tgt_embed_dim", "=", "getattr", "(", "args", ",", "\"tgt_embed_dim\"", ",", "512", ")", "\n", "args", ".", "conv_dec_config", "=", "getattr", "(", "args", ",", "\"conv_dec_config\"", ",", "\"((256, 3, True),) * 4\"", ")", "\n", "args", ".", "transformer_dec_config", "=", "getattr", "(", "\n", "args", ",", "\"transformer_dec_config\"", ",", "\"((512, 8, 2048, True, 0.15, 0.15, 0.15),) * 6\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv_lm.FConvLanguageModel.__init__": [[17, 19], ["fairseq.models.FairseqLanguageModel.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["    ", "def", "__init__", "(", "self", ",", "decoder", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "decoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv_lm.FConvLanguageModel.add_args": [[20, 38], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add model-specific arguments to the parser.\"\"\"", "\n", "parser", ".", "add_argument", "(", "'--dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-layers'", ",", "type", "=", "str", ",", "metavar", "=", "'EXPR'", ",", "\n", "help", "=", "'decoder layers [(dim, kernel_size), ...]'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-out-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder output embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--adaptive-softmax-cutoff'", ",", "metavar", "=", "'EXPR'", ",", "\n", "help", "=", "'comma separated list of adaptive softmax cutoff points. '", "\n", "'Must be used with adaptive_loss criterion'", ")", "\n", "parser", ".", "add_argument", "(", "'--adaptive-softmax-dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'sets adaptive softmax dropout for the tail projections'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-attention'", ",", "type", "=", "str", ",", "metavar", "=", "'EXPR'", ",", "\n", "help", "=", "'decoder attention [True, ...]'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv_lm.FConvLanguageModel.build_model": [[39, 65], ["fconv_lm.base_lm_architecture", "fairseq.models.fconv.FConvDecoder", "fconv_lm.FConvLanguageModel", "hasattr", "hasattr", "eval", "eval", "fairseq.options.eval_str_list"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv_lm.base_lm_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.eval_str_list"], ["", "@", "classmethod", "\n", "def", "build_model", "(", "cls", ",", "args", ",", "task", ")", ":", "\n", "        ", "\"\"\"Build a new model instance.\"\"\"", "\n", "# make sure all arguments are present in older models", "\n", "base_lm_architecture", "(", "args", ")", "\n", "\n", "if", "hasattr", "(", "args", ",", "'max_target_positions'", ")", "and", "not", "hasattr", "(", "args", ",", "'tokens_per_sample'", ")", ":", "\n", "            ", "args", ".", "tokens_per_sample", "=", "args", ".", "max_target_positions", "\n", "\n", "", "decoder", "=", "FConvDecoder", "(", "\n", "dictionary", "=", "task", ".", "target_dictionary", ",", "\n", "embed_dim", "=", "args", ".", "decoder_embed_dim", ",", "\n", "convolutions", "=", "eval", "(", "args", ".", "decoder_layers", ")", ",", "\n", "out_embed_dim", "=", "args", ".", "decoder_embed_dim", ",", "\n", "attention", "=", "eval", "(", "args", ".", "decoder_attention", ")", ",", "\n", "dropout", "=", "args", ".", "dropout", ",", "\n", "max_positions", "=", "args", ".", "tokens_per_sample", ",", "\n", "share_embed", "=", "False", ",", "\n", "positional_embeddings", "=", "False", ",", "\n", "adaptive_softmax_cutoff", "=", "(", "\n", "options", ".", "eval_str_list", "(", "args", ".", "adaptive_softmax_cutoff", ",", "type", "=", "int", ")", "\n", "if", "args", ".", "criterion", "==", "'adaptive_loss'", "else", "None", "\n", ")", ",", "\n", "adaptive_softmax_dropout", "=", "args", ".", "adaptive_softmax_dropout", ",", "\n", ")", "\n", "return", "FConvLanguageModel", "(", "decoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv_lm.base_lm_architecture": [[67, 75], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture"], ["", "", "@", "register_model_architecture", "(", "'fconv_lm'", ",", "'fconv_lm'", ")", "\n", "def", "base_lm_architecture", "(", "args", ")", ":", "\n", "    ", "args", ".", "dropout", "=", "getattr", "(", "args", ",", "'dropout'", ",", "0.1", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "128", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "'[(1268, 4)] * 13'", ")", "\n", "args", ".", "decoder_attention", "=", "getattr", "(", "args", ",", "'decoder_attention'", ",", "'False'", ")", "\n", "args", ".", "adaptive_softmax_cutoff", "=", "getattr", "(", "args", ",", "'adaptive_softmax_cutoff'", ",", "None", ")", "\n", "args", ".", "adaptive_softmax_dropout", "=", "getattr", "(", "args", ",", "'adaptive_softmax_dropout'", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv_lm.fconv_lm_dauphin_wikitext103": [[77, 91], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "fconv_lm.base_lm_architecture"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv_lm.base_lm_architecture"], ["", "@", "register_model_architecture", "(", "'fconv_lm'", ",", "'fconv_lm_dauphin_wikitext103'", ")", "\n", "def", "fconv_lm_dauphin_wikitext103", "(", "args", ")", ":", "\n", "    ", "layers", "=", "'[(850, 6)] * 3'", "\n", "layers", "+=", "' + [(850, 1)] * 1'", "\n", "layers", "+=", "' + [(850, 5)] * 4'", "\n", "layers", "+=", "' + [(850, 1)] * 1'", "\n", "layers", "+=", "' + [(850, 4)] * 3'", "\n", "layers", "+=", "' + [(1024, 4)] * 1'", "\n", "layers", "+=", "' + [(2048, 4)] * 1'", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "280", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "layers", ")", "\n", "args", ".", "decoder_attention", "=", "getattr", "(", "args", ",", "'decoder_attention'", ",", "'False'", ")", "\n", "args", ".", "adaptive_softmax_cutoff", "=", "getattr", "(", "args", ",", "'adaptive_softmax_cutoff'", ",", "'10000,20000,200000'", ")", "\n", "base_lm_architecture", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv_lm.fconv_lm_dauphin_gbw": [[93, 105], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "fconv_lm.base_lm_architecture"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv_lm.base_lm_architecture"], ["", "@", "register_model_architecture", "(", "'fconv_lm'", ",", "'fconv_lm_dauphin_gbw'", ")", "\n", "def", "fconv_lm_dauphin_gbw", "(", "args", ")", ":", "\n", "    ", "layers", "=", "'[(512, 5)]'", "\n", "layers", "+=", "' + [(128, 1, 0), (128, 5, 0), (512, 1, 3)] * 3'", "\n", "layers", "+=", "' + [(512, 1, 0), (512, 5, 0), (1024, 1, 3)] * 3'", "\n", "layers", "+=", "' + [(1024, 1, 0), (1024, 5, 0), (2048, 1, 3)] * 6'", "\n", "layers", "+=", "' + [(1024, 1, 0), (1024, 5, 0), (4096, 1, 3)]'", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "128", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "layers", ")", "\n", "args", ".", "decoder_attention", "=", "getattr", "(", "args", ",", "'decoder_attention'", ",", "'False'", ")", "\n", "args", ".", "adaptive_softmax_cutoff", "=", "getattr", "(", "args", ",", "'adaptive_softmax_cutoff'", ",", "'10000,50000,200000'", ")", "\n", "base_lm_architecture", "(", "args", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer_from_pretrained_xlm.TransformerFromPretrainedXLMModel.add_args": [[23, 42], ["fairseq.models.transformer.TransformerModel.add_args", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.add_args"], ["    ", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add model-specific arguments to the parser.\"\"\"", "\n", "TransformerModel", ".", "add_args", "(", "parser", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--pretrained-xlm-checkpoint\"", ",", "\n", "type", "=", "str", ",", "\n", "metavar", "=", "\"STR\"", ",", "\n", "help", "=", "\"XLM model to use for initializing transformer encoder and/or decoder\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--init-encoder-only\"", ",", "\n", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"if set, don't load the XLM weights and embeddings into decoder\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--init-decoder-only\"", ",", "\n", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"if set, don't load the XLM weights and embeddings into encoder\"", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer_from_pretrained_xlm.TransformerFromPretrainedXLMModel.build_model": [[44, 64], ["hasattr", "super().build_model", "isinstance", "isinstance", "getattr", "getattr"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv_lm.LightConvLanguageModel.build_model"], ["", "@", "classmethod", "\n", "def", "build_model", "(", "self", ",", "args", ",", "task", ",", "cls_dictionary", "=", "MaskedLMDictionary", ")", ":", "\n", "        ", "assert", "hasattr", "(", "args", ",", "\"pretrained_xlm_checkpoint\"", ")", ",", "(", "\n", "\"You must specify a path for --pretrained-xlm-checkpoint to use \"", "\n", "\"--arch transformer_from_pretrained_xlm\"", "\n", ")", "\n", "assert", "isinstance", "(", "task", ".", "source_dictionary", ",", "cls_dictionary", ")", "and", "isinstance", "(", "\n", "task", ".", "target_dictionary", ",", "cls_dictionary", "\n", ")", ",", "(", "\n", "\"You should use a MaskedLMDictionary when using --arch \"", "\n", "\"transformer_from_pretrained_xlm because the pretrained XLM model \"", "\n", "\"was trained using data binarized with MaskedLMDictionary. \"", "\n", "\"For translation, you may want to use --task \"", "\n", "\"translation_from_pretrained_xlm\"", "\n", ")", "\n", "assert", "not", "(", "\n", "getattr", "(", "args", ",", "\"init_encoder_only\"", ",", "False", ")", "\n", "and", "getattr", "(", "args", ",", "\"init_decoder_only\"", ",", "False", ")", "\n", ")", ",", "\"Only one of --init-encoder-only and --init-decoder-only can be set.\"", "\n", "return", "super", "(", ")", ".", "build_model", "(", "args", ",", "task", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer_from_pretrained_xlm.TransformerFromPretrainedXLMModel.build_encoder": [[65, 68], ["transformer_from_pretrained_xlm.TransformerEncoderFromPretrainedXLM"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "build_encoder", "(", "cls", ",", "args", ",", "src_dict", ",", "embed_tokens", ")", ":", "\n", "        ", "return", "TransformerEncoderFromPretrainedXLM", "(", "args", ",", "src_dict", ",", "embed_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer_from_pretrained_xlm.TransformerFromPretrainedXLMModel.build_decoder": [[69, 72], ["transformer_from_pretrained_xlm.TransformerDecoderFromPretrainedXLM"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "build_decoder", "(", "cls", ",", "args", ",", "tgt_dict", ",", "embed_tokens", ")", ":", "\n", "        ", "return", "TransformerDecoderFromPretrainedXLM", "(", "args", ",", "tgt_dict", ",", "embed_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer_from_pretrained_xlm.TransformerEncoderFromPretrainedXLM.__init__": [[115, 130], ["fairseq.models.transformer.TransformerEncoder.__init__", "getattr", "hasattr", "transformer_from_pretrained_xlm.upgrade_state_dict_with_xlm_weights", "transformer_from_pretrained_xlm.TransformerEncoderFromPretrainedXLM.load_state_dict", "transformer_from_pretrained_xlm.TransformerEncoderFromPretrainedXLM.state_dict"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer_from_pretrained_xlm.upgrade_state_dict_with_xlm_weights", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.load_state_dict", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.state_dict"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "dictionary", ",", "embed_tokens", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "dictionary", ",", "embed_tokens", ")", "\n", "if", "getattr", "(", "args", ",", "'init_decoder_only'", ",", "False", ")", ":", "\n", "# Don't load XLM weights for encoder if --init-decoder-only", "\n", "            ", "return", "\n", "\n", "", "assert", "hasattr", "(", "args", ",", "\"pretrained_xlm_checkpoint\"", ")", ",", "(", "\n", "\"--pretrained-xlm-checkpoint must be specified to load Transformer \"", "\n", "\"encoder from pretrained XLM\"", "\n", ")", "\n", "xlm_loaded_state_dict", "=", "upgrade_state_dict_with_xlm_weights", "(", "\n", "state_dict", "=", "self", ".", "state_dict", "(", ")", ",", "\n", "pretrained_xlm_checkpoint", "=", "args", ".", "pretrained_xlm_checkpoint", ",", "\n", ")", "\n", "self", ".", "load_state_dict", "(", "xlm_loaded_state_dict", ",", "strict", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer_from_pretrained_xlm.TransformerDecoderFromPretrainedXLM.__init__": [[134, 149], ["fairseq.models.transformer.TransformerDecoder.__init__", "getattr", "hasattr", "transformer_from_pretrained_xlm.upgrade_state_dict_with_xlm_weights", "transformer_from_pretrained_xlm.TransformerDecoderFromPretrainedXLM.load_state_dict", "transformer_from_pretrained_xlm.TransformerDecoderFromPretrainedXLM.state_dict"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer_from_pretrained_xlm.upgrade_state_dict_with_xlm_weights", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.load_state_dict", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.state_dict"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "dictionary", ",", "embed_tokens", ",", "no_encoder_attn", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "dictionary", ",", "embed_tokens", ",", "no_encoder_attn", ")", "\n", "if", "getattr", "(", "args", ",", "'init_encoder_only'", ",", "False", ")", ":", "\n", "# Don't load XLM weights for decoder if --init-encoder-only", "\n", "            ", "return", "\n", "", "assert", "hasattr", "(", "args", ",", "\"pretrained_xlm_checkpoint\"", ")", ",", "(", "\n", "\"--pretrained-xlm-checkpoint must be specified to load Transformer \"", "\n", "\"decoder from pretrained XLM\"", "\n", ")", "\n", "\n", "xlm_loaded_state_dict", "=", "upgrade_state_dict_with_xlm_weights", "(", "\n", "state_dict", "=", "self", ".", "state_dict", "(", ")", ",", "\n", "pretrained_xlm_checkpoint", "=", "args", ".", "pretrained_xlm_checkpoint", ",", "\n", ")", "\n", "self", ".", "load_state_dict", "(", "xlm_loaded_state_dict", ",", "strict", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer_from_pretrained_xlm.upgrade_state_dict_with_xlm_weights": [[74, 111], ["fairseq.checkpoint_utils.load_checkpoint_to_cpu", "xlm_state_dict.keys", "os.path.exists", "IOError", "str", "key.find", "state_dict.keys"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.checkpoint_utils.load_checkpoint_to_cpu", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists"], ["", "", "def", "upgrade_state_dict_with_xlm_weights", "(", "\n", "state_dict", ":", "Dict", "[", "str", ",", "Any", "]", ",", "pretrained_xlm_checkpoint", ":", "str", "\n", ")", "->", "Dict", "[", "str", ",", "Any", "]", ":", "\n", "    ", "\"\"\"\n    Load XLM weights into a Transformer encoder or decoder model.\n\n    Args:\n        state_dict: state dict for either TransformerEncoder or\n            TransformerDecoder\n        pretrained_xlm_checkpoint: checkpoint to load XLM weights from\n\n    Raises:\n        AssertionError: If architecture (num layers, attention heads, etc.)\n            does not match between the current Transformer encoder or\n            decoder and the pretrained_xlm_checkpoint\n    \"\"\"", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "pretrained_xlm_checkpoint", ")", ":", "\n", "        ", "raise", "IOError", "(", "\"Model file not found: {}\"", ".", "format", "(", "pretrained_xlm_checkpoint", ")", ")", "\n", "\n", "", "state", "=", "checkpoint_utils", ".", "load_checkpoint_to_cpu", "(", "pretrained_xlm_checkpoint", ")", "\n", "xlm_state_dict", "=", "state", "[", "\"model\"", "]", "\n", "for", "key", "in", "xlm_state_dict", ".", "keys", "(", ")", ":", "\n", "\n", "        ", "for", "search_key", "in", "[", "\"embed_tokens\"", ",", "\"embed_positions\"", ",", "\"layers\"", "]", ":", "\n", "            ", "if", "search_key", "in", "key", ":", "\n", "                ", "subkey", "=", "key", "[", "key", ".", "find", "(", "search_key", ")", ":", "]", "\n", "assert", "subkey", "in", "state_dict", ",", "(", "\n", "\"{} Transformer encoder / decoder \"", "\n", "\"state_dict does not contain {}. Cannot \"", "\n", "\"load {} from pretrained XLM checkpoint \"", "\n", "\"{} into Transformer.\"", ".", "format", "(", "\n", "str", "(", "state_dict", ".", "keys", "(", ")", ")", ",", "\n", "subkey", ",", "key", ",", "pretrained_xlm_checkpoint", ")", "\n", ")", "\n", "\n", "state_dict", "[", "subkey", "]", "=", "xlm_state_dict", "[", "key", "]", "\n", "", "", "", "return", "state_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer_from_pretrained_xlm.base_architecture": [[151, 156], ["fairseq.models.register_model_architecture", "fairseq.models.transformer.base_architecture"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.base_architecture"], ["", "", "@", "register_model_architecture", "(", "\n", "\"transformer_from_pretrained_xlm\"", ",", "\"transformer_from_pretrained_xlm\"", "\n", ")", "\n", "def", "base_architecture", "(", "args", ")", ":", "\n", "    ", "transformer_base_architecture", "(", "args", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv_self_att.FConvModelSelfAtt.hub_models": [[33, 39], ["None"], "methods", ["None"], ["    ", "@", "classmethod", "\n", "def", "hub_models", "(", "cls", ")", ":", "\n", "        ", "return", "{", "\n", "'conv.stories'", ":", "'https://dl.fbaipublicfiles.com/fairseq/models/stories_checkpoint.tar.bz2'", ",", "\n", "# Test set containing dictionaries", "\n", "'data.stories'", ":", "'https://dl.fbaipublicfiles.com/fairseq/data/stories_test.tar.bz2'", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv_self_att.FConvModelSelfAtt.__init__": [[41, 52], ["fairseq.models.FairseqEncoderDecoderModel.__init__", "sum", "fairseq.models.CompositeEncoder"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["", "def", "__init__", "(", "self", ",", "encoder", ",", "decoder", ",", "pretrained_encoder", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "encoder", ",", "decoder", ")", "\n", "self", ".", "encoder", ".", "num_attention_layers", "=", "sum", "(", "layer", "is", "not", "None", "for", "layer", "in", "decoder", ".", "attention", ")", "\n", "self", ".", "pretrained_encoder", "=", "pretrained_encoder", "\n", "if", "self", ".", "pretrained_encoder", "is", "None", ":", "\n", "            ", "encoders", "=", "{", "'encoder'", ":", "encoder", "}", "\n", "", "else", ":", "\n", "            ", "encoders", "=", "{", "'encoder'", ":", "encoder", ",", "'pretrained'", ":", "self", ".", "pretrained_encoder", "}", "\n", "# for fusion model, CompositeEncoder contains both pretrained and training encoders", "\n", "# these are forwarded and then combined in the decoder", "\n", "", "self", ".", "encoder", "=", "CompositeEncoder", "(", "encoders", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv_self_att.FConvModelSelfAtt.add_args": [[53, 91], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add model-specific arguments to the parser.\"\"\"", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'--dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'encoder embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-layers'", ",", "type", "=", "str", ",", "metavar", "=", "'EXPR'", ",", "\n", "help", "=", "'encoder layers [(dim, kernel_size), ...]'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-layers'", ",", "type", "=", "str", ",", "metavar", "=", "'EXPR'", ",", "\n", "help", "=", "'decoder layers [(dim, kernel_size), ...]'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-out-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder output embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-attention'", ",", "type", "=", "str", ",", "metavar", "=", "'EXPR'", ",", "\n", "help", "=", "'decoder attention [True, ...]'", ")", "\n", "parser", ".", "add_argument", "(", "'--self-attention'", ",", "type", "=", "str", ",", "metavar", "=", "'EXPR'", ",", "\n", "help", "=", "'decoder self-attention layers, ex: [True] + [False]*5'", ")", "\n", "parser", ".", "add_argument", "(", "'--multihead-attention-nheads'", ",", "type", "=", "int", ",", "\n", "help", "=", "'Number of heads to use in attention'", ")", "\n", "parser", ".", "add_argument", "(", "'--multihead-self-attention-nheads'", ",", "type", "=", "int", ",", "\n", "help", "=", "'Number of heads to use in self-attention'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-attention'", ",", "type", "=", "str", ",", "metavar", "=", "'EXPR'", ",", "\n", "help", "=", "'encoder attention [True, ...]'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-attention-nheads'", ",", "type", "=", "int", ",", "\n", "help", "=", "'Number of heads to use in encoder attention'", ")", "\n", "parser", ".", "add_argument", "(", "'--project-input'", ",", "type", "=", "str", ",", "metavar", "=", "'EXPR'", ",", "\n", "help", "=", "'Use projections in self-attention [True, ...]'", ")", "\n", "parser", ".", "add_argument", "(", "'--gated-attention'", ",", "type", "=", "str", ",", "metavar", "=", "'EXPR'", ",", "\n", "help", "=", "'Use GLU layers in self-attention projections [True, ...]'", ")", "\n", "parser", ".", "add_argument", "(", "'--downsample'", ",", "type", "=", "str", ",", "metavar", "=", "'EXPR'", ",", "\n", "help", "=", "'Use downsampling in self-attention [True, ...]'", ")", "\n", "parser", ".", "add_argument", "(", "'--pretrained-checkpoint'", ",", "metavar", "=", "'DIR'", ",", "\n", "help", "=", "'path to load checkpoint from pretrained model'", ")", "\n", "parser", ".", "add_argument", "(", "'--pretrained'", ",", "type", "=", "str", ",", "metavar", "=", "'EXPR'", ",", "\n", "help", "=", "'use pretrained model when training [True, ...]'", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv_self_att.FConvModelSelfAtt.build_model": [[93, 143], ["eval", "fconv_self_att.FConvModelSelfAtt.FConvEncoder", "fconv_self_att.FConvModelSelfAtt.FConvDecoder", "fconv_self_att.FConvModelSelfAtt.FConvModelSelfAtt", "print", "trained_decoder.parameters", "trained_encoder.parameters", "list", "list", "eval", "eval", "eval", "eval", "eval", "eval", "eval", "eval", "fairseq.checkpoint_utils.load_model_ensemble", "trained_model.children", "trained_model.children"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.checkpoint_utils.load_model_ensemble"], ["", "@", "classmethod", "\n", "def", "build_model", "(", "cls", ",", "args", ",", "task", ")", ":", "\n", "        ", "\"\"\"Build a new model instance.\"\"\"", "\n", "trained_encoder", ",", "trained_decoder", "=", "None", ",", "None", "\n", "pretrained", "=", "eval", "(", "args", ".", "pretrained", ")", "\n", "if", "pretrained", ":", "\n", "            ", "print", "(", "\"| loading pretrained model\"", ")", "\n", "trained_model", "=", "checkpoint_utils", ".", "load_model_ensemble", "(", "\n", "filenames", "=", "[", "args", ".", "pretrained_checkpoint", "]", ",", "\n", "task", "=", "task", ",", "\n", ")", "[", "0", "]", "[", "0", "]", "\n", "trained_decoder", "=", "list", "(", "trained_model", ".", "children", "(", ")", ")", "[", "1", "]", "\n", "trained_encoder", "=", "list", "(", "trained_model", ".", "children", "(", ")", ")", "[", "0", "]", "\n", "\n", "# freeze pretrained model", "\n", "for", "param", "in", "trained_decoder", ".", "parameters", "(", ")", ":", "\n", "                ", "param", ".", "requires_grad", "=", "False", "\n", "", "for", "param", "in", "trained_encoder", ".", "parameters", "(", ")", ":", "\n", "                ", "param", ".", "requires_grad", "=", "False", "\n", "\n", "", "", "encoder", "=", "FConvEncoder", "(", "\n", "task", ".", "source_dictionary", ",", "\n", "embed_dim", "=", "args", ".", "encoder_embed_dim", ",", "\n", "convolutions", "=", "eval", "(", "args", ".", "encoder_layers", ")", ",", "\n", "dropout", "=", "args", ".", "dropout", ",", "\n", "max_positions", "=", "args", ".", "max_source_positions", ",", "\n", "attention", "=", "eval", "(", "args", ".", "encoder_attention", ")", ",", "\n", "attention_nheads", "=", "args", ".", "encoder_attention_nheads", "\n", ")", "\n", "\n", "decoder", "=", "FConvDecoder", "(", "\n", "task", ".", "target_dictionary", ",", "\n", "embed_dim", "=", "args", ".", "decoder_embed_dim", ",", "\n", "convolutions", "=", "eval", "(", "args", ".", "decoder_layers", ")", ",", "\n", "out_embed_dim", "=", "args", ".", "decoder_out_embed_dim", ",", "\n", "attention", "=", "eval", "(", "args", ".", "decoder_attention", ")", ",", "\n", "dropout", "=", "args", ".", "dropout", ",", "\n", "max_positions", "=", "args", ".", "max_target_positions", ",", "\n", "selfattention", "=", "eval", "(", "args", ".", "self_attention", ")", ",", "\n", "attention_nheads", "=", "args", ".", "multihead_attention_nheads", ",", "\n", "selfattention_nheads", "=", "args", ".", "multihead_self_attention_nheads", ",", "\n", "project_input", "=", "eval", "(", "args", ".", "project_input", ")", ",", "\n", "gated_attention", "=", "eval", "(", "args", ".", "gated_attention", ")", ",", "\n", "downsample", "=", "eval", "(", "args", ".", "downsample", ")", ",", "\n", "pretrained", "=", "pretrained", ",", "\n", "trained_decoder", "=", "trained_decoder", "\n", ")", "\n", "model", "=", "FConvModelSelfAtt", "(", "encoder", ",", "decoder", ",", "trained_encoder", ")", "\n", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv_self_att.FConvModelSelfAtt.pretrained": [[144, 147], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "pretrained", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "pretrained_encoder", "is", "not", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv_self_att.FConvEncoder.__init__": [[151, 197], ["fairseq.models.FairseqEncoder.__init__", "len", "dictionary.pad", "fconv_self_att.Embedding", "fconv_self_att.PositionalEmbedding", "fconv_self_att.FConvEncoder.FConvEncoder.__init__.expand_bool_array"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.Embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.positional_embedding.PositionalEmbedding"], ["def", "__init__", "(", "\n", "self", ",", "dictionary", ",", "embed_dim", "=", "512", ",", "max_positions", "=", "1024", ",", "\n", "convolutions", "=", "(", "(", "512", ",", "3", ")", ",", ")", "*", "20", ",", "dropout", "=", "0.1", ",", "attention", "=", "False", ",", "\n", "attention_nheads", "=", "1", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ")", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "num_attention_layers", "=", "None", "\n", "\n", "num_embeddings", "=", "len", "(", "dictionary", ")", "\n", "self", ".", "padding_idx", "=", "dictionary", ".", "pad", "(", ")", "\n", "self", ".", "embed_tokens", "=", "Embedding", "(", "num_embeddings", ",", "embed_dim", ",", "self", ".", "padding_idx", ")", "\n", "self", ".", "embed_positions", "=", "PositionalEmbedding", "(", "\n", "max_positions", ",", "\n", "embed_dim", ",", "\n", "self", ".", "padding_idx", ",", "\n", ")", "\n", "\n", "def", "expand_bool_array", "(", "val", ")", ":", "\n", "            ", "if", "isinstance", "(", "val", ",", "bool", ")", ":", "\n", "# expand True into [True, True, ...] and do the same with False", "\n", "                ", "return", "[", "val", "]", "*", "len", "(", "convolutions", ")", "\n", "", "return", "val", "\n", "\n", "", "attention", "=", "expand_bool_array", "(", "attention", ")", "\n", "\n", "in_channels", "=", "convolutions", "[", "0", "]", "[", "0", "]", "\n", "self", ".", "fc1", "=", "Linear", "(", "embed_dim", ",", "in_channels", ",", "dropout", "=", "dropout", ")", "\n", "self", ".", "projections", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "convolutions", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "attention", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "attproj", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "i", ",", "(", "out_channels", ",", "kernel_size", ")", "in", "enumerate", "(", "convolutions", ")", ":", "\n", "            ", "self", ".", "projections", ".", "append", "(", "\n", "Linear", "(", "in_channels", ",", "out_channels", ")", "if", "in_channels", "!=", "out_channels", "else", "None", "\n", ")", "\n", "self", ".", "convolutions", ".", "append", "(", "\n", "ConvTBC", "(", "in_channels", ",", "out_channels", "*", "2", ",", "kernel_size", ",", "dropout", "=", "dropout", ")", "\n", ")", "\n", "\n", "self", ".", "attention", ".", "append", "(", "\n", "SelfAttention", "(", "out_channels", ",", "embed_dim", ",", "attention_nheads", ")", "if", "attention", "[", "i", "]", "else", "None", "\n", ")", "\n", "in_channels", "=", "out_channels", "\n", "\n", "", "self", ".", "fc2", "=", "Linear", "(", "in_channels", ",", "embed_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv_self_att.FConvEncoder.forward": [[198, 250], ["torch.dropout", "torch.dropout", "torch.dropout", "attention.transpose", "fconv_self_att.FConvEncoder.FConvEncoder.fc1", "src_tokens.eq().t", "attention.transpose", "zip", "attention.transpose", "fconv_self_att.FConvEncoder.FConvEncoder.fc2", "fairseq.modules.GradMultiply.apply", "fconv_self_att.FConvEncoder.FConvEncoder.embed_tokens", "fconv_self_att.FConvEncoder.FConvEncoder.embed_positions", "encoder_padding_mask.t.t.any", "torch.dropout", "torch.dropout", "torch.dropout", "torch.pad", "torch.pad", "torch.pad", "conv", "torch.glu", "torch.glu", "torch.glu", "encoder_padding_mask.t.t.t", "attention.masked_fill", "math.sqrt", "src_tokens.eq", "proj", "attention.masked_fill", "attention", "math.sqrt", "encoder_padding_mask.t.t.unsqueeze", "attention.transpose.transpose", "encoder_padding_mask.t.t.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad"], ["", "def", "forward", "(", "self", ",", "src_tokens", ",", "src_lengths", ")", ":", "\n", "# embed tokens and positions", "\n", "        ", "x", "=", "self", ".", "embed_tokens", "(", "src_tokens", ")", "+", "self", ".", "embed_positions", "(", "src_tokens", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "input_embedding", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "# project to size of convolution", "\n", "x", "=", "self", ".", "fc1", "(", "x", ")", "\n", "\n", "encoder_padding_mask", "=", "src_tokens", ".", "eq", "(", "self", ".", "padding_idx", ")", ".", "t", "(", ")", "# -> T x B", "\n", "if", "not", "encoder_padding_mask", ".", "any", "(", ")", ":", "\n", "            ", "encoder_padding_mask", "=", "None", "\n", "\n", "# B x T x C -> T x B x C", "\n", "", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "# temporal convolutions", "\n", "for", "proj", ",", "conv", ",", "attention", "in", "zip", "(", "self", ".", "projections", ",", "self", ".", "convolutions", ",", "self", ".", "attention", ")", ":", "\n", "            ", "residual", "=", "x", "if", "proj", "is", "None", "else", "proj", "(", "x", ")", "\n", "\n", "if", "encoder_padding_mask", "is", "not", "None", ":", "\n", "                ", "x", "=", "x", ".", "masked_fill", "(", "encoder_padding_mask", ".", "unsqueeze", "(", "-", "1", ")", ",", "0", ")", "\n", "\n", "", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "padding_l", "=", "(", "conv", ".", "kernel_size", "[", "0", "]", "-", "1", ")", "//", "2", "\n", "padding_r", "=", "conv", ".", "kernel_size", "[", "0", "]", "//", "2", "\n", "x", "=", "F", ".", "pad", "(", "x", ",", "(", "0", ",", "0", ",", "0", ",", "0", ",", "padding_l", ",", "padding_r", ")", ")", "\n", "x", "=", "conv", "(", "x", ")", "\n", "x", "=", "F", ".", "glu", "(", "x", ",", "dim", "=", "2", ")", "\n", "if", "attention", "is", "not", "None", ":", "\n", "                ", "x", "=", "attention", "(", "x", ")", "\n", "", "x", "=", "(", "x", "+", "residual", ")", "*", "math", ".", "sqrt", "(", "0.5", ")", "\n", "\n", "# T x B x C -> B x T x C", "\n", "", "x", "=", "x", ".", "transpose", "(", "1", ",", "0", ")", "\n", "\n", "# project back to size of embedding", "\n", "x", "=", "self", ".", "fc2", "(", "x", ")", "\n", "\n", "if", "encoder_padding_mask", "is", "not", "None", ":", "\n", "            ", "encoder_padding_mask", "=", "encoder_padding_mask", ".", "t", "(", ")", "# -> B x T", "\n", "x", "=", "x", ".", "masked_fill", "(", "encoder_padding_mask", ".", "unsqueeze", "(", "-", "1", ")", ",", "0", ")", "\n", "\n", "# scale gradients (this only affects backward, not forward)", "\n", "", "x", "=", "GradMultiply", ".", "apply", "(", "x", ",", "1.0", "/", "(", "2.0", "*", "self", ".", "num_attention_layers", ")", ")", "\n", "\n", "# add output to input embedding for attention", "\n", "y", "=", "(", "x", "+", "input_embedding", ".", "transpose", "(", "0", ",", "1", ")", ")", "*", "math", ".", "sqrt", "(", "0.5", ")", "\n", "\n", "return", "{", "\n", "'encoder_out'", ":", "(", "x", ",", "y", ")", ",", "\n", "'encoder_padding_mask'", ":", "encoder_padding_mask", ",", "# B x T", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv_self_att.FConvEncoder.reorder_encoder_out": [[252, 268], ["tuple", "encoder_out[].index_select", "tuple", "eo.index_select", "eo.index_select"], "methods", ["None"], ["", "def", "reorder_encoder_out", "(", "self", ",", "encoder_out", ",", "new_order", ")", ":", "\n", "        ", "encoder_out", "[", "'encoder_out'", "]", "=", "tuple", "(", "\n", "eo", ".", "index_select", "(", "0", ",", "new_order", ")", "for", "eo", "in", "encoder_out", "[", "'encoder_out'", "]", "\n", ")", "\n", "\n", "if", "encoder_out", "[", "'encoder_padding_mask'", "]", "is", "not", "None", ":", "\n", "            ", "encoder_out", "[", "'encoder_padding_mask'", "]", "=", "encoder_out", "[", "'encoder_padding_mask'", "]", ".", "index_select", "(", "0", ",", "new_order", ")", "\n", "\n", "", "if", "'pretrained'", "in", "encoder_out", ":", "\n", "            ", "encoder_out", "[", "'pretrained'", "]", "[", "'encoder_out'", "]", "=", "tuple", "(", "\n", "eo", ".", "index_select", "(", "0", ",", "new_order", ")", "\n", "for", "eo", "in", "encoder_out", "[", "'pretrained'", "]", "[", "'encoder_out'", "]", "\n", ")", "\n", "\n", "", "return", "encoder_out", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv_self_att.FConvEncoder.max_positions": [[269, 272], ["fconv_self_att.FConvEncoder.FConvEncoder.embed_positions.max_positions"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.max_positions"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum input length supported by the encoder.\"\"\"", "\n", "return", "self", ".", "embed_positions", ".", "max_positions", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv_self_att.FConvDecoder.__init__": [[276, 380], ["fairseq.models.FairseqDecoder.__init__", "fconv_self_att.FConvDecoder.FConvDecoder.register_buffer", "fconv_self_att.FConvDecoder.FConvDecoder.__init__.expand_bool_array"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["def", "__init__", "(", "\n", "self", ",", "dictionary", ",", "embed_dim", "=", "512", ",", "out_embed_dim", "=", "256", ",", "max_positions", "=", "1024", ",", "\n", "convolutions", "=", "(", "(", "512", ",", "3", ")", ",", ")", "*", "8", ",", "attention", "=", "True", ",", "dropout", "=", "0.1", ",", "\n", "selfattention", "=", "False", ",", "attention_nheads", "=", "1", ",", "selfattention_nheads", "=", "1", ",", "\n", "project_input", "=", "False", ",", "gated_attention", "=", "False", ",", "downsample", "=", "False", ",", "\n", "pretrained", "=", "False", ",", "trained_decoder", "=", "None", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ")", "\n", "self", ".", "register_buffer", "(", "'version'", ",", "torch", ".", "Tensor", "(", "[", "2", "]", ")", ")", "\n", "self", ".", "pretrained", "=", "pretrained", "\n", "self", ".", "pretrained_decoder", "=", "trained_decoder", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "need_attn", "=", "True", "\n", "in_channels", "=", "convolutions", "[", "0", "]", "[", "0", "]", "\n", "\n", "def", "expand_bool_array", "(", "val", ")", ":", "\n", "            ", "if", "isinstance", "(", "val", ",", "bool", ")", ":", "\n", "# expand True into [True, True, ...] and do the same with False", "\n", "                ", "return", "[", "val", "]", "*", "len", "(", "convolutions", ")", "\n", "", "return", "val", "\n", "\n", "", "attention", "=", "expand_bool_array", "(", "attention", ")", "\n", "selfattention", "=", "expand_bool_array", "(", "selfattention", ")", "\n", "\n", "if", "not", "isinstance", "(", "attention", ",", "list", ")", "or", "len", "(", "attention", ")", "!=", "len", "(", "convolutions", ")", ":", "\n", "            ", "raise", "ValueError", "(", "'Attention is expected to be a list of booleans of '", "\n", "'length equal to the number of layers.'", ")", "\n", "\n", "", "num_embeddings", "=", "len", "(", "dictionary", ")", "\n", "padding_idx", "=", "dictionary", ".", "pad", "(", ")", "\n", "self", ".", "embed_tokens", "=", "Embedding", "(", "num_embeddings", ",", "embed_dim", ",", "padding_idx", ")", "\n", "\n", "self", ".", "embed_positions", "=", "PositionalEmbedding", "(", "\n", "max_positions", ",", "\n", "embed_dim", ",", "\n", "padding_idx", ",", "\n", ")", "\n", "\n", "self", ".", "fc1", "=", "Linear", "(", "embed_dim", ",", "in_channels", ",", "dropout", "=", "dropout", ")", "\n", "self", ".", "projections", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "convolutions", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "attention", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "selfattention", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "attproj", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "i", ",", "(", "out_channels", ",", "kernel_size", ")", "in", "enumerate", "(", "convolutions", ")", ":", "\n", "            ", "self", ".", "projections", ".", "append", "(", "\n", "Linear", "(", "in_channels", ",", "out_channels", ")", "if", "in_channels", "!=", "out_channels", "else", "None", "\n", ")", "\n", "self", ".", "convolutions", ".", "append", "(", "\n", "LinearizedConv1d", "(", "\n", "in_channels", ",", "out_channels", "*", "2", ",", "kernel_size", ",", "\n", "padding", "=", "(", "kernel_size", "-", "1", ")", ",", "dropout", "=", "dropout", ",", "\n", ")", "\n", ")", "\n", "\n", "self", ".", "attention", ".", "append", "(", "\n", "DownsampledMultiHeadAttention", "(", "\n", "out_channels", ",", "embed_dim", ",", "attention_nheads", ",", "\n", "project_input", "=", "project_input", ",", "gated", "=", "False", ",", "downsample", "=", "False", ",", "\n", ")", "if", "attention", "[", "i", "]", "else", "None", "\n", ")", "\n", "\n", "self", ".", "attproj", ".", "append", "(", "\n", "Linear", "(", "out_channels", ",", "embed_dim", ",", "dropout", "=", "dropout", ")", "if", "attention", "[", "i", "]", "else", "None", "\n", ")", "\n", "self", ".", "selfattention", ".", "append", "(", "\n", "SelfAttention", "(", "\n", "out_channels", ",", "embed_dim", ",", "selfattention_nheads", ",", "\n", "project_input", "=", "project_input", ",", "gated", "=", "gated_attention", ",", "\n", "downsample", "=", "downsample", ",", "\n", ")", "if", "selfattention", "[", "i", "]", "else", "None", "\n", ")", "\n", "in_channels", "=", "out_channels", "\n", "\n", "", "self", ".", "fc2", "=", "Linear", "(", "in_channels", ",", "out_embed_dim", ")", "\n", "self", ".", "fc3", "=", "Linear", "(", "out_embed_dim", ",", "num_embeddings", ",", "dropout", "=", "dropout", ")", "\n", "\n", "# model fusion", "\n", "if", "self", ".", "pretrained", ":", "\n", "# independent gates are learned from the concatenated input", "\n", "            ", "self", ".", "gate1", "=", "nn", ".", "Sequential", "(", "Linear", "(", "out_embed_dim", "*", "2", ",", "out_embed_dim", ")", ",", "nn", ".", "Sigmoid", "(", ")", ")", "\n", "self", ".", "gate2", "=", "nn", ".", "Sequential", "(", "Linear", "(", "out_embed_dim", "*", "2", ",", "out_embed_dim", ")", ",", "nn", ".", "Sigmoid", "(", ")", ")", "\n", "# pretrained and trained models are joined", "\n", "self", ".", "joining", "=", "nn", ".", "Sequential", "(", "\n", "Linear", "(", "out_embed_dim", "*", "2", ",", "out_embed_dim", "*", "2", ")", ",", "\n", "LayerNorm", "(", "out_embed_dim", "*", "2", ")", ",", "\n", "nn", ".", "GLU", "(", ")", ",", "\n", "Linear", "(", "out_embed_dim", ",", "out_embed_dim", "*", "2", ")", ",", "\n", "LayerNorm", "(", "out_embed_dim", "*", "2", ")", ",", "\n", "nn", ".", "GLU", "(", ")", ",", "\n", "Linear", "(", "out_embed_dim", ",", "out_embed_dim", ")", ",", "\n", "LayerNorm", "(", "out_embed_dim", ")", "\n", ")", "\n", "# pretrained model contains an output layer that is nhid -> vocab size", "\n", "# but the models are combined in their hidden state", "\n", "# the hook stores the output of the pretrained model forward", "\n", "self", ".", "pretrained_outputs", "=", "{", "}", "\n", "\n", "def", "save_output", "(", ")", ":", "\n", "                ", "def", "hook", "(", "a", ",", "b", ",", "output", ")", ":", "\n", "                    ", "self", ".", "pretrained_outputs", "[", "\"out\"", "]", "=", "output", "\n", "", "return", "hook", "\n", "\n", "", "self", ".", "pretrained_decoder", ".", "fc2", ".", "register_forward_hook", "(", "save_output", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv_self_att.FConvDecoder.forward": [[381, 451], ["fconv_self_att.FConvDecoder.FConvDecoder._split_encoder_out", "fconv_self_att.FConvDecoder.FConvDecoder.embed_positions", "torch.dropout", "torch.dropout", "torch.dropout", "fconv_self_att.FConvDecoder.FConvDecoder.transpose", "fconv_self_att.FConvDecoder.FConvDecoder.fc1", "fconv_self_att.FConvDecoder.FConvDecoder.transpose", "zip", "fconv_self_att.FConvDecoder.FConvDecoder.transpose", "fconv_self_att.FConvDecoder.FConvDecoder.fc2", "torch.dropout", "torch.dropout", "torch.dropout", "fconv_self_att.FConvDecoder.FConvDecoder.embed_tokens", "torch.dropout", "torch.dropout", "torch.dropout", "conv", "torch.glu", "torch.glu", "torch.glu", "fconv_self_att.FConvDecoder.FConvDecoder.fc3", "fconv_self_att.FConvDecoder.FConvDecoder.pretrained_decoder.forward", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "fconv_self_att.FConvDecoder.FConvDecoder.gate1", "fconv_self_att.FConvDecoder.FConvDecoder.gate2", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "fconv_self_att.FConvDecoder.FConvDecoder.joining", "fconv_self_att.FConvDecoder.FConvDecoder.fc3", "proj", "attention", "fconv_self_att.FConvDecoder.FConvDecoder.", "math.sqrt", "attproj", "avg_attn_scores.add_"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv.FConvDecoder._split_encoder_out", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.model.AMRModel.forward"], ["", "", "def", "forward", "(", "self", ",", "prev_output_tokens", ",", "encoder_out", ")", ":", "\n", "        ", "trained_encoder_out", "=", "encoder_out", "[", "'pretrained'", "]", "if", "self", ".", "pretrained", "else", "None", "\n", "encoder_out", "=", "encoder_out", "[", "'encoder'", "]", "[", "'encoder_out'", "]", "\n", "\n", "encoder_a", ",", "encoder_b", "=", "self", ".", "_split_encoder_out", "(", "encoder_out", ")", "\n", "\n", "# embed positions", "\n", "positions", "=", "self", ".", "embed_positions", "(", "prev_output_tokens", ")", "\n", "\n", "# embed tokens and positions", "\n", "x", "=", "self", ".", "embed_tokens", "(", "prev_output_tokens", ")", "+", "positions", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "target_embedding", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "# project to size of convolution", "\n", "x", "=", "self", ".", "fc1", "(", "x", ")", "\n", "\n", "# B x T x C -> T x B x C", "\n", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "# temporal convolutions", "\n", "avg_attn_scores", "=", "None", "\n", "for", "proj", ",", "conv", ",", "attention", ",", "selfattention", ",", "attproj", "in", "zip", "(", "\n", "self", ".", "projections", ",", "self", ".", "convolutions", ",", "self", ".", "attention", ",", "self", ".", "selfattention", ",", "self", ".", "attproj", "\n", ")", ":", "\n", "            ", "residual", "=", "x", "if", "proj", "is", "None", "else", "proj", "(", "x", ")", "\n", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "conv", "(", "x", ")", "\n", "x", "=", "F", ".", "glu", "(", "x", ",", "dim", "=", "2", ")", "\n", "\n", "# attention", "\n", "if", "attention", "is", "not", "None", ":", "\n", "                ", "r", "=", "x", "\n", "x", ",", "attn_scores", "=", "attention", "(", "attproj", "(", "x", ")", "+", "target_embedding", ",", "encoder_a", ",", "encoder_b", ")", "\n", "x", "=", "x", "+", "r", "\n", "if", "not", "self", ".", "training", "and", "self", ".", "need_attn", ":", "\n", "                    ", "if", "avg_attn_scores", "is", "None", ":", "\n", "                        ", "avg_attn_scores", "=", "attn_scores", "\n", "", "else", ":", "\n", "                        ", "avg_attn_scores", ".", "add_", "(", "attn_scores", ")", "\n", "\n", "", "", "", "if", "selfattention", "is", "not", "None", ":", "\n", "                ", "x", "=", "selfattention", "(", "x", ")", "\n", "\n", "", "x", "=", "(", "x", "+", "residual", ")", "*", "math", ".", "sqrt", "(", "0.5", ")", "\n", "\n", "# T x B x C -> B x T x C", "\n", "", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "# project back to size of vocabulary", "\n", "x", "=", "self", ".", "fc2", "(", "x", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "if", "not", "self", ".", "pretrained", ":", "\n", "            ", "x", "=", "self", ".", "fc3", "(", "x", ")", "\n", "\n", "# fusion gating", "\n", "", "if", "self", ".", "pretrained", ":", "\n", "            ", "trained_x", ",", "_", "=", "self", ".", "pretrained_decoder", ".", "forward", "(", "prev_output_tokens", ",", "trained_encoder_out", ")", "\n", "y", "=", "torch", ".", "cat", "(", "[", "x", ",", "self", ".", "pretrained_outputs", "[", "\"out\"", "]", "]", ",", "dim", "=", "-", "1", ")", "\n", "gate1", "=", "self", ".", "gate1", "(", "y", ")", "\n", "gate2", "=", "self", ".", "gate2", "(", "y", ")", "\n", "gated_x1", "=", "gate1", "*", "x", "\n", "gated_x2", "=", "gate2", "*", "self", ".", "pretrained_outputs", "[", "\"out\"", "]", "\n", "fusion", "=", "torch", ".", "cat", "(", "[", "gated_x1", ",", "gated_x2", "]", ",", "dim", "=", "-", "1", ")", "\n", "fusion", "=", "self", ".", "joining", "(", "fusion", ")", "\n", "fusion_output", "=", "self", ".", "fc3", "(", "fusion", ")", "\n", "return", "fusion_output", ",", "avg_attn_scores", "\n", "", "else", ":", "\n", "            ", "return", "x", ",", "avg_attn_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv_self_att.FConvDecoder.max_positions": [[452, 455], ["fconv_self_att.FConvDecoder.FConvDecoder.embed_positions.max_positions"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.max_positions"], ["", "", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum output length supported by the decoder.\"\"\"", "\n", "return", "self", ".", "embed_positions", ".", "max_positions", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv_self_att.FConvDecoder.make_generation_fast_": [[456, 458], ["None"], "methods", ["None"], ["", "def", "make_generation_fast_", "(", "self", ",", "need_attn", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "need_attn", "=", "need_attn", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv_self_att.FConvDecoder._split_encoder_out": [[459, 467], ["encoder_a.transpose().contiguous.transpose().contiguous.transpose().contiguous", "encoder_b.transpose().contiguous.transpose().contiguous.transpose().contiguous", "encoder_a.transpose().contiguous.transpose().contiguous.transpose", "encoder_b.transpose().contiguous.transpose().contiguous.transpose"], "methods", ["None"], ["", "def", "_split_encoder_out", "(", "self", ",", "encoder_out", ")", ":", "\n", "        ", "\"\"\"Split and transpose encoder outputs.\"\"\"", "\n", "# transpose only once to speed up attention layers", "\n", "encoder_a", ",", "encoder_b", "=", "encoder_out", "\n", "encoder_a", "=", "encoder_a", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "encoder_b", "=", "encoder_b", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "result", "=", "(", "encoder_a", ",", "encoder_b", ")", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv_self_att.SelfAttention.__init__": [[471, 481], ["torch.Module.__init__", "fairseq.modules.DownsampledMultiHeadAttention", "fconv_self_att.Linear", "fconv_self_att.Linear", "fconv_self_att.Linear", "fairseq.modules.LayerNorm"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.layer_norm.LayerNorm"], ["    ", "def", "__init__", "(", "self", ",", "out_channels", ",", "embed_dim", ",", "num_heads", ",", "project_input", "=", "False", ",", "gated", "=", "False", ",", "downsample", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "attention", "=", "DownsampledMultiHeadAttention", "(", "\n", "out_channels", ",", "embed_dim", ",", "num_heads", ",", "dropout", "=", "0", ",", "bias", "=", "True", ",", "\n", "project_input", "=", "project_input", ",", "gated", "=", "gated", ",", "downsample", "=", "downsample", ",", "\n", ")", "\n", "self", ".", "in_proj_q", "=", "Linear", "(", "out_channels", ",", "embed_dim", ")", "\n", "self", ".", "in_proj_k", "=", "Linear", "(", "out_channels", ",", "embed_dim", ")", "\n", "self", ".", "in_proj_v", "=", "Linear", "(", "out_channels", ",", "embed_dim", ")", "\n", "self", ".", "ln", "=", "LayerNorm", "(", "out_channels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv_self_att.SelfAttention.forward": [[482, 489], ["fconv_self_att.SelfAttention.SelfAttention.in_proj_q", "fconv_self_att.SelfAttention.SelfAttention.in_proj_k", "fconv_self_att.SelfAttention.SelfAttention.in_proj_v", "fconv_self_att.SelfAttention.SelfAttention.attention", "fconv_self_att.SelfAttention.SelfAttention.ln"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.multihead_attention.MultiheadAttention.in_proj_q", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.multihead_attention.MultiheadAttention.in_proj_k", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.multihead_attention.MultiheadAttention.in_proj_v"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "residual", "=", "x", "\n", "query", "=", "self", ".", "in_proj_q", "(", "x", ")", "\n", "key", "=", "self", ".", "in_proj_k", "(", "x", ")", "\n", "value", "=", "self", ".", "in_proj_v", "(", "x", ")", "\n", "x", ",", "_", "=", "self", ".", "attention", "(", "query", ",", "key", ",", "value", ",", "mask_future_timesteps", "=", "True", ",", "use_scalar_bias", "=", "True", ")", "\n", "return", "self", ".", "ln", "(", "x", "+", "residual", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv_self_att.Embedding": [[491, 495], ["torch.Embedding", "nn.Embedding.weight.data.normal_"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.Embedding"], ["", "", "def", "Embedding", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", ")", ":", "\n", "    ", "m", "=", "nn", ".", "Embedding", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", "=", "padding_idx", ")", "\n", "m", ".", "weight", ".", "data", ".", "normal_", "(", "0", ",", "0.1", ")", "\n", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv_self_att.PositionalEmbedding": [[497, 501], ["fairseq.modules.LearnedPositionalEmbedding", "fairseq.modules.LearnedPositionalEmbedding.weight.data.normal_"], "function", ["None"], ["", "def", "PositionalEmbedding", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", ")", ":", "\n", "    ", "m", "=", "LearnedPositionalEmbedding", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", ")", "\n", "m", ".", "weight", ".", "data", ".", "normal_", "(", "0", ",", "0.1", ")", "\n", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv_self_att.Linear": [[503, 509], ["torch.Linear", "nn.Linear.weight.data.normal_", "nn.Linear.bias.data.zero_", "math.sqrt"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear"], ["", "def", "Linear", "(", "in_features", ",", "out_features", ",", "dropout", "=", "0.", ")", ":", "\n", "    ", "\"\"\"Weight-normalized Linear layer (input: N x T x C)\"\"\"", "\n", "m", "=", "nn", ".", "Linear", "(", "in_features", ",", "out_features", ")", "\n", "m", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0", ",", "std", "=", "math", ".", "sqrt", "(", "(", "1", "-", "dropout", ")", "/", "in_features", ")", ")", "\n", "m", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv_self_att.LinearizedConv1d": [[511, 518], ["fairseq.modules.LinearizedConvolution", "math.sqrt", "fairseq.modules.LinearizedConvolution.weight.data.normal_", "fairseq.modules.LinearizedConvolution.bias.data.zero_"], "function", ["None"], ["", "def", "LinearizedConv1d", "(", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "dropout", "=", "0.", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Weight-normalized Conv1d layer optimized for decoding\"\"\"", "\n", "m", "=", "LinearizedConvolution", "(", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "**", "kwargs", ")", "\n", "std", "=", "math", ".", "sqrt", "(", "(", "4", "*", "(", "1.0", "-", "dropout", ")", ")", "/", "(", "m", ".", "kernel_size", "[", "0", "]", "*", "in_channels", ")", ")", "\n", "m", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0", ",", "std", "=", "std", ")", "\n", "m", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv_self_att.ConvTBC": [[520, 528], ["fconv_self_att.ConvTBC", "math.sqrt", "ConvTBC.weight.data.normal_", "ConvTBC.bias.data.zero_"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv.ConvTBC"], ["", "def", "ConvTBC", "(", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "dropout", "=", "0", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Weight-normalized Conv1d layer\"\"\"", "\n", "from", "fairseq", ".", "modules", "import", "ConvTBC", "\n", "m", "=", "ConvTBC", "(", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "**", "kwargs", ")", "\n", "std", "=", "math", ".", "sqrt", "(", "(", "4", "*", "(", "1.0", "-", "dropout", ")", ")", "/", "(", "m", ".", "kernel_size", "[", "0", "]", "*", "in_channels", ")", ")", "\n", "m", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0", ",", "std", "=", "std", ")", "\n", "m", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv_self_att.base_architecture": [[530, 549], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture"], ["", "@", "register_model_architecture", "(", "'fconv_self_att'", ",", "'fconv_self_att'", ")", "\n", "def", "base_architecture", "(", "args", ")", ":", "\n", "    ", "args", ".", "dropout", "=", "getattr", "(", "args", ",", "'dropout'", ",", "0.1", ")", "\n", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "512", ")", "\n", "args", ".", "encoder_layers", "=", "getattr", "(", "args", ",", "'encoder_layers'", ",", "'[(512, 3)] * 3'", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "512", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "'[(512, 3)] * 8'", ")", "\n", "args", ".", "decoder_out_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_out_embed_dim'", ",", "256", ")", "\n", "args", ".", "decoder_attention", "=", "getattr", "(", "args", ",", "'decoder_attention'", ",", "'True'", ")", "\n", "args", ".", "self_attention", "=", "getattr", "(", "args", ",", "'self_attention'", ",", "'False'", ")", "\n", "args", ".", "encoder_attention", "=", "getattr", "(", "args", ",", "'encoder_attention'", ",", "'False'", ")", "\n", "args", ".", "multihead_attention_nheads", "=", "getattr", "(", "args", ",", "'multihead_attention_nheads'", ",", "1", ")", "\n", "args", ".", "multihead_self_attention_nheads", "=", "getattr", "(", "args", ",", "'multihead_self_attention_nheads'", ",", "1", ")", "\n", "args", ".", "encoder_attention_nheads", "=", "getattr", "(", "args", ",", "'encoder_attention_nheads'", ",", "1", ")", "\n", "args", ".", "project_input", "=", "getattr", "(", "args", ",", "'project_input'", ",", "'False'", ")", "\n", "args", ".", "gated_attention", "=", "getattr", "(", "args", ",", "'gated_attention'", ",", "'False'", ")", "\n", "args", ".", "downsample", "=", "getattr", "(", "args", ",", "'downsample'", ",", "'False'", ")", "\n", "args", ".", "pretrained_checkpoint", "=", "getattr", "(", "args", ",", "'pretrained_checkpoint'", ",", "''", ")", "\n", "args", ".", "pretrained", "=", "getattr", "(", "args", ",", "'pretrained'", ",", "'False'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv_self_att.fconv_self_att_wp": [[551, 564], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "fconv_self_att.base_architecture"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.base_architecture"], ["", "@", "register_model_architecture", "(", "'fconv_self_att'", ",", "'fconv_self_att_wp'", ")", "\n", "def", "fconv_self_att_wp", "(", "args", ")", ":", "\n", "    ", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "256", ")", "\n", "args", ".", "encoder_layers", "=", "getattr", "(", "args", ",", "'encoder_layers'", ",", "'[(128, 3)] * 2 + [(512,3)] * 1'", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "256", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "'[(512, 4)] * 4 + [(768, 4)] * 2 + [(1024, 4)] * 1'", ")", "\n", "args", ".", "decoder_out_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_out_embed_dim'", ",", "256", ")", "\n", "args", ".", "self_attention", "=", "getattr", "(", "args", ",", "'self_attention'", ",", "'True'", ")", "\n", "args", ".", "multihead_self_attention_nheads", "=", "getattr", "(", "args", ",", "'multihead_self_attention_nheads'", ",", "4", ")", "\n", "args", ".", "project_input", "=", "getattr", "(", "args", ",", "'project_input'", ",", "'True'", ")", "\n", "args", ".", "gated_attention", "=", "getattr", "(", "args", ",", "'gated_attention'", ",", "'True'", ")", "\n", "args", ".", "downsample", "=", "getattr", "(", "args", ",", "'downsample'", ",", "'True'", ")", "\n", "base_architecture", "(", "args", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_encoder.FairseqEncoder.__init__": [[12, 15], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["def", "__init__", "(", "self", ",", "dictionary", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dictionary", "=", "dictionary", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_encoder.FairseqEncoder.forward": [[16, 25], ["None"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "src_tokens", ",", "src_lengths", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            src_tokens (LongTensor): tokens in the source language of shape\n                `(batch, src_len)`\n            src_lengths (LongTensor): lengths of each source sentence of shape\n                `(batch)`\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_encoder.FairseqEncoder.reorder_encoder_out": [[26, 38], ["None"], "methods", ["None"], ["", "def", "reorder_encoder_out", "(", "self", ",", "encoder_out", ",", "new_order", ")", ":", "\n", "        ", "\"\"\"\n        Reorder encoder output according to `new_order`.\n\n        Args:\n            encoder_out: output from the ``forward()`` method\n            new_order (LongTensor): desired order\n\n        Returns:\n            `encoder_out` rearranged according to `new_order`\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_encoder.FairseqEncoder.max_positions": [[39, 42], ["None"], "methods", ["None"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum input length supported by the encoder.\"\"\"", "\n", "return", "1e6", "# an arbitrary large number", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_encoder.FairseqEncoder.upgrade_state_dict": [[43, 46], ["None"], "methods", ["None"], ["", "def", "upgrade_state_dict", "(", "self", ",", "state_dict", ")", ":", "\n", "        ", "\"\"\"Upgrade a (possibly old) state dict for new versions of fairseq.\"\"\"", "\n", "return", "state_dict", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_incremental_decoder.FairseqIncrementalDecoder.__init__": [[31, 33], ["fairseq.models.FairseqDecoder.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["def", "__init__", "(", "self", ",", "dictionary", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_incremental_decoder.FairseqIncrementalDecoder.forward": [[34, 50], ["None"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "prev_output_tokens", ",", "encoder_out", "=", "None", ",", "incremental_state", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            prev_output_tokens (LongTensor): shifted output tokens of shape\n                `(batch, tgt_len)`, for teacher forcing\n            encoder_out (dict, optional): output from the encoder, used for\n                encoder-side attention\n            incremental_state (dict, optional): dictionary used for storing\n                state during :ref:`Incremental decoding`\n\n        Returns:\n            tuple:\n                - the decoder's output of shape `(batch, tgt_len, vocab)`\n                - a dictionary with any model-specific outputs\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_incremental_decoder.FairseqIncrementalDecoder.extract_features": [[51, 59], ["None"], "methods", ["None"], ["", "def", "extract_features", "(", "self", ",", "prev_output_tokens", ",", "encoder_out", "=", "None", ",", "incremental_state", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Returns:\n            tuple:\n                - the decoder's features of shape `(batch, tgt_len, embed_dim)`\n                - a dictionary with any model-specific outputs\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_incremental_decoder.FairseqIncrementalDecoder.reorder_incremental_state": [[60, 76], ["set", "fairseq_incremental_decoder.FairseqIncrementalDecoder.apply", "hasattr", "set.add", "module.reorder_incremental_state"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.add", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.linearized_convolution.LinearizedConvolution.reorder_incremental_state"], ["", "def", "reorder_incremental_state", "(", "self", ",", "incremental_state", ",", "new_order", ")", ":", "\n", "        ", "\"\"\"Reorder incremental state.\n\n        This should be called when the order of the input has changed from the\n        previous time step. A typical use case is beam search, where the input\n        order changes between time steps based on the selection of beams.\n        \"\"\"", "\n", "seen", "=", "set", "(", ")", "\n", "\n", "def", "apply_reorder_incremental_state", "(", "module", ")", ":", "\n", "            ", "if", "module", "!=", "self", "and", "hasattr", "(", "module", ",", "'reorder_incremental_state'", ")", "and", "module", "not", "in", "seen", ":", "\n", "                ", "seen", ".", "add", "(", "module", ")", "\n", "module", ".", "reorder_incremental_state", "(", "incremental_state", ",", "new_order", ")", "\n", "\n", "", "", "self", ".", "apply", "(", "apply_reorder_incremental_state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_incremental_decoder.FairseqIncrementalDecoder.set_beam_size": [[77, 90], ["getattr", "set", "fairseq_incremental_decoder.FairseqIncrementalDecoder.apply", "hasattr", "set.add", "module.set_beam_size"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.add", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.beamable_mm.BeamableMM.set_beam_size"], ["", "def", "set_beam_size", "(", "self", ",", "beam_size", ")", ":", "\n", "        ", "\"\"\"Sets the beam size in the decoder and all children.\"\"\"", "\n", "if", "getattr", "(", "self", ",", "'_beam_size'", ",", "-", "1", ")", "!=", "beam_size", ":", "\n", "            ", "seen", "=", "set", "(", ")", "\n", "\n", "def", "apply_set_beam_size", "(", "module", ")", ":", "\n", "                ", "if", "module", "!=", "self", "and", "hasattr", "(", "module", ",", "'set_beam_size'", ")", "and", "module", "not", "in", "seen", ":", "\n", "                    ", "seen", ".", "add", "(", "module", ")", "\n", "module", ".", "set_beam_size", "(", "beam_size", ")", "\n", "\n", "", "", "self", ".", "apply", "(", "apply_set_beam_size", ")", "\n", "self", ".", "_beam_size", "=", "beam_size", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lstm.LSTMModel.__init__": [[23, 25], ["fairseq.models.FairseqEncoderDecoderModel.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["    ", "def", "__init__", "(", "self", ",", "encoder", ",", "decoder", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "encoder", ",", "decoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lstm.LSTMModel.add_args": [[26, 77], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add model-specific arguments to the parser.\"\"\"", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'--dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'encoder embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-embed-path'", ",", "type", "=", "str", ",", "metavar", "=", "'STR'", ",", "\n", "help", "=", "'path to pre-trained encoder embedding'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-freeze-embed'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'freeze encoder embeddings'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-hidden-size'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'encoder hidden size'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-layers'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'number of encoder layers'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-bidirectional'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'make all layers of encoder bidirectional'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-embed-path'", ",", "type", "=", "str", ",", "metavar", "=", "'STR'", ",", "\n", "help", "=", "'path to pre-trained decoder embedding'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-freeze-embed'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'freeze decoder embeddings'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-hidden-size'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder hidden size'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-layers'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'number of decoder layers'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-out-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder output embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-attention'", ",", "type", "=", "str", ",", "metavar", "=", "'BOOL'", ",", "\n", "help", "=", "'decoder attention'", ")", "\n", "parser", ".", "add_argument", "(", "'--adaptive-softmax-cutoff'", ",", "metavar", "=", "'EXPR'", ",", "\n", "help", "=", "'comma separated list of adaptive softmax cutoff points. '", "\n", "'Must be used with adaptive_loss criterion'", ")", "\n", "parser", ".", "add_argument", "(", "'--share-decoder-input-output-embed'", ",", "default", "=", "False", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "'share decoder input and output embeddings'", ")", "\n", "parser", ".", "add_argument", "(", "'--share-all-embeddings'", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'share encoder, decoder and output embeddings'", "\n", "' (requires shared dictionary and embed dim)'", ")", "\n", "\n", "# Granular dropout settings (if not specified these default to --dropout)", "\n", "parser", ".", "add_argument", "(", "'--encoder-dropout-in'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability for encoder input embedding'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-dropout-out'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability for encoder output'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-dropout-in'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability for decoder input embedding'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-dropout-out'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability for decoder output'", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lstm.LSTMModel.build_model": [[79, 171], ["lstm.base_architecture", "lstm.LSTMEncoder", "lstm.LSTMDecoder", "cls", "ValueError", "len", "dictionary.pad", "lstm.Embedding", "fairseq.utils.parse_embedding", "fairseq.utils.print_embed_overlap", "fairseq.utils.load_embedding", "lstm.LSTMModel.build_model.load_pretrained_embedding_from_file"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.base_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.masked_lm_dictionary.BertDictionary.cls", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.Embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.parse_embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.print_embed_overlap", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.load_embedding"], ["", "@", "classmethod", "\n", "def", "build_model", "(", "cls", ",", "args", ",", "task", ")", ":", "\n", "        ", "\"\"\"Build a new model instance.\"\"\"", "\n", "# make sure that all args are properly defaulted (in case there are any new ones)", "\n", "base_architecture", "(", "args", ")", "\n", "\n", "if", "args", ".", "encoder_layers", "!=", "args", ".", "decoder_layers", ":", "\n", "            ", "raise", "ValueError", "(", "'--encoder-layers must match --decoder-layers'", ")", "\n", "\n", "", "def", "load_pretrained_embedding_from_file", "(", "embed_path", ",", "dictionary", ",", "embed_dim", ")", ":", "\n", "            ", "num_embeddings", "=", "len", "(", "dictionary", ")", "\n", "padding_idx", "=", "dictionary", ".", "pad", "(", ")", "\n", "embed_tokens", "=", "Embedding", "(", "num_embeddings", ",", "embed_dim", ",", "padding_idx", ")", "\n", "embed_dict", "=", "utils", ".", "parse_embedding", "(", "embed_path", ")", "\n", "utils", ".", "print_embed_overlap", "(", "embed_dict", ",", "dictionary", ")", "\n", "return", "utils", ".", "load_embedding", "(", "embed_dict", ",", "dictionary", ",", "embed_tokens", ")", "\n", "\n", "", "if", "args", ".", "encoder_embed_path", ":", "\n", "            ", "pretrained_encoder_embed", "=", "load_pretrained_embedding_from_file", "(", "\n", "args", ".", "encoder_embed_path", ",", "task", ".", "source_dictionary", ",", "args", ".", "encoder_embed_dim", ")", "\n", "", "else", ":", "\n", "            ", "num_embeddings", "=", "len", "(", "task", ".", "source_dictionary", ")", "\n", "pretrained_encoder_embed", "=", "Embedding", "(", "\n", "num_embeddings", ",", "args", ".", "encoder_embed_dim", ",", "task", ".", "source_dictionary", ".", "pad", "(", ")", "\n", ")", "\n", "\n", "", "if", "args", ".", "share_all_embeddings", ":", "\n", "# double check all parameters combinations are valid", "\n", "            ", "if", "task", ".", "source_dictionary", "!=", "task", ".", "target_dictionary", ":", "\n", "                ", "raise", "ValueError", "(", "'--share-all-embeddings requires a joint dictionary'", ")", "\n", "", "if", "args", ".", "decoder_embed_path", "and", "(", "\n", "args", ".", "decoder_embed_path", "!=", "args", ".", "encoder_embed_path", ")", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "'--share-all-embed not compatible with --decoder-embed-path'", "\n", ")", "\n", "", "if", "args", ".", "encoder_embed_dim", "!=", "args", ".", "decoder_embed_dim", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "'--share-all-embeddings requires --encoder-embed-dim to '", "\n", "'match --decoder-embed-dim'", "\n", ")", "\n", "", "pretrained_decoder_embed", "=", "pretrained_encoder_embed", "\n", "args", ".", "share_decoder_input_output_embed", "=", "True", "\n", "", "else", ":", "\n", "# separate decoder input embeddings", "\n", "            ", "pretrained_decoder_embed", "=", "None", "\n", "if", "args", ".", "decoder_embed_path", ":", "\n", "                ", "pretrained_decoder_embed", "=", "load_pretrained_embedding_from_file", "(", "\n", "args", ".", "decoder_embed_path", ",", "\n", "task", ".", "target_dictionary", ",", "\n", "args", ".", "decoder_embed_dim", "\n", ")", "\n", "# one last double check of parameter combinations", "\n", "", "", "if", "args", ".", "share_decoder_input_output_embed", "and", "(", "\n", "args", ".", "decoder_embed_dim", "!=", "args", ".", "decoder_out_embed_dim", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "'--share-decoder-input-output-embeddings requires '", "\n", "'--decoder-embed-dim to match --decoder-out-embed-dim'", "\n", ")", "\n", "\n", "", "if", "args", ".", "encoder_freeze_embed", ":", "\n", "            ", "pretrained_encoder_embed", ".", "weight", ".", "requires_grad", "=", "False", "\n", "", "if", "args", ".", "decoder_freeze_embed", ":", "\n", "            ", "pretrained_decoder_embed", ".", "weight", ".", "requires_grad", "=", "False", "\n", "\n", "", "encoder", "=", "LSTMEncoder", "(", "\n", "dictionary", "=", "task", ".", "source_dictionary", ",", "\n", "embed_dim", "=", "args", ".", "encoder_embed_dim", ",", "\n", "hidden_size", "=", "args", ".", "encoder_hidden_size", ",", "\n", "num_layers", "=", "args", ".", "encoder_layers", ",", "\n", "dropout_in", "=", "args", ".", "encoder_dropout_in", ",", "\n", "dropout_out", "=", "args", ".", "encoder_dropout_out", ",", "\n", "bidirectional", "=", "args", ".", "encoder_bidirectional", ",", "\n", "pretrained_embed", "=", "pretrained_encoder_embed", ",", "\n", ")", "\n", "decoder", "=", "LSTMDecoder", "(", "\n", "dictionary", "=", "task", ".", "target_dictionary", ",", "\n", "embed_dim", "=", "args", ".", "decoder_embed_dim", ",", "\n", "hidden_size", "=", "args", ".", "decoder_hidden_size", ",", "\n", "out_embed_dim", "=", "args", ".", "decoder_out_embed_dim", ",", "\n", "num_layers", "=", "args", ".", "decoder_layers", ",", "\n", "dropout_in", "=", "args", ".", "decoder_dropout_in", ",", "\n", "dropout_out", "=", "args", ".", "decoder_dropout_out", ",", "\n", "attention", "=", "options", ".", "eval_bool", "(", "args", ".", "decoder_attention", ")", ",", "\n", "encoder_output_units", "=", "encoder", ".", "output_units", ",", "\n", "pretrained_embed", "=", "pretrained_decoder_embed", ",", "\n", "share_input_output_embed", "=", "args", ".", "share_decoder_input_output_embed", ",", "\n", "adaptive_softmax_cutoff", "=", "(", "\n", "options", ".", "eval_str_list", "(", "args", ".", "adaptive_softmax_cutoff", ",", "type", "=", "int", ")", "\n", "if", "args", ".", "criterion", "==", "'adaptive_loss'", "else", "None", "\n", ")", ",", "\n", ")", "\n", "return", "cls", "(", "encoder", ",", "decoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lstm.LSTMEncoder.__init__": [[175, 207], ["fairseq.models.FairseqEncoder.__init__", "len", "dictionary.pad", "lstm.LSTM", "lstm.Embedding"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lstm.LSTM", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.Embedding"], ["def", "__init__", "(", "\n", "self", ",", "dictionary", ",", "embed_dim", "=", "512", ",", "hidden_size", "=", "512", ",", "num_layers", "=", "1", ",", "\n", "dropout_in", "=", "0.1", ",", "dropout_out", "=", "0.1", ",", "bidirectional", "=", "False", ",", "\n", "left_pad", "=", "True", ",", "pretrained_embed", "=", "None", ",", "padding_value", "=", "0.", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ")", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "self", ".", "dropout_in", "=", "dropout_in", "\n", "self", ".", "dropout_out", "=", "dropout_out", "\n", "self", ".", "bidirectional", "=", "bidirectional", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "\n", "num_embeddings", "=", "len", "(", "dictionary", ")", "\n", "self", ".", "padding_idx", "=", "dictionary", ".", "pad", "(", ")", "\n", "if", "pretrained_embed", "is", "None", ":", "\n", "            ", "self", ".", "embed_tokens", "=", "Embedding", "(", "num_embeddings", ",", "embed_dim", ",", "self", ".", "padding_idx", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "embed_tokens", "=", "pretrained_embed", "\n", "\n", "", "self", ".", "lstm", "=", "LSTM", "(", "\n", "input_size", "=", "embed_dim", ",", "\n", "hidden_size", "=", "hidden_size", ",", "\n", "num_layers", "=", "num_layers", ",", "\n", "dropout", "=", "self", ".", "dropout_out", "if", "num_layers", ">", "1", "else", "0.", ",", "\n", "bidirectional", "=", "bidirectional", ",", "\n", ")", "\n", "self", ".", "left_pad", "=", "left_pad", "\n", "self", ".", "padding_value", "=", "padding_value", "\n", "\n", "self", ".", "output_units", "=", "hidden_size", "\n", "if", "bidirectional", ":", "\n", "            ", "self", ".", "output_units", "*=", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lstm.LSTMEncoder.forward": [[208, 258], ["fairseq.utils.convert_padding_direction.size", "lstm.LSTMEncoder.embed_tokens", "torch.dropout", "torch.dropout", "torch.dropout", "torch.dropout.transpose", "torch.utils.rnn.pack_padded_sequence", "torch.utils.rnn.pack_padded_sequence", "torch.utils.rnn.pack_padded_sequence", "torch.dropout.new_zeros", "torch.dropout.new_zeros", "lstm.LSTMEncoder.lstm", "torch.utils.rnn.pad_packed_sequence", "torch.utils.rnn.pad_packed_sequence", "torch.utils.rnn.pad_packed_sequence", "torch.dropout", "torch.dropout", "torch.dropout", "fairseq.utils.convert_padding_direction.eq().t", "fairseq.utils.convert_padding_direction", "src_lengths.data.tolist", "list", "lstm.LSTMEncoder.forward.combine_bidir"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.convert_padding_direction"], ["", "", "def", "forward", "(", "self", ",", "src_tokens", ",", "src_lengths", ")", ":", "\n", "        ", "if", "self", ".", "left_pad", ":", "\n", "# nn.utils.rnn.pack_padded_sequence requires right-padding;", "\n", "# convert left-padding to right-padding", "\n", "            ", "src_tokens", "=", "utils", ".", "convert_padding_direction", "(", "\n", "src_tokens", ",", "\n", "self", ".", "padding_idx", ",", "\n", "left_to_right", "=", "True", ",", "\n", ")", "\n", "\n", "", "bsz", ",", "seqlen", "=", "src_tokens", ".", "size", "(", ")", "\n", "\n", "# embed tokens", "\n", "x", "=", "self", ".", "embed_tokens", "(", "src_tokens", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout_in", ",", "training", "=", "self", ".", "training", ")", "\n", "\n", "# B x T x C -> T x B x C", "\n", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "# pack embedded source tokens into a PackedSequence", "\n", "packed_x", "=", "nn", ".", "utils", ".", "rnn", ".", "pack_padded_sequence", "(", "x", ",", "src_lengths", ".", "data", ".", "tolist", "(", ")", ")", "\n", "\n", "# apply LSTM", "\n", "if", "self", ".", "bidirectional", ":", "\n", "            ", "state_size", "=", "2", "*", "self", ".", "num_layers", ",", "bsz", ",", "self", ".", "hidden_size", "\n", "", "else", ":", "\n", "            ", "state_size", "=", "self", ".", "num_layers", ",", "bsz", ",", "self", ".", "hidden_size", "\n", "", "h0", "=", "x", ".", "new_zeros", "(", "*", "state_size", ")", "\n", "c0", "=", "x", ".", "new_zeros", "(", "*", "state_size", ")", "\n", "packed_outs", ",", "(", "final_hiddens", ",", "final_cells", ")", "=", "self", ".", "lstm", "(", "packed_x", ",", "(", "h0", ",", "c0", ")", ")", "\n", "\n", "# unpack outputs and apply dropout", "\n", "x", ",", "_", "=", "nn", ".", "utils", ".", "rnn", ".", "pad_packed_sequence", "(", "packed_outs", ",", "padding_value", "=", "self", ".", "padding_value", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout_out", ",", "training", "=", "self", ".", "training", ")", "\n", "assert", "list", "(", "x", ".", "size", "(", ")", ")", "==", "[", "seqlen", ",", "bsz", ",", "self", ".", "output_units", "]", "\n", "\n", "if", "self", ".", "bidirectional", ":", "\n", "\n", "            ", "def", "combine_bidir", "(", "outs", ")", ":", "\n", "                ", "out", "=", "outs", ".", "view", "(", "self", ".", "num_layers", ",", "2", ",", "bsz", ",", "-", "1", ")", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", "\n", "return", "out", ".", "view", "(", "self", ".", "num_layers", ",", "bsz", ",", "-", "1", ")", "\n", "\n", "", "final_hiddens", "=", "combine_bidir", "(", "final_hiddens", ")", "\n", "final_cells", "=", "combine_bidir", "(", "final_cells", ")", "\n", "\n", "", "encoder_padding_mask", "=", "src_tokens", ".", "eq", "(", "self", ".", "padding_idx", ")", ".", "t", "(", ")", "\n", "\n", "return", "{", "\n", "'encoder_out'", ":", "(", "x", ",", "final_hiddens", ",", "final_cells", ")", ",", "\n", "'encoder_padding_mask'", ":", "encoder_padding_mask", "if", "encoder_padding_mask", ".", "any", "(", ")", "else", "None", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lstm.LSTMEncoder.reorder_encoder_out": [[260, 269], ["tuple", "encoder_out[].index_select", "eo.index_select"], "methods", ["None"], ["", "def", "reorder_encoder_out", "(", "self", ",", "encoder_out", ",", "new_order", ")", ":", "\n", "        ", "encoder_out", "[", "'encoder_out'", "]", "=", "tuple", "(", "\n", "eo", ".", "index_select", "(", "1", ",", "new_order", ")", "\n", "for", "eo", "in", "encoder_out", "[", "'encoder_out'", "]", "\n", ")", "\n", "if", "encoder_out", "[", "'encoder_padding_mask'", "]", "is", "not", "None", ":", "\n", "            ", "encoder_out", "[", "'encoder_padding_mask'", "]", "=", "encoder_out", "[", "'encoder_padding_mask'", "]", ".", "index_select", "(", "1", ",", "new_order", ")", "\n", "", "return", "encoder_out", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lstm.LSTMEncoder.max_positions": [[270, 273], ["int"], "methods", ["None"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum input length supported by the encoder.\"\"\"", "\n", "return", "int", "(", "1e5", ")", "# an arbitrary large number", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lstm.AttentionLayer.__init__": [[276, 281], ["torch.Module.__init__", "lstm.Linear", "lstm.Linear"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear"], ["    ", "def", "__init__", "(", "self", ",", "input_embed_dim", ",", "source_embed_dim", ",", "output_embed_dim", ",", "bias", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "input_proj", "=", "Linear", "(", "input_embed_dim", ",", "source_embed_dim", ",", "bias", "=", "bias", ")", "\n", "self", ".", "output_proj", "=", "Linear", "(", "input_embed_dim", "+", "source_embed_dim", ",", "output_embed_dim", ",", "bias", "=", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lstm.AttentionLayer.forward": [[282, 306], ["lstm.AttentionLayer.input_proj", "torch.softmax", "torch.softmax", "torch.softmax", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "attn_scores.float().masked_fill_().type_as.float().masked_fill_().type_as.float().masked_fill_().type_as", "lstm.AttentionLayer.output_proj", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.tanh.unsqueeze", "torch.tanh.unsqueeze", "torch.tanh.unsqueeze", "attn_scores.float().masked_fill_().type_as.float().masked_fill_().type_as.float().masked_fill_", "attn_scores.float().masked_fill_().type_as.float().masked_fill_().type_as.unsqueeze", "float", "attn_scores.float().masked_fill_().type_as.float().masked_fill_().type_as.float"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax"], ["", "def", "forward", "(", "self", ",", "input", ",", "source_hids", ",", "encoder_padding_mask", ")", ":", "\n", "# input: bsz x input_embed_dim", "\n", "# source_hids: srclen x bsz x output_embed_dim", "\n", "\n", "# x: bsz x output_embed_dim", "\n", "        ", "x", "=", "self", ".", "input_proj", "(", "input", ")", "\n", "\n", "# compute attention", "\n", "attn_scores", "=", "(", "source_hids", "*", "x", ".", "unsqueeze", "(", "0", ")", ")", ".", "sum", "(", "dim", "=", "2", ")", "\n", "\n", "# don't attend over padding", "\n", "if", "encoder_padding_mask", "is", "not", "None", ":", "\n", "            ", "attn_scores", "=", "attn_scores", ".", "float", "(", ")", ".", "masked_fill_", "(", "\n", "encoder_padding_mask", ",", "\n", "float", "(", "'-inf'", ")", "\n", ")", ".", "type_as", "(", "attn_scores", ")", "# FP16 support: cast to float and back", "\n", "\n", "", "attn_scores", "=", "F", ".", "softmax", "(", "attn_scores", ",", "dim", "=", "0", ")", "# srclen x bsz", "\n", "\n", "# sum weighted sources", "\n", "x", "=", "(", "attn_scores", ".", "unsqueeze", "(", "2", ")", "*", "source_hids", ")", ".", "sum", "(", "dim", "=", "0", ")", "\n", "\n", "x", "=", "torch", ".", "tanh", "(", "self", ".", "output_proj", "(", "torch", ".", "cat", "(", "(", "x", ",", "input", ")", ",", "dim", "=", "1", ")", ")", ")", "\n", "return", "x", ",", "attn_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lstm.LSTMDecoder.__init__": [[310, 357], ["fairseq.models.FairseqIncrementalDecoder.__init__", "len", "dictionary.pad", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "lstm.Embedding", "lstm.Linear", "lstm.Linear", "lstm.AttentionLayer", "lstm.Linear", "fairseq.modules.AdaptiveSoftmax", "lstm.LSTMCell", "lstm.Linear", "range"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.Embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lstm.LSTMCell", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear"], ["def", "__init__", "(", "\n", "self", ",", "dictionary", ",", "embed_dim", "=", "512", ",", "hidden_size", "=", "512", ",", "out_embed_dim", "=", "512", ",", "\n", "num_layers", "=", "1", ",", "dropout_in", "=", "0.1", ",", "dropout_out", "=", "0.1", ",", "attention", "=", "True", ",", "\n", "encoder_output_units", "=", "512", ",", "pretrained_embed", "=", "None", ",", "\n", "share_input_output_embed", "=", "False", ",", "adaptive_softmax_cutoff", "=", "None", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ")", "\n", "self", ".", "dropout_in", "=", "dropout_in", "\n", "self", ".", "dropout_out", "=", "dropout_out", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "share_input_output_embed", "=", "share_input_output_embed", "\n", "self", ".", "need_attn", "=", "True", "\n", "\n", "self", ".", "adaptive_softmax", "=", "None", "\n", "num_embeddings", "=", "len", "(", "dictionary", ")", "\n", "padding_idx", "=", "dictionary", ".", "pad", "(", ")", "\n", "if", "pretrained_embed", "is", "None", ":", "\n", "            ", "self", ".", "embed_tokens", "=", "Embedding", "(", "num_embeddings", ",", "embed_dim", ",", "padding_idx", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "embed_tokens", "=", "pretrained_embed", "\n", "\n", "", "self", ".", "encoder_output_units", "=", "encoder_output_units", "\n", "if", "encoder_output_units", "!=", "hidden_size", ":", "\n", "            ", "self", ".", "encoder_hidden_proj", "=", "Linear", "(", "encoder_output_units", ",", "hidden_size", ")", "\n", "self", ".", "encoder_cell_proj", "=", "Linear", "(", "encoder_output_units", ",", "hidden_size", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "encoder_hidden_proj", "=", "self", ".", "encoder_cell_proj", "=", "None", "\n", "", "self", ".", "layers", "=", "nn", ".", "ModuleList", "(", "[", "\n", "LSTMCell", "(", "\n", "input_size", "=", "hidden_size", "+", "embed_dim", "if", "layer", "==", "0", "else", "hidden_size", ",", "\n", "hidden_size", "=", "hidden_size", ",", "\n", ")", "\n", "for", "layer", "in", "range", "(", "num_layers", ")", "\n", "]", ")", "\n", "if", "attention", ":", "\n", "# TODO make bias configurable", "\n", "            ", "self", ".", "attention", "=", "AttentionLayer", "(", "hidden_size", ",", "encoder_output_units", ",", "hidden_size", ",", "bias", "=", "False", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "attention", "=", "None", "\n", "", "if", "hidden_size", "!=", "out_embed_dim", ":", "\n", "            ", "self", ".", "additional_fc", "=", "Linear", "(", "hidden_size", ",", "out_embed_dim", ")", "\n", "", "if", "adaptive_softmax_cutoff", "is", "not", "None", ":", "\n", "# setting adaptive_softmax dropout to dropout_out for now but can be redefined", "\n", "            ", "self", ".", "adaptive_softmax", "=", "AdaptiveSoftmax", "(", "num_embeddings", ",", "hidden_size", ",", "adaptive_softmax_cutoff", ",", "\n", "dropout", "=", "dropout_out", ")", "\n", "", "elif", "not", "self", ".", "share_input_output_embed", ":", "\n", "            ", "self", ".", "fc_out", "=", "Linear", "(", "out_embed_dim", ",", "num_embeddings", ",", "dropout", "=", "dropout_out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lstm.LSTMDecoder.forward": [[358, 448], ["prev_output_tokens.size", "encoder_outs.size", "lstm.LSTMDecoder.embed_tokens", "torch.dropout", "torch.dropout", "torch.dropout", "lstm.LSTMDecoder.transpose", "fairseq.utils.get_incremental_state", "lstm.LSTMDecoder.new_zeros", "range", "fairseq.utils.set_incremental_state", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "lstm.LSTMDecoder.transpose", "len", "lstm.LSTMDecoder.new_zeros", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "enumerate", "torch.dropout", "torch.dropout", "torch.dropout", "outs.append", "attn_scores.transpose.transpose.transpose", "hasattr", "rnn", "torch.dropout", "torch.dropout", "torch.dropout", "lstm.LSTMDecoder.attention", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "lstm.LSTMDecoder.additional_fc", "torch.dropout", "torch.dropout", "torch.dropout", "torch.linear", "torch.linear", "torch.linear", "lstm.LSTMDecoder.fc_out", "range", "range", "lstm.LSTMDecoder.encoder_hidden_proj", "lstm.LSTMDecoder.encoder_cell_proj"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.get_incremental_state", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.set_incremental_state", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "", "def", "forward", "(", "self", ",", "prev_output_tokens", ",", "encoder_out", ",", "incremental_state", "=", "None", ")", ":", "\n", "        ", "encoder_padding_mask", "=", "encoder_out", "[", "'encoder_padding_mask'", "]", "\n", "encoder_out", "=", "encoder_out", "[", "'encoder_out'", "]", "\n", "\n", "if", "incremental_state", "is", "not", "None", ":", "\n", "            ", "prev_output_tokens", "=", "prev_output_tokens", "[", ":", ",", "-", "1", ":", "]", "\n", "", "bsz", ",", "seqlen", "=", "prev_output_tokens", ".", "size", "(", ")", "\n", "\n", "# get outputs from encoder", "\n", "encoder_outs", ",", "encoder_hiddens", ",", "encoder_cells", "=", "encoder_out", "[", ":", "3", "]", "\n", "srclen", "=", "encoder_outs", ".", "size", "(", "0", ")", "\n", "\n", "# embed tokens", "\n", "x", "=", "self", ".", "embed_tokens", "(", "prev_output_tokens", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout_in", ",", "training", "=", "self", ".", "training", ")", "\n", "\n", "# B x T x C -> T x B x C", "\n", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "# initialize previous states (or get from cache during incremental generation)", "\n", "cached_state", "=", "utils", ".", "get_incremental_state", "(", "self", ",", "incremental_state", ",", "'cached_state'", ")", "\n", "if", "cached_state", "is", "not", "None", ":", "\n", "            ", "prev_hiddens", ",", "prev_cells", ",", "input_feed", "=", "cached_state", "\n", "", "else", ":", "\n", "            ", "num_layers", "=", "len", "(", "self", ".", "layers", ")", "\n", "prev_hiddens", "=", "[", "encoder_hiddens", "[", "i", "]", "for", "i", "in", "range", "(", "num_layers", ")", "]", "\n", "prev_cells", "=", "[", "encoder_cells", "[", "i", "]", "for", "i", "in", "range", "(", "num_layers", ")", "]", "\n", "if", "self", ".", "encoder_hidden_proj", "is", "not", "None", ":", "\n", "                ", "prev_hiddens", "=", "[", "self", ".", "encoder_hidden_proj", "(", "x", ")", "for", "x", "in", "prev_hiddens", "]", "\n", "prev_cells", "=", "[", "self", ".", "encoder_cell_proj", "(", "x", ")", "for", "x", "in", "prev_cells", "]", "\n", "", "input_feed", "=", "x", ".", "new_zeros", "(", "bsz", ",", "self", ".", "hidden_size", ")", "\n", "\n", "", "attn_scores", "=", "x", ".", "new_zeros", "(", "srclen", ",", "seqlen", ",", "bsz", ")", "\n", "outs", "=", "[", "]", "\n", "for", "j", "in", "range", "(", "seqlen", ")", ":", "\n", "# input feeding: concatenate context vector from previous time step", "\n", "            ", "input", "=", "torch", ".", "cat", "(", "(", "x", "[", "j", ",", ":", ",", ":", "]", ",", "input_feed", ")", ",", "dim", "=", "1", ")", "\n", "\n", "for", "i", ",", "rnn", "in", "enumerate", "(", "self", ".", "layers", ")", ":", "\n", "# recurrent cell", "\n", "                ", "hidden", ",", "cell", "=", "rnn", "(", "input", ",", "(", "prev_hiddens", "[", "i", "]", ",", "prev_cells", "[", "i", "]", ")", ")", "\n", "\n", "# hidden state becomes the input to the next layer", "\n", "input", "=", "F", ".", "dropout", "(", "hidden", ",", "p", "=", "self", ".", "dropout_out", ",", "training", "=", "self", ".", "training", ")", "\n", "\n", "# save state for next time step", "\n", "prev_hiddens", "[", "i", "]", "=", "hidden", "\n", "prev_cells", "[", "i", "]", "=", "cell", "\n", "\n", "# apply attention using the last layer's hidden state", "\n", "", "if", "self", ".", "attention", "is", "not", "None", ":", "\n", "                ", "out", ",", "attn_scores", "[", ":", ",", "j", ",", ":", "]", "=", "self", ".", "attention", "(", "hidden", ",", "encoder_outs", ",", "encoder_padding_mask", ")", "\n", "", "else", ":", "\n", "                ", "out", "=", "hidden", "\n", "", "out", "=", "F", ".", "dropout", "(", "out", ",", "p", "=", "self", ".", "dropout_out", ",", "training", "=", "self", ".", "training", ")", "\n", "\n", "# input feeding", "\n", "input_feed", "=", "out", "\n", "\n", "# save final output", "\n", "outs", ".", "append", "(", "out", ")", "\n", "\n", "# cache previous states (no-op except during incremental generation)", "\n", "", "utils", ".", "set_incremental_state", "(", "\n", "self", ",", "incremental_state", ",", "'cached_state'", ",", "\n", "(", "prev_hiddens", ",", "prev_cells", ",", "input_feed", ")", ",", "\n", ")", "\n", "\n", "# collect outputs across time steps", "\n", "x", "=", "torch", ".", "cat", "(", "outs", ",", "dim", "=", "0", ")", ".", "view", "(", "seqlen", ",", "bsz", ",", "self", ".", "hidden_size", ")", "\n", "\n", "# T x B x C -> B x T x C", "\n", "x", "=", "x", ".", "transpose", "(", "1", ",", "0", ")", "\n", "\n", "# srclen x tgtlen x bsz -> bsz x tgtlen x srclen", "\n", "if", "not", "self", ".", "training", "and", "self", ".", "need_attn", ":", "\n", "            ", "attn_scores", "=", "attn_scores", ".", "transpose", "(", "0", ",", "2", ")", "\n", "", "else", ":", "\n", "            ", "attn_scores", "=", "None", "\n", "\n", "# project back to size of vocabulary", "\n", "", "if", "self", ".", "adaptive_softmax", "is", "None", ":", "\n", "            ", "if", "hasattr", "(", "self", ",", "'additional_fc'", ")", ":", "\n", "                ", "x", "=", "self", ".", "additional_fc", "(", "x", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout_out", ",", "training", "=", "self", ".", "training", ")", "\n", "", "if", "self", ".", "share_input_output_embed", ":", "\n", "                ", "x", "=", "F", ".", "linear", "(", "x", ",", "self", ".", "embed_tokens", ".", "weight", ")", "\n", "", "else", ":", "\n", "                ", "x", "=", "self", ".", "fc_out", "(", "x", ")", "\n", "", "", "return", "x", ",", "attn_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lstm.LSTMDecoder.reorder_incremental_state": [[449, 462], ["super().reorder_incremental_state", "fairseq.utils.get_incremental_state", "tuple", "fairseq.utils.set_incremental_state", "isinstance", "state.index_select", "map", "lstm.LSTMDecoder.reorder_incremental_state.reorder_state"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.linearized_convolution.LinearizedConvolution.reorder_incremental_state", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.get_incremental_state", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.set_incremental_state"], ["", "def", "reorder_incremental_state", "(", "self", ",", "incremental_state", ",", "new_order", ")", ":", "\n", "        ", "super", "(", ")", ".", "reorder_incremental_state", "(", "incremental_state", ",", "new_order", ")", "\n", "cached_state", "=", "utils", ".", "get_incremental_state", "(", "self", ",", "incremental_state", ",", "'cached_state'", ")", "\n", "if", "cached_state", "is", "None", ":", "\n", "            ", "return", "\n", "\n", "", "def", "reorder_state", "(", "state", ")", ":", "\n", "            ", "if", "isinstance", "(", "state", ",", "list", ")", ":", "\n", "                ", "return", "[", "reorder_state", "(", "state_i", ")", "for", "state_i", "in", "state", "]", "\n", "", "return", "state", ".", "index_select", "(", "0", ",", "new_order", ")", "\n", "\n", "", "new_state", "=", "tuple", "(", "map", "(", "reorder_state", ",", "cached_state", ")", ")", "\n", "utils", ".", "set_incremental_state", "(", "self", ",", "incremental_state", ",", "'cached_state'", ",", "new_state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lstm.LSTMDecoder.max_positions": [[463, 466], ["int"], "methods", ["None"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum output length supported by the decoder.\"\"\"", "\n", "return", "int", "(", "1e5", ")", "# an arbitrary large number", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lstm.LSTMDecoder.make_generation_fast_": [[467, 469], ["None"], "methods", ["None"], ["", "def", "make_generation_fast_", "(", "self", ",", "need_attn", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "need_attn", "=", "need_attn", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lstm.Embedding": [[471, 476], ["torch.Embedding", "torch.init.uniform_", "torch.init.constant_"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.Embedding"], ["", "", "def", "Embedding", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", ")", ":", "\n", "    ", "m", "=", "nn", ".", "Embedding", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", "=", "padding_idx", ")", "\n", "nn", ".", "init", ".", "uniform_", "(", "m", ".", "weight", ",", "-", "0.1", ",", "0.1", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "weight", "[", "padding_idx", "]", ",", "0", ")", "\n", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lstm.LSTM": [[478, 484], ["torch.LSTM", "nn.LSTM.named_parameters", "param.data.uniform_"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lstm.LSTM"], ["", "def", "LSTM", "(", "input_size", ",", "hidden_size", ",", "**", "kwargs", ")", ":", "\n", "    ", "m", "=", "nn", ".", "LSTM", "(", "input_size", ",", "hidden_size", ",", "**", "kwargs", ")", "\n", "for", "name", ",", "param", "in", "m", ".", "named_parameters", "(", ")", ":", "\n", "        ", "if", "'weight'", "in", "name", "or", "'bias'", "in", "name", ":", "\n", "            ", "param", ".", "data", ".", "uniform_", "(", "-", "0.1", ",", "0.1", ")", "\n", "", "", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lstm.LSTMCell": [[486, 492], ["torch.LSTMCell", "nn.LSTMCell.named_parameters", "param.data.uniform_"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lstm.LSTMCell"], ["", "def", "LSTMCell", "(", "input_size", ",", "hidden_size", ",", "**", "kwargs", ")", ":", "\n", "    ", "m", "=", "nn", ".", "LSTMCell", "(", "input_size", ",", "hidden_size", ",", "**", "kwargs", ")", "\n", "for", "name", ",", "param", "in", "m", ".", "named_parameters", "(", ")", ":", "\n", "        ", "if", "'weight'", "in", "name", "or", "'bias'", "in", "name", ":", "\n", "            ", "param", ".", "data", ".", "uniform_", "(", "-", "0.1", ",", "0.1", ")", "\n", "", "", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lstm.Linear": [[494, 501], ["torch.Linear", "nn.Linear.weight.data.uniform_", "nn.Linear.bias.data.uniform_"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear"], ["", "def", "Linear", "(", "in_features", ",", "out_features", ",", "bias", "=", "True", ",", "dropout", "=", "0", ")", ":", "\n", "    ", "\"\"\"Linear layer (input: N x T x C)\"\"\"", "\n", "m", "=", "nn", ".", "Linear", "(", "in_features", ",", "out_features", ",", "bias", "=", "bias", ")", "\n", "m", ".", "weight", ".", "data", ".", "uniform_", "(", "-", "0.1", ",", "0.1", ")", "\n", "if", "bias", ":", "\n", "        ", "m", ".", "bias", ".", "data", ".", "uniform_", "(", "-", "0.1", ",", "0.1", ")", "\n", "", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lstm.base_architecture": [[503, 526], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture"], ["", "@", "register_model_architecture", "(", "'lstm'", ",", "'lstm'", ")", "\n", "def", "base_architecture", "(", "args", ")", ":", "\n", "    ", "args", ".", "dropout", "=", "getattr", "(", "args", ",", "'dropout'", ",", "0.1", ")", "\n", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "512", ")", "\n", "args", ".", "encoder_embed_path", "=", "getattr", "(", "args", ",", "'encoder_embed_path'", ",", "None", ")", "\n", "args", ".", "encoder_freeze_embed", "=", "getattr", "(", "args", ",", "'encoder_freeze_embed'", ",", "False", ")", "\n", "args", ".", "encoder_hidden_size", "=", "getattr", "(", "args", ",", "'encoder_hidden_size'", ",", "args", ".", "encoder_embed_dim", ")", "\n", "args", ".", "encoder_layers", "=", "getattr", "(", "args", ",", "'encoder_layers'", ",", "1", ")", "\n", "args", ".", "encoder_bidirectional", "=", "getattr", "(", "args", ",", "'encoder_bidirectional'", ",", "False", ")", "\n", "args", ".", "encoder_dropout_in", "=", "getattr", "(", "args", ",", "'encoder_dropout_in'", ",", "args", ".", "dropout", ")", "\n", "args", ".", "encoder_dropout_out", "=", "getattr", "(", "args", ",", "'encoder_dropout_out'", ",", "args", ".", "dropout", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "512", ")", "\n", "args", ".", "decoder_embed_path", "=", "getattr", "(", "args", ",", "'decoder_embed_path'", ",", "None", ")", "\n", "args", ".", "decoder_freeze_embed", "=", "getattr", "(", "args", ",", "'decoder_freeze_embed'", ",", "False", ")", "\n", "args", ".", "decoder_hidden_size", "=", "getattr", "(", "args", ",", "'decoder_hidden_size'", ",", "args", ".", "decoder_embed_dim", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "1", ")", "\n", "args", ".", "decoder_out_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_out_embed_dim'", ",", "512", ")", "\n", "args", ".", "decoder_attention", "=", "getattr", "(", "args", ",", "'decoder_attention'", ",", "'1'", ")", "\n", "args", ".", "decoder_dropout_in", "=", "getattr", "(", "args", ",", "'decoder_dropout_in'", ",", "args", ".", "dropout", ")", "\n", "args", ".", "decoder_dropout_out", "=", "getattr", "(", "args", ",", "'decoder_dropout_out'", ",", "args", ".", "dropout", ")", "\n", "args", ".", "share_decoder_input_output_embed", "=", "getattr", "(", "args", ",", "'share_decoder_input_output_embed'", ",", "False", ")", "\n", "args", ".", "share_all_embeddings", "=", "getattr", "(", "args", ",", "'share_all_embeddings'", ",", "False", ")", "\n", "args", ".", "adaptive_softmax_cutoff", "=", "getattr", "(", "args", ",", "'adaptive_softmax_cutoff'", ",", "'10000,50000,200000'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lstm.lstm_wiseman_iwslt_de_en": [[528, 539], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "lstm.base_architecture"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.base_architecture"], ["", "@", "register_model_architecture", "(", "'lstm'", ",", "'lstm_wiseman_iwslt_de_en'", ")", "\n", "def", "lstm_wiseman_iwslt_de_en", "(", "args", ")", ":", "\n", "    ", "args", ".", "dropout", "=", "getattr", "(", "args", ",", "'dropout'", ",", "0.1", ")", "\n", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "256", ")", "\n", "args", ".", "encoder_dropout_in", "=", "getattr", "(", "args", ",", "'encoder_dropout_in'", ",", "0", ")", "\n", "args", ".", "encoder_dropout_out", "=", "getattr", "(", "args", ",", "'encoder_dropout_out'", ",", "0", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "256", ")", "\n", "args", ".", "decoder_out_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_out_embed_dim'", ",", "256", ")", "\n", "args", ".", "decoder_dropout_in", "=", "getattr", "(", "args", ",", "'decoder_dropout_in'", ",", "0", ")", "\n", "args", ".", "decoder_dropout_out", "=", "getattr", "(", "args", ",", "'decoder_dropout_out'", ",", "args", ".", "dropout", ")", "\n", "base_architecture", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lstm.lstm_luong_wmt_en_de": [[541, 551], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "lstm.base_architecture"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.base_architecture"], ["", "@", "register_model_architecture", "(", "'lstm'", ",", "'lstm_luong_wmt_en_de'", ")", "\n", "def", "lstm_luong_wmt_en_de", "(", "args", ")", ":", "\n", "    ", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "1000", ")", "\n", "args", ".", "encoder_layers", "=", "getattr", "(", "args", ",", "'encoder_layers'", ",", "4", ")", "\n", "args", ".", "encoder_dropout_out", "=", "getattr", "(", "args", ",", "'encoder_dropout_out'", ",", "0", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "1000", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "4", ")", "\n", "args", ".", "decoder_out_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_out_embed_dim'", ",", "1000", ")", "\n", "args", ".", "decoder_dropout_out", "=", "getattr", "(", "args", ",", "'decoder_dropout_out'", ",", "0", ")", "\n", "base_architecture", "(", "args", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv.LightConvModel.__init__": [[50, 52], ["fairseq.models.FairseqEncoderDecoderModel.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["def", "__init__", "(", "self", ",", "encoder", ",", "decoder", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "encoder", ",", "decoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv.LightConvModel.add_args": [[53, 125], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "fairseq.options.eval_str_list", "fairseq.options.eval_str_list"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.eval_str_list", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.eval_str_list"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add model-specific arguments to the parser.\"\"\"", "\n", "parser", ".", "add_argument", "(", "'--dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability'", ")", "\n", "parser", ".", "add_argument", "(", "'--attention-dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability for attention weights'", ")", "\n", "parser", ".", "add_argument", "(", "'--relu-dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability after ReLU in FFN'", ")", "\n", "parser", ".", "add_argument", "(", "'--input-dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability of the inputs'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-embed-path'", ",", "type", "=", "str", ",", "metavar", "=", "'STR'", ",", "\n", "help", "=", "'path to pre-trained encoder embedding'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'encoder embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-conv-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'encoder embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-ffn-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'encoder embedding dimension for FFN'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-layers'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'num encoder layers'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-attention-heads'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'num encoder attention heads or LightConv/DynamicConv heads'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-normalize-before'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'apply layernorm before each encoder block'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-learned-pos'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'use learned positional embeddings in the encoder'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-embed-path'", ",", "type", "=", "str", ",", "metavar", "=", "'STR'", ",", "\n", "help", "=", "'path to pre-trained decoder embedding'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-conv-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-ffn-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder embedding dimension for FFN'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-layers'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'num decoder layers'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-attention-heads'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'num decoder attention heads or LightConv/DynamicConv heads'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-learned-pos'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'use learned positional embeddings in the decoder'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-normalize-before'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'apply layernorm before each decoder block'", ")", "\n", "parser", ".", "add_argument", "(", "'--share-decoder-input-output-embed'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'share decoder input and output embeddings'", ")", "\n", "parser", ".", "add_argument", "(", "'--share-all-embeddings'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'share encoder, decoder and output embeddings'", "\n", "' (requires shared dictionary and embed dim)'", ")", "\n", "parser", ".", "add_argument", "(", "'--adaptive-softmax-cutoff'", ",", "metavar", "=", "'EXPR'", ",", "\n", "help", "=", "'comma separated list of adaptive softmax cutoff points. '", "\n", "'Must be used with adaptive_loss criterion'", ")", ",", "\n", "parser", ".", "add_argument", "(", "'--adaptive-softmax-dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'sets adaptive softmax dropout for the tail projections'", ")", "\n", "\n", "\"\"\"LightConv and DynamicConv arguments\"\"\"", "\n", "parser", ".", "add_argument", "(", "'--encoder-kernel-size-list'", ",", "type", "=", "lambda", "x", ":", "options", ".", "eval_str_list", "(", "x", ",", "int", ")", ",", "\n", "help", "=", "'list of kernel size (default: \"[3,7,15,31,31,31,31]\")'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-kernel-size-list'", ",", "type", "=", "lambda", "x", ":", "options", ".", "eval_str_list", "(", "x", ",", "int", ")", ",", "\n", "help", "=", "'list of kernel size (default: \"[3,7,15,31,31,31]\")'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-glu'", ",", "type", "=", "options", ".", "eval_bool", ",", "\n", "help", "=", "'glu after in proj'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-glu'", ",", "type", "=", "options", ".", "eval_bool", ",", "\n", "help", "=", "'glu after in proj'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-conv-type'", ",", "default", "=", "'dynamic'", ",", "type", "=", "str", ",", "\n", "choices", "=", "[", "'dynamic'", ",", "'lightweight'", "]", ",", "\n", "help", "=", "'type of convolution'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-conv-type'", ",", "default", "=", "'dynamic'", ",", "type", "=", "str", ",", "\n", "choices", "=", "[", "'dynamic'", ",", "'lightweight'", "]", ",", "\n", "help", "=", "'type of convolution'", ")", "\n", "parser", ".", "add_argument", "(", "'--weight-softmax'", ",", "default", "=", "True", ",", "type", "=", "options", ".", "eval_bool", ")", "\n", "parser", ".", "add_argument", "(", "'--weight-dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability for conv weights'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv.LightConvModel.build_model": [[126, 175], ["lightconv.base_architecture", "lightconv.LightConvEncoder", "lightconv.LightConvDecoder", "lightconv.LightConvModel", "hasattr", "hasattr", "len", "dictionary.pad", "lightconv.Embedding", "lightconv.LightConvModel.build_model.build_embedding"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.base_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.Embedding"], ["", "@", "classmethod", "\n", "def", "build_model", "(", "cls", ",", "args", ",", "task", ")", ":", "\n", "        ", "\"\"\"Build a new model instance.\"\"\"", "\n", "\n", "# make sure all arguments are present in older models", "\n", "base_architecture", "(", "args", ")", "\n", "\n", "if", "not", "hasattr", "(", "args", ",", "'max_source_positions'", ")", ":", "\n", "            ", "args", ".", "max_source_positions", "=", "1024", "\n", "", "if", "not", "hasattr", "(", "args", ",", "'max_target_positions'", ")", ":", "\n", "            ", "args", ".", "max_target_positions", "=", "1024", "\n", "\n", "", "src_dict", ",", "tgt_dict", "=", "task", ".", "source_dictionary", ",", "task", ".", "target_dictionary", "\n", "\n", "def", "build_embedding", "(", "dictionary", ",", "embed_dim", ",", "path", "=", "None", ")", ":", "\n", "            ", "num_embeddings", "=", "len", "(", "dictionary", ")", "\n", "padding_idx", "=", "dictionary", ".", "pad", "(", ")", "\n", "emb", "=", "Embedding", "(", "num_embeddings", ",", "embed_dim", ",", "padding_idx", ")", "\n", "# if provided, load from preloaded dictionaries", "\n", "if", "path", ":", "\n", "                ", "embed_dict", "=", "utils", ".", "parse_embedding", "(", "path", ")", "\n", "utils", ".", "load_embedding", "(", "embed_dict", ",", "dictionary", ",", "emb", ")", "\n", "", "return", "emb", "\n", "\n", "", "if", "args", ".", "share_all_embeddings", ":", "\n", "            ", "if", "src_dict", "!=", "tgt_dict", ":", "\n", "                ", "raise", "RuntimeError", "(", "'--share-all-embeddings requires a joined dictionary'", ")", "\n", "", "if", "args", ".", "encoder_embed_dim", "!=", "args", ".", "decoder_embed_dim", ":", "\n", "                ", "raise", "RuntimeError", "(", "\n", "'--share-all-embeddings requires --encoder-embed-dim to match --decoder-embed-dim'", ")", "\n", "", "if", "args", ".", "decoder_embed_path", "and", "(", "\n", "args", ".", "decoder_embed_path", "!=", "args", ".", "encoder_embed_path", ")", ":", "\n", "                ", "raise", "RuntimeError", "(", "'--share-all-embeddings not compatible with --decoder-embed-path'", ")", "\n", "", "encoder_embed_tokens", "=", "build_embedding", "(", "\n", "src_dict", ",", "args", ".", "encoder_embed_dim", ",", "args", ".", "encoder_embed_path", "\n", ")", "\n", "decoder_embed_tokens", "=", "encoder_embed_tokens", "\n", "args", ".", "share_decoder_input_output_embed", "=", "True", "\n", "", "else", ":", "\n", "            ", "encoder_embed_tokens", "=", "build_embedding", "(", "\n", "src_dict", ",", "args", ".", "encoder_embed_dim", ",", "args", ".", "encoder_embed_path", "\n", ")", "\n", "decoder_embed_tokens", "=", "build_embedding", "(", "\n", "tgt_dict", ",", "args", ".", "decoder_embed_dim", ",", "args", ".", "decoder_embed_path", "\n", ")", "\n", "\n", "", "encoder", "=", "LightConvEncoder", "(", "args", ",", "src_dict", ",", "encoder_embed_tokens", ")", "\n", "decoder", "=", "LightConvDecoder", "(", "args", ",", "tgt_dict", ",", "decoder_embed_tokens", ")", "\n", "return", "LightConvModel", "(", "encoder", ",", "decoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv.LightConvEncoder.__init__": [[188, 212], ["fairseq.models.FairseqEncoder.__init__", "math.sqrt", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "lightconv.LightConvEncoder.layers.extend", "lightconv.LightConvEncoder.register_buffer", "fairseq.modules.PositionalEmbedding", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "fairseq.modules.LayerNorm", "lightconv.LightConvEncoderLayer", "range"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.positional_embedding.PositionalEmbedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.layer_norm.LayerNorm"], ["def", "__init__", "(", "self", ",", "args", ",", "dictionary", ",", "embed_tokens", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ")", "\n", "self", ".", "dropout", "=", "args", ".", "dropout", "\n", "\n", "embed_dim", "=", "embed_tokens", ".", "embedding_dim", "\n", "self", ".", "padding_idx", "=", "embed_tokens", ".", "padding_idx", "\n", "self", ".", "max_source_positions", "=", "args", ".", "max_source_positions", "\n", "\n", "self", ".", "embed_tokens", "=", "embed_tokens", "\n", "self", ".", "embed_scale", "=", "math", ".", "sqrt", "(", "embed_dim", ")", "\n", "self", ".", "embed_positions", "=", "PositionalEmbedding", "(", "\n", "args", ".", "max_source_positions", ",", "embed_dim", ",", "self", ".", "padding_idx", ",", "\n", "learned", "=", "args", ".", "encoder_learned_pos", ",", "\n", ")", "if", "not", "args", ".", "no_token_positional_embeddings", "else", "None", "\n", "\n", "self", ".", "layers", "=", "nn", ".", "ModuleList", "(", "[", "]", ")", "\n", "self", ".", "layers", ".", "extend", "(", "[", "\n", "LightConvEncoderLayer", "(", "args", ",", "kernel_size", "=", "args", ".", "encoder_kernel_size_list", "[", "i", "]", ")", "\n", "for", "i", "in", "range", "(", "args", ".", "encoder_layers", ")", "\n", "]", ")", "\n", "self", ".", "register_buffer", "(", "'version'", ",", "torch", ".", "Tensor", "(", "[", "2", "]", ")", ")", "\n", "self", ".", "normalize", "=", "args", ".", "encoder_normalize_before", "\n", "if", "self", ".", "normalize", ":", "\n", "            ", "self", ".", "layer_norm", "=", "LayerNorm", "(", "embed_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv.LightConvEncoder.forward": [[213, 250], ["torch.dropout", "torch.dropout", "torch.dropout", "lightconv.LightConvEncoder.transpose", "src_tokens.eq", "lightconv.LightConvEncoder.embed_tokens", "lightconv.LightConvEncoder.embed_positions", "src_tokens.eq.any", "layer", "lightconv.LightConvEncoder.layer_norm"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "src_tokens", ",", "**", "unused", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            src_tokens (LongTensor): tokens in the source language of shape\n                `(batch, src_len)`\n\n        Returns:\n            dict:\n                - **encoder_out** (Tensor): the last encoder layer's output of\n                  shape `(src_len, batch, embed_dim)`\n                - **encoder_padding_mask** (ByteTensor): the positions of\n                  padding elements of shape `(batch, src_len)`\n        \"\"\"", "\n", "# embed tokens and positions", "\n", "x", "=", "self", ".", "embed_scale", "*", "self", ".", "embed_tokens", "(", "src_tokens", ")", "\n", "if", "self", ".", "embed_positions", "is", "not", "None", ":", "\n", "            ", "x", "+=", "self", ".", "embed_positions", "(", "src_tokens", ")", "\n", "", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "\n", "# B x T x C -> T x B x C", "\n", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "# compute padding mask", "\n", "encoder_padding_mask", "=", "src_tokens", ".", "eq", "(", "self", ".", "padding_idx", ")", "\n", "if", "not", "encoder_padding_mask", ".", "any", "(", ")", ":", "\n", "            ", "encoder_padding_mask", "=", "None", "\n", "\n", "# encoder layers", "\n", "", "for", "layer", "in", "self", ".", "layers", ":", "\n", "            ", "x", "=", "layer", "(", "x", ",", "encoder_padding_mask", ")", "\n", "\n", "", "if", "self", ".", "normalize", ":", "\n", "            ", "x", "=", "self", ".", "layer_norm", "(", "x", ")", "\n", "\n", "", "return", "{", "\n", "'encoder_out'", ":", "x", ",", "# T x B x C", "\n", "'encoder_padding_mask'", ":", "encoder_padding_mask", ",", "# B x T", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv.LightConvEncoder.reorder_encoder_out": [[252, 270], ["encoder_out[].index_select", "encoder_out[].index_select"], "methods", ["None"], ["", "def", "reorder_encoder_out", "(", "self", ",", "encoder_out", ",", "new_order", ")", ":", "\n", "        ", "\"\"\"\n        Reorder encoder output according to *new_order*.\n\n        Args:\n            encoder_out: output from the ``forward()`` method\n            new_order (LongTensor): desired order\n\n        Returns:\n            *encoder_out* rearranged according to *new_order*\n        \"\"\"", "\n", "if", "encoder_out", "[", "'encoder_out'", "]", "is", "not", "None", ":", "\n", "            ", "encoder_out", "[", "'encoder_out'", "]", "=", "encoder_out", "[", "'encoder_out'", "]", ".", "index_select", "(", "1", ",", "new_order", ")", "\n", "", "if", "encoder_out", "[", "'encoder_padding_mask'", "]", "is", "not", "None", ":", "\n", "            ", "encoder_out", "[", "'encoder_padding_mask'", "]", "=", "encoder_out", "[", "'encoder_padding_mask'", "]", ".", "index_select", "(", "0", ",", "new_order", ")", "\n", "", "return", "encoder_out", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv.LightConvEncoder.max_positions": [[271, 276], ["min", "lightconv.LightConvEncoder.embed_positions.max_positions"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.max_positions"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum input length supported by the encoder.\"\"\"", "\n", "if", "self", ".", "embed_positions", "is", "None", ":", "\n", "            ", "return", "self", ".", "max_source_positions", "\n", "", "return", "min", "(", "self", ".", "max_source_positions", ",", "self", ".", "embed_positions", ".", "max_positions", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv.LightConvDecoder.__init__": [[291, 341], ["fairseq.models.FairseqIncrementalDecoder.__init__", "math.sqrt", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "lightconv.LightConvDecoder.layers.extend", "lightconv.LightConvDecoder.register_buffer", "lightconv.Linear", "fairseq.modules.PositionalEmbedding", "lightconv.Linear", "fairseq.modules.AdaptiveSoftmax", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "fairseq.modules.LayerNorm", "lightconv.LightConvDecoderLayer", "len", "fairseq.options.eval_str_list", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.init.normal_", "torch.init.normal_", "torch.init.normal_", "range", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "len"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.positional_embedding.PositionalEmbedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.layer_norm.LayerNorm", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.eval_str_list"], ["def", "__init__", "(", "self", ",", "args", ",", "dictionary", ",", "embed_tokens", ",", "no_encoder_attn", "=", "False", ",", "final_norm", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ")", "\n", "self", ".", "dropout", "=", "args", ".", "dropout", "\n", "self", ".", "share_input_output_embed", "=", "args", ".", "share_decoder_input_output_embed", "\n", "\n", "input_embed_dim", "=", "embed_tokens", ".", "embedding_dim", "\n", "embed_dim", "=", "args", ".", "decoder_embed_dim", "\n", "output_embed_dim", "=", "args", ".", "decoder_output_dim", "\n", "\n", "padding_idx", "=", "embed_tokens", ".", "padding_idx", "\n", "self", ".", "max_target_positions", "=", "args", ".", "max_target_positions", "\n", "\n", "self", ".", "embed_tokens", "=", "embed_tokens", "\n", "self", ".", "embed_scale", "=", "math", ".", "sqrt", "(", "embed_dim", ")", "# todo: try with input_embed_dim", "\n", "\n", "self", ".", "project_in_dim", "=", "Linear", "(", "input_embed_dim", ",", "embed_dim", ",", "bias", "=", "False", ")", "if", "embed_dim", "!=", "input_embed_dim", "else", "None", "\n", "\n", "self", ".", "embed_positions", "=", "PositionalEmbedding", "(", "\n", "args", ".", "max_target_positions", ",", "embed_dim", ",", "padding_idx", ",", "\n", "learned", "=", "args", ".", "decoder_learned_pos", ",", "\n", ")", "if", "not", "args", ".", "no_token_positional_embeddings", "else", "None", "\n", "\n", "self", ".", "layers", "=", "nn", ".", "ModuleList", "(", "[", "]", ")", "\n", "self", ".", "layers", ".", "extend", "(", "[", "\n", "LightConvDecoderLayer", "(", "args", ",", "no_encoder_attn", ",", "kernel_size", "=", "args", ".", "decoder_kernel_size_list", "[", "i", "]", ")", "\n", "for", "i", "in", "range", "(", "args", ".", "decoder_layers", ")", "\n", "]", ")", "\n", "\n", "self", ".", "adaptive_softmax", "=", "None", "\n", "\n", "self", ".", "project_out_dim", "=", "Linear", "(", "embed_dim", ",", "output_embed_dim", ",", "bias", "=", "False", ")", "if", "embed_dim", "!=", "output_embed_dim", "and", "not", "args", ".", "tie_adaptive_weights", "else", "None", "\n", "\n", "if", "args", ".", "adaptive_softmax_cutoff", "is", "not", "None", ":", "\n", "            ", "self", ".", "adaptive_softmax", "=", "AdaptiveSoftmax", "(", "\n", "len", "(", "dictionary", ")", ",", "\n", "output_embed_dim", ",", "\n", "options", ".", "eval_str_list", "(", "args", ".", "adaptive_softmax_cutoff", ",", "type", "=", "int", ")", ",", "\n", "dropout", "=", "args", ".", "adaptive_softmax_dropout", ",", "\n", "adaptive_inputs", "=", "embed_tokens", "if", "args", ".", "tie_adaptive_weights", "else", "None", ",", "\n", "factor", "=", "args", ".", "adaptive_softmax_factor", ",", "\n", "tie_proj", "=", "args", ".", "tie_adaptive_proj", ",", "\n", ")", "\n", "", "elif", "not", "self", ".", "share_input_output_embed", ":", "\n", "            ", "self", ".", "embed_out", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "len", "(", "dictionary", ")", ",", "output_embed_dim", ")", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "self", ".", "embed_out", ",", "mean", "=", "0", ",", "std", "=", "output_embed_dim", "**", "-", "0.5", ")", "\n", "", "self", ".", "register_buffer", "(", "'version'", ",", "torch", ".", "Tensor", "(", "[", "2", "]", ")", ")", "\n", "self", ".", "normalize", "=", "args", ".", "decoder_normalize_before", "and", "final_norm", "\n", "if", "self", ".", "normalize", ":", "\n", "            ", "self", ".", "layer_norm", "=", "LayerNorm", "(", "embed_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv.LightConvDecoder.forward": [[342, 413], ["torch.dropout", "torch.dropout", "torch.dropout", "torch.linear.transpose", "torch.linear.transpose", "lightconv.LightConvDecoder.embed_positions", "lightconv.LightConvDecoder.embed_tokens", "lightconv.LightConvDecoder.project_in_dim", "layer", "inner_states.append", "lightconv.LightConvDecoder.layer_norm", "lightconv.LightConvDecoder.project_out_dim", "torch.linear", "torch.linear", "torch.linear", "torch.linear", "torch.linear", "torch.linear"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "", "def", "forward", "(", "self", ",", "prev_output_tokens", ",", "encoder_out", "=", "None", ",", "incremental_state", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            prev_output_tokens (LongTensor): previous decoder outputs of shape\n                `(batch, tgt_len)`, for teacher forcing\n            encoder_out (Tensor, optional): output from the encoder, used for\n                encoder-side attention\n            incremental_state (dict): dictionary used for storing state during\n                :ref:`Incremental decoding`\n\n        Returns:\n            tuple:\n                - the last decoder layer's output of shape `(batch, tgt_len,\n                  vocab)`\n                - the last decoder layer's attention weights of shape `(batch,\n                  tgt_len, src_len)`\n        \"\"\"", "\n", "# embed positions", "\n", "positions", "=", "self", ".", "embed_positions", "(", "\n", "prev_output_tokens", ",", "\n", "incremental_state", "=", "incremental_state", ",", "\n", ")", "if", "self", ".", "embed_positions", "is", "not", "None", "else", "None", "\n", "\n", "if", "incremental_state", "is", "not", "None", ":", "\n", "            ", "prev_output_tokens", "=", "prev_output_tokens", "[", ":", ",", "-", "1", ":", "]", "\n", "if", "positions", "is", "not", "None", ":", "\n", "                ", "positions", "=", "positions", "[", ":", ",", "-", "1", ":", "]", "\n", "\n", "# embed tokens and positions", "\n", "", "", "x", "=", "self", ".", "embed_scale", "*", "self", ".", "embed_tokens", "(", "prev_output_tokens", ")", "\n", "\n", "if", "self", ".", "project_in_dim", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "project_in_dim", "(", "x", ")", "\n", "\n", "", "if", "positions", "is", "not", "None", ":", "\n", "            ", "x", "+=", "positions", "\n", "", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "\n", "# B x T x C -> T x B x C", "\n", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "attn", "=", "None", "\n", "\n", "inner_states", "=", "[", "x", "]", "\n", "\n", "# decoder layers", "\n", "for", "layer", "in", "self", ".", "layers", ":", "\n", "            ", "x", ",", "attn", "=", "layer", "(", "\n", "x", ",", "\n", "encoder_out", "[", "'encoder_out'", "]", "if", "encoder_out", "is", "not", "None", "else", "None", ",", "\n", "encoder_out", "[", "'encoder_padding_mask'", "]", "if", "encoder_out", "is", "not", "None", "else", "None", ",", "\n", "incremental_state", ",", "\n", ")", "\n", "inner_states", ".", "append", "(", "x", ")", "\n", "\n", "", "if", "self", ".", "normalize", ":", "\n", "            ", "x", "=", "self", ".", "layer_norm", "(", "x", ")", "\n", "\n", "# T x B x C -> B x T x C", "\n", "", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "if", "self", ".", "project_out_dim", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "project_out_dim", "(", "x", ")", "\n", "\n", "", "if", "self", ".", "adaptive_softmax", "is", "None", ":", "\n", "# project back to size of vocabulary", "\n", "            ", "if", "self", ".", "share_input_output_embed", ":", "\n", "                ", "x", "=", "F", ".", "linear", "(", "x", ",", "self", ".", "embed_tokens", ".", "weight", ")", "\n", "", "else", ":", "\n", "                ", "x", "=", "F", ".", "linear", "(", "x", ",", "self", ".", "embed_out", ")", "\n", "\n", "", "", "return", "x", ",", "{", "'attn'", ":", "attn", ",", "'inner_states'", ":", "inner_states", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv.LightConvDecoder.max_positions": [[414, 419], ["min", "lightconv.LightConvDecoder.embed_positions.max_positions"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.max_positions"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum output length supported by the decoder.\"\"\"", "\n", "if", "self", ".", "embed_positions", "is", "None", ":", "\n", "            ", "return", "self", ".", "max_target_positions", "\n", "", "return", "min", "(", "self", ".", "max_target_positions", ",", "self", ".", "embed_positions", ".", "max_positions", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv.LightConvDecoder.buffered_future_mask": [[420, 427], ["tensor.size", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "lightconv.LightConvDecoder._future_mask.size", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "hasattr", "fairseq.utils.fill_with_neg_inf", "fairseq.utils.fill_with_neg_inf", "tensor.new", "lightconv.LightConvDecoder._future_mask.resize_"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.fill_with_neg_inf", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.fill_with_neg_inf"], ["", "def", "buffered_future_mask", "(", "self", ",", "tensor", ")", ":", "\n", "        ", "dim", "=", "tensor", ".", "size", "(", "0", ")", "\n", "if", "not", "hasattr", "(", "self", ",", "'_future_mask'", ")", "or", "self", ".", "_future_mask", "is", "None", "or", "self", ".", "_future_mask", ".", "device", "!=", "tensor", ".", "device", ":", "\n", "            ", "self", ".", "_future_mask", "=", "torch", ".", "triu", "(", "utils", ".", "fill_with_neg_inf", "(", "tensor", ".", "new", "(", "dim", ",", "dim", ")", ")", ",", "1", ")", "\n", "", "if", "self", ".", "_future_mask", ".", "size", "(", "0", ")", "<", "dim", ":", "\n", "            ", "self", ".", "_future_mask", "=", "torch", ".", "triu", "(", "utils", ".", "fill_with_neg_inf", "(", "self", ".", "_future_mask", ".", "resize_", "(", "dim", ",", "dim", ")", ")", ",", "1", ")", "\n", "", "return", "self", ".", "_future_mask", "[", ":", "dim", ",", ":", "dim", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv.LightConvEncoderLayer.__init__": [[437, 470], ["torch.Module.__init__", "lightconv.Linear", "lightconv.Linear", "lightconv.Linear", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "lightconv.Linear", "torch.GLU", "torch.GLU", "torch.GLU", "lightconv.Linear", "fairseq.modules.LightweightConv1dTBC", "fairseq.modules.DynamicConv1dTBC", "fairseq.modules.LayerNorm", "range"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.layer_norm.LayerNorm"], ["def", "__init__", "(", "self", ",", "args", ",", "kernel_size", "=", "0", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "embed_dim", "=", "args", ".", "encoder_embed_dim", "\n", "self", ".", "conv_dim", "=", "args", ".", "encoder_conv_dim", "\n", "padding_l", "=", "kernel_size", "//", "2", "if", "kernel_size", "%", "2", "==", "1", "else", "(", "(", "kernel_size", "-", "1", ")", "//", "2", ",", "kernel_size", "//", "2", ")", "\n", "\n", "if", "args", ".", "encoder_glu", ":", "\n", "            ", "self", ".", "linear1", "=", "Linear", "(", "self", ".", "embed_dim", ",", "2", "*", "self", ".", "conv_dim", ")", "\n", "self", ".", "act", "=", "nn", ".", "GLU", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "linear1", "=", "Linear", "(", "self", ".", "embed_dim", ",", "self", ".", "conv_dim", ")", "\n", "self", ".", "act", "=", "None", "\n", "", "if", "args", ".", "encoder_conv_type", "==", "'lightweight'", ":", "\n", "            ", "self", ".", "conv", "=", "LightweightConv1dTBC", "(", "self", ".", "conv_dim", ",", "kernel_size", ",", "padding_l", "=", "padding_l", ",", "\n", "weight_softmax", "=", "args", ".", "weight_softmax", ",", "\n", "num_heads", "=", "args", ".", "encoder_attention_heads", ",", "\n", "weight_dropout", "=", "args", ".", "weight_dropout", ")", "\n", "", "elif", "args", ".", "encoder_conv_type", "==", "'dynamic'", ":", "\n", "            ", "self", ".", "conv", "=", "DynamicConv1dTBC", "(", "self", ".", "conv_dim", ",", "kernel_size", ",", "padding_l", "=", "padding_l", ",", "\n", "weight_softmax", "=", "args", ".", "weight_softmax", ",", "\n", "num_heads", "=", "args", ".", "encoder_attention_heads", ",", "\n", "weight_dropout", "=", "args", ".", "weight_dropout", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "", "self", ".", "linear2", "=", "Linear", "(", "self", ".", "conv_dim", ",", "self", ".", "embed_dim", ")", "\n", "\n", "self", ".", "dropout", "=", "args", ".", "dropout", "\n", "self", ".", "relu_dropout", "=", "args", ".", "relu_dropout", "\n", "self", ".", "input_dropout", "=", "args", ".", "input_dropout", "\n", "self", ".", "normalize_before", "=", "args", ".", "encoder_normalize_before", "\n", "self", ".", "fc1", "=", "Linear", "(", "self", ".", "embed_dim", ",", "args", ".", "encoder_ffn_embed_dim", ")", "\n", "self", ".", "fc2", "=", "Linear", "(", "args", ".", "encoder_ffn_embed_dim", ",", "self", ".", "embed_dim", ")", "\n", "self", ".", "layer_norms", "=", "nn", ".", "ModuleList", "(", "[", "LayerNorm", "(", "self", ".", "embed_dim", ")", "for", "_", "in", "range", "(", "2", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv.LightConvEncoderLayer.forward": [[471, 504], ["lightconv.LightConvEncoderLayer.maybe_layer_norm", "torch.dropout", "torch.dropout", "torch.dropout", "lightconv.LightConvEncoderLayer.linear1", "lightconv.LightConvEncoderLayer.conv", "lightconv.LightConvEncoderLayer.linear2", "torch.dropout", "torch.dropout", "torch.dropout", "lightconv.LightConvEncoderLayer.maybe_layer_norm", "lightconv.LightConvEncoderLayer.maybe_layer_norm", "torch.relu", "torch.relu", "torch.relu", "torch.dropout", "torch.dropout", "torch.dropout", "lightconv.LightConvEncoderLayer.fc2", "torch.dropout", "torch.dropout", "torch.dropout", "lightconv.LightConvEncoderLayer.maybe_layer_norm", "lightconv.LightConvEncoderLayer.act", "x.masked_fill.masked_fill.masked_fill", "lightconv.LightConvEncoderLayer.fc1", "encoder_padding_mask.transpose().unsqueeze", "encoder_padding_mask.transpose"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.TransformerDecoderLayer.maybe_layer_norm", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.TransformerDecoderLayer.maybe_layer_norm", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.TransformerDecoderLayer.maybe_layer_norm", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.TransformerDecoderLayer.maybe_layer_norm", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.stack_state_machine.StackStateMachine.act"], ["", "def", "forward", "(", "self", ",", "x", ",", "encoder_padding_mask", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`\n            encoder_padding_mask (ByteTensor): binary ByteTensor of shape\n                `(batch, src_len)` where padding elements are indicated by ``1``.\n\n        Returns:\n            encoded output of shape `(batch, src_len, embed_dim)`\n        \"\"\"", "\n", "residual", "=", "x", "\n", "x", "=", "self", ".", "maybe_layer_norm", "(", "0", ",", "x", ",", "before", "=", "True", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "input_dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "self", ".", "linear1", "(", "x", ")", "\n", "if", "self", ".", "act", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "act", "(", "x", ")", "\n", "", "if", "encoder_padding_mask", "is", "not", "None", ":", "\n", "            ", "x", "=", "x", ".", "masked_fill", "(", "encoder_padding_mask", ".", "transpose", "(", "0", ",", "1", ")", ".", "unsqueeze", "(", "2", ")", ",", "0", ")", "\n", "", "x", "=", "self", ".", "conv", "(", "x", ")", "\n", "x", "=", "self", ".", "linear2", "(", "x", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "residual", "+", "x", "\n", "x", "=", "self", ".", "maybe_layer_norm", "(", "0", ",", "x", ",", "after", "=", "True", ")", "\n", "\n", "residual", "=", "x", "\n", "x", "=", "self", ".", "maybe_layer_norm", "(", "1", ",", "x", ",", "before", "=", "True", ")", "\n", "x", "=", "F", ".", "relu", "(", "self", ".", "fc1", "(", "x", ")", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "relu_dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "self", ".", "fc2", "(", "x", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "residual", "+", "x", "\n", "x", "=", "self", ".", "maybe_layer_norm", "(", "1", ",", "x", ",", "after", "=", "True", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv.LightConvEncoderLayer.maybe_layer_norm": [[505, 511], ["None"], "methods", ["None"], ["", "def", "maybe_layer_norm", "(", "self", ",", "i", ",", "x", ",", "before", "=", "False", ",", "after", "=", "False", ")", ":", "\n", "        ", "assert", "before", "^", "after", "\n", "if", "after", "^", "self", ".", "normalize_before", ":", "\n", "            ", "return", "self", ".", "layer_norms", "[", "i", "]", "(", "x", ")", "\n", "", "else", ":", "\n", "            ", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv.LightConvEncoderLayer.extra_repr": [[512, 515], ["None"], "methods", ["None"], ["", "", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "return", "'dropout={}, relu_dropout={}, input_dropout={}, normalize_before={}'", ".", "format", "(", "\n", "self", ".", "dropout", ",", "self", ".", "relu_dropout", ",", "self", ".", "input_dropout", ",", "self", ".", "normalize_before", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv.LightConvDecoderLayer.__init__": [[527, 573], ["torch.Module.__init__", "lightconv.Linear", "fairseq.modules.LayerNorm", "lightconv.Linear", "lightconv.Linear", "fairseq.modules.LayerNorm", "lightconv.Linear", "torch.GLU", "torch.GLU", "torch.GLU", "lightconv.Linear", "fairseq.modules.LightweightConv1dTBC", "fairseq.modules.MultiheadAttention", "fairseq.modules.LayerNorm", "fairseq.modules.DynamicConv1dTBC"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.layer_norm.LayerNorm", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.layer_norm.LayerNorm", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.layer_norm.LayerNorm"], ["def", "__init__", "(", "self", ",", "args", ",", "no_encoder_attn", "=", "False", ",", "kernel_size", "=", "0", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "embed_dim", "=", "args", ".", "decoder_embed_dim", "\n", "self", ".", "conv_dim", "=", "args", ".", "decoder_conv_dim", "\n", "if", "args", ".", "decoder_glu", ":", "\n", "            ", "self", ".", "linear1", "=", "Linear", "(", "self", ".", "embed_dim", ",", "2", "*", "self", ".", "conv_dim", ")", "\n", "self", ".", "act", "=", "nn", ".", "GLU", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "linear1", "=", "Linear", "(", "self", ".", "embed_dim", ",", "self", ".", "conv_dim", ")", "\n", "self", ".", "act", "=", "None", "\n", "", "if", "args", ".", "decoder_conv_type", "==", "'lightweight'", ":", "\n", "            ", "self", ".", "conv", "=", "LightweightConv1dTBC", "(", "self", ".", "conv_dim", ",", "kernel_size", ",", "padding_l", "=", "kernel_size", "-", "1", ",", "\n", "weight_softmax", "=", "args", ".", "weight_softmax", ",", "\n", "num_heads", "=", "args", ".", "decoder_attention_heads", ",", "\n", "weight_dropout", "=", "args", ".", "weight_dropout", ")", "\n", "", "elif", "args", ".", "decoder_conv_type", "==", "'dynamic'", ":", "\n", "            ", "self", ".", "conv", "=", "DynamicConv1dTBC", "(", "self", ".", "conv_dim", ",", "kernel_size", ",", "padding_l", "=", "kernel_size", "-", "1", ",", "\n", "weight_softmax", "=", "args", ".", "weight_softmax", ",", "\n", "num_heads", "=", "args", ".", "decoder_attention_heads", ",", "\n", "weight_dropout", "=", "args", ".", "weight_dropout", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "", "self", ".", "linear2", "=", "Linear", "(", "self", ".", "conv_dim", ",", "self", ".", "embed_dim", ")", "\n", "\n", "self", ".", "dropout", "=", "args", ".", "dropout", "\n", "self", ".", "relu_dropout", "=", "args", ".", "relu_dropout", "\n", "self", ".", "input_dropout", "=", "args", ".", "input_dropout", "\n", "self", ".", "normalize_before", "=", "args", ".", "decoder_normalize_before", "\n", "\n", "self", ".", "conv_layer_norm", "=", "LayerNorm", "(", "self", ".", "embed_dim", ")", "\n", "\n", "if", "no_encoder_attn", ":", "\n", "            ", "self", ".", "encoder_attn", "=", "None", "\n", "self", ".", "encoder_attn_layer_norm", "=", "None", "\n", "", "else", ":", "\n", "            ", "self", ".", "encoder_attn", "=", "MultiheadAttention", "(", "\n", "self", ".", "embed_dim", ",", "args", ".", "decoder_attention_heads", ",", "\n", "dropout", "=", "args", ".", "attention_dropout", ",", "encoder_decoder_attention", "=", "True", "\n", ")", "\n", "self", ".", "encoder_attn_layer_norm", "=", "LayerNorm", "(", "self", ".", "embed_dim", ")", "\n", "\n", "", "self", ".", "fc1", "=", "Linear", "(", "self", ".", "embed_dim", ",", "args", ".", "decoder_ffn_embed_dim", ")", "\n", "self", ".", "fc2", "=", "Linear", "(", "args", ".", "decoder_ffn_embed_dim", ",", "self", ".", "embed_dim", ")", "\n", "\n", "self", ".", "final_layer_norm", "=", "LayerNorm", "(", "self", ".", "embed_dim", ")", "\n", "self", ".", "need_attn", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv.LightConvDecoderLayer.forward": [[574, 634], ["lightconv.LightConvDecoderLayer.maybe_layer_norm", "torch.dropout", "torch.dropout", "torch.dropout", "lightconv.LightConvDecoderLayer.linear1", "lightconv.LightConvDecoderLayer.conv", "lightconv.LightConvDecoderLayer.linear2", "torch.dropout", "torch.dropout", "torch.dropout", "lightconv.LightConvDecoderLayer.maybe_layer_norm", "lightconv.LightConvDecoderLayer.maybe_layer_norm", "torch.relu", "torch.relu", "torch.relu", "torch.dropout", "torch.dropout", "torch.dropout", "lightconv.LightConvDecoderLayer.fc2", "torch.dropout", "torch.dropout", "torch.dropout", "lightconv.LightConvDecoderLayer.maybe_layer_norm", "lightconv.LightConvDecoderLayer.conv._set_input_buffer", "lightconv.LightConvDecoderLayer.act", "lightconv.LightConvDecoderLayer.maybe_layer_norm", "lightconv.LightConvDecoderLayer.encoder_attn", "torch.dropout", "torch.dropout", "torch.dropout", "lightconv.LightConvDecoderLayer.maybe_layer_norm", "lightconv.LightConvDecoderLayer.fc1", "lightconv.LightConvDecoderLayer.encoder_attn._set_input_buffer"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.TransformerDecoderLayer.maybe_layer_norm", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.TransformerDecoderLayer.maybe_layer_norm", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.TransformerDecoderLayer.maybe_layer_norm", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.TransformerDecoderLayer.maybe_layer_norm", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.linearized_convolution.LinearizedConvolution._set_input_buffer", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.stack_state_machine.StackStateMachine.act", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.TransformerDecoderLayer.maybe_layer_norm", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.TransformerDecoderLayer.maybe_layer_norm", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.linearized_convolution.LinearizedConvolution._set_input_buffer"], ["", "def", "forward", "(", "self", ",", "x", ",", "encoder_out", ",", "encoder_padding_mask", ",", "incremental_state", ",", "\n", "prev_conv_state", "=", "None", ",", "prev_attn_state", "=", "None", ",", "conv_mask", "=", "None", ",", "\n", "conv_padding_mask", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`\n            encoder_padding_mask (ByteTensor): binary ByteTensor of shape\n                `(batch, src_len)` where padding elements are indicated by ``1``.\n\n        Returns:\n            encoded output of shape `(batch, src_len, embed_dim)`\n        \"\"\"", "\n", "residual", "=", "x", "\n", "x", "=", "self", ".", "maybe_layer_norm", "(", "self", ".", "conv_layer_norm", ",", "x", ",", "before", "=", "True", ")", "\n", "if", "prev_conv_state", "is", "not", "None", ":", "\n", "            ", "if", "incremental_state", "is", "None", ":", "\n", "                ", "incremental_state", "=", "{", "}", "\n", "", "self", ".", "conv", ".", "_set_input_buffer", "(", "incremental_state", ",", "prev_conv_state", ")", "\n", "", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "input_dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "self", ".", "linear1", "(", "x", ")", "\n", "if", "self", ".", "act", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "act", "(", "x", ")", "\n", "", "x", "=", "self", ".", "conv", "(", "x", ",", "incremental_state", "=", "incremental_state", ")", "\n", "x", "=", "self", ".", "linear2", "(", "x", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "residual", "+", "x", "\n", "x", "=", "self", ".", "maybe_layer_norm", "(", "self", ".", "conv_layer_norm", ",", "x", ",", "after", "=", "True", ")", "\n", "\n", "attn", "=", "None", "\n", "if", "self", ".", "encoder_attn", "is", "not", "None", ":", "\n", "            ", "residual", "=", "x", "\n", "x", "=", "self", ".", "maybe_layer_norm", "(", "self", ".", "encoder_attn_layer_norm", ",", "x", ",", "before", "=", "True", ")", "\n", "if", "prev_attn_state", "is", "not", "None", ":", "\n", "                ", "if", "incremental_state", "is", "None", ":", "\n", "                    ", "incremental_state", "=", "{", "}", "\n", "", "prev_key", ",", "prev_value", "=", "prev_attn_state", "\n", "saved_state", "=", "{", "\"prev_key\"", ":", "prev_key", ",", "\"prev_value\"", ":", "prev_value", "}", "\n", "self", ".", "encoder_attn", ".", "_set_input_buffer", "(", "incremental_state", ",", "saved_state", ")", "\n", "", "x", ",", "attn", "=", "self", ".", "encoder_attn", "(", "\n", "query", "=", "x", ",", "\n", "key", "=", "encoder_out", ",", "\n", "value", "=", "encoder_out", ",", "\n", "key_padding_mask", "=", "encoder_padding_mask", ",", "\n", "incremental_state", "=", "incremental_state", ",", "\n", "static_kv", "=", "True", ",", "\n", "need_weights", "=", "(", "not", "self", ".", "training", "and", "self", ".", "need_attn", ")", ",", "\n", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "residual", "+", "x", "\n", "x", "=", "self", ".", "maybe_layer_norm", "(", "self", ".", "encoder_attn_layer_norm", ",", "x", ",", "after", "=", "True", ")", "\n", "\n", "", "residual", "=", "x", "\n", "x", "=", "self", ".", "maybe_layer_norm", "(", "self", ".", "final_layer_norm", ",", "x", ",", "before", "=", "True", ")", "\n", "x", "=", "F", ".", "relu", "(", "self", ".", "fc1", "(", "x", ")", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "relu_dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "self", ".", "fc2", "(", "x", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "residual", "+", "x", "\n", "x", "=", "self", ".", "maybe_layer_norm", "(", "self", ".", "final_layer_norm", ",", "x", ",", "after", "=", "True", ")", "\n", "return", "x", ",", "attn", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv.LightConvDecoderLayer.maybe_layer_norm": [[635, 641], ["layer_norm"], "methods", ["None"], ["", "def", "maybe_layer_norm", "(", "self", ",", "layer_norm", ",", "x", ",", "before", "=", "False", ",", "after", "=", "False", ")", ":", "\n", "        ", "assert", "before", "^", "after", "\n", "if", "after", "^", "self", ".", "normalize_before", ":", "\n", "            ", "return", "layer_norm", "(", "x", ")", "\n", "", "else", ":", "\n", "            ", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv.LightConvDecoderLayer.make_generation_fast_": [[642, 644], ["None"], "methods", ["None"], ["", "", "def", "make_generation_fast_", "(", "self", ",", "need_attn", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "need_attn", "=", "need_attn", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv.LightConvDecoderLayer.extra_repr": [[645, 648], ["None"], "methods", ["None"], ["", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "return", "'dropout={}, relu_dropout={}, input_dropout={}, normalize_before={}'", ".", "format", "(", "\n", "self", ".", "dropout", ",", "self", ".", "relu_dropout", ",", "self", ".", "input_dropout", ",", "self", ".", "normalize_before", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv.Embedding": [[650, 655], ["torch.Embedding", "torch.init.normal_", "torch.init.constant_"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.Embedding"], ["", "", "def", "Embedding", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", ")", ":", "\n", "    ", "m", "=", "nn", ".", "Embedding", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", "=", "padding_idx", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "m", ".", "weight", ",", "mean", "=", "0", ",", "std", "=", "embedding_dim", "**", "-", "0.5", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "weight", "[", "padding_idx", "]", ",", "0", ")", "\n", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv.Linear": [[657, 663], ["torch.Linear", "torch.init.xavier_uniform_", "torch.init.constant_"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear"], ["", "def", "Linear", "(", "in_features", ",", "out_features", ",", "bias", "=", "True", ")", ":", "\n", "    ", "m", "=", "nn", ".", "Linear", "(", "in_features", ",", "out_features", ",", "bias", ")", "\n", "nn", ".", "init", ".", "xavier_uniform_", "(", "m", ".", "weight", ")", "\n", "if", "bias", ":", "\n", "        ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0.", ")", "\n", "", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv.base_architecture": [[665, 708], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture"], ["", "@", "register_model_architecture", "(", "'lightconv'", ",", "'lightconv'", ")", "\n", "def", "base_architecture", "(", "args", ")", ":", "\n", "    ", "args", ".", "encoder_embed_path", "=", "getattr", "(", "args", ",", "'encoder_embed_path'", ",", "None", ")", "\n", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "512", ")", "\n", "args", ".", "encoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_ffn_embed_dim'", ",", "2048", ")", "\n", "args", ".", "encoder_layers", "=", "getattr", "(", "args", ",", "'encoder_layers'", ",", "7", ")", "\n", "args", ".", "encoder_attention_heads", "=", "getattr", "(", "args", ",", "'encoder_attention_heads'", ",", "8", ")", "\n", "args", ".", "encoder_normalize_before", "=", "getattr", "(", "args", ",", "'encoder_normalize_before'", ",", "False", ")", "\n", "args", ".", "encoder_learned_pos", "=", "getattr", "(", "args", ",", "'encoder_learned_pos'", ",", "False", ")", "\n", "args", ".", "decoder_embed_path", "=", "getattr", "(", "args", ",", "'decoder_embed_path'", ",", "None", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "args", ".", "encoder_embed_dim", ")", "\n", "args", ".", "decoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_ffn_embed_dim'", ",", "args", ".", "encoder_ffn_embed_dim", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "6", ")", "\n", "args", ".", "decoder_attention_heads", "=", "getattr", "(", "args", ",", "'decoder_attention_heads'", ",", "8", ")", "\n", "args", ".", "decoder_normalize_before", "=", "getattr", "(", "args", ",", "'decoder_normalize_before'", ",", "False", ")", "\n", "args", ".", "decoder_learned_pos", "=", "getattr", "(", "args", ",", "'decoder_learned_pos'", ",", "False", ")", "\n", "args", ".", "attention_dropout", "=", "getattr", "(", "args", ",", "'attention_dropout'", ",", "0.", ")", "\n", "args", ".", "relu_dropout", "=", "getattr", "(", "args", ",", "'relu_dropout'", ",", "0.", ")", "\n", "args", ".", "dropout", "=", "getattr", "(", "args", ",", "'dropout'", ",", "0.1", ")", "\n", "args", ".", "adaptive_softmax_cutoff", "=", "getattr", "(", "args", ",", "'adaptive_softmax_cutoff'", ",", "None", ")", "\n", "args", ".", "adaptive_softmax_dropout", "=", "getattr", "(", "args", ",", "'adaptive_softmax_dropout'", ",", "0", ")", "\n", "args", ".", "share_decoder_input_output_embed", "=", "getattr", "(", "args", ",", "'share_decoder_input_output_embed'", ",", "False", ")", "\n", "args", ".", "share_all_embeddings", "=", "getattr", "(", "args", ",", "'share_all_embeddings'", ",", "False", ")", "\n", "args", ".", "no_token_positional_embeddings", "=", "getattr", "(", "args", ",", "'no_token_positional_embeddings'", ",", "False", ")", "\n", "\n", "args", ".", "decoder_output_dim", "=", "getattr", "(", "args", ",", "'decoder_output_dim'", ",", "args", ".", "decoder_embed_dim", ")", "\n", "args", ".", "decoder_input_dim", "=", "getattr", "(", "args", ",", "'decoder_input_dim'", ",", "args", ".", "decoder_embed_dim", ")", "\n", "\n", "args", ".", "encoder_conv_dim", "=", "getattr", "(", "args", ",", "'encoder_conv_dim'", ",", "args", ".", "encoder_embed_dim", ")", "\n", "args", ".", "decoder_conv_dim", "=", "getattr", "(", "args", ",", "'decoder_conv_dim'", ",", "args", ".", "decoder_embed_dim", ")", "\n", "\n", "args", ".", "encoder_kernel_size_list", "=", "getattr", "(", "args", ",", "'encoder_kernel_size_list'", ",", "[", "3", ",", "7", ",", "15", ",", "31", ",", "31", ",", "31", ",", "31", "]", ")", "\n", "args", ".", "decoder_kernel_size_list", "=", "getattr", "(", "args", ",", "'decoder_kernel_size_list'", ",", "[", "3", ",", "7", ",", "15", ",", "31", ",", "31", ",", "31", "]", ")", "\n", "if", "len", "(", "args", ".", "encoder_kernel_size_list", ")", "==", "1", ":", "\n", "        ", "args", ".", "encoder_kernel_size_list", "=", "args", ".", "encoder_kernel_size_list", "*", "args", ".", "encoder_layers", "\n", "", "if", "len", "(", "args", ".", "decoder_kernel_size_list", ")", "==", "1", ":", "\n", "        ", "args", ".", "decoder_kernel_size_list", "=", "args", ".", "decoder_kernel_size_list", "*", "args", ".", "decoder_layers", "\n", "", "assert", "len", "(", "args", ".", "encoder_kernel_size_list", ")", "==", "args", ".", "encoder_layers", ",", "\"encoder_kernel_size_list doesn't match encoder_layers\"", "\n", "assert", "len", "(", "args", ".", "decoder_kernel_size_list", ")", "==", "args", ".", "decoder_layers", ",", "\"decoder_kernel_size_list doesn't match decoder_layers\"", "\n", "args", ".", "encoder_glu", "=", "getattr", "(", "args", ",", "'encoder_glu'", ",", "True", ")", "\n", "args", ".", "decoder_glu", "=", "getattr", "(", "args", ",", "'decoder_glu'", ",", "True", ")", "\n", "args", ".", "input_dropout", "=", "getattr", "(", "args", ",", "'input_dropout'", ",", "0.1", ")", "\n", "args", ".", "weight_dropout", "=", "getattr", "(", "args", ",", "'weight_dropout'", ",", "args", ".", "attention_dropout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv.lightconv_iwslt_de_en": [[710, 726], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "lightconv.base_architecture"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.base_architecture"], ["", "@", "register_model_architecture", "(", "'lightconv'", ",", "'lightconv_iwslt_de_en'", ")", "\n", "def", "lightconv_iwslt_de_en", "(", "args", ")", ":", "\n", "    ", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "512", ")", "\n", "args", ".", "encoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_ffn_embed_dim'", ",", "1024", ")", "\n", "args", ".", "encoder_attention_heads", "=", "getattr", "(", "args", ",", "'encoder_attention_heads'", ",", "4", ")", "\n", "args", ".", "encoder_layers", "=", "getattr", "(", "args", ",", "'encoder_layers'", ",", "7", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "512", ")", "\n", "args", ".", "decoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_ffn_embed_dim'", ",", "1024", ")", "\n", "args", ".", "decoder_attention_heads", "=", "getattr", "(", "args", ",", "'decoder_attention_heads'", ",", "4", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "6", ")", "\n", "args", ".", "attention_dropout", "=", "getattr", "(", "args", ",", "'attention_dropout'", ",", "0.1", ")", "\n", "args", ".", "weight_dropout", "=", "getattr", "(", "args", ",", "'weight_dropout'", ",", "0.1", ")", "\n", "args", ".", "encoder_glu", "=", "getattr", "(", "args", ",", "'encoder_glu'", ",", "False", ")", "\n", "args", ".", "decoder_glu", "=", "getattr", "(", "args", ",", "'decoder_glu'", ",", "False", ")", "\n", "args", ".", "input_dropout", "=", "getattr", "(", "args", ",", "'input_dropout'", ",", "0.0", ")", "\n", "base_architecture", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv.lightconv_wmt_en_de": [[728, 731], ["fairseq.models.register_model_architecture", "lightconv.base_architecture"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.base_architecture"], ["", "@", "register_model_architecture", "(", "'lightconv'", ",", "'lightconv_wmt_en_de'", ")", "\n", "def", "lightconv_wmt_en_de", "(", "args", ")", ":", "\n", "    ", "base_architecture", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv.lightconv_wmt_en_de_big": [[733, 745], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "lightconv.base_architecture"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.base_architecture"], ["", "@", "register_model_architecture", "(", "'lightconv'", ",", "'lightconv_wmt_en_de_big'", ")", "\n", "def", "lightconv_wmt_en_de_big", "(", "args", ")", ":", "\n", "    ", "args", ".", "attention_dropout", "=", "getattr", "(", "args", ",", "'attention_dropout'", ",", "0.1", ")", "\n", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "1024", ")", "\n", "args", ".", "encoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_ffn_embed_dim'", ",", "4096", ")", "\n", "args", ".", "encoder_attention_heads", "=", "getattr", "(", "args", ",", "'encoder_attention_heads'", ",", "16", ")", "\n", "args", ".", "encoder_normalize_before", "=", "getattr", "(", "args", ",", "'encoder_normalize_before'", ",", "False", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "1024", ")", "\n", "args", ".", "decoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_ffn_embed_dim'", ",", "4096", ")", "\n", "args", ".", "decoder_attention_heads", "=", "getattr", "(", "args", ",", "'decoder_attention_heads'", ",", "16", ")", "\n", "args", ".", "dropout", "=", "getattr", "(", "args", ",", "'dropout'", ",", "0.3", ")", "\n", "base_architecture", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv.lightconv_wmt_en_fr_big": [[747, 751], ["fairseq.models.register_model_architecture", "getattr", "lightconv.lightconv_wmt_en_de_big"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv.lightconv_wmt_en_de_big"], ["", "@", "register_model_architecture", "(", "'lightconv'", ",", "'lightconv_wmt_en_fr_big'", ")", "\n", "def", "lightconv_wmt_en_fr_big", "(", "args", ")", ":", "\n", "    ", "args", ".", "dropout", "=", "getattr", "(", "args", ",", "'dropout'", ",", "0.1", ")", "\n", "lightconv_wmt_en_de_big", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv.lightconv_wmt_zh_en_big": [[753, 759], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "lightconv.lightconv_wmt_en_de_big"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv.lightconv_wmt_en_de_big"], ["", "@", "register_model_architecture", "(", "'lightconv'", ",", "'lightconv_wmt_zh_en_big'", ")", "\n", "def", "lightconv_wmt_zh_en_big", "(", "args", ")", ":", "\n", "    ", "args", ".", "dropout", "=", "getattr", "(", "args", ",", "'dropout'", ",", "0.2", ")", "\n", "args", ".", "attention_dropout", "=", "getattr", "(", "args", ",", "'attention_dropout'", ",", "0.2", ")", "\n", "args", ".", "weight_dropout", "=", "getattr", "(", "args", ",", "'weight_dropout'", ",", "0.2", ")", "\n", "lightconv_wmt_en_de_big", "(", "args", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv.FConvModel.hub_models": [[44, 50], ["None"], "methods", ["None"], ["@", "classmethod", "\n", "def", "hub_models", "(", "cls", ")", ":", "\n", "        ", "return", "{", "\n", "'conv.wmt14.en-fr'", ":", "'https://dl.fbaipublicfiles.com/fairseq/models/wmt14.v2.en-fr.fconv-py.tar.bz2'", ",", "\n", "'conv.wmt14.en-de'", ":", "'https://dl.fbaipublicfiles.com/fairseq/models/wmt14.en-de.fconv-py.tar.bz2'", ",", "\n", "'conv.wmt17.en-de'", ":", "'https://dl.fbaipublicfiles.com/fairseq/models/wmt17.v2.en-de.fconv-py.tar.bz2'", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv.FConvModel.__init__": [[52, 55], ["fairseq.models.FairseqEncoderDecoderModel.__init__", "sum"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["", "def", "__init__", "(", "self", ",", "encoder", ",", "decoder", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "encoder", ",", "decoder", ")", "\n", "self", ".", "encoder", ".", "num_attention_layers", "=", "sum", "(", "layer", "is", "not", "None", "for", "layer", "in", "decoder", ".", "attention", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv.FConvModel.add_args": [[56, 80], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add model-specific arguments to the parser.\"\"\"", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'--dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'encoder embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-embed-path'", ",", "type", "=", "str", ",", "metavar", "=", "'STR'", ",", "\n", "help", "=", "'path to pre-trained encoder embedding'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-layers'", ",", "type", "=", "str", ",", "metavar", "=", "'EXPR'", ",", "\n", "help", "=", "'encoder layers [(dim, kernel_size), ...]'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-embed-path'", ",", "type", "=", "str", ",", "metavar", "=", "'STR'", ",", "\n", "help", "=", "'path to pre-trained decoder embedding'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-layers'", ",", "type", "=", "str", ",", "metavar", "=", "'EXPR'", ",", "\n", "help", "=", "'decoder layers [(dim, kernel_size), ...]'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-out-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder output embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-attention'", ",", "type", "=", "str", ",", "metavar", "=", "'EXPR'", ",", "\n", "help", "=", "'decoder attention [True, ...]'", ")", "\n", "parser", ".", "add_argument", "(", "'--share-input-output-embed'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'share input and output embeddings (requires'", "\n", "' --decoder-out-embed-dim and --decoder-embed-dim'", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv.FConvModel.build_model": [[84, 120], ["fconv.base_architecture", "fconv.FConvEncoder", "fconv.FConvDecoder", "fconv.FConvModel", "fairseq.utils.parse_embedding", "fairseq.utils.print_embed_overlap", "fairseq.utils.parse_embedding", "fairseq.utils.print_embed_overlap", "eval", "eval", "eval"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.base_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.parse_embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.print_embed_overlap", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.parse_embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.print_embed_overlap", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval"], ["", "@", "classmethod", "\n", "def", "build_model", "(", "cls", ",", "args", ",", "task", ")", ":", "\n", "        ", "\"\"\"Build a new model instance.\"\"\"", "\n", "# make sure that all args are properly defaulted (in case there are any new ones)", "\n", "base_architecture", "(", "args", ")", "\n", "\n", "encoder_embed_dict", "=", "None", "\n", "if", "args", ".", "encoder_embed_path", ":", "\n", "            ", "encoder_embed_dict", "=", "utils", ".", "parse_embedding", "(", "args", ".", "encoder_embed_path", ")", "\n", "utils", ".", "print_embed_overlap", "(", "encoder_embed_dict", ",", "task", ".", "source_dictionary", ")", "\n", "\n", "", "decoder_embed_dict", "=", "None", "\n", "if", "args", ".", "decoder_embed_path", ":", "\n", "            ", "decoder_embed_dict", "=", "utils", ".", "parse_embedding", "(", "args", ".", "decoder_embed_path", ")", "\n", "utils", ".", "print_embed_overlap", "(", "decoder_embed_dict", ",", "task", ".", "target_dictionary", ")", "\n", "\n", "", "encoder", "=", "FConvEncoder", "(", "\n", "dictionary", "=", "task", ".", "source_dictionary", ",", "\n", "embed_dim", "=", "args", ".", "encoder_embed_dim", ",", "\n", "embed_dict", "=", "encoder_embed_dict", ",", "\n", "convolutions", "=", "eval", "(", "args", ".", "encoder_layers", ")", ",", "\n", "dropout", "=", "args", ".", "dropout", ",", "\n", "max_positions", "=", "args", ".", "max_source_positions", ",", "\n", ")", "\n", "decoder", "=", "FConvDecoder", "(", "\n", "dictionary", "=", "task", ".", "target_dictionary", ",", "\n", "embed_dim", "=", "args", ".", "decoder_embed_dim", ",", "\n", "embed_dict", "=", "decoder_embed_dict", ",", "\n", "convolutions", "=", "eval", "(", "args", ".", "decoder_layers", ")", ",", "\n", "out_embed_dim", "=", "args", ".", "decoder_out_embed_dim", ",", "\n", "attention", "=", "eval", "(", "args", ".", "decoder_attention", ")", ",", "\n", "dropout", "=", "args", ".", "dropout", ",", "\n", "max_positions", "=", "args", ".", "max_target_positions", ",", "\n", "share_embed", "=", "args", ".", "share_input_output_embed", ",", "\n", ")", "\n", "return", "FConvModel", "(", "encoder", ",", "decoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv.FConvEncoder.__init__": [[140, 187], ["fairseq.models.FairseqEncoder.__init__", "len", "dictionary.pad", "fconv.Embedding", "fconv.PositionalEmbedding", "fconv.extend_conv_spec", "fconv.Linear", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "enumerate", "fconv.Linear", "fairseq.utils.load_embedding", "fconv.FConvEncoder.projections.append", "fconv.FConvEncoder.convolutions.append", "fconv.FConvEncoder.residuals.append", "layer_in_channels.append", "fconv.ConvTBC", "fconv.Linear"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.Embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.positional_embedding.PositionalEmbedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv.extend_conv_spec", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.load_embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv.ConvTBC", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear"], ["def", "__init__", "(", "\n", "self", ",", "dictionary", ",", "embed_dim", "=", "512", ",", "embed_dict", "=", "None", ",", "max_positions", "=", "1024", ",", "\n", "convolutions", "=", "(", "(", "512", ",", "3", ")", ",", ")", "*", "20", ",", "dropout", "=", "0.1", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ")", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "num_attention_layers", "=", "None", "\n", "\n", "num_embeddings", "=", "len", "(", "dictionary", ")", "\n", "self", ".", "padding_idx", "=", "dictionary", ".", "pad", "(", ")", "\n", "self", ".", "embed_tokens", "=", "Embedding", "(", "num_embeddings", ",", "embed_dim", ",", "self", ".", "padding_idx", ")", "\n", "if", "embed_dict", ":", "\n", "            ", "self", ".", "embed_tokens", "=", "utils", ".", "load_embedding", "(", "embed_dict", ",", "self", ".", "dictionary", ",", "self", ".", "embed_tokens", ")", "\n", "\n", "", "self", ".", "embed_positions", "=", "PositionalEmbedding", "(", "\n", "max_positions", ",", "\n", "embed_dim", ",", "\n", "self", ".", "padding_idx", ",", "\n", ")", "\n", "\n", "convolutions", "=", "extend_conv_spec", "(", "convolutions", ")", "\n", "in_channels", "=", "convolutions", "[", "0", "]", "[", "0", "]", "\n", "self", ".", "fc1", "=", "Linear", "(", "embed_dim", ",", "in_channels", ",", "dropout", "=", "dropout", ")", "\n", "self", ".", "projections", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "convolutions", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "residuals", "=", "[", "]", "\n", "\n", "layer_in_channels", "=", "[", "in_channels", "]", "\n", "for", "_", ",", "(", "out_channels", ",", "kernel_size", ",", "residual", ")", "in", "enumerate", "(", "convolutions", ")", ":", "\n", "            ", "if", "residual", "==", "0", ":", "\n", "                ", "residual_dim", "=", "out_channels", "\n", "", "else", ":", "\n", "                ", "residual_dim", "=", "layer_in_channels", "[", "-", "residual", "]", "\n", "", "self", ".", "projections", ".", "append", "(", "Linear", "(", "residual_dim", ",", "out_channels", ")", "\n", "if", "residual_dim", "!=", "out_channels", "else", "None", ")", "\n", "if", "kernel_size", "%", "2", "==", "1", ":", "\n", "                ", "padding", "=", "kernel_size", "//", "2", "\n", "", "else", ":", "\n", "                ", "padding", "=", "0", "\n", "", "self", ".", "convolutions", ".", "append", "(", "\n", "ConvTBC", "(", "in_channels", ",", "out_channels", "*", "2", ",", "kernel_size", ",", "\n", "dropout", "=", "dropout", ",", "padding", "=", "padding", ")", "\n", ")", "\n", "self", ".", "residuals", ".", "append", "(", "residual", ")", "\n", "in_channels", "=", "out_channels", "\n", "layer_in_channels", ".", "append", "(", "out_channels", ")", "\n", "", "self", ".", "fc2", "=", "Linear", "(", "in_channels", ",", "embed_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv.FConvEncoder.forward": [[188, 268], ["torch.dropout", "torch.dropout", "torch.dropout", "fconv.FConvEncoder.fc1", "src_tokens.eq().t", "conv.transpose", "zip", "conv.transpose", "fconv.FConvEncoder.fc2", "fairseq.modules.GradMultiply.apply", "fconv.FConvEncoder.embed_tokens", "fconv.FConvEncoder.embed_positions", "encoder_padding_mask.t.t.any", "torch.dropout", "torch.dropout", "torch.dropout", "torch.glu", "torch.glu", "torch.glu", "residuals.append", "encoder_padding_mask.t.t.t", "conv.masked_fill", "math.sqrt", "src_tokens.eq", "conv.masked_fill", "conv", "torch.pad", "torch.pad", "torch.pad", "conv", "encoder_padding_mask.t.t.unsqueeze", "proj", "encoder_padding_mask.t.t.unsqueeze", "math.sqrt"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad"], ["", "def", "forward", "(", "self", ",", "src_tokens", ",", "src_lengths", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            src_tokens (LongTensor): tokens in the source language of shape\n                `(batch, src_len)`\n            src_lengths (LongTensor): lengths of each source sentence of shape\n                `(batch)`\n\n        Returns:\n            dict:\n                - **encoder_out** (tuple): a tuple with two elements, where the\n                  first element is the last encoder layer's output and the\n                  second element is the same quantity summed with the input\n                  embedding (used for attention). The shape of both tensors is\n                  `(batch, src_len, embed_dim)`.\n                - **encoder_padding_mask** (ByteTensor): the positions of\n                  padding elements of shape `(batch, src_len)`\n        \"\"\"", "\n", "# embed tokens and positions", "\n", "x", "=", "self", ".", "embed_tokens", "(", "src_tokens", ")", "+", "self", ".", "embed_positions", "(", "src_tokens", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "input_embedding", "=", "x", "\n", "\n", "# project to size of convolution", "\n", "x", "=", "self", ".", "fc1", "(", "x", ")", "\n", "\n", "# used to mask padding in input", "\n", "encoder_padding_mask", "=", "src_tokens", ".", "eq", "(", "self", ".", "padding_idx", ")", ".", "t", "(", ")", "# -> T x B", "\n", "if", "not", "encoder_padding_mask", ".", "any", "(", ")", ":", "\n", "            ", "encoder_padding_mask", "=", "None", "\n", "\n", "# B x T x C -> T x B x C", "\n", "", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "residuals", "=", "[", "x", "]", "\n", "# temporal convolutions", "\n", "for", "proj", ",", "conv", ",", "res_layer", "in", "zip", "(", "self", ".", "projections", ",", "self", ".", "convolutions", ",", "self", ".", "residuals", ")", ":", "\n", "            ", "if", "res_layer", ">", "0", ":", "\n", "                ", "residual", "=", "residuals", "[", "-", "res_layer", "]", "\n", "residual", "=", "residual", "if", "proj", "is", "None", "else", "proj", "(", "residual", ")", "\n", "", "else", ":", "\n", "                ", "residual", "=", "None", "\n", "\n", "", "if", "encoder_padding_mask", "is", "not", "None", ":", "\n", "                ", "x", "=", "x", ".", "masked_fill", "(", "encoder_padding_mask", ".", "unsqueeze", "(", "-", "1", ")", ",", "0", ")", "\n", "\n", "", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "if", "conv", ".", "kernel_size", "[", "0", "]", "%", "2", "==", "1", ":", "\n", "# padding is implicit in the conv", "\n", "                ", "x", "=", "conv", "(", "x", ")", "\n", "", "else", ":", "\n", "                ", "padding_l", "=", "(", "conv", ".", "kernel_size", "[", "0", "]", "-", "1", ")", "//", "2", "\n", "padding_r", "=", "conv", ".", "kernel_size", "[", "0", "]", "//", "2", "\n", "x", "=", "F", ".", "pad", "(", "x", ",", "(", "0", ",", "0", ",", "0", ",", "0", ",", "padding_l", ",", "padding_r", ")", ")", "\n", "x", "=", "conv", "(", "x", ")", "\n", "", "x", "=", "F", ".", "glu", "(", "x", ",", "dim", "=", "2", ")", "\n", "\n", "if", "residual", "is", "not", "None", ":", "\n", "                ", "x", "=", "(", "x", "+", "residual", ")", "*", "math", ".", "sqrt", "(", "0.5", ")", "\n", "", "residuals", ".", "append", "(", "x", ")", "\n", "\n", "# T x B x C -> B x T x C", "\n", "", "x", "=", "x", ".", "transpose", "(", "1", ",", "0", ")", "\n", "\n", "# project back to size of embedding", "\n", "x", "=", "self", ".", "fc2", "(", "x", ")", "\n", "\n", "if", "encoder_padding_mask", "is", "not", "None", ":", "\n", "            ", "encoder_padding_mask", "=", "encoder_padding_mask", ".", "t", "(", ")", "# -> B x T", "\n", "x", "=", "x", ".", "masked_fill", "(", "encoder_padding_mask", ".", "unsqueeze", "(", "-", "1", ")", ",", "0", ")", "\n", "\n", "# scale gradients (this only affects backward, not forward)", "\n", "", "x", "=", "GradMultiply", ".", "apply", "(", "x", ",", "1.0", "/", "(", "2.0", "*", "self", ".", "num_attention_layers", ")", ")", "\n", "\n", "# add output to input embedding for attention", "\n", "y", "=", "(", "x", "+", "input_embedding", ")", "*", "math", ".", "sqrt", "(", "0.5", ")", "\n", "\n", "return", "{", "\n", "'encoder_out'", ":", "(", "x", ",", "y", ")", ",", "\n", "'encoder_padding_mask'", ":", "encoder_padding_mask", ",", "# B x T", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv.FConvEncoder.reorder_encoder_out": [[270, 280], ["encoder_out[].index_select", "[].index_select", "[].index_select"], "methods", ["None"], ["", "def", "reorder_encoder_out", "(", "self", ",", "encoder_out", ",", "new_order", ")", ":", "\n", "        ", "if", "encoder_out", "[", "'encoder_out'", "]", "is", "not", "None", ":", "\n", "            ", "encoder_out", "[", "'encoder_out'", "]", "=", "(", "\n", "encoder_out", "[", "'encoder_out'", "]", "[", "0", "]", ".", "index_select", "(", "0", ",", "new_order", ")", ",", "\n", "encoder_out", "[", "'encoder_out'", "]", "[", "1", "]", ".", "index_select", "(", "0", ",", "new_order", ")", ",", "\n", ")", "\n", "", "if", "encoder_out", "[", "'encoder_padding_mask'", "]", "is", "not", "None", ":", "\n", "            ", "encoder_out", "[", "'encoder_padding_mask'", "]", "=", "encoder_out", "[", "'encoder_padding_mask'", "]", ".", "index_select", "(", "0", ",", "new_order", ")", "\n", "", "return", "encoder_out", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv.FConvEncoder.max_positions": [[281, 284], ["fconv.FConvEncoder.embed_positions.max_positions"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.max_positions"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum input length supported by the encoder.\"\"\"", "\n", "return", "self", ".", "embed_positions", ".", "max_positions", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv.AttentionLayer.__init__": [[287, 295], ["torch.Module.__init__", "fconv.Linear", "fconv.Linear"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear"], ["    ", "def", "__init__", "(", "self", ",", "conv_channels", ",", "embed_dim", ",", "bmm", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# projects from output of convolution to embedding dimension", "\n", "self", ".", "in_projection", "=", "Linear", "(", "conv_channels", ",", "embed_dim", ")", "\n", "# projects from embedding dimension to convolution size", "\n", "self", ".", "out_projection", "=", "Linear", "(", "embed_dim", ",", "conv_channels", ")", "\n", "\n", "self", ".", "bmm", "=", "bmm", "if", "bmm", "is", "not", "None", "else", "torch", ".", "bmm", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv.AttentionLayer.forward": [[296, 330], ["fconv.AttentionLayer.bmm", "x.float().masked_fill().type_as.float().masked_fill().type_as.size", "torch.softmax", "torch.softmax", "torch.softmax", "x.float().masked_fill().type_as.float().masked_fill().type_as.view", "fconv.AttentionLayer.bmm", "encoder_out[].size", "math.sqrt", "x.float().masked_fill().type_as.float().masked_fill().type_as.float().masked_fill().type_as", "x.float().masked_fill().type_as.float().masked_fill().type_as.view", "s.unsqueeze.unsqueeze.unsqueeze", "math.sqrt", "fconv.AttentionLayer.in_projection", "encoder_padding_mask.type_as().sum", "fconv.AttentionLayer.out_projection", "x.float().masked_fill().type_as.float().masked_fill().type_as.float().masked_fill", "math.sqrt", "s.unsqueeze.unsqueeze.rsqrt", "encoder_padding_mask.unsqueeze", "float", "encoder_padding_mask.type_as", "x.float().masked_fill().type_as.float().masked_fill().type_as.float"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "forward", "(", "self", ",", "x", ",", "target_embedding", ",", "encoder_out", ",", "encoder_padding_mask", ")", ":", "\n", "        ", "residual", "=", "x", "\n", "\n", "# attention", "\n", "x", "=", "(", "self", ".", "in_projection", "(", "x", ")", "+", "target_embedding", ")", "*", "math", ".", "sqrt", "(", "0.5", ")", "\n", "x", "=", "self", ".", "bmm", "(", "x", ",", "encoder_out", "[", "0", "]", ")", "\n", "\n", "# don't attend over padding", "\n", "if", "encoder_padding_mask", "is", "not", "None", ":", "\n", "            ", "x", "=", "x", ".", "float", "(", ")", ".", "masked_fill", "(", "\n", "encoder_padding_mask", ".", "unsqueeze", "(", "1", ")", ",", "\n", "float", "(", "'-inf'", ")", "\n", ")", ".", "type_as", "(", "x", ")", "# FP16 support: cast to float and back", "\n", "\n", "# softmax over last dim", "\n", "", "sz", "=", "x", ".", "size", "(", ")", "\n", "x", "=", "F", ".", "softmax", "(", "x", ".", "view", "(", "sz", "[", "0", "]", "*", "sz", "[", "1", "]", ",", "sz", "[", "2", "]", ")", ",", "dim", "=", "1", ")", "\n", "x", "=", "x", ".", "view", "(", "sz", ")", "\n", "attn_scores", "=", "x", "\n", "\n", "x", "=", "self", ".", "bmm", "(", "x", ",", "encoder_out", "[", "1", "]", ")", "\n", "\n", "# scale attention output (respecting potentially different lengths)", "\n", "s", "=", "encoder_out", "[", "1", "]", ".", "size", "(", "1", ")", "\n", "if", "encoder_padding_mask", "is", "None", ":", "\n", "            ", "x", "=", "x", "*", "(", "s", "*", "math", ".", "sqrt", "(", "1.0", "/", "s", ")", ")", "\n", "", "else", ":", "\n", "            ", "s", "=", "s", "-", "encoder_padding_mask", ".", "type_as", "(", "x", ")", ".", "sum", "(", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "# exclude padding", "\n", "s", "=", "s", ".", "unsqueeze", "(", "-", "1", ")", "\n", "x", "=", "x", "*", "(", "s", "*", "s", ".", "rsqrt", "(", ")", ")", "\n", "\n", "# project back", "\n", "", "x", "=", "(", "self", ".", "out_projection", "(", "x", ")", "+", "residual", ")", "*", "math", ".", "sqrt", "(", "0.5", ")", "\n", "return", "x", ",", "attn_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv.AttentionLayer.make_generation_fast_": [[331, 336], ["fconv.AttentionLayer.add_module", "fairseq.modules.BeamableMM"], "methods", ["None"], ["", "def", "make_generation_fast_", "(", "self", ",", "beamable_mm_beam_size", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Replace torch.bmm with BeamableMM.\"\"\"", "\n", "if", "beamable_mm_beam_size", "is", "not", "None", ":", "\n", "            ", "del", "self", ".", "bmm", "\n", "self", ".", "add_module", "(", "'bmm'", ",", "BeamableMM", "(", "beamable_mm_beam_size", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv.FConvDecoder.__init__": [[341, 414], ["fairseq.models.FairseqIncrementalDecoder.__init__", "fconv.FConvDecoder.register_buffer", "fconv.extend_conv_spec", "isinstance", "len", "dictionary.pad", "fconv.Embedding", "fconv.Linear", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "enumerate", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "ValueError", "fairseq.utils.load_embedding", "fconv.PositionalEmbedding", "fconv.FConvDecoder.projections.append", "fconv.FConvDecoder.convolutions.append", "fconv.FConvDecoder.attention.append", "fconv.FConvDecoder.residuals.append", "layer_in_channels.append", "fairseq.modules.AdaptiveSoftmax", "fconv.Linear", "len", "isinstance", "len", "len", "fconv.LinearizedConv1d", "torch.Linear", "torch.Linear", "torch.Linear", "fconv.Linear", "fconv.Linear", "fconv.AttentionLayer"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv.extend_conv_spec", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.Embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.load_embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.positional_embedding.PositionalEmbedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv.LinearizedConv1d", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear"], ["def", "__init__", "(", "\n", "self", ",", "dictionary", ",", "embed_dim", "=", "512", ",", "embed_dict", "=", "None", ",", "out_embed_dim", "=", "256", ",", "\n", "max_positions", "=", "1024", ",", "convolutions", "=", "(", "(", "512", ",", "3", ")", ",", ")", "*", "20", ",", "attention", "=", "True", ",", "\n", "dropout", "=", "0.1", ",", "share_embed", "=", "False", ",", "positional_embeddings", "=", "True", ",", "\n", "adaptive_softmax_cutoff", "=", "None", ",", "adaptive_softmax_dropout", "=", "0", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ")", "\n", "self", ".", "register_buffer", "(", "'version'", ",", "torch", ".", "Tensor", "(", "[", "2", "]", ")", ")", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "need_attn", "=", "True", "\n", "\n", "convolutions", "=", "extend_conv_spec", "(", "convolutions", ")", "\n", "in_channels", "=", "convolutions", "[", "0", "]", "[", "0", "]", "\n", "if", "isinstance", "(", "attention", ",", "bool", ")", ":", "\n", "# expand True into [True, True, ...] and do the same with False", "\n", "            ", "attention", "=", "[", "attention", "]", "*", "len", "(", "convolutions", ")", "\n", "", "if", "not", "isinstance", "(", "attention", ",", "list", ")", "or", "len", "(", "attention", ")", "!=", "len", "(", "convolutions", ")", ":", "\n", "            ", "raise", "ValueError", "(", "'Attention is expected to be a list of booleans of '", "\n", "'length equal to the number of layers.'", ")", "\n", "\n", "", "num_embeddings", "=", "len", "(", "dictionary", ")", "\n", "padding_idx", "=", "dictionary", ".", "pad", "(", ")", "\n", "self", ".", "embed_tokens", "=", "Embedding", "(", "num_embeddings", ",", "embed_dim", ",", "padding_idx", ")", "\n", "if", "embed_dict", ":", "\n", "            ", "self", ".", "embed_tokens", "=", "utils", ".", "load_embedding", "(", "embed_dict", ",", "self", ".", "dictionary", ",", "self", ".", "embed_tokens", ")", "\n", "\n", "", "self", ".", "embed_positions", "=", "PositionalEmbedding", "(", "\n", "max_positions", ",", "\n", "embed_dim", ",", "\n", "padding_idx", ",", "\n", ")", "if", "positional_embeddings", "else", "None", "\n", "\n", "self", ".", "fc1", "=", "Linear", "(", "embed_dim", ",", "in_channels", ",", "dropout", "=", "dropout", ")", "\n", "self", ".", "projections", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "convolutions", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "attention", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "residuals", "=", "[", "]", "\n", "\n", "layer_in_channels", "=", "[", "in_channels", "]", "\n", "for", "i", ",", "(", "out_channels", ",", "kernel_size", ",", "residual", ")", "in", "enumerate", "(", "convolutions", ")", ":", "\n", "            ", "if", "residual", "==", "0", ":", "\n", "                ", "residual_dim", "=", "out_channels", "\n", "", "else", ":", "\n", "                ", "residual_dim", "=", "layer_in_channels", "[", "-", "residual", "]", "\n", "", "self", ".", "projections", ".", "append", "(", "Linear", "(", "residual_dim", ",", "out_channels", ")", "\n", "if", "residual_dim", "!=", "out_channels", "else", "None", ")", "\n", "self", ".", "convolutions", ".", "append", "(", "\n", "LinearizedConv1d", "(", "in_channels", ",", "out_channels", "*", "2", ",", "kernel_size", ",", "\n", "padding", "=", "(", "kernel_size", "-", "1", ")", ",", "dropout", "=", "dropout", ")", "\n", ")", "\n", "self", ".", "attention", ".", "append", "(", "AttentionLayer", "(", "out_channels", ",", "embed_dim", ")", "\n", "if", "attention", "[", "i", "]", "else", "None", ")", "\n", "self", ".", "residuals", ".", "append", "(", "residual", ")", "\n", "in_channels", "=", "out_channels", "\n", "layer_in_channels", ".", "append", "(", "out_channels", ")", "\n", "\n", "", "self", ".", "adaptive_softmax", "=", "None", "\n", "self", ".", "fc2", "=", "self", ".", "fc3", "=", "None", "\n", "\n", "if", "adaptive_softmax_cutoff", "is", "not", "None", ":", "\n", "            ", "assert", "not", "share_embed", "\n", "self", ".", "adaptive_softmax", "=", "AdaptiveSoftmax", "(", "num_embeddings", ",", "in_channels", ",", "adaptive_softmax_cutoff", ",", "\n", "dropout", "=", "adaptive_softmax_dropout", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "fc2", "=", "Linear", "(", "in_channels", ",", "out_embed_dim", ")", "\n", "if", "share_embed", ":", "\n", "                ", "assert", "out_embed_dim", "==", "embed_dim", ",", "\"Shared embed weights implies same dimensions \"", "\" out_embed_dim={} vs embed_dim={}\"", ".", "format", "(", "out_embed_dim", ",", "embed_dim", ")", "\n", "self", ".", "fc3", "=", "nn", ".", "Linear", "(", "out_embed_dim", ",", "num_embeddings", ")", "\n", "self", ".", "fc3", ".", "weight", "=", "self", ".", "embed_tokens", ".", "weight", "\n", "", "else", ":", "\n", "                ", "self", ".", "fc3", "=", "Linear", "(", "out_embed_dim", ",", "num_embeddings", ",", "dropout", "=", "dropout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv.FConvDecoder.forward": [[415, 489], ["fconv.FConvDecoder._embed_tokens", "torch.dropout", "torch.dropout", "torch.dropout", "fconv.FConvDecoder.fc1", "fconv.FConvDecoder._transpose_if_training", "len", "zip", "fconv.FConvDecoder._transpose_if_training", "fconv.FConvDecoder._split_encoder_out", "fconv.FConvDecoder.embed_positions", "torch.dropout", "torch.dropout", "torch.dropout", "conv", "torch.glu", "torch.glu", "torch.glu", "residuals.append", "fconv.FConvDecoder.fc2", "torch.dropout", "torch.dropout", "torch.dropout", "fconv.FConvDecoder.fc3", "fconv.FConvDecoder._transpose_if_training", "attention", "fconv.FConvDecoder._transpose_if_training", "proj", "math.sqrt", "avg_attn_scores.add_"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv.FConvDecoder._embed_tokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv.FConvDecoder._transpose_if_training", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv.FConvDecoder._transpose_if_training", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv.FConvDecoder._split_encoder_out", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv.FConvDecoder._transpose_if_training", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv.FConvDecoder._transpose_if_training"], ["", "", "", "def", "forward", "(", "self", ",", "prev_output_tokens", ",", "encoder_out", "=", "None", ",", "incremental_state", "=", "None", ",", "**", "unused", ")", ":", "\n", "        ", "if", "encoder_out", "is", "not", "None", ":", "\n", "            ", "encoder_padding_mask", "=", "encoder_out", "[", "'encoder_padding_mask'", "]", "\n", "encoder_out", "=", "encoder_out", "[", "'encoder_out'", "]", "\n", "\n", "# split and transpose encoder outputs", "\n", "encoder_a", ",", "encoder_b", "=", "self", ".", "_split_encoder_out", "(", "encoder_out", ",", "incremental_state", ")", "\n", "\n", "", "if", "self", ".", "embed_positions", "is", "not", "None", ":", "\n", "            ", "pos_embed", "=", "self", ".", "embed_positions", "(", "prev_output_tokens", ",", "incremental_state", ")", "\n", "", "else", ":", "\n", "            ", "pos_embed", "=", "0", "\n", "\n", "", "if", "incremental_state", "is", "not", "None", ":", "\n", "            ", "prev_output_tokens", "=", "prev_output_tokens", "[", ":", ",", "-", "1", ":", "]", "\n", "", "x", "=", "self", ".", "_embed_tokens", "(", "prev_output_tokens", ",", "incremental_state", ")", "\n", "\n", "# embed tokens and combine with positional embeddings", "\n", "x", "+=", "pos_embed", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "target_embedding", "=", "x", "\n", "\n", "# project to size of convolution", "\n", "x", "=", "self", ".", "fc1", "(", "x", ")", "\n", "\n", "# B x T x C -> T x B x C", "\n", "x", "=", "self", ".", "_transpose_if_training", "(", "x", ",", "incremental_state", ")", "\n", "\n", "# temporal convolutions", "\n", "avg_attn_scores", "=", "None", "\n", "num_attn_layers", "=", "len", "(", "self", ".", "attention", ")", "\n", "residuals", "=", "[", "x", "]", "\n", "for", "proj", ",", "conv", ",", "attention", ",", "res_layer", "in", "zip", "(", "self", ".", "projections", ",", "self", ".", "convolutions", ",", "self", ".", "attention", ",", "\n", "self", ".", "residuals", ")", ":", "\n", "            ", "if", "res_layer", ">", "0", ":", "\n", "                ", "residual", "=", "residuals", "[", "-", "res_layer", "]", "\n", "residual", "=", "residual", "if", "proj", "is", "None", "else", "proj", "(", "residual", ")", "\n", "", "else", ":", "\n", "                ", "residual", "=", "None", "\n", "\n", "", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "conv", "(", "x", ",", "incremental_state", ")", "\n", "x", "=", "F", ".", "glu", "(", "x", ",", "dim", "=", "2", ")", "\n", "\n", "# attention", "\n", "if", "attention", "is", "not", "None", ":", "\n", "                ", "x", "=", "self", ".", "_transpose_if_training", "(", "x", ",", "incremental_state", ")", "\n", "\n", "x", ",", "attn_scores", "=", "attention", "(", "x", ",", "target_embedding", ",", "(", "encoder_a", ",", "encoder_b", ")", ",", "encoder_padding_mask", ")", "\n", "\n", "if", "not", "self", ".", "training", "and", "self", ".", "need_attn", ":", "\n", "                    ", "attn_scores", "=", "attn_scores", "/", "num_attn_layers", "\n", "if", "avg_attn_scores", "is", "None", ":", "\n", "                        ", "avg_attn_scores", "=", "attn_scores", "\n", "", "else", ":", "\n", "                        ", "avg_attn_scores", ".", "add_", "(", "attn_scores", ")", "\n", "\n", "", "", "x", "=", "self", ".", "_transpose_if_training", "(", "x", ",", "incremental_state", ")", "\n", "\n", "# residual", "\n", "", "if", "residual", "is", "not", "None", ":", "\n", "                ", "x", "=", "(", "x", "+", "residual", ")", "*", "math", ".", "sqrt", "(", "0.5", ")", "\n", "", "residuals", ".", "append", "(", "x", ")", "\n", "\n", "# T x B x C -> B x T x C", "\n", "", "x", "=", "self", ".", "_transpose_if_training", "(", "x", ",", "incremental_state", ")", "\n", "\n", "# project back to size of vocabulary if not using adaptive softmax", "\n", "if", "self", ".", "fc2", "is", "not", "None", "and", "self", ".", "fc3", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "fc2", "(", "x", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "self", ".", "fc3", "(", "x", ")", "\n", "\n", "", "return", "x", ",", "avg_attn_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv.FConvDecoder.reorder_incremental_state": [[490, 496], ["super().reorder_incremental_state", "fairseq.utils.get_incremental_state", "tuple", "fairseq.utils.set_incremental_state", "eo.index_select"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.linearized_convolution.LinearizedConvolution.reorder_incremental_state", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.get_incremental_state", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.set_incremental_state"], ["", "def", "reorder_incremental_state", "(", "self", ",", "incremental_state", ",", "new_order", ")", ":", "\n", "        ", "super", "(", ")", ".", "reorder_incremental_state", "(", "incremental_state", ",", "new_order", ")", "\n", "encoder_out", "=", "utils", ".", "get_incremental_state", "(", "self", ",", "incremental_state", ",", "'encoder_out'", ")", "\n", "if", "encoder_out", "is", "not", "None", ":", "\n", "            ", "encoder_out", "=", "tuple", "(", "eo", ".", "index_select", "(", "0", ",", "new_order", ")", "for", "eo", "in", "encoder_out", ")", "\n", "utils", ".", "set_incremental_state", "(", "self", ",", "incremental_state", ",", "'encoder_out'", ",", "encoder_out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv.FConvDecoder.max_positions": [[497, 500], ["fconv.FConvDecoder.embed_positions.max_positions", "float"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.max_positions"], ["", "", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum output length supported by the decoder.\"\"\"", "\n", "return", "self", ".", "embed_positions", ".", "max_positions", "(", ")", "if", "self", ".", "embed_positions", "is", "not", "None", "else", "float", "(", "'inf'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv.FConvDecoder.upgrade_state_dict": [[501, 510], ["fairseq.utils.item", "enumerate", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.utils.remove_weight_norm", "torch.utils.remove_weight_norm", "torch.utils.remove_weight_norm", "torch.utils.weight_norm", "torch.utils.weight_norm", "torch.utils.weight_norm", "state_dict.get", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item"], ["", "def", "upgrade_state_dict", "(", "self", ",", "state_dict", ")", ":", "\n", "        ", "if", "utils", ".", "item", "(", "state_dict", ".", "get", "(", "'decoder.version'", ",", "torch", ".", "Tensor", "(", "[", "1", "]", ")", ")", "[", "0", "]", ")", "<", "2", ":", "\n", "# old models use incorrect weight norm dimension", "\n", "            ", "for", "i", ",", "conv", "in", "enumerate", "(", "self", ".", "convolutions", ")", ":", "\n", "# reconfigure weight norm", "\n", "                ", "nn", ".", "utils", ".", "remove_weight_norm", "(", "conv", ")", "\n", "self", ".", "convolutions", "[", "i", "]", "=", "nn", ".", "utils", ".", "weight_norm", "(", "conv", ",", "dim", "=", "0", ")", "\n", "", "state_dict", "[", "'decoder.version'", "]", "=", "torch", ".", "Tensor", "(", "[", "1", "]", ")", "\n", "", "return", "state_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv.FConvDecoder.make_generation_fast_": [[511, 513], ["None"], "methods", ["None"], ["", "def", "make_generation_fast_", "(", "self", ",", "need_attn", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "need_attn", "=", "need_attn", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv.FConvDecoder._embed_tokens": [[514, 519], ["fconv.FConvDecoder.embed_tokens"], "methods", ["None"], ["", "def", "_embed_tokens", "(", "self", ",", "tokens", ",", "incremental_state", ")", ":", "\n", "        ", "if", "incremental_state", "is", "not", "None", ":", "\n", "# keep only the last token for incremental forward pass", "\n", "            ", "tokens", "=", "tokens", "[", ":", ",", "-", "1", ":", "]", "\n", "", "return", "self", ".", "embed_tokens", "(", "tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv.FConvDecoder._split_encoder_out": [[520, 537], ["fairseq.utils.get_incremental_state", "encoder_a.transpose().contiguous.transpose().contiguous.transpose().contiguous", "fairseq.utils.set_incremental_state", "encoder_a.transpose().contiguous.transpose().contiguous.transpose"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.get_incremental_state", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.set_incremental_state"], ["", "def", "_split_encoder_out", "(", "self", ",", "encoder_out", ",", "incremental_state", ")", ":", "\n", "        ", "\"\"\"Split and transpose encoder outputs.\n\n        This is cached when doing incremental inference.\n        \"\"\"", "\n", "cached_result", "=", "utils", ".", "get_incremental_state", "(", "self", ",", "incremental_state", ",", "'encoder_out'", ")", "\n", "if", "cached_result", "is", "not", "None", ":", "\n", "            ", "return", "cached_result", "\n", "\n", "# transpose only once to speed up attention layers", "\n", "", "encoder_a", ",", "encoder_b", "=", "encoder_out", "\n", "encoder_a", "=", "encoder_a", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", "\n", "result", "=", "(", "encoder_a", ",", "encoder_b", ")", "\n", "\n", "if", "incremental_state", "is", "not", "None", ":", "\n", "            ", "utils", ".", "set_incremental_state", "(", "self", ",", "incremental_state", ",", "'encoder_out'", ",", "result", ")", "\n", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv.FConvDecoder._transpose_if_training": [[538, 542], ["x.transpose.transpose.transpose"], "methods", ["None"], ["", "def", "_transpose_if_training", "(", "self", ",", "x", ",", "incremental_state", ")", ":", "\n", "        ", "if", "incremental_state", "is", "None", ":", "\n", "            ", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv.extend_conv_spec": [[544, 559], ["tuple", "len", "extended.append", "len", "extended.append", "Exception", "str"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "", "def", "extend_conv_spec", "(", "convolutions", ")", ":", "\n", "    ", "\"\"\"\n    Extends convolutional spec that is a list of tuples of 2 or 3 parameters\n    (kernel size, dim size and optionally how many layers behind to look for residual)\n    to default the residual propagation param if it is not specified\n    \"\"\"", "\n", "extended", "=", "[", "]", "\n", "for", "spec", "in", "convolutions", ":", "\n", "        ", "if", "len", "(", "spec", ")", "==", "3", ":", "\n", "            ", "extended", ".", "append", "(", "spec", ")", "\n", "", "elif", "len", "(", "spec", ")", "==", "2", ":", "\n", "            ", "extended", ".", "append", "(", "spec", "+", "(", "1", ",", ")", ")", "\n", "", "else", ":", "\n", "            ", "raise", "Exception", "(", "'invalid number of parameters in convolution spec '", "+", "str", "(", "spec", ")", "+", "'. expected 2 or 3'", ")", "\n", "", "", "return", "tuple", "(", "extended", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv.Embedding": [[561, 566], ["torch.Embedding", "torch.init.normal_", "torch.init.constant_"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.Embedding"], ["", "def", "Embedding", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", ")", ":", "\n", "    ", "m", "=", "nn", ".", "Embedding", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", "=", "padding_idx", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "m", ".", "weight", ",", "0", ",", "0.1", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "weight", "[", "padding_idx", "]", ",", "0", ")", "\n", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv.PositionalEmbedding": [[568, 573], ["fairseq.modules.LearnedPositionalEmbedding", "torch.init.normal_", "torch.init.constant_"], "function", ["None"], ["", "def", "PositionalEmbedding", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", ")", ":", "\n", "    ", "m", "=", "LearnedPositionalEmbedding", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "m", ".", "weight", ",", "0", ",", "0.1", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "weight", "[", "padding_idx", "]", ",", "0", ")", "\n", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv.Linear": [[575, 581], ["torch.Linear", "torch.init.normal_", "torch.init.constant_", "torch.utils.weight_norm", "math.sqrt"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear"], ["", "def", "Linear", "(", "in_features", ",", "out_features", ",", "dropout", "=", "0", ")", ":", "\n", "    ", "\"\"\"Weight-normalized Linear layer (input: N x T x C)\"\"\"", "\n", "m", "=", "nn", ".", "Linear", "(", "in_features", ",", "out_features", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "m", ".", "weight", ",", "mean", "=", "0", ",", "std", "=", "math", ".", "sqrt", "(", "(", "1", "-", "dropout", ")", "/", "in_features", ")", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0", ")", "\n", "return", "nn", ".", "utils", ".", "weight_norm", "(", "m", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv.LinearizedConv1d": [[583, 590], ["fairseq.modules.LinearizedConvolution", "math.sqrt", "torch.init.normal_", "torch.init.constant_", "torch.utils.weight_norm"], "function", ["None"], ["", "def", "LinearizedConv1d", "(", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "dropout", "=", "0", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Weight-normalized Conv1d layer optimized for decoding\"\"\"", "\n", "m", "=", "LinearizedConvolution", "(", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "**", "kwargs", ")", "\n", "std", "=", "math", ".", "sqrt", "(", "(", "4", "*", "(", "1.0", "-", "dropout", ")", ")", "/", "(", "m", ".", "kernel_size", "[", "0", "]", "*", "in_channels", ")", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "m", ".", "weight", ",", "mean", "=", "0", ",", "std", "=", "std", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0", ")", "\n", "return", "nn", ".", "utils", ".", "weight_norm", "(", "m", ",", "dim", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv.ConvTBC": [[592, 600], ["fconv.ConvTBC", "math.sqrt", "torch.init.normal_", "torch.init.constant_", "torch.utils.weight_norm"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv.ConvTBC"], ["", "def", "ConvTBC", "(", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "dropout", "=", "0", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"Weight-normalized Conv1d layer\"\"\"", "\n", "from", "fairseq", ".", "modules", "import", "ConvTBC", "\n", "m", "=", "ConvTBC", "(", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "**", "kwargs", ")", "\n", "std", "=", "math", ".", "sqrt", "(", "(", "4", "*", "(", "1.0", "-", "dropout", ")", ")", "/", "(", "m", ".", "kernel_size", "[", "0", "]", "*", "in_channels", ")", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "m", ".", "weight", ",", "mean", "=", "0", ",", "std", "=", "std", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0", ")", "\n", "return", "nn", ".", "utils", ".", "weight_norm", "(", "m", ",", "dim", "=", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv.base_architecture": [[602, 614], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture"], ["", "@", "register_model_architecture", "(", "'fconv'", ",", "'fconv'", ")", "\n", "def", "base_architecture", "(", "args", ")", ":", "\n", "    ", "args", ".", "dropout", "=", "getattr", "(", "args", ",", "'dropout'", ",", "0.1", ")", "\n", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "512", ")", "\n", "args", ".", "encoder_embed_path", "=", "getattr", "(", "args", ",", "'encoder_embed_path'", ",", "None", ")", "\n", "args", ".", "encoder_layers", "=", "getattr", "(", "args", ",", "'encoder_layers'", ",", "'[(512, 3)] * 20'", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "512", ")", "\n", "args", ".", "decoder_embed_path", "=", "getattr", "(", "args", ",", "'decoder_embed_path'", ",", "None", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "'[(512, 3)] * 20'", ")", "\n", "args", ".", "decoder_out_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_out_embed_dim'", ",", "256", ")", "\n", "args", ".", "decoder_attention", "=", "getattr", "(", "args", ",", "'decoder_attention'", ",", "'True'", ")", "\n", "args", ".", "share_input_output_embed", "=", "getattr", "(", "args", ",", "'share_input_output_embed'", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv.fconv_iwslt_de_en": [[616, 624], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "fconv.base_architecture"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.base_architecture"], ["", "@", "register_model_architecture", "(", "'fconv'", ",", "'fconv_iwslt_de_en'", ")", "\n", "def", "fconv_iwslt_de_en", "(", "args", ")", ":", "\n", "    ", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "256", ")", "\n", "args", ".", "encoder_layers", "=", "getattr", "(", "args", ",", "'encoder_layers'", ",", "'[(256, 3)] * 4'", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "256", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "'[(256, 3)] * 3'", ")", "\n", "args", ".", "decoder_out_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_out_embed_dim'", ",", "256", ")", "\n", "base_architecture", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv.fconv_wmt_en_ro": [[626, 630], ["fairseq.models.register_model_architecture", "getattr", "fconv.base_architecture"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.base_architecture"], ["", "@", "register_model_architecture", "(", "'fconv'", ",", "'fconv_wmt_en_ro'", ")", "\n", "def", "fconv_wmt_en_ro", "(", "args", ")", ":", "\n", "    ", "args", ".", "decoder_out_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_out_embed_dim'", ",", "512", ")", "\n", "base_architecture", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv.fconv_wmt_en_de": [[632, 644], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "fconv.base_architecture"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.base_architecture"], ["", "@", "register_model_architecture", "(", "'fconv'", ",", "'fconv_wmt_en_de'", ")", "\n", "def", "fconv_wmt_en_de", "(", "args", ")", ":", "\n", "    ", "convs", "=", "'[(512, 3)] * 9'", "# first 9 layers have 512 units", "\n", "convs", "+=", "' + [(1024, 3)] * 4'", "# next 4 layers have 1024 units", "\n", "convs", "+=", "' + [(2048, 1)] * 2'", "# final 2 layers use 1x1 convolutions", "\n", "\n", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "768", ")", "\n", "args", ".", "encoder_layers", "=", "getattr", "(", "args", ",", "'encoder_layers'", ",", "convs", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "768", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "convs", ")", "\n", "args", ".", "decoder_out_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_out_embed_dim'", ",", "512", ")", "\n", "base_architecture", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fconv.fconv_wmt_en_fr": [[646, 660], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "fconv.base_architecture"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.base_architecture"], ["", "@", "register_model_architecture", "(", "'fconv'", ",", "'fconv_wmt_en_fr'", ")", "\n", "def", "fconv_wmt_en_fr", "(", "args", ")", ":", "\n", "    ", "convs", "=", "'[(512, 3)] * 6'", "# first 6 layers have 512 units", "\n", "convs", "+=", "' + [(768, 3)] * 4'", "# next 4 layers have 768 units", "\n", "convs", "+=", "' + [(1024, 3)] * 3'", "# next 3 layers have 1024 units", "\n", "convs", "+=", "' + [(2048, 1)] * 1'", "# next 1 layer uses 1x1 convolutions", "\n", "convs", "+=", "' + [(4096, 1)] * 1'", "# final 1 layer uses 1x1 convolutions", "\n", "\n", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "768", ")", "\n", "args", ".", "encoder_layers", "=", "getattr", "(", "args", ",", "'encoder_layers'", ",", "convs", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "768", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "convs", ")", "\n", "args", ".", "decoder_out_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_out_embed_dim'", ",", "512", ")", "\n", "base_architecture", "(", "args", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.masked_lm.MaskedLMModel.__init__": [[31, 41], ["fairseq.models.BaseFairseqModel.__init__", "getattr", "masked_lm.MaskedLMModel.apply"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["\n", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add task-specific arguments to the parser.\"\"\"", "\n", "parser", ".", "add_argument", "(", "'data'", ",", "help", "=", "'colon separated path to data directories list, \\\n                            will be iterated upon during epochs in round-robin manner'", ")", "\n", "parser", ".", "add_argument", "(", "'--sample-break-mode'", ",", "default", "=", "'complete'", ",", "\n", "choices", "=", "[", "'none'", ",", "'complete'", ",", "'complete_doc'", ",", "'eos'", "]", ",", "\n", "help", "=", "'If omitted or \"none\", fills each sample with tokens-per-sample '", "\n", "'tokens. If set to \"complete\", splits samples only at the end '", "\n", "'of sentence, but may include multiple sentences per sample. '", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.masked_lm.MaskedLMModel.add_args": [[42, 101], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "fairseq.utils.get_available_activation_fns", "fairseq.utils.get_available_activation_fns"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.get_available_activation_fns", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.get_available_activation_fns"], ["'\"complete_doc\" is similar but respects doc boundaries. '", "\n", "'If set to \"eos\", includes only one sentence per sample.'", ")", "\n", "parser", ".", "add_argument", "(", "'--tokens-per-sample'", ",", "default", "=", "512", ",", "type", "=", "int", ",", "\n", "help", "=", "'max number of total tokens over all segments '", "\n", "'per sample for BERT dataset'", ")", "\n", "parser", ".", "add_argument", "(", "'--mask-prob'", ",", "default", "=", "0.15", ",", "type", "=", "float", ",", "\n", "help", "=", "'probability of replacing a token with mask'", ")", "\n", "parser", ".", "add_argument", "(", "'--leave-unmasked-prob'", ",", "default", "=", "0.1", ",", "type", "=", "float", ",", "\n", "help", "=", "'probability that a masked token is unmasked'", ")", "\n", "parser", ".", "add_argument", "(", "'--random-token-prob'", ",", "default", "=", "0.1", ",", "type", "=", "float", ",", "\n", "help", "=", "'probability of replacing a token with a random token'", ")", "\n", "parser", ".", "add_argument", "(", "'--freq-weighted-replacement'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'sample random replacement words based on word frequencies'", ")", "\n", "parser", ".", "add_argument", "(", "'--mask-whole-words'", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'mask whole words; you may also want to set --bpe'", ")", "\n", "\n", "", "def", "__init__", "(", "self", ",", "args", ",", "dictionary", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ")", "\n", "self", ".", "dictionary", "=", "dictionary", "\n", "self", ".", "seed", "=", "args", ".", "seed", "\n", "\n", "# add mask token", "\n", "self", ".", "mask_idx", "=", "dictionary", ".", "add_symbol", "(", "'<mask>'", ")", "\n", "\n", "", "@", "classmethod", "\n", "def", "setup_task", "(", "cls", ",", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "paths", "=", "args", ".", "data", ".", "split", "(", "':'", ")", "\n", "assert", "len", "(", "paths", ")", ">", "0", "\n", "dictionary", "=", "Dictionary", ".", "load", "(", "os", ".", "path", ".", "join", "(", "paths", "[", "0", "]", ",", "'dict.txt'", ")", ")", "\n", "print", "(", "'| dictionary: {} types'", ".", "format", "(", "len", "(", "dictionary", ")", ")", ")", "\n", "return", "cls", "(", "args", ",", "dictionary", ")", "\n", "\n", "", "def", "load_dataset", "(", "self", ",", "split", ",", "epoch", "=", "0", ",", "combine", "=", "False", ")", ":", "\n", "        ", "\"\"\"Load a given dataset split.\n\n        Args:\n            split (str): name of the split (e.g., train, valid, test)\n        \"\"\"", "\n", "paths", "=", "self", ".", "args", ".", "data", ".", "split", "(", "':'", ")", "\n", "assert", "len", "(", "paths", ")", ">", "0", "\n", "data_path", "=", "paths", "[", "epoch", "%", "len", "(", "paths", ")", "]", "\n", "split_path", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "split", ")", "\n", "\n", "dataset", "=", "data_utils", ".", "load_indexed_dataset", "(", "\n", "split_path", ",", "\n", "self", ".", "source_dictionary", ",", "\n", "self", ".", "args", ".", "dataset_impl", ",", "\n", "combine", "=", "combine", ",", "\n", ")", "\n", "if", "dataset", "is", "None", ":", "\n", "            ", "raise", "FileNotFoundError", "(", "'Dataset not found: {} ({})'", ".", "format", "(", "split", ",", "split_path", ")", ")", "\n", "\n", "# create continuous blocks of tokens", "\n", "", "dataset", "=", "TokenBlockDataset", "(", "\n", "dataset", ",", "\n", "dataset", ".", "sizes", ",", "\n", "self", ".", "args", ".", "tokens_per_sample", "-", "1", ",", "# one less for <s>", "\n", "pad", "=", "self", ".", "source_dictionary", ".", "pad", "(", ")", ",", "\n", "eos", "=", "self", ".", "source_dictionary", ".", "eos", "(", ")", ",", "\n", "break_mode", "=", "self", ".", "args", ".", "sample_break_mode", ",", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.masked_lm.MaskedLMModel.forward": [[102, 104], ["masked_lm.MaskedLMModel.encoder"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.FairseqMultiModel.encoder"], [")", "\n", "print", "(", "'| loaded {} batches from: {}'", ".", "format", "(", "len", "(", "dataset", ")", ",", "split_path", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.masked_lm.MaskedLMModel.max_positions": [[105, 107], ["None"], "methods", ["None"], ["# prepend beginning-of-sentence token (<s>, equiv. to [CLS] in BERT)", "\n", "dataset", "=", "PrependTokenDataset", "(", "dataset", ",", "self", ".", "source_dictionary", ".", "bos", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.masked_lm.MaskedLMModel.build_model": [[108, 122], ["masked_lm.base_architecture", "print", "masked_lm.MaskedLMEncoder", "cls", "hasattr"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.base_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.masked_lm_dictionary.BertDictionary.cls"], ["# create masked input and targets", "\n", "if", "self", ".", "args", ".", "mask_whole_words", ":", "\n", "            ", "bpe", "=", "encoders", ".", "build_bpe", "(", "self", ".", "args", ")", "\n", "if", "bpe", "is", "not", "None", ":", "\n", "\n", "                ", "def", "is_beginning_of_word", "(", "i", ")", ":", "\n", "                    ", "if", "i", "<", "self", ".", "source_dictionary", ".", "nspecial", ":", "\n", "# special elements are always considered beginnings", "\n", "                        ", "return", "True", "\n", "", "tok", "=", "self", ".", "source_dictionary", "[", "i", "]", "\n", "if", "tok", ".", "startswith", "(", "'madeupword'", ")", ":", "\n", "                        ", "return", "True", "\n", "", "try", ":", "\n", "                        ", "return", "bpe", ".", "is_beginning_of_word", "(", "tok", ")", "\n", "", "except", "ValueError", ":", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.masked_lm.MaskedLMEncoder.__init__": [[129, 191], ["fairseq.models.FairseqEncoder.__init__", "dictionary.pad", "dictionary.__len__", "fairseq.modules.TransformerSentenceEncoder", "torch.Linear", "torch.Linear", "torch.Linear", "fairseq.utils.get_activation_fn", "torch.Linear", "torch.Linear", "torch.Linear", "fairseq.utils.get_activation_fn", "fairseq.modules.LayerNorm", "getattr", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.AMRDataset.__len__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.get_activation_fn", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.get_activation_fn", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.layer_norm.LayerNorm", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear"], ["            ", "mask_whole_words", "=", "None", "\n", "\n", "", "src_dataset", ",", "tgt_dataset", "=", "MaskTokensDataset", ".", "apply_mask", "(", "\n", "dataset", ",", "\n", "self", ".", "source_dictionary", ",", "\n", "pad_idx", "=", "self", ".", "source_dictionary", ".", "pad", "(", ")", ",", "\n", "mask_idx", "=", "self", ".", "mask_idx", ",", "\n", "seed", "=", "self", ".", "args", ".", "seed", ",", "\n", "mask_prob", "=", "self", ".", "args", ".", "mask_prob", ",", "\n", "leave_unmasked_prob", "=", "self", ".", "args", ".", "leave_unmasked_prob", ",", "\n", "random_token_prob", "=", "self", ".", "args", ".", "random_token_prob", ",", "\n", "freq_weighted_replacement", "=", "self", ".", "args", ".", "freq_weighted_replacement", ",", "\n", "mask_whole_words", "=", "mask_whole_words", ",", "\n", ")", "\n", "\n", "with", "data_utils", ".", "numpy_seed", "(", "self", ".", "args", ".", "seed", "+", "epoch", ")", ":", "\n", "            ", "shuffle", "=", "np", ".", "random", ".", "permutation", "(", "len", "(", "src_dataset", ")", ")", "\n", "\n", "", "self", ".", "datasets", "[", "split", "]", "=", "SortDataset", "(", "\n", "NestedDictionaryDataset", "(", "\n", "{", "\n", "'id'", ":", "IdDataset", "(", ")", ",", "\n", "'net_input'", ":", "{", "\n", "'src_tokens'", ":", "PadDataset", "(", "\n", "src_dataset", ",", "\n", "pad_idx", "=", "self", ".", "source_dictionary", ".", "pad", "(", ")", ",", "\n", "left_pad", "=", "False", ",", "\n", ")", ",", "\n", "'src_lengths'", ":", "NumelDataset", "(", "src_dataset", ",", "reduce", "=", "False", ")", ",", "\n", "}", ",", "\n", "'target'", ":", "PadDataset", "(", "\n", "tgt_dataset", ",", "\n", "pad_idx", "=", "self", ".", "source_dictionary", ".", "pad", "(", ")", ",", "\n", "left_pad", "=", "False", ",", "\n", ")", ",", "\n", "'nsentences'", ":", "NumSamplesDataset", "(", ")", ",", "\n", "'ntokens'", ":", "NumelDataset", "(", "src_dataset", ",", "reduce", "=", "True", ")", ",", "\n", "}", ",", "\n", "sizes", "=", "[", "src_dataset", ".", "sizes", "]", ",", "\n", ")", ",", "\n", "sort_order", "=", "[", "\n", "shuffle", ",", "\n", "src_dataset", ".", "sizes", ",", "\n", "]", ",", "\n", ")", "\n", "\n", "", "def", "build_dataset_for_inference", "(", "self", ",", "src_tokens", ",", "src_lengths", ",", "sort", "=", "True", ")", ":", "\n", "        ", "src_dataset", "=", "PadDataset", "(", "\n", "TokenBlockDataset", "(", "\n", "src_tokens", ",", "\n", "src_lengths", ",", "\n", "self", ".", "args", ".", "tokens_per_sample", "-", "1", ",", "# one less for <s>", "\n", "pad", "=", "self", ".", "source_dictionary", ".", "pad", "(", ")", ",", "\n", "eos", "=", "self", ".", "source_dictionary", ".", "eos", "(", ")", ",", "\n", "break_mode", "=", "'eos'", ",", "\n", ")", ",", "\n", "pad_idx", "=", "self", ".", "source_dictionary", ".", "pad", "(", ")", ",", "\n", "left_pad", "=", "False", ",", "\n", ")", "\n", "src_dataset", "=", "PrependTokenDataset", "(", "src_dataset", ",", "self", ".", "source_dictionary", ".", "bos", "(", ")", ")", "\n", "src_dataset", "=", "NestedDictionaryDataset", "(", "\n", "{", "\n", "'id'", ":", "IdDataset", "(", ")", ",", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.masked_lm.MaskedLMEncoder.forward": [[193, 244], ["masked_lm.MaskedLMEncoder.sentence_encoder", "inner_states[].transpose", "masked_lm.MaskedLMEncoder.layer_norm", "masked_lm.MaskedLMEncoder.pooler_activation", "masked_lm.MaskedLMEncoder.activation_fn", "masked_lm.MaskedLMEncoder.masked_lm_pooler", "hasattr", "torch.linear", "torch.linear", "torch.linear", "masked_lm.MaskedLMEncoder.sentence_projection_layer", "masked_lm.MaskedLMEncoder.lm_head_transform_weight", "masked_lm.MaskedLMEncoder.embed_out"], "methods", ["None"], ["'src_tokens'", ":", "src_dataset", ",", "\n", "'src_lengths'", ":", "NumelDataset", "(", "src_dataset", ",", "reduce", "=", "False", ")", ",", "\n", "}", ",", "\n", "}", ",", "\n", "sizes", "=", "src_lengths", ",", "\n", ")", "\n", "if", "sort", ":", "\n", "            ", "src_dataset", "=", "SortDataset", "(", "src_dataset", ",", "sort_order", "=", "[", "src_lengths", "]", ")", "\n", "", "return", "src_dataset", "\n", "\n", "", "@", "property", "\n", "def", "source_dictionary", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "dictionary", "\n", "\n", "", "@", "property", "\n", "def", "target_dictionary", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "dictionary", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.masked_lm.MaskedLMEncoder.max_positions": [[246, 249], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.masked_lm.MaskedLMEncoder.upgrade_state_dict_named": [[250, 267], ["isinstance", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "list", "state_dict.keys"], "methods", ["None"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.masked_lm.base_architecture": [[269, 295], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.masked_lm.bert_base_architecture": [[297, 323], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "masked_lm.base_architecture"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.base_architecture"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.masked_lm.bert_large_architecture": [[325, 332], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "masked_lm.bert_base_architecture"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.masked_lm.bert_base_architecture"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.masked_lm.xlm_architecture": [[334, 358], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "masked_lm.base_architecture"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.base_architecture"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.BaseFairseqModel.__init__": [[23, 26], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_is_generation_fast", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.BaseFairseqModel.add_args": [[27, 31], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add model-specific arguments to the parser.\"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.BaseFairseqModel.build_model": [[32, 36], ["NotImplementedError"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "build_model", "(", "cls", ",", "args", ",", "task", ")", ":", "\n", "        ", "\"\"\"Build a new model instance.\"\"\"", "\n", "raise", "NotImplementedError", "(", "'Model must implement the build_model method'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.BaseFairseqModel.get_targets": [[37, 40], ["None"], "methods", ["None"], ["", "def", "get_targets", "(", "self", ",", "sample", ",", "net_output", ")", ":", "\n", "        ", "\"\"\"Get targets from either the sample or the net's output.\"\"\"", "\n", "return", "sample", "[", "'target'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.BaseFairseqModel.get_normalized_probs": [[41, 52], ["hasattr", "fairseq_model.BaseFairseqModel.decoder.get_normalized_probs", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "net_output.float", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.softmax", "torch.softmax", "torch.softmax"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_decoder.FairseqDecoder.get_normalized_probs", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.log_softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.log_softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.log_softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax"], ["", "def", "get_normalized_probs", "(", "self", ",", "net_output", ",", "log_probs", ",", "sample", "=", "None", ")", ":", "\n", "        ", "\"\"\"Get normalized probabilities (or log probs) from a net's output.\"\"\"", "\n", "if", "hasattr", "(", "self", ",", "'decoder'", ")", ":", "\n", "            ", "return", "self", ".", "decoder", ".", "get_normalized_probs", "(", "net_output", ",", "log_probs", ",", "sample", ")", "\n", "", "elif", "torch", ".", "is_tensor", "(", "net_output", ")", ":", "\n", "            ", "logits", "=", "net_output", ".", "float", "(", ")", "\n", "if", "log_probs", ":", "\n", "                ", "return", "F", ".", "log_softmax", "(", "logits", ",", "dim", "=", "-", "1", ")", "\n", "", "else", ":", "\n", "                ", "return", "F", ".", "softmax", "(", "logits", ",", "dim", "=", "-", "1", ")", "\n", "", "", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.BaseFairseqModel.extract_features": [[53, 56], ["fairseq_model.BaseFairseqModel."], "methods", ["None"], ["", "def", "extract_features", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Similar to *forward* but only return features.\"\"\"", "\n", "return", "self", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.BaseFairseqModel.max_positions": [[57, 60], ["None"], "methods", ["None"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum length supported by the model.\"\"\"", "\n", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.BaseFairseqModel.load_state_dict": [[61, 70], ["fairseq_model.BaseFairseqModel.upgrade_state_dict", "super().load_state_dict"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_decoder.FairseqDecoder.upgrade_state_dict", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.load_state_dict"], ["", "def", "load_state_dict", "(", "self", ",", "state_dict", ",", "strict", "=", "True", ")", ":", "\n", "        ", "\"\"\"Copies parameters and buffers from *state_dict* into this module and\n        its descendants.\n\n        Overrides the method in :class:`nn.Module`. Compared with that method\n        this additionally \"upgrades\" *state_dicts* from old checkpoints.\n        \"\"\"", "\n", "self", ".", "upgrade_state_dict", "(", "state_dict", ")", "\n", "return", "super", "(", ")", ".", "load_state_dict", "(", "state_dict", ",", "strict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.BaseFairseqModel.upgrade_state_dict": [[71, 74], ["fairseq_model.BaseFairseqModel.upgrade_state_dict_named"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.TransformerEncoderLayer.upgrade_state_dict_named"], ["", "def", "upgrade_state_dict", "(", "self", ",", "state_dict", ")", ":", "\n", "        ", "\"\"\"Upgrade old state dicts to work with newer code.\"\"\"", "\n", "self", ".", "upgrade_state_dict_named", "(", "state_dict", ",", "''", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.BaseFairseqModel.upgrade_state_dict_named": [[75, 97], ["fairseq_model.BaseFairseqModel.upgrade_state_dict_named.do_upgrade"], "methods", ["None"], ["", "def", "upgrade_state_dict_named", "(", "self", ",", "state_dict", ",", "name", ")", ":", "\n", "        ", "\"\"\"Upgrade old state dicts to work with newer code.\n\n        Args:\n            state_dict (dict): state dictionary to upgrade, in place\n            name (str): the state dict key corresponding to the current module\n        \"\"\"", "\n", "assert", "state_dict", "is", "not", "None", "\n", "\n", "def", "do_upgrade", "(", "m", ",", "prefix", ")", ":", "\n", "            ", "if", "len", "(", "prefix", ")", ">", "0", ":", "\n", "                ", "prefix", "+=", "'.'", "\n", "\n", "", "for", "n", ",", "c", "in", "m", ".", "named_children", "(", ")", ":", "\n", "                ", "name", "=", "prefix", "+", "n", "\n", "if", "hasattr", "(", "c", ",", "'upgrade_state_dict_named'", ")", ":", "\n", "                    ", "c", ".", "upgrade_state_dict_named", "(", "state_dict", ",", "name", ")", "\n", "", "elif", "hasattr", "(", "c", ",", "'upgrade_state_dict'", ")", ":", "\n", "                    ", "c", ".", "upgrade_state_dict", "(", "state_dict", ")", "\n", "", "do_upgrade", "(", "c", ",", "name", ")", "\n", "\n", "", "", "do_upgrade", "(", "self", ",", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.BaseFairseqModel.make_generation_fast_": [[98, 130], ["fairseq_model.BaseFairseqModel.apply", "set", "fairseq_model.BaseFairseqModel.apply", "fairseq_model.BaseFairseqModel.eval", "torch.utils.remove_weight_norm", "torch.utils.remove_weight_norm", "torch.utils.remove_weight_norm", "hasattr", "set.add", "module.make_generation_fast_", "RuntimeError"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.add", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.TransformerDecoderLayer.make_generation_fast_"], ["", "def", "make_generation_fast_", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Optimize model for faster generation.\"\"\"", "\n", "if", "self", ".", "_is_generation_fast", ":", "\n", "            ", "return", "# only apply once", "\n", "", "self", ".", "_is_generation_fast", "=", "True", "\n", "\n", "# remove weight norm from all modules in the network", "\n", "def", "apply_remove_weight_norm", "(", "module", ")", ":", "\n", "            ", "try", ":", "\n", "                ", "nn", ".", "utils", ".", "remove_weight_norm", "(", "module", ")", "\n", "", "except", "ValueError", ":", "# this module didn't have weight norm", "\n", "                ", "return", "\n", "\n", "", "", "self", ".", "apply", "(", "apply_remove_weight_norm", ")", "\n", "\n", "seen", "=", "set", "(", ")", "\n", "\n", "def", "apply_make_generation_fast_", "(", "module", ")", ":", "\n", "            ", "if", "module", "!=", "self", "and", "hasattr", "(", "module", ",", "'make_generation_fast_'", ")", "and", "module", "not", "in", "seen", ":", "\n", "                ", "seen", ".", "add", "(", "module", ")", "\n", "module", ".", "make_generation_fast_", "(", "**", "kwargs", ")", "\n", "\n", "", "", "self", ".", "apply", "(", "apply_make_generation_fast_", ")", "\n", "\n", "def", "train", "(", "mode", "=", "True", ")", ":", "\n", "            ", "if", "mode", ":", "\n", "                ", "raise", "RuntimeError", "(", "'cannot train after make_generation_fast'", ")", "\n", "\n", "# this model should no longer be used for training", "\n", "", "", "self", ".", "eval", "(", ")", "\n", "self", ".", "train", "=", "train", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.BaseFairseqModel.prepare_for_onnx_export_": [[131, 142], ["set", "fairseq_model.BaseFairseqModel.apply", "hasattr", "set.add", "module.prepare_for_onnx_export_"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.add", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.TransformerDecoderLayer.prepare_for_onnx_export_"], ["", "def", "prepare_for_onnx_export_", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Make model exportable via ONNX trace.\"\"\"", "\n", "seen", "=", "set", "(", ")", "\n", "\n", "def", "apply_prepare_for_onnx_export_", "(", "module", ")", ":", "\n", "            ", "if", "module", "!=", "self", "and", "hasattr", "(", "module", ",", "'prepare_for_onnx_export_'", ")", "and", "module", "not", "in", "seen", ":", "\n", "                ", "seen", ".", "add", "(", "module", ")", "\n", "module", ".", "prepare_for_onnx_export_", "(", "**", "kwargs", ")", "\n", "\n", "", "", "self", ".", "apply", "(", "apply_prepare_for_onnx_export_", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.BaseFairseqModel.from_pretrained": [[143, 176], ["hub_utils.from_pretrained", "print", "hub_utils.GeneratorHubInterface", "cls.hub_models"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.model_gottbert.GottbertModel.from_pretrained", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.model_gottbert.GottbertModel.hub_models"], ["", "@", "classmethod", "\n", "def", "from_pretrained", "(", "cls", ",", "model_name_or_path", ",", "checkpoint_file", "=", "'model.pt'", ",", "data_name_or_path", "=", "'.'", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Load a :class:`~fairseq.models.FairseqModel` from a pre-trained model\n        file. Downloads and caches the pre-trained model file if needed.\n\n        The base implementation returns a\n        :class:`~fairseq.hub_utils.GeneratorHubInterface`, which can be used to\n        generate translations or sample from language models. The underlying\n        :class:`~fairseq.models.FairseqModel` can be accessed via the\n        *generator.models* attribute.\n\n        Other models may override this to implement custom hub interfaces.\n\n        Args:\n            model_name_or_path (str): either the name of a pre-trained model to\n                load or a path/URL to a pre-trained model state dict\n            checkpoint_file (str, optional): colon-separated list of checkpoint\n                files in the model archive to ensemble (default: 'model.pt')\n            data_name_or_path (str, optional): point args.data to the archive\n                at the given path/URL. Can start with '.' or './' to reuse the\n                model archive path.\n        \"\"\"", "\n", "from", "fairseq", "import", "hub_utils", "\n", "x", "=", "hub_utils", ".", "from_pretrained", "(", "\n", "model_name_or_path", ",", "\n", "checkpoint_file", ",", "\n", "data_name_or_path", ",", "\n", "archive_map", "=", "cls", ".", "hub_models", "(", ")", ",", "\n", "**", "kwargs", ",", "\n", ")", "\n", "print", "(", "x", "[", "'args'", "]", ")", "\n", "return", "hub_utils", ".", "GeneratorHubInterface", "(", "x", "[", "'args'", "]", ",", "x", "[", "'task'", "]", ",", "x", "[", "'models'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.BaseFairseqModel.hub_models": [[177, 180], ["None"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "hub_models", "(", "cls", ")", ":", "\n", "        ", "return", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.FairseqEncoderDecoderModel.__init__": [[190, 197], ["fairseq_model.BaseFairseqModel.__init__", "isinstance", "isinstance"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["def", "__init__", "(", "self", ",", "encoder", ",", "decoder", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "encoder", "=", "encoder", "\n", "self", ".", "decoder", "=", "decoder", "\n", "assert", "isinstance", "(", "self", ".", "encoder", ",", "FairseqEncoder", ")", "\n", "assert", "isinstance", "(", "self", ".", "decoder", ",", "FairseqDecoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.FairseqEncoderDecoderModel.forward": [[198, 224], ["fairseq_model.FairseqEncoderDecoderModel.encoder", "fairseq_model.FairseqEncoderDecoderModel.decoder"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.FairseqMultiModel.encoder", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.FairseqMultiModel.decoder"], ["", "def", "forward", "(", "self", ",", "src_tokens", ",", "src_lengths", ",", "prev_output_tokens", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Run the forward pass for an encoder-decoder model.\n\n        First feed a batch of source tokens through the encoder. Then, feed the\n        encoder output and previous decoder outputs (i.e., teacher forcing) to\n        the decoder to produce the next outputs::\n\n            encoder_out = self.encoder(src_tokens, src_lengths)\n            return self.decoder(prev_output_tokens, encoder_out)\n\n        Args:\n            src_tokens (LongTensor): tokens in the source language of shape\n                `(batch, src_len)`\n            src_lengths (LongTensor): source sentence lengths of shape `(batch)`\n            prev_output_tokens (LongTensor): previous decoder outputs of shape\n                `(batch, tgt_len)`, for teacher forcing\n\n        Returns:\n            tuple:\n                - the decoder's output of shape `(batch, tgt_len, vocab)`\n                - a dictionary with any model-specific outputs\n        \"\"\"", "\n", "encoder_out", "=", "self", ".", "encoder", "(", "src_tokens", ",", "src_lengths", "=", "src_lengths", ",", "**", "kwargs", ")", "\n", "decoder_out", "=", "self", ".", "decoder", "(", "prev_output_tokens", ",", "encoder_out", "=", "encoder_out", ",", "**", "kwargs", ")", "\n", "return", "decoder_out", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.FairseqEncoderDecoderModel.extract_features": [[225, 244], ["fairseq_model.FairseqEncoderDecoderModel.encoder", "fairseq_model.FairseqEncoderDecoderModel.decoder.extract_features"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.FairseqMultiModel.encoder", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.extract_features"], ["", "def", "extract_features", "(", "self", ",", "src_tokens", ",", "src_lengths", ",", "prev_output_tokens", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Similar to *forward* but only return features.\n\n        Returns:\n            tuple:\n                - the decoder's features of shape `(batch, tgt_len, embed_dim)`\n                - a dictionary with any model-specific outputs\n        \"\"\"", "\n", "encoder_out", "=", "self", ".", "encoder", "(", "src_tokens", ",", "src_lengths", "=", "src_lengths", ",", "**", "kwargs", ")", "\n", "# FIXME: Because this abstraction at this level is so constraining, we", "\n", "# need to hack removing unwanted arguments from the decoder extra", "\n", "# arguments at this high level of abstraction. ", "\n", "filtered_kwargs", "=", "{", "\n", "key", ":", "value", "for", "key", ",", "value", "in", "kwargs", "\n", "if", "key", "not", "in", "[", "'src_wordpieces'", ",", "'src_wp2w'", "]", "\n", "}", "\n", "features", "=", "self", ".", "decoder", ".", "extract_features", "(", "prev_output_tokens", ",", "encoder_out", "=", "encoder_out", ",", "**", "filtered_kwargs", ")", "\n", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.FairseqEncoderDecoderModel.output_layer": [[245, 248], ["fairseq_model.FairseqEncoderDecoderModel.decoder.output_layer"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.TransformerDecoder.output_layer"], ["", "def", "output_layer", "(", "self", ",", "features", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Project features to the default output size (typically vocabulary size).\"\"\"", "\n", "return", "self", ".", "decoder", ".", "output_layer", "(", "features", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.FairseqEncoderDecoderModel.max_positions": [[249, 252], ["fairseq_model.FairseqEncoderDecoderModel.encoder.max_positions", "fairseq_model.FairseqEncoderDecoderModel.decoder.max_positions"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.max_positions", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.max_positions"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum length supported by the model.\"\"\"", "\n", "return", "(", "self", ".", "encoder", ".", "max_positions", "(", ")", ",", "self", ".", "decoder", ".", "max_positions", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.FairseqEncoderDecoderModel.max_decoder_positions": [[253, 256], ["fairseq_model.FairseqEncoderDecoderModel.decoder.max_positions"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.max_positions"], ["", "def", "max_decoder_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum length supported by the decoder.\"\"\"", "\n", "return", "self", ".", "decoder", ".", "max_positions", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.FairseqModel.__init__": [[260, 266], ["fairseq_model.FairseqEncoderDecoderModel.__init__", "fairseq.utils.deprecation_warning"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.deprecation_warning"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "utils", ".", "deprecation_warning", "(", "\n", "'FairseqModel is deprecated, please use FairseqEncoderDecoderModel '", "\n", "'or BaseFairseqModel instead'", ",", "\n", "stacklevel", "=", "4", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.FairseqMultiModel.__init__": [[272, 283], ["fairseq_model.BaseFairseqModel.__init__", "list", "torch.ModuleDict", "torch.ModuleDict", "torch.ModuleDict", "encoders.keys", "decoders.keys", "encoders.keys", "isinstance", "isinstance", "fairseq_model.FairseqModel"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["def", "__init__", "(", "self", ",", "encoders", ",", "decoders", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "encoders", ".", "keys", "(", ")", "==", "decoders", ".", "keys", "(", ")", "\n", "self", ".", "keys", "=", "list", "(", "encoders", ".", "keys", "(", ")", ")", "\n", "for", "key", "in", "self", ".", "keys", ":", "\n", "            ", "assert", "isinstance", "(", "encoders", "[", "key", "]", ",", "FairseqEncoder", ")", "\n", "assert", "isinstance", "(", "decoders", "[", "key", "]", ",", "FairseqDecoder", ")", "\n", "\n", "", "self", ".", "models", "=", "nn", ".", "ModuleDict", "(", "{", "\n", "key", ":", "FairseqModel", "(", "encoders", "[", "key", "]", ",", "decoders", "[", "key", "]", ")", "\n", "for", "key", "in", "self", ".", "keys", "\n", "}", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.FairseqMultiModel.build_shared_embeddings": [[285, 315], ["any", "build_embedding", "ValueError"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "build_shared_embeddings", "(", "\n", "dicts", ":", "Dict", "[", "str", ",", "Dictionary", "]", ",", "\n", "langs", ":", "List", "[", "str", "]", ",", "\n", "embed_dim", ":", "int", ",", "\n", "build_embedding", ":", "callable", ",", "\n", "pretrained_embed_path", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Helper function to build shared embeddings for a set of languages after\n        checking that all dicts corresponding to those languages are equivalent.\n\n        Args:\n            dicts: Dict of lang_id to its corresponding Dictionary\n            langs: languages that we want to share embeddings for\n            embed_dim: embedding dimension\n            build_embedding: callable function to actually build the embedding\n            pretrained_embed_path: Optional path to load pretrained embeddings\n        \"\"\"", "\n", "shared_dict", "=", "dicts", "[", "langs", "[", "0", "]", "]", "\n", "if", "any", "(", "dicts", "[", "lang", "]", "!=", "shared_dict", "for", "lang", "in", "langs", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "'--share-*-embeddings requires a joined dictionary: '", "\n", "'--share-encoder-embeddings requires a joined source '", "\n", "'dictionary, --share-decoder-embeddings requires a joined '", "\n", "'target dictionary, and --share-all-embeddings requires a '", "\n", "'joint source + target dictionary.'", "\n", ")", "\n", "", "return", "build_embedding", "(", "\n", "shared_dict", ",", "embed_dim", ",", "pretrained_embed_path", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.FairseqMultiModel.forward": [[317, 325], ["fairseq_model.FairseqMultiModel.models[].encoder", "fairseq_model.FairseqMultiModel.models[].decoder"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.FairseqMultiModel.encoder", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.FairseqMultiModel.decoder"], ["", "def", "forward", "(", "self", ",", "src_tokens", ",", "src_lengths", ",", "prev_output_tokens", ",", "**", "kwargs", ")", ":", "\n", "        ", "decoder_outs", "=", "{", "}", "\n", "for", "key", "in", "self", ".", "keys", ":", "\n", "            ", "encoder_out", "=", "self", ".", "models", "[", "key", "]", ".", "encoder", "(", "src_tokens", ",", "src_lengths", ",", "**", "kwargs", ")", "\n", "decoder_outs", "[", "key", "]", "=", "self", ".", "models", "[", "key", "]", ".", "decoder", "(", "\n", "prev_output_tokens", ",", "encoder_out", ",", "**", "kwargs", ",", "\n", ")", "\n", "", "return", "decoder_outs", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.FairseqMultiModel.max_positions": [[326, 331], ["fairseq_model.FairseqMultiModel.models[].encoder.max_positions", "fairseq_model.FairseqMultiModel.models[].decoder.max_positions"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.max_positions", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.max_positions"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum length supported by the model.\"\"\"", "\n", "return", "{", "\n", "key", ":", "(", "self", ".", "models", "[", "key", "]", ".", "encoder", ".", "max_positions", "(", ")", ",", "self", ".", "models", "[", "key", "]", ".", "decoder", ".", "max_positions", "(", ")", ")", "\n", "for", "key", "in", "self", ".", "keys", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.FairseqMultiModel.max_decoder_positions": [[333, 336], ["min", "model.decoder.max_positions", "fairseq_model.FairseqMultiModel.models.values"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.max_positions"], ["", "def", "max_decoder_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum length supported by the decoder.\"\"\"", "\n", "return", "min", "(", "model", ".", "decoder", ".", "max_positions", "(", ")", "for", "model", "in", "self", ".", "models", ".", "values", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.FairseqMultiModel.encoder": [[337, 340], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "encoder", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "models", "[", "self", ".", "keys", "[", "0", "]", "]", ".", "encoder", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.FairseqMultiModel.decoder": [[341, 344], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "decoder", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "models", "[", "self", ".", "keys", "[", "0", "]", "]", ".", "decoder", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.FairseqLanguageModel.__init__": [[353, 357], ["fairseq_model.BaseFairseqModel.__init__", "isinstance"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["def", "__init__", "(", "self", ",", "decoder", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "decoder", "=", "decoder", "\n", "assert", "isinstance", "(", "self", ".", "decoder", ",", "FairseqDecoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.FairseqLanguageModel.forward": [[358, 375], ["fairseq_model.FairseqLanguageModel.decoder"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.FairseqMultiModel.decoder"], ["", "def", "forward", "(", "self", ",", "src_tokens", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Run the forward pass for a decoder-only model.\n\n        Feeds a batch of tokens through the decoder to predict the next tokens.\n\n        Args:\n            src_tokens (LongTensor): tokens on which to condition the decoder,\n                of shape `(batch, tgt_len)`\n            src_lengths (LongTensor): source sentence lengths of shape `(batch)`\n\n        Returns:\n            tuple:\n                - the decoder's output of shape `(batch, seq_len, vocab)`\n                - a dictionary with any model-specific outputs\n        \"\"\"", "\n", "return", "self", ".", "decoder", "(", "src_tokens", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.FairseqLanguageModel.extract_features": [[376, 386], ["fairseq_model.FairseqLanguageModel.decoder.extract_features"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.extract_features"], ["", "def", "extract_features", "(", "self", ",", "src_tokens", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Similar to *forward* but only return features.\n\n        Returns:\n            tuple:\n                - the decoder's features of shape `(batch, seq_len, embed_dim)`\n                - a dictionary with any model-specific outputs\n        \"\"\"", "\n", "return", "self", ".", "decoder", ".", "extract_features", "(", "src_tokens", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.FairseqLanguageModel.output_layer": [[387, 390], ["fairseq_model.FairseqLanguageModel.decoder.output_layer"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.TransformerDecoder.output_layer"], ["", "def", "output_layer", "(", "self", ",", "features", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Project features to the default output size (typically vocabulary size).\"\"\"", "\n", "return", "self", ".", "decoder", ".", "output_layer", "(", "features", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.FairseqLanguageModel.max_positions": [[391, 394], ["fairseq_model.FairseqLanguageModel.decoder.max_positions"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.max_positions"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum length supported by the model.\"\"\"", "\n", "return", "self", ".", "decoder", ".", "max_positions", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.FairseqLanguageModel.max_decoder_positions": [[395, 398], ["fairseq_model.FairseqLanguageModel.decoder.max_positions"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.max_positions"], ["", "def", "max_decoder_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum length supported by the decoder.\"\"\"", "\n", "return", "self", ".", "decoder", ".", "max_positions", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.FairseqLanguageModel.supported_targets": [[399, 402], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "supported_targets", "(", "self", ")", ":", "\n", "        ", "return", "{", "'future'", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.FairseqEncoderModel.__init__": [[411, 415], ["fairseq_model.BaseFairseqModel.__init__", "isinstance"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["def", "__init__", "(", "self", ",", "encoder", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "encoder", "=", "encoder", "\n", "assert", "isinstance", "(", "self", ".", "encoder", ",", "FairseqEncoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.FairseqEncoderModel.forward": [[416, 430], ["fairseq_model.FairseqEncoderModel.encoder"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.FairseqMultiModel.encoder"], ["", "def", "forward", "(", "self", ",", "src_tokens", ",", "src_lengths", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Run the forward pass for a encoder-only model.\n\n        Feeds a batch of tokens through the encoder to generate features.\n\n        Args:\n            src_tokens (LongTensor): input tokens of shape `(batch, src_len)`\n            src_lengths (LongTensor): source sentence lengths of shape `(batch)`\n\n        Returns:\n            the encoder's output, typically of shape `(batch, src_len, features)`\n        \"\"\"", "\n", "return", "self", ".", "encoder", "(", "src_tokens", ",", "src_lengths", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.FairseqEncoderModel.get_normalized_probs": [[431, 441], ["torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "encoder_out.float", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.softmax", "torch.softmax", "torch.softmax"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.log_softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.log_softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.log_softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax"], ["", "def", "get_normalized_probs", "(", "self", ",", "net_output", ",", "log_probs", ",", "sample", "=", "None", ")", ":", "\n", "        ", "\"\"\"Get normalized probabilities (or log probs) from a net's output.\"\"\"", "\n", "encoder_out", "=", "net_output", "[", "'encoder_out'", "]", "\n", "if", "torch", ".", "is_tensor", "(", "encoder_out", ")", ":", "\n", "            ", "logits", "=", "encoder_out", ".", "float", "(", ")", "\n", "if", "log_probs", ":", "\n", "                ", "return", "F", ".", "log_softmax", "(", "logits", ",", "dim", "=", "-", "1", ")", "\n", "", "else", ":", "\n", "                ", "return", "F", ".", "softmax", "(", "logits", ",", "dim", "=", "-", "1", ")", "\n", "", "", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.FairseqEncoderModel.max_positions": [[442, 445], ["fairseq_model.FairseqEncoderModel.encoder.max_positions"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.max_positions"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum length supported by the model.\"\"\"", "\n", "return", "self", ".", "encoder", ".", "max_positions", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.wav2vec.Wav2VecModel.add_args": [[22, 67], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["None"], ["    ", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add model-specific arguments to the parser.\"\"\"", "\n", "parser", ".", "add_argument", "(", "'--prediction-steps'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "help", "=", "'number of steps ahead to predict'", ")", "\n", "parser", ".", "add_argument", "(", "'--sample-distance'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'sample distance from target. does not work properly with cross-sampling'", ")", "\n", "parser", ".", "add_argument", "(", "'--cross-sample-negatives'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'whether to sample negatives across examples in the same batch'", ")", "\n", "parser", ".", "add_argument", "(", "'--num-negatives'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "help", "=", "'number of negative examples'", ")", "\n", "parser", ".", "add_argument", "(", "'--conv-feature-layers'", ",", "type", "=", "str", ",", "metavar", "=", "'EXPR'", ",", "\n", "help", "=", "'convolutional feature extraction layers [(dim, kernel_size, stride), ...]'", ")", "\n", "parser", ".", "add_argument", "(", "'--conv-aggregator-layers'", ",", "type", "=", "str", ",", "metavar", "=", "'EXPR'", ",", "\n", "help", "=", "'convolutional feature extraction layers [(dim, kernel_size, stride), ...]'", ")", "\n", "parser", ".", "add_argument", "(", "'--dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "help", "=", "'dropout to apply within the model'", ")", "\n", "parser", ".", "add_argument", "(", "'--dropout-features'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "help", "=", "'dropout to apply to the features'", ")", "\n", "parser", ".", "add_argument", "(", "'--dropout-agg'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "help", "=", "'dropout to apply after aggregation step'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder'", ",", "type", "=", "str", ",", "choices", "=", "[", "'cnn'", "]", ",", "help", "=", "'type of encoder to use'", ")", "\n", "parser", ".", "add_argument", "(", "'--aggregator'", ",", "type", "=", "str", ",", "choices", "=", "[", "'cnn'", ",", "'gru'", "]", ",", "\n", "help", "=", "'type of aggregator to use'", ")", "\n", "parser", ".", "add_argument", "(", "'--gru-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "help", "=", "'GRU dimensionality'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--no-conv-bias'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'if set, does not learn bias for conv layers'", ")", "\n", "parser", ".", "add_argument", "(", "'--agg-zero-pad'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'if set, zero pads in aggregator instead of repl pad'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--skip-connections-feat'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'if set, adds skip connections to the feature extractor'", ")", "\n", "parser", ".", "add_argument", "(", "'--skip-connections-agg'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'if set, adds skip connections to the aggregator'", ")", "\n", "parser", ".", "add_argument", "(", "'--residual-scale'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'scales residual by sqrt(value)'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--log-compression'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'if set, adds a log compression to feature extractor'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--balanced-classes'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'if set, loss is scaled to balance for number of negatives'", ")", "\n", "parser", ".", "add_argument", "(", "'--project-features'", ",", "choices", "=", "[", "'none'", ",", "'same'", ",", "'new'", "]", ",", "\n", "help", "=", "'if not none, features are projected using the (same or new) aggregator'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--non-affine-group-norm'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'if set, group norm is not affine'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--offset'", ",", "help", "=", "'if set, introduces an offset from target to predictions. '", "\n", "'if set to \"auto\", it is computed automatically from the receptive field'", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.wav2vec.Wav2VecModel.build_model": [[69, 79], ["wav2vec.base_wav2vec_architecture", "wav2vec.Wav2VecModel", "print"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.wav2vec.base_wav2vec_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["", "@", "classmethod", "\n", "def", "build_model", "(", "cls", ",", "args", ",", "task", ")", ":", "\n", "        ", "\"\"\"Build a new model instance.\"\"\"", "\n", "\n", "# make sure all arguments are present in older models", "\n", "base_wav2vec_architecture", "(", "args", ")", "\n", "\n", "model", "=", "Wav2VecModel", "(", "args", ")", "\n", "print", "(", "model", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.wav2vec.Wav2VecModel.__init__": [[80, 171], ["BaseFairseqModel.__init__", "int", "wav2vec.Wav2VecModel.__init__.make_aggregator"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "prediction_steps", "=", "args", ".", "prediction_steps", "\n", "\n", "offset", "=", "args", ".", "offset", "\n", "\n", "if", "args", ".", "encoder", "==", "'cnn'", ":", "\n", "            ", "feature_enc_layers", "=", "eval", "(", "args", ".", "conv_feature_layers", ")", "\n", "self", ".", "feature_extractor", "=", "ConvFeatureExtractionModel", "(", "\n", "conv_layers", "=", "feature_enc_layers", ",", "\n", "dropout", "=", "0.", ",", "\n", "log_compression", "=", "args", ".", "log_compression", ",", "\n", "skip_connections", "=", "args", ".", "skip_connections_feat", ",", "\n", "residual_scale", "=", "args", ".", "residual_scale", ",", "\n", "non_affine_group_norm", "=", "args", ".", "non_affine_group_norm", ",", "\n", ")", "\n", "embed", "=", "feature_enc_layers", "[", "-", "1", "]", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "raise", "Exception", "(", "'unknown encoder type '", "+", "args", ".", "encoder", ")", "\n", "\n", "", "if", "args", ".", "offset", "==", "'auto'", ":", "\n", "            ", "assert", "args", ".", "encoder", "==", "'cnn'", "\n", "jin", "=", "0", "\n", "rin", "=", "0", "\n", "for", "_", ",", "k", ",", "stride", "in", "feature_enc_layers", ":", "\n", "                ", "if", "rin", "==", "0", ":", "\n", "                    ", "rin", "=", "k", "\n", "", "rin", "=", "rin", "+", "(", "k", "-", "1", ")", "*", "jin", "\n", "if", "jin", "==", "0", ":", "\n", "                    ", "jin", "=", "stride", "\n", "", "else", ":", "\n", "                    ", "jin", "*=", "stride", "\n", "", "", "offset", "=", "math", ".", "ceil", "(", "rin", "/", "jin", ")", "\n", "\n", "", "offset", "=", "int", "(", "offset", ")", "\n", "\n", "def", "make_aggregator", "(", ")", ":", "\n", "            ", "if", "args", ".", "aggregator", "==", "'cnn'", ":", "\n", "                ", "agg_layers", "=", "eval", "(", "args", ".", "conv_aggregator_layers", ")", "\n", "agg_dim", "=", "agg_layers", "[", "-", "1", "]", "[", "0", "]", "\n", "feature_aggregator", "=", "ConvAggegator", "(", "\n", "conv_layers", "=", "agg_layers", ",", "\n", "embed", "=", "embed", ",", "\n", "dropout", "=", "args", ".", "dropout", ",", "\n", "skip_connections", "=", "args", ".", "skip_connections_agg", ",", "\n", "residual_scale", "=", "args", ".", "residual_scale", ",", "\n", "non_affine_group_norm", "=", "args", ".", "non_affine_group_norm", ",", "\n", "conv_bias", "=", "not", "args", ".", "no_conv_bias", ",", "\n", "zero_pad", "=", "args", ".", "agg_zero_pad", ",", "\n", ")", "\n", "", "elif", "args", ".", "aggregator", "==", "'gru'", ":", "\n", "                ", "agg_dim", "=", "args", ".", "gru_dim", "\n", "feature_aggregator", "=", "nn", ".", "Sequential", "(", "\n", "TransposeLast", "(", ")", ",", "\n", "nn", ".", "GRU", "(", "\n", "input_size", "=", "embed", ",", "\n", "hidden_size", "=", "agg_dim", ",", "\n", "num_layers", "=", "1", ",", "\n", "dropout", "=", "args", ".", "dropout", ",", "\n", ")", ",", "\n", "TransposeLast", "(", "deconstruct_idx", "=", "0", ")", ",", "\n", ")", "\n", "", "else", ":", "\n", "                ", "raise", "Exception", "(", "'unknown aggregator type '", "+", "args", ".", "aggregator", ")", "\n", "\n", "", "return", "feature_aggregator", ",", "agg_dim", "\n", "\n", "", "self", ".", "feature_aggregator", ",", "agg_dim", "=", "make_aggregator", "(", ")", "\n", "\n", "self", ".", "wav2vec_predictions", "=", "Wav2VecPredictionsModel", "(", "\n", "in_dim", "=", "agg_dim", ",", "\n", "out_dim", "=", "embed", ",", "\n", "prediction_steps", "=", "args", ".", "prediction_steps", ",", "\n", "n_negatives", "=", "args", ".", "num_negatives", ",", "\n", "cross_sample_negatives", "=", "args", ".", "cross_sample_negatives", ",", "\n", "sample_distance", "=", "args", ".", "sample_distance", ",", "\n", "dropout", "=", "args", ".", "dropout", ",", "\n", "offset", "=", "offset", ",", "\n", "balanced_classes", "=", "args", ".", "balanced_classes", ",", "\n", ")", "\n", "\n", "self", ".", "dropout_feats", "=", "nn", ".", "Dropout", "(", "p", "=", "args", ".", "dropout_features", ")", "\n", "self", ".", "dropout_agg", "=", "nn", ".", "Dropout", "(", "p", "=", "args", ".", "dropout_agg", ")", "\n", "\n", "if", "args", ".", "project_features", "==", "'none'", ":", "\n", "            ", "self", ".", "project_features", "=", "None", "\n", "", "elif", "args", ".", "project_features", "==", "'same'", ":", "\n", "            ", "self", ".", "project_features", "=", "self", ".", "feature_aggregator", "\n", "", "elif", "args", ".", "project_features", "==", "'new'", ":", "\n", "            ", "self", ".", "project_features", ",", "_", "=", "make_aggregator", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.wav2vec.Wav2VecModel.forward": [[172, 188], ["wav2vec.Wav2VecModel.feature_extractor", "wav2vec.Wav2VecModel.dropout_feats", "wav2vec.Wav2VecModel.feature_aggregator", "wav2vec.Wav2VecModel.dropout_agg", "wav2vec.Wav2VecModel.wav2vec_predictions", "wav2vec.Wav2VecModel.project_features"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "source", ")", ":", "\n", "        ", "result", "=", "{", "}", "\n", "\n", "features", "=", "self", ".", "feature_extractor", "(", "source", ")", "\n", "\n", "x", "=", "self", ".", "dropout_feats", "(", "features", ")", "\n", "x", "=", "self", ".", "feature_aggregator", "(", "x", ")", "\n", "x", "=", "self", ".", "dropout_agg", "(", "x", ")", "\n", "\n", "if", "self", ".", "project_features", "is", "not", "None", ":", "\n", "            ", "features", "=", "self", ".", "project_features", "(", "features", ")", "\n", "", "x", ",", "targets", "=", "self", ".", "wav2vec_predictions", "(", "x", ",", "features", ")", "\n", "result", "[", "'cpc_logits'", "]", "=", "x", "\n", "result", "[", "'cpc_targets'", "]", "=", "targets", "\n", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.wav2vec.Wav2VecModel.upgrade_state_dict_named": [[189, 191], ["None"], "methods", ["None"], ["", "def", "upgrade_state_dict_named", "(", "self", ",", "state_dict", ",", "name", ")", ":", "\n", "        ", "return", "state_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.wav2vec.Wav2VecModel.max_positions": [[192, 195], ["None"], "methods", ["None"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum length supported by the model.\"\"\"", "\n", "return", "sys", ".", "maxsize", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.wav2vec.Wav2VecModel.get_logits": [[196, 199], ["None"], "methods", ["None"], ["", "def", "get_logits", "(", "self", ",", "net_output", ")", ":", "\n", "        ", "logits", "=", "net_output", "[", "'cpc_logits'", "]", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.wav2vec.Wav2VecModel.get_targets": [[200, 203], ["t.contiguous"], "methods", ["None"], ["", "def", "get_targets", "(", "self", ",", "sample", ",", "net_output", ",", "expand_steps", "=", "True", ")", ":", "\n", "        ", "t", "=", "net_output", "[", "'cpc_targets'", "]", "\n", "return", "t", ".", "contiguous", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.wav2vec.Wav2VecModel.get_target_weights": [[204, 209], ["isinstance"], "methods", ["None"], ["", "def", "get_target_weights", "(", "self", ",", "targets", ",", "net_output", ")", ":", "\n", "        ", "targets", "=", "net_output", "[", "'cpc_targets'", "]", "\n", "if", "isinstance", "(", "targets", ",", "tuple", ")", "and", "targets", "[", "-", "1", "]", "is", "not", "None", ":", "\n", "            ", "return", "targets", "[", "-", "1", "]", "\n", "", "return", "1.", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.wav2vec.TransposeLast.__init__": [[212, 215], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["    ", "def", "__init__", "(", "self", ",", "deconstruct_idx", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "deconstruct_idx", "=", "deconstruct_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.wav2vec.TransposeLast.forward": [[216, 220], ["x.transpose"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "self", ".", "deconstruct_idx", "is", "not", "None", ":", "\n", "            ", "x", "=", "x", "[", "self", ".", "deconstruct_idx", "]", "\n", "", "return", "x", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.wav2vec.Fp32GroupNorm.__init__": [[223, 225], ["torch.GroupNorm.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.wav2vec.Fp32GroupNorm.forward": [[226, 231], ["torch.group_norm", "torch.group_norm", "torch.group_norm", "torch.group_norm.type_as", "input.float", "wav2vec.Fp32GroupNorm.weight.float", "wav2vec.Fp32GroupNorm.bias.float"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "output", "=", "F", ".", "group_norm", "(", "\n", "input", ".", "float", "(", ")", ",", "self", ".", "num_groups", ",", "self", ".", "weight", ".", "float", "(", ")", "if", "self", ".", "weight", "is", "not", "None", "else", "None", ",", "\n", "self", ".", "bias", ".", "float", "(", ")", "if", "self", ".", "bias", "is", "not", "None", "else", "None", ",", "self", ".", "eps", ")", "\n", "return", "output", ".", "type_as", "(", "input", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.wav2vec.Fp32LayerNorm.__init__": [[234, 236], ["torch.LayerNorm.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.wav2vec.Fp32LayerNorm.forward": [[237, 242], ["torch.layer_norm", "torch.layer_norm", "torch.layer_norm", "torch.layer_norm.type_as", "input.float", "wav2vec.Fp32LayerNorm.weight.float", "wav2vec.Fp32LayerNorm.bias.float"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "output", "=", "F", ".", "layer_norm", "(", "\n", "input", ".", "float", "(", ")", ",", "self", ".", "normalized_shape", ",", "self", ".", "weight", ".", "float", "(", ")", "if", "self", ".", "weight", "is", "not", "None", "else", "None", ",", "\n", "self", ".", "bias", ".", "float", "(", ")", "if", "self", ".", "bias", "is", "not", "None", "else", "None", ",", "self", ".", "eps", ")", "\n", "return", "output", ".", "type_as", "(", "input", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.wav2vec.ConvFeatureExtractionModel.__init__": [[258, 279], ["torch.Module.__init__", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "enumerate", "math.sqrt", "torch.Sequential", "torch.Sequential", "torch.Sequential", "wav2vec.ConvFeatureExtractionModel.conv_layers.append", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Dropout", "torch.Dropout", "torch.Dropout", "wav2vec.norm_block", "torch.ReLU", "torch.ReLU", "torch.ReLU", "wav2vec.ConvFeatureExtractionModel.__init__.block"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.wav2vec.norm_block"], ["    ", "def", "__init__", "(", "self", ",", "conv_layers", ",", "dropout", ",", "log_compression", ",", "skip_connections", ",", "residual_scale", ",", "non_affine_group_norm", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "def", "block", "(", "n_in", ",", "n_out", ",", "k", ",", "stride", ")", ":", "\n", "            ", "return", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv1d", "(", "n_in", ",", "n_out", ",", "k", ",", "stride", "=", "stride", ",", "bias", "=", "False", ")", ",", "\n", "nn", ".", "Dropout", "(", "p", "=", "dropout", ")", ",", "\n", "norm_block", "(", "is_layer_norm", "=", "False", ",", "dim", "=", "n_out", ",", "affine", "=", "not", "non_affine_group_norm", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", ")", "\n", "\n", "", "in_d", "=", "1", "\n", "self", ".", "conv_layers", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "i", ",", "(", "dim", ",", "k", ",", "stride", ")", "in", "enumerate", "(", "conv_layers", ")", ":", "\n", "            ", "self", ".", "conv_layers", ".", "append", "(", "\n", "block", "(", "in_d", ",", "dim", ",", "k", ",", "stride", ")", ")", "\n", "in_d", "=", "dim", "\n", "\n", "", "self", ".", "log_compression", "=", "log_compression", "\n", "self", ".", "skip_connections", "=", "skip_connections", "\n", "self", ".", "residual_scale", "=", "math", ".", "sqrt", "(", "residual_scale", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.wav2vec.ConvFeatureExtractionModel.forward": [[280, 299], ["x.log.log.unsqueeze", "conv", "x.log.log.abs", "x.log.log.log", "x.log.log.size", "residual.size", "x.log.log.size", "residual.size"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# BxT -> BxCxT", "\n", "        ", "x", "=", "x", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "for", "conv", "in", "self", ".", "conv_layers", ":", "\n", "            ", "residual", "=", "x", "\n", "x", "=", "conv", "(", "x", ")", "\n", "if", "self", ".", "skip_connections", "and", "x", ".", "size", "(", "1", ")", "==", "residual", ".", "size", "(", "1", ")", ":", "\n", "                ", "tsz", "=", "x", ".", "size", "(", "2", ")", "\n", "r_tsz", "=", "residual", ".", "size", "(", "2", ")", "\n", "residual", "=", "residual", "[", "...", ",", ":", ":", "r_tsz", "//", "tsz", "]", "[", "...", ",", ":", "tsz", "]", "\n", "x", "=", "(", "x", "+", "residual", ")", "*", "self", ".", "residual_scale", "\n", "\n", "", "", "if", "self", ".", "log_compression", ":", "\n", "            ", "x", "=", "x", ".", "abs", "(", ")", "\n", "x", "=", "x", "+", "1", "\n", "x", "=", "x", ".", "log", "(", ")", "\n", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.wav2vec.ZeroPad1d.__init__": [[302, 306], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["    ", "def", "__init__", "(", "self", ",", "pad_left", ",", "pad_right", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "pad_left", "=", "pad_left", "\n", "self", ".", "pad_right", "=", "pad_right", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.wav2vec.ZeroPad1d.forward": [[307, 309], ["torch.pad", "torch.pad", "torch.pad"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "F", ".", "pad", "(", "x", ",", "(", "self", ".", "pad_left", ",", "self", ".", "pad_right", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.wav2vec.ConvAggegator.__init__": [[312, 348], ["torch.Module.__init__", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "enumerate", "torch.Sequential", "torch.Sequential", "torch.Sequential", "math.sqrt", "torch.Sequential", "torch.Sequential", "torch.Sequential", "wav2vec.ConvAggegator.conv_layers.append", "wav2vec.ZeroPad1d", "torch.ReplicationPad1d", "torch.ReplicationPad1d", "torch.ReplicationPad1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Dropout", "torch.Dropout", "torch.Dropout", "wav2vec.norm_block", "torch.ReLU", "torch.ReLU", "torch.ReLU", "wav2vec.ConvAggegator.residual_proj.append", "wav2vec.ConvAggegator.residual_proj.append", "wav2vec.ConvAggegator.__init__.block"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.wav2vec.norm_block", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["    ", "def", "__init__", "(", "self", ",", "conv_layers", ",", "embed", ",", "dropout", ",", "skip_connections", ",", "residual_scale", ",", "non_affine_group_norm", ",", "conv_bias", ",", "\n", "zero_pad", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "def", "block", "(", "n_in", ",", "n_out", ",", "k", ",", "stride", ")", ":", "\n", "# padding dims only really make sense for stride = 1", "\n", "            ", "ka", "=", "k", "//", "2", "\n", "kb", "=", "ka", "-", "1", "if", "k", "%", "2", "==", "0", "else", "ka", "\n", "\n", "pad", "=", "ZeroPad1d", "(", "ka", "+", "kb", ",", "0", ")", "if", "zero_pad", "else", "nn", ".", "ReplicationPad1d", "(", "(", "ka", "+", "kb", ",", "0", ")", ")", "\n", "\n", "return", "nn", ".", "Sequential", "(", "\n", "pad", ",", "\n", "nn", ".", "Conv1d", "(", "n_in", ",", "n_out", ",", "k", ",", "stride", "=", "stride", ",", "bias", "=", "conv_bias", ")", ",", "\n", "nn", ".", "Dropout", "(", "p", "=", "dropout", ")", ",", "\n", "norm_block", "(", "False", ",", "n_out", ",", "affine", "=", "not", "non_affine_group_norm", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", ")", "\n", "\n", "", "in_d", "=", "embed", "\n", "self", ".", "conv_layers", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "residual_proj", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "i", ",", "(", "dim", ",", "k", ",", "stride", ")", "in", "enumerate", "(", "conv_layers", ")", ":", "\n", "            ", "if", "in_d", "!=", "dim", "and", "skip_connections", ":", "\n", "                ", "self", ".", "residual_proj", ".", "append", "(", "\n", "nn", ".", "Conv1d", "(", "in_d", ",", "dim", ",", "1", ",", "bias", "=", "False", ")", ",", "\n", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "residual_proj", ".", "append", "(", "None", ")", "\n", "\n", "", "self", ".", "conv_layers", ".", "append", "(", "\n", "block", "(", "in_d", ",", "dim", ",", "k", ",", "stride", ")", ")", "\n", "in_d", "=", "dim", "\n", "", "self", ".", "conv_layers", "=", "nn", ".", "Sequential", "(", "*", "self", ".", "conv_layers", ")", "\n", "self", ".", "skip_connections", "=", "skip_connections", "\n", "self", ".", "residual_scale", "=", "math", ".", "sqrt", "(", "residual_scale", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.wav2vec.ConvAggegator.forward": [[349, 358], ["zip", "conv", "rproj"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "for", "rproj", ",", "conv", "in", "zip", "(", "self", ".", "residual_proj", ",", "self", ".", "conv_layers", ")", ":", "\n", "            ", "residual", "=", "x", "\n", "x", "=", "conv", "(", "x", ")", "\n", "if", "self", ".", "skip_connections", ":", "\n", "                ", "if", "rproj", "is", "not", "None", ":", "\n", "                    ", "residual", "=", "rproj", "(", "residual", ")", "\n", "", "x", "=", "(", "x", "+", "residual", ")", "*", "self", ".", "residual_scale", "\n", "", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.wav2vec.Wav2VecPredictionsModel.__init__": [[361, 373], ["torch.Module.__init__", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "torch.Dropout", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_dim", ",", "out_dim", ",", "prediction_steps", ",", "n_negatives", ",", "cross_sample_negatives", ",", "sample_distance", ",", "\n", "dropout", ",", "offset", ",", "balanced_classes", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "n_negatives", "=", "n_negatives", "\n", "self", ".", "cross_sample_negatives", "=", "cross_sample_negatives", "\n", "self", ".", "sample_distance", "=", "sample_distance", "\n", "\n", "self", ".", "project_to_steps", "=", "nn", ".", "ConvTranspose2d", "(", "in_dim", ",", "out_dim", ",", "(", "1", ",", "prediction_steps", ")", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "p", "=", "dropout", ")", "\n", "self", ".", "offset", "=", "offset", "\n", "self", ".", "balanced_classes", "=", "balanced_classes", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.wav2vec.Wav2VecPredictionsModel.sample_negatives": [[374, 402], ["y.contiguous().view.contiguous().view.transpose", "y.contiguous().view.contiguous().view.contiguous().view", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "negs.view().permute.view().permute.view().permute", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "range", "y.contiguous().view.contiguous().view.contiguous", "min", "negs.view().permute.view().permute.view", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.randint.view", "torch.randint.view", "torch.randint.view"], "methods", ["None"], ["", "def", "sample_negatives", "(", "self", ",", "y", ")", ":", "\n", "        ", "bsz", ",", "fsz", ",", "tsz", "=", "y", ".", "shape", "\n", "\n", "y", "=", "y", ".", "transpose", "(", "0", ",", "1", ")", "# BCT -> CBT", "\n", "y", "=", "y", ".", "contiguous", "(", ")", ".", "view", "(", "fsz", ",", "-", "1", ")", "# CBT => C(BxT)", "\n", "\n", "if", "self", ".", "cross_sample_negatives", ":", "\n", "            ", "high", "=", "tsz", "*", "bsz", "\n", "assert", "self", ".", "sample_distance", "is", "None", ",", "'sample distance is not supported with cross sampling'", "\n", "", "else", ":", "\n", "            ", "high", "=", "tsz", "if", "self", ".", "sample_distance", "is", "None", "else", "min", "(", "tsz", ",", "self", ".", "sample_distance", ")", "\n", "\n", "", "neg_idxs", "=", "torch", ".", "randint", "(", "low", "=", "0", ",", "high", "=", "high", ",", "size", "=", "(", "bsz", ",", "self", ".", "n_negatives", "*", "tsz", ")", ")", "\n", "\n", "if", "self", ".", "sample_distance", "is", "not", "None", "and", "self", ".", "sample_distance", "<", "tsz", ":", "\n", "            ", "neg_idxs", "+=", "torch", ".", "cat", "(", "\n", "[", "torch", ".", "arange", "(", "start", "=", "1", ",", "end", "=", "tsz", "-", "self", ".", "sample_distance", ",", "device", "=", "neg_idxs", ".", "device", ",", "dtype", "=", "neg_idxs", ".", "dtype", ")", ",", "\n", "torch", ".", "arange", "(", "start", "=", "tsz", "-", "self", ".", "sample_distance", ",", "end", "=", "tsz", "-", "self", ".", "sample_distance", "*", "2", "-", "1", ",", "step", "=", "-", "1", ",", "\n", "device", "=", "neg_idxs", ".", "device", ",", "dtype", "=", "neg_idxs", ".", "dtype", ")", "]", ")", "\n", "\n", "", "if", "not", "self", ".", "cross_sample_negatives", ":", "\n", "            ", "for", "i", "in", "range", "(", "1", ",", "bsz", ")", ":", "\n", "                ", "neg_idxs", "[", "i", "]", "+=", "i", "*", "high", "\n", "\n", "", "", "negs", "=", "y", "[", "...", ",", "neg_idxs", ".", "view", "(", "-", "1", ")", "]", "\n", "negs", "=", "negs", ".", "view", "(", "fsz", ",", "bsz", ",", "self", ".", "n_negatives", ",", "tsz", ")", ".", "permute", "(", "2", ",", "1", ",", "0", ",", "3", ")", "# to NxBxCxT", "\n", "\n", "return", "negs", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.wav2vec.Wav2VecPredictionsModel.forward": [[403, 435], ["wav2vec.Wav2VecPredictionsModel.sample_negatives", "y.unsqueeze.unsqueeze.unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "x.unsqueeze().expand.unsqueeze().expand.unsqueeze", "wav2vec.Wav2VecPredictionsModel.project_to_steps", "wav2vec.Wav2VecPredictionsModel.dropout", "x.unsqueeze().expand.unsqueeze().expand.unsqueeze().expand", "min", "x.unsqueeze().expand.unsqueeze().expand.new", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "range", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.full_like", "torch.full_like", "torch.full_like", "torch.full_like", "torch.full_like", "torch.full_like", "torch.full_like", "torch.full_like", "torch.full_like", "x.unsqueeze().expand.new.numel", "x.unsqueeze().expand.new.numel", "x.unsqueeze().expand.unsqueeze().expand.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.wav2vec.Wav2VecPredictionsModel.sample_negatives", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "forward", "(", "self", ",", "x", ",", "y", ")", ":", "\n", "        ", "negatives", "=", "self", ".", "sample_negatives", "(", "y", ")", "\n", "y", "=", "y", ".", "unsqueeze", "(", "0", ")", "\n", "targets", "=", "torch", ".", "cat", "(", "[", "y", ",", "negatives", "]", ",", "dim", "=", "0", ")", "\n", "\n", "x", "=", "x", ".", "unsqueeze", "(", "-", "1", ")", "\n", "x", "=", "self", ".", "project_to_steps", "(", "x", ")", "# BxCxTxS", "\n", "x", "=", "self", ".", "dropout", "(", "x", ")", "\n", "x", "=", "x", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "targets", ".", "size", "(", "0", ")", ",", "-", "1", ",", "-", "1", ",", "-", "1", ",", "-", "1", ")", "\n", "\n", "copies", ",", "bsz", ",", "dim", ",", "tsz", ",", "steps", "=", "x", ".", "shape", "\n", "steps", "=", "min", "(", "steps", ",", "tsz", "-", "self", ".", "offset", ")", "\n", "predictions", "=", "x", ".", "new", "(", "bsz", "*", "copies", "*", "(", "tsz", "-", "self", ".", "offset", "+", "1", ")", "*", "steps", "-", "(", "(", "steps", "+", "1", ")", "*", "steps", "//", "2", ")", "*", "copies", "*", "bsz", ")", "\n", "labels", "=", "torch", ".", "zeros_like", "(", "predictions", ")", "\n", "weights", "=", "torch", ".", "full_like", "(", "labels", ",", "1", "/", "self", ".", "n_negatives", ")", "if", "self", ".", "balanced_classes", "else", "None", "\n", "\n", "start", "=", "end", "=", "0", "\n", "for", "i", "in", "range", "(", "steps", ")", ":", "\n", "            ", "offset", "=", "i", "+", "self", ".", "offset", "\n", "end", "=", "start", "+", "(", "tsz", "-", "offset", ")", "*", "bsz", "*", "copies", "\n", "pos_num", "=", "(", "end", "-", "start", ")", "//", "copies", "\n", "predictions", "[", "start", ":", "end", "]", "=", "(", "x", "[", "...", ",", ":", "-", "offset", ",", "i", "]", "*", "targets", "[", "...", ",", "offset", ":", "]", ")", ".", "sum", "(", "dim", "=", "2", ")", ".", "flatten", "(", ")", "\n", "labels", "[", "start", ":", "start", "+", "pos_num", "]", "=", "1.", "\n", "if", "weights", "is", "not", "None", ":", "\n", "                ", "weights", "[", "start", ":", "start", "+", "pos_num", "]", "=", "1.", "\n", "", "start", "=", "end", "\n", "", "assert", "end", "==", "predictions", ".", "numel", "(", ")", ",", "'{} != {}'", ".", "format", "(", "end", ",", "predictions", ".", "numel", "(", ")", ")", "\n", "\n", "if", "weights", "is", "not", "None", ":", "\n", "            ", "labels", "=", "(", "labels", ",", "weights", ")", "\n", "\n", "", "return", "predictions", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.wav2vec.norm_block": [[244, 255], ["torch.Sequential", "wav2vec.Fp32GroupNorm", "wav2vec.TransposeLast", "wav2vec.Fp32LayerNorm", "wav2vec.TransposeLast"], "function", ["None"], ["", "", "def", "norm_block", "(", "is_layer_norm", ",", "dim", ",", "affine", "=", "True", ")", ":", "\n", "    ", "if", "is_layer_norm", ":", "\n", "        ", "mod", "=", "nn", ".", "Sequential", "(", "\n", "TransposeLast", "(", ")", ",", "\n", "Fp32LayerNorm", "(", "dim", ",", "elementwise_affine", "=", "affine", ")", ",", "\n", "TransposeLast", "(", ")", ",", "\n", ")", "\n", "", "else", ":", "\n", "        ", "mod", "=", "Fp32GroupNorm", "(", "1", ",", "dim", ",", "affine", "=", "affine", ")", "\n", "\n", "", "return", "mod", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.wav2vec.base_wav2vec_architecture": [[437, 474], ["register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture"], ["", "", "@", "register_model_architecture", "(", "'wav2vec'", ",", "'wav2vec'", ")", "\n", "def", "base_wav2vec_architecture", "(", "args", ")", ":", "\n", "    ", "conv_feature_layers", "=", "'[(512, 10, 5)]'", "\n", "conv_feature_layers", "+=", "' + [(512, 8, 4)]'", "\n", "conv_feature_layers", "+=", "' + [(512, 4, 2)] * 3'", "\n", "args", ".", "conv_feature_layers", "=", "getattr", "(", "args", ",", "'conv_feature_layers'", ",", "conv_feature_layers", ")", "\n", "\n", "args", ".", "conv_aggregator_layers", "=", "getattr", "(", "args", ",", "'conv_aggregator_layers'", ",", "'[(512, 3, 1)] * 9'", ")", "\n", "\n", "args", ".", "prediction_steps", "=", "getattr", "(", "args", ",", "'prediction_steps'", ",", "12", ")", "\n", "args", ".", "num_negatives", "=", "getattr", "(", "args", ",", "'num_negatives'", ",", "1", ")", "\n", "args", ".", "sample_distance", "=", "getattr", "(", "args", ",", "'sample_distance'", ",", "None", ")", "\n", "args", ".", "cross_sample_negatives", "=", "getattr", "(", "args", ",", "'cross_sample_negatives'", ",", "False", ")", "\n", "\n", "args", ".", "dropout", "=", "getattr", "(", "args", ",", "'dropout'", ",", "0.", ")", "\n", "args", ".", "dropout_features", "=", "getattr", "(", "args", ",", "'dropout_features'", ",", "0.", ")", "\n", "args", ".", "dropout_agg", "=", "getattr", "(", "args", ",", "'dropout_agg'", ",", "0.", ")", "\n", "args", ".", "encoder", "=", "getattr", "(", "args", ",", "'encoder'", ",", "'cnn'", ")", "\n", "args", ".", "aggregator", "=", "getattr", "(", "args", ",", "'aggregator'", ",", "'cnn'", ")", "\n", "\n", "args", ".", "skip_connections_feat", "=", "getattr", "(", "args", ",", "'skip_connections_feat'", ",", "False", ")", "\n", "args", ".", "skip_connections_agg", "=", "getattr", "(", "args", ",", "'skip_connections_agg'", ",", "False", ")", "\n", "args", ".", "residual_scale", "=", "getattr", "(", "args", ",", "'residual_scale'", ",", "0.5", ")", "\n", "\n", "args", ".", "gru_dim", "=", "getattr", "(", "args", ",", "'gru_dim'", ",", "512", ")", "\n", "\n", "args", ".", "no_conv_bias", "=", "getattr", "(", "args", ",", "'no_conv_bias'", ",", "False", ")", "\n", "args", ".", "agg_zero_pad", "=", "getattr", "(", "args", ",", "'agg_zero_pad'", ",", "False", ")", "\n", "\n", "args", ".", "log_compression", "=", "getattr", "(", "args", ",", "'log_compression'", ",", "False", ")", "\n", "\n", "args", ".", "balanced_classes", "=", "getattr", "(", "args", ",", "'balanced_classes'", ",", "False", ")", "\n", "args", ".", "project_features", "=", "getattr", "(", "args", ",", "'project_features'", ",", "'none'", ")", "\n", "\n", "args", ".", "non_affine_group_norm", "=", "getattr", "(", "args", ",", "'non_affine_group_norm'", ",", "False", ")", "\n", "\n", "args", ".", "offset", "=", "getattr", "(", "args", ",", "'offset'", ",", "'auto'", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer_lm.TransformerLanguageModel.hub_models": [[27, 35], ["None"], "methods", ["None"], ["    ", "@", "classmethod", "\n", "def", "hub_models", "(", "cls", ")", ":", "\n", "        ", "return", "{", "\n", "'transformer_lm.gbw.adaptive_huge'", ":", "'https://dl.fbaipublicfiles.com/fairseq/models/lm/adaptive_lm_gbw_huge.tar.bz2'", ",", "\n", "'transformer_lm.wiki103.adaptive'", ":", "'https://dl.fbaipublicfiles.com/fairseq/models/lm/adaptive_lm_wiki103.tar.bz2'", ",", "\n", "'transformer_lm.wmt19.en'", ":", "'https://dl.fbaipublicfiles.com/fairseq/models/lm/wmt19.en.tar.bz2'", ",", "\n", "'transformer_lm.wmt19.de'", ":", "'https://dl.fbaipublicfiles.com/fairseq/models/lm/wmt19.de.tar.bz2'", ",", "\n", "'transformer_lm.wmt19.ru'", ":", "'https://dl.fbaipublicfiles.com/fairseq/models/lm/wmt19.ru.tar.bz2'", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer_lm.TransformerLanguageModel.__init__": [[37, 39], ["fairseq.models.FairseqLanguageModel.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["", "def", "__init__", "(", "self", ",", "decoder", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "decoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer_lm.TransformerLanguageModel.add_args": [[40, 101], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "fairseq.utils.get_available_activation_fns"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.get_available_activation_fns"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add model-specific arguments to the parser.\"\"\"", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'--activation-fn'", ",", "\n", "choices", "=", "utils", ".", "get_available_activation_fns", "(", ")", ",", "\n", "help", "=", "'activation function to use'", ")", "\n", "parser", ".", "add_argument", "(", "'--dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability'", ")", "\n", "parser", ".", "add_argument", "(", "'--attention-dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability for attention weights'", ")", "\n", "parser", ".", "add_argument", "(", "'--activation-dropout'", ",", "'--relu-dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability after activation in FFN.'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-output-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder output dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-input-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder input dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-ffn-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder embedding dimension for FFN'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-layers'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'num decoder layers'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-attention-heads'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'num decoder attention heads'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-normalize-before'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'apply layernorm before each decoder block'", ")", "\n", "parser", ".", "add_argument", "(", "'--no-decoder-final-norm'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'don\\'t add an extra layernorm after the last decoder block'", ")", "\n", "parser", ".", "add_argument", "(", "'--adaptive-softmax-cutoff'", ",", "metavar", "=", "'EXPR'", ",", "\n", "help", "=", "'comma separated list of adaptive softmax cutoff points. '", "\n", "'Must be used with adaptive_loss criterion'", ")", "\n", "parser", ".", "add_argument", "(", "'--adaptive-softmax-dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'sets adaptive softmax dropout for the tail projections'", ")", "\n", "parser", ".", "add_argument", "(", "'--adaptive-softmax-factor'", ",", "type", "=", "float", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'adaptive input factor'", ")", "\n", "parser", ".", "add_argument", "(", "'--no-token-positional-embeddings'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'if set, disables positional embeddings (outside self attention)'", ")", "\n", "parser", ".", "add_argument", "(", "'--share-decoder-input-output-embed'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'share decoder input and output embeddings'", ")", "\n", "parser", ".", "add_argument", "(", "'--character-embeddings'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'if set, uses character embedding convolutions to produce token embeddings'", ")", "\n", "parser", ".", "add_argument", "(", "'--character-filters'", ",", "type", "=", "str", ",", "metavar", "=", "'LIST'", ",", "\n", "default", "=", "'[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]'", ",", "\n", "help", "=", "'size of character embeddings'", ")", "\n", "parser", ".", "add_argument", "(", "'--character-embedding-dim'", ",", "default", "=", "4", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'size of character embeddings'", ")", "\n", "parser", ".", "add_argument", "(", "'--char-embedder-highway-layers'", ",", "default", "=", "2", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'number of highway layers for character token embeddder'", ")", "\n", "parser", ".", "add_argument", "(", "'--adaptive-input'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'if set, uses adaptive input'", ")", "\n", "parser", ".", "add_argument", "(", "'--adaptive-input-factor'", ",", "type", "=", "float", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'adaptive input factor'", ")", "\n", "parser", ".", "add_argument", "(", "'--adaptive-input-cutoff'", ",", "metavar", "=", "'EXPR'", ",", "\n", "help", "=", "'comma separated list of adaptive input cutoff points.'", ")", "\n", "parser", ".", "add_argument", "(", "'--tie-adaptive-weights'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'if set, ties the weights of adaptive softmax and adaptive input'", ")", "\n", "parser", ".", "add_argument", "(", "'--tie-adaptive-proj'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'if set, ties the projection weights of adaptive softmax and adaptive input'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-learned-pos'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'use learned positional embeddings in the decoder'", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer_lm.TransformerLanguageModel.build_model": [[103, 139], ["transformer_lm.base_lm_architecture", "fairseq.models.transformer.TransformerDecoder", "transformer_lm.TransformerLanguageModel", "getattr", "getattr", "fairseq.modules.CharacterTokenEmbedder", "eval", "fairseq.modules.AdaptiveInput", "fairseq.models.transformer.Embedding", "len", "task.source_dictionary.pad", "fairseq.options.eval_str_list", "len", "task.source_dictionary.pad"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv_lm.base_lm_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.Embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.eval_str_list", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad"], ["", "@", "classmethod", "\n", "def", "build_model", "(", "cls", ",", "args", ",", "task", ")", ":", "\n", "        ", "\"\"\"Build a new model instance.\"\"\"", "\n", "\n", "# make sure all arguments are present in older models", "\n", "base_lm_architecture", "(", "args", ")", "\n", "\n", "if", "getattr", "(", "args", ",", "'max_target_positions'", ",", "None", ")", "is", "None", ":", "\n", "            ", "args", ".", "max_target_positions", "=", "getattr", "(", "args", ",", "'tokens_per_sample'", ",", "DEFAULT_MAX_TARGET_POSITIONS", ")", "\n", "\n", "", "if", "args", ".", "character_embeddings", ":", "\n", "            ", "embed_tokens", "=", "CharacterTokenEmbedder", "(", "\n", "task", ".", "source_dictionary", ",", "eval", "(", "args", ".", "character_filters", ")", ",", "\n", "args", ".", "character_embedding_dim", ",", "args", ".", "decoder_embed_dim", ",", "\n", "args", ".", "char_embedder_highway_layers", ",", "\n", ")", "\n", "", "elif", "args", ".", "adaptive_input", ":", "\n", "            ", "embed_tokens", "=", "AdaptiveInput", "(", "\n", "len", "(", "task", ".", "source_dictionary", ")", ",", "task", ".", "source_dictionary", ".", "pad", "(", ")", ",", "args", ".", "decoder_input_dim", ",", "\n", "args", ".", "adaptive_input_factor", ",", "args", ".", "decoder_embed_dim", ",", "\n", "options", ".", "eval_str_list", "(", "args", ".", "adaptive_input_cutoff", ",", "type", "=", "int", ")", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "embed_tokens", "=", "Embedding", "(", "len", "(", "task", ".", "source_dictionary", ")", ",", "args", ".", "decoder_input_dim", ",", "task", ".", "source_dictionary", ".", "pad", "(", ")", ")", "\n", "\n", "", "if", "args", ".", "tie_adaptive_weights", ":", "\n", "            ", "assert", "args", ".", "adaptive_input", "\n", "assert", "args", ".", "adaptive_input_factor", "==", "args", ".", "adaptive_softmax_factor", "\n", "assert", "args", ".", "adaptive_softmax_cutoff", "==", "args", ".", "adaptive_input_cutoff", ",", "'{} != {}'", ".", "format", "(", "\n", "args", ".", "adaptive_softmax_cutoff", ",", "args", ".", "adaptive_input_cutoff", ")", "\n", "assert", "args", ".", "decoder_input_dim", "==", "args", ".", "decoder_output_dim", "\n", "\n", "", "decoder", "=", "TransformerDecoder", "(", "\n", "args", ",", "task", ".", "target_dictionary", ",", "embed_tokens", ",", "no_encoder_attn", "=", "True", ",", "\n", ")", "\n", "return", "TransformerLanguageModel", "(", "decoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer_lm.base_lm_architecture": [[141, 184], ["fairseq.models.register_model_architecture", "hasattr", "hasattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture"], ["", "", "@", "register_model_architecture", "(", "'transformer_lm'", ",", "'transformer_lm'", ")", "\n", "def", "base_lm_architecture", "(", "args", ")", ":", "\n", "# backward compatibility for older model checkpoints", "\n", "    ", "if", "hasattr", "(", "args", ",", "'no_tie_adaptive_proj'", ")", ":", "\n", "# previous models defined --no-tie-adaptive-proj, so use the existence of", "\n", "# that option to determine if this is an \"old\" model checkpoint", "\n", "        ", "args", ".", "no_decoder_final_norm", "=", "True", "# old models always set this to True", "\n", "if", "args", ".", "no_tie_adaptive_proj", "is", "False", ":", "\n", "            ", "args", ".", "tie_adaptive_proj", "=", "True", "\n", "", "", "if", "hasattr", "(", "args", ",", "'decoder_final_norm'", ")", ":", "\n", "        ", "args", ".", "no_decoder_final_norm", "=", "not", "args", ".", "decoder_final_norm", "\n", "\n", "", "args", ".", "dropout", "=", "getattr", "(", "args", ",", "'dropout'", ",", "0.1", ")", "\n", "args", ".", "attention_dropout", "=", "getattr", "(", "args", ",", "'attention_dropout'", ",", "0.0", ")", "\n", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "512", ")", "\n", "args", ".", "decoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_ffn_embed_dim'", ",", "2048", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "6", ")", "\n", "args", ".", "decoder_attention_heads", "=", "getattr", "(", "args", ",", "'decoder_attention_heads'", ",", "8", ")", "\n", "args", ".", "adaptive_softmax_cutoff", "=", "getattr", "(", "args", ",", "'adaptive_softmax_cutoff'", ",", "None", ")", "\n", "args", ".", "adaptive_softmax_dropout", "=", "getattr", "(", "args", ",", "'adaptive_softmax_dropout'", ",", "0", ")", "\n", "args", ".", "adaptive_softmax_factor", "=", "getattr", "(", "args", ",", "'adaptive_softmax_factor'", ",", "4", ")", "\n", "args", ".", "decoder_learned_pos", "=", "getattr", "(", "args", ",", "'decoder_learned_pos'", ",", "False", ")", "\n", "args", ".", "activation_fn", "=", "getattr", "(", "args", ",", "'activation_fn'", ",", "'relu'", ")", "\n", "\n", "args", ".", "add_bos_token", "=", "getattr", "(", "args", ",", "'add_bos_token'", ",", "False", ")", "\n", "args", ".", "no_token_positional_embeddings", "=", "getattr", "(", "args", ",", "'no_token_positional_embeddings'", ",", "False", ")", "\n", "args", ".", "share_decoder_input_output_embed", "=", "getattr", "(", "args", ",", "'share_decoder_input_output_embed'", ",", "False", ")", "\n", "args", ".", "character_embeddings", "=", "getattr", "(", "args", ",", "'character_embeddings'", ",", "False", ")", "\n", "\n", "args", ".", "decoder_output_dim", "=", "getattr", "(", "args", ",", "'decoder_output_dim'", ",", "args", ".", "decoder_embed_dim", ")", "\n", "args", ".", "decoder_input_dim", "=", "getattr", "(", "args", ",", "'decoder_input_dim'", ",", "args", ".", "decoder_embed_dim", ")", "\n", "\n", "# Model training is not stable without this", "\n", "args", ".", "decoder_normalize_before", "=", "True", "\n", "args", ".", "no_decoder_final_norm", "=", "getattr", "(", "args", ",", "'no_decoder_final_norm'", ",", "False", ")", "\n", "\n", "args", ".", "adaptive_input", "=", "getattr", "(", "args", ",", "'adaptive_input'", ",", "False", ")", "\n", "args", ".", "adaptive_input_factor", "=", "getattr", "(", "args", ",", "'adaptive_input_factor'", ",", "4", ")", "\n", "args", ".", "adaptive_input_cutoff", "=", "getattr", "(", "args", ",", "'adaptive_input_cutoff'", ",", "None", ")", "\n", "\n", "args", ".", "tie_adaptive_weights", "=", "getattr", "(", "args", ",", "'tie_adaptive_weights'", ",", "False", ")", "\n", "args", ".", "tie_adaptive_proj", "=", "getattr", "(", "args", ",", "'tie_adaptive_proj'", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer_lm.transformer_lm_big": [[186, 193], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "transformer_lm.base_lm_architecture"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv_lm.base_lm_architecture"], ["", "@", "register_model_architecture", "(", "'transformer_lm'", ",", "'transformer_lm_big'", ")", "\n", "def", "transformer_lm_big", "(", "args", ")", ":", "\n", "    ", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "12", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "1024", ")", "\n", "args", ".", "decoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_ffn_embed_dim'", ",", "4096", ")", "\n", "args", ".", "decoder_attention_heads", "=", "getattr", "(", "args", ",", "'decoder_attention_heads'", ",", "16", ")", "\n", "base_lm_architecture", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer_lm.transformer_lm_baevski_wiki103": [[195, 211], ["fairseq.models.register_model_architecture", "fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "transformer_lm.transformer_lm_big"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer_lm.transformer_lm_big"], ["", "@", "register_model_architecture", "(", "'transformer_lm'", ",", "'transformer_lm_wiki103'", ")", "\n", "@", "register_model_architecture", "(", "'transformer_lm'", ",", "'transformer_lm_baevski_wiki103'", ")", "\n", "def", "transformer_lm_baevski_wiki103", "(", "args", ")", ":", "\n", "    ", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "16", ")", "\n", "args", ".", "decoder_attention_heads", "=", "getattr", "(", "args", ",", "'decoder_attention_heads'", ",", "8", ")", "\n", "args", ".", "dropout", "=", "getattr", "(", "args", ",", "'dropout'", ",", "0.3", ")", "\n", "args", ".", "adaptive_input", "=", "getattr", "(", "args", ",", "'adaptive_input'", ",", "True", ")", "\n", "args", ".", "tie_adaptive_weights", "=", "getattr", "(", "args", ",", "'tie_adaptive_weights'", ",", "True", ")", "\n", "args", ".", "adaptive_input_cutoff", "=", "getattr", "(", "args", ",", "'adaptive_input_cutoff'", ",", "'20000,60000'", ")", "\n", "args", ".", "adaptive_softmax_cutoff", "=", "getattr", "(", "args", ",", "'adaptive_softmax_cutoff'", ",", "'20000,60000'", ")", "\n", "args", ".", "adaptive_softmax_dropout", "=", "getattr", "(", "args", ",", "'adaptive_softmax_dropout'", ",", "0.2", ")", "\n", "args", ".", "attention_dropout", "=", "getattr", "(", "args", ",", "'attention_dropout'", ",", "0.1", ")", "\n", "args", ".", "activation_dropout", "=", "getattr", "(", "args", ",", "'activation_dropout'", ",", "0.1", ")", "\n", "args", ".", "no_decoder_final_norm", "=", "getattr", "(", "args", ",", "'no_decoder_final_norm'", ",", "True", ")", "\n", "args", ".", "tie_adaptive_proj", "=", "getattr", "(", "args", ",", "'tie_adaptive_proj'", ",", "True", ")", "\n", "transformer_lm_big", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer_lm.transformer_lm_baevski_gbw": [[213, 221], ["fairseq.models.register_model_architecture", "fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "transformer_lm.transformer_lm_big"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer_lm.transformer_lm_big"], ["", "@", "register_model_architecture", "(", "'transformer_lm'", ",", "'transformer_lm_gbw'", ")", "\n", "@", "register_model_architecture", "(", "'transformer_lm'", ",", "'transformer_lm_baevski_gbw'", ")", "\n", "def", "transformer_lm_baevski_gbw", "(", "args", ")", ":", "\n", "    ", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "512", ")", "\n", "args", ".", "dropout", "=", "getattr", "(", "args", ",", "'dropout'", ",", "0.1", ")", "\n", "args", ".", "attention_dropout", "=", "getattr", "(", "args", ",", "'attention_dropout'", ",", "0.1", ")", "\n", "args", ".", "no_decoder_final_norm", "=", "getattr", "(", "args", ",", "'no_decoder_final_norm'", ",", "True", ")", "\n", "transformer_lm_big", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer_lm.transformer_lm_gpt": [[223, 233], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "transformer_lm.base_lm_architecture"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv_lm.base_lm_architecture"], ["", "@", "register_model_architecture", "(", "'transformer_lm'", ",", "'transformer_lm_gpt'", ")", "\n", "def", "transformer_lm_gpt", "(", "args", ")", ":", "\n", "    ", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "768", ")", "\n", "args", ".", "decoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_ffn_embed_dim'", ",", "3072", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "12", ")", "\n", "args", ".", "decoder_attention_heads", "=", "getattr", "(", "args", ",", "'decoder_attention_heads'", ",", "12", ")", "\n", "args", ".", "dropout", "=", "getattr", "(", "args", ",", "'dropout'", ",", "0.1", ")", "\n", "args", ".", "attention_dropout", "=", "getattr", "(", "args", ",", "'attention_dropout'", ",", "0.1", ")", "\n", "args", ".", "activation_fn", "=", "getattr", "(", "args", ",", "'activation_fn'", ",", "'gelu'", ")", "\n", "base_lm_architecture", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer_lm.transformer_lm_gpt2_small": [[235, 245], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "transformer_lm.base_lm_architecture"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv_lm.base_lm_architecture"], ["", "@", "register_model_architecture", "(", "'transformer_lm'", ",", "'transformer_lm_gpt2_small'", ")", "\n", "def", "transformer_lm_gpt2_small", "(", "args", ")", ":", "\n", "    ", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "1024", ")", "\n", "args", ".", "decoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_ffn_embed_dim'", ",", "4096", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "24", ")", "\n", "args", ".", "decoder_attention_heads", "=", "getattr", "(", "args", ",", "'decoder_attention_heads'", ",", "16", ")", "\n", "args", ".", "dropout", "=", "getattr", "(", "args", ",", "'dropout'", ",", "0.1", ")", "\n", "args", ".", "attention_dropout", "=", "getattr", "(", "args", ",", "'attention_dropout'", ",", "0.1", ")", "\n", "args", ".", "activation_fn", "=", "getattr", "(", "args", ",", "'activation_fn'", ",", "'gelu'", ")", "\n", "base_lm_architecture", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer_lm.transformer_lm_gpt2_medium": [[247, 257], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "transformer_lm.base_lm_architecture"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv_lm.base_lm_architecture"], ["", "@", "register_model_architecture", "(", "'transformer_lm'", ",", "'transformer_lm_gpt2_medium'", ")", "\n", "def", "transformer_lm_gpt2_medium", "(", "args", ")", ":", "\n", "    ", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "1280", ")", "\n", "args", ".", "decoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_ffn_embed_dim'", ",", "5120", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "36", ")", "\n", "args", ".", "decoder_attention_heads", "=", "getattr", "(", "args", ",", "'decoder_attention_heads'", ",", "20", ")", "\n", "args", ".", "dropout", "=", "getattr", "(", "args", ",", "'dropout'", ",", "0.1", ")", "\n", "args", ".", "attention_dropout", "=", "getattr", "(", "args", ",", "'attention_dropout'", ",", "0.1", ")", "\n", "args", ".", "activation_fn", "=", "getattr", "(", "args", ",", "'activation_fn'", ",", "'gelu'", ")", "\n", "base_lm_architecture", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer_lm.transformer_lm_gpt2_big": [[259, 269], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "transformer_lm.base_lm_architecture"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv_lm.base_lm_architecture"], ["", "@", "register_model_architecture", "(", "'transformer_lm'", ",", "'transformer_lm_gpt2_big'", ")", "\n", "def", "transformer_lm_gpt2_big", "(", "args", ")", ":", "\n", "    ", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "1600", ")", "\n", "args", ".", "decoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_ffn_embed_dim'", ",", "6400", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "48", ")", "\n", "args", ".", "decoder_attention_heads", "=", "getattr", "(", "args", ",", "'decoder_attention_heads'", ",", "25", ")", "\n", "args", ".", "dropout", "=", "getattr", "(", "args", ",", "'dropout'", ",", "0.1", ")", "\n", "args", ".", "attention_dropout", "=", "getattr", "(", "args", ",", "'attention_dropout'", ",", "0.1", ")", "\n", "args", ".", "activation_fn", "=", "getattr", "(", "args", ",", "'activation_fn'", ",", "'gelu'", ")", "\n", "base_lm_architecture", "(", "args", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.build_model": [[47, 49], ["ARCH_MODEL_REGISTRY[].build_model"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv_lm.LightConvLanguageModel.build_model"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model": [[51, 80], ["ValueError", "issubclass", "ValueError"], "function", ["None"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture": [[82, 120], ["ARCH_MODEL_INV_REGISTRY.setdefault().append", "ValueError", "ValueError", "callable", "ValueError", "ARCH_MODEL_INV_REGISTRY.setdefault"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.composite_encoder.CompositeEncoder.__init__": [[20, 25], ["fairseq.models.FairseqEncoder.__init__", "composite_encoder.CompositeEncoder.add_module", "next", "iter", "encoders.values"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["def", "__init__", "(", "self", ",", "encoders", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "next", "(", "iter", "(", "encoders", ".", "values", "(", ")", ")", ")", ".", "dictionary", ")", "\n", "self", ".", "encoders", "=", "encoders", "\n", "for", "key", "in", "self", ".", "encoders", ":", "\n", "            ", "self", ".", "add_module", "(", "key", ",", "self", ".", "encoders", "[", "key", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.composite_encoder.CompositeEncoder.forward": [[26, 42], ["None"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "src_tokens", ",", "src_lengths", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            src_tokens (LongTensor): tokens in the source language of shape\n                `(batch, src_len)`\n            src_lengths (LongTensor): lengths of each source sentence of shape\n                `(batch)`\n\n        Returns:\n            dict:\n                the outputs from each Encoder\n        \"\"\"", "\n", "encoder_out", "=", "{", "}", "\n", "for", "key", "in", "self", ".", "encoders", ":", "\n", "            ", "encoder_out", "[", "key", "]", "=", "self", ".", "encoders", "[", "key", "]", "(", "src_tokens", ",", "src_lengths", ")", "\n", "", "return", "encoder_out", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.composite_encoder.CompositeEncoder.reorder_encoder_out": [[43, 48], ["composite_encoder.CompositeEncoder.encoders[].reorder_encoder_out"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.sequence_generator.EnsembleModel.reorder_encoder_out"], ["", "def", "reorder_encoder_out", "(", "self", ",", "encoder_out", ",", "new_order", ")", ":", "\n", "        ", "\"\"\"Reorder encoder output according to new_order.\"\"\"", "\n", "for", "key", "in", "self", ".", "encoders", ":", "\n", "            ", "encoder_out", "[", "key", "]", "=", "self", ".", "encoders", "[", "key", "]", ".", "reorder_encoder_out", "(", "encoder_out", "[", "key", "]", ",", "new_order", ")", "\n", "", "return", "encoder_out", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.composite_encoder.CompositeEncoder.max_positions": [[49, 51], ["min", "composite_encoder.CompositeEncoder.encoders[].max_positions"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.max_positions"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "return", "min", "(", "[", "self", ".", "encoders", "[", "key", "]", ".", "max_positions", "(", ")", "for", "key", "in", "self", ".", "encoders", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.composite_encoder.CompositeEncoder.upgrade_state_dict": [[52, 56], ["composite_encoder.CompositeEncoder.encoders[].upgrade_state_dict"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_decoder.FairseqDecoder.upgrade_state_dict"], ["", "def", "upgrade_state_dict", "(", "self", ",", "state_dict", ")", ":", "\n", "        ", "for", "key", "in", "self", ".", "encoders", ":", "\n", "            ", "self", ".", "encoders", "[", "key", "]", ".", "upgrade_state_dict", "(", "state_dict", ")", "\n", "", "return", "state_dict", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_decoder.FairseqDecoder.__init__": [[14, 18], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["def", "__init__", "(", "self", ",", "dictionary", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dictionary", "=", "dictionary", "\n", "self", ".", "onnx_trace", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_decoder.FairseqDecoder.forward": [[19, 35], ["fairseq_decoder.FairseqDecoder.extract_features", "fairseq_decoder.FairseqDecoder.output_layer"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.extract_features", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.TransformerDecoder.output_layer"], ["", "def", "forward", "(", "self", ",", "prev_output_tokens", ",", "encoder_out", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            prev_output_tokens (LongTensor): shifted output tokens of shape\n                `(batch, tgt_len)`, for teacher forcing\n            encoder_out (dict, optional): output from the encoder, used for\n                encoder-side attention\n\n        Returns:\n            tuple:\n                - the decoder's output of shape `(batch, tgt_len, vocab)`\n                - a dictionary with any model-specific outputs\n        \"\"\"", "\n", "x", ",", "extra", "=", "self", ".", "extract_features", "(", "prev_output_tokens", ",", "encoder_out", "=", "encoder_out", ",", "**", "kwargs", ")", "\n", "x", "=", "self", ".", "output_layer", "(", "x", ")", "\n", "return", "x", ",", "extra", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_decoder.FairseqDecoder.extract_features": [[36, 44], ["None"], "methods", ["None"], ["", "def", "extract_features", "(", "self", ",", "prev_output_tokens", ",", "encoder_out", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Returns:\n            tuple:\n                - the decoder's features of shape `(batch, tgt_len, embed_dim)`\n                - a dictionary with any model-specific outputs\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_decoder.FairseqDecoder.output_layer": [[45, 53], ["None"], "methods", ["None"], ["", "def", "output_layer", "(", "self", ",", "features", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Project features to the default output size, e.g., vocabulary size.\n\n        Args:\n            features (Tensor): features returned by *extract_features*.\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_decoder.FairseqDecoder.get_normalized_probs": [[54, 71], ["hasattr", "fairseq_decoder.FairseqDecoder.adaptive_softmax.get_log_prob", "fairseq.utils.log_softmax", "fairseq.utils.softmax", "fairseq_decoder.FairseqDecoder.exp_"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.adaptive_softmax.AdaptiveSoftmax.get_log_prob", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.log_softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax"], ["", "def", "get_normalized_probs", "(", "self", ",", "net_output", ",", "log_probs", ",", "sample", ")", ":", "\n", "        ", "\"\"\"Get normalized probabilities (or log probs) from a net's output.\"\"\"", "\n", "\n", "if", "hasattr", "(", "self", ",", "'adaptive_softmax'", ")", "and", "self", ".", "adaptive_softmax", "is", "not", "None", ":", "\n", "            ", "if", "sample", "is", "not", "None", ":", "\n", "                ", "assert", "'target'", "in", "sample", "\n", "target", "=", "sample", "[", "'target'", "]", "\n", "", "else", ":", "\n", "                ", "target", "=", "None", "\n", "", "out", "=", "self", ".", "adaptive_softmax", ".", "get_log_prob", "(", "net_output", "[", "0", "]", ",", "target", "=", "target", ")", "\n", "return", "out", ".", "exp_", "(", ")", "if", "not", "log_probs", "else", "out", "\n", "\n", "", "logits", "=", "net_output", "[", "0", "]", "\n", "if", "log_probs", ":", "\n", "            ", "return", "utils", ".", "log_softmax", "(", "logits", ",", "dim", "=", "-", "1", ",", "onnx_trace", "=", "self", ".", "onnx_trace", ")", "\n", "", "else", ":", "\n", "            ", "return", "utils", ".", "softmax", "(", "logits", ",", "dim", "=", "-", "1", ",", "onnx_trace", "=", "self", ".", "onnx_trace", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_decoder.FairseqDecoder.max_positions": [[72, 75], ["None"], "methods", ["None"], ["", "", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum input length supported by the decoder.\"\"\"", "\n", "return", "1e6", "# an arbitrary large number", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_decoder.FairseqDecoder.upgrade_state_dict": [[76, 79], ["None"], "methods", ["None"], ["", "def", "upgrade_state_dict", "(", "self", ",", "state_dict", ")", ":", "\n", "        ", "\"\"\"Upgrade a (possibly old) state dict for new versions of fairseq.\"\"\"", "\n", "return", "state_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_decoder.FairseqDecoder.prepare_for_onnx_export_": [[80, 82], ["None"], "methods", ["None"], ["", "def", "prepare_for_onnx_export_", "(", "self", ")", ":", "\n", "        ", "self", ".", "onnx_trace", "=", "True", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.TransformerModel.hub_models": [[56, 71], ["None"], "methods", ["None"], ["@", "classmethod", "\n", "def", "hub_models", "(", "cls", ")", ":", "\n", "# fmt: off", "\n", "        ", "return", "{", "\n", "'transformer.wmt14.en-fr'", ":", "'https://dl.fbaipublicfiles.com/fairseq/models/wmt14.en-fr.joined-dict.transformer.tar.bz2'", ",", "\n", "'transformer.wmt16.en-de'", ":", "'https://dl.fbaipublicfiles.com/fairseq/models/wmt16.en-de.joined-dict.transformer.tar.bz2'", ",", "\n", "'transformer.wmt18.en-de'", ":", "'https://dl.fbaipublicfiles.com/fairseq/models/wmt18.en-de.ensemble.tar.gz'", ",", "\n", "'transformer.wmt19.en-de'", ":", "'https://dl.fbaipublicfiles.com/fairseq/models/wmt19.en-de.joined-dict.ensemble.tar.gz'", ",", "\n", "'transformer.wmt19.en-ru'", ":", "'https://dl.fbaipublicfiles.com/fairseq/models/wmt19.en-ru.ensemble.tar.gz'", ",", "\n", "'transformer.wmt19.de-en'", ":", "'https://dl.fbaipublicfiles.com/fairseq/models/wmt19.de-en.joined-dict.ensemble.tar.gz'", ",", "\n", "'transformer.wmt19.ru-en'", ":", "'https://dl.fbaipublicfiles.com/fairseq/models/wmt19.ru-en.ensemble.tar.gz'", ",", "\n", "'transformer.wmt19.en-de.single_model'", ":", "'https://dl.fbaipublicfiles.com/fairseq/models/wmt19.en-de.joined-dict.single_model.tar.gz'", ",", "\n", "'transformer.wmt19.en-ru.single_model'", ":", "'https://dl.fbaipublicfiles.com/fairseq/models/wmt19.en-ru.single_model.tar.gz'", ",", "\n", "'transformer.wmt19.de-en.single_model'", ":", "'https://dl.fbaipublicfiles.com/fairseq/models/wmt19.de-en.joined-dict.single_model.tar.gz'", ",", "\n", "'transformer.wmt19.ru-en.single_model'", ":", "'https://dl.fbaipublicfiles.com/fairseq/models/wmt19.ru-en.single_model.tar.gz'", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.TransformerModel.__init__": [[74, 76], ["fairseq.models.FairseqEncoderDecoderModel.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["", "def", "__init__", "(", "self", ",", "encoder", ",", "decoder", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "encoder", ",", "decoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.TransformerModel.add_args": [[77, 142], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "fairseq.utils.get_available_activation_fns"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.get_available_activation_fns"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add model-specific arguments to the parser.\"\"\"", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'--activation-fn'", ",", "\n", "choices", "=", "utils", ".", "get_available_activation_fns", "(", ")", ",", "\n", "help", "=", "'activation function to use'", ")", "\n", "parser", ".", "add_argument", "(", "'--dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability'", ")", "\n", "parser", ".", "add_argument", "(", "'--attention-dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability for attention weights'", ")", "\n", "parser", ".", "add_argument", "(", "'--activation-dropout'", ",", "'--relu-dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability after activation in FFN.'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-embed-path'", ",", "type", "=", "str", ",", "metavar", "=", "'STR'", ",", "\n", "help", "=", "'path to pre-trained encoder embedding'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'encoder embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-ffn-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'encoder embedding dimension for FFN'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-layers'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'num encoder layers'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-attention-heads'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'num encoder attention heads'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-normalize-before'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'apply layernorm before each encoder block'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-learned-pos'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'use learned positional embeddings in the encoder'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-embed-path'", ",", "type", "=", "str", ",", "metavar", "=", "'STR'", ",", "\n", "help", "=", "'path to pre-trained decoder embedding'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-ffn-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder embedding dimension for FFN'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-layers'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'num decoder layers'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-attention-heads'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'num decoder attention heads'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-learned-pos'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'use learned positional embeddings in the decoder'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-normalize-before'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'apply layernorm before each decoder block'", ")", "\n", "parser", ".", "add_argument", "(", "'--share-decoder-input-output-embed'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'share decoder input and output embeddings'", ")", "\n", "parser", ".", "add_argument", "(", "'--share-all-embeddings'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'share encoder, decoder and output embeddings'", "\n", "' (requires shared dictionary and embed dim)'", ")", "\n", "parser", ".", "add_argument", "(", "'--no-token-positional-embeddings'", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'if set, disables positional embeddings (outside self attention)'", ")", "\n", "parser", ".", "add_argument", "(", "'--adaptive-softmax-cutoff'", ",", "metavar", "=", "'EXPR'", ",", "\n", "help", "=", "'comma separated list of adaptive softmax cutoff points. '", "\n", "'Must be used with adaptive_loss criterion'", ")", ",", "\n", "parser", ".", "add_argument", "(", "'--adaptive-softmax-dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'sets adaptive softmax dropout for the tail projections'", ")", "\n", "# Use stack transformer", "\n", "parser", ".", "add_argument", "(", "'--encode-state-machine'", ",", "type", "=", "bool", ",", "\n", "help", "=", "'controls encoding of stack and buffer'", ")", "\n", "# control BERT backprop", "\n", "parser", ".", "add_argument", "(", "'--bert-backprop'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Backpropagate through BERT'", ",", "default", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'--no-bert-precompute'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Compute BERT on the fly (debugging)'", ",", "\n", "default", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'--pretrained-embed-dim'", ",", "type", "=", "int", ",", "\n", "help", "=", "'Peetrained embeddings size'", ",", "\n", "default", "=", "768", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.TransformerModel.build_model": [[145, 194], ["transformer.base_architecture", "cls.build_encoder", "cls.build_decoder", "transformer.TransformerModel", "hasattr", "hasattr", "len", "dictionary.pad", "transformer.Embedding", "transformer.TransformerModel.build_model.build_embedding"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.base_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.TransformerModel.build_encoder", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.TransformerModel.build_decoder", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.Embedding"], ["", "@", "classmethod", "\n", "def", "build_model", "(", "cls", ",", "args", ",", "task", ")", ":", "\n", "        ", "\"\"\"Build a new model instance.\"\"\"", "\n", "\n", "# make sure all arguments are present in older models", "\n", "base_architecture", "(", "args", ")", "\n", "\n", "if", "not", "hasattr", "(", "args", ",", "'max_source_positions'", ")", ":", "\n", "            ", "args", ".", "max_source_positions", "=", "DEFAULT_MAX_SOURCE_POSITIONS", "\n", "", "if", "not", "hasattr", "(", "args", ",", "'max_target_positions'", ")", ":", "\n", "            ", "args", ".", "max_target_positions", "=", "DEFAULT_MAX_TARGET_POSITIONS", "\n", "\n", "", "src_dict", ",", "tgt_dict", "=", "task", ".", "source_dictionary", ",", "task", ".", "target_dictionary", "\n", "\n", "def", "build_embedding", "(", "dictionary", ",", "embed_dim", ",", "path", "=", "None", ")", ":", "\n", "            ", "num_embeddings", "=", "len", "(", "dictionary", ")", "\n", "padding_idx", "=", "dictionary", ".", "pad", "(", ")", "\n", "emb", "=", "Embedding", "(", "num_embeddings", ",", "embed_dim", ",", "padding_idx", ")", "\n", "# if provided, load from preloaded dictionaries", "\n", "if", "path", ":", "\n", "                ", "embed_dict", "=", "utils", ".", "parse_embedding", "(", "path", ")", "\n", "utils", ".", "load_embedding", "(", "embed_dict", ",", "dictionary", ",", "emb", ")", "\n", "", "return", "emb", "\n", "\n", "", "if", "args", ".", "share_all_embeddings", ":", "\n", "            ", "if", "src_dict", "!=", "tgt_dict", ":", "\n", "                ", "raise", "ValueError", "(", "'--share-all-embeddings requires a joined dictionary'", ")", "\n", "", "if", "args", ".", "encoder_embed_dim", "!=", "args", ".", "decoder_embed_dim", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "'--share-all-embeddings requires --encoder-embed-dim to match --decoder-embed-dim'", ")", "\n", "", "if", "args", ".", "decoder_embed_path", "and", "(", "\n", "args", ".", "decoder_embed_path", "!=", "args", ".", "encoder_embed_path", ")", ":", "\n", "                ", "raise", "ValueError", "(", "'--share-all-embeddings not compatible with --decoder-embed-path'", ")", "\n", "", "encoder_embed_tokens", "=", "build_embedding", "(", "\n", "src_dict", ",", "args", ".", "encoder_embed_dim", ",", "args", ".", "encoder_embed_path", "\n", ")", "\n", "decoder_embed_tokens", "=", "encoder_embed_tokens", "\n", "args", ".", "share_decoder_input_output_embed", "=", "True", "\n", "", "else", ":", "\n", "            ", "encoder_embed_tokens", "=", "build_embedding", "(", "\n", "src_dict", ",", "args", ".", "encoder_embed_dim", ",", "args", ".", "encoder_embed_path", "\n", ")", "\n", "decoder_embed_tokens", "=", "build_embedding", "(", "\n", "tgt_dict", ",", "args", ".", "decoder_embed_dim", ",", "args", ".", "decoder_embed_path", "\n", ")", "\n", "\n", "", "encoder", "=", "cls", ".", "build_encoder", "(", "args", ",", "src_dict", ",", "encoder_embed_tokens", ")", "\n", "decoder", "=", "cls", ".", "build_decoder", "(", "args", ",", "tgt_dict", ",", "decoder_embed_tokens", ")", "\n", "return", "TransformerModel", "(", "encoder", ",", "decoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.TransformerModel.build_encoder": [[195, 198], ["transformer.TransformerEncoder"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "build_encoder", "(", "cls", ",", "args", ",", "src_dict", ",", "embed_tokens", ")", ":", "\n", "        ", "return", "TransformerEncoder", "(", "args", ",", "src_dict", ",", "embed_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.TransformerModel.build_decoder": [[199, 202], ["transformer.TransformerDecoder"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "build_decoder", "(", "cls", ",", "args", ",", "tgt_dict", ",", "embed_tokens", ")", ":", "\n", "        ", "return", "TransformerDecoder", "(", "args", ",", "tgt_dict", ",", "embed_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.TransformerEncoder.__init__": [[215, 257], ["fairseq.models.FairseqEncoder.__init__", "transformer.TransformerEncoder.register_buffer", "transformer.Linear", "math.sqrt", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "transformer.TransformerEncoder.layers.extend", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.hub.load", "torch.hub.load", "torch.hub.load", "torch.hub.load", "torch.hub.load", "torch.hub.load", "torch.hub.load", "torch.hub.load", "torch.hub.load", "torch.hub.load.cuda", "torch.hub.load.cuda", "torch.hub.load.cuda", "fairseq.modules.PositionalEmbedding", "fairseq.modules.LayerNorm", "fairseq.modules.TransformerEncoderLayer", "range"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.positional_embedding.PositionalEmbedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.layer_norm.LayerNorm"], ["def", "__init__", "(", "self", ",", "args", ",", "dictionary", ",", "embed_tokens", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ")", "\n", "self", ".", "register_buffer", "(", "'version'", ",", "torch", ".", "Tensor", "(", "[", "3", "]", ")", ")", "\n", "\n", "self", ".", "dropout", "=", "args", ".", "dropout", "\n", "self", ".", "bert_backprop", "=", "args", ".", "bert_backprop", "\n", "self", ".", "no_bert_precompute", "=", "args", ".", "no_bert_precompute", "\n", "\n", "# backprop needs on the fly extraction", "\n", "if", "self", ".", "bert_backprop", "or", "self", ".", "no_bert_precompute", ":", "\n", "            ", "roberta", "=", "torch", ".", "hub", ".", "load", "(", "'pytorch/fairseq'", ",", "'roberta.base'", ")", "\n", "roberta", ".", "cuda", "(", ")", "\n", "self", ".", "roberta", "=", "roberta", "\n", "# if args.no_bert_precompute:", "\n", "#    # Set BERT to purely evaluation mode", "\n", "#    self.roberta.eval()", "\n", "\n", "", "embed_dim", "=", "embed_tokens", ".", "embedding_dim", "\n", "self", ".", "padding_idx", "=", "embed_tokens", ".", "padding_idx", "\n", "self", ".", "max_source_positions", "=", "args", ".", "max_source_positions", "\n", "\n", "# BERT embeddings as input", "\n", "input_embed_dim", "=", "args", ".", "pretrained_embed_dim", "\n", "self", ".", "subspace", "=", "Linear", "(", "input_embed_dim", ",", "embed_dim", ",", "bias", "=", "False", ")", "\n", "\n", "self", ".", "embed_tokens", "=", "embed_tokens", "\n", "self", ".", "embed_scale", "=", "math", ".", "sqrt", "(", "embed_dim", ")", "\n", "self", ".", "embed_positions", "=", "PositionalEmbedding", "(", "\n", "args", ".", "max_source_positions", ",", "embed_dim", ",", "self", ".", "padding_idx", ",", "\n", "learned", "=", "args", ".", "encoder_learned_pos", ",", "\n", ")", "if", "not", "args", ".", "no_token_positional_embeddings", "else", "None", "\n", "\n", "self", ".", "layers", "=", "nn", ".", "ModuleList", "(", "[", "]", ")", "\n", "self", ".", "layers", ".", "extend", "(", "[", "\n", "TransformerEncoderLayer", "(", "args", ")", "\n", "for", "i", "in", "range", "(", "args", ".", "encoder_layers", ")", "\n", "]", ")", "\n", "\n", "if", "args", ".", "encoder_normalize_before", ":", "\n", "            ", "self", ".", "layer_norm", "=", "LayerNorm", "(", "embed_dim", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "layer_norm", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.TransformerEncoder.forward": [[258, 337], ["transformer.TransformerEncoder.subspace", "torch.dropout", "torch.dropout", "torch.dropout", "transformer.TransformerEncoder.transpose", "src_tokens.eq", "transformer.TransformerEncoder.roberta.extract_features", "last_layer[].view", "scatter_mean", "source_fix_emb2.flip.flip.flip", "src_tokens.eq.any", "layer", "transformer.TransformerEncoder.layer_norm", "src_wp2w.unsqueeze", "source_fix_emb2.flip.flip.detach"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.extract_features"], ["", "", "def", "forward", "(", "self", ",", "src_tokens", ",", "src_lengths", ",", "memory", ",", "memory_pos", ",", "source_fix_emb", ",", "src_wordpieces", ",", "src_wp2w", ",", "**", "unused", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            src_tokens (LongTensor): tokens in the source language of shape\n                `(batch, src_len)`\n            src_lengths (torch.LongTensor): lengths of each source sentence of\n                shape `(batch)`\n\n        Returns:\n            dict:\n                - **encoder_out** (Tensor): the last encoder layer's output of\n                  shape `(src_len, batch, embed_dim)`\n                - **encoder_padding_mask** (ByteTensor): the positions of\n                  padding elements of shape `(batch, src_len)`\n        \"\"\"", "\n", "\n", "# embed tokens and positions", "\n", "# x = self.embed_scale * self.embed_tokens(src_tokens)", "\n", "# if self.embed_positions is not None:", "\n", "#     x += self.embed_positions(src_tokens)", "\n", "\n", "if", "self", ".", "bert_backprop", "or", "self", ".", "no_bert_precompute", ":", "\n", "\n", "# This is moved here since it gives impor errors. Wont be installed", "\n", "# by default", "\n", "            ", "from", "torch_scatter", "import", "scatter_mean", "\n", "\n", "# extract roberta on the fly", "\n", "last_layer", "=", "self", ".", "roberta", ".", "extract_features", "(", "src_wordpieces", ")", "\n", "# remove sentence start", "\n", "bsize", ",", "max_len", ",", "emb_size", "=", "last_layer", ".", "shape", "\n", "mask", "=", "(", "src_wordpieces", "!=", "0", ")", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "last_layer", ".", "shape", ")", "\n", "last_layer", "=", "last_layer", "[", "mask", "]", ".", "view", "(", "(", "bsize", ",", "max_len", "-", "1", ",", "emb_size", ")", ")", "\n", "# remove sentence end", "\n", "last_layer", "=", "last_layer", "[", ":", ",", ":", "-", "1", ",", ":", "]", "\n", "# apply scatter, src_wp2w was inverted in pre-processing to use", "\n", "# scatter's left side padding . We need to flip the result.", "\n", "source_fix_emb2", "=", "scatter_mean", "(", "\n", "last_layer", ",", "\n", "src_wp2w", ".", "unsqueeze", "(", "2", ")", ",", "\n", "dim", "=", "1", "\n", ")", "\n", "source_fix_emb2", "=", "source_fix_emb2", ".", "flip", "(", "1", ")", "\n", "# Remove extra padding", "\n", "source_fix_emb2", "=", "source_fix_emb2", "[", ":", ",", "-", "src_tokens", ".", "shape", "[", "1", "]", ":", ",", ":", "]", "\n", "\n", "# do not backprop for on-the-fly computing", "\n", "if", "self", ".", "no_bert_precompute", ":", "\n", "                ", "bert_embeddings", "=", "source_fix_emb2", ".", "detach", "(", ")", "\n", "", "else", ":", "\n", "                ", "bert_embeddings", "=", "source_fix_emb2", "\n", "\n", "# DEBUG: check precomputed and on the fly sufficiently close", "\n", "# abs(source_fix_emb2 - source_fix_emb).max()", "\n", "", "", "else", ":", "\n", "# use pre-extracted roberta", "\n", "            ", "bert_embeddings", "=", "source_fix_emb", "\n", "\n", "", "x", "=", "self", ".", "subspace", "(", "bert_embeddings", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "\n", "# B x T x C -> T x B x C", "\n", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "# compute padding mask", "\n", "encoder_padding_mask", "=", "src_tokens", ".", "eq", "(", "self", ".", "padding_idx", ")", "\n", "if", "not", "encoder_padding_mask", ".", "any", "(", ")", ":", "\n", "            ", "encoder_padding_mask", "=", "None", "\n", "\n", "# encoder layers", "\n", "", "for", "layer", "in", "self", ".", "layers", ":", "\n", "            ", "x", "=", "layer", "(", "x", ",", "encoder_padding_mask", ")", "\n", "\n", "", "if", "self", ".", "layer_norm", ":", "\n", "            ", "x", "=", "self", ".", "layer_norm", "(", "x", ")", "\n", "\n", "", "return", "{", "\n", "'encoder_out'", ":", "x", ",", "# T x B x C", "\n", "'encoder_padding_mask'", ":", "encoder_padding_mask", ",", "# B x T", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.TransformerEncoder.reorder_encoder_out": [[339, 357], ["encoder_out[].index_select", "encoder_out[].index_select"], "methods", ["None"], ["", "def", "reorder_encoder_out", "(", "self", ",", "encoder_out", ",", "new_order", ")", ":", "\n", "        ", "\"\"\"\n        Reorder encoder output according to *new_order*.\n\n        Args:\n            encoder_out: output from the ``forward()`` method\n            new_order (LongTensor): desired order\n\n        Returns:\n            *encoder_out* rearranged according to *new_order*\n        \"\"\"", "\n", "if", "encoder_out", "[", "'encoder_out'", "]", "is", "not", "None", ":", "\n", "            ", "encoder_out", "[", "'encoder_out'", "]", "=", "encoder_out", "[", "'encoder_out'", "]", ".", "index_select", "(", "1", ",", "new_order", ")", "\n", "", "if", "encoder_out", "[", "'encoder_padding_mask'", "]", "is", "not", "None", ":", "\n", "            ", "encoder_out", "[", "'encoder_padding_mask'", "]", "=", "encoder_out", "[", "'encoder_padding_mask'", "]", ".", "index_select", "(", "0", ",", "new_order", ")", "\n", "", "return", "encoder_out", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.TransformerEncoder.max_positions": [[358, 363], ["min", "transformer.TransformerEncoder.embed_positions.max_positions"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.max_positions"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum input length supported by the encoder.\"\"\"", "\n", "if", "self", ".", "embed_positions", "is", "None", ":", "\n", "            ", "return", "self", ".", "max_source_positions", "\n", "", "return", "min", "(", "self", ".", "max_source_positions", ",", "self", ".", "embed_positions", ".", "max_positions", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.TransformerEncoder.upgrade_state_dict_named": [[364, 382], ["isinstance", "range", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "len", "transformer.TransformerEncoder.layers[].upgrade_state_dict_named", "fairseq.utils.item", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "state_dict.get", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.TransformerEncoderLayer.upgrade_state_dict_named", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item"], ["", "def", "upgrade_state_dict_named", "(", "self", ",", "state_dict", ",", "name", ")", ":", "\n", "        ", "\"\"\"Upgrade a (possibly old) state dict for new versions of fairseq.\"\"\"", "\n", "if", "isinstance", "(", "self", ".", "embed_positions", ",", "SinusoidalPositionalEmbedding", ")", ":", "\n", "            ", "weights_key", "=", "'{}.embed_positions.weights'", ".", "format", "(", "name", ")", "\n", "if", "weights_key", "in", "state_dict", ":", "\n", "                ", "del", "state_dict", "[", "weights_key", "]", "\n", "", "state_dict", "[", "'{}.embed_positions._float_tensor'", ".", "format", "(", "name", ")", "]", "=", "torch", ".", "FloatTensor", "(", "1", ")", "\n", "", "for", "i", "in", "range", "(", "len", "(", "self", ".", "layers", ")", ")", ":", "\n", "# update layer norms", "\n", "            ", "self", ".", "layers", "[", "i", "]", ".", "upgrade_state_dict_named", "(", "state_dict", ",", "\"{}.layers.{}\"", ".", "format", "(", "name", ",", "i", ")", ")", "\n", "\n", "", "version_key", "=", "'{}.version'", ".", "format", "(", "name", ")", "\n", "if", "utils", ".", "item", "(", "state_dict", ".", "get", "(", "version_key", ",", "torch", ".", "Tensor", "(", "[", "1", "]", ")", ")", "[", "0", "]", ")", "<", "2", ":", "\n", "# earlier checkpoints did not normalize after the stack of layers", "\n", "            ", "self", ".", "layer_norm", "=", "None", "\n", "self", ".", "normalize", "=", "False", "\n", "state_dict", "[", "version_key", "]", "=", "torch", ".", "Tensor", "(", "[", "1", "]", ")", "\n", "", "return", "state_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.TransformerDecoder.__init__": [[397, 460], ["fairseq.models.FairseqIncrementalDecoder.__init__", "transformer.TransformerDecoder.register_buffer", "math.sqrt", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "transformer.TransformerDecoder.layers.extend", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "fairseq.modules.PositionalEmbedding", "transformer.Linear", "fairseq.modules.PositionalEmbedding", "transformer.Linear", "fairseq.modules.AdaptiveSoftmax", "fairseq.modules.LayerNorm", "fairseq.modules.TransformerDecoderLayer", "len", "fairseq.options.eval_str_list", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.init.normal_", "torch.init.normal_", "torch.init.normal_", "getattr", "range", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "len"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.positional_embedding.PositionalEmbedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.positional_embedding.PositionalEmbedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.layer_norm.LayerNorm", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.eval_str_list"], ["def", "__init__", "(", "self", ",", "args", ",", "dictionary", ",", "embed_tokens", ",", "no_encoder_attn", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ")", "\n", "self", ".", "register_buffer", "(", "'version'", ",", "torch", ".", "Tensor", "(", "[", "3", "]", ")", ")", "\n", "\n", "self", ".", "dropout", "=", "args", ".", "dropout", "\n", "self", ".", "share_input_output_embed", "=", "args", ".", "share_decoder_input_output_embed", "\n", "\n", "input_embed_dim", "=", "embed_tokens", ".", "embedding_dim", "\n", "embed_dim", "=", "args", ".", "decoder_embed_dim", "\n", "self", ".", "output_embed_dim", "=", "args", ".", "decoder_output_dim", "\n", "\n", "padding_idx", "=", "embed_tokens", ".", "padding_idx", "\n", "self", ".", "max_target_positions", "=", "args", ".", "max_target_positions", "\n", "\n", "self", ".", "embed_tokens", "=", "embed_tokens", "\n", "self", ".", "embed_scale", "=", "math", ".", "sqrt", "(", "embed_dim", ")", "# todo: try with input_embed_dim", "\n", "\n", "# controls the use of stack transformer", "\n", "self", ".", "encode_state_machine", "=", "args", ".", "encode_state_machine", "\n", "\n", "if", "self", ".", "encode_state_machine", ":", "\n", "# positions of buffer and stack for each time step", "\n", "            ", "self", ".", "embed_stack_positions", "=", "PositionalEmbedding", "(", "\n", "args", ".", "max_target_positions", ",", "args", ".", "decoder_embed_dim", ",", "\n", "padding_idx", ",", "learned", "=", "args", ".", "decoder_learned_pos", ",", "\n", ")", "\n", "\n", "", "self", ".", "project_in_dim", "=", "Linear", "(", "input_embed_dim", ",", "embed_dim", ",", "bias", "=", "False", ")", "if", "embed_dim", "!=", "input_embed_dim", "else", "None", "\n", "\n", "self", ".", "embed_positions", "=", "PositionalEmbedding", "(", "\n", "args", ".", "max_target_positions", ",", "embed_dim", ",", "padding_idx", ",", "\n", "learned", "=", "args", ".", "decoder_learned_pos", ",", "\n", ")", "if", "not", "args", ".", "no_token_positional_embeddings", "else", "None", "\n", "\n", "self", ".", "layers", "=", "nn", ".", "ModuleList", "(", "[", "]", ")", "\n", "self", ".", "layers", ".", "extend", "(", "[", "\n", "TransformerDecoderLayer", "(", "args", ",", "no_encoder_attn", ")", "\n", "for", "_", "in", "range", "(", "args", ".", "decoder_layers", ")", "\n", "]", ")", "\n", "\n", "self", ".", "adaptive_softmax", "=", "None", "\n", "\n", "self", ".", "project_out_dim", "=", "Linear", "(", "embed_dim", ",", "self", ".", "output_embed_dim", ",", "bias", "=", "False", ")", "if", "embed_dim", "!=", "self", ".", "output_embed_dim", "and", "not", "args", ".", "tie_adaptive_weights", "else", "None", "\n", "\n", "if", "args", ".", "adaptive_softmax_cutoff", "is", "not", "None", ":", "\n", "            ", "self", ".", "adaptive_softmax", "=", "AdaptiveSoftmax", "(", "\n", "len", "(", "dictionary", ")", ",", "\n", "self", ".", "output_embed_dim", ",", "\n", "options", ".", "eval_str_list", "(", "args", ".", "adaptive_softmax_cutoff", ",", "type", "=", "int", ")", ",", "\n", "dropout", "=", "args", ".", "adaptive_softmax_dropout", ",", "\n", "adaptive_inputs", "=", "embed_tokens", "if", "args", ".", "tie_adaptive_weights", "else", "None", ",", "\n", "factor", "=", "args", ".", "adaptive_softmax_factor", ",", "\n", "tie_proj", "=", "args", ".", "tie_adaptive_proj", ",", "\n", ")", "\n", "", "elif", "not", "self", ".", "share_input_output_embed", ":", "\n", "            ", "self", ".", "embed_out", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "len", "(", "dictionary", ")", ",", "self", ".", "output_embed_dim", ")", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "self", ".", "embed_out", ",", "mean", "=", "0", ",", "std", "=", "self", ".", "output_embed_dim", "**", "-", "0.5", ")", "\n", "\n", "", "if", "args", ".", "decoder_normalize_before", "and", "not", "getattr", "(", "args", ",", "'no_decoder_final_norm'", ",", "False", ")", ":", "\n", "            ", "self", ".", "layer_norm", "=", "LayerNorm", "(", "embed_dim", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "layer_norm", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.TransformerDecoder.forward": [[461, 497], ["transformer.TransformerDecoder.extract_features", "transformer.TransformerDecoder.output_layer"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.extract_features", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.TransformerDecoder.output_layer"], ["", "", "def", "forward", "(", "self", ",", "prev_output_tokens", ",", "encoder_out", ",", "memory", ",", "memory_pos", ",", "\n", "incremental_state", "=", "None", ",", "logits_mask", "=", "None", ",", "logits_indices", "=", "None", ",", "\n", "**", "unused", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            prev_output_tokens (LongTensor): previous decoder outputs of shape\n                `(batch, tgt_len)`, for teacher forcing\n            encoder_out (Tensor, optional): output from the encoder, used for\n                encoder-side attention\n            incremental_state (dict): dictionary used for storing state during\n                :ref:`Incremental decoding`\n\n        Returns:\n            tuple:\n                - the decoder's output of shape `(batch, tgt_len, vocab)`\n                - a dictionary with any model-specific outputs\n        \"\"\"", "\n", "x", ",", "extra", "=", "self", ".", "extract_features", "(", "\n", "prev_output_tokens", ",", "\n", "memory", ",", "\n", "memory_pos", ",", "\n", "encoder_out", ",", "\n", "incremental_state", "\n", ")", "\n", "x", "=", "self", ".", "output_layer", "(", "\n", "x", ",", "\n", "logits_mask", "=", "logits_mask", ",", "\n", "logits_indices", "=", "logits_indices", "\n", ")", "\n", "\n", "# DEBUG: (consumes time)", "\n", "# if (x != x).any():", "\n", "#    import pdb; pdb.set_trace()", "\n", "#    print()", "\n", "\n", "return", "x", ",", "extra", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.TransformerDecoder.extract_features": [[498, 577], ["torch.dropout", "torch.dropout", "torch.dropout", "transformer.TransformerDecoder.transpose", "enumerate", "transformer.TransformerDecoder.transpose", "transformer.TransformerDecoder.embed_positions", "transformer.TransformerDecoder.embed_tokens", "transformer.TransformerDecoder.project_in_dim", "layer", "inner_states.append", "transformer.TransformerDecoder.layer_norm", "transformer.TransformerDecoder.project_out_dim", "transition_amr_parser.stack_transformer.stack_state_machine.state_machine_encoder", "transformer.TransformerDecoder.buffered_future_mask"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.stack_state_machine.state_machine_encoder", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.TransformerDecoder.buffered_future_mask"], ["", "def", "extract_features", "(", "self", ",", "prev_output_tokens", ",", "memory", ",", "memory_pos", ",", "encoder_out", "=", "None", ",", "incremental_state", "=", "None", ",", "**", "unused", ")", ":", "\n", "        ", "\"\"\"\n        Similar to *forward* but only return features.\n\n        Returns:\n            tuple:\n                - the decoder's features of shape `(batch, tgt_len, embed_dim)`\n                - a dictionary with any model-specific outputs\n        \"\"\"", "\n", "# embed positions", "\n", "positions", "=", "self", ".", "embed_positions", "(", "\n", "prev_output_tokens", ",", "\n", "incremental_state", "=", "incremental_state", ",", "\n", ")", "if", "self", ".", "embed_positions", "is", "not", "None", "else", "None", "\n", "\n", "if", "incremental_state", "is", "not", "None", ":", "\n", "# It needs only the last auto-regressive element. Rest is cached.", "\n", "            ", "prev_output_tokens", "=", "prev_output_tokens", "[", ":", ",", "-", "1", ":", "]", "\n", "if", "positions", "is", "not", "None", ":", "\n", "                ", "positions", "=", "positions", "[", ":", ",", "-", "1", ":", "]", "\n", "", "memory", "=", "memory", "[", ":", ",", ":", ",", "-", "1", ":", "]", "\n", "memory_pos", "=", "memory_pos", "[", ":", ",", ":", ",", "-", "1", ":", "]", "\n", "\n", "# embed tokens and positions", "\n", "", "x", "=", "self", ".", "embed_scale", "*", "self", ".", "embed_tokens", "(", "prev_output_tokens", ")", "\n", "\n", "if", "self", ".", "project_in_dim", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "project_in_dim", "(", "x", ")", "\n", "\n", "", "if", "positions", "is", "not", "None", ":", "\n", "            ", "x", "+=", "positions", "\n", "", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "\n", "# B x T x C -> T x B x C", "\n", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "attn", "=", "None", "\n", "\n", "inner_states", "=", "[", "x", "]", "\n", "\n", "# decoder layers", "\n", "for", "layer_index", ",", "layer", "in", "enumerate", "(", "self", ".", "layers", ")", ":", "\n", "\n", "# Encode state of state machine as attention masks and encoded", "\n", "# token positions changing for each target action", "\n", "            ", "if", "self", ".", "encode_state_machine", "is", "None", ":", "\n", "                ", "head_attention_masks", "=", "None", "\n", "head_positions", "=", "None", "\n", "", "else", ":", "\n", "                ", "head_attention_masks", ",", "head_positions", "=", "state_machine_encoder", "(", "\n", "self", ".", "encode_state_machine", ",", "\n", "memory", ",", "\n", "memory_pos", ",", "\n", "layer", ".", "encoder_attn", ".", "num_heads", ",", "\n", "self", ".", "embed_stack_positions", ",", "\n", "layer_index", ",", "\n", "encoder_out", "[", "'encoder_padding_mask'", "]", "if", "encoder_out", "is", "not", "None", "else", "None", "\n", ")", "\n", "\n", "", "x", ",", "attn", "=", "layer", "(", "\n", "x", ",", "\n", "encoder_out", "[", "'encoder_out'", "]", "if", "encoder_out", "is", "not", "None", "else", "None", ",", "\n", "encoder_out", "[", "'encoder_padding_mask'", "]", "if", "encoder_out", "is", "not", "None", "else", "None", ",", "\n", "incremental_state", ",", "\n", "self_attn_mask", "=", "self", ".", "buffered_future_mask", "(", "x", ")", "if", "incremental_state", "is", "None", "else", "None", ",", "\n", "head_attention_masks", "=", "head_attention_masks", ",", "\n", "head_positions", "=", "head_positions", "\n", ")", "\n", "inner_states", ".", "append", "(", "x", ")", "\n", "\n", "", "if", "self", ".", "layer_norm", ":", "\n", "            ", "x", "=", "self", ".", "layer_norm", "(", "x", ")", "\n", "\n", "# T x B x C -> B x T x C", "\n", "", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "if", "self", ".", "project_out_dim", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "project_out_dim", "(", "x", ")", "\n", "\n", "", "return", "x", ",", "{", "'attn'", ":", "attn", ",", "'inner_states'", ":", "inner_states", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.TransformerDecoder.output_layer": [[578, 610], ["torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.linear", "torch.linear", "torch.linear", "float", "torch.linear", "torch.linear", "torch.linear", "list", "features.new_ones", "float", "logits_indices.keys"], "methods", ["None"], ["", "def", "output_layer", "(", "self", ",", "features", ",", "logits_mask", "=", "None", ",", "logits_indices", "=", "None", ",", "\n", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Project features to the vocabulary size.\"\"\"", "\n", "if", "self", ".", "adaptive_softmax", "is", "None", ":", "\n", "# project back to size of vocabulary", "\n", "            ", "if", "self", ".", "share_input_output_embed", ":", "\n", "                ", "emb_weights", "=", "self", ".", "embed_tokens", ".", "weight", "\n", "", "else", ":", "\n", "                ", "emb_weights", "=", "self", ".", "embed_out", "\n", "", "if", "logits_indices", ":", "\n", "\n", "# indices of active logits", "\n", "                ", "indices", "=", "torch", ".", "tensor", "(", "list", "(", "logits_indices", ".", "keys", "(", ")", ")", ")", "\n", "# compute only active logits", "\n", "# (batch_size, target_size, target_emb_size)", "\n", "active_output", "=", "F", ".", "linear", "(", "features", ",", "emb_weights", "[", "indices", ",", ":", "]", ")", "\n", "# forbid masked elements", "\n", "active_output", "[", "logits_mask", "==", "0", "]", "=", "float", "(", "\"-Inf\"", ")", "\n", "# assign output", "\n", "emb_size", "=", "emb_weights", ".", "shape", "[", "0", "]", "\n", "batch_size", ",", "target_size", ",", "_", "=", "features", ".", "shape", "\n", "out_shape", "=", "(", "batch_size", ",", "target_size", ",", "emb_size", ")", "\n", "output", "=", "features", ".", "new_ones", "(", "out_shape", ")", "*", "float", "(", "\"-Inf\"", ")", "\n", "output", "[", ":", ",", ":", ",", "indices", "]", "=", "active_output", "\n", "\n", "", "else", ":", "\n", "                ", "output", "=", "F", ".", "linear", "(", "features", ",", "emb_weights", ")", "\n", "", "", "else", ":", "\n", "            ", "assert", "not", "logits_mask", "\n", "output", "=", "features", "\n", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.TransformerDecoder.max_positions": [[611, 616], ["min", "transformer.TransformerDecoder.embed_positions.max_positions"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.max_positions"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum output length supported by the decoder.\"\"\"", "\n", "if", "self", ".", "embed_positions", "is", "None", ":", "\n", "            ", "return", "self", ".", "max_target_positions", "\n", "", "return", "min", "(", "self", ".", "max_target_positions", ",", "self", ".", "embed_positions", ".", "max_positions", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.TransformerDecoder.buffered_future_mask": [[617, 622], ["tensor.size", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "hasattr", "transformer.TransformerDecoder._future_mask.size", "fairseq.utils.fill_with_neg_inf", "tensor.new"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.fill_with_neg_inf"], ["", "def", "buffered_future_mask", "(", "self", ",", "tensor", ")", ":", "\n", "        ", "dim", "=", "tensor", ".", "size", "(", "0", ")", "\n", "if", "not", "hasattr", "(", "self", ",", "'_future_mask'", ")", "or", "self", ".", "_future_mask", "is", "None", "or", "self", ".", "_future_mask", ".", "device", "!=", "tensor", ".", "device", "or", "self", ".", "_future_mask", ".", "size", "(", "0", ")", "<", "dim", ":", "\n", "            ", "self", ".", "_future_mask", "=", "torch", ".", "triu", "(", "utils", ".", "fill_with_neg_inf", "(", "tensor", ".", "new", "(", "dim", ",", "dim", ")", ")", ",", "1", ")", "\n", "", "return", "self", ".", "_future_mask", "[", ":", "dim", ",", ":", "dim", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.TransformerDecoder.upgrade_state_dict_named": [[623, 653], ["isinstance", "range", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "len", "layer_norm_map.items", "fairseq.utils.item", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "state_dict.get", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item"], ["", "def", "upgrade_state_dict_named", "(", "self", ",", "state_dict", ",", "name", ")", ":", "\n", "        ", "\"\"\"Upgrade a (possibly old) state dict for new versions of fairseq.\"\"\"", "\n", "if", "isinstance", "(", "self", ".", "embed_positions", ",", "SinusoidalPositionalEmbedding", ")", ":", "\n", "            ", "weights_key", "=", "'{}.embed_positions.weights'", ".", "format", "(", "name", ")", "\n", "if", "weights_key", "in", "state_dict", ":", "\n", "                ", "del", "state_dict", "[", "weights_key", "]", "\n", "", "state_dict", "[", "'{}.embed_positions._float_tensor'", ".", "format", "(", "name", ")", "]", "=", "torch", ".", "FloatTensor", "(", "1", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "len", "(", "self", ".", "layers", ")", ")", ":", "\n", "# update layer norms", "\n", "            ", "layer_norm_map", "=", "{", "\n", "'0'", ":", "'self_attn_layer_norm'", ",", "\n", "'1'", ":", "'encoder_attn_layer_norm'", ",", "\n", "'2'", ":", "'final_layer_norm'", "\n", "}", "\n", "for", "old", ",", "new", "in", "layer_norm_map", ".", "items", "(", ")", ":", "\n", "                ", "for", "m", "in", "(", "'weight'", ",", "'bias'", ")", ":", "\n", "                    ", "k", "=", "'{}.layers.{}.layer_norms.{}.{}'", ".", "format", "(", "name", ",", "i", ",", "old", ",", "m", ")", "\n", "if", "k", "in", "state_dict", ":", "\n", "                        ", "state_dict", "[", "'{}.layers.{}.{}.{}'", ".", "format", "(", "name", ",", "i", ",", "new", ",", "m", ")", "]", "=", "state_dict", "[", "k", "]", "\n", "del", "state_dict", "[", "k", "]", "\n", "\n", "", "", "", "", "version_key", "=", "'{}.version'", ".", "format", "(", "name", ")", "\n", "if", "utils", ".", "item", "(", "state_dict", ".", "get", "(", "version_key", ",", "torch", ".", "Tensor", "(", "[", "1", "]", ")", ")", "[", "0", "]", ")", "<", "2", ":", "\n", "# earlier checkpoints did not normalize after the stack of layers", "\n", "            ", "self", ".", "layer_norm", "=", "None", "\n", "self", ".", "normalize", "=", "False", "\n", "state_dict", "[", "version_key", "]", "=", "torch", ".", "Tensor", "(", "[", "1", "]", ")", "\n", "\n", "", "return", "state_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.Embedding": [[655, 660], ["torch.Embedding", "torch.init.normal_", "torch.init.constant_"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.Embedding"], ["", "", "def", "Embedding", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", ")", ":", "\n", "    ", "m", "=", "nn", ".", "Embedding", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", "=", "padding_idx", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "m", ".", "weight", ",", "mean", "=", "0", ",", "std", "=", "embedding_dim", "**", "-", "0.5", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "weight", "[", "padding_idx", "]", ",", "0", ")", "\n", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.Linear": [[662, 668], ["torch.Linear", "torch.init.xavier_uniform_", "torch.init.constant_"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear"], ["", "def", "Linear", "(", "in_features", ",", "out_features", ",", "bias", "=", "True", ")", ":", "\n", "    ", "m", "=", "nn", ".", "Linear", "(", "in_features", ",", "out_features", ",", "bias", ")", "\n", "nn", ".", "init", ".", "xavier_uniform_", "(", "m", ".", "weight", ")", "\n", "if", "bias", ":", "\n", "        ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0.", ")", "\n", "", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.base_architecture": [[670, 699], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture"], ["", "@", "register_model_architecture", "(", "'transformer'", ",", "'transformer'", ")", "\n", "def", "base_architecture", "(", "args", ")", ":", "\n", "    ", "args", ".", "encoder_embed_path", "=", "getattr", "(", "args", ",", "'encoder_embed_path'", ",", "None", ")", "\n", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "512", ")", "\n", "args", ".", "encoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_ffn_embed_dim'", ",", "2048", ")", "\n", "args", ".", "encoder_layers", "=", "getattr", "(", "args", ",", "'encoder_layers'", ",", "6", ")", "\n", "args", ".", "encoder_attention_heads", "=", "getattr", "(", "args", ",", "'encoder_attention_heads'", ",", "8", ")", "\n", "args", ".", "encoder_normalize_before", "=", "getattr", "(", "args", ",", "'encoder_normalize_before'", ",", "False", ")", "\n", "args", ".", "encoder_learned_pos", "=", "getattr", "(", "args", ",", "'encoder_learned_pos'", ",", "False", ")", "\n", "args", ".", "decoder_embed_path", "=", "getattr", "(", "args", ",", "'decoder_embed_path'", ",", "None", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "args", ".", "encoder_embed_dim", ")", "\n", "args", ".", "decoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_ffn_embed_dim'", ",", "args", ".", "encoder_ffn_embed_dim", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "6", ")", "\n", "args", ".", "decoder_attention_heads", "=", "getattr", "(", "args", ",", "'decoder_attention_heads'", ",", "8", ")", "\n", "args", ".", "decoder_normalize_before", "=", "getattr", "(", "args", ",", "'decoder_normalize_before'", ",", "False", ")", "\n", "args", ".", "decoder_learned_pos", "=", "getattr", "(", "args", ",", "'decoder_learned_pos'", ",", "False", ")", "\n", "args", ".", "attention_dropout", "=", "getattr", "(", "args", ",", "'attention_dropout'", ",", "0.", ")", "\n", "args", ".", "activation_dropout", "=", "getattr", "(", "args", ",", "'activation_dropout'", ",", "0.", ")", "\n", "args", ".", "activation_fn", "=", "getattr", "(", "args", ",", "'activation_fn'", ",", "'relu'", ")", "\n", "args", ".", "dropout", "=", "getattr", "(", "args", ",", "'dropout'", ",", "0.1", ")", "\n", "args", ".", "adaptive_softmax_cutoff", "=", "getattr", "(", "args", ",", "'adaptive_softmax_cutoff'", ",", "None", ")", "\n", "args", ".", "adaptive_softmax_dropout", "=", "getattr", "(", "args", ",", "'adaptive_softmax_dropout'", ",", "0", ")", "\n", "args", ".", "share_decoder_input_output_embed", "=", "getattr", "(", "args", ",", "'share_decoder_input_output_embed'", ",", "False", ")", "\n", "args", ".", "share_all_embeddings", "=", "getattr", "(", "args", ",", "'share_all_embeddings'", ",", "False", ")", "\n", "args", ".", "no_token_positional_embeddings", "=", "getattr", "(", "args", ",", "'no_token_positional_embeddings'", ",", "False", ")", "\n", "args", ".", "adaptive_input", "=", "getattr", "(", "args", ",", "'adaptive_input'", ",", "False", ")", "\n", "\n", "args", ".", "decoder_output_dim", "=", "getattr", "(", "args", ",", "'decoder_output_dim'", ",", "args", ".", "decoder_embed_dim", ")", "\n", "args", ".", "decoder_input_dim", "=", "getattr", "(", "args", ",", "'decoder_input_dim'", ",", "args", ".", "decoder_embed_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.transformer_iwslt_de_en": [[701, 712], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "transformer.base_architecture"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.base_architecture"], ["", "@", "register_model_architecture", "(", "'transformer'", ",", "'transformer_iwslt_de_en'", ")", "\n", "def", "transformer_iwslt_de_en", "(", "args", ")", ":", "\n", "    ", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "512", ")", "\n", "args", ".", "encoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_ffn_embed_dim'", ",", "1024", ")", "\n", "args", ".", "encoder_attention_heads", "=", "getattr", "(", "args", ",", "'encoder_attention_heads'", ",", "4", ")", "\n", "args", ".", "encoder_layers", "=", "getattr", "(", "args", ",", "'encoder_layers'", ",", "6", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "512", ")", "\n", "args", ".", "decoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_ffn_embed_dim'", ",", "1024", ")", "\n", "args", ".", "decoder_attention_heads", "=", "getattr", "(", "args", ",", "'decoder_attention_heads'", ",", "4", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "6", ")", "\n", "base_architecture", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.transformer_wmt_en_de": [[714, 717], ["fairseq.models.register_model_architecture", "transformer.base_architecture"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.base_architecture"], ["", "@", "register_model_architecture", "(", "'transformer'", ",", "'transformer_wmt_en_de'", ")", "\n", "def", "transformer_wmt_en_de", "(", "args", ")", ":", "\n", "    ", "base_architecture", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.transformer_vaswani_wmt_en_de_big": [[720, 731], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "transformer.base_architecture"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.base_architecture"], ["", "@", "register_model_architecture", "(", "'transformer'", ",", "'transformer_vaswani_wmt_en_de_big'", ")", "\n", "def", "transformer_vaswani_wmt_en_de_big", "(", "args", ")", ":", "\n", "    ", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "1024", ")", "\n", "args", ".", "encoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_ffn_embed_dim'", ",", "4096", ")", "\n", "args", ".", "encoder_attention_heads", "=", "getattr", "(", "args", ",", "'encoder_attention_heads'", ",", "16", ")", "\n", "args", ".", "encoder_normalize_before", "=", "getattr", "(", "args", ",", "'encoder_normalize_before'", ",", "False", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "1024", ")", "\n", "args", ".", "decoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_ffn_embed_dim'", ",", "4096", ")", "\n", "args", ".", "decoder_attention_heads", "=", "getattr", "(", "args", ",", "'decoder_attention_heads'", ",", "16", ")", "\n", "args", ".", "dropout", "=", "getattr", "(", "args", ",", "'dropout'", ",", "0.3", ")", "\n", "base_architecture", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.transformer_vaswani_wmt_en_fr_big": [[733, 737], ["fairseq.models.register_model_architecture", "getattr", "transformer.transformer_vaswani_wmt_en_de_big"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.transformer_vaswani_wmt_en_de_big"], ["", "@", "register_model_architecture", "(", "'transformer'", ",", "'transformer_vaswani_wmt_en_fr_big'", ")", "\n", "def", "transformer_vaswani_wmt_en_fr_big", "(", "args", ")", ":", "\n", "    ", "args", ".", "dropout", "=", "getattr", "(", "args", ",", "'dropout'", ",", "0.1", ")", "\n", "transformer_vaswani_wmt_en_de_big", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.transformer_wmt_en_de_big": [[739, 743], ["fairseq.models.register_model_architecture", "getattr", "transformer.transformer_vaswani_wmt_en_de_big"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.transformer_vaswani_wmt_en_de_big"], ["", "@", "register_model_architecture", "(", "'transformer'", ",", "'transformer_wmt_en_de_big'", ")", "\n", "def", "transformer_wmt_en_de_big", "(", "args", ")", ":", "\n", "    ", "args", ".", "attention_dropout", "=", "getattr", "(", "args", ",", "'attention_dropout'", ",", "0.1", ")", "\n", "transformer_vaswani_wmt_en_de_big", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.transformer_wmt_en_de_big_t2t": [[746, 753], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "transformer.transformer_vaswani_wmt_en_de_big"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.transformer_vaswani_wmt_en_de_big"], ["", "@", "register_model_architecture", "(", "'transformer'", ",", "'transformer_wmt_en_de_big_t2t'", ")", "\n", "def", "transformer_wmt_en_de_big_t2t", "(", "args", ")", ":", "\n", "    ", "args", ".", "encoder_normalize_before", "=", "getattr", "(", "args", ",", "'encoder_normalize_before'", ",", "True", ")", "\n", "args", ".", "decoder_normalize_before", "=", "getattr", "(", "args", ",", "'decoder_normalize_before'", ",", "True", ")", "\n", "args", ".", "attention_dropout", "=", "getattr", "(", "args", ",", "'attention_dropout'", ",", "0.1", ")", "\n", "args", ".", "activation_dropout", "=", "getattr", "(", "args", ",", "'activation_dropout'", ",", "0.1", ")", "\n", "transformer_vaswani_wmt_en_de_big", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.transformer_2x2": [[755, 768], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "transformer.base_architecture"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.base_architecture"], ["", "@", "register_model_architecture", "(", "'transformer'", ",", "'transformer_2x2'", ")", "\n", "def", "transformer_2x2", "(", "args", ")", ":", "\n", "    ", "args", ".", "encode_state_machine", "=", "getattr", "(", "args", ",", "'encode_state_machine'", ",", "None", ")", "\n", "#", "\n", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "256", ")", "\n", "args", ".", "encoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_ffn_embed_dim'", ",", "512", ")", "\n", "args", ".", "encoder_attention_heads", "=", "getattr", "(", "args", ",", "'encoder_attention_heads'", ",", "4", ")", "\n", "args", ".", "encoder_layers", "=", "getattr", "(", "args", ",", "'encoder_layers'", ",", "2", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "256", ")", "\n", "args", ".", "decoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_ffn_embed_dim'", ",", "512", ")", "\n", "args", ".", "decoder_attention_heads", "=", "getattr", "(", "args", ",", "'decoder_attention_heads'", ",", "4", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "2", ")", "\n", "base_architecture", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.transformer_6x6": [[770, 783], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "transformer.base_architecture"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.base_architecture"], ["", "@", "register_model_architecture", "(", "'transformer'", ",", "'transformer_6x6'", ")", "\n", "def", "transformer_6x6", "(", "args", ")", ":", "\n", "    ", "args", ".", "encode_state_machine", "=", "getattr", "(", "args", ",", "'encode_state_machine'", ",", "None", ")", "\n", "#", "\n", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "256", ")", "\n", "args", ".", "encoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_ffn_embed_dim'", ",", "512", ")", "\n", "args", ".", "encoder_attention_heads", "=", "getattr", "(", "args", ",", "'encoder_attention_heads'", ",", "4", ")", "\n", "args", ".", "encoder_layers", "=", "getattr", "(", "args", ",", "'encoder_layers'", ",", "6", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "256", ")", "\n", "args", ".", "decoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_ffn_embed_dim'", ",", "512", ")", "\n", "args", ".", "decoder_attention_heads", "=", "getattr", "(", "args", ",", "'decoder_attention_heads'", ",", "4", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "6", ")", "\n", "base_architecture", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.transformer_3x8": [[785, 798], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "transformer.base_architecture"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.base_architecture"], ["", "@", "register_model_architecture", "(", "'transformer'", ",", "'transformer_3x8'", ")", "\n", "def", "transformer_3x8", "(", "args", ")", ":", "\n", "    ", "args", ".", "encode_state_machine", "=", "getattr", "(", "args", ",", "'encode_state_machine'", ",", "None", ")", "\n", "#", "\n", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "256", ")", "\n", "args", ".", "encoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_ffn_embed_dim'", ",", "512", ")", "\n", "args", ".", "encoder_attention_heads", "=", "getattr", "(", "args", ",", "'encoder_attention_heads'", ",", "4", ")", "\n", "args", ".", "encoder_layers", "=", "getattr", "(", "args", ",", "'encoder_layers'", ",", "3", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "256", ")", "\n", "args", ".", "decoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_ffn_embed_dim'", ",", "512", ")", "\n", "args", ".", "decoder_attention_heads", "=", "getattr", "(", "args", ",", "'decoder_attention_heads'", ",", "4", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "8", ")", "\n", "base_architecture", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.stack_transformer_2x2_layer0": [[803, 816], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "transformer.base_architecture"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.base_architecture"], ["", "@", "register_model_architecture", "(", "'transformer'", ",", "'stack_transformer_2x2_layer0'", ")", "\n", "def", "stack_transformer_2x2_layer0", "(", "args", ")", ":", "\n", "    ", "args", ".", "encode_state_machine", "=", "getattr", "(", "args", ",", "'encode_state_machine'", ",", "\"layer0\"", ")", "\n", "#", "\n", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "256", ")", "\n", "args", ".", "encoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_ffn_embed_dim'", ",", "512", ")", "\n", "args", ".", "encoder_attention_heads", "=", "getattr", "(", "args", ",", "'encoder_attention_heads'", ",", "4", ")", "\n", "args", ".", "encoder_layers", "=", "getattr", "(", "args", ",", "'encoder_layers'", ",", "2", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "256", ")", "\n", "args", ".", "decoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_ffn_embed_dim'", ",", "512", ")", "\n", "args", ".", "decoder_attention_heads", "=", "getattr", "(", "args", ",", "'decoder_attention_heads'", ",", "4", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "2", ")", "\n", "base_architecture", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.stack_transformer_6x6_layer0": [[818, 831], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "transformer.base_architecture"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.base_architecture"], ["", "@", "register_model_architecture", "(", "'transformer'", ",", "'stack_transformer_6x6_layer0'", ")", "\n", "def", "stack_transformer_6x6_layer0", "(", "args", ")", ":", "\n", "    ", "args", ".", "encode_state_machine", "=", "getattr", "(", "args", ",", "'encode_state_machine'", ",", "\"layer0\"", ")", "\n", "#", "\n", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "256", ")", "\n", "args", ".", "encoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_ffn_embed_dim'", ",", "512", ")", "\n", "args", ".", "encoder_attention_heads", "=", "getattr", "(", "args", ",", "'encoder_attention_heads'", ",", "4", ")", "\n", "args", ".", "encoder_layers", "=", "getattr", "(", "args", ",", "'encoder_layers'", ",", "6", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "256", ")", "\n", "args", ".", "decoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_ffn_embed_dim'", ",", "512", ")", "\n", "args", ".", "decoder_attention_heads", "=", "getattr", "(", "args", ",", "'decoder_attention_heads'", ",", "4", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "6", ")", "\n", "base_architecture", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.stack_transformer_2x2_nopos_layer0": [[833, 846], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "transformer.base_architecture"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.base_architecture"], ["", "@", "register_model_architecture", "(", "'transformer'", ",", "'stack_transformer_2x2_nopos_layer0'", ")", "\n", "def", "stack_transformer_2x2_nopos_layer0", "(", "args", ")", ":", "\n", "    ", "args", ".", "encode_state_machine", "=", "getattr", "(", "args", ",", "'encode_state_machine'", ",", "\"layer0_nopos\"", ")", "\n", "#", "\n", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "256", ")", "\n", "args", ".", "encoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_ffn_embed_dim'", ",", "512", ")", "\n", "args", ".", "encoder_attention_heads", "=", "getattr", "(", "args", ",", "'encoder_attention_heads'", ",", "4", ")", "\n", "args", ".", "encoder_layers", "=", "getattr", "(", "args", ",", "'encoder_layers'", ",", "2", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "256", ")", "\n", "args", ".", "decoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_ffn_embed_dim'", ",", "512", ")", "\n", "args", ".", "decoder_attention_heads", "=", "getattr", "(", "args", ",", "'decoder_attention_heads'", ",", "4", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "2", ")", "\n", "base_architecture", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.stack_transformer_2x2_nopos": [[848, 861], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "transformer.base_architecture"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.base_architecture"], ["", "@", "register_model_architecture", "(", "'transformer'", ",", "'stack_transformer_2x2_nopos'", ")", "\n", "def", "stack_transformer_2x2_nopos", "(", "args", ")", ":", "\n", "    ", "args", ".", "encode_state_machine", "=", "getattr", "(", "args", ",", "'encode_state_machine'", ",", "\"all-layers_nopos\"", ")", "\n", "#", "\n", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "256", ")", "\n", "args", ".", "encoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_ffn_embed_dim'", ",", "512", ")", "\n", "args", ".", "encoder_attention_heads", "=", "getattr", "(", "args", ",", "'encoder_attention_heads'", ",", "4", ")", "\n", "args", ".", "encoder_layers", "=", "getattr", "(", "args", ",", "'encoder_layers'", ",", "2", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "256", ")", "\n", "args", ".", "decoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_ffn_embed_dim'", ",", "512", ")", "\n", "args", ".", "decoder_attention_heads", "=", "getattr", "(", "args", ",", "'decoder_attention_heads'", ",", "4", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "2", ")", "\n", "base_architecture", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.stack_transformer_6x6": [[863, 876], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "transformer.base_architecture"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.base_architecture"], ["", "@", "register_model_architecture", "(", "'transformer'", ",", "'stack_transformer_6x6'", ")", "\n", "def", "stack_transformer_6x6", "(", "args", ")", ":", "\n", "    ", "args", ".", "encode_state_machine", "=", "getattr", "(", "args", ",", "'encode_state_machine'", ",", "\"all-layers\"", ")", "\n", "#", "\n", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "256", ")", "\n", "args", ".", "encoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_ffn_embed_dim'", ",", "512", ")", "\n", "args", ".", "encoder_attention_heads", "=", "getattr", "(", "args", ",", "'encoder_attention_heads'", ",", "4", ")", "\n", "args", ".", "encoder_layers", "=", "getattr", "(", "args", ",", "'encoder_layers'", ",", "6", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "256", ")", "\n", "args", ".", "decoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_ffn_embed_dim'", ",", "512", ")", "\n", "args", ".", "decoder_attention_heads", "=", "getattr", "(", "args", ",", "'decoder_attention_heads'", ",", "4", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "6", ")", "\n", "base_architecture", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.stack_transformer_6x6_nopos": [[878, 891], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "transformer.base_architecture"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.base_architecture"], ["", "@", "register_model_architecture", "(", "'transformer'", ",", "'stack_transformer_6x6_nopos'", ")", "\n", "def", "stack_transformer_6x6_nopos", "(", "args", ")", ":", "\n", "    ", "args", ".", "encode_state_machine", "=", "getattr", "(", "args", ",", "'encode_state_machine'", ",", "\"all-layers_nopos\"", ")", "\n", "#", "\n", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "256", ")", "\n", "args", ".", "encoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_ffn_embed_dim'", ",", "512", ")", "\n", "args", ".", "encoder_attention_heads", "=", "getattr", "(", "args", ",", "'encoder_attention_heads'", ",", "4", ")", "\n", "args", ".", "encoder_layers", "=", "getattr", "(", "args", ",", "'encoder_layers'", ",", "6", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "256", ")", "\n", "args", ".", "decoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_ffn_embed_dim'", ",", "512", ")", "\n", "args", ".", "decoder_attention_heads", "=", "getattr", "(", "args", ",", "'decoder_attention_heads'", ",", "4", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "6", ")", "\n", "base_architecture", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.stack_transformer_6x6_tops_nopos": [[893, 906], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "transformer.base_architecture"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.base_architecture"], ["", "@", "register_model_architecture", "(", "'transformer'", ",", "'stack_transformer_6x6_tops_nopos'", ")", "\n", "def", "stack_transformer_6x6_tops_nopos", "(", "args", ")", ":", "\n", "    ", "args", ".", "encode_state_machine", "=", "getattr", "(", "args", ",", "'encode_state_machine'", ",", "\"stack_top_nopos\"", ")", "\n", "#", "\n", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "256", ")", "\n", "args", ".", "encoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_ffn_embed_dim'", ",", "512", ")", "\n", "args", ".", "encoder_attention_heads", "=", "getattr", "(", "args", ",", "'encoder_attention_heads'", ",", "4", ")", "\n", "args", ".", "encoder_layers", "=", "getattr", "(", "args", ",", "'encoder_layers'", ",", "6", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "256", ")", "\n", "args", ".", "decoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_ffn_embed_dim'", ",", "512", ")", "\n", "args", ".", "decoder_attention_heads", "=", "getattr", "(", "args", ",", "'decoder_attention_heads'", ",", "4", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "6", ")", "\n", "base_architecture", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.stack_transformer_6x6_only_buffer_nopos": [[908, 921], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "transformer.base_architecture"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.base_architecture"], ["", "@", "register_model_architecture", "(", "'transformer'", ",", "'stack_transformer_6x6_only_buffer_nopos'", ")", "\n", "def", "stack_transformer_6x6_only_buffer_nopos", "(", "args", ")", ":", "\n", "    ", "args", ".", "encode_state_machine", "=", "getattr", "(", "args", ",", "'encode_state_machine'", ",", "\"only_buffer_nopos\"", ")", "\n", "#", "\n", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "256", ")", "\n", "args", ".", "encoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_ffn_embed_dim'", ",", "512", ")", "\n", "args", ".", "encoder_attention_heads", "=", "getattr", "(", "args", ",", "'encoder_attention_heads'", ",", "4", ")", "\n", "args", ".", "encoder_layers", "=", "getattr", "(", "args", ",", "'encoder_layers'", ",", "6", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "256", ")", "\n", "args", ".", "decoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_ffn_embed_dim'", ",", "512", ")", "\n", "args", ".", "decoder_attention_heads", "=", "getattr", "(", "args", ",", "'decoder_attention_heads'", ",", "4", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "6", ")", "\n", "base_architecture", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.stack_transformer_6x6_only_stack_nopos": [[923, 936], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "transformer.base_architecture"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.base_architecture"], ["", "@", "register_model_architecture", "(", "'transformer'", ",", "'stack_transformer_6x6_only_stack_nopos'", ")", "\n", "def", "stack_transformer_6x6_only_stack_nopos", "(", "args", ")", ":", "\n", "    ", "args", ".", "encode_state_machine", "=", "getattr", "(", "args", ",", "'encode_state_machine'", ",", "\"only_stack_nopos\"", ")", "\n", "#", "\n", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "256", ")", "\n", "args", ".", "encoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_ffn_embed_dim'", ",", "512", ")", "\n", "args", ".", "encoder_attention_heads", "=", "getattr", "(", "args", ",", "'encoder_attention_heads'", ",", "4", ")", "\n", "args", ".", "encoder_layers", "=", "getattr", "(", "args", ",", "'encoder_layers'", ",", "6", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "256", ")", "\n", "args", ".", "decoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_ffn_embed_dim'", ",", "512", ")", "\n", "args", ".", "decoder_attention_heads", "=", "getattr", "(", "args", ",", "'decoder_attention_heads'", ",", "4", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "6", ")", "\n", "base_architecture", "(", "args", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.multilingual_transformer.MultilingualTransformerModel.__init__": [[40, 42], ["fairseq.models.FairseqMultiModel.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["def", "__init__", "(", "self", ",", "encoders", ",", "decoders", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "encoders", ",", "decoders", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.multilingual_transformer.MultilingualTransformerModel.add_args": [[43, 55], ["fairseq.models.transformer.TransformerModel.add_args", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.add_args"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add model-specific arguments to the parser.\"\"\"", "\n", "TransformerModel", ".", "add_args", "(", "parser", ")", "\n", "parser", ".", "add_argument", "(", "'--share-encoder-embeddings'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'share encoder embeddings across languages'", ")", "\n", "parser", ".", "add_argument", "(", "'--share-decoder-embeddings'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'share decoder embeddings across languages'", ")", "\n", "parser", ".", "add_argument", "(", "'--share-encoders'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'share encoders across languages'", ")", "\n", "parser", ".", "add_argument", "(", "'--share-decoders'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'share decoders across languages'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.multilingual_transformer.MultilingualTransformerModel.build_model": [[56, 166], ["isinstance", "multilingual_transformer.base_multilingual_architecture", "zip", "multilingual_transformer.MultilingualTransformerModel", "hasattr", "hasattr", "len", "dictionary.pad", "fairseq.models.transformer.Embedding", "fairseq.models.FairseqMultiModel.build_shared_embeddings", "multilingual_transformer.MultilingualTransformerModel.build_model.get_encoder"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.multilingual_transformer.base_multilingual_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.Embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.FairseqMultiModel.build_shared_embeddings", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.gpt2_bpe_utils.get_encoder"], ["", "@", "classmethod", "\n", "def", "build_model", "(", "cls", ",", "args", ",", "task", ")", ":", "\n", "        ", "\"\"\"Build a new model instance.\"\"\"", "\n", "from", "fairseq", ".", "tasks", ".", "multilingual_translation", "import", "MultilingualTranslationTask", "\n", "assert", "isinstance", "(", "task", ",", "MultilingualTranslationTask", ")", "\n", "\n", "# make sure all arguments are present in older models", "\n", "base_multilingual_architecture", "(", "args", ")", "\n", "\n", "if", "not", "hasattr", "(", "args", ",", "'max_source_positions'", ")", ":", "\n", "            ", "args", ".", "max_source_positions", "=", "1024", "\n", "", "if", "not", "hasattr", "(", "args", ",", "'max_target_positions'", ")", ":", "\n", "            ", "args", ".", "max_target_positions", "=", "1024", "\n", "\n", "", "src_langs", "=", "[", "lang_pair", ".", "split", "(", "'-'", ")", "[", "0", "]", "for", "lang_pair", "in", "task", ".", "model_lang_pairs", "]", "\n", "tgt_langs", "=", "[", "lang_pair", ".", "split", "(", "'-'", ")", "[", "1", "]", "for", "lang_pair", "in", "task", ".", "model_lang_pairs", "]", "\n", "\n", "if", "args", ".", "share_encoders", ":", "\n", "            ", "args", ".", "share_encoder_embeddings", "=", "True", "\n", "", "if", "args", ".", "share_decoders", ":", "\n", "            ", "args", ".", "share_decoder_embeddings", "=", "True", "\n", "\n", "", "def", "build_embedding", "(", "dictionary", ",", "embed_dim", ",", "path", "=", "None", ")", ":", "\n", "            ", "num_embeddings", "=", "len", "(", "dictionary", ")", "\n", "padding_idx", "=", "dictionary", ".", "pad", "(", ")", "\n", "emb", "=", "Embedding", "(", "num_embeddings", ",", "embed_dim", ",", "padding_idx", ")", "\n", "# if provided, load from preloaded dictionaries", "\n", "if", "path", ":", "\n", "                ", "embed_dict", "=", "utils", ".", "parse_embedding", "(", "path", ")", "\n", "utils", ".", "load_embedding", "(", "embed_dict", ",", "dictionary", ",", "emb", ")", "\n", "", "return", "emb", "\n", "\n", "# build shared embeddings (if applicable)", "\n", "", "shared_encoder_embed_tokens", ",", "shared_decoder_embed_tokens", "=", "None", ",", "None", "\n", "if", "args", ".", "share_all_embeddings", ":", "\n", "            ", "if", "args", ".", "encoder_embed_dim", "!=", "args", ".", "decoder_embed_dim", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "'--share-all-embeddings requires --encoder-embed-dim to match --decoder-embed-dim'", ")", "\n", "", "if", "args", ".", "decoder_embed_path", "and", "(", "\n", "args", ".", "decoder_embed_path", "!=", "args", ".", "encoder_embed_path", ")", ":", "\n", "                ", "raise", "ValueError", "(", "'--share-all-embeddings not compatible with --decoder-embed-path'", ")", "\n", "", "shared_encoder_embed_tokens", "=", "FairseqMultiModel", ".", "build_shared_embeddings", "(", "\n", "dicts", "=", "task", ".", "dicts", ",", "\n", "langs", "=", "task", ".", "langs", ",", "\n", "embed_dim", "=", "args", ".", "encoder_embed_dim", ",", "\n", "build_embedding", "=", "build_embedding", ",", "\n", "pretrained_embed_path", "=", "args", ".", "encoder_embed_path", ",", "\n", ")", "\n", "shared_decoder_embed_tokens", "=", "shared_encoder_embed_tokens", "\n", "args", ".", "share_decoder_input_output_embed", "=", "True", "\n", "", "else", ":", "\n", "            ", "if", "args", ".", "share_encoder_embeddings", ":", "\n", "                ", "shared_encoder_embed_tokens", "=", "(", "\n", "FairseqMultiModel", ".", "build_shared_embeddings", "(", "\n", "dicts", "=", "task", ".", "dicts", ",", "\n", "langs", "=", "src_langs", ",", "\n", "embed_dim", "=", "args", ".", "encoder_embed_dim", ",", "\n", "build_embedding", "=", "build_embedding", ",", "\n", "pretrained_embed_path", "=", "args", ".", "encoder_embed_path", ",", "\n", ")", "\n", ")", "\n", "", "if", "args", ".", "share_decoder_embeddings", ":", "\n", "                ", "shared_decoder_embed_tokens", "=", "(", "\n", "FairseqMultiModel", ".", "build_shared_embeddings", "(", "\n", "dicts", "=", "task", ".", "dicts", ",", "\n", "langs", "=", "tgt_langs", ",", "\n", "embed_dim", "=", "args", ".", "decoder_embed_dim", ",", "\n", "build_embedding", "=", "build_embedding", ",", "\n", "pretrained_embed_path", "=", "args", ".", "decoder_embed_path", ",", "\n", ")", "\n", ")", "\n", "\n", "# encoders/decoders for each language", "\n", "", "", "lang_encoders", ",", "lang_decoders", "=", "{", "}", ",", "{", "}", "\n", "\n", "def", "get_encoder", "(", "lang", ")", ":", "\n", "            ", "if", "lang", "not", "in", "lang_encoders", ":", "\n", "                ", "if", "shared_encoder_embed_tokens", "is", "not", "None", ":", "\n", "                    ", "encoder_embed_tokens", "=", "shared_encoder_embed_tokens", "\n", "", "else", ":", "\n", "                    ", "encoder_embed_tokens", "=", "build_embedding", "(", "\n", "task", ".", "dicts", "[", "lang", "]", ",", "args", ".", "encoder_embed_dim", ",", "args", ".", "encoder_embed_path", "\n", ")", "\n", "", "lang_encoders", "[", "lang", "]", "=", "TransformerEncoder", "(", "args", ",", "task", ".", "dicts", "[", "lang", "]", ",", "encoder_embed_tokens", ")", "\n", "", "return", "lang_encoders", "[", "lang", "]", "\n", "\n", "", "def", "get_decoder", "(", "lang", ")", ":", "\n", "            ", "if", "lang", "not", "in", "lang_decoders", ":", "\n", "                ", "if", "shared_decoder_embed_tokens", "is", "not", "None", ":", "\n", "                    ", "decoder_embed_tokens", "=", "shared_decoder_embed_tokens", "\n", "", "else", ":", "\n", "                    ", "decoder_embed_tokens", "=", "build_embedding", "(", "\n", "task", ".", "dicts", "[", "lang", "]", ",", "args", ".", "decoder_embed_dim", ",", "args", ".", "decoder_embed_path", "\n", ")", "\n", "", "lang_decoders", "[", "lang", "]", "=", "TransformerDecoder", "(", "args", ",", "task", ".", "dicts", "[", "lang", "]", ",", "decoder_embed_tokens", ")", "\n", "", "return", "lang_decoders", "[", "lang", "]", "\n", "\n", "# shared encoders/decoders (if applicable)", "\n", "", "shared_encoder", ",", "shared_decoder", "=", "None", ",", "None", "\n", "if", "args", ".", "share_encoders", ":", "\n", "            ", "shared_encoder", "=", "get_encoder", "(", "src_langs", "[", "0", "]", ")", "\n", "", "if", "args", ".", "share_decoders", ":", "\n", "            ", "shared_decoder", "=", "get_decoder", "(", "tgt_langs", "[", "0", "]", ")", "\n", "\n", "", "encoders", ",", "decoders", "=", "OrderedDict", "(", ")", ",", "OrderedDict", "(", ")", "\n", "for", "lang_pair", ",", "src", ",", "tgt", "in", "zip", "(", "task", ".", "model_lang_pairs", ",", "src_langs", ",", "tgt_langs", ")", ":", "\n", "            ", "encoders", "[", "lang_pair", "]", "=", "shared_encoder", "if", "shared_encoder", "is", "not", "None", "else", "get_encoder", "(", "src", ")", "\n", "decoders", "[", "lang_pair", "]", "=", "shared_decoder", "if", "shared_decoder", "is", "not", "None", "else", "get_decoder", "(", "tgt", ")", "\n", "\n", "", "return", "MultilingualTransformerModel", "(", "encoders", ",", "decoders", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.multilingual_transformer.MultilingualTransformerModel.load_state_dict": [[167, 175], ["state_dict.copy", "state_dict.items", "super().load_state_dict", "k.startswith", "k.split"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.copy", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.load_state_dict", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["", "def", "load_state_dict", "(", "self", ",", "state_dict", ",", "strict", "=", "True", ")", ":", "\n", "        ", "state_dict_subset", "=", "state_dict", ".", "copy", "(", ")", "\n", "for", "k", ",", "_", "in", "state_dict", ".", "items", "(", ")", ":", "\n", "            ", "assert", "k", ".", "startswith", "(", "'models.'", ")", "\n", "lang_pair", "=", "k", ".", "split", "(", "'.'", ")", "[", "1", "]", "\n", "if", "lang_pair", "not", "in", "self", ".", "models", ":", "\n", "                ", "del", "state_dict_subset", "[", "k", "]", "\n", "", "", "super", "(", ")", ".", "load_state_dict", "(", "state_dict_subset", ",", "strict", "=", "strict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.multilingual_transformer.base_multilingual_architecture": [[177, 184], ["fairseq.models.register_model_architecture", "fairseq.models.transformer.base_architecture", "getattr", "getattr", "getattr", "getattr"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.base_architecture"], ["", "", "@", "register_model_architecture", "(", "'multilingual_transformer'", ",", "'multilingual_transformer'", ")", "\n", "def", "base_multilingual_architecture", "(", "args", ")", ":", "\n", "    ", "base_architecture", "(", "args", ")", "\n", "args", ".", "share_encoder_embeddings", "=", "getattr", "(", "args", ",", "'share_encoder_embeddings'", ",", "False", ")", "\n", "args", ".", "share_decoder_embeddings", "=", "getattr", "(", "args", ",", "'share_decoder_embeddings'", ",", "False", ")", "\n", "args", ".", "share_encoders", "=", "getattr", "(", "args", ",", "'share_encoders'", ",", "False", ")", "\n", "args", ".", "share_decoders", "=", "getattr", "(", "args", ",", "'share_decoders'", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.multilingual_transformer.multilingual_transformer_iwslt_de_en": [[186, 197], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "multilingual_transformer.base_multilingual_architecture"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.multilingual_transformer.base_multilingual_architecture"], ["", "@", "register_model_architecture", "(", "'multilingual_transformer'", ",", "'multilingual_transformer_iwslt_de_en'", ")", "\n", "def", "multilingual_transformer_iwslt_de_en", "(", "args", ")", ":", "\n", "    ", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "512", ")", "\n", "args", ".", "encoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_ffn_embed_dim'", ",", "1024", ")", "\n", "args", ".", "encoder_attention_heads", "=", "getattr", "(", "args", ",", "'encoder_attention_heads'", ",", "4", ")", "\n", "args", ".", "encoder_layers", "=", "getattr", "(", "args", ",", "'encoder_layers'", ",", "6", ")", "\n", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "512", ")", "\n", "args", ".", "decoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_ffn_embed_dim'", ",", "1024", ")", "\n", "args", ".", "decoder_attention_heads", "=", "getattr", "(", "args", ",", "'decoder_attention_heads'", ",", "4", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "6", ")", "\n", "base_multilingual_architecture", "(", "args", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv_lm.LightConvLanguageModel.__init__": [[24, 26], ["fairseq.models.FairseqLanguageModel.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["    ", "def", "__init__", "(", "self", ",", "decoder", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "decoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv_lm.LightConvLanguageModel.add_args": [[27, 96], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "fairseq.options.eval_str_list"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.eval_str_list"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add model-specific arguments to the parser.\"\"\"", "\n", "parser", ".", "add_argument", "(", "'--dropout'", ",", "default", "=", "0.1", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability'", ")", "\n", "parser", ".", "add_argument", "(", "'--attention-dropout'", ",", "default", "=", "0.", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability for attention weights'", ")", "\n", "parser", ".", "add_argument", "(", "'--relu-dropout'", ",", "default", "=", "0.", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability after ReLU in FFN'", ")", "\n", "parser", ".", "add_argument", "(", "'--input-dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability of the inputs'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-output-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder output dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-input-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder input dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-ffn-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'decoder embedding dimension for FFN'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-layers'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'num decoder layers'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-attention-heads'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'num decoder attention heads or LightConv/DynamicConv heads'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-normalize-before'", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'apply layernorm before each decoder block'", ")", "\n", "parser", ".", "add_argument", "(", "'--adaptive-softmax-cutoff'", ",", "metavar", "=", "'EXPR'", ",", "\n", "help", "=", "'comma separated list of adaptive softmax cutoff points. '", "\n", "'Must be used with adaptive_loss criterion'", ")", "\n", "parser", ".", "add_argument", "(", "'--adaptive-softmax-dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'sets adaptive softmax dropout for the tail projections'", ")", "\n", "parser", ".", "add_argument", "(", "'--adaptive-softmax-factor'", ",", "type", "=", "float", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'adaptive input factor'", ")", "\n", "parser", ".", "add_argument", "(", "'--no-token-positional-embeddings'", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'if set, disables positional embeddings (outside self attention)'", ")", "\n", "parser", ".", "add_argument", "(", "'--share-decoder-input-output-embed'", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'share decoder input and output embeddings'", ")", "\n", "parser", ".", "add_argument", "(", "'--character-embeddings'", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'if set, uses character embedding convolutions to produce token embeddings'", ")", "\n", "parser", ".", "add_argument", "(", "'--character-filters'", ",", "type", "=", "str", ",", "metavar", "=", "'LIST'", ",", "\n", "default", "=", "'[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]'", ",", "\n", "help", "=", "'size of character embeddings'", ")", "\n", "parser", ".", "add_argument", "(", "'--character-embedding-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "default", "=", "4", ",", "\n", "help", "=", "'size of character embeddings'", ")", "\n", "parser", ".", "add_argument", "(", "'--char-embedder-highway-layers'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "default", "=", "2", ",", "\n", "help", "=", "'number of highway layers for character token embeddder'", ")", "\n", "parser", ".", "add_argument", "(", "'--adaptive-input'", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'if set, uses adaptive input'", ")", "\n", "parser", ".", "add_argument", "(", "'--adaptive-input-factor'", ",", "type", "=", "float", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'adaptive input factor'", ")", "\n", "parser", ".", "add_argument", "(", "'--adaptive-input-cutoff'", ",", "metavar", "=", "'EXPR'", ",", "\n", "help", "=", "'comma separated list of adaptive input cutoff points.'", ")", "\n", "parser", ".", "add_argument", "(", "'--tie-adaptive-weights'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'if set, ties the weights of adaptive softmax and adaptive input'", ")", "\n", "parser", ".", "add_argument", "(", "'--tie-adaptive-proj'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'if set, ties the projection weights of adaptive softmax and adaptive input'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-learned-pos'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'use learned positional embeddings in the decoder'", ")", "\n", "\n", "\"\"\"LightConv and DynamicConv arguments\"\"\"", "\n", "parser", ".", "add_argument", "(", "'--decoder-kernel-size-list'", ",", "type", "=", "lambda", "x", ":", "options", ".", "eval_str_list", "(", "x", ",", "int", ")", ",", "\n", "help", "=", "'list of kernel size (default: \"[3,7,15,31,31,31]\")'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-glu'", ",", "type", "=", "options", ".", "eval_bool", ",", "\n", "help", "=", "'glu after in proj'", ")", "\n", "parser", ".", "add_argument", "(", "'--decoder-conv-type'", ",", "default", "=", "'dynamic'", ",", "type", "=", "str", ",", "\n", "choices", "=", "[", "'dynamic'", ",", "'lightweight'", "]", ",", "\n", "help", "=", "'type of convolution'", ")", "\n", "parser", ".", "add_argument", "(", "'--weight-softmax'", ",", "default", "=", "True", ",", "type", "=", "options", ".", "eval_bool", ")", "\n", "parser", ".", "add_argument", "(", "'--weight-dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability for conv weights'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv_lm.LightConvLanguageModel.build_model": [[97, 131], ["lightconv_lm.base_lm_architecture", "fairseq.models.lightconv.LightConvDecoder", "lightconv_lm.LightConvLanguageModel", "hasattr", "hasattr", "fairseq.modules.CharacterTokenEmbedder", "eval", "fairseq.modules.AdaptiveInput", "fairseq.models.lightconv.Embedding", "len", "task.dictionary.pad", "fairseq.options.eval_str_list", "len", "task.dictionary.pad"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv_lm.base_lm_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.Embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.eval_str_list", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad"], ["", "@", "classmethod", "\n", "def", "build_model", "(", "cls", ",", "args", ",", "task", ")", ":", "\n", "        ", "\"\"\"Build a new model instance.\"\"\"", "\n", "\n", "# make sure all arguments are present in older models", "\n", "base_lm_architecture", "(", "args", ")", "\n", "\n", "if", "not", "hasattr", "(", "args", ",", "'max_source_positions'", ")", ":", "\n", "            ", "args", ".", "max_source_positions", "=", "args", ".", "tokens_per_sample", "\n", "", "if", "not", "hasattr", "(", "args", ",", "'max_target_positions'", ")", ":", "\n", "            ", "args", ".", "max_target_positions", "=", "args", ".", "tokens_per_sample", "\n", "\n", "", "if", "args", ".", "character_embeddings", ":", "\n", "            ", "embed_tokens", "=", "CharacterTokenEmbedder", "(", "task", ".", "dictionary", ",", "eval", "(", "args", ".", "character_filters", ")", ",", "\n", "args", ".", "character_embedding_dim", ",", "\n", "args", ".", "decoder_embed_dim", ",", "\n", "args", ".", "char_embedder_highway_layers", ",", "\n", ")", "\n", "", "elif", "args", ".", "adaptive_input", ":", "\n", "            ", "embed_tokens", "=", "AdaptiveInput", "(", "len", "(", "task", ".", "dictionary", ")", ",", "task", ".", "dictionary", ".", "pad", "(", ")", ",", "args", ".", "decoder_input_dim", ",", "\n", "args", ".", "adaptive_input_factor", ",", "args", ".", "decoder_embed_dim", ",", "\n", "options", ".", "eval_str_list", "(", "args", ".", "adaptive_input_cutoff", ",", "type", "=", "int", ")", ")", "\n", "", "else", ":", "\n", "            ", "embed_tokens", "=", "Embedding", "(", "len", "(", "task", ".", "dictionary", ")", ",", "args", ".", "decoder_input_dim", ",", "task", ".", "dictionary", ".", "pad", "(", ")", ")", "\n", "\n", "", "if", "args", ".", "tie_adaptive_weights", ":", "\n", "            ", "assert", "args", ".", "adaptive_input", "\n", "assert", "args", ".", "adaptive_input_factor", "==", "args", ".", "adaptive_softmax_factor", "\n", "assert", "args", ".", "adaptive_softmax_cutoff", "==", "args", ".", "adaptive_input_cutoff", ",", "'{} != {}'", ".", "format", "(", "\n", "args", ".", "adaptive_softmax_cutoff", ",", "args", ".", "adaptive_input_cutoff", ")", "\n", "assert", "args", ".", "decoder_input_dim", "==", "args", ".", "decoder_output_dim", "\n", "\n", "", "decoder", "=", "LightConvDecoder", "(", "args", ",", "task", ".", "output_dictionary", ",", "embed_tokens", ",", "no_encoder_attn", "=", "True", ",", "final_norm", "=", "False", ")", "\n", "return", "LightConvLanguageModel", "(", "decoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv_lm.base_lm_architecture": [[133, 162], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "getattr", "len"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture"], ["", "", "@", "register_model_architecture", "(", "'lightconv_lm'", ",", "'lightconv_lm'", ")", "\n", "def", "base_lm_architecture", "(", "args", ")", ":", "\n", "    ", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "512", ")", "\n", "args", ".", "decoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_ffn_embed_dim'", ",", "2048", ")", "\n", "args", ".", "decoder_layers", "=", "getattr", "(", "args", ",", "'decoder_layers'", ",", "6", ")", "\n", "args", ".", "decoder_attention_heads", "=", "getattr", "(", "args", ",", "'decoder_attention_heads'", ",", "8", ")", "\n", "args", ".", "adaptive_softmax_cutoff", "=", "getattr", "(", "args", ",", "'adaptive_softmax_cutoff'", ",", "None", ")", "\n", "args", ".", "adaptive_softmax_dropout", "=", "getattr", "(", "args", ",", "'adaptive_softmax_dropout'", ",", "0", ")", "\n", "args", ".", "adaptive_softmax_factor", "=", "getattr", "(", "args", ",", "'adaptive_softmax_factor'", ",", "4", ")", "\n", "args", ".", "decoder_learned_pos", "=", "getattr", "(", "args", ",", "'decoder_learned_pos'", ",", "False", ")", "\n", "\n", "args", ".", "character_embeddings", "=", "getattr", "(", "args", ",", "'character_embeddings'", ",", "False", ")", "\n", "\n", "args", ".", "decoder_output_dim", "=", "getattr", "(", "args", ",", "'decoder_output_dim'", ",", "args", ".", "decoder_embed_dim", ")", "\n", "args", ".", "decoder_input_dim", "=", "getattr", "(", "args", ",", "'decoder_input_dim'", ",", "args", ".", "decoder_embed_dim", ")", "\n", "\n", "# The model training is not stable without this", "\n", "args", ".", "decoder_normalize_before", "=", "True", "\n", "\n", "args", ".", "adaptive_input", "=", "getattr", "(", "args", ",", "'adaptive_input'", ",", "False", ")", "\n", "args", ".", "adaptive_input_factor", "=", "getattr", "(", "args", ",", "'adaptive_input_factor'", ",", "4", ")", "\n", "args", ".", "adaptive_input_cutoff", "=", "getattr", "(", "args", ",", "'adaptive_input_cutoff'", ",", "None", ")", "\n", "\n", "args", ".", "tie_adaptive_weights", "=", "getattr", "(", "args", ",", "'tie_adaptive_weights'", ",", "False", ")", "\n", "args", ".", "tie_adaptive_proj", "=", "getattr", "(", "args", ",", "'tie_adaptive_proj'", ",", "False", ")", "\n", "\n", "args", ".", "decoder_kernel_size_list", "=", "getattr", "(", "args", ",", "'decoder_kernel_size_list'", ",", "[", "3", ",", "7", ",", "15", ",", "31", ",", "31", ",", "31", "]", ")", "\n", "if", "len", "(", "args", ".", "decoder_kernel_size_list", ")", "==", "1", ":", "\n", "        ", "args", ".", "decoder_kernel_size_list", "=", "args", ".", "decoder_kernel_size_list", "*", "args", ".", "decoder_layers", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv_lm.lightconv_lm_gbw": [[164, 172], ["fairseq.models.register_model_architecture", "getattr", "getattr", "getattr", "getattr", "getattr", "lightconv_lm.base_lm_architecture"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.__init__.register_model_architecture", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv_lm.base_lm_architecture"], ["", "", "@", "register_model_architecture", "(", "'lightconv_lm'", ",", "'lightconv_lm_gbw'", ")", "\n", "def", "lightconv_lm_gbw", "(", "args", ")", ":", "\n", "    ", "args", ".", "decoder_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_embed_dim'", ",", "512", ")", "\n", "args", ".", "dropout", "=", "getattr", "(", "args", ",", "'dropout'", ",", "0.1", ")", "\n", "args", ".", "attention_dropout", "=", "getattr", "(", "args", ",", "'attention_dropout'", ",", "0.1", ")", "\n", "args", ".", "decoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'decoder_ffn_embed_dim'", ",", "4096", ")", "\n", "args", ".", "decoder_attention_heads", "=", "getattr", "(", "args", ",", "'decoder_attention_heads'", ",", "16", ")", "\n", "base_lm_architecture", "(", "args", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.distributed_fairseq_model.DistributedFairseqModel": [[14, 67], ["isinstance", "_DistributedFairseqModel", "dict", "dict", "ValueError", "super().__init__", "super().__getattr__", "hasattr", "super().__getattr__", "inspect.getargspec", "inspect.getargspec", "getattr"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["def", "DistributedFairseqModel", "(", "args", ",", "model", ")", ":", "\n", "    ", "\"\"\"\n    Wrap a *model* to support distributed data parallel training.\n\n    This is similar to the built-in DistributedDataParallel, but allows\n    additional configuration of the DistributedDataParallel class to\n    use, and also provides easier access to the wrapped model by\n    forwarding requests for missing attributes to the wrapped model.\n\n    Args:\n        args (argparse.Namespace): fairseq args\n        model (BaseFairseqModel): model to wrap\n    \"\"\"", "\n", "# determine which DDP class to extend", "\n", "assert", "isinstance", "(", "model", ",", "BaseFairseqModel", ")", "\n", "if", "args", ".", "ddp_backend", "==", "'c10d'", ":", "\n", "        ", "ddp_class", "=", "parallel", ".", "DistributedDataParallel", "\n", "init_kwargs", "=", "dict", "(", "\n", "module", "=", "model", ",", "\n", "device_ids", "=", "[", "args", ".", "device_id", "]", ",", "\n", "output_device", "=", "args", ".", "device_id", ",", "\n", "broadcast_buffers", "=", "False", ",", "\n", "bucket_cap_mb", "=", "args", ".", "bucket_cap_mb", ",", "\n", ")", "\n", "# Maintain backward compatibility", "\n", "if", "'check_reduction'", "in", "inspect", ".", "getargspec", "(", "ddp_class", ")", "[", "0", "]", ":", "\n", "            ", "init_kwargs", "[", "'check_reduction'", "]", "=", "True", "\n", "", "if", "'find_unused_parameters'", "in", "inspect", ".", "getargspec", "(", "ddp_class", ")", "[", "0", "]", ":", "\n", "            ", "init_kwargs", "[", "'find_unused_parameters'", "]", "=", "args", ".", "find_unused_parameters", "\n", "", "", "elif", "args", ".", "ddp_backend", "==", "'no_c10d'", ":", "\n", "        ", "ddp_class", "=", "LegacyDistributedDataParallel", "\n", "init_kwargs", "=", "dict", "(", "\n", "module", "=", "model", ",", "\n", "world_size", "=", "args", ".", "distributed_world_size", ",", "\n", "buffer_size", "=", "2", "**", "28", ",", "\n", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "'Unknown --ddp-backend: '", "+", "args", ".", "ddp_backend", ")", "\n", "\n", "", "class", "_DistributedFairseqModel", "(", "ddp_class", ")", ":", "\n", "        ", "\"\"\"Extend DistributedDataParallel to check for missing\n        attributes in the wrapped module.\"\"\"", "\n", "\n", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "            ", "super", "(", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n", "", "def", "__getattr__", "(", "self", ",", "name", ")", ":", "\n", "            ", "wrapped_module", "=", "super", "(", ")", ".", "__getattr__", "(", "'module'", ")", "\n", "if", "hasattr", "(", "wrapped_module", ",", "name", ")", ":", "\n", "                ", "return", "getattr", "(", "wrapped_module", ",", "name", ")", "\n", "", "return", "super", "(", ")", ".", "__getattr__", "(", "name", ")", "\n", "\n", "", "", "return", "_DistributedFairseqModel", "(", "**", "init_kwargs", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.cross_entropy_acc.CrossEntropyWithAccCriterion.__init__": [[19, 21], ["fairseq.criterions.FairseqCriterion.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "task", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "task", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.cross_entropy_acc.CrossEntropyWithAccCriterion.compute_loss": [[22, 44], ["target.view.view.view", "model.get_normalized_probs", "getattr", "lprobs.transpose.transpose.view", "torch.nll_loss", "torch.nll_loss", "hasattr", "logging.warning", "lprobs.transpose.transpose.transpose", "lprobs.transpose.transpose.size"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_decoder.FairseqDecoder.get_normalized_probs", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "compute_loss", "(", "self", ",", "model", ",", "net_output", ",", "target", ",", "reduction", ",", "log_probs", ")", ":", "\n", "# N, T -> N * T", "\n", "        ", "target", "=", "target", ".", "view", "(", "-", "1", ")", "\n", "lprobs", "=", "model", ".", "get_normalized_probs", "(", "net_output", ",", "log_probs", "=", "log_probs", ")", "\n", "if", "not", "hasattr", "(", "lprobs", ",", "\"batch_first\"", ")", ":", "\n", "            ", "logging", ".", "warning", "(", "\n", "\"ERROR: we need to know whether \"", "\n", "\"batch first for the net output; \"", "\n", "\"you need to set batch_first attribute for the return value of \"", "\n", "\"model.get_normalized_probs. Now, we assume this is true, but \"", "\n", "\"in the future, we will raise exception instead. \"", "\n", ")", "\n", "", "batch_first", "=", "getattr", "(", "lprobs", ",", "\"batch_first\"", ",", "True", ")", "\n", "if", "not", "batch_first", ":", "\n", "            ", "lprobs", "=", "lprobs", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "# N, T, D -> N * T, D", "\n", "", "lprobs", "=", "lprobs", ".", "view", "(", "-", "1", ",", "lprobs", ".", "size", "(", "-", "1", ")", ")", "\n", "loss", "=", "F", ".", "nll_loss", "(", "\n", "lprobs", ",", "target", ",", "ignore_index", "=", "self", ".", "padding_idx", ",", "reduction", "=", "reduction", "\n", ")", "\n", "return", "lprobs", ",", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.cross_entropy_acc.CrossEntropyWithAccCriterion.get_logging_output": [[45, 67], ["target.view.view.view", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "sample[].size", "fairseq.utils.item", "sample[].size", "fairseq.utils.item", "fairseq.utils.item", "torch.sum().item", "torch.sum().item", "torch.sum().item", "torch.sum().item", "lprobs.argmax().masked_select", "target.view.view.masked_select", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "lprobs.argmax"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.argmax"], ["", "def", "get_logging_output", "(", "self", ",", "sample", ",", "target", ",", "lprobs", ",", "loss", ")", ":", "\n", "        ", "target", "=", "target", ".", "view", "(", "-", "1", ")", "\n", "mask", "=", "target", "!=", "self", ".", "padding_idx", "\n", "correct", "=", "torch", ".", "sum", "(", "\n", "lprobs", ".", "argmax", "(", "1", ")", ".", "masked_select", "(", "mask", ")", "==", "target", ".", "masked_select", "(", "mask", ")", "\n", ")", "\n", "total", "=", "torch", ".", "sum", "(", "mask", ")", "\n", "sample_size", "=", "(", "\n", "sample", "[", "\"target\"", "]", ".", "size", "(", "0", ")", "if", "self", ".", "args", ".", "sentence_avg", "else", "sample", "[", "\"ntokens\"", "]", "\n", ")", "\n", "\n", "logging_output", "=", "{", "\n", "\"loss\"", ":", "utils", ".", "item", "(", "loss", ".", "data", ")", ",", "# * sample['ntokens'],", "\n", "\"ntokens\"", ":", "sample", "[", "\"ntokens\"", "]", ",", "\n", "\"nsentences\"", ":", "sample", "[", "\"target\"", "]", ".", "size", "(", "0", ")", ",", "\n", "\"sample_size\"", ":", "sample_size", ",", "\n", "\"correct\"", ":", "utils", ".", "item", "(", "correct", ".", "data", ")", ",", "\n", "\"total\"", ":", "utils", ".", "item", "(", "total", ".", "data", ")", ",", "\n", "\"nframes\"", ":", "torch", ".", "sum", "(", "sample", "[", "\"net_input\"", "]", "[", "\"src_lengths\"", "]", ")", ".", "item", "(", ")", ",", "\n", "}", "\n", "\n", "return", "sample_size", ",", "logging_output", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.cross_entropy_acc.CrossEntropyWithAccCriterion.forward": [[68, 100], ["model", "model.get_targets", "cross_entropy_acc.CrossEntropyWithAccCriterion.compute_loss", "cross_entropy_acc.CrossEntropyWithAccCriterion.get_logging_output"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.model", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.wav2vec.Wav2VecModel.get_targets", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.label_smoothed_cross_entropy.LabelSmoothedCrossEntropyCriterion.compute_loss", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.cross_entropy_acc.CrossEntropyWithAccCriterion.get_logging_output"], ["", "def", "forward", "(", "self", ",", "model", ",", "sample", ",", "reduction", "=", "\"sum\"", ",", "log_probs", "=", "True", ")", ":", "\n", "        ", "\"\"\"Computes the cross entropy with accuracy metric for the given sample.\n\n        This is similar to CrossEntropyCriterion in fairseq, but also\n        computes accuracy metrics as part of logging\n\n        Args:\n            logprobs (Torch.tensor) of shape N, T, D i.e.\n                batchsize, timesteps, dimensions\n            targets (Torch.tensor) of shape N, T  i.e batchsize, timesteps\n\n        Returns:\n        tuple: With three elements:\n            1) the loss\n            2) the sample size, which is used as the denominator for the gradient\n            3) logging outputs to display while training\n\n        TODO:\n            * Currently this Criterion will only work with LSTMEncoderModels or\n            FairseqModels which have decoder, or Models which return TorchTensor\n            as net_output.\n            We need to make a change to support all FairseqEncoder models.\n        \"\"\"", "\n", "net_output", "=", "model", "(", "**", "sample", "[", "\"net_input\"", "]", ")", "\n", "target", "=", "model", ".", "get_targets", "(", "sample", ",", "net_output", ")", "\n", "lprobs", ",", "loss", "=", "self", ".", "compute_loss", "(", "\n", "model", ",", "net_output", ",", "target", ",", "reduction", ",", "log_probs", "\n", ")", "\n", "sample_size", ",", "logging_output", "=", "self", ".", "get_logging_output", "(", "\n", "sample", ",", "target", ",", "lprobs", ",", "loss", "\n", ")", "\n", "return", "loss", ",", "sample_size", ",", "logging_output", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.cross_entropy_acc.CrossEntropyWithAccCriterion.aggregate_logging_outputs": [[101, 130], ["sum", "sum", "sum", "sum", "sum", "sum", "sum", "log.get", "log.get", "log.get", "log.get", "log.get", "log.get", "log.get", "math.log", "math.log"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.log"], ["", "@", "staticmethod", "\n", "def", "aggregate_logging_outputs", "(", "logging_outputs", ")", ":", "\n", "        ", "\"\"\"Aggregate logging outputs from data parallel training.\"\"\"", "\n", "correct_sum", "=", "sum", "(", "log", ".", "get", "(", "\"correct\"", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "total_sum", "=", "sum", "(", "log", ".", "get", "(", "\"total\"", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "loss_sum", "=", "sum", "(", "log", ".", "get", "(", "\"loss\"", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "ntokens", "=", "sum", "(", "log", ".", "get", "(", "\"ntokens\"", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "nsentences", "=", "sum", "(", "log", ".", "get", "(", "\"nsentences\"", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "sample_size", "=", "sum", "(", "log", ".", "get", "(", "\"sample_size\"", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "nframes", "=", "sum", "(", "log", ".", "get", "(", "\"nframes\"", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "agg_output", "=", "{", "\n", "\"loss\"", ":", "loss_sum", "/", "sample_size", "/", "math", ".", "log", "(", "2", ")", "if", "sample_size", ">", "0", "else", "0.0", ",", "\n", "# if args.sentence_avg, then sample_size is nsentences, then loss", "\n", "# is per-sentence loss; else sample_size is ntokens, the loss", "\n", "# becomes per-output token loss", "\n", "\"ntokens\"", ":", "ntokens", ",", "\n", "\"nsentences\"", ":", "nsentences", ",", "\n", "\"nframes\"", ":", "nframes", ",", "\n", "\"sample_size\"", ":", "sample_size", ",", "\n", "\"acc\"", ":", "correct_sum", "*", "100.0", "/", "total_sum", "if", "total_sum", ">", "0", "else", "0.0", ",", "\n", "\"correct\"", ":", "correct_sum", ",", "\n", "\"total\"", ":", "total_sum", ",", "\n", "# total is the number of validate tokens", "\n", "}", "\n", "if", "sample_size", "!=", "ntokens", ":", "\n", "            ", "agg_output", "[", "\"nll_loss\"", "]", "=", "loss_sum", "/", "ntokens", "/", "math", ".", "log", "(", "2", ")", "\n", "# loss: per output token loss", "\n", "# nll_loss: per sentence loss", "\n", "", "return", "agg_output", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.legacy_masked_lm.LegacyMaskedLmLoss.__init__": [[51, 53], ["FairseqCriterion.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["        ", "return", "BertDictionary", ".", "load", "(", "filename", ")", "\n", "\n", "", "@", "classmethod", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.legacy_masked_lm.LegacyMaskedLmLoss.add_args": [[54, 62], ["parser.add_argument", "parser.add_argument"], "methods", ["None"], ["def", "build_dictionary", "(", "cls", ",", "filenames", ",", "workers", "=", "1", ",", "threshold", "=", "-", "1", ",", "nwords", "=", "-", "1", ",", "padding_factor", "=", "8", ")", ":", "\n", "        ", "d", "=", "BertDictionary", "(", ")", "\n", "for", "filename", "in", "filenames", ":", "\n", "            ", "Dictionary", ".", "add_file_to_dictionary", "(", "filename", ",", "d", ",", "tokenizer", ".", "tokenize_line", ",", "workers", ")", "\n", "", "d", ".", "finalize", "(", "threshold", "=", "threshold", ",", "nwords", "=", "nwords", ",", "padding_factor", "=", "padding_factor", ")", "\n", "return", "d", "\n", "\n", "", "@", "property", "\n", "def", "target_dictionary", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.legacy_masked_lm.LegacyMaskedLmLoss.forward": [[64, 126], ["model", "lm_logits.view.view.view", "sample[].view", "legacy_masked_lm.compute_cross_entropy_loss", "fairseq.utils.strip_pad().numel", "lm_logits.view.view.size", "sample[].view", "sample[].view.size", "fairseq.utils.strip_pad", "legacy_masked_lm.compute_cross_entropy_loss", "fairseq.utils.item", "fairseq.utils.item", "fairseq.utils.item"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.model", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.legacy_masked_lm.compute_cross_entropy_loss", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.strip_pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.legacy_masked_lm.compute_cross_entropy_loss", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item"], ["\n", "", "@", "classmethod", "\n", "def", "setup_task", "(", "cls", ",", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Setup the task.\n        \"\"\"", "\n", "paths", "=", "args", ".", "data", ".", "split", "(", "':'", ")", "\n", "assert", "len", "(", "paths", ")", ">", "0", "\n", "dictionary", "=", "BertDictionary", ".", "load", "(", "os", ".", "path", ".", "join", "(", "paths", "[", "0", "]", ",", "'dict.txt'", ")", ")", "\n", "print", "(", "'| dictionary: {} types'", ".", "format", "(", "len", "(", "dictionary", ")", ")", ")", "\n", "\n", "return", "cls", "(", "args", ",", "dictionary", ")", "\n", "\n", "", "def", "load_dataset", "(", "self", ",", "split", ",", "epoch", "=", "0", ",", "combine", "=", "False", ")", ":", "\n", "        ", "\"\"\"Load a given dataset split.\n        Args:\n            split (str): name of the split (e.g., train, valid, test)\n        \"\"\"", "\n", "loaded_datasets", "=", "[", "]", "\n", "\n", "paths", "=", "self", ".", "args", ".", "data", ".", "split", "(", "':'", ")", "\n", "assert", "len", "(", "paths", ")", ">", "0", "\n", "data_path", "=", "paths", "[", "epoch", "%", "len", "(", "paths", ")", "]", "\n", "print", "(", "\"| data_path\"", ",", "data_path", ")", "\n", "\n", "for", "k", "in", "itertools", ".", "count", "(", ")", ":", "\n", "            ", "split_k", "=", "split", "+", "(", "str", "(", "k", ")", "if", "k", ">", "0", "else", "''", ")", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "split_k", ")", "\n", "ds", "=", "indexed_dataset", ".", "make_dataset", "(", "\n", "path", ",", "\n", "impl", "=", "self", ".", "args", ".", "dataset_impl", ",", "\n", "fix_lua_indexing", "=", "True", ",", "\n", "dictionary", "=", "self", ".", "dictionary", ",", "\n", ")", "\n", "\n", "if", "ds", "is", "None", ":", "\n", "                ", "if", "k", ">", "0", ":", "\n", "                    ", "break", "\n", "", "else", ":", "\n", "                    ", "raise", "FileNotFoundError", "(", "'Dataset not found: {} ({})'", ".", "format", "(", "split", ",", "data_path", ")", ")", "\n", "\n", "", "", "with", "data_utils", ".", "numpy_seed", "(", "self", ".", "seed", "+", "k", ")", ":", "\n", "                ", "loaded_datasets", ".", "append", "(", "\n", "BlockPairDataset", "(", "\n", "ds", ",", "\n", "self", ".", "dictionary", ",", "\n", "ds", ".", "sizes", ",", "\n", "self", ".", "args", ".", "tokens_per_sample", ",", "\n", "break_mode", "=", "self", ".", "args", ".", "break_mode", ",", "\n", "doc_break_size", "=", "1", ",", "\n", ")", "\n", ")", "\n", "\n", "", "print", "(", "'| {} {} {} examples'", ".", "format", "(", "data_path", ",", "split_k", ",", "len", "(", "loaded_datasets", "[", "-", "1", "]", ")", ")", ")", "\n", "\n", "if", "not", "combine", ":", "\n", "                ", "break", "\n", "\n", "", "", "if", "len", "(", "loaded_datasets", ")", "==", "1", ":", "\n", "            ", "dataset", "=", "loaded_datasets", "[", "0", "]", "\n", "sizes", "=", "dataset", ".", "sizes", "\n", "", "else", ":", "\n", "            ", "dataset", "=", "ConcatDataset", "(", "loaded_datasets", ")", "\n", "sizes", "=", "np", ".", "concatenate", "(", "[", "ds", ".", "sizes", "for", "ds", "in", "loaded_datasets", "]", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.legacy_masked_lm.LegacyMaskedLmLoss.aggregate_logging_outputs": [[127, 148], ["sum", "sum", "sum", "sum", "sum", "sum", "log.get", "log.get", "log.get", "log.get", "log.get", "log.get", "math.log", "math.log", "math.log", "math.log"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.log"], ["\n", "", "self", ".", "datasets", "[", "split", "]", "=", "MaskedLMDataset", "(", "\n", "dataset", "=", "dataset", ",", "\n", "sizes", "=", "sizes", ",", "\n", "vocab", "=", "self", ".", "dictionary", ",", "\n", "pad_idx", "=", "self", ".", "dictionary", ".", "pad", "(", ")", ",", "\n", "mask_idx", "=", "self", ".", "dictionary", ".", "mask", "(", ")", ",", "\n", "classif_token_idx", "=", "self", ".", "dictionary", ".", "cls", "(", ")", ",", "\n", "sep_token_idx", "=", "self", ".", "dictionary", ".", "sep", "(", ")", ",", "\n", "shuffle", "=", "self", ".", "args", ".", "shuffle_dataset", ",", "\n", "seed", "=", "self", ".", "seed", ",", "\n", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.legacy_masked_lm.compute_cross_entropy_loss": [[15, 31], ["torch.nll_loss", "logits.size", "targets.size", "torch.log_softmax"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.log_softmax"], [")", "\n", "\n", "from", "fairseq", ".", "data", "import", "Dictionary", "\n", "from", "fairseq", ".", "data", ".", "legacy", ".", "block_pair_dataset", "import", "BlockPairDataset", "\n", "from", "fairseq", ".", "data", ".", "legacy", ".", "masked_lm_dataset", "import", "MaskedLMDataset", "\n", "from", "fairseq", ".", "data", ".", "legacy", ".", "masked_lm_dictionary", "import", "BertDictionary", "\n", "\n", "from", ".", "import", "FairseqTask", ",", "register_task", "\n", "\n", "\n", "@", "register_task", "(", "'legacy_masked_lm'", ")", "\n", "class", "LegacyMaskedLMTask", "(", "FairseqTask", ")", ":", "\n", "    ", "\"\"\"\n    Task for training Masked LM (BERT) model.\n    Args:\n        dictionary (Dictionary): the dictionary for the input of the task\n    \"\"\"", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.fairseq_criterion.FairseqCriterion.__init__": [[11, 16], ["torch.nn.modules.loss._Loss.__init__", "task.target_dictionary.pad"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "task", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "args", "=", "args", "\n", "self", ".", "task", "=", "task", "\n", "self", ".", "padding_idx", "=", "task", ".", "target_dictionary", ".", "pad", "(", ")", "if", "task", ".", "target_dictionary", "is", "not", "None", "else", "-", "100", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.fairseq_criterion.FairseqCriterion.add_args": [[17, 21], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add criterion-specific arguments to the parser.\"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.fairseq_criterion.FairseqCriterion.build_criterion": [[22, 25], ["cls"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.masked_lm_dictionary.BertDictionary.cls"], ["", "@", "classmethod", "\n", "def", "build_criterion", "(", "cls", ",", "args", ",", "task", ")", ":", "\n", "        ", "return", "cls", "(", "args", ",", "task", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.fairseq_criterion.FairseqCriterion.forward": [[26, 35], ["None"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "model", ",", "sample", ",", "reduce", "=", "True", ")", ":", "\n", "        ", "\"\"\"Compute the loss for the given sample.\n\n        Returns a tuple with three elements:\n        1) the loss\n        2) the sample size, which is used as the denominator for the gradient\n        3) logging outputs to display while training\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.fairseq_criterion.FairseqCriterion.aggregate_logging_outputs": [[36, 40], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "aggregate_logging_outputs", "(", "logging_outputs", ")", ":", "\n", "        ", "\"\"\"Aggregate logging outputs from data parallel training.\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.fairseq_criterion.FairseqCriterion.grad_denom": [[41, 45], ["sum"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "grad_denom", "(", "sample_sizes", ")", ":", "\n", "        ", "\"\"\"Compute the gradient denominator for a set of sample sizes.\"\"\"", "\n", "return", "sum", "(", "sample_sizes", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.cross_entropy.CrossEntropyCriterion.__init__": [[17, 19], ["FairseqCriterion.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "task", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "task", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.cross_entropy.CrossEntropyCriterion.forward": [[20, 38], ["model", "cross_entropy.CrossEntropyCriterion.compute_loss", "sample[].size", "sample[].size", "fairseq.utils.item"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.model", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.label_smoothed_cross_entropy.LabelSmoothedCrossEntropyCriterion.compute_loss", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item"], ["", "def", "forward", "(", "self", ",", "model", ",", "sample", ",", "reduce", "=", "True", ")", ":", "\n", "        ", "\"\"\"Compute the loss for the given sample.\n\n        Returns a tuple with three elements:\n        1) the loss\n        2) the sample size, which is used as the denominator for the gradient\n        3) logging outputs to display while training\n        \"\"\"", "\n", "net_output", "=", "model", "(", "**", "sample", "[", "'net_input'", "]", ")", "\n", "loss", ",", "_", "=", "self", ".", "compute_loss", "(", "model", ",", "net_output", ",", "sample", ",", "reduce", "=", "reduce", ")", "\n", "sample_size", "=", "sample", "[", "'target'", "]", ".", "size", "(", "0", ")", "if", "self", ".", "args", ".", "sentence_avg", "else", "sample", "[", "'ntokens'", "]", "\n", "logging_output", "=", "{", "\n", "'loss'", ":", "utils", ".", "item", "(", "loss", ".", "data", ")", "if", "reduce", "else", "loss", ".", "data", ",", "\n", "'ntokens'", ":", "sample", "[", "'ntokens'", "]", ",", "\n", "'nsentences'", ":", "sample", "[", "'target'", "]", ".", "size", "(", "0", ")", ",", "\n", "'sample_size'", ":", "sample_size", ",", "\n", "}", "\n", "return", "loss", ",", "sample_size", ",", "logging_output", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.cross_entropy.CrossEntropyCriterion.compute_loss": [[39, 50], ["model.get_normalized_probs", "lprobs.view.view.view", "model.get_targets().view", "torch.nll_loss", "lprobs.view.view.size", "model.get_targets"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_decoder.FairseqDecoder.get_normalized_probs", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.wav2vec.Wav2VecModel.get_targets"], ["", "def", "compute_loss", "(", "self", ",", "model", ",", "net_output", ",", "sample", ",", "reduce", "=", "True", ")", ":", "\n", "        ", "lprobs", "=", "model", ".", "get_normalized_probs", "(", "net_output", ",", "log_probs", "=", "True", ")", "\n", "lprobs", "=", "lprobs", ".", "view", "(", "-", "1", ",", "lprobs", ".", "size", "(", "-", "1", ")", ")", "\n", "target", "=", "model", ".", "get_targets", "(", "sample", ",", "net_output", ")", ".", "view", "(", "-", "1", ")", "\n", "loss", "=", "F", ".", "nll_loss", "(", "\n", "lprobs", ",", "\n", "target", ",", "\n", "ignore_index", "=", "self", ".", "padding_idx", ",", "\n", "reduction", "=", "'sum'", "if", "reduce", "else", "'none'", ",", "\n", ")", "\n", "return", "loss", ",", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.cross_entropy.CrossEntropyCriterion.aggregate_logging_outputs": [[51, 67], ["sum", "sum", "sum", "sum", "log.get", "log.get", "log.get", "log.get", "math.log", "math.log"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.log"], ["", "@", "staticmethod", "\n", "def", "aggregate_logging_outputs", "(", "logging_outputs", ")", ":", "\n", "        ", "\"\"\"Aggregate logging outputs from data parallel training.\"\"\"", "\n", "loss_sum", "=", "sum", "(", "log", ".", "get", "(", "'loss'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "ntokens", "=", "sum", "(", "log", ".", "get", "(", "'ntokens'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "nsentences", "=", "sum", "(", "log", ".", "get", "(", "'nsentences'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "sample_size", "=", "sum", "(", "log", ".", "get", "(", "'sample_size'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "agg_output", "=", "{", "\n", "'loss'", ":", "loss_sum", "/", "sample_size", "/", "math", ".", "log", "(", "2", ")", "if", "sample_size", ">", "0", "else", "0.", ",", "\n", "'ntokens'", ":", "ntokens", ",", "\n", "'nsentences'", ":", "nsentences", ",", "\n", "'sample_size'", ":", "sample_size", ",", "\n", "}", "\n", "if", "sample_size", "!=", "ntokens", ":", "\n", "            ", "agg_output", "[", "'nll_loss'", "]", "=", "loss_sum", "/", "ntokens", "/", "math", ".", "log", "(", "2", ")", "\n", "", "return", "agg_output", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.adaptive_loss.AdaptiveLoss.__init__": [[20, 26], ["FairseqCriterion.__init__", "Exception"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["def", "__init__", "(", "self", ",", "args", ",", "task", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "task", ")", "\n", "\n", "if", "args", ".", "ddp_backend", "==", "'c10d'", ":", "\n", "            ", "raise", "Exception", "(", "\n", "'AdaptiveLoss is not compatible with the c10d '", "\n", "'version of DistributedDataParallel. Please use '", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.adaptive_loss.AdaptiveLoss.forward": [[30, 75], ["model", "model.get_targets", "orig_target.view.view.size", "orig_target.view.view.view", "orig_target.view.view.size", "adaptive_softmax", "net_output[].new().zero_", "range", "fairseq.utils.strip_pad", "fairseq.utils.strip_pad.numel", "hasattr", "len", "len", "len", "sample[].size", "net_output[].new", "torch.cross_entropy", "fairseq.utils.item", "target[].min", "target[].max", "logits[].size"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.model", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.wav2vec.Wav2VecModel.get_targets", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.strip_pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "", "def", "forward", "(", "self", ",", "model", ",", "sample", ",", "reduce", "=", "True", ")", ":", "\n", "        ", "\"\"\"Compute the loss for the given sample.\n\n        Returns a tuple with three elements:\n        1) the loss\n        2) the sample size, which is used as the denominator for the gradient\n        3) logging outputs to display while training\n        \"\"\"", "\n", "\n", "assert", "hasattr", "(", "model", ".", "decoder", ",", "'adaptive_softmax'", ")", "and", "model", ".", "decoder", ".", "adaptive_softmax", "is", "not", "None", "\n", "adaptive_softmax", "=", "model", ".", "decoder", ".", "adaptive_softmax", "\n", "\n", "net_output", "=", "model", "(", "**", "sample", "[", "'net_input'", "]", ")", "\n", "orig_target", "=", "model", ".", "get_targets", "(", "sample", ",", "net_output", ")", "\n", "\n", "nsentences", "=", "orig_target", ".", "size", "(", "0", ")", "\n", "orig_target", "=", "orig_target", ".", "view", "(", "-", "1", ")", "\n", "\n", "bsz", "=", "orig_target", ".", "size", "(", "0", ")", "\n", "\n", "logits", ",", "target", "=", "adaptive_softmax", "(", "net_output", "[", "0", "]", ",", "orig_target", ")", "\n", "assert", "len", "(", "target", ")", "==", "len", "(", "logits", ")", "\n", "\n", "loss", "=", "net_output", "[", "0", "]", ".", "new", "(", "1", "if", "reduce", "else", "bsz", ")", ".", "zero_", "(", ")", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "target", ")", ")", ":", "\n", "            ", "if", "target", "[", "i", "]", "is", "not", "None", ":", "\n", "                ", "assert", "(", "target", "[", "i", "]", ".", "min", "(", ")", ">=", "0", "and", "target", "[", "i", "]", ".", "max", "(", ")", "<=", "logits", "[", "i", "]", ".", "size", "(", "1", ")", ")", "\n", "loss", "+=", "F", ".", "cross_entropy", "(", "\n", "logits", "[", "i", "]", ",", "\n", "target", "[", "i", "]", ",", "\n", "ignore_index", "=", "self", ".", "padding_idx", ",", "\n", "reduction", "=", "'sum'", "if", "reduce", "else", "'none'", ",", "\n", ")", "\n", "\n", "", "", "orig", "=", "utils", ".", "strip_pad", "(", "orig_target", ",", "self", ".", "padding_idx", ")", "\n", "ntokens", "=", "orig", ".", "numel", "(", ")", "\n", "sample_size", "=", "sample", "[", "'target'", "]", ".", "size", "(", "0", ")", "if", "self", ".", "args", ".", "sentence_avg", "else", "ntokens", "\n", "logging_output", "=", "{", "\n", "'loss'", ":", "utils", ".", "item", "(", "loss", ".", "data", ")", "if", "reduce", "else", "loss", ".", "data", ",", "\n", "'ntokens'", ":", "ntokens", ",", "\n", "'nsentences'", ":", "nsentences", ",", "\n", "'sample_size'", ":", "sample_size", ",", "\n", "}", "\n", "return", "loss", ",", "sample_size", ",", "logging_output", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.adaptive_loss.AdaptiveLoss.aggregate_logging_outputs": [[76, 93], ["sum", "sum", "sum", "sum", "log.get", "log.get", "log.get", "log.get", "math.log", "math.log", "math.log"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.log"], ["", "@", "staticmethod", "\n", "def", "aggregate_logging_outputs", "(", "logging_outputs", ")", ":", "\n", "        ", "\"\"\"Aggregate logging outputs from data parallel training.\"\"\"", "\n", "loss_sum", "=", "sum", "(", "log", ".", "get", "(", "'loss'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "ntokens", "=", "sum", "(", "log", ".", "get", "(", "'ntokens'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "nsentences", "=", "sum", "(", "log", ".", "get", "(", "'nsentences'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "sample_size", "=", "sum", "(", "log", ".", "get", "(", "'sample_size'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "agg_output", "=", "{", "\n", "'loss'", ":", "loss_sum", "/", "sample_size", "/", "math", ".", "log", "(", "2", ")", "if", "sample_size", ">", "0", "else", "0.", ",", "\n", "'nll_loss'", ":", "loss_sum", "/", "sample_size", "/", "math", ".", "log", "(", "2", ")", "if", "sample_size", ">", "0", "else", "0.", ",", "\n", "'ntokens'", ":", "ntokens", ",", "\n", "'nsentences'", ":", "nsentences", ",", "\n", "'sample_size'", ":", "sample_size", ",", "\n", "}", "\n", "if", "sample_size", "!=", "ntokens", ":", "\n", "            ", "agg_output", "[", "'nll_loss'", "]", "=", "loss_sum", "/", "ntokens", "/", "math", ".", "log", "(", "2", ")", "if", "ntokens", ">", "0", "else", "0.", "\n", "", "return", "agg_output", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.label_smoothed_cross_entropy.LabelSmoothedCrossEntropyCriterion.__init__": [[44, 47], ["FairseqCriterion.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "task", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "task", ")", "\n", "self", ".", "eps", "=", "args", ".", "label_smoothing", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.label_smoothed_cross_entropy.LabelSmoothedCrossEntropyCriterion.add_args": [[48, 54], ["parser.add_argument"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add criterion-specific arguments to the parser.\"\"\"", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'--label-smoothing'", ",", "default", "=", "0.", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'epsilon for label smoothing, 0 means no label smoothing'", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.label_smoothed_cross_entropy.LabelSmoothedCrossEntropyCriterion.forward": [[56, 75], ["model", "label_smoothed_cross_entropy.LabelSmoothedCrossEntropyCriterion.compute_loss", "sample[].size", "sample[].size", "fairseq.utils.item", "fairseq.utils.item"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.model", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.label_smoothed_cross_entropy.LabelSmoothedCrossEntropyCriterion.compute_loss", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item"], ["", "def", "forward", "(", "self", ",", "model", ",", "sample", ",", "reduce", "=", "True", ")", ":", "\n", "        ", "\"\"\"Compute the loss for the given sample.\n\n        Returns a tuple with three elements:\n        1) the loss\n        2) the sample size, which is used as the denominator for the gradient\n        3) logging outputs to display while training\n        \"\"\"", "\n", "net_output", "=", "model", "(", "**", "sample", "[", "'net_input'", "]", ")", "\n", "loss", ",", "nll_loss", "=", "self", ".", "compute_loss", "(", "model", ",", "net_output", ",", "sample", ",", "reduce", "=", "reduce", ")", "\n", "sample_size", "=", "sample", "[", "'target'", "]", ".", "size", "(", "0", ")", "if", "self", ".", "args", ".", "sentence_avg", "else", "sample", "[", "'ntokens'", "]", "\n", "logging_output", "=", "{", "\n", "'loss'", ":", "utils", ".", "item", "(", "loss", ".", "data", ")", "if", "reduce", "else", "loss", ".", "data", ",", "\n", "'nll_loss'", ":", "utils", ".", "item", "(", "nll_loss", ".", "data", ")", "if", "reduce", "else", "nll_loss", ".", "data", ",", "\n", "'ntokens'", ":", "sample", "[", "'ntokens'", "]", ",", "\n", "'nsentences'", ":", "sample", "[", "'target'", "]", ".", "size", "(", "0", ")", ",", "\n", "'sample_size'", ":", "sample_size", ",", "\n", "}", "\n", "return", "loss", ",", "sample_size", ",", "logging_output", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.label_smoothed_cross_entropy.LabelSmoothedCrossEntropyCriterion.compute_loss": [[76, 84], ["model.get_normalized_probs", "lprobs.view.view.view", "model.get_targets().view", "label_smoothed_cross_entropy.label_smoothed_nll_loss", "lprobs.view.view.size", "model.get_targets"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_decoder.FairseqDecoder.get_normalized_probs", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.label_smoothed_cross_entropy.label_smoothed_nll_loss", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.wav2vec.Wav2VecModel.get_targets"], ["", "def", "compute_loss", "(", "self", ",", "model", ",", "net_output", ",", "sample", ",", "reduce", "=", "True", ")", ":", "\n", "        ", "lprobs", "=", "model", ".", "get_normalized_probs", "(", "net_output", ",", "log_probs", "=", "True", ")", "\n", "lprobs", "=", "lprobs", ".", "view", "(", "-", "1", ",", "lprobs", ".", "size", "(", "-", "1", ")", ")", "\n", "target", "=", "model", ".", "get_targets", "(", "sample", ",", "net_output", ")", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "loss", ",", "nll_loss", "=", "label_smoothed_nll_loss", "(", "\n", "lprobs", ",", "target", ",", "self", ".", "eps", ",", "ignore_index", "=", "self", ".", "padding_idx", ",", "reduce", "=", "reduce", ",", "\n", ")", "\n", "return", "loss", ",", "nll_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.label_smoothed_cross_entropy.LabelSmoothedCrossEntropyCriterion.aggregate_logging_outputs": [[85, 97], ["sum", "sum", "sum", "log.get", "log.get", "log.get", "math.log", "math.log", "sum", "sum", "log.get", "log.get"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.log"], ["", "@", "staticmethod", "\n", "def", "aggregate_logging_outputs", "(", "logging_outputs", ")", ":", "\n", "        ", "\"\"\"Aggregate logging outputs from data parallel training.\"\"\"", "\n", "ntokens", "=", "sum", "(", "log", ".", "get", "(", "'ntokens'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "nsentences", "=", "sum", "(", "log", ".", "get", "(", "'nsentences'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "sample_size", "=", "sum", "(", "log", ".", "get", "(", "'sample_size'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "return", "{", "\n", "'loss'", ":", "sum", "(", "log", ".", "get", "(", "'loss'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "/", "sample_size", "/", "math", ".", "log", "(", "2", ")", "if", "sample_size", ">", "0", "else", "0.", ",", "\n", "'nll_loss'", ":", "sum", "(", "log", ".", "get", "(", "'nll_loss'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "/", "ntokens", "/", "math", ".", "log", "(", "2", ")", "if", "ntokens", ">", "0", "else", "0.", ",", "\n", "'ntokens'", ":", "ntokens", ",", "\n", "'nsentences'", ":", "nsentences", ",", "\n", "'sample_size'", ":", "sample_size", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.label_smoothed_cross_entropy.label_smoothed_nll_loss": [[13, 39], ["target[].unsqueeze.dim", "target[].unsqueeze.unsqueeze", "target[].unsqueeze", "lprobs.gather", "lprobs[].sum", "nll_loss.sum.squeeze", "smooth_loss.sum.squeeze", "nll_loss.sum.sum", "smooth_loss.sum.sum", "NotImplementedError", "lprobs.size", "lprobs.dim", "lprobs.dim", "target[].unsqueeze.ne().view", "target[].unsqueeze.ne", "target[].unsqueeze.ne", "float"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["def", "label_smoothed_nll_loss", "(", "lprobs", ",", "target", ",", "epsilon", ",", "ignore_index", "=", "None", ",", "reduce", "=", "True", ")", ":", "\n", "    ", "if", "target", ".", "dim", "(", ")", "==", "lprobs", ".", "dim", "(", ")", "-", "1", ":", "\n", "        ", "target", "=", "target", ".", "unsqueeze", "(", "-", "1", ")", "\n", "\n", "# remove entries of ignored indices", "\n", "", "if", "ignore_index", "is", "not", "None", ":", "\n", "# FIXME: May have broken other cases in orther to make the smoothed", "\n", "# loss suppor -Inf logits", "\n", "        ", "assert", "lprobs", ".", "dim", "(", ")", "==", "2", "\n", "lprobs", "=", "lprobs", "[", "target", ".", "ne", "(", "ignore_index", ")", ".", "view", "(", "-", "1", ")", ",", ":", "]", "\n", "target", "=", "target", "[", "target", ".", "ne", "(", "ignore_index", ")", "]", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "", "nll_loss", "=", "-", "lprobs", ".", "gather", "(", "dim", "=", "-", "1", ",", "index", "=", "target", ")", "\n", "# support -Inf", "\n", "smooth_loss", "=", "-", "lprobs", "[", "lprobs", "!=", "float", "(", "\"-Inf\"", ")", "]", ".", "sum", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "if", "ignore_index", "is", "None", ":", "\n", "        ", "nll_loss", "=", "nll_loss", ".", "squeeze", "(", "-", "1", ")", "\n", "smooth_loss", "=", "smooth_loss", ".", "squeeze", "(", "-", "1", ")", "\n", "", "if", "reduce", ":", "\n", "        ", "nll_loss", "=", "nll_loss", ".", "sum", "(", ")", "\n", "smooth_loss", "=", "smooth_loss", ".", "sum", "(", ")", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\"Suporting -Inf removed non reduce mode\"", ")", "\n", "", "eps_i", "=", "epsilon", "/", "lprobs", ".", "size", "(", "-", "1", ")", "\n", "loss", "=", "(", "1.", "-", "epsilon", ")", "*", "nll_loss", "+", "eps_i", "*", "smooth_loss", "\n", "return", "loss", ",", "nll_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.masked_lm.MaskedLmLoss.__init__": [[22, 24], ["FairseqCriterion.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["SortDataset", ",", "\n", "TokenBlockDataset", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.masked_lm.MaskedLmLoss.forward": [[25, 55], ["model.get_targets", "torch.nll_loss", "torch.nll_loss", "model.get_targets.ne().int().sum().item", "model", "torch.log_softmax", "torch.log_softmax", "model.get_targets.view", "logits.view", "model.get_targets.ne().int().sum", "fairseq.utils.item", "logits.size", "model.get_targets.ne().int", "model.get_targets.ne"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.wav2vec.Wav2VecModel.get_targets", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.model", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.log_softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.log_softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["from", "fairseq", ".", "tasks", "import", "FairseqTask", ",", "register_task", "\n", "\n", "\n", "@", "register_task", "(", "'masked_lm'", ")", "\n", "class", "MaskedLMTask", "(", "FairseqTask", ")", ":", "\n", "    ", "\"\"\"Task for training masked language models (e.g., BERT, RoBERTa).\"\"\"", "\n", "\n", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add task-specific arguments to the parser.\"\"\"", "\n", "parser", ".", "add_argument", "(", "'data'", ",", "help", "=", "'colon separated path to data directories list, \\\n                            will be iterated upon during epochs in round-robin manner'", ")", "\n", "parser", ".", "add_argument", "(", "'--sample-break-mode'", ",", "default", "=", "'complete'", ",", "\n", "choices", "=", "[", "'none'", ",", "'complete'", ",", "'complete_doc'", ",", "'eos'", "]", ",", "\n", "help", "=", "'If omitted or \"none\", fills each sample with tokens-per-sample '", "\n", "'tokens. If set to \"complete\", splits samples only at the end '", "\n", "'of sentence, but may include multiple sentences per sample. '", "\n", "'\"complete_doc\" is similar but respects doc boundaries. '", "\n", "'If set to \"eos\", includes only one sentence per sample.'", ")", "\n", "parser", ".", "add_argument", "(", "'--tokens-per-sample'", ",", "default", "=", "512", ",", "type", "=", "int", ",", "\n", "help", "=", "'max number of total tokens over all segments '", "\n", "'per sample for BERT dataset'", ")", "\n", "parser", ".", "add_argument", "(", "'--mask-prob'", ",", "default", "=", "0.15", ",", "type", "=", "float", ",", "\n", "help", "=", "'probability of replacing a token with mask'", ")", "\n", "parser", ".", "add_argument", "(", "'--leave-unmasked-prob'", ",", "default", "=", "0.1", ",", "type", "=", "float", ",", "\n", "help", "=", "'probability that a masked token is unmasked'", ")", "\n", "parser", ".", "add_argument", "(", "'--random-token-prob'", ",", "default", "=", "0.1", ",", "type", "=", "float", ",", "\n", "help", "=", "'probability of replacing a token with a random token'", ")", "\n", "parser", ".", "add_argument", "(", "'--freq-weighted-replacement'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'sample random replacement words based on word frequencies'", ")", "\n", "parser", ".", "add_argument", "(", "'--mask-whole-words'", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.masked_lm.MaskedLmLoss.aggregate_logging_outputs": [[56, 71], ["sum", "sum", "sum", "sum", "log.get", "log.get", "log.get", "log.get", "math.log"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.log"], ["help", "=", "'mask whole words; you may also want to set --bpe'", ")", "\n", "\n", "", "def", "__init__", "(", "self", ",", "args", ",", "dictionary", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ")", "\n", "self", ".", "dictionary", "=", "dictionary", "\n", "self", ".", "seed", "=", "args", ".", "seed", "\n", "\n", "# add mask token", "\n", "self", ".", "mask_idx", "=", "dictionary", ".", "add_symbol", "(", "'<mask>'", ")", "\n", "\n", "", "@", "classmethod", "\n", "def", "setup_task", "(", "cls", ",", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "paths", "=", "args", ".", "data", ".", "split", "(", "':'", ")", "\n", "assert", "len", "(", "paths", ")", ">", "0", "\n", "dictionary", "=", "Dictionary", ".", "load", "(", "os", ".", "path", ".", "join", "(", "paths", "[", "0", "]", ",", "'dict.txt'", ")", ")", "\n", "print", "(", "'| dictionary: {} types'", ".", "format", "(", "len", "(", "dictionary", ")", ")", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.sentence_ranking.SentenceRankingCriterion.__init__": [[19, 25], ["FairseqCriterion.__init__", "open"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["RawLabelDataset", ",", "\n", "RightPadDataset", ",", "\n", "SortDataset", ",", "\n", "TruncateDataset", ",", "\n", ")", "\n", "\n", "from", ".", "import", "FairseqTask", ",", "register_task", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.sentence_ranking.SentenceRankingCriterion.__del__": [[26, 29], ["sentence_ranking.SentenceRankingCriterion.prediction_h.close"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.GoldMiner.close"], ["\n", "\n", "@", "register_task", "(", "'sentence_ranking'", ")", "\n", "class", "SentenceRankingTask", "(", "FairseqTask", ")", ":", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.sentence_ranking.SentenceRankingCriterion.add_args": [[30, 35], ["parser.add_argument"], "methods", ["None"], ["    ", "\"\"\"\n    Ranking task on multiple sentences.\n\n    Args:\n        dictionary (Dictionary): the dictionary for the input of the task\n    \"\"\"", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.sentence_ranking.SentenceRankingCriterion.forward": [[37, 87], ["range", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat.size", "torch.cat.size", "model", "scores.append", "model.get_targets().view", "torch.nll_loss", "torch.nll_loss", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.cat.argmax", "torch.cat.argmax", "enumerate", "logging_output.update", "torch.log_softmax", "torch.log_softmax", "zip", "fairseq.utils.item", "model.get_targets", "sample[].tolist", "torch.cat.argmax.tolist", "targets[].item", "print", "print", "torch.cat.max", "torch.cat.max"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.model", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.argmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.argmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.log_softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.log_softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.wav2vec.Wav2VecModel.get_targets", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add task-specific arguments to the parser.\"\"\"", "\n", "parser", ".", "add_argument", "(", "'data'", ",", "metavar", "=", "'FILE'", ",", "\n", "help", "=", "'file prefix for data'", ")", "\n", "parser", ".", "add_argument", "(", "'--num-classes'", ",", "type", "=", "int", ",", "\n", "help", "=", "'number of sentences to be ranked'", ")", "\n", "parser", ".", "add_argument", "(", "'--init-token'", ",", "type", "=", "int", ",", "\n", "help", "=", "'add token at the beginning of each batch item'", ")", "\n", "parser", ".", "add_argument", "(", "'--separator-token'", ",", "type", "=", "int", ",", "\n", "help", "=", "'add separator token between inputs'", ")", "\n", "parser", ".", "add_argument", "(", "'--no-shuffle'", ",", "action", "=", "'store_true'", ")", "\n", "parser", ".", "add_argument", "(", "'--truncate-sequence'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Truncate sequence to max_positions'", ")", "\n", "parser", ".", "add_argument", "(", "'--max-option-length'", ",", "type", "=", "int", ",", "\n", "help", "=", "'max length for each option'", ")", "\n", "\n", "", "def", "__init__", "(", "self", ",", "args", ",", "dictionary", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ")", "\n", "self", ".", "dictionary", "=", "dictionary", "\n", "\n", "", "@", "classmethod", "\n", "def", "load_dictionary", "(", "cls", ",", "args", ",", "filename", ",", "source", "=", "True", ")", ":", "\n", "        ", "\"\"\"Load the dictionary from the filename\n\n        Args:\n            filename (str): the filename\n        \"\"\"", "\n", "dictionary", "=", "Dictionary", ".", "load", "(", "filename", ")", "\n", "dictionary", ".", "add_symbol", "(", "'<mask>'", ")", "\n", "return", "dictionary", "\n", "\n", "", "@", "classmethod", "\n", "def", "setup_task", "(", "cls", ",", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "assert", "args", ".", "criterion", "==", "'sentence_ranking'", ",", "'Must set --criterion=sentence_ranking'", "\n", "\n", "# load data dictionary", "\n", "data_dict", "=", "cls", ".", "load_dictionary", "(", "\n", "args", ",", "\n", "os", ".", "path", ".", "join", "(", "args", ".", "data", ",", "'input0'", ",", "'dict.txt'", ")", ",", "\n", "source", "=", "True", ",", "\n", ")", "\n", "print", "(", "'| [input] dictionary: {} types'", ".", "format", "(", "len", "(", "data_dict", ")", ")", ")", "\n", "return", "SentenceRankingTask", "(", "args", ",", "data_dict", ")", "\n", "\n", "", "def", "load_dataset", "(", "self", ",", "split", ",", "combine", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Load a given dataset split (e.g., train, valid, test).\"\"\"", "\n", "\n", "def", "get_path", "(", "type", ",", "split", ")", ":", "\n", "            ", "return", "os", ".", "path", ".", "join", "(", "self", ".", "args", ".", "data", ",", "type", ",", "split", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.sentence_ranking.SentenceRankingCriterion.aggregate_logging_outputs": [[88, 110], ["sum", "sum", "sum", "sum", "sum", "agg_output.update", "log.get", "log.get", "log.get", "log.get", "math.log", "len", "math.log", "log.get"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.log"], ["\n", "", "def", "make_dataset", "(", "type", ",", "dictionary", ")", ":", "\n", "            ", "split_path", "=", "get_path", "(", "type", ",", "split", ")", "\n", "\n", "dataset", "=", "data_utils", ".", "load_indexed_dataset", "(", "\n", "split_path", ",", "\n", "self", ".", "source_dictionary", ",", "\n", "self", ".", "args", ".", "dataset_impl", ",", "\n", "combine", "=", "combine", ",", "\n", ")", "\n", "return", "dataset", "\n", "\n", "", "input0", "=", "make_dataset", "(", "'input0'", ",", "self", ".", "source_dictionary", ")", "\n", "input_options", "=", "[", "\n", "make_dataset", "(", "\n", "'input{idx}'", ".", "format", "(", "idx", "=", "idx", "+", "1", ")", ",", "\n", "self", ".", "source_dictionary", "\n", ")", "\n", "for", "idx", "in", "range", "(", "self", ".", "args", ".", "num_classes", ")", "\n", "]", "\n", "\n", "if", "self", ".", "args", ".", "separator_token", "is", "not", "None", ":", "\n", "            ", "input0", "=", "PrependTokenDataset", "(", "input0", ",", "self", ".", "args", ".", "separator_token", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.sentence_prediction.SentencePredictionCriterion.add_args": [[19, 24], ["parser.add_argument"], "methods", ["None"], ["PrependTokenDataset", ",", "\n", "RawLabelDataset", ",", "\n", "RightPadDataset", ",", "\n", "SortDataset", ",", "\n", "StripTokenDataset", ",", "\n", "TruncateDataset", ",", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.sentence_prediction.SentencePredictionCriterion.forward": [[26, 74], ["model", "model.get_targets().view", "targets.float.float.numel", "hasattr", "torch.nll_loss", "torch.nll_loss", "logits.squeeze().float.squeeze().float.squeeze().float", "targets.float.float.float", "torch.mse_loss", "torch.mse_loss", "logging_output.update", "model.get_targets", "torch.log_softmax", "torch.log_softmax", "fairseq.utils.item", "logits.squeeze().float.squeeze().float.max", "logits.squeeze().float.squeeze().float.squeeze"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.model", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.wav2vec.Wav2VecModel.get_targets", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.log_softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.log_softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item"], ["\n", "from", ".", "import", "FairseqTask", ",", "register_task", "\n", "\n", "\n", "@", "register_task", "(", "'sentence_prediction'", ")", "\n", "class", "SentencePredictionTask", "(", "FairseqTask", ")", ":", "\n", "    ", "\"\"\"\n    Sentence (or sentence pair) prediction (classification or regression) task.\n\n    Args:\n        dictionary (Dictionary): the dictionary for the input of the task\n    \"\"\"", "\n", "\n", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add task-specific arguments to the parser.\"\"\"", "\n", "parser", ".", "add_argument", "(", "'data'", ",", "metavar", "=", "'FILE'", ",", "\n", "help", "=", "'file prefix for data'", ")", "\n", "parser", ".", "add_argument", "(", "'--num-classes'", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "'number of classes'", ")", "\n", "parser", ".", "add_argument", "(", "'--init-token'", ",", "type", "=", "int", ",", "default", "=", "None", ",", "\n", "help", "=", "'add token at the beginning of each batch item'", ")", "\n", "parser", ".", "add_argument", "(", "'--separator-token'", ",", "type", "=", "int", ",", "default", "=", "None", ",", "\n", "help", "=", "'add separator token between inputs'", ")", "\n", "parser", ".", "add_argument", "(", "'--regression-target'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'--no-shuffle'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'--truncate-sequence'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "\n", "help", "=", "'Truncate sequence to max_sequence_length'", ")", "\n", "\n", "", "def", "__init__", "(", "self", ",", "args", ",", "data_dictionary", ",", "label_dictionary", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ")", "\n", "self", ".", "dictionary", "=", "data_dictionary", "\n", "self", ".", "label_dictionary", "=", "label_dictionary", "\n", "\n", "", "@", "classmethod", "\n", "def", "load_dictionary", "(", "cls", ",", "args", ",", "filename", ",", "source", "=", "True", ")", ":", "\n", "        ", "\"\"\"Load the dictionary from the filename\n\n        Args:\n            filename (str): the filename\n        \"\"\"", "\n", "dictionary", "=", "Dictionary", ".", "load", "(", "filename", ")", "\n", "dictionary", ".", "add_symbol", "(", "'<mask>'", ")", "\n", "return", "dictionary", "\n", "\n", "", "@", "classmethod", "\n", "def", "setup_task", "(", "cls", ",", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "assert", "args", ".", "num_classes", ">", "0", ",", "'Must set --num-classes'", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.sentence_prediction.SentencePredictionCriterion.aggregate_logging_outputs": [[75, 97], ["sum", "sum", "sum", "sum", "sum", "agg_output.update", "log.get", "log.get", "log.get", "log.get", "math.log", "len", "math.log", "log.get"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.log"], ["args", ".", "tokens_per_sample", "=", "args", ".", "max_positions", "\n", "\n", "# load data dictionary", "\n", "data_dict", "=", "cls", ".", "load_dictionary", "(", "\n", "args", ",", "\n", "os", ".", "path", ".", "join", "(", "args", ".", "data", ",", "'input0'", ",", "'dict.txt'", ")", ",", "\n", "source", "=", "True", ",", "\n", ")", "\n", "print", "(", "'| [input] dictionary: {} types'", ".", "format", "(", "len", "(", "data_dict", ")", ")", ")", "\n", "\n", "label_dict", "=", "None", "\n", "if", "not", "args", ".", "regression_target", ":", "\n", "# load label dictionary", "\n", "            ", "label_dict", "=", "cls", ".", "load_dictionary", "(", "\n", "args", ",", "\n", "os", ".", "path", ".", "join", "(", "args", ".", "data", ",", "'label'", ",", "'dict.txt'", ")", ",", "\n", "source", "=", "False", ",", "\n", ")", "\n", "print", "(", "'| [label] dictionary: {} types'", ".", "format", "(", "len", "(", "label_dict", ")", ")", ")", "\n", "", "else", ":", "\n", "            ", "label_dict", "=", "data_dict", "\n", "", "return", "SentencePredictionTask", "(", "args", ",", "data_dict", ",", "label_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.composite_loss.CompositeLoss.add_args": [[17, 23], ["parser.add_argument"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add criterion-specific arguments to the parser.\"\"\"", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'--underlying-criterion'", ",", "type", "=", "str", ",", "metavar", "=", "'VAL'", ",", "required", "=", "True", ",", "\n", "help", "=", "'underlying criterion to use for the composite loss'", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.composite_loss.CompositeLoss.build_underlying_criterion": [[25, 33], ["task.build_criterion"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.composite_loss.CompositeLoss.build_criterion"], ["", "@", "staticmethod", "\n", "def", "build_underlying_criterion", "(", "args", ",", "task", ")", ":", "\n", "        ", "saved_criterion", "=", "args", ".", "criterion", "\n", "args", ".", "criterion", "=", "args", ".", "underlying_criterion", "\n", "assert", "saved_criterion", "!=", "args", ".", "underlying_criterion", "\n", "underlying_criterion", "=", "task", ".", "build_criterion", "(", "args", ")", "\n", "args", ".", "criterion", "=", "saved_criterion", "\n", "return", "underlying_criterion", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.composite_loss.CompositeLoss.build_criterion": [[34, 92], ["composite_loss.CompositeLoss.build_underlying_criterion", "_CompositeLoss", "FairseqCriterion.__init__", "composite_loss.CompositeLoss.model.get_normalized_probs", "FairseqCriterion.__init__", "model", "targets[].size", "[].new().float().zero_", "zip", "[].new().float().zero_.div_", "len", "composite_loss.CompositeLoss.build_underlying_criterion", "FakeModel", "composite_loss.CompositeLoss.underlying_criterion", "len", "fairseq.utils.item", "[].new().float", "[].new"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.composite_loss.CompositeLoss.build_underlying_criterion", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_decoder.FairseqDecoder.get_normalized_probs", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.model", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.composite_loss.CompositeLoss.build_underlying_criterion", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item"], ["", "@", "classmethod", "\n", "def", "build_criterion", "(", "cls", ",", "args", ",", "task", ")", ":", "\n", "        ", "underlying_criterion", "=", "CompositeLoss", ".", "build_underlying_criterion", "(", "args", ",", "task", ")", "\n", "\n", "class", "FakeModel", "(", "nn", ".", "Module", ")", ":", "\n", "\n", "            ", "def", "__init__", "(", "self", ",", "model", ",", "net_out", ",", "target", ")", ":", "\n", "                ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "model", "=", "model", "\n", "self", ".", "net_out", "=", "net_out", "\n", "self", ".", "target", "=", "target", "\n", "\n", "", "def", "forward", "(", "self", ",", "**", "unused", ")", ":", "\n", "                ", "return", "self", ".", "net_out", "\n", "\n", "", "def", "get_normalized_probs", "(", "self", ",", "net_output", ",", "log_probs", ",", "sample", "=", "None", ")", ":", "\n", "                ", "return", "self", ".", "model", ".", "get_normalized_probs", "(", "net_output", ",", "log_probs", ",", "sample", "=", "sample", ")", "\n", "\n", "", "def", "get_targets", "(", "self", ",", "*", "unused", ")", ":", "\n", "                ", "return", "self", ".", "target", "\n", "\n", "", "@", "property", "\n", "def", "decoder", "(", "self", ")", ":", "\n", "                ", "return", "self", ".", "model", ".", "decoder", "\n", "\n", "", "", "class", "_CompositeLoss", "(", "FairseqCriterion", ")", ":", "\n", "\n", "            ", "def", "__init__", "(", "self", ",", "args", ",", "task", ",", "underlying_criterion", ")", ":", "\n", "                ", "super", "(", ")", ".", "__init__", "(", "args", ",", "task", ")", "\n", "self", ".", "underlying_criterion", "=", "underlying_criterion", "\n", "\n", "", "def", "forward", "(", "self", ",", "model", ",", "sample", ",", "reduce", "=", "True", ")", ":", "\n", "                ", "net_outputs", "=", "model", "(", "**", "sample", "[", "'net_input'", "]", ")", "\n", "targets", "=", "sample", "[", "'target'", "]", "\n", "\n", "bsz", "=", "targets", "[", "0", "]", ".", "size", "(", "0", ")", "\n", "loss", "=", "net_outputs", "[", "0", "]", "[", "0", "]", ".", "new", "(", "1", "if", "reduce", "else", "bsz", ")", ".", "float", "(", ")", ".", "zero_", "(", ")", "\n", "\n", "sample_size", "=", "0", "\n", "logging_output", "=", "{", "}", "\n", "for", "o", ",", "t", "in", "zip", "(", "net_outputs", "[", "0", "]", ",", "targets", ")", ":", "\n", "                    ", "m", "=", "FakeModel", "(", "model", ",", "(", "o", ",", "net_outputs", "[", "1", "]", ")", ",", "t", ")", "\n", "sample", "[", "'target'", "]", "=", "t", "\n", "l", ",", "ss", ",", "logging_output", "=", "self", ".", "underlying_criterion", "(", "m", ",", "sample", ",", "reduce", ")", "\n", "loss", "+=", "l", "\n", "sample_size", "+=", "ss", "\n", "\n", "", "loss", ".", "div_", "(", "len", "(", "targets", ")", ")", "\n", "sample_size", "/=", "len", "(", "targets", ")", "\n", "\n", "logging_output", "[", "'loss'", "]", "=", "utils", ".", "item", "(", "loss", ".", "data", ")", "if", "reduce", "else", "loss", ".", "data", "\n", "return", "loss", ",", "sample_size", ",", "logging_output", "\n", "\n", "", "@", "staticmethod", "\n", "def", "aggregate_logging_outputs", "(", "logging_outputs", ")", ":", "\n", "                ", "return", "underlying_criterion", ".", "__class__", ".", "aggregate_logging_outputs", "(", "logging_outputs", ")", "\n", "\n", "", "", "return", "_CompositeLoss", "(", "args", ",", "task", ",", "underlying_criterion", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.binary_cross_entropy.BinaryCrossEntropyCriterion.__init__": [[18, 20], ["FairseqCriterion.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "task", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "task", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.binary_cross_entropy.BinaryCrossEntropyCriterion.forward": [[21, 55], ["model", "model.get_logits().float", "model.get_targets().float", "hasattr", "torch.binary_cross_entropy_with_logits", "torch.binary_cross_entropy_with_logits", "model.get_targets().float.numel", "model.get_target_weights", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "torch.is_tensor", "loss.sum.sum.sum", "model.get_logits().float.size", "model.get_logits", "model.get_targets", "weights.float.float.float", "fairseq.utils.item"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.model", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.wav2vec.Wav2VecModel.get_target_weights", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.wav2vec.Wav2VecModel.get_logits", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.wav2vec.Wav2VecModel.get_targets", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item"], ["", "def", "forward", "(", "self", ",", "model", ",", "sample", ",", "reduce", "=", "True", ")", ":", "\n", "        ", "\"\"\"Compute the loss for the given sample.\n\n        Returns a tuple with three elements:\n        1) the loss\n        2) the sample size, which is used as the denominator for the gradient\n        3) logging outputs to display while training\n        \"\"\"", "\n", "net_output", "=", "model", "(", "**", "sample", "[", "'net_input'", "]", ")", "\n", "logits", "=", "model", ".", "get_logits", "(", "net_output", ")", ".", "float", "(", ")", "\n", "target", "=", "model", ".", "get_targets", "(", "sample", ",", "net_output", ",", "expand_steps", "=", "False", ")", ".", "float", "(", ")", "\n", "\n", "if", "hasattr", "(", "model", ",", "'get_target_weights'", ")", ":", "\n", "            ", "weights", "=", "model", ".", "get_target_weights", "(", "target", ",", "net_output", ")", "\n", "if", "torch", ".", "is_tensor", "(", "weights", ")", ":", "\n", "                ", "weights", "=", "weights", ".", "float", "(", ")", "\n", "", "", "else", ":", "\n", "            ", "weights", "=", "1.", "\n", "\n", "", "loss", "=", "F", ".", "binary_cross_entropy_with_logits", "(", "logits", ",", "target", ",", "reduce", "=", "False", ")", "\n", "\n", "loss", "=", "loss", "*", "weights", "\n", "\n", "if", "reduce", ":", "\n", "            ", "loss", "=", "loss", ".", "sum", "(", ")", "\n", "\n", "", "sample_size", "=", "target", ".", "numel", "(", ")", "\n", "logging_output", "=", "{", "\n", "'loss'", ":", "utils", ".", "item", "(", "loss", ".", "data", ")", "if", "reduce", "else", "loss", ".", "data", ",", "\n", "'ntokens'", ":", "sample_size", ",", "\n", "'nsentences'", ":", "logits", ".", "size", "(", "0", ")", ",", "\n", "'sample_size'", ":", "sample_size", ",", "\n", "}", "\n", "return", "loss", ",", "sample_size", ",", "logging_output", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.binary_cross_entropy.BinaryCrossEntropyCriterion.aggregate_logging_outputs": [[56, 72], ["sum", "sum", "sum", "sum", "log.get", "log.get", "log.get", "log.get", "math.log", "math.log"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.log"], ["", "@", "staticmethod", "\n", "def", "aggregate_logging_outputs", "(", "logging_outputs", ")", ":", "\n", "        ", "\"\"\"Aggregate logging outputs from data parallel training.\"\"\"", "\n", "loss_sum", "=", "sum", "(", "log", ".", "get", "(", "'loss'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "ntokens", "=", "sum", "(", "log", ".", "get", "(", "'ntokens'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "nsentences", "=", "sum", "(", "log", ".", "get", "(", "'nsentences'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "sample_size", "=", "sum", "(", "log", ".", "get", "(", "'sample_size'", ",", "0", ")", "for", "log", "in", "logging_outputs", ")", "\n", "agg_output", "=", "{", "\n", "'loss'", ":", "loss_sum", "/", "sample_size", "/", "math", ".", "log", "(", "2", ")", ",", "\n", "'ntokens'", ":", "ntokens", ",", "\n", "'nsentences'", ":", "nsentences", ",", "\n", "'sample_size'", ":", "sample_size", ",", "\n", "}", "\n", "if", "sample_size", "!=", "ntokens", ":", "\n", "            ", "agg_output", "[", "'nll_loss'", "]", "=", "loss_sum", "/", "ntokens", "/", "math", ".", "log", "(", "2", ")", "\n", "", "return", "agg_output", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.asr_dataset.AsrDataset.__init__": [[32, 58], ["all", "int", "len", "len", "len", "len", "len", "len", "len", "len", "len"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "aud_paths", ",", "aud_durations_ms", ",", "tgt", ",", "\n", "tgt_dict", ",", "ids", ",", "speakers", ",", "\n", "num_mel_bins", "=", "80", ",", "frame_length", "=", "25.0", ",", "frame_shift", "=", "10.0", "\n", ")", ":", "\n", "        ", "assert", "frame_length", ">", "0", "\n", "assert", "frame_shift", ">", "0", "\n", "assert", "all", "(", "x", ">", "frame_length", "for", "x", "in", "aud_durations_ms", ")", "\n", "self", ".", "frame_sizes", "=", "[", "\n", "int", "(", "1", "+", "(", "d", "-", "frame_length", ")", "/", "frame_shift", ")", "\n", "for", "d", "in", "aud_durations_ms", "\n", "]", "\n", "\n", "assert", "len", "(", "aud_paths", ")", ">", "0", "\n", "assert", "len", "(", "aud_paths", ")", "==", "len", "(", "aud_durations_ms", ")", "\n", "assert", "len", "(", "aud_paths", ")", "==", "len", "(", "tgt", ")", "\n", "assert", "len", "(", "aud_paths", ")", "==", "len", "(", "ids", ")", "\n", "assert", "len", "(", "aud_paths", ")", "==", "len", "(", "speakers", ")", "\n", "self", ".", "aud_paths", "=", "aud_paths", "\n", "self", ".", "tgt_dict", "=", "tgt_dict", "\n", "self", ".", "tgt", "=", "tgt", "\n", "self", ".", "ids", "=", "ids", "\n", "self", ".", "speakers", "=", "speakers", "\n", "self", ".", "num_mel_bins", "=", "num_mel_bins", "\n", "self", ".", "frame_length", "=", "frame_length", "\n", "self", ".", "frame_shift", "=", "frame_shift", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.asr_dataset.AsrDataset.__getitem__": [[59, 81], ["torchaudio.load_wav", "kaldi.fbank", "data_utils.apply_mv_norm", "collaters.Seq2SeqCollater", "os.path.exists", "FileNotFoundError", "asr_dataset.AsrDataset.tgt_dict.pad", "asr_dataset.AsrDataset.tgt_dict.eos", "data_utils.apply_mv_norm.detach"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.data_utils.apply_mv_norm", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "import", "torchaudio", "\n", "import", "torchaudio", ".", "compliance", ".", "kaldi", "as", "kaldi", "\n", "tgt_item", "=", "self", ".", "tgt", "[", "index", "]", "if", "self", ".", "tgt", "is", "not", "None", "else", "None", "\n", "\n", "path", "=", "self", ".", "aud_paths", "[", "index", "]", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "path", ")", ":", "\n", "            ", "raise", "FileNotFoundError", "(", "\"Audio file not found: {}\"", ".", "format", "(", "path", ")", ")", "\n", "", "sound", ",", "sample_rate", "=", "torchaudio", ".", "load_wav", "(", "path", ")", "\n", "output", "=", "kaldi", ".", "fbank", "(", "\n", "sound", ",", "\n", "num_mel_bins", "=", "self", ".", "num_mel_bins", ",", "\n", "frame_length", "=", "self", ".", "frame_length", ",", "\n", "frame_shift", "=", "self", ".", "frame_shift", "\n", ")", "\n", "output_cmvn", "=", "data_utils", ".", "apply_mv_norm", "(", "output", ")", "\n", "self", ".", "collater", "=", "Seq2SeqCollater", "(", "\n", "0", ",", "1", ",", "pad_index", "=", "self", ".", "tgt_dict", ".", "pad", "(", ")", ",", "\n", "eos_index", "=", "self", ".", "tgt_dict", ".", "eos", "(", ")", ",", "move_eos_to_beginning", "=", "True", "\n", ")", "\n", "\n", "return", "{", "\"id\"", ":", "index", ",", "\"data\"", ":", "[", "output_cmvn", ".", "detach", "(", ")", ",", "tgt_item", "]", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.asr_dataset.AsrDataset.__len__": [[82, 84], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "aud_paths", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.asr_dataset.AsrDataset.collater": [[85, 95], ["asr_dataset.AsrDataset.collater.collate"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.language_pair_dataset.collate"], ["", "def", "collater", "(", "self", ",", "samples", ")", ":", "\n", "        ", "\"\"\"Merge a list of samples to form a mini-batch.\n\n        Args:\n            samples (List[int]): sample indices to collate\n\n        Returns:\n            dict: a mini-batch suitable for forwarding with a Model\n        \"\"\"", "\n", "return", "self", ".", "collater", ".", "collate", "(", "samples", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.asr_dataset.AsrDataset.num_tokens": [[96, 98], ["None"], "methods", ["None"], ["", "def", "num_tokens", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "frame_sizes", "[", "index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.asr_dataset.AsrDataset.size": [[99, 105], ["len"], "methods", ["None"], ["", "def", "size", "(", "self", ",", "index", ")", ":", "\n", "        ", "\"\"\"Return an example's size as a float or tuple. This value is used when\n        filtering a dataset with ``--max-positions``.\"\"\"", "\n", "return", "(", "\n", "self", ".", "frame_sizes", "[", "index", "]", ",", "\n", "len", "(", "self", ".", "tgt", "[", "index", "]", ")", "if", "self", ".", "tgt", "is", "not", "None", "else", "0", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.asr_dataset.AsrDataset.ordered_indices": [[107, 111], ["numpy.arange", "len"], "methods", ["None"], ["", "def", "ordered_indices", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return an ordered list of indices. Batches will be constructed based\n        on this order.\"\"\"", "\n", "return", "np", ".", "arange", "(", "len", "(", "self", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.collaters.Seq2SeqCollater.__init__": [[29, 42], ["None"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "\n", "feature_index", "=", "0", ",", "\n", "label_index", "=", "1", ",", "\n", "pad_index", "=", "1", ",", "\n", "eos_index", "=", "2", ",", "\n", "move_eos_to_beginning", "=", "True", ",", "\n", ")", ":", "\n", "        ", "self", ".", "feature_index", "=", "feature_index", "\n", "self", ".", "label_index", "=", "label_index", "\n", "self", ".", "pad_index", "=", "pad_index", "\n", "self", ".", "eos_index", "=", "eos_index", "\n", "self", ".", "move_eos_to_beginning", "=", "move_eos_to_beginning", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.collaters.Seq2SeqCollater._collate_frames": [[43, 59], ["max", "frames[].size", "frames[].new().fill_", "enumerate", "frame.size", "frames[].new", "len", "v.size"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "_collate_frames", "(", "self", ",", "frames", ")", ":", "\n", "        ", "\"\"\"Convert a list of 2d frames into a padded 3d tensor\n        Args:\n            frames (list): list of 2d frames of size L[i]*f_dim. Where L[i] is\n                length of i-th frame and f_dim is static dimension of features\n        Returns:\n            3d tensor of size len(frames)*len_max*f_dim where len_max is max of L[i]\n        \"\"\"", "\n", "len_max", "=", "max", "(", "frame", ".", "size", "(", "0", ")", "for", "frame", "in", "frames", ")", "\n", "f_dim", "=", "frames", "[", "0", "]", ".", "size", "(", "1", ")", "\n", "res", "=", "frames", "[", "0", "]", ".", "new", "(", "len", "(", "frames", ")", ",", "len_max", ",", "f_dim", ")", ".", "fill_", "(", "0.0", ")", "\n", "\n", "for", "i", ",", "v", "in", "enumerate", "(", "frames", ")", ":", "\n", "            ", "res", "[", "i", ",", ":", "v", ".", "size", "(", "0", ")", "]", "=", "v", "\n", "\n", "", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.collaters.Seq2SeqCollater.collate": [[60, 130], ["torch.LongTensor", "collaters.Seq2SeqCollater._collate_frames", "torch.LongTensor", "torch.LongTensor.sort", "id.index_select.index_select.index_select", "frames.index_select.index_select.index_select", "len", "isinstance", "isinstance", "parsed_samples.append", "samples[].get", "sum", "fairseq.data.data_utils.collate_tokens", "torch.from_numpy().long.index_select", "torch.LongTensor().index_select", "fairseq.data.data_utils.collate_tokens", "prev_output_tokens.index_select.index_select.index_select", "sum", "len", "torch.from_numpy", "torch.from_numpy().long", "s[].size", "len", "torch.LongTensor", "len", "torch.from_numpy", "s[].size"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.collaters.Seq2SeqCollater._collate_frames", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.data_utils.collate_tokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.data_utils.collate_tokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "collate", "(", "self", ",", "samples", ")", ":", "\n", "        ", "\"\"\"\n        utility function to collate samples into batch for speech recognition.\n        \"\"\"", "\n", "if", "len", "(", "samples", ")", "==", "0", ":", "\n", "            ", "return", "{", "}", "\n", "\n", "# parse samples into torch tensors", "\n", "", "parsed_samples", "=", "[", "]", "\n", "for", "s", "in", "samples", ":", "\n", "# skip invalid samples", "\n", "            ", "if", "s", "[", "\"data\"", "]", "[", "self", ".", "feature_index", "]", "is", "None", ":", "\n", "                ", "continue", "\n", "", "source", "=", "s", "[", "\"data\"", "]", "[", "self", ".", "feature_index", "]", "\n", "if", "isinstance", "(", "source", ",", "(", "np", ".", "ndarray", ",", "np", ".", "generic", ")", ")", ":", "\n", "                ", "source", "=", "torch", ".", "from_numpy", "(", "source", ")", "\n", "", "target", "=", "s", "[", "\"data\"", "]", "[", "self", ".", "label_index", "]", "\n", "if", "isinstance", "(", "target", ",", "(", "np", ".", "ndarray", ",", "np", ".", "generic", ")", ")", ":", "\n", "                ", "target", "=", "torch", ".", "from_numpy", "(", "target", ")", ".", "long", "(", ")", "\n", "\n", "", "parsed_sample", "=", "{", "\"id\"", ":", "s", "[", "\"id\"", "]", ",", "\"source\"", ":", "source", ",", "\"target\"", ":", "target", "}", "\n", "parsed_samples", ".", "append", "(", "parsed_sample", ")", "\n", "", "samples", "=", "parsed_samples", "\n", "\n", "id", "=", "torch", ".", "LongTensor", "(", "[", "s", "[", "\"id\"", "]", "for", "s", "in", "samples", "]", ")", "\n", "frames", "=", "self", ".", "_collate_frames", "(", "[", "s", "[", "\"source\"", "]", "for", "s", "in", "samples", "]", ")", "\n", "# sort samples by descending number of frames", "\n", "frames_lengths", "=", "torch", ".", "LongTensor", "(", "[", "s", "[", "\"source\"", "]", ".", "size", "(", "0", ")", "for", "s", "in", "samples", "]", ")", "\n", "frames_lengths", ",", "sort_order", "=", "frames_lengths", ".", "sort", "(", "descending", "=", "True", ")", "\n", "id", "=", "id", ".", "index_select", "(", "0", ",", "sort_order", ")", "\n", "frames", "=", "frames", ".", "index_select", "(", "0", ",", "sort_order", ")", "\n", "\n", "target", "=", "None", "\n", "target_lengths", "=", "None", "\n", "prev_output_tokens", "=", "None", "\n", "if", "samples", "[", "0", "]", ".", "get", "(", "\"target\"", ",", "None", ")", "is", "not", "None", ":", "\n", "            ", "ntokens", "=", "sum", "(", "len", "(", "s", "[", "\"target\"", "]", ")", "for", "s", "in", "samples", ")", "\n", "target", "=", "fairseq_data_utils", ".", "collate_tokens", "(", "\n", "[", "s", "[", "\"target\"", "]", "for", "s", "in", "samples", "]", ",", "\n", "self", ".", "pad_index", ",", "\n", "self", ".", "eos_index", ",", "\n", "left_pad", "=", "False", ",", "\n", "move_eos_to_beginning", "=", "False", ",", "\n", ")", "\n", "target", "=", "target", ".", "index_select", "(", "0", ",", "sort_order", ")", "\n", "target_lengths", "=", "torch", ".", "LongTensor", "(", "\n", "[", "s", "[", "\"target\"", "]", ".", "size", "(", "0", ")", "for", "s", "in", "samples", "]", "\n", ")", ".", "index_select", "(", "0", ",", "sort_order", ")", "\n", "prev_output_tokens", "=", "fairseq_data_utils", ".", "collate_tokens", "(", "\n", "[", "s", "[", "\"target\"", "]", "for", "s", "in", "samples", "]", ",", "\n", "self", ".", "pad_index", ",", "\n", "self", ".", "eos_index", ",", "\n", "left_pad", "=", "False", ",", "\n", "move_eos_to_beginning", "=", "self", ".", "move_eos_to_beginning", ",", "\n", ")", "\n", "prev_output_tokens", "=", "prev_output_tokens", ".", "index_select", "(", "0", ",", "sort_order", ")", "\n", "", "else", ":", "\n", "            ", "ntokens", "=", "sum", "(", "len", "(", "s", "[", "\"source\"", "]", ")", "for", "s", "in", "samples", ")", "\n", "\n", "", "batch", "=", "{", "\n", "\"id\"", ":", "id", ",", "\n", "\"ntokens\"", ":", "ntokens", ",", "\n", "\"net_input\"", ":", "{", "\"src_tokens\"", ":", "frames", ",", "\"src_lengths\"", ":", "frames_lengths", "}", ",", "\n", "\"target\"", ":", "target", ",", "\n", "\"target_lengths\"", ":", "target_lengths", ",", "\n", "\"nsentences\"", ":", "len", "(", "samples", ")", ",", "\n", "}", "\n", "if", "prev_output_tokens", "is", "not", "None", ":", "\n", "            ", "batch", "[", "\"net_input\"", "]", "[", "\"prev_output_tokens\"", "]", "=", "prev_output_tokens", "\n", "", "return", "batch", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.data_utils.calc_mean_invstddev": [[9, 19], ["feature.mean", "feature.var", "len", "ValueError", "feature.size", "torch.sqrt", "torch.sqrt"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.mean", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["def", "calc_mean_invstddev", "(", "feature", ")", ":", "\n", "    ", "if", "len", "(", "feature", ".", "size", "(", ")", ")", "!=", "2", ":", "\n", "        ", "raise", "ValueError", "(", "\"We expect the input feature to be 2-D tensor\"", ")", "\n", "", "mean", "=", "feature", ".", "mean", "(", "0", ")", "\n", "var", "=", "feature", ".", "var", "(", "0", ")", "\n", "# avoid division by ~zero", "\n", "eps", "=", "1e-8", "\n", "if", "(", "var", "<", "eps", ")", ".", "any", "(", ")", ":", "\n", "        ", "return", "mean", ",", "1.0", "/", "(", "torch", ".", "sqrt", "(", "var", ")", "+", "eps", ")", "\n", "", "return", "mean", ",", "1.0", "/", "torch", ".", "sqrt", "(", "var", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.data_utils.apply_mv_norm": [[21, 25], ["data_utils.calc_mean_invstddev"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.data_utils.calc_mean_invstddev"], ["", "def", "apply_mv_norm", "(", "features", ")", ":", "\n", "    ", "mean", ",", "invstddev", "=", "calc_mean_invstddev", "(", "features", ")", "\n", "res", "=", "(", "features", "-", "mean", ")", "*", "invstddev", "\n", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.data_utils.lengths_to_encoder_padding_mask": [[27, 61], ["torch.max().item", "lengths.size", "torch.arange().to().view().expand", "lengths.view().expand", "torch.max", "encoder_padding_mask.t", "torch.arange().to().view", "lengths.view", "torch.arange().to", "torch.arange"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "lengths_to_encoder_padding_mask", "(", "lengths", ",", "batch_first", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    convert lengths (a 1-D Long/Int tensor) to 2-D binary tensor\n\n    Args:\n        lengths: a (B, )-shaped tensor\n\n    Return:\n        max_length: maximum length of B sequences\n        encoder_padding_mask: a (max_length, B) binary mask, where\n        [t, b] = 0 for t < lengths[b] and 1 otherwise\n\n    TODO:\n        kernelize this function if benchmarking shows this function is slow\n    \"\"\"", "\n", "max_lengths", "=", "torch", ".", "max", "(", "lengths", ")", ".", "item", "(", ")", "\n", "bsz", "=", "lengths", ".", "size", "(", "0", ")", "\n", "encoder_padding_mask", "=", "torch", ".", "arange", "(", "\n", "max_lengths", "\n", ")", ".", "to", "(", "# a (T, ) tensor with [0, ..., T-1]", "\n", "lengths", ".", "device", "\n", ")", ".", "view", "(", "# move to the right device", "\n", "1", ",", "max_lengths", "\n", ")", ".", "expand", "(", "# reshape to (1, T)-shaped tensor", "\n", "bsz", ",", "-", "1", "\n", ")", ">=", "lengths", ".", "view", "(", "# expand to (B, T)-shaped tensor", "\n", "bsz", ",", "1", "\n", ")", ".", "expand", "(", "\n", "-", "1", ",", "max_lengths", "\n", ")", "\n", "if", "not", "batch_first", ":", "\n", "        ", "return", "encoder_padding_mask", ".", "t", "(", ")", ",", "max_lengths", "\n", "", "else", ":", "\n", "        ", "return", "encoder_padding_mask", ",", "max_lengths", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.concat_sentences_dataset.ConcatSentencesDataset.__init__": [[13, 18], ["FairseqDataset.__init__", "all", "len", "len"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", "datasets", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "datasets", "=", "datasets", "\n", "assert", "all", "(", "len", "(", "ds", ")", "==", "len", "(", "datasets", "[", "0", "]", ")", "for", "ds", "in", "datasets", ")", ",", "'datasets must have the same length'", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.concat_sentences_dataset.ConcatSentencesDataset.__getitem__": [[19, 21], ["torch.cat"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "torch", ".", "cat", "(", "[", "ds", "[", "index", "]", "for", "ds", "in", "self", ".", "datasets", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.concat_sentences_dataset.ConcatSentencesDataset.__len__": [[22, 24], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "datasets", "[", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.concat_sentences_dataset.ConcatSentencesDataset.collater": [[25, 27], ["concat_sentences_dataset.ConcatSentencesDataset.datasets[].collater"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.collater"], ["", "def", "collater", "(", "self", ",", "samples", ")", ":", "\n", "        ", "return", "self", ".", "datasets", "[", "0", "]", ".", "collater", "(", "samples", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.concat_sentences_dataset.ConcatSentencesDataset.sizes": [[28, 31], ["sum"], "methods", ["None"], ["", "@", "property", "\n", "def", "sizes", "(", "self", ")", ":", "\n", "        ", "return", "sum", "(", "ds", ".", "sizes", "for", "ds", "in", "self", ".", "datasets", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.concat_sentences_dataset.ConcatSentencesDataset.num_tokens": [[32, 34], ["sum", "ds.num_tokens"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.num_tokens"], ["", "def", "num_tokens", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "sum", "(", "ds", ".", "num_tokens", "(", "index", ")", "for", "ds", "in", "self", ".", "datasets", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.concat_sentences_dataset.ConcatSentencesDataset.size": [[35, 37], ["sum", "ds.size"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "size", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "sum", "(", "ds", ".", "size", "(", "index", ")", "for", "ds", "in", "self", ".", "datasets", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.concat_sentences_dataset.ConcatSentencesDataset.ordered_indices": [[38, 40], ["concat_sentences_dataset.ConcatSentencesDataset.datasets[].ordered_indices"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.ordered_indices"], ["", "def", "ordered_indices", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "datasets", "[", "0", "]", ".", "ordered_indices", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.concat_sentences_dataset.ConcatSentencesDataset.supports_prefetch": [[41, 45], ["any", "getattr"], "methods", ["None"], ["", "@", "property", "\n", "def", "supports_prefetch", "(", "self", ")", ":", "\n", "        ", "return", "any", "(", "\n", "getattr", "(", "ds", ",", "'supports_prefetch'", ",", "False", ")", "for", "ds", "in", "self", ".", "datasets", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.concat_sentences_dataset.ConcatSentencesDataset.prefetch": [[47, 51], ["getattr", "ds.prefetch"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.block_pair_dataset.BlockPairDataset.prefetch"], ["", "def", "prefetch", "(", "self", ",", "indices", ")", ":", "\n", "        ", "for", "ds", "in", "self", ".", "datasets", ":", "\n", "            ", "if", "getattr", "(", "ds", ",", "'supports_prefetch'", ",", "False", ")", ":", "\n", "                ", "ds", ".", "prefetch", "(", "indices", ")", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.offset_tokens_dataset.OffsetTokensDataset.__init__": [[11, 14], ["BaseWrapperDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dataset", ",", "offset", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dataset", ")", "\n", "self", ".", "offset", "=", "offset", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.offset_tokens_dataset.OffsetTokensDataset.__getitem__": [[15, 17], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "return", "self", ".", "dataset", "[", "idx", "]", "+", "self", ".", "offset", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.fairseq_dataset.FairseqDataset.__getitem__": [[13, 15], ["None"], "methods", ["None"], ["def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.fairseq_dataset.FairseqDataset.__len__": [[16, 18], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.fairseq_dataset.FairseqDataset.collater": [[19, 29], ["None"], "methods", ["None"], ["", "def", "collater", "(", "self", ",", "samples", ")", ":", "\n", "        ", "\"\"\"Merge a list of samples to form a mini-batch.\n\n        Args:\n            samples (List[dict]): samples to collate\n\n        Returns:\n            dict: a mini-batch suitable for forwarding with a Model\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.fairseq_dataset.FairseqDataset.num_tokens": [[30, 34], ["None"], "methods", ["None"], ["", "def", "num_tokens", "(", "self", ",", "index", ")", ":", "\n", "        ", "\"\"\"Return the number of tokens in a sample. This value is used to\n        enforce ``--max-tokens`` during batching.\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.fairseq_dataset.FairseqDataset.size": [[35, 39], ["None"], "methods", ["None"], ["", "def", "size", "(", "self", ",", "index", ")", ":", "\n", "        ", "\"\"\"Return an example's size as a float or tuple. This value is used when\n        filtering a dataset with ``--max-positions``.\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.fairseq_dataset.FairseqDataset.ordered_indices": [[40, 44], ["numpy.arange", "len"], "methods", ["None"], ["", "def", "ordered_indices", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return an ordered list of indices. Batches will be constructed based\n        on this order.\"\"\"", "\n", "return", "np", ".", "arange", "(", "len", "(", "self", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.fairseq_dataset.FairseqDataset.supports_prefetch": [[45, 49], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "supports_prefetch", "(", "self", ")", ":", "\n", "        ", "\"\"\"Whether this dataset supports prefetching.\"\"\"", "\n", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.fairseq_dataset.FairseqDataset.prefetch": [[50, 53], ["None"], "methods", ["None"], ["", "def", "prefetch", "(", "self", ",", "indices", ")", ":", "\n", "        ", "\"\"\"Prefetch the data required for this epoch.\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.fairseq_dataset.FairseqDataset.set_epoch": [[54, 56], ["None"], "methods", ["None"], ["", "def", "set_epoch", "(", "self", ",", "epoch", ")", ":", "\n", "        ", "pass", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.id_dataset.IdDataset.__getitem__": [[13, 15], ["None"], "methods", ["None"], ["    ", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "index", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.id_dataset.IdDataset.__len__": [[16, 18], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.id_dataset.IdDataset.collater": [[19, 21], ["torch.tensor"], "methods", ["None"], ["", "def", "collater", "(", "self", ",", "samples", ")", ":", "\n", "        ", "return", "torch", ".", "tensor", "(", "samples", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.backtranslation_dataset.BacktranslationDataset.__init__": [[78, 95], ["torch.cuda.is_available"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "\n", "tgt_dataset", ",", "\n", "src_dict", ",", "\n", "tgt_dict", "=", "None", ",", "\n", "backtranslation_fn", "=", "None", ",", "\n", "output_collater", "=", "None", ",", "\n", "cuda", "=", "True", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "self", ".", "tgt_dataset", "=", "tgt_dataset", "\n", "self", ".", "backtranslation_fn", "=", "backtranslation_fn", "\n", "self", ".", "output_collater", "=", "output_collater", "if", "output_collater", "is", "not", "None", "else", "tgt_dataset", ".", "collater", "\n", "self", ".", "cuda", "=", "cuda", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "False", "\n", "self", ".", "src_dict", "=", "src_dict", "\n", "self", ".", "tgt_dict", "=", "tgt_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.backtranslation_dataset.BacktranslationDataset.__getitem__": [[96, 103], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "\"\"\"\n        Returns a single sample from *tgt_dataset*. Note that backtranslation is\n        not applied in this step; use :func:`collater` instead to backtranslate\n        a batch of samples.\n        \"\"\"", "\n", "return", "self", ".", "tgt_dataset", "[", "index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.backtranslation_dataset.BacktranslationDataset.__len__": [[104, 106], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "tgt_dataset", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.backtranslation_dataset.BacktranslationDataset.set_backtranslation_fn": [[107, 109], ["None"], "methods", ["None"], ["", "def", "set_backtranslation_fn", "(", "self", ",", "backtranslation_fn", ")", ":", "\n", "        ", "self", ".", "backtranslation_fn", "=", "backtranslation_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.backtranslation_dataset.BacktranslationDataset.collater": [[110, 140], ["samples[].get", "backtranslation_dataset.backtranslate_samples", "backtranslation_dataset.BacktranslationDataset.output_collater", "backtranslation_dataset.BacktranslationDataset.backtranslation_fn"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.backtranslation_dataset.backtranslate_samples"], ["", "def", "collater", "(", "self", ",", "samples", ")", ":", "\n", "        ", "\"\"\"Merge and backtranslate a list of samples to form a mini-batch.\n\n        Using the samples from *tgt_dataset*, load a collated target sample to\n        feed to the backtranslation model. Then take the backtranslation with\n        the best score as the source and the original input as the target.\n\n        Note: we expect *tgt_dataset* to provide a function `collater()` that\n        will collate samples into the format expected by *backtranslation_fn*.\n        After backtranslation, we will feed the new list of samples (i.e., the\n        `(backtranslated source, original source)` pairs) to *output_collater*\n        and return the result.\n\n        Args:\n            samples (List[dict]): samples to backtranslate and collate\n\n        Returns:\n            dict: a mini-batch with keys coming from *output_collater*\n        \"\"\"", "\n", "if", "samples", "[", "0", "]", ".", "get", "(", "'is_dummy'", ",", "False", ")", ":", "\n", "            ", "return", "samples", "\n", "", "samples", "=", "backtranslate_samples", "(", "\n", "samples", "=", "samples", ",", "\n", "collate_fn", "=", "self", ".", "tgt_dataset", ".", "collater", ",", "\n", "generate_fn", "=", "(", "\n", "lambda", "net_input", ":", "self", ".", "backtranslation_fn", "(", "net_input", ")", "\n", ")", ",", "\n", "cuda", "=", "self", ".", "cuda", ",", "\n", ")", "\n", "return", "self", ".", "output_collater", "(", "samples", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.backtranslation_dataset.BacktranslationDataset.num_tokens": [[141, 144], ["backtranslation_dataset.BacktranslationDataset.tgt_dataset.num_tokens"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.num_tokens"], ["", "def", "num_tokens", "(", "self", ",", "index", ")", ":", "\n", "        ", "\"\"\"Just use the tgt dataset num_tokens\"\"\"", "\n", "return", "self", ".", "tgt_dataset", ".", "num_tokens", "(", "index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.backtranslation_dataset.BacktranslationDataset.ordered_indices": [[145, 148], ["backtranslation_dataset.BacktranslationDataset.tgt_dataset.ordered_indices"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.ordered_indices"], ["", "def", "ordered_indices", "(", "self", ")", ":", "\n", "        ", "\"\"\"Just use the tgt dataset ordered_indices\"\"\"", "\n", "return", "self", ".", "tgt_dataset", ".", "ordered_indices", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.backtranslation_dataset.BacktranslationDataset.size": [[149, 159], ["backtranslation_dataset.BacktranslationDataset.tgt_dataset.size"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "size", "(", "self", ",", "index", ")", ":", "\n", "        ", "\"\"\"Return an example's size as a float or tuple. This value is used\n        when filtering a dataset with ``--max-positions``.\n\n        Note: we use *tgt_dataset* to approximate the length of the source\n        sentence, since we do not know the actual length until after\n        backtranslation.\n        \"\"\"", "\n", "tgt_size", "=", "self", ".", "tgt_dataset", ".", "size", "(", "index", ")", "[", "0", "]", "\n", "return", "(", "tgt_size", ",", "tgt_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.backtranslation_dataset.BacktranslationDataset.supports_prefetch": [[160, 163], ["getattr"], "methods", ["None"], ["", "@", "property", "\n", "def", "supports_prefetch", "(", "self", ")", ":", "\n", "        ", "return", "getattr", "(", "self", ".", "tgt_dataset", ",", "'supports_prefetch'", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.backtranslation_dataset.BacktranslationDataset.prefetch": [[164, 166], ["backtranslation_dataset.BacktranslationDataset.tgt_dataset.prefetch"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.block_pair_dataset.BlockPairDataset.prefetch"], ["", "def", "prefetch", "(", "self", ",", "indices", ")", ":", "\n", "        ", "return", "self", ".", "tgt_dataset", ".", "prefetch", "(", "indices", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.backtranslation_dataset.backtranslate_samples": [[13, 49], ["collate_fn", "generate_fn", "fairseq.utils.move_to_cuda", "id.item", "[].cpu", "zip", "id.item"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.move_to_cuda", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item"], ["def", "backtranslate_samples", "(", "samples", ",", "collate_fn", ",", "generate_fn", ",", "cuda", "=", "True", ")", ":", "\n", "    ", "\"\"\"Backtranslate a list of samples.\n\n    Given an input (*samples*) of the form:\n\n        [{'id': 1, 'source': 'hallo welt'}]\n\n    this will return:\n\n        [{'id': 1, 'source': 'hello world', 'target': 'hallo welt'}]\n\n    Args:\n        samples (List[dict]): samples to backtranslate. Individual samples are\n            expected to have a 'source' key, which will become the 'target'\n            after backtranslation.\n        collate_fn (callable): function to collate samples into a mini-batch\n        generate_fn (callable): function to generate backtranslations\n        cuda (bool): use GPU for generation (default: ``True``)\n\n    Returns:\n        List[dict]: an updated list of samples with a backtranslated source\n    \"\"\"", "\n", "collated_samples", "=", "collate_fn", "(", "samples", ")", "\n", "s", "=", "utils", ".", "move_to_cuda", "(", "collated_samples", ")", "if", "cuda", "else", "collated_samples", "\n", "generated_sources", "=", "generate_fn", "(", "s", ")", "\n", "\n", "id_to_src", "=", "{", "\n", "sample", "[", "'id'", "]", ":", "sample", "[", "'source'", "]", "for", "sample", "in", "samples", "\n", "}", "\n", "\n", "# Go through each tgt sentence in batch and its corresponding best", "\n", "# generated hypothesis and create a backtranslation data pair", "\n", "# {id: id, source: generated backtranslation, target: original tgt}", "\n", "return", "[", "\n", "{", "'id'", ":", "id", ".", "item", "(", ")", ",", "'target'", ":", "id_to_src", "[", "id", ".", "item", "(", ")", "]", ",", "'source'", ":", "hypos", "[", "0", "]", "[", "'tokens'", "]", ".", "cpu", "(", ")", "}", "\n", "for", "id", ",", "hypos", "in", "zip", "(", "collated_samples", "[", "'id'", "]", ",", "generated_sources", ")", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.lru_cache_dataset.LRUCacheDataset.__init__": [[13, 15], ["BaseWrapperDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dataset", ",", "token", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dataset", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.lru_cache_dataset.LRUCacheDataset.__getitem__": [[16, 19], ["functools.lru_cache"], "methods", ["None"], ["", "@", "lru_cache", "(", "maxsize", "=", "8", ")", "\n", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "dataset", "[", "index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.lru_cache_dataset.LRUCacheDataset.collater": [[20, 23], ["functools.lru_cache", "lru_cache_dataset.LRUCacheDataset.dataset.collater"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.collater"], ["", "@", "lru_cache", "(", "maxsize", "=", "8", ")", "\n", "def", "collater", "(", "self", ",", "samples", ")", ":", "\n", "        ", "return", "self", ".", "dataset", ".", "collater", "(", "samples", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.token_block_dataset.TokenBlockDataset.__init__": [[37, 141], ["fairseq.data.FairseqDataset.__init__", "numpy.array", "numpy.array", "fairseq.data.plasma_utils.PlasmaArray", "fairseq.data.plasma_utils.PlasmaArray", "fairseq.data.plasma_utils.PlasmaArray", "len", "len", "len", "sum", "math.ceil", "numpy.stack", "token_block_dataset.DatasetSearcher", "numpy.empty", "enumerate", "min", "token_block_dataset.TokenBlockDataset.__init__.block_at"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["def", "__init__", "(", "\n", "self", ",", "dataset", ",", "sizes", ",", "block_size", ",", "pad", ",", "eos", ",", "break_mode", "=", "None", ",", "\n", "include_targets", "=", "False", ",", "document_sep_len", "=", "1", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dataset", "=", "dataset", "\n", "self", ".", "pad", "=", "pad", "\n", "self", ".", "eos", "=", "eos", "\n", "self", ".", "include_targets", "=", "include_targets", "\n", "slice_indices", "=", "[", "]", "\n", "\n", "assert", "len", "(", "dataset", ")", "==", "len", "(", "sizes", ")", "\n", "assert", "len", "(", "dataset", ")", ">", "0", "\n", "sizes", "=", "np", ".", "array", "(", "sizes", ",", "dtype", "=", "int", ")", "\n", "if", "break_mode", "is", "None", "or", "break_mode", "==", "'none'", ":", "\n", "            ", "total_size", "=", "sum", "(", "sizes", ")", "\n", "length", "=", "math", ".", "ceil", "(", "total_size", "/", "block_size", ")", "\n", "\n", "def", "block_at", "(", "i", ")", ":", "\n", "                ", "start", "=", "i", "*", "block_size", "\n", "end", "=", "min", "(", "start", "+", "block_size", ",", "total_size", ")", "\n", "return", "(", "start", ",", "end", ")", "\n", "\n", "", "slice_indices", "=", "[", "block_at", "(", "i", ")", "for", "i", "in", "range", "(", "length", ")", "]", "\n", "", "elif", "break_mode", "==", "'complete'", ":", "\n", "            ", "tok_idx", "=", "0", "\n", "sz_idx", "=", "0", "\n", "curr_size", "=", "0", "\n", "while", "sz_idx", "<", "len", "(", "sizes", ")", ":", "\n", "                ", "if", "curr_size", "+", "sizes", "[", "sz_idx", "]", "<=", "block_size", "or", "curr_size", "==", "0", ":", "\n", "                    ", "curr_size", "+=", "sizes", "[", "sz_idx", "]", "\n", "sz_idx", "+=", "1", "\n", "", "else", ":", "\n", "                    ", "slice_indices", ".", "append", "(", "(", "tok_idx", ",", "tok_idx", "+", "curr_size", ")", ")", "\n", "tok_idx", "+=", "curr_size", "\n", "curr_size", "=", "0", "\n", "", "", "if", "curr_size", ">", "0", ":", "\n", "                ", "slice_indices", ".", "append", "(", "(", "tok_idx", ",", "tok_idx", "+", "curr_size", ")", ")", "\n", "", "", "elif", "break_mode", "==", "'complete_doc'", ":", "\n", "            ", "tok_idx", "=", "0", "\n", "sz_idx", "=", "0", "\n", "curr_size", "=", "0", "\n", "while", "sz_idx", "<", "len", "(", "sizes", ")", ":", "\n", "                ", "if", "(", "\n", "(", "curr_size", "+", "sizes", "[", "sz_idx", "]", "<=", "block_size", "or", "curr_size", "==", "0", ")", "\n", "# an empty sentence indicates end-of-document:", "\n", "and", "sizes", "[", "sz_idx", "]", "!=", "document_sep_len", "\n", ")", ":", "\n", "                    ", "curr_size", "+=", "sizes", "[", "sz_idx", "]", "\n", "sz_idx", "+=", "1", "\n", "", "else", ":", "\n", "                    ", "slice_indices", ".", "append", "(", "(", "tok_idx", ",", "tok_idx", "+", "curr_size", ")", ")", "\n", "tok_idx", "+=", "curr_size", "\n", "curr_size", "=", "0", "\n", "if", "sizes", "[", "sz_idx", "]", "==", "document_sep_len", ":", "\n", "                        ", "tok_idx", "+=", "sizes", "[", "sz_idx", "]", "\n", "sz_idx", "+=", "1", "\n", "", "", "", "if", "curr_size", ">", "0", ":", "\n", "                ", "slice_indices", ".", "append", "(", "(", "tok_idx", ",", "tok_idx", "+", "curr_size", ")", ")", "\n", "", "", "elif", "break_mode", "==", "'eos'", ":", "\n", "            ", "slice_indices", "=", "np", ".", "empty", "(", "(", "len", "(", "sizes", ")", ",", "2", ")", ",", "dtype", "=", "int", ")", "\n", "if", "not", "torch", ".", "is_tensor", "(", "sizes", ")", ":", "\n", "                ", "sizes", "=", "torch", ".", "tensor", "(", "sizes", ")", "\n", "", "cumsum", "=", "torch", ".", "cumsum", "(", "sizes", ",", "dim", "=", "0", ")", "\n", "slice_indices", "[", "0", "]", "=", "[", "0", ",", "sizes", "[", "0", "]", "]", "\n", "if", "len", "(", "cumsum", ")", ">", "1", ":", "\n", "                ", "slice_indices", "[", "1", ":", "]", "=", "cumsum", ".", "unfold", "(", "0", ",", "2", ",", "1", ")", "\n", "", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "'Invalid break_mode: '", "+", "break_mode", ")", "\n", "\n", "", "slice_indices", "=", "np", ".", "array", "(", "slice_indices", ",", "dtype", "=", "int", ")", "\n", "self", ".", "_sizes", "=", "slice_indices", "[", ":", ",", "1", "]", "-", "slice_indices", "[", ":", ",", "0", "]", "\n", "\n", "# build index mapping block indices to the underlying dataset indices", "\n", "if", "break_mode", "==", "'eos'", ":", "\n", "# much faster version for eos break mode", "\n", "            ", "block_to_dataset_index", "=", "np", ".", "stack", "(", "\n", "[", "\n", "np", ".", "arange", "(", "len", "(", "sizes", ")", ")", ",", "# starting index in dataset", "\n", "np", ".", "zeros", "(", "len", "(", "sizes", ")", ",", "dtype", "=", "np", ".", "long", ")", ",", "# starting offset within starting index", "\n", "np", ".", "arange", "(", "len", "(", "sizes", ")", ")", "# ending index in dataset", "\n", "]", ",", "\n", "1", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "ds", "=", "DatasetSearcher", "(", "sizes", ")", "\n", "block_to_dataset_index", "=", "np", ".", "empty", "(", "(", "len", "(", "slice_indices", ")", ",", "3", ")", ",", "dtype", "=", "int", ")", "\n", "for", "i", ",", "(", "s", ",", "e", ")", "in", "enumerate", "(", "slice_indices", ")", ":", "\n", "                ", "ds", ".", "seek", "(", "s", ")", "\n", "start_ds_idx", "=", "ds", ".", "current_index", "\n", "start_offset", "=", "ds", ".", "current_offset", "\n", "if", "e", "<=", "s", ":", "\n", "                    ", "continue", "\n", "", "ds", ".", "seek", "(", "e", "-", "1", ")", "\n", "end_ds_idx", "=", "ds", ".", "current_index", "\n", "block_to_dataset_index", "[", "i", "]", "=", "(", "\n", "start_ds_idx", ",", "# starting index in dataset", "\n", "start_offset", ",", "# starting offset within starting index", "\n", "end_ds_idx", ",", "# ending index in dataset", "\n", ")", "\n", "\n", "", "", "self", ".", "_slice_indices", "=", "plasma_utils", ".", "PlasmaArray", "(", "slice_indices", ")", "\n", "self", ".", "_sizes", "=", "plasma_utils", ".", "PlasmaArray", "(", "self", ".", "_sizes", ")", "\n", "self", ".", "_block_to_dataset_index", "=", "plasma_utils", ".", "PlasmaArray", "(", "block_to_dataset_index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.token_block_dataset.TokenBlockDataset.slice_indices": [[142, 145], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "slice_indices", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_slice_indices", ".", "array", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.token_block_dataset.TokenBlockDataset.sizes": [[146, 149], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "sizes", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_sizes", ".", "array", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.token_block_dataset.TokenBlockDataset.block_to_dataset_index": [[150, 153], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "block_to_dataset_index", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_block_to_dataset_index", ".", "array", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.token_block_dataset.TokenBlockDataset.__getitem__": [[154, 180], ["torch.cat", "torch.cat", "torch.cat", "range", "torch.cat", "item.new", "item.new", "item.new"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "start_ds_idx", ",", "start_offset", ",", "end_ds_idx", "=", "self", ".", "block_to_dataset_index", "[", "index", "]", "\n", "buffer", "=", "torch", ".", "cat", "(", "[", "\n", "self", ".", "dataset", "[", "idx", "]", "for", "idx", "in", "range", "(", "start_ds_idx", ",", "end_ds_idx", "+", "1", ")", "\n", "]", ")", "\n", "slice_s", ",", "slice_e", "=", "self", ".", "slice_indices", "[", "index", "]", "\n", "length", "=", "slice_e", "-", "slice_s", "\n", "s", ",", "e", "=", "start_offset", ",", "start_offset", "+", "length", "\n", "item", "=", "buffer", "[", "s", ":", "e", "]", "\n", "\n", "if", "self", ".", "include_targets", ":", "\n", "# *target* is the original sentence (=item)", "\n", "# *source* is shifted right by 1 (maybe left-padded with eos)", "\n", "# *past_target* is shifted right by 2 (left-padded as needed)", "\n", "            ", "if", "s", "==", "0", ":", "\n", "                ", "source", "=", "torch", ".", "cat", "(", "[", "item", ".", "new", "(", "[", "self", ".", "eos", "]", ")", ",", "buffer", "[", "0", ":", "e", "-", "1", "]", "]", ")", "\n", "past_target", "=", "torch", ".", "cat", "(", "[", "item", ".", "new", "(", "[", "self", ".", "pad", ",", "self", ".", "eos", "]", ")", ",", "buffer", "[", "0", ":", "e", "-", "2", "]", "]", ")", "\n", "", "else", ":", "\n", "                ", "source", "=", "buffer", "[", "s", "-", "1", ":", "e", "-", "1", "]", "\n", "if", "s", "==", "1", ":", "\n", "                    ", "past_target", "=", "torch", ".", "cat", "(", "[", "item", ".", "new", "(", "[", "self", ".", "eos", "]", ")", ",", "buffer", "[", "0", ":", "e", "-", "2", "]", "]", ")", "\n", "", "else", ":", "\n", "                    ", "past_target", "=", "buffer", "[", "s", "-", "2", ":", "e", "-", "2", "]", "\n", "\n", "", "", "return", "source", ",", "item", ",", "past_target", "\n", "", "return", "item", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.token_block_dataset.TokenBlockDataset.__len__": [[181, 183], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "slice_indices", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.token_block_dataset.TokenBlockDataset.supports_prefetch": [[184, 187], ["getattr"], "methods", ["None"], ["", "@", "property", "\n", "def", "supports_prefetch", "(", "self", ")", ":", "\n", "        ", "return", "getattr", "(", "self", ".", "dataset", ",", "'supports_prefetch'", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.token_block_dataset.TokenBlockDataset.prefetch": [[188, 194], ["token_block_dataset.TokenBlockDataset.dataset.prefetch", "range"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.block_pair_dataset.BlockPairDataset.prefetch"], ["", "def", "prefetch", "(", "self", ",", "indices", ")", ":", "\n", "        ", "self", ".", "dataset", ".", "prefetch", "(", "{", "\n", "ds_idx", "\n", "for", "index", "in", "indices", "\n", "for", "start_ds_idx", ",", "_", ",", "end_ds_idx", "in", "[", "self", ".", "block_to_dataset_index", "[", "index", "]", "]", "\n", "for", "ds_idx", "in", "range", "(", "start_ds_idx", ",", "end_ds_idx", "+", "1", ")", "\n", "}", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.token_block_dataset.DatasetSearcher.__init__": [[201, 204], ["token_block_dataset.DatasetSearcher.reset"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.reset"], ["def", "__init__", "(", "self", ",", "sizes", ")", ":", "\n", "        ", "self", ".", "sizes", "=", "sizes", "\n", "self", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.token_block_dataset.DatasetSearcher.reset": [[205, 209], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "current_index", "=", "0", "# index in underlying dataset", "\n", "self", ".", "current_offset", "=", "0", "# offset within current index in underlying dataset", "\n", "self", ".", "current_i", "=", "0", "# \"flat\" index", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.token_block_dataset.DatasetSearcher.seek": [[210, 226], ["token_block_dataset.DatasetSearcher.reset", "token_block_dataset.DatasetSearcher.seek"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.reset", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.token_block_dataset.DatasetSearcher.seek"], ["", "def", "seek", "(", "self", ",", "i", ")", ":", "\n", "        ", "assert", "i", ">=", "0", "\n", "if", "i", "<", "self", ".", "current_i", ":", "\n", "            ", "self", ".", "reset", "(", ")", "\n", "", "if", "i", ">", "self", ".", "current_i", ":", "\n", "            ", "to_consume", "=", "i", "-", "self", ".", "current_i", "\n", "remaining", "=", "self", ".", "sizes", "[", "self", ".", "current_index", "]", "-", "self", ".", "current_offset", "\n", "if", "remaining", ">", "to_consume", ":", "\n", "                ", "self", ".", "current_offset", "+=", "to_consume", "\n", "self", ".", "current_i", "+=", "to_consume", "\n", "", "else", ":", "\n", "                ", "self", ".", "current_i", "+=", "remaining", "\n", "self", ".", "current_index", "+=", "1", "\n", "self", ".", "current_offset", "=", "0", "\n", "self", ".", "seek", "(", "i", ")", "\n", "", "", "assert", "self", ".", "current_i", "==", "i", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.lm_context_window_dataset.LMContextWindowDataset.__init__": [[17, 25], ["isinstance", "numpy.empty"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "dataset", ",", "tokens_per_sample", ",", "context_window", ",", "pad_idx", ")", ":", "\n", "        ", "assert", "isinstance", "(", "dataset", ",", "MonolingualDataset", ")", "\n", "assert", "context_window", ">", "0", "\n", "self", ".", "dataset", "=", "dataset", "\n", "self", ".", "tokens_per_sample", "=", "tokens_per_sample", "\n", "self", ".", "context_window", "=", "context_window", "\n", "self", ".", "pad_idx", "=", "pad_idx", "\n", "self", ".", "prev_tokens", "=", "np", ".", "empty", "(", "[", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.lm_context_window_dataset.LMContextWindowDataset.__getitem__": [[26, 28], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "dataset", "[", "index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.lm_context_window_dataset.LMContextWindowDataset.__len__": [[29, 31], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "dataset", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.lm_context_window_dataset.LMContextWindowDataset.collater": [[32, 62], ["lm_context_window_dataset.LMContextWindowDataset.dataset.collater", "numpy.empty", "numpy.full", "toks.ne().long().sum().cpu", "range", "torch.from_numpy", "torch.from_numpy", "numpy.full", "numpy.concatenate", "len", "len", "toks.ne().long().sum", "len", "len", "toks[].numpy", "toks.ne().long", "len", "len", "len", "toks.ne"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.collater"], ["", "def", "collater", "(", "self", ",", "samples", ")", ":", "\n", "        ", "sample", "=", "self", ".", "dataset", ".", "collater", "(", "samples", ")", "\n", "\n", "pad", "=", "self", ".", "pad_idx", "\n", "max_sample_len", "=", "self", ".", "tokens_per_sample", "+", "self", ".", "context_window", "\n", "\n", "bsz", ",", "tsz", "=", "sample", "[", "'net_input'", "]", "[", "'src_tokens'", "]", ".", "shape", "\n", "start_idxs", "=", "[", "0", "]", "*", "bsz", "\n", "toks", "=", "sample", "[", "'net_input'", "]", "[", "'src_tokens'", "]", "\n", "lengths", "=", "sample", "[", "'net_input'", "]", "[", "'src_lengths'", "]", "\n", "tgt", "=", "sample", "[", "'target'", "]", "\n", "new_toks", "=", "np", ".", "empty", "(", "[", "bsz", ",", "tsz", "+", "self", ".", "context_window", "]", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "new_tgt", "=", "np", ".", "full", "(", "[", "bsz", ",", "tsz", "+", "self", ".", "context_window", "]", ",", "pad", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "sample_lens", "=", "toks", ".", "ne", "(", "pad", ")", ".", "long", "(", ")", ".", "sum", "(", "dim", "=", "1", ")", ".", "cpu", "(", ")", "\n", "for", "i", "in", "range", "(", "bsz", ")", ":", "\n", "            ", "sample_len", "=", "sample_lens", "[", "i", "]", "\n", "extra", "=", "len", "(", "self", ".", "prev_tokens", ")", "+", "sample_len", "-", "max_sample_len", "\n", "if", "extra", ">", "0", ":", "\n", "                ", "self", ".", "prev_tokens", "=", "self", ".", "prev_tokens", "[", "extra", ":", "]", "\n", "", "pads", "=", "np", ".", "full", "(", "self", ".", "context_window", "-", "len", "(", "self", ".", "prev_tokens", ")", ",", "pad", ")", "\n", "new_toks", "[", "i", "]", "=", "np", ".", "concatenate", "(", "[", "self", ".", "prev_tokens", ",", "toks", "[", "i", "]", ".", "numpy", "(", ")", ",", "pads", "]", ")", "\n", "new_tgt", "[", "i", ",", "len", "(", "self", ".", "prev_tokens", ")", ":", "len", "(", "self", ".", "prev_tokens", ")", "+", "len", "(", "tgt", "[", "i", "]", ")", "]", "=", "tgt", "[", "i", "]", "\n", "start_idxs", "[", "i", "]", "=", "len", "(", "self", ".", "prev_tokens", ")", "\n", "lengths", "[", "i", "]", "+=", "len", "(", "self", ".", "prev_tokens", ")", "\n", "self", ".", "prev_tokens", "=", "new_toks", "[", "i", "]", "[", "new_toks", "[", "i", "]", "!=", "pad", "]", "[", "-", "self", ".", "context_window", ":", "]", "\n", "", "sample", "[", "'net_input'", "]", "[", "'src_tokens'", "]", "=", "torch", ".", "from_numpy", "(", "new_toks", ")", "\n", "sample", "[", "'target'", "]", "=", "torch", ".", "from_numpy", "(", "new_tgt", ")", "\n", "sample", "[", "'start_indices'", "]", "=", "start_idxs", "\n", "\n", "return", "sample", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.lm_context_window_dataset.LMContextWindowDataset.num_tokens": [[63, 65], ["lm_context_window_dataset.LMContextWindowDataset.dataset.num_tokens"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.num_tokens"], ["", "def", "num_tokens", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "dataset", ".", "num_tokens", "(", "index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.lm_context_window_dataset.LMContextWindowDataset.size": [[66, 68], ["lm_context_window_dataset.LMContextWindowDataset.dataset.size"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "size", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "dataset", ".", "size", "(", "index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.lm_context_window_dataset.LMContextWindowDataset.ordered_indices": [[69, 72], ["numpy.arange", "len"], "methods", ["None"], ["", "def", "ordered_indices", "(", "self", ")", ":", "\n", "# NOTE we don't shuffle the data to retain access to the previous dataset elements", "\n", "        ", "return", "np", ".", "arange", "(", "len", "(", "self", ".", "dataset", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.lm_context_window_dataset.LMContextWindowDataset.supports_prefetch": [[73, 76], ["getattr"], "methods", ["None"], ["", "@", "property", "\n", "def", "supports_prefetch", "(", "self", ")", ":", "\n", "        ", "return", "getattr", "(", "self", ".", "dataset", ",", "'supports_prefetch'", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.lm_context_window_dataset.LMContextWindowDataset.prefetch": [[77, 79], ["lm_context_window_dataset.LMContextWindowDataset.dataset.prefetch"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.block_pair_dataset.BlockPairDataset.prefetch"], ["", "def", "prefetch", "(", "self", ",", "indices", ")", ":", "\n", "        ", "return", "self", ".", "dataset", ".", "prefetch", "(", "indices", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.CountingIterator.__init__": [[25, 30], ["iter", "len"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "iterable", ",", "start", "=", "0", ")", ":", "\n", "        ", "self", ".", "iterable", "=", "iterable", "\n", "self", ".", "count", "=", "start", "\n", "self", ".", "itr", "=", "iter", "(", "self", ")", "\n", "self", ".", "len", "=", "start", "+", "len", "(", "iterable", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.CountingIterator.__len__": [[31, 33], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "len", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.CountingIterator.__iter__": [[34, 38], ["None"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "for", "x", "in", "self", ".", "iterable", ":", "\n", "            ", "self", ".", "count", "+=", "1", "\n", "yield", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.CountingIterator.__next__": [[39, 41], ["next"], "methods", ["None"], ["", "", "def", "__next__", "(", "self", ")", ":", "\n", "        ", "return", "next", "(", "self", ".", "itr", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.CountingIterator.has_next": [[42, 45], ["len"], "methods", ["None"], ["", "def", "has_next", "(", "self", ")", ":", "\n", "        ", "\"\"\"Whether the iterator has been exhausted.\"\"\"", "\n", "return", "self", ".", "count", "<", "len", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.CountingIterator.skip": [[46, 50], ["next", "itertools.islice"], "methods", ["None"], ["", "def", "skip", "(", "self", ",", "num_to_skip", ")", ":", "\n", "        ", "\"\"\"Fast-forward the iterator by skipping *num_to_skip* elements.\"\"\"", "\n", "next", "(", "itertools", ".", "islice", "(", "self", ".", "itr", ",", "num_to_skip", ",", "num_to_skip", ")", ",", "None", ")", "\n", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.EpochBatchIterating.__len__": [[53, 55], ["None"], "methods", ["None"], ["    ", "def", "__len__", "(", "self", ")", "->", "int", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.EpochBatchIterating.next_epoch_itr": [[56, 58], ["None"], "methods", ["None"], ["", "def", "next_epoch_itr", "(", "self", ",", "shuffle", "=", "True", ",", "fix_batches_to_gpus", "=", "False", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.EpochBatchIterating.end_of_epoch": [[59, 62], ["None"], "methods", ["None"], ["", "def", "end_of_epoch", "(", "self", ")", "->", "bool", ":", "\n", "        ", "\"\"\"Returns whether the most recent epoch iterator has been exhausted\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.EpochBatchIterating.iterations_in_epoch": [[63, 66], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "iterations_in_epoch", "(", "self", ")", "->", "int", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.EpochBatchIterating.state_dict": [[67, 69], ["None"], "methods", ["None"], ["", "def", "state_dict", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.EpochBatchIterating.load_state_dict": [[70, 72], ["None"], "methods", ["None"], ["", "def", "load_state_dict", "(", "self", ",", "state_dict", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.StreamingEpochBatchIterator.__init__": [[75, 84], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "\n", "self", ",", "dataset", ",", "epoch", "=", "0", ",", "num_shards", "=", "1", ",", "shard_id", "=", "0", ",", "\n", ")", ":", "\n", "# assert isinstance(dataset, torch.utils.data.Dataset)", "\n", "        ", "self", ".", "dataset", "=", "dataset", "\n", "self", ".", "epoch", "=", "epoch", "\n", "self", ".", "_current_epoch_iterator", "=", "None", "\n", "self", ".", "num_shards", "=", "num_shards", "\n", "self", ".", "shard_id", "=", "shard_id", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.StreamingEpochBatchIterator.next_epoch_itr": [[85, 95], ["iterators.CountingIterator", "iterators.ShardedIterator"], "methods", ["None"], ["", "def", "next_epoch_itr", "(", "self", ",", "shuffle", "=", "True", ",", "fix_batches_to_gpus", "=", "False", ")", ":", "\n", "        ", "self", ".", "epoch", "+=", "1", "\n", "self", ".", "_current_epoch_iterator", "=", "CountingIterator", "(", "\n", "iterable", "=", "ShardedIterator", "(", "\n", "iterable", "=", "self", ".", "dataset", ",", "\n", "num_shards", "=", "self", ".", "num_shards", ",", "\n", "shard_id", "=", "self", ".", "shard_id", ",", "\n", ")", ",", "\n", ")", "\n", "return", "self", ".", "_current_epoch_iterator", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.StreamingEpochBatchIterator.end_of_epoch": [[96, 98], ["iterators.StreamingEpochBatchIterator._current_epoch_iterator.has_next"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.CountingIterator.has_next"], ["", "def", "end_of_epoch", "(", "self", ")", "->", "bool", ":", "\n", "        ", "return", "not", "self", ".", "_current_epoch_iterator", ".", "has_next", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.StreamingEpochBatchIterator.iterations_in_epoch": [[99, 104], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "iterations_in_epoch", "(", "self", ")", "->", "int", ":", "\n", "        ", "if", "self", ".", "_current_epoch_iterator", "is", "not", "None", ":", "\n", "            ", "return", "self", ".", "_current_epoch_iterator", ".", "count", "\n", "", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.StreamingEpochBatchIterator.state_dict": [[105, 108], ["None"], "methods", ["None"], ["", "def", "state_dict", "(", "self", ")", ":", "\n", "        ", "return", "{", "\n", "'epoch'", ":", "self", ".", "epoch", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.StreamingEpochBatchIterator.load_state_dict": [[110, 112], ["None"], "methods", ["None"], ["", "def", "load_state_dict", "(", "self", ",", "state_dict", ")", ":", "\n", "        ", "self", ".", "epoch", "=", "state_dict", "[", "'epoch'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.EpochBatchIterator.__init__": [[143, 160], ["isinstance", "tuple", "getattr"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "dataset", ",", "collate_fn", ",", "batch_sampler", ",", "seed", "=", "1", ",", "num_shards", "=", "1", ",", "shard_id", "=", "0", ",", "\n", "num_workers", "=", "0", ",", "epoch", "=", "0", ",", "\n", ")", ":", "\n", "        ", "assert", "isinstance", "(", "dataset", ",", "torch", ".", "utils", ".", "data", ".", "Dataset", ")", "\n", "self", ".", "dataset", "=", "dataset", "\n", "self", ".", "collate_fn", "=", "collate_fn", "\n", "self", ".", "frozen_batches", "=", "tuple", "(", "batch_sampler", ")", "\n", "self", ".", "seed", "=", "seed", "\n", "self", ".", "num_shards", "=", "num_shards", "\n", "self", ".", "shard_id", "=", "shard_id", "\n", "self", ".", "num_workers", "=", "num_workers", "\n", "\n", "self", ".", "epoch", "=", "epoch", "\n", "self", ".", "_cur_epoch_itr", "=", "None", "\n", "self", ".", "_next_epoch_itr", "=", "None", "\n", "self", ".", "_supports_prefetch", "=", "getattr", "(", "dataset", ",", "'supports_prefetch'", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.EpochBatchIterator.__len__": [[161, 163], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "frozen_batches", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.EpochBatchIterator.next_epoch_itr": [[164, 184], ["iterators.EpochBatchIterator.dataset.set_epoch", "iterators.EpochBatchIterator._get_iterator_for_epoch"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.nested_dictionary_dataset.NestedDictionaryDataset.set_epoch", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.EpochBatchIterator._get_iterator_for_epoch"], ["", "def", "next_epoch_itr", "(", "self", ",", "shuffle", "=", "True", ",", "fix_batches_to_gpus", "=", "False", ")", ":", "\n", "        ", "\"\"\"Return a new iterator over the dataset.\n\n        Args:\n            shuffle (bool, optional): shuffle batches before returning the\n                iterator (default: True).\n            fix_batches_to_gpus: ensure that batches are always\n                allocated to the same shards across epochs. Requires\n                that :attr:`dataset` supports prefetching (default: False).\n        \"\"\"", "\n", "if", "self", ".", "_next_epoch_itr", "is", "not", "None", ":", "\n", "            ", "self", ".", "_cur_epoch_itr", "=", "self", ".", "_next_epoch_itr", "\n", "self", ".", "_next_epoch_itr", "=", "None", "\n", "", "else", ":", "\n", "            ", "self", ".", "epoch", "+=", "1", "\n", "self", ".", "_cur_epoch_itr", "=", "self", ".", "_get_iterator_for_epoch", "(", "\n", "self", ".", "epoch", ",", "shuffle", ",", "fix_batches_to_gpus", "=", "fix_batches_to_gpus", ",", "\n", ")", "\n", "", "self", ".", "dataset", ".", "set_epoch", "(", "self", ".", "epoch", ")", "\n", "return", "self", ".", "_cur_epoch_itr", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.EpochBatchIterator.end_of_epoch": [[185, 188], ["iterators.EpochBatchIterator._cur_epoch_itr.has_next"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.CountingIterator.has_next"], ["", "def", "end_of_epoch", "(", "self", ")", "->", "bool", ":", "\n", "        ", "\"\"\"Returns whether the most recent epoch iterator has been exhausted\"\"\"", "\n", "return", "not", "self", ".", "_cur_epoch_itr", ".", "has_next", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.EpochBatchIterator.iterations_in_epoch": [[189, 197], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "iterations_in_epoch", "(", "self", ")", ":", "\n", "        ", "\"\"\"The number of consumed batches in the current epoch.\"\"\"", "\n", "if", "self", ".", "_cur_epoch_itr", "is", "not", "None", ":", "\n", "            ", "return", "self", ".", "_cur_epoch_itr", ".", "count", "\n", "", "elif", "self", ".", "_next_epoch_itr", "is", "not", "None", ":", "\n", "            ", "return", "self", ".", "_next_epoch_itr", ".", "count", "\n", "", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.EpochBatchIterator.state_dict": [[198, 203], ["None"], "methods", ["None"], ["", "def", "state_dict", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns a dictionary containing a whole state of the iterator.\"\"\"", "\n", "return", "{", "\n", "'epoch'", ":", "self", ".", "epoch", ",", "\n", "'iterations_in_epoch'", ":", "self", ".", "iterations_in_epoch", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.EpochBatchIterator.load_state_dict": [[205, 215], ["state_dict.get", "iterators.EpochBatchIterator._get_iterator_for_epoch", "state_dict.get"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.EpochBatchIterator._get_iterator_for_epoch"], ["", "def", "load_state_dict", "(", "self", ",", "state_dict", ")", ":", "\n", "        ", "\"\"\"Copies the state of the iterator from the given *state_dict*.\"\"\"", "\n", "self", ".", "epoch", "=", "state_dict", "[", "'epoch'", "]", "\n", "itr_pos", "=", "state_dict", ".", "get", "(", "'iterations_in_epoch'", ",", "0", ")", "\n", "if", "itr_pos", ">", "0", ":", "\n", "# fast-forward epoch iterator", "\n", "            ", "self", ".", "_next_epoch_itr", "=", "self", ".", "_get_iterator_for_epoch", "(", "\n", "self", ".", "epoch", ",", "\n", "shuffle", "=", "state_dict", ".", "get", "(", "'shuffle'", ",", "True", ")", ",", "\n", "offset", "=", "itr_pos", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.EpochBatchIterator._get_iterator_for_epoch": [[217, 259], ["iterators.CountingIterator", "list", "iterators.EpochBatchIterator.dataset.prefetch", "list", "torch.utils.data.DataLoader", "data_utils.numpy_seed", "numpy.random.shuffle", "iterators.EpochBatchIterator._get_iterator_for_epoch.shuffle_batches"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.block_pair_dataset.BlockPairDataset.prefetch", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.data_utils.numpy_seed"], ["", "", "def", "_get_iterator_for_epoch", "(", "self", ",", "epoch", ",", "shuffle", ",", "fix_batches_to_gpus", "=", "False", ",", "offset", "=", "0", ")", ":", "\n", "\n", "        ", "def", "shuffle_batches", "(", "batches", ",", "seed", ")", ":", "\n", "# set seed based on the seed and epoch number so that we get", "\n", "# reproducible results when resuming from checkpoints", "\n", "            ", "with", "data_utils", ".", "numpy_seed", "(", "seed", ")", ":", "\n", "                ", "np", ".", "random", ".", "shuffle", "(", "batches", ")", "\n", "", "return", "batches", "\n", "\n", "", "if", "self", ".", "_supports_prefetch", ":", "\n", "            ", "batches", "=", "self", ".", "frozen_batches", "\n", "\n", "if", "shuffle", "and", "not", "fix_batches_to_gpus", ":", "\n", "                ", "batches", "=", "shuffle_batches", "(", "list", "(", "batches", ")", ",", "self", ".", "seed", "+", "epoch", ")", "\n", "\n", "", "batches", "=", "list", "(", "ShardedIterator", "(", "\n", "batches", ",", "self", ".", "num_shards", ",", "self", ".", "shard_id", ",", "fill_value", "=", "[", "]", "\n", ")", ")", "\n", "self", ".", "dataset", ".", "prefetch", "(", "[", "i", "for", "s", "in", "batches", "for", "i", "in", "s", "]", ")", "\n", "\n", "if", "shuffle", "and", "fix_batches_to_gpus", ":", "\n", "                ", "batches", "=", "shuffle_batches", "(", "batches", ",", "self", ".", "seed", "+", "epoch", "+", "self", ".", "shard_id", ")", "\n", "", "", "else", ":", "\n", "            ", "if", "shuffle", ":", "\n", "                ", "batches", "=", "shuffle_batches", "(", "list", "(", "self", ".", "frozen_batches", ")", ",", "self", ".", "seed", "+", "epoch", ")", "\n", "", "else", ":", "\n", "                ", "batches", "=", "self", ".", "frozen_batches", "\n", "", "batches", "=", "list", "(", "ShardedIterator", "(", "\n", "batches", ",", "self", ".", "num_shards", ",", "self", ".", "shard_id", ",", "fill_value", "=", "[", "]", "\n", ")", ")", "\n", "\n", "", "if", "offset", ">", "0", "and", "offset", ">=", "len", "(", "batches", ")", ":", "\n", "            ", "return", "None", "\n", "\n", "", "return", "CountingIterator", "(", "\n", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "self", ".", "dataset", ",", "\n", "collate_fn", "=", "self", ".", "collate_fn", ",", "\n", "batch_sampler", "=", "batches", "[", "offset", ":", "]", ",", "\n", "num_workers", "=", "self", ".", "num_workers", ",", "\n", ")", ",", "\n", "start", "=", "offset", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.GroupedIterator.__init__": [[270, 275], ["int", "int", "math.ceil", "math.ceil", "len", "float", "getattr", "float"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "iterable", ",", "chunk_size", ")", ":", "\n", "        ", "self", ".", "_len", "=", "int", "(", "math", ".", "ceil", "(", "len", "(", "iterable", ")", "/", "float", "(", "chunk_size", ")", ")", ")", "\n", "self", ".", "offset", "=", "int", "(", "math", ".", "ceil", "(", "getattr", "(", "iterable", ",", "'count'", ",", "0", ")", "/", "float", "(", "chunk_size", ")", ")", ")", "\n", "self", ".", "itr", "=", "iterable", "\n", "self", ".", "chunk_size", "=", "chunk_size", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.GroupedIterator.__len__": [[276, 278], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_len", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.GroupedIterator.__iter__": [[279, 281], ["None"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.GroupedIterator.__next__": [[282, 291], ["range", "chunk.append", "next", "len"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "__next__", "(", "self", ")", ":", "\n", "        ", "chunk", "=", "[", "]", "\n", "try", ":", "\n", "            ", "for", "_", "in", "range", "(", "self", ".", "chunk_size", ")", ":", "\n", "                ", "chunk", ".", "append", "(", "next", "(", "self", ".", "itr", ")", ")", "\n", "", "", "except", "StopIteration", "as", "e", ":", "\n", "            ", "if", "len", "(", "chunk", ")", "==", "0", ":", "\n", "                ", "raise", "e", "\n", "", "", "return", "chunk", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.ShardedIterator.__init__": [[304, 316], ["itertools.zip_longest", "ValueError", "len", "range", "itertools.islice", "len", "len"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "iterable", ",", "num_shards", ",", "shard_id", ",", "fill_value", "=", "None", ")", ":", "\n", "        ", "if", "shard_id", "<", "0", "or", "shard_id", ">=", "num_shards", ":", "\n", "            ", "raise", "ValueError", "(", "'shard_id must be between 0 and num_shards'", ")", "\n", "\n", "", "self", ".", "_sharded_len", "=", "len", "(", "iterable", ")", "//", "num_shards", "\n", "if", "len", "(", "iterable", ")", "%", "num_shards", ">", "0", ":", "\n", "            ", "self", ".", "_sharded_len", "+=", "1", "\n", "\n", "", "self", ".", "itr", "=", "itertools", ".", "zip_longest", "(", "\n", "range", "(", "self", ".", "_sharded_len", ")", ",", "\n", "itertools", ".", "islice", "(", "iterable", ",", "shard_id", ",", "len", "(", "iterable", ")", ",", "num_shards", ")", ",", "\n", "fillvalue", "=", "fill_value", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.ShardedIterator.__len__": [[318, 320], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_sharded_len", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.ShardedIterator.__iter__": [[321, 323], ["None"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.ShardedIterator.__next__": [[324, 326], ["next"], "methods", ["None"], ["", "def", "__next__", "(", "self", ")", ":", "\n", "        ", "return", "next", "(", "self", ".", "itr", ")", "[", "1", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.data_utils.infer_language_pair": [[17, 25], ["os.listdir", "filename.split", "parts[].split", "len", "len", "parts[].split"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["        ", "return", "mean", ",", "1.0", "/", "(", "torch", ".", "sqrt", "(", "var", ")", "+", "eps", ")", "\n", "", "return", "mean", ",", "1.0", "/", "torch", ".", "sqrt", "(", "var", ")", "\n", "\n", "\n", "", "def", "apply_mv_norm", "(", "features", ")", ":", "\n", "    ", "mean", ",", "invstddev", "=", "calc_mean_invstddev", "(", "features", ")", "\n", "res", "=", "(", "features", "-", "mean", ")", "*", "invstddev", "\n", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.data_utils.collate_tokens": [[27, 46], ["max", "values[].new().fill_", "enumerate", "data_utils.collate_tokens.copy_tensor"], "function", ["None"], ["", "def", "lengths_to_encoder_padding_mask", "(", "lengths", ",", "batch_first", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    convert lengths (a 1-D Long/Int tensor) to 2-D binary tensor\n\n    Args:\n        lengths: a (B, )-shaped tensor\n\n    Return:\n        max_length: maximum length of B sequences\n        encoder_padding_mask: a (max_length, B) binary mask, where\n        [t, b] = 0 for t < lengths[b] and 1 otherwise\n\n    TODO:\n        kernelize this function if benchmarking shows this function is slow\n    \"\"\"", "\n", "max_lengths", "=", "torch", ".", "max", "(", "lengths", ")", ".", "item", "(", ")", "\n", "bsz", "=", "lengths", ".", "size", "(", "0", ")", "\n", "encoder_padding_mask", "=", "torch", ".", "arange", "(", "\n", "max_lengths", "\n", ")", ".", "to", "(", "# a (T, ) tensor with [0, ..., T-1]", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.data_utils.load_indexed_dataset": [[48, 91], ["itertools.count", "indexed_dataset.make_dataset", "print", "datasets.append", "len", "indexed_dataset.infer_dataset_impl", "len", "ConcatDataset", "str", "len"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.make_dataset", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.infer_dataset_impl"], [")", ".", "view", "(", "# move to the right device", "\n", "1", ",", "max_lengths", "\n", ")", ".", "expand", "(", "# reshape to (1, T)-shaped tensor", "\n", "bsz", ",", "-", "1", "\n", ")", ">=", "lengths", ".", "view", "(", "# expand to (B, T)-shaped tensor", "\n", "bsz", ",", "1", "\n", ")", ".", "expand", "(", "\n", "-", "1", ",", "max_lengths", "\n", ")", "\n", "if", "not", "batch_first", ":", "\n", "        ", "return", "encoder_padding_mask", ".", "t", "(", ")", ",", "max_lengths", "\n", "", "else", ":", "\n", "        ", "return", "encoder_padding_mask", ",", "max_lengths", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.data_utils.numpy_seed": [[93, 108], ["numpy.random.get_state", "numpy.random.seed", "len", "int", "numpy.random.set_state", "hash"], "function", ["None"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.data_utils.collect_filtered": [[110, 125], ["function", "filtered.append"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.data_utils.filter_by_size": [[127, 180], ["data_utils.collect_filtered", "len", "print", "isinstance", "isinstance", "isinstance", "Exception", "size_fn", "size_fn", "isinstance", "all", "all", "len", "len", "set", "set", "isinstance", "isinstance", "all", "isinstance", "all", "size_fn", "max_positions.keys", "size_fn.keys", "all", "size_fn", "size_fn", "zip", "zip", "size_fn", "size_fn", "zip", "size_fn().values", "size_fn"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.data_utils.collect_filtered", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.data_utils.batch_by_size": [[182, 240], ["float", "float", "sample_lens.append", "max", "data_utils.batch_by_size.is_batch_full"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.data_utils.process_bpe_symbol": [[242, 248], ["sentence.replace().replace().strip.replace().replace().strip", "sentence.replace().replace().strip.replace().replace", "sentence.replace().replace().strip.replace"], "function", ["None"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.__init__": [[20, 40], ["dictionary.Dictionary.add_symbol", "dictionary.Dictionary.add_symbol", "dictionary.Dictionary.add_symbol", "dictionary.Dictionary.add_symbol", "len", "dictionary.Dictionary.add_symbol"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol"], ["def", "__init__", "(", "\n", "self", ",", "\n", "pad", "=", "'<pad>'", ",", "\n", "eos", "=", "'</s>'", ",", "\n", "unk", "=", "'<unk>'", ",", "\n", "bos", "=", "'<s>'", ",", "\n", "extra_special_symbols", "=", "None", ",", "\n", ")", ":", "\n", "        ", "self", ".", "unk_word", ",", "self", ".", "pad_word", ",", "self", ".", "eos_word", "=", "unk", ",", "pad", ",", "eos", "\n", "self", ".", "symbols", "=", "[", "]", "\n", "self", ".", "count", "=", "[", "]", "\n", "self", ".", "indices", "=", "{", "}", "\n", "self", ".", "bos_index", "=", "self", ".", "add_symbol", "(", "bos", ")", "\n", "self", ".", "pad_index", "=", "self", ".", "add_symbol", "(", "pad", ")", "\n", "self", ".", "eos_index", "=", "self", ".", "add_symbol", "(", "eos", ")", "\n", "self", ".", "unk_index", "=", "self", ".", "add_symbol", "(", "unk", ")", "\n", "if", "extra_special_symbols", ":", "\n", "            ", "for", "s", "in", "extra_special_symbols", ":", "\n", "                ", "self", ".", "add_symbol", "(", "s", ")", "\n", "", "", "self", ".", "nspecial", "=", "len", "(", "self", ".", "symbols", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.__eq__": [[41, 43], ["None"], "methods", ["None"], ["", "def", "__eq__", "(", "self", ",", "other", ")", ":", "\n", "        ", "return", "self", ".", "indices", "==", "other", ".", "indices", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.__getitem__": [[44, 48], ["len"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "if", "idx", "<", "len", "(", "self", ".", "symbols", ")", ":", "\n", "            ", "return", "self", ".", "symbols", "[", "idx", "]", "\n", "", "return", "self", ".", "unk_word", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.__len__": [[49, 52], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns the number of symbols in the dictionary\"\"\"", "\n", "return", "len", "(", "self", ".", "symbols", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.__contains__": [[53, 55], ["None"], "methods", ["None"], ["", "def", "__contains__", "(", "self", ",", "sym", ")", ":", "\n", "        ", "return", "sym", "in", "self", ".", "indices", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.index": [[56, 62], ["isinstance"], "methods", ["None"], ["", "def", "index", "(", "self", ",", "sym", ")", ":", "\n", "        ", "\"\"\"Returns the index of the specified symbol\"\"\"", "\n", "assert", "isinstance", "(", "sym", ",", "str", ")", "\n", "if", "sym", "in", "self", ".", "indices", ":", "\n", "            ", "return", "self", ".", "indices", "[", "sym", "]", "\n", "", "return", "self", ".", "unk_index", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.string": [[63, 79], ["split_token.join", "fairseq.data.data_utils.process_bpe_symbol", "torch.is_tensor", "tensor.dim", "dictionary.Dictionary.unk", "dictionary.Dictionary.unk_string", "dictionary.Dictionary.string.token_string"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.data_utils.process_bpe_symbol", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.unk", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.unk_string"], ["", "def", "string", "(", "self", ",", "tensor", ",", "bpe_symbol", "=", "None", ",", "escape_unk", "=", "False", ",", "split_token", "=", "' '", ")", ":", "\n", "        ", "\"\"\"Helper for converting a tensor of token indices to a string.\n\n        Can optionally remove BPE symbols or escape <unk> words.\n        \"\"\"", "\n", "if", "torch", ".", "is_tensor", "(", "tensor", ")", "and", "tensor", ".", "dim", "(", ")", "==", "2", ":", "\n", "            ", "return", "'\\n'", ".", "join", "(", "self", ".", "string", "(", "t", ",", "bpe_symbol", ",", "escape_unk", ")", "for", "t", "in", "tensor", ")", "\n", "\n", "", "def", "token_string", "(", "i", ")", ":", "\n", "            ", "if", "i", "==", "self", ".", "unk", "(", ")", ":", "\n", "                ", "return", "self", ".", "unk_string", "(", "escape_unk", ")", "\n", "", "else", ":", "\n", "                ", "return", "self", "[", "i", "]", "\n", "\n", "", "", "sent", "=", "split_token", ".", "join", "(", "token_string", "(", "i", ")", "for", "i", "in", "tensor", "if", "i", "!=", "self", ".", "eos", "(", ")", ")", "\n", "return", "data_utils", ".", "process_bpe_symbol", "(", "sent", ",", "bpe_symbol", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.unk_string": [[80, 86], ["None"], "methods", ["None"], ["", "def", "unk_string", "(", "self", ",", "escape", "=", "False", ")", ":", "\n", "        ", "\"\"\"Return unknown string, optionally escaped as: <<unk>>\"\"\"", "\n", "if", "escape", ":", "\n", "            ", "return", "'<{}>'", ".", "format", "(", "self", ".", "unk_word", ")", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "unk_word", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol": [[87, 99], ["len", "dictionary.Dictionary.symbols.append", "dictionary.Dictionary.count.append"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "", "def", "add_symbol", "(", "self", ",", "word", ",", "n", "=", "1", ")", ":", "\n", "        ", "\"\"\"Adds a word to the dictionary\"\"\"", "\n", "if", "word", "in", "self", ".", "indices", ":", "\n", "            ", "idx", "=", "self", ".", "indices", "[", "word", "]", "\n", "self", ".", "count", "[", "idx", "]", "=", "self", ".", "count", "[", "idx", "]", "+", "n", "\n", "return", "idx", "\n", "", "else", ":", "\n", "            ", "idx", "=", "len", "(", "self", ".", "symbols", ")", "\n", "self", ".", "indices", "[", "word", "]", "=", "idx", "\n", "self", ".", "symbols", ".", "append", "(", "word", ")", "\n", "self", ".", "count", ".", "append", "(", "n", ")", "\n", "return", "idx", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.update": [[100, 112], ["len", "dictionary.Dictionary.symbols.append", "dictionary.Dictionary.count.append"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "", "def", "update", "(", "self", ",", "new_dict", ")", ":", "\n", "        ", "\"\"\"Updates counts from new dictionary.\"\"\"", "\n", "for", "word", "in", "new_dict", ".", "symbols", ":", "\n", "            ", "idx2", "=", "new_dict", ".", "indices", "[", "word", "]", "\n", "if", "word", "in", "self", ".", "indices", ":", "\n", "                ", "idx", "=", "self", ".", "indices", "[", "word", "]", "\n", "self", ".", "count", "[", "idx", "]", "=", "self", ".", "count", "[", "idx", "]", "+", "new_dict", ".", "count", "[", "idx2", "]", "\n", "", "else", ":", "\n", "                ", "idx", "=", "len", "(", "self", ".", "symbols", ")", "\n", "self", ".", "indices", "[", "word", "]", "=", "idx", "\n", "self", ".", "symbols", ".", "append", "(", "word", ")", "\n", "self", ".", "count", ".", "append", "(", "new_dict", ".", "count", "[", "idx2", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.finalize": [[113, 157], ["dict", "collections.Counter", "collections.Counter.most_common", "len", "list", "list", "len", "zip", "dict", "len", "len", "range", "sorted", "len", "new_symbols.append", "new_count.append", "len", "new_symbols.append", "new_count.append", "len", "zip"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "", "", "def", "finalize", "(", "self", ",", "threshold", "=", "-", "1", ",", "nwords", "=", "-", "1", ",", "padding_factor", "=", "8", ")", ":", "\n", "        ", "\"\"\"Sort symbols by frequency in descending order, ignoring special ones.\n\n        Args:\n            - threshold defines the minimum word count\n            - nwords defines the total number of words in the final dictionary,\n                including special symbols\n            - padding_factor can be used to pad the dictionary size to be a\n                multiple of 8, which is important on some hardware (e.g., Nvidia\n                Tensor Cores).\n        \"\"\"", "\n", "if", "nwords", "<=", "0", ":", "\n", "            ", "nwords", "=", "len", "(", "self", ")", "\n", "\n", "", "new_indices", "=", "dict", "(", "zip", "(", "self", ".", "symbols", "[", ":", "self", ".", "nspecial", "]", ",", "range", "(", "self", ".", "nspecial", ")", ")", ")", "\n", "new_symbols", "=", "self", ".", "symbols", "[", ":", "self", ".", "nspecial", "]", "\n", "new_count", "=", "self", ".", "count", "[", ":", "self", ".", "nspecial", "]", "\n", "\n", "c", "=", "Counter", "(", "dict", "(", "sorted", "(", "zip", "(", "self", ".", "symbols", "[", "self", ".", "nspecial", ":", "]", ",", "self", ".", "count", "[", "self", ".", "nspecial", ":", "]", ")", ")", ")", ")", "\n", "for", "symbol", ",", "count", "in", "c", ".", "most_common", "(", "nwords", "-", "self", ".", "nspecial", ")", ":", "\n", "            ", "if", "count", ">=", "threshold", ":", "\n", "                ", "new_indices", "[", "symbol", "]", "=", "len", "(", "new_symbols", ")", "\n", "new_symbols", ".", "append", "(", "symbol", ")", "\n", "new_count", ".", "append", "(", "count", ")", "\n", "", "else", ":", "\n", "                ", "break", "\n", "\n", "", "", "threshold_nwords", "=", "len", "(", "new_symbols", ")", "\n", "if", "padding_factor", ">", "1", ":", "\n", "            ", "i", "=", "0", "\n", "while", "threshold_nwords", "%", "padding_factor", "!=", "0", ":", "\n", "                ", "symbol", "=", "'madeupword{:04d}'", ".", "format", "(", "i", ")", "\n", "new_indices", "[", "symbol", "]", "=", "len", "(", "new_symbols", ")", "\n", "new_symbols", ".", "append", "(", "symbol", ")", "\n", "new_count", ".", "append", "(", "0", ")", "\n", "i", "+=", "1", "\n", "threshold_nwords", "+=", "1", "\n", "\n", "", "", "assert", "len", "(", "new_symbols", ")", "%", "padding_factor", "==", "0", "\n", "assert", "len", "(", "new_symbols", ")", "==", "len", "(", "new_indices", ")", "\n", "\n", "self", ".", "count", "=", "list", "(", "new_count", ")", "\n", "self", ".", "symbols", "=", "list", "(", "new_symbols", ")", "\n", "self", ".", "indices", "=", "new_indices", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.bos": [[158, 161], ["None"], "methods", ["None"], ["", "def", "bos", "(", "self", ")", ":", "\n", "        ", "\"\"\"Helper to get index of beginning-of-sentence symbol\"\"\"", "\n", "return", "self", ".", "bos_index", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad": [[162, 165], ["None"], "methods", ["None"], ["", "def", "pad", "(", "self", ")", ":", "\n", "        ", "\"\"\"Helper to get index of pad symbol\"\"\"", "\n", "return", "self", ".", "pad_index", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos": [[166, 169], ["None"], "methods", ["None"], ["", "def", "eos", "(", "self", ")", ":", "\n", "        ", "\"\"\"Helper to get index of end-of-sentence symbol\"\"\"", "\n", "return", "self", ".", "eos_index", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.unk": [[170, 173], ["None"], "methods", ["None"], ["", "def", "unk", "(", "self", ")", ":", "\n", "        ", "\"\"\"Helper to get index of unk symbol\"\"\"", "\n", "return", "self", ".", "unk_index", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load": [[174, 187], ["cls", "cls.add_from_file"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.masked_lm_dictionary.BertDictionary.cls", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_from_file"], ["", "@", "classmethod", "\n", "def", "load", "(", "cls", ",", "f", ",", "ignore_utf_errors", "=", "False", ")", ":", "\n", "        ", "\"\"\"Loads the dictionary from a text file with the format:\n\n        ```\n        <symbol0> <count0>\n        <symbol1> <count1>\n        ...\n        ```\n        \"\"\"", "\n", "d", "=", "cls", "(", ")", "\n", "d", ".", "add_from_file", "(", "f", ",", "ignore_utf_errors", ")", "\n", "return", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_from_file": [[188, 219], ["isinstance", "f.readlines", "dictionary.Dictionary._load_meta", "line.rfind", "int", "len", "dictionary.Dictionary.symbols.append", "dictionary.Dictionary.count.append", "ValueError", "Exception", "open", "dictionary.Dictionary.add_from_file", "open", "dictionary.Dictionary.add_from_file"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.readlines", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary._load_meta", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_from_file", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_from_file"], ["", "def", "add_from_file", "(", "self", ",", "f", ",", "ignore_utf_errors", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Loads a pre-existing dictionary from a text file and adds its symbols\n        to this instance.\n        \"\"\"", "\n", "if", "isinstance", "(", "f", ",", "str", ")", ":", "\n", "            ", "try", ":", "\n", "                ", "if", "not", "ignore_utf_errors", ":", "\n", "                    ", "with", "open", "(", "f", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "fd", ":", "\n", "                        ", "self", ".", "add_from_file", "(", "fd", ")", "\n", "", "", "else", ":", "\n", "                    ", "with", "open", "(", "f", ",", "'r'", ",", "encoding", "=", "'utf-8'", ",", "errors", "=", "'ignore'", ")", "as", "fd", ":", "\n", "                        ", "self", ".", "add_from_file", "(", "fd", ")", "\n", "", "", "", "except", "FileNotFoundError", "as", "fnfe", ":", "\n", "                ", "raise", "fnfe", "\n", "", "except", "UnicodeError", ":", "\n", "                ", "raise", "Exception", "(", "\"Incorrect encoding detected in {}, please \"", "\n", "\"rebuild the dataset\"", ".", "format", "(", "f", ")", ")", "\n", "", "return", "\n", "\n", "", "lines", "=", "f", ".", "readlines", "(", ")", "\n", "indices_start_line", "=", "self", ".", "_load_meta", "(", "lines", ")", "\n", "for", "line", "in", "lines", "[", "indices_start_line", ":", "]", ":", "\n", "            ", "idx", "=", "line", ".", "rfind", "(", "' '", ")", "\n", "if", "idx", "==", "-", "1", ":", "\n", "                ", "raise", "ValueError", "(", "\"Incorrect dictionary format, expected '<token> <cnt>'\"", ")", "\n", "", "word", "=", "line", "[", ":", "idx", "]", "\n", "count", "=", "int", "(", "line", "[", "idx", "+", "1", ":", "]", ")", "\n", "self", ".", "indices", "[", "word", "]", "=", "len", "(", "self", ".", "symbols", ")", "\n", "self", ".", "symbols", ".", "append", "(", "word", ")", "\n", "self", ".", "count", ".", "append", "(", "count", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary._save": [[220, 227], ["isinstance", "os.makedirs", "print", "os.path.dirname", "open", "dictionary.Dictionary.save"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.save"], ["", "", "def", "_save", "(", "self", ",", "f", ",", "kv_iterator", ")", ":", "\n", "        ", "if", "isinstance", "(", "f", ",", "str", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "os", ".", "path", ".", "dirname", "(", "f", ")", ",", "exist_ok", "=", "True", ")", "\n", "with", "open", "(", "f", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "fd", ":", "\n", "                ", "return", "self", ".", "save", "(", "fd", ")", "\n", "", "", "for", "k", ",", "v", "in", "kv_iterator", ":", "\n", "            ", "print", "(", "'{} {}'", ".", "format", "(", "k", ",", "v", ")", ",", "file", "=", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary._get_meta": [[228, 230], ["None"], "methods", ["None"], ["", "", "def", "_get_meta", "(", "self", ")", ":", "\n", "        ", "return", "[", "]", ",", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary._load_meta": [[231, 233], ["None"], "methods", ["None"], ["", "def", "_load_meta", "(", "self", ",", "lines", ")", ":", "\n", "        ", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.save": [[234, 238], ["dictionary.Dictionary._get_meta", "dictionary.Dictionary._save", "zip"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary._get_meta", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary._save"], ["", "def", "save", "(", "self", ",", "f", ")", ":", "\n", "        ", "\"\"\"Stores dictionary into a text file\"\"\"", "\n", "ex_keys", ",", "ex_vals", "=", "self", ".", "_get_meta", "(", ")", "\n", "self", ".", "_save", "(", "f", ",", "zip", "(", "ex_keys", "+", "self", ".", "symbols", "[", "self", ".", "nspecial", ":", "]", ",", "ex_vals", "+", "self", ".", "count", "[", "self", ".", "nspecial", ":", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.dummy_sentence": [[239, 243], ["torch.Tensor().uniform_().long", "dictionary.Dictionary.eos", "torch.Tensor().uniform_", "len", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos"], ["", "def", "dummy_sentence", "(", "self", ",", "length", ")", ":", "\n", "        ", "t", "=", "torch", ".", "Tensor", "(", "length", ")", ".", "uniform_", "(", "self", ".", "nspecial", "+", "1", ",", "len", "(", "self", ")", ")", ".", "long", "(", ")", "\n", "t", "[", "-", "1", "]", "=", "self", ".", "eos", "(", ")", "\n", "return", "t", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.encode_line": [[244, 263], ["line_tokenizer", "len", "torch.IntTensor", "enumerate", "list", "reversed", "dictionary.Dictionary.add_symbol", "dictionary.Dictionary.index", "consumer"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.index"], ["", "def", "encode_line", "(", "self", ",", "line", ",", "line_tokenizer", "=", "tokenize_line", ",", "add_if_not_exist", "=", "True", ",", "\n", "consumer", "=", "None", ",", "append_eos", "=", "True", ",", "reverse_order", "=", "False", ")", ":", "\n", "        ", "words", "=", "line_tokenizer", "(", "line", ")", "\n", "if", "reverse_order", ":", "\n", "            ", "words", "=", "list", "(", "reversed", "(", "words", ")", ")", "\n", "", "nwords", "=", "len", "(", "words", ")", "\n", "ids", "=", "torch", ".", "IntTensor", "(", "nwords", "+", "1", "if", "append_eos", "else", "nwords", ")", "\n", "\n", "for", "i", ",", "word", "in", "enumerate", "(", "words", ")", ":", "\n", "            ", "if", "add_if_not_exist", ":", "\n", "                ", "idx", "=", "self", ".", "add_symbol", "(", "word", ")", "\n", "", "else", ":", "\n", "                ", "idx", "=", "self", ".", "index", "(", "word", ")", "\n", "", "if", "consumer", "is", "not", "None", ":", "\n", "                ", "consumer", "(", "word", ",", "idx", ")", "\n", "", "ids", "[", "i", "]", "=", "idx", "\n", "", "if", "append_eos", ":", "\n", "            ", "ids", "[", "nwords", "]", "=", "self", ".", "eos_index", "\n", "", "return", "ids", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary._add_file_to_dictionary_single_worker": [[264, 284], ["collections.Counter", "open", "f.seek", "f.readline", "os.fstat", "fairseq.binarizer.safe_readline", "tokenize", "collections.Counter.update", "f.readline", "f.fileno", "collections.Counter.update", "f.tell"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.token_block_dataset.DatasetSearcher.seek", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.binarizer.safe_readline", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.hub_utils.GeneratorHubInterface.tokenize", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update"], ["", "@", "staticmethod", "\n", "def", "_add_file_to_dictionary_single_worker", "(", "filename", ",", "tokenize", ",", "eos_word", ",", "worker_id", "=", "0", ",", "num_workers", "=", "1", ")", ":", "\n", "        ", "counter", "=", "Counter", "(", ")", "\n", "with", "open", "(", "filename", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "            ", "size", "=", "os", ".", "fstat", "(", "f", ".", "fileno", "(", ")", ")", ".", "st_size", "\n", "chunk_size", "=", "size", "//", "num_workers", "\n", "offset", "=", "worker_id", "*", "chunk_size", "\n", "end", "=", "offset", "+", "chunk_size", "\n", "f", ".", "seek", "(", "offset", ")", "\n", "if", "offset", ">", "0", ":", "\n", "                ", "safe_readline", "(", "f", ")", "# drop first incomplete line", "\n", "", "line", "=", "f", ".", "readline", "(", ")", "\n", "while", "line", ":", "\n", "                ", "for", "word", "in", "tokenize", "(", "line", ")", ":", "\n", "                    ", "counter", ".", "update", "(", "[", "word", "]", ")", "\n", "", "counter", ".", "update", "(", "[", "eos_word", "]", ")", "\n", "if", "f", ".", "tell", "(", ")", ">", "end", ":", "\n", "                    ", "break", "\n", "", "line", "=", "f", ".", "readline", "(", ")", "\n", "", "", "return", "counter", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_file_to_dictionary": [[285, 305], ["sorted", "multiprocessing.Pool", "range", "multiprocessing.Pool.close", "multiprocessing.Pool.join", "dictionary.Dictionary.add_file_to_dictionary.merge_result"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.GoldMiner.close"], ["", "@", "staticmethod", "\n", "def", "add_file_to_dictionary", "(", "filename", ",", "dict", ",", "tokenize", ",", "num_workers", ")", ":", "\n", "        ", "def", "merge_result", "(", "counter", ")", ":", "\n", "            ", "for", "w", ",", "c", "in", "sorted", "(", "counter", ".", "items", "(", ")", ")", ":", "\n", "                ", "dict", ".", "add_symbol", "(", "w", ",", "c", ")", "\n", "\n", "", "", "if", "num_workers", ">", "1", ":", "\n", "            ", "pool", "=", "Pool", "(", "processes", "=", "num_workers", ")", "\n", "results", "=", "[", "]", "\n", "for", "worker_id", "in", "range", "(", "num_workers", ")", ":", "\n", "                ", "results", ".", "append", "(", "pool", ".", "apply_async", "(", "\n", "Dictionary", ".", "_add_file_to_dictionary_single_worker", ",", "\n", "(", "filename", ",", "tokenize", ",", "dict", ".", "eos_word", ",", "worker_id", ",", "num_workers", ")", "\n", ")", ")", "\n", "", "pool", ".", "close", "(", ")", "\n", "pool", ".", "join", "(", ")", "\n", "for", "r", "in", "results", ":", "\n", "                ", "merge_result", "(", "r", ".", "get", "(", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "merge_result", "(", "Dictionary", ".", "_add_file_to_dictionary_single_worker", "(", "filename", ",", "tokenize", ",", "dict", ".", "eos_word", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.TruncatedDictionary.__init__": [[309, 318], ["type", "min", "len"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "wrapped_dict", ",", "length", ")", ":", "\n", "        ", "self", ".", "__class__", "=", "type", "(", "\n", "wrapped_dict", ".", "__class__", ".", "__name__", ",", "\n", "(", "self", ".", "__class__", ",", "wrapped_dict", ".", "__class__", ")", ",", "\n", "{", "}", "\n", ")", "\n", "self", ".", "__dict__", "=", "wrapped_dict", ".", "__dict__", "\n", "self", ".", "wrapped_dict", "=", "wrapped_dict", "\n", "self", ".", "length", "=", "min", "(", "len", "(", "self", ".", "wrapped_dict", ")", ",", "length", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.TruncatedDictionary.__len__": [[319, 321], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "length", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.TruncatedDictionary.__getitem__": [[322, 326], ["dictionary.TruncatedDictionary.wrapped_dict.unk"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.unk"], ["", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "if", "i", "<", "self", ".", "length", ":", "\n", "            ", "return", "self", ".", "wrapped_dict", "[", "i", "]", "\n", "", "return", "self", ".", "wrapped_dict", ".", "unk", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.prepend_token_dataset.PrependTokenDataset.__init__": [[14, 21], ["BaseWrapperDataset.__init__", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dataset", ",", "token", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dataset", ")", "\n", "self", ".", "token", "=", "token", "\n", "if", "token", "is", "not", "None", ":", "\n", "            ", "self", ".", "_sizes", "=", "np", ".", "array", "(", "dataset", ".", "sizes", ")", "+", "1", "\n", "", "else", ":", "\n", "            ", "self", ".", "_sizes", "=", "dataset", ".", "sizes", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.prepend_token_dataset.PrependTokenDataset.__getitem__": [[22, 27], ["torch.cat", "torch.cat.new"], "methods", ["None"], ["", "", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "item", "=", "self", ".", "dataset", "[", "idx", "]", "\n", "if", "self", ".", "token", "is", "not", "None", ":", "\n", "            ", "item", "=", "torch", ".", "cat", "(", "[", "item", ".", "new", "(", "[", "self", ".", "token", "]", ")", ",", "item", "]", ")", "\n", "", "return", "item", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.prepend_token_dataset.PrependTokenDataset.sizes": [[28, 31], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "sizes", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_sizes", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.prepend_token_dataset.PrependTokenDataset.num_tokens": [[32, 37], ["prepend_token_dataset.PrependTokenDataset.dataset.num_tokens"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.num_tokens"], ["", "def", "num_tokens", "(", "self", ",", "index", ")", ":", "\n", "        ", "n", "=", "self", ".", "dataset", ".", "num_tokens", "(", "index", ")", "\n", "if", "self", ".", "token", "is", "not", "None", ":", "\n", "            ", "n", "+=", "1", "\n", "", "return", "n", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.prepend_token_dataset.PrependTokenDataset.size": [[38, 43], ["prepend_token_dataset.PrependTokenDataset.dataset.size"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "size", "(", "self", ",", "index", ")", ":", "\n", "        ", "n", "=", "self", ".", "dataset", ".", "size", "(", "index", ")", "\n", "if", "self", ".", "token", "is", "not", "None", ":", "\n", "            ", "n", "+=", "1", "\n", "", "return", "n", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.transform_eos_dataset.TransformEosDataset.__init__": [[25, 60], ["torch.LongTensor", "isinstance", "ValueError", "ValueError", "ValueError"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "\n", "dataset", ",", "\n", "eos", ",", "\n", "append_eos_to_src", "=", "False", ",", "\n", "remove_eos_from_src", "=", "False", ",", "\n", "append_eos_to_tgt", "=", "False", ",", "\n", "remove_eos_from_tgt", "=", "False", ",", "\n", "has_target", "=", "True", ",", "\n", ")", ":", "\n", "        ", "if", "not", "isinstance", "(", "dataset", ",", "FairseqDataset", ")", ":", "\n", "            ", "raise", "ValueError", "(", "'dataset must be an instance of FairseqDataset'", ")", "\n", "", "if", "append_eos_to_src", "and", "remove_eos_from_src", ":", "\n", "            ", "raise", "ValueError", "(", "'cannot combine append_eos_to_src and remove_eos_from_src'", ")", "\n", "", "if", "append_eos_to_tgt", "and", "remove_eos_from_tgt", ":", "\n", "            ", "raise", "ValueError", "(", "'cannot combine append_eos_to_tgt and remove_eos_from_tgt'", ")", "\n", "\n", "", "self", ".", "dataset", "=", "dataset", "\n", "self", ".", "eos", "=", "torch", ".", "LongTensor", "(", "[", "eos", "]", ")", "\n", "self", ".", "append_eos_to_src", "=", "append_eos_to_src", "\n", "self", ".", "remove_eos_from_src", "=", "remove_eos_from_src", "\n", "self", ".", "append_eos_to_tgt", "=", "append_eos_to_tgt", "\n", "self", ".", "remove_eos_from_tgt", "=", "remove_eos_from_tgt", "\n", "self", ".", "has_target", "=", "has_target", "\n", "\n", "# precompute how we should adjust the reported sizes", "\n", "self", ".", "_src_delta", "=", "0", "\n", "self", ".", "_src_delta", "+=", "1", "if", "append_eos_to_src", "else", "0", "\n", "self", ".", "_src_delta", "-=", "1", "if", "remove_eos_from_src", "else", "0", "\n", "self", ".", "_tgt_delta", "=", "0", "\n", "self", ".", "_tgt_delta", "+=", "1", "if", "append_eos_to_tgt", "else", "0", "\n", "self", ".", "_tgt_delta", "-=", "1", "if", "remove_eos_from_tgt", "else", "0", "\n", "\n", "self", ".", "_checked_src", "=", "False", "\n", "self", ".", "_checked_tgt", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.transform_eos_dataset.TransformEosDataset._check_src": [[61, 65], ["None"], "methods", ["None"], ["", "def", "_check_src", "(", "self", ",", "src", ",", "expect_eos", ")", ":", "\n", "        ", "if", "not", "self", ".", "_checked_src", ":", "\n", "            ", "assert", "(", "src", "[", "-", "1", "]", "==", "self", ".", "eos", "[", "0", "]", ")", "==", "expect_eos", "\n", "self", ".", "_checked_src", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.transform_eos_dataset.TransformEosDataset._check_tgt": [[66, 70], ["None"], "methods", ["None"], ["", "", "def", "_check_tgt", "(", "self", ",", "tgt", ",", "expect_eos", ")", ":", "\n", "        ", "if", "self", ".", "has_target", "and", "not", "self", ".", "_checked_tgt", ":", "\n", "            ", "assert", "(", "tgt", "[", "-", "1", "]", "==", "self", ".", "eos", "[", "0", "]", ")", "==", "expect_eos", "\n", "self", ".", "_checked_tgt", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.transform_eos_dataset.TransformEosDataset.__getitem__": [[71, 73], ["None"], "methods", ["None"], ["", "", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "dataset", "[", "index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.transform_eos_dataset.TransformEosDataset.__len__": [[74, 76], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "dataset", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.transform_eos_dataset.TransformEosDataset.collater": [[77, 96], ["list", "transform_eos_dataset.TransformEosDataset.dataset.collater", "map", "transform_eos_dataset.TransformEosDataset._check_src", "torch.cat", "transform_eos_dataset.TransformEosDataset._check_src", "transform_eos_dataset.TransformEosDataset._check_tgt", "torch.cat", "transform_eos_dataset.TransformEosDataset._check_tgt"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.collater", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.transform_eos_dataset.TransformEosDataset._check_src", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.transform_eos_dataset.TransformEosDataset._check_src", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.transform_eos_dataset.TransformEosDataset._check_tgt", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.transform_eos_dataset.TransformEosDataset._check_tgt"], ["", "def", "collater", "(", "self", ",", "samples", ")", ":", "\n", "\n", "        ", "def", "transform", "(", "item", ")", ":", "\n", "            ", "if", "self", ".", "append_eos_to_src", ":", "\n", "                ", "self", ".", "_check_src", "(", "item", "[", "'source'", "]", ",", "expect_eos", "=", "False", ")", "\n", "item", "[", "'source'", "]", "=", "torch", ".", "cat", "(", "[", "item", "[", "'source'", "]", ",", "self", ".", "eos", "]", ")", "\n", "", "if", "self", ".", "remove_eos_from_src", ":", "\n", "                ", "self", ".", "_check_src", "(", "item", "[", "'source'", "]", ",", "expect_eos", "=", "True", ")", "\n", "item", "[", "'source'", "]", "=", "item", "[", "'source'", "]", "[", ":", "-", "1", "]", "\n", "", "if", "self", ".", "append_eos_to_tgt", ":", "\n", "                ", "self", ".", "_check_tgt", "(", "item", "[", "'target'", "]", ",", "expect_eos", "=", "False", ")", "\n", "item", "[", "'target'", "]", "=", "torch", ".", "cat", "(", "[", "item", "[", "'target'", "]", ",", "self", ".", "eos", "]", ")", "\n", "", "if", "self", ".", "remove_eos_from_tgt", ":", "\n", "                ", "self", ".", "_check_tgt", "(", "item", "[", "'target'", "]", ",", "expect_eos", "=", "True", ")", "\n", "item", "[", "'target'", "]", "=", "item", "[", "'target'", "]", "[", ":", "-", "1", "]", "\n", "", "return", "item", "\n", "\n", "", "samples", "=", "list", "(", "map", "(", "transform", ",", "samples", ")", ")", "\n", "return", "self", ".", "dataset", ".", "collater", "(", "samples", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.transform_eos_dataset.TransformEosDataset.num_tokens": [[97, 99], ["transform_eos_dataset.TransformEosDataset.dataset.num_tokens"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.num_tokens"], ["", "def", "num_tokens", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "dataset", ".", "num_tokens", "(", "index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.transform_eos_dataset.TransformEosDataset.size": [[100, 106], ["transform_eos_dataset.TransformEosDataset.dataset.size", "transform_eos_dataset.TransformEosDataset.dataset.size"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "size", "(", "self", ",", "index", ")", ":", "\n", "        ", "if", "self", ".", "has_target", ":", "\n", "            ", "src_len", ",", "tgt_len", "=", "self", ".", "dataset", ".", "size", "(", "index", ")", "\n", "return", "(", "src_len", "+", "self", ".", "_src_delta", ",", "tgt_len", "+", "self", ".", "_tgt_delta", ")", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "dataset", ".", "size", "(", "index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.transform_eos_dataset.TransformEosDataset.ordered_indices": [[107, 111], ["transform_eos_dataset.TransformEosDataset.dataset.ordered_indices"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.ordered_indices"], ["", "", "def", "ordered_indices", "(", "self", ")", ":", "\n", "# NOTE: we assume that the ordering does not change based on the", "\n", "# addition or removal of eos", "\n", "        ", "return", "self", ".", "dataset", ".", "ordered_indices", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.transform_eos_dataset.TransformEosDataset.supports_prefetch": [[112, 115], ["getattr"], "methods", ["None"], ["", "@", "property", "\n", "def", "supports_prefetch", "(", "self", ")", ":", "\n", "        ", "return", "getattr", "(", "self", ".", "dataset", ",", "'supports_prefetch'", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.transform_eos_dataset.TransformEosDataset.prefetch": [[116, 118], ["transform_eos_dataset.TransformEosDataset.dataset.prefetch"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.block_pair_dataset.BlockPairDataset.prefetch"], ["", "def", "prefetch", "(", "self", ",", "indices", ")", ":", "\n", "        ", "return", "self", ".", "dataset", ".", "prefetch", "(", "indices", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.noising.WordNoising.__init__": [[14, 32], ["numpy.array", "numpy.array", "noising.WordNoising.dictionary[].endswith", "range", "noising.WordNoising.dictionary[].endswith", "len", "range", "len"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "dictionary", ",", "bpe_cont_marker", "=", "\"@@\"", ",", "bpe_end_marker", "=", "None", ")", ":", "\n", "        ", "self", ".", "dictionary", "=", "dictionary", "\n", "self", ".", "bpe_end", "=", "None", "\n", "if", "bpe_cont_marker", ":", "\n", "            ", "self", ".", "bpe_end", "=", "np", ".", "array", "(", "[", "\n", "not", "self", ".", "dictionary", "[", "i", "]", ".", "endswith", "(", "bpe_cont_marker", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "dictionary", ")", ")", "\n", "]", ")", "\n", "", "elif", "bpe_end_marker", ":", "\n", "            ", "self", ".", "bpe_end", "=", "np", ".", "array", "(", "[", "\n", "self", ".", "dictionary", "[", "i", "]", ".", "endswith", "(", "bpe_end_marker", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "dictionary", ")", ")", "\n", "]", ")", "\n", "\n", "", "self", ".", "get_word_idx", "=", "(", "\n", "self", ".", "_get_bpe_word_idx", "\n", "if", "self", ".", "bpe_end", "is", "not", "None", "\n", "else", "self", ".", "_get_token_idx", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.noising.WordNoising.noising": [[34, 36], ["NotImplementedError"], "methods", ["None"], ["", "def", "noising", "(", "self", ",", "x", ",", "lengths", ",", "noising_prob", "=", "0.0", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.noising.WordNoising._get_bpe_word_idx": [[37, 57], ["numpy.array", "bpe_end[].cumsum", "x.size", "x.size", "word_idx.max"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.concat_dataset.ConcatDataset.cumsum", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "_get_bpe_word_idx", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"\n        Given a list of BPE tokens, for every index in the tokens list,\n        return the index of the word grouping that it belongs to.\n        For example, for input x corresponding to [\"how\", \"are\", \"y@@\", \"ou\"],\n        return [[0], [1], [2], [2]].\n        \"\"\"", "\n", "# x: (T x B)", "\n", "bpe_end", "=", "self", ".", "bpe_end", "[", "x", "]", "\n", "\n", "if", "(", "x", ".", "size", "(", "0", ")", "==", "1", "and", "x", ".", "size", "(", "1", ")", "==", "1", ")", ":", "\n", "# Special case when we only have one word in x. If x = [[N]],", "\n", "# bpe_end is a scalar (bool) instead of a 2-dim array of bools,", "\n", "# which makes the sum operation below fail.", "\n", "            ", "return", "np", ".", "array", "(", "[", "[", "0", "]", "]", ")", "\n", "\n", "# do a reduce front sum to generate word ids", "\n", "", "word_idx", "=", "bpe_end", "[", ":", ":", "-", "1", "]", ".", "cumsum", "(", "0", ")", "[", ":", ":", "-", "1", "]", "\n", "word_idx", "=", "word_idx", ".", "max", "(", "0", ")", "[", "None", ",", ":", "]", "-", "word_idx", "\n", "return", "word_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.noising.WordNoising._get_token_idx": [[58, 66], ["torch.t", "numpy.array", "numpy.transpose", "range", "len"], "methods", ["None"], ["", "def", "_get_token_idx", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"\n        This is to extend noising functions to be able to apply to non-bpe\n        tokens, e.g. word or characters.\n        \"\"\"", "\n", "x", "=", "torch", ".", "t", "(", "x", ")", "\n", "word_idx", "=", "np", ".", "array", "(", "[", "range", "(", "len", "(", "x_i", ")", ")", "for", "x_i", "in", "x", "]", ")", "\n", "return", "np", ".", "transpose", "(", "word_idx", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.noising.WordDropout.__init__": [[73, 76], ["noising.WordNoising.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["def", "__init__", "(", "self", ",", "dictionary", ",", "default_dropout_prob", "=", "0.1", ",", "bpe_cont_marker", "=", "\"@@\"", ",", "bpe_end_marker", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ",", "bpe_cont_marker", ",", "bpe_end_marker", ")", "\n", "self", ".", "default_dropout_prob", "=", "default_dropout_prob", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.noising.WordDropout.noising": [[77, 142], ["noising.WordDropout.get_word_idx", "range", "torch.LongTensor", "torch.LongTensor().fill_", "range", "lengths.size", "x[].tolist", "sentences.append", "torch.LongTensor.append", "noising.WordDropout.dictionary.pad", "torch.LongTensor.size", "modified_x[].copy_", "max", "noising.WordDropout.dictionary.eos", "numpy.append", "len", "new_s.insert", "len", "torch.LongTensor", "torch.LongTensor", "numpy.random.rand", "numpy.random.rand", "enumerate", "len", "torch.LongTensor.max", "torch.LongTensor.size", "numpy.random.randint", "len", "noising.WordDropout.dictionary.eos", "len"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos"], ["", "def", "noising", "(", "self", ",", "x", ",", "lengths", ",", "dropout_prob", "=", "None", ",", "blank_idx", "=", "None", ")", ":", "\n", "        ", "if", "dropout_prob", "is", "None", ":", "\n", "            ", "dropout_prob", "=", "self", ".", "default_dropout_prob", "\n", "# x: (T x B), lengths: B", "\n", "", "if", "dropout_prob", "==", "0", ":", "\n", "            ", "return", "x", ",", "lengths", "\n", "\n", "", "assert", "0", "<", "dropout_prob", "<", "1", "\n", "\n", "# be sure to drop entire words", "\n", "word_idx", "=", "self", ".", "get_word_idx", "(", "x", ")", "\n", "sentences", "=", "[", "]", "\n", "modified_lengths", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "lengths", ".", "size", "(", "0", ")", ")", ":", "\n", "# Since dropout probabilities need to apply over non-pad tokens,", "\n", "# it is not trivial to generate the keep mask without consider", "\n", "# input lengths; otherwise, this could be done outside the loop", "\n", "\n", "# We want to drop whole words based on word_idx grouping", "\n", "            ", "num_words", "=", "max", "(", "word_idx", "[", ":", ",", "i", "]", ")", "+", "1", "\n", "\n", "# ith example: [x0, x1, ..., eos, pad, ..., pad]", "\n", "# We should only generate keep probs for non-EOS tokens. Thus if the", "\n", "# input sentence ends in EOS, the last word idx is not included in", "\n", "# the dropout mask generation and we append True to always keep EOS.", "\n", "# Otherwise, just generate the dropout mask for all word idx", "\n", "# positions.", "\n", "has_eos", "=", "x", "[", "lengths", "[", "i", "]", "-", "1", ",", "i", "]", "==", "self", ".", "dictionary", ".", "eos", "(", ")", "\n", "if", "has_eos", ":", "# has eos?", "\n", "                ", "keep", "=", "np", ".", "random", ".", "rand", "(", "num_words", "-", "1", ")", ">=", "dropout_prob", "\n", "keep", "=", "np", ".", "append", "(", "keep", ",", "[", "True", "]", ")", "# keep EOS symbol", "\n", "", "else", ":", "\n", "                ", "keep", "=", "np", ".", "random", ".", "rand", "(", "num_words", ")", ">=", "dropout_prob", "\n", "\n", "", "words", "=", "x", "[", ":", "lengths", "[", "i", "]", ",", "i", "]", ".", "tolist", "(", ")", "\n", "\n", "# TODO: speed up the following loop", "\n", "# drop words from the input according to keep", "\n", "new_s", "=", "[", "\n", "w", "if", "keep", "[", "word_idx", "[", "j", ",", "i", "]", "]", "else", "blank_idx", "\n", "for", "j", ",", "w", "in", "enumerate", "(", "words", ")", "\n", "]", "\n", "new_s", "=", "[", "w", "for", "w", "in", "new_s", "if", "w", "is", "not", "None", "]", "\n", "# we need to have at least one word in the sentence (more than the", "\n", "# start / end sentence symbols)", "\n", "if", "len", "(", "new_s", ")", "<=", "1", ":", "\n", "# insert at beginning in case the only token left is EOS", "\n", "# EOS should be at end of list.", "\n", "                ", "new_s", ".", "insert", "(", "0", ",", "words", "[", "np", ".", "random", ".", "randint", "(", "0", ",", "len", "(", "words", ")", ")", "]", ")", "\n", "", "assert", "len", "(", "new_s", ")", ">=", "1", "and", "(", "\n", "not", "has_eos", "# Either don't have EOS at end or last token is EOS", "\n", "or", "(", "len", "(", "new_s", ")", ">=", "2", "and", "new_s", "[", "-", "1", "]", "==", "self", ".", "dictionary", ".", "eos", "(", ")", ")", "\n", ")", ",", "\"New sentence is invalid.\"", "\n", "sentences", ".", "append", "(", "new_s", ")", "\n", "modified_lengths", ".", "append", "(", "len", "(", "new_s", ")", ")", "\n", "# re-construct input", "\n", "", "modified_lengths", "=", "torch", ".", "LongTensor", "(", "modified_lengths", ")", "\n", "modified_x", "=", "torch", ".", "LongTensor", "(", "\n", "modified_lengths", ".", "max", "(", ")", ",", "\n", "modified_lengths", ".", "size", "(", "0", ")", "\n", ")", ".", "fill_", "(", "self", ".", "dictionary", ".", "pad", "(", ")", ")", "\n", "for", "i", "in", "range", "(", "modified_lengths", ".", "size", "(", "0", ")", ")", ":", "\n", "            ", "modified_x", "[", ":", "modified_lengths", "[", "i", "]", ",", "i", "]", ".", "copy_", "(", "torch", ".", "LongTensor", "(", "sentences", "[", "i", "]", ")", ")", "\n", "\n", "", "return", "modified_x", ",", "modified_lengths", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.noising.WordShuffle.__init__": [[147, 150], ["noising.WordNoising.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["def", "__init__", "(", "self", ",", "dictionary", ",", "default_max_shuffle_distance", "=", "3", ",", "bpe_cont_marker", "=", "\"@@\"", ",", "bpe_end_marker", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ",", "bpe_cont_marker", ",", "bpe_end_marker", ")", "\n", "self", ".", "default_max_shuffle_distance", "=", "3", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.noising.WordShuffle.noising": [[151, 185], ["numpy.random.uniform", "noising.WordShuffle.get_word_idx", "x.clone", "range", "lengths.size", "scores.argsort", "x2[].copy_", "noising.WordShuffle.dictionary.eos", "numpy.arange", "x.size", "x.size", "torch.from_numpy"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "noising", "(", "self", ",", "x", ",", "lengths", ",", "max_shuffle_distance", "=", "None", ")", ":", "\n", "        ", "if", "max_shuffle_distance", "is", "None", ":", "\n", "            ", "max_shuffle_distance", "=", "self", ".", "default_max_shuffle_distance", "\n", "# x: (T x B), lengths: B", "\n", "", "if", "max_shuffle_distance", "==", "0", ":", "\n", "            ", "return", "x", ",", "lengths", "\n", "\n", "# max_shuffle_distance < 1 will return the same sequence", "\n", "", "assert", "max_shuffle_distance", ">", "1", "\n", "\n", "# define noise word scores", "\n", "noise", "=", "np", ".", "random", ".", "uniform", "(", "\n", "0", ",", "\n", "max_shuffle_distance", ",", "\n", "size", "=", "(", "x", ".", "size", "(", "0", ")", ",", "x", ".", "size", "(", "1", ")", ")", ",", "\n", ")", "\n", "noise", "[", "0", "]", "=", "-", "1", "# do not move start sentence symbol", "\n", "# be sure to shuffle entire words", "\n", "word_idx", "=", "self", ".", "get_word_idx", "(", "x", ")", "\n", "x2", "=", "x", ".", "clone", "(", ")", "\n", "for", "i", "in", "range", "(", "lengths", ".", "size", "(", "0", ")", ")", ":", "\n", "            ", "length_no_eos", "=", "lengths", "[", "i", "]", "\n", "if", "x", "[", "lengths", "[", "i", "]", "-", "1", ",", "i", "]", "==", "self", ".", "dictionary", ".", "eos", "(", ")", ":", "\n", "                ", "length_no_eos", "=", "lengths", "[", "i", "]", "-", "1", "\n", "# generate a random permutation", "\n", "", "scores", "=", "word_idx", "[", ":", "length_no_eos", ",", "i", "]", "+", "noise", "[", "word_idx", "[", ":", "length_no_eos", ",", "i", "]", ",", "i", "]", "\n", "# ensure no reordering inside a word", "\n", "scores", "+=", "1e-6", "*", "np", ".", "arange", "(", "length_no_eos", ")", "\n", "permutation", "=", "scores", ".", "argsort", "(", ")", "\n", "# shuffle words", "\n", "x2", "[", ":", "length_no_eos", ",", "i", "]", ".", "copy_", "(", "\n", "x2", "[", ":", "length_no_eos", ",", "i", "]", "[", "torch", ".", "from_numpy", "(", "permutation", ")", "]", "\n", ")", "\n", "", "return", "x2", ",", "lengths", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.noising.UnsupervisedMTNoising.__init__": [[192, 215], ["noising.WordNoising.__init__", "noising.WordDropout", "noising.WordShuffle"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "dictionary", ",", "\n", "max_word_shuffle_distance", ",", "\n", "word_dropout_prob", ",", "\n", "word_blanking_prob", ",", "\n", "bpe_cont_marker", "=", "\"@@\"", ",", "\n", "bpe_end_marker", "=", "None", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ")", "\n", "self", ".", "max_word_shuffle_distance", "=", "max_word_shuffle_distance", "\n", "self", ".", "word_dropout_prob", "=", "word_dropout_prob", "\n", "self", ".", "word_blanking_prob", "=", "word_blanking_prob", "\n", "\n", "self", ".", "word_dropout", "=", "WordDropout", "(", "\n", "dictionary", "=", "dictionary", ",", "\n", "bpe_cont_marker", "=", "bpe_cont_marker", ",", "\n", "bpe_end_marker", "=", "bpe_end_marker", ",", "\n", ")", "\n", "self", ".", "word_shuffle", "=", "WordShuffle", "(", "\n", "dictionary", "=", "dictionary", ",", "\n", "bpe_cont_marker", "=", "bpe_cont_marker", ",", "\n", "bpe_end_marker", "=", "bpe_end_marker", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.noising.UnsupervisedMTNoising.noising": [[217, 239], ["noising.UnsupervisedMTNoising.word_shuffle.noising", "noising.UnsupervisedMTNoising.word_dropout.noising", "noising.UnsupervisedMTNoising.word_dropout.noising", "noising.UnsupervisedMTNoising.dictionary.unk"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.noising.UnsupervisedMTNoising.noising", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.noising.UnsupervisedMTNoising.noising", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.noising.UnsupervisedMTNoising.noising", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.unk"], ["", "def", "noising", "(", "self", ",", "x", ",", "lengths", ")", ":", "\n", "# 1. Word Shuffle", "\n", "        ", "noisy_src_tokens", ",", "noisy_src_lengths", "=", "self", ".", "word_shuffle", ".", "noising", "(", "\n", "x", "=", "x", ",", "\n", "lengths", "=", "lengths", ",", "\n", "max_shuffle_distance", "=", "self", ".", "max_word_shuffle_distance", ",", "\n", ")", "\n", "# 2. Word Dropout", "\n", "noisy_src_tokens", ",", "noisy_src_lengths", "=", "self", ".", "word_dropout", ".", "noising", "(", "\n", "x", "=", "noisy_src_tokens", ",", "\n", "lengths", "=", "noisy_src_lengths", ",", "\n", "dropout_prob", "=", "self", ".", "word_dropout_prob", ",", "\n", ")", "\n", "# 3. Word Blanking", "\n", "noisy_src_tokens", ",", "noisy_src_lengths", "=", "self", ".", "word_dropout", ".", "noising", "(", "\n", "x", "=", "noisy_src_tokens", ",", "\n", "lengths", "=", "noisy_src_lengths", ",", "\n", "dropout_prob", "=", "self", ".", "word_blanking_prob", ",", "\n", "blank_idx", "=", "self", ".", "dictionary", ".", "unk", "(", ")", ",", "\n", ")", "\n", "\n", "return", "noisy_src_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.noising.NoisingDataset.__init__": [[242, 280], ["noising_class"], "methods", ["None"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "src_dataset", ",", "\n", "src_dict", ",", "\n", "seed", ",", "\n", "noiser", "=", "None", ",", "\n", "noising_class", "=", "UnsupervisedMTNoising", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Wrap a :class:`~torch.utils.data.Dataset` and apply noise to the\n        samples based on the supplied noising configuration.\n\n        Args:\n            src_dataset (~torch.utils.data.Dataset): dataset to wrap.\n                to build self.src_dataset --\n                a LanguagePairDataset with src dataset as the source dataset and\n                None as the target dataset. Should NOT have padding so that\n                src_lengths are accurately calculated by language_pair_dataset\n                collate function.\n                We use language_pair_dataset here to encapsulate the tgt_dataset\n                so we can re-use the LanguagePairDataset collater to format the\n                batches in the structure that SequenceGenerator expects.\n            src_dict (~fairseq.data.Dictionary): source dictionary\n            seed (int): seed to use when generating random noise\n            noiser (WordNoising): a pre-initialized :class:`WordNoising`\n                instance. If this is None, a new instance will be created using\n                *noising_class* and *kwargs*.\n            noising_class (class, optional): class to use to initialize a\n                default :class:`WordNoising` instance.\n            kwargs (dict, optional): arguments to initialize the default\n                :class:`WordNoising` instance given by *noiser*.\n        \"\"\"", "\n", "self", ".", "src_dataset", "=", "src_dataset", "\n", "self", ".", "src_dict", "=", "src_dict", "\n", "self", ".", "seed", "=", "seed", "\n", "self", ".", "noiser", "=", "noiser", "if", "noiser", "is", "not", "None", "else", "noising_class", "(", "\n", "dictionary", "=", "src_dict", ",", "**", "kwargs", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.noising.NoisingDataset.__getitem__": [[282, 302], ["torch.LongTensor", "src_tokens.unsqueeze.unsqueeze.unsqueeze", "torch.t", "torch.t", "fairseq.data.data_utils.numpy_seed", "noising.NoisingDataset.noiser.noising", "len"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.data_utils.numpy_seed", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.noising.UnsupervisedMTNoising.noising"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "\"\"\"\n        Returns a single noisy sample. Multiple samples are fed to the collater\n        create a noising dataset batch.\n        \"\"\"", "\n", "src_tokens", "=", "self", ".", "src_dataset", "[", "index", "]", "\n", "src_lengths", "=", "torch", ".", "LongTensor", "(", "[", "len", "(", "src_tokens", ")", "]", ")", "\n", "src_tokens", "=", "src_tokens", ".", "unsqueeze", "(", "0", ")", "\n", "\n", "# Transpose src tokens to fit expected shape of x in noising function", "\n", "# (batch size, sequence length) -> (sequence length, batch size)", "\n", "src_tokens_t", "=", "torch", ".", "t", "(", "src_tokens", ")", "\n", "\n", "with", "data_utils", ".", "numpy_seed", "(", "self", ".", "seed", "+", "index", ")", ":", "\n", "            ", "noisy_src_tokens", "=", "self", ".", "noiser", ".", "noising", "(", "src_tokens_t", ",", "src_lengths", ")", "\n", "\n", "# Transpose back to expected src_tokens format", "\n", "# (sequence length, 1) -> (1, sequence length)", "\n", "", "noisy_src_tokens", "=", "torch", ".", "t", "(", "noisy_src_tokens", ")", "\n", "return", "noisy_src_tokens", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.noising.NoisingDataset.__len__": [[303, 308], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        The length of the noising dataset is the length of src.\n        \"\"\"", "\n", "return", "len", "(", "self", ".", "src_dataset", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.noising.NoisingDataset.supports_prefetch": [[309, 312], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "supports_prefetch", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "src_dataset", ".", "supports_prefetch", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.noising.NoisingDataset.prefetch": [[313, 316], ["noising.NoisingDataset.src_dataset.prefetch"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.block_pair_dataset.BlockPairDataset.prefetch"], ["", "def", "prefetch", "(", "self", ",", "indices", ")", ":", "\n", "        ", "if", "self", ".", "src_dataset", ".", "supports_prefetch", ":", "\n", "            ", "self", ".", "src_dataset", ".", "prefetch", "(", "indices", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.IndexedDataset.__init__": [[116, 122], ["FairseqDataset.__init__", "indexed_dataset.IndexedDataset.read_index"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.IndexedDataset.read_index"], ["def", "__init__", "(", "self", ",", "path", ",", "fix_lua_indexing", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "path", "=", "path", "\n", "self", ".", "fix_lua_indexing", "=", "fix_lua_indexing", "\n", "self", ".", "data_file", "=", "None", "\n", "self", ".", "read_index", "(", "path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.IndexedDataset.read_index": [[123, 138], ["open", "f.read", "f.read", "struct.unpack", "struct.unpack", "indexed_dataset.read_longs", "indexed_dataset.read_longs", "indexed_dataset.read_longs", "indexed_dataset.index_file_path", "struct.unpack", "f.read", "f.read"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.read_longs", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.read_longs", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.read_longs", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.index_file_path"], ["", "def", "read_index", "(", "self", ",", "path", ")", ":", "\n", "        ", "with", "open", "(", "index_file_path", "(", "path", ")", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "magic", "=", "f", ".", "read", "(", "8", ")", "\n", "assert", "magic", "==", "self", ".", "_HDR_MAGIC", ",", "(", "\n", "'Index file doesn\\'t match expected format. '", "\n", "'Make sure that --dataset-impl is configured properly.'", "\n", ")", "\n", "version", "=", "f", ".", "read", "(", "8", ")", "\n", "assert", "struct", ".", "unpack", "(", "'<Q'", ",", "version", ")", "==", "(", "1", ",", ")", "\n", "code", ",", "self", ".", "element_size", "=", "struct", ".", "unpack", "(", "'<QQ'", ",", "f", ".", "read", "(", "16", ")", ")", "\n", "self", ".", "dtype", "=", "dtypes", "[", "code", "]", "\n", "self", ".", "_len", ",", "self", ".", "s", "=", "struct", ".", "unpack", "(", "'<QQ'", ",", "f", ".", "read", "(", "16", ")", ")", "\n", "self", ".", "dim_offsets", "=", "read_longs", "(", "f", ",", "self", ".", "_len", "+", "1", ")", "\n", "self", ".", "data_offsets", "=", "read_longs", "(", "f", ",", "self", ".", "_len", "+", "1", ")", "\n", "self", ".", "sizes", "=", "read_longs", "(", "f", ",", "self", ".", "s", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.IndexedDataset.read_data": [[139, 141], ["open", "indexed_dataset.data_file_path"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.data_file_path"], ["", "", "def", "read_data", "(", "self", ",", "path", ")", ":", "\n", "        ", "self", ".", "data_file", "=", "open", "(", "data_file_path", "(", "path", ")", ",", "'rb'", ",", "buffering", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.IndexedDataset.check_index": [[142, 145], ["IndexError"], "methods", ["None"], ["", "def", "check_index", "(", "self", ",", "i", ")", ":", "\n", "        ", "if", "i", "<", "0", "or", "i", ">=", "self", ".", "_len", ":", "\n", "            ", "raise", "IndexError", "(", "'index out of range'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.IndexedDataset.__del__": [[146, 149], ["indexed_dataset.IndexedDataset.data_file.close"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.GoldMiner.close"], ["", "", "def", "__del__", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "data_file", ":", "\n", "            ", "self", ".", "data_file", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.IndexedDataset.__getitem__": [[150, 163], ["functools.lru_cache", "indexed_dataset.IndexedDataset.check_index", "numpy.empty", "indexed_dataset.IndexedDataset.data_file.seek", "indexed_dataset.IndexedDataset.data_file.readinto", "torch.from_numpy().long", "indexed_dataset.IndexedDataset.read_data", "torch.from_numpy"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.IndexedRawTextDataset.check_index", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.token_block_dataset.DatasetSearcher.seek", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.IndexedRawTextDataset.read_data"], ["", "", "@", "lru_cache", "(", "maxsize", "=", "8", ")", "\n", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "if", "not", "self", ".", "data_file", ":", "\n", "            ", "self", ".", "read_data", "(", "self", ".", "path", ")", "\n", "", "self", ".", "check_index", "(", "i", ")", "\n", "tensor_size", "=", "self", ".", "sizes", "[", "self", ".", "dim_offsets", "[", "i", "]", ":", "self", ".", "dim_offsets", "[", "i", "+", "1", "]", "]", "\n", "a", "=", "np", ".", "empty", "(", "tensor_size", ",", "dtype", "=", "self", ".", "dtype", ")", "\n", "self", ".", "data_file", ".", "seek", "(", "self", ".", "data_offsets", "[", "i", "]", "*", "self", ".", "element_size", ")", "\n", "self", ".", "data_file", ".", "readinto", "(", "a", ")", "\n", "item", "=", "torch", ".", "from_numpy", "(", "a", ")", ".", "long", "(", ")", "\n", "if", "self", ".", "fix_lua_indexing", ":", "\n", "            ", "item", "-=", "1", "# subtract 1 for 0-based indexing", "\n", "", "return", "item", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.IndexedDataset.__len__": [[164, 166], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_len", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.IndexedDataset.num_tokens": [[167, 169], ["None"], "methods", ["None"], ["", "def", "num_tokens", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "sizes", "[", "index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.IndexedDataset.size": [[170, 172], ["None"], "methods", ["None"], ["", "def", "size", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "sizes", "[", "index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.IndexedDataset.exists": [[173, 177], ["os.path.exists", "os.path.exists", "indexed_dataset.index_file_path", "indexed_dataset.data_file_path"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.index_file_path", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.data_file_path"], ["", "@", "staticmethod", "\n", "def", "exists", "(", "path", ")", ":", "\n", "        ", "return", "(", "\n", "os", ".", "path", ".", "exists", "(", "index_file_path", "(", "path", ")", ")", "and", "os", ".", "path", ".", "exists", "(", "data_file_path", "(", "path", ")", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.IndexedDataset.supports_prefetch": [[179, 182], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "supports_prefetch", "(", "self", ")", ":", "\n", "        ", "return", "False", "# avoid prefetching to save memory", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.IndexedCachedDataset.__init__": [[186, 190], ["indexed_dataset.IndexedDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["    ", "def", "__init__", "(", "self", ",", "path", ",", "fix_lua_indexing", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "path", ",", "fix_lua_indexing", "=", "fix_lua_indexing", ")", "\n", "self", ".", "cache", "=", "None", "\n", "self", ".", "cache_index", "=", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.IndexedCachedDataset.supports_prefetch": [[191, 194], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "supports_prefetch", "(", "self", ")", ":", "\n", "        ", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.IndexedCachedDataset.prefetch": [[195, 218], ["all", "sorted", "numpy.empty", "indexed_dataset.IndexedCachedDataset.cache_index.clear", "indexed_dataset.IndexedCachedDataset.read_data", "set", "indexed_dataset.IndexedCachedDataset.data_file.seek", "indexed_dataset.IndexedCachedDataset.data_file.readinto", "indexed_dataset.IndexedCachedDataset.data_file.close"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.clear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.IndexedRawTextDataset.read_data", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.token_block_dataset.DatasetSearcher.seek", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.GoldMiner.close"], ["", "def", "prefetch", "(", "self", ",", "indices", ")", ":", "\n", "        ", "if", "all", "(", "i", "in", "self", ".", "cache_index", "for", "i", "in", "indices", ")", ":", "\n", "            ", "return", "\n", "", "if", "not", "self", ".", "data_file", ":", "\n", "            ", "self", ".", "read_data", "(", "self", ".", "path", ")", "\n", "", "indices", "=", "sorted", "(", "set", "(", "indices", ")", ")", "\n", "total_size", "=", "0", "\n", "for", "i", "in", "indices", ":", "\n", "            ", "total_size", "+=", "self", ".", "data_offsets", "[", "i", "+", "1", "]", "-", "self", ".", "data_offsets", "[", "i", "]", "\n", "", "self", ".", "cache", "=", "np", ".", "empty", "(", "total_size", ",", "dtype", "=", "self", ".", "dtype", ")", "\n", "ptx", "=", "0", "\n", "self", ".", "cache_index", ".", "clear", "(", ")", "\n", "for", "i", "in", "indices", ":", "\n", "            ", "self", ".", "cache_index", "[", "i", "]", "=", "ptx", "\n", "size", "=", "self", ".", "data_offsets", "[", "i", "+", "1", "]", "-", "self", ".", "data_offsets", "[", "i", "]", "\n", "a", "=", "self", ".", "cache", "[", "ptx", ":", "ptx", "+", "size", "]", "\n", "self", ".", "data_file", ".", "seek", "(", "self", ".", "data_offsets", "[", "i", "]", "*", "self", ".", "element_size", ")", "\n", "self", ".", "data_file", ".", "readinto", "(", "a", ")", "\n", "ptx", "+=", "size", "\n", "", "if", "self", ".", "data_file", ":", "\n", "# close and delete data file after prefetch so we can pickle", "\n", "            ", "self", ".", "data_file", ".", "close", "(", ")", "\n", "self", ".", "data_file", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.IndexedCachedDataset.__getitem__": [[219, 230], ["functools.lru_cache", "indexed_dataset.IndexedCachedDataset.check_index", "numpy.empty", "numpy.copyto", "torch.from_numpy().long", "torch.from_numpy"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.IndexedRawTextDataset.check_index"], ["", "", "@", "lru_cache", "(", "maxsize", "=", "8", ")", "\n", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "self", ".", "check_index", "(", "i", ")", "\n", "tensor_size", "=", "self", ".", "sizes", "[", "self", ".", "dim_offsets", "[", "i", "]", ":", "self", ".", "dim_offsets", "[", "i", "+", "1", "]", "]", "\n", "a", "=", "np", ".", "empty", "(", "tensor_size", ",", "dtype", "=", "self", ".", "dtype", ")", "\n", "ptx", "=", "self", ".", "cache_index", "[", "i", "]", "\n", "np", ".", "copyto", "(", "a", ",", "self", ".", "cache", "[", "ptx", ":", "ptx", "+", "a", ".", "size", "]", ")", "\n", "item", "=", "torch", ".", "from_numpy", "(", "a", ")", ".", "long", "(", ")", "\n", "if", "self", ".", "fix_lua_indexing", ":", "\n", "            ", "item", "-=", "1", "# subtract 1 for 0-based indexing", "\n", "", "return", "item", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.IndexedRawTextDataset.__init__": [[236, 244], ["indexed_dataset.IndexedRawTextDataset.read_data", "len"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.IndexedRawTextDataset.read_data"], ["def", "__init__", "(", "self", ",", "path", ",", "dictionary", ",", "append_eos", "=", "True", ",", "reverse_order", "=", "False", ")", ":", "\n", "        ", "self", ".", "tokens_list", "=", "[", "]", "\n", "self", ".", "lines", "=", "[", "]", "\n", "self", ".", "sizes", "=", "[", "]", "\n", "self", ".", "append_eos", "=", "append_eos", "\n", "self", ".", "reverse_order", "=", "reverse_order", "\n", "self", ".", "read_data", "(", "path", ",", "dictionary", ")", "\n", "self", ".", "size", "=", "len", "(", "self", ".", "tokens_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.IndexedRawTextDataset.read_data": [[245, 256], ["numpy.array", "open", "indexed_dataset.IndexedRawTextDataset.lines.append", "dictionary.encode_line().long", "indexed_dataset.IndexedRawTextDataset.tokens_list.append", "indexed_dataset.IndexedRawTextDataset.sizes.append", "line.strip", "len", "dictionary.encode_line"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.encode_line"], ["", "def", "read_data", "(", "self", ",", "path", ",", "dictionary", ")", ":", "\n", "        ", "with", "open", "(", "path", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "            ", "for", "line", "in", "f", ":", "\n", "                ", "self", ".", "lines", ".", "append", "(", "line", ".", "strip", "(", "'\\n'", ")", ")", "\n", "tokens", "=", "dictionary", ".", "encode_line", "(", "\n", "line", ",", "add_if_not_exist", "=", "False", ",", "\n", "append_eos", "=", "self", ".", "append_eos", ",", "reverse_order", "=", "self", ".", "reverse_order", ",", "\n", ")", ".", "long", "(", ")", "\n", "self", ".", "tokens_list", ".", "append", "(", "tokens", ")", "\n", "self", ".", "sizes", ".", "append", "(", "len", "(", "tokens", ")", ")", "\n", "", "", "self", ".", "sizes", "=", "np", ".", "array", "(", "self", ".", "sizes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.IndexedRawTextDataset.check_index": [[257, 260], ["IndexError"], "methods", ["None"], ["", "def", "check_index", "(", "self", ",", "i", ")", ":", "\n", "        ", "if", "i", "<", "0", "or", "i", ">=", "self", ".", "size", ":", "\n", "            ", "raise", "IndexError", "(", "'index out of range'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.IndexedRawTextDataset.__getitem__": [[261, 265], ["functools.lru_cache", "indexed_dataset.IndexedRawTextDataset.check_index"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.IndexedRawTextDataset.check_index"], ["", "", "@", "lru_cache", "(", "maxsize", "=", "8", ")", "\n", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "self", ".", "check_index", "(", "i", ")", "\n", "return", "self", ".", "tokens_list", "[", "i", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.IndexedRawTextDataset.get_original_text": [[266, 269], ["indexed_dataset.IndexedRawTextDataset.check_index"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.IndexedRawTextDataset.check_index"], ["", "def", "get_original_text", "(", "self", ",", "i", ")", ":", "\n", "        ", "self", ".", "check_index", "(", "i", ")", "\n", "return", "self", ".", "lines", "[", "i", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.IndexedRawTextDataset.__del__": [[270, 272], ["None"], "methods", ["None"], ["", "def", "__del__", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.IndexedRawTextDataset.__len__": [[273, 275], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "size", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.IndexedRawTextDataset.num_tokens": [[276, 278], ["None"], "methods", ["None"], ["", "def", "num_tokens", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "sizes", "[", "index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.IndexedRawTextDataset.size": [[279, 281], ["None"], "methods", ["None"], ["", "def", "size", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "sizes", "[", "index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.IndexedRawTextDataset.exists": [[282, 285], ["os.path.exists"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists"], ["", "@", "staticmethod", "\n", "def", "exists", "(", "path", ")", ":", "\n", "        ", "return", "os", ".", "path", ".", "exists", "(", "path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.IndexedDatasetBuilder.__init__": [[298, 305], ["open"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "out_file", ",", "dtype", "=", "np", ".", "int32", ")", ":", "\n", "        ", "self", ".", "out_file", "=", "open", "(", "out_file", ",", "'wb'", ")", "\n", "self", ".", "dtype", "=", "dtype", "\n", "self", ".", "data_offsets", "=", "[", "0", "]", "\n", "self", ".", "dim_offsets", "=", "[", "0", "]", "\n", "self", ".", "sizes", "=", "[", "]", "\n", "self", ".", "element_size", "=", "self", ".", "element_sizes", "[", "self", ".", "dtype", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.IndexedDatasetBuilder.add_item": [[306, 313], ["indexed_dataset.IndexedDatasetBuilder.out_file.write", "indexed_dataset.IndexedDatasetBuilder.data_offsets.append", "tensor.size", "indexed_dataset.IndexedDatasetBuilder.dim_offsets.append", "numpy.array", "indexed_dataset.IndexedDatasetBuilder.sizes.append", "len", "tensor.numpy", "tensor.size"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "add_item", "(", "self", ",", "tensor", ")", ":", "\n", "# +1 for Lua compatibility", "\n", "        ", "bytes", "=", "self", ".", "out_file", ".", "write", "(", "np", ".", "array", "(", "tensor", ".", "numpy", "(", ")", "+", "1", ",", "dtype", "=", "self", ".", "dtype", ")", ")", "\n", "self", ".", "data_offsets", ".", "append", "(", "self", ".", "data_offsets", "[", "-", "1", "]", "+", "bytes", "/", "self", ".", "element_size", ")", "\n", "for", "s", "in", "tensor", ".", "size", "(", ")", ":", "\n", "            ", "self", ".", "sizes", ".", "append", "(", "s", ")", "\n", "", "self", ".", "dim_offsets", ".", "append", "(", "self", ".", "dim_offsets", "[", "-", "1", "]", "+", "len", "(", "tensor", ".", "size", "(", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.IndexedDatasetBuilder.merge_file_": [[314, 333], ["indexed_dataset.IndexedDataset", "indexed_dataset.IndexedDatasetBuilder.sizes.extend", "indexed_dataset.IndexedDatasetBuilder.data_offsets.append", "indexed_dataset.IndexedDatasetBuilder.dim_offsets.append", "open", "indexed_dataset.data_file_path", "f.read", "indexed_dataset.IndexedDatasetBuilder.out_file.write"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.data_file_path", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write"], ["", "def", "merge_file_", "(", "self", ",", "another_file", ")", ":", "\n", "        ", "index", "=", "IndexedDataset", "(", "another_file", ")", "\n", "assert", "index", ".", "dtype", "==", "self", ".", "dtype", "\n", "\n", "begin", "=", "self", ".", "data_offsets", "[", "-", "1", "]", "\n", "for", "offset", "in", "index", ".", "data_offsets", "[", "1", ":", "]", ":", "\n", "            ", "self", ".", "data_offsets", ".", "append", "(", "begin", "+", "offset", ")", "\n", "", "self", ".", "sizes", ".", "extend", "(", "index", ".", "sizes", ")", "\n", "begin", "=", "self", ".", "dim_offsets", "[", "-", "1", "]", "\n", "for", "dim_offset", "in", "index", ".", "dim_offsets", "[", "1", ":", "]", ":", "\n", "            ", "self", ".", "dim_offsets", ".", "append", "(", "begin", "+", "dim_offset", ")", "\n", "\n", "", "with", "open", "(", "data_file_path", "(", "another_file", ")", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "while", "True", ":", "\n", "                ", "data", "=", "f", ".", "read", "(", "1024", ")", "\n", "if", "data", ":", "\n", "                    ", "self", ".", "out_file", ".", "write", "(", "data", ")", "\n", "", "else", ":", "\n", "                    ", "break", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.IndexedDatasetBuilder.finalize": [[334, 345], ["indexed_dataset.IndexedDatasetBuilder.out_file.close", "open", "open.write", "open.write", "open.write", "open.write", "indexed_dataset.write_longs", "indexed_dataset.write_longs", "indexed_dataset.write_longs", "open.close", "struct.pack", "struct.pack", "struct.pack", "indexed_dataset.code", "len", "len"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.GoldMiner.close", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.write_longs", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.write_longs", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.write_longs", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.GoldMiner.close", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.code"], ["", "", "", "", "def", "finalize", "(", "self", ",", "index_file", ")", ":", "\n", "        ", "self", ".", "out_file", ".", "close", "(", ")", "\n", "index", "=", "open", "(", "index_file", ",", "'wb'", ")", "\n", "index", ".", "write", "(", "b'TNTIDX\\x00\\x00'", ")", "\n", "index", ".", "write", "(", "struct", ".", "pack", "(", "'<Q'", ",", "1", ")", ")", "\n", "index", ".", "write", "(", "struct", ".", "pack", "(", "'<QQ'", ",", "code", "(", "self", ".", "dtype", ")", ",", "self", ".", "element_size", ")", ")", "\n", "index", ".", "write", "(", "struct", ".", "pack", "(", "'<QQ'", ",", "len", "(", "self", ".", "data_offsets", ")", "-", "1", ",", "len", "(", "self", ".", "sizes", ")", ")", ")", "\n", "write_longs", "(", "index", ",", "self", ".", "dim_offsets", ")", "\n", "write_longs", "(", "index", ",", "self", ".", "data_offsets", ")", "\n", "write_longs", "(", "index", ",", "self", ".", "sizes", ")", "\n", "index", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.__init__": [[443, 451], ["super().__init__", "indexed_dataset.MMapIndexedDataset._do_init"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset._do_init"], ["", "", "def", "__init__", "(", "self", ",", "path", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "_path", "=", "None", "\n", "self", ".", "_index", "=", "None", "\n", "self", ".", "_bin_buffer", "=", "None", "\n", "\n", "self", ".", "_do_init", "(", "path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.__getstate__": [[452, 454], ["None"], "methods", ["None"], ["", "def", "__getstate__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_path", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.__setstate__": [[455, 457], ["indexed_dataset.MMapIndexedDataset._do_init"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset._do_init"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "self", ".", "_do_init", "(", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset._do_init": [[458, 465], ["indexed_dataset.MMapIndexedDataset.Index", "indexed_dataset._warmup_mmap_file", "numpy.memmap", "memoryview", "indexed_dataset.index_file_path", "indexed_dataset.data_file_path", "indexed_dataset.data_file_path"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset._warmup_mmap_file", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.index_file_path", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.data_file_path", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.data_file_path"], ["", "def", "_do_init", "(", "self", ",", "path", ")", ":", "\n", "        ", "self", ".", "_path", "=", "path", "\n", "self", ".", "_index", "=", "self", ".", "Index", "(", "index_file_path", "(", "self", ".", "_path", ")", ")", "\n", "\n", "_warmup_mmap_file", "(", "data_file_path", "(", "self", ".", "_path", ")", ")", "\n", "self", ".", "_bin_buffer_mmap", "=", "np", ".", "memmap", "(", "data_file_path", "(", "self", ".", "_path", ")", ",", "mode", "=", "'r'", ",", "order", "=", "'C'", ")", "\n", "self", ".", "_bin_buffer", "=", "memoryview", "(", "self", ".", "_bin_buffer_mmap", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.__del__": [[466, 470], ["indexed_dataset.MMapIndexedDataset._bin_buffer_mmap._mmap.close"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.GoldMiner.close"], ["", "def", "__del__", "(", "self", ")", ":", "\n", "        ", "self", ".", "_bin_buffer_mmap", ".", "_mmap", ".", "close", "(", ")", "\n", "del", "self", ".", "_bin_buffer_mmap", "\n", "del", "self", ".", "_index", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.__len__": [[471, 473], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "_index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.__getitem__": [[474, 483], ["functools.lru_cache", "numpy.frombuffer", "torch.from_numpy", "np_array.astype.astype.astype"], "methods", ["None"], ["", "@", "lru_cache", "(", "maxsize", "=", "8", ")", "\n", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "ptr", ",", "size", "=", "self", ".", "_index", "[", "i", "]", "\n", "np_array", "=", "np", ".", "frombuffer", "(", "self", ".", "_bin_buffer", ",", "dtype", "=", "self", ".", "_index", ".", "dtype", ",", "count", "=", "size", ",", "offset", "=", "ptr", ")", "\n", "# FIXME: This is barely improves over previous hack", "\n", "if", "self", ".", "_index", ".", "dtype", "!=", "np", ".", "float32", ":", "\n", "            ", "np_array", "=", "np_array", ".", "astype", "(", "np", ".", "int64", ")", "\n", "\n", "", "return", "torch", ".", "from_numpy", "(", "np_array", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.sizes": [[484, 487], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "sizes", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_index", ".", "sizes", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.supports_prefetch": [[488, 491], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "supports_prefetch", "(", "self", ")", ":", "\n", "        ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists": [[492, 496], ["os.path.exists", "os.path.exists", "indexed_dataset.index_file_path", "indexed_dataset.data_file_path"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.index_file_path", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.data_file_path"], ["", "@", "staticmethod", "\n", "def", "exists", "(", "path", ")", ":", "\n", "        ", "return", "(", "\n", "os", ".", "path", ".", "exists", "(", "index_file_path", "(", "path", ")", ")", "and", "os", ".", "path", ".", "exists", "(", "data_file_path", "(", "path", ")", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDatasetBuilder.__init__": [[500, 504], ["open"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "out_file", ",", "dtype", "=", "np", ".", "int64", ")", ":", "\n", "        ", "self", ".", "_data_file", "=", "open", "(", "out_file", ",", "'wb'", ")", "\n", "self", ".", "_dtype", "=", "dtype", "\n", "self", ".", "_sizes", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDatasetBuilder.add_item": [[505, 509], ["numpy.array", "indexed_dataset.MMapIndexedDatasetBuilder._data_file.write", "indexed_dataset.MMapIndexedDatasetBuilder._sizes.append", "tensor.numpy", "numpy.array.tobytes"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "add_item", "(", "self", ",", "tensor", ")", ":", "\n", "        ", "np_array", "=", "np", ".", "array", "(", "tensor", ".", "numpy", "(", ")", ",", "dtype", "=", "self", ".", "_dtype", ")", "\n", "self", ".", "_data_file", ".", "write", "(", "np_array", ".", "tobytes", "(", "order", "=", "'C'", ")", ")", "\n", "self", ".", "_sizes", ".", "append", "(", "np_array", ".", "size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDatasetBuilder.merge_file_": [[510, 521], ["MMapIndexedDataset.Index", "indexed_dataset.index_file_path", "indexed_dataset.MMapIndexedDatasetBuilder._sizes.append", "open", "shutil.copyfileobj", "indexed_dataset.data_file_path"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.index_file_path", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.data_file_path"], ["", "def", "merge_file_", "(", "self", ",", "another_file", ")", ":", "\n", "# Concatenate index", "\n", "        ", "index", "=", "MMapIndexedDataset", ".", "Index", "(", "index_file_path", "(", "another_file", ")", ")", "\n", "assert", "index", ".", "dtype", "==", "self", ".", "_dtype", "\n", "\n", "for", "size", "in", "index", ".", "sizes", ":", "\n", "            ", "self", ".", "_sizes", ".", "append", "(", "size", ")", "\n", "\n", "# Concatenate data", "\n", "", "with", "open", "(", "data_file_path", "(", "another_file", ")", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "shutil", ".", "copyfileobj", "(", "f", ",", "self", ".", "_data_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDatasetBuilder.finalize": [[522, 527], ["indexed_dataset.MMapIndexedDatasetBuilder._data_file.close", "MMapIndexedDataset.Index.writer", "index.write"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.GoldMiner.close", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.io.writer", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write"], ["", "", "def", "finalize", "(", "self", ",", "index_file", ")", ":", "\n", "        ", "self", ".", "_data_file", ".", "close", "(", ")", "\n", "\n", "with", "MMapIndexedDataset", ".", "Index", ".", "writer", "(", "index_file", ",", "self", ".", "_dtype", ")", "as", "index", ":", "\n", "            ", "index", ".", "write", "(", "self", ".", "_sizes", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.__best_fitting_dtype": [[17, 22], ["None"], "function", ["None"], ["def", "__best_fitting_dtype", "(", "vocab_size", "=", "None", ")", ":", "\n", "    ", "if", "vocab_size", "is", "not", "None", "and", "vocab_size", "<", "65500", ":", "\n", "        ", "return", "np", ".", "uint16", "\n", "", "else", ":", "\n", "        ", "return", "np", ".", "int32", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.get_available_dataset_impl": [[24, 26], ["None"], "function", ["None"], ["", "", "def", "get_available_dataset_impl", "(", ")", ":", "\n", "    ", "return", "[", "'raw'", ",", "'lazy'", ",", "'cached'", ",", "'mmap'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.infer_dataset_impl": [[28, 42], ["indexed_dataset.IndexedRawTextDataset.exists", "indexed_dataset.IndexedDataset.exists", "open", "f.read", "indexed_dataset.index_file_path"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.index_file_path"], ["", "def", "infer_dataset_impl", "(", "path", ")", ":", "\n", "    ", "if", "IndexedRawTextDataset", ".", "exists", "(", "path", ")", ":", "\n", "        ", "return", "'raw'", "\n", "", "elif", "IndexedDataset", ".", "exists", "(", "path", ")", ":", "\n", "        ", "with", "open", "(", "index_file_path", "(", "path", ")", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "magic", "=", "f", ".", "read", "(", "8", ")", "\n", "if", "magic", "==", "IndexedDataset", ".", "_HDR_MAGIC", ":", "\n", "                ", "return", "'cached'", "\n", "", "elif", "magic", "==", "MMapIndexedDataset", ".", "Index", ".", "_HDR_MAGIC", "[", ":", "8", "]", ":", "\n", "                ", "return", "'mmap'", "\n", "", "else", ":", "\n", "                ", "return", "None", "\n", "", "", "", "else", ":", "\n", "        ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.make_builder": [[44, 51], ["indexed_dataset.MMapIndexedDatasetBuilder", "indexed_dataset.IndexedDatasetBuilder", "indexed_dataset.__best_fitting_dtype"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.__best_fitting_dtype"], ["", "", "def", "make_builder", "(", "out_file", ",", "impl", ",", "vocab_size", "=", "None", ",", "dtype", "=", "None", ")", ":", "\n", "    ", "if", "impl", "==", "'mmap'", ":", "\n", "        ", "if", "dtype", "is", "None", ":", "\n", "            ", "dtype", "=", "__best_fitting_dtype", "(", "vocab_size", ")", "\n", "", "return", "MMapIndexedDatasetBuilder", "(", "out_file", ",", "dtype", "=", "dtype", ")", "\n", "", "else", ":", "\n", "        ", "return", "IndexedDatasetBuilder", "(", "out_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.make_dataset": [[53, 64], ["indexed_dataset.IndexedRawTextDataset.exists", "indexed_dataset.IndexedRawTextDataset", "indexed_dataset.IndexedDataset.exists", "indexed_dataset.IndexedDataset", "indexed_dataset.IndexedDataset.exists", "indexed_dataset.IndexedCachedDataset", "indexed_dataset.MMapIndexedDataset.exists", "indexed_dataset.MMapIndexedDataset"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists"], ["", "", "def", "make_dataset", "(", "path", ",", "impl", ",", "fix_lua_indexing", "=", "False", ",", "dictionary", "=", "None", ")", ":", "\n", "    ", "if", "impl", "==", "'raw'", "and", "IndexedRawTextDataset", ".", "exists", "(", "path", ")", ":", "\n", "        ", "assert", "dictionary", "is", "not", "None", "\n", "return", "IndexedRawTextDataset", "(", "path", ",", "dictionary", ")", "\n", "", "elif", "impl", "==", "'lazy'", "and", "IndexedDataset", ".", "exists", "(", "path", ")", ":", "\n", "        ", "return", "IndexedDataset", "(", "path", ",", "fix_lua_indexing", "=", "fix_lua_indexing", ")", "\n", "", "elif", "impl", "==", "'cached'", "and", "IndexedDataset", ".", "exists", "(", "path", ")", ":", "\n", "        ", "return", "IndexedCachedDataset", "(", "path", ",", "fix_lua_indexing", "=", "fix_lua_indexing", ")", "\n", "", "elif", "impl", "==", "'mmap'", "and", "MMapIndexedDataset", ".", "exists", "(", "path", ")", ":", "\n", "        ", "return", "MMapIndexedDataset", "(", "path", ")", "\n", "", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.dataset_exists": [[66, 73], ["indexed_dataset.IndexedRawTextDataset.exists", "indexed_dataset.MMapIndexedDataset.exists", "indexed_dataset.IndexedDataset.exists"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists"], ["", "def", "dataset_exists", "(", "path", ",", "impl", ")", ":", "\n", "    ", "if", "impl", "==", "'raw'", ":", "\n", "        ", "return", "IndexedRawTextDataset", ".", "exists", "(", "path", ")", "\n", "", "elif", "impl", "==", "'mmap'", ":", "\n", "        ", "return", "MMapIndexedDataset", ".", "exists", "(", "path", ")", "\n", "", "else", ":", "\n", "        ", "return", "IndexedDataset", ".", "exists", "(", "path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.read_longs": [[75, 79], ["numpy.empty", "f.readinto"], "function", ["None"], ["", "", "def", "read_longs", "(", "f", ",", "n", ")", ":", "\n", "    ", "a", "=", "np", ".", "empty", "(", "n", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "f", ".", "readinto", "(", "a", ")", "\n", "return", "a", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.write_longs": [[81, 83], ["f.write", "numpy.array"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write"], ["", "def", "write_longs", "(", "f", ",", "a", ")", ":", "\n", "    ", "f", ".", "write", "(", "np", ".", "array", "(", "a", ",", "dtype", "=", "np", ".", "int64", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.code": [[97, 102], ["dtypes.keys", "ValueError"], "function", ["None"], ["def", "code", "(", "dtype", ")", ":", "\n", "    ", "for", "k", "in", "dtypes", ".", "keys", "(", ")", ":", "\n", "        ", "if", "dtypes", "[", "k", "]", "==", "dtype", ":", "\n", "            ", "return", "k", "\n", "", "", "raise", "ValueError", "(", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.index_file_path": [[104, 106], ["None"], "function", ["None"], ["", "def", "index_file_path", "(", "prefix_path", ")", ":", "\n", "    ", "return", "prefix_path", "+", "'.idx'", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.data_file_path": [[108, 110], ["None"], "function", ["None"], ["", "def", "data_file_path", "(", "prefix_path", ")", ":", "\n", "    ", "return", "prefix_path", "+", "'.bin'", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset._warmup_mmap_file": [[347, 351], ["open", "stream.read"], "function", ["None"], ["", "", "def", "_warmup_mmap_file", "(", "path", ")", ":", "\n", "    ", "with", "open", "(", "path", ",", "'rb'", ")", "as", "stream", ":", "\n", "        ", "while", "stream", ".", "read", "(", "100", "*", "1024", "*", "1024", ")", ":", "\n", "            ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.truncate_dataset.TruncateDataset.__init__": [[13, 18], ["BaseWrapperDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dataset", ",", "truncation_length", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dataset", ")", "\n", "assert", "truncation_length", "is", "not", "None", "\n", "self", ".", "truncation_length", "=", "truncation_length", "\n", "self", ".", "dataset", "=", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.truncate_dataset.TruncateDataset.__getitem__": [[19, 25], ["item.size"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "item", "=", "self", ".", "dataset", "[", "index", "]", "\n", "item_len", "=", "item", ".", "size", "(", "0", ")", "\n", "if", "item_len", ">", "self", ".", "truncation_length", ":", "\n", "            ", "item", "=", "item", "[", ":", "self", ".", "truncation_length", "]", "\n", "", "return", "item", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.truncate_dataset.TruncateDataset.sizes": [[26, 29], ["numpy.minimum"], "methods", ["None"], ["", "@", "property", "\n", "def", "sizes", "(", "self", ")", ":", "\n", "        ", "return", "np", ".", "minimum", "(", "self", ".", "dataset", ".", "sizes", ",", "self", ".", "truncation_length", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.truncate_dataset.TruncateDataset.__len__": [[30, 32], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "dataset", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.base_wrapper_dataset.BaseWrapperDataset.__init__": [[13, 16], ["FairseqDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dataset", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dataset", "=", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.base_wrapper_dataset.BaseWrapperDataset.__getitem__": [[17, 19], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "dataset", "[", "index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.base_wrapper_dataset.BaseWrapperDataset.__len__": [[20, 22], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "dataset", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.base_wrapper_dataset.BaseWrapperDataset.collater": [[23, 28], ["hasattr", "base_wrapper_dataset.BaseWrapperDataset.dataset.collater", "torch.utils.data.dataloader.default_collate"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.collater"], ["", "def", "collater", "(", "self", ",", "samples", ")", ":", "\n", "        ", "if", "hasattr", "(", "self", ".", "dataset", ",", "'collater'", ")", ":", "\n", "            ", "return", "self", ".", "dataset", ".", "collater", "(", "samples", ")", "\n", "", "else", ":", "\n", "            ", "return", "default_collate", "(", "samples", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.base_wrapper_dataset.BaseWrapperDataset.sizes": [[29, 32], ["None"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "sizes", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "dataset", ".", "sizes", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.base_wrapper_dataset.BaseWrapperDataset.num_tokens": [[33, 35], ["base_wrapper_dataset.BaseWrapperDataset.dataset.num_tokens"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.num_tokens"], ["", "def", "num_tokens", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "dataset", ".", "num_tokens", "(", "index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.base_wrapper_dataset.BaseWrapperDataset.size": [[36, 38], ["base_wrapper_dataset.BaseWrapperDataset.dataset.size"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "size", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "dataset", ".", "size", "(", "index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.base_wrapper_dataset.BaseWrapperDataset.ordered_indices": [[39, 41], ["base_wrapper_dataset.BaseWrapperDataset.dataset.ordered_indices"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.ordered_indices"], ["", "def", "ordered_indices", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "dataset", ".", "ordered_indices", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.base_wrapper_dataset.BaseWrapperDataset.supports_prefetch": [[42, 45], ["getattr"], "methods", ["None"], ["", "@", "property", "\n", "def", "supports_prefetch", "(", "self", ")", ":", "\n", "        ", "return", "getattr", "(", "self", ".", "dataset", ",", "'supports_prefetch'", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.base_wrapper_dataset.BaseWrapperDataset.prefetch": [[46, 48], ["base_wrapper_dataset.BaseWrapperDataset.dataset.prefetch"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.block_pair_dataset.BlockPairDataset.prefetch"], ["", "def", "prefetch", "(", "self", ",", "indices", ")", ":", "\n", "        ", "self", ".", "dataset", ".", "prefetch", "(", "indices", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.base_wrapper_dataset.BaseWrapperDataset.set_epoch": [[49, 53], ["super().set_epoch", "hasattr", "base_wrapper_dataset.BaseWrapperDataset.dataset.set_epoch"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.nested_dictionary_dataset.NestedDictionaryDataset.set_epoch", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.nested_dictionary_dataset.NestedDictionaryDataset.set_epoch"], ["", "def", "set_epoch", "(", "self", ",", "epoch", ")", ":", "\n", "        ", "super", "(", ")", ".", "set_epoch", "(", "epoch", ")", "\n", "if", "hasattr", "(", "self", ".", "dataset", ",", "'set_epoch'", ")", ":", "\n", "            ", "self", ".", "dataset", ".", "set_epoch", "(", "epoch", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.strip_token_dataset.StripTokenDataset.__init__": [[11, 14], ["BaseWrapperDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dataset", ",", "id_to_strip", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dataset", ")", "\n", "self", ".", "id_to_strip", "=", "id_to_strip", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.strip_token_dataset.StripTokenDataset.__getitem__": [[15, 18], ["item.ne"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "item", "=", "self", ".", "dataset", "[", "index", "]", "\n", "return", "item", "[", "item", ".", "ne", "(", "self", ".", "id_to_strip", ")", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.transform_eos_lang_pair_dataset.TransformEosLangPairDataset.__init__": [[27, 40], ["None"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "\n", "dataset", ":", "FairseqDataset", ",", "\n", "src_eos", ":", "int", ",", "\n", "new_src_eos", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "tgt_bos", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "new_tgt_bos", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", ")", ":", "\n", "        ", "self", ".", "dataset", "=", "dataset", "\n", "self", ".", "src_eos", "=", "src_eos", "\n", "self", ".", "new_src_eos", "=", "new_src_eos", "\n", "self", ".", "tgt_bos", "=", "tgt_bos", "\n", "self", ".", "new_tgt_bos", "=", "new_tgt_bos", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.transform_eos_lang_pair_dataset.TransformEosLangPairDataset.__getitem__": [[41, 43], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "dataset", "[", "index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.transform_eos_lang_pair_dataset.TransformEosLangPairDataset.__len__": [[44, 46], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "dataset", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.transform_eos_lang_pair_dataset.TransformEosLangPairDataset.collater": [[47, 60], ["transform_eos_lang_pair_dataset.TransformEosLangPairDataset.dataset.collater"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.collater"], ["", "def", "collater", "(", "self", ",", "samples", ")", ":", "\n", "        ", "samples", "=", "self", ".", "dataset", ".", "collater", "(", "samples", ")", "\n", "\n", "# TODO: support different padding direction", "\n", "if", "self", ".", "new_src_eos", "is", "not", "None", ":", "\n", "            ", "assert", "(", "samples", "[", "'net_input'", "]", "[", "'src_tokens'", "]", "[", ":", ",", "-", "1", "]", "!=", "self", ".", "src_eos", ")", ".", "sum", "(", ")", "==", "0", "\n", "samples", "[", "'net_input'", "]", "[", "'src_tokens'", "]", "[", ":", ",", "-", "1", "]", "=", "self", ".", "new_src_eos", "\n", "\n", "", "if", "self", ".", "new_tgt_bos", "is", "not", "None", ":", "\n", "            ", "assert", "(", "samples", "[", "'net_input'", "]", "[", "'prev_output_tokens'", "]", "[", ":", ",", "0", "]", "!=", "self", ".", "tgt_bos", ")", ".", "sum", "(", ")", "==", "0", "\n", "samples", "[", "'net_input'", "]", "[", "'prev_output_tokens'", "]", "[", ":", ",", "0", "]", "=", "self", ".", "new_tgt_bos", "\n", "\n", "", "return", "samples", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.transform_eos_lang_pair_dataset.TransformEosLangPairDataset.num_tokens": [[61, 63], ["transform_eos_lang_pair_dataset.TransformEosLangPairDataset.dataset.num_tokens"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.num_tokens"], ["", "def", "num_tokens", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "dataset", ".", "num_tokens", "(", "index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.transform_eos_lang_pair_dataset.TransformEosLangPairDataset.size": [[64, 66], ["transform_eos_lang_pair_dataset.TransformEosLangPairDataset.dataset.size"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "size", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "dataset", ".", "size", "(", "index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.transform_eos_lang_pair_dataset.TransformEosLangPairDataset.ordered_indices": [[67, 69], ["transform_eos_lang_pair_dataset.TransformEosLangPairDataset.dataset.ordered_indices"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.ordered_indices"], ["", "def", "ordered_indices", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "dataset", ".", "ordered_indices", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.transform_eos_lang_pair_dataset.TransformEosLangPairDataset.supports_prefetch": [[70, 73], ["getattr"], "methods", ["None"], ["", "@", "property", "\n", "def", "supports_prefetch", "(", "self", ")", ":", "\n", "        ", "return", "getattr", "(", "self", ".", "dataset", ",", "'supports_prefetch'", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.transform_eos_lang_pair_dataset.TransformEosLangPairDataset.prefetch": [[74, 76], ["transform_eos_lang_pair_dataset.TransformEosLangPairDataset.dataset.prefetch"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.block_pair_dataset.BlockPairDataset.prefetch"], ["", "def", "prefetch", "(", "self", ",", "indices", ")", ":", "\n", "        ", "return", "self", ".", "dataset", ".", "prefetch", "(", "indices", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.num_samples_dataset.NumSamplesDataset.__getitem__": [[11, 13], ["None"], "methods", ["None"], ["    ", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.num_samples_dataset.NumSamplesDataset.__len__": [[14, 16], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.num_samples_dataset.NumSamplesDataset.collater": [[17, 19], ["sum"], "methods", ["None"], ["", "def", "collater", "(", "self", ",", "samples", ")", ":", "\n", "        ", "return", "sum", "(", "samples", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.sort_dataset.SortDataset.__init__": [[13, 20], ["BaseWrapperDataset.__init__", "all", "isinstance", "len", "len"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dataset", ",", "sort_order", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dataset", ")", "\n", "if", "not", "isinstance", "(", "sort_order", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "            ", "sort_order", "=", "[", "sort_order", "]", "\n", "", "self", ".", "sort_order", "=", "sort_order", "\n", "\n", "assert", "all", "(", "len", "(", "so", ")", "==", "len", "(", "dataset", ")", "for", "so", "in", "sort_order", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.sort_dataset.SortDataset.ordered_indices": [[21, 23], ["numpy.lexsort"], "methods", ["None"], ["", "def", "ordered_indices", "(", "self", ")", ":", "\n", "        ", "return", "np", ".", "lexsort", "(", "self", ".", "sort_order", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.multi_corpus_sampled_dataset.MultiCorpusSampledDataset.__init__": [[31, 49], ["FairseqDataset.__init__", "isinstance", "datasets.items", "isinstance", "dataset.__len__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.AMRDataset.__len__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "datasets", ":", "Dict", "[", "str", ",", "FairseqDataset", "]", ",", "\n", "sampling_func", ":", "Callable", "[", "[", "List", "]", ",", "int", "]", "=", "None", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "isinstance", "(", "datasets", ",", "OrderedDict", ")", "\n", "self", ".", "datasets", "=", "datasets", "\n", "if", "sampling_func", "is", "None", ":", "\n", "            ", "sampling_func", "=", "uniform_sampler", "\n", "", "self", ".", "sampling_func", "=", "sampling_func", "\n", "\n", "self", ".", "total_num_instances", "=", "0", "\n", "for", "_", ",", "dataset", "in", "datasets", ".", "items", "(", ")", ":", "\n", "            ", "assert", "isinstance", "(", "dataset", ",", "FairseqDataset", ")", "\n", "self", ".", "total_num_instances", "+=", "dataset", ".", "__len__", "(", ")", "\n", "\n", "", "self", ".", "_ordered_indices", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.multi_corpus_sampled_dataset.MultiCorpusSampledDataset.__len__": [[50, 55], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Length of this dataset is the sum of individual datasets\n        \"\"\"", "\n", "return", "self", ".", "total_num_instances", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.multi_corpus_sampled_dataset.MultiCorpusSampledDataset.ordered_indices": [[56, 70], ["numpy.arange", "collections.OrderedDict", "len", "dataset.ordered_indices", "multi_corpus_sampled_dataset.MultiCorpusSampledDataset.datasets.items"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.ordered_indices"], ["", "def", "ordered_indices", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Ordered indices for batching. Here we call the underlying\n        dataset's ordered_indices() so that we get the same random ordering\n        as we would have from using the underlying dataset directly.\n        \"\"\"", "\n", "if", "self", ".", "_ordered_indices", "is", "None", ":", "\n", "            ", "self", ".", "_ordered_indices", "=", "OrderedDict", "(", "\n", "[", "\n", "(", "key", ",", "dataset", ".", "ordered_indices", "(", ")", ")", "\n", "for", "key", ",", "dataset", "in", "self", ".", "datasets", ".", "items", "(", ")", "\n", "]", "\n", ")", "\n", "", "return", "np", ".", "arange", "(", "len", "(", "self", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.multi_corpus_sampled_dataset.MultiCorpusSampledDataset._map_index_to_dataset": [[71, 83], ["len"], "methods", ["None"], ["", "def", "_map_index_to_dataset", "(", "self", ",", "key", ":", "int", ",", "index", ":", "int", ")", ":", "\n", "        ", "\"\"\"\n        Different underlying datasets have different lengths. In order to ensure\n        we are not accessing an index outside the range of the current dataset\n        size, we wrap around. This function should be called after we have\n        created an ordering for this and all underlying datasets.\n        \"\"\"", "\n", "assert", "(", "\n", "self", ".", "_ordered_indices", "is", "not", "None", "\n", ")", ",", "\"Must call MultiCorpusSampledDataset.ordered_indices() first\"", "\n", "mapped_index", "=", "index", "%", "len", "(", "self", ".", "datasets", "[", "key", "]", ")", "\n", "return", "self", ".", "_ordered_indices", "[", "key", "]", "[", "mapped_index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.multi_corpus_sampled_dataset.MultiCorpusSampledDataset.__getitem__": [[84, 94], ["collections.OrderedDict", "multi_corpus_sampled_dataset.MultiCorpusSampledDataset.datasets.items", "multi_corpus_sampled_dataset.MultiCorpusSampledDataset._map_index_to_dataset"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.multi_corpus_sampled_dataset.MultiCorpusSampledDataset._map_index_to_dataset"], ["", "def", "__getitem__", "(", "self", ",", "index", ":", "int", ")", ":", "\n", "        ", "\"\"\"\n        Get the item associated with index from each underlying dataset.\n        Since index is in the range of [0, TotalNumInstances], we need to\n        map the index to the dataset before retrieving the item.\n        \"\"\"", "\n", "return", "OrderedDict", "(", "\n", "[", "\n", "(", "key", ",", "dataset", "[", "self", ".", "_map_index_to_dataset", "(", "key", ",", "index", ")", "]", ")", "\n", "for", "key", ",", "dataset", "in", "self", ".", "datasets", ".", "items", "(", ")", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.multi_corpus_sampled_dataset.MultiCorpusSampledDataset.collater": [[97, 111], ["multi_corpus_sampled_dataset.MultiCorpusSampledDataset.sampling_func", "multi_corpus_sampled_dataset.MultiCorpusSampledDataset.datasets[].collater", "len", "list", "multi_corpus_sampled_dataset.MultiCorpusSampledDataset.datasets.keys"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.collater"], ["", "def", "collater", "(", "self", ",", "samples", ":", "List", "[", "Dict", "]", ")", ":", "\n", "        ", "\"\"\"\n        Generate a mini-batch for this dataset.\n        To convert this into a regular mini-batch we use the following\n        logic:\n            1. Select a dataset using the specified probability distribution.\n            2. Call the collater function of the selected dataset.\n        \"\"\"", "\n", "if", "len", "(", "samples", ")", "==", "0", ":", "\n", "            ", "return", "None", "\n", "\n", "", "selected_key", "=", "self", ".", "sampling_func", "(", "list", "(", "self", ".", "datasets", ".", "keys", "(", ")", ")", ")", "\n", "selected_samples", "=", "[", "sample", "[", "selected_key", "]", "for", "sample", "in", "samples", "]", "\n", "return", "self", ".", "datasets", "[", "selected_key", "]", ".", "collater", "(", "selected_samples", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.multi_corpus_sampled_dataset.MultiCorpusSampledDataset.num_tokens": [[112, 121], ["max", "dataset.num_tokens", "multi_corpus_sampled_dataset.MultiCorpusSampledDataset._map_index_to_dataset", "multi_corpus_sampled_dataset.MultiCorpusSampledDataset.datasets.items"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.num_tokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.multi_corpus_sampled_dataset.MultiCorpusSampledDataset._map_index_to_dataset"], ["", "def", "num_tokens", "(", "self", ",", "index", ":", "int", ")", ":", "\n", "        ", "\"\"\"\n        Return an example's length (number of tokens), used for batching. Here\n        we return the max across all examples at index across all underlying\n        datasets.\n        \"\"\"", "\n", "return", "max", "(", "\n", "dataset", ".", "num_tokens", "(", "self", ".", "_map_index_to_dataset", "(", "key", ",", "index", ")", ")", "\n", "for", "key", ",", "dataset", "in", "self", ".", "datasets", ".", "items", "(", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.multi_corpus_sampled_dataset.MultiCorpusSampledDataset.size": [[123, 132], ["max", "dataset.size", "multi_corpus_sampled_dataset.MultiCorpusSampledDataset._map_index_to_dataset", "multi_corpus_sampled_dataset.MultiCorpusSampledDataset.datasets.items"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.multi_corpus_sampled_dataset.MultiCorpusSampledDataset._map_index_to_dataset"], ["", "def", "size", "(", "self", ",", "index", ":", "int", ")", ":", "\n", "        ", "\"\"\"\n        Return an example's size as a float or tuple. Here we return the max\n        across all underlying datasets. This value is used when filtering a\n        dataset with max-positions.\n        \"\"\"", "\n", "return", "max", "(", "\n", "dataset", ".", "size", "(", "self", ".", "_map_index_to_dataset", "(", "key", ",", "index", ")", ")", "\n", "for", "key", ",", "dataset", "in", "self", ".", "datasets", ".", "items", "(", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.multi_corpus_sampled_dataset.MultiCorpusSampledDataset.supports_prefetch": [[134, 139], ["all", "getattr", "multi_corpus_sampled_dataset.MultiCorpusSampledDataset.datasets.values"], "methods", ["None"], ["", "@", "property", "\n", "def", "supports_prefetch", "(", "self", ")", ":", "\n", "        ", "return", "all", "(", "\n", "getattr", "(", "dataset", ",", "\"supports_prefetch\"", ",", "False", ")", "\n", "for", "dataset", "in", "self", ".", "datasets", ".", "values", "(", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.multi_corpus_sampled_dataset.MultiCorpusSampledDataset.prefetch": [[141, 145], ["multi_corpus_sampled_dataset.MultiCorpusSampledDataset.datasets.items", "dataset.prefetch", "multi_corpus_sampled_dataset.MultiCorpusSampledDataset._map_index_to_dataset"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.block_pair_dataset.BlockPairDataset.prefetch", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.multi_corpus_sampled_dataset.MultiCorpusSampledDataset._map_index_to_dataset"], ["", "def", "prefetch", "(", "self", ",", "indices", ")", ":", "\n", "        ", "for", "key", ",", "dataset", "in", "self", ".", "datasets", ".", "items", "(", ")", ":", "\n", "            ", "dataset", ".", "prefetch", "(", "\n", "[", "self", ".", "_map_index_to_dataset", "(", "key", ",", "index", ")", "for", "index", "in", "indices", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.multi_corpus_sampled_dataset.uniform_sampler": [[14, 17], ["numpy.random.choice().item", "numpy.random.choice"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item"], ["def", "uniform_sampler", "(", "x", ")", ":", "\n", "# Sample from uniform distribution", "\n", "    ", "return", "np", ".", "random", ".", "choice", "(", "x", ",", "1", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.mask_tokens_dataset.MaskTokensDataset.apply_mask": [[45, 52], ["LRUCacheDataset", "LRUCacheDataset", "LRUCacheDataset", "cls", "cls"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.masked_lm_dictionary.BertDictionary.cls", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.masked_lm_dictionary.BertDictionary.cls"], ["@", "classmethod", "\n", "def", "apply_mask", "(", "cls", ",", "dataset", ":", "torch", ".", "utils", ".", "data", ".", "Dataset", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Return the source and target datasets for masked LM training.\"\"\"", "\n", "dataset", "=", "LRUCacheDataset", "(", "dataset", ")", "\n", "return", "(", "\n", "LRUCacheDataset", "(", "cls", "(", "dataset", ",", "*", "args", ",", "**", "kwargs", ",", "return_masked_tokens", "=", "False", ")", ")", ",", "\n", "LRUCacheDataset", "(", "cls", "(", "dataset", ",", "*", "args", ",", "**", "kwargs", ",", "return_masked_tokens", "=", "True", ")", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.mask_tokens_dataset.MaskTokensDataset.__init__": [[54, 93], ["numpy.array", "numpy.ones", "numpy.ones.sum", "len"], "methods", ["None"], ["", "def", "__init__", "(", "\n", "self", ",", "\n", "dataset", ":", "torch", ".", "utils", ".", "data", ".", "Dataset", ",", "\n", "vocab", ":", "Dictionary", ",", "\n", "pad_idx", ":", "int", ",", "\n", "mask_idx", ":", "int", ",", "\n", "return_masked_tokens", ":", "bool", "=", "False", ",", "\n", "seed", ":", "int", "=", "1", ",", "\n", "mask_prob", ":", "float", "=", "0.15", ",", "\n", "leave_unmasked_prob", ":", "float", "=", "0.1", ",", "\n", "random_token_prob", ":", "float", "=", "0.1", ",", "\n", "freq_weighted_replacement", ":", "bool", "=", "False", ",", "\n", "mask_whole_words", ":", "torch", ".", "Tensor", "=", "None", ",", "\n", ")", ":", "\n", "        ", "assert", "0.0", "<", "mask_prob", "<", "1.0", "\n", "assert", "0.0", "<=", "random_token_prob", "<=", "1.0", "\n", "assert", "0.0", "<=", "leave_unmasked_prob", "<=", "1.0", "\n", "assert", "random_token_prob", "+", "leave_unmasked_prob", "<=", "1.0", "\n", "\n", "self", ".", "dataset", "=", "dataset", "\n", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "pad_idx", "=", "pad_idx", "\n", "self", ".", "mask_idx", "=", "mask_idx", "\n", "self", ".", "return_masked_tokens", "=", "return_masked_tokens", "\n", "self", ".", "seed", "=", "seed", "\n", "self", ".", "mask_prob", "=", "mask_prob", "\n", "self", ".", "leave_unmasked_prob", "=", "leave_unmasked_prob", "\n", "self", ".", "random_token_prob", "=", "random_token_prob", "\n", "self", ".", "mask_whole_words", "=", "mask_whole_words", "\n", "\n", "if", "random_token_prob", ">", "0.0", ":", "\n", "            ", "if", "freq_weighted_replacement", ":", "\n", "                ", "weights", "=", "np", ".", "array", "(", "self", ".", "vocab", ".", "count", ")", "\n", "", "else", ":", "\n", "                ", "weights", "=", "np", ".", "ones", "(", "len", "(", "self", ".", "vocab", ")", ")", "\n", "", "weights", "[", ":", "self", ".", "vocab", ".", "nspecial", "]", "=", "0", "\n", "self", ".", "weights", "=", "weights", "/", "weights", ".", "sum", "(", ")", "\n", "\n", "", "self", ".", "epoch", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.mask_tokens_dataset.MaskTokensDataset.set_epoch": [[94, 96], ["None"], "methods", ["None"], ["", "def", "set_epoch", "(", "self", ",", "epoch", ",", "**", "unused", ")", ":", "\n", "        ", "self", ".", "epoch", "=", "epoch", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.mask_tokens_dataset.MaskTokensDataset.__getitem__": [[97, 173], ["functools.lru_cache", "fairseq.data.data_utils.numpy_seed", "len", "numpy.full", "int", "numpy.copy", "torch.from_numpy", "mask_tokens_dataset.MaskTokensDataset.mask_whole_words.gather", "mask_tokens_dataset.MaskTokensDataset.nonzero().view", "len", "list", "numpy.full", "torch.from_numpy", "numpy.repeat", "numpy.repeat.sum", "numpy.split", "len", "map", "numpy.random.rand", "numpy.random.choice", "numpy.repeat", "len", "numpy.random.choice", "mask_tokens_dataset.MaskTokensDataset.nonzero", "torch.from_numpy", "numpy.random.rand", "numpy.repeat", "numpy.repeat.sum", "len", "numpy.repeat.astype", "numpy.random.rand"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.data_utils.numpy_seed", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.copy", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["", "@", "lru_cache", "(", "maxsize", "=", "8", ")", "\n", "def", "__getitem__", "(", "self", ",", "index", ":", "int", ")", ":", "\n", "        ", "with", "data_utils", ".", "numpy_seed", "(", "self", ".", "seed", ",", "self", ".", "epoch", ",", "index", ")", ":", "\n", "            ", "item", "=", "self", ".", "dataset", "[", "index", "]", "\n", "sz", "=", "len", "(", "item", ")", "\n", "\n", "assert", "self", ".", "mask_idx", "not", "in", "item", ",", "'Dataset contains mask_idx (={}), this is not expected!'", ".", "format", "(", "\n", "self", ".", "mask_idx", ",", "\n", ")", "\n", "\n", "if", "self", ".", "mask_whole_words", "is", "not", "None", ":", "\n", "                ", "word_begins_mask", "=", "self", ".", "mask_whole_words", ".", "gather", "(", "0", ",", "item", ")", "\n", "word_begins_idx", "=", "word_begins_mask", ".", "nonzero", "(", ")", ".", "view", "(", "-", "1", ")", "\n", "sz", "=", "len", "(", "word_begins_idx", ")", "\n", "words", "=", "np", ".", "split", "(", "word_begins_mask", ",", "word_begins_idx", ")", "[", "1", ":", "]", "\n", "assert", "len", "(", "words", ")", "==", "sz", "\n", "word_lens", "=", "list", "(", "map", "(", "len", ",", "words", ")", ")", "\n", "\n", "# decide elements to mask", "\n", "", "mask", "=", "np", ".", "full", "(", "sz", ",", "False", ")", "\n", "num_mask", "=", "int", "(", "\n", "# add a random number for probabilistic rounding", "\n", "self", ".", "mask_prob", "*", "sz", "+", "np", ".", "random", ".", "rand", "(", ")", "\n", ")", "\n", "mask", "[", "np", ".", "random", ".", "choice", "(", "sz", ",", "num_mask", ",", "replace", "=", "False", ")", "]", "=", "True", "\n", "\n", "if", "self", ".", "return_masked_tokens", ":", "\n", "# exit early if we're just returning the masked tokens", "\n", "# (i.e., the targets for masked LM training)", "\n", "                ", "if", "self", ".", "mask_whole_words", "is", "not", "None", ":", "\n", "                    ", "mask", "=", "np", ".", "repeat", "(", "mask", ",", "word_lens", ")", "\n", "", "new_item", "=", "np", ".", "full", "(", "len", "(", "mask", ")", ",", "self", ".", "pad_idx", ")", "\n", "new_item", "[", "mask", "]", "=", "item", "[", "torch", ".", "from_numpy", "(", "mask", ".", "astype", "(", "np", ".", "uint8", ")", ")", "]", "\n", "return", "torch", ".", "from_numpy", "(", "new_item", ")", "\n", "\n", "# decide unmasking and random replacement", "\n", "", "rand_or_unmask_prob", "=", "self", ".", "random_token_prob", "+", "self", ".", "leave_unmasked_prob", "\n", "if", "rand_or_unmask_prob", ">", "0.0", ":", "\n", "                ", "rand_or_unmask", "=", "mask", "&", "(", "np", ".", "random", ".", "rand", "(", "sz", ")", "<", "rand_or_unmask_prob", ")", "\n", "if", "self", ".", "random_token_prob", "==", "0.0", ":", "\n", "                    ", "unmask", "=", "rand_or_unmask", "\n", "rand_mask", "=", "None", "\n", "", "elif", "self", ".", "leave_unmasked_prob", "==", "0.0", ":", "\n", "                    ", "unmask", "=", "None", "\n", "rand_mask", "=", "rand_or_unmask", "\n", "", "else", ":", "\n", "                    ", "unmask_prob", "=", "self", ".", "leave_unmasked_prob", "/", "rand_or_unmask_prob", "\n", "decision", "=", "np", ".", "random", ".", "rand", "(", "sz", ")", "<", "unmask_prob", "\n", "unmask", "=", "rand_or_unmask", "&", "decision", "\n", "rand_mask", "=", "rand_or_unmask", "&", "(", "~", "decision", ")", "\n", "", "", "else", ":", "\n", "                ", "unmask", "=", "rand_mask", "=", "None", "\n", "\n", "", "if", "unmask", "is", "not", "None", ":", "\n", "                ", "mask", "=", "mask", "^", "unmask", "\n", "\n", "", "if", "self", ".", "mask_whole_words", "is", "not", "None", ":", "\n", "                ", "mask", "=", "np", ".", "repeat", "(", "mask", ",", "word_lens", ")", "\n", "\n", "", "new_item", "=", "np", ".", "copy", "(", "item", ")", "\n", "new_item", "[", "mask", "]", "=", "self", ".", "mask_idx", "\n", "if", "rand_mask", "is", "not", "None", ":", "\n", "                ", "num_rand", "=", "rand_mask", ".", "sum", "(", ")", "\n", "if", "num_rand", ">", "0", ":", "\n", "                    ", "if", "self", ".", "mask_whole_words", "is", "not", "None", ":", "\n", "                        ", "rand_mask", "=", "np", ".", "repeat", "(", "rand_mask", ",", "word_lens", ")", "\n", "num_rand", "=", "rand_mask", ".", "sum", "(", ")", "\n", "\n", "", "new_item", "[", "rand_mask", "]", "=", "np", ".", "random", ".", "choice", "(", "\n", "len", "(", "self", ".", "vocab", ")", ",", "\n", "num_rand", ",", "\n", "p", "=", "self", ".", "weights", ",", "\n", ")", "\n", "\n", "", "", "return", "torch", ".", "from_numpy", "(", "new_item", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.raw_label_dataset.RawLabelDataset.__init__": [[13, 16], ["FairseqDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["    ", "def", "__init__", "(", "self", ",", "labels", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "labels", "=", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.raw_label_dataset.RawLabelDataset.__getitem__": [[17, 19], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "labels", "[", "index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.raw_label_dataset.RawLabelDataset.__len__": [[20, 22], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.raw_label_dataset.RawLabelDataset.collater": [[23, 25], ["torch.tensor"], "methods", ["None"], ["", "def", "collater", "(", "self", ",", "samples", ")", ":", "\n", "        ", "return", "torch", ".", "tensor", "(", "samples", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.monolingual_dataset.MonolingualDataset.__init__": [[62, 77], ["numpy.array", "all", "len"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "dataset", ",", "sizes", ",", "src_vocab", ",", "tgt_vocab", ",", "add_eos_for_other_targets", ",", "shuffle", ",", "\n", "targets", "=", "None", ",", "add_bos_token", "=", "False", ")", ":", "\n", "        ", "self", ".", "dataset", "=", "dataset", "\n", "self", ".", "sizes", "=", "np", ".", "array", "(", "sizes", ")", "\n", "self", ".", "vocab", "=", "src_vocab", "\n", "self", ".", "tgt_vocab", "=", "tgt_vocab", "\n", "self", ".", "add_eos_for_other_targets", "=", "add_eos_for_other_targets", "\n", "self", ".", "shuffle", "=", "shuffle", "\n", "self", ".", "add_bos_token", "=", "add_bos_token", "\n", "\n", "assert", "targets", "is", "None", "or", "all", "(", "t", "in", "{", "'self'", ",", "'future'", ",", "'past'", "}", "for", "t", "in", "targets", ")", ",", "\"targets must be none or one of 'self', 'future', 'past'\"", "\n", "if", "targets", "is", "not", "None", "and", "len", "(", "targets", ")", "==", "0", ":", "\n", "            ", "targets", "=", "None", "\n", "", "self", ".", "targets", "=", "targets", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.monolingual_dataset.MonolingualDataset.__getitem__": [[78, 95], ["monolingual_dataset.MonolingualDataset._maybe_add_bos", "monolingual_dataset.MonolingualDataset._make_source_target"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.monolingual_dataset.MonolingualDataset._maybe_add_bos", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.monolingual_dataset.MonolingualDataset._make_source_target"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "if", "self", ".", "targets", "is", "not", "None", ":", "\n", "# *future_target* is the original sentence", "\n", "# *source* is shifted right by 1 (maybe left-padded with eos)", "\n", "# *past_target* is shifted right by 2 (left-padded as needed)", "\n", "#", "\n", "# Left-to-right language models should condition on *source* and", "\n", "# predict *future_target*.", "\n", "# Right-to-left language models should condition on *source* and", "\n", "# predict *past_target*.", "\n", "            ", "source", ",", "future_target", ",", "past_target", "=", "self", ".", "dataset", "[", "index", "]", "\n", "source", ",", "target", "=", "self", ".", "_make_source_target", "(", "source", ",", "future_target", ",", "past_target", ")", "\n", "", "else", ":", "\n", "            ", "source", "=", "self", ".", "dataset", "[", "index", "]", "\n", "target", "=", "None", "\n", "", "source", ",", "target", "=", "self", ".", "_maybe_add_bos", "(", "source", ",", "target", ")", "\n", "return", "{", "'id'", ":", "index", ",", "'source'", ":", "source", ",", "'target'", ":", "target", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.monolingual_dataset.MonolingualDataset.__len__": [[96, 98], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "dataset", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.monolingual_dataset.MonolingualDataset._make_source_target": [[99, 131], ["monolingual_dataset.MonolingualDataset._filter_vocab", "torch.cat", "len", "monolingual_dataset.MonolingualDataset.vocab.eos", "torch.cat", "torch.cat", "target.append", "torch.cat.new", "target.append", "torch.cat.new", "torch.cat.new", "target.append", "Exception", "monolingual_dataset.MonolingualDataset.vocab.eos", "monolingual_dataset.MonolingualDataset.vocab.pad", "monolingual_dataset.MonolingualDataset.vocab.pad"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.monolingual_dataset.MonolingualDataset._filter_vocab", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad"], ["", "def", "_make_source_target", "(", "self", ",", "source", ",", "future_target", ",", "past_target", ")", ":", "\n", "        ", "if", "self", ".", "targets", "is", "not", "None", ":", "\n", "            ", "target", "=", "[", "]", "\n", "\n", "if", "self", ".", "add_eos_for_other_targets", "and", "(", "(", "'self'", "in", "self", ".", "targets", ")", "or", "(", "'past'", "in", "self", ".", "targets", ")", ")", "and", "source", "[", "-", "1", "]", "!=", "self", ".", "vocab", ".", "eos", "(", ")", ":", "\n", "# append eos at the end of source", "\n", "                ", "source", "=", "torch", ".", "cat", "(", "[", "source", ",", "source", ".", "new", "(", "[", "self", ".", "vocab", ".", "eos", "(", ")", "]", ")", "]", ")", "\n", "\n", "if", "'future'", "in", "self", ".", "targets", ":", "\n", "                    ", "future_target", "=", "torch", ".", "cat", "(", "[", "future_target", ",", "future_target", ".", "new", "(", "[", "self", ".", "vocab", ".", "pad", "(", ")", "]", ")", "]", ")", "\n", "", "if", "'past'", "in", "self", ".", "targets", ":", "\n", "# first token is before the start of sentence which is only used in \"none\" break mode when", "\n", "# add_eos_for_other_targets is False", "\n", "                    ", "past_target", "=", "torch", ".", "cat", "(", "[", "past_target", ".", "new", "(", "[", "self", ".", "vocab", ".", "pad", "(", ")", "]", ")", ",", "past_target", "[", "1", ":", "]", ",", "source", "[", "-", "2", ",", "None", "]", "]", ")", "\n", "\n", "", "", "for", "t", "in", "self", ".", "targets", ":", "\n", "                ", "if", "t", "==", "'self'", ":", "\n", "                    ", "target", ".", "append", "(", "source", ")", "\n", "", "elif", "t", "==", "'future'", ":", "\n", "                    ", "target", ".", "append", "(", "future_target", ")", "\n", "", "elif", "t", "==", "'past'", ":", "\n", "                    ", "target", ".", "append", "(", "past_target", ")", "\n", "", "else", ":", "\n", "                    ", "raise", "Exception", "(", "'invalid target '", "+", "t", ")", "\n", "\n", "", "", "if", "len", "(", "target", ")", "==", "1", ":", "\n", "                ", "target", "=", "target", "[", "0", "]", "\n", "", "", "else", ":", "\n", "            ", "target", "=", "future_target", "\n", "\n", "", "return", "source", ",", "self", ".", "_filter_vocab", "(", "target", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.monolingual_dataset.MonolingualDataset._maybe_add_bos": [[132, 138], ["torch.cat", "torch.cat", "torch.cat.new", "torch.cat.new", "monolingual_dataset.MonolingualDataset.vocab.bos", "monolingual_dataset.MonolingualDataset.tgt_vocab.bos"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.bos", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.bos"], ["", "def", "_maybe_add_bos", "(", "self", ",", "source", ",", "target", ")", ":", "\n", "        ", "if", "self", ".", "add_bos_token", ":", "\n", "            ", "source", "=", "torch", ".", "cat", "(", "[", "source", ".", "new", "(", "[", "self", ".", "vocab", ".", "bos", "(", ")", "]", ")", ",", "source", "]", ")", "\n", "if", "target", "is", "not", "None", ":", "\n", "                ", "target", "=", "torch", ".", "cat", "(", "[", "target", ".", "new", "(", "[", "self", ".", "tgt_vocab", ".", "bos", "(", ")", "]", ")", ",", "target", "]", ")", "\n", "", "", "return", "source", ",", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.monolingual_dataset.MonolingualDataset._filter_vocab": [[139, 151], ["len", "len", "isinstance", "monolingual_dataset.MonolingualDataset._filter_vocab._filter"], "methods", ["None"], ["", "def", "_filter_vocab", "(", "self", ",", "target", ")", ":", "\n", "        ", "if", "len", "(", "self", ".", "tgt_vocab", ")", "!=", "len", "(", "self", ".", "vocab", ")", ":", "\n", "            ", "def", "_filter", "(", "target", ")", ":", "\n", "                ", "mask", "=", "target", ".", "ge", "(", "len", "(", "self", ".", "tgt_vocab", ")", ")", "\n", "if", "mask", ".", "any", "(", ")", ":", "\n", "                    ", "target", "[", "mask", "]", "=", "self", ".", "tgt_vocab", ".", "unk", "(", ")", "\n", "", "return", "target", "\n", "\n", "", "if", "isinstance", "(", "target", ",", "list", ")", ":", "\n", "                ", "return", "[", "_filter", "(", "t", ")", "for", "t", "in", "target", "]", "\n", "", "return", "_filter", "(", "target", ")", "\n", "", "return", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.monolingual_dataset.MonolingualDataset.collater": [[152, 174], ["monolingual_dataset.collate", "monolingual_dataset.MonolingualDataset.vocab.pad", "monolingual_dataset.MonolingualDataset.vocab.eos"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.language_pair_dataset.collate", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos"], ["", "def", "collater", "(", "self", ",", "samples", ")", ":", "\n", "        ", "\"\"\"Merge a list of samples to form a mini-batch.\n\n        Args:\n            samples (List[dict]): samples to collate\n\n        Returns:\n            dict: a mini-batch with the following keys:\n\n                - `id` (LongTensor): example IDs in the original input order\n                - `ntokens` (int): total number of tokens in the batch\n                - `net_input` (dict): the input to the Model, containing keys:\n\n                  - `src_tokens` (LongTensor): a padded 2D Tensor of tokens in\n                    the source sentence of shape `(bsz, src_len)`. Padding will\n                    appear on the right.\n\n                - `target` (LongTensor): a padded 2D Tensor of tokens in the\n                  target sentence of shape `(bsz, tgt_len)`. Padding will appear\n                  on the right.\n        \"\"\"", "\n", "return", "collate", "(", "samples", ",", "self", ".", "vocab", ".", "pad", "(", ")", ",", "self", ".", "vocab", ".", "eos", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.monolingual_dataset.MonolingualDataset.num_tokens": [[175, 179], ["None"], "methods", ["None"], ["", "def", "num_tokens", "(", "self", ",", "index", ")", ":", "\n", "        ", "\"\"\"Return the number of tokens in a sample. This value is used to\n        enforce ``--max-tokens`` during batching.\"\"\"", "\n", "return", "self", ".", "sizes", "[", "index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.monolingual_dataset.MonolingualDataset.size": [[180, 184], ["None"], "methods", ["None"], ["", "def", "size", "(", "self", ",", "index", ")", ":", "\n", "        ", "\"\"\"Return an example's size as a float or tuple. This value is used when\n        filtering a dataset with ``--max-positions``.\"\"\"", "\n", "return", "self", ".", "sizes", "[", "index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.monolingual_dataset.MonolingualDataset.ordered_indices": [[185, 194], ["order.append", "numpy.lexsort", "numpy.random.permutation", "numpy.arange", "len", "len"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "ordered_indices", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return an ordered list of indices. Batches will be constructed based\n        on this order.\"\"\"", "\n", "if", "self", ".", "shuffle", ":", "\n", "            ", "order", "=", "[", "np", ".", "random", ".", "permutation", "(", "len", "(", "self", ")", ")", "]", "\n", "", "else", ":", "\n", "            ", "order", "=", "[", "np", ".", "arange", "(", "len", "(", "self", ")", ")", "]", "\n", "", "order", ".", "append", "(", "self", ".", "sizes", ")", "\n", "return", "np", ".", "lexsort", "(", "order", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.monolingual_dataset.MonolingualDataset.supports_prefetch": [[195, 198], ["getattr"], "methods", ["None"], ["", "@", "property", "\n", "def", "supports_prefetch", "(", "self", ")", ":", "\n", "        ", "return", "getattr", "(", "self", ".", "dataset", ",", "'supports_prefetch'", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.monolingual_dataset.MonolingualDataset.prefetch": [[199, 201], ["monolingual_dataset.MonolingualDataset.dataset.prefetch"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.block_pair_dataset.BlockPairDataset.prefetch"], ["", "def", "prefetch", "(", "self", ",", "indices", ")", ":", "\n", "        ", "self", ".", "dataset", ".", "prefetch", "(", "indices", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.monolingual_dataset.collate": [[12, 47], ["monolingual_dataset.collate.merge"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.translation_moe.score.merge"], ["def", "collate", "(", "samples", ",", "pad_idx", ",", "eos_idx", ")", ":", "\n", "    ", "if", "len", "(", "samples", ")", "==", "0", ":", "\n", "        ", "return", "{", "}", "\n", "\n", "", "def", "merge", "(", "key", ",", "is_list", "=", "False", ")", ":", "\n", "        ", "if", "is_list", ":", "\n", "            ", "res", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "samples", "[", "0", "]", "[", "key", "]", ")", ")", ":", "\n", "                ", "res", ".", "append", "(", "data_utils", ".", "collate_tokens", "(", "\n", "[", "s", "[", "key", "]", "[", "i", "]", "for", "s", "in", "samples", "]", ",", "pad_idx", ",", "eos_idx", ",", "left_pad", "=", "False", ",", "\n", ")", ")", "\n", "", "return", "res", "\n", "", "else", ":", "\n", "            ", "return", "data_utils", ".", "collate_tokens", "(", "\n", "[", "s", "[", "key", "]", "for", "s", "in", "samples", "]", ",", "pad_idx", ",", "eos_idx", ",", "left_pad", "=", "False", ",", "\n", ")", "\n", "\n", "", "", "src_tokens", "=", "merge", "(", "'source'", ")", "\n", "if", "samples", "[", "0", "]", "[", "'target'", "]", "is", "not", "None", ":", "\n", "        ", "is_target_list", "=", "isinstance", "(", "samples", "[", "0", "]", "[", "'target'", "]", ",", "list", ")", "\n", "target", "=", "merge", "(", "'target'", ",", "is_target_list", ")", "\n", "", "else", ":", "\n", "        ", "target", "=", "src_tokens", "\n", "\n", "", "return", "{", "\n", "'id'", ":", "torch", ".", "LongTensor", "(", "[", "s", "[", "'id'", "]", "for", "s", "in", "samples", "]", ")", ",", "\n", "'nsentences'", ":", "len", "(", "samples", ")", ",", "\n", "'ntokens'", ":", "sum", "(", "len", "(", "s", "[", "'source'", "]", ")", "for", "s", "in", "samples", ")", ",", "\n", "'net_input'", ":", "{", "\n", "'src_tokens'", ":", "src_tokens", ",", "\n", "'src_lengths'", ":", "torch", ".", "LongTensor", "(", "[", "\n", "s", "[", "'source'", "]", ".", "numel", "(", ")", "for", "s", "in", "samples", "\n", "]", ")", ",", "\n", "}", ",", "\n", "'target'", ":", "target", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.list_dataset.ListDataset.__init__": [[11, 14], ["BaseWrapperDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dataset", ",", "sizes", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dataset", ")", "\n", "self", ".", "_sizes", "=", "sizes", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.list_dataset.ListDataset.collater": [[15, 17], ["None"], "methods", ["None"], ["", "def", "collater", "(", "self", ",", "samples", ")", ":", "\n", "        ", "return", "samples", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.list_dataset.ListDataset.sizes": [[18, 21], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "sizes", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_sizes", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.list_dataset.ListDataset.num_tokens": [[22, 24], ["None"], "methods", ["None"], ["", "def", "num_tokens", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "sizes", "[", "index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.list_dataset.ListDataset.size": [[25, 27], ["None"], "methods", ["None"], ["", "def", "size", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "sizes", "[", "index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.list_dataset.ListDataset.set_epoch": [[28, 30], ["None"], "methods", ["None"], ["", "def", "set_epoch", "(", "self", ",", "epoch", ")", ":", "\n", "        ", "pass", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.nested_dictionary_dataset.NestedDictionaryDataset.__init__": [[49, 63], ["FairseqDataset.__init__", "nested_dictionary_dataset._flatten", "nested_dictionary_dataset.NestedDictionaryDataset.defn.values", "len", "isinstance", "isinstance", "ValueError", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.nested_dictionary_dataset._flatten"], ["    ", "def", "__init__", "(", "self", ",", "defn", ",", "sizes", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "defn", "=", "_flatten", "(", "defn", ")", "\n", "self", ".", "sizes", "=", "[", "sizes", "]", "if", "not", "isinstance", "(", "sizes", ",", "(", "list", ",", "tuple", ")", ")", "else", "sizes", "\n", "\n", "first", "=", "None", "\n", "for", "v", "in", "self", ".", "defn", ".", "values", "(", ")", ":", "\n", "            ", "if", "not", "isinstance", "(", "v", ",", "(", "FairseqDataset", ",", "torch", ".", "utils", ".", "data", ".", "Dataset", ",", ")", ")", ":", "\n", "                ", "raise", "ValueError", "(", "'Expected Dataset but found: {}'", ".", "format", "(", "v", ".", "__class__", ")", ")", "\n", "", "first", "=", "first", "or", "v", "\n", "if", "len", "(", "v", ")", ">", "0", ":", "\n", "                ", "assert", "len", "(", "v", ")", "==", "len", "(", "first", ")", ",", "'dataset lengths must match'", "\n", "\n", "", "", "self", ".", "_len", "=", "len", "(", "first", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.nested_dictionary_dataset.NestedDictionaryDataset.__getitem__": [[64, 66], ["collections.OrderedDict", "nested_dictionary_dataset.NestedDictionaryDataset.defn.items"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "OrderedDict", "(", "(", "k", ",", "ds", "[", "index", "]", ")", "for", "k", ",", "ds", "in", "self", ".", "defn", ".", "items", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.nested_dictionary_dataset.NestedDictionaryDataset.__len__": [[67, 69], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_len", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.nested_dictionary_dataset.NestedDictionaryDataset.collater": [[70, 88], ["collections.OrderedDict", "nested_dictionary_dataset.NestedDictionaryDataset.defn.items", "nested_dictionary_dataset._unflatten", "len", "ds.collater", "torch.utils.data.dataloader.default_collate"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.nested_dictionary_dataset._unflatten", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.collater"], ["", "def", "collater", "(", "self", ",", "samples", ")", ":", "\n", "        ", "\"\"\"Merge a list of samples to form a mini-batch.\n\n        Args:\n            samples (List[dict]): samples to collate\n\n        Returns:\n            dict: a mini-batch suitable for forwarding with a Model\n        \"\"\"", "\n", "if", "len", "(", "samples", ")", "==", "0", ":", "\n", "            ", "return", "{", "}", "\n", "", "sample", "=", "OrderedDict", "(", ")", "\n", "for", "k", ",", "ds", "in", "self", ".", "defn", ".", "items", "(", ")", ":", "\n", "            ", "try", ":", "\n", "                ", "sample", "[", "k", "]", "=", "ds", ".", "collater", "(", "[", "s", "[", "k", "]", "for", "s", "in", "samples", "]", ")", "\n", "", "except", "NotImplementedError", ":", "\n", "                ", "sample", "[", "k", "]", "=", "default_collate", "(", "[", "s", "[", "k", "]", "for", "s", "in", "samples", "]", ")", "\n", "", "", "return", "_unflatten", "(", "sample", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.nested_dictionary_dataset.NestedDictionaryDataset.num_tokens": [[89, 93], ["max"], "methods", ["None"], ["", "def", "num_tokens", "(", "self", ",", "index", ")", ":", "\n", "        ", "\"\"\"Return the number of tokens in a sample. This value is used to\n        enforce ``--max-tokens`` during batching.\"\"\"", "\n", "return", "max", "(", "s", "[", "index", "]", "for", "s", "in", "self", ".", "sizes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.nested_dictionary_dataset.NestedDictionaryDataset.size": [[94, 101], ["len"], "methods", ["None"], ["", "def", "size", "(", "self", ",", "index", ")", ":", "\n", "        ", "\"\"\"Return an example's size as a float or tuple. This value is used when\n        filtering a dataset with ``--max-positions``.\"\"\"", "\n", "if", "len", "(", "self", ".", "sizes", ")", "==", "1", ":", "\n", "            ", "return", "self", ".", "sizes", "[", "0", "]", "[", "index", "]", "\n", "", "else", ":", "\n", "            ", "return", "(", "s", "[", "index", "]", "for", "s", "in", "self", ".", "sizes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.nested_dictionary_dataset.NestedDictionaryDataset.supports_prefetch": [[102, 106], ["any", "nested_dictionary_dataset.NestedDictionaryDataset.defn.values"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "supports_prefetch", "(", "self", ")", ":", "\n", "        ", "\"\"\"Whether this dataset supports prefetching.\"\"\"", "\n", "return", "any", "(", "ds", ".", "supports_prefetch", "for", "ds", "in", "self", ".", "defn", ".", "values", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.nested_dictionary_dataset.NestedDictionaryDataset.prefetch": [[107, 112], ["nested_dictionary_dataset.NestedDictionaryDataset.defn.values", "getattr", "ds.prefetch"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.block_pair_dataset.BlockPairDataset.prefetch"], ["", "def", "prefetch", "(", "self", ",", "indices", ")", ":", "\n", "        ", "\"\"\"Prefetch the data required for this epoch.\"\"\"", "\n", "for", "ds", "in", "self", ".", "defn", ".", "values", "(", ")", ":", "\n", "            ", "if", "getattr", "(", "ds", ",", "'supports_prefetch'", ",", "False", ")", ":", "\n", "                ", "ds", ".", "prefetch", "(", "indices", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.nested_dictionary_dataset.NestedDictionaryDataset.set_epoch": [[113, 117], ["super().set_epoch", "nested_dictionary_dataset.NestedDictionaryDataset.defn.values", "ds.set_epoch"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.nested_dictionary_dataset.NestedDictionaryDataset.set_epoch", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.nested_dictionary_dataset.NestedDictionaryDataset.set_epoch"], ["", "", "", "def", "set_epoch", "(", "self", ",", "epoch", ")", ":", "\n", "        ", "super", "(", ")", ".", "set_epoch", "(", "epoch", ")", "\n", "for", "ds", "in", "self", ".", "defn", ".", "values", "(", ")", ":", "\n", "            ", "ds", ".", "set_epoch", "(", "epoch", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.nested_dictionary_dataset._flatten": [[14, 29], ["collections.OrderedDict", "isinstance", "dico.items", "isinstance", "collections.OrderedDict.update", "enumerate", "collections.OrderedDict", "nested_dictionary_dataset._flatten", "collections.OrderedDict.update", "nested_dictionary_dataset._flatten", "str"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.nested_dictionary_dataset._flatten", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.nested_dictionary_dataset._flatten"], ["def", "_flatten", "(", "dico", ",", "prefix", "=", "None", ")", ":", "\n", "    ", "\"\"\"Flatten a nested dictionary.\"\"\"", "\n", "new_dico", "=", "OrderedDict", "(", ")", "\n", "if", "isinstance", "(", "dico", ",", "dict", ")", ":", "\n", "        ", "prefix", "=", "prefix", "+", "'.'", "if", "prefix", "is", "not", "None", "else", "''", "\n", "for", "k", ",", "v", "in", "dico", ".", "items", "(", ")", ":", "\n", "            ", "if", "v", "is", "None", ":", "\n", "                ", "continue", "\n", "", "new_dico", ".", "update", "(", "_flatten", "(", "v", ",", "prefix", "+", "k", ")", ")", "\n", "", "", "elif", "isinstance", "(", "dico", ",", "list", ")", ":", "\n", "        ", "for", "i", ",", "v", "in", "enumerate", "(", "dico", ")", ":", "\n", "            ", "new_dico", ".", "update", "(", "_flatten", "(", "v", ",", "prefix", "+", "'.['", "+", "str", "(", "i", ")", "+", "']'", ")", ")", "\n", "", "", "else", ":", "\n", "        ", "new_dico", "=", "OrderedDict", "(", "{", "prefix", ":", "dico", "}", ")", "\n", "", "return", "new_dico", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.nested_dictionary_dataset._unflatten": [[31, 45], ["collections.OrderedDict", "dico.items", "full_k.split.split", "int.startswith", "int.endswith", "int", "collections.OrderedDict"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["", "def", "_unflatten", "(", "dico", ")", ":", "\n", "    ", "\"\"\"Unflatten a flattened dictionary into a nested dictionary.\"\"\"", "\n", "new_dico", "=", "OrderedDict", "(", ")", "\n", "for", "full_k", ",", "v", "in", "dico", ".", "items", "(", ")", ":", "\n", "        ", "full_k", "=", "full_k", ".", "split", "(", "'.'", ")", "\n", "node", "=", "new_dico", "\n", "for", "k", "in", "full_k", "[", ":", "-", "1", "]", ":", "\n", "            ", "if", "k", ".", "startswith", "(", "'['", ")", "and", "k", ".", "endswith", "(", "']'", ")", ":", "\n", "                ", "k", "=", "int", "(", "k", "[", "1", ":", "-", "1", "]", ")", "\n", "", "if", "k", "not", "in", "node", ":", "\n", "                ", "node", "[", "k", "]", "=", "OrderedDict", "(", ")", "\n", "", "node", "=", "node", "[", "k", "]", "\n", "", "node", "[", "full_k", "[", "-", "1", "]", "]", "=", "v", "\n", "", "return", "new_dico", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.plasma_utils.PlasmaArray.__init__": [[18, 30], ["object.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["def", "__init__", "(", "self", ",", "array", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "array", "=", "array", "\n", "self", ".", "disable", "=", "array", ".", "nbytes", "<", "134217728", "# disable for arrays <128MB", "\n", "self", ".", "object_id", "=", "None", "\n", "self", ".", "path", "=", "None", "\n", "\n", "# variables with underscores shouldn't be pickled", "\n", "self", ".", "_client", "=", "None", "\n", "self", ".", "_server", "=", "None", "\n", "self", ".", "_server_tmp", "=", "None", "\n", "self", ".", "_plasma", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.plasma_utils.PlasmaArray.plasma": [[31, 40], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "plasma", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_plasma", "is", "None", "and", "not", "self", ".", "disable", ":", "\n", "            ", "try", ":", "\n", "                ", "import", "pyarrow", ".", "plasma", "as", "plasma", "\n", "self", ".", "_plasma", "=", "plasma", "\n", "", "except", "ImportError", ":", "\n", "                ", "self", ".", "_plasma", "=", "None", "\n", "", "", "return", "self", ".", "_plasma", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.plasma_utils.PlasmaArray.start_server": [[41, 52], ["tempfile.NamedTemporaryFile", "subprocess.Popen", "str", "int"], "methods", ["None"], ["", "def", "start_server", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "plasma", "is", "None", "or", "self", ".", "_server", "is", "not", "None", ":", "\n", "            ", "return", "\n", "", "assert", "self", ".", "object_id", "is", "None", "\n", "assert", "self", ".", "path", "is", "None", "\n", "self", ".", "_server_tmp", "=", "tempfile", ".", "NamedTemporaryFile", "(", ")", "\n", "self", ".", "path", "=", "self", ".", "_server_tmp", ".", "name", "\n", "self", ".", "_server", "=", "subprocess", ".", "Popen", "(", "[", "\n", "'plasma_store'", ",", "\n", "'-m'", ",", "str", "(", "int", "(", "1.05", "*", "self", ".", "array", ".", "nbytes", ")", ")", ",", "\n", "'-s'", ",", "self", ".", "path", ",", "\n", "]", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.plasma_utils.PlasmaArray.client": [[54, 60], ["plasma_utils.PlasmaArray.plasma.connect"], "methods", ["None"], ["", "@", "property", "\n", "def", "client", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_client", "is", "None", ":", "\n", "            ", "assert", "self", ".", "path", "is", "not", "None", "\n", "self", ".", "_client", "=", "self", ".", "plasma", ".", "connect", "(", "self", ".", "path", ")", "\n", "", "return", "self", ".", "_client", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.plasma_utils.PlasmaArray.__getstate__": [[61, 74], ["plasma_utils.PlasmaArray.__dict__.copy", "plasma_utils.PlasmaArray.start_server", "plasma_utils.PlasmaArray.client.put"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.copy", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.plasma_utils.PlasmaArray.start_server"], ["", "def", "__getstate__", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "plasma", "is", "None", ":", "\n", "            ", "return", "self", ".", "__dict__", "\n", "", "if", "self", ".", "object_id", "is", "None", ":", "\n", "            ", "self", ".", "start_server", "(", ")", "\n", "self", ".", "object_id", "=", "self", ".", "client", ".", "put", "(", "self", ".", "array", ")", "\n", "", "state", "=", "self", ".", "__dict__", ".", "copy", "(", ")", "\n", "del", "state", "[", "'array'", "]", "\n", "state", "[", "'_client'", "]", "=", "None", "\n", "state", "[", "'_server'", "]", "=", "None", "\n", "state", "[", "'_server_tmp'", "]", "=", "None", "\n", "state", "[", "'_plasma'", "]", "=", "None", "\n", "return", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.plasma_utils.PlasmaArray.__setstate__": [[75, 80], ["plasma_utils.PlasmaArray.__dict__.update", "plasma_utils.PlasmaArray.client.get"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "self", ".", "__dict__", ".", "update", "(", "state", ")", "\n", "if", "self", ".", "plasma", "is", "None", ":", "\n", "            ", "return", "\n", "", "self", ".", "array", "=", "self", ".", "client", ".", "get", "(", "self", ".", "object_id", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.plasma_utils.PlasmaArray.__del__": [[81, 87], ["plasma_utils.PlasmaArray._server.kill", "plasma_utils.PlasmaArray._server_tmp.close"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.GoldMiner.close"], ["", "def", "__del__", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_server", "is", "not", "None", ":", "\n", "            ", "self", ".", "_server", ".", "kill", "(", ")", "\n", "self", ".", "_server", "=", "None", "\n", "self", ".", "_server_tmp", ".", "close", "(", ")", "\n", "self", ".", "_server_tmp", "=", "None", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.concat_dataset.ConcatDataset.cumsum": [[15, 23], ["zip", "int", "r.append", "len"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["    ", "@", "staticmethod", "\n", "def", "cumsum", "(", "sequence", ",", "sample_ratios", ")", ":", "\n", "        ", "r", ",", "s", "=", "[", "]", ",", "0", "\n", "for", "e", ",", "ratio", "in", "zip", "(", "sequence", ",", "sample_ratios", ")", ":", "\n", "            ", "curr_len", "=", "int", "(", "ratio", "*", "len", "(", "e", ")", ")", "\n", "r", ".", "append", "(", "curr_len", "+", "s", ")", "\n", "s", "+=", "curr_len", "\n", "", "return", "r", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.concat_dataset.ConcatDataset.__init__": [[24, 33], ["FairseqDataset.__init__", "list", "isinstance", "concat_dataset.ConcatDataset.cumsum", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.concat_dataset.ConcatDataset.cumsum"], ["", "def", "__init__", "(", "self", ",", "datasets", ",", "sample_ratios", "=", "1", ")", ":", "\n", "        ", "super", "(", "ConcatDataset", ",", "self", ")", ".", "__init__", "(", ")", "\n", "assert", "len", "(", "datasets", ")", ">", "0", ",", "\"datasets should not be an empty iterable\"", "\n", "self", ".", "datasets", "=", "list", "(", "datasets", ")", "\n", "if", "isinstance", "(", "sample_ratios", ",", "int", ")", ":", "\n", "            ", "sample_ratios", "=", "[", "sample_ratios", "]", "*", "len", "(", "self", ".", "datasets", ")", "\n", "", "self", ".", "sample_ratios", "=", "sample_ratios", "\n", "self", ".", "cumulative_sizes", "=", "self", ".", "cumsum", "(", "self", ".", "datasets", ",", "sample_ratios", ")", "\n", "self", ".", "real_sizes", "=", "[", "len", "(", "d", ")", "for", "d", "in", "self", ".", "datasets", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.concat_dataset.ConcatDataset.__len__": [[34, 36], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "cumulative_sizes", "[", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.concat_dataset.ConcatDataset.__getitem__": [[37, 40], ["concat_dataset.ConcatDataset._get_dataset_and_sample_index"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.concat_dataset.ConcatDataset._get_dataset_and_sample_index"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "dataset_idx", ",", "sample_idx", "=", "self", ".", "_get_dataset_and_sample_index", "(", "idx", ")", "\n", "return", "self", ".", "datasets", "[", "dataset_idx", "]", "[", "sample_idx", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.concat_dataset.ConcatDataset._get_dataset_and_sample_index": [[41, 49], ["bisect.bisect_right"], "methods", ["None"], ["", "def", "_get_dataset_and_sample_index", "(", "self", ",", "idx", ":", "int", ")", ":", "\n", "        ", "dataset_idx", "=", "bisect", ".", "bisect_right", "(", "self", ".", "cumulative_sizes", ",", "idx", ")", "\n", "if", "dataset_idx", "==", "0", ":", "\n", "            ", "sample_idx", "=", "idx", "\n", "", "else", ":", "\n", "            ", "sample_idx", "=", "idx", "-", "self", ".", "cumulative_sizes", "[", "dataset_idx", "-", "1", "]", "\n", "", "sample_idx", "=", "sample_idx", "%", "self", ".", "real_sizes", "[", "dataset_idx", "]", "\n", "return", "dataset_idx", ",", "sample_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.concat_dataset.ConcatDataset.collater": [[50, 56], ["hasattr", "concat_dataset.ConcatDataset.datasets[].collater", "torch.utils.data.dataloader.default_collate"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.collater"], ["", "def", "collater", "(", "self", ",", "samples", ")", ":", "\n", "# For now only supports datasets with same underlying collater implementations", "\n", "        ", "if", "hasattr", "(", "self", ".", "datasets", "[", "0", "]", ",", "'collater'", ")", ":", "\n", "            ", "return", "self", ".", "datasets", "[", "0", "]", ".", "collater", "(", "samples", ")", "\n", "", "else", ":", "\n", "            ", "return", "default_collate", "(", "samples", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.concat_dataset.ConcatDataset.size": [[57, 63], ["concat_dataset.ConcatDataset._get_dataset_and_sample_index", "concat_dataset.ConcatDataset.datasets[].size"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.concat_dataset.ConcatDataset._get_dataset_and_sample_index", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "", "def", "size", "(", "self", ",", "idx", ":", "int", ")", ":", "\n", "        ", "\"\"\"\n        Return an example's size as a float or tuple.\n        \"\"\"", "\n", "dataset_idx", ",", "sample_idx", "=", "self", ".", "_get_dataset_and_sample_index", "(", "idx", ")", "\n", "return", "self", ".", "datasets", "[", "dataset_idx", "]", ".", "size", "(", "sample_idx", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.concat_dataset.ConcatDataset.num_tokens": [[64, 66], ["numpy.max", "concat_dataset.ConcatDataset.size"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "num_tokens", "(", "self", ",", "index", ":", "int", ")", ":", "\n", "        ", "return", "np", ".", "max", "(", "self", ".", "size", "(", "index", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.concat_dataset.ConcatDataset.sizes": [[67, 71], ["numpy.concatenate", "numpy.tile", "zip"], "methods", ["None"], ["", "@", "property", "\n", "def", "sizes", "(", "self", ")", ":", "\n", "        ", "return", "np", ".", "concatenate", "(", "\n", "[", "np", ".", "tile", "(", "ds", ".", "sizes", ",", "sr", ")", "for", "ds", ",", "sr", "in", "zip", "(", "self", ".", "datasets", ",", "self", ".", "sample_ratios", ")", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.concat_dataset.ConcatDataset.supports_prefetch": [[73, 76], ["all"], "methods", ["None"], ["", "@", "property", "\n", "def", "supports_prefetch", "(", "self", ")", ":", "\n", "        ", "return", "all", "(", "d", ".", "supports_prefetch", "for", "d", "in", "self", ".", "datasets", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.concat_dataset.ConcatDataset.ordered_indices": [[77, 82], ["numpy.argsort"], "methods", ["None"], ["", "def", "ordered_indices", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Returns indices sorted by length. So less padding is needed.\n        \"\"\"", "\n", "return", "np", ".", "argsort", "(", "self", ".", "sizes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.concat_dataset.ConcatDataset.prefetch": [[83, 90], ["zip", "len", "getattr", "ds.prefetch"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.block_pair_dataset.BlockPairDataset.prefetch"], ["", "def", "prefetch", "(", "self", ",", "indices", ")", ":", "\n", "        ", "frm", "=", "0", "\n", "for", "to", ",", "ds", "in", "zip", "(", "self", ".", "cumulative_sizes", ",", "self", ".", "datasets", ")", ":", "\n", "            ", "real_size", "=", "len", "(", "ds", ")", "\n", "if", "getattr", "(", "ds", ",", "'supports_prefetch'", ",", "False", ")", ":", "\n", "                ", "ds", ".", "prefetch", "(", "[", "(", "i", "-", "frm", ")", "%", "real_size", "for", "i", "in", "indices", "if", "frm", "<=", "i", "<", "to", "]", ")", "\n", "", "frm", "=", "to", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.language_pair_dataset.LanguagePairDataset.__init__": [[214, 265], ["numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "src_dict.pad", "tgt_dict.pad", "src_dict.eos", "tgt_dict.eos", "src_dict.unk", "tgt_dict.unk"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.unk", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.unk"], ["def", "__init__", "(", "\n", "self", ",", "src", ",", "src_sizes", ",", "src_dict", ",", "\n", "src_fix_emb", ",", "src_fix_emb_sizes", ",", "\n", "src_wordpieces", ",", "src_wordpieces_sizes", ",", "\n", "src_wp2w", ",", "src_wp2w_sizes", ",", "\n", "tgt", ",", "tgt_sizes", ",", "tgt_dict", ",", "\n", "memory", ",", "memory_sizes", ",", "\n", "mem_pos", ",", "mem_pos_sizes", ",", "\n", "target_masks", ",", "target_masks_sizes", ",", "\n", "active_logits", ",", "active_logits_sizes", ",", "\n", "left_pad_source", "=", "True", ",", "left_pad_target", "=", "False", ",", "\n", "max_source_positions", "=", "1024", ",", "max_target_positions", "=", "1024", ",", "\n", "shuffle", "=", "True", ",", "input_feeding", "=", "True", ",", "remove_eos_from_source", "=", "False", ",", "append_eos_to_target", "=", "False", ",", "\n", "state_machine", "=", "True", "\n", ")", ":", "\n", "        ", "if", "tgt_dict", "is", "not", "None", ":", "\n", "            ", "assert", "src_dict", ".", "pad", "(", ")", "==", "tgt_dict", ".", "pad", "(", ")", "\n", "assert", "src_dict", ".", "eos", "(", ")", "==", "tgt_dict", ".", "eos", "(", ")", "\n", "assert", "src_dict", ".", "unk", "(", ")", "==", "tgt_dict", ".", "unk", "(", ")", "\n", "", "self", ".", "src", "=", "src", "\n", "self", ".", "tgt", "=", "tgt", "\n", "self", ".", "src_sizes", "=", "np", ".", "array", "(", "src_sizes", ")", "\n", "self", ".", "tgt_sizes", "=", "np", ".", "array", "(", "tgt_sizes", ")", "if", "tgt_sizes", "is", "not", "None", "else", "None", "\n", "self", ".", "src_dict", "=", "src_dict", "\n", "self", ".", "tgt_dict", "=", "tgt_dict", "\n", "# dataset variables", "\n", "self", ".", "src_fix_emb", "=", "src_fix_emb", "\n", "self", ".", "src_fix_emb_sizes", "=", "src_fix_emb_sizes", "\n", "self", ".", "src_wordpieces", "=", "src_wordpieces", "\n", "self", ".", "src_wordpieces_sizes", "=", "src_wordpieces_sizes", "\n", "self", ".", "src_wp2w", "=", "src_wp2w", "\n", "self", ".", "src_wp2w_sizes", "=", "src_wp2w_sizes", "\n", "self", ".", "memory", "=", "memory", "\n", "self", ".", "mem_pos", "=", "mem_pos", "\n", "self", ".", "memory_sizes", "=", "np", ".", "array", "(", "memory_sizes", ")", "\n", "self", ".", "mem_pos_sizes", "=", "np", ".", "array", "(", "mem_pos_sizes", ")", "\n", "self", ".", "target_masks", "=", "target_masks", "\n", "self", ".", "target_masks_sizes", "=", "np", ".", "array", "(", "target_masks_sizes", ")", "\n", "self", ".", "active_logits", "=", "active_logits", "\n", "self", ".", "active_logits_sizes", "=", "np", ".", "array", "(", "active_logits_sizes", ")", "\n", "# other", "\n", "self", ".", "left_pad_source", "=", "left_pad_source", "\n", "self", ".", "left_pad_target", "=", "left_pad_target", "\n", "self", ".", "max_source_positions", "=", "max_source_positions", "\n", "self", ".", "max_target_positions", "=", "max_target_positions", "\n", "self", ".", "shuffle", "=", "shuffle", "\n", "self", ".", "input_feeding", "=", "input_feeding", "\n", "self", ".", "remove_eos_from_source", "=", "remove_eos_from_source", "\n", "self", ".", "append_eos_to_target", "=", "append_eos_to_target", "\n", "# compute or not state of state machine", "\n", "self", ".", "state_machine", "=", "state_machine", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.language_pair_dataset.LanguagePairDataset.__getitem__": [[266, 313], ["language_pair_dataset.LanguagePairDataset.src_fix_emb[].view", "language_pair_dataset.LanguagePairDataset.memory[].view().transpose", "language_pair_dataset.LanguagePairDataset.mem_pos[].view().transpose", "memory_item.type.type.type", "memory_pos_item.type.type.type", "target_masks.type.type.type", "active_logits.type.type.type", "language_pair_dataset.LanguagePairDataset.type", "language_pair_dataset.LanguagePairDataset.type", "language_pair_dataset.LanguagePairDataset.type", "language_pair_dataset.LanguagePairDataset.type", "language_pair_dataset.LanguagePairDataset.src_dict.eos", "language_pair_dataset.LanguagePairDataset.memory[].view", "language_pair_dataset.LanguagePairDataset.mem_pos[].view", "language_pair_dataset.LanguagePairDataset.tgt_dict.eos", "language_pair_dataset.LanguagePairDataset.src_dict.eos", "torch.cat", "torch.LongTensor"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "tgt_item", "=", "self", ".", "tgt", "[", "index", "]", "if", "self", ".", "tgt", "is", "not", "None", "else", "None", "\n", "src_item", "=", "self", ".", "src", "[", "index", "]", "\n", "\n", "# Deduce pretrained embeddings size", "\n", "pretrained_embed_dim", "=", "self", ".", "src_fix_emb", "[", "index", "]", ".", "shape", "[", "0", "]", "//", "src_item", ".", "shape", "[", "0", "]", "\n", "shape_factor", "=", "(", "self", ".", "src_fix_emb", "[", "index", "]", ".", "shape", "[", "0", "]", "//", "pretrained_embed_dim", ",", "pretrained_embed_dim", ")", "\n", "src_fix_emb_item", "=", "self", ".", "src_fix_emb", "[", "index", "]", ".", "view", "(", "*", "shape_factor", ")", "\n", "src_wordpieces_item", "=", "self", ".", "src_wordpieces", "[", "index", "]", "\n", "src_wp2w_item", "=", "self", ".", "src_wp2w", "[", "index", "]", "\n", "shape_factor", "=", "(", "tgt_item", ".", "shape", "[", "0", "]", ",", "src_item", ".", "shape", "[", "0", "]", ")", "\n", "memory_item", "=", "self", ".", "memory", "[", "index", "]", ".", "view", "(", "*", "shape_factor", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "memory_pos_item", "=", "self", ".", "mem_pos", "[", "index", "]", ".", "view", "(", "*", "shape_factor", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "target_masks", "=", "self", ".", "target_masks", "[", "index", "]", "\n", "active_logits", "=", "self", ".", "active_logits", "[", "index", "]", "\n", "\n", "# Cast to float to simplify mask manipulation", "\n", "memory_item", "=", "memory_item", ".", "type", "(", "src_fix_emb_item", ".", "type", "(", ")", ")", "\n", "memory_pos_item", "=", "memory_pos_item", ".", "type", "(", "src_fix_emb_item", ".", "type", "(", ")", ")", "\n", "target_masks", "=", "target_masks", ".", "type", "(", "src_fix_emb_item", ".", "type", "(", ")", ")", "\n", "active_logits", "=", "active_logits", ".", "type", "(", "src_fix_emb_item", ".", "type", "(", ")", ")", "\n", "\n", "# Append EOS to end of tgt sentence if it does not have an EOS and remove", "\n", "# EOS from end of src sentence if it exists. This is useful when we use", "\n", "# use existing datasets for opposite directions i.e., when we want to", "\n", "# use tgt_dataset as src_dataset and vice versa", "\n", "if", "self", ".", "append_eos_to_target", ":", "\n", "            ", "eos", "=", "self", ".", "tgt_dict", ".", "eos", "(", ")", "if", "self", ".", "tgt_dict", "else", "self", ".", "src_dict", ".", "eos", "(", ")", "\n", "if", "self", ".", "tgt", "and", "self", ".", "tgt", "[", "index", "]", "[", "-", "1", "]", "!=", "eos", ":", "\n", "                ", "tgt_item", "=", "torch", ".", "cat", "(", "[", "self", ".", "tgt", "[", "index", "]", ",", "torch", ".", "LongTensor", "(", "[", "eos", "]", ")", "]", ")", "\n", "\n", "", "", "if", "self", ".", "remove_eos_from_source", ":", "\n", "            ", "eos", "=", "self", ".", "src_dict", ".", "eos", "(", ")", "\n", "if", "self", ".", "src", "[", "index", "]", "[", "-", "1", "]", "==", "eos", ":", "\n", "                ", "src_item", "=", "self", ".", "src", "[", "index", "]", "[", ":", "-", "1", "]", "\n", "\n", "", "", "return", "{", "\n", "'id'", ":", "index", ",", "\n", "'source'", ":", "src_item", ",", "\n", "'source_fix_emb'", ":", "src_fix_emb_item", ",", "\n", "'src_wordpieces'", ":", "src_wordpieces_item", ",", "\n", "'src_wp2w'", ":", "src_wp2w_item", ",", "\n", "'target'", ":", "tgt_item", ",", "\n", "'memory'", ":", "memory_item", ",", "\n", "'memory_pos'", ":", "memory_pos_item", ",", "\n", "'target_masks'", ":", "target_masks", ",", "\n", "'active_logits'", ":", "active_logits", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.language_pair_dataset.LanguagePairDataset.__len__": [[315, 317], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "src", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.language_pair_dataset.LanguagePairDataset.collater": [[318, 352], ["language_pair_dataset.collate", "language_pair_dataset.LanguagePairDataset.src_dict.pad", "language_pair_dataset.LanguagePairDataset.src_dict.eos"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.language_pair_dataset.collate", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos"], ["", "def", "collater", "(", "self", ",", "samples", ")", ":", "\n", "        ", "\"\"\"Merge a list of samples to form a mini-batch.\n\n        Args:\n            samples (List[dict]): samples to collate\n\n        Returns:\n            dict: a mini-batch with the following keys:\n\n                - `id` (LongTensor): example IDs in the original input order\n                - `ntokens` (int): total number of tokens in the batch\n                - `net_input` (dict): the input to the Model, containing keys:\n\n                  - `src_tokens` (LongTensor): a padded 2D Tensor of tokens in\n                    the source sentence of shape `(bsz, src_len)`. Padding will\n                    appear on the left if *left_pad_source* is ``True``.\n                  - `src_lengths` (LongTensor): 1D Tensor of the unpadded\n                    lengths of each source sentence of shape `(bsz)`\n                  - `prev_output_tokens` (LongTensor): a padded 2D Tensor of\n                    tokens in the target sentence, shifted right by one\n                    position for teacher forcing, of shape `(bsz, tgt_len)`.\n                    This key will not be present if *input_feeding* is\n                    ``False``.  Padding will appear on the left if\n                    *left_pad_target* is ``True``.\n\n                - `target` (LongTensor): a padded 2D Tensor of tokens in the\n                  target sentence of shape `(bsz, tgt_len)`. Padding will appear\n                  on the left if *left_pad_target* is ``True``.\n        \"\"\"", "\n", "return", "collate", "(", "\n", "samples", ",", "pad_idx", "=", "self", ".", "src_dict", ".", "pad", "(", ")", ",", "eos_idx", "=", "self", ".", "src_dict", ".", "eos", "(", ")", ",", "\n", "left_pad_source", "=", "self", ".", "left_pad_source", ",", "left_pad_target", "=", "self", ".", "left_pad_target", ",", "\n", "input_feeding", "=", "self", ".", "input_feeding", ",", "\n", "state_machine", "=", "self", ".", "state_machine", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.language_pair_dataset.LanguagePairDataset.num_tokens": [[354, 358], ["max"], "methods", ["None"], ["", "def", "num_tokens", "(", "self", ",", "index", ")", ":", "\n", "        ", "\"\"\"Return the number of tokens in a sample. This value is used to\n        enforce ``--max-tokens`` during batching.\"\"\"", "\n", "return", "max", "(", "self", ".", "src_sizes", "[", "index", "]", ",", "self", ".", "tgt_sizes", "[", "index", "]", "if", "self", ".", "tgt_sizes", "is", "not", "None", "else", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.language_pair_dataset.LanguagePairDataset.size": [[359, 363], ["None"], "methods", ["None"], ["", "def", "size", "(", "self", ",", "index", ")", ":", "\n", "        ", "\"\"\"Return an example's size as a float or tuple. This value is used when\n        filtering a dataset with ``--max-positions``.\"\"\"", "\n", "return", "(", "self", ".", "src_sizes", "[", "index", "]", ",", "self", ".", "tgt_sizes", "[", "index", "]", "if", "self", ".", "tgt_sizes", "is", "not", "None", "else", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.language_pair_dataset.LanguagePairDataset.ordered_indices": [[364, 374], ["numpy.random.permutation", "numpy.arange", "len", "len", "numpy.argsort", "numpy.argsort"], "methods", ["None"], ["", "def", "ordered_indices", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return an ordered list of indices. Batches will be constructed based\n        on this order.\"\"\"", "\n", "if", "self", ".", "shuffle", ":", "\n", "            ", "indices", "=", "np", ".", "random", ".", "permutation", "(", "len", "(", "self", ")", ")", "\n", "", "else", ":", "\n", "            ", "indices", "=", "np", ".", "arange", "(", "len", "(", "self", ")", ")", "\n", "", "if", "self", ".", "tgt_sizes", "is", "not", "None", ":", "\n", "            ", "indices", "=", "indices", "[", "np", ".", "argsort", "(", "self", ".", "tgt_sizes", "[", "indices", "]", ",", "kind", "=", "'mergesort'", ")", "]", "\n", "", "return", "indices", "[", "np", ".", "argsort", "(", "self", ".", "src_sizes", "[", "indices", "]", ",", "kind", "=", "'mergesort'", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.language_pair_dataset.LanguagePairDataset.supports_prefetch": [[375, 383], ["getattr", "getattr", "getattr", "getattr", "getattr"], "methods", ["None"], ["", "@", "property", "\n", "def", "supports_prefetch", "(", "self", ")", ":", "\n", "        ", "return", "(", "\n", "getattr", "(", "self", ".", "src", ",", "'supports_prefetch'", ",", "False", ")", "\n", "and", "getattr", "(", "self", ".", "src_fix_emb", ",", "'supports_prefetch'", ",", "False", ")", "\n", "and", "(", "getattr", "(", "self", ".", "tgt", ",", "'supports_prefetch'", ",", "False", ")", "or", "self", ".", "tgt", "is", "None", ")", "\n", "and", "getattr", "(", "self", ".", "memory", ",", "'supports_prefetch'", ",", "False", ")", "\n", "and", "getattr", "(", "self", ".", "mem_pos", ",", "'supports_prefetch'", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.language_pair_dataset.LanguagePairDataset.prefetch": [[386, 393], ["language_pair_dataset.LanguagePairDataset.src.prefetch", "language_pair_dataset.LanguagePairDataset.src_fix_emb.prefetch", "language_pair_dataset.LanguagePairDataset.memory.prefetch", "language_pair_dataset.LanguagePairDataset.mem_pos.prefetch", "language_pair_dataset.LanguagePairDataset.tgt.prefetch"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.block_pair_dataset.BlockPairDataset.prefetch", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.block_pair_dataset.BlockPairDataset.prefetch", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.block_pair_dataset.BlockPairDataset.prefetch", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.block_pair_dataset.BlockPairDataset.prefetch", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.block_pair_dataset.BlockPairDataset.prefetch"], ["", "def", "prefetch", "(", "self", ",", "indices", ")", ":", "\n", "        ", "self", ".", "src", ".", "prefetch", "(", "indices", ")", "\n", "if", "self", ".", "tgt", "is", "not", "None", ":", "\n", "            ", "self", ".", "tgt", ".", "prefetch", "(", "indices", ")", "\n", "", "self", ".", "src_fix_emb", ".", "prefetch", "(", "indices", ")", "\n", "self", ".", "memory", ".", "prefetch", "(", "indices", ")", "\n", "self", ".", "mem_pos", ".", "prefetch", "(", "indices", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.language_pair_dataset.collate": [[18, 183], ["torch.LongTensor", "language_pair_dataset.collate.merge"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.translation_moe.score.merge"], ["def", "collate", "(", "\n", "samples", ",", "pad_idx", ",", "eos_idx", ",", "left_pad_source", "=", "True", ",", "left_pad_target", "=", "False", ",", "\n", "input_feeding", "=", "True", ",", "state_machine", "=", "True", "\n", ")", ":", "\n", "    ", "if", "len", "(", "samples", ")", "==", "0", ":", "\n", "        ", "return", "{", "}", "\n", "\n", "", "def", "merge", "(", "key", ",", "left_pad", ",", "move_eos_to_beginning", "=", "False", ")", ":", "\n", "        ", "return", "data_utils", ".", "collate_tokens", "(", "\n", "[", "s", "[", "key", "]", "for", "s", "in", "samples", "]", ",", "\n", "pad_idx", ",", "eos_idx", ",", "left_pad", ",", "move_eos_to_beginning", ",", "\n", ")", "\n", "\n", "", "id", "=", "torch", ".", "LongTensor", "(", "[", "s", "[", "'id'", "]", "for", "s", "in", "samples", "]", ")", "\n", "src_tokens", "=", "merge", "(", "'source'", ",", "left_pad", "=", "left_pad_source", ")", "\n", "# sort by descending source length", "\n", "src_lengths", "=", "torch", ".", "LongTensor", "(", "[", "s", "[", "'source'", "]", ".", "numel", "(", ")", "for", "s", "in", "samples", "]", ")", "\n", "src_lengths", ",", "sort_order", "=", "src_lengths", ".", "sort", "(", "descending", "=", "True", ")", "\n", "id", "=", "id", ".", "index_select", "(", "0", ",", "sort_order", ")", "\n", "src_tokens", "=", "src_tokens", ".", "index_select", "(", "0", ",", "sort_order", ")", "\n", "\n", "prev_output_tokens", "=", "None", "\n", "target", "=", "None", "\n", "if", "samples", "[", "0", "]", ".", "get", "(", "'target'", ",", "None", ")", "is", "not", "None", ":", "\n", "        ", "target", "=", "merge", "(", "'target'", ",", "left_pad", "=", "left_pad_target", ")", "\n", "# we will need for sanity check furtehr down", "\n", "no_sorted_target", "=", "target", ".", "clone", "(", ")", "\n", "target", "=", "target", ".", "index_select", "(", "0", ",", "sort_order", ")", "\n", "# needed for sanity checks", "\n", "tgt_legths", "=", "torch", ".", "LongTensor", "(", "[", "len", "(", "s", "[", "'target'", "]", ")", "for", "s", "in", "samples", "]", ")", "\n", "ntokens", "=", "tgt_legths", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "tgt_legths", "=", "tgt_legths", ".", "index_select", "(", "0", ",", "sort_order", ")", "\n", "\n", "if", "input_feeding", ":", "\n", "# we create a shifted version of targets for feeding the", "\n", "# previous output token(s) into the next decoder step", "\n", "            ", "prev_output_tokens", "=", "merge", "(", "\n", "'target'", ",", "\n", "left_pad", "=", "left_pad_target", ",", "\n", "move_eos_to_beginning", "=", "True", ",", "\n", ")", "\n", "prev_output_tokens", "=", "prev_output_tokens", ".", "index_select", "(", "0", ",", "sort_order", ")", "\n", "", "", "else", ":", "\n", "        ", "ntokens", "=", "sum", "(", "len", "(", "s", "[", "'source'", "]", ")", "for", "s", "in", "samples", ")", "\n", "\n", "", "if", "samples", "[", "0", "]", ".", "get", "(", "'orig_tokens'", ",", "None", ")", "is", "not", "None", ":", "\n", "        ", "batch_orig_tokens", "=", "[", "s", "[", "\"orig_tokens\"", "]", "for", "s", "in", "samples", "]", "\n", "orig_tokens", "=", "[", "]", "\n", "for", "index", "in", "sort_order", ":", "\n", "            ", "orig_tokens", ".", "append", "(", "batch_orig_tokens", "[", "index", "]", ")", "\n", "", "", "else", ":", "\n", "        ", "orig_tokens", "=", "None", "\n", "\n", "# Pre-trained embeddings", "\n", "", "def", "merge_embeddings", "(", "key", ",", "left_pad_source", ",", "move_eos_to_beginning", ")", ":", "\n", "        ", "return", "collate_embeddings", "(", "\n", "[", "s", "[", "key", "]", "for", "s", "in", "samples", "]", ",", "\n", "pad_idx", ",", "eos_idx", ",", "left_pad_source", ",", "move_eos_to_beginning", "\n", ")", "\n", "\n", "", "source_fix_emb", "=", "merge_embeddings", "(", "\n", "'source_fix_emb'", ",", "\n", "left_pad_source", ",", "\n", "False", "\n", "#left_pad_target", "\n", ")", "\n", "source_fix_emb", "=", "source_fix_emb", ".", "index_select", "(", "0", ",", "sort_order", ")", "\n", "\n", "# Word-pieces", "\n", "src_wordpieces", "=", "merge", "(", "'src_wordpieces'", ",", "left_pad", "=", "left_pad_source", ")", "\n", "src_wordpieces", "=", "src_wordpieces", ".", "index_select", "(", "0", ",", "sort_order", ")", "\n", "\n", "def", "merge_wp_idx", "(", "key", ",", "left_pad", ",", "move_eos_to_beginning", "=", "False", ")", ":", "\n", "        ", "return", "collate_wp_idx", "(", "\n", "[", "s", "[", "key", "]", "for", "s", "in", "samples", "]", ",", "\n", "pad_idx", ",", "eos_idx", ",", "left_pad", ",", "move_eos_to_beginning", ",", "\n", "reverse", "=", "True", "\n", ")", "\n", "\n", "# Wordpiece to word mapping", "\n", "", "src_wp2w", "=", "merge_wp_idx", "(", "'src_wp2w'", ",", "left_pad", "=", "left_pad_source", ")", "\n", "src_wp2w", "=", "src_wp2w", ".", "index_select", "(", "0", ",", "sort_order", ")", "\n", "\n", "#     # DEBUG: Inline RoBERTa", "\n", "#     from torch_scatter import scatter_mean", "\n", "#     # extract roberta from collated", "\n", "#     roberta = torch.hub.load('pytorch/fairseq', 'roberta.base')", "\n", "#     roberta.eval()", "\n", "#     last_layer = roberta.extract_features(src_wordpieces)", "\n", "#     # remove sentence start", "\n", "#     bsize, max_len, emb_size = last_layer.shape", "\n", "#     mask = (src_wordpieces != 0).unsqueeze(2).expand(last_layer.shape)", "\n", "#     last_layer = last_layer[mask].view((bsize, max_len - 1, emb_size))", "\n", "#     # remove sentence end", "\n", "#     last_layer = last_layer[:, :-1, :]", "\n", "#     # apply scatter, flip before to have left-side padding", "\n", "#     source_fix_emb2 = scatter_mean(last_layer, src_wp2w.unsqueeze(2), dim=1)", "\n", "#     source_fix_emb2 = source_fix_emb2.flip(1)", "\n", "#     # Remove extra padding", "\n", "#     source_fix_emb2 = source_fix_emb2[:, -src_tokens.shape[1]:, :]", "\n", "#     abs(source_fix_emb2 - source_fix_emb).max()", "\n", "#     # DEBUG: Inline RoBERTa", "\n", "\n", "# source masks", "\n", "def", "merge_masks", "(", "key", ",", "left_pad_source", ",", "left_pad_target", ")", ":", "\n", "        ", "return", "collate_masks", "(", "\n", "[", "s", "[", "key", "]", "for", "s", "in", "samples", "]", ",", "\n", "pad_idx", ",", "eos_idx", ",", "left_pad_source", ",", "left_pad_target", "\n", ")", "\n", "\n", "# target masks", "\n", "# get sub-set of active logits for this batch and mask for each individual", "\n", "# sentence and target time step ", "\n", "", "def", "merge_target_masks", "(", "left_pad_target", ")", ":", "\n", "        ", "return", "collate_target_masks", "(", "\n", "[", "(", "s", "[", "'target_masks'", "]", ",", "s", "[", "'active_logits'", "]", ",", "len", "(", "s", "[", "'target'", "]", ")", ")", "for", "s", "in", "samples", "]", ",", "\n", "pad_idx", ",", "eos_idx", ",", "left_pad_target", "=", "left_pad_target", ",", "move_eos_to_beginning", "=", "False", ",", "\n", "target", "=", "no_sorted_target", "\n", ")", "\n", "\n", "", "if", "state_machine", ":", "\n", "\n", "# stack info", "\n", "        ", "memory", "=", "merge_masks", "(", "'memory'", ",", "left_pad_source", ",", "left_pad_target", ")", "\n", "memory", "=", "memory", ".", "index_select", "(", "0", ",", "sort_order", ")", "\n", "memory_pos", "=", "merge_masks", "(", "'memory_pos'", ",", "left_pad_source", ",", "left_pad_target", ")", "\n", "memory_pos", "=", "memory_pos", ".", "index_select", "(", "0", ",", "sort_order", ")", "\n", "# active logits ", "\n", "logits_mask", ",", "logits_indices", "=", "merge_target_masks", "(", "left_pad_target", ")", "\n", "logits_mask", "=", "logits_mask", ".", "index_select", "(", "0", ",", "sort_order", ")", "\n", "\n", "", "else", ":", "\n", "\n", "        ", "memory", "=", "None", "\n", "memory_pos", "=", "None", "\n", "logits_indices", "=", "None", "\n", "logits_mask", "=", "None", "\n", "\n", "# batch variables", "\n", "", "batch", "=", "{", "\n", "'id'", ":", "id", ",", "\n", "'nsentences'", ":", "len", "(", "samples", ")", ",", "\n", "'ntokens'", ":", "ntokens", ",", "\n", "'net_input'", ":", "{", "\n", "'src_tokens'", ":", "src_tokens", ",", "\n", "'src_lengths'", ":", "src_lengths", ",", "\n", "'source_fix_emb'", ":", "source_fix_emb", ",", "\n", "'src_wordpieces'", ":", "src_wordpieces", ",", "\n", "'src_wp2w'", ":", "src_wp2w", ",", "\n", "'memory'", ":", "memory", ",", "\n", "'memory_pos'", ":", "memory_pos", ",", "\n", "'logits_mask'", ":", "logits_mask", ",", "\n", "'logits_indices'", ":", "logits_indices", ",", "\n", "'orig_tokens'", ":", "orig_tokens", "\n", "}", ",", "\n", "'target'", ":", "target", ",", "\n", "}", "\n", "if", "prev_output_tokens", "is", "not", "None", ":", "\n", "        ", "batch", "[", "'net_input'", "]", "[", "'prev_output_tokens'", "]", "=", "prev_output_tokens", "\n", "\n", "# sanity check batch", "\n", "# from fairseq.debug_tools import sanity_check_collated_batch", "\n", "# sanity_check_collated_batch(batch, pad_idx, left_pad_source, left_pad_target, tgt_legths)", "\n", "\n", "", "return", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.numel_dataset.NumelDataset.__init__": [[14, 17], ["BaseWrapperDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dataset", ",", "reduce", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dataset", ")", "\n", "self", ".", "reduce", "=", "reduce", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.numel_dataset.NumelDataset.__getitem__": [[18, 24], ["torch.is_tensor", "torch.numel", "numpy.size"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "item", "=", "self", ".", "dataset", "[", "index", "]", "\n", "if", "torch", ".", "is_tensor", "(", "item", ")", ":", "\n", "            ", "return", "torch", ".", "numel", "(", "item", ")", "\n", "", "else", ":", "\n", "            ", "return", "np", ".", "size", "(", "item", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.numel_dataset.NumelDataset.__len__": [[25, 27], ["len"], "methods", ["None"], ["", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "dataset", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.numel_dataset.NumelDataset.collater": [[28, 33], ["sum", "torch.tensor"], "methods", ["None"], ["", "def", "collater", "(", "self", ",", "samples", ")", ":", "\n", "        ", "if", "self", ".", "reduce", ":", "\n", "            ", "return", "sum", "(", "samples", ")", "\n", "", "else", ":", "\n", "            ", "return", "torch", ".", "tensor", "(", "samples", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.round_robin_zip_datasets.RoundRobinZipDatasets.__init__": [[26, 41], ["FairseqDataset.__init__", "isinstance", "datasets.items", "isinstance", "len", "len"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["def", "__init__", "(", "self", ",", "datasets", ",", "eval_key", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "isinstance", "(", "datasets", ",", "OrderedDict", ")", "\n", "self", ".", "datasets", "=", "datasets", "\n", "self", ".", "eval_key", "=", "eval_key", "\n", "\n", "self", ".", "longest_dataset", "=", "None", "\n", "self", ".", "longest_dataset_key", "=", "None", "\n", "for", "key", ",", "dataset", "in", "datasets", ".", "items", "(", ")", ":", "\n", "            ", "assert", "isinstance", "(", "dataset", ",", "FairseqDataset", ")", "\n", "if", "self", ".", "longest_dataset", "is", "None", "or", "len", "(", "dataset", ")", ">", "len", "(", "self", ".", "longest_dataset", ")", ":", "\n", "                ", "self", ".", "longest_dataset", "=", "dataset", "\n", "self", ".", "longest_dataset_key", "=", "key", "\n", "\n", "", "", "self", ".", "_ordered_indices", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.round_robin_zip_datasets.RoundRobinZipDatasets._map_index": [[42, 46], ["len"], "methods", ["None"], ["", "def", "_map_index", "(", "self", ",", "key", ",", "index", ")", ":", "\n", "        ", "assert", "self", ".", "_ordered_indices", "is", "not", "None", ",", "'Must call RoundRobinZipDatasets.ordered_indices() first'", "\n", "return", "self", ".", "_ordered_indices", "[", "key", "]", "[", "index", "%", "len", "(", "self", ".", "datasets", "[", "key", "]", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.round_robin_zip_datasets.RoundRobinZipDatasets.__getitem__": [[47, 56], ["collections.OrderedDict", "round_robin_zip_datasets.RoundRobinZipDatasets._map_index", "round_robin_zip_datasets.RoundRobinZipDatasets.datasets.items", "round_robin_zip_datasets.RoundRobinZipDatasets._map_index"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.round_robin_zip_datasets.RoundRobinZipDatasets._map_index", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.round_robin_zip_datasets.RoundRobinZipDatasets._map_index"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "if", "self", ".", "eval_key", "is", "None", ":", "\n", "            ", "return", "OrderedDict", "(", "[", "\n", "(", "key", ",", "dataset", "[", "self", ".", "_map_index", "(", "key", ",", "index", ")", "]", ")", "\n", "for", "key", ",", "dataset", "in", "self", ".", "datasets", ".", "items", "(", ")", "\n", "]", ")", "\n", "", "else", ":", "\n", "# at evaluation time it's useful to pass-through batches from a single key", "\n", "            ", "return", "self", ".", "datasets", "[", "self", ".", "eval_key", "]", "[", "self", ".", "_map_index", "(", "self", ".", "eval_key", ",", "index", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.round_robin_zip_datasets.RoundRobinZipDatasets.__len__": [[57, 59], ["len"], "methods", ["None"], ["", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "longest_dataset", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.round_robin_zip_datasets.RoundRobinZipDatasets.collater": [[60, 72], ["len", "collections.OrderedDict", "round_robin_zip_datasets.RoundRobinZipDatasets.datasets[].collater", "dataset.collater", "round_robin_zip_datasets.RoundRobinZipDatasets.datasets.items"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.collater", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.collater"], ["", "def", "collater", "(", "self", ",", "samples", ")", ":", "\n", "        ", "\"\"\"Merge a list of samples to form a mini-batch.\"\"\"", "\n", "if", "len", "(", "samples", ")", "==", "0", ":", "\n", "            ", "return", "None", "\n", "", "if", "self", ".", "eval_key", "is", "None", ":", "\n", "            ", "return", "OrderedDict", "(", "[", "\n", "(", "key", ",", "dataset", ".", "collater", "(", "[", "sample", "[", "key", "]", "for", "sample", "in", "samples", "]", ")", ")", "\n", "for", "key", ",", "dataset", "in", "self", ".", "datasets", ".", "items", "(", ")", "\n", "]", ")", "\n", "", "else", ":", "\n", "# at evaluation time it's useful to pass-through batches from a single key", "\n", "            ", "return", "self", ".", "datasets", "[", "self", ".", "eval_key", "]", ".", "collater", "(", "samples", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.round_robin_zip_datasets.RoundRobinZipDatasets.num_tokens": [[73, 79], ["max", "dataset.num_tokens", "round_robin_zip_datasets.RoundRobinZipDatasets._map_index", "round_robin_zip_datasets.RoundRobinZipDatasets.datasets.items"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.num_tokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.round_robin_zip_datasets.RoundRobinZipDatasets._map_index"], ["", "", "def", "num_tokens", "(", "self", ",", "index", ")", ":", "\n", "        ", "\"\"\"Return an example's length (number of tokens), used for batching.\"\"\"", "\n", "# TODO make it configurable whether to use max() or sum() here", "\n", "return", "max", "(", "\n", "dataset", ".", "num_tokens", "(", "self", ".", "_map_index", "(", "key", ",", "index", ")", ")", "\n", "for", "key", ",", "dataset", "in", "self", ".", "datasets", ".", "items", "(", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.round_robin_zip_datasets.RoundRobinZipDatasets.size": [[81, 87], ["dataset.size", "round_robin_zip_datasets.RoundRobinZipDatasets._map_index", "round_robin_zip_datasets.RoundRobinZipDatasets.datasets.items"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.round_robin_zip_datasets.RoundRobinZipDatasets._map_index"], ["", "def", "size", "(", "self", ",", "index", ")", ":", "\n", "        ", "\"\"\"Return an example's size as a float or tuple. This value is used when\n        filtering a dataset with ``--max-positions``.\"\"\"", "\n", "return", "{", "\n", "key", ":", "dataset", ".", "size", "(", "self", ".", "_map_index", "(", "key", ",", "index", ")", ")", "\n", "for", "key", ",", "dataset", "in", "self", ".", "datasets", ".", "items", "(", ")", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.round_robin_zip_datasets.RoundRobinZipDatasets.ordered_indices": [[89, 100], ["numpy.arange", "collections.OrderedDict", "len", "dataset.ordered_indices", "round_robin_zip_datasets.RoundRobinZipDatasets.datasets.items"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.ordered_indices"], ["", "def", "ordered_indices", "(", "self", ")", ":", "\n", "        ", "\"\"\"Ordered indices for batching.\"\"\"", "\n", "if", "self", ".", "_ordered_indices", "is", "None", ":", "\n", "# Call the underlying dataset's ordered_indices() here, so that we", "\n", "# get the same random ordering as we would have from using the", "\n", "# underlying dataset directly.", "\n", "            ", "self", ".", "_ordered_indices", "=", "OrderedDict", "(", "[", "\n", "(", "key", ",", "dataset", ".", "ordered_indices", "(", ")", ")", "\n", "for", "key", ",", "dataset", "in", "self", ".", "datasets", ".", "items", "(", ")", "\n", "]", ")", "\n", "", "return", "np", ".", "arange", "(", "len", "(", "self", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.round_robin_zip_datasets.RoundRobinZipDatasets.supports_prefetch": [[101, 106], ["all", "getattr", "round_robin_zip_datasets.RoundRobinZipDatasets.datasets.values"], "methods", ["None"], ["", "@", "property", "\n", "def", "supports_prefetch", "(", "self", ")", ":", "\n", "        ", "return", "all", "(", "\n", "getattr", "(", "dataset", ",", "'supports_prefetch'", ",", "False", ")", "\n", "for", "dataset", "in", "self", ".", "datasets", ".", "values", "(", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.round_robin_zip_datasets.RoundRobinZipDatasets.prefetch": [[108, 111], ["round_robin_zip_datasets.RoundRobinZipDatasets.datasets.items", "dataset.prefetch", "round_robin_zip_datasets.RoundRobinZipDatasets._map_index"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.block_pair_dataset.BlockPairDataset.prefetch", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.round_robin_zip_datasets.RoundRobinZipDatasets._map_index"], ["", "def", "prefetch", "(", "self", ",", "indices", ")", ":", "\n", "        ", "for", "key", ",", "dataset", "in", "self", ".", "datasets", ".", "items", "(", ")", ":", "\n", "            ", "dataset", ".", "prefetch", "(", "[", "self", ".", "_map_index", "(", "key", ",", "index", ")", "for", "index", "in", "indices", "]", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.pad_dataset.PadDataset.__init__": [[13, 17], ["BaseWrapperDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dataset", ",", "pad_idx", ",", "left_pad", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dataset", ")", "\n", "self", ".", "pad_idx", "=", "pad_idx", "\n", "self", ".", "left_pad", "=", "left_pad", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.pad_dataset.PadDataset.collater": [[18, 20], ["fairseq.data.data_utils.collate_tokens"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.data_utils.collate_tokens"], ["", "def", "collater", "(", "self", ",", "samples", ")", ":", "\n", "        ", "return", "data_utils", ".", "collate_tokens", "(", "samples", ",", "self", ".", "pad_idx", ",", "left_pad", "=", "self", ".", "left_pad", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.pad_dataset.LeftPadDataset.__init__": [[24, 26], ["pad_dataset.PadDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dataset", ",", "pad_idx", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dataset", ",", "pad_idx", ",", "left_pad", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.pad_dataset.RightPadDataset.__init__": [[30, 32], ["pad_dataset.PadDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dataset", ",", "pad_idx", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dataset", ",", "pad_idx", ",", "left_pad", "=", "False", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.get_preprocessing_parser": [[15, 19], ["options.get_parser", "options.add_preprocess_args"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.get_parser", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.add_preprocess_args"], ["def", "get_preprocessing_parser", "(", "default_task", "=", "'translation'", ")", ":", "\n", "    ", "parser", "=", "get_parser", "(", "'Preprocessing'", ",", "default_task", ")", "\n", "add_preprocess_args", "(", "parser", ")", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.get_training_parser": [[21, 29], ["options.get_parser", "options.add_dataset_args", "options.add_distributed_training_args", "options.add_model_args", "options.add_optimization_args", "options.add_checkpoint_args"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.get_parser", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.add_dataset_args", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.add_distributed_training_args", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.add_model_args", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.add_optimization_args", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.add_checkpoint_args"], ["", "def", "get_training_parser", "(", "default_task", "=", "'translation'", ")", ":", "\n", "    ", "parser", "=", "get_parser", "(", "'Trainer'", ",", "default_task", ")", "\n", "add_dataset_args", "(", "parser", ",", "train", "=", "True", ")", "\n", "add_distributed_training_args", "(", "parser", ")", "\n", "add_model_args", "(", "parser", ")", "\n", "add_optimization_args", "(", "parser", ")", "\n", "add_checkpoint_args", "(", "parser", ")", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.get_generation_parser": [[31, 38], ["options.get_parser", "options.add_dataset_args", "options.add_generation_args", "options.add_interactive_args"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.get_parser", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.add_dataset_args", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.add_generation_args", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.add_interactive_args"], ["", "def", "get_generation_parser", "(", "interactive", "=", "False", ",", "default_task", "=", "'translation'", ")", ":", "\n", "    ", "parser", "=", "get_parser", "(", "'Generation'", ",", "default_task", ")", "\n", "add_dataset_args", "(", "parser", ",", "gen", "=", "True", ")", "\n", "add_generation_args", "(", "parser", ")", "\n", "if", "interactive", ":", "\n", "        ", "add_interactive_args", "(", "parser", ")", "\n", "", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.get_interactive_generation_parser": [[40, 42], ["options.get_generation_parser"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.get_generation_parser"], ["", "def", "get_interactive_generation_parser", "(", "default_task", "=", "'translation'", ")", ":", "\n", "    ", "return", "get_generation_parser", "(", "interactive", "=", "True", ",", "default_task", "=", "default_task", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.get_eval_lm_parser": [[44, 49], ["options.get_parser", "options.add_dataset_args", "options.add_eval_lm_args"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.get_parser", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.add_dataset_args", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.add_eval_lm_args"], ["", "def", "get_eval_lm_parser", "(", "default_task", "=", "'language_modeling'", ")", ":", "\n", "    ", "parser", "=", "get_parser", "(", "'Evaluate Language Model'", ",", "default_task", ")", "\n", "add_dataset_args", "(", "parser", ",", "gen", "=", "True", ")", "\n", "add_eval_lm_args", "(", "parser", ")", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.eval_str_list": [[51, 60], ["isinstance", "eval", "list", "map", "type"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval"], ["", "def", "eval_str_list", "(", "x", ",", "type", "=", "float", ")", ":", "\n", "    ", "if", "x", "is", "None", ":", "\n", "        ", "return", "None", "\n", "", "if", "isinstance", "(", "x", ",", "str", ")", ":", "\n", "        ", "x", "=", "eval", "(", "x", ")", "\n", "", "try", ":", "\n", "        ", "return", "list", "(", "map", "(", "type", ",", "x", ")", ")", "\n", "", "except", "TypeError", ":", "\n", "        ", "return", "[", "type", "(", "x", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.eval_bool": [[62, 69], ["bool", "eval"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval"], ["", "", "def", "eval_bool", "(", "x", ",", "default", "=", "False", ")", ":", "\n", "    ", "if", "x", "is", "None", ":", "\n", "        ", "return", "default", "\n", "", "try", ":", "\n", "        ", "return", "bool", "(", "eval", "(", "x", ")", ")", "\n", "", "except", "TypeError", ":", "\n", "        ", "return", "default", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.parse_args_and_arch": [[71, 148], ["parser.parse_known_args", "hasattr", "REGISTRIES.items", "hasattr", "getattr", "getattr", "hasattr", "options.parse_args_and_arch", "argparse.ArgumentParser", "argparse.ArgumentParser.set_defaults", "argparse.ArgumentParser.parse_args", "argparse.Namespace", "parser.add_argument_group", "ARCH_MODEL_REGISTRY[].add_args", "getattr", "TASK_REGISTRY[].add_args", "FairseqBMUF.add_args", "parser.parse_known_args", "parser.parse_args", "hasattr", "hasattr", "hasattr", "cls.add_args", "vars().items", "vars().items", "vars", "vars"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.parse_args_and_arch", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.registry.set_defaults", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.add_args", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.add_args", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.add_args", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.add_args"], ["", "", "def", "parse_args_and_arch", "(", "parser", ",", "input_args", "=", "None", ",", "parse_known", "=", "False", ",", "suppress_defaults", "=", "False", ")", ":", "\n", "    ", "if", "suppress_defaults", ":", "\n", "# Parse args without any default values. This requires us to parse", "\n", "# twice, once to identify all the necessary task/model args, and a second", "\n", "# time with all defaults set to None.", "\n", "        ", "args", "=", "parse_args_and_arch", "(", "\n", "parser", ",", "\n", "input_args", "=", "input_args", ",", "\n", "parse_known", "=", "parse_known", ",", "\n", "suppress_defaults", "=", "False", ",", "\n", ")", "\n", "suppressed_parser", "=", "argparse", ".", "ArgumentParser", "(", "add_help", "=", "False", ",", "parents", "=", "[", "parser", "]", ")", "\n", "suppressed_parser", ".", "set_defaults", "(", "**", "{", "k", ":", "None", "for", "k", ",", "v", "in", "vars", "(", "args", ")", ".", "items", "(", ")", "}", ")", "\n", "args", "=", "suppressed_parser", ".", "parse_args", "(", "input_args", ")", "\n", "return", "argparse", ".", "Namespace", "(", "**", "{", "\n", "k", ":", "v", "\n", "for", "k", ",", "v", "in", "vars", "(", "args", ")", ".", "items", "(", ")", "\n", "if", "v", "is", "not", "None", "\n", "}", ")", "\n", "\n", "", "from", "fairseq", ".", "models", "import", "ARCH_MODEL_REGISTRY", ",", "ARCH_CONFIG_REGISTRY", "\n", "\n", "# The parser doesn't know about model/criterion/optimizer-specific args, so", "\n", "# we parse twice. First we parse the model/criterion/optimizer, then we", "\n", "# parse a second time after adding the *-specific arguments.", "\n", "# If input_args is given, we will parse those args instead of sys.argv.", "\n", "args", ",", "_", "=", "parser", ".", "parse_known_args", "(", "input_args", ")", "\n", "\n", "# Add model-specific args to parser.", "\n", "if", "hasattr", "(", "args", ",", "'arch'", ")", ":", "\n", "        ", "model_specific_group", "=", "parser", ".", "add_argument_group", "(", "\n", "'Model-specific configuration'", ",", "\n", "# Only include attributes which are explicitly given as command-line", "\n", "# arguments or which have default values.", "\n", "argument_default", "=", "argparse", ".", "SUPPRESS", ",", "\n", ")", "\n", "ARCH_MODEL_REGISTRY", "[", "args", ".", "arch", "]", ".", "add_args", "(", "model_specific_group", ")", "\n", "\n", "# Add *-specific args to parser.", "\n", "", "from", "fairseq", ".", "registry", "import", "REGISTRIES", "\n", "for", "registry_name", ",", "REGISTRY", "in", "REGISTRIES", ".", "items", "(", ")", ":", "\n", "        ", "choice", "=", "getattr", "(", "args", ",", "registry_name", ",", "None", ")", "\n", "if", "choice", "is", "not", "None", ":", "\n", "            ", "cls", "=", "REGISTRY", "[", "'registry'", "]", "[", "choice", "]", "\n", "if", "hasattr", "(", "cls", ",", "'add_args'", ")", ":", "\n", "                ", "cls", ".", "add_args", "(", "parser", ")", "\n", "", "", "", "if", "hasattr", "(", "args", ",", "'task'", ")", ":", "\n", "        ", "from", "fairseq", ".", "tasks", "import", "TASK_REGISTRY", "\n", "TASK_REGISTRY", "[", "args", ".", "task", "]", ".", "add_args", "(", "parser", ")", "\n", "", "if", "getattr", "(", "args", ",", "'use_bmuf'", ",", "False", ")", ":", "\n", "# hack to support extra args for block distributed data parallelism", "\n", "        ", "from", "fairseq", ".", "optim", ".", "bmuf", "import", "FairseqBMUF", "\n", "FairseqBMUF", ".", "add_args", "(", "parser", ")", "\n", "\n", "# Parse a second time.", "\n", "", "if", "parse_known", ":", "\n", "        ", "args", ",", "extra", "=", "parser", ".", "parse_known_args", "(", "input_args", ")", "\n", "", "else", ":", "\n", "        ", "args", "=", "parser", ".", "parse_args", "(", "input_args", ")", "\n", "extra", "=", "None", "\n", "\n", "# Post-process args.", "\n", "", "if", "hasattr", "(", "args", ",", "'max_sentences_valid'", ")", "and", "args", ".", "max_sentences_valid", "is", "None", ":", "\n", "        ", "args", ".", "max_sentences_valid", "=", "args", ".", "max_sentences", "\n", "", "if", "hasattr", "(", "args", ",", "'max_tokens_valid'", ")", "and", "args", ".", "max_tokens_valid", "is", "None", ":", "\n", "        ", "args", ".", "max_tokens_valid", "=", "args", ".", "max_tokens", "\n", "", "if", "getattr", "(", "args", ",", "'memory_efficient_fp16'", ",", "False", ")", ":", "\n", "        ", "args", ".", "fp16", "=", "True", "\n", "\n", "# Apply architecture configuration.", "\n", "", "if", "hasattr", "(", "args", ",", "'arch'", ")", ":", "\n", "        ", "ARCH_CONFIG_REGISTRY", "[", "args", ".", "arch", "]", "(", "args", ")", "\n", "\n", "", "if", "parse_known", ":", "\n", "        ", "return", "args", ",", "extra", "\n", "", "else", ":", "\n", "        ", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.get_parser": [[150, 204], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_known_args", "fairseq.utils.import_user_module", "argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "REGISTRIES.items", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "TASK_REGISTRY.keys", "registry_name.replace", "REGISTRY[].keys"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.import_user_module"], ["", "", "def", "get_parser", "(", "desc", ",", "default_task", "=", "'translation'", ")", ":", "\n", "# Before creating the true parser, we need to import optional user module", "\n", "# in order to eagerly import custom tasks, optimizers, architectures, etc.", "\n", "    ", "usr_parser", "=", "argparse", ".", "ArgumentParser", "(", "add_help", "=", "False", ",", "allow_abbrev", "=", "False", ")", "\n", "usr_parser", ".", "add_argument", "(", "'--user-dir'", ",", "default", "=", "None", ")", "\n", "usr_args", ",", "_", "=", "usr_parser", ".", "parse_known_args", "(", ")", "\n", "utils", ".", "import_user_module", "(", "usr_args", ")", "\n", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", "allow_abbrev", "=", "False", ")", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'--no-progress-bar'", ",", "action", "=", "'store_true'", ",", "help", "=", "'disable progress bar'", ")", "\n", "parser", ".", "add_argument", "(", "'--log-interval'", ",", "type", "=", "int", ",", "default", "=", "1000", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'log progress every N batches (when progress bar is disabled)'", ")", "\n", "parser", ".", "add_argument", "(", "'--log-format'", ",", "default", "=", "None", ",", "help", "=", "'log format to use'", ",", "\n", "choices", "=", "[", "'json'", ",", "'none'", ",", "'simple'", ",", "'tqdm'", "]", ")", "\n", "parser", ".", "add_argument", "(", "'--tensorboard-logdir'", ",", "metavar", "=", "'DIR'", ",", "default", "=", "''", ",", "\n", "help", "=", "'path to save logs for tensorboard, should match --logdir '", "\n", "'of running tensorboard (default: no tensorboard logging)'", ")", "\n", "parser", ".", "add_argument", "(", "\"--tbmf-wrapper\"", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"[FB only] \"", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "default", "=", "1", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'pseudo random number generator seed'", ")", "\n", "parser", ".", "add_argument", "(", "'--cpu'", ",", "action", "=", "'store_true'", ",", "help", "=", "'use CPU instead of CUDA'", ")", "\n", "parser", ".", "add_argument", "(", "'--fp16'", ",", "action", "=", "'store_true'", ",", "help", "=", "'use FP16'", ")", "\n", "parser", ".", "add_argument", "(", "'--memory-efficient-fp16'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'use a memory-efficient version of FP16 training; implies --fp16'", ")", "\n", "parser", ".", "add_argument", "(", "'--fp16-init-scale'", ",", "default", "=", "2", "**", "7", ",", "type", "=", "int", ",", "\n", "help", "=", "'default FP16 loss scale'", ")", "\n", "parser", ".", "add_argument", "(", "'--fp16-scale-window'", ",", "type", "=", "int", ",", "\n", "help", "=", "'number of updates before increasing loss scale'", ")", "\n", "parser", ".", "add_argument", "(", "'--fp16-scale-tolerance'", ",", "default", "=", "0.0", ",", "type", "=", "float", ",", "\n", "help", "=", "'pct of updates that can overflow before decreasing the loss scale'", ")", "\n", "parser", ".", "add_argument", "(", "'--min-loss-scale'", ",", "default", "=", "1e-4", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'minimum FP16 loss scale, after which training is stopped'", ")", "\n", "parser", ".", "add_argument", "(", "'--threshold-loss-scale'", ",", "type", "=", "float", ",", "\n", "help", "=", "'threshold FP16 loss scale from below'", ")", "\n", "parser", ".", "add_argument", "(", "'--user-dir'", ",", "default", "=", "None", ",", "\n", "help", "=", "'path to a python module containing custom extensions (tasks and/or architectures)'", ")", "\n", "\n", "from", "fairseq", ".", "registry", "import", "REGISTRIES", "\n", "for", "registry_name", ",", "REGISTRY", "in", "REGISTRIES", ".", "items", "(", ")", ":", "\n", "        ", "parser", ".", "add_argument", "(", "\n", "'--'", "+", "registry_name", ".", "replace", "(", "'_'", ",", "'-'", ")", ",", "\n", "default", "=", "REGISTRY", "[", "'default'", "]", ",", "\n", "choices", "=", "REGISTRY", "[", "'registry'", "]", ".", "keys", "(", ")", ",", "\n", ")", "\n", "\n", "# Task definitions can be found under fairseq/tasks/", "\n", "", "from", "fairseq", ".", "tasks", "import", "TASK_REGISTRY", "\n", "parser", ".", "add_argument", "(", "'--task'", ",", "metavar", "=", "'TASK'", ",", "default", "=", "default_task", ",", "\n", "choices", "=", "TASK_REGISTRY", ".", "keys", "(", ")", ",", "\n", "help", "=", "'task'", ")", "\n", "# fmt: on", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.add_preprocess_args": [[206, 262], ["parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "options.add_state_machine_args", "fairseq.data.indexed_dataset.get_available_dataset_impl"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.add_state_machine_args", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.get_available_dataset_impl"], ["", "def", "add_preprocess_args", "(", "parser", ")", ":", "\n", "    ", "group", "=", "parser", ".", "add_argument_group", "(", "'Preprocessing'", ")", "\n", "# fmt: off", "\n", "group", ".", "add_argument", "(", "\"-s\"", ",", "\"--source-lang\"", ",", "default", "=", "None", ",", "metavar", "=", "\"SRC\"", ",", "\n", "help", "=", "\"source language\"", ")", "\n", "group", ".", "add_argument", "(", "\"-t\"", ",", "\"--target-lang\"", ",", "default", "=", "None", ",", "metavar", "=", "\"TARGET\"", ",", "\n", "help", "=", "\"target language\"", ")", "\n", "group", ".", "add_argument", "(", "\"--trainpref\"", ",", "metavar", "=", "\"FP\"", ",", "default", "=", "None", ",", "\n", "help", "=", "\"train file prefix\"", ")", "\n", "group", ".", "add_argument", "(", "\"--validpref\"", ",", "metavar", "=", "\"FP\"", ",", "default", "=", "None", ",", "\n", "help", "=", "\"comma separated, valid file prefixes\"", ")", "\n", "group", ".", "add_argument", "(", "\"--testpref\"", ",", "metavar", "=", "\"FP\"", ",", "default", "=", "None", ",", "\n", "help", "=", "\"comma separated, test file prefixes\"", ")", "\n", "group", ".", "add_argument", "(", "\"--destdir\"", ",", "metavar", "=", "\"DIR\"", ",", "default", "=", "\"data-bin\"", ",", "\n", "help", "=", "\"destination dir\"", ")", "\n", "group", ".", "add_argument", "(", "\"--thresholdtgt\"", ",", "metavar", "=", "\"N\"", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "\"map words appearing less than threshold times to unknown\"", ")", "\n", "group", ".", "add_argument", "(", "\"--thresholdsrc\"", ",", "metavar", "=", "\"N\"", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "\"map words appearing less than threshold times to unknown\"", ")", "\n", "group", ".", "add_argument", "(", "\"--tgtdict\"", ",", "metavar", "=", "\"FP\"", ",", "\n", "help", "=", "\"reuse given target dictionary\"", ")", "\n", "group", ".", "add_argument", "(", "\"--srcdict\"", ",", "metavar", "=", "\"FP\"", ",", "\n", "help", "=", "\"reuse given source dictionary\"", ")", "\n", "group", ".", "add_argument", "(", "\"--nwordstgt\"", ",", "metavar", "=", "\"N\"", ",", "default", "=", "-", "1", ",", "type", "=", "int", ",", "\n", "help", "=", "\"number of target words to retain\"", ")", "\n", "group", ".", "add_argument", "(", "\"--nwordssrc\"", ",", "metavar", "=", "\"N\"", ",", "default", "=", "-", "1", ",", "type", "=", "int", ",", "\n", "help", "=", "\"number of source words to retain\"", ")", "\n", "group", ".", "add_argument", "(", "\"--alignfile\"", ",", "metavar", "=", "\"ALIGN\"", ",", "default", "=", "None", ",", "\n", "help", "=", "\"an alignment file (optional)\"", ")", "\n", "parser", ".", "add_argument", "(", "'--dataset-impl'", ",", "metavar", "=", "'FORMAT'", ",", "default", "=", "'mmap'", ",", "\n", "choices", "=", "get_available_dataset_impl", "(", ")", ",", "\n", "help", "=", "'output dataset implementation'", ")", "\n", "group", ".", "add_argument", "(", "\"--joined-dictionary\"", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"Generate joined dictionary\"", ")", "\n", "group", ".", "add_argument", "(", "\"--only-source\"", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"Only process the source language\"", ")", "\n", "group", ".", "add_argument", "(", "\"--padding-factor\"", ",", "metavar", "=", "\"N\"", ",", "default", "=", "8", ",", "type", "=", "int", ",", "\n", "help", "=", "\"Pad dictionary size to be multiple of N\"", ")", "\n", "group", ".", "add_argument", "(", "\"--workers\"", ",", "metavar", "=", "\"N\"", ",", "default", "=", "1", ",", "type", "=", "int", ",", "\n", "help", "=", "\"number of parallel workers\"", ")", "\n", "\n", "# for pretrained external embeddings", "\n", "group", ".", "add_argument", "(", "\"--pretrained-embed\"", ",", "default", "=", "'roberta.base'", ",", "\n", "help", "=", "\"Type of pretrained embedding\"", ")", "\n", "# NOTE: Previous default \"17 18 19 20 21 22 23 24\"", "\n", "group", ".", "add_argument", "(", "'--bert-layers'", ",", "nargs", "=", "'+'", ",", "type", "=", "int", ",", "\n", "help", "=", "'RoBERTa layers to extract (default last)'", ")", "\n", "# wether to separate by space or tab", "\n", "group", ".", "add_argument", "(", "\"--tokenize-by-whitespace\"", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Tokenize by whitespace or tab\"", ")", "\n", "\n", "# for stack-transformer", "\n", "add_state_machine_args", "(", "group", ")", "\n", "\n", "# fmt: on", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.add_dataset_args": [[264, 308], ["parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "fairseq.data.indexed_dataset.get_available_dataset_impl"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.get_available_dataset_impl"], ["", "def", "add_dataset_args", "(", "parser", ",", "train", "=", "False", ",", "gen", "=", "False", ")", ":", "\n", "    ", "group", "=", "parser", ".", "add_argument_group", "(", "'Dataset and data loading'", ")", "\n", "# fmt: off", "\n", "group", ".", "add_argument", "(", "'--num-workers'", ",", "default", "=", "1", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'how many subprocesses to use for data loading'", ")", "\n", "group", ".", "add_argument", "(", "'--skip-invalid-size-inputs-valid-test'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'ignore too long or too short lines in valid and test set'", ")", "\n", "group", ".", "add_argument", "(", "'--max-tokens'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'maximum number of tokens in a batch'", ")", "\n", "group", ".", "add_argument", "(", "'--max-sentences'", ",", "'--batch-size'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'maximum number of sentences in a batch'", ")", "\n", "group", ".", "add_argument", "(", "'--required-batch-size-multiple'", ",", "default", "=", "8", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'batch size will be a multiplier of this value'", ")", "\n", "parser", ".", "add_argument", "(", "'--dataset-impl'", ",", "metavar", "=", "'FORMAT'", ",", "\n", "choices", "=", "get_available_dataset_impl", "(", ")", ",", "\n", "help", "=", "'output dataset implementation'", ")", "\n", "if", "train", ":", "\n", "        ", "group", ".", "add_argument", "(", "'--train-subset'", ",", "default", "=", "'train'", ",", "metavar", "=", "'SPLIT'", ",", "\n", "choices", "=", "[", "'train'", ",", "'valid'", ",", "'test'", "]", ",", "\n", "help", "=", "'data subset to use for training (train, valid, test)'", ")", "\n", "group", ".", "add_argument", "(", "'--valid-subset'", ",", "default", "=", "'valid'", ",", "metavar", "=", "'SPLIT'", ",", "\n", "help", "=", "'comma separated list of data subsets to use for validation'", "\n", "' (train, valid, valid1, test, test1)'", ")", "\n", "group", ".", "add_argument", "(", "'--validate-interval'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'validate every N epochs'", ")", "\n", "group", ".", "add_argument", "(", "'--disable-validation'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'disable validation'", ")", "\n", "group", ".", "add_argument", "(", "'--max-tokens-valid'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'maximum number of tokens in a validation batch'", "\n", "' (defaults to --max-tokens)'", ")", "\n", "group", ".", "add_argument", "(", "'--max-sentences-valid'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'maximum number of sentences in a validation batch'", "\n", "' (defaults to --max-sentences)'", ")", "\n", "group", ".", "add_argument", "(", "'--curriculum'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'don\\'t shuffle batches for first N epochs'", ")", "\n", "", "if", "gen", ":", "\n", "        ", "group", ".", "add_argument", "(", "'--gen-subset'", ",", "default", "=", "'test'", ",", "metavar", "=", "'SPLIT'", ",", "\n", "help", "=", "'data subset to generate (train, valid, test)'", ")", "\n", "group", ".", "add_argument", "(", "'--num-shards'", ",", "default", "=", "1", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'shard generation over N shards'", ")", "\n", "group", ".", "add_argument", "(", "'--shard-id'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "metavar", "=", "'ID'", ",", "\n", "help", "=", "'id of the shard to generate (id < num_shards)'", ")", "\n", "# fmt: on", "\n", "", "return", "group", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.add_distributed_training_args": [[310, 343], ["parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "max", "torch.cuda.device_count"], "function", ["None"], ["", "def", "add_distributed_training_args", "(", "parser", ")", ":", "\n", "    ", "group", "=", "parser", ".", "add_argument_group", "(", "'Distributed training'", ")", "\n", "# fmt: off", "\n", "group", ".", "add_argument", "(", "'--distributed-world-size'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "default", "=", "max", "(", "1", ",", "torch", ".", "cuda", ".", "device_count", "(", ")", ")", ",", "\n", "help", "=", "'total number of GPUs across all nodes (default: all visible GPUs)'", ")", "\n", "group", ".", "add_argument", "(", "'--distributed-rank'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "'rank of the current worker'", ")", "\n", "group", ".", "add_argument", "(", "'--distributed-backend'", ",", "default", "=", "'nccl'", ",", "type", "=", "str", ",", "\n", "help", "=", "'distributed backend'", ")", "\n", "group", ".", "add_argument", "(", "'--distributed-init-method'", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "'typically tcp://hostname:port that will be used to '", "\n", "'establish initial connetion'", ")", "\n", "group", ".", "add_argument", "(", "'--distributed-port'", ",", "default", "=", "-", "1", ",", "type", "=", "int", ",", "\n", "help", "=", "'port number (not required if using --distributed-init-method)'", ")", "\n", "group", ".", "add_argument", "(", "'--device-id'", ",", "'--local_rank'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "'which GPU to use (usually configured automatically)'", ")", "\n", "group", ".", "add_argument", "(", "'--distributed-no-spawn'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'do not spawn multiple processes even if multiple GPUs are visible'", ")", "\n", "group", ".", "add_argument", "(", "'--ddp-backend'", ",", "default", "=", "'c10d'", ",", "type", "=", "str", ",", "\n", "choices", "=", "[", "'c10d'", ",", "'no_c10d'", "]", ",", "\n", "help", "=", "'DistributedDataParallel backend'", ")", "\n", "group", ".", "add_argument", "(", "'--bucket-cap-mb'", ",", "default", "=", "25", ",", "type", "=", "int", ",", "metavar", "=", "'MB'", ",", "\n", "help", "=", "'bucket size for reduction'", ")", "\n", "group", ".", "add_argument", "(", "'--fix-batches-to-gpus'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'don\\'t shuffle batches between GPUs; this reduces overall '", "\n", "'randomness and may affect precision but avoids the cost of '", "\n", "'re-reading the data'", ")", "\n", "group", ".", "add_argument", "(", "'--find-unused-parameters'", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'disable unused parameter detection (not applicable to '", "\n", "'no_c10d ddp-backend'", ")", "\n", "# fmt: on", "\n", "return", "group", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.add_optimization_args": [[345, 373], ["parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "options.eval_str_list"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.eval_str_list"], ["", "def", "add_optimization_args", "(", "parser", ")", ":", "\n", "    ", "group", "=", "parser", ".", "add_argument_group", "(", "'Optimization'", ")", "\n", "# fmt: off", "\n", "group", ".", "add_argument", "(", "'--max-epoch'", ",", "'--me'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'force stop training at specified epoch'", ")", "\n", "group", ".", "add_argument", "(", "'--max-update'", ",", "'--mu'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'force stop training at specified update'", ")", "\n", "group", ".", "add_argument", "(", "'--clip-norm'", ",", "default", "=", "25", ",", "type", "=", "float", ",", "metavar", "=", "'NORM'", ",", "\n", "help", "=", "'clip threshold of gradients'", ")", "\n", "group", ".", "add_argument", "(", "'--sentence-avg'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'normalize gradients by the number of sentences in a batch'", "\n", "' (default is to normalize by number of tokens)'", ")", "\n", "group", ".", "add_argument", "(", "'--update-freq'", ",", "default", "=", "'1'", ",", "metavar", "=", "'N1,N2,...,N_K'", ",", "\n", "type", "=", "lambda", "uf", ":", "eval_str_list", "(", "uf", ",", "type", "=", "int", ")", ",", "\n", "help", "=", "'update parameters every N_i batches, when in epoch i'", ")", "\n", "group", ".", "add_argument", "(", "'--lr'", ",", "'--learning-rate'", ",", "default", "=", "'0.25'", ",", "type", "=", "eval_str_list", ",", "\n", "metavar", "=", "'LR_1,LR_2,...,LR_N'", ",", "\n", "help", "=", "'learning rate for the first N epochs; all epochs >N using LR_N'", "\n", "' (note: this may be interpreted differently depending on --lr-scheduler)'", ")", "\n", "group", ".", "add_argument", "(", "'--min-lr'", ",", "default", "=", "-", "1", ",", "type", "=", "float", ",", "metavar", "=", "'LR'", ",", "\n", "help", "=", "'stop training when the learning rate reaches this minimum'", ")", "\n", "group", ".", "add_argument", "(", "'--use-bmuf'", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'specify global optimizer for syncing models on different GPUs/shards'", ")", "\n", "\n", "\n", "group", ".", "add_argument", "(", "'--burnthrough'", ",", "'--bt'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "help", "=", "'Do not evaluate nor save until this epoch'", ")", "\n", "# fmt: on", "\n", "return", "group", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.add_checkpoint_args": [[375, 415], ["parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument"], "function", ["None"], ["", "def", "add_checkpoint_args", "(", "parser", ")", ":", "\n", "    ", "group", "=", "parser", ".", "add_argument_group", "(", "'Checkpointing'", ")", "\n", "# fmt: off", "\n", "group", ".", "add_argument", "(", "'--save-dir'", ",", "metavar", "=", "'DIR'", ",", "default", "=", "'checkpoints'", ",", "\n", "help", "=", "'path to save checkpoints'", ")", "\n", "group", ".", "add_argument", "(", "'--restore-file'", ",", "default", "=", "'checkpoint_last.pt'", ",", "\n", "help", "=", "'filename from which to load checkpoint '", "\n", "'(default: <save-dir>/checkpoint_last.pt'", ")", "\n", "group", ".", "add_argument", "(", "'--reset-dataloader'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'if set, does not reload dataloader state from the checkpoint'", ")", "\n", "group", ".", "add_argument", "(", "'--reset-lr-scheduler'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'if set, does not load lr scheduler state from the checkpoint'", ")", "\n", "group", ".", "add_argument", "(", "'--reset-meters'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'if set, does not load meters from the checkpoint'", ")", "\n", "group", ".", "add_argument", "(", "'--reset-optimizer'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'if set, does not load optimizer state from the checkpoint'", ")", "\n", "group", ".", "add_argument", "(", "'--optimizer-overrides'", ",", "default", "=", "\"{}\"", ",", "type", "=", "str", ",", "metavar", "=", "'DICT'", ",", "\n", "help", "=", "'a dictionary used to override optimizer args when loading a checkpoint'", ")", "\n", "group", ".", "add_argument", "(", "'--save-interval'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'save a checkpoint every N epochs'", ")", "\n", "group", ".", "add_argument", "(", "'--save-interval-updates'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'save a checkpoint (and validate) every N updates'", ")", "\n", "group", ".", "add_argument", "(", "'--keep-interval-updates'", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'keep the last N checkpoints saved with --save-interval-updates'", ")", "\n", "group", ".", "add_argument", "(", "'--keep-last-epochs'", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'keep last N epoch checkpoints'", ")", "\n", "group", ".", "add_argument", "(", "'--no-save'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'don\\'t save models or checkpoints'", ")", "\n", "group", ".", "add_argument", "(", "'--no-epoch-checkpoints'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'only store last and best checkpoints'", ")", "\n", "group", ".", "add_argument", "(", "'--no-last-checkpoints'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'don\\'t store last checkpoints'", ")", "\n", "group", ".", "add_argument", "(", "'--no-save-optimizer-state'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'don\\'t save optimizer-state as part of checkpoint'", ")", "\n", "group", ".", "add_argument", "(", "'--best-checkpoint-metric'", ",", "type", "=", "str", ",", "default", "=", "'loss'", ",", "\n", "help", "=", "'metric to use for saving \"best\" checkpoints'", ")", "\n", "group", ".", "add_argument", "(", "'--maximize-best-checkpoint-metric'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'select the largest metric value for saving \"best\" checkpoints'", ")", "\n", "# fmt: on", "\n", "return", "group", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.add_common_eval_args": [[417, 433], ["options.add_state_machine_args", "group.add_argument", "group.add_argument", "group.add_argument", "group.add_argument", "group.add_argument"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.add_state_machine_args"], ["", "def", "add_common_eval_args", "(", "group", ")", ":", "\n", "\n", "    ", "add_state_machine_args", "(", "group", ")", "\n", "\n", "# fmt: off", "\n", "group", ".", "add_argument", "(", "'--path'", ",", "metavar", "=", "'FILE'", ",", "\n", "help", "=", "'path(s) to model file(s), colon separated'", ")", "\n", "group", ".", "add_argument", "(", "'--remove-bpe'", ",", "nargs", "=", "'?'", ",", "const", "=", "'@@ '", ",", "default", "=", "None", ",", "\n", "help", "=", "'remove BPE tokens before scoring (can be set to sentencepiece)'", ")", "\n", "group", ".", "add_argument", "(", "'--quiet'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'only print final scores'", ")", "\n", "group", ".", "add_argument", "(", "'--model-overrides'", ",", "default", "=", "\"{}\"", ",", "type", "=", "str", ",", "metavar", "=", "'DICT'", ",", "\n", "help", "=", "'a dictionary used to override model args at generation '", "\n", "'that were used during model training'", ")", "\n", "group", ".", "add_argument", "(", "'--results-path'", ",", "metavar", "=", "'RESDIR'", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "'path to save eval results (optional)\"'", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.add_state_machine_args": [[436, 450], ["group.add_argument", "group.add_argument", "group.add_argument", "group.add_argument", "group.add_argument", "group.add_argument"], "function", ["None"], ["", "def", "add_state_machine_args", "(", "group", ")", ":", "\n", "    ", "group", ".", "add_argument", "(", "'--machine-type'", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "choices", "=", "[", "'AMR'", ",", "'NER'", ",", "'NER2'", ",", "'WSD'", ",", "'MCR'", ",", "'SRL'", ",", "'dep-parsing'", "]", ",", "\n", "help", "=", "'Type of state machine used in decoding'", ")", "\n", "group", ".", "add_argument", "(", "'--machine-rules'", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "'json files with extra rules for the state machine'", ")", "\n", "group", ".", "add_argument", "(", "'--entity-rules'", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "'json files with entity rules for the state machine'", ")", "\n", "group", ".", "add_argument", "(", "'--gold-annotations'", ",", "type", "=", "str", ",", "nargs", "=", "'+'", ",", "\n", "help", "=", "'Path to gold annotations file used for reward calculation'", ")", "\n", "group", ".", "add_argument", "(", "'--gold-episode-ratio'", ",", "type", "=", "float", ",", "\n", "help", "=", "'Ratio of episodes in batch set to follow gold'", ")", "\n", "group", ".", "add_argument", "(", "'--batch-normalize-reward'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Normalize rewards in batch'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.add_eval_lm_args": [[452, 465], ["parser.add_argument_group", "options.add_common_eval_args", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.add_common_eval_args"], ["", "def", "add_eval_lm_args", "(", "parser", ")", ":", "\n", "    ", "group", "=", "parser", ".", "add_argument_group", "(", "'LM Evaluation'", ")", "\n", "add_common_eval_args", "(", "group", ")", "\n", "# fmt: off", "\n", "group", ".", "add_argument", "(", "'--output-word-probs'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'if set, outputs words and their predicted log probabilities to standard output'", ")", "\n", "group", ".", "add_argument", "(", "'--output-word-stats'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'if set, outputs word statistics such as word count, average probability, etc'", ")", "\n", "group", ".", "add_argument", "(", "'--context-window'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'ensures that every evaluated token has access to a context of at least this size,'", "\n", "' if possible'", ")", "\n", "group", ".", "add_argument", "(", "'--softmax-batch'", ",", "default", "=", "sys", ".", "maxsize", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'if BxT is more than this, will batch the softmax over vocab to this amount of tokens'", "\n", "' in order to fit into GPU memory'", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.add_generation_args": [[469, 525], ["parser.add_argument_group", "options.add_common_eval_args", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.add_common_eval_args"], ["", "def", "add_generation_args", "(", "parser", ")", ":", "\n", "    ", "group", "=", "parser", ".", "add_argument_group", "(", "'Generation'", ")", "\n", "add_common_eval_args", "(", "group", ")", "\n", "# fmt: off", "\n", "group", ".", "add_argument", "(", "'--beam'", ",", "default", "=", "5", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'beam size'", ")", "\n", "group", ".", "add_argument", "(", "'--nbest'", ",", "default", "=", "1", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'number of hypotheses to output'", ")", "\n", "group", ".", "add_argument", "(", "'--max-len-a'", ",", "default", "=", "0", ",", "type", "=", "float", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "(", "'generate sequences of maximum length ax + b, '", "\n", "'where x is the source length'", ")", ")", "\n", "group", ".", "add_argument", "(", "'--max-len-b'", ",", "default", "=", "200", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "(", "'generate sequences of maximum length ax + b, '", "\n", "'where x is the source length'", ")", ")", "\n", "group", ".", "add_argument", "(", "'--min-len'", ",", "default", "=", "1", ",", "type", "=", "float", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "(", "'minimum generation length'", ")", ")", "\n", "group", ".", "add_argument", "(", "'--match-source-len'", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "(", "'generations should match the source length'", ")", ")", "\n", "group", ".", "add_argument", "(", "'--no-early-stop'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'deprecated'", ")", "\n", "group", ".", "add_argument", "(", "'--unnormalized'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'compare unnormalized hypothesis scores'", ")", "\n", "group", ".", "add_argument", "(", "'--no-beamable-mm'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'don\\'t use BeamableMM in attention layers'", ")", "\n", "group", ".", "add_argument", "(", "'--lenpen'", ",", "default", "=", "1", ",", "type", "=", "float", ",", "\n", "help", "=", "'length penalty: <1.0 favors shorter, >1.0 favors longer sentences'", ")", "\n", "group", ".", "add_argument", "(", "'--unkpen'", ",", "default", "=", "0", ",", "type", "=", "float", ",", "\n", "help", "=", "'unknown word penalty: <0 produces more unks, >0 produces fewer'", ")", "\n", "group", ".", "add_argument", "(", "'--replace-unk'", ",", "nargs", "=", "'?'", ",", "const", "=", "True", ",", "default", "=", "None", ",", "\n", "help", "=", "'perform unknown replacement (optionally with alignment dictionary)'", ")", "\n", "group", ".", "add_argument", "(", "'--sacrebleu'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'score with sacrebleu'", ")", "\n", "group", ".", "add_argument", "(", "'--score-reference'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'just score the reference translation'", ")", "\n", "group", ".", "add_argument", "(", "'--prefix-size'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "metavar", "=", "'PS'", ",", "\n", "help", "=", "'initialize generation by target prefix of given length'", ")", "\n", "group", ".", "add_argument", "(", "'--no-repeat-ngram-size'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'ngram blocking such that this size ngram cannot be repeated in the generation'", ")", "\n", "group", ".", "add_argument", "(", "'--sampling'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'sample hypotheses instead of using beam search'", ")", "\n", "group", ".", "add_argument", "(", "'--sampling-topk'", ",", "default", "=", "-", "1", ",", "type", "=", "int", ",", "metavar", "=", "'PS'", ",", "\n", "help", "=", "'sample from top K likely next words instead of all words'", ")", "\n", "group", ".", "add_argument", "(", "'--sampling-topp'", ",", "default", "=", "-", "1.0", ",", "type", "=", "float", ",", "metavar", "=", "'PS'", ",", "\n", "help", "=", "'sample from the smallest set whose cumulative probability mass exceeds p for next words'", ")", "\n", "group", ".", "add_argument", "(", "'--temperature'", ",", "default", "=", "1.", ",", "type", "=", "float", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'temperature for generation'", ")", "\n", "group", ".", "add_argument", "(", "'--diverse-beam-groups'", ",", "default", "=", "-", "1", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'number of groups for Diverse Beam Search'", ")", "\n", "group", ".", "add_argument", "(", "'--diverse-beam-strength'", ",", "default", "=", "0.5", ",", "type", "=", "float", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'strength of diversity penalty for Diverse Beam Search'", ")", "\n", "group", ".", "add_argument", "(", "'--print-alignment'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'if set, uses attention feedback to compute and print alignment to source tokens'", ")", "\n", "group", ".", "add_argument", "(", "\"--tokenize-by-whitespace\"", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Tokenize by whitespace or tab\"", ")", "\n", "# fmt: on", "\n", "return", "group", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.add_interactive_args": [[527, 534], ["parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument"], "function", ["None"], ["", "def", "add_interactive_args", "(", "parser", ")", ":", "\n", "    ", "group", "=", "parser", ".", "add_argument_group", "(", "'Interactive'", ")", "\n", "# fmt: off", "\n", "group", ".", "add_argument", "(", "'--buffer-size'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'read this many sentences into a buffer before processing them'", ")", "\n", "group", ".", "add_argument", "(", "'--input'", ",", "default", "=", "'-'", ",", "type", "=", "str", ",", "metavar", "=", "'FILE'", ",", "\n", "help", "=", "'file to read from; use - for stdin'", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.add_model_args": [[537, 554], ["parser.add_argument_group", "parser.add_argument_group.add_argument", "ARCH_MODEL_REGISTRY.keys"], "function", ["None"], ["", "def", "add_model_args", "(", "parser", ")", ":", "\n", "    ", "group", "=", "parser", ".", "add_argument_group", "(", "'Model configuration'", ")", "\n", "# fmt: off", "\n", "\n", "# Model definitions can be found under fairseq/models/", "\n", "#", "\n", "# The model architecture can be specified in several ways.", "\n", "# In increasing order of priority:", "\n", "# 1) model defaults (lowest priority)", "\n", "# 2) --arch argument", "\n", "# 3) --encoder/decoder-* arguments (highest priority)", "\n", "from", "fairseq", ".", "models", "import", "ARCH_MODEL_REGISTRY", "\n", "group", ".", "add_argument", "(", "'--arch'", ",", "'-a'", ",", "default", "=", "'fconv'", ",", "metavar", "=", "'ARCH'", ",", "required", "=", "True", ",", "\n", "choices", "=", "ARCH_MODEL_REGISTRY", ".", "keys", "(", ")", ",", "\n", "help", "=", "'Model Architecture'", ")", "\n", "# fmt: on", "\n", "return", "group", "\n", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.distributed_utils.is_master": [[18, 20], ["None"], "function", ["None"], ["def", "is_master", "(", "args", ")", ":", "\n", "    ", "return", "args", ".", "distributed_rank", "==", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.distributed_utils.infer_init_method": [[22, 69], ["all", "int", "int", "os.environ.get", "os.environ.get", "subprocess.check_output", "int", "os.environ.get", "os.environ.get", "int", "int", "int", "int", "int", "int", "int", "[].decode", "os.environ.get", "os.environ.get", "os.environ.get", "os.environ.get", "os.environ.get", "subprocess.check_output.split"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.decode", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["", "def", "infer_init_method", "(", "args", ")", ":", "\n", "    ", "if", "args", ".", "distributed_init_method", "is", "not", "None", ":", "\n", "        ", "return", "\n", "\n", "# support torch.distributed.launch", "\n", "", "if", "all", "(", "key", "in", "os", ".", "environ", "for", "key", "in", "[", "\n", "'MASTER_ADDR'", ",", "'MASTER_PORT'", ",", "'WORLD_SIZE'", ",", "'RANK'", "\n", "]", ")", ":", "\n", "        ", "args", ".", "distributed_init_method", "=", "'env://'", "\n", "args", ".", "distributed_world_size", "=", "int", "(", "os", ".", "environ", "[", "'WORLD_SIZE'", "]", ")", "\n", "args", ".", "distributed_rank", "=", "int", "(", "os", ".", "environ", "[", "'RANK'", "]", ")", "\n", "\n", "# we can determine the init method automatically for Slurm", "\n", "", "elif", "args", ".", "distributed_port", ">", "0", ":", "\n", "        ", "node_list", "=", "os", ".", "environ", ".", "get", "(", "'SLURM_STEP_NODELIST'", ")", "\n", "if", "node_list", "is", "None", ":", "\n", "            ", "node_list", "=", "os", ".", "environ", ".", "get", "(", "'SLURM_JOB_NODELIST'", ")", "\n", "", "if", "node_list", "is", "not", "None", ":", "\n", "            ", "try", ":", "\n", "                ", "hostnames", "=", "subprocess", ".", "check_output", "(", "[", "'scontrol'", ",", "'show'", ",", "'hostnames'", ",", "node_list", "]", ")", "\n", "args", ".", "distributed_init_method", "=", "'tcp://{host}:{port}'", ".", "format", "(", "\n", "host", "=", "hostnames", ".", "split", "(", ")", "[", "0", "]", ".", "decode", "(", "'utf-8'", ")", ",", "\n", "port", "=", "args", ".", "distributed_port", ",", "\n", ")", "\n", "nnodes", "=", "int", "(", "os", ".", "environ", ".", "get", "(", "'SLURM_NNODES'", ")", ")", "\n", "ntasks_per_node", "=", "os", ".", "environ", ".", "get", "(", "'SLURM_NTASKS_PER_NODE'", ")", "\n", "if", "ntasks_per_node", "is", "not", "None", ":", "\n", "                    ", "ntasks_per_node", "=", "int", "(", "ntasks_per_node", ")", "\n", "", "else", ":", "\n", "                    ", "ntasks", "=", "int", "(", "os", ".", "environ", ".", "get", "(", "'SLURM_NTASKS'", ")", ")", "\n", "nnodes", "=", "int", "(", "os", ".", "environ", ".", "get", "(", "'SLURM_NNODES'", ")", ")", "\n", "assert", "ntasks", "%", "nnodes", "==", "0", "\n", "ntasks_per_node", "=", "int", "(", "ntasks", "/", "nnodes", ")", "\n", "", "if", "ntasks_per_node", "==", "1", ":", "\n", "                    ", "assert", "args", ".", "distributed_world_size", "%", "nnodes", "==", "0", "\n", "gpus_per_node", "=", "args", ".", "distributed_world_size", "//", "nnodes", "\n", "node_id", "=", "int", "(", "os", ".", "environ", ".", "get", "(", "'SLURM_NODEID'", ")", ")", "\n", "args", ".", "distributed_rank", "=", "node_id", "*", "gpus_per_node", "\n", "", "else", ":", "\n", "                    ", "assert", "ntasks_per_node", "==", "args", ".", "distributed_world_size", "//", "nnodes", "\n", "args", ".", "distributed_no_spawn", "=", "True", "\n", "args", ".", "distributed_rank", "=", "int", "(", "os", ".", "environ", ".", "get", "(", "'SLURM_PROCID'", ")", ")", "\n", "args", ".", "device_id", "=", "int", "(", "os", ".", "environ", ".", "get", "(", "'SLURM_LOCALID'", ")", ")", "\n", "", "", "except", "subprocess", ".", "CalledProcessError", "as", "e", ":", "# scontrol failed", "\n", "                ", "raise", "e", "\n", "", "except", "FileNotFoundError", ":", "# Slurm is not installed", "\n", "                ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.distributed_utils.distributed_init": [[71, 96], ["torch.distributed.is_initialized", "torch.distributed.is_initialized", "torch.distributed.get_rank", "torch.distributed.get_rank", "ValueError", "warnings.warn", "distributed_utils.suppress_output.print", "torch.init_process_group", "distributed_utils.suppress_output.print", "torch.all_reduce", "distributed_utils.suppress_output", "torch.rand().cuda", "torch.rand().cuda", "distributed_utils.is_master", "socket.gethostname", "torch.rand", "torch.rand"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.distributed_utils.get_rank", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.distributed_utils.get_rank", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.distributed_utils.all_reduce", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.distributed_utils.suppress_output", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.distributed_utils.is_master"], ["", "", "", "", "def", "distributed_init", "(", "args", ")", ":", "\n", "    ", "if", "args", ".", "distributed_world_size", "==", "1", ":", "\n", "        ", "raise", "ValueError", "(", "'Cannot initialize distributed with distributed_world_size=1'", ")", "\n", "\n", "", "if", "torch", ".", "distributed", ".", "is_initialized", "(", ")", ":", "\n", "        ", "warnings", ".", "warn", "(", "'Distributed is already initialized, cannot initialize twice!'", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "'| distributed init (rank {}): {}'", ".", "format", "(", "\n", "args", ".", "distributed_rank", ",", "args", ".", "distributed_init_method", ")", ",", "flush", "=", "True", ")", "\n", "dist", ".", "init_process_group", "(", "\n", "backend", "=", "args", ".", "distributed_backend", ",", "\n", "init_method", "=", "args", ".", "distributed_init_method", ",", "\n", "world_size", "=", "args", ".", "distributed_world_size", ",", "\n", "rank", "=", "args", ".", "distributed_rank", ",", "\n", ")", "\n", "print", "(", "'| initialized host {} as rank {}'", ".", "format", "(", "\n", "socket", ".", "gethostname", "(", ")", ",", "args", ".", "distributed_rank", ")", ",", "flush", "=", "True", ")", "\n", "\n", "# perform a dummy all-reduce to initialize the NCCL communicator", "\n", "dist", ".", "all_reduce", "(", "torch", ".", "rand", "(", "1", ")", ".", "cuda", "(", ")", ")", "\n", "\n", "suppress_output", "(", "is_master", "(", "args", ")", ")", "\n", "\n", "", "args", ".", "distributed_rank", "=", "torch", ".", "distributed", ".", "get_rank", "(", ")", "\n", "return", "args", ".", "distributed_rank", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.distributed_utils.suppress_output": [[98, 109], ["kwargs.pop", "builtin_print"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop"], ["", "def", "suppress_output", "(", "is_master", ")", ":", "\n", "    ", "\"\"\"Suppress printing on the current device. Force printing with `force=True`.\"\"\"", "\n", "import", "builtins", "as", "__builtin__", "\n", "builtin_print", "=", "__builtin__", ".", "print", "\n", "\n", "def", "print", "(", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "force", "=", "kwargs", ".", "pop", "(", "'force'", ",", "False", ")", "\n", "if", "is_master", "or", "force", ":", "\n", "            ", "builtin_print", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n", "", "", "__builtin__", ".", "print", "=", "print", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.distributed_utils.get_rank": [[111, 113], ["torch.get_rank"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.distributed_utils.get_rank"], ["", "def", "get_rank", "(", ")", ":", "\n", "    ", "return", "dist", ".", "get_rank", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.distributed_utils.get_world_size": [[115, 117], ["torch.get_world_size"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.distributed_utils.get_world_size"], ["", "def", "get_world_size", "(", ")", ":", "\n", "    ", "return", "dist", ".", "get_world_size", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.distributed_utils.get_default_group": [[119, 121], ["None"], "function", ["None"], ["", "def", "get_default_group", "(", ")", ":", "\n", "    ", "return", "dist", ".", "group", ".", "WORLD", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.distributed_utils.all_reduce": [[123, 127], ["torch.all_reduce", "distributed_utils.get_default_group"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.distributed_utils.all_reduce", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.distributed_utils.get_default_group"], ["", "def", "all_reduce", "(", "tensor", ",", "group", "=", "None", ")", ":", "\n", "    ", "if", "group", "is", "None", ":", "\n", "        ", "group", "=", "get_default_group", "(", ")", "\n", "", "return", "dist", ".", "all_reduce", "(", "tensor", ",", "group", "=", "group", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.distributed_utils.all_gather_list": [[129, 179], ["distributed_utils.get_rank", "distributed_utils.get_world_size", "buffer.zero_", "pickle.dumps", "len", "torch.ByteTensor", "torch.ByteTensor", "buffer[].copy_", "distributed_utils.all_reduce", "torch.cuda.ByteTensor", "torch.cuda.ByteTensor", "torch.ByteTensor().pin_memory", "torch.ByteTensor().pin_memory", "ValueError", "list", "range", "hasattr", "all_gather_list._buffer.numel", "Exception", "torch.ByteTensor", "torch.ByteTensor", "fairseq.utils.item", "result.append", "fairseq.utils.item", "pickle.loads", "bytes", "out_buffer[].tolist"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.distributed_utils.get_rank", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.distributed_utils.get_world_size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.distributed_utils.all_reduce", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item"], ["", "def", "all_gather_list", "(", "data", ",", "group", "=", "None", ",", "max_size", "=", "16384", ")", ":", "\n", "    ", "\"\"\"Gathers arbitrary data from all nodes into a list.\n\n    Similar to :func:`~torch.distributed.all_gather` but for arbitrary Python\n    data. Note that *data* must be picklable.\n\n    Args:\n        data (Any): data from the local worker to be gathered on other workers\n        group (optional): group of the collective\n        max_size (int, optional): maximum size of the data to be gathered\n            across workers\n    \"\"\"", "\n", "rank", "=", "get_rank", "(", ")", "\n", "world_size", "=", "get_world_size", "(", ")", "\n", "\n", "buffer_size", "=", "max_size", "*", "world_size", "\n", "if", "not", "hasattr", "(", "all_gather_list", ",", "'_buffer'", ")", "or", "all_gather_list", ".", "_buffer", ".", "numel", "(", ")", "<", "buffer_size", ":", "\n", "        ", "all_gather_list", ".", "_buffer", "=", "torch", ".", "cuda", ".", "ByteTensor", "(", "buffer_size", ")", "\n", "all_gather_list", ".", "_cpu_buffer", "=", "torch", ".", "ByteTensor", "(", "max_size", ")", ".", "pin_memory", "(", ")", "\n", "", "buffer", "=", "all_gather_list", ".", "_buffer", "\n", "buffer", ".", "zero_", "(", ")", "\n", "cpu_buffer", "=", "all_gather_list", ".", "_cpu_buffer", "\n", "\n", "enc", "=", "pickle", ".", "dumps", "(", "data", ")", "\n", "enc_size", "=", "len", "(", "enc", ")", "\n", "if", "enc_size", "+", "2", ">", "max_size", ":", "\n", "        ", "raise", "ValueError", "(", "'encoded data exceeds max_size: {}'", ".", "format", "(", "enc_size", "+", "2", ")", ")", "\n", "", "assert", "max_size", "<", "255", "*", "256", "\n", "\n", "cpu_buffer", "[", "0", "]", "=", "enc_size", "//", "255", "# this encoding works for max_size < 65k", "\n", "cpu_buffer", "[", "1", "]", "=", "enc_size", "%", "255", "\n", "cpu_buffer", "[", "2", ":", "enc_size", "+", "2", "]", "=", "torch", ".", "ByteTensor", "(", "list", "(", "enc", ")", ")", "\n", "start", "=", "rank", "*", "max_size", "\n", "size", "=", "enc_size", "+", "2", "\n", "buffer", "[", "start", ":", "start", "+", "size", "]", ".", "copy_", "(", "cpu_buffer", "[", ":", "size", "]", ")", "\n", "\n", "all_reduce", "(", "buffer", ",", "group", "=", "group", ")", "\n", "\n", "try", ":", "\n", "        ", "result", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "world_size", ")", ":", "\n", "            ", "out_buffer", "=", "buffer", "[", "i", "*", "max_size", ":", "(", "i", "+", "1", ")", "*", "max_size", "]", "\n", "size", "=", "(", "255", "*", "utils", ".", "item", "(", "out_buffer", "[", "0", "]", ")", ")", "+", "utils", ".", "item", "(", "out_buffer", "[", "1", "]", ")", "\n", "if", "size", ">", "0", ":", "\n", "                ", "result", ".", "append", "(", "pickle", ".", "loads", "(", "bytes", "(", "out_buffer", "[", "2", ":", "size", "+", "2", "]", ".", "tolist", "(", ")", ")", ")", ")", "\n", "", "", "return", "result", "\n", "", "except", "pickle", ".", "UnpicklingError", ":", "\n", "        ", "raise", "Exception", "(", "\n", "'Unable to unpickle data from other workers. all_gather_list requires all '", "\n", "'workers to enter the function together, so this error usually indicates '", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.legacy_distributed_data_parallel.LegacyDistributedDataParallel.__init__": [[44, 68], ["torch.nn.Module.__init__", "min", "legacy_distributed_data_parallel.LegacyDistributedDataParallel._register_grad_hook", "sum", "p.numel", "module.parameters"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.legacy_distributed_data_parallel.LegacyDistributedDataParallel._register_grad_hook"], ["def", "__init__", "(", "self", ",", "module", ",", "world_size", ",", "process_group", "=", "None", ",", "buffer_size", "=", "2", "**", "28", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "module", "=", "module", "\n", "self", ".", "world_size", "=", "world_size", "\n", "self", ".", "process_group", "=", "process_group", "\n", "\n", "# Never use a bigger buffer than the number of model params", "\n", "self", ".", "buffer_size", "=", "min", "(", "buffer_size", ",", "sum", "(", "p", ".", "numel", "(", ")", "for", "p", "in", "module", ".", "parameters", "(", ")", ")", ")", "\n", "self", ".", "buffer", "=", "None", "\n", "\n", "# Flag used by the NCCL backend to make sure we only reduce gradients", "\n", "# one time in the execution engine", "\n", "self", ".", "need_reduction", "=", "False", "\n", "\n", "# We can also forcibly accumulate grads locally and only do the", "\n", "# all-reduce at some later time", "\n", "self", ".", "accumulate_grads", "=", "False", "\n", "\n", "# For NCCL backend, since every single NCCL call is asynchoronous, we", "\n", "# therefore directly enqueue all the NCCL reduction calls to the", "\n", "# default CUDA stream without spawning up other reduction threads.", "\n", "# This achieves the best performance.", "\n", "self", ".", "_register_grad_hook", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.legacy_distributed_data_parallel.LegacyDistributedDataParallel.__getstate__": [[69, 72], ["copy.copy"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.copy"], ["", "def", "__getstate__", "(", "self", ")", ":", "\n", "        ", "attrs", "=", "copy", ".", "copy", "(", "self", ".", "__dict__", ")", "\n", "return", "attrs", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.legacy_distributed_data_parallel.LegacyDistributedDataParallel.__setstate__": [[73, 76], ["super().__setstate__", "legacy_distributed_data_parallel.LegacyDistributedDataParallel._register_grad_hook"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.legacy_distributed_data_parallel.LegacyDistributedDataParallel.__setstate__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.legacy_distributed_data_parallel.LegacyDistributedDataParallel._register_grad_hook"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "super", "(", ")", ".", "__setstate__", "(", "state", ")", "\n", "self", ".", "_register_grad_hook", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.legacy_distributed_data_parallel.LegacyDistributedDataParallel.no_sync": [[77, 84], ["None"], "methods", ["None"], ["", "@", "contextmanager", "\n", "def", "no_sync", "(", "self", ")", ":", "\n", "        ", "\"\"\"A context manager to disable gradient synchronization.\"\"\"", "\n", "old_accumulate_grads", "=", "self", ".", "accumulate_grads", "\n", "self", ".", "accumulate_grads", "=", "True", "\n", "yield", "\n", "self", ".", "accumulate_grads", "=", "old_accumulate_grads", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.legacy_distributed_data_parallel.LegacyDistributedDataParallel.forward": [[85, 87], ["legacy_distributed_data_parallel.LegacyDistributedDataParallel.module"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "self", ".", "module", "(", "*", "inputs", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.legacy_distributed_data_parallel.LegacyDistributedDataParallel._register_grad_hook": [[88, 181], ["legacy_distributed_data_parallel.LegacyDistributedDataParallel.module.parameters", "distributed_utils.all_reduce", "legacy_distributed_data_parallel.LegacyDistributedDataParallel.module.parameters", "len", "torch.zeros_like.div_", "p.numel", "next().new", "param.numel", "len", "legacy_distributed_data_parallel.LegacyDistributedDataParallel._register_grad_hook.all_reduce"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.distributed_utils.all_reduce", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.distributed_utils.all_reduce"], ["", "def", "_register_grad_hook", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        This function registers the callback all-reduction function for the\n        NCCL backend. All gradients will be all reduced in one single step.\n        The NCCL reduction will directly be enqueued into the default CUDA\n        stream. Therefore, no synchronization is needed.\n        \"\"\"", "\n", "\n", "def", "all_reduce", "(", "params", ")", ":", "\n", "            ", "buffer", "=", "self", ".", "buffer", "\n", "nonzero_buffer", "=", "False", "\n", "if", "len", "(", "params", ")", ">", "1", ":", "\n", "                ", "offset", "=", "0", "\n", "for", "p", "in", "params", ":", "\n", "                    ", "sz", "=", "p", ".", "numel", "(", ")", "\n", "if", "p", ".", "grad", "is", "not", "None", ":", "\n", "                        ", "buffer", "[", "offset", ":", "offset", "+", "sz", "]", ".", "copy_", "(", "p", ".", "grad", ".", "data", ".", "view", "(", "-", "1", ")", ")", "\n", "nonzero_buffer", "=", "True", "\n", "", "else", ":", "\n", "                        ", "buffer", "[", "offset", ":", "offset", "+", "sz", "]", ".", "zero_", "(", ")", "\n", "", "offset", "+=", "sz", "\n", "", "", "else", ":", "\n", "# we only have a single grad to all-reduce", "\n", "                ", "p", "=", "params", "[", "0", "]", "\n", "if", "p", ".", "grad", "is", "not", "None", ":", "\n", "                    ", "buffer", "=", "p", ".", "grad", ".", "data", "\n", "nonzero_buffer", "=", "True", "\n", "", "elif", "p", ".", "numel", "(", ")", "<=", "self", ".", "buffer", ".", "numel", "(", ")", ":", "\n", "                    ", "buffer", "=", "buffer", "[", ":", "p", ".", "numel", "(", ")", "]", "\n", "buffer", ".", "zero_", "(", ")", "\n", "", "else", ":", "\n", "                    ", "buffer", "=", "torch", ".", "zeros_like", "(", "p", ")", "\n", "\n", "", "", "if", "nonzero_buffer", ":", "\n", "                ", "buffer", ".", "div_", "(", "self", ".", "world_size", ")", "\n", "\n", "", "distributed_utils", ".", "all_reduce", "(", "buffer", ",", "self", ".", "process_group", ")", "\n", "\n", "# copy all-reduced grads back into their original place", "\n", "offset", "=", "0", "\n", "for", "p", "in", "params", ":", "\n", "                ", "sz", "=", "p", ".", "numel", "(", ")", "\n", "if", "p", ".", "grad", "is", "not", "None", ":", "\n", "                    ", "p", ".", "grad", ".", "data", ".", "copy_", "(", "buffer", "[", "offset", ":", "offset", "+", "sz", "]", ".", "view_as", "(", "p", ")", ")", "\n", "", "else", ":", "\n", "                    ", "p", ".", "grad", "=", "buffer", "[", "offset", ":", "offset", "+", "sz", "]", ".", "view_as", "(", "p", ")", ".", "clone", "(", ")", "\n", "", "offset", "+=", "sz", "\n", "\n", "", "", "def", "reduction_fn", "(", ")", ":", "\n", "# This function only needs to be called once", "\n", "            ", "if", "not", "self", ".", "need_reduction", "or", "self", ".", "accumulate_grads", ":", "\n", "                ", "return", "\n", "", "self", ".", "need_reduction", "=", "False", "\n", "\n", "if", "self", ".", "buffer", "is", "None", ":", "\n", "                ", "self", ".", "buffer", "=", "next", "(", "self", ".", "module", ".", "parameters", "(", ")", ")", ".", "new", "(", "self", ".", "buffer_size", ")", "\n", "\n", "# All-reduce the gradients in buckets", "\n", "", "offset", "=", "0", "\n", "buffered_params", "=", "[", "]", "\n", "for", "param", "in", "self", ".", "module", ".", "parameters", "(", ")", ":", "\n", "                ", "if", "not", "param", ".", "requires_grad", ":", "\n", "                    ", "continue", "\n", "", "if", "param", ".", "grad", "is", "None", ":", "\n", "                    ", "param", ".", "grad", "=", "torch", ".", "zeros_like", "(", "param", ")", "\n", "", "if", "param", ".", "grad", ".", "requires_grad", ":", "\n", "                    ", "raise", "RuntimeError", "(", "\"DistributedDataParallel only works \"", "\n", "\"with gradients that don't require \"", "\n", "\"grad\"", ")", "\n", "", "sz", "=", "param", ".", "numel", "(", ")", "\n", "if", "sz", ">", "self", ".", "buffer", ".", "numel", "(", ")", ":", "\n", "# all-reduce big params directly", "\n", "                    ", "all_reduce", "(", "[", "param", "]", ")", "\n", "", "else", ":", "\n", "                    ", "if", "offset", "+", "sz", ">", "self", ".", "buffer", ".", "numel", "(", ")", ":", "\n", "                        ", "all_reduce", "(", "buffered_params", ")", "\n", "offset", "=", "0", "\n", "buffered_params", ".", "clear", "(", ")", "\n", "", "buffered_params", ".", "append", "(", "param", ")", "\n", "offset", "+=", "sz", "\n", "\n", "", "", "if", "len", "(", "buffered_params", ")", ">", "0", ":", "\n", "                ", "all_reduce", "(", "buffered_params", ")", "\n", "\n", "# Now register the reduction hook on the parameters", "\n", "", "", "for", "p", "in", "self", ".", "module", ".", "parameters", "(", ")", ":", "\n", "\n", "            ", "def", "allreduce_hook", "(", "*", "unused", ")", ":", "\n", "                ", "self", ".", "need_reduction", "=", "True", "\n", "Variable", ".", "_execution_engine", ".", "queue_callback", "(", "reduction_fn", ")", "\n", "\n", "", "if", "p", ".", "requires_grad", ":", "\n", "                ", "p", ".", "register_hook", "(", "allreduce_hook", ")", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.progress_bar.__init__": [[69, 78], ["getattr"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "iterable", ",", "epoch", "=", "None", ",", "prefix", "=", "None", ")", ":", "\n", "        ", "self", ".", "iterable", "=", "iterable", "\n", "self", ".", "offset", "=", "getattr", "(", "iterable", ",", "'offset'", ",", "0", ")", "\n", "self", ".", "epoch", "=", "epoch", "\n", "self", ".", "prefix", "=", "''", "\n", "if", "epoch", "is", "not", "None", ":", "\n", "            ", "self", ".", "prefix", "+=", "'| epoch {:03d}'", ".", "format", "(", "epoch", ")", "\n", "", "if", "prefix", "is", "not", "None", ":", "\n", "            ", "self", ".", "prefix", "+=", "' | {}'", ".", "format", "(", "prefix", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.progress_bar.__len__": [[79, 81], ["len"], "methods", ["None"], ["", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "iterable", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.progress_bar.__enter__": [[82, 84], ["None"], "methods", ["None"], ["", "def", "__enter__", "(", "self", ")", ":", "\n", "        ", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.progress_bar.__exit__": [[85, 87], ["None"], "methods", ["None"], ["", "def", "__exit__", "(", "self", ",", "*", "exc", ")", ":", "\n", "        ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.progress_bar.__iter__": [[88, 90], ["None"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.progress_bar.log": [[91, 94], ["None"], "methods", ["None"], ["", "def", "log", "(", "self", ",", "stats", ",", "tag", "=", "''", ",", "step", "=", "None", ")", ":", "\n", "        ", "\"\"\"Log intermediate stats according to log_interval.\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.progress_bar.print": [[95, 98], ["None"], "methods", ["None"], ["", "def", "print", "(", "self", ",", "stats", ",", "tag", "=", "''", ",", "step", "=", "None", ")", ":", "\n", "        ", "\"\"\"Print end-of-epoch stats.\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.progress_bar._str_commas": [[99, 102], ["stats[].strip", "stats.keys"], "methods", ["None"], ["", "def", "_str_commas", "(", "self", ",", "stats", ")", ":", "\n", "        ", "return", "', '", ".", "join", "(", "key", "+", "'='", "+", "stats", "[", "key", "]", ".", "strip", "(", ")", "\n", "for", "key", "in", "stats", ".", "keys", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.progress_bar._str_pipes": [[103, 106], ["stats[].strip", "stats.keys"], "methods", ["None"], ["", "def", "_str_pipes", "(", "self", ",", "stats", ")", ":", "\n", "        ", "return", "' | '", ".", "join", "(", "key", "+", "' '", "+", "stats", "[", "key", "]", ".", "strip", "(", ")", "\n", "for", "key", "in", "stats", ".", "keys", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.progress_bar._format_stats": [[107, 113], ["collections.OrderedDict", "collections.OrderedDict.keys", "str", "progress_bar.format_stat"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.format_stat"], ["", "def", "_format_stats", "(", "self", ",", "stats", ")", ":", "\n", "        ", "postfix", "=", "OrderedDict", "(", "stats", ")", "\n", "# Preprocess stats according to datatype", "\n", "for", "key", "in", "postfix", ".", "keys", "(", ")", ":", "\n", "            ", "postfix", "[", "key", "]", "=", "str", "(", "format_stat", "(", "postfix", "[", "key", "]", ")", ")", "\n", "", "return", "postfix", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.json_progress_bar.__init__": [[118, 122], ["progress_bar.progress_bar.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["def", "__init__", "(", "self", ",", "iterable", ",", "epoch", "=", "None", ",", "prefix", "=", "None", ",", "log_interval", "=", "1000", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "iterable", ",", "epoch", ",", "prefix", ")", "\n", "self", ".", "log_interval", "=", "log_interval", "\n", "self", ".", "stats", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.json_progress_bar.__iter__": [[123, 132], ["float", "enumerate", "len", "progress_bar.json_progress_bar._format_stats", "progress_bar.json_progress_bar.print"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.json_progress_bar._format_stats", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "size", "=", "float", "(", "len", "(", "self", ".", "iterable", ")", ")", "\n", "for", "i", ",", "obj", "in", "enumerate", "(", "self", ".", "iterable", ",", "start", "=", "self", ".", "offset", ")", ":", "\n", "            ", "yield", "obj", "\n", "if", "self", ".", "stats", "is", "not", "None", "and", "i", ">", "0", "and", "self", ".", "log_interval", "is", "not", "None", "and", "i", "%", "self", ".", "log_interval", "==", "0", ":", "\n", "                ", "update", "=", "self", ".", "epoch", "-", "1", "+", "float", "(", "i", "/", "size", ")", "if", "self", ".", "epoch", "is", "not", "None", "else", "None", "\n", "stats", "=", "self", ".", "_format_stats", "(", "self", ".", "stats", ",", "epoch", "=", "self", ".", "epoch", ",", "update", "=", "update", ")", "\n", "print", "(", "json", ".", "dumps", "(", "stats", ")", ",", "flush", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.json_progress_bar.log": [[133, 136], ["None"], "methods", ["None"], ["", "", "", "def", "log", "(", "self", ",", "stats", ",", "tag", "=", "''", ",", "step", "=", "None", ")", ":", "\n", "        ", "\"\"\"Log intermediate stats according to log_interval.\"\"\"", "\n", "self", ".", "stats", "=", "stats", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.json_progress_bar.print": [[137, 144], ["progress_bar.json_progress_bar._format_stats", "progress_bar.json_progress_bar.print"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.json_progress_bar._format_stats", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["", "def", "print", "(", "self", ",", "stats", ",", "tag", "=", "''", ",", "step", "=", "None", ")", ":", "\n", "        ", "\"\"\"Print end-of-epoch stats.\"\"\"", "\n", "self", ".", "stats", "=", "stats", "\n", "if", "tag", "!=", "''", ":", "\n", "            ", "self", ".", "stats", "=", "OrderedDict", "(", "[", "(", "tag", "+", "'_'", "+", "k", ",", "v", ")", "for", "k", ",", "v", "in", "self", ".", "stats", ".", "items", "(", ")", "]", ")", "\n", "", "stats", "=", "self", ".", "_format_stats", "(", "self", ".", "stats", ",", "epoch", "=", "self", ".", "epoch", ")", "\n", "print", "(", "json", ".", "dumps", "(", "stats", ")", ",", "flush", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.json_progress_bar._format_stats": [[145, 155], ["collections.OrderedDict", "stats.keys", "round", "progress_bar.format_stat"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.format_stat"], ["", "def", "_format_stats", "(", "self", ",", "stats", ",", "epoch", "=", "None", ",", "update", "=", "None", ")", ":", "\n", "        ", "postfix", "=", "OrderedDict", "(", ")", "\n", "if", "epoch", "is", "not", "None", ":", "\n", "            ", "postfix", "[", "'epoch'", "]", "=", "epoch", "\n", "", "if", "update", "is", "not", "None", ":", "\n", "            ", "postfix", "[", "'update'", "]", "=", "round", "(", "update", ",", "3", ")", "\n", "# Preprocess stats according to datatype", "\n", "", "for", "key", "in", "stats", ".", "keys", "(", ")", ":", "\n", "            ", "postfix", "[", "key", "]", "=", "format_stat", "(", "stats", "[", "key", "]", ")", "\n", "", "return", "postfix", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.noop_progress_bar.__init__": [[160, 162], ["progress_bar.progress_bar.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["def", "__init__", "(", "self", ",", "iterable", ",", "epoch", "=", "None", ",", "prefix", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "iterable", ",", "epoch", ",", "prefix", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.noop_progress_bar.__iter__": [[163, 166], ["None"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "for", "obj", "in", "self", ".", "iterable", ":", "\n", "            ", "yield", "obj", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.noop_progress_bar.log": [[167, 170], ["None"], "methods", ["None"], ["", "", "def", "log", "(", "self", ",", "stats", ",", "tag", "=", "''", ",", "step", "=", "None", ")", ":", "\n", "        ", "\"\"\"Log intermediate stats according to log_interval.\"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.noop_progress_bar.print": [[171, 174], ["None"], "methods", ["None"], ["", "def", "print", "(", "self", ",", "stats", ",", "tag", "=", "''", ",", "step", "=", "None", ")", ":", "\n", "        ", "\"\"\"Print end-of-epoch stats.\"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.simple_progress_bar.__init__": [[179, 183], ["progress_bar.progress_bar.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["def", "__init__", "(", "self", ",", "iterable", ",", "epoch", "=", "None", ",", "prefix", "=", "None", ",", "log_interval", "=", "1000", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "iterable", ",", "epoch", ",", "prefix", ")", "\n", "self", ".", "log_interval", "=", "log_interval", "\n", "self", ".", "stats", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.simple_progress_bar.__iter__": [[184, 193], ["len", "enumerate", "progress_bar.simple_progress_bar._str_commas", "progress_bar.simple_progress_bar.print"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.progress_bar._str_commas", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "size", "=", "len", "(", "self", ".", "iterable", ")", "\n", "for", "i", ",", "obj", "in", "enumerate", "(", "self", ".", "iterable", ",", "start", "=", "self", ".", "offset", ")", ":", "\n", "            ", "yield", "obj", "\n", "if", "self", ".", "stats", "is", "not", "None", "and", "i", ">", "0", "and", "self", ".", "log_interval", "is", "not", "None", "and", "i", "%", "self", ".", "log_interval", "==", "0", ":", "\n", "                ", "postfix", "=", "self", ".", "_str_commas", "(", "self", ".", "stats", ")", "\n", "print", "(", "'{}:  {:5d} / {:d} {}'", ".", "format", "(", "self", ".", "prefix", ",", "i", ",", "size", ",", "postfix", ")", ",", "\n", "flush", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.simple_progress_bar.log": [[194, 197], ["progress_bar.simple_progress_bar._format_stats"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.json_progress_bar._format_stats"], ["", "", "", "def", "log", "(", "self", ",", "stats", ",", "tag", "=", "''", ",", "step", "=", "None", ")", ":", "\n", "        ", "\"\"\"Log intermediate stats according to log_interval.\"\"\"", "\n", "self", ".", "stats", "=", "self", ".", "_format_stats", "(", "stats", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.simple_progress_bar.print": [[198, 202], ["progress_bar.simple_progress_bar._str_pipes", "progress_bar.simple_progress_bar.print"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.progress_bar._str_pipes", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["", "def", "print", "(", "self", ",", "stats", ",", "tag", "=", "''", ",", "step", "=", "None", ")", ":", "\n", "        ", "\"\"\"Print end-of-epoch stats.\"\"\"", "\n", "postfix", "=", "self", ".", "_str_pipes", "(", "self", ".", "_format_stats", "(", "stats", ")", ")", "\n", "print", "(", "'{} | {}'", ".", "format", "(", "self", ".", "prefix", ",", "postfix", ")", ",", "flush", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tqdm_progress_bar.__init__": [[207, 211], ["progress_bar.progress_bar.__init__", "tqdm"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["def", "__init__", "(", "self", ",", "iterable", ",", "epoch", "=", "None", ",", "prefix", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "iterable", ",", "epoch", ",", "prefix", ")", "\n", "from", "tqdm", "import", "tqdm", "\n", "self", ".", "tqdm", "=", "tqdm", "(", "iterable", ",", "self", ".", "prefix", ",", "leave", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tqdm_progress_bar.__iter__": [[212, 214], ["iter"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "return", "iter", "(", "self", ".", "tqdm", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tqdm_progress_bar.log": [[215, 218], ["progress_bar.tqdm_progress_bar.tqdm.set_postfix", "progress_bar.tqdm_progress_bar._format_stats"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.json_progress_bar._format_stats"], ["", "def", "log", "(", "self", ",", "stats", ",", "tag", "=", "''", ",", "step", "=", "None", ")", ":", "\n", "        ", "\"\"\"Log intermediate stats according to log_interval.\"\"\"", "\n", "self", ".", "tqdm", ".", "set_postfix", "(", "self", ".", "_format_stats", "(", "stats", ")", ",", "refresh", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tqdm_progress_bar.print": [[219, 223], ["progress_bar.tqdm_progress_bar._str_pipes", "progress_bar.tqdm_progress_bar.tqdm.write", "progress_bar.tqdm_progress_bar._format_stats"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.progress_bar._str_pipes", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.json_progress_bar._format_stats"], ["", "def", "print", "(", "self", ",", "stats", ",", "tag", "=", "''", ",", "step", "=", "None", ")", ":", "\n", "        ", "\"\"\"Print end-of-epoch stats.\"\"\"", "\n", "postfix", "=", "self", ".", "_str_pipes", "(", "self", ".", "_format_stats", "(", "stats", ")", ")", "\n", "self", ".", "tqdm", ".", "write", "(", "'{} | {}'", ".", "format", "(", "self", ".", "tqdm", ".", "desc", ",", "postfix", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.__init__": [[228, 241], ["progress_bar.tensorboard_log_wrapper.print"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["def", "__init__", "(", "self", ",", "wrapped_bar", ",", "tensorboard_logdir", ",", "args", ")", ":", "\n", "        ", "self", ".", "wrapped_bar", "=", "wrapped_bar", "\n", "self", ".", "tensorboard_logdir", "=", "tensorboard_logdir", "\n", "self", ".", "args", "=", "args", "\n", "\n", "try", ":", "\n", "            ", "from", "tensorboardX", "import", "SummaryWriter", "\n", "self", ".", "SummaryWriter", "=", "SummaryWriter", "\n", "self", ".", "_writers", "=", "{", "}", "\n", "", "except", "ImportError", ":", "\n", "            ", "print", "(", "\"tensorboard or required dependencies not found, \"", "\n", "\"please see README for using tensorboard. (e.g. pip install tensorboardX)\"", ")", "\n", "self", ".", "SummaryWriter", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper._writer": [[242, 252], ["progress_bar.tensorboard_log_wrapper.SummaryWriter", "progress_bar.tensorboard_log_wrapper._writers[].add_text", "progress_bar.tensorboard_log_wrapper._writers[].add_text", "os.path.join", "str", "vars"], "methods", ["None"], ["", "", "def", "_writer", "(", "self", ",", "key", ")", ":", "\n", "        ", "if", "self", ".", "SummaryWriter", "is", "None", ":", "\n", "            ", "return", "None", "\n", "", "if", "key", "not", "in", "self", ".", "_writers", ":", "\n", "            ", "self", ".", "_writers", "[", "key", "]", "=", "self", ".", "SummaryWriter", "(", "\n", "os", ".", "path", ".", "join", "(", "self", ".", "tensorboard_logdir", ",", "key", ")", ",", "\n", ")", "\n", "self", ".", "_writers", "[", "key", "]", ".", "add_text", "(", "'args'", ",", "str", "(", "vars", "(", "self", ".", "args", ")", ")", ")", "\n", "self", ".", "_writers", "[", "key", "]", ".", "add_text", "(", "'sys.argv'", ",", "\" \"", ".", "join", "(", "sys", ".", "argv", ")", ")", "\n", "", "return", "self", ".", "_writers", "[", "key", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.__iter__": [[253, 255], ["iter"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "return", "iter", "(", "self", ".", "wrapped_bar", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.log": [[256, 260], ["progress_bar.tensorboard_log_wrapper._log_to_tensorboard", "progress_bar.tensorboard_log_wrapper.wrapped_bar.log"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper._log_to_tensorboard", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.log"], ["", "def", "log", "(", "self", ",", "stats", ",", "tag", "=", "''", ",", "step", "=", "None", ")", ":", "\n", "        ", "\"\"\"Log intermediate stats to tensorboard.\"\"\"", "\n", "self", ".", "_log_to_tensorboard", "(", "stats", ",", "tag", ",", "step", ")", "\n", "self", ".", "wrapped_bar", ".", "log", "(", "stats", ",", "tag", "=", "tag", ",", "step", "=", "step", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print": [[261, 265], ["progress_bar.tensorboard_log_wrapper._log_to_tensorboard", "progress_bar.tensorboard_log_wrapper.wrapped_bar.print"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper._log_to_tensorboard", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["", "def", "print", "(", "self", ",", "stats", ",", "tag", "=", "''", ",", "step", "=", "None", ")", ":", "\n", "        ", "\"\"\"Print end-of-epoch stats.\"\"\"", "\n", "self", ".", "_log_to_tensorboard", "(", "stats", ",", "tag", ",", "step", ")", "\n", "self", ".", "wrapped_bar", ".", "print", "(", "stats", ",", "tag", "=", "tag", ",", "step", "=", "step", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.__exit__": [[266, 270], ["getattr().values", "writer.close", "getattr"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.GoldMiner.close"], ["", "def", "__exit__", "(", "self", ",", "*", "exc", ")", ":", "\n", "        ", "for", "writer", "in", "getattr", "(", "self", ",", "'_writers'", ",", "{", "}", ")", ".", "values", "(", ")", ":", "\n", "            ", "writer", ".", "close", "(", ")", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper._log_to_tensorboard": [[271, 282], ["progress_bar.tensorboard_log_wrapper._writer", "stats.keys", "isinstance", "progress_bar.tensorboard_log_wrapper.add_scalar", "isinstance", "progress_bar.tensorboard_log_wrapper.add_scalar"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper._writer"], ["", "def", "_log_to_tensorboard", "(", "self", ",", "stats", ",", "tag", "=", "''", ",", "step", "=", "None", ")", ":", "\n", "        ", "writer", "=", "self", ".", "_writer", "(", "tag", ")", "\n", "if", "writer", "is", "None", ":", "\n", "            ", "return", "\n", "", "if", "step", "is", "None", ":", "\n", "            ", "step", "=", "stats", "[", "'num_updates'", "]", "\n", "", "for", "key", "in", "stats", ".", "keys", "(", ")", "-", "{", "'num_updates'", "}", ":", "\n", "            ", "if", "isinstance", "(", "stats", "[", "key", "]", ",", "AverageMeter", ")", ":", "\n", "                ", "writer", ".", "add_scalar", "(", "key", ",", "stats", "[", "key", "]", ".", "val", ",", "step", ")", "\n", "", "elif", "isinstance", "(", "stats", "[", "key", "]", ",", "Number", ")", ":", "\n", "                ", "writer", ".", "add_scalar", "(", "key", ",", "stats", "[", "key", "]", ",", "step", ")", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.build_progress_bar": [[22, 53], ["progress_bar.json_progress_bar", "fairseq.distributed_utils.is_master", "g_tbmf_wrapper", "sys.stderr.isatty", "progress_bar.noop_progress_bar", "fairseq.distributed_utils.is_master", "progress_bar.tensorboard_log_wrapper", "progress_bar.simple_progress_bar", "progress_bar.tqdm_progress_bar", "ValueError", "ImportError"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.distributed_utils.is_master", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.distributed_utils.is_master"], ["def", "build_progress_bar", "(", "args", ",", "iterator", ",", "epoch", "=", "None", ",", "prefix", "=", "None", ",", "default", "=", "'tqdm'", ",", "no_progress_bar", "=", "'none'", ")", ":", "\n", "    ", "if", "args", ".", "log_format", "is", "None", ":", "\n", "        ", "args", ".", "log_format", "=", "no_progress_bar", "if", "args", ".", "no_progress_bar", "else", "default", "\n", "\n", "", "if", "args", ".", "log_format", "==", "'tqdm'", "and", "not", "sys", ".", "stderr", ".", "isatty", "(", ")", ":", "\n", "        ", "args", ".", "log_format", "=", "'simple'", "\n", "\n", "", "if", "args", ".", "log_format", "==", "'json'", ":", "\n", "        ", "bar", "=", "json_progress_bar", "(", "iterator", ",", "epoch", ",", "prefix", ",", "args", ".", "log_interval", ")", "\n", "", "elif", "args", ".", "log_format", "==", "'none'", ":", "\n", "        ", "bar", "=", "noop_progress_bar", "(", "iterator", ",", "epoch", ",", "prefix", ")", "\n", "", "elif", "args", ".", "log_format", "==", "'simple'", ":", "\n", "        ", "bar", "=", "simple_progress_bar", "(", "iterator", ",", "epoch", ",", "prefix", ",", "args", ".", "log_interval", ")", "\n", "", "elif", "args", ".", "log_format", "==", "'tqdm'", ":", "\n", "        ", "bar", "=", "tqdm_progress_bar", "(", "iterator", ",", "epoch", ",", "prefix", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "'Unknown log format: {}'", ".", "format", "(", "args", ".", "log_format", ")", ")", "\n", "\n", "", "if", "args", ".", "tbmf_wrapper", "and", "distributed_utils", ".", "is_master", "(", "args", ")", ":", "\n", "        ", "global", "g_tbmf_wrapper", "\n", "if", "g_tbmf_wrapper", "is", "None", ":", "\n", "            ", "try", ":", "\n", "                ", "from", "fairseq", ".", "fb_tbmf_wrapper", "import", "fb_tbmf_wrapper", "\n", "", "except", "Exception", ":", "\n", "                ", "raise", "ImportError", "(", "\"fb_tbmf_wrapper package not found.\"", ")", "\n", "", "g_tbmf_wrapper", "=", "fb_tbmf_wrapper", "\n", "", "bar", "=", "g_tbmf_wrapper", "(", "bar", ",", "args", ",", "args", ".", "log_interval", ")", "\n", "", "elif", "args", ".", "tensorboard_logdir", "and", "distributed_utils", ".", "is_master", "(", "args", ")", ":", "\n", "        ", "bar", "=", "tensorboard_log_wrapper", "(", "bar", ",", "args", ".", "tensorboard_logdir", ",", "args", ")", "\n", "\n", "", "return", "bar", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.format_stat": [[55, 65], ["isinstance", "isinstance", "isinstance", "isinstance", "round", "round"], "function", ["None"], ["", "def", "format_stat", "(", "stat", ")", ":", "\n", "    ", "if", "isinstance", "(", "stat", ",", "Number", ")", ":", "\n", "        ", "stat", "=", "'{:g}'", ".", "format", "(", "stat", ")", "\n", "", "elif", "isinstance", "(", "stat", ",", "AverageMeter", ")", ":", "\n", "        ", "stat", "=", "'{:.3f}'", ".", "format", "(", "stat", ".", "avg", ")", "\n", "", "elif", "isinstance", "(", "stat", ",", "TimeMeter", ")", ":", "\n", "        ", "stat", "=", "'{:g}'", ".", "format", "(", "round", "(", "stat", ".", "avg", ")", ")", "\n", "", "elif", "isinstance", "(", "stat", ",", "StopwatchMeter", ")", ":", "\n", "        ", "stat", "=", "'{:g}'", ".", "format", "(", "round", "(", "stat", ".", "sum", ")", ")", "\n", "", "return", "stat", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.hub_utils.GeneratorHubInterface.__init__": [[71, 98], ["torch.nn.Module.__init__", "torch.nn.ModuleList", "fairseq.utils.load_align_dict", "fairseq.data.encoders.build_tokenizer", "fairseq.data.encoders.build_bpe", "hub_utils.GeneratorHubInterface.register_buffer", "model.make_generation_fast_", "getattr", "torch.tensor", "getattr", "getattr", "getattr"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.load_align_dict", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.TransformerDecoderLayer.make_generation_fast_"], ["def", "__init__", "(", "self", ",", "args", ",", "task", ",", "models", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "args", "=", "args", "\n", "self", ".", "task", "=", "task", "\n", "self", ".", "models", "=", "nn", ".", "ModuleList", "(", "models", ")", "\n", "self", ".", "src_dict", "=", "task", ".", "source_dictionary", "\n", "self", ".", "tgt_dict", "=", "task", ".", "target_dictionary", "\n", "\n", "# optimize model for generation", "\n", "for", "model", "in", "self", ".", "models", ":", "\n", "            ", "model", ".", "make_generation_fast_", "(", "\n", "beamable_mm_beam_size", "=", "(", "\n", "None", "if", "getattr", "(", "args", ",", "'no_beamable_mm'", ",", "False", ")", "\n", "else", "getattr", "(", "args", ",", "'beam'", ",", "5", ")", "\n", ")", ",", "\n", "need_attn", "=", "getattr", "(", "args", ",", "'print_alignment'", ",", "False", ")", ",", "\n", ")", "\n", "\n", "# Load alignment dictionary for unknown word replacement", "\n", "# (None if no unknown word replacement, empty if no path to align dictionary)", "\n", "", "self", ".", "align_dict", "=", "utils", ".", "load_align_dict", "(", "getattr", "(", "args", ",", "'replace_unk'", ",", "None", ")", ")", "\n", "\n", "self", ".", "tokenizer", "=", "encoders", ".", "build_tokenizer", "(", "args", ")", "\n", "self", ".", "bpe", "=", "encoders", ".", "build_bpe", "(", "args", ")", "\n", "\n", "# this is useful for determining the device", "\n", "self", ".", "register_buffer", "(", "'_float_tensor'", ",", "torch", ".", "tensor", "(", "[", "0", "]", ",", "dtype", "=", "torch", ".", "float", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.hub_utils.GeneratorHubInterface.device": [[99, 102], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "device", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_float_tensor", ".", "device", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.hub_utils.GeneratorHubInterface.translate": [[103, 105], ["hub_utils.GeneratorHubInterface.sample"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.hub_utils.GeneratorHubInterface.sample"], ["", "def", "translate", "(", "self", ",", "sentence", ":", "str", ",", "beam", ":", "int", "=", "5", ",", "verbose", ":", "bool", "=", "False", ",", "**", "kwargs", ")", "->", "str", ":", "\n", "        ", "return", "self", ".", "sample", "(", "sentence", ",", "beam", ",", "verbose", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.hub_utils.GeneratorHubInterface.sample": [[106, 110], ["hub_utils.GeneratorHubInterface.encode", "hub_utils.GeneratorHubInterface.decode", "hub_utils.GeneratorHubInterface.generate"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.encode", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.decode", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.sequence_generator.SequenceGenerator.generate"], ["", "def", "sample", "(", "self", ",", "sentence", ":", "str", ",", "beam", ":", "int", "=", "1", ",", "verbose", ":", "bool", "=", "False", ",", "**", "kwargs", ")", "->", "str", ":", "\n", "        ", "input", "=", "self", ".", "encode", "(", "sentence", ")", "\n", "hypo", "=", "self", ".", "generate", "(", "input", ",", "beam", ",", "verbose", ",", "**", "kwargs", ")", "[", "0", "]", "[", "'tokens'", "]", "\n", "return", "self", ".", "decode", "(", "hypo", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.hub_utils.GeneratorHubInterface.generate": [[111, 145], ["hub_utils.GeneratorHubInterface._build_sample", "copy.copy", "kwargs.items", "hub_utils.GeneratorHubInterface.task.build_generator", "hub_utils.GeneratorHubInterface.task.inference_step", "setattr", "hub_utils.GeneratorHubInterface.string", "print", "getattr", "getattr", "hub_utils.GeneratorHubInterface.decode", "print", "print", "hub_utils.GeneratorHubInterface.generate.getarg"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.hub_utils.GeneratorHubInterface._build_sample", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.copy", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.fairseq_task.FairseqTask.build_generator", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.translation_moe.TranslationMoETask.inference_step", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.hub_utils.GeneratorHubInterface.string", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.decode", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["", "def", "generate", "(", "self", ",", "tokens", ":", "torch", ".", "LongTensor", ",", "beam", ":", "int", "=", "5", ",", "verbose", ":", "bool", "=", "False", ",", "**", "kwargs", ")", "->", "torch", ".", "LongTensor", ":", "\n", "        ", "sample", "=", "self", ".", "_build_sample", "(", "tokens", ")", "\n", "\n", "# build generator using current args as well as any kwargs", "\n", "gen_args", "=", "copy", ".", "copy", "(", "self", ".", "args", ")", "\n", "gen_args", ".", "beam", "=", "beam", "\n", "for", "k", ",", "v", "in", "kwargs", ".", "items", "(", ")", ":", "\n", "            ", "setattr", "(", "gen_args", ",", "k", ",", "v", ")", "\n", "", "generator", "=", "self", ".", "task", ".", "build_generator", "(", "gen_args", ")", "\n", "\n", "translations", "=", "self", ".", "task", ".", "inference_step", "(", "generator", ",", "self", ".", "models", ",", "sample", ")", "\n", "\n", "if", "verbose", ":", "\n", "            ", "src_str_with_unk", "=", "self", ".", "string", "(", "tokens", ")", "\n", "print", "(", "'S\\t{}'", ".", "format", "(", "src_str_with_unk", ")", ")", "\n", "\n", "", "def", "getarg", "(", "name", ",", "default", ")", ":", "\n", "            ", "return", "getattr", "(", "gen_args", ",", "name", ",", "getattr", "(", "self", ".", "args", ",", "name", ",", "default", ")", ")", "\n", "\n", "# Process top predictions", "\n", "", "hypos", "=", "translations", "[", "0", "]", "\n", "if", "verbose", ":", "\n", "            ", "for", "hypo", "in", "hypos", ":", "\n", "                ", "hypo_str", "=", "self", ".", "decode", "(", "hypo", "[", "'tokens'", "]", ")", "\n", "print", "(", "'H\\t{}\\t{}'", ".", "format", "(", "hypo", "[", "'score'", "]", ",", "hypo_str", ")", ")", "\n", "print", "(", "'P\\t{}'", ".", "format", "(", "\n", "' '", ".", "join", "(", "map", "(", "lambda", "x", ":", "'{:.4f}'", ".", "format", "(", "x", ")", ",", "hypo", "[", "'positional_scores'", "]", ".", "tolist", "(", ")", ")", ")", "\n", ")", ")", "\n", "if", "hypo", "[", "'alignment'", "]", "is", "not", "None", "and", "getarg", "(", "'print_alignment'", ",", "False", ")", ":", "\n", "                    ", "print", "(", "'A\\t{}'", ".", "format", "(", "\n", "' '", ".", "join", "(", "map", "(", "lambda", "x", ":", "str", "(", "utils", ".", "item", "(", "x", ")", ")", ",", "hypo", "[", "'alignment'", "]", ".", "int", "(", ")", ".", "cpu", "(", ")", ")", ")", "\n", ")", ")", "\n", "\n", "", "", "", "return", "hypos", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.hub_utils.GeneratorHubInterface.encode": [[146, 150], ["hub_utils.GeneratorHubInterface.tokenize", "hub_utils.GeneratorHubInterface.apply_bpe", "hub_utils.GeneratorHubInterface.binarize"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.hub_utils.GeneratorHubInterface.tokenize", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.hub_utils.GeneratorHubInterface.apply_bpe", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.binarizer.Binarizer.binarize"], ["", "def", "encode", "(", "self", ",", "sentence", ":", "str", ")", "->", "torch", ".", "LongTensor", ":", "\n", "        ", "sentence", "=", "self", ".", "tokenize", "(", "sentence", ")", "\n", "sentence", "=", "self", ".", "apply_bpe", "(", "sentence", ")", "\n", "return", "self", ".", "binarize", "(", "sentence", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.hub_utils.GeneratorHubInterface.decode": [[151, 155], ["hub_utils.GeneratorHubInterface.string", "hub_utils.GeneratorHubInterface.remove_bpe", "hub_utils.GeneratorHubInterface.detokenize"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.hub_utils.GeneratorHubInterface.string", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.hub_utils.GeneratorHubInterface.remove_bpe", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.hub_utils.GeneratorHubInterface.detokenize"], ["", "def", "decode", "(", "self", ",", "tokens", ":", "torch", ".", "LongTensor", ")", "->", "str", ":", "\n", "        ", "sentence", "=", "self", ".", "string", "(", "tokens", ")", "\n", "sentence", "=", "self", ".", "remove_bpe", "(", "sentence", ")", "\n", "return", "self", ".", "detokenize", "(", "sentence", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.hub_utils.GeneratorHubInterface.tokenize": [[156, 160], ["hub_utils.GeneratorHubInterface.tokenizer.encode"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.encode"], ["", "def", "tokenize", "(", "self", ",", "sentence", ":", "str", ")", "->", "str", ":", "\n", "        ", "if", "self", ".", "tokenizer", "is", "not", "None", ":", "\n", "            ", "sentence", "=", "self", ".", "tokenizer", ".", "encode", "(", "sentence", ")", "\n", "", "return", "sentence", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.hub_utils.GeneratorHubInterface.detokenize": [[161, 165], ["hub_utils.GeneratorHubInterface.tokenizer.decode"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.decode"], ["", "def", "detokenize", "(", "self", ",", "sentence", ":", "str", ")", "->", "str", ":", "\n", "        ", "if", "self", ".", "tokenizer", "is", "not", "None", ":", "\n", "            ", "sentence", "=", "self", ".", "tokenizer", ".", "decode", "(", "sentence", ")", "\n", "", "return", "sentence", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.hub_utils.GeneratorHubInterface.apply_bpe": [[166, 170], ["hub_utils.GeneratorHubInterface.bpe.encode"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.encode"], ["", "def", "apply_bpe", "(", "self", ",", "sentence", ":", "str", ")", "->", "str", ":", "\n", "        ", "if", "self", ".", "bpe", "is", "not", "None", ":", "\n", "            ", "sentence", "=", "self", ".", "bpe", ".", "encode", "(", "sentence", ")", "\n", "", "return", "sentence", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.hub_utils.GeneratorHubInterface.remove_bpe": [[171, 175], ["hub_utils.GeneratorHubInterface.bpe.decode"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.decode"], ["", "def", "remove_bpe", "(", "self", ",", "sentence", ":", "str", ")", "->", "str", ":", "\n", "        ", "if", "self", ".", "bpe", "is", "not", "None", ":", "\n", "            ", "sentence", "=", "self", ".", "bpe", ".", "decode", "(", "sentence", ")", "\n", "", "return", "sentence", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.hub_utils.GeneratorHubInterface.binarize": [[176, 178], ["hub_utils.GeneratorHubInterface.src_dict.encode_line().long", "hub_utils.GeneratorHubInterface.src_dict.encode_line"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.encode_line"], ["", "def", "binarize", "(", "self", ",", "sentence", ":", "str", ")", "->", "torch", ".", "LongTensor", ":", "\n", "        ", "return", "self", ".", "src_dict", ".", "encode_line", "(", "sentence", ",", "add_if_not_exist", "=", "False", ")", ".", "long", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.hub_utils.GeneratorHubInterface.string": [[179, 181], ["hub_utils.GeneratorHubInterface.tgt_dict.string"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.hub_utils.GeneratorHubInterface.string"], ["", "def", "string", "(", "self", ",", "tokens", ":", "torch", ".", "LongTensor", ")", "->", "str", ":", "\n", "        ", "return", "self", ".", "tgt_dict", ".", "string", "(", "tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.hub_utils.GeneratorHubInterface._build_sample": [[182, 191], ["torch.is_tensor", "hub_utils.GeneratorHubInterface.task.build_dataset_for_inference", "hub_utils.GeneratorHubInterface.collater", "fairseq.utils.apply_to_sample", "src_tokens.numel", "tensor.to"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.masked_lm.MaskedLMTask.build_dataset_for_inference", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.collater", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.apply_to_sample"], ["", "def", "_build_sample", "(", "self", ",", "src_tokens", ":", "torch", ".", "LongTensor", ")", ":", "\n", "        ", "assert", "torch", ".", "is_tensor", "(", "src_tokens", ")", "\n", "dataset", "=", "self", ".", "task", ".", "build_dataset_for_inference", "(", "[", "src_tokens", "]", ",", "[", "src_tokens", ".", "numel", "(", ")", "]", ")", "\n", "sample", "=", "dataset", ".", "collater", "(", "[", "dataset", "[", "0", "]", "]", ")", "\n", "sample", "=", "utils", ".", "apply_to_sample", "(", "\n", "lambda", "tensor", ":", "tensor", ".", "to", "(", "self", ".", "device", ")", ",", "\n", "sample", "\n", ")", "\n", "return", "sample", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.hub_utils.BPEHubInterface.__init__": [[196, 201], ["object.__init__", "argparse.Namespace", "fairseq.data.encoders.build_bpe"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["def", "__init__", "(", "self", ",", "bpe", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "args", "=", "argparse", ".", "Namespace", "(", "bpe", "=", "bpe", ",", "**", "kwargs", ")", "\n", "self", ".", "bpe", "=", "encoders", ".", "build_bpe", "(", "args", ")", "\n", "assert", "self", ".", "bpe", "is", "not", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.hub_utils.BPEHubInterface.encode": [[202, 204], ["hub_utils.BPEHubInterface.bpe.encode"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.encode"], ["", "def", "encode", "(", "self", ",", "sentence", ":", "str", ")", "->", "str", ":", "\n", "        ", "return", "self", ".", "bpe", ".", "encode", "(", "sentence", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.hub_utils.BPEHubInterface.decode": [[205, 207], ["hub_utils.BPEHubInterface.bpe.decode"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.decode"], ["", "def", "decode", "(", "self", ",", "sentence", ":", "str", ")", "->", "str", ":", "\n", "        ", "return", "self", ".", "bpe", ".", "decode", "(", "sentence", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.hub_utils.TokenizerHubInterface.__init__": [[212, 217], ["object.__init__", "argparse.Namespace", "fairseq.data.encoders.build_tokenizer"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["def", "__init__", "(", "self", ",", "tokenizer", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "args", "=", "argparse", ".", "Namespace", "(", "tokenizer", "=", "tokenizer", ",", "**", "kwargs", ")", "\n", "self", ".", "tokenizer", "=", "encoders", ".", "build_tokenizer", "(", "args", ")", "\n", "assert", "self", ".", "tokenizer", "is", "not", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.hub_utils.TokenizerHubInterface.encode": [[218, 220], ["hub_utils.TokenizerHubInterface.tokenizer.encode"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.encode"], ["", "def", "encode", "(", "self", ",", "sentence", ":", "str", ")", "->", "str", ":", "\n", "        ", "return", "self", ".", "tokenizer", ".", "encode", "(", "sentence", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.hub_utils.TokenizerHubInterface.decode": [[221, 223], ["hub_utils.TokenizerHubInterface.tokenizer.decode"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.decode"], ["", "def", "decode", "(", "self", ",", "sentence", ":", "str", ")", "->", "str", ":", "\n", "        ", "return", "self", ".", "tokenizer", ".", "decode", "(", "sentence", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.hub_utils.from_pretrained": [[18, 62], ["file_utils.load_archive_file", "data_name_or_path.startswith", "checkpoint_utils.load_model_ensemble_and_task", "os.path.abspath", "file_utils.load_archive_file", "os.path.join", "os.path.exists", "fairseq.utils.import_user_module", "os.path.join", "argparse.Namespace", "os.path.join", "checkpoint_file.split"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.file_utils.load_archive_file", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.checkpoint_utils.load_model_ensemble_and_task", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.file_utils.load_archive_file", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.import_user_module", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["def", "from_pretrained", "(", "\n", "model_name_or_path", ",", "\n", "checkpoint_file", "=", "'model.pt'", ",", "\n", "data_name_or_path", "=", "'.'", ",", "\n", "archive_map", "=", "None", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "    ", "from", "fairseq", "import", "checkpoint_utils", ",", "file_utils", "\n", "\n", "if", "archive_map", "is", "not", "None", ":", "\n", "        ", "if", "model_name_or_path", "in", "archive_map", ":", "\n", "            ", "model_name_or_path", "=", "archive_map", "[", "model_name_or_path", "]", "\n", "", "if", "data_name_or_path", "is", "not", "None", "and", "data_name_or_path", "in", "archive_map", ":", "\n", "            ", "data_name_or_path", "=", "archive_map", "[", "data_name_or_path", "]", "\n", "\n", "", "", "model_path", "=", "file_utils", ".", "load_archive_file", "(", "model_name_or_path", ")", "\n", "\n", "# convenience hack for loading data and BPE codes from model archive", "\n", "if", "data_name_or_path", ".", "startswith", "(", "'.'", ")", ":", "\n", "        ", "kwargs", "[", "'data'", "]", "=", "os", ".", "path", ".", "abspath", "(", "os", ".", "path", ".", "join", "(", "model_path", ",", "data_name_or_path", ")", ")", "\n", "", "else", ":", "\n", "        ", "kwargs", "[", "'data'", "]", "=", "file_utils", ".", "load_archive_file", "(", "data_name_or_path", ")", "\n", "", "for", "file", ",", "arg", "in", "{", "\n", "'code'", ":", "'bpe_codes'", ",", "\n", "'bpecodes'", ":", "'bpe_codes'", ",", "\n", "'sentencepiece.bpe.model'", ":", "'sentencepiece_vocab'", ",", "\n", "}", ".", "items", "(", ")", ":", "\n", "#print('SEE MODEL_PATH',model_path,kwargs)", "\n", "        ", "path", "=", "os", ".", "path", ".", "join", "(", "model_path", ",", "file", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "path", ")", ":", "\n", "            ", "kwargs", "[", "arg", "]", "=", "path", "\n", "\n", "", "", "if", "'user_dir'", "in", "kwargs", ":", "\n", "        ", "utils", ".", "import_user_module", "(", "argparse", ".", "Namespace", "(", "user_dir", "=", "kwargs", "[", "'user_dir'", "]", ")", ")", "\n", "\n", "", "models", ",", "args", ",", "task", "=", "checkpoint_utils", ".", "load_model_ensemble_and_task", "(", "\n", "[", "os", ".", "path", ".", "join", "(", "model_path", ",", "cpt", ")", "for", "cpt", "in", "checkpoint_file", ".", "split", "(", "':'", ")", "]", ",", "\n", "arg_overrides", "=", "kwargs", ",", "\n", ")", "\n", "\n", "return", "{", "\n", "'args'", ":", "args", ",", "\n", "'task'", ":", "task", ",", "\n", "'models'", ":", "models", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.sequence_scorer.SequenceScorer.__init__": [[15, 19], ["tgt_dict.pad"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad"], ["def", "__init__", "(", "self", ",", "tgt_dict", ",", "softmax_batch", "=", "None", ")", ":", "\n", "        ", "self", ".", "pad", "=", "tgt_dict", ".", "pad", "(", ")", "\n", "self", ".", "softmax_batch", "=", "softmax_batch", "or", "sys", ".", "maxsize", "\n", "assert", "self", ".", "softmax_batch", ">", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.sequence_scorer.SequenceScorer.generate": [[20, 115], ["torch.no_grad", "avg_probs.size", "range", "curr_prob.new.gather", "model.eval", "model.forward", "sequence_scorer.SequenceScorer.generate.batch_for_softmax"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.model.AMRModel.forward"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "generate", "(", "self", ",", "models", ",", "sample", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Score a batch of translations.\"\"\"", "\n", "net_input", "=", "sample", "[", "'net_input'", "]", "\n", "\n", "def", "batch_for_softmax", "(", "dec_out", ",", "target", ")", ":", "\n", "# assumes decoder_out[0] is the only thing needed (may not be correct for future models!)", "\n", "            ", "first", ",", "rest", "=", "dec_out", "[", "0", "]", ",", "dec_out", "[", "1", ":", "]", "\n", "bsz", ",", "tsz", ",", "dim", "=", "first", ".", "shape", "\n", "if", "bsz", "*", "tsz", "<", "self", ".", "softmax_batch", ":", "\n", "                ", "yield", "dec_out", ",", "target", ",", "True", "\n", "", "else", ":", "\n", "                ", "flat", "=", "first", ".", "contiguous", "(", ")", ".", "view", "(", "1", ",", "-", "1", ",", "dim", ")", "\n", "flat_tgt", "=", "target", ".", "contiguous", "(", ")", ".", "view", "(", "flat", ".", "shape", "[", ":", "-", "1", "]", ")", "\n", "s", "=", "0", "\n", "while", "s", "<", "flat", ".", "size", "(", "1", ")", ":", "\n", "                    ", "e", "=", "s", "+", "self", ".", "softmax_batch", "\n", "yield", "(", "flat", "[", ":", ",", "s", ":", "e", "]", ",", ")", "+", "rest", ",", "flat_tgt", "[", ":", ",", "s", ":", "e", "]", ",", "False", "\n", "s", "=", "e", "\n", "\n", "", "", "", "def", "gather_target_probs", "(", "probs", ",", "target", ")", ":", "\n", "            ", "probs", "=", "probs", ".", "gather", "(", "\n", "dim", "=", "2", ",", "\n", "index", "=", "target", ".", "unsqueeze", "(", "-", "1", ")", ",", "\n", ")", "\n", "return", "probs", "\n", "\n", "", "orig_target", "=", "sample", "[", "'target'", "]", "\n", "\n", "# compute scores for each model in the ensemble", "\n", "avg_probs", "=", "None", "\n", "avg_attn", "=", "None", "\n", "for", "model", "in", "models", ":", "\n", "            ", "model", ".", "eval", "(", ")", "\n", "decoder_out", "=", "model", ".", "forward", "(", "**", "net_input", ")", "\n", "attn", "=", "decoder_out", "[", "1", "]", "\n", "\n", "batched", "=", "batch_for_softmax", "(", "decoder_out", ",", "orig_target", ")", "\n", "probs", ",", "idx", "=", "None", ",", "0", "\n", "for", "bd", ",", "tgt", ",", "is_single", "in", "batched", ":", "\n", "                ", "sample", "[", "'target'", "]", "=", "tgt", "\n", "curr_prob", "=", "model", ".", "get_normalized_probs", "(", "bd", ",", "log_probs", "=", "len", "(", "models", ")", "==", "1", ",", "sample", "=", "sample", ")", ".", "data", "\n", "if", "is_single", ":", "\n", "                    ", "probs", "=", "gather_target_probs", "(", "curr_prob", ",", "orig_target", ")", "\n", "", "else", ":", "\n", "                    ", "if", "probs", "is", "None", ":", "\n", "                        ", "probs", "=", "curr_prob", ".", "new", "(", "orig_target", ".", "numel", "(", ")", ")", "\n", "", "step", "=", "curr_prob", ".", "size", "(", "0", ")", "*", "curr_prob", ".", "size", "(", "1", ")", "\n", "end", "=", "step", "+", "idx", "\n", "tgt_probs", "=", "gather_target_probs", "(", "curr_prob", ".", "view", "(", "tgt", ".", "shape", "+", "(", "curr_prob", ".", "size", "(", "-", "1", ")", ",", ")", ")", ",", "tgt", ")", "\n", "probs", "[", "idx", ":", "end", "]", "=", "tgt_probs", ".", "view", "(", "-", "1", ")", "\n", "idx", "=", "end", "\n", "", "sample", "[", "'target'", "]", "=", "orig_target", "\n", "\n", "", "probs", "=", "probs", ".", "view", "(", "sample", "[", "'target'", "]", ".", "shape", ")", "\n", "\n", "if", "avg_probs", "is", "None", ":", "\n", "                ", "avg_probs", "=", "probs", "\n", "", "else", ":", "\n", "                ", "avg_probs", ".", "add_", "(", "probs", ")", "\n", "", "if", "attn", "is", "not", "None", "and", "torch", ".", "is_tensor", "(", "attn", ")", ":", "\n", "                ", "attn", "=", "attn", ".", "data", "\n", "if", "avg_attn", "is", "None", ":", "\n", "                    ", "avg_attn", "=", "attn", "\n", "", "else", ":", "\n", "                    ", "avg_attn", ".", "add_", "(", "attn", ")", "\n", "", "", "", "if", "len", "(", "models", ")", ">", "1", ":", "\n", "            ", "avg_probs", ".", "div_", "(", "len", "(", "models", ")", ")", "\n", "avg_probs", ".", "log_", "(", ")", "\n", "if", "avg_attn", "is", "not", "None", ":", "\n", "                ", "avg_attn", ".", "div_", "(", "len", "(", "models", ")", ")", "\n", "\n", "", "", "bsz", "=", "avg_probs", ".", "size", "(", "0", ")", "\n", "hypos", "=", "[", "]", "\n", "start_idxs", "=", "sample", "[", "'start_indices'", "]", "if", "'start_indices'", "in", "sample", "else", "[", "0", "]", "*", "bsz", "\n", "for", "i", "in", "range", "(", "bsz", ")", ":", "\n", "# remove padding from ref", "\n", "            ", "ref", "=", "utils", ".", "strip_pad", "(", "sample", "[", "'target'", "]", "[", "i", ",", "start_idxs", "[", "i", "]", ":", "]", ",", "self", ".", "pad", ")", "if", "sample", "[", "'target'", "]", "is", "not", "None", "else", "None", "\n", "tgt_len", "=", "ref", ".", "numel", "(", ")", "\n", "avg_probs_i", "=", "avg_probs", "[", "i", "]", "[", "start_idxs", "[", "i", "]", ":", "start_idxs", "[", "i", "]", "+", "tgt_len", "]", "\n", "score_i", "=", "avg_probs_i", ".", "sum", "(", ")", "/", "tgt_len", "\n", "if", "avg_attn", "is", "not", "None", ":", "\n", "                ", "avg_attn_i", "=", "avg_attn", "[", "i", ",", "start_idxs", "[", "i", "]", ":", "]", "\n", "_", ",", "alignment", "=", "avg_attn_i", ".", "max", "(", "dim", "=", "0", ")", "\n", "", "else", ":", "\n", "                ", "avg_attn_i", "=", "alignment", "=", "None", "\n", "", "hypos", ".", "append", "(", "[", "{", "\n", "'tokens'", ":", "ref", ",", "\n", "'score'", ":", "score_i", ",", "\n", "'attention'", ":", "avg_attn_i", ",", "\n", "'alignment'", ":", "alignment", ",", "\n", "'positional_scores'", ":", "avg_probs_i", ",", "\n", "}", "]", ")", "\n", "", "return", "hypos", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.tokenizer.tokenize_line": [[11, 15], ["SPACE_NORMALIZER.sub", "line.strip.strip", "line.strip.split"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["def", "tokenize_line", "(", "line", ")", ":", "\n", "    ", "line", "=", "SPACE_NORMALIZER", ".", "sub", "(", "\" \"", ",", "line", ")", "\n", "line", "=", "line", ".", "strip", "(", ")", "\n", "return", "line", ".", "split", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.tokenizer.tab_tokenize": [[17, 20], ["line.strip.strip", "line.strip.split"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["", "def", "tab_tokenize", "(", "line", ")", ":", "\n", "    ", "line", "=", "line", ".", "strip", "(", ")", "\n", "return", "line", ".", "split", "(", "'\\t'", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.file_utils.load_archive_file": [[52, 88], ["file_utils.cached_path", "print", "print", "os.path.isdir", "tempfile.mkdtemp", "print", "os.remove", "shutil.move", "shutil.rmtree", "print", "tarfile.open", "os.path.commonprefix", "archive.extractall", "os.path.join", "os.path.splitext", "archive.getnames"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.file_utils.cached_path", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["def", "load_archive_file", "(", "archive_file", ")", ":", "\n", "# redirect to the cache, if necessary", "\n", "    ", "try", ":", "\n", "        ", "resolved_archive_file", "=", "cached_path", "(", "archive_file", ",", "cache_dir", "=", "None", ")", "\n", "", "except", "EnvironmentError", ":", "\n", "        ", "print", "(", "\n", "\"Archive name '{}' was not found in archive name list. \"", "\n", "\"We assumed '{}' was a path or URL but couldn't find any file \"", "\n", "\"associated to this path or URL.\"", ".", "format", "(", "\n", "archive_file", ",", "\n", "archive_file", ",", "\n", ")", "\n", ")", "\n", "return", "None", "\n", "\n", "", "if", "resolved_archive_file", "==", "archive_file", ":", "\n", "        ", "print", "(", "\"loading archive file {}\"", ".", "format", "(", "archive_file", ")", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"loading archive file {} from cache at {}\"", ".", "format", "(", "\n", "archive_file", ",", "resolved_archive_file", ")", ")", "\n", "\n", "# Extract archive to temp dir and replace .tar.bz2 if necessary", "\n", "", "tempdir", "=", "None", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "resolved_archive_file", ")", ":", "\n", "        ", "tempdir", "=", "tempfile", ".", "mkdtemp", "(", ")", "\n", "print", "(", "\"extracting archive file {} to temp dir {}\"", ".", "format", "(", "\n", "resolved_archive_file", ",", "tempdir", ")", ")", "\n", "ext", "=", "os", ".", "path", ".", "splitext", "(", "archive_file", ")", "[", "1", "]", "[", "1", ":", "]", "\n", "with", "tarfile", ".", "open", "(", "resolved_archive_file", ",", "'r:'", "+", "ext", ")", "as", "archive", ":", "\n", "            ", "top_dir", "=", "os", ".", "path", ".", "commonprefix", "(", "archive", ".", "getnames", "(", ")", ")", "\n", "archive", ".", "extractall", "(", "tempdir", ")", "\n", "", "os", ".", "remove", "(", "resolved_archive_file", ")", "\n", "shutil", ".", "move", "(", "os", ".", "path", ".", "join", "(", "tempdir", ",", "top_dir", ")", ",", "resolved_archive_file", ")", "\n", "shutil", ".", "rmtree", "(", "tempdir", ")", "\n", "\n", "", "return", "resolved_archive_file", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.file_utils.url_to_filename": [[90, 106], ["url.encode", "hashlib.sha256", "hashlib.sha256.hexdigest", "etag.encode", "hashlib.sha256", "hashlib.sha256.hexdigest"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.encode", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.encode"], ["", "def", "url_to_filename", "(", "url", ",", "etag", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Convert `url` into a hashed filename in a repeatable way.\n    If `etag` is specified, append its hash to the URL's, delimited\n    by a period.\n    \"\"\"", "\n", "url_bytes", "=", "url", ".", "encode", "(", "'utf-8'", ")", "\n", "url_hash", "=", "sha256", "(", "url_bytes", ")", "\n", "filename", "=", "url_hash", ".", "hexdigest", "(", ")", "\n", "\n", "if", "etag", ":", "\n", "        ", "etag_bytes", "=", "etag", ".", "encode", "(", "'utf-8'", ")", "\n", "etag_hash", "=", "sha256", "(", "etag_bytes", ")", "\n", "filename", "+=", "'.'", "+", "etag_hash", ".", "hexdigest", "(", ")", "\n", "\n", "", "return", "filename", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.file_utils.filename_to_url": [[108, 132], ["isinstance", "os.path.join", "str", "os.path.exists", "EnvironmentError", "os.path.exists", "EnvironmentError", "io.open", "json.load"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load"], ["", "def", "filename_to_url", "(", "filename", ",", "cache_dir", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Return the url and etag (which may be ``None``) stored for `filename`.\n    Raise ``EnvironmentError`` if `filename` or its stored metadata do not exist.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "PYTORCH_FAIRSEQ_CACHE", "\n", "", "if", "isinstance", "(", "cache_dir", ",", "Path", ")", ":", "\n", "        ", "cache_dir", "=", "str", "(", "cache_dir", ")", "\n", "\n", "", "cache_path", "=", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "filename", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "cache_path", ")", ":", "\n", "        ", "raise", "EnvironmentError", "(", "\"file {} not found\"", ".", "format", "(", "cache_path", ")", ")", "\n", "\n", "", "meta_path", "=", "cache_path", "+", "'.json'", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "meta_path", ")", ":", "\n", "        ", "raise", "EnvironmentError", "(", "\"file {} not found\"", ".", "format", "(", "meta_path", ")", ")", "\n", "\n", "", "with", "open", "(", "meta_path", ",", "encoding", "=", "\"utf-8\"", ")", "as", "meta_file", ":", "\n", "        ", "metadata", "=", "json", ".", "load", "(", "meta_file", ")", "\n", "", "url", "=", "metadata", "[", "'url'", "]", "\n", "etag", "=", "metadata", "[", "'etag'", "]", "\n", "\n", "return", "url", ",", "etag", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.file_utils.cached_path": [[134, 162], ["isinstance", "isinstance", "urlparse", "str", "str", "file_utils.get_from_cache", "os.path.exists", "EnvironmentError", "ValueError"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.file_utils.get_from_cache", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists"], ["", "def", "cached_path", "(", "url_or_filename", ",", "cache_dir", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Given something that might be a URL (or might be a local path),\n    determine which. If it's a URL, download the file and cache it, and\n    return the path to the cached file. If it's already a local path,\n    make sure the file exists and then return the path.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "PYTORCH_FAIRSEQ_CACHE", "\n", "", "if", "isinstance", "(", "url_or_filename", ",", "Path", ")", ":", "\n", "        ", "url_or_filename", "=", "str", "(", "url_or_filename", ")", "\n", "", "if", "isinstance", "(", "cache_dir", ",", "Path", ")", ":", "\n", "        ", "cache_dir", "=", "str", "(", "cache_dir", ")", "\n", "\n", "", "parsed", "=", "urlparse", "(", "url_or_filename", ")", "\n", "\n", "if", "parsed", ".", "scheme", "in", "(", "'http'", ",", "'https'", ",", "'s3'", ")", ":", "\n", "# URL, so get it from the cache (downloading if necessary)", "\n", "        ", "return", "get_from_cache", "(", "url_or_filename", ",", "cache_dir", ")", "\n", "", "elif", "os", ".", "path", ".", "exists", "(", "url_or_filename", ")", ":", "\n", "# File, and it exists.", "\n", "        ", "return", "url_or_filename", "\n", "", "elif", "parsed", ".", "scheme", "==", "''", ":", "\n", "# File, but it doesn't exist.", "\n", "        ", "raise", "EnvironmentError", "(", "\"file {} not found\"", ".", "format", "(", "url_or_filename", ")", ")", "\n", "", "else", ":", "\n", "# Something unknown", "\n", "        ", "raise", "ValueError", "(", "\"unable to parse {} as a URL or as a local path\"", ".", "format", "(", "url_or_filename", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.file_utils.split_s3_path": [[164, 175], ["urlparse", "s3_path.startswith", "ValueError"], "function", ["None"], ["", "", "def", "split_s3_path", "(", "url", ")", ":", "\n", "    ", "\"\"\"Split a full s3 path into the bucket name and path.\"\"\"", "\n", "parsed", "=", "urlparse", "(", "url", ")", "\n", "if", "not", "parsed", ".", "netloc", "or", "not", "parsed", ".", "path", ":", "\n", "        ", "raise", "ValueError", "(", "\"bad s3 path {}\"", ".", "format", "(", "url", ")", ")", "\n", "", "bucket_name", "=", "parsed", ".", "netloc", "\n", "s3_path", "=", "parsed", ".", "path", "\n", "# Remove '/' at beginning of path.", "\n", "if", "s3_path", ".", "startswith", "(", "\"/\"", ")", ":", "\n", "        ", "s3_path", "=", "s3_path", "[", "1", ":", "]", "\n", "", "return", "bucket_name", ",", "s3_path", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.file_utils.s3_request": [[177, 195], ["functools.wraps", "func", "int", "EnvironmentError"], "function", ["None"], ["", "def", "s3_request", "(", "func", ")", ":", "\n", "    ", "\"\"\"\n    Wrapper function for s3 requests in order to create more helpful error\n    messages.\n    \"\"\"", "\n", "\n", "@", "wraps", "(", "func", ")", "\n", "def", "wrapper", "(", "url", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "from", "botocore", ".", "exceptions", "import", "ClientError", "\n", "try", ":", "\n", "            ", "return", "func", "(", "url", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "", "except", "ClientError", "as", "exc", ":", "\n", "            ", "if", "int", "(", "exc", ".", "response", "[", "\"Error\"", "]", "[", "\"Code\"", "]", ")", "==", "404", ":", "\n", "                ", "raise", "EnvironmentError", "(", "\"file {} not found\"", ".", "format", "(", "url", ")", ")", "\n", "", "else", ":", "\n", "                ", "raise", "\n", "\n", "", "", "", "return", "wrapper", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.file_utils.s3_etag": [[197, 205], ["boto3.resource", "file_utils.split_s3_path", "boto3.resource.Object"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.file_utils.split_s3_path"], ["", "@", "s3_request", "\n", "def", "s3_etag", "(", "url", ")", ":", "\n", "    ", "\"\"\"Check ETag on S3 object.\"\"\"", "\n", "import", "boto3", "\n", "s3_resource", "=", "boto3", ".", "resource", "(", "\"s3\"", ")", "\n", "bucket_name", ",", "s3_path", "=", "split_s3_path", "(", "url", ")", "\n", "s3_object", "=", "s3_resource", ".", "Object", "(", "bucket_name", ",", "s3_path", ")", "\n", "return", "s3_object", ".", "e_tag", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.file_utils.s3_get": [[207, 214], ["boto3.resource", "file_utils.split_s3_path", "boto3.resource.Bucket().download_fileobj", "boto3.resource.Bucket"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.file_utils.split_s3_path"], ["", "@", "s3_request", "\n", "def", "s3_get", "(", "url", ",", "temp_file", ")", ":", "\n", "    ", "\"\"\"Pull a file directly from S3.\"\"\"", "\n", "import", "boto3", "\n", "s3_resource", "=", "boto3", ".", "resource", "(", "\"s3\"", ")", "\n", "bucket_name", ",", "s3_path", "=", "split_s3_path", "(", "url", ")", "\n", "s3_resource", ".", "Bucket", "(", "bucket_name", ")", ".", "download_fileobj", "(", "s3_path", ",", "temp_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.file_utils.http_get": [[216, 228], ["requests.get", "requests.get.headers.get", "tqdm", "requests.get.iter_content", "tqdm.close", "int", "tqdm.update", "temp_file.write", "len"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.GoldMiner.close", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write"], ["", "def", "http_get", "(", "url", ",", "temp_file", ")", ":", "\n", "    ", "import", "requests", "\n", "from", "tqdm", "import", "tqdm", "\n", "req", "=", "requests", ".", "get", "(", "url", ",", "stream", "=", "True", ")", "\n", "content_length", "=", "req", ".", "headers", ".", "get", "(", "'Content-Length'", ")", "\n", "total", "=", "int", "(", "content_length", ")", "if", "content_length", "is", "not", "None", "else", "None", "\n", "progress", "=", "tqdm", "(", "unit", "=", "\"B\"", ",", "total", "=", "total", ")", "\n", "for", "chunk", "in", "req", ".", "iter_content", "(", "chunk_size", "=", "1024", ")", ":", "\n", "        ", "if", "chunk", ":", "# filter out keep-alive new chunks", "\n", "            ", "progress", ".", "update", "(", "len", "(", "chunk", ")", ")", "\n", "temp_file", ".", "write", "(", "chunk", ")", "\n", "", "", "progress", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.file_utils.get_from_cache": [[230, 301], ["isinstance", "url.startswith", "file_utils.url_to_filename", "os.path.join", "str", "os.path.exists", "os.makedirs", "file_utils.s3_etag", "fnmatch.filter", "list", "os.path.exists", "requests.head", "os.path.exists", "os.listdir", "filter", "os.path.join", "tempfile.NamedTemporaryFile", "logger.info", "url.startswith", "temp_file.flush", "temp_file.seek", "logger.info", "logger.info", "logger.info", "requests.head.headers.get", "file_utils.s3_get", "file_utils.http_get", "io.open", "shutil.copyfileobj", "io.open", "json.dumps", "meta_file.write", "s.endswith"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.file_utils.url_to_filename", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.file_utils.s3_etag", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.token_block_dataset.DatasetSearcher.seek", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.file_utils.s3_get", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.file_utils.http_get", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write"], ["", "def", "get_from_cache", "(", "url", ",", "cache_dir", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Given a URL, look for the corresponding dataset in the local cache.\n    If it's not there, download it. Then return the path to the cached file.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "PYTORCH_FAIRSEQ_CACHE", "\n", "", "if", "isinstance", "(", "cache_dir", ",", "Path", ")", ":", "\n", "        ", "cache_dir", "=", "str", "(", "cache_dir", ")", "\n", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "cache_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "cache_dir", ")", "\n", "\n", "# Get eTag to add to filename, if it exists.", "\n", "", "if", "url", ".", "startswith", "(", "\"s3://\"", ")", ":", "\n", "        ", "etag", "=", "s3_etag", "(", "url", ")", "\n", "", "else", ":", "\n", "        ", "try", ":", "\n", "            ", "import", "requests", "\n", "response", "=", "requests", ".", "head", "(", "url", ",", "allow_redirects", "=", "True", ")", "\n", "if", "response", ".", "status_code", "!=", "200", ":", "\n", "                ", "etag", "=", "None", "\n", "", "else", ":", "\n", "                ", "etag", "=", "response", ".", "headers", ".", "get", "(", "\"ETag\"", ")", "\n", "", "", "except", "EnvironmentError", ":", "\n", "            ", "etag", "=", "None", "\n", "\n", "", "", "filename", "=", "url_to_filename", "(", "url", ",", "etag", ")", "\n", "\n", "# get cache path to put the file", "\n", "cache_path", "=", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "filename", ")", "\n", "\n", "# If we don't have a connection (etag is None) and can't identify the file", "\n", "# try to get the last downloaded one", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "cache_path", ")", "and", "etag", "is", "None", ":", "\n", "        ", "matching_files", "=", "fnmatch", ".", "filter", "(", "os", ".", "listdir", "(", "cache_dir", ")", ",", "filename", "+", "'.*'", ")", "\n", "matching_files", "=", "list", "(", "filter", "(", "lambda", "s", ":", "not", "s", ".", "endswith", "(", "'.json'", ")", ",", "matching_files", ")", ")", "\n", "if", "matching_files", ":", "\n", "            ", "cache_path", "=", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "matching_files", "[", "-", "1", "]", ")", "\n", "\n", "", "", "if", "not", "os", ".", "path", ".", "exists", "(", "cache_path", ")", ":", "\n", "# Download to temporary file, then copy to cache dir once finished.", "\n", "# Otherwise you get corrupt cache entries if the download gets interrupted.", "\n", "        ", "with", "tempfile", ".", "NamedTemporaryFile", "(", ")", "as", "temp_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"%s not found in cache, downloading to %s\"", ",", "url", ",", "temp_file", ".", "name", ")", "\n", "\n", "# GET file object", "\n", "if", "url", ".", "startswith", "(", "\"s3://\"", ")", ":", "\n", "                ", "s3_get", "(", "url", ",", "temp_file", ")", "\n", "", "else", ":", "\n", "                ", "http_get", "(", "url", ",", "temp_file", ")", "\n", "\n", "# we are copying the file before closing it, so flush to avoid truncation", "\n", "", "temp_file", ".", "flush", "(", ")", "\n", "# shutil.copyfileobj() starts at the current position, so go to the start", "\n", "temp_file", ".", "seek", "(", "0", ")", "\n", "\n", "logger", ".", "info", "(", "\"copying %s to cache at %s\"", ",", "temp_file", ".", "name", ",", "cache_path", ")", "\n", "with", "open", "(", "cache_path", ",", "'wb'", ")", "as", "cache_file", ":", "\n", "                ", "shutil", ".", "copyfileobj", "(", "temp_file", ",", "cache_file", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"creating metadata file for %s\"", ",", "cache_path", ")", "\n", "meta", "=", "{", "'url'", ":", "url", ",", "'etag'", ":", "etag", "}", "\n", "meta_path", "=", "cache_path", "+", "'.json'", "\n", "with", "open", "(", "meta_path", ",", "'w'", ")", "as", "meta_file", ":", "\n", "                ", "output_string", "=", "json", ".", "dumps", "(", "meta", ")", "\n", "meta_file", ".", "write", "(", "output_string", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"removing temp file %s\"", ",", "temp_file", ".", "name", ")", "\n", "\n", "", "", "return", "cache_path", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.file_utils.read_set_from_file": [[303, 313], ["set", "io.open", "set.add", "line.rstrip"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.add"], ["", "def", "read_set_from_file", "(", "filename", ")", ":", "\n", "    ", "'''\n    Extract a de-duped collection (set) of text from a file.\n    Expected file format is one item per line.\n    '''", "\n", "collection", "=", "set", "(", ")", "\n", "with", "open", "(", "filename", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "file_", ":", "\n", "        ", "for", "line", "in", "file_", ":", "\n", "            ", "collection", ".", "add", "(", "line", ".", "rstrip", "(", ")", ")", "\n", "", "", "return", "collection", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.file_utils.get_file_extension": [[315, 319], ["os.path.splitext", "ext.lower"], "function", ["None"], ["", "def", "get_file_extension", "(", "path", ",", "dot", "=", "True", ",", "lower", "=", "True", ")", ":", "\n", "    ", "ext", "=", "os", ".", "path", ".", "splitext", "(", "path", ")", "[", "1", "]", "\n", "ext", "=", "ext", "if", "dot", "else", "ext", "[", "1", ":", "]", "\n", "return", "ext", ".", "lower", "(", ")", "if", "lower", "else", "ext", "\n", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.bleu.SacrebleuScorer.__init__": [[37, 41], ["bleu.SacrebleuScorer.reset"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.reset"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "import", "sacrebleu", "\n", "self", ".", "sacrebleu", "=", "sacrebleu", "\n", "self", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.bleu.SacrebleuScorer.reset": [[42, 47], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ",", "one_init", "=", "False", ")", ":", "\n", "        ", "if", "one_init", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "", "self", ".", "ref", "=", "[", "]", "\n", "self", ".", "sys", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.bleu.SacrebleuScorer.add_string": [[48, 51], ["bleu.SacrebleuScorer.ref.append", "bleu.SacrebleuScorer.sys.append"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "add_string", "(", "self", ",", "ref", ",", "pred", ")", ":", "\n", "        ", "self", ".", "ref", ".", "append", "(", "ref", ")", "\n", "self", ".", "sys", ".", "append", "(", "pred", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.bleu.SacrebleuScorer.score": [[52, 54], ["bleu.SacrebleuScorer.result_string"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.bleu.Scorer.result_string"], ["", "def", "score", "(", "self", ",", "order", "=", "4", ")", ":", "\n", "        ", "return", "self", ".", "result_string", "(", "order", ")", ".", "score", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.bleu.SacrebleuScorer.result_string": [[55, 59], ["bleu.SacrebleuScorer.sacrebleu.corpus_bleu"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.translation_moe.score.corpus_bleu"], ["", "def", "result_string", "(", "self", ",", "order", "=", "4", ")", ":", "\n", "        ", "if", "order", "!=", "4", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "", "return", "self", ".", "sacrebleu", ".", "corpus_bleu", "(", "self", ".", "sys", ",", "[", "self", ".", "ref", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.bleu.Scorer.__init__": [[62, 68], ["bleu.BleuStat", "bleu.Scorer.reset"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.reset"], ["    ", "def", "__init__", "(", "self", ",", "pad", ",", "eos", ",", "unk", ")", ":", "\n", "        ", "self", ".", "stat", "=", "BleuStat", "(", ")", "\n", "self", ".", "pad", "=", "pad", "\n", "self", ".", "eos", "=", "eos", "\n", "self", ".", "unk", "=", "unk", "\n", "self", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.bleu.Scorer.reset": [[69, 74], ["C.bleu_one_init", "C.bleu_zero_init", "ctypes.byref", "ctypes.byref"], "methods", ["None"], ["", "def", "reset", "(", "self", ",", "one_init", "=", "False", ")", ":", "\n", "        ", "if", "one_init", ":", "\n", "            ", "C", ".", "bleu_one_init", "(", "ctypes", ".", "byref", "(", "self", ".", "stat", ")", ")", "\n", "", "else", ":", "\n", "            ", "C", ".", "bleu_zero_init", "(", "ctypes", ".", "byref", "(", "self", ".", "stat", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.bleu.Scorer.add": [[75, 99], ["ref.clone", "rref.contiguous().view.contiguous().view.contiguous().view", "pred.contiguous().view.contiguous().view.contiguous().view", "C.bleu_add", "isinstance", "TypeError", "isinstance", "TypeError", "rref.contiguous().view.contiguous().view.lt().any", "ctypes.byref", "ctypes.c_size_t", "ctypes.c_void_p", "ctypes.c_size_t", "ctypes.c_void_p", "ctypes.c_int", "ctypes.c_int", "rref.contiguous().view.contiguous().view.eq", "rref.contiguous().view.contiguous().view.contiguous", "pred.contiguous().view.contiguous().view.contiguous", "rref.contiguous().view.contiguous().view.size", "rref.contiguous().view.contiguous().view.data_ptr", "pred.contiguous().view.contiguous().view.size", "pred.contiguous().view.contiguous().view.data_ptr", "type", "type", "rref.contiguous().view.contiguous().view.lt"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "", "def", "add", "(", "self", ",", "ref", ",", "pred", ")", ":", "\n", "        ", "if", "not", "isinstance", "(", "ref", ",", "torch", ".", "IntTensor", ")", ":", "\n", "            ", "raise", "TypeError", "(", "'ref must be a torch.IntTensor (got {})'", "\n", ".", "format", "(", "type", "(", "ref", ")", ")", ")", "\n", "", "if", "not", "isinstance", "(", "pred", ",", "torch", ".", "IntTensor", ")", ":", "\n", "            ", "raise", "TypeError", "(", "'pred must be a torch.IntTensor(got {})'", "\n", ".", "format", "(", "type", "(", "pred", ")", ")", ")", "\n", "\n", "# don't match unknown words", "\n", "", "rref", "=", "ref", ".", "clone", "(", ")", "\n", "assert", "not", "rref", ".", "lt", "(", "0", ")", ".", "any", "(", ")", "\n", "rref", "[", "rref", ".", "eq", "(", "self", ".", "unk", ")", "]", "=", "-", "999", "\n", "\n", "rref", "=", "rref", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", "\n", "pred", "=", "pred", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", "\n", "\n", "C", ".", "bleu_add", "(", "\n", "ctypes", ".", "byref", "(", "self", ".", "stat", ")", ",", "\n", "ctypes", ".", "c_size_t", "(", "rref", ".", "size", "(", "0", ")", ")", ",", "\n", "ctypes", ".", "c_void_p", "(", "rref", ".", "data_ptr", "(", ")", ")", ",", "\n", "ctypes", ".", "c_size_t", "(", "pred", ".", "size", "(", "0", ")", ")", ",", "\n", "ctypes", ".", "c_void_p", "(", "pred", ".", "data_ptr", "(", ")", ")", ",", "\n", "ctypes", ".", "c_int", "(", "self", ".", "pad", ")", ",", "\n", "ctypes", ".", "c_int", "(", "self", ".", "eos", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.bleu.Scorer.score": [[100, 104], ["sum", "bleu.Scorer.brevity", "math.exp", "math.log", "float", "bleu.Scorer.precision"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.bleu.Scorer.brevity", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.bleu.Scorer.precision"], ["", "def", "score", "(", "self", ",", "order", "=", "4", ")", ":", "\n", "        ", "psum", "=", "sum", "(", "math", ".", "log", "(", "p", ")", "if", "p", ">", "0", "else", "float", "(", "'-Inf'", ")", "\n", "for", "p", "in", "self", ".", "precision", "(", ")", "[", ":", "order", "]", ")", "\n", "return", "self", ".", "brevity", "(", ")", "*", "math", ".", "exp", "(", "psum", "/", "order", ")", "*", "100", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.bleu.Scorer.precision": [[105, 114], ["bleu.Scorer.precision.ratio"], "methods", ["None"], ["", "def", "precision", "(", "self", ")", ":", "\n", "        ", "def", "ratio", "(", "a", ",", "b", ")", ":", "\n", "            ", "return", "a", "/", "b", "if", "b", ">", "0", "else", "0", "\n", "\n", "", "return", "[", "\n", "ratio", "(", "self", ".", "stat", ".", "match1", ",", "self", ".", "stat", ".", "count1", ")", ",", "\n", "ratio", "(", "self", ".", "stat", ".", "match2", ",", "self", ".", "stat", ".", "count2", ")", ",", "\n", "ratio", "(", "self", ".", "stat", ".", "match3", ",", "self", ".", "stat", ".", "count3", ")", ",", "\n", "ratio", "(", "self", ".", "stat", ".", "match4", ",", "self", ".", "stat", ".", "count4", ")", ",", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.bleu.Scorer.brevity": [[116, 119], ["min", "math.exp"], "methods", ["None"], ["", "def", "brevity", "(", "self", ")", ":", "\n", "        ", "r", "=", "self", ".", "stat", ".", "reflen", "/", "self", ".", "stat", ".", "predlen", "\n", "return", "min", "(", "1", ",", "math", ".", "exp", "(", "1", "-", "r", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.bleu.Scorer.result_string": [[120, 130], ["range", "fmt.format", "bleu.Scorer.score", "bleu.Scorer.brevity", "bleu.Scorer.precision"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.bleu.Scorer.score", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.bleu.Scorer.brevity", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.bleu.Scorer.precision"], ["", "def", "result_string", "(", "self", ",", "order", "=", "4", ")", ":", "\n", "        ", "assert", "order", "<=", "4", ",", "\"BLEU scores for order > 4 aren't supported\"", "\n", "fmt", "=", "'BLEU{} = {:2.2f}, {:2.1f}'", "\n", "for", "_", "in", "range", "(", "1", ",", "order", ")", ":", "\n", "            ", "fmt", "+=", "'/{:2.1f}'", "\n", "", "fmt", "+=", "' (BP={:.3f}, ratio={:.3f}, syslen={}, reflen={})'", "\n", "bleup", "=", "[", "p", "*", "100", "for", "p", "in", "self", ".", "precision", "(", ")", "[", ":", "order", "]", "]", "\n", "return", "fmt", ".", "format", "(", "order", ",", "self", ".", "score", "(", "order", "=", "order", ")", ",", "*", "bleup", ",", "\n", "self", ".", "brevity", "(", ")", ",", "self", ".", "stat", ".", "predlen", "/", "self", ".", "stat", ".", "reflen", ",", "\n", "self", ".", "stat", ".", "predlen", ",", "self", ".", "stat", ".", "reflen", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.__init__": [[34, 59], ["trainer.Trainer.init_meters", "torch.cuda.is_available", "trainer.Trainer._model.half", "trainer.Trainer.criterion.cuda", "trainer.Trainer._model.cuda"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.init_meters"], ["def", "__init__", "(", "self", ",", "args", ",", "task", ",", "model", ",", "criterion", ",", "dummy_batch", "=", "None", ",", "oom_batch", "=", "None", ")", ":", "\n", "        ", "self", ".", "args", "=", "args", "\n", "self", ".", "task", "=", "task", "\n", "\n", "# copy model and criterion to current device", "\n", "self", ".", "criterion", "=", "criterion", "\n", "self", ".", "_model", "=", "model", "\n", "self", ".", "cuda", "=", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "cpu", "\n", "if", "args", ".", "fp16", ":", "\n", "            ", "self", ".", "_model", "=", "self", ".", "_model", ".", "half", "(", ")", "\n", "", "if", "self", ".", "cuda", ":", "\n", "            ", "self", ".", "criterion", "=", "self", ".", "criterion", ".", "cuda", "(", ")", "\n", "self", ".", "_model", "=", "self", ".", "_model", ".", "cuda", "(", ")", "\n", "\n", "", "self", ".", "_dummy_batch", "=", "dummy_batch", "\n", "self", ".", "_oom_batch", "=", "oom_batch", "or", "dummy_batch", "\n", "\n", "self", ".", "_lr_scheduler", "=", "None", "\n", "self", ".", "_num_updates", "=", "0", "\n", "self", ".", "_optim_history", "=", "None", "\n", "self", ".", "_optimizer", "=", "None", "\n", "self", ".", "_prev_grad_norm", "=", "None", "\n", "self", ".", "_wrapped_model", "=", "None", "\n", "\n", "self", ".", "init_meters", "(", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.init_meters": [[60, 77], ["collections.OrderedDict", "fairseq.meters.AverageMeter", "fairseq.meters.AverageMeter", "fairseq.meters.AverageMeter", "fairseq.meters.AverageMeter", "fairseq.meters.TimeMeter", "fairseq.meters.TimeMeter", "fairseq.meters.AverageMeter", "fairseq.meters.AverageMeter", "fairseq.meters.AverageMeter", "fairseq.meters.AverageMeter", "fairseq.meters.AverageMeter", "fairseq.meters.TimeMeter", "fairseq.meters.StopwatchMeter", "fairseq.meters.AverageMeter"], "methods", ["None"], ["", "def", "init_meters", "(", "self", ",", "args", ")", ":", "\n", "        ", "self", ".", "meters", "=", "OrderedDict", "(", ")", "\n", "self", ".", "meters", "[", "'train_loss'", "]", "=", "AverageMeter", "(", ")", "\n", "self", ".", "meters", "[", "'train_nll_loss'", "]", "=", "AverageMeter", "(", ")", "\n", "self", ".", "meters", "[", "'valid_loss'", "]", "=", "AverageMeter", "(", ")", "\n", "self", ".", "meters", "[", "'valid_nll_loss'", "]", "=", "AverageMeter", "(", ")", "\n", "self", ".", "meters", "[", "'wps'", "]", "=", "TimeMeter", "(", ")", "# words per second", "\n", "self", ".", "meters", "[", "'ups'", "]", "=", "TimeMeter", "(", ")", "# updates per second", "\n", "self", ".", "meters", "[", "'wpb'", "]", "=", "AverageMeter", "(", ")", "# words per batch", "\n", "self", ".", "meters", "[", "'bsz'", "]", "=", "AverageMeter", "(", ")", "# sentences per batch", "\n", "self", ".", "meters", "[", "'gnorm'", "]", "=", "AverageMeter", "(", ")", "# gradient norm", "\n", "self", ".", "meters", "[", "'clip'", "]", "=", "AverageMeter", "(", ")", "# % of updates clipped", "\n", "self", ".", "meters", "[", "'oom'", "]", "=", "AverageMeter", "(", ")", "# out of memory", "\n", "if", "args", ".", "fp16", ":", "\n", "            ", "self", ".", "meters", "[", "'loss_scale'", "]", "=", "AverageMeter", "(", ")", "# dynamic loss scale", "\n", "", "self", ".", "meters", "[", "'wall'", "]", "=", "TimeMeter", "(", ")", "# wall time in seconds", "\n", "self", ".", "meters", "[", "'train_wall'", "]", "=", "StopwatchMeter", "(", ")", "# train wall time in seconds", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.model": [[78, 88], ["fairseq.models.DistributedFairseqModel"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.distributed_fairseq_model.DistributedFairseqModel"], ["", "@", "property", "\n", "def", "model", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_wrapped_model", "is", "None", ":", "\n", "            ", "if", "self", ".", "args", ".", "distributed_world_size", ">", "1", "and", "not", "self", ".", "args", ".", "use_bmuf", ":", "\n", "                ", "self", ".", "_wrapped_model", "=", "models", ".", "DistributedFairseqModel", "(", "\n", "self", ".", "args", ",", "self", ".", "_model", ",", "\n", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "_wrapped_model", "=", "self", ".", "_model", "\n", "", "", "return", "self", ".", "_wrapped_model", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.optimizer": [[89, 94], ["trainer.Trainer._build_optimizer"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer._build_optimizer"], ["", "@", "property", "\n", "def", "optimizer", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_optimizer", "is", "None", ":", "\n", "            ", "self", ".", "_build_optimizer", "(", ")", "\n", "", "return", "self", ".", "_optimizer", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.lr_scheduler": [[95, 100], ["trainer.Trainer._build_optimizer"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer._build_optimizer"], ["", "@", "property", "\n", "def", "lr_scheduler", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_lr_scheduler", "is", "None", ":", "\n", "            ", "self", ".", "_build_optimizer", "(", ")", "# this will initialize self._lr_scheduler", "\n", "", "return", "self", ".", "_lr_scheduler", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer._build_optimizer": [[101, 123], ["list", "fairseq.optim.lr_scheduler.build_lr_scheduler", "trainer.Trainer._lr_scheduler.step_update", "filter", "fairseq.optim.build_optimizer", "fairseq.optim.FairseqBMUF", "trainer.Trainer.model.parameters", "print", "fairseq.optim.MemoryEfficientFP16Optimizer.build_optimizer", "fairseq.optim.FP16Optimizer.build_optimizer", "print", "torch.cuda.get_device_capability", "torch.cuda.get_device_capability"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.polynomial_decay_schedule.PolynomialDecaySchedule.step_update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.__init__.build_optimizer", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.__init__.build_optimizer", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.__init__.build_optimizer", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["", "def", "_build_optimizer", "(", "self", ")", ":", "\n", "        ", "params", "=", "list", "(", "filter", "(", "lambda", "p", ":", "p", ".", "requires_grad", ",", "self", ".", "model", ".", "parameters", "(", ")", ")", ")", "\n", "if", "self", ".", "args", ".", "fp16", ":", "\n", "            ", "if", "self", ".", "cuda", "and", "torch", ".", "cuda", ".", "get_device_capability", "(", "0", ")", "[", "0", "]", "<", "7", ":", "\n", "                ", "print", "(", "'| WARNING: your device does NOT support faster training with --fp16, '", "\n", "'please switch to FP32 which is likely to be faster'", ")", "\n", "", "if", "self", ".", "args", ".", "memory_efficient_fp16", ":", "\n", "                ", "self", ".", "_optimizer", "=", "optim", ".", "MemoryEfficientFP16Optimizer", ".", "build_optimizer", "(", "self", ".", "args", ",", "params", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "_optimizer", "=", "optim", ".", "FP16Optimizer", ".", "build_optimizer", "(", "self", ".", "args", ",", "params", ")", "\n", "", "", "else", ":", "\n", "            ", "if", "self", ".", "cuda", "and", "torch", ".", "cuda", ".", "get_device_capability", "(", "0", ")", "[", "0", "]", ">=", "7", ":", "\n", "                ", "print", "(", "'| NOTICE: your device may support faster training with --fp16'", ")", "\n", "", "self", ".", "_optimizer", "=", "optim", ".", "build_optimizer", "(", "self", ".", "args", ",", "params", ")", "\n", "\n", "", "if", "self", ".", "args", ".", "use_bmuf", ":", "\n", "            ", "self", ".", "_optimizer", "=", "optim", ".", "FairseqBMUF", "(", "self", ".", "args", ",", "params", ",", "self", ".", "_optimizer", ")", "\n", "\n", "# We should initialize the learning rate scheduler immediately after", "\n", "# building the optimizer, so that the initial learning rate is set.", "\n", "", "self", ".", "_lr_scheduler", "=", "lr_scheduler", ".", "build_lr_scheduler", "(", "self", ".", "args", ",", "self", ".", "optimizer", ")", "\n", "self", ".", "_lr_scheduler", ".", "step_update", "(", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.save_checkpoint": [[124, 132], ["fairseq.distributed_utils.is_master", "fairseq.checkpoint_utils.save_state", "trainer.Trainer.get_model().state_dict", "trainer.Trainer.get_num_updates", "trainer.Trainer.get_model"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.distributed_utils.is_master", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.checkpoint_utils.save_state", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.state_dict", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.bmuf.FairseqBMUF.get_num_updates", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.get_model"], ["", "def", "save_checkpoint", "(", "self", ",", "filename", ",", "extra_state", ")", ":", "\n", "        ", "\"\"\"Save all training state in a checkpoint file.\"\"\"", "\n", "if", "distributed_utils", ".", "is_master", "(", "self", ".", "args", ")", ":", "# only save one checkpoint", "\n", "            ", "extra_state", "[", "'train_meters'", "]", "=", "self", ".", "meters", "\n", "checkpoint_utils", ".", "save_state", "(", "\n", "filename", ",", "self", ".", "args", ",", "self", ".", "get_model", "(", ")", ".", "state_dict", "(", ")", ",", "self", ".", "criterion", ",", "\n", "self", ".", "optimizer", ",", "self", ".", "lr_scheduler", ",", "self", ".", "get_num_updates", "(", ")", ",", "\n", "self", ".", "_optim_history", ",", "extra_state", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.load_checkpoint": [[134, 197], ["os.path.exists", "fairseq.checkpoint_utils.load_checkpoint_to_cpu", "fairseq.checkpoint_utils.load_checkpoint_to_cpu.get", "trainer.Trainer._build_optimizer", "trainer.Trainer.optimizer.load_state_dict", "trainer.Trainer.set_num_updates", "print", "trainer.Trainer.lr_step", "print", "trainer.Trainer.get_model().load_state_dict", "trainer.Trainer.lr_scheduler.load_state_dict", "trainer.Trainer.meters.update", "trainer.Trainer.meters.values", "Exception", "trainer.Trainer.get_num_updates", "isinstance", "trainer.Trainer.get_model", "meter.reset"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.checkpoint_utils.load_checkpoint_to_cpu", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer._build_optimizer", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.load_state_dict", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.bmuf.FairseqBMUF.set_num_updates", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.lr_step", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.load_state_dict", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.load_state_dict", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.bmuf.FairseqBMUF.get_num_updates", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.get_model", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.reset"], ["", "", "def", "load_checkpoint", "(", "\n", "self", ",", "\n", "filename", ",", "\n", "reset_optimizer", "=", "False", ",", "\n", "reset_lr_scheduler", "=", "False", ",", "\n", "optimizer_overrides", "=", "None", ",", "\n", "reset_meters", "=", "False", ",", "\n", ")", ":", "\n", "        ", "\"\"\"Load all training state from a checkpoint file.\"\"\"", "\n", "extra_state", ",", "self", ".", "_optim_history", ",", "last_optim_state", "=", "None", ",", "[", "]", ",", "None", "\n", "\n", "if", "os", ".", "path", ".", "exists", "(", "filename", ")", ":", "\n", "            ", "state", "=", "checkpoint_utils", ".", "load_checkpoint_to_cpu", "(", "filename", ")", "\n", "\n", "# load model parameters", "\n", "try", ":", "\n", "                ", "self", ".", "get_model", "(", ")", ".", "load_state_dict", "(", "state", "[", "'model'", "]", ",", "strict", "=", "True", ")", "\n", "", "except", "Exception", ":", "\n", "                ", "raise", "Exception", "(", "\n", "'Cannot load model parameters from checkpoint, '", "\n", "'please ensure that the architectures match.'", "\n", ")", "\n", "\n", "", "extra_state", "=", "state", "[", "'extra_state'", "]", "\n", "self", ".", "_optim_history", "=", "state", "[", "'optimizer_history'", "]", "\n", "last_optim_state", "=", "state", ".", "get", "(", "'last_optimizer_state'", ",", "None", ")", "\n", "\n", "", "if", "last_optim_state", "is", "not", "None", "and", "not", "reset_optimizer", ":", "\n", "# rebuild optimizer after loading model, since params may have changed", "\n", "            ", "self", ".", "_build_optimizer", "(", ")", "\n", "\n", "# only reload optimizer and lr_scheduler if they match", "\n", "last_optim", "=", "self", ".", "_optim_history", "[", "-", "1", "]", "\n", "assert", "last_optim", "[", "'criterion_name'", "]", "==", "self", ".", "criterion", ".", "__class__", ".", "__name__", ",", "'Criterion does not match; please reset the optimizer (--reset-optimizer).'", "\n", "assert", "last_optim", "[", "'optimizer_name'", "]", "==", "self", ".", "optimizer", ".", "__class__", ".", "__name__", ",", "'Optimizer does not match; please reset the optimizer (--reset-optimizer).'", "\n", "\n", "if", "not", "reset_lr_scheduler", ":", "\n", "                ", "self", ".", "lr_scheduler", ".", "load_state_dict", "(", "last_optim", "[", "'lr_scheduler_state'", "]", ")", "\n", "", "self", ".", "optimizer", ".", "load_state_dict", "(", "last_optim_state", ",", "optimizer_overrides", ")", "\n", "\n", "self", ".", "set_num_updates", "(", "last_optim", "[", "'num_updates'", "]", ")", "\n", "\n", "", "if", "extra_state", "is", "not", "None", ":", "\n", "            ", "epoch", "=", "extra_state", "[", "'train_iterator'", "]", "[", "'epoch'", "]", "\n", "print", "(", "'| loaded checkpoint {} (epoch {} @ {} updates)'", ".", "format", "(", "\n", "filename", ",", "epoch", ",", "self", ".", "get_num_updates", "(", ")", ")", ")", "\n", "\n", "self", ".", "lr_step", "(", "epoch", ")", "\n", "\n", "if", "'train_meters'", "in", "extra_state", "and", "not", "reset_meters", ":", "\n", "                ", "self", ".", "meters", ".", "update", "(", "extra_state", "[", "'train_meters'", "]", ")", "\n", "del", "extra_state", "[", "'train_meters'", "]", "\n", "\n", "# reset TimeMeters, since their start times don't make sense anymore", "\n", "for", "meter", "in", "self", ".", "meters", ".", "values", "(", ")", ":", "\n", "                    ", "if", "isinstance", "(", "meter", ",", "TimeMeter", ")", ":", "\n", "                        ", "meter", ".", "reset", "(", ")", "\n", "", "", "", "", "else", ":", "\n", "            ", "print", "(", "'| no existing checkpoint found {}'", ".", "format", "(", "filename", ")", ")", "\n", "\n", "", "return", "extra_state", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.get_train_iterator": [[198, 217], ["print", "trainer.Trainer.task.load_dataset", "trainer.Trainer.task.get_batch_iterator", "trainer.Trainer.task.dataset", "fairseq.utils.resolve_max_positions", "trainer.Trainer.task.max_positions", "trainer.Trainer.model.max_positions"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.sentence_prediction.SentencePredictionTask.load_dataset", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.fairseq_task.FairseqTask.get_batch_iterator", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.fairseq_task.FairseqTask.dataset", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.resolve_max_positions", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.max_positions", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.max_positions"], ["", "def", "get_train_iterator", "(", "self", ",", "epoch", ",", "combine", "=", "True", ")", ":", "\n", "        ", "\"\"\"Return an EpochBatchIterator over the training set for a given epoch.\"\"\"", "\n", "print", "(", "'| loading train data for epoch {}'", ".", "format", "(", "epoch", ")", ")", "\n", "self", ".", "task", ".", "load_dataset", "(", "self", ".", "args", ".", "train_subset", ",", "epoch", "=", "epoch", ",", "combine", "=", "combine", ")", "\n", "return", "self", ".", "task", ".", "get_batch_iterator", "(", "\n", "dataset", "=", "self", ".", "task", ".", "dataset", "(", "self", ".", "args", ".", "train_subset", ")", ",", "\n", "max_tokens", "=", "self", ".", "args", ".", "max_tokens", ",", "\n", "max_sentences", "=", "self", ".", "args", ".", "max_sentences", ",", "\n", "max_positions", "=", "utils", ".", "resolve_max_positions", "(", "\n", "self", ".", "task", ".", "max_positions", "(", ")", ",", "\n", "self", ".", "model", ".", "max_positions", "(", ")", ",", "\n", ")", ",", "\n", "ignore_invalid_inputs", "=", "True", ",", "\n", "required_batch_size_multiple", "=", "self", ".", "args", ".", "required_batch_size_multiple", ",", "\n", "seed", "=", "self", ".", "args", ".", "seed", ",", "\n", "num_shards", "=", "self", ".", "args", ".", "distributed_world_size", ",", "\n", "shard_id", "=", "self", ".", "args", ".", "distributed_rank", ",", "\n", "num_workers", "=", "self", ".", "args", ".", "num_workers", ",", "\n", "epoch", "=", "epoch", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.train_step": [[219, 381], ["trainer.Trainer._set_seed", "trainer.Trainer.model.train", "trainer.Trainer.criterion.train", "trainer.Trainer.zero_grad", "enumerate", "trainer.Trainer.meters[].update", "trainer.Trainer.task.aggregate_logging_outputs", "trainer.Trainer.task.grad_denom", "trainer.Trainer.meters[].stop", "trainer.Trainer.meters[].start", "trainer.Trainer._prepare_sample", "trainer.Trainer.handle_ooms", "zip", "list", "list", "sum", "len", "print", "trainer.Trainer.zero_grad", "all", "Exception", "trainer.Trainer.optimizer.clip_grad_norm", "trainer.Trainer.optimizer.step", "trainer.Trainer.set_num_updates", "trainer.Trainer.task.update_step", "trainer.Trainer.get", "trainer.Trainer.get", "trainer.Trainer.meters[].update", "trainer.Trainer.meters[].update", "trainer.Trainer.meters[].update", "trainer.Trainer.meters[].update", "trainer.Trainer.meters[].update", "trainer.Trainer.meters[].update", "trainer.Trainer.meters[].update", "trainer.Trainer.meters[].reset", "trainer.Trainer.meters[].update", "trainer.Trainer._prepare_sample", "itertools.chain.from_iterable", "itertools.chain.from_iterable", "len", "trainer.Trainer.optimizer.multiply_grads", "trainer.Trainer.get", "trainer.Trainer.meters[].update", "trainer.Trainer.meters[].update", "print", "trainer.Trainer.zero_grad", "hasattr", "trainer.Trainer.model.no_sync", "contextlib.ExitStack", "trainer.Trainer.train_step.maybe_no_sync"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer._set_seed", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq-stack-transformer.train.train", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq-stack-transformer.train.train", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.zero_grad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.binary_cross_entropy.BinaryCrossEntropyCriterion.aggregate_logging_outputs", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.fairseq_criterion.FairseqCriterion.grad_denom", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.meters.StopwatchMeter.stop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.meters.StopwatchMeter.start", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer._prepare_sample", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.handle_ooms", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.zero_grad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.clip_grad_norm", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.polynomial_decay_schedule.PolynomialDecaySchedule.step", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.bmuf.FairseqBMUF.set_num_updates", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.fairseq_task.FairseqTask.update_step", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.reset", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer._prepare_sample", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.multiply_grads", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.zero_grad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.legacy_distributed_data_parallel.LegacyDistributedDataParallel.no_sync"], ["", "def", "train_step", "(", "self", ",", "samples", ",", "dummy_batch", "=", "False", ",", "raise_oom", "=", "False", ")", ":", "\n", "        ", "\"\"\"Do forward, backward and parameter update.\"\"\"", "\n", "if", "self", ".", "_dummy_batch", "is", "None", ":", "\n", "            ", "self", ".", "_dummy_batch", "=", "samples", "[", "0", "]", "\n", "\n", "", "self", ".", "_set_seed", "(", ")", "\n", "self", ".", "model", ".", "train", "(", ")", "\n", "self", ".", "criterion", ".", "train", "(", ")", "\n", "self", ".", "zero_grad", "(", ")", "\n", "\n", "if", "not", "dummy_batch", ":", "\n", "            ", "self", ".", "meters", "[", "'train_wall'", "]", ".", "start", "(", ")", "\n", "\n", "# forward and backward pass", "\n", "", "logging_outputs", ",", "sample_sizes", ",", "ooms", "=", "[", "]", ",", "[", "]", ",", "0", "\n", "for", "i", ",", "sample", "in", "enumerate", "(", "samples", ")", ":", "\n", "            ", "sample", "=", "self", ".", "_prepare_sample", "(", "sample", ")", "\n", "if", "sample", "is", "None", ":", "\n", "# when sample is None, run forward/backward on a dummy batch", "\n", "# and ignore the resulting gradients", "\n", "                ", "sample", "=", "self", ".", "_prepare_sample", "(", "self", ".", "_dummy_batch", ")", "\n", "ignore_grad", "=", "True", "\n", "", "else", ":", "\n", "                ", "ignore_grad", "=", "False", "\n", "\n", "", "def", "maybe_no_sync", "(", ")", ":", "\n", "                ", "\"\"\"\n                Whenever *samples* contains more than one mini-batch, we\n                want to accumulate gradients locally and only call\n                all-reduce in the last backwards pass.\n                \"\"\"", "\n", "if", "(", "\n", "self", ".", "args", ".", "distributed_world_size", ">", "1", "\n", "and", "hasattr", "(", "self", ".", "model", ",", "'no_sync'", ")", "\n", "and", "i", "<", "len", "(", "samples", ")", "-", "1", "\n", ")", ":", "\n", "                    ", "return", "self", ".", "model", ".", "no_sync", "(", ")", "\n", "", "else", ":", "\n", "                    ", "return", "contextlib", ".", "ExitStack", "(", ")", "# dummy contextmanager", "\n", "\n", "", "", "try", ":", "\n", "                ", "with", "maybe_no_sync", "(", ")", ":", "\n", "# forward and backward", "\n", "                    ", "loss", ",", "sample_size", ",", "logging_output", "=", "self", ".", "task", ".", "train_step", "(", "\n", "sample", ",", "self", ".", "model", ",", "self", ".", "criterion", ",", "self", ".", "optimizer", ",", "\n", "ignore_grad", "\n", ")", "\n", "\n", "", "if", "not", "ignore_grad", ":", "\n", "                    ", "logging_outputs", ".", "append", "(", "logging_output", ")", "\n", "sample_sizes", ".", "append", "(", "sample_size", ")", "\n", "", "", "except", "RuntimeError", "as", "e", ":", "\n", "                ", "if", "'out of memory'", "in", "str", "(", "e", ")", ":", "\n", "                    ", "msg", "=", "(", "\n", "'| WARNING: ran out of memory with exception: '", "\n", "+", "'{};'", ".", "format", "(", "e", ")", "\n", "+", "'\\n Skipping batch'", "\n", ")", "\n", "# TODO: print should really go to logger, this print goes", "\n", "# to stdout, which is buffered, which in many case is not", "\n", "# printed out if another exception happens", "\n", "# print(msg)", "\n", "print", "(", "msg", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "if", "raise_oom", ":", "\n", "                        ", "raise", "ValueError", "(", "msg", ")", "\n", "", "ooms", "+=", "1", "\n", "self", ".", "zero_grad", "(", ")", "\n", "", "else", ":", "\n", "                    ", "raise", "e", "\n", "\n", "", "", "", "if", "ooms", ">", "0", "and", "self", ".", "_oom_batch", "is", "not", "None", ":", "\n", "            ", "self", ".", "handle_ooms", "(", "ooms", ")", "\n", "\n", "", "if", "dummy_batch", ":", "\n", "            ", "return", "None", "\n", "\n", "# gather logging outputs from all replicas", "\n", "", "if", "self", ".", "args", ".", "distributed_world_size", ">", "1", "and", "(", "\n", "(", "not", "self", ".", "args", ".", "use_bmuf", ")", "\n", "or", "(", "\n", "self", ".", "args", ".", "use_bmuf", "\n", "and", "(", "self", ".", "get_num_updates", "(", ")", "+", "1", ")", "%", "self", ".", "args", ".", "global_sync_iter", "==", "0", "\n", ")", "\n", ")", ":", "\n", "            ", "logging_outputs", ",", "sample_sizes", ",", "ooms", ",", "prev_norms", "=", "zip", "(", "*", "distributed_utils", ".", "all_gather_list", "(", "\n", "[", "logging_outputs", ",", "sample_sizes", ",", "ooms", ",", "self", ".", "_prev_grad_norm", "]", ",", "\n", ")", ")", "\n", "logging_outputs", "=", "list", "(", "chain", ".", "from_iterable", "(", "logging_outputs", ")", ")", "\n", "sample_sizes", "=", "list", "(", "chain", ".", "from_iterable", "(", "sample_sizes", ")", ")", "\n", "ooms", "=", "sum", "(", "ooms", ")", "\n", "\n", "if", "not", "self", ".", "args", ".", "use_bmuf", ":", "\n", "                ", "assert", "(", "\n", "all", "(", "norm", "==", "prev_norms", "[", "0", "]", "for", "norm", "in", "prev_norms", ")", "\n", "or", "all", "(", "math", ".", "isnan", "(", "norm", ")", "or", "math", ".", "isinf", "(", "norm", ")", "for", "norm", "in", "prev_norms", ")", "\n", ")", ",", "'Fatal error: gradients are inconsistent between workers'", "\n", "\n", "", "", "self", ".", "meters", "[", "'oom'", "]", ".", "update", "(", "ooms", ",", "len", "(", "samples", ")", ")", "\n", "if", "ooms", "==", "self", ".", "args", ".", "distributed_world_size", "*", "len", "(", "samples", ")", ":", "\n", "            ", "print", "(", "'| WARNING: OOM in all workers, skipping update'", ")", "\n", "self", ".", "zero_grad", "(", ")", "\n", "return", "None", "\n", "\n", "# aggregate logging outputs and sample sizes", "\n", "", "logging_output", "=", "self", ".", "task", ".", "aggregate_logging_outputs", "(", "\n", "logging_outputs", ",", "self", ".", "criterion", "\n", ")", "\n", "sample_size", "=", "self", ".", "task", ".", "grad_denom", "(", "sample_sizes", ",", "self", ".", "criterion", ")", "\n", "\n", "if", "not", "all", "(", "k", "in", "logging_output", "for", "k", "in", "[", "'ntokens'", ",", "'nsentences'", "]", ")", ":", "\n", "            ", "raise", "Exception", "(", "(", "\n", "'Please update the {}.aggregate_logging_outputs() method to '", "\n", "'return ntokens and nsentences'", "\n", ")", ".", "format", "(", "self", ".", "task", ".", "__class__", ".", "__name__", ")", ")", "\n", "\n", "", "try", ":", "\n", "# normalize grads by sample size", "\n", "            ", "if", "sample_size", ">", "0", ":", "\n", "                ", "self", ".", "optimizer", ".", "multiply_grads", "(", "self", ".", "args", ".", "distributed_world_size", "/", "float", "(", "sample_size", ")", ")", "\n", "\n", "# clip grads", "\n", "", "grad_norm", "=", "self", ".", "optimizer", ".", "clip_grad_norm", "(", "self", ".", "args", ".", "clip_norm", ")", "\n", "self", ".", "_prev_grad_norm", "=", "grad_norm", "\n", "\n", "# take an optimization step", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "self", ".", "set_num_updates", "(", "self", ".", "get_num_updates", "(", ")", "+", "1", ")", "\n", "\n", "# task specific update per step", "\n", "self", ".", "task", ".", "update_step", "(", "self", ".", "_num_updates", ")", "\n", "\n", "# update meters", "\n", "ntokens", "=", "logging_output", ".", "get", "(", "'ntokens'", ",", "0", ")", "\n", "nsentences", "=", "logging_output", ".", "get", "(", "'nsentences'", ",", "0", ")", "\n", "self", ".", "meters", "[", "'wps'", "]", ".", "update", "(", "ntokens", ")", "\n", "self", ".", "meters", "[", "'ups'", "]", ".", "update", "(", "1.", ")", "\n", "self", ".", "meters", "[", "'wpb'", "]", ".", "update", "(", "ntokens", ")", "\n", "self", ".", "meters", "[", "'bsz'", "]", ".", "update", "(", "nsentences", ")", "\n", "self", ".", "meters", "[", "'gnorm'", "]", ".", "update", "(", "grad_norm", ")", "\n", "self", ".", "meters", "[", "'clip'", "]", ".", "update", "(", "\n", "1.", "if", "grad_norm", ">", "self", ".", "args", ".", "clip_norm", "and", "self", ".", "args", ".", "clip_norm", ">", "0", "else", "0.", "\n", ")", "\n", "self", ".", "meters", "[", "'train_loss'", "]", ".", "update", "(", "logging_output", ".", "get", "(", "'loss'", ",", "0", ")", ",", "sample_size", ")", "\n", "if", "'train_acc'", "in", "self", ".", "meters", ":", "\n", "                ", "self", ".", "meters", "[", "'train_acc'", "]", ".", "update", "(", "\n", "logging_output", ".", "get", "(", "'acc'", ",", "0", ")", ",", "sample_size", ")", "\n", "\n", "", "if", "'nll_loss'", "in", "logging_output", ":", "\n", "                ", "self", ".", "meters", "[", "'train_nll_loss'", "]", ".", "update", "(", "logging_output", ".", "get", "(", "'nll_loss'", ",", "0", ")", ",", "ntokens", ")", "\n", "", "", "except", "OverflowError", "as", "e", ":", "\n", "            ", "print", "(", "'| WARNING: overflow detected, '", "+", "str", "(", "e", ")", ")", "\n", "self", ".", "zero_grad", "(", ")", "\n", "logging_output", "=", "None", "\n", "\n", "", "if", "self", ".", "args", ".", "fp16", ":", "\n", "            ", "self", ".", "meters", "[", "'loss_scale'", "]", ".", "reset", "(", ")", "\n", "self", ".", "meters", "[", "'loss_scale'", "]", ".", "update", "(", "self", ".", "optimizer", ".", "scaler", ".", "loss_scale", ")", "\n", "\n", "", "self", ".", "meters", "[", "'train_wall'", "]", ".", "stop", "(", ")", "\n", "\n", "return", "logging_output", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.valid_step": [[382, 444], ["trainer.Trainer.task.aggregate_logging_outputs", "trainer.Trainer.task.grad_denom", "list.get", "trainer.Trainer.meters[].update", "torch.no_grad", "trainer.Trainer.model.eval", "trainer.Trainer.criterion.eval", "trainer.Trainer._prepare_sample", "zip", "list", "list", "list.get", "trainer.Trainer.meters[].update", "trainer.Trainer.meters[].update", "trainer.Trainer._prepare_sample", "trainer.Trainer.task.valid_step", "list.get", "list.get", "fairseq.distributed_utils.all_gather_list", "print", "trainer.Trainer.model.parameters", "trainer.Trainer.valid_step", "str", "torch.cuda.empty_cache"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.binary_cross_entropy.BinaryCrossEntropyCriterion.aggregate_logging_outputs", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.criterions.fairseq_criterion.FairseqCriterion.grad_denom", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer._prepare_sample", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer._prepare_sample", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.valid_step", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.distributed_utils.all_gather_list", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.valid_step"], ["", "def", "valid_step", "(", "self", ",", "sample", ",", "raise_oom", "=", "False", ")", ":", "\n", "        ", "\"\"\"Do forward pass in evaluation mode.\"\"\"", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "self", ".", "model", ".", "eval", "(", ")", "\n", "self", ".", "criterion", ".", "eval", "(", ")", "\n", "\n", "sample", "=", "self", ".", "_prepare_sample", "(", "sample", ")", "\n", "if", "sample", "is", "None", ":", "\n", "                ", "sample", "=", "self", ".", "_prepare_sample", "(", "self", ".", "_dummy_batch", ")", "\n", "ignore_results", "=", "True", "\n", "", "else", ":", "\n", "                ", "ignore_results", "=", "False", "\n", "\n", "", "try", ":", "\n", "                ", "_loss", ",", "sample_size", ",", "logging_output", "=", "self", ".", "task", ".", "valid_step", "(", "\n", "sample", ",", "self", ".", "model", ",", "self", ".", "criterion", "\n", ")", "\n", "", "except", "RuntimeError", "as", "e", ":", "\n", "                ", "if", "'out of memory'", "in", "str", "(", "e", ")", "and", "not", "raise_oom", ":", "\n", "                    ", "print", "(", "'| WARNING: ran out of memory, retrying batch'", ")", "\n", "for", "p", "in", "self", ".", "model", ".", "parameters", "(", ")", ":", "\n", "                        ", "if", "p", ".", "grad", "is", "not", "None", ":", "\n", "                            ", "p", ".", "grad", "=", "None", "# free some memory", "\n", "", "", "if", "self", ".", "cuda", ":", "\n", "                        ", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "", "return", "self", ".", "valid_step", "(", "sample", ",", "raise_oom", "=", "True", ")", "\n", "", "else", ":", "\n", "                    ", "raise", "e", "\n", "\n", "", "", "if", "ignore_results", ":", "\n", "                ", "logging_output", ",", "sample_size", "=", "{", "}", ",", "0", "\n", "\n", "# gather logging outputs from all replicas", "\n", "", "", "if", "self", ".", "args", ".", "distributed_world_size", ">", "1", ":", "\n", "            ", "logging_output", ",", "sample_size", "=", "zip", "(", "*", "distributed_utils", ".", "all_gather_list", "(", "\n", "[", "logging_output", ",", "sample_size", "]", ",", "\n", ")", ")", "\n", "logging_output", "=", "list", "(", "logging_output", ")", "\n", "sample_size", "=", "list", "(", "sample_size", ")", "\n", "", "else", ":", "\n", "            ", "logging_output", "=", "[", "logging_output", "]", "\n", "sample_size", "=", "[", "sample_size", "]", "\n", "\n", "# aggregate logging outputs and sample sizes", "\n", "", "logging_output", "=", "self", ".", "task", ".", "aggregate_logging_outputs", "(", "\n", "logging_output", ",", "self", ".", "criterion", "\n", ")", "\n", "sample_size", "=", "self", ".", "task", ".", "grad_denom", "(", "\n", "sample_size", ",", "self", ".", "criterion", "\n", ")", "\n", "\n", "# update meters for validation", "\n", "ntokens", "=", "logging_output", ".", "get", "(", "'ntokens'", ",", "0", ")", "\n", "self", ".", "meters", "[", "'valid_loss'", "]", ".", "update", "(", "logging_output", ".", "get", "(", "'loss'", ",", "0", ")", ",", "sample_size", ")", "\n", "if", "'valid_acc'", "in", "self", ".", "meters", ":", "\n", "            ", "self", ".", "meters", "[", "'valid_acc'", "]", ".", "update", "(", "\n", "logging_output", ".", "get", "(", "'acc'", ",", "0", ")", ",", "sample_size", ")", "\n", "\n", "", "if", "'nll_loss'", "in", "logging_output", ":", "\n", "            ", "self", ".", "meters", "[", "'valid_nll_loss'", "]", ".", "update", "(", "logging_output", ".", "get", "(", "'nll_loss'", ",", "0", ")", ",", "ntokens", ")", "\n", "\n", "", "return", "logging_output", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.dummy_train_step": [[445, 449], ["trainer.Trainer.train_step", "trainer.Trainer.zero_grad"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.train_step", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.zero_grad"], ["", "def", "dummy_train_step", "(", "self", ",", "dummy_batch", ")", ":", "\n", "        ", "\"\"\"Dummy training step for warming caching allocator.\"\"\"", "\n", "self", ".", "train_step", "(", "dummy_batch", ",", "dummy_batch", "=", "True", ")", "\n", "self", ".", "zero_grad", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.handle_ooms": [[450, 458], ["range", "trainer.Trainer.train_step"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.train_step"], ["", "def", "handle_ooms", "(", "self", ",", "number_of_ooms", ")", ":", "\n", "        ", "\"\"\"\n        c10d accumulates/syncs gradients between gpus during backward pass.\n        In case of OOMs, gpus may fail to sync, so we manually iterate\n        extra to make sure each gpu makes same number of iterations.\n        \"\"\"", "\n", "for", "_", "in", "range", "(", "number_of_ooms", ")", ":", "\n", "            ", "self", ".", "train_step", "(", "[", "self", ".", "_oom_batch", "]", ",", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.zero_grad": [[459, 461], ["trainer.Trainer.optimizer.zero_grad"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.zero_grad"], ["", "", "def", "zero_grad", "(", "self", ")", ":", "\n", "        ", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.lr_step": [[462, 467], ["trainer.Trainer.lr_scheduler.step", "trainer.Trainer.lr_step_update"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.polynomial_decay_schedule.PolynomialDecaySchedule.step", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.lr_step_update"], ["", "def", "lr_step", "(", "self", ",", "epoch", ",", "val_loss", "=", "None", ")", ":", "\n", "        ", "\"\"\"Adjust the learning rate based on the validation loss.\"\"\"", "\n", "self", ".", "lr_scheduler", ".", "step", "(", "epoch", ",", "val_loss", ")", "\n", "# prefer updating the LR based on the number of steps", "\n", "return", "self", ".", "lr_step_update", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.lr_step_update": [[468, 471], ["trainer.Trainer.lr_scheduler.step_update", "trainer.Trainer.get_num_updates"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.polynomial_decay_schedule.PolynomialDecaySchedule.step_update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.bmuf.FairseqBMUF.get_num_updates"], ["", "def", "lr_step_update", "(", "self", ")", ":", "\n", "        ", "\"\"\"Update the learning rate after each update.\"\"\"", "\n", "return", "self", ".", "lr_scheduler", ".", "step_update", "(", "self", ".", "get_num_updates", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.get_lr": [[472, 475], ["trainer.Trainer.optimizer.get_lr"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.get_lr"], ["", "def", "get_lr", "(", "self", ")", ":", "\n", "        ", "\"\"\"Get the current learning rate.\"\"\"", "\n", "return", "self", ".", "optimizer", ".", "get_lr", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.get_model": [[476, 479], ["None"], "methods", ["None"], ["", "def", "get_model", "(", "self", ")", ":", "\n", "        ", "\"\"\"Get the (non-wrapped) model instance.\"\"\"", "\n", "return", "self", ".", "_model", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.get_meter": [[480, 485], ["None"], "methods", ["None"], ["", "def", "get_meter", "(", "self", ",", "name", ")", ":", "\n", "        ", "\"\"\"Get a specific meter by name.\"\"\"", "\n", "if", "name", "not", "in", "self", ".", "meters", ":", "\n", "            ", "return", "None", "\n", "", "return", "self", ".", "meters", "[", "name", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.get_num_updates": [[486, 489], ["None"], "methods", ["None"], ["", "def", "get_num_updates", "(", "self", ")", ":", "\n", "        ", "\"\"\"Get the number of parameters updates.\"\"\"", "\n", "return", "self", ".", "_num_updates", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.set_num_updates": [[490, 494], ["trainer.Trainer.lr_step_update"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.lr_step_update"], ["", "def", "set_num_updates", "(", "self", ",", "num_updates", ")", ":", "\n", "        ", "\"\"\"Set the number of parameters updates.\"\"\"", "\n", "self", ".", "_num_updates", "=", "num_updates", "\n", "self", ".", "lr_step_update", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer._prepare_sample": [[495, 511], ["fairseq.utils.move_to_cuda", "fairseq.utils.apply_to_sample", "len", "t.half"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.move_to_cuda", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.apply_to_sample"], ["", "def", "_prepare_sample", "(", "self", ",", "sample", ")", ":", "\n", "        ", "if", "sample", "is", "None", "or", "len", "(", "sample", ")", "==", "0", ":", "\n", "            ", "return", "None", "\n", "\n", "", "if", "self", ".", "cuda", ":", "\n", "            ", "sample", "=", "utils", ".", "move_to_cuda", "(", "sample", ")", "\n", "\n", "", "def", "apply_half", "(", "t", ")", ":", "\n", "            ", "if", "t", ".", "dtype", "is", "torch", ".", "float32", ":", "\n", "                ", "return", "t", ".", "half", "(", ")", "\n", "", "return", "t", "\n", "\n", "", "if", "self", ".", "args", ".", "fp16", ":", "\n", "            ", "sample", "=", "utils", ".", "apply_to_sample", "(", "apply_half", ",", "sample", ")", "\n", "\n", "", "return", "sample", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer._set_seed": [[512, 519], ["torch.manual_seed", "trainer.Trainer.get_num_updates", "torch.cuda.manual_seed"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.bmuf.FairseqBMUF.get_num_updates"], ["", "def", "_set_seed", "(", "self", ")", ":", "\n", "# Set seed based on args.seed and the update number so that we get", "\n", "# reproducible results when resuming from checkpoints", "\n", "        ", "seed", "=", "self", ".", "args", ".", "seed", "+", "self", ".", "get_num_updates", "(", ")", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "if", "self", ".", "cuda", ":", "\n", "            ", "torch", ".", "cuda", ".", "manual_seed", "(", "seed", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.pdb.MultiprocessingPdb.__init__": [[29, 31], ["pdb.Pdb.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "pdb", ".", "Pdb", ".", "__init__", "(", "self", ",", "nosigint", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.pdb.MultiprocessingPdb._cmdloop": [[32, 43], ["pdb.MultiprocessingPdb.cmdloop", "os.fdopen"], "methods", ["None"], ["", "def", "_cmdloop", "(", "self", ")", ":", "\n", "        ", "stdin_bak", "=", "sys", ".", "stdin", "\n", "with", "_stdin_lock", ":", "\n", "            ", "try", ":", "\n", "                ", "if", "_stdin_fd", "is", "not", "None", ":", "\n", "                    ", "if", "not", "_stdin", "[", "0", "]", ":", "\n", "                        ", "_stdin", "[", "0", "]", "=", "os", ".", "fdopen", "(", "_stdin_fd", ")", "\n", "", "sys", ".", "stdin", "=", "_stdin", "[", "0", "]", "\n", "", "self", ".", "cmdloop", "(", ")", "\n", "", "finally", ":", "\n", "                ", "sys", ".", "stdin", "=", "stdin_bak", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.pdb.set_trace": [[45, 48], ["pdb.MultiprocessingPdb", "MultiprocessingPdb.set_trace", "sys._getframe"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.pdb.set_trace"], ["", "", "", "", "def", "set_trace", "(", ")", ":", "\n", "    ", "pdb", "=", "MultiprocessingPdb", "(", ")", "\n", "pdb", ".", "set_trace", "(", "sys", ".", "_getframe", "(", ")", ".", "f_back", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.registry.setup_registry": [[12, 63], ["registry_name[].replace.startswith", "registry_name[].replace", "set", "getattr", "hasattr", "registry.set_defaults", "getattr.", "getattr", "set.add", "ValueError", "ValueError", "ValueError", "issubclass"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.registry.set_defaults", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.add"], ["def", "setup_registry", "(", "\n", "registry_name", ":", "str", ",", "\n", "base_class", "=", "None", ",", "\n", "default", "=", "None", ",", "\n", ")", ":", "\n", "    ", "assert", "registry_name", ".", "startswith", "(", "'--'", ")", "\n", "registry_name", "=", "registry_name", "[", "2", ":", "]", ".", "replace", "(", "'-'", ",", "'_'", ")", "\n", "\n", "REGISTRY", "=", "{", "}", "\n", "REGISTRY_CLASS_NAMES", "=", "set", "(", ")", "\n", "\n", "# maintain a registry of all registries", "\n", "if", "registry_name", "in", "REGISTRIES", ":", "\n", "        ", "return", "# registry already exists", "\n", "", "REGISTRIES", "[", "registry_name", "]", "=", "{", "\n", "'registry'", ":", "REGISTRY", ",", "\n", "'default'", ":", "default", ",", "\n", "}", "\n", "\n", "def", "build_x", "(", "args", ",", "*", "extra_args", ",", "**", "extra_kwargs", ")", ":", "\n", "        ", "choice", "=", "getattr", "(", "args", ",", "registry_name", ",", "None", ")", "\n", "if", "choice", "is", "None", ":", "\n", "            ", "return", "None", "\n", "", "cls", "=", "REGISTRY", "[", "choice", "]", "\n", "if", "hasattr", "(", "cls", ",", "'build_'", "+", "registry_name", ")", ":", "\n", "            ", "builder", "=", "getattr", "(", "cls", ",", "'build_'", "+", "registry_name", ")", "\n", "", "else", ":", "\n", "            ", "builder", "=", "cls", "\n", "", "set_defaults", "(", "args", ",", "cls", ")", "\n", "return", "builder", "(", "args", ",", "*", "extra_args", ",", "**", "extra_kwargs", ")", "\n", "\n", "", "def", "register_x", "(", "name", ")", ":", "\n", "\n", "        ", "def", "register_x_cls", "(", "cls", ")", ":", "\n", "            ", "if", "name", "in", "REGISTRY", ":", "\n", "                ", "raise", "ValueError", "(", "'Cannot register duplicate {} ({})'", ".", "format", "(", "registry_name", ",", "name", ")", ")", "\n", "", "if", "cls", ".", "__name__", "in", "REGISTRY_CLASS_NAMES", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "'Cannot register {} with duplicate class name ({})'", ".", "format", "(", "\n", "registry_name", ",", "cls", ".", "__name__", ",", "\n", ")", "\n", ")", "\n", "", "if", "base_class", "is", "not", "None", "and", "not", "issubclass", "(", "cls", ",", "base_class", ")", ":", "\n", "                ", "raise", "ValueError", "(", "'{} must extend {}'", ".", "format", "(", "cls", ".", "__name__", ",", "base_class", ".", "__name__", ")", ")", "\n", "", "REGISTRY", "[", "name", "]", "=", "cls", "\n", "REGISTRY_CLASS_NAMES", ".", "add", "(", "cls", ".", "__name__", ")", "\n", "return", "cls", "\n", "\n", "", "return", "register_x_cls", "\n", "\n", "", "return", "build_x", ",", "register_x", ",", "REGISTRY", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.registry.set_defaults": [[65, 81], ["argparse.ArgumentParser", "cls.add_args", "argparse.Namespace", "vars().items", "hasattr", "vars", "hasattr", "setattr", "hasattr", "setattr"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.add_args"], ["", "def", "set_defaults", "(", "args", ",", "cls", ")", ":", "\n", "    ", "\"\"\"Helper to set default arguments based on *add_args*.\"\"\"", "\n", "if", "not", "hasattr", "(", "cls", ",", "'add_args'", ")", ":", "\n", "        ", "return", "\n", "", "parser", "=", "argparse", ".", "ArgumentParser", "(", "argument_default", "=", "argparse", ".", "SUPPRESS", ",", "allow_abbrev", "=", "False", ")", "\n", "cls", ".", "add_args", "(", "parser", ")", "\n", "# copied from argparse.py:", "\n", "defaults", "=", "argparse", ".", "Namespace", "(", ")", "\n", "for", "action", "in", "parser", ".", "_actions", ":", "\n", "        ", "if", "action", ".", "dest", "is", "not", "argparse", ".", "SUPPRESS", ":", "\n", "            ", "if", "not", "hasattr", "(", "defaults", ",", "action", ".", "dest", ")", ":", "\n", "                ", "if", "action", ".", "default", "is", "not", "argparse", ".", "SUPPRESS", ":", "\n", "                    ", "setattr", "(", "defaults", ",", "action", ".", "dest", ",", "action", ".", "default", ")", "\n", "", "", "", "", "for", "key", ",", "default_value", "in", "vars", "(", "defaults", ")", ".", "items", "(", ")", ":", "\n", "        ", "if", "not", "hasattr", "(", "args", ",", "key", ")", ":", "\n", "            ", "setattr", "(", "args", ",", "key", ",", "default_value", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.binarizer.Binarizer.binarize": [[24, 54], ["collections.Counter", "open", "f.seek", "binarizer.safe_readline", "sum", "collections.Counter.update", "dict.encode_line", "len", "consumer", "f.readline", "collections.Counter.values", "f.tell"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.token_block_dataset.DatasetSearcher.seek", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.binarizer.safe_readline", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.encode_line"], ["    ", "@", "staticmethod", "\n", "def", "binarize", "(", "filename", ",", "dict", ",", "consumer", ",", "tokenize", "=", "tokenize_line", ",", "append_eos", "=", "True", ",", "reverse_order", "=", "False", ",", "\n", "offset", "=", "0", ",", "end", "=", "-", "1", ")", ":", "\n", "        ", "nseq", ",", "ntok", "=", "0", ",", "0", "\n", "replaced", "=", "Counter", "(", ")", "\n", "\n", "def", "replaced_consumer", "(", "word", ",", "idx", ")", ":", "\n", "            ", "if", "idx", "==", "dict", ".", "unk_index", "and", "word", "!=", "dict", ".", "unk_word", ":", "\n", "                ", "replaced", ".", "update", "(", "[", "word", "]", ")", "\n", "\n", "", "", "with", "open", "(", "filename", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "            ", "f", ".", "seek", "(", "offset", ")", "\n", "# next(f) breaks f.tell(), hence readline() must be used", "\n", "line", "=", "safe_readline", "(", "f", ")", "\n", "while", "line", ":", "\n", "                ", "if", "end", ">", "0", "and", "f", ".", "tell", "(", ")", ">", "end", ":", "\n", "                    ", "break", "\n", "", "ids", "=", "dict", ".", "encode_line", "(", "\n", "line", "=", "line", ",", "\n", "line_tokenizer", "=", "tokenize", ",", "\n", "add_if_not_exist", "=", "False", ",", "\n", "consumer", "=", "replaced_consumer", ",", "\n", "append_eos", "=", "append_eos", ",", "\n", "reverse_order", "=", "reverse_order", ",", "\n", ")", "\n", "nseq", "+=", "1", "\n", "ntok", "+=", "len", "(", "ids", ")", "\n", "consumer", "(", "ids", ")", "\n", "line", "=", "f", ".", "readline", "(", ")", "\n", "", "", "return", "{", "'nseq'", ":", "nseq", ",", "'nunk'", ":", "sum", "(", "replaced", ".", "values", "(", ")", ")", ",", "'ntok'", ":", "ntok", ",", "'replaced'", ":", "replaced", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.binarizer.Binarizer.find_offsets": [[55, 66], ["open", "range", "os.fstat", "f.seek", "binarizer.safe_readline", "f.tell", "f.fileno", "range"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.token_block_dataset.DatasetSearcher.seek", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.binarizer.safe_readline"], ["", "@", "staticmethod", "\n", "def", "find_offsets", "(", "filename", ",", "num_chunks", ")", ":", "\n", "        ", "with", "open", "(", "filename", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "            ", "size", "=", "os", ".", "fstat", "(", "f", ".", "fileno", "(", ")", ")", ".", "st_size", "\n", "chunk_size", "=", "size", "//", "num_chunks", "\n", "offsets", "=", "[", "0", "for", "_", "in", "range", "(", "num_chunks", "+", "1", ")", "]", "\n", "for", "i", "in", "range", "(", "1", ",", "num_chunks", ")", ":", "\n", "                ", "f", ".", "seek", "(", "chunk_size", "*", "i", ")", "\n", "safe_readline", "(", "f", ")", "\n", "offsets", "[", "i", "]", "=", "f", ".", "tell", "(", ")", "\n", "", "return", "offsets", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.binarizer.safe_readline": [[12, 20], ["f.tell", "f.readline", "f.seek"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.token_block_dataset.DatasetSearcher.seek"], ["def", "safe_readline", "(", "f", ")", ":", "\n", "    ", "pos", "=", "f", ".", "tell", "(", ")", "\n", "while", "True", ":", "\n", "        ", "try", ":", "\n", "            ", "return", "f", ".", "readline", "(", ")", "\n", "", "except", "UnicodeDecodeError", ":", "\n", "            ", "pos", "-=", "1", "\n", "f", ".", "seek", "(", "pos", ")", "# search where this character begins", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.meters.AverageMeter.__init__": [[11, 13], ["meters.AverageMeter.reset"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.reset"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.meters.AverageMeter.reset": [[14, 19], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "val", "=", "0", "\n", "self", ".", "avg", "=", "0", "\n", "self", ".", "sum", "=", "0", "\n", "self", ".", "count", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.meters.AverageMeter.update": [[20, 25], ["None"], "methods", ["None"], ["", "def", "update", "(", "self", ",", "val", ",", "n", "=", "1", ")", ":", "\n", "        ", "self", ".", "val", "=", "val", "\n", "self", ".", "sum", "+=", "val", "*", "n", "\n", "self", ".", "count", "+=", "n", "\n", "self", ".", "avg", "=", "self", ".", "sum", "/", "self", ".", "count", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.meters.TimeMeter.__init__": [[29, 31], ["meters.TimeMeter.reset"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.reset"], ["def", "__init__", "(", "self", ",", "init", "=", "0", ")", ":", "\n", "        ", "self", ".", "reset", "(", "init", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.meters.TimeMeter.reset": [[32, 36], ["time.time"], "methods", ["None"], ["", "def", "reset", "(", "self", ",", "init", "=", "0", ")", ":", "\n", "        ", "self", ".", "init", "=", "init", "\n", "self", ".", "start", "=", "time", ".", "time", "(", ")", "\n", "self", ".", "n", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.meters.TimeMeter.update": [[37, 39], ["None"], "methods", ["None"], ["", "def", "update", "(", "self", ",", "val", "=", "1", ")", ":", "\n", "        ", "self", ".", "n", "+=", "val", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.meters.TimeMeter.avg": [[40, 43], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "avg", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "n", "/", "self", ".", "elapsed_time", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.meters.TimeMeter.elapsed_time": [[44, 47], ["time.time"], "methods", ["None"], ["", "@", "property", "\n", "def", "elapsed_time", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "init", "+", "(", "time", ".", "time", "(", ")", "-", "self", ".", "start", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.meters.StopwatchMeter.__init__": [[51, 53], ["meters.StopwatchMeter.reset"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.reset"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.meters.StopwatchMeter.start": [[54, 56], ["time.time"], "methods", ["None"], ["", "def", "start", "(", "self", ")", ":", "\n", "        ", "self", ".", "start_time", "=", "time", ".", "time", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.meters.StopwatchMeter.stop": [[57, 63], ["time.time"], "methods", ["None"], ["", "def", "stop", "(", "self", ",", "n", "=", "1", ")", ":", "\n", "        ", "if", "self", ".", "start_time", "is", "not", "None", ":", "\n", "            ", "delta", "=", "time", ".", "time", "(", ")", "-", "self", ".", "start_time", "\n", "self", ".", "sum", "+=", "delta", "\n", "self", ".", "n", "+=", "n", "\n", "self", ".", "start_time", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.meters.StopwatchMeter.reset": [[64, 68], ["None"], "methods", ["None"], ["", "", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "sum", "=", "0", "\n", "self", ".", "n", "=", "0", "\n", "self", ".", "start_time", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.meters.StopwatchMeter.avg": [[69, 72], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "avg", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "sum", "/", "self", ".", "n", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.search.Search.__init__": [[13, 21], ["tgt_dict.pad", "tgt_dict.unk", "tgt_dict.eos", "len"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.unk", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos"], ["    ", "def", "__init__", "(", "self", ",", "tgt_dict", ")", ":", "\n", "        ", "self", ".", "pad", "=", "tgt_dict", ".", "pad", "(", ")", "\n", "self", ".", "unk", "=", "tgt_dict", ".", "unk", "(", ")", "\n", "self", ".", "eos", "=", "tgt_dict", ".", "eos", "(", ")", "\n", "self", ".", "vocab_size", "=", "len", "(", "tgt_dict", ")", "\n", "self", ".", "scores_buf", "=", "None", "\n", "self", ".", "indices_buf", "=", "None", "\n", "self", ".", "beams_buf", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.search.Search._init_buffers": [[22, 27], ["t.new", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor", "torch.LongTensor"], "methods", ["None"], ["", "def", "_init_buffers", "(", "self", ",", "t", ")", ":", "\n", "        ", "if", "self", ".", "scores_buf", "is", "None", ":", "\n", "            ", "self", ".", "scores_buf", "=", "t", ".", "new", "(", ")", "\n", "self", ".", "indices_buf", "=", "torch", ".", "LongTensor", "(", ")", ".", "to", "(", "device", "=", "t", ".", "device", ")", "\n", "self", ".", "beams_buf", "=", "torch", ".", "LongTensor", "(", ")", ".", "to", "(", "device", "=", "t", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.search.Search.step": [[28, 49], ["None"], "methods", ["None"], ["", "", "def", "step", "(", "self", ",", "step", ",", "lprobs", ",", "scores", ")", ":", "\n", "        ", "\"\"\"Take a single search step.\n\n        Args:\n            step: the current search step, starting at 0\n            lprobs: (bsz x input_beam_size x vocab_size)\n                the model's log-probabilities over the vocabulary at the current step\n            scores: (bsz x input_beam_size x step)\n                the historical model scores of each hypothesis up to this point\n\n        Return: A tuple of (scores, indices, beams) where:\n            scores: (bsz x output_beam_size)\n                the scores of the chosen elements; output_beam_size can be\n                larger than input_beam_size, e.g., we may return\n                2*input_beam_size to account for EOS\n            indices: (bsz x output_beam_size)\n                the indices of the chosen elements\n            beams: (bsz x output_beam_size)\n                the hypothesis ids of the chosen elements, in the range [0, input_beam_size)\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.search.Search.set_src_lengths": [[50, 52], ["None"], "methods", ["None"], ["", "def", "set_src_lengths", "(", "self", ",", "src_lengths", ")", ":", "\n", "        ", "self", ".", "src_lengths", "=", "src_lengths", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.search.BeamSearch.__init__": [[56, 58], ["search.Search.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["    ", "def", "__init__", "(", "self", ",", "tgt_dict", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "tgt_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.search.BeamSearch.step": [[59, 84], ["search.Search._init_buffers", "lprobs[].contiguous.size", "torch.topk", "torch.div", "search.BeamSearch.indices_buf.fmod_", "lprobs[].contiguous", "lprobs[].contiguous.add_", "lprobs[].contiguous.view", "scores[].unsqueeze", "min", "lprobs[].contiguous.view().size", "lprobs[].contiguous.view"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.search.Search._init_buffers", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "step", "(", "self", ",", "step", ",", "lprobs", ",", "scores", ")", ":", "\n", "        ", "super", "(", ")", ".", "_init_buffers", "(", "lprobs", ")", "\n", "bsz", ",", "beam_size", ",", "vocab_size", "=", "lprobs", ".", "size", "(", ")", "\n", "\n", "if", "step", "==", "0", ":", "\n", "# at the first step all hypotheses are equally likely, so use", "\n", "# only the first beam", "\n", "            ", "lprobs", "=", "lprobs", "[", ":", ",", ":", ":", "beam_size", ",", ":", "]", ".", "contiguous", "(", ")", "\n", "", "else", ":", "\n", "# make probs contain cumulative scores for each hypothesis", "\n", "            ", "lprobs", ".", "add_", "(", "scores", "[", ":", ",", ":", ",", "step", "-", "1", "]", ".", "unsqueeze", "(", "-", "1", ")", ")", "\n", "\n", "", "torch", ".", "topk", "(", "\n", "lprobs", ".", "view", "(", "bsz", ",", "-", "1", ")", ",", "\n", "k", "=", "min", "(", "\n", "# Take the best 2 x beam_size predictions. We'll choose the first", "\n", "# beam_size of these which don't predict eos to continue with.", "\n", "beam_size", "*", "2", ",", "\n", "lprobs", ".", "view", "(", "bsz", ",", "-", "1", ")", ".", "size", "(", "1", ")", "-", "1", ",", "# -1 so we never select pad", "\n", ")", ",", "\n", "out", "=", "(", "self", ".", "scores_buf", ",", "self", ".", "indices_buf", ")", ",", "\n", ")", "\n", "torch", ".", "div", "(", "self", ".", "indices_buf", ",", "vocab_size", ",", "out", "=", "self", ".", "beams_buf", ")", "\n", "self", ".", "indices_buf", ".", "fmod_", "(", "vocab_size", ")", "\n", "return", "self", ".", "scores_buf", ",", "self", ".", "indices_buf", ",", "self", ".", "beams_buf", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.search.LengthConstrainedBeamSearch.__init__": [[88, 95], ["search.Search.__init__", "search.BeamSearch"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["    ", "def", "__init__", "(", "self", ",", "tgt_dict", ",", "min_len_a", ",", "min_len_b", ",", "max_len_a", ",", "max_len_b", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "tgt_dict", ")", "\n", "self", ".", "min_len_a", "=", "min_len_a", "\n", "self", ".", "min_len_b", "=", "min_len_b", "\n", "self", ".", "max_len_a", "=", "max_len_a", "\n", "self", ".", "max_len_b", "=", "max_len_b", "\n", "self", ".", "beam", "=", "BeamSearch", "(", "tgt_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.search.LengthConstrainedBeamSearch.step": [[96, 103], ["search.LengthConstrainedBeamSearch.beam.step"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.polynomial_decay_schedule.PolynomialDecaySchedule.step"], ["", "def", "step", "(", "self", ",", "step", ",", "lprobs", ",", "scores", ")", ":", "\n", "        ", "min_lens", "=", "self", ".", "min_len_a", "*", "self", ".", "src_lengths", "+", "self", ".", "min_len_b", "\n", "max_lens", "=", "self", ".", "max_len_a", "*", "self", ".", "src_lengths", "+", "self", ".", "max_len_b", "\n", "lprobs", "[", "step", "<", "min_lens", ",", ":", ",", "self", ".", "eos", "]", "=", "-", "math", ".", "inf", "\n", "lprobs", "[", "step", "==", "max_lens", ",", ":", ",", "self", ".", "eos", "]", "=", "0", "\n", "lprobs", "[", "step", ">", "max_lens", ",", ":", ",", "self", ".", "eos", "]", "=", "-", "math", ".", "inf", "\n", "return", "self", ".", "beam", ".", "step", "(", "step", ",", "lprobs", ",", "scores", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.search.DiverseBeamSearch.__init__": [[115, 121], ["search.Search.__init__", "search.BeamSearch"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["def", "__init__", "(", "self", ",", "tgt_dict", ",", "num_groups", ",", "diversity_strength", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "tgt_dict", ")", "\n", "self", ".", "num_groups", "=", "num_groups", "\n", "self", ".", "diversity_strength", "=", "-", "diversity_strength", "\n", "self", ".", "diversity_buf", "=", "None", "\n", "self", ".", "beam", "=", "BeamSearch", "(", "tgt_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.search.DiverseBeamSearch.step": [[122, 165], ["search.Search._init_buffers", "lprobs.size", "torch.zeros", "range", "torch.stack().view", "torch.stack().view", "torch.stack().view", "ValueError", "lprobs.new", "lprobs[].size", "search.DiverseBeamSearch.beam.step", "beams_buf.mul_().add_", "scores_G.append", "indices_G.append", "beams_G.append", "search.DiverseBeamSearch.diversity_buf.scatter_add_", "torch.add", "lprobs_g.contiguous.contiguous.contiguous", "scores_buf.clone", "indices_buf.clone", "beams_buf.clone", "search.DiverseBeamSearch.diversity_buf.new_ones", "torch.stack", "torch.stack", "torch.stack", "search.DiverseBeamSearch.diversity_buf.unsqueeze", "beams_buf.mul_", "indices_buf.size"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.search.Search._init_buffers", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.polynomial_decay_schedule.PolynomialDecaySchedule.step", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.add", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "step", "(", "self", ",", "step", ",", "lprobs", ",", "scores", ")", ":", "\n", "        ", "super", "(", ")", ".", "_init_buffers", "(", "lprobs", ")", "\n", "bsz", ",", "beam_size", ",", "vocab_size", "=", "lprobs", ".", "size", "(", ")", "\n", "if", "beam_size", "%", "self", ".", "num_groups", "!=", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "'DiverseBeamSearch requires --beam to be divisible by the number of groups'", "\n", ")", "\n", "\n", "# initialize diversity penalty", "\n", "", "if", "self", ".", "diversity_buf", "is", "None", ":", "\n", "            ", "self", ".", "diversity_buf", "=", "lprobs", ".", "new", "(", ")", "\n", "", "torch", ".", "zeros", "(", "lprobs", "[", ":", ",", "0", ",", ":", "]", ".", "size", "(", ")", ",", "out", "=", "self", ".", "diversity_buf", ")", "\n", "\n", "scores_G", ",", "indices_G", ",", "beams_G", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "for", "g", "in", "range", "(", "self", ".", "num_groups", ")", ":", "\n", "            ", "lprobs_g", "=", "lprobs", "[", ":", ",", "g", ":", ":", "self", ".", "num_groups", ",", ":", "]", "\n", "scores_g", "=", "scores", "[", ":", ",", "g", ":", ":", "self", ".", "num_groups", ",", ":", "]", "if", "step", ">", "0", "else", "None", "\n", "\n", "# apply diversity penalty", "\n", "if", "g", ">", "0", ":", "\n", "                ", "lprobs_g", "=", "torch", ".", "add", "(", "lprobs_g", ",", "self", ".", "diversity_strength", ",", "self", ".", "diversity_buf", ".", "unsqueeze", "(", "1", ")", ")", "\n", "", "else", ":", "\n", "                ", "lprobs_g", "=", "lprobs_g", ".", "contiguous", "(", ")", "\n", "\n", "", "scores_buf", ",", "indices_buf", ",", "beams_buf", "=", "self", ".", "beam", ".", "step", "(", "step", ",", "lprobs_g", ",", "scores_g", ")", "\n", "beams_buf", ".", "mul_", "(", "self", ".", "num_groups", ")", ".", "add_", "(", "g", ")", "\n", "\n", "scores_G", ".", "append", "(", "scores_buf", ".", "clone", "(", ")", ")", "\n", "indices_G", ".", "append", "(", "indices_buf", ".", "clone", "(", ")", ")", "\n", "beams_G", ".", "append", "(", "beams_buf", ".", "clone", "(", ")", ")", "\n", "\n", "# update diversity penalty", "\n", "self", ".", "diversity_buf", ".", "scatter_add_", "(", "\n", "1", ",", "\n", "indices_buf", ",", "\n", "self", ".", "diversity_buf", ".", "new_ones", "(", "indices_buf", ".", "size", "(", ")", ")", "\n", ")", "\n", "\n", "# interleave results from different groups", "\n", "", "self", ".", "scores_buf", "=", "torch", ".", "stack", "(", "scores_G", ",", "dim", "=", "2", ",", "out", "=", "self", ".", "scores_buf", ")", ".", "view", "(", "bsz", ",", "-", "1", ")", "\n", "self", ".", "indices_buf", "=", "torch", ".", "stack", "(", "indices_G", ",", "dim", "=", "2", ",", "out", "=", "self", ".", "indices_buf", ")", ".", "view", "(", "bsz", ",", "-", "1", ")", "\n", "self", ".", "beams_buf", "=", "torch", ".", "stack", "(", "beams_G", ",", "dim", "=", "2", ",", "out", "=", "self", ".", "beams_buf", ")", ".", "view", "(", "bsz", ",", "-", "1", ")", "\n", "return", "self", ".", "scores_buf", ",", "self", ".", "indices_buf", ",", "self", ".", "beams_buf", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.search.Sampling.__init__": [[169, 173], ["search.Search.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["    ", "def", "__init__", "(", "self", ",", "tgt_dict", ",", "sampling_topk", "=", "-", "1", ",", "sampling_topp", "=", "-", "1.0", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "tgt_dict", ")", "\n", "self", ".", "sampling_topk", "=", "sampling_topk", "\n", "self", ".", "sampling_topp", "=", "sampling_topp", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.search.Sampling._sample_topp": [[174, 218], ["lprobs.exp_", "lprobs.exp_.sort", "sorted_probs.cumsum", "sorted_probs.cumsum.lt", "mask.scatter_.scatter_.cumsum", "last_included.clamp_", "mask.scatter_.scatter_.scatter_", "last_included.max", "truncated_probs.masked_fill_", "mask.scatter_.scatter_.size"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.concat_dataset.ConcatDataset.cumsum", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.concat_dataset.ConcatDataset.cumsum", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "_sample_topp", "(", "self", ",", "lprobs", ")", ":", "\n", "        ", "\"\"\"Sample among the smallest set of elements whose cumulative probability mass exceeds p.\n\n        See `\"The Curious Case of Neural Text Degeneration\"\n        (Holtzman et al., 2019) <https://arxiv.org/abs/1904.09751>`_.\n\n        Args:\n            lprobs: (bsz x input_beam_size x vocab_size)\n                the model's log-probabilities over the vocabulary at the current step\n\n        Return: A tuple of (trimed_probs, truncated_indices) where:\n            trimed_probs: (bsz x input_beam_size x ?)\n                the model's probabilities over the elements selected to sample from. The\n                width of the third dimension is determined by top-P.\n            truncated_indices: (bsz x input_beam_size x ?)\n                the indices of the chosen elements.\n        \"\"\"", "\n", "probs", "=", "lprobs", ".", "exp_", "(", ")", "\n", "\n", "# sort the last dimension (vocab dimension) in descending order", "\n", "sorted_probs", ",", "sorted_indices", "=", "probs", ".", "sort", "(", "descending", "=", "True", ")", "\n", "\n", "# compute a mask to indicate the words to be included in the top-P set.", "\n", "cumsum_probs", "=", "sorted_probs", ".", "cumsum", "(", "dim", "=", "2", ")", "\n", "mask", "=", "cumsum_probs", ".", "lt", "(", "self", ".", "sampling_topp", ")", "\n", "\n", "# note that mask was computed by 'lt'. One more word needs to be included", "\n", "# so that the cumulative probability mass can exceed p.", "\n", "cumsum_mask", "=", "mask", ".", "cumsum", "(", "dim", "=", "2", ")", "\n", "last_included", "=", "cumsum_mask", "[", ":", ",", ":", ",", "-", "1", ":", "]", "\n", "last_included", ".", "clamp_", "(", "0", ",", "mask", ".", "size", "(", ")", "[", "2", "]", "-", "1", ")", "\n", "mask", "=", "mask", ".", "scatter_", "(", "2", ",", "last_included", ",", "1", ")", "\n", "\n", "# truncate unnecessary dims.", "\n", "max_dim", "=", "last_included", ".", "max", "(", ")", "\n", "truncated_mask", "=", "mask", "[", ":", ",", ":", ",", ":", "max_dim", "+", "1", "]", "\n", "truncated_probs", "=", "sorted_probs", "[", ":", ",", ":", ",", ":", "max_dim", "+", "1", "]", "\n", "truncated_indices", "=", "sorted_indices", "[", ":", ",", ":", ",", ":", "max_dim", "+", "1", "]", "\n", "\n", "# trim the words that are not in top-P by setting their probabilities", "\n", "# to 0, so that they would not be sampled later.", "\n", "trim_mask", "=", "(", "~", "truncated_mask", ")", "\n", "trimed_probs", "=", "truncated_probs", ".", "masked_fill_", "(", "trim_mask", ",", "0", ")", "\n", "return", "trimed_probs", ",", "truncated_indices", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.search.Sampling.step": [[219, 296], ["search.Search._init_buffers", "lprobs[].contiguous.size", "torch.gather", "search.Sampling.scores_buf.log_().view", "search.Sampling.indices_buf.add_", "lprobs[].contiguous", "search.Sampling._sample_topp", "torch.multinomial().view", "torch.multinomial().view", "lprobs_nopad.exp_.expand", "torch.gather().squeeze", "search.Sampling.indices_buf.new_zeros", "torch.arange().repeat", "search.Sampling.scores_buf.add_", "lprobs_nopad.topk", "lprobs_nopad.exp_", "lprobs_nopad.exp_", "search.Sampling.indices_buf.unsqueeze", "search.Sampling.scores_buf.log_", "torch.gather", "torch.multinomial", "torch.multinomial", "torch.gather", "torch.arange", "lprobs_nopad.exp_.view", "lprobs_nopad.exp_.view", "top_indices.expand", "search.Sampling.indices_buf.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.search.Search._init_buffers", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.search.Sampling._sample_topp"], ["", "def", "step", "(", "self", ",", "step", ",", "lprobs", ",", "scores", ")", ":", "\n", "        ", "super", "(", ")", ".", "_init_buffers", "(", "lprobs", ")", "\n", "bsz", ",", "beam_size", ",", "vocab_size", "=", "lprobs", ".", "size", "(", ")", "\n", "\n", "if", "step", "==", "0", ":", "\n", "# at the first step all hypotheses are equally likely, so use", "\n", "# only the first beam", "\n", "            ", "lprobs", "=", "lprobs", "[", ":", ",", ":", ":", "beam_size", ",", ":", "]", ".", "contiguous", "(", ")", "\n", "\n", "# we exclude the first two vocab items, one of which is pad", "\n", "", "assert", "self", ".", "pad", "<=", "1", ",", "'sampling assumes the first two symbols can be ignored'", "\n", "lprobs_nopad", "=", "lprobs", "[", ":", ",", ":", ",", "2", ":", "]", "\n", "\n", "if", "self", ".", "sampling_topp", ">", "0", ":", "\n", "# only sample from the smallest set of words whose cumulative probability mass exceeds p", "\n", "            ", "probs_nopad", ",", "top_indices", "=", "self", ".", "_sample_topp", "(", "lprobs_nopad", ")", "\n", "", "elif", "self", ".", "sampling_topk", ">", "0", ":", "\n", "# only sample from top-k candidates", "\n", "            ", "lprobs_nopad", ",", "top_indices", "=", "lprobs_nopad", ".", "topk", "(", "self", ".", "sampling_topk", ")", "\n", "probs_nopad", "=", "lprobs_nopad", ".", "exp_", "(", ")", "\n", "", "else", ":", "\n", "            ", "probs_nopad", "=", "lprobs_nopad", ".", "exp_", "(", ")", "\n", "\n", "# sample", "\n", "", "if", "step", "==", "0", ":", "\n", "            ", "self", ".", "indices_buf", "=", "torch", ".", "multinomial", "(", "\n", "probs_nopad", ".", "view", "(", "bsz", ",", "-", "1", ")", ",", "\n", "beam_size", ",", "\n", "replacement", "=", "True", ",", "\n", "out", "=", "self", ".", "indices_buf", ",", "\n", ")", ".", "view", "(", "bsz", ",", "beam_size", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "indices_buf", "=", "torch", ".", "multinomial", "(", "\n", "probs_nopad", ".", "view", "(", "bsz", "*", "beam_size", ",", "-", "1", ")", ",", "\n", "1", ",", "\n", "replacement", "=", "True", ",", "\n", "out", "=", "self", ".", "indices_buf", ",", "\n", ")", ".", "view", "(", "bsz", ",", "beam_size", ")", "\n", "\n", "", "if", "step", "==", "0", ":", "\n", "# expand to beam size", "\n", "            ", "probs_nopad", "=", "probs_nopad", ".", "expand", "(", "bsz", ",", "beam_size", ",", "-", "1", ")", "\n", "\n", "# gather scores", "\n", "", "torch", ".", "gather", "(", "\n", "probs_nopad", ",", "\n", "dim", "=", "2", ",", "\n", "index", "=", "self", ".", "indices_buf", ".", "unsqueeze", "(", "-", "1", ")", ",", "\n", "out", "=", "self", ".", "scores_buf", ",", "\n", ")", "\n", "self", ".", "scores_buf", "=", "self", ".", "scores_buf", ".", "log_", "(", ")", ".", "view", "(", "bsz", ",", "-", "1", ")", "\n", "\n", "# remap indices if using top-k or top-P sampling", "\n", "if", "self", ".", "sampling_topk", ">", "0", "or", "self", ".", "sampling_topp", ">", "0", ":", "\n", "            ", "self", ".", "indices_buf", "=", "torch", ".", "gather", "(", "\n", "top_indices", ".", "expand", "(", "bsz", ",", "beam_size", ",", "-", "1", ")", ",", "\n", "dim", "=", "2", ",", "\n", "index", "=", "self", ".", "indices_buf", ".", "unsqueeze", "(", "-", "1", ")", ",", "\n", ")", ".", "squeeze", "(", "2", ")", "\n", "\n", "# remap indices since we excluded the first two vocab items", "\n", "", "self", ".", "indices_buf", ".", "add_", "(", "2", ")", "\n", "\n", "if", "step", "==", "0", ":", "\n", "            ", "self", ".", "beams_buf", "=", "self", ".", "indices_buf", ".", "new_zeros", "(", "bsz", ",", "beam_size", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "beams_buf", "=", "torch", ".", "arange", "(", "0", ",", "beam_size", ",", "out", "=", "self", ".", "beams_buf", ")", ".", "repeat", "(", "bsz", ",", "1", ")", "\n", "# make scores cumulative", "\n", "self", ".", "scores_buf", ".", "add_", "(", "\n", "torch", ".", "gather", "(", "\n", "scores", "[", ":", ",", ":", ",", "step", "-", "1", "]", ",", "\n", "dim", "=", "1", ",", "\n", "index", "=", "self", ".", "beams_buf", ",", "\n", ")", "\n", ")", "\n", "\n", "", "return", "self", ".", "scores_buf", ",", "self", ".", "indices_buf", ",", "self", ".", "beams_buf", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.checkpoint_utils.save_checkpoint": [[21, 91], ["getattr", "meters.StopwatchMeter", "meters.StopwatchMeter.start", "epoch_itr.end_of_epoch", "trainer.get_num_updates", "collections.OrderedDict", "hasattr", "best_function", "epoch_itr.state_dict", "extra_state.update", "os.path.join", "len", "trainer.save_checkpoint", "meters.StopwatchMeter.stop", "print", "checkpoint_utils.checkpoint_paths", "checkpoint_utils.checkpoint_paths", "distributed_utils.is_master", "checkpoint_utils.save_checkpoint.is_better"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.meters.StopwatchMeter.start", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.iterators.EpochBatchIterator.end_of_epoch", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.bmuf.FairseqBMUF.get_num_updates", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.state_dict", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.checkpoint_utils.save_checkpoint", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.meters.StopwatchMeter.stop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.checkpoint_utils.checkpoint_paths", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.checkpoint_utils.checkpoint_paths", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.distributed_utils.is_master"], ["def", "save_checkpoint", "(", "args", ",", "trainer", ",", "epoch_itr", ",", "val_loss", ")", ":", "\n", "    ", "from", "fairseq", "import", "distributed_utils", ",", "meters", "\n", "\n", "prev_best", "=", "getattr", "(", "save_checkpoint", ",", "'best'", ",", "val_loss", ")", "\n", "if", "val_loss", "is", "not", "None", ":", "\n", "        ", "best_function", "=", "max", "if", "args", ".", "maximize_best_checkpoint_metric", "else", "min", "\n", "save_checkpoint", ".", "best", "=", "best_function", "(", "val_loss", ",", "prev_best", ")", "\n", "\n", "", "if", "args", ".", "no_save", "or", "not", "distributed_utils", ".", "is_master", "(", "args", ")", ":", "\n", "        ", "return", "\n", "\n", "", "def", "is_better", "(", "a", ",", "b", ")", ":", "\n", "        ", "return", "a", ">=", "b", "if", "args", ".", "maximize_best_checkpoint_metric", "else", "a", "<=", "b", "\n", "\n", "", "write_timer", "=", "meters", ".", "StopwatchMeter", "(", ")", "\n", "write_timer", ".", "start", "(", ")", "\n", "\n", "epoch", "=", "epoch_itr", ".", "epoch", "\n", "end_of_epoch", "=", "epoch_itr", ".", "end_of_epoch", "(", ")", "\n", "updates", "=", "trainer", ".", "get_num_updates", "(", ")", "\n", "\n", "checkpoint_conds", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "checkpoint_conds", "[", "'checkpoint{}.pt'", ".", "format", "(", "epoch", ")", "]", "=", "(", "\n", "end_of_epoch", "and", "not", "args", ".", "no_epoch_checkpoints", "and", "\n", "epoch", "%", "args", ".", "save_interval", "==", "0", "\n", ")", "\n", "checkpoint_conds", "[", "'checkpoint_{}_{}.pt'", ".", "format", "(", "epoch", ",", "updates", ")", "]", "=", "(", "\n", "not", "end_of_epoch", "and", "args", ".", "save_interval_updates", ">", "0", "and", "\n", "updates", "%", "args", ".", "save_interval_updates", "==", "0", "\n", ")", "\n", "checkpoint_conds", "[", "'checkpoint_best.pt'", "]", "=", "(", "\n", "val_loss", "is", "not", "None", "and", "\n", "(", "not", "hasattr", "(", "save_checkpoint", ",", "'best'", ")", "or", "is_better", "(", "val_loss", ",", "save_checkpoint", ".", "best", ")", ")", "\n", ")", "\n", "checkpoint_conds", "[", "'checkpoint_last.pt'", "]", "=", "not", "args", ".", "no_last_checkpoints", "\n", "\n", "extra_state", "=", "{", "\n", "'train_iterator'", ":", "epoch_itr", ".", "state_dict", "(", ")", ",", "\n", "'val_loss'", ":", "val_loss", ",", "\n", "}", "\n", "if", "hasattr", "(", "save_checkpoint", ",", "'best'", ")", ":", "\n", "        ", "extra_state", ".", "update", "(", "{", "'best'", ":", "save_checkpoint", ".", "best", "}", ")", "\n", "\n", "", "checkpoints", "=", "[", "os", ".", "path", ".", "join", "(", "args", ".", "save_dir", ",", "fn", ")", "for", "fn", ",", "cond", "in", "checkpoint_conds", ".", "items", "(", ")", "if", "cond", "]", "\n", "if", "len", "(", "checkpoints", ")", ">", "0", ":", "\n", "        ", "trainer", ".", "save_checkpoint", "(", "checkpoints", "[", "0", "]", ",", "extra_state", ")", "\n", "for", "cp", "in", "checkpoints", "[", "1", ":", "]", ":", "\n", "            ", "shutil", ".", "copyfile", "(", "checkpoints", "[", "0", "]", ",", "cp", ")", "\n", "\n", "", "write_timer", ".", "stop", "(", ")", "\n", "print", "(", "'| saved checkpoint {} (epoch {} @ {} updates) (writing took {} seconds)'", ".", "format", "(", "\n", "checkpoints", "[", "0", "]", ",", "epoch", ",", "updates", ",", "write_timer", ".", "sum", ")", ")", "\n", "\n", "", "if", "not", "end_of_epoch", "and", "args", ".", "keep_interval_updates", ">", "0", ":", "\n", "# remove old checkpoints; checkpoints are sorted in descending order", "\n", "        ", "checkpoints", "=", "checkpoint_paths", "(", "\n", "args", ".", "save_dir", ",", "pattern", "=", "r'checkpoint_\\d+_(\\d+)\\.pt'", ",", "\n", ")", "\n", "for", "old_chk", "in", "checkpoints", "[", "args", ".", "keep_interval_updates", ":", "]", ":", "\n", "            ", "if", "os", ".", "path", ".", "lexists", "(", "old_chk", ")", ":", "\n", "                ", "os", ".", "remove", "(", "old_chk", ")", "\n", "\n", "", "", "", "if", "args", ".", "keep_last_epochs", ">", "0", ":", "\n", "# remove old epoch checkpoints; checkpoints are sorted in descending order", "\n", "        ", "checkpoints", "=", "checkpoint_paths", "(", "\n", "args", ".", "save_dir", ",", "pattern", "=", "r'checkpoint(\\d+)\\.pt'", ",", "\n", ")", "\n", "for", "old_chk", "in", "checkpoints", "[", "args", ".", "keep_last_epochs", ":", "]", ":", "\n", "            ", "if", "os", ".", "path", ".", "lexists", "(", "old_chk", ")", ":", "\n", "                ", "os", ".", "remove", "(", "old_chk", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.checkpoint_utils.load_checkpoint": [[93, 131], ["trainer.load_checkpoint", "trainer.lr_step", "os.makedirs", "os.path.join", "eval", "trainer.get_train_iterator", "trainer.get_train_iterator.load_state_dict", "trainer.get_train_iterator"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.checkpoint_utils.load_checkpoint", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.lr_step", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.get_train_iterator", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.load_state_dict", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer.get_train_iterator"], ["", "", "", "", "def", "load_checkpoint", "(", "args", ",", "trainer", ")", ":", "\n", "    ", "\"\"\"Load a checkpoint and restore the training iterator.\"\"\"", "\n", "# only one worker should attempt to create the required dir", "\n", "if", "args", ".", "distributed_rank", "==", "0", ":", "\n", "        ", "os", ".", "makedirs", "(", "args", ".", "save_dir", ",", "exist_ok", "=", "True", ")", "\n", "\n", "", "if", "args", ".", "restore_file", "==", "'checkpoint_last.pt'", ":", "\n", "        ", "checkpoint_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "save_dir", ",", "'checkpoint_last.pt'", ")", "\n", "", "else", ":", "\n", "        ", "checkpoint_path", "=", "args", ".", "restore_file", "\n", "\n", "", "extra_state", "=", "trainer", ".", "load_checkpoint", "(", "\n", "checkpoint_path", ",", "\n", "args", ".", "reset_optimizer", ",", "\n", "args", ".", "reset_lr_scheduler", ",", "\n", "eval", "(", "args", ".", "optimizer_overrides", ")", ",", "\n", "reset_meters", "=", "args", ".", "reset_meters", ",", "\n", ")", "\n", "\n", "if", "(", "\n", "extra_state", "is", "not", "None", "\n", "and", "'best'", "in", "extra_state", "\n", "and", "not", "args", ".", "reset_optimizer", "\n", "and", "not", "args", ".", "reset_meters", "\n", ")", ":", "\n", "        ", "save_checkpoint", ".", "best", "=", "extra_state", "[", "'best'", "]", "\n", "\n", "", "if", "extra_state", "is", "not", "None", "and", "not", "args", ".", "reset_dataloader", ":", "\n", "# restore iterator from checkpoint", "\n", "        ", "itr_state", "=", "extra_state", "[", "'train_iterator'", "]", "\n", "epoch_itr", "=", "trainer", ".", "get_train_iterator", "(", "epoch", "=", "itr_state", "[", "'epoch'", "]", ")", "\n", "epoch_itr", ".", "load_state_dict", "(", "itr_state", ")", "\n", "", "else", ":", "\n", "        ", "epoch_itr", "=", "trainer", ".", "get_train_iterator", "(", "epoch", "=", "0", ")", "\n", "\n", "", "trainer", ".", "lr_step", "(", "epoch_itr", ".", "epoch", ")", "\n", "\n", "return", "extra_state", ",", "epoch_itr", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.checkpoint_utils.load_checkpoint_to_cpu": [[133, 145], ["print", "torch.load", "checkpoint_utils._upgrade_state_dict", "arg_overrides.items", "setattr", "torch.serialization.default_restore_location"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.checkpoint_utils._upgrade_state_dict"], ["", "def", "load_checkpoint_to_cpu", "(", "path", ",", "arg_overrides", "=", "None", ")", ":", "\n", "    ", "\"\"\"Loads a checkpoint to CPU (with upgrading for backward compatibility).\"\"\"", "\n", "print", "(", "'LOAD_______'", ",", "path", ")", "\n", "state", "=", "torch", ".", "load", "(", "\n", "path", ",", "map_location", "=", "lambda", "s", ",", "l", ":", "default_restore_location", "(", "s", ",", "'cpu'", ")", ",", "\n", ")", "\n", "args", "=", "state", "[", "'args'", "]", "\n", "if", "arg_overrides", "is", "not", "None", ":", "\n", "        ", "for", "arg_name", ",", "arg_val", "in", "arg_overrides", ".", "items", "(", ")", ":", "\n", "            ", "setattr", "(", "args", ",", "arg_name", ",", "arg_val", ")", "\n", "", "", "state", "=", "_upgrade_state_dict", "(", "state", ")", "\n", "return", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.checkpoint_utils.load_model_ensemble": [[147, 158], ["checkpoint_utils.load_model_ensemble_and_task"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.checkpoint_utils.load_model_ensemble_and_task"], ["", "def", "load_model_ensemble", "(", "filenames", ",", "arg_overrides", "=", "None", ",", "task", "=", "None", ")", ":", "\n", "    ", "\"\"\"Loads an ensemble of models.\n\n    Args:\n        filenames (List[str]): checkpoint files to load\n        arg_overrides (Dict[str,Any], optional): override model args that\n            were used during model training\n        task (fairseq.tasks.FairseqTask, optional): task to use for loading\n    \"\"\"", "\n", "ensemble", ",", "args", ",", "_task", "=", "load_model_ensemble_and_task", "(", "filenames", ",", "arg_overrides", ",", "task", ")", "\n", "return", "ensemble", ",", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.checkpoint_utils.load_model_ensemble_and_task": [[160, 178], ["checkpoint_utils.load_checkpoint_to_cpu", "tasks.setup_task.build_model", "task.build_model.load_state_dict", "ensemble.append", "os.path.exists", "IOError", "tasks.setup_task"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.checkpoint_utils.load_checkpoint_to_cpu", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lightconv_lm.LightConvLanguageModel.build_model", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.load_state_dict", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.tasks.sentence_prediction.SentencePredictionTask.setup_task"], ["", "def", "load_model_ensemble_and_task", "(", "filenames", ",", "arg_overrides", "=", "None", ",", "task", "=", "None", ")", ":", "\n", "    ", "from", "fairseq", "import", "tasks", "\n", "\n", "ensemble", "=", "[", "]", "\n", "for", "filename", "in", "filenames", ":", "\n", "        ", "if", "not", "os", ".", "path", ".", "exists", "(", "filename", ")", ":", "\n", "            ", "raise", "IOError", "(", "'Model file not found: {}'", ".", "format", "(", "filename", ")", ")", "\n", "", "state", "=", "load_checkpoint_to_cpu", "(", "filename", ",", "arg_overrides", ")", "\n", "\n", "args", "=", "state", "[", "'args'", "]", "\n", "if", "task", "is", "None", ":", "\n", "            ", "task", "=", "tasks", ".", "setup_task", "(", "args", ")", "\n", "\n", "# build model for ensemble", "\n", "", "model", "=", "task", ".", "build_model", "(", "args", ")", "\n", "model", ".", "load_state_dict", "(", "state", "[", "'model'", "]", ",", "strict", "=", "True", ")", "\n", "ensemble", ".", "append", "(", "model", ")", "\n", "", "return", "ensemble", ",", "args", ",", "task", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.checkpoint_utils.checkpoint_paths": [[180, 197], ["re.compile", "os.listdir", "enumerate", "re.compile.fullmatch", "os.path.join", "entries.append", "sorted", "int", "len", "pt_regexp.fullmatch.group", "pt_regexp.fullmatch.group", "pt_regexp.fullmatch.groups"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "checkpoint_paths", "(", "path", ",", "pattern", "=", "r'checkpoint(\\d+)\\.pt'", ")", ":", "\n", "    ", "\"\"\"Retrieves all checkpoints found in `path` directory.\n\n    Checkpoints are identified by matching filename to the specified pattern. If\n    the pattern contains groups, the result will be sorted by the first group in\n    descending order.\n    \"\"\"", "\n", "pt_regexp", "=", "re", ".", "compile", "(", "pattern", ")", "\n", "files", "=", "os", ".", "listdir", "(", "path", ")", "\n", "\n", "entries", "=", "[", "]", "\n", "for", "i", ",", "f", "in", "enumerate", "(", "files", ")", ":", "\n", "        ", "m", "=", "pt_regexp", ".", "fullmatch", "(", "f", ")", "\n", "if", "m", "is", "not", "None", ":", "\n", "            ", "idx", "=", "int", "(", "m", ".", "group", "(", "1", ")", ")", "if", "len", "(", "m", ".", "groups", "(", ")", ")", ">", "0", "else", "i", "\n", "entries", ".", "append", "(", "(", "idx", ",", "m", ".", "group", "(", "0", ")", ")", ")", "\n", "", "", "return", "[", "os", ".", "path", ".", "join", "(", "path", ",", "x", "[", "1", "]", ")", "for", "x", "in", "sorted", "(", "entries", ",", "reverse", "=", "True", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.checkpoint_utils.torch_persistent_save": [[199, 206], ["range", "torch.save", "logging.error", "traceback.format_exc"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.save"], ["", "def", "torch_persistent_save", "(", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "    ", "for", "i", "in", "range", "(", "3", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "return", "torch", ".", "save", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "", "except", "Exception", ":", "\n", "            ", "if", "i", "==", "2", ":", "\n", "                ", "logging", ".", "error", "(", "traceback", ".", "format_exc", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.checkpoint_utils.convert_state_dict_type": [[208, 220], ["isinstance", "collections.OrderedDict", "state_dict.items", "isinstance", "checkpoint_utils.convert_state_dict_type", "torch.is_tensor", "checkpoint_utils.convert_state_dict_type", "state_dict.type"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.checkpoint_utils.convert_state_dict_type", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.checkpoint_utils.convert_state_dict_type"], ["", "", "", "", "def", "convert_state_dict_type", "(", "state_dict", ",", "ttype", "=", "torch", ".", "FloatTensor", ")", ":", "\n", "    ", "if", "isinstance", "(", "state_dict", ",", "dict", ")", ":", "\n", "        ", "cpu_dict", "=", "OrderedDict", "(", ")", "\n", "for", "k", ",", "v", "in", "state_dict", ".", "items", "(", ")", ":", "\n", "            ", "cpu_dict", "[", "k", "]", "=", "convert_state_dict_type", "(", "v", ")", "\n", "", "return", "cpu_dict", "\n", "", "elif", "isinstance", "(", "state_dict", ",", "list", ")", ":", "\n", "        ", "return", "[", "convert_state_dict_type", "(", "v", ")", "for", "v", "in", "state_dict", "]", "\n", "", "elif", "torch", ".", "is_tensor", "(", "state_dict", ")", ":", "\n", "        ", "return", "state_dict", ".", "type", "(", "ttype", ")", "\n", "", "else", ":", "\n", "        ", "return", "state_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.checkpoint_utils.save_state": [[222, 246], ["checkpoint_utils.torch_persistent_save", "checkpoint_utils.convert_state_dict_type", "optimizer.state_dict", "lr_scheduler.state_dict"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.checkpoint_utils.torch_persistent_save", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.checkpoint_utils.convert_state_dict_type", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.state_dict", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.state_dict"], ["", "", "def", "save_state", "(", "\n", "filename", ",", "args", ",", "model_state_dict", ",", "criterion", ",", "optimizer", ",", "lr_scheduler", ",", "\n", "num_updates", ",", "optim_history", "=", "None", ",", "extra_state", "=", "None", ",", "\n", ")", ":", "\n", "    ", "if", "optim_history", "is", "None", ":", "\n", "        ", "optim_history", "=", "[", "]", "\n", "", "if", "extra_state", "is", "None", ":", "\n", "        ", "extra_state", "=", "{", "}", "\n", "", "state_dict", "=", "{", "\n", "'args'", ":", "args", ",", "\n", "'model'", ":", "model_state_dict", "if", "model_state_dict", "else", "{", "}", ",", "\n", "'optimizer_history'", ":", "optim_history", "+", "[", "\n", "{", "\n", "'criterion_name'", ":", "criterion", ".", "__class__", ".", "__name__", ",", "\n", "'optimizer_name'", ":", "optimizer", ".", "__class__", ".", "__name__", ",", "\n", "'lr_scheduler_state'", ":", "lr_scheduler", ".", "state_dict", "(", ")", ",", "\n", "'num_updates'", ":", "num_updates", ",", "\n", "}", "\n", "]", ",", "\n", "'extra_state'", ":", "extra_state", ",", "\n", "}", "\n", "if", "not", "args", ".", "no_save_optimizer_state", ":", "\n", "        ", "state_dict", "[", "'last_optimizer_state'", "]", "=", "convert_state_dict_type", "(", "optimizer", ".", "state_dict", "(", ")", ")", "\n", "", "torch_persistent_save", "(", "state_dict", ",", "filename", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.checkpoint_utils._upgrade_state_dict": [[248, 314], ["registry.set_defaults", "registry.set_defaults", "registry.REGISTRIES.items", "hasattr", "hasattr", "getattr", "hasattr", "state[].get", "registry.set_defaults"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.registry.set_defaults", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.registry.set_defaults", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.registry.set_defaults"], ["", "def", "_upgrade_state_dict", "(", "state", ")", ":", "\n", "    ", "\"\"\"Helper for upgrading old model checkpoints.\"\"\"", "\n", "from", "fairseq", "import", "models", ",", "registry", ",", "tasks", "\n", "\n", "# add optimizer_history", "\n", "if", "'optimizer_history'", "not", "in", "state", ":", "\n", "        ", "state", "[", "'optimizer_history'", "]", "=", "[", "\n", "{", "\n", "'criterion_name'", ":", "'CrossEntropyCriterion'", ",", "\n", "'best_loss'", ":", "state", "[", "'best_loss'", "]", ",", "\n", "}", ",", "\n", "]", "\n", "state", "[", "'last_optimizer_state'", "]", "=", "state", "[", "'optimizer'", "]", "\n", "del", "state", "[", "'optimizer'", "]", "\n", "del", "state", "[", "'best_loss'", "]", "\n", "# move extra_state into sub-dictionary", "\n", "", "if", "'epoch'", "in", "state", "and", "'extra_state'", "not", "in", "state", ":", "\n", "        ", "state", "[", "'extra_state'", "]", "=", "{", "\n", "'epoch'", ":", "state", "[", "'epoch'", "]", ",", "\n", "'batch_offset'", ":", "state", "[", "'batch_offset'", "]", ",", "\n", "'val_loss'", ":", "state", "[", "'val_loss'", "]", ",", "\n", "}", "\n", "del", "state", "[", "'epoch'", "]", "\n", "del", "state", "[", "'batch_offset'", "]", "\n", "del", "state", "[", "'val_loss'", "]", "\n", "# reduce optimizer history's memory usage (only keep the last state)", "\n", "", "if", "'optimizer'", "in", "state", "[", "'optimizer_history'", "]", "[", "-", "1", "]", ":", "\n", "        ", "state", "[", "'last_optimizer_state'", "]", "=", "state", "[", "'optimizer_history'", "]", "[", "-", "1", "]", "[", "'optimizer'", "]", "\n", "for", "optim_hist", "in", "state", "[", "'optimizer_history'", "]", ":", "\n", "            ", "del", "optim_hist", "[", "'optimizer'", "]", "\n", "# record the optimizer class name", "\n", "", "", "if", "'optimizer_name'", "not", "in", "state", "[", "'optimizer_history'", "]", "[", "-", "1", "]", ":", "\n", "        ", "state", "[", "'optimizer_history'", "]", "[", "-", "1", "]", "[", "'optimizer_name'", "]", "=", "'FairseqNAG'", "\n", "# move best_loss into lr_scheduler_state", "\n", "", "if", "'lr_scheduler_state'", "not", "in", "state", "[", "'optimizer_history'", "]", "[", "-", "1", "]", ":", "\n", "        ", "state", "[", "'optimizer_history'", "]", "[", "-", "1", "]", "[", "'lr_scheduler_state'", "]", "=", "{", "\n", "'best'", ":", "state", "[", "'optimizer_history'", "]", "[", "-", "1", "]", "[", "'best_loss'", "]", ",", "\n", "}", "\n", "del", "state", "[", "'optimizer_history'", "]", "[", "-", "1", "]", "[", "'best_loss'", "]", "\n", "# keep track of number of updates", "\n", "", "if", "'num_updates'", "not", "in", "state", "[", "'optimizer_history'", "]", "[", "-", "1", "]", ":", "\n", "        ", "state", "[", "'optimizer_history'", "]", "[", "-", "1", "]", "[", "'num_updates'", "]", "=", "0", "\n", "# old model checkpoints may not have separate source/target positions", "\n", "", "if", "hasattr", "(", "state", "[", "'args'", "]", ",", "'max_positions'", ")", "and", "not", "hasattr", "(", "state", "[", "'args'", "]", ",", "'max_source_positions'", ")", ":", "\n", "        ", "state", "[", "'args'", "]", ".", "max_source_positions", "=", "state", "[", "'args'", "]", ".", "max_positions", "\n", "state", "[", "'args'", "]", ".", "max_target_positions", "=", "state", "[", "'args'", "]", ".", "max_positions", "\n", "# use stateful training data iterator", "\n", "", "if", "'train_iterator'", "not", "in", "state", "[", "'extra_state'", "]", ":", "\n", "        ", "state", "[", "'extra_state'", "]", "[", "'train_iterator'", "]", "=", "{", "\n", "'epoch'", ":", "state", "[", "'extra_state'", "]", "[", "'epoch'", "]", ",", "\n", "'iterations_in_epoch'", ":", "state", "[", "'extra_state'", "]", ".", "get", "(", "'batch_offset'", ",", "0", ")", ",", "\n", "}", "\n", "# default to translation task", "\n", "", "if", "not", "hasattr", "(", "state", "[", "'args'", "]", ",", "'task'", ")", ":", "\n", "        ", "state", "[", "'args'", "]", ".", "task", "=", "'translation'", "\n", "\n", "# set any missing default values in the task, model or other registries", "\n", "", "registry", ".", "set_defaults", "(", "state", "[", "'args'", "]", ",", "tasks", ".", "TASK_REGISTRY", "[", "state", "[", "'args'", "]", ".", "task", "]", ")", "\n", "registry", ".", "set_defaults", "(", "state", "[", "'args'", "]", ",", "models", ".", "ARCH_MODEL_REGISTRY", "[", "state", "[", "'args'", "]", ".", "arch", "]", ")", "\n", "for", "registry_name", ",", "REGISTRY", "in", "registry", ".", "REGISTRIES", ".", "items", "(", ")", ":", "\n", "        ", "choice", "=", "getattr", "(", "state", "[", "'args'", "]", ",", "registry_name", ",", "None", ")", "\n", "if", "choice", "is", "not", "None", ":", "\n", "            ", "cls", "=", "REGISTRY", "[", "'registry'", "]", "[", "choice", "]", "\n", "registry", ".", "set_defaults", "(", "state", "[", "'args'", "]", ",", "cls", ")", "\n", "\n", "", "", "return", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.checkpoint_utils.load_pretrained_component_from_model": [[316, 345], ["checkpoint_utils.load_checkpoint_to_cpu", "isinstance", "collections.OrderedDict", "state[].keys", "component.load_state_dict", "os.path.exists", "IOError", "isinstance", "key.startswith", "ValueError", "len"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.checkpoint_utils.load_checkpoint_to_cpu", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.load_state_dict", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists"], ["", "def", "load_pretrained_component_from_model", "(", "\n", "component", ":", "Union", "[", "FairseqEncoder", ",", "FairseqDecoder", "]", ",", "checkpoint", ":", "str", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Load a pretrained FairseqEncoder or FairseqDecoder from checkpoint into the\n    provided `component` object. If state_dict fails to load, there may be a\n    mismatch in the architecture of the corresponding `component` found in the\n    `checkpoint` file.\n    \"\"\"", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "checkpoint", ")", ":", "\n", "        ", "raise", "IOError", "(", "'Model file not found: {}'", ".", "format", "(", "checkpoint", ")", ")", "\n", "", "state", "=", "load_checkpoint_to_cpu", "(", "checkpoint", ")", "\n", "if", "isinstance", "(", "component", ",", "FairseqEncoder", ")", ":", "\n", "        ", "component_type", "=", "\"encoder\"", "\n", "", "elif", "isinstance", "(", "component", ",", "FairseqDecoder", ")", ":", "\n", "        ", "component_type", "=", "\"decoder\"", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"component to load must be either a FairseqEncoder or \"", "\n", "\"FairseqDecoder. Loading other component types are not supported.\"", "\n", ")", "\n", "", "component_state_dict", "=", "OrderedDict", "(", ")", "\n", "for", "key", "in", "state", "[", "\"model\"", "]", ".", "keys", "(", ")", ":", "\n", "        ", "if", "key", ".", "startswith", "(", "component_type", ")", ":", "\n", "# encoder.input_layers.0.0.weight --> input_layers.0.0.weight", "\n", "            ", "component_subkey", "=", "key", "[", "len", "(", "component_type", ")", "+", "1", ":", "]", "\n", "component_state_dict", "[", "component_subkey", "]", "=", "state", "[", "\"model\"", "]", "[", "key", "]", "\n", "", "", "component", ".", "load_state_dict", "(", "component_state_dict", ",", "strict", "=", "True", ")", "\n", "return", "component", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.checkpoint_utils.verify_checkpoint_directory": [[347, 359], ["os.path.join", "os.path.exists", "os.makedirs", "os.remove", "open", "print"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["", "def", "verify_checkpoint_directory", "(", "save_dir", ":", "str", ")", "->", "None", ":", "\n", "    ", "if", "not", "os", ".", "path", ".", "exists", "(", "save_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "save_dir", ",", "exist_ok", "=", "True", ")", "\n", "", "temp_file_path", "=", "os", ".", "path", ".", "join", "(", "save_dir", ",", "'dummy'", ")", "\n", "try", ":", "\n", "        ", "with", "open", "(", "temp_file_path", ",", "'w'", ")", ":", "\n", "            ", "pass", "\n", "", "", "except", "OSError", "as", "e", ":", "\n", "        ", "print", "(", "'| Unable to access checkpoint save directory: {}'", ".", "format", "(", "save_dir", ")", ")", "\n", "raise", "e", "\n", "", "else", ":", "\n", "        ", "os", ".", "remove", "(", "temp_file_path", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.sequence_generator.SequenceGenerator.__init__": [[18, 105], ["tgt_dict.pad", "tgt_dict.unk", "tgt_dict.eos", "len", "min", "fairseq.search.Sampling", "fairseq.search.DiverseBeamSearch", "fairseq.search.LengthConstrainedBeamSearch", "fairseq.search.BeamSearch"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.unk", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "tgt_dict", ",", "\n", "beam_size", "=", "1", ",", "\n", "max_len_a", "=", "0", ",", "\n", "max_len_b", "=", "200", ",", "\n", "min_len", "=", "1", ",", "\n", "normalize_scores", "=", "True", ",", "\n", "len_penalty", "=", "1.", ",", "\n", "unk_penalty", "=", "0.", ",", "\n", "retain_dropout", "=", "False", ",", "\n", "sampling", "=", "False", ",", "\n", "sampling_topk", "=", "-", "1", ",", "\n", "sampling_topp", "=", "-", "1.0", ",", "\n", "temperature", "=", "1.", ",", "\n", "diverse_beam_groups", "=", "-", "1", ",", "\n", "diverse_beam_strength", "=", "0.5", ",", "\n", "match_source_len", "=", "False", ",", "\n", "no_repeat_ngram_size", "=", "0", ",", "\n", "state_machine", "=", "None", "\n", ")", ":", "\n", "        ", "\"\"\"Generates translations of a given source sentence.\n\n        Args:\n            tgt_dict (~fairseq.data.Dictionary): target dictionary\n            beam_size (int, optional): beam width (default: 1)\n            max_len_a/b (int, optional): generate sequences of maximum length\n                ax + b, where x is the source length\n            min_len (int, optional): the minimum length of the generated output\n                (not including end-of-sentence)\n            normalize_scores (bool, optional): normalize scores by the length\n                of the output (default: True)\n            len_penalty (float, optional): length penalty, where <1.0 favors\n                shorter, >1.0 favors longer sentences (default: 1.0)\n            unk_penalty (float, optional): unknown word penalty, where <0\n                produces more unks, >0 produces fewer (default: 0.0)\n            retain_dropout (bool, optional): use dropout when generating\n                (default: False)\n            sampling (bool, optional): sample outputs instead of beam search\n                (default: False)\n            sampling_topk (int, optional): only sample among the top-k choices\n                at each step (default: -1)\n            sampling_topp (float, optional): only sample among the smallest set\n                of words whose cumulative probability mass exceeds p\n                at each step (default: -1.0)\n            temperature (float, optional): temperature, where values\n                >1.0 produce more uniform samples and values <1.0 produce\n                sharper samples (default: 1.0)\n            diverse_beam_groups/strength (float, optional): parameters for\n                Diverse Beam Search sampling\n            match_source_len (bool, optional): outputs should match the source\n                length (default: False)\n        \"\"\"", "\n", "self", ".", "pad", "=", "tgt_dict", ".", "pad", "(", ")", "\n", "self", ".", "unk", "=", "tgt_dict", ".", "unk", "(", ")", "\n", "self", ".", "eos", "=", "tgt_dict", ".", "eos", "(", ")", "\n", "self", ".", "vocab_size", "=", "len", "(", "tgt_dict", ")", "\n", "self", ".", "beam_size", "=", "beam_size", "\n", "# the max beam size is the dictionary size - 1, since we never select pad", "\n", "self", ".", "beam_size", "=", "min", "(", "beam_size", ",", "self", ".", "vocab_size", "-", "1", ")", "\n", "self", ".", "max_len_a", "=", "max_len_a", "\n", "self", ".", "max_len_b", "=", "max_len_b", "\n", "self", ".", "min_len", "=", "min_len", "\n", "self", ".", "normalize_scores", "=", "normalize_scores", "\n", "self", ".", "len_penalty", "=", "len_penalty", "\n", "self", ".", "unk_penalty", "=", "unk_penalty", "\n", "self", ".", "retain_dropout", "=", "retain_dropout", "\n", "self", ".", "temperature", "=", "temperature", "\n", "self", ".", "match_source_len", "=", "match_source_len", "\n", "self", ".", "no_repeat_ngram_size", "=", "no_repeat_ngram_size", "\n", "# Type of state machine used", "\n", "self", ".", "state_machine", "=", "state_machine", "\n", "\n", "assert", "sampling_topk", "<", "0", "or", "sampling", ",", "'--sampling-topk requires --sampling'", "\n", "assert", "sampling_topp", "<", "0", "or", "sampling", ",", "'--sampling-topp requires --sampling'", "\n", "assert", "temperature", ">", "0", ",", "'--temperature must be greater than 0'", "\n", "\n", "if", "sampling", ":", "\n", "            ", "self", ".", "search", "=", "search", ".", "Sampling", "(", "tgt_dict", ",", "sampling_topk", ",", "sampling_topp", ")", "\n", "", "elif", "diverse_beam_groups", ">", "0", ":", "\n", "            ", "self", ".", "search", "=", "search", ".", "DiverseBeamSearch", "(", "tgt_dict", ",", "diverse_beam_groups", ",", "diverse_beam_strength", ")", "\n", "", "elif", "match_source_len", ":", "\n", "            ", "self", ".", "search", "=", "search", ".", "LengthConstrainedBeamSearch", "(", "\n", "tgt_dict", ",", "min_len_a", "=", "1", ",", "min_len_b", "=", "0", ",", "max_len_a", "=", "1", ",", "max_len_b", "=", "0", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "search", "=", "search", ".", "BeamSearch", "(", "tgt_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.sequence_generator.SequenceGenerator.generate": [[106, 569], ["torch.no_grad", "sequence_generator.EnsembleModel", "src_tokens.size", "sequence_generator.EnsembleModel.forward_encoder", "torch.arange().view().repeat().view", "new_order.to().long.to().long.to().long", "sequence_generator.EnsembleModel.reorder_encoder_out", "src_tokens.new().float().fill_", "replicate_first_beam.clone", "src_tokens.new().long().fill_", "replicate_first_beam.clone", "src_tokens.new_zeros().eq", "torch.arange().type_as", "sequence_generator.SequenceGenerator.state_machine.reset", "range", "range", "EnsembleModel.eval", "src_lengths.max().item", "int", "replicate_first_beam.index_select", "set", "enumerate", "src_tokens[].clone().detach", "src_lengths[].clone().detach", "sequence_generator.SequenceGenerator.state_machine.get_active_logits", "sequence_generator.EnsembleModel.forward_decoder", "transition_amr_parser.stack_transformer.amr_state_machine.fix_shift_multi_task", "replicate_first_beam.type_as", "scores_buf.type_as.type_as.type_as", "sequence_generator.SequenceGenerator.generate.buffer"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.sequence_generator.EnsembleModel.forward_encoder", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.sequence_generator.EnsembleModel.reorder_encoder_out", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.reset", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.get_active_logits", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.sequence_generator.EnsembleModel.forward_decoder", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.fix_shift_multi_task"], ["", "", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "generate", "(", "\n", "self", ",", "\n", "models", ",", "\n", "sample", ",", "\n", "prefix_tokens", "=", "None", ",", "\n", "bos_token", "=", "None", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "\"\"\"Generate a batch of translations.\n\n        Args:\n            models (List[~fairseq.models.FairseqModel]): ensemble of models\n            sample (dict): batch\n            prefix_tokens (torch.LongTensor, optional): force decoder to begin\n                with these tokens\n        \"\"\"", "\n", "model", "=", "EnsembleModel", "(", "models", ")", "\n", "if", "not", "self", ".", "retain_dropout", ":", "\n", "            ", "model", ".", "eval", "(", ")", "\n", "\n", "# model.forward normally channels prev_output_tokens into the decoder", "\n", "# separately, but SequenceGenerator directly calls model.encoder", "\n", "", "encoder_input", "=", "{", "\n", "k", ":", "v", "for", "k", ",", "v", "in", "sample", "[", "'net_input'", "]", ".", "items", "(", ")", "\n", "if", "k", "!=", "'prev_output_tokens'", "\n", "}", "\n", "\n", "# Add dummy memory and memory pos", "\n", "encoder_input", "[", "'memory'", "]", "=", "None", "\n", "encoder_input", "[", "'memory_pos'", "]", "=", "None", "\n", "\n", "src_tokens", "=", "encoder_input", "[", "'src_tokens'", "]", "\n", "src_lengths", "=", "(", "src_tokens", ".", "ne", "(", "self", ".", "eos", ")", "&", "src_tokens", ".", "ne", "(", "self", ".", "pad", ")", ")", ".", "long", "(", ")", ".", "sum", "(", "dim", "=", "1", ")", "\n", "input_size", "=", "src_tokens", ".", "size", "(", ")", "\n", "# batch dimension goes first followed by source lengths", "\n", "bsz", "=", "input_size", "[", "0", "]", "\n", "src_len", "=", "input_size", "[", "1", "]", "\n", "beam_size", "=", "self", ".", "beam_size", "\n", "\n", "if", "self", ".", "match_source_len", ":", "\n", "            ", "max_len", "=", "src_lengths", ".", "max", "(", ")", ".", "item", "(", ")", "\n", "", "else", ":", "\n", "# FIXME: Hard coded limit for transition-based parsing", "\n", "            ", "max_len", "=", "int", "(", "math", ".", "ceil", "(", "src_lengths", ".", "max", "(", ")", ".", "item", "(", ")", "*", "25", ")", ")", "\n", "\n", "# compute the encoder output for each beam", "\n", "", "encoder_outs", "=", "model", ".", "forward_encoder", "(", "encoder_input", ")", "\n", "new_order", "=", "torch", ".", "arange", "(", "bsz", ")", ".", "view", "(", "-", "1", ",", "1", ")", ".", "repeat", "(", "1", ",", "beam_size", ")", ".", "view", "(", "-", "1", ")", "\n", "new_order", "=", "new_order", ".", "to", "(", "src_tokens", ".", "device", ")", ".", "long", "(", ")", "\n", "encoder_outs", "=", "model", ".", "reorder_encoder_out", "(", "encoder_outs", ",", "new_order", ")", "\n", "\n", "# initialize buffers", "\n", "scores", "=", "src_tokens", ".", "new", "(", "bsz", "*", "beam_size", ",", "max_len", "+", "1", ")", ".", "float", "(", ")", ".", "fill_", "(", "0", ")", "\n", "scores_buf", "=", "scores", ".", "clone", "(", ")", "\n", "tokens", "=", "src_tokens", ".", "new", "(", "bsz", "*", "beam_size", ",", "max_len", "+", "2", ")", ".", "long", "(", ")", ".", "fill_", "(", "self", ".", "pad", ")", "\n", "tokens_buf", "=", "tokens", ".", "clone", "(", ")", "\n", "tokens", "[", ":", ",", "0", "]", "=", "self", ".", "eos", "if", "bos_token", "is", "None", "else", "bos_token", "\n", "attn", ",", "attn_buf", "=", "None", ",", "None", "\n", "nonpad_idxs", "=", "None", "\n", "\n", "# The blacklist indicates candidates that should be ignored.", "\n", "# For example, suppose we're sampling and have already finalized 2/5", "\n", "# samples. Then the blacklist would mark 2 positions as being ignored,", "\n", "# so that we only finalize the remaining 3 samples.", "\n", "blacklist", "=", "src_tokens", ".", "new_zeros", "(", "bsz", ",", "beam_size", ")", ".", "eq", "(", "-", "1", ")", "# forward and backward-compatible False mask", "\n", "\n", "# list of completed sentences", "\n", "finalized", "=", "[", "[", "]", "for", "i", "in", "range", "(", "bsz", ")", "]", "\n", "finished", "=", "[", "False", "for", "i", "in", "range", "(", "bsz", ")", "]", "\n", "num_remaining_sent", "=", "bsz", "\n", "\n", "# number of candidate hypos per step", "\n", "cand_size", "=", "2", "*", "beam_size", "# 2 x beam size in case half are EOS", "\n", "\n", "# offset arrays for converting between different indexing schemes", "\n", "bbsz_offsets", "=", "(", "torch", ".", "arange", "(", "0", ",", "bsz", ")", "*", "beam_size", ")", ".", "unsqueeze", "(", "1", ")", ".", "type_as", "(", "tokens", ")", "\n", "cand_offsets", "=", "torch", ".", "arange", "(", "0", ",", "cand_size", ")", ".", "type_as", "(", "tokens", ")", "\n", "\n", "# helper function for allocating buffers on the fly", "\n", "buffers", "=", "{", "}", "\n", "\n", "def", "buffer", "(", "name", ",", "type_of", "=", "tokens", ")", ":", "# noqa", "\n", "            ", "if", "name", "not", "in", "buffers", ":", "\n", "                ", "buffers", "[", "name", "]", "=", "type_of", ".", "new", "(", ")", "\n", "", "return", "buffers", "[", "name", "]", "\n", "\n", "", "def", "is_finished", "(", "sent", ",", "step", ",", "unfin_idx", ")", ":", "\n", "            ", "\"\"\"\n            Check whether we've finished generation for a given sentence, by\n            comparing the worst score among finalized hypotheses to the best\n            possible score among unfinalized hypotheses.\n            \"\"\"", "\n", "assert", "len", "(", "finalized", "[", "sent", "]", ")", "<=", "beam_size", "\n", "if", "len", "(", "finalized", "[", "sent", "]", ")", "==", "beam_size", ":", "\n", "                ", "return", "True", "\n", "", "return", "False", "\n", "\n", "", "def", "finalize_hypos", "(", "step", ",", "bbsz_idx", ",", "eos_scores", ")", ":", "\n", "            ", "\"\"\"\n            Finalize the given hypotheses at this step, while keeping the total\n            number of finalized hypotheses per sentence <= beam_size.\n\n            Note: the input must be in the desired finalization order, so that\n            hypotheses that appear earlier in the input are preferred to those\n            that appear later.\n\n            Args:\n                step: current time step\n                bbsz_idx: A vector of indices in the range [0, bsz*beam_size),\n                    indicating which hypotheses to finalize\n                eos_scores: A vector of the same size as bbsz_idx containing\n                    scores for each hypothesis\n            \"\"\"", "\n", "assert", "bbsz_idx", ".", "numel", "(", ")", "==", "eos_scores", ".", "numel", "(", ")", "\n", "\n", "# clone relevant token and attention tensors", "\n", "tokens_clone", "=", "tokens", ".", "index_select", "(", "0", ",", "bbsz_idx", ")", "\n", "tokens_clone", "=", "tokens_clone", "[", ":", ",", "1", ":", "step", "+", "2", "]", "# skip the first index, which is EOS", "\n", "assert", "not", "tokens_clone", ".", "eq", "(", "self", ".", "eos", ")", ".", "any", "(", ")", "\n", "tokens_clone", "[", ":", ",", "step", "]", "=", "self", ".", "eos", "\n", "attn_clone", "=", "attn", ".", "index_select", "(", "0", ",", "bbsz_idx", ")", "[", ":", ",", ":", ",", "1", ":", "step", "+", "2", "]", "if", "attn", "is", "not", "None", "else", "None", "\n", "\n", "# compute scores per token position", "\n", "pos_scores", "=", "scores", ".", "index_select", "(", "0", ",", "bbsz_idx", ")", "[", ":", ",", ":", "step", "+", "1", "]", "\n", "pos_scores", "[", ":", ",", "step", "]", "=", "eos_scores", "\n", "# convert from cumulative to per-position scores", "\n", "pos_scores", "[", ":", ",", "1", ":", "]", "=", "pos_scores", "[", ":", ",", "1", ":", "]", "-", "pos_scores", "[", ":", ",", ":", "-", "1", "]", "\n", "\n", "# normalize sentence-level scores", "\n", "if", "self", ".", "normalize_scores", ":", "\n", "                ", "eos_scores", "/=", "(", "step", "+", "1", ")", "**", "self", ".", "len_penalty", "\n", "\n", "", "cum_unfin", "=", "[", "]", "\n", "prev", "=", "0", "\n", "for", "f", "in", "finished", ":", "\n", "                ", "if", "f", ":", "\n", "                    ", "prev", "+=", "1", "\n", "", "else", ":", "\n", "                    ", "cum_unfin", ".", "append", "(", "prev", ")", "\n", "\n", "", "", "sents_seen", "=", "set", "(", ")", "\n", "for", "i", ",", "(", "idx", ",", "score", ")", "in", "enumerate", "(", "zip", "(", "bbsz_idx", ".", "tolist", "(", ")", ",", "eos_scores", ".", "tolist", "(", ")", ")", ")", ":", "\n", "                ", "unfin_idx", "=", "idx", "//", "beam_size", "\n", "sent", "=", "unfin_idx", "+", "cum_unfin", "[", "unfin_idx", "]", "\n", "\n", "sents_seen", ".", "add", "(", "(", "sent", ",", "unfin_idx", ")", ")", "\n", "\n", "if", "self", ".", "match_source_len", "and", "step", ">", "src_lengths", "[", "unfin_idx", "]", ":", "\n", "                    ", "score", "=", "-", "math", ".", "inf", "\n", "\n", "", "def", "get_hypo", "(", ")", ":", "\n", "\n", "                    ", "if", "attn_clone", "is", "not", "None", ":", "\n", "# remove padding tokens from attn scores", "\n", "                        ", "hypo_attn", "=", "attn_clone", "[", "i", "]", "[", "nonpad_idxs", "[", "sent", "]", "]", "\n", "_", ",", "alignment", "=", "hypo_attn", ".", "max", "(", "dim", "=", "0", ")", "\n", "", "else", ":", "\n", "                        ", "hypo_attn", "=", "None", "\n", "alignment", "=", "None", "\n", "\n", "", "return", "{", "\n", "'tokens'", ":", "tokens_clone", "[", "i", "]", ",", "\n", "'score'", ":", "score", ",", "\n", "'attention'", ":", "hypo_attn", ",", "# src_len x tgt_len", "\n", "'alignment'", ":", "alignment", ",", "\n", "'positional_scores'", ":", "pos_scores", "[", "i", "]", ",", "\n", "}", "\n", "\n", "", "if", "len", "(", "finalized", "[", "sent", "]", ")", "<", "beam_size", ":", "\n", "                    ", "finalized", "[", "sent", "]", ".", "append", "(", "get_hypo", "(", ")", ")", "\n", "\n", "", "", "newly_finished", "=", "[", "]", "\n", "for", "sent", ",", "unfin_idx", "in", "sents_seen", ":", "\n", "# check termination conditions for this sentence", "\n", "                ", "if", "not", "finished", "[", "sent", "]", "and", "is_finished", "(", "sent", ",", "step", ",", "unfin_idx", ")", ":", "\n", "                    ", "finished", "[", "sent", "]", "=", "True", "\n", "newly_finished", ".", "append", "(", "unfin_idx", ")", "\n", "", "", "return", "newly_finished", "\n", "\n", "# Initialize state machine and get first states", "\n", "# get rules from model folder", "\n", "", "self", ".", "state_machine", ".", "reset", "(", "\n", "src_tokens", "[", "new_order", ",", ":", "]", ".", "clone", "(", ")", ".", "detach", "(", ")", ",", "\n", "src_lengths", "[", "new_order", "]", ".", "clone", "(", ")", ".", "detach", "(", ")", ",", "\n", "tokens", ".", "shape", "[", "1", "]", "\n", ")", "\n", "\n", "reorder_state", "=", "None", "\n", "batch_idxs", "=", "None", "\n", "for", "step", "in", "range", "(", "max_len", "+", "1", ")", ":", "# one extra step for EOS marker", "\n", "# reorder decoder internal states based on the prev choice of beams", "\n", "            ", "if", "reorder_state", "is", "not", "None", ":", "\n", "                ", "if", "batch_idxs", "is", "not", "None", ":", "\n", "# update beam indices to take into account removed sentences", "\n", "                    ", "corr", "=", "batch_idxs", "-", "torch", ".", "arange", "(", "batch_idxs", ".", "numel", "(", ")", ")", ".", "type_as", "(", "batch_idxs", ")", "\n", "reorder_state", ".", "view", "(", "-", "1", ",", "beam_size", ")", ".", "add_", "(", "corr", ".", "unsqueeze", "(", "-", "1", ")", "*", "beam_size", ")", "\n", "", "model", ".", "reorder_incremental_state", "(", "reorder_state", ")", "\n", "encoder_outs", "=", "model", ".", "reorder_encoder_out", "(", "encoder_outs", ",", "reorder_state", ")", "\n", "\n", "# reorder state machine", "\n", "self", ".", "state_machine", ".", "reoder_machine", "(", "reorder_state", ")", "\n", "\n", "# Update state machine", "\n", "update_machine", "(", "step", "-", "1", ",", "tokens", ",", "scores", ",", "self", ".", "state_machine", ")", "\n", "\n", "# get active logit indices for this time-step for the entire batch", "\n", "# and masks for each sentence. Gather all variables defining the", "\n", "# parser state", "\n", "", "logits_indices", ",", "logits_mask", "=", "self", ".", "state_machine", ".", "get_active_logits", "(", ")", "\n", "parser_state", "=", "(", "\n", "self", ".", "state_machine", ".", "memory", "[", ":", ",", ":", ",", ":", "step", "+", "1", "]", ".", "clone", "(", ")", ",", "\n", "self", ".", "state_machine", ".", "memory_pos", "[", ":", ",", ":", ",", ":", "step", "+", "1", "]", ".", "clone", "(", ")", ",", "\n", "logits_mask", ",", "\n", "logits_indices", "\n", ")", "\n", "\n", "# call model with pre-computed encoder, previous generated actions", "\n", "# tokens                      (bsz * beam_size, max_len + 2)       long", "\n", "# encoder_out                 (src_len, bsz * beam_size, emb_dim)  float", "\n", "# self.state_machine.memory        (bsz * beam_size, src_len, max_len)  ", "\n", "# self.state_machine.memory_pos    (bsz * beam_size, src_len, max_len)", "\n", "# (tokens) and state machine status", "\n", "lprobs", ",", "avg_attn_scores", "=", "model", ".", "forward_decoder", "(", "\n", "tokens", "[", ":", ",", ":", "step", "+", "1", "]", ",", "encoder_outs", ",", "parser_state", ",", "temperature", "=", "self", ".", "temperature", "\n", ")", "\n", "\n", "# Fix lprobs for labeled SHIFT", "\n", "lprobs", "=", "fix_shift_multi_task", "(", "\n", "lprobs", ",", "\n", "self", ".", "state_machine", ",", "\n", "self", ".", "state_machine", ".", "tgt_dict", ",", "\n", "logits_indices", "\n", ")", "\n", "\n", "# handle min and max length constraints", "\n", "if", "step", ">=", "max_len", ":", "\n", "                ", "lprobs", "[", ":", ",", ":", "self", ".", "eos", "]", "=", "-", "math", ".", "inf", "\n", "lprobs", "[", ":", ",", "self", ".", "eos", "+", "1", ":", "]", "=", "-", "math", ".", "inf", "\n", "# FIXME: Added this to avoid no option, why dont they do this?", "\n", "lprobs", "[", ":", ",", "self", ".", "eos", "]", "=", "0.0", "\n", "", "elif", "step", "<", "self", ".", "min_len", ":", "\n", "                ", "lprobs", "[", ":", ",", "self", ".", "eos", "]", "=", "-", "math", ".", "inf", "\n", "\n", "# handle prefix tokens (possibly with different lengths)", "\n", "", "if", "prefix_tokens", "is", "not", "None", "and", "step", "<", "prefix_tokens", ".", "size", "(", "1", ")", ":", "\n", "                ", "prefix_toks", "=", "prefix_tokens", "[", ":", ",", "step", "]", ".", "unsqueeze", "(", "-", "1", ")", ".", "repeat", "(", "1", ",", "beam_size", ")", ".", "view", "(", "-", "1", ")", "\n", "prefix_lprobs", "=", "lprobs", ".", "gather", "(", "-", "1", ",", "prefix_toks", ".", "unsqueeze", "(", "-", "1", ")", ")", "\n", "prefix_mask", "=", "prefix_toks", ".", "ne", "(", "self", ".", "pad", ")", "\n", "lprobs", "[", "prefix_mask", "]", "=", "-", "math", ".", "inf", "\n", "lprobs", "[", "prefix_mask", "]", "=", "lprobs", "[", "prefix_mask", "]", ".", "scatter_", "(", "\n", "-", "1", ",", "prefix_toks", "[", "prefix_mask", "]", ".", "unsqueeze", "(", "-", "1", ")", ",", "prefix_lprobs", "\n", ")", "\n", "# if prefix includes eos, then we should make sure tokens and", "\n", "# scores are the same across all beams", "\n", "eos_mask", "=", "prefix_toks", ".", "eq", "(", "self", ".", "eos", ")", "\n", "if", "eos_mask", ".", "any", "(", ")", ":", "\n", "# validate that the first beam matches the prefix", "\n", "                    ", "first_beam", "=", "tokens", "[", "eos_mask", "]", ".", "view", "(", "-", "1", ",", "beam_size", ",", "tokens", ".", "size", "(", "-", "1", ")", ")", "[", ":", ",", "0", ",", "1", ":", "step", "+", "1", "]", "\n", "eos_mask_batch_dim", "=", "eos_mask", ".", "view", "(", "-", "1", ",", "beam_size", ")", "[", ":", ",", "0", "]", "\n", "target_prefix", "=", "prefix_tokens", "[", "eos_mask_batch_dim", "]", "[", ":", ",", ":", "step", "]", "\n", "assert", "(", "first_beam", "==", "target_prefix", ")", ".", "all", "(", ")", "\n", "\n", "def", "replicate_first_beam", "(", "tensor", ",", "mask", ")", ":", "\n", "                        ", "tensor", "=", "tensor", ".", "view", "(", "-", "1", ",", "beam_size", ",", "tensor", ".", "size", "(", "-", "1", ")", ")", "\n", "tensor", "[", "mask", "]", "=", "tensor", "[", "mask", "]", "[", ":", ",", ":", "1", ",", ":", "]", "\n", "return", "tensor", ".", "view", "(", "-", "1", ",", "tensor", ".", "size", "(", "-", "1", ")", ")", "\n", "\n", "# copy tokens, scores and lprobs from the first beam to all beams", "\n", "", "tokens", "=", "replicate_first_beam", "(", "tokens", ",", "eos_mask_batch_dim", ")", "\n", "scores", "=", "replicate_first_beam", "(", "scores", ",", "eos_mask_batch_dim", ")", "\n", "lprobs", "=", "replicate_first_beam", "(", "lprobs", ",", "eos_mask_batch_dim", ")", "\n", "\n", "", "", "if", "self", ".", "no_repeat_ngram_size", ">", "0", ":", "\n", "# for each beam and batch sentence, generate a list of previous ngrams", "\n", "                ", "gen_ngrams", "=", "[", "{", "}", "for", "bbsz_idx", "in", "range", "(", "bsz", "*", "beam_size", ")", "]", "\n", "for", "bbsz_idx", "in", "range", "(", "bsz", "*", "beam_size", ")", ":", "\n", "                    ", "gen_tokens", "=", "tokens", "[", "bbsz_idx", "]", ".", "tolist", "(", ")", "\n", "for", "ngram", "in", "zip", "(", "*", "[", "gen_tokens", "[", "i", ":", "]", "for", "i", "in", "range", "(", "self", ".", "no_repeat_ngram_size", ")", "]", ")", ":", "\n", "                        ", "gen_ngrams", "[", "bbsz_idx", "]", "[", "tuple", "(", "ngram", "[", ":", "-", "1", "]", ")", "]", "=", "gen_ngrams", "[", "bbsz_idx", "]", ".", "get", "(", "tuple", "(", "ngram", "[", ":", "-", "1", "]", ")", ",", "[", "]", ")", "+", "[", "ngram", "[", "-", "1", "]", "]", "\n", "\n", "# Record attention scores", "\n", "", "", "", "if", "avg_attn_scores", "is", "not", "None", ":", "\n", "                ", "if", "attn", "is", "None", ":", "\n", "                    ", "attn", "=", "scores", ".", "new", "(", "bsz", "*", "beam_size", ",", "src_tokens", ".", "size", "(", "1", ")", ",", "max_len", "+", "2", ")", "\n", "attn_buf", "=", "attn", ".", "clone", "(", ")", "\n", "nonpad_idxs", "=", "src_tokens", ".", "ne", "(", "self", ".", "pad", ")", "\n", "", "attn", "[", ":", ",", ":", ",", "step", "+", "1", "]", ".", "copy_", "(", "avg_attn_scores", ")", "\n", "\n", "", "scores", "=", "scores", ".", "type_as", "(", "lprobs", ")", "\n", "scores_buf", "=", "scores_buf", ".", "type_as", "(", "lprobs", ")", "\n", "eos_bbsz_idx", "=", "buffer", "(", "'eos_bbsz_idx'", ")", "\n", "eos_scores", "=", "buffer", "(", "'eos_scores'", ",", "type_of", "=", "scores", ")", "\n", "\n", "self", ".", "search", ".", "set_src_lengths", "(", "src_lengths", ")", "\n", "\n", "if", "self", ".", "no_repeat_ngram_size", ">", "0", ":", "\n", "                ", "def", "calculate_banned_tokens", "(", "bbsz_idx", ")", ":", "\n", "# before decoding the next token, prevent decoding of ngrams that have already appeared", "\n", "                    ", "ngram_index", "=", "tuple", "(", "tokens", "[", "bbsz_idx", ",", "step", "+", "2", "-", "self", ".", "no_repeat_ngram_size", ":", "step", "+", "1", "]", ".", "tolist", "(", ")", ")", "\n", "return", "gen_ngrams", "[", "bbsz_idx", "]", ".", "get", "(", "ngram_index", ",", "[", "]", ")", "\n", "\n", "", "if", "step", "+", "2", "-", "self", ".", "no_repeat_ngram_size", ">=", "0", ":", "\n", "# no banned tokens if we haven't generated no_repeat_ngram_size tokens yet", "\n", "                    ", "banned_tokens", "=", "[", "calculate_banned_tokens", "(", "bbsz_idx", ")", "for", "bbsz_idx", "in", "range", "(", "bsz", "*", "beam_size", ")", "]", "\n", "", "else", ":", "\n", "                    ", "banned_tokens", "=", "[", "[", "]", "for", "bbsz_idx", "in", "range", "(", "bsz", "*", "beam_size", ")", "]", "\n", "\n", "", "for", "bbsz_idx", "in", "range", "(", "bsz", "*", "beam_size", ")", ":", "\n", "                    ", "lprobs", "[", "bbsz_idx", ",", "banned_tokens", "[", "bbsz_idx", "]", "]", "=", "-", "math", ".", "inf", "\n", "\n", "# Beam search", "\n", "# lprobs  (batch_size, in_beam_size, vocab_size)", "\n", "# ->", "\n", "# cand_scores  (batch_size, output_beam_size)", "\n", "# cand_indices (batch_size, output_beam_size)", "\n", "# cand_beams   (batch_size, output_beam_size)", "\n", "", "", "cand_scores", ",", "cand_indices", ",", "cand_beams", "=", "self", ".", "search", ".", "step", "(", "\n", "step", ",", "\n", "lprobs", ".", "view", "(", "bsz", ",", "-", "1", ",", "self", ".", "vocab_size", ")", ",", "\n", "scores", ".", "view", "(", "bsz", ",", "beam_size", ",", "-", "1", ")", "[", ":", ",", ":", ",", ":", "step", "]", ",", "\n", ")", "\n", "\n", "# cand_bbsz_idx contains beam indices for the top candidate", "\n", "# hypotheses, with a range of values: [0, bsz*beam_size),", "\n", "# and dimensions: [bsz, cand_size]", "\n", "cand_bbsz_idx", "=", "cand_beams", ".", "add", "(", "bbsz_offsets", ")", "\n", "\n", "# finalize hypotheses that end in eos (except for blacklisted ones)", "\n", "eos_mask", "=", "cand_indices", ".", "eq", "(", "self", ".", "eos", ")", "\n", "eos_mask", "[", ":", ",", ":", "beam_size", "]", "[", "blacklist", "]", "=", "0", "\n", "\n", "# only consider eos when it's among the top beam_size indices", "\n", "torch", ".", "masked_select", "(", "\n", "cand_bbsz_idx", "[", ":", ",", ":", "beam_size", "]", ",", "\n", "mask", "=", "eos_mask", "[", ":", ",", ":", "beam_size", "]", ",", "\n", "out", "=", "eos_bbsz_idx", ",", "\n", ")", "\n", "\n", "finalized_sents", "=", "set", "(", ")", "\n", "if", "eos_bbsz_idx", ".", "numel", "(", ")", ">", "0", ":", "\n", "                ", "torch", ".", "masked_select", "(", "\n", "cand_scores", "[", ":", ",", ":", "beam_size", "]", ",", "\n", "mask", "=", "eos_mask", "[", ":", ",", ":", "beam_size", "]", ",", "\n", "out", "=", "eos_scores", ",", "\n", ")", "\n", "finalized_sents", "=", "finalize_hypos", "(", "step", ",", "eos_bbsz_idx", ",", "eos_scores", ")", "\n", "num_remaining_sent", "-=", "len", "(", "finalized_sents", ")", "\n", "\n", "", "assert", "num_remaining_sent", ">=", "0", "\n", "if", "num_remaining_sent", "==", "0", ":", "\n", "                ", "break", "\n", "#print(step,max_len)", "\n", "", "assert", "step", "<", "max_len", "\n", "\n", "if", "len", "(", "finalized_sents", ")", ">", "0", ":", "\n", "                ", "new_bsz", "=", "bsz", "-", "len", "(", "finalized_sents", ")", "\n", "\n", "# construct batch_idxs which holds indices of batches to keep for the next pass", "\n", "batch_mask", "=", "cand_indices", ".", "new_ones", "(", "bsz", ")", "\n", "batch_mask", "[", "cand_indices", ".", "new", "(", "finalized_sents", ")", "]", "=", "0", "\n", "batch_idxs", "=", "batch_mask", ".", "nonzero", "(", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "\n", "eos_mask", "=", "eos_mask", "[", "batch_idxs", "]", "\n", "cand_beams", "=", "cand_beams", "[", "batch_idxs", "]", "\n", "bbsz_offsets", ".", "resize_", "(", "new_bsz", ",", "1", ")", "\n", "cand_bbsz_idx", "=", "cand_beams", ".", "add", "(", "bbsz_offsets", ")", "\n", "cand_scores", "=", "cand_scores", "[", "batch_idxs", "]", "\n", "cand_indices", "=", "cand_indices", "[", "batch_idxs", "]", "\n", "if", "prefix_tokens", "is", "not", "None", ":", "\n", "                    ", "prefix_tokens", "=", "prefix_tokens", "[", "batch_idxs", "]", "\n", "", "src_lengths", "=", "src_lengths", "[", "batch_idxs", "]", "\n", "blacklist", "=", "blacklist", "[", "batch_idxs", "]", "\n", "\n", "scores", "=", "scores", ".", "view", "(", "bsz", ",", "-", "1", ")", "[", "batch_idxs", "]", ".", "view", "(", "new_bsz", "*", "beam_size", ",", "-", "1", ")", "\n", "scores_buf", ".", "resize_as_", "(", "scores", ")", "\n", "tokens", "=", "tokens", ".", "view", "(", "bsz", ",", "-", "1", ")", "[", "batch_idxs", "]", ".", "view", "(", "new_bsz", "*", "beam_size", ",", "-", "1", ")", "\n", "tokens_buf", ".", "resize_as_", "(", "tokens", ")", "\n", "if", "attn", "is", "not", "None", ":", "\n", "                    ", "attn", "=", "attn", ".", "view", "(", "bsz", ",", "-", "1", ")", "[", "batch_idxs", "]", ".", "view", "(", "new_bsz", "*", "beam_size", ",", "attn", ".", "size", "(", "1", ")", ",", "-", "1", ")", "\n", "attn_buf", ".", "resize_as_", "(", "attn", ")", "\n", "", "bsz", "=", "new_bsz", "\n", "", "else", ":", "\n", "                ", "batch_idxs", "=", "None", "\n", "\n", "# Set active_mask so that values > cand_size indicate eos or", "\n", "# blacklisted hypos and values < cand_size indicate candidate", "\n", "# active hypos. After this, the min values per row are the top", "\n", "# candidate active hypos.", "\n", "", "active_mask", "=", "buffer", "(", "'active_mask'", ")", "\n", "eos_mask", "[", ":", ",", ":", "beam_size", "]", "|=", "blacklist", "\n", "torch", ".", "add", "(", "\n", "eos_mask", ".", "type_as", "(", "cand_offsets", ")", "*", "cand_size", ",", "\n", "cand_offsets", "[", ":", "eos_mask", ".", "size", "(", "1", ")", "]", ",", "\n", "out", "=", "active_mask", ",", "\n", ")", "\n", "\n", "# get the top beam_size active hypotheses, which are just the hypos", "\n", "# with the smallest values in active_mask", "\n", "active_hypos", ",", "new_blacklist", "=", "buffer", "(", "'active_hypos'", ")", ",", "buffer", "(", "'new_blacklist'", ")", "\n", "torch", ".", "topk", "(", "\n", "active_mask", ",", "k", "=", "beam_size", ",", "dim", "=", "1", ",", "largest", "=", "False", ",", "\n", "out", "=", "(", "new_blacklist", ",", "active_hypos", ")", "\n", ")", "\n", "\n", "# update blacklist to ignore any finalized hypos", "\n", "blacklist", "=", "new_blacklist", ".", "ge", "(", "cand_size", ")", "[", ":", ",", ":", "beam_size", "]", "\n", "assert", "(", "~", "blacklist", ")", ".", "any", "(", "dim", "=", "1", ")", ".", "all", "(", ")", "\n", "\n", "active_bbsz_idx", "=", "buffer", "(", "'active_bbsz_idx'", ")", "\n", "torch", ".", "gather", "(", "\n", "cand_bbsz_idx", ",", "dim", "=", "1", ",", "index", "=", "active_hypos", ",", "\n", "out", "=", "active_bbsz_idx", ",", "\n", ")", "\n", "active_scores", "=", "torch", ".", "gather", "(", "\n", "cand_scores", ",", "dim", "=", "1", ",", "index", "=", "active_hypos", ",", "\n", "out", "=", "scores", "[", ":", ",", "step", "]", ".", "view", "(", "bsz", ",", "beam_size", ")", ",", "\n", ")", "\n", "\n", "active_bbsz_idx", "=", "active_bbsz_idx", ".", "view", "(", "-", "1", ")", "\n", "active_scores", "=", "active_scores", ".", "view", "(", "-", "1", ")", "\n", "\n", "# copy tokens and scores for active hypotheses", "\n", "torch", ".", "index_select", "(", "\n", "tokens", "[", ":", ",", ":", "step", "+", "1", "]", ",", "dim", "=", "0", ",", "index", "=", "active_bbsz_idx", ",", "\n", "out", "=", "tokens_buf", "[", ":", ",", ":", "step", "+", "1", "]", ",", "\n", ")", "\n", "torch", ".", "gather", "(", "\n", "cand_indices", ",", "dim", "=", "1", ",", "index", "=", "active_hypos", ",", "\n", "out", "=", "tokens_buf", ".", "view", "(", "bsz", ",", "beam_size", ",", "-", "1", ")", "[", ":", ",", ":", ",", "step", "+", "1", "]", ",", "\n", ")", "\n", "if", "step", ">", "0", ":", "\n", "                ", "torch", ".", "index_select", "(", "\n", "scores", "[", ":", ",", ":", "step", "]", ",", "dim", "=", "0", ",", "index", "=", "active_bbsz_idx", ",", "\n", "out", "=", "scores_buf", "[", ":", ",", ":", "step", "]", ",", "\n", ")", "\n", "", "torch", ".", "gather", "(", "\n", "cand_scores", ",", "dim", "=", "1", ",", "index", "=", "active_hypos", ",", "\n", "out", "=", "scores_buf", ".", "view", "(", "bsz", ",", "beam_size", ",", "-", "1", ")", "[", ":", ",", ":", ",", "step", "]", ",", "\n", ")", "\n", "\n", "# copy attention for active hypotheses", "\n", "if", "attn", "is", "not", "None", ":", "\n", "                ", "torch", ".", "index_select", "(", "\n", "attn", "[", ":", ",", ":", ",", ":", "step", "+", "2", "]", ",", "dim", "=", "0", ",", "index", "=", "active_bbsz_idx", ",", "\n", "out", "=", "attn_buf", "[", ":", ",", ":", ",", ":", "step", "+", "2", "]", ",", "\n", ")", "\n", "\n", "# swap buffers", "\n", "", "tokens", ",", "tokens_buf", "=", "tokens_buf", ",", "tokens", "\n", "scores", ",", "scores_buf", "=", "scores_buf", ",", "scores", "\n", "if", "attn", "is", "not", "None", ":", "\n", "                ", "attn", ",", "attn_buf", "=", "attn_buf", ",", "attn", "\n", "\n", "# reorder incremental state in decoder", "\n", "", "reorder_state", "=", "active_bbsz_idx", "\n", "\n", "# sort by score descending", "\n", "", "for", "sent", "in", "range", "(", "len", "(", "finalized", ")", ")", ":", "\n", "            ", "finalized", "[", "sent", "]", "=", "sorted", "(", "finalized", "[", "sent", "]", ",", "key", "=", "lambda", "r", ":", "r", "[", "'score'", "]", ",", "reverse", "=", "True", ")", "\n", "\n", "", "return", "finalized", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.sequence_generator.EnsembleModel.__init__": [[574, 580], ["super().__init__", "torch.nn.ModuleList", "all", "isinstance"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["def", "__init__", "(", "self", ",", "models", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "models", "=", "torch", ".", "nn", ".", "ModuleList", "(", "models", ")", "\n", "self", ".", "incremental_states", "=", "None", "\n", "if", "all", "(", "isinstance", "(", "m", ".", "decoder", ",", "FairseqIncrementalDecoder", ")", "for", "m", "in", "models", ")", ":", "\n", "            ", "self", ".", "incremental_states", "=", "{", "m", ":", "{", "}", "for", "m", "in", "models", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.sequence_generator.EnsembleModel.has_encoder": [[581, 583], ["hasattr"], "methods", ["None"], ["", "", "def", "has_encoder", "(", "self", ")", ":", "\n", "        ", "return", "hasattr", "(", "self", ".", "models", "[", "0", "]", ",", "'encoder'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.sequence_generator.EnsembleModel.max_decoder_positions": [[584, 586], ["min", "m.max_decoder_positions"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.sequence_generator.EnsembleModel.max_decoder_positions"], ["", "def", "max_decoder_positions", "(", "self", ")", ":", "\n", "        ", "return", "min", "(", "m", ".", "max_decoder_positions", "(", ")", "for", "m", "in", "self", ".", "models", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.sequence_generator.EnsembleModel.forward_encoder": [[587, 592], ["torch.no_grad", "sequence_generator.EnsembleModel.has_encoder", "model.encoder"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.sequence_generator.EnsembleModel.has_encoder", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.FairseqMultiModel.encoder"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "forward_encoder", "(", "self", ",", "encoder_input", ")", ":", "\n", "        ", "if", "not", "self", ".", "has_encoder", "(", ")", ":", "\n", "            ", "return", "None", "\n", "", "return", "[", "model", ".", "encoder", "(", "**", "encoder_input", ")", "for", "model", "in", "self", ".", "models", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.sequence_generator.EnsembleModel.forward_decoder": [[593, 628], ["torch.no_grad", "zip", "len", "sequence_generator.EnsembleModel._decode_one", "sequence_generator.EnsembleModel._decode_one", "log_probs.append", "torch.logsumexp", "math.log", "avg_attn.div_", "torch.stack", "len", "len", "sequence_generator.EnsembleModel.has_encoder", "avg_attn.add_"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.sequence_generator.EnsembleModel._decode_one", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.sequence_generator.EnsembleModel._decode_one", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.sequence_generator.EnsembleModel.has_encoder"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "forward_decoder", "(", "self", ",", "tokens", ",", "encoder_outs", ",", "parser_state", ",", "temperature", "=", "1", ")", ":", "\n", "        ", "if", "len", "(", "self", ".", "models", ")", "==", "1", ":", "\n", "            ", "return", "self", ".", "_decode_one", "(", "\n", "tokens", ",", "\n", "self", ".", "models", "[", "0", "]", ",", "\n", "encoder_outs", "[", "0", "]", "if", "self", ".", "has_encoder", "(", ")", "else", "None", ",", "\n", "parser_state", ",", "\n", "self", ".", "incremental_states", ",", "\n", "log_probs", "=", "True", ",", "\n", "temperature", "=", "temperature", ",", "\n", ")", "\n", "\n", "", "log_probs", "=", "[", "]", "\n", "avg_attn", "=", "None", "\n", "for", "model", ",", "encoder_out", "in", "zip", "(", "self", ".", "models", ",", "encoder_outs", ")", ":", "\n", "            ", "probs", ",", "attn", "=", "self", ".", "_decode_one", "(", "\n", "tokens", ",", "\n", "model", ",", "\n", "encoder_out", ",", "\n", "parser_state", ",", "\n", "self", ".", "incremental_states", ",", "\n", "log_probs", "=", "True", ",", "\n", "temperature", "=", "temperature", "\n", ")", "\n", "log_probs", ".", "append", "(", "probs", ")", "\n", "if", "attn", "is", "not", "None", ":", "\n", "                ", "if", "avg_attn", "is", "None", ":", "\n", "                    ", "avg_attn", "=", "attn", "\n", "", "else", ":", "\n", "                    ", "avg_attn", ".", "add_", "(", "attn", ")", "\n", "", "", "", "avg_probs", "=", "torch", ".", "logsumexp", "(", "torch", ".", "stack", "(", "log_probs", ",", "dim", "=", "0", ")", ",", "dim", "=", "0", ")", "-", "math", ".", "log", "(", "len", "(", "self", ".", "models", ")", ")", "\n", "if", "avg_attn", "is", "not", "None", ":", "\n", "            ", "avg_attn", ".", "div_", "(", "len", "(", "self", ".", "models", ")", ")", "\n", "", "return", "avg_probs", ",", "avg_attn", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.sequence_generator.EnsembleModel._decode_one": [[629, 661], ["model.get_normalized_probs", "list", "NotImplementedError", "list", "decoder_out[].div_", "type", "attn.get.get.get", "model.decoder", "model.decoder"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_decoder.FairseqDecoder.get_normalized_probs", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.FairseqMultiModel.decoder", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.fairseq_model.FairseqMultiModel.decoder"], ["", "def", "_decode_one", "(", "\n", "self", ",", "tokens", ",", "model", ",", "encoder_out", ",", "parser_state", ",", "incremental_states", ",", "\n", "log_probs", ",", "temperature", "=", "1.", ",", "\n", ")", ":", "\n", "        ", "if", "self", ".", "incremental_states", "is", "not", "None", ":", "\n", "\n", "# unpack state", "\n", "            ", "memory", ",", "memory_pos", ",", "logits_mask", ",", "logits_indices", "=", "parser_state", "\n", "\n", "decoder_out", "=", "list", "(", "model", ".", "decoder", "(", "\n", "tokens", ",", "\n", "encoder_out", ",", "\n", "memory", ",", "\n", "memory_pos", ",", "\n", "incremental_state", "=", "self", ".", "incremental_states", "[", "model", "]", ",", "\n", "logits_mask", "=", "logits_mask", ",", "\n", "logits_indices", "=", "logits_indices", "\n", ")", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", ")", "\n", "decoder_out", "=", "list", "(", "model", ".", "decoder", "(", "tokens", ",", "encoder_out", ")", ")", "\n", "", "decoder_out", "[", "0", "]", "=", "decoder_out", "[", "0", "]", "[", ":", ",", "-", "1", ":", ",", ":", "]", "\n", "if", "temperature", "!=", "1.", ":", "\n", "            ", "decoder_out", "[", "0", "]", ".", "div_", "(", "temperature", ")", "\n", "", "attn", "=", "decoder_out", "[", "1", "]", "\n", "if", "type", "(", "attn", ")", "is", "dict", ":", "\n", "            ", "attn", "=", "attn", ".", "get", "(", "'attn'", ",", "None", ")", "\n", "", "if", "attn", "is", "not", "None", ":", "\n", "            ", "attn", "=", "attn", "[", ":", ",", "-", "1", ",", ":", "]", "\n", "", "probs", "=", "model", ".", "get_normalized_probs", "(", "decoder_out", ",", "log_probs", "=", "log_probs", ")", "\n", "probs", "=", "probs", "[", ":", ",", "-", "1", ",", ":", "]", "\n", "return", "probs", ",", "attn", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.sequence_generator.EnsembleModel.reorder_encoder_out": [[662, 668], ["sequence_generator.EnsembleModel.has_encoder", "model.encoder.reorder_encoder_out", "zip"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.sequence_generator.EnsembleModel.has_encoder", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.sequence_generator.EnsembleModel.reorder_encoder_out"], ["", "def", "reorder_encoder_out", "(", "self", ",", "encoder_outs", ",", "new_order", ")", ":", "\n", "        ", "if", "not", "self", ".", "has_encoder", "(", ")", ":", "\n", "            ", "return", "\n", "", "return", "[", "\n", "model", ".", "encoder", ".", "reorder_encoder_out", "(", "encoder_out", ",", "new_order", ")", "\n", "for", "model", ",", "encoder_out", "in", "zip", "(", "self", ".", "models", ",", "encoder_outs", ")", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.sequence_generator.EnsembleModel.reorder_incremental_state": [[670, 675], ["model.decoder.reorder_incremental_state"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.linearized_convolution.LinearizedConvolution.reorder_incremental_state"], ["", "def", "reorder_incremental_state", "(", "self", ",", "new_order", ")", ":", "\n", "        ", "if", "self", ".", "incremental_states", "is", "None", ":", "\n", "            ", "return", "\n", "", "for", "model", "in", "self", ".", "models", ":", "\n", "            ", "model", ".", "decoder", ".", "reorder_incremental_state", "(", "self", ".", "incremental_states", "[", "model", "]", ",", "new_order", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.load_ensemble_for_inference": [[24, 32], ["utils.deprecation_warning", "checkpoint_utils.load_model_ensemble"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.deprecation_warning", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.checkpoint_utils.load_model_ensemble"], ["d", ".", "add_symbol", "(", "token", ")", "\n", "", "d", ".", "finalize", "(", "padding_factor", "=", "1", ")", "# don't add extra padding symbols", "\n", "return", "d", "\n", "\n", "\n", "", "def", "dummy_dataloader", "(", "\n", "samples", ",", "\n", "padding_idx", "=", "1", ",", "\n", "eos_idx", "=", "2", ",", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.apply_to_sample": [[35, 53], ["utils.apply_to_sample._apply"], "function", ["None"], ["    ", "if", "batch_size", "is", "None", ":", "\n", "        ", "batch_size", "=", "len", "(", "samples", ")", "\n", "\n", "# add any missing data to samples", "\n", "", "for", "i", ",", "sample", "in", "enumerate", "(", "samples", ")", ":", "\n", "        ", "if", "'id'", "not", "in", "sample", ":", "\n", "            ", "sample", "[", "'id'", "]", "=", "i", "\n", "\n", "# create dataloader", "\n", "", "", "dataset", "=", "TestDataset", "(", "samples", ")", "\n", "dataloader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "dataset", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "collate_fn", "=", "(", "lambda", "samples", ":", "collate", "(", "samples", ",", "padding_idx", ",", "eos_idx", ")", ")", ",", "\n", ")", "\n", "return", "iter", "(", "dataloader", ")", "\n", "\n", "\n", "", "def", "sequence_generator_setup", "(", ")", ":", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.move_to_cuda": [[55, 61], ["utils.apply_to_sample", "tensor.cuda"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.apply_to_sample"], ["    ", "d", "=", "dummy_dictionary", "(", "vocab_size", "=", "2", ")", "\n", "\n", "eos", "=", "d", ".", "eos", "(", ")", "\n", "w1", "=", "4", "\n", "w2", "=", "5", "\n", "\n", "# construct source data", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils._get_full_incremental_state_key": [[66, 76], ["hasattr"], "function", ["None"], ["unk", "=", "0.", "\n", "args", ".", "beam_probs", "=", "[", "\n", "# step 0:", "\n", "torch", ".", "FloatTensor", "(", "[", "\n", "# eos      w1   w2", "\n", "# sentence 1:", "\n", "[", "0.0", ",", "unk", ",", "0.9", ",", "0.1", "]", ",", "# beam 1", "\n", "[", "0.0", ",", "unk", ",", "0.9", ",", "0.1", "]", ",", "# beam 2", "\n", "# sentence 2:", "\n", "[", "0.0", ",", "unk", ",", "0.7", ",", "0.3", "]", ",", "\n", "[", "0.0", ",", "unk", ",", "0.7", ",", "0.3", "]", ",", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.get_incremental_state": [[78, 84], ["utils._get_full_incremental_state_key"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils._get_full_incremental_state_key"], ["# step 1:", "\n", "torch", ".", "FloatTensor", "(", "[", "\n", "# eos      w1   w2       prefix", "\n", "# sentence 1:", "\n", "[", "1.0", ",", "unk", ",", "0.0", ",", "0.0", "]", ",", "# w1: 0.9  (emit: w1 <eos>: 0.9*1.0)", "\n", "[", "0.0", ",", "unk", ",", "0.9", ",", "0.1", "]", ",", "# w2: 0.1", "\n", "# sentence 2:", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.set_incremental_state": [[86, 91], ["utils._get_full_incremental_state_key"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils._get_full_incremental_state_key"], ["[", "0.00", ",", "unk", ",", "0.10", ",", "0.9", "]", ",", "# w2: 0.3", "\n", "]", ")", ",", "\n", "# step 2:", "\n", "torch", ".", "FloatTensor", "(", "[", "\n", "# eos      w1   w2       prefix", "\n", "# sentence 1:", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.load_align_dict": [[93, 108], ["isinstance", "len", "open", "line.split"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["[", "0.6", ",", "unk", ",", "0.2", ",", "0.2", "]", ",", "# w2 w2: 0.1*0.1  (emit: w2 w2 <eos>: 0.1*0.1*0.6)", "\n", "# sentence 2:", "\n", "[", "0.60", ",", "unk", ",", "0.4", ",", "0.00", "]", ",", "# w1 w2: 0.7*0.4  (emit: w1 w2 <eos>: 0.7*0.4*0.6)", "\n", "[", "0.01", ",", "unk", ",", "0.0", ",", "0.99", "]", ",", "# w2 w2: 0.3*0.9", "\n", "]", ")", ",", "\n", "# step 3:", "\n", "torch", ".", "FloatTensor", "(", "[", "\n", "# eos      w1   w2       prefix", "\n", "# sentence 1:", "\n", "[", "1.0", ",", "unk", ",", "0.0", ",", "0.0", "]", ",", "# w2 w1 w2: 0.1*0.9*0.9  (emit: w2 w1 w2 <eos>: 0.1*0.9*0.9*1.0)", "\n", "[", "1.0", ",", "unk", ",", "0.0", ",", "0.0", "]", ",", "# w2 w1 w1: 0.1*0.9*0.1  (emit: w2 w1 w1 <eos>: 0.1*0.9*0.1*1.0)", "\n", "# sentence 2:", "\n", "[", "0.1", ",", "unk", ",", "0.5", ",", "0.4", "]", ",", "# w2 w2 w2: 0.3*0.9*0.99  (emit: w2 w2 w2 <eos>: 0.3*0.9*0.99*0.1)", "\n", "[", "1.0", ",", "unk", ",", "0.0", ",", "0.0", "]", ",", "# w1 w2 w1: 0.7*0.4*0.4  (emit: w1 w2 w1 <eos>: 0.7*0.4*0.4*1.0)", "\n", "]", ")", ",", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.print_embed_overlap": [[110, 115], ["set", "set", "len", "print", "embed_dict.keys", "len"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["task", "=", "TestTranslationTask", ".", "setup_task", "(", "args", ",", "d", ",", "d", ")", "\n", "model", "=", "task", ".", "build_model", "(", "args", ")", "\n", "tgt_dict", "=", "task", ".", "target_dictionary", "\n", "\n", "return", "tgt_dict", ",", "w1", ",", "w2", ",", "src_tokens", ",", "src_lengths", ",", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.parse_embedding": [[117, 135], ["open", "next", "line.rstrip().split", "torch.Tensor", "torch.Tensor", "line.rstrip", "float"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["", "class", "TestDataset", "(", "torch", ".", "utils", ".", "data", ".", "Dataset", ")", ":", "\n", "\n", "    ", "def", "__init__", "(", "self", ",", "data", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "data", "=", "data", "\n", "self", ".", "sizes", "=", "None", "\n", "\n", "", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "data", "[", "index", "]", "\n", "\n", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "data", ")", "\n", "\n", "\n", "", "", "class", "TestTranslationTask", "(", "FairseqTask", ")", ":", "\n", "\n", "    ", "def", "__init__", "(", "self", ",", "args", ",", "src_dict", ",", "tgt_dict", ",", "model", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ")", "\n", "self", ".", "src_dict", "=", "src_dict", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.load_embedding": [[137, 143], ["range", "len"], "function", ["None"], ["self", ".", "model", "=", "model", "\n", "\n", "", "@", "classmethod", "\n", "def", "setup_task", "(", "cls", ",", "args", ",", "src_dict", "=", "None", ",", "tgt_dict", "=", "None", ",", "model", "=", "None", ")", ":", "\n", "        ", "return", "cls", "(", "args", ",", "src_dict", ",", "tgt_dict", ",", "model", ")", "\n", "\n", "", "def", "build_model", "(", "self", ",", "args", ")", ":", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.replace_unk": [[145, 156], ["fairseq.tokenizer.tokenize_line", "enumerate", "tokenizer.tokenize_line", "align_dict.get"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.tokenizer.tokenize_line", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.tokenizer.tokenize_line"], ["\n", "", "@", "property", "\n", "def", "source_dictionary", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "src_dict", "\n", "\n", "", "@", "property", "\n", "def", "target_dictionary", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "tgt_dict", "\n", "\n", "\n", "", "", "class", "TestModel", "(", "FairseqEncoderDecoderModel", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "encoder", ",", "decoder", ")", ":", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.post_process_prediction": [[158, 168], ["tgt_dict.string", "utils.replace_unk", "tgt_dict.encode_line", "tgt_dict.unk_string"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.hub_utils.GeneratorHubInterface.string", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.replace_unk", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.encode_line", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.unk_string"], ["\n", "", "@", "classmethod", "\n", "def", "build_model", "(", "cls", ",", "args", ",", "task", ")", ":", "\n", "        ", "encoder", "=", "TestEncoder", "(", "args", ",", "task", ".", "source_dictionary", ")", "\n", "decoder", "=", "TestIncrementalDecoder", "(", "args", ",", "task", ".", "target_dictionary", ")", "\n", "return", "cls", "(", "encoder", ",", "decoder", ")", "\n", "\n", "\n", "", "", "class", "TestEncoder", "(", "FairseqEncoder", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "args", ",", "dictionary", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.make_positions": [[170, 183], ["tensor.ne().int", "tensor.ne", "torch.cumsum().type_as", "torch.cumsum().type_as", "torch.cumsum", "torch.cumsum"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.concat_dataset.ConcatDataset.cumsum", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.concat_dataset.ConcatDataset.cumsum"], ["\n", "", "def", "forward", "(", "self", ",", "src_tokens", ",", "src_lengths", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "src_tokens", "\n", "\n", "", "def", "reorder_encoder_out", "(", "self", ",", "encoder_out", ",", "new_order", ")", ":", "\n", "        ", "return", "encoder_out", ".", "index_select", "(", "0", ",", "new_order", ")", "\n", "\n", "\n", "", "", "class", "TestIncrementalDecoder", "(", "FairseqIncrementalDecoder", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "args", ",", "dictionary", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ")", "\n", "assert", "hasattr", "(", "args", ",", "'beam_probs'", ")", "or", "hasattr", "(", "args", ",", "'probs'", ")", "\n", "args", ".", "max_decoder_positions", "=", "getattr", "(", "args", ",", "'max_decoder_positions'", ",", "100", ")", "\n", "self", ".", "args", "=", "args", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.strip_pad": [[185, 187], ["tensor.ne"], "function", ["None"], ["", "def", "forward", "(", "self", ",", "prev_output_tokens", ",", "encoder_out", "=", "None", ",", "incremental_state", "=", "None", ")", ":", "\n", "        ", "if", "incremental_state", "is", "not", "None", ":", "\n", "            ", "prev_output_tokens", "=", "prev_output_tokens", "[", ":", ",", "-", "1", ":", "]", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.buffered_arange": [[189, 195], ["hasattr", "torch.LongTensor", "torch.LongTensor", "buffered_arange.buf.numel", "torch.arange", "torch.arange"], "function", ["None"], ["vocab", "=", "len", "(", "self", ".", "dictionary", ")", "\n", "src_len", "=", "encoder_out", ".", "size", "(", "1", ")", "\n", "tgt_len", "=", "prev_output_tokens", ".", "size", "(", "1", ")", "\n", "\n", "# determine number of steps", "\n", "if", "incremental_state", "is", "not", "None", ":", "\n", "# cache step number", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.convert_padding_direction": [[197, 217], ["src_tokens.eq", "src_tokens.size", "buffered_arange().type_as().expand_as", "src_tokens.eq.long().sum", "src_tokens.gather", "src_tokens.eq.any", "torch.remainder", "torch.remainder", "torch.remainder", "torch.remainder", "pad_mask[].any", "pad_mask[].any", "buffered_arange().type_as", "src_tokens.eq.long", "utils.buffered_arange"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.buffered_arange"], ["if", "step", "is", "None", ":", "\n", "                ", "step", "=", "0", "\n", "", "utils", ".", "set_incremental_state", "(", "self", ",", "incremental_state", ",", "'step'", ",", "step", "+", "1", ")", "\n", "steps", "=", "[", "step", "]", "\n", "", "else", ":", "\n", "            ", "steps", "=", "list", "(", "range", "(", "tgt_len", ")", ")", "\n", "\n", "# define output in terms of raw probs", "\n", "", "if", "hasattr", "(", "self", ".", "args", ",", "'probs'", ")", ":", "\n", "            ", "assert", "self", ".", "args", ".", "probs", ".", "dim", "(", ")", "==", "3", ",", "'expected probs to have size bsz*steps*vocab'", "\n", "probs", "=", "self", ".", "args", ".", "probs", ".", "index_select", "(", "1", ",", "torch", ".", "LongTensor", "(", "steps", ")", ")", "\n", "", "else", ":", "\n", "            ", "probs", "=", "torch", ".", "FloatTensor", "(", "bbsz", ",", "len", "(", "steps", ")", ",", "vocab", ")", ".", "zero_", "(", ")", "\n", "for", "i", ",", "step", "in", "enumerate", "(", "steps", ")", ":", "\n", "# args.beam_probs gives the probability for every vocab element,", "\n", "# starting with eos, then unknown, and then the rest of the vocab", "\n", "                ", "if", "step", "<", "len", "(", "self", ".", "args", ".", "beam_probs", ")", ":", "\n", "                    ", "probs", "[", ":", ",", "i", ",", "self", ".", "dictionary", ".", "eos", "(", ")", ":", "]", "=", "self", ".", "args", ".", "beam_probs", "[", "step", "]", "\n", "", "else", ":", "\n", "                    ", "probs", "[", ":", ",", "i", ",", "self", ".", "dictionary", ".", "eos", "(", ")", "]", "=", "1.0", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item": [[219, 225], ["hasattr", "hasattr", "tensor.item"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item"], ["# random attention", "\n", "", "", "", "attn", "=", "torch", ".", "rand", "(", "bbsz", ",", "tgt_len", ",", "src_len", ")", "\n", "\n", "dev", "=", "prev_output_tokens", ".", "device", "\n", "return", "probs", ".", "to", "(", "dev", ")", ",", "attn", ".", "to", "(", "dev", ")", "\n", "\n", "", "def", "get_normalized_probs", "(", "self", ",", "net_output", ",", "log_probs", ",", "_", ")", ":", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.clip_grad_norm_": [[227, 233], ["utils.item", "torch.norm", "torch.norm", "tensor.mul_"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item"], ["        ", "probs", "=", "net_output", "[", "0", "]", "\n", "if", "log_probs", ":", "\n", "            ", "return", "probs", ".", "log", "(", ")", "\n", "", "else", ":", "\n", "            ", "return", "probs", "\n", "\n", "", "", "def", "max_positions", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.fill_with_neg_inf": [[235, 238], ["t.float().fill_().type_as", "t.float().fill_", "float", "t.float"], "function", ["None"], ["", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.resolve_max_positions": [[240, 276], ["copy.deepcopy", "min", "isinstance", "isinstance", "min", "isinstance", "utils.resolve_max_positions.map_value_update"], "function", ["None"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.import_user_module": [[278, 294], ["getattr", "os.path.abspath", "os.path.split", "os.path.exists", "os.path.join", "os.path.exists", "sys.path.insert", "importlib.import_module", "sys.path.pop", "os.path.dirname"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax": [[296, 301], ["torch.softmax", "torch.softmax", "x.float"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.log_softmax": [[303, 308], ["torch.log_softmax", "torch.log_softmax", "x.float"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.log_softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.log_softmax"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.get_perplexity": [[310, 315], ["math.pow", "float"], "function", ["None"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.deprecation_warning": [[317, 320], ["warnings.warn"], "function", ["None"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.get_activation_fn": [[322, 339], ["utils.deprecation_warning", "RuntimeError"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.deprecation_warning"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.get_available_activation_fns": [[341, 349], ["None"], "function", ["None"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval": [[352, 358], ["model.eval", "model.train"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq-stack-transformer.train.train"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.adamax.FairseqAdamax.__init__": [[14, 17], ["FairseqOptimizer.__init__", "adamax.Adamax"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "params", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "params", ")", "\n", "self", ".", "_optimizer", "=", "Adamax", "(", "params", ",", "**", "self", ".", "optimizer_config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.adamax.FairseqAdamax.add_args": [[18, 30], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add optimizer-specific arguments to the parser.\"\"\"", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'--adamax-betas'", ",", "default", "=", "'(0.9, 0.999)'", ",", "metavar", "=", "'B'", ",", "\n", "help", "=", "'betas for Adam optimizer'", ")", "\n", "parser", ".", "add_argument", "(", "'--adamax-eps'", ",", "type", "=", "float", ",", "default", "=", "1e-8", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'epsilon for Adam optimizer'", ")", "\n", "parser", ".", "add_argument", "(", "'--weight-decay'", ",", "'--wd'", ",", "default", "=", "0.0", ",", "type", "=", "float", ",", "metavar", "=", "'WD'", ",", "\n", "help", "=", "'weight decay'", ")", "\n", "parser", ".", "add_argument", "(", "'--no-bias-correction'", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'disable bias correction'", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.adamax.FairseqAdamax.optimizer_config": [[32, 46], ["eval"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval"], ["", "@", "property", "\n", "def", "optimizer_config", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Return a kwarg dictionary that will be used to override optimizer\n        args stored in checkpoints. This allows us to load a checkpoint and\n        resume training using a different set of optimizer args, e.g., with a\n        different learning rate.\n        \"\"\"", "\n", "return", "{", "\n", "'lr'", ":", "self", ".", "args", ".", "lr", "[", "0", "]", ",", "\n", "'betas'", ":", "eval", "(", "self", ".", "args", ".", "adamax_betas", ")", ",", "\n", "'eps'", ":", "self", ".", "args", ".", "adamax_eps", ",", "\n", "'weight_decay'", ":", "self", ".", "args", ".", "weight_decay", ",", "\n", "'bias_correction'", ":", "not", "self", ".", "args", ".", "no_bias_correction", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.adamax.Adamax.__init__": [[70, 86], ["dict", "super().__init__", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "2e-3", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "eps", "=", "1e-8", ",", "\n", "weight_decay", "=", "0", ",", "bias_correction", "=", "True", ")", ":", "\n", "        ", "if", "not", "0.0", "<=", "lr", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid learning rate: {}\"", ".", "format", "(", "lr", ")", ")", "\n", "", "if", "not", "0.0", "<=", "eps", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid epsilon value: {}\"", ".", "format", "(", "eps", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "0", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter at index 0: {}\"", ".", "format", "(", "betas", "[", "0", "]", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "1", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter at index 1: {}\"", ".", "format", "(", "betas", "[", "1", "]", ")", ")", "\n", "", "if", "not", "0.0", "<=", "weight_decay", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid weight_decay value: {}\"", ".", "format", "(", "weight_decay", ")", ")", "\n", "\n", "", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "betas", "=", "betas", ",", "eps", "=", "eps", ",", "weight_decay", "=", "weight_decay", ",", "\n", "bias_correction", "=", "bias_correction", ")", "\n", "super", "(", "Adamax", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.adamax.Adamax.supports_memory_efficient_fp16": [[87, 90], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "supports_memory_efficient_fp16", "(", "self", ")", ":", "\n", "        ", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.adamax.Adamax.step": [[91, 152], ["closure", "p.grad.data.float", "p.data.float", "exp_avg.mul_().add_", "torch.max", "torch.max", "torch.max", "torch.max", "p.data.float.addcdiv_", "p.data.copy_", "RuntimeError", "len", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "state[].type_as", "state[].type_as", "exp_inf.mul_", "p.grad.data.float.abs_", "p.data.float.add_", "exp_inf.add", "exp_avg.mul_"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.add"], ["", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", ".", "data", ".", "float", "(", ")", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "'Adamax does not support sparse gradients'", ")", "\n", "\n", "", "p_data_fp32", "=", "p", ".", "data", ".", "float", "(", ")", "\n", "\n", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "state", "[", "'exp_avg'", "]", "=", "torch", ".", "zeros_like", "(", "p_data_fp32", ")", "\n", "state", "[", "'exp_inf'", "]", "=", "torch", ".", "zeros_like", "(", "p_data_fp32", ")", "\n", "", "else", ":", "\n", "                    ", "state", "[", "'exp_avg'", "]", "=", "state", "[", "'exp_avg'", "]", ".", "type_as", "(", "p_data_fp32", ")", "\n", "state", "[", "'exp_inf'", "]", "=", "state", "[", "'exp_inf'", "]", ".", "type_as", "(", "p_data_fp32", ")", "\n", "\n", "", "exp_avg", ",", "exp_inf", "=", "state", "[", "'exp_avg'", "]", ",", "state", "[", "'exp_inf'", "]", "\n", "beta1", ",", "beta2", "=", "group", "[", "'betas'", "]", "\n", "eps", "=", "group", "[", "'eps'", "]", "\n", "\n", "state", "[", "'step'", "]", "+=", "1", "\n", "\n", "# Update biased first moment estimate.", "\n", "exp_avg", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "1", "-", "beta1", ",", "grad", ")", "\n", "\n", "# Update the exponentially weighted infinity norm.", "\n", "torch", ".", "max", "(", "\n", "exp_inf", ".", "mul_", "(", "beta2", ")", ",", "\n", "grad", ".", "abs_", "(", ")", ",", "\n", "out", "=", "exp_inf", ",", "\n", ")", "\n", "\n", "step_size", "=", "group", "[", "'lr'", "]", "\n", "if", "group", "[", "'bias_correction'", "]", ":", "\n", "                    ", "bias_correction", "=", "1", "-", "beta1", "**", "state", "[", "'step'", "]", "\n", "step_size", "/=", "bias_correction", "\n", "\n", "", "if", "group", "[", "'weight_decay'", "]", "!=", "0", ":", "\n", "                    ", "p_data_fp32", ".", "add_", "(", "-", "group", "[", "'weight_decay'", "]", "*", "group", "[", "'lr'", "]", ",", "p_data_fp32", ")", "\n", "\n", "", "p_data_fp32", ".", "addcdiv_", "(", "-", "step_size", ",", "exp_avg", ",", "exp_inf", ".", "add", "(", "eps", ")", ")", "\n", "\n", "p", ".", "data", ".", "copy_", "(", "p_data_fp32", ")", "\n", "\n", "", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.adagrad.Adagrad.__init__": [[13, 16], ["FairseqOptimizer.__init__", "torch.optim.Adagrad"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "params", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "params", ")", "\n", "self", ".", "_optimizer", "=", "torch", ".", "optim", ".", "Adagrad", "(", "params", ",", "**", "self", ".", "optimizer_config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.adagrad.Adagrad.add_args": [[17, 23], ["parser.add_argument"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add optimizer-specific arguments to the parser.\"\"\"", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'--weight-decay'", ",", "'--wd'", ",", "default", "=", "0.0", ",", "type", "=", "float", ",", "metavar", "=", "'WD'", ",", "\n", "help", "=", "'weight decay'", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.adagrad.Adagrad.optimizer_config": [[25, 36], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "optimizer_config", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Return a kwarg dictionary that will be used to override optimizer\n        args stored in checkpoints. This allows us to load a checkpoint and\n        resume training using a different set of optimizer args, e.g., with a\n        different learning rate.\n        \"\"\"", "\n", "return", "{", "\n", "'lr'", ":", "self", ".", "args", ".", "lr", "[", "0", "]", ",", "\n", "'weight_decay'", ":", "self", ".", "args", ".", "weight_decay", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fairseq_optimizer.FairseqOptimizer.__init__": [[13, 17], ["object.__init__", "list"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "params", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "args", "=", "args", "\n", "self", ".", "params", "=", "list", "(", "params", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fairseq_optimizer.FairseqOptimizer.add_args": [[18, 22], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add optimizer-specific arguments to the parser.\"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fairseq_optimizer.FairseqOptimizer.optimizer": [[23, 31], ["hasattr", "isinstance", "ValueError"], "methods", ["None"], ["", "@", "property", "\n", "def", "optimizer", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return a torch.optim.optimizer.Optimizer instance.\"\"\"", "\n", "if", "not", "hasattr", "(", "self", ",", "'_optimizer'", ")", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "", "if", "not", "isinstance", "(", "self", ".", "_optimizer", ",", "torch", ".", "optim", ".", "Optimizer", ")", ":", "\n", "            ", "raise", "ValueError", "(", "'_optimizer must be an instance of torch.optim.Optimizer'", ")", "\n", "", "return", "self", ".", "_optimizer", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fairseq_optimizer.FairseqOptimizer.optimizer_config": [[32, 41], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "optimizer_config", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Return a kwarg dictionary that will be used to override optimizer\n        args stored in checkpoints. This allows us to load a checkpoint and\n        resume training using a different set of optimizer args, e.g., with a\n        different learning rate.\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fairseq_optimizer.FairseqOptimizer.__getstate__": [[42, 44], ["fairseq_optimizer.FairseqOptimizer._optimizer.__getstate__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fairseq_optimizer.FairseqOptimizer.__getstate__"], ["", "def", "__getstate__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_optimizer", ".", "__getstate__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fairseq_optimizer.FairseqOptimizer.get_lr": [[45, 48], ["None"], "methods", ["None"], ["", "def", "get_lr", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the current learning rate.\"\"\"", "\n", "return", "self", ".", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fairseq_optimizer.FairseqOptimizer.set_lr": [[49, 53], ["None"], "methods", ["None"], ["", "def", "set_lr", "(", "self", ",", "lr", ")", ":", "\n", "        ", "\"\"\"Set the learning rate.\"\"\"", "\n", "for", "param_group", "in", "self", ".", "optimizer", ".", "param_groups", ":", "\n", "            ", "param_group", "[", "'lr'", "]", "=", "lr", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fairseq_optimizer.FairseqOptimizer.state_dict": [[54, 57], ["fairseq_optimizer.FairseqOptimizer.optimizer.state_dict"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.state_dict"], ["", "", "def", "state_dict", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the optimizer's state dict.\"\"\"", "\n", "return", "self", ".", "optimizer", ".", "state_dict", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fairseq_optimizer.FairseqOptimizer.load_state_dict": [[58, 72], ["fairseq_optimizer.FairseqOptimizer.optimizer.load_state_dict", "len", "group.update"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.load_state_dict", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update"], ["", "def", "load_state_dict", "(", "self", ",", "state_dict", ",", "optimizer_overrides", "=", "None", ")", ":", "\n", "        ", "\"\"\"Load an optimizer state dict.\n\n        In general we should prefer the configuration of the existing optimizer\n        instance (e.g., learning rate) over that found in the state_dict. This\n        allows us to resume training from a checkpoint using a new set of\n        optimizer args.\n        \"\"\"", "\n", "self", ".", "optimizer", ".", "load_state_dict", "(", "state_dict", ")", "\n", "\n", "if", "optimizer_overrides", "is", "not", "None", "and", "len", "(", "optimizer_overrides", ")", ">", "0", ":", "\n", "# override learning rate, momentum, etc. with latest values", "\n", "            ", "for", "group", "in", "self", ".", "optimizer", ".", "param_groups", ":", "\n", "                ", "group", ".", "update", "(", "optimizer_overrides", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fairseq_optimizer.FairseqOptimizer.backward": [[73, 76], ["loss.backward"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.scalar_bias.ScalarBias.backward"], ["", "", "", "def", "backward", "(", "self", ",", "loss", ")", ":", "\n", "        ", "\"\"\"Computes the sum of gradients of the given tensor w.r.t. graph leaves.\"\"\"", "\n", "loss", ".", "backward", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fairseq_optimizer.FairseqOptimizer.multiply_grads": [[77, 82], ["p.grad.data.mul_"], "methods", ["None"], ["", "def", "multiply_grads", "(", "self", ",", "c", ")", ":", "\n", "        ", "\"\"\"Multiplies grads by a constant *c*.\"\"\"", "\n", "for", "p", "in", "self", ".", "params", ":", "\n", "            ", "if", "p", ".", "grad", "is", "not", "None", ":", "\n", "                ", "p", ".", "grad", ".", "data", ".", "mul_", "(", "c", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fairseq_optimizer.FairseqOptimizer.clip_grad_norm": [[83, 89], ["torch.nn.utils.clip_grad_norm_", "math.sqrt", "sum", "p.grad.data.norm"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.clip_grad_norm_"], ["", "", "", "def", "clip_grad_norm", "(", "self", ",", "max_norm", ")", ":", "\n", "        ", "\"\"\"Clips gradient norm.\"\"\"", "\n", "if", "max_norm", ">", "0", ":", "\n", "            ", "return", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "params", ",", "max_norm", ")", "\n", "", "else", ":", "\n", "            ", "return", "math", ".", "sqrt", "(", "sum", "(", "p", ".", "grad", ".", "data", ".", "norm", "(", ")", "**", "2", "for", "p", "in", "self", ".", "params", "if", "p", ".", "grad", "is", "not", "None", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fairseq_optimizer.FairseqOptimizer.step": [[90, 93], ["fairseq_optimizer.FairseqOptimizer.optimizer.step"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.polynomial_decay_schedule.PolynomialDecaySchedule.step"], ["", "", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\"\"\"", "\n", "self", ".", "optimizer", ".", "step", "(", "closure", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fairseq_optimizer.FairseqOptimizer.zero_grad": [[94, 100], ["fairseq_optimizer.FairseqOptimizer.optimizer.zero_grad"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.zero_grad"], ["", "def", "zero_grad", "(", "self", ")", ":", "\n", "        ", "\"\"\"Clears the gradients of all optimized parameters.\"\"\"", "\n", "for", "group", "in", "self", ".", "optimizer", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "p", ".", "grad", "=", "None", "\n", "", "", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fairseq_optimizer.FairseqOptimizer.supports_memory_efficient_fp16": [[101, 106], ["hasattr"], "methods", ["None"], ["", "@", "property", "\n", "def", "supports_memory_efficient_fp16", "(", "self", ")", ":", "\n", "        ", "if", "hasattr", "(", "self", ".", "optimizer", ",", "'supports_memory_efficient_fp16'", ")", ":", "\n", "            ", "return", "self", ".", "optimizer", ".", "supports_memory_efficient_fp16", "\n", "", "return", "False", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.adadelta.Adadelta.__init__": [[13, 16], ["FairseqOptimizer.__init__", "torch.optim.Adadelta"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "params", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "params", ")", "\n", "self", ".", "_optimizer", "=", "torch", ".", "optim", ".", "Adadelta", "(", "params", ",", "**", "self", ".", "optimizer_config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.adadelta.Adadelta.add_args": [[17, 28], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add optimizer-specific arguments to the parser.\"\"\"", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'--adadelta-rho'", ",", "type", "=", "float", ",", "default", "=", "0.9", ",", "metavar", "=", "'RHO'", ",", "\n", "help", "=", "'coefficient used for computing a running average of squared gradients'", ")", "\n", "parser", ".", "add_argument", "(", "'--adadelta-eps'", ",", "type", "=", "float", ",", "default", "=", "1e-6", ",", "metavar", "=", "'EPS'", ",", "\n", "help", "=", "'term added to the denominator to improve numerical stability'", ")", "\n", "parser", ".", "add_argument", "(", "'--weight-decay'", ",", "'--wd'", ",", "default", "=", "0.0", ",", "type", "=", "float", ",", "metavar", "=", "'WD'", ",", "\n", "help", "=", "'weight decay'", ")", "\n", "parser", ".", "add_argument", "(", "'--anneal-eps'", ",", "action", "=", "'store_true'", ",", "help", "=", "'flag to anneal eps'", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.adadelta.Adadelta.optimizer_config": [[30, 43], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "optimizer_config", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Return a kwarg dictionary that will be used to override optimizer\n        args stored in checkpoints. This allows us to load a checkpoint and\n        resume training using a different set of optimizer args, e.g., with a\n        different learning rate.\n        \"\"\"", "\n", "return", "{", "\n", "'lr'", ":", "self", ".", "args", ".", "lr", "[", "0", "]", ",", "\n", "'rho'", ":", "self", ".", "args", ".", "adadelta_rho", ",", "\n", "'eps'", ":", "self", ".", "args", ".", "adadelta_eps", ",", "\n", "'weight_decay'", ":", "self", ".", "args", ".", "weight_decay", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.bmuf.FairseqBMUF.__init__": [[22, 34], ["FairseqOptimizer.__init__", "bmuf.FairseqBMUF._reset_local_data"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.bmuf.FairseqBMUF._reset_local_data"], ["def", "__init__", "(", "self", ",", "args", ",", "params", ",", "optimizer", ")", ":", "\n", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "params", ")", "\n", "self", ".", "_optimizer", "=", "optimizer", "\n", "self", ".", "params", "=", "params", "\n", "self", ".", "_num_updates", "=", "0", "\n", "self", ".", "sync_iter", "=", "self", ".", "args", ".", "global_sync_iter", "\n", "self", ".", "block_momentum", "=", "1", "-", "1.0", "/", "self", ".", "args", ".", "distributed_world_size", "\n", "self", ".", "block_lr", "=", "self", ".", "args", ".", "block_lr", "\n", "self", ".", "_reset_local_data", "(", ")", "\n", "self", ".", "warmup_iteration", "=", "self", ".", "args", ".", "warmup_iterations", "\n", "self", ".", "use_nbm", "=", "self", ".", "args", ".", "use_nbm", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.bmuf.FairseqBMUF.add_args": [[35, 58], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add optimizer-specific arguments to the parser.\"\"\"", "\n", "parser", ".", "add_argument", "(", "\n", "\"--block-lr\"", ",", "default", "=", "1", ",", "type", "=", "float", ",", "help", "=", "\"block learning rate for bmuf\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--global-sync-iter\"", ",", "\n", "default", "=", "10", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Iteration for syncing global model\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--warmup-iterations\"", ",", "\n", "default", "=", "500", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"warmup iterations for model to broadcast\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--use-nbm\"", ",", "\n", "default", "=", "True", ",", "\n", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"Specify whether you want to use classical BM / Nesterov BM\"", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.bmuf.FairseqBMUF.optimizer": [[60, 63], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "optimizer", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_optimizer", ".", "optimizer", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.bmuf.FairseqBMUF.optimizer_config": [[64, 67], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "optimizer_config", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_optimizer", ".", "optimizer_config", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.bmuf.FairseqBMUF.get_lr": [[68, 70], ["bmuf.FairseqBMUF._optimizer.get_lr"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.get_lr"], ["", "def", "get_lr", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_optimizer", ".", "get_lr", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.bmuf.FairseqBMUF.set_lr": [[71, 73], ["bmuf.FairseqBMUF._optimizer.set_lr"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.set_lr"], ["", "def", "set_lr", "(", "self", ",", "lr", ")", ":", "\n", "        ", "self", ".", "_optimizer", ".", "set_lr", "(", "lr", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.bmuf.FairseqBMUF.state_dict": [[74, 76], ["bmuf.FairseqBMUF._optimizer.state_dict"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.state_dict"], ["", "def", "state_dict", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_optimizer", ".", "state_dict", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.bmuf.FairseqBMUF.load_state_dict": [[77, 79], ["bmuf.FairseqBMUF._optimizer.load_state_dict"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.load_state_dict"], ["", "def", "load_state_dict", "(", "self", ",", "state_dict", ",", "optimizer_overrides", "=", "None", ")", ":", "\n", "        ", "self", ".", "_optimizer", ".", "load_state_dict", "(", "state_dict", ",", "optimizer_overrides", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.bmuf.FairseqBMUF.multiply_grads": [[80, 83], ["bmuf.FairseqBMUF._optimizer.multiply_grads"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.multiply_grads"], ["", "def", "multiply_grads", "(", "self", ",", "c", ")", ":", "\n", "        ", "\"\"\"Multiplies grads by a constant *c*.\"\"\"", "\n", "self", ".", "_optimizer", ".", "multiply_grads", "(", "c", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.bmuf.FairseqBMUF.clip_grad_norm": [[84, 87], ["bmuf.FairseqBMUF._optimizer.clip_grad_norm"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.clip_grad_norm"], ["", "def", "clip_grad_norm", "(", "self", ",", "max_norm", ")", ":", "\n", "        ", "\"\"\"Clips gradient norm.\"\"\"", "\n", "return", "self", ".", "_optimizer", ".", "clip_grad_norm", "(", "max_norm", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.bmuf.FairseqBMUF._sync_block": [[88, 97], ["bmuf.FairseqBMUF._allreduce_parameter", "bmuf.FairseqBMUF.get_num_updates", "bmuf.FairseqBMUF._BM_before_sync", "bmuf.FairseqBMUF._BM_after_sync"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.bmuf.FairseqBMUF._allreduce_parameter", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.bmuf.FairseqBMUF.get_num_updates", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.bmuf.FairseqBMUF._BM_before_sync", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.bmuf.FairseqBMUF._BM_after_sync"], ["", "def", "_sync_block", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "get_num_updates", "(", ")", "%", "self", ".", "sync_iter", "==", "0", ":", "\n", "            ", "if", "self", ".", "block_momentum", "!=", "0", ":", "\n", "                ", "self", ".", "_BM_before_sync", "(", ")", "\n", "\n", "", "self", ".", "_allreduce_parameter", "(", ")", "\n", "\n", "if", "self", ".", "block_momentum", "!=", "0", ":", "\n", "                ", "self", ".", "_BM_after_sync", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.bmuf.FairseqBMUF._broadcast_model": [[98, 116], ["torch.broadcast", "torch.broadcast", "torch.broadcast", "torch.broadcast", "bmuf.FairseqBMUF.get_num_updates"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.bmuf.FairseqBMUF.get_num_updates"], ["", "", "", "def", "_broadcast_model", "(", "self", ",", "rootRank", "=", "0", ")", ":", "\n", "        ", "if", "(", "\n", "self", ".", "warmup_iteration", "!=", "0", "\n", "and", "self", ".", "get_num_updates", "(", ")", "%", "self", ".", "warmup_iteration", "==", "0", "\n", ")", ":", "\n", "            ", "self", ".", "warmup_iteration", "=", "0", "\n", "\n", "# broadcast the local model", "\n", "for", "param", "in", "self", ".", "params", ":", "\n", "                ", "dist", ".", "broadcast", "(", "param", ".", "data", ",", "rootRank", ")", "\n", "\n", "# Also, broadcast the local parameters", "\n", "", "for", "param", "in", "(", "\n", "self", ".", "params_localprev", "\n", "+", "self", ".", "smoothed_grads_localprev", "\n", "+", "self", ".", "grads_localprev", "\n", ")", ":", "\n", "                ", "dist", ".", "broadcast", "(", "param", ",", "src", "=", "rootRank", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.bmuf.FairseqBMUF.step": [[117, 125], ["bmuf.FairseqBMUF._optimizer.step", "bmuf.FairseqBMUF.set_num_updates", "bmuf.FairseqBMUF._broadcast_model", "bmuf.FairseqBMUF._sync_block", "bmuf.FairseqBMUF.get_num_updates"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.polynomial_decay_schedule.PolynomialDecaySchedule.step", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.bmuf.FairseqBMUF.set_num_updates", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.bmuf.FairseqBMUF._broadcast_model", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.bmuf.FairseqBMUF._sync_block", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.bmuf.FairseqBMUF.get_num_updates"], ["", "", "", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\"\"\"", "\n", "self", ".", "_optimizer", ".", "step", "(", "closure", ")", "\n", "self", ".", "set_num_updates", "(", "self", ".", "get_num_updates", "(", ")", "+", "1", ")", "\n", "if", "self", ".", "warmup_iteration", "!=", "0", ":", "\n", "            ", "self", ".", "_broadcast_model", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_sync_block", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.bmuf.FairseqBMUF.zero_grad": [[126, 129], ["bmuf.FairseqBMUF._optimizer.zero_grad"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.zero_grad"], ["", "", "def", "zero_grad", "(", "self", ")", ":", "\n", "        ", "\"\"\"Clears the gradients of all optimized parameters.\"\"\"", "\n", "self", ".", "_optimizer", ".", "zero_grad", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.bmuf.FairseqBMUF.get_num_updates": [[130, 133], ["None"], "methods", ["None"], ["", "def", "get_num_updates", "(", "self", ")", ":", "\n", "        ", "\"\"\"Get the number of parameters updates.\"\"\"", "\n", "return", "self", ".", "_num_updates", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.bmuf.FairseqBMUF.set_num_updates": [[134, 137], ["None"], "methods", ["None"], ["", "def", "set_num_updates", "(", "self", ",", "num_updates", ")", ":", "\n", "        ", "\"\"\"Set the number of parameters updates.\"\"\"", "\n", "self", ".", "_num_updates", "=", "num_updates", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.bmuf.FairseqBMUF._reset_local_data": [[138, 150], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "zip", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "p.data.new_zeros", "p.data.new_zeros", "copy_param.copy_", "p.data.size", "p.data.size"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "_reset_local_data", "(", "self", ")", ":", "\n", "        ", "self", ".", "params_localprev", "=", "[", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "for", "p", "in", "self", ".", "params", "]", "\n", "\n", "self", ".", "smoothed_grads_localprev", "=", "[", "\n", "p", ".", "data", ".", "new_zeros", "(", "p", ".", "data", ".", "size", "(", ")", ")", "for", "p", "in", "self", ".", "params", "\n", "]", "\n", "self", ".", "grads_localprev", "=", "[", "p", ".", "data", ".", "new_zeros", "(", "p", ".", "data", ".", "size", "(", ")", ")", "for", "p", "in", "self", ".", "params", "]", "\n", "\n", "# initialize", "\n", "for", "param", ",", "copy_param", "in", "zip", "(", "self", ".", "params", ",", "self", ".", "params_localprev", ")", ":", "\n", "            ", "copy_param", ".", "copy_", "(", "param", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.bmuf.FairseqBMUF._BM_before_sync": [[151, 161], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "enumerate", "zip"], "methods", ["None"], ["", "", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "_BM_before_sync", "(", "self", ")", ":", "\n", "# prev_param is basically the global copy from the previously finished", "\n", "# synchronisation. param.data is local parameter after block_sync_freq", "\n", "# for the local gpu. so grad is difference between previously synced", "\n", "# model and currrent local model.", "\n", "        ", "for", "index", ",", "(", "param", ",", "prev_param", ")", "in", "enumerate", "(", "\n", "zip", "(", "self", ".", "params", ",", "self", ".", "params_localprev", ")", "\n", ")", ":", "\n", "            ", "self", ".", "grads_localprev", "[", "index", "]", "=", "prev_param", "-", "param", ".", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.bmuf.FairseqBMUF._allreduce_parameter": [[162, 169], ["enumerate", "float", "torch.all_reduce", "torch.all_reduce", "torch.get_world_size", "torch.get_world_size"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.distributed_utils.all_reduce", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.distributed_utils.all_reduce", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.distributed_utils.get_world_size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.distributed_utils.get_world_size"], ["", "", "def", "_allreduce_parameter", "(", "self", ")", ":", "\n", "        ", "for", "index", ",", "param", "in", "enumerate", "(", "self", ".", "params", ")", ":", "\n", "            ", "sync_para", "=", "(", "\n", "param", ".", "data", "if", "self", ".", "block_momentum", "==", "0", "else", "self", ".", "grads_localprev", "[", "index", "]", "\n", ")", "\n", "sync_para", "/=", "float", "(", "dist", ".", "get_world_size", "(", ")", ")", "\n", "dist", ".", "all_reduce", "(", "sync_para", ",", "op", "=", "dist", ".", "ReduceOp", ".", "SUM", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.bmuf.FairseqBMUF._BM_after_sync": [[170, 200], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "enumerate", "zip", "param.data.copy_", "prev_param.copy_", "param.data.copy_"], "methods", ["None"], ["", "", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "_BM_after_sync", "(", "self", ")", ":", "\n", "        ", "for", "index", ",", "(", "param", ",", "prev_param", ",", "smoothed_grad", ",", "grad", ")", "in", "enumerate", "(", "\n", "zip", "(", "\n", "self", ".", "params", ",", "\n", "self", ".", "params_localprev", ",", "\n", "self", ".", "smoothed_grads_localprev", ",", "\n", "# all machines would share the same value of smoothed_grad, since it is", "\n", "# always computed on synchronized gradients.", "\n", "self", ".", "grads_localprev", ",", "\n", ")", "\n", ")", ":", "\n", "# prev_param is basically last syncrhornized parameter. though", "\n", "# smoothed_grad is local, all processes will have same value of", "\n", "# smoothed_grad and hence param is globally synchronized copy.", "\n", "# This is essentially a first-order infinite impulse response (IIR)", "\n", "# filter with the gain (1 - BM)*BM_lr:", "\n", "# smoothed_grad(t)=BM * smoothed_grad(t-1) + (1 - BM)*BM_lr*grad(t)", "\n", "            ", "smoothed_grad", "=", "(", "\n", "smoothed_grad", "*", "self", ".", "block_momentum", "\n", "+", "grad", "*", "(", "1", "-", "self", ".", "block_momentum", ")", "*", "self", ".", "block_lr", "\n", ")", "\n", "param", ".", "data", ".", "copy_", "(", "prev_param", "-", "smoothed_grad", ")", "\n", "# A Nesterov momentum here is to do a partial weight update before", "\n", "# calculating the gradient", "\n", "if", "self", ".", "use_nbm", ":", "\n", "                ", "param", ".", "data", ".", "copy_", "(", "param", ".", "data", "-", "self", ".", "block_momentum", "*", "smoothed_grad", ")", "\n", "# backup for the next synchronization.", "\n", "", "self", ".", "smoothed_grads_localprev", "[", "index", "]", "=", "smoothed_grad", "\n", "prev_param", ".", "copy_", "(", "param", ".", "data", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.DynamicLossScaler.__init__": [[15, 28], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "\n", "self", ",", "init_scale", "=", "2.", "**", "15", ",", "scale_factor", "=", "2.", ",", "scale_window", "=", "2000", ",", "\n", "tolerance", "=", "0.05", ",", "threshold", "=", "None", ",", "\n", ")", ":", "\n", "        ", "self", ".", "loss_scale", "=", "init_scale", "\n", "self", ".", "scale_factor", "=", "scale_factor", "\n", "self", ".", "scale_window", "=", "scale_window", "\n", "self", ".", "tolerance", "=", "tolerance", "\n", "self", ".", "threshold", "=", "threshold", "\n", "self", ".", "_iter", "=", "0", "\n", "self", ".", "_last_overflow_iter", "=", "-", "1", "\n", "self", ".", "_last_rescale_iter", "=", "-", "1", "\n", "self", ".", "_overflows_since_rescale", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.DynamicLossScaler.update_scale": [[29, 43], ["float", "fp16_optimizer.DynamicLossScaler._decrease_loss_scale"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.DynamicLossScaler._decrease_loss_scale"], ["", "def", "update_scale", "(", "self", ",", "overflow", ")", ":", "\n", "        ", "iter_since_rescale", "=", "self", ".", "_iter", "-", "self", ".", "_last_rescale_iter", "\n", "if", "overflow", ":", "\n", "            ", "self", ".", "_last_overflow_iter", "=", "self", ".", "_iter", "\n", "self", ".", "_overflows_since_rescale", "+=", "1", "\n", "pct_overflow", "=", "self", ".", "_overflows_since_rescale", "/", "float", "(", "iter_since_rescale", ")", "\n", "if", "pct_overflow", ">=", "self", ".", "tolerance", ":", "\n", "                ", "self", ".", "_decrease_loss_scale", "(", ")", "\n", "self", ".", "_last_rescale_iter", "=", "self", ".", "_iter", "\n", "self", ".", "_overflows_since_rescale", "=", "0", "\n", "", "", "elif", "(", "self", ".", "_iter", "-", "self", ".", "_last_overflow_iter", ")", "%", "self", ".", "scale_window", "==", "0", ":", "\n", "            ", "self", ".", "loss_scale", "*=", "self", ".", "scale_factor", "\n", "self", ".", "_last_rescale_iter", "=", "self", ".", "_iter", "\n", "", "self", ".", "_iter", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.DynamicLossScaler._decrease_loss_scale": [[44, 48], ["max"], "methods", ["None"], ["", "def", "_decrease_loss_scale", "(", "self", ")", ":", "\n", "        ", "self", ".", "loss_scale", "/=", "self", ".", "scale_factor", "\n", "if", "self", ".", "threshold", "is", "not", "None", ":", "\n", "            ", "self", ".", "loss_scale", "=", "max", "(", "self", ".", "loss_scale", ",", "self", ".", "threshold", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.DynamicLossScaler.has_overflow": [[49, 55], ["float"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "has_overflow", "(", "grad_norm", ")", ":", "\n", "# detect inf and nan", "\n", "        ", "if", "grad_norm", "==", "float", "(", "'inf'", ")", "or", "grad_norm", "!=", "grad_norm", ":", "\n", "            ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.FP16Optimizer.__init__": [[62, 82], ["fairseq.optim.FairseqOptimizer.__init__", "fp16_optimizer.DynamicLossScaler", "getattr", "len", "ValueError"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["def", "__init__", "(", "self", ",", "args", ",", "params", ",", "fp32_optimizer", ",", "fp32_params", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "params", ")", "\n", "self", ".", "fp32_optimizer", "=", "fp32_optimizer", "\n", "self", ".", "fp32_params", "=", "fp32_params", "\n", "\n", "if", "getattr", "(", "args", ",", "'fp16_scale_window'", ",", "None", ")", "is", "None", ":", "\n", "            ", "if", "len", "(", "args", ".", "update_freq", ")", ">", "1", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "'--fp16-scale-window must be given explicitly when using a '", "\n", "'custom --update-freq schedule'", "\n", ")", "\n", "", "scale_window", "=", "2", "**", "14", "/", "args", ".", "distributed_world_size", "/", "args", ".", "update_freq", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "scale_window", "=", "args", ".", "fp16_scale_window", "\n", "\n", "", "self", ".", "scaler", "=", "DynamicLossScaler", "(", "\n", "init_scale", "=", "args", ".", "fp16_init_scale", ",", "\n", "scale_window", "=", "scale_window", ",", "\n", "tolerance", "=", "args", ".", "fp16_scale_tolerance", ",", "\n", "threshold", "=", "args", ".", "threshold_loss_scale", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.FP16Optimizer.build_optimizer": [[84, 104], ["sum", "params[].new().float().new", "torch.nn.Parameter", "torch.nn.Parameter.data.new", "fairseq.optim.build_optimizer", "cls", "p.data.numel", "fp32_params[].copy_", "p.data.numel", "params[].new().float", "p.data.view", "params[].new"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.__init__.build_optimizer", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.masked_lm_dictionary.BertDictionary.cls"], ["", "@", "classmethod", "\n", "def", "build_optimizer", "(", "cls", ",", "args", ",", "params", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            args (argparse.Namespace): fairseq args\n            params (iterable): iterable of parameters to optimize\n        \"\"\"", "\n", "# create FP32 copy of parameters and grads", "\n", "total_param_size", "=", "sum", "(", "p", ".", "data", ".", "numel", "(", ")", "for", "p", "in", "params", ")", "\n", "fp32_params", "=", "params", "[", "0", "]", ".", "new", "(", "0", ")", ".", "float", "(", ")", ".", "new", "(", "total_param_size", ")", "\n", "offset", "=", "0", "\n", "for", "p", "in", "params", ":", "\n", "            ", "numel", "=", "p", ".", "data", ".", "numel", "(", ")", "\n", "fp32_params", "[", "offset", ":", "offset", "+", "numel", "]", ".", "copy_", "(", "p", ".", "data", ".", "view", "(", "-", "1", ")", ")", "\n", "offset", "+=", "numel", "\n", "", "fp32_params", "=", "torch", ".", "nn", ".", "Parameter", "(", "fp32_params", ")", "\n", "fp32_params", ".", "grad", "=", "fp32_params", ".", "data", ".", "new", "(", "total_param_size", ")", "\n", "\n", "fp32_optimizer", "=", "optim", ".", "build_optimizer", "(", "args", ",", "[", "fp32_params", "]", ")", "\n", "return", "cls", "(", "args", ",", "params", ",", "fp32_optimizer", ",", "fp32_params", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.FP16Optimizer.optimizer": [[105, 108], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "optimizer", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "fp32_optimizer", ".", "optimizer", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.FP16Optimizer.optimizer_config": [[109, 112], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "optimizer_config", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "fp32_optimizer", ".", "optimizer_config", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.FP16Optimizer.get_lr": [[113, 115], ["fp16_optimizer.FP16Optimizer.fp32_optimizer.get_lr"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.get_lr"], ["", "def", "get_lr", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "fp32_optimizer", ".", "get_lr", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.FP16Optimizer.set_lr": [[116, 118], ["fp16_optimizer.FP16Optimizer.fp32_optimizer.set_lr"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.set_lr"], ["", "def", "set_lr", "(", "self", ",", "lr", ")", ":", "\n", "        ", "self", ".", "fp32_optimizer", ".", "set_lr", "(", "lr", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.FP16Optimizer.state_dict": [[119, 124], ["fp16_optimizer.FP16Optimizer.fp32_optimizer.state_dict"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.state_dict"], ["", "def", "state_dict", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the optimizer's state dict.\"\"\"", "\n", "state_dict", "=", "self", ".", "fp32_optimizer", ".", "state_dict", "(", ")", "\n", "state_dict", "[", "'loss_scale'", "]", "=", "self", ".", "scaler", ".", "loss_scale", "\n", "return", "state_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.FP16Optimizer.load_state_dict": [[125, 136], ["fp16_optimizer.FP16Optimizer.fp32_optimizer.load_state_dict"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.load_state_dict"], ["", "def", "load_state_dict", "(", "self", ",", "state_dict", ",", "optimizer_overrides", "=", "None", ")", ":", "\n", "        ", "\"\"\"Load an optimizer state dict.\n\n        In general we should prefer the configuration of the existing optimizer\n        instance (e.g., learning rate) over that found in the state_dict. This\n        allows us to resume training from a checkpoint using a new set of\n        optimizer args.\n        \"\"\"", "\n", "if", "'loss_scale'", "in", "state_dict", ":", "\n", "            ", "self", ".", "scaler", ".", "loss_scale", "=", "state_dict", "[", "'loss_scale'", "]", "\n", "", "self", ".", "fp32_optimizer", ".", "load_state_dict", "(", "state_dict", ",", "optimizer_overrides", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.FP16Optimizer.backward": [[137, 147], ["loss.backward"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.scalar_bias.ScalarBias.backward"], ["", "def", "backward", "(", "self", ",", "loss", ")", ":", "\n", "        ", "\"\"\"Computes the sum of gradients of the given tensor w.r.t. graph leaves.\n\n        Compared to :func:`fairseq.optim.FairseqOptimizer.backward`, this\n        function additionally dynamically scales the loss to avoid gradient\n        underflow.\n        \"\"\"", "\n", "loss", "=", "loss", "*", "self", ".", "scaler", ".", "loss_scale", "\n", "loss", ".", "backward", "(", ")", "\n", "self", ".", "_needs_sync", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.FP16Optimizer._sync_fp16_grads_to_fp32": [[148, 164], ["fp16_optimizer.FP16Optimizer.fp32_params.grad.data.mul_", "grad_data.numel", "fp16_optimizer.FP16Optimizer.fp32_params.grad.data[].copy_", "p.data.new_zeros", "grad_data.view"], "methods", ["None"], ["", "def", "_sync_fp16_grads_to_fp32", "(", "self", ",", "multiply_grads", "=", "1.", ")", ":", "\n", "        ", "if", "self", ".", "_needs_sync", ":", "\n", "# copy FP16 grads to FP32", "\n", "            ", "offset", "=", "0", "\n", "for", "p", "in", "self", ".", "params", ":", "\n", "                ", "if", "not", "p", ".", "requires_grad", ":", "\n", "                    ", "continue", "\n", "", "grad_data", "=", "p", ".", "grad", ".", "data", "if", "p", ".", "grad", "is", "not", "None", "else", "p", ".", "data", ".", "new_zeros", "(", "p", ".", "data", ".", "shape", ")", "\n", "numel", "=", "grad_data", ".", "numel", "(", ")", "\n", "self", ".", "fp32_params", ".", "grad", ".", "data", "[", "offset", ":", "offset", "+", "numel", "]", ".", "copy_", "(", "grad_data", ".", "view", "(", "-", "1", ")", ")", "\n", "offset", "+=", "numel", "\n", "\n", "# correct for dynamic loss scaler", "\n", "", "self", ".", "fp32_params", ".", "grad", ".", "data", ".", "mul_", "(", "multiply_grads", "/", "self", ".", "scaler", ".", "loss_scale", ")", "\n", "\n", "self", ".", "_needs_sync", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.FP16Optimizer.multiply_grads": [[165, 171], ["fp16_optimizer.FP16Optimizer._sync_fp16_grads_to_fp32", "fp16_optimizer.FP16Optimizer.fp32_params.grad.data.mul_"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.FP16Optimizer._sync_fp16_grads_to_fp32"], ["", "", "def", "multiply_grads", "(", "self", ",", "c", ")", ":", "\n", "        ", "\"\"\"Multiplies grads by a constant ``c``.\"\"\"", "\n", "if", "self", ".", "_needs_sync", ":", "\n", "            ", "self", ".", "_sync_fp16_grads_to_fp32", "(", "c", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "fp32_params", ".", "grad", ".", "data", ".", "mul_", "(", "c", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.FP16Optimizer.clip_grad_norm": [[172, 191], ["fp16_optimizer.FP16Optimizer._sync_fp16_grads_to_fp32", "fairseq.utils.clip_grad_norm_", "fp16_optimizer.DynamicLossScaler.has_overflow", "fp16_optimizer.FP16Optimizer.scaler.update_scale", "OverflowError", "FloatingPointError", "str"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.FP16Optimizer._sync_fp16_grads_to_fp32", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.clip_grad_norm_", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.DynamicLossScaler.has_overflow", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.DynamicLossScaler.update_scale"], ["", "", "def", "clip_grad_norm", "(", "self", ",", "max_norm", ")", ":", "\n", "        ", "\"\"\"Clips gradient norm and updates dynamic loss scaler.\"\"\"", "\n", "self", ".", "_sync_fp16_grads_to_fp32", "(", ")", "\n", "grad_norm", "=", "utils", ".", "clip_grad_norm_", "(", "self", ".", "fp32_params", ".", "grad", ".", "data", ",", "max_norm", ")", "\n", "\n", "# detect overflow and adjust loss scale", "\n", "overflow", "=", "DynamicLossScaler", ".", "has_overflow", "(", "grad_norm", ")", "\n", "self", ".", "scaler", ".", "update_scale", "(", "overflow", ")", "\n", "if", "overflow", ":", "\n", "            ", "if", "self", ".", "scaler", ".", "loss_scale", "<=", "self", ".", "args", ".", "min_loss_scale", ":", "\n", "# Use FloatingPointError as an uncommon error that parent", "\n", "# functions can safely catch to stop training.", "\n", "                ", "raise", "FloatingPointError", "(", "(", "\n", "'Minimum loss scale reached ({}). Your loss is probably exploding. '", "\n", "'Try lowering the learning rate, using gradient clipping or '", "\n", "'increasing the batch size.'", "\n", ")", ".", "format", "(", "self", ".", "args", ".", "min_loss_scale", ")", ")", "\n", "", "raise", "OverflowError", "(", "'setting loss scale to: '", "+", "str", "(", "self", ".", "scaler", ".", "loss_scale", ")", ")", "\n", "", "return", "grad_norm", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.FP16Optimizer.step": [[192, 205], ["fp16_optimizer.FP16Optimizer._sync_fp16_grads_to_fp32", "fp16_optimizer.FP16Optimizer.fp32_optimizer.step", "p.data.numel", "p.data.copy_", "fp16_optimizer.FP16Optimizer.fp32_params.data[].view_as"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.FP16Optimizer._sync_fp16_grads_to_fp32", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.polynomial_decay_schedule.PolynomialDecaySchedule.step"], ["", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\"\"\"", "\n", "self", ".", "_sync_fp16_grads_to_fp32", "(", ")", "\n", "self", ".", "fp32_optimizer", ".", "step", "(", "closure", ")", "\n", "\n", "# copy FP32 params back into FP16 model", "\n", "offset", "=", "0", "\n", "for", "p", "in", "self", ".", "params", ":", "\n", "            ", "if", "not", "p", ".", "requires_grad", ":", "\n", "                ", "continue", "\n", "", "numel", "=", "p", ".", "data", ".", "numel", "(", ")", "\n", "p", ".", "data", ".", "copy_", "(", "self", ".", "fp32_params", ".", "data", "[", "offset", ":", "offset", "+", "numel", "]", ".", "view_as", "(", "p", ".", "data", ")", ")", "\n", "offset", "+=", "numel", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.FP16Optimizer.zero_grad": [[206, 211], ["None"], "methods", ["None"], ["", "", "def", "zero_grad", "(", "self", ")", ":", "\n", "        ", "\"\"\"Clears the gradients of all optimized parameters.\"\"\"", "\n", "for", "p", "in", "self", ".", "params", ":", "\n", "            ", "p", ".", "grad", "=", "None", "\n", "", "self", ".", "_needs_sync", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.__init__": [[229, 253], ["fairseq.optim.FairseqOptimizer.__init__", "fp16_optimizer.DynamicLossScaler", "ValueError", "getattr", "len", "ValueError"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["def", "__init__", "(", "self", ",", "args", ",", "params", ",", "optimizer", ")", ":", "\n", "        ", "if", "not", "optimizer", ".", "supports_memory_efficient_fp16", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "'Unsupported optimizer: {}'", ".", "format", "(", "optimizer", ".", "__class__", ".", "__name__", ")", "\n", ")", "\n", "\n", "", "super", "(", ")", ".", "__init__", "(", "args", ",", "params", ")", "\n", "self", ".", "wrapped_optimizer", "=", "optimizer", "\n", "\n", "if", "getattr", "(", "args", ",", "'fp16_scale_window'", ",", "None", ")", "is", "None", ":", "\n", "            ", "if", "len", "(", "args", ".", "update_freq", ")", ">", "1", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "'--fp16-scale-window must be given explicitly when using a '", "\n", "'custom --update-freq schedule'", "\n", ")", "\n", "", "scale_window", "=", "2", "**", "14", "/", "args", ".", "distributed_world_size", "/", "args", ".", "update_freq", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "scale_window", "=", "args", ".", "fp16_scale_window", "\n", "\n", "", "self", ".", "scaler", "=", "DynamicLossScaler", "(", "\n", "init_scale", "=", "args", ".", "fp16_init_scale", ",", "\n", "scale_window", "=", "scale_window", ",", "\n", "tolerance", "=", "args", ".", "fp16_scale_tolerance", ",", "\n", "threshold", "=", "args", ".", "threshold_loss_scale", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.build_optimizer": [[255, 264], ["fairseq.optim.build_optimizer", "cls"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.__init__.build_optimizer", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.masked_lm_dictionary.BertDictionary.cls"], ["", "@", "classmethod", "\n", "def", "build_optimizer", "(", "cls", ",", "args", ",", "params", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            args (argparse.Namespace): fairseq args\n            params (iterable): iterable of parameters to optimize\n        \"\"\"", "\n", "fp16_optimizer", "=", "optim", ".", "build_optimizer", "(", "args", ",", "params", ")", "\n", "return", "cls", "(", "args", ",", "params", ",", "fp16_optimizer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.optimizer": [[265, 268], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "optimizer", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "wrapped_optimizer", ".", "optimizer", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.optimizer_config": [[269, 272], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "optimizer_config", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "wrapped_optimizer", ".", "optimizer_config", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.get_lr": [[273, 275], ["fp16_optimizer.MemoryEfficientFP16Optimizer.wrapped_optimizer.get_lr"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.get_lr"], ["", "def", "get_lr", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "wrapped_optimizer", ".", "get_lr", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.set_lr": [[276, 278], ["fp16_optimizer.MemoryEfficientFP16Optimizer.wrapped_optimizer.set_lr"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.set_lr"], ["", "def", "set_lr", "(", "self", ",", "lr", ")", ":", "\n", "        ", "self", ".", "wrapped_optimizer", ".", "set_lr", "(", "lr", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.state_dict": [[279, 284], ["fp16_optimizer.MemoryEfficientFP16Optimizer.wrapped_optimizer.state_dict"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.state_dict"], ["", "def", "state_dict", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the optimizer's state dict.\"\"\"", "\n", "state_dict", "=", "self", ".", "wrapped_optimizer", ".", "state_dict", "(", ")", "\n", "state_dict", "[", "'loss_scale'", "]", "=", "self", ".", "scaler", ".", "loss_scale", "\n", "return", "state_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.load_state_dict": [[285, 316], ["fp16_optimizer.MemoryEfficientFP16Optimizer.wrapped_optimizer.load_state_dict", "state_dict[].items", "zip", "itertools.chain", "itertools.chain"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.load_state_dict"], ["", "def", "load_state_dict", "(", "self", ",", "state_dict", ",", "optimizer_overrides", "=", "None", ")", ":", "\n", "        ", "\"\"\"Load an optimizer state dict.\n\n        In general we should prefer the configuration of the existing optimizer\n        instance (e.g., learning rate) over that found in the state_dict. This\n        allows us to resume training from a checkpoint using a new set of\n        optimizer args.\n        \"\"\"", "\n", "if", "'loss_scale'", "in", "state_dict", ":", "\n", "            ", "self", ".", "scaler", ".", "loss_scale", "=", "state_dict", "[", "'loss_scale'", "]", "\n", "\n", "", "self", ".", "wrapped_optimizer", ".", "load_state_dict", "(", "state_dict", ",", "optimizer_overrides", ")", "\n", "\n", "# Hack: PyTorch automatically casts the optimizer state to match the", "\n", "# type of the current parameters. But with --memory-efficient-fp16 the", "\n", "# params are FP16 while the optimizer state is FP32 and we don't want", "\n", "# to cast. A workaround is to manually copy back the original state", "\n", "# after the optimizer has been loaded.", "\n", "groups", "=", "self", ".", "optimizer", ".", "param_groups", "\n", "saved_groups", "=", "state_dict", "[", "'param_groups'", "]", "\n", "id_map", "=", "{", "\n", "old_id", ":", "p", "\n", "for", "old_id", ",", "p", "in", "zip", "(", "\n", "chain", "(", "*", "(", "g", "[", "'params'", "]", "for", "g", "in", "saved_groups", ")", ")", ",", "\n", "chain", "(", "*", "(", "g", "[", "'params'", "]", "for", "g", "in", "groups", ")", ")", "\n", ")", "\n", "}", "\n", "for", "k", ",", "v", "in", "state_dict", "[", "'state'", "]", ".", "items", "(", ")", ":", "\n", "            ", "if", "k", "in", "id_map", ":", "\n", "                ", "param", "=", "id_map", "[", "k", "]", "\n", "self", ".", "optimizer", ".", "state", "[", "param", "]", "=", "v", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.backward": [[317, 327], ["loss.backward"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.scalar_bias.ScalarBias.backward"], ["", "", "", "def", "backward", "(", "self", ",", "loss", ")", ":", "\n", "        ", "\"\"\"Computes the sum of gradients of the given tensor w.r.t. graph leaves.\n\n        Compared to :func:`fairseq.optim.FairseqOptimizer.backward`, this\n        function additionally dynamically scales the loss to avoid gradient\n        underflow.\n        \"\"\"", "\n", "loss", "=", "loss", "*", "self", ".", "scaler", ".", "loss_scale", "\n", "loss", ".", "backward", "(", ")", "\n", "self", ".", "_grads_are_scaled", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer._unscale_grads": [[328, 336], ["fp16_optimizer.MemoryEfficientFP16Optimizer.wrapped_optimizer.multiply_grads"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.multiply_grads"], ["", "def", "_unscale_grads", "(", "self", ",", "multiply_grads", "=", "1.", ")", ":", "\n", "        ", "if", "self", ".", "_grads_are_scaled", ":", "\n", "            ", "self", ".", "_grads_are_scaled", "=", "False", "\n", "\n", "# correct for dynamic loss scaler", "\n", "self", ".", "wrapped_optimizer", ".", "multiply_grads", "(", "multiply_grads", "/", "self", ".", "scaler", ".", "loss_scale", ")", "\n", "", "else", ":", "\n", "            ", "assert", "multiply_grads", "==", "1.", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.multiply_grads": [[337, 343], ["fp16_optimizer.MemoryEfficientFP16Optimizer._unscale_grads", "fp16_optimizer.MemoryEfficientFP16Optimizer.wrapped_optimizer.multiply_grads"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer._unscale_grads", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.multiply_grads"], ["", "", "def", "multiply_grads", "(", "self", ",", "c", ")", ":", "\n", "        ", "\"\"\"Multiplies grads by a constant *c*.\"\"\"", "\n", "if", "self", ".", "_grads_are_scaled", ":", "\n", "            ", "self", ".", "_unscale_grads", "(", "c", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "wrapped_optimizer", ".", "multiply_grads", "(", "c", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.clip_grad_norm": [[344, 364], ["fp16_optimizer.MemoryEfficientFP16Optimizer._unscale_grads", "fp16_optimizer.MemoryEfficientFP16Optimizer.wrapped_optimizer.clip_grad_norm", "fp16_optimizer.DynamicLossScaler.has_overflow", "fp16_optimizer.MemoryEfficientFP16Optimizer.scaler.update_scale", "OverflowError", "FloatingPointError", "str"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer._unscale_grads", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.clip_grad_norm", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.DynamicLossScaler.has_overflow", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.DynamicLossScaler.update_scale"], ["", "", "def", "clip_grad_norm", "(", "self", ",", "max_norm", ")", ":", "\n", "        ", "\"\"\"Clips gradient norm and updates dynamic loss scaler.\"\"\"", "\n", "self", ".", "_unscale_grads", "(", ")", "\n", "grad_norm", "=", "self", ".", "wrapped_optimizer", ".", "clip_grad_norm", "(", "max_norm", ")", "\n", "\n", "# detect overflow and adjust loss scale", "\n", "overflow", "=", "DynamicLossScaler", ".", "has_overflow", "(", "grad_norm", ")", "\n", "self", ".", "scaler", ".", "update_scale", "(", "overflow", ")", "\n", "if", "overflow", ":", "\n", "            ", "if", "self", ".", "scaler", ".", "loss_scale", "<=", "self", ".", "args", ".", "min_loss_scale", ":", "\n", "# Use FloatingPointError as an uncommon error that parent", "\n", "# functions can safely catch to stop training.", "\n", "                ", "raise", "FloatingPointError", "(", "(", "\n", "'Minimum loss scale reached ({}). Your loss is probably exploding. '", "\n", "'Try lowering the learning rate, using gradient clipping or '", "\n", "'increasing the batch size.'", "\n", ")", ".", "format", "(", "self", ".", "args", ".", "min_loss_scale", ")", ")", "\n", "", "raise", "OverflowError", "(", "'setting loss scale to: '", "+", "str", "(", "self", ".", "scaler", ".", "loss_scale", ")", ")", "\n", "\n", "", "return", "grad_norm", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.step": [[365, 369], ["fp16_optimizer.MemoryEfficientFP16Optimizer._unscale_grads", "fp16_optimizer.MemoryEfficientFP16Optimizer.wrapped_optimizer.step"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer._unscale_grads", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.polynomial_decay_schedule.PolynomialDecaySchedule.step"], ["", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\"\"\"", "\n", "self", ".", "_unscale_grads", "(", ")", "\n", "self", ".", "wrapped_optimizer", ".", "step", "(", "closure", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.zero_grad": [[370, 374], ["fp16_optimizer.MemoryEfficientFP16Optimizer.wrapped_optimizer.zero_grad"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.zero_grad"], ["", "def", "zero_grad", "(", "self", ")", ":", "\n", "        ", "\"\"\"Clears the gradients of all optimized parameters.\"\"\"", "\n", "self", ".", "wrapped_optimizer", ".", "zero_grad", "(", ")", "\n", "self", ".", "_grads_are_scaled", "=", "False", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.adam.FairseqAdam.__init__": [[18, 28], ["FairseqOptimizer.__init__", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "adam.Adam", "adam.FusedAdam", "adam.Adam"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "params", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "params", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "try", ":", "\n", "                ", "from", "apex", ".", "optimizers", "import", "FusedAdam", "as", "_FusedAdam", "# noqa", "\n", "self", ".", "_optimizer", "=", "FusedAdam", "(", "params", ",", "**", "self", ".", "optimizer_config", ")", "\n", "", "except", "ImportError", ":", "\n", "                ", "self", ".", "_optimizer", "=", "Adam", "(", "params", ",", "**", "self", ".", "optimizer_config", ")", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "_optimizer", "=", "Adam", "(", "params", ",", "**", "self", ".", "optimizer_config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.adam.FairseqAdam.add_args": [[29, 39], ["parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add optimizer-specific arguments to the parser.\"\"\"", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'--adam-betas'", ",", "default", "=", "'(0.9, 0.999)'", ",", "metavar", "=", "'B'", ",", "\n", "help", "=", "'betas for Adam optimizer'", ")", "\n", "parser", ".", "add_argument", "(", "'--adam-eps'", ",", "type", "=", "float", ",", "default", "=", "1e-8", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'epsilon for Adam optimizer'", ")", "\n", "parser", ".", "add_argument", "(", "'--weight-decay'", ",", "'--wd'", ",", "default", "=", "0.0", ",", "type", "=", "float", ",", "metavar", "=", "'WD'", ",", "\n", "help", "=", "'weight decay'", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.adam.FairseqAdam.optimizer_config": [[41, 55], ["eval", "adam.FairseqAdam.args.adam_betas.replace"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval"], ["", "@", "property", "\n", "def", "optimizer_config", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Return a kwarg dictionary that will be used to override optimizer\n        args stored in checkpoints. This allows us to load a checkpoint and\n        resume training using a different set of optimizer args, e.g., with a\n        different learning rate.\n        \"\"\"", "\n", "return", "{", "\n", "'lr'", ":", "self", ".", "args", ".", "lr", "[", "0", "]", ",", "\n", "# extra scaping for command line literal artifacts", "\n", "'betas'", ":", "eval", "(", "self", ".", "args", ".", "adam_betas", ".", "replace", "(", "'\\''", ",", "''", ")", ")", ",", "\n", "'eps'", ":", "self", ".", "args", ".", "adam_eps", ",", "\n", "'weight_decay'", ":", "self", ".", "args", ".", "weight_decay", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.adam.Adam.__init__": [[85, 90], ["dict", "super().__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "1e-3", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "eps", "=", "1e-8", ",", "\n", "weight_decay", "=", "0", ",", "amsgrad", "=", "False", ")", ":", "\n", "        ", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "betas", "=", "betas", ",", "eps", "=", "eps", ",", "\n", "weight_decay", "=", "weight_decay", ",", "amsgrad", "=", "amsgrad", ")", "\n", "super", "(", "Adam", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.adam.Adam.supports_memory_efficient_fp16": [[91, 94], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "supports_memory_efficient_fp16", "(", "self", ")", ":", "\n", "        ", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.adam.Adam.step": [[95, 165], ["closure", "p.grad.data.float", "p.data.float", "exp_avg.mul_().add_", "exp_avg_sq.mul_().addcmul_", "p.data.float.addcdiv_", "p.data.copy_", "RuntimeError", "len", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "state[].type_as", "state[].type_as", "torch.max", "torch.max", "torch.max", "torch.max", "max_exp_avg_sq.sqrt().add_", "exp_avg_sq.sqrt().add_", "p.data.float.add_", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "state[].type_as", "exp_avg.mul_", "exp_avg_sq.mul_", "math.sqrt", "max_exp_avg_sq.sqrt", "exp_avg_sq.sqrt"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", ".", "data", ".", "float", "(", ")", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "'Adam does not support sparse gradients, please consider SparseAdam instead'", ")", "\n", "", "amsgrad", "=", "group", "[", "'amsgrad'", "]", "\n", "\n", "p_data_fp32", "=", "p", ".", "data", ".", "float", "(", ")", "\n", "\n", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "# Exponential moving average of gradient values", "\n", "state", "[", "'exp_avg'", "]", "=", "torch", ".", "zeros_like", "(", "p_data_fp32", ")", "\n", "# Exponential moving average of squared gradient values", "\n", "state", "[", "'exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p_data_fp32", ")", "\n", "if", "amsgrad", ":", "\n", "# Maintains max of all exp. moving avg. of sq. grad. values", "\n", "                        ", "state", "[", "'max_exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p_data_fp32", ")", "\n", "", "", "else", ":", "\n", "                    ", "state", "[", "'exp_avg'", "]", "=", "state", "[", "'exp_avg'", "]", ".", "type_as", "(", "p_data_fp32", ")", "\n", "state", "[", "'exp_avg_sq'", "]", "=", "state", "[", "'exp_avg_sq'", "]", ".", "type_as", "(", "p_data_fp32", ")", "\n", "if", "amsgrad", ":", "\n", "                        ", "state", "[", "'max_exp_avg_sq'", "]", "=", "state", "[", "'max_exp_avg_sq'", "]", ".", "type_as", "(", "p_data_fp32", ")", "\n", "\n", "", "", "exp_avg", ",", "exp_avg_sq", "=", "state", "[", "'exp_avg'", "]", ",", "state", "[", "'exp_avg_sq'", "]", "\n", "if", "amsgrad", ":", "\n", "                    ", "max_exp_avg_sq", "=", "state", "[", "'max_exp_avg_sq'", "]", "\n", "", "beta1", ",", "beta2", "=", "group", "[", "'betas'", "]", "\n", "\n", "state", "[", "'step'", "]", "+=", "1", "\n", "\n", "# Decay the first and second moment running average coefficient", "\n", "exp_avg", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "1", "-", "beta1", ",", "grad", ")", "\n", "exp_avg_sq", ".", "mul_", "(", "beta2", ")", ".", "addcmul_", "(", "1", "-", "beta2", ",", "grad", ",", "grad", ")", "\n", "if", "amsgrad", ":", "\n", "# Maintains the maximum of all 2nd moment running avg. till now", "\n", "                    ", "torch", ".", "max", "(", "max_exp_avg_sq", ",", "exp_avg_sq", ",", "out", "=", "max_exp_avg_sq", ")", "\n", "# Use the max. for normalizing running avg. of gradient", "\n", "denom", "=", "max_exp_avg_sq", ".", "sqrt", "(", ")", ".", "add_", "(", "group", "[", "'eps'", "]", ")", "\n", "", "else", ":", "\n", "                    ", "denom", "=", "exp_avg_sq", ".", "sqrt", "(", ")", ".", "add_", "(", "group", "[", "'eps'", "]", ")", "\n", "\n", "", "bias_correction1", "=", "1", "-", "beta1", "**", "state", "[", "'step'", "]", "\n", "bias_correction2", "=", "1", "-", "beta2", "**", "state", "[", "'step'", "]", "\n", "step_size", "=", "group", "[", "'lr'", "]", "*", "math", ".", "sqrt", "(", "bias_correction2", ")", "/", "bias_correction1", "\n", "\n", "if", "group", "[", "'weight_decay'", "]", "!=", "0", ":", "\n", "                    ", "p_data_fp32", ".", "add_", "(", "-", "group", "[", "'weight_decay'", "]", "*", "group", "[", "'lr'", "]", ",", "p_data_fp32", ")", "\n", "\n", "", "p_data_fp32", ".", "addcdiv_", "(", "-", "step_size", ",", "exp_avg", ",", "denom", ")", "\n", "\n", "p", ".", "data", ".", "copy_", "(", "p_data_fp32", ")", "\n", "\n", "", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.adam.FusedAdam.__init__": [[199, 214], ["importlib.import_module", "dict", "super().__init__", "RuntimeError"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["def", "__init__", "(", "self", ",", "params", ",", "\n", "lr", "=", "1e-3", ",", "bias_correction", "=", "True", ",", "\n", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "eps", "=", "1e-8", ",", "eps_inside_sqrt", "=", "False", ",", "\n", "weight_decay", "=", "0.", ",", "max_grad_norm", "=", "0.", ",", "amsgrad", "=", "False", ")", ":", "\n", "        ", "global", "fused_adam_cuda", "\n", "import", "importlib", "\n", "fused_adam_cuda", "=", "importlib", ".", "import_module", "(", "\"fused_adam_cuda\"", ")", "\n", "\n", "if", "amsgrad", ":", "\n", "            ", "raise", "RuntimeError", "(", "'FusedAdam does not support the AMSGrad variant.'", ")", "\n", "", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "bias_correction", "=", "bias_correction", ",", "\n", "betas", "=", "betas", ",", "eps", "=", "eps", ",", "weight_decay", "=", "weight_decay", ",", "\n", "max_grad_norm", "=", "max_grad_norm", ")", "\n", "super", "(", "FusedAdam", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "self", ".", "eps_mode", "=", "0", "if", "eps_inside_sqrt", "else", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.adam.FusedAdam.supports_memory_efficient_fp16": [[215, 218], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "supports_memory_efficient_fp16", "(", "self", ")", ":", "\n", "        ", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.adam.FusedAdam.step": [[219, 312], ["zip", "closure", "isinstance", "zip", "len", "len", "p.data.float", "fused_adam_cuda.adam", "type", "len", "RuntimeError", "len", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "state[].type_as", "state[].type_as"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "closure", "=", "None", ",", "grads", "=", "None", ",", "scale", "=", "1.", ",", "grad_norms", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n            grads (list of tensors, optional): weight gradient to use for the\n                optimizer update. If gradients have type torch.half, parameters\n                are expected to be in type torch.float. (default: None)\n            output params (list of tensors, optional): A reduced precision copy\n                of the updated weights written out in addition to the regular\n                updated weights. Have to be of same type as gradients. (default: None)\n            scale (float, optional): factor to divide gradient tensor values\n                by before applying to weights. (default: 1)\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "if", "grads", "is", "None", ":", "\n", "            ", "grads_group", "=", "[", "None", "]", "*", "len", "(", "self", ".", "param_groups", ")", "\n", "# backward compatibility", "\n", "# assuming a list/generator of parameter means single group", "\n", "", "elif", "isinstance", "(", "grads", ",", "types", ".", "GeneratorType", ")", ":", "\n", "            ", "grads_group", "=", "[", "grads", "]", "\n", "", "elif", "type", "(", "grads", "[", "0", "]", ")", "!=", "list", ":", "\n", "            ", "grads_group", "=", "[", "grads", "]", "\n", "", "else", ":", "\n", "            ", "grads_group", "=", "grads", "\n", "\n", "", "if", "grad_norms", "is", "None", ":", "\n", "            ", "grad_norms", "=", "[", "None", "]", "*", "len", "(", "self", ".", "param_groups", ")", "\n", "\n", "", "for", "group", ",", "grads_this_group", ",", "grad_norm", "in", "zip", "(", "self", ".", "param_groups", ",", "grads_group", ",", "grad_norms", ")", ":", "\n", "            ", "if", "grads_this_group", "is", "None", ":", "\n", "               ", "grads_this_group", "=", "[", "None", "]", "*", "len", "(", "group", "[", "'params'", "]", ")", "\n", "\n", "# compute combined scale factor for this group", "\n", "", "combined_scale", "=", "scale", "\n", "if", "group", "[", "'max_grad_norm'", "]", ">", "0", ":", "\n", "# norm is in fact norm*scale", "\n", "                ", "clip", "=", "(", "(", "grad_norm", "/", "scale", ")", "+", "1e-6", ")", "/", "group", "[", "'max_grad_norm'", "]", "\n", "if", "clip", ">", "1", ":", "\n", "                    ", "combined_scale", "=", "clip", "*", "scale", "\n", "\n", "", "", "bias_correction", "=", "1", "if", "group", "[", "'bias_correction'", "]", "else", "0", "\n", "\n", "for", "p", ",", "grad", "in", "zip", "(", "group", "[", "'params'", "]", ",", "grads_this_group", ")", ":", "\n", "#note: p.grad should not ever be set for correct operation of mixed precision optimizer that sometimes sends None gradients", "\n", "                ", "if", "p", ".", "grad", "is", "None", "and", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "if", "grad", "is", "None", ":", "\n", "                    ", "grad", "=", "p", ".", "grad", ".", "data", "\n", "", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "'FusedAdam does not support sparse gradients, please consider SparseAdam instead'", ")", "\n", "\n", "", "p_data_fp32", "=", "p", ".", "data", ".", "float", "(", ")", "\n", "\n", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "# Exponential moving average of gradient values", "\n", "state", "[", "'exp_avg'", "]", "=", "torch", ".", "zeros_like", "(", "p_data_fp32", ")", "\n", "# Exponential moving average of squared gradient values", "\n", "state", "[", "'exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p_data_fp32", ")", "\n", "", "else", ":", "\n", "                    ", "state", "[", "'exp_avg'", "]", "=", "state", "[", "'exp_avg'", "]", ".", "type_as", "(", "p_data_fp32", ")", "\n", "state", "[", "'exp_avg_sq'", "]", "=", "state", "[", "'exp_avg_sq'", "]", ".", "type_as", "(", "p_data_fp32", ")", "\n", "\n", "", "exp_avg", "=", "state", "[", "'exp_avg'", "]", "\n", "exp_avg_sq", "=", "state", "[", "'exp_avg_sq'", "]", "\n", "beta1", ",", "beta2", "=", "group", "[", "'betas'", "]", "\n", "\n", "state", "[", "'step'", "]", "+=", "1", "\n", "\n", "out_p", "=", "p", ".", "data", "\n", "fused_adam_cuda", ".", "adam", "(", "p_data_fp32", ",", "\n", "out_p", ",", "\n", "exp_avg", ",", "\n", "exp_avg_sq", ",", "\n", "grad", ",", "\n", "group", "[", "'lr'", "]", ",", "\n", "beta1", ",", "\n", "beta2", ",", "\n", "group", "[", "'eps'", "]", ",", "\n", "combined_scale", ",", "\n", "state", "[", "'step'", "]", ",", "\n", "self", ".", "eps_mode", ",", "\n", "bias_correction", ",", "\n", "group", "[", "'weight_decay'", "]", ")", "\n", "\n", "", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.__init__.build_optimizer": [[29, 32], ["list", "_build_optimizer", "filter"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.trainer.Trainer._build_optimizer"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.nag.FairseqNAG.__init__": [[14, 17], ["FairseqOptimizer.__init__", "nag.NAG"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "params", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "params", ")", "\n", "self", ".", "_optimizer", "=", "NAG", "(", "params", ",", "**", "self", ".", "optimizer_config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.nag.FairseqNAG.add_args": [[18, 26], ["parser.add_argument", "parser.add_argument"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add optimizer-specific arguments to the parser.\"\"\"", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'--momentum'", ",", "default", "=", "0.99", ",", "type", "=", "float", ",", "metavar", "=", "'M'", ",", "\n", "help", "=", "'momentum factor'", ")", "\n", "parser", ".", "add_argument", "(", "'--weight-decay'", ",", "'--wd'", ",", "default", "=", "0.0", ",", "type", "=", "float", ",", "metavar", "=", "'WD'", ",", "\n", "help", "=", "'weight decay'", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.nag.FairseqNAG.optimizer_config": [[28, 40], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "optimizer_config", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Return a kwarg dictionary that will be used to override optimizer\n        args stored in checkpoints. This allows us to load a checkpoint and\n        resume training using a different set of optimizer args, e.g., with a\n        different learning rate.\n        \"\"\"", "\n", "return", "{", "\n", "'lr'", ":", "self", ".", "args", ".", "lr", "[", "0", "]", ",", "\n", "'momentum'", ":", "self", ".", "args", ".", "momentum", ",", "\n", "'weight_decay'", ":", "self", ".", "args", ".", "weight_decay", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.nag.NAG.__init__": [[44, 47], ["dict", "torch.optim.optimizer.Optimizer.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["    ", "def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "required", ",", "momentum", "=", "0", ",", "weight_decay", "=", "0", ")", ":", "\n", "        ", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "lr_old", "=", "lr", ",", "momentum", "=", "momentum", ",", "weight_decay", "=", "weight_decay", ")", "\n", "super", "(", "NAG", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.nag.NAG.supports_memory_efficient_fp16": [[48, 51], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "supports_memory_efficient_fp16", "(", "self", ")", ":", "\n", "        ", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.nag.NAG.step": [[52, 97], ["closure", "group.get", "p.data.float", "p.grad.data.float", "p.data.float.add_", "p.data.float.add_", "buf.mul_().add_", "p.data.copy_", "torch.zeros_like", "param_state[].type_as", "p.data.float.mul_", "buf.mul_"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "weight_decay", "=", "group", "[", "'weight_decay'", "]", "\n", "momentum", "=", "group", "[", "'momentum'", "]", "\n", "lr", "=", "group", "[", "'lr'", "]", "\n", "lr_old", "=", "group", ".", "get", "(", "'lr_old'", ",", "lr", ")", "\n", "lr_correct", "=", "lr", "/", "lr_old", "\n", "\n", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "\n", "", "p_data_fp32", "=", "p", ".", "data", ".", "float", "(", ")", "\n", "\n", "d_p", "=", "p", ".", "grad", ".", "data", ".", "float", "(", ")", "\n", "param_state", "=", "self", ".", "state", "[", "p", "]", "\n", "if", "'momentum_buffer'", "not", "in", "param_state", ":", "\n", "                    ", "param_state", "[", "'momentum_buffer'", "]", "=", "torch", ".", "zeros_like", "(", "d_p", ")", "\n", "", "else", ":", "\n", "                    ", "param_state", "[", "'momentum_buffer'", "]", "=", "param_state", "[", "'momentum_buffer'", "]", ".", "type_as", "(", "d_p", ")", "\n", "\n", "", "buf", "=", "param_state", "[", "'momentum_buffer'", "]", "\n", "\n", "if", "weight_decay", "!=", "0", ":", "\n", "                    ", "p_data_fp32", ".", "mul_", "(", "1", "-", "lr", "*", "weight_decay", ")", "\n", "", "p_data_fp32", ".", "add_", "(", "momentum", "*", "momentum", "*", "lr_correct", ",", "buf", ")", "\n", "p_data_fp32", ".", "add_", "(", "-", "(", "1", "+", "momentum", ")", "*", "lr", ",", "d_p", ")", "\n", "\n", "buf", ".", "mul_", "(", "momentum", "*", "lr_correct", ")", ".", "add_", "(", "-", "lr", ",", "d_p", ")", "\n", "\n", "p", ".", "data", ".", "copy_", "(", "p_data_fp32", ")", "\n", "\n", "", "group", "[", "'lr_old'", "]", "=", "lr", "\n", "\n", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.sgd.SGD.__init__": [[13, 16], ["FairseqOptimizer.__init__", "torch.optim.SGD"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "params", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "params", ")", "\n", "self", ".", "_optimizer", "=", "torch", ".", "optim", ".", "SGD", "(", "params", ",", "**", "self", ".", "optimizer_config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.sgd.SGD.add_args": [[17, 25], ["parser.add_argument", "parser.add_argument"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add optimizer-specific arguments to the parser.\"\"\"", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'--momentum'", ",", "default", "=", "0.0", ",", "type", "=", "float", ",", "metavar", "=", "'M'", ",", "\n", "help", "=", "'momentum factor'", ")", "\n", "parser", ".", "add_argument", "(", "'--weight-decay'", ",", "'--wd'", ",", "default", "=", "0.0", ",", "type", "=", "float", ",", "metavar", "=", "'WD'", ",", "\n", "help", "=", "'weight decay'", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.sgd.SGD.optimizer_config": [[27, 39], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "optimizer_config", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Return a kwarg dictionary that will be used to override optimizer\n        args stored in checkpoints. This allows us to load a checkpoint and\n        resume training using a different set of optimizer args, e.g., with a\n        different learning rate.\n        \"\"\"", "\n", "return", "{", "\n", "'lr'", ":", "self", ".", "args", ".", "lr", "[", "0", "]", ",", "\n", "'momentum'", ":", "self", ".", "args", ".", "momentum", ",", "\n", "'weight_decay'", ":", "self", ".", "args", ".", "weight_decay", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.adafactor.FairseqAdafactor.__init__": [[15, 18], ["FairseqOptimizer.__init__", "adafactor.Adafactor"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "params", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "params", ")", "\n", "self", ".", "_optimizer", "=", "Adafactor", "(", "params", ",", "**", "self", ".", "optimizer_config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.adafactor.FairseqAdafactor.add_args": [[19, 39], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add optimizer-specific arguments to the parser.\"\"\"", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'--adafactor-eps'", ",", "default", "=", "'(1e-30, 1e-3)'", ",", "metavar", "=", "\"E\"", ",", "\n", "help", "=", "'epsilons for Adafactor optimizer'", ")", "\n", "parser", ".", "add_argument", "(", "'--clip-threshold'", ",", "type", "=", "float", ",", "default", "=", "1.0", ",", "metavar", "=", "\"C\"", ",", "\n", "help", "=", "'threshold for clipping update root mean square'", ")", "\n", "parser", ".", "add_argument", "(", "'--decay-rate'", ",", "type", "=", "float", ",", "default", "=", "-", "0.8", ",", "metavar", "=", "\"D\"", ",", "\n", "help", "=", "'decay rate of the second moment estimator'", ")", "\n", "parser", ".", "add_argument", "(", "'--beta1'", ",", "type", "=", "float", ",", "default", "=", "None", ",", "metavar", "=", "\"B\"", ",", "\n", "help", "=", "'beta for first moment estimator. Optional'", ")", "\n", "parser", ".", "add_argument", "(", "'--scale-parameter'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'scale learning rate by root mean square of parameter.'", ")", "\n", "parser", ".", "add_argument", "(", "'--weight-decay'", ",", "'--wd'", ",", "default", "=", "0.0", ",", "type", "=", "float", ",", "metavar", "=", "'WD'", ",", "\n", "help", "=", "'weight decay'", ")", "\n", "parser", ".", "add_argument", "(", "'--warmup-init'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'use relative step for warm-up learning rate schedule'", ")", "\n", "parser", ".", "add_argument", "(", "'--relative-step'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'set learning rate to inverse square root of timestep.'", "\n", "'If false, external learning rate applied'", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.adafactor.FairseqAdafactor.optimizer_config": [[42, 62], ["eval"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval"], ["", "@", "property", "\n", "def", "optimizer_config", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Return a kwarg dictionary that will be used to override optimizer\n        args stored in checkpoints. This allows us to load a checkpoint and\n        resume training using a different set of optimizer args, e.g., with a\n        different learning rate.\n        Note : Convergence issues empirically observed with fp16 on.\n               Might require search for appropriate configuration.\n        \"\"\"", "\n", "return", "{", "\n", "'lr'", ":", "self", ".", "args", ".", "lr", "[", "0", "]", ",", "\n", "'eps'", ":", "eval", "(", "self", ".", "args", ".", "adafactor_eps", ")", ",", "\n", "'clip_threshold'", ":", "self", ".", "args", ".", "clip_threshold", ",", "\n", "'beta1'", ":", "self", ".", "args", ".", "beta1", ",", "\n", "'decay_rate'", ":", "self", ".", "args", ".", "decay_rate", ",", "\n", "'scale_parameter'", ":", "self", ".", "args", ".", "scale_parameter", ",", "\n", "'weight_decay'", ":", "self", ".", "args", ".", "weight_decay", ",", "\n", "'relative_step'", ":", "self", ".", "args", ".", "relative_step", ",", "\n", "'warmup_init'", ":", "self", ".", "args", ".", "warmup_init", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.adafactor.Adafactor.__init__": [[93, 100], ["dict", "super().__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "None", ",", "eps", "=", "(", "1e-30", ",", "1e-3", ")", ",", "clip_threshold", "=", "1.0", ",", "\n", "decay_rate", "=", "-", "0.8", ",", "beta1", "=", "None", ",", "weight_decay", "=", "0.0", ",", "scale_parameter", "=", "True", ",", "\n", "relative_step", "=", "True", ",", "warmup_init", "=", "False", ")", ":", "\n", "        ", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "eps", "=", "eps", ",", "clip_threshold", "=", "clip_threshold", ",", "decay_rate", "=", "decay_rate", ",", "\n", "beta1", "=", "beta1", ",", "weight_decay", "=", "weight_decay", ",", "scale_parameter", "=", "scale_parameter", ",", "\n", "relative_step", "=", "relative_step", ",", "warmup_init", "=", "warmup_init", ")", "\n", "super", "(", "Adafactor", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.adafactor.Adafactor.supports_memory_efficient_fp16": [[101, 104], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "supports_memory_efficient_fp16", "(", "self", ")", ":", "\n", "        ", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.adafactor.Adafactor._get_lr": [[105, 114], ["min", "max", "math.sqrt"], "methods", ["None"], ["", "def", "_get_lr", "(", "self", ",", "param_group", ",", "param_state", ")", ":", "\n", "        ", "rel_step_sz", "=", "param_group", "[", "'lr'", "]", "\n", "if", "param_group", "[", "'relative_step'", "]", ":", "\n", "            ", "min_step", "=", "1e-6", "*", "param_state", "[", "'step'", "]", "if", "param_group", "[", "'warmup_init'", "]", "else", "1e-2", "\n", "rel_step_sz", "=", "min", "(", "min_step", ",", "1.0", "/", "math", ".", "sqrt", "(", "param_state", "[", "'step'", "]", ")", ")", "\n", "", "param_scale", "=", "1.0", "\n", "if", "param_group", "[", "'scale_parameter'", "]", ":", "\n", "            ", "param_scale", "=", "max", "(", "param_group", "[", "'eps'", "]", "[", "1", "]", ",", "param_state", "[", "'RMS'", "]", ")", "\n", "", "return", "param_scale", "*", "rel_step_sz", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.adafactor.Adafactor._get_options": [[115, 119], ["len"], "methods", ["None"], ["", "def", "_get_options", "(", "self", ",", "param_group", ",", "param_shape", ")", ":", "\n", "        ", "factored", "=", "len", "(", "param_shape", ")", ">=", "2", "\n", "use_first_moment", "=", "param_group", "[", "'beta1'", "]", "is", "not", "None", "\n", "return", "factored", ",", "use_first_moment", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.adafactor.Adafactor._rms": [[120, 122], ["tensor.norm", "tensor.numel"], "methods", ["None"], ["", "def", "_rms", "(", "self", ",", "tensor", ")", ":", "\n", "        ", "return", "tensor", ".", "norm", "(", "2", ")", "/", "(", "tensor", ".", "numel", "(", ")", "**", "0.5", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.adafactor.Adafactor._approx_sq_grad": [[123, 127], ["exp_avg_sq_col.unsqueeze().rsqrt", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "exp_avg_sq_col.unsqueeze", "exp_avg_sq_row.mean"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.mean"], ["", "def", "_approx_sq_grad", "(", "self", ",", "exp_avg_sq_row", ",", "exp_avg_sq_col", ",", "output", ")", ":", "\n", "        ", "r_factor", "=", "(", "exp_avg_sq_row", "/", "exp_avg_sq_row", ".", "mean", "(", "dim", "=", "-", "1", ")", ")", ".", "rsqrt_", "(", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "c_factor", "=", "exp_avg_sq_col", ".", "unsqueeze", "(", "-", "2", ")", ".", "rsqrt", "(", ")", "\n", "torch", ".", "mul", "(", "r_factor", ",", "c_factor", ",", "out", "=", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.adafactor.Adafactor.step": [[128, 214], ["closure", "p.grad.data.float", "adafactor.Adafactor._get_options", "p.data.float", "adafactor.Adafactor._rms", "adafactor.Adafactor._get_lr", "update.div_", "update.mul_", "p.data.float.add_", "p.data.copy_", "RuntimeError", "len", "math.pow", "exp_avg_sq_row.mul_().add_", "exp_avg_sq_col.mul_().add_", "adafactor.Adafactor._approx_sq_grad", "update.mul_", "exp_avg_sq.mul_().add_", "torch.rsqrt().mul_", "torch.rsqrt().mul_", "torch.rsqrt().mul_", "torch.rsqrt().mul_", "max", "exp_avg.mul_().add_", "p.data.float.add_", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros().type_as", "torch.zeros().type_as", "torch.zeros().type_as", "torch.zeros().type_as", "torch.zeros().type_as", "torch.zeros().type_as", "torch.zeros().type_as", "torch.zeros().type_as", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "state[].type_as", "state[].type_as", "state[].type_as", "state[].type_as", "update.mean", "update.mean", "exp_avg_sq_row.mul_", "exp_avg_sq_col.mul_", "exp_avg_sq.mul_", "torch.rsqrt", "torch.rsqrt", "torch.rsqrt", "torch.rsqrt", "adafactor.Adafactor._rms", "exp_avg.mul_", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.adafactor.Adafactor._get_options", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.adafactor.Adafactor._rms", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.adafactor.Adafactor._get_lr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.adafactor.Adafactor._approx_sq_grad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.mean", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.mean", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.adafactor.Adafactor._rms"], ["", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", ".", "data", ".", "float", "(", ")", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "'Adafactor does not support sparse gradients.'", ")", "\n", "\n", "", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "grad_shape", "=", "grad", ".", "shape", "\n", "\n", "factored", ",", "use_first_moment", "=", "self", ".", "_get_options", "(", "group", ",", "grad_shape", ")", "\n", "# State Initialization", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "\n", "if", "use_first_moment", ":", "\n", "# Exponential moving average of gradient values", "\n", "                        ", "state", "[", "'exp_avg'", "]", "=", "torch", ".", "zeros_like", "(", "grad", ")", "\n", "", "if", "factored", ":", "\n", "                        ", "state", "[", "'exp_avg_sq_row'", "]", "=", "torch", ".", "zeros", "(", "grad_shape", "[", ":", "-", "1", "]", ")", ".", "type_as", "(", "grad", ")", "\n", "state", "[", "'exp_avg_sq_col'", "]", "=", "torch", ".", "zeros", "(", "grad_shape", "[", ":", "-", "2", "]", "+", "grad_shape", "[", "-", "1", ":", "]", ")", ".", "type_as", "(", "grad", ")", "\n", "", "else", ":", "\n", "                        ", "state", "[", "'exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "grad", ")", "\n", "\n", "", "state", "[", "'RMS'", "]", "=", "0", "\n", "", "else", ":", "\n", "                    ", "if", "use_first_moment", ":", "\n", "                        ", "state", "[", "'exp_avg'", "]", "=", "state", "[", "'exp_avg'", "]", ".", "type_as", "(", "grad", ")", "\n", "", "if", "factored", ":", "\n", "                        ", "state", "[", "'exp_avg_sq_row'", "]", "=", "state", "[", "'exp_avg_sq_row'", "]", ".", "type_as", "(", "grad", ")", "\n", "state", "[", "'exp_avg_sq_col'", "]", "=", "state", "[", "'exp_avg_sq_col'", "]", ".", "type_as", "(", "grad", ")", "\n", "", "else", ":", "\n", "                        ", "state", "[", "'exp_avg_sq'", "]", "=", "state", "[", "'exp_avg_sq'", "]", ".", "type_as", "(", "grad", ")", "\n", "\n", "", "", "p_data_fp32", "=", "p", ".", "data", ".", "float", "(", ")", "\n", "\n", "state", "[", "'step'", "]", "+=", "1", "\n", "state", "[", "'RMS'", "]", "=", "self", ".", "_rms", "(", "p_data_fp32", ")", "\n", "group", "[", "'lr'", "]", "=", "self", ".", "_get_lr", "(", "group", ",", "state", ")", "\n", "\n", "beta2t", "=", "1.0", "-", "math", ".", "pow", "(", "state", "[", "'step'", "]", ",", "group", "[", "'decay_rate'", "]", ")", "\n", "update", "=", "(", "grad", "**", "2", ")", "+", "group", "[", "'eps'", "]", "[", "0", "]", "\n", "if", "factored", ":", "\n", "                    ", "exp_avg_sq_row", "=", "state", "[", "'exp_avg_sq_row'", "]", "\n", "exp_avg_sq_col", "=", "state", "[", "'exp_avg_sq_col'", "]", "\n", "\n", "exp_avg_sq_row", ".", "mul_", "(", "beta2t", ")", ".", "add_", "(", "1.0", "-", "beta2t", ",", "update", ".", "mean", "(", "dim", "=", "-", "1", ")", ")", "\n", "exp_avg_sq_col", ".", "mul_", "(", "beta2t", ")", ".", "add_", "(", "1.0", "-", "beta2t", ",", "update", ".", "mean", "(", "dim", "=", "-", "2", ")", ")", "\n", "\n", "# Approximation of exponential moving average of square of gradient", "\n", "self", ".", "_approx_sq_grad", "(", "exp_avg_sq_row", ",", "exp_avg_sq_col", ",", "update", ")", "\n", "update", ".", "mul_", "(", "grad", ")", "\n", "", "else", ":", "\n", "                    ", "exp_avg_sq", "=", "state", "[", "'exp_avg_sq'", "]", "\n", "\n", "exp_avg_sq", ".", "mul_", "(", "beta2t", ")", ".", "add_", "(", "1.0", "-", "beta2t", ",", "update", ")", "\n", "torch", ".", "rsqrt", "(", "exp_avg_sq", ",", "out", "=", "update", ")", ".", "mul_", "(", "grad", ")", "\n", "\n", "", "update", ".", "div_", "(", "max", "(", "1.0", ",", "self", ".", "_rms", "(", "update", ")", "/", "group", "[", "'clip_threshold'", "]", ")", ")", "\n", "update", ".", "mul_", "(", "group", "[", "'lr'", "]", ")", "\n", "\n", "if", "use_first_moment", ":", "\n", "                    ", "exp_avg", "=", "state", "[", "'exp_avg'", "]", "\n", "exp_avg", ".", "mul_", "(", "group", "[", "'beta1'", "]", ")", ".", "add_", "(", "1", "-", "group", "[", "'beta1'", "]", ",", "update", ")", "\n", "update", "=", "exp_avg", "\n", "\n", "", "if", "group", "[", "'weight_decay'", "]", "!=", "0", ":", "\n", "                    ", "p_data_fp32", ".", "add_", "(", "-", "group", "[", "'weight_decay'", "]", "*", "group", "[", "'lr'", "]", ",", "p_data_fp32", ")", "\n", "\n", "", "p_data_fp32", ".", "add_", "(", "-", "update", ")", "\n", "\n", "p", ".", "data", ".", "copy_", "(", "p_data_fp32", ")", "\n", "\n", "", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.reduce_lr_on_plateau.ReduceLROnPlateau.__init__": [[15, 25], ["FairseqLRScheduler.__init__", "torch.optim.lr_scheduler.ReduceLROnPlateau", "len", "ValueError"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["def", "__init__", "(", "self", ",", "args", ",", "optimizer", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "optimizer", ")", "\n", "if", "len", "(", "args", ".", "lr", ")", ">", "1", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "'Cannot use a fixed learning rate schedule with reduce_lr_on_plateau.'", "\n", "' Consider --lr-scheduler=fixed instead.'", "\n", ")", "\n", "", "self", ".", "lr_scheduler", "=", "torch", ".", "optim", ".", "lr_scheduler", ".", "ReduceLROnPlateau", "(", "\n", "self", ".", "optimizer", ".", "optimizer", ",", "patience", "=", "0", ",", "factor", "=", "args", ".", "lr_shrink", ",", "\n", "threshold", "=", "args", ".", "lr_threshold", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.reduce_lr_on_plateau.ReduceLROnPlateau.add_args": [[26, 35], ["parser.add_argument", "parser.add_argument"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add arguments to the parser for this LR scheduler.\"\"\"", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'--lr-shrink'", ",", "default", "=", "0.1", ",", "type", "=", "float", ",", "metavar", "=", "'LS'", ",", "\n", "help", "=", "'shrink factor for annealing, lr_new = (lr * lr_shrink)'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr-threshold'", ",", "default", "=", "1e-4", ",", "type", "=", "float", ",", "metavar", "=", "'LT'", ",", "\n", "help", "=", "'Threshold for measuring the new optimum, \\\n                            to only focus on significant changes'", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.reduce_lr_on_plateau.ReduceLROnPlateau.state_dict": [[37, 42], ["None"], "methods", ["None"], ["", "def", "state_dict", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the LR scheduler state dict.\"\"\"", "\n", "return", "{", "\n", "'best'", ":", "self", ".", "lr_scheduler", ".", "best", ",", "\n", "'last_epoch'", ":", "self", ".", "lr_scheduler", ".", "last_epoch", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.reduce_lr_on_plateau.ReduceLROnPlateau.load_state_dict": [[44, 49], ["None"], "methods", ["None"], ["", "def", "load_state_dict", "(", "self", ",", "state_dict", ")", ":", "\n", "        ", "\"\"\"Load an LR scheduler state dict.\"\"\"", "\n", "self", ".", "lr_scheduler", ".", "best", "=", "state_dict", "[", "'best'", "]", "\n", "if", "'last_epoch'", "in", "state_dict", ":", "\n", "            ", "self", ".", "lr_scheduler", ".", "last_epoch", "=", "state_dict", "[", "'last_epoch'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.reduce_lr_on_plateau.ReduceLROnPlateau.step": [[50, 57], ["reduce_lr_on_plateau.ReduceLROnPlateau.optimizer.get_lr", "reduce_lr_on_plateau.ReduceLROnPlateau.lr_scheduler.step"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.get_lr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.polynomial_decay_schedule.PolynomialDecaySchedule.step"], ["", "", "def", "step", "(", "self", ",", "epoch", ",", "val_loss", "=", "None", ")", ":", "\n", "        ", "\"\"\"Update the learning rate at the end of the given epoch.\"\"\"", "\n", "if", "val_loss", "is", "not", "None", ":", "\n", "            ", "self", ".", "lr_scheduler", ".", "step", "(", "val_loss", ",", "epoch", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "lr_scheduler", ".", "last_epoch", "=", "epoch", "\n", "", "return", "self", ".", "optimizer", ".", "get_lr", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.inverse_square_root_schedule.InverseSquareRootSchedule.__init__": [[29, 49], ["FairseqLRScheduler.__init__", "inverse_square_root_schedule.InverseSquareRootSchedule.optimizer.set_lr", "len", "ValueError"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.set_lr"], ["def", "__init__", "(", "self", ",", "args", ",", "optimizer", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "optimizer", ")", "\n", "if", "len", "(", "args", ".", "lr", ")", ">", "1", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "'Cannot use a fixed learning rate schedule with inverse_sqrt.'", "\n", "' Consider --lr-scheduler=fixed instead.'", "\n", ")", "\n", "", "warmup_end_lr", "=", "args", ".", "lr", "[", "0", "]", "\n", "if", "args", ".", "warmup_init_lr", "<", "0", ":", "\n", "            ", "args", ".", "warmup_init_lr", "=", "0", "if", "args", ".", "warmup_updates", ">", "0", "else", "warmup_end_lr", "\n", "\n", "# linearly warmup for the first args.warmup_updates", "\n", "", "self", ".", "lr_step", "=", "(", "warmup_end_lr", "-", "args", ".", "warmup_init_lr", ")", "/", "args", ".", "warmup_updates", "\n", "\n", "# then, decay prop. to the inverse square root of the update number", "\n", "self", ".", "decay_factor", "=", "warmup_end_lr", "*", "args", ".", "warmup_updates", "**", "0.5", "\n", "\n", "# initial learning rate", "\n", "self", ".", "lr", "=", "args", ".", "warmup_init_lr", "\n", "self", ".", "optimizer", ".", "set_lr", "(", "self", ".", "lr", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.inverse_square_root_schedule.InverseSquareRootSchedule.add_args": [[50, 58], ["parser.add_argument", "parser.add_argument"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add arguments to the parser for this LR scheduler.\"\"\"", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'--warmup-updates'", ",", "default", "=", "4000", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'warmup the learning rate linearly for the first N updates'", ")", "\n", "parser", ".", "add_argument", "(", "'--warmup-init-lr'", ",", "default", "=", "-", "1", ",", "type", "=", "float", ",", "metavar", "=", "'LR'", ",", "\n", "help", "=", "'initial learning rate during warmup phase; default is args.lr'", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.inverse_square_root_schedule.InverseSquareRootSchedule.step": [[60, 65], ["super().step", "inverse_square_root_schedule.InverseSquareRootSchedule.optimizer.get_lr"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.polynomial_decay_schedule.PolynomialDecaySchedule.step", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.get_lr"], ["", "def", "step", "(", "self", ",", "epoch", ",", "val_loss", "=", "None", ")", ":", "\n", "        ", "\"\"\"Update the learning rate at the end of the given epoch.\"\"\"", "\n", "super", "(", ")", ".", "step", "(", "epoch", ",", "val_loss", ")", "\n", "# we don't change the learning rate at epoch boundaries", "\n", "return", "self", ".", "optimizer", ".", "get_lr", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.inverse_square_root_schedule.InverseSquareRootSchedule.step_update": [[66, 74], ["inverse_square_root_schedule.InverseSquareRootSchedule.optimizer.set_lr"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.set_lr"], ["", "def", "step_update", "(", "self", ",", "num_updates", ")", ":", "\n", "        ", "\"\"\"Update the learning rate after each update.\"\"\"", "\n", "if", "num_updates", "<", "self", ".", "args", ".", "warmup_updates", ":", "\n", "            ", "self", ".", "lr", "=", "self", ".", "args", ".", "warmup_init_lr", "+", "num_updates", "*", "self", ".", "lr_step", "\n", "", "else", ":", "\n", "            ", "self", ".", "lr", "=", "self", ".", "decay_factor", "*", "num_updates", "**", "-", "0.5", "\n", "", "self", ".", "optimizer", ".", "set_lr", "(", "self", ".", "lr", ")", "\n", "return", "self", ".", "lr", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.__init__": [[11, 18], ["object.__init__", "isinstance", "ValueError"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "optimizer", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "if", "not", "isinstance", "(", "optimizer", ",", "FairseqOptimizer", ")", ":", "\n", "            ", "raise", "ValueError", "(", "'optimizer must be an instance of FairseqOptimizer'", ")", "\n", "", "self", ".", "args", "=", "args", "\n", "self", ".", "optimizer", "=", "optimizer", "\n", "self", ".", "best", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.add_args": [[19, 23], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add arguments to the parser for this LR scheduler.\"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.state_dict": [[24, 27], ["None"], "methods", ["None"], ["", "def", "state_dict", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the LR scheduler state dict.\"\"\"", "\n", "return", "{", "'best'", ":", "self", ".", "best", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.load_state_dict": [[28, 31], ["None"], "methods", ["None"], ["", "def", "load_state_dict", "(", "self", ",", "state_dict", ")", ":", "\n", "        ", "\"\"\"Load an LR scheduler state dict.\"\"\"", "\n", "self", ".", "best", "=", "state_dict", "[", "'best'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.step": [[32, 39], ["min"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "epoch", ",", "val_loss", "=", "None", ")", ":", "\n", "        ", "\"\"\"Update the learning rate at the end of the given epoch.\"\"\"", "\n", "if", "val_loss", "is", "not", "None", ":", "\n", "            ", "if", "self", ".", "best", "is", "None", ":", "\n", "                ", "self", ".", "best", "=", "val_loss", "\n", "", "else", ":", "\n", "                ", "self", ".", "best", "=", "min", "(", "self", ".", "best", ",", "val_loss", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.step_update": [[40, 43], ["fairseq_lr_scheduler.FairseqLRScheduler.optimizer.get_lr"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.get_lr"], ["", "", "", "def", "step_update", "(", "self", ",", "num_updates", ")", ":", "\n", "        ", "\"\"\"Update the learning rate after each update.\"\"\"", "\n", "return", "self", ".", "optimizer", ".", "get_lr", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.fixed_schedule.FixedSchedule.__init__": [[13, 24], ["FairseqLRScheduler.__init__", "getattr"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["def", "__init__", "(", "self", ",", "args", ",", "optimizer", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "optimizer", ")", "\n", "\n", "# set defaults", "\n", "args", ".", "warmup_updates", "=", "getattr", "(", "args", ",", "'warmup_updates'", ",", "0", ")", "or", "0", "\n", "\n", "self", ".", "lr", "=", "args", ".", "lr", "[", "0", "]", "\n", "if", "args", ".", "warmup_updates", ">", "0", ":", "\n", "            ", "self", ".", "warmup_factor", "=", "1.", "/", "args", ".", "warmup_updates", "\n", "", "else", ":", "\n", "            ", "self", ".", "warmup_factor", "=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.fixed_schedule.FixedSchedule.add_args": [[25, 35], ["parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add arguments to the parser for this LR scheduler.\"\"\"", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'--force-anneal'", ",", "'--fa'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'force annealing at specified epoch'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr-shrink'", ",", "default", "=", "0.1", ",", "type", "=", "float", ",", "metavar", "=", "'LS'", ",", "\n", "help", "=", "'shrink factor for annealing, lr_new = (lr * lr_shrink)'", ")", "\n", "parser", ".", "add_argument", "(", "'--warmup-updates'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'warmup the learning rate linearly for the first N updates'", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.fixed_schedule.FixedSchedule.get_next_lr": [[37, 46], ["min", "len"], "methods", ["None"], ["", "def", "get_next_lr", "(", "self", ",", "epoch", ")", ":", "\n", "        ", "lrs", "=", "self", ".", "args", ".", "lr", "\n", "if", "self", ".", "args", ".", "force_anneal", "is", "None", "or", "epoch", "<", "self", ".", "args", ".", "force_anneal", ":", "\n", "# use fixed LR schedule", "\n", "            ", "next_lr", "=", "lrs", "[", "min", "(", "epoch", ",", "len", "(", "lrs", ")", "-", "1", ")", "]", "\n", "", "else", ":", "\n", "# annneal based on lr_shrink", "\n", "            ", "next_lr", "=", "lrs", "[", "-", "1", "]", "*", "self", ".", "args", ".", "lr_shrink", "**", "(", "epoch", "+", "1", "-", "self", ".", "args", ".", "force_anneal", ")", "\n", "", "return", "next_lr", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.fixed_schedule.FixedSchedule.step": [[47, 53], ["super().step", "fixed_schedule.FixedSchedule.get_next_lr", "fixed_schedule.FixedSchedule.optimizer.set_lr", "fixed_schedule.FixedSchedule.optimizer.get_lr"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.polynomial_decay_schedule.PolynomialDecaySchedule.step", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.polynomial_decay_schedule.PolynomialDecaySchedule.get_next_lr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.set_lr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.get_lr"], ["", "def", "step", "(", "self", ",", "epoch", ",", "val_loss", "=", "None", ")", ":", "\n", "        ", "\"\"\"Update the learning rate at the end of the given epoch.\"\"\"", "\n", "super", "(", ")", ".", "step", "(", "epoch", ",", "val_loss", ")", "\n", "self", ".", "lr", "=", "self", ".", "get_next_lr", "(", "epoch", ")", "\n", "self", ".", "optimizer", ".", "set_lr", "(", "self", ".", "warmup_factor", "*", "self", ".", "lr", ")", "\n", "return", "self", ".", "optimizer", ".", "get_lr", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.fixed_schedule.FixedSchedule.step_update": [[54, 60], ["fixed_schedule.FixedSchedule.optimizer.get_lr", "fixed_schedule.FixedSchedule.optimizer.set_lr", "float"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.get_lr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.set_lr"], ["", "def", "step_update", "(", "self", ",", "num_updates", ")", ":", "\n", "        ", "\"\"\"Update the learning rate after each update.\"\"\"", "\n", "if", "self", ".", "args", ".", "warmup_updates", ">", "0", "and", "num_updates", "<=", "self", ".", "args", ".", "warmup_updates", ":", "\n", "            ", "self", ".", "warmup_factor", "=", "num_updates", "/", "float", "(", "self", ".", "args", ".", "warmup_updates", ")", "\n", "self", ".", "optimizer", ".", "set_lr", "(", "self", ".", "warmup_factor", "*", "self", ".", "lr", ")", "\n", "", "return", "self", ".", "optimizer", ".", "get_lr", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.triangular_lr_scheduler.TriangularSchedule.__init__": [[18, 38], ["FairseqLRScheduler.__init__", "triangular_lr_scheduler.TriangularSchedule.optimizer.set_lr", "len", "ValueError"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.set_lr"], ["def", "__init__", "(", "self", ",", "args", ",", "optimizer", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "optimizer", ")", "\n", "if", "len", "(", "args", ".", "lr", ")", ">", "1", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "'Cannot use a fixed learning rate schedule with triangular.'", "\n", "' Consider --lr-scheduler=fixed instead.'", "\n", ")", "\n", "\n", "", "lr", "=", "args", ".", "lr", "[", "0", "]", "\n", "\n", "assert", "args", ".", "max_lr", ">", "lr", ",", "'max_lr must be more than lr'", "\n", "self", ".", "min_lr", "=", "lr", "\n", "self", ".", "max_lr", "=", "args", ".", "max_lr", "\n", "self", ".", "stepsize", "=", "args", ".", "lr_period_updates", "//", "2", "\n", "self", ".", "lr_shrink", "=", "args", ".", "lr_shrink", "\n", "self", ".", "shrink_min", "=", "args", ".", "shrink_min", "\n", "\n", "# initial learning rate", "\n", "self", ".", "lr", "=", "self", ".", "min_lr", "\n", "self", ".", "optimizer", ".", "set_lr", "(", "self", ".", "lr", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.triangular_lr_scheduler.TriangularSchedule.add_args": [[39, 51], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add arguments to the parser for this LR scheduler.\"\"\"", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'--max-lr'", ",", "required", "=", "True", ",", "type", "=", "float", ",", "metavar", "=", "'LR'", ",", "\n", "help", "=", "'max learning rate, must be more than args.lr'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr-period-updates'", ",", "default", "=", "5000", ",", "type", "=", "float", ",", "metavar", "=", "'LR'", ",", "\n", "help", "=", "'initial number of updates per period (cycle length)'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr-shrink'", ",", "default", "=", "0.1", ",", "type", "=", "float", ",", "metavar", "=", "'LS'", ",", "\n", "help", "=", "'shrink factor for annealing'", ")", "\n", "parser", ".", "add_argument", "(", "'--shrink-min'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'if set, also shrinks min lr'", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.triangular_lr_scheduler.TriangularSchedule.step": [[53, 58], ["super().step", "triangular_lr_scheduler.TriangularSchedule.optimizer.get_lr"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.polynomial_decay_schedule.PolynomialDecaySchedule.step", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.get_lr"], ["", "def", "step", "(", "self", ",", "epoch", ",", "val_loss", "=", "None", ")", ":", "\n", "        ", "\"\"\"Update the learning rate at the end of the given epoch.\"\"\"", "\n", "super", "(", ")", ".", "step", "(", "epoch", ",", "val_loss", ")", "\n", "# we don't change the learning rate at epoch boundaries", "\n", "return", "self", ".", "optimizer", ".", "get_lr", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.triangular_lr_scheduler.TriangularSchedule.step_update": [[59, 75], ["math.floor", "abs", "triangular_lr_scheduler.TriangularSchedule.optimizer.set_lr", "max"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.set_lr"], ["", "def", "step_update", "(", "self", ",", "num_updates", ")", ":", "\n", "        ", "\"\"\"Update the learning rate after each update.\"\"\"", "\n", "cycle", "=", "math", ".", "floor", "(", "num_updates", "/", "(", "2", "*", "self", ".", "stepsize", ")", ")", "\n", "\n", "lr_shrink", "=", "self", ".", "lr_shrink", "**", "cycle", "\n", "max_lr", "=", "self", ".", "max_lr", "*", "lr_shrink", "\n", "if", "self", ".", "shrink_min", ":", "\n", "            ", "min_lr", "=", "self", ".", "min_lr", "*", "lr_shrink", "\n", "", "else", ":", "\n", "            ", "min_lr", "=", "self", ".", "min_lr", "\n", "\n", "", "x", "=", "abs", "(", "num_updates", "/", "self", ".", "stepsize", "-", "2", "*", "(", "cycle", "+", "1", ")", "+", "1", ")", "\n", "self", ".", "lr", "=", "min_lr", "+", "(", "max_lr", "-", "min_lr", ")", "*", "max", "(", "0", ",", "(", "1", "-", "x", ")", ")", "\n", "\n", "self", ".", "optimizer", ".", "set_lr", "(", "self", ".", "lr", ")", "\n", "return", "self", ".", "lr", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.cosine_lr_scheduler.CosineSchedule.__init__": [[35, 71], ["FairseqLRScheduler.__init__", "cosine_lr_scheduler.CosineSchedule.optimizer.set_lr", "len", "ValueError"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.set_lr"], ["def", "__init__", "(", "self", ",", "args", ",", "optimizer", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "optimizer", ")", "\n", "if", "len", "(", "args", ".", "lr", ")", ">", "1", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "'Cannot use a fixed learning rate schedule with cosine.'", "\n", "' Consider --lr-scheduler=fixed instead.'", "\n", ")", "\n", "\n", "", "warmup_end_lr", "=", "args", ".", "max_lr", "\n", "if", "args", ".", "warmup_init_lr", "<", "0", ":", "\n", "            ", "args", ".", "warmup_init_lr", "=", "args", ".", "lr", "[", "0", "]", "\n", "\n", "", "self", ".", "min_lr", "=", "args", ".", "lr", "[", "0", "]", "\n", "self", ".", "max_lr", "=", "args", ".", "max_lr", "\n", "\n", "assert", "self", ".", "max_lr", ">", "self", ".", "min_lr", ",", "'max_lr must be more than lr'", "\n", "\n", "self", ".", "t_mult", "=", "args", ".", "t_mult", "\n", "self", ".", "period", "=", "args", ".", "lr_period_updates", "\n", "\n", "if", "self", ".", "period", "<=", "0", ":", "\n", "            ", "assert", "args", ".", "max_update", ">=", "0", ",", "'Either --max_update or --lr-period-updates must be set'", "\n", "self", ".", "period", "=", "args", ".", "max_update", "-", "args", ".", "warmup_updates", "\n", "\n", "", "if", "args", ".", "warmup_updates", ">", "0", ":", "\n", "# linearly warmup for the first args.warmup_updates", "\n", "            ", "self", ".", "lr_step", "=", "(", "warmup_end_lr", "-", "args", ".", "warmup_init_lr", ")", "/", "args", ".", "warmup_updates", "\n", "", "else", ":", "\n", "            ", "self", ".", "lr_step", "=", "1", "\n", "\n", "", "self", ".", "warmup_updates", "=", "args", ".", "warmup_updates", "\n", "self", ".", "lr_shrink", "=", "args", ".", "lr_shrink", "\n", "\n", "# initial learning rate", "\n", "self", ".", "lr", "=", "args", ".", "warmup_init_lr", "\n", "self", ".", "optimizer", ".", "set_lr", "(", "self", ".", "lr", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.cosine_lr_scheduler.CosineSchedule.add_args": [[72, 88], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add arguments to the parser for this LR scheduler.\"\"\"", "\n", "# fmt: off", "\n", "parser", ".", "add_argument", "(", "'--warmup-updates'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'warmup the learning rate linearly for the first N updates'", ")", "\n", "parser", ".", "add_argument", "(", "'--warmup-init-lr'", ",", "default", "=", "-", "1", ",", "type", "=", "float", ",", "metavar", "=", "'LR'", ",", "\n", "help", "=", "'initial learning rate during warmup phase; default is args.lr'", ")", "\n", "parser", ".", "add_argument", "(", "'--max-lr'", ",", "type", "=", "float", ",", "metavar", "=", "'LR'", ",", "\n", "help", "=", "'max learning rate, must be more than args.lr'", ")", "\n", "parser", ".", "add_argument", "(", "'--t-mult'", ",", "default", "=", "1", ",", "type", "=", "float", ",", "metavar", "=", "'LR'", ",", "\n", "help", "=", "'factor to grow the length of each period'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr-period-updates'", ",", "default", "=", "-", "1", ",", "type", "=", "float", ",", "metavar", "=", "'LR'", ",", "\n", "help", "=", "'initial number of updates per period'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr-shrink'", ",", "default", "=", "0.1", ",", "type", "=", "float", ",", "metavar", "=", "'LS'", ",", "\n", "help", "=", "'shrink factor for annealing'", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.cosine_lr_scheduler.CosineSchedule.step": [[90, 95], ["super().step", "cosine_lr_scheduler.CosineSchedule.optimizer.get_lr"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.polynomial_decay_schedule.PolynomialDecaySchedule.step", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.get_lr"], ["", "def", "step", "(", "self", ",", "epoch", ",", "val_loss", "=", "None", ")", ":", "\n", "        ", "\"\"\"Update the learning rate at the end of the given epoch.\"\"\"", "\n", "super", "(", ")", ".", "step", "(", "epoch", ",", "val_loss", ")", "\n", "# we don't change the learning rate at epoch boundaries", "\n", "return", "self", ".", "optimizer", ".", "get_lr", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.cosine_lr_scheduler.CosineSchedule.step_update": [[96, 119], ["cosine_lr_scheduler.CosineSchedule.optimizer.set_lr", "math.floor", "math.floor", "math.log", "math.cos"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.set_lr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.log"], ["", "def", "step_update", "(", "self", ",", "num_updates", ")", ":", "\n", "        ", "\"\"\"Update the learning rate after each update.\"\"\"", "\n", "if", "num_updates", "<", "self", ".", "args", ".", "warmup_updates", ":", "\n", "            ", "self", ".", "lr", "=", "self", ".", "args", ".", "warmup_init_lr", "+", "num_updates", "*", "self", ".", "lr_step", "\n", "", "else", ":", "\n", "            ", "curr_updates", "=", "num_updates", "-", "self", ".", "args", ".", "warmup_updates", "\n", "if", "self", ".", "t_mult", "!=", "1", ":", "\n", "                ", "i", "=", "math", ".", "floor", "(", "math", ".", "log", "(", "1", "-", "curr_updates", "/", "self", ".", "period", "*", "(", "1", "-", "self", ".", "t_mult", ")", ",", "self", ".", "t_mult", ")", ")", "\n", "t_i", "=", "self", ".", "t_mult", "**", "i", "*", "self", ".", "period", "\n", "t_curr", "=", "curr_updates", "-", "(", "1", "-", "self", ".", "t_mult", "**", "i", ")", "/", "(", "1", "-", "self", ".", "t_mult", ")", "*", "self", ".", "period", "\n", "", "else", ":", "\n", "                ", "i", "=", "math", ".", "floor", "(", "curr_updates", "/", "self", ".", "period", ")", "\n", "t_i", "=", "self", ".", "period", "\n", "t_curr", "=", "curr_updates", "-", "(", "self", ".", "period", "*", "i", ")", "\n", "\n", "", "lr_shrink", "=", "self", ".", "lr_shrink", "**", "i", "\n", "min_lr", "=", "self", ".", "min_lr", "*", "lr_shrink", "\n", "max_lr", "=", "self", ".", "max_lr", "*", "lr_shrink", "\n", "\n", "self", ".", "lr", "=", "min_lr", "+", "0.5", "*", "(", "max_lr", "-", "min_lr", ")", "*", "(", "1", "+", "math", ".", "cos", "(", "math", ".", "pi", "*", "t_curr", "/", "t_i", ")", ")", "\n", "\n", "", "self", ".", "optimizer", ".", "set_lr", "(", "self", ".", "lr", ")", "\n", "return", "self", ".", "lr", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.polynomial_decay_schedule.PolynomialDecaySchedule.__init__": [[13, 28], ["FairseqLRScheduler.__init__", "polynomial_decay_schedule.PolynomialDecaySchedule.optimizer.set_lr", "getattr"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.set_lr"], ["def", "__init__", "(", "self", ",", "args", ",", "optimizer", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ",", "optimizer", ")", "\n", "\n", "# set defaults", "\n", "args", ".", "warmup_updates", "=", "getattr", "(", "args", ",", "'warmup_updates'", ",", "0", ")", "or", "0", "\n", "\n", "self", ".", "lr", "=", "args", ".", "lr", "[", "0", "]", "\n", "if", "args", ".", "warmup_updates", ">", "0", ":", "\n", "            ", "self", ".", "warmup_factor", "=", "1.", "/", "args", ".", "warmup_updates", "\n", "", "else", ":", "\n", "            ", "self", ".", "warmup_factor", "=", "1", "\n", "", "self", ".", "end_learning_rate", "=", "args", ".", "end_learning_rate", "\n", "self", ".", "total_num_update", "=", "args", ".", "total_num_update", "\n", "self", ".", "power", "=", "args", ".", "power", "\n", "self", ".", "optimizer", ".", "set_lr", "(", "self", ".", "warmup_factor", "*", "self", ".", "lr", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.polynomial_decay_schedule.PolynomialDecaySchedule.add_args": [[29, 39], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add arguments to the parser for this LR scheduler.\"\"\"", "\n", "parser", ".", "add_argument", "(", "'--force-anneal'", ",", "'--fa'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'force annealing at specified epoch'", ")", "\n", "parser", ".", "add_argument", "(", "'--warmup-updates'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'warmup the learning rate linearly for the first N updates'", ")", "\n", "parser", ".", "add_argument", "(", "'--end-learning-rate'", ",", "default", "=", "0.0", ",", "type", "=", "float", ")", "\n", "parser", ".", "add_argument", "(", "'--power'", ",", "default", "=", "1.0", ",", "type", "=", "float", ")", "\n", "parser", ".", "add_argument", "(", "'--total-num-update'", ",", "default", "=", "1000000", ",", "type", "=", "int", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.polynomial_decay_schedule.PolynomialDecaySchedule.get_next_lr": [[40, 49], ["polynomial_decay_schedule.PolynomialDecaySchedule.optimizer.get_lr", "min", "len"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.get_lr"], ["", "def", "get_next_lr", "(", "self", ",", "epoch", ")", ":", "\n", "        ", "lrs", "=", "self", ".", "args", ".", "lr", "\n", "if", "self", ".", "args", ".", "force_anneal", "is", "None", "or", "epoch", "<", "self", ".", "args", ".", "force_anneal", ":", "\n", "# use fixed LR schedule", "\n", "            ", "next_lr", "=", "lrs", "[", "min", "(", "epoch", ",", "len", "(", "lrs", ")", "-", "1", ")", "]", "\n", "", "else", ":", "\n", "# annneal based on lr_shrink", "\n", "            ", "next_lr", "=", "self", ".", "optimizer", ".", "get_lr", "(", ")", "\n", "", "return", "next_lr", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.polynomial_decay_schedule.PolynomialDecaySchedule.step": [[50, 56], ["super().step", "polynomial_decay_schedule.PolynomialDecaySchedule.get_next_lr", "polynomial_decay_schedule.PolynomialDecaySchedule.optimizer.set_lr", "polynomial_decay_schedule.PolynomialDecaySchedule.optimizer.get_lr"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.polynomial_decay_schedule.PolynomialDecaySchedule.step", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.polynomial_decay_schedule.PolynomialDecaySchedule.get_next_lr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.set_lr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.get_lr"], ["", "def", "step", "(", "self", ",", "epoch", ",", "val_loss", "=", "None", ")", ":", "\n", "        ", "\"\"\"Update the learning rate at the end of the given epoch.\"\"\"", "\n", "super", "(", ")", ".", "step", "(", "epoch", ",", "val_loss", ")", "\n", "self", ".", "lr", "=", "self", ".", "get_next_lr", "(", "epoch", ")", "\n", "self", ".", "optimizer", ".", "set_lr", "(", "self", ".", "warmup_factor", "*", "self", ".", "lr", ")", "\n", "return", "self", ".", "optimizer", ".", "get_lr", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.polynomial_decay_schedule.PolynomialDecaySchedule.step_update": [[57, 71], ["polynomial_decay_schedule.PolynomialDecaySchedule.optimizer.set_lr", "polynomial_decay_schedule.PolynomialDecaySchedule.optimizer.get_lr", "float"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.set_lr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.get_lr"], ["", "def", "step_update", "(", "self", ",", "num_updates", ")", ":", "\n", "        ", "\"\"\"Update the learning rate after each update.\"\"\"", "\n", "if", "self", ".", "args", ".", "warmup_updates", ">", "0", "and", "num_updates", "<=", "self", ".", "args", ".", "warmup_updates", ":", "\n", "            ", "self", ".", "warmup_factor", "=", "num_updates", "/", "float", "(", "self", ".", "args", ".", "warmup_updates", ")", "\n", "lr", "=", "self", ".", "warmup_factor", "*", "self", ".", "lr", "\n", "", "elif", "num_updates", ">=", "self", ".", "total_num_update", ":", "\n", "            ", "lr", "=", "self", ".", "end_learning_rate", "\n", "", "else", ":", "\n", "            ", "warmup", "=", "self", ".", "args", ".", "warmup_updates", "\n", "lr_range", "=", "self", ".", "lr", "-", "self", ".", "end_learning_rate", "\n", "pct_remaining", "=", "1", "-", "(", "num_updates", "-", "warmup", ")", "/", "(", "self", ".", "total_num_update", "-", "warmup", ")", "\n", "lr", "=", "lr_range", "*", "pct_remaining", "**", "(", "self", ".", "power", ")", "+", "self", ".", "end_learning_rate", "\n", "", "self", ".", "optimizer", ".", "set_lr", "(", "lr", ")", "\n", "return", "self", ".", "optimizer", ".", "get_lr", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.positional_embedding.PositionalEmbedding": [[12, 34], ["learned_positional_embedding.LearnedPositionalEmbedding", "torch.init.normal_", "sinusoidal_positional_embedding.SinusoidalPositionalEmbedding", "torch.init.constant_"], "function", ["None"], ["def", "PositionalEmbedding", "(", "\n", "num_embeddings", ":", "int", ",", "\n", "embedding_dim", ":", "int", ",", "\n", "padding_idx", ":", "int", ",", "\n", "learned", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "    ", "if", "learned", ":", "\n", "# if padding_idx is specified then offset the embedding ids by", "\n", "# this index and adjust num_embeddings appropriately", "\n", "# TODO: The right place for this offset would be inside", "\n", "# LearnedPositionalEmbedding. Move this there for a cleaner implementation.", "\n", "        ", "if", "padding_idx", "is", "not", "None", ":", "\n", "            ", "num_embeddings", "=", "num_embeddings", "+", "padding_idx", "+", "1", "\n", "", "m", "=", "LearnedPositionalEmbedding", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "m", ".", "weight", ",", "mean", "=", "0", ",", "std", "=", "embedding_dim", "**", "-", "0.5", ")", "\n", "if", "padding_idx", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "weight", "[", "padding_idx", "]", ",", "0", ")", "\n", "", "", "else", ":", "\n", "        ", "m", "=", "SinusoidalPositionalEmbedding", "(", "\n", "embedding_dim", ",", "padding_idx", ",", "init_size", "=", "num_embeddings", "+", "padding_idx", "+", "1", ",", "\n", ")", "\n", "", "return", "m", "\n", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_sentence_encoder_layer.TransformerSentenceEncoderLayer.__init__": [[23, 61], ["torch.Module.__init__", "fairseq.utils.get_activation_fn", "fairseq.modules.MultiheadAttention", "fairseq.modules.LayerNorm", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "fairseq.modules.LayerNorm"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.get_activation_fn", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.layer_norm.LayerNorm", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.layer_norm.LayerNorm"], ["def", "__init__", "(", "\n", "self", ",", "\n", "embedding_dim", ":", "float", "=", "768", ",", "\n", "ffn_embedding_dim", ":", "float", "=", "3072", ",", "\n", "num_attention_heads", ":", "float", "=", "8", ",", "\n", "dropout", ":", "float", "=", "0.1", ",", "\n", "attention_dropout", ":", "float", "=", "0.1", ",", "\n", "activation_dropout", ":", "float", "=", "0.1", ",", "\n", "activation_fn", ":", "str", "=", "'relu'", ",", "\n", "add_bias_kv", ":", "bool", "=", "False", ",", "\n", "add_zero_attn", ":", "bool", "=", "False", ",", "\n", "export", ":", "bool", "=", "False", ",", "\n", ")", "->", "None", ":", "\n", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# Initialize parameters", "\n", "self", ".", "embedding_dim", "=", "embedding_dim", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "activation_dropout", "=", "activation_dropout", "\n", "\n", "# Initialize blocks", "\n", "self", ".", "activation_fn", "=", "utils", ".", "get_activation_fn", "(", "activation_fn", ")", "\n", "self", ".", "self_attn", "=", "MultiheadAttention", "(", "\n", "self", ".", "embedding_dim", ",", "\n", "num_attention_heads", ",", "\n", "dropout", "=", "attention_dropout", ",", "\n", "add_bias_kv", "=", "add_bias_kv", ",", "\n", "add_zero_attn", "=", "add_zero_attn", ",", "\n", "self_attention", "=", "True", "\n", ")", "\n", "\n", "# layer norm associated with the self attention layer", "\n", "self", ".", "self_attn_layer_norm", "=", "LayerNorm", "(", "self", ".", "embedding_dim", ",", "export", "=", "export", ")", "\n", "self", ".", "fc1", "=", "nn", ".", "Linear", "(", "self", ".", "embedding_dim", ",", "ffn_embedding_dim", ")", "\n", "self", ".", "fc2", "=", "nn", ".", "Linear", "(", "ffn_embedding_dim", ",", "self", ".", "embedding_dim", ")", "\n", "\n", "# layer norm associated with the position wise feed-forward NN", "\n", "self", ".", "final_layer_norm", "=", "LayerNorm", "(", "self", ".", "embedding_dim", ",", "export", "=", "export", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_sentence_encoder_layer.TransformerSentenceEncoderLayer.forward": [[62, 93], ["transformer_sentence_encoder_layer.TransformerSentenceEncoderLayer.self_attn", "torch.dropout", "torch.dropout", "torch.dropout", "transformer_sentence_encoder_layer.TransformerSentenceEncoderLayer.self_attn_layer_norm", "transformer_sentence_encoder_layer.TransformerSentenceEncoderLayer.activation_fn", "torch.dropout", "torch.dropout", "torch.dropout", "transformer_sentence_encoder_layer.TransformerSentenceEncoderLayer.fc2", "torch.dropout", "torch.dropout", "torch.dropout", "transformer_sentence_encoder_layer.TransformerSentenceEncoderLayer.final_layer_norm", "transformer_sentence_encoder_layer.TransformerSentenceEncoderLayer.fc1"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "x", ":", "torch", ".", "Tensor", ",", "\n", "self_attn_mask", ":", "torch", ".", "Tensor", "=", "None", ",", "\n", "self_attn_padding_mask", ":", "torch", ".", "Tensor", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        LayerNorm is applied either before or after the self-attention/ffn\n        modules similar to the original Transformer imlementation.\n        \"\"\"", "\n", "residual", "=", "x", "\n", "x", ",", "attn", "=", "self", ".", "self_attn", "(", "\n", "query", "=", "x", ",", "\n", "key", "=", "x", ",", "\n", "value", "=", "x", ",", "\n", "key_padding_mask", "=", "self_attn_padding_mask", ",", "\n", "need_weights", "=", "False", ",", "\n", "attn_mask", "=", "self_attn_mask", ",", "\n", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "residual", "+", "x", "\n", "x", "=", "self", ".", "self_attn_layer_norm", "(", "x", ")", "\n", "\n", "residual", "=", "x", "\n", "x", "=", "self", ".", "activation_fn", "(", "self", ".", "fc1", "(", "x", ")", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "activation_dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "self", ".", "fc2", "(", "x", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "residual", "+", "x", "\n", "x", "=", "self", ".", "final_layer_norm", "(", "x", ")", "\n", "return", "x", ",", "attn", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.adaptive_input.AdaptiveInput.__init__": [[15, 57], ["torch.nn.Module.__init__", "torch.nn.ModuleList", "range", "adaptive_input.AdaptiveInput.apply", "adaptive_input.AdaptiveInput.register_buffer", "len", "int", "torch.nn.Sequential", "adaptive_input.AdaptiveInput.embeddings.append", "isinstance", "torch.FloatTensor", "torch.nn.Embedding", "torch.nn.Linear", "torch.nn.init.normal_", "torch.nn.init.constant_", "hasattr", "torch.nn.init.xavier_uniform_"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.Embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "vocab_size", ":", "int", ",", "\n", "padding_idx", ":", "int", ",", "\n", "initial_dim", ":", "int", ",", "\n", "factor", ":", "float", ",", "\n", "output_dim", ":", "int", ",", "\n", "cutoff", ":", "List", "[", "int", "]", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "if", "vocab_size", ">", "cutoff", "[", "-", "1", "]", ":", "\n", "            ", "cutoff", "=", "cutoff", "+", "[", "vocab_size", "]", "\n", "", "else", ":", "\n", "            ", "assert", "vocab_size", "==", "cutoff", "[", "\n", "-", "1", "]", ",", "'cannot specify cutoff larger than vocab size'", "\n", "\n", "", "self", ".", "cutoff", "=", "cutoff", "\n", "self", ".", "embedding_dim", "=", "output_dim", "\n", "self", ".", "padding_idx", "=", "padding_idx", "\n", "\n", "self", ".", "embeddings", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "cutoff", ")", ")", ":", "\n", "            ", "prev", "=", "self", ".", "cutoff", "[", "i", "-", "1", "]", "if", "i", ">", "0", "else", "0", "\n", "size", "=", "self", ".", "cutoff", "[", "i", "]", "-", "prev", "\n", "dim", "=", "int", "(", "initial_dim", "//", "(", "factor", "**", "i", ")", ")", "\n", "seq", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Embedding", "(", "size", ",", "dim", ",", "padding_idx", ")", ",", "\n", "nn", ".", "Linear", "(", "dim", ",", "output_dim", ",", "bias", "=", "False", ")", "\n", ")", "\n", "self", ".", "embeddings", ".", "append", "(", "seq", ")", "\n", "\n", "", "def", "init_weights", "(", "m", ")", ":", "\n", "            ", "if", "isinstance", "(", "m", ",", "nn", ".", "Embedding", ")", ":", "\n", "                ", "nn", ".", "init", ".", "normal_", "(", "m", ".", "weight", ",", "mean", "=", "0", ",", "std", "=", "m", ".", "weight", ".", "shape", "[", "1", "]", "**", "-", "0.5", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "weight", "[", "padding_idx", "]", ",", "0", ")", "\n", "", "elif", "hasattr", "(", "m", ",", "'weight'", ")", ":", "\n", "                ", "nn", ".", "init", ".", "xavier_uniform_", "(", "m", ".", "weight", ")", "\n", "\n", "", "", "self", ".", "apply", "(", "init_weights", ")", "\n", "\n", "self", ".", "register_buffer", "(", "'_float_tensor'", ",", "torch", ".", "FloatTensor", "(", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.adaptive_input.AdaptiveInput.weights_for_band": [[58, 60], ["None"], "methods", ["None"], ["", "def", "weights_for_band", "(", "self", ",", "band", ":", "int", ")", ":", "\n", "        ", "return", "self", ".", "embeddings", "[", "band", "]", "[", "0", "]", ".", "weight", ",", "self", ".", "embeddings", "[", "band", "]", "[", "1", "]", ".", "weight", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.adaptive_input.AdaptiveInput.forward": [[61, 73], ["adaptive_input.AdaptiveInput._float_tensor.new", "range", "len", "input.lt", "input.lt.any", "input.lt.mul_", "input.ge"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ":", "torch", ".", "Tensor", ")", ":", "\n", "        ", "result", "=", "self", ".", "_float_tensor", ".", "new", "(", "input", ".", "shape", "+", "(", "self", ".", "embedding_dim", ",", ")", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "cutoff", ")", ")", ":", "\n", "            ", "mask", "=", "input", ".", "lt", "(", "self", ".", "cutoff", "[", "i", "]", ")", "\n", "if", "i", ">", "0", ":", "\n", "                ", "mask", ".", "mul_", "(", "input", ".", "ge", "(", "self", ".", "cutoff", "[", "i", "-", "1", "]", ")", ")", "\n", "chunk_input", "=", "input", "[", "mask", "]", "-", "self", ".", "cutoff", "[", "i", "-", "1", "]", "\n", "", "else", ":", "\n", "                ", "chunk_input", "=", "input", "[", "mask", "]", "\n", "", "if", "mask", ".", "any", "(", ")", ":", "\n", "                ", "result", "[", "mask", "]", "=", "self", ".", "embeddings", "[", "i", "]", "(", "chunk_input", ")", "\n", "", "", "return", "result", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.grad_multiply.GradMultiply.forward": [[10, 15], ["x.new"], "methods", ["None"], ["    ", "@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "x", ",", "scale", ")", ":", "\n", "        ", "ctx", ".", "scale", "=", "scale", "\n", "res", "=", "x", ".", "new", "(", "x", ")", "\n", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.grad_multiply.GradMultiply.backward": [[16, 19], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "grad", ")", ":", "\n", "        ", "return", "grad", "*", "ctx", ".", "scale", ",", "None", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.learned_positional_embedding.LearnedPositionalEmbedding.__init__": [[19, 27], ["torch.Embedding.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "num_embeddings", ":", "int", ",", "\n", "embedding_dim", ":", "int", ",", "\n", "padding_idx", ":", "int", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", ")", "\n", "self", ".", "onnx_trace", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.learned_positional_embedding.LearnedPositionalEmbedding.forward": [[28, 44], ["super().forward", "input.data.new().fill_", "fairseq.utils.make_positions", "int", "input.data.new", "input.size"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.model.AMRModel.forward", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.make_positions", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "forward", "(", "self", ",", "input", ",", "incremental_state", "=", "None", ",", "positions", "=", "None", ")", ":", "\n", "        ", "\"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"", "\n", "assert", "(", "\n", "(", "positions", "is", "None", ")", "or", "(", "self", ".", "padding_idx", "is", "None", ")", "\n", ")", ",", "\"If positions is pre-computed then padding_idx should not be set.\"", "\n", "\n", "if", "positions", "is", "None", ":", "\n", "            ", "if", "incremental_state", "is", "not", "None", ":", "\n", "# positions is the same for every token when decoding a single step", "\n", "# Without the int() cast, it doesn't work in some cases when exporting to ONNX", "\n", "                ", "positions", "=", "input", ".", "data", ".", "new", "(", "1", ",", "1", ")", ".", "fill_", "(", "int", "(", "self", ".", "padding_idx", "+", "input", ".", "size", "(", "1", ")", ")", ")", "\n", "", "else", ":", "\n", "                ", "positions", "=", "utils", ".", "make_positions", "(", "\n", "input", ".", "data", ",", "self", ".", "padding_idx", ",", "onnx_trace", "=", "self", ".", "onnx_trace", ",", "\n", ")", "\n", "", "", "return", "super", "(", ")", ".", "forward", "(", "positions", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.learned_positional_embedding.LearnedPositionalEmbedding.max_positions": [[45, 51], ["None"], "methods", ["None"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum number of supported positions.\"\"\"", "\n", "if", "self", ".", "padding_idx", "is", "not", "None", ":", "\n", "            ", "return", "self", ".", "num_embeddings", "-", "self", ".", "padding_idx", "-", "1", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "num_embeddings", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.multihead_attention.MultiheadAttention.__init__": [[20, 72], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "multihead_attention.MultiheadAttention.reset_parameters", "hasattr", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "multihead_attention.MultiheadAttention.register_parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.highway.Highway.reset_parameters"], ["def", "__init__", "(", "self", ",", "embed_dim", ",", "num_heads", ",", "kdim", "=", "None", ",", "vdim", "=", "None", ",", "dropout", "=", "0.", ",", "bias", "=", "True", ",", "\n", "add_bias_kv", "=", "False", ",", "add_zero_attn", "=", "False", ",", "self_attention", "=", "False", ",", "\n", "encoder_decoder_attention", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "embed_dim", "=", "embed_dim", "\n", "self", ".", "kdim", "=", "kdim", "if", "kdim", "is", "not", "None", "else", "embed_dim", "\n", "self", ".", "vdim", "=", "vdim", "if", "vdim", "is", "not", "None", "else", "embed_dim", "\n", "self", ".", "qkv_same_dim", "=", "self", ".", "kdim", "==", "embed_dim", "and", "self", ".", "vdim", "==", "embed_dim", "\n", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "head_dim", "=", "embed_dim", "//", "num_heads", "\n", "assert", "self", ".", "head_dim", "*", "num_heads", "==", "self", ".", "embed_dim", ",", "\"embed_dim must be divisible by num_heads\"", "\n", "self", ".", "scaling", "=", "self", ".", "head_dim", "**", "-", "0.5", "\n", "\n", "self", ".", "self_attention", "=", "self_attention", "\n", "self", ".", "encoder_decoder_attention", "=", "encoder_decoder_attention", "\n", "\n", "assert", "not", "self", ".", "self_attention", "or", "self", ".", "qkv_same_dim", ",", "'Self-attention requires query, key and '", "'value to be of the same size'", "\n", "\n", "if", "self", ".", "qkv_same_dim", ":", "\n", "            ", "self", ".", "in_proj_weight", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "3", "*", "embed_dim", ",", "embed_dim", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "k_proj_weight", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "embed_dim", ",", "self", ".", "kdim", ")", ")", "\n", "self", ".", "v_proj_weight", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "embed_dim", ",", "self", ".", "vdim", ")", ")", "\n", "self", ".", "q_proj_weight", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "embed_dim", ",", "embed_dim", ")", ")", "\n", "\n", "", "if", "bias", ":", "\n", "            ", "self", ".", "in_proj_bias", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "3", "*", "embed_dim", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "register_parameter", "(", "'in_proj_bias'", ",", "None", ")", "\n", "\n", "", "self", ".", "out_proj", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "embed_dim", ",", "bias", "=", "bias", ")", "\n", "\n", "if", "add_bias_kv", ":", "\n", "            ", "self", ".", "bias_k", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "1", ",", "1", ",", "embed_dim", ")", ")", "\n", "self", ".", "bias_v", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "1", ",", "1", ",", "embed_dim", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "bias_k", "=", "self", ".", "bias_v", "=", "None", "\n", "\n", "", "self", ".", "add_zero_attn", "=", "add_zero_attn", "\n", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n", "self", ".", "onnx_trace", "=", "False", "\n", "\n", "self", ".", "enable_torch_version", "=", "False", "\n", "if", "hasattr", "(", "F", ",", "\"multi_head_attention_forward\"", ")", ":", "\n", "            ", "self", ".", "enable_torch_version", "=", "True", "\n", "", "else", ":", "\n", "            ", "self", ".", "enable_torch_version", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.multihead_attention.MultiheadAttention.prepare_for_onnx_export_": [[73, 75], ["None"], "methods", ["None"], ["", "", "def", "prepare_for_onnx_export_", "(", "self", ")", ":", "\n", "        ", "self", ".", "onnx_trace", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.multihead_attention.MultiheadAttention.reset_parameters": [[76, 92], ["torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.xavier_normal_", "torch.nn.init.xavier_normal_", "torch.nn.init.xavier_normal_", "torch.nn.init.xavier_normal_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "qkv_same_dim", ":", "\n", "            ", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "in_proj_weight", ")", "\n", "", "else", ":", "\n", "            ", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "k_proj_weight", ")", "\n", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "v_proj_weight", ")", "\n", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "q_proj_weight", ")", "\n", "\n", "", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "out_proj", ".", "weight", ")", "\n", "if", "self", ".", "in_proj_bias", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "constant_", "(", "self", ".", "in_proj_bias", ",", "0.", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "self", ".", "out_proj", ".", "bias", ",", "0.", ")", "\n", "", "if", "self", ".", "bias_k", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "xavier_normal_", "(", "self", ".", "bias_k", ")", "\n", "", "if", "self", ".", "bias_v", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "xavier_normal_", "(", "self", ".", "bias_v", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.multihead_attention.MultiheadAttention.forward": [[93, 366], ["query.size", "multihead_attention.MultiheadAttention.contiguous().view().transpose", "torch.cat.size", "torch.cat.size", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "multihead_attention.MultiheadAttention.apply_sparse_mask", "fairseq.utils.softmax().type_as", "torch.dropout", "torch.dropout", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "multihead_attention.MultiheadAttention.out_proj", "list", "multihead_attention.MultiheadAttention._get_input_buffer", "multihead_attention.MultiheadAttention.in_proj_qkv", "NotImplementedError", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat.contiguous().view().transpose", "torch.cat.contiguous().view().transpose", "torch.cat.contiguous().view().transpose", "torch.cat.contiguous().view().transpose", "torch.cat.view", "torch.cat.view", "torch.cat.view", "torch.cat.view", "multihead_attention.MultiheadAttention._set_input_buffer", "multihead_attention.MultiheadAttention.transpose().contiguous().view().transpose", "multihead_attention.MultiheadAttention.transpose().contiguous().view().transpose", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat.transpose", "torch.cat.transpose", "list", "attn_mask.repeat.repeat.unsqueeze", "attn_weights.masked_fill.masked_fill.view", "attn_weights.masked_fill.masked_fill.view", "list", "attn.transpose().contiguous().view.transpose().contiguous().view.contiguous().view", "attn.transpose().contiguous().view.transpose().contiguous().view.transpose().contiguous().view", "attn_weights.masked_fill.masked_fill.view", "query.size", "torch.multi_head_attention_forward", "torch.multi_head_attention_forward", "torch.multi_head_attention_forward", "torch.multi_head_attention_forward", "multihead_attention.MultiheadAttention.in_proj_q", "multihead_attention.MultiheadAttention.in_proj_q", "multihead_attention.MultiheadAttention.in_proj_k", "multihead_attention.MultiheadAttention.in_proj_v", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "multihead_attention.MultiheadAttention.contiguous().view", "saved_state[].view", "saved_state[].view", "torch.Size", "torch.Size", "torch.Size", "torch.Size", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "attn_weights.masked_fill.masked_fill.size", "NotImplementedError", "attn_mask.repeat.repeat.repeat", "torch.where().type_as", "torch.where().type_as", "torch.where().type_as", "torch.where().type_as", "attn_weights.masked_fill.masked_fill.masked_fill", "fairseq.utils.softmax", "attn.transpose().contiguous().view.transpose().contiguous().view.size", "attn.transpose().contiguous().view.transpose().contiguous().view.size", "attn_weights.masked_fill.masked_fill.sum", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "multihead_attention.MultiheadAttention.in_proj_k", "multihead_attention.MultiheadAttention.in_proj_v", "multihead_attention.MultiheadAttention.in_proj_k", "multihead_attention.MultiheadAttention.in_proj_v", "multihead_attention.MultiheadAttention.bias_k.repeat", "multihead_attention.MultiheadAttention.bias_v.repeat", "torch.cat.contiguous().view", "torch.cat.contiguous().view", "torch.cat.contiguous().view", "torch.cat.contiguous().view", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "multihead_attention.MultiheadAttention.transpose().contiguous().view", "multihead_attention.MultiheadAttention.transpose().contiguous().view", "torch.cat.new_zeros", "torch.cat.new_zeros", "torch.cat.new_zeros", "torch.cat.new_zeros", "attn_weights.masked_fill.masked_fill.size", "torch.cat.unsqueeze().unsqueeze", "torch.cat.unsqueeze().unsqueeze", "float", "attn.transpose().contiguous().view.transpose().contiguous().view.contiguous", "attn.transpose().contiguous().view.transpose().contiguous().view.transpose().contiguous", "attn_mask.repeat.repeat.new_zeros", "torch.cat.new_zeros", "torch.cat.new_zeros", "multihead_attention.MultiheadAttention.contiguous", "attn_mask.repeat.repeat.new_zeros", "torch.zeros().type_as", "torch.zeros().type_as", "torch.zeros().type_as", "torch.zeros().type_as", "torch.where", "torch.where", "torch.where", "torch.where", "attn_weights.masked_fill.masked_fill.transpose().unsqueeze", "attn_mask.repeat.repeat.size", "torch.cat.size", "torch.cat.size", "torch.cat.contiguous", "torch.cat.contiguous", "torch.cat.contiguous", "torch.cat.contiguous", "multihead_attention.MultiheadAttention.transpose().contiguous", "multihead_attention.MultiheadAttention.transpose().contiguous", "attn_mask.repeat.repeat.size", "torch.cat.unsqueeze().unsqueeze", "torch.cat.unsqueeze().unsqueeze", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "attn_weights.masked_fill.masked_fill.float", "torch.cat.unsqueeze", "torch.cat.unsqueeze", "attn.transpose().contiguous().view.transpose().contiguous().view.transpose", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "attn_weights.masked_fill.masked_fill.transpose", "multihead_attention.MultiheadAttention.transpose", "multihead_attention.MultiheadAttention.transpose", "torch.cat.size", "torch.cat.size", "multihead_attention.MultiheadAttention.unsqueeze", "torch.cat.unsqueeze", "torch.cat.unsqueeze", "float"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.sparse_multihead_attention.SparseMultiheadAttention.apply_sparse_mask", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.linearized_convolution.LinearizedConvolution._get_input_buffer", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.multihead_attention.MultiheadAttention.in_proj_qkv", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.linearized_convolution.LinearizedConvolution._set_input_buffer", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.multihead_attention.MultiheadAttention.in_proj_q", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.multihead_attention.MultiheadAttention.in_proj_q", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.multihead_attention.MultiheadAttention.in_proj_k", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.multihead_attention.MultiheadAttention.in_proj_v", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.multihead_attention.MultiheadAttention.in_proj_k", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.multihead_attention.MultiheadAttention.in_proj_v", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.multihead_attention.MultiheadAttention.in_proj_k", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.multihead_attention.MultiheadAttention.in_proj_v", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "", "def", "forward", "(", "self", ",", "query", ",", "key", ",", "value", ",", "key_padding_mask", "=", "None", ",", "incremental_state", "=", "None", ",", "\n", "need_weights", "=", "True", ",", "static_kv", "=", "False", ",", "attn_mask", "=", "None", ",", "\n", "head_attention_masks", "=", "None", ",", "head_positions", "=", "None", ")", ":", "\n", "        ", "\"\"\"Input shape: Time x Batch x Channel\n\n        Timesteps can be masked by supplying a T x T mask in the\n        `attn_mask` argument. Padding elements can be excluded from\n        the key by passing a binary ByteTensor (`key_padding_mask`) with shape:\n        batch x src_len, where padding elements are indicated by 1s.\n        \"\"\"", "\n", "tgt_len", ",", "bsz", ",", "embed_dim", "=", "query", ".", "size", "(", ")", "\n", "assert", "embed_dim", "==", "self", ".", "embed_dim", "\n", "assert", "list", "(", "query", ".", "size", "(", ")", ")", "==", "[", "tgt_len", ",", "bsz", ",", "embed_dim", "]", "\n", "\n", "if", "self", ".", "enable_torch_version", "and", "not", "self", ".", "onnx_trace", "and", "incremental_state", "is", "None", "and", "not", "static_kv", ":", "\n", "            ", "if", "self", ".", "qkv_same_dim", ":", "\n", "                ", "return", "F", ".", "multi_head_attention_forward", "(", "query", ",", "key", ",", "value", ",", "\n", "self", ".", "embed_dim", ",", "self", ".", "num_heads", ",", "\n", "self", ".", "in_proj_weight", ",", "\n", "self", ".", "in_proj_bias", ",", "self", ".", "bias_k", ",", "self", ".", "bias_v", ",", "\n", "self", ".", "add_zero_attn", ",", "self", ".", "dropout", ",", "\n", "self", ".", "out_proj", ".", "weight", ",", "self", ".", "out_proj", ".", "bias", ",", "\n", "self", ".", "training", ",", "key_padding_mask", ",", "need_weights", ",", "\n", "attn_mask", ")", "\n", "", "else", ":", "\n", "                ", "return", "F", ".", "multi_head_attention_forward", "(", "query", ",", "key", ",", "value", ",", "\n", "self", ".", "embed_dim", ",", "self", ".", "num_heads", ",", "\n", "torch", ".", "empty", "(", "[", "0", "]", ")", ",", "\n", "self", ".", "in_proj_bias", ",", "self", ".", "bias_k", ",", "self", ".", "bias_v", ",", "\n", "self", ".", "add_zero_attn", ",", "self", ".", "dropout", ",", "\n", "self", ".", "out_proj", ".", "weight", ",", "self", ".", "out_proj", ".", "bias", ",", "\n", "self", ".", "training", ",", "key_padding_mask", ",", "need_weights", ",", "\n", "attn_mask", ",", "use_separate_proj_weight", "=", "True", ",", "\n", "q_proj_weight", "=", "self", ".", "q_proj_weight", ",", "\n", "k_proj_weight", "=", "self", ".", "k_proj_weight", ",", "\n", "v_proj_weight", "=", "self", ".", "v_proj_weight", ")", "\n", "\n", "", "", "if", "incremental_state", "is", "not", "None", ":", "\n", "            ", "saved_state", "=", "self", ".", "_get_input_buffer", "(", "incremental_state", ")", "\n", "if", "'prev_key'", "in", "saved_state", ":", "\n", "# previous time steps are cached - no need to recompute", "\n", "# key and value if they are static", "\n", "                ", "if", "static_kv", ":", "\n", "                    ", "assert", "self", ".", "encoder_decoder_attention", "and", "not", "self", ".", "self_attention", "\n", "key", "=", "value", "=", "None", "\n", "", "", "", "else", ":", "\n", "            ", "saved_state", "=", "None", "\n", "\n", "# encoder-decoder attention", "\n", "", "if", "self", ".", "self_attention", ":", "\n", "# self-attention", "\n", "            ", "q", ",", "k", ",", "v", "=", "self", ".", "in_proj_qkv", "(", "query", ")", "\n", "", "elif", "self", ".", "encoder_decoder_attention", ":", "\n", "# encoder-decoder attention", "\n", "            ", "q", "=", "self", ".", "in_proj_q", "(", "query", ")", "\n", "if", "key", "is", "None", ":", "\n", "                ", "assert", "value", "is", "None", "\n", "k", "=", "v", "=", "None", "\n", "", "else", ":", "\n", "\n", "# key/value linear projections ", "\n", "# (source_size, batch_size, target_emb_size) ", "\n", "# -> ", "\n", "# (source_size, batch_size, target_emb_size *2)", "\n", "# -> (chunked) ", "\n", "# (source_size, batch_size, target_emb_size) * 2", "\n", "                ", "k", "=", "self", ".", "in_proj_k", "(", "key", ")", "\n", "v", "=", "self", ".", "in_proj_v", "(", "key", ")", "\n", "\n", "", "if", "head_positions", "is", "not", "None", ":", "\n", "# project position embeddings", "\n", "# (batch_size, source_size, target_size, target_emb_size)", "\n", "# ->", "\n", "# (batch_size, source_size, target_size, target_emb_size) * 2", "\n", "                ", "head_pos_emb_k", "=", "self", ".", "in_proj_k", "(", "head_positions", ")", "\n", "head_pos_emb_v", "=", "self", ".", "in_proj_v", "(", "head_positions", ")", "\n", "# FIXME: this assumes first two heads are stack/buffer ", "\n", "# only the first two heads get stack/buffer pos added", "\n", "head_dim", "=", "head_pos_emb_k", ".", "shape", "[", "3", "]", "//", "self", ".", "num_heads", "\n", "head_pos_emb_k", "[", ":", ",", ":", ",", ":", ",", "2", "*", "head_dim", ":", "]", "=", "0", "\n", "head_pos_emb_v", "[", ":", ",", ":", ",", ":", ",", "2", "*", "head_dim", ":", "]", "=", "0", "\n", "\n", "# sanity check: assuming first action is a shift, this is", "\n", "# just inserting position zero at leftmost element", "\n", "# assert (head_pos_emb_k[0, 1:, 1, :] == head_pos_emb_k[0, :-1, 0, :]).all()", "\n", "# assert (head_pos_emb_k[0, 0, 1, :] ==  head_pos_emb_k[0, 0, 0, :]).all()", "\n", "\n", "", "", "else", ":", "\n", "            ", "q", "=", "self", ".", "in_proj_q", "(", "query", ")", "\n", "k", "=", "self", ".", "in_proj_k", "(", "key", ")", "\n", "v", "=", "self", ".", "in_proj_v", "(", "value", ")", "\n", "", "q", "*=", "self", ".", "scaling", "\n", "\n", "if", "self", ".", "bias_k", "is", "not", "None", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\n", "\"attention bias not implemented for stack-trasnformer\"", "\n", ")", "\n", "assert", "self", ".", "bias_v", "is", "not", "None", "\n", "k", "=", "torch", ".", "cat", "(", "[", "k", ",", "self", ".", "bias_k", ".", "repeat", "(", "1", ",", "bsz", ",", "1", ")", "]", ")", "\n", "v", "=", "torch", ".", "cat", "(", "[", "v", ",", "self", ".", "bias_v", ".", "repeat", "(", "1", ",", "bsz", ",", "1", ")", "]", ")", "\n", "if", "attn_mask", "is", "not", "None", ":", "\n", "                ", "attn_mask", "=", "torch", ".", "cat", "(", "[", "attn_mask", ",", "attn_mask", ".", "new_zeros", "(", "attn_mask", ".", "size", "(", "0", ")", ",", "1", ")", "]", ",", "dim", "=", "1", ")", "\n", "", "if", "key_padding_mask", "is", "not", "None", ":", "\n", "                ", "key_padding_mask", "=", "torch", ".", "cat", "(", "\n", "[", "key_padding_mask", ",", "key_padding_mask", ".", "new_zeros", "(", "key_padding_mask", ".", "size", "(", "0", ")", ",", "1", ")", "]", ",", "dim", "=", "1", ")", "\n", "\n", "# NOTE: It splits the emb_dim across heads and unfolds heads in batch", "\n", "# dimension. This is not standard multi-head attention!", "\n", "#", "\n", "# (target_size, batch_size, target_emb_size) ", "\n", "# ->", "\n", "# (target_size, batch_size * num_heads, target_emb_size / num_heads) ", "\n", "# -> ", "\n", "# (batch_size * num_heads, target_size, target_emb_size / num_heads)", "\n", "#", "\n", "# dimension. Heads for same batch element are contiguous on that", "\n", "# dimensions. See example", "\n", "# dummy = torch.zeros(q.shape)   ", "\n", "# dummy[:, 0, :] = torch.ones(dummy[:, 0, :].shape)  ", "\n", "# dummy2 = dummy.view(tgt_len, bsz * self.num_heads, self.head_dim).transpose(0, 1)", "\n", "# (dummy2[:self.num_heads, :, :] == 1).sum() == (dummy2 == 1).sum()", "\n", "#", "\n", "", "", "q", "=", "q", ".", "contiguous", "(", ")", ".", "view", "(", "tgt_len", ",", "bsz", "*", "self", ".", "num_heads", ",", "self", ".", "head_dim", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "if", "k", "is", "not", "None", ":", "\n", "            ", "k", "=", "k", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "bsz", "*", "self", ".", "num_heads", ",", "self", ".", "head_dim", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "", "if", "v", "is", "not", "None", ":", "\n", "            ", "v", "=", "v", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "bsz", "*", "self", ".", "num_heads", ",", "self", ".", "head_dim", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "", "if", "saved_state", "is", "not", "None", ":", "\n", "# saved states are stored with shape (bsz, num_heads, seq_len, head_dim)", "\n", "            ", "if", "'prev_key'", "in", "saved_state", ":", "\n", "                ", "prev_key", "=", "saved_state", "[", "'prev_key'", "]", ".", "view", "(", "bsz", "*", "self", ".", "num_heads", ",", "-", "1", ",", "self", ".", "head_dim", ")", "\n", "if", "static_kv", ":", "\n", "                    ", "k", "=", "prev_key", "\n", "", "else", ":", "\n", "                    ", "k", "=", "torch", ".", "cat", "(", "(", "prev_key", ",", "k", ")", ",", "dim", "=", "1", ")", "\n", "", "", "if", "'prev_value'", "in", "saved_state", ":", "\n", "                ", "prev_value", "=", "saved_state", "[", "'prev_value'", "]", ".", "view", "(", "bsz", "*", "self", ".", "num_heads", ",", "-", "1", ",", "self", ".", "head_dim", ")", "\n", "if", "static_kv", ":", "\n", "                    ", "v", "=", "prev_value", "\n", "", "else", ":", "\n", "                    ", "v", "=", "torch", ".", "cat", "(", "(", "prev_value", ",", "v", ")", ",", "dim", "=", "1", ")", "\n", "\n", "# This saves key and value for this head with key e.g.", "\n", "# 'MultiheadAttention.1.attn_state'", "\n", "", "", "saved_state", "[", "'prev_key'", "]", "=", "k", ".", "view", "(", "bsz", ",", "self", ".", "num_heads", ",", "-", "1", ",", "self", ".", "head_dim", ")", "\n", "saved_state", "[", "'prev_value'", "]", "=", "v", ".", "view", "(", "bsz", ",", "self", ".", "num_heads", ",", "-", "1", ",", "self", ".", "head_dim", ")", "\n", "\n", "self", ".", "_set_input_buffer", "(", "incremental_state", ",", "saved_state", ")", "\n", "\n", "", "src_len", "=", "k", ".", "size", "(", "1", ")", "\n", "if", "head_positions", "is", "not", "None", ":", "\n", "# (batch_size, source_size, target_size, target_emb_size) ", "\n", "# -> ", "\n", "# (batch_size * num_heads, source_size, target_size, target_emb_size / num_heads)", "\n", "# assert (head_pos_emb_k[0, 1:, 1, :] == head_pos_emb_k[0, :-1, 0, :]).all()", "\n", "            ", "head_pos_emb_k", "=", "head_pos_emb_k", ".", "transpose", "(", "0", ",", "2", ")", ".", "contiguous", "(", ")", ".", "view", "(", "tgt_len", ",", "src_len", ",", "bsz", "*", "self", ".", "num_heads", ",", "self", ".", "head_dim", ")", ".", "transpose", "(", "0", ",", "2", ")", "\n", "head_pos_emb_v", "=", "head_pos_emb_v", ".", "transpose", "(", "0", ",", "2", ")", ".", "contiguous", "(", ")", ".", "view", "(", "tgt_len", ",", "src_len", ",", "bsz", "*", "self", ".", "num_heads", ",", "self", ".", "head_dim", ")", ".", "transpose", "(", "0", ",", "2", ")", "\n", "\n", "# This is part of a workaround to get around fork/join parallelism", "\n", "# not supporting Optional types.", "\n", "", "if", "key_padding_mask", "is", "not", "None", "and", "key_padding_mask", ".", "shape", "==", "torch", ".", "Size", "(", "[", "]", ")", ":", "\n", "            ", "key_padding_mask", "=", "None", "\n", "\n", "", "if", "key_padding_mask", "is", "not", "None", ":", "\n", "            ", "assert", "key_padding_mask", ".", "size", "(", "0", ")", "==", "bsz", "\n", "assert", "key_padding_mask", ".", "size", "(", "1", ")", "==", "src_len", "\n", "\n", "", "if", "self", ".", "add_zero_attn", ":", "\n", "            ", "src_len", "+=", "1", "\n", "k", "=", "torch", ".", "cat", "(", "[", "k", ",", "k", ".", "new_zeros", "(", "(", "k", ".", "size", "(", "0", ")", ",", "1", ")", "+", "k", ".", "size", "(", ")", "[", "2", ":", "]", ")", "]", ",", "dim", "=", "1", ")", "\n", "v", "=", "torch", ".", "cat", "(", "[", "v", ",", "v", ".", "new_zeros", "(", "(", "v", ".", "size", "(", "0", ")", ",", "1", ")", "+", "v", ".", "size", "(", ")", "[", "2", ":", "]", ")", "]", ",", "dim", "=", "1", ")", "\n", "if", "attn_mask", "is", "not", "None", ":", "\n", "                ", "attn_mask", "=", "torch", ".", "cat", "(", "[", "attn_mask", ",", "attn_mask", ".", "new_zeros", "(", "attn_mask", ".", "size", "(", "0", ")", ",", "1", ")", "]", ",", "dim", "=", "1", ")", "\n", "", "if", "key_padding_mask", "is", "not", "None", ":", "\n", "                ", "key_padding_mask", "=", "torch", ".", "cat", "(", "\n", "[", "key_padding_mask", ",", "torch", ".", "zeros", "(", "key_padding_mask", ".", "size", "(", "0", ")", ",", "1", ")", ".", "type_as", "(", "key_padding_mask", ")", "]", ",", "dim", "=", "1", ")", "\n", "\n", "# Compute unnormalized attention", "\n", "# q              (batch_size * num_heads, target_size, target_emb_size / num_heads)", "\n", "# k              (batch_size * num_heads, source_size, target_emb_size / num_heads)", "\n", "# ->", "\n", "# attn_weights   (batch_size * num_heads, source_size, target_size)", "\n", "", "", "attn_weights", "=", "torch", ".", "bmm", "(", "q", ",", "k", ".", "transpose", "(", "1", ",", "2", ")", ")", "\n", "\n", "if", "head_positions", "is", "not", "None", ":", "\n", "# if buffer/stack positions provided, add them to attention computation", "\n", "# Note that batched inner product is here implemented as", "\n", "# elements-wise product and sum across common axis", "\n", "# head_positions (batch_size * num_heads, source_size, target_size, target_emb_size / num_heads) ", "\n", "# q              (batch_size * num_heads, target_size, target_emb_size / num_heads)", "\n", "# ->", "\n", "# attn_weights   (batch_size * num_heads, source_size, target_size)", "\n", "            ", "attn_weights", "+=", "self", ".", "scaling", "*", "(", "q", ".", "unsqueeze", "(", "1", ")", "*", "head_pos_emb_k", ")", ".", "sum", "(", "3", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "\n", "# FIXME: What is this", "\n", "", "attn_weights", "=", "self", ".", "apply_sparse_mask", "(", "attn_weights", ",", "tgt_len", ",", "src_len", ",", "bsz", ")", "\n", "\n", "assert", "list", "(", "attn_weights", ".", "size", "(", ")", ")", "==", "[", "bsz", "*", "self", ".", "num_heads", ",", "tgt_len", ",", "src_len", "]", "\n", "\n", "# Stack/Buffer individual mask per head", "\n", "if", "head_attention_masks", "is", "not", "None", ":", "\n", "            ", "if", "self", ".", "onnx_trace", ":", "\n", "# dunno whats this better die", "\n", "              ", "raise", "NotImplementedError", "(", ")", "\n", "# mask in log domain with pre_mask", "\n", "", "attn_weights", "+=", "head_attention_masks", "[", "0", "]", "\n", "\n", "", "if", "attn_mask", "is", "not", "None", ":", "\n", "            ", "attn_mask", "=", "attn_mask", ".", "unsqueeze", "(", "0", ")", "\n", "if", "self", ".", "onnx_trace", ":", "\n", "                ", "attn_mask", "=", "attn_mask", ".", "repeat", "(", "attn_weights", ".", "size", "(", "0", ")", ",", "1", ",", "1", ")", "\n", "", "attn_weights", "+=", "attn_mask", "\n", "\n", "", "if", "key_padding_mask", "is", "not", "None", ":", "\n", "# don't attend to padding symbols", "\n", "            ", "attn_weights", "=", "attn_weights", ".", "view", "(", "bsz", ",", "self", ".", "num_heads", ",", "tgt_len", ",", "src_len", ")", "\n", "if", "self", ".", "onnx_trace", ":", "\n", "                ", "attn_weights", "=", "torch", ".", "where", "(", "\n", "key_padding_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", ",", "\n", "torch", ".", "Tensor", "(", "[", "float", "(", "\"-Inf\"", ")", "]", ")", ",", "\n", "attn_weights", ".", "float", "(", ")", "\n", ")", ".", "type_as", "(", "attn_weights", ")", "\n", "", "else", ":", "\n", "                ", "attn_weights", "=", "attn_weights", ".", "masked_fill", "(", "\n", "key_padding_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", ",", "\n", "float", "(", "'-inf'", ")", ",", "\n", ")", "\n", "", "attn_weights", "=", "attn_weights", ".", "view", "(", "bsz", "*", "self", ".", "num_heads", ",", "tgt_len", ",", "src_len", ")", "\n", "\n", "", "attn_weights", "=", "utils", ".", "softmax", "(", "\n", "attn_weights", ",", "dim", "=", "-", "1", ",", "onnx_trace", "=", "self", ".", "onnx_trace", ",", "\n", ")", ".", "type_as", "(", "attn_weights", ")", "\n", "attn_weights", "=", "F", ".", "dropout", "(", "attn_weights", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "\n", "# post mask for empty buffer/stack", "\n", "if", "head_attention_masks", "is", "not", "None", ":", "\n", "# sanity check, this really blocks only all inf weights", "\n", "            ", "attn_weights", "=", "attn_weights", "*", "head_attention_masks", "[", "1", "]", "\n", "\n", "# Compute attended source", "\n", "# attn_weights   (batch_size * num_heads, target_size, source_size)", "\n", "# v              (batch_size * num_heads, source_size, target_emb_size / num_heads)", "\n", "# ->", "\n", "# attn           (batch_size * num_heads, target_size, target_emb_size / num_heads)", "\n", "", "attn", "=", "torch", ".", "bmm", "(", "attn_weights", ",", "v", ")", "\n", "if", "head_positions", "is", "not", "None", ":", "\n", "# if buffer/stack positions provided, add them to attention computation", "\n", "# Note that batched inner product is here implemented as", "\n", "# elements-wise product and sum across common axis", "\n", "# attn_weights   (batch_size * num_heads, target_size, source_size)", "\n", "# head_positions (batch_size * num_heads, source_size, target_size, target_emb_size / num_heads) ", "\n", "# ->", "\n", "# attn           (batch_size * num_heads, target_size, target_emb_size / num_heads)", "\n", "            ", "attn", "+=", "(", "attn_weights", ".", "transpose", "(", "1", ",", "2", ")", ".", "unsqueeze", "(", "3", ")", "*", "head_pos_emb_v", ")", ".", "sum", "(", "1", ")", "\n", "\n", "", "assert", "list", "(", "attn", ".", "size", "(", ")", ")", "==", "[", "bsz", "*", "self", ".", "num_heads", ",", "tgt_len", ",", "self", ".", "head_dim", "]", "\n", "if", "(", "self", ".", "onnx_trace", "and", "attn", ".", "size", "(", "1", ")", "==", "1", ")", ":", "\n", "# when ONNX tracing a single decoder step (sequence length == 1)", "\n", "# the transpose is a no-op copy before view, thus unnecessary", "\n", "            ", "attn", "=", "attn", ".", "contiguous", "(", ")", ".", "view", "(", "tgt_len", ",", "bsz", ",", "embed_dim", ")", "\n", "", "else", ":", "\n", "            ", "attn", "=", "attn", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", ".", "view", "(", "tgt_len", ",", "bsz", ",", "embed_dim", ")", "\n", "", "attn", "=", "self", ".", "out_proj", "(", "attn", ")", "\n", "\n", "if", "need_weights", ":", "\n", "# average attention weights over heads", "\n", "            ", "attn_weights", "=", "attn_weights", ".", "view", "(", "bsz", ",", "self", ".", "num_heads", ",", "tgt_len", ",", "src_len", ")", "\n", "attn_weights", "=", "attn_weights", ".", "sum", "(", "dim", "=", "1", ")", "/", "self", ".", "num_heads", "\n", "", "else", ":", "\n", "            ", "attn_weights", "=", "None", "\n", "\n", "", "return", "attn", ",", "attn_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.multihead_attention.MultiheadAttention.in_proj_qkv": [[367, 369], ["multihead_attention.MultiheadAttention._in_proj().chunk", "multihead_attention.MultiheadAttention._in_proj"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.multihead_attention.MultiheadAttention._in_proj"], ["", "def", "in_proj_qkv", "(", "self", ",", "query", ")", ":", "\n", "        ", "return", "self", ".", "_in_proj", "(", "query", ")", ".", "chunk", "(", "3", ",", "dim", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.multihead_attention.MultiheadAttention.in_proj_q": [[370, 378], ["multihead_attention.MultiheadAttention._in_proj", "torch.linear", "torch.linear"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.multihead_attention.MultiheadAttention._in_proj"], ["", "def", "in_proj_q", "(", "self", ",", "query", ")", ":", "\n", "        ", "if", "self", ".", "qkv_same_dim", ":", "\n", "            ", "return", "self", ".", "_in_proj", "(", "query", ",", "end", "=", "self", ".", "embed_dim", ")", "\n", "", "else", ":", "\n", "            ", "bias", "=", "self", ".", "in_proj_bias", "\n", "if", "bias", "is", "not", "None", ":", "\n", "                ", "bias", "=", "bias", "[", ":", "self", ".", "embed_dim", "]", "\n", "", "return", "F", ".", "linear", "(", "query", ",", "self", ".", "q_proj_weight", ",", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.multihead_attention.MultiheadAttention.in_proj_k": [[379, 388], ["multihead_attention.MultiheadAttention._in_proj", "torch.linear", "torch.linear"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.multihead_attention.MultiheadAttention._in_proj"], ["", "", "def", "in_proj_k", "(", "self", ",", "key", ")", ":", "\n", "        ", "if", "self", ".", "qkv_same_dim", ":", "\n", "            ", "return", "self", ".", "_in_proj", "(", "key", ",", "start", "=", "self", ".", "embed_dim", ",", "end", "=", "2", "*", "self", ".", "embed_dim", ")", "\n", "", "else", ":", "\n", "            ", "weight", "=", "self", ".", "k_proj_weight", "\n", "bias", "=", "self", ".", "in_proj_bias", "\n", "if", "bias", "is", "not", "None", ":", "\n", "                ", "bias", "=", "bias", "[", "self", ".", "embed_dim", ":", "2", "*", "self", ".", "embed_dim", "]", "\n", "", "return", "F", ".", "linear", "(", "key", ",", "weight", ",", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.multihead_attention.MultiheadAttention.in_proj_v": [[389, 398], ["multihead_attention.MultiheadAttention._in_proj", "torch.linear", "torch.linear"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.multihead_attention.MultiheadAttention._in_proj"], ["", "", "def", "in_proj_v", "(", "self", ",", "value", ")", ":", "\n", "        ", "if", "self", ".", "qkv_same_dim", ":", "\n", "            ", "return", "self", ".", "_in_proj", "(", "value", ",", "start", "=", "2", "*", "self", ".", "embed_dim", ")", "\n", "", "else", ":", "\n", "            ", "weight", "=", "self", ".", "v_proj_weight", "\n", "bias", "=", "self", ".", "in_proj_bias", "\n", "if", "bias", "is", "not", "None", ":", "\n", "                ", "bias", "=", "bias", "[", "2", "*", "self", ".", "embed_dim", ":", "]", "\n", "", "return", "F", ".", "linear", "(", "value", ",", "weight", ",", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.multihead_attention.MultiheadAttention._in_proj": [[399, 406], ["torch.linear", "torch.linear"], "methods", ["None"], ["", "", "def", "_in_proj", "(", "self", ",", "input", ",", "start", "=", "0", ",", "end", "=", "None", ")", ":", "\n", "        ", "weight", "=", "self", ".", "in_proj_weight", "\n", "bias", "=", "self", ".", "in_proj_bias", "\n", "weight", "=", "weight", "[", "start", ":", "end", ",", ":", "]", "\n", "if", "bias", "is", "not", "None", ":", "\n", "            ", "bias", "=", "bias", "[", "start", ":", "end", "]", "\n", "", "return", "F", ".", "linear", "(", "input", ",", "weight", ",", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.multihead_attention.MultiheadAttention.reorder_incremental_state": [[407, 414], ["multihead_attention.MultiheadAttention._get_input_buffer", "multihead_attention.MultiheadAttention.keys", "multihead_attention.MultiheadAttention._set_input_buffer", "input_buffer[].index_select"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.linearized_convolution.LinearizedConvolution._get_input_buffer", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.linearized_convolution.LinearizedConvolution._set_input_buffer"], ["", "def", "reorder_incremental_state", "(", "self", ",", "incremental_state", ",", "new_order", ")", ":", "\n", "        ", "\"\"\"Reorder buffered internal state (for incremental generation).\"\"\"", "\n", "input_buffer", "=", "self", ".", "_get_input_buffer", "(", "incremental_state", ")", "\n", "if", "input_buffer", "is", "not", "None", ":", "\n", "            ", "for", "k", "in", "input_buffer", ".", "keys", "(", ")", ":", "\n", "                ", "input_buffer", "[", "k", "]", "=", "input_buffer", "[", "k", "]", ".", "index_select", "(", "0", ",", "new_order", ")", "\n", "", "self", ".", "_set_input_buffer", "(", "incremental_state", ",", "input_buffer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.multihead_attention.MultiheadAttention._get_input_buffer": [[415, 421], ["fairseq.utils.get_incremental_state"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.get_incremental_state"], ["", "", "def", "_get_input_buffer", "(", "self", ",", "incremental_state", ")", ":", "\n", "        ", "return", "utils", ".", "get_incremental_state", "(", "\n", "self", ",", "\n", "incremental_state", ",", "\n", "'attn_state'", ",", "\n", ")", "or", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.multihead_attention.MultiheadAttention._set_input_buffer": [[422, 428], ["fairseq.utils.set_incremental_state"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.set_incremental_state"], ["", "def", "_set_input_buffer", "(", "self", ",", "incremental_state", ",", "buffer", ")", ":", "\n", "        ", "utils", ".", "set_incremental_state", "(", "\n", "self", ",", "\n", "incremental_state", ",", "\n", "'attn_state'", ",", "\n", "buffer", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.multihead_attention.MultiheadAttention.apply_sparse_mask": [[430, 432], ["None"], "methods", ["None"], ["", "def", "apply_sparse_mask", "(", "self", ",", "attn_weights", ",", "tgt_len", ",", "src_len", ",", "bsz", ")", ":", "\n", "        ", "return", "attn_weights", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.dynamic_convolution.DynamicConv1dTBC.__init__": [[46, 69], ["torch.Module.__init__", "dynamic_convolution.DynamicConv1dTBC.reset_parameters", "dynamic_convolution.Linear", "dynamic_convolution.Linear", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.highway.Highway.reset_parameters", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear"], ["def", "__init__", "(", "self", ",", "input_size", ",", "kernel_size", "=", "1", ",", "padding_l", "=", "None", ",", "num_heads", "=", "1", ",", "\n", "weight_dropout", "=", "0.", ",", "weight_softmax", "=", "False", ",", "\n", "renorm_padding", "=", "False", ",", "bias", "=", "False", ",", "conv_bias", "=", "False", ",", "\n", "query_size", "=", "None", ",", "in_proj", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "input_size", "=", "input_size", "\n", "self", ".", "query_size", "=", "input_size", "if", "query_size", "is", "None", "else", "query_size", "\n", "self", ".", "kernel_size", "=", "kernel_size", "\n", "self", ".", "padding_l", "=", "padding_l", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "self", ".", "weight_dropout", "=", "weight_dropout", "\n", "self", ".", "weight_softmax", "=", "weight_softmax", "\n", "self", ".", "renorm_padding", "=", "renorm_padding", "\n", "\n", "if", "in_proj", ":", "\n", "            ", "self", ".", "weight_linear", "=", "Linear", "(", "self", ".", "input_size", ",", "self", ".", "input_size", "+", "num_heads", "*", "kernel_size", "*", "1", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "weight_linear", "=", "Linear", "(", "self", ".", "query_size", ",", "num_heads", "*", "kernel_size", "*", "1", ",", "bias", "=", "bias", ")", "\n", "", "if", "conv_bias", ":", "\n", "            ", "self", ".", "conv_bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "input_size", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "conv_bias", "=", "None", "\n", "", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.dynamic_convolution.DynamicConv1dTBC.in_proj": [[70, 73], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "in_proj", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "weight_linear", ".", "out_features", "==", "self", ".", "input_size", "+", "self", ".", "num_heads", "*", "self", ".", "kernel_size", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.dynamic_convolution.DynamicConv1dTBC.reset_parameters": [[74, 78], ["dynamic_convolution.DynamicConv1dTBC.weight_linear.reset_parameters", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.highway.Highway.reset_parameters"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "self", ".", "weight_linear", ".", "reset_parameters", "(", ")", "\n", "if", "self", ".", "conv_bias", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "constant_", "(", "self", ".", "conv_bias", ",", "0.", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.dynamic_convolution.DynamicConv1dTBC.forward": [[79, 102], ["dynamic_convolution.DynamicConv1dTBC._forward_unfolded", "dynamic_convolution.DynamicConv1dTBC._forward_expanded", "x.size", "dynamic_convolution.DynamicConv1dTBC.conv_bias.view"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.lightweight_convolution.LightweightConv1dTBC._forward_unfolded", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.lightweight_convolution.LightweightConv1dTBC._forward_expanded", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "", "def", "forward", "(", "self", ",", "x", ",", "incremental_state", "=", "None", ",", "query", "=", "None", ",", "unfold", "=", "None", ")", ":", "\n", "        ", "'''Assuming the input, x, of the shape T x B x C and producing an output in the shape T x B x C\n        args:\n            x: Input of shape T x B x C, i.e. (timesteps, batch_size, input_size)\n            incremental_state: A dict to keep the state\n            unfold: unfold the input or not. If not, we use the matrix trick instead\n            query: use the specified query to predict the conv filters\n        '''", "\n", "unfold", "=", "x", ".", "size", "(", "0", ")", ">", "512", "if", "unfold", "is", "None", "else", "unfold", "# use unfold mode as default for long sequence to save memory", "\n", "unfold", "=", "unfold", "or", "(", "incremental_state", "is", "not", "None", ")", "\n", "assert", "query", "is", "None", "or", "not", "self", ".", "in_proj", "\n", "\n", "if", "query", "is", "None", ":", "\n", "            ", "query", "=", "x", "\n", "\n", "", "if", "unfold", ":", "\n", "            ", "output", "=", "self", ".", "_forward_unfolded", "(", "x", ",", "incremental_state", ",", "query", ")", "\n", "", "else", ":", "\n", "            ", "output", "=", "self", ".", "_forward_expanded", "(", "x", ",", "incremental_state", ",", "query", ")", "\n", "\n", "", "if", "self", ".", "conv_bias", "is", "not", "None", ":", "\n", "            ", "output", "=", "output", "+", "self", ".", "conv_bias", ".", "view", "(", "1", ",", "1", ",", "-", "1", ")", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.dynamic_convolution.DynamicConv1dTBC._forward_unfolded": [[103, 154], ["dynamic_convolution.DynamicConv1dTBC.narrow().contiguous.size", "weight.narrow.narrow.narrow", "torch.dropout", "torch.dropout", "torch.dropout", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "output.view.view.view", "dynamic_convolution.DynamicConv1dTBC.weight_linear", "dynamic_convolution.DynamicConv1dTBC.narrow().contiguous", "dynamic_convolution.DynamicConv1dTBC.narrow().contiguous().view", "dynamic_convolution.DynamicConv1dTBC.weight_linear().view", "dynamic_convolution.DynamicConv1dTBC._get_input_buffer", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "x_unfold.view.view.view", "unfold.unfold1d", "x_unfold.view.view.view", "torch.softmax", "torch.softmax", "torch.softmax", "weight.narrow.narrow.size", "torch.softmax", "torch.softmax", "torch.softmax", "weight.narrow.narrow.unsqueeze", "dynamic_convolution.DynamicConv1dTBC.narrow().contiguous.new", "dynamic_convolution.DynamicConv1dTBC._set_input_buffer", "weight.narrow.narrow.narrow", "dynamic_convolution.DynamicConv1dTBC.narrow", "dynamic_convolution.DynamicConv1dTBC.narrow().contiguous", "dynamic_convolution.DynamicConv1dTBC.weight_linear", "dynamic_convolution.DynamicConv1dTBC.narrow().contiguous.unsqueeze", "dynamic_convolution.DynamicConv1dTBC.narrow", "x_unfold.view.view.size"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.linearized_convolution.LinearizedConvolution._get_input_buffer", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.unfold.unfold1d", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.linearized_convolution.LinearizedConvolution._set_input_buffer", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "_forward_unfolded", "(", "self", ",", "x", ",", "incremental_state", ",", "query", ")", ":", "\n", "        ", "'''The conventional implementation of convolutions.\n        Unfolding the input by having a window shifting to the right.'''", "\n", "T", ",", "B", ",", "C", "=", "x", ".", "size", "(", ")", "\n", "K", ",", "H", "=", "self", ".", "kernel_size", ",", "self", ".", "num_heads", "\n", "R", "=", "C", "//", "H", "\n", "assert", "R", "*", "H", "==", "C", "==", "self", ".", "input_size", "\n", "\n", "if", "self", ".", "in_proj", ":", "\n", "            ", "proj", "=", "self", ".", "weight_linear", "(", "x", ")", "\n", "x", "=", "proj", ".", "narrow", "(", "2", ",", "0", ",", "self", ".", "input_size", ")", ".", "contiguous", "(", ")", "\n", "weight", "=", "proj", ".", "narrow", "(", "2", ",", "self", ".", "input_size", ",", "H", "*", "K", ")", ".", "contiguous", "(", ")", ".", "view", "(", "T", "*", "B", "*", "H", ",", "-", "1", ")", "\n", "", "else", ":", "\n", "            ", "weight", "=", "self", ".", "weight_linear", "(", "query", ")", ".", "view", "(", "T", "*", "B", "*", "H", ",", "-", "1", ")", "\n", "\n", "# renorm_padding is only implemented in _forward_expanded", "\n", "", "assert", "not", "self", ".", "renorm_padding", "or", "incremental_state", "is", "not", "None", "\n", "\n", "if", "incremental_state", "is", "not", "None", ":", "\n", "            ", "input_buffer", "=", "self", ".", "_get_input_buffer", "(", "incremental_state", ")", "\n", "if", "input_buffer", "is", "None", ":", "\n", "                ", "input_buffer", "=", "x", ".", "new", "(", ")", "\n", "", "x_unfold", "=", "torch", ".", "cat", "(", "[", "input_buffer", ",", "x", ".", "unsqueeze", "(", "3", ")", "]", ",", "dim", "=", "3", ")", "\n", "if", "self", ".", "kernel_size", ">", "1", ":", "\n", "                ", "self", ".", "_set_input_buffer", "(", "incremental_state", ",", "x_unfold", "[", ":", ",", ":", ",", ":", ",", "-", "self", ".", "kernel_size", "+", "1", ":", "]", ")", "\n", "", "x_unfold", "=", "x_unfold", ".", "view", "(", "T", "*", "B", "*", "H", ",", "R", ",", "-", "1", ")", "\n", "", "else", ":", "\n", "            ", "padding_l", "=", "self", ".", "padding_l", "\n", "if", "K", ">", "T", "and", "padding_l", "==", "K", "-", "1", ":", "\n", "                ", "weight", "=", "weight", ".", "narrow", "(", "1", ",", "K", "-", "T", ",", "T", ")", "\n", "K", ",", "padding_l", "=", "T", ",", "T", "-", "1", "\n", "# unfold the input: T x B x C --> T' x B x C x K", "\n", "", "x_unfold", "=", "unfold1d", "(", "x", ",", "K", ",", "padding_l", ",", "0", ")", "\n", "x_unfold", "=", "x_unfold", ".", "view", "(", "T", "*", "B", "*", "H", ",", "R", ",", "K", ")", "\n", "\n", "", "if", "self", ".", "weight_softmax", "and", "not", "self", ".", "renorm_padding", ":", "\n", "            ", "weight", "=", "F", ".", "softmax", "(", "weight", ",", "dim", "=", "1", ")", "\n", "", "weight", "=", "weight", ".", "narrow", "(", "1", ",", "0", ",", "K", ")", "\n", "\n", "if", "incremental_state", "is", "not", "None", ":", "\n", "            ", "weight", "=", "weight", "[", ":", ",", "-", "x_unfold", ".", "size", "(", "2", ")", ":", "]", "\n", "K", "=", "weight", ".", "size", "(", "1", ")", "\n", "\n", "", "if", "self", ".", "weight_softmax", "and", "self", ".", "renorm_padding", ":", "\n", "            ", "weight", "=", "F", ".", "softmax", "(", "weight", ",", "dim", "=", "1", ")", "\n", "\n", "", "weight", "=", "F", ".", "dropout", "(", "weight", ",", "self", ".", "weight_dropout", ",", "training", "=", "self", ".", "training", ",", "inplace", "=", "False", ")", "\n", "\n", "output", "=", "torch", ".", "bmm", "(", "x_unfold", ",", "weight", ".", "unsqueeze", "(", "2", ")", ")", "# T*B*H x R x 1", "\n", "output", "=", "output", ".", "view", "(", "T", ",", "B", ",", "C", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.dynamic_convolution.DynamicConv1dTBC._forward_expanded": [[155, 201], ["dynamic_convolution.DynamicConv1dTBC.narrow().contiguous.size", "weight.narrow.narrow.narrow().contiguous", "weight.narrow.narrow.view().transpose", "dynamic_convolution.DynamicConv1dTBC.narrow().contiguous.view().transpose", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "output.transpose().contiguous().view.transpose().contiguous().view.transpose().contiguous().view", "dynamic_convolution.DynamicConv1dTBC.weight_linear", "dynamic_convolution.DynamicConv1dTBC.narrow().contiguous", "dynamic_convolution.DynamicConv1dTBC.narrow().contiguous().view", "dynamic_convolution.DynamicConv1dTBC.weight_linear().view", "torch.dropout", "torch.dropout", "torch.dropout", "weight.narrow.narrow.new().fill_", "weight_expanded.narrow.narrow.as_strided().copy_", "weight_expanded.narrow.narrow.narrow", "torch.softmax", "torch.softmax", "torch.softmax", "torch.dropout", "torch.dropout", "torch.dropout", "weight.narrow.narrow.new_zeros", "weight_expanded.narrow.narrow.as_strided().copy_", "weight_expanded.narrow.narrow.narrow", "torch.softmax", "torch.softmax", "torch.softmax", "weight.narrow.narrow.narrow", "weight.narrow.narrow.view", "dynamic_convolution.DynamicConv1dTBC.narrow().contiguous.view", "float", "weight.narrow.narrow.narrow", "output.transpose().contiguous().view.transpose().contiguous().view.transpose().contiguous", "dynamic_convolution.DynamicConv1dTBC.narrow", "dynamic_convolution.DynamicConv1dTBC.narrow().contiguous", "dynamic_convolution.DynamicConv1dTBC.weight_linear", "weight.narrow.narrow.new", "weight_expanded.narrow.narrow.as_strided", "weight_expanded.narrow.narrow.as_strided", "output.transpose().contiguous().view.transpose().contiguous().view.transpose", "dynamic_convolution.DynamicConv1dTBC.narrow"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax"], ["", "def", "_forward_expanded", "(", "self", ",", "x", ",", "incremental_stat", ",", "query", ")", ":", "\n", "        ", "'''Turn the convolution filters into band matrices and do matrix multiplication.\n        This is faster when the sequence is short, but less memory efficient.\n        This is not used in the decoder during inference.\n        '''", "\n", "T", ",", "B", ",", "C", "=", "x", ".", "size", "(", ")", "\n", "K", ",", "H", "=", "self", ".", "kernel_size", ",", "self", ".", "num_heads", "\n", "R", "=", "C", "//", "H", "\n", "assert", "R", "*", "H", "==", "C", "==", "self", ".", "input_size", "\n", "if", "self", ".", "in_proj", ":", "\n", "            ", "proj", "=", "self", ".", "weight_linear", "(", "x", ")", "\n", "x", "=", "proj", ".", "narrow", "(", "2", ",", "0", ",", "self", ".", "input_size", ")", ".", "contiguous", "(", ")", "\n", "weight", "=", "proj", ".", "narrow", "(", "2", ",", "self", ".", "input_size", ",", "H", "*", "K", ")", ".", "contiguous", "(", ")", ".", "view", "(", "T", "*", "B", "*", "H", ",", "-", "1", ")", "\n", "", "else", ":", "\n", "            ", "weight", "=", "self", ".", "weight_linear", "(", "query", ")", ".", "view", "(", "T", "*", "B", "*", "H", ",", "-", "1", ")", "\n", "\n", "", "if", "not", "self", ".", "renorm_padding", ":", "\n", "            ", "if", "self", ".", "weight_softmax", ":", "\n", "                ", "weight", "=", "F", ".", "softmax", "(", "weight", ",", "dim", "=", "1", ")", "\n", "", "weight", "=", "F", ".", "dropout", "(", "weight", ",", "self", ".", "weight_dropout", ",", "training", "=", "self", ".", "training", ",", "inplace", "=", "False", ")", "\n", "", "weight", "=", "weight", ".", "narrow", "(", "1", ",", "0", ",", "K", ")", ".", "contiguous", "(", ")", "\n", "weight", "=", "weight", ".", "view", "(", "T", ",", "B", "*", "H", ",", "K", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "x", "=", "x", ".", "view", "(", "T", ",", "B", "*", "H", ",", "R", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "if", "self", ".", "weight_softmax", "and", "self", ".", "renorm_padding", ":", "\n", "# turn the convolution filters into band matrices", "\n", "            ", "weight_expanded", "=", "weight", ".", "new", "(", "B", "*", "H", ",", "T", ",", "T", "+", "K", "-", "1", ")", ".", "fill_", "(", "float", "(", "'-inf'", ")", ")", "\n", "weight_expanded", ".", "as_strided", "(", "(", "B", "*", "H", ",", "T", ",", "K", ")", ",", "(", "T", "*", "(", "T", "+", "K", "-", "1", ")", ",", "T", "+", "K", ",", "1", ")", ")", ".", "copy_", "(", "weight", ")", "\n", "weight_expanded", "=", "weight_expanded", ".", "narrow", "(", "2", ",", "self", ".", "padding_l", ",", "T", ")", "\n", "# normalize the weight over valid positions like self-attention", "\n", "weight_expanded", "=", "F", ".", "softmax", "(", "weight_expanded", ",", "dim", "=", "2", ")", "\n", "weight_expanded", "=", "F", ".", "dropout", "(", "weight_expanded", ",", "self", ".", "weight_dropout", ",", "training", "=", "self", ".", "training", ",", "inplace", "=", "False", ")", "\n", "", "else", ":", "\n", "            ", "P", "=", "self", ".", "padding_l", "\n", "# For efficieny, we cut the kernel size and reduce the padding when the kernel is larger than the length", "\n", "if", "K", ">", "T", "and", "P", "==", "K", "-", "1", ":", "\n", "                ", "weight", "=", "weight", ".", "narrow", "(", "2", ",", "K", "-", "T", ",", "T", ")", "\n", "K", ",", "P", "=", "T", ",", "T", "-", "1", "\n", "# turn the convolution filters into band matrices", "\n", "", "weight_expanded", "=", "weight", ".", "new_zeros", "(", "B", "*", "H", ",", "T", ",", "T", "+", "K", "-", "1", ",", "requires_grad", "=", "False", ")", "\n", "weight_expanded", ".", "as_strided", "(", "(", "B", "*", "H", ",", "T", ",", "K", ")", ",", "(", "T", "*", "(", "T", "+", "K", "-", "1", ")", ",", "T", "+", "K", ",", "1", ")", ")", ".", "copy_", "(", "weight", ")", "\n", "weight_expanded", "=", "weight_expanded", ".", "narrow", "(", "2", ",", "P", ",", "T", ")", "# B*H x T x T", "\n", "\n", "", "output", "=", "torch", ".", "bmm", "(", "weight_expanded", ",", "x", ")", "\n", "output", "=", "output", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", ".", "view", "(", "T", ",", "B", ",", "C", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.dynamic_convolution.DynamicConv1dTBC.reorder_incremental_state": [[202, 207], ["dynamic_convolution.DynamicConv1dTBC._get_input_buffer", "input_buffer.index_select.index_select.index_select", "dynamic_convolution.DynamicConv1dTBC._set_input_buffer"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.linearized_convolution.LinearizedConvolution._get_input_buffer", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.linearized_convolution.LinearizedConvolution._set_input_buffer"], ["", "def", "reorder_incremental_state", "(", "self", ",", "incremental_state", ",", "new_order", ")", ":", "\n", "        ", "input_buffer", "=", "self", ".", "_get_input_buffer", "(", "incremental_state", ")", "\n", "if", "input_buffer", "is", "not", "None", ":", "\n", "            ", "input_buffer", "=", "input_buffer", ".", "index_select", "(", "1", ",", "new_order", ")", "\n", "self", ".", "_set_input_buffer", "(", "incremental_state", ",", "input_buffer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.dynamic_convolution.DynamicConv1dTBC._get_input_buffer": [[208, 210], ["fairseq.utils.get_incremental_state"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.get_incremental_state"], ["", "", "def", "_get_input_buffer", "(", "self", ",", "incremental_state", ")", ":", "\n", "        ", "return", "utils", ".", "get_incremental_state", "(", "self", ",", "incremental_state", ",", "'input_buffer'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.dynamic_convolution.DynamicConv1dTBC._set_input_buffer": [[211, 213], ["fairseq.utils.set_incremental_state"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.set_incremental_state"], ["", "def", "_set_input_buffer", "(", "self", ",", "incremental_state", ",", "new_buffer", ")", ":", "\n", "        ", "return", "utils", ".", "set_incremental_state", "(", "self", ",", "incremental_state", ",", "'input_buffer'", ",", "new_buffer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.dynamic_convolution.DynamicConv1dTBC.extra_repr": [[214, 226], ["None"], "methods", ["None"], ["", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "s", "=", "'{}, kernel_size={}, padding_l={}, num_heads={}, weight_softmax={}, conv_bias={}, renorm_padding={}, in_proj={}'", ".", "format", "(", "\n", "self", ".", "input_size", ",", "self", ".", "kernel_size", ",", "self", ".", "padding_l", ",", "\n", "self", ".", "num_heads", ",", "self", ".", "weight_softmax", ",", "self", ".", "conv_bias", "is", "not", "None", ",", "self", ".", "renorm_padding", ",", "\n", "self", ".", "in_proj", ",", "\n", ")", "\n", "\n", "if", "self", ".", "query_size", "!=", "self", ".", "input_size", ":", "\n", "            ", "s", "+=", "', query_size={}'", ".", "format", "(", "self", ".", "query_size", ")", "\n", "", "if", "self", ".", "weight_dropout", ">", "0.", ":", "\n", "            ", "s", "+=", "', weight_dropout={}'", ".", "format", "(", "self", ".", "weight_dropout", ")", "\n", "", "return", "s", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.dynamic_convolution.Linear": [[14, 20], ["torch.Linear", "torch.init.xavier_uniform_", "torch.init.constant_"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear"], ["def", "Linear", "(", "in_features", ",", "out_features", ",", "bias", "=", "True", ")", ":", "\n", "    ", "m", "=", "nn", ".", "Linear", "(", "in_features", ",", "out_features", ",", "bias", ")", "\n", "nn", ".", "init", ".", "xavier_uniform_", "(", "m", ".", "weight", ")", "\n", "if", "bias", ":", "\n", "        ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0.", ")", "\n", "", "return", "m", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.adaptive_softmax.TiedLinear.__init__": [[15, 19], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["    ", "def", "__init__", "(", "self", ",", "weight", ",", "transpose", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "weight", "=", "weight", "\n", "self", ".", "transpose", "=", "transpose", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.adaptive_softmax.TiedLinear.forward": [[20, 22], ["torch.linear", "torch.linear", "adaptive_softmax.TiedLinear.weight.t"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "return", "F", ".", "linear", "(", "input", ",", "self", ".", "weight", ".", "t", "(", ")", "if", "self", ".", "transpose", "else", "self", ".", "weight", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.adaptive_softmax.TiedHeadModule.__init__": [[25, 41], ["torch.nn.Module.__init__", "tied_emb.size", "adaptive_softmax.TiedLinear", "torch.nn.Linear", "torch.nn.Linear", "adaptive_softmax.TiedHeadModule.register_buffer", "torch.nn.Sequential", "torch.nn.Sequential", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear"], ["    ", "def", "__init__", "(", "self", ",", "weights", ",", "input_dim", ",", "num_classes", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "tied_emb", ",", "_", "=", "weights", "\n", "self", ".", "num_words", ",", "emb_dim", "=", "tied_emb", ".", "size", "(", ")", "\n", "\n", "self", ".", "word_proj", "=", "TiedLinear", "(", "tied_emb", ",", "transpose", "=", "False", ")", "\n", "if", "input_dim", "!=", "emb_dim", ":", "\n", "            ", "self", ".", "word_proj", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "input_dim", ",", "emb_dim", ",", "bias", "=", "False", ")", ",", "\n", "self", ".", "word_proj", ",", "\n", ")", "\n", "\n", "", "self", ".", "class_proj", "=", "nn", ".", "Linear", "(", "input_dim", ",", "num_classes", ",", "bias", "=", "False", ")", "\n", "self", ".", "out_dim", "=", "self", ".", "num_words", "+", "num_classes", "\n", "\n", "self", ".", "register_buffer", "(", "'_float_tensor'", ",", "torch", ".", "FloatTensor", "(", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.adaptive_softmax.TiedHeadModule.forward": [[42, 48], ["functools.reduce", "adaptive_softmax.TiedHeadModule._float_tensor.new", "adaptive_softmax.TiedHeadModule.word_proj", "adaptive_softmax.TiedHeadModule.class_proj", "input.view", "input.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "inp_sz", "=", "functools", ".", "reduce", "(", "operator", ".", "mul", ",", "input", ".", "shape", "[", ":", "-", "1", "]", ",", "1", ")", "\n", "out", "=", "self", ".", "_float_tensor", ".", "new", "(", "inp_sz", ",", "self", ".", "out_dim", ")", "\n", "out", "[", ":", ",", ":", "self", ".", "num_words", "]", "=", "self", ".", "word_proj", "(", "input", ".", "view", "(", "inp_sz", ",", "-", "1", ")", ")", "\n", "out", "[", ":", ",", "self", ".", "num_words", ":", "]", "=", "self", ".", "class_proj", "(", "input", ".", "view", "(", "inp_sz", ",", "-", "1", ")", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.adaptive_softmax.AdaptiveSoftmax.__init__": [[57, 90], ["torch.nn.Module.__init__", "torch.nn.LogSoftmax", "torch.nn.LogSoftmax", "adaptive_softmax.AdaptiveSoftmax._make_tail", "adaptive_softmax.AdaptiveSoftmax.apply", "adaptive_softmax.AdaptiveSoftmax.register_buffer", "adaptive_softmax.TiedHeadModule", "torch.nn.Linear", "torch.nn.Linear", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "len", "adaptive_inputs.weights_for_band", "hasattr", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "len", "isinstance", "isinstance"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.adaptive_softmax.AdaptiveSoftmax._make_tail", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.adaptive_input.AdaptiveInput.weights_for_band"], ["def", "__init__", "(", "self", ",", "vocab_size", ",", "input_dim", ",", "cutoff", ",", "dropout", ",", "factor", "=", "4.", ",", "adaptive_inputs", "=", "None", ",", "tie_proj", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "if", "vocab_size", ">", "cutoff", "[", "-", "1", "]", ":", "\n", "            ", "cutoff", "=", "cutoff", "+", "[", "vocab_size", "]", "\n", "", "else", ":", "\n", "            ", "assert", "vocab_size", "==", "cutoff", "[", "\n", "-", "1", "]", ",", "'cannot specify cutoff larger than vocab size'", "\n", "\n", "", "output_dim", "=", "cutoff", "[", "0", "]", "+", "len", "(", "cutoff", ")", "-", "1", "\n", "\n", "self", ".", "vocab_size", "=", "vocab_size", "\n", "self", ".", "cutoff", "=", "cutoff", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "input_dim", "=", "input_dim", "\n", "self", ".", "factor", "=", "factor", "\n", "\n", "self", ".", "lsm", "=", "nn", ".", "LogSoftmax", "(", "dim", "=", "1", ")", "\n", "\n", "if", "adaptive_inputs", "is", "not", "None", ":", "\n", "            ", "self", ".", "head", "=", "TiedHeadModule", "(", "adaptive_inputs", ".", "weights_for_band", "(", "0", ")", ",", "input_dim", ",", "len", "(", "cutoff", ")", "-", "1", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "head", "=", "nn", ".", "Linear", "(", "input_dim", ",", "output_dim", ",", "bias", "=", "False", ")", "\n", "\n", "", "self", ".", "_make_tail", "(", "adaptive_inputs", ",", "tie_proj", ")", "\n", "\n", "def", "init_weights", "(", "m", ")", ":", "\n", "            ", "if", "hasattr", "(", "m", ",", "'weight'", ")", "and", "not", "isinstance", "(", "m", ",", "TiedLinear", ")", "and", "not", "isinstance", "(", "m", ",", "TiedHeadModule", ")", ":", "\n", "                ", "nn", ".", "init", ".", "xavier_uniform_", "(", "m", ".", "weight", ")", "\n", "\n", "", "", "self", ".", "apply", "(", "init_weights", ")", "\n", "\n", "self", ".", "register_buffer", "(", "'version'", ",", "torch", ".", "LongTensor", "(", "[", "1", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.adaptive_softmax.AdaptiveSoftmax._make_tail": [[91, 116], ["torch.nn.ModuleList", "torch.nn.ModuleList", "range", "int", "torch.nn.Sequential", "torch.nn.Sequential", "adaptive_softmax.AdaptiveSoftmax.tail.append", "len", "adaptive_inputs.weights_for_band", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "torch.nn.Dropout", "adaptive_softmax.TiedLinear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "adaptive_softmax.TiedLinear", "tied_proj.size", "tied_proj.size"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.adaptive_input.AdaptiveInput.weights_for_band", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "_make_tail", "(", "self", ",", "adaptive_inputs", "=", "None", ",", "tie_proj", "=", "False", ")", ":", "\n", "        ", "self", ".", "tail", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "cutoff", ")", "-", "1", ")", ":", "\n", "            ", "dim", "=", "int", "(", "self", ".", "input_dim", "//", "self", ".", "factor", "**", "(", "i", "+", "1", ")", ")", "\n", "\n", "tied_emb", ",", "tied_proj", "=", "adaptive_inputs", ".", "weights_for_band", "(", "i", "+", "1", ")", "if", "adaptive_inputs", "is", "not", "None", "else", "(", "None", ",", "None", ")", "\n", "\n", "if", "tied_proj", "is", "not", "None", ":", "\n", "                ", "if", "tie_proj", ":", "\n", "                    ", "proj", "=", "TiedLinear", "(", "tied_proj", ",", "transpose", "=", "True", ")", "\n", "", "else", ":", "\n", "                    ", "proj", "=", "nn", ".", "Linear", "(", "tied_proj", ".", "size", "(", "0", ")", ",", "tied_proj", ".", "size", "(", "1", ")", ",", "bias", "=", "False", ")", "\n", "", "", "else", ":", "\n", "                ", "proj", "=", "nn", ".", "Linear", "(", "self", ".", "input_dim", ",", "dim", ",", "bias", "=", "False", ")", "\n", "\n", "", "m", "=", "nn", ".", "Sequential", "(", "\n", "proj", ",", "\n", "nn", ".", "Dropout", "(", "self", ".", "dropout", ")", ",", "\n", "nn", ".", "Linear", "(", "\n", "dim", ",", "self", ".", "cutoff", "[", "i", "+", "1", "]", "-", "self", ".", "cutoff", "[", "i", "]", ",", "bias", "=", "False", ",", "\n", ")", "if", "tied_emb", "is", "None", "else", "TiedLinear", "(", "tied_emb", ",", "transpose", "=", "False", ")", ",", "\n", ")", "\n", "\n", "self", ".", "tail", ".", "append", "(", "m", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.adaptive_softmax.AdaptiveSoftmax.upgrade_state_dict_named": [[117, 121], ["Exception"], "methods", ["None"], ["", "", "def", "upgrade_state_dict_named", "(", "self", ",", "state_dict", ",", "name", ")", ":", "\n", "        ", "version_name", "=", "name", "+", "'.version'", "\n", "if", "version_name", "not", "in", "state_dict", ":", "\n", "            ", "raise", "Exception", "(", "'This version of the model is no longer supported'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.adaptive_softmax.AdaptiveSoftmax.adapt_target": [[122, 146], ["target.view.view.view", "range", "target.view.view.clone", "target.view.view.ge().mul", "target.view.ge().mul.any", "len", "target.view.view.lt", "target_idxs.append", "new_target.append", "target_idxs.append", "new_target.append", "target.view.view.ge", "target.view.ge().mul.nonzero().squeeze", "target[].add", "target.view.ge().mul.nonzero"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.add"], ["", "", "def", "adapt_target", "(", "self", ",", "target", ")", ":", "\n", "        ", "\"\"\"\n        In order to be efficient, the AdaptiveSoftMax does not compute the\n        scores for all the word of the vocabulary for all the examples. It is\n        thus necessary to call the method adapt_target of the AdaptiveSoftMax\n        layer inside each forward pass.\n        \"\"\"", "\n", "\n", "target", "=", "target", ".", "view", "(", "-", "1", ")", "\n", "new_target", "=", "[", "target", ".", "clone", "(", ")", "]", "\n", "target_idxs", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "cutoff", ")", "-", "1", ")", ":", "\n", "            ", "mask", "=", "target", ".", "ge", "(", "self", ".", "cutoff", "[", "i", "]", ")", ".", "mul", "(", "target", ".", "lt", "(", "self", ".", "cutoff", "[", "i", "+", "1", "]", ")", ")", "\n", "new_target", "[", "0", "]", "[", "mask", "]", "=", "self", ".", "cutoff", "[", "0", "]", "+", "i", "\n", "\n", "if", "mask", ".", "any", "(", ")", ":", "\n", "                ", "target_idxs", ".", "append", "(", "mask", ".", "nonzero", "(", ")", ".", "squeeze", "(", "1", ")", ")", "\n", "new_target", ".", "append", "(", "target", "[", "mask", "]", ".", "add", "(", "-", "self", ".", "cutoff", "[", "i", "]", ")", ")", "\n", "", "else", ":", "\n", "                ", "target_idxs", ".", "append", "(", "None", ")", "\n", "new_target", ".", "append", "(", "None", ")", "\n", "\n", "", "", "return", "new_target", ",", "target_idxs", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.adaptive_softmax.AdaptiveSoftmax.forward": [[147, 169], ["torch.dropout.contiguous().view", "torch.dropout", "torch.dropout", "adaptive_softmax.AdaptiveSoftmax.adapt_target", "range", "torch.dropout.size", "adaptive_softmax.AdaptiveSoftmax.head", "len", "torch.dropout.contiguous", "output.append", "output.append", "torch.dropout.index_select"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.adaptive_softmax.AdaptiveSoftmax.adapt_target", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "forward", "(", "self", ",", "input", ",", "target", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            input: (b x t x d)\n            target: (b x t)\n        Returns:\n            2 lists: output for each cutoff section and new targets by cut off\n        \"\"\"", "\n", "\n", "input", "=", "input", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "input", ".", "size", "(", "-", "1", ")", ")", "\n", "input", "=", "F", ".", "dropout", "(", "input", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "\n", "new_target", ",", "target_idxs", "=", "self", ".", "adapt_target", "(", "target", ")", "\n", "output", "=", "[", "self", ".", "head", "(", "input", ")", "]", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "target_idxs", ")", ")", ":", "\n", "            ", "if", "target_idxs", "[", "i", "]", "is", "not", "None", ":", "\n", "                ", "output", ".", "append", "(", "self", ".", "tail", "[", "i", "]", "(", "input", ".", "index_select", "(", "0", ",", "target_idxs", "[", "i", "]", ")", ")", ")", "\n", "", "else", ":", "\n", "                ", "output", ".", "append", "(", "None", ")", "\n", "\n", "", "", "return", "output", ",", "new_target", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.adaptive_softmax.AdaptiveSoftmax.get_log_prob": [[170, 207], ["input.contiguous().view.contiguous().view.size", "input.contiguous().view.contiguous().view.contiguous().view", "adaptive_softmax.AdaptiveSoftmax.head", "adaptive_softmax.AdaptiveSoftmax.new_zeros", "adaptive_softmax.AdaptiveSoftmax.lsm", "log_probs[].clone", "range", "log_probs.view.view.view", "adaptive_softmax.AdaptiveSoftmax.adapt_target", "input.contiguous().view.contiguous().view.size", "len", "len", "input.contiguous().view.contiguous().view.contiguous", "tail_out.copy_", "adaptive_softmax.AdaptiveSoftmax.lsm().add_", "tail_out.copy_", "adaptive_softmax.AdaptiveSoftmax.lsm().add_", "adaptive_softmax.AdaptiveSoftmax.lsm", "adaptive_softmax.AdaptiveSoftmax.lsm"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.adaptive_softmax.AdaptiveSoftmax.adapt_target", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "get_log_prob", "(", "self", ",", "input", ",", "target", ")", ":", "\n", "        ", "\"\"\"\n        Computes the log probabilities for all the words of the vocabulary,\n        given a 2D tensor of hidden vectors.\n        \"\"\"", "\n", "\n", "bsz", ",", "length", ",", "dim", "=", "input", ".", "size", "(", ")", "\n", "input", "=", "input", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "dim", ")", "\n", "\n", "if", "target", "is", "not", "None", ":", "\n", "            ", "_", ",", "target_idxs", "=", "self", ".", "adapt_target", "(", "target", ")", "\n", "", "else", ":", "\n", "            ", "target_idxs", "=", "None", "\n", "\n", "", "head_y", "=", "self", ".", "head", "(", "input", ")", "\n", "log_probs", "=", "head_y", ".", "new_zeros", "(", "input", ".", "size", "(", "0", ")", ",", "self", ".", "vocab_size", ")", "\n", "\n", "head_sz", "=", "self", ".", "cutoff", "[", "0", "]", "+", "len", "(", "self", ".", "tail", ")", "\n", "log_probs", "[", ":", ",", ":", "head_sz", "]", "=", "self", ".", "lsm", "(", "head_y", ")", "\n", "tail_priors", "=", "log_probs", "[", ":", ",", "self", ".", "cutoff", "[", "0", "]", ":", "head_sz", "]", ".", "clone", "(", ")", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "tail", ")", ")", ":", "\n", "            ", "start", "=", "self", ".", "cutoff", "[", "i", "]", "\n", "end", "=", "self", ".", "cutoff", "[", "i", "+", "1", "]", "\n", "\n", "if", "target_idxs", "is", "None", ":", "\n", "                ", "tail_out", "=", "log_probs", "[", ":", ",", "start", ":", "end", "]", "\n", "tail_out", ".", "copy_", "(", "self", ".", "tail", "[", "i", "]", "(", "input", ")", ")", "\n", "log_probs", "[", ":", ",", "start", ":", "end", "]", "=", "self", ".", "lsm", "(", "tail_out", ")", ".", "add_", "(", "tail_priors", "[", ":", ",", "i", ",", "None", "]", ")", "\n", "", "elif", "target_idxs", "[", "i", "]", "is", "not", "None", ":", "\n", "                ", "idxs", "=", "target_idxs", "[", "i", "]", "\n", "tail_out", "=", "log_probs", "[", "idxs", ",", "start", ":", "end", "]", "\n", "tail_out", ".", "copy_", "(", "self", ".", "tail", "[", "i", "]", "(", "input", "[", "idxs", "]", ")", ")", "\n", "log_probs", "[", "idxs", ",", "start", ":", "end", "]", "=", "self", ".", "lsm", "(", "tail_out", ")", ".", "add_", "(", "tail_priors", "[", "idxs", ",", "i", ",", "None", "]", ")", "\n", "\n", "", "", "log_probs", "=", "log_probs", ".", "view", "(", "bsz", ",", "length", ",", "-", "1", ")", "\n", "return", "log_probs", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.beamable_mm.BeamableMM.__init__": [[18, 21], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["def", "__init__", "(", "self", ",", "beam_size", "=", "None", ")", ":", "\n", "        ", "super", "(", "BeamableMM", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "beam_size", "=", "beam_size", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.beamable_mm.BeamableMM.forward": [[22, 45], ["input1[].unfold().transpose", "input1[].unfold().transpose.bmm.view", "input1[].unfold().transpose.bmm", "input1[].unfold().transpose.dim", "input1[].unfold().transpose.size", "input1[].unfold().transpose.size", "input2.unfold", "input1[].unfold().transpose.size", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "input1[].unfold().transpose.bmm", "input1[].unfold"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "forward", "(", "self", ",", "input1", ",", "input2", ")", ":", "\n", "        ", "if", "(", "\n", "not", "self", ".", "training", "and", "# test mode", "\n", "self", ".", "beam_size", "is", "not", "None", "and", "# beam size is set", "\n", "input1", ".", "dim", "(", ")", "==", "3", "and", "# only support batched input", "\n", "input1", ".", "size", "(", "1", ")", "==", "1", "# single time step update", "\n", ")", ":", "\n", "            ", "bsz", ",", "beam", "=", "input1", ".", "size", "(", "0", ")", ",", "self", ".", "beam_size", "\n", "\n", "# bsz x 1 x nhu --> bsz/beam x beam x nhu", "\n", "input1", "=", "input1", "[", ":", ",", "0", ",", ":", "]", ".", "unfold", "(", "0", ",", "beam", ",", "beam", ")", ".", "transpose", "(", "2", ",", "1", ")", "\n", "\n", "# bsz x sz2 x nhu --> bsz/beam x sz2 x nhu", "\n", "input2", "=", "input2", ".", "unfold", "(", "0", ",", "beam", ",", "beam", ")", "[", ":", ",", ":", ",", ":", ",", "0", "]", "\n", "\n", "# use non batched operation if bsz = beam", "\n", "if", "input1", ".", "size", "(", "0", ")", "==", "1", ":", "\n", "                ", "output", "=", "torch", ".", "mm", "(", "input1", "[", "0", ",", ":", ",", ":", "]", ",", "input2", "[", "0", ",", ":", ",", ":", "]", ")", "\n", "", "else", ":", "\n", "                ", "output", "=", "input1", ".", "bmm", "(", "input2", ")", "\n", "", "return", "output", ".", "view", "(", "bsz", ",", "1", ",", "-", "1", ")", "\n", "", "else", ":", "\n", "            ", "return", "input1", ".", "bmm", "(", "input2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.beamable_mm.BeamableMM.set_beam_size": [[46, 48], ["None"], "methods", ["None"], ["", "", "def", "set_beam_size", "(", "self", ",", "beam_size", ")", ":", "\n", "        ", "self", ".", "beam_size", "=", "beam_size", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.conv_tbc.ConvTBC.__init__": [[16, 26], ["super().__init__", "torch.nn.modules.utils._single", "torch.nn.modules.utils._single", "torch.nn.Parameter", "torch.nn.Parameter", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "padding", "=", "0", ")", ":", "\n", "        ", "super", "(", "ConvTBC", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "in_channels", "=", "in_channels", "\n", "self", ".", "out_channels", "=", "out_channels", "\n", "self", ".", "kernel_size", "=", "_single", "(", "kernel_size", ")", "\n", "self", ".", "padding", "=", "_single", "(", "padding", ")", "\n", "\n", "self", ".", "weight", "=", "torch", ".", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "\n", "self", ".", "kernel_size", "[", "0", "]", ",", "in_channels", ",", "out_channels", ")", ")", "\n", "self", ".", "bias", "=", "torch", ".", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "out_channels", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.conv_tbc.ConvTBC.forward": [[27, 29], ["torch.conv_tbc", "input.contiguous"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "return", "torch", ".", "conv_tbc", "(", "input", ".", "contiguous", "(", ")", ",", "self", ".", "weight", ",", "self", ".", "bias", ",", "self", ".", "padding", "[", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.conv_tbc.ConvTBC.__repr__": [[30, 37], ["s.format"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "s", "=", "(", "'{name}({in_channels}, {out_channels}, kernel_size={kernel_size}'", "\n", "', padding={padding}'", ")", "\n", "if", "self", ".", "bias", "is", "None", ":", "\n", "            ", "s", "+=", "', bias=False'", "\n", "", "s", "+=", "')'", "\n", "return", "s", ".", "format", "(", "name", "=", "self", ".", "__class__", ".", "__name__", ",", "**", "self", ".", "__dict__", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.character_token_embedder.CharacterTokenEmbedder.__init__": [[21, 59], ["super().__init__", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.ModuleList", "torch.nn.ModuleList", "sum", "torch.nn.Linear", "torch.nn.Linear", "character_token_embedder.CharacterTokenEmbedder.reset_parameters", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "character_token_embedder.CharacterTokenEmbedder.convolutions.append", "highway.Highway", "character_token_embedder.CharacterTokenEmbedder.set_vocab", "torch.nn.Conv1d", "torch.nn.Conv1d"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.Embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.Embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.highway.Highway.reset_parameters", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.character_token_embedder.CharacterTokenEmbedder.set_vocab"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "vocab", ":", "Dictionary", ",", "\n", "filters", ":", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", ",", "\n", "char_embed_dim", ":", "int", ",", "\n", "word_embed_dim", ":", "int", ",", "\n", "highway_layers", ":", "int", ",", "\n", "max_char_len", ":", "int", "=", "50", ",", "\n", "char_inputs", ":", "bool", "=", "False", "\n", ")", ":", "\n", "        ", "super", "(", "CharacterTokenEmbedder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "onnx_trace", "=", "False", "\n", "self", ".", "embedding_dim", "=", "word_embed_dim", "\n", "self", ".", "max_char_len", "=", "max_char_len", "\n", "self", ".", "char_embeddings", "=", "nn", ".", "Embedding", "(", "257", ",", "char_embed_dim", ",", "padding_idx", "=", "0", ")", "\n", "self", ".", "symbol_embeddings", "=", "nn", ".", "Parameter", "(", "torch", ".", "FloatTensor", "(", "2", ",", "word_embed_dim", ")", ")", "\n", "self", ".", "eos_idx", ",", "self", ".", "unk_idx", "=", "0", ",", "1", "\n", "self", ".", "char_inputs", "=", "char_inputs", "\n", "\n", "self", ".", "convolutions", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "width", ",", "out_c", "in", "filters", ":", "\n", "            ", "self", ".", "convolutions", ".", "append", "(", "\n", "nn", ".", "Conv1d", "(", "char_embed_dim", ",", "out_c", ",", "kernel_size", "=", "width", ")", "\n", ")", "\n", "\n", "", "last_dim", "=", "sum", "(", "f", "[", "1", "]", "for", "f", "in", "filters", ")", "\n", "\n", "self", ".", "highway", "=", "Highway", "(", "last_dim", ",", "highway_layers", ")", "if", "highway_layers", ">", "0", "else", "None", "\n", "\n", "self", ".", "projection", "=", "nn", ".", "Linear", "(", "last_dim", ",", "word_embed_dim", ")", "\n", "\n", "assert", "vocab", "is", "not", "None", "or", "char_inputs", ",", "\"vocab must be set if not using char inputs\"", "\n", "self", ".", "vocab", "=", "None", "\n", "if", "vocab", "is", "not", "None", ":", "\n", "            ", "self", ".", "set_vocab", "(", "vocab", ",", "max_char_len", ")", "\n", "\n", "", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.character_token_embedder.CharacterTokenEmbedder.prepare_for_onnx_export_": [[60, 62], ["None"], "methods", ["None"], ["", "def", "prepare_for_onnx_export_", "(", "self", ")", ":", "\n", "        ", "self", ".", "onnx_trace", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.character_token_embedder.CharacterTokenEmbedder.set_vocab": [[63, 84], ["torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "range", "len", "len", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "print", "vocab[].encode", "len", "len"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.encode"], ["", "def", "set_vocab", "(", "self", ",", "vocab", ",", "max_char_len", ")", ":", "\n", "        ", "word_to_char", "=", "torch", ".", "LongTensor", "(", "len", "(", "vocab", ")", ",", "max_char_len", ")", "\n", "\n", "truncated", "=", "0", "\n", "for", "i", "in", "range", "(", "len", "(", "vocab", ")", ")", ":", "\n", "            ", "if", "i", "<", "vocab", ".", "nspecial", ":", "\n", "                ", "char_idxs", "=", "[", "0", "]", "*", "max_char_len", "\n", "", "else", ":", "\n", "                ", "chars", "=", "vocab", "[", "i", "]", ".", "encode", "(", ")", "\n", "# +1 for padding", "\n", "char_idxs", "=", "[", "c", "+", "1", "for", "c", "in", "chars", "]", "+", "[", "0", "]", "*", "(", "max_char_len", "-", "len", "(", "chars", ")", ")", "\n", "", "if", "len", "(", "char_idxs", ")", ">", "max_char_len", ":", "\n", "                ", "truncated", "+=", "1", "\n", "char_idxs", "=", "char_idxs", "[", ":", "max_char_len", "]", "\n", "", "word_to_char", "[", "i", "]", "=", "torch", ".", "LongTensor", "(", "char_idxs", ")", "\n", "\n", "", "if", "truncated", ">", "0", ":", "\n", "            ", "print", "(", "'Truncated {} words longer than {} characters'", ".", "format", "(", "truncated", ",", "max_char_len", ")", ")", "\n", "\n", "", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "word_to_char", "=", "word_to_char", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.character_token_embedder.CharacterTokenEmbedder.padding_idx": [[85, 88], ["fairseq.data.Dictionary().pad", "character_token_embedder.CharacterTokenEmbedder.vocab.pad", "fairseq.data.Dictionary"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad"], ["", "@", "property", "\n", "def", "padding_idx", "(", "self", ")", ":", "\n", "        ", "return", "Dictionary", "(", ")", ".", "pad", "(", ")", "if", "self", ".", "vocab", "is", "None", "else", "self", ".", "vocab", ".", "pad", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.character_token_embedder.CharacterTokenEmbedder.reset_parameters": [[89, 96], ["torch.nn.init.xavier_normal_", "torch.nn.init.xavier_normal_", "torch.nn.init.xavier_normal_", "torch.nn.init.xavier_normal_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.constant_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "nn", ".", "init", ".", "xavier_normal_", "(", "self", ".", "char_embeddings", ".", "weight", ")", "\n", "nn", ".", "init", ".", "xavier_normal_", "(", "self", ".", "symbol_embeddings", ")", "\n", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "projection", ".", "weight", ")", "\n", "\n", "nn", ".", "init", ".", "constant_", "(", "self", ".", "char_embeddings", ".", "weight", "[", "self", ".", "char_embeddings", ".", "padding_idx", "]", ",", "0.", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "self", ".", "projection", ".", "bias", ",", "0.", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.character_token_embedder.CharacterTokenEmbedder.forward": [[97, 136], ["character_token_embedder.CharacterTokenEmbedder._convolve", "torch.where.view", "torch.where.view", "input.view", "chars[].eq", "chars[].eq", "input.view.eq.any", "input.view", "character_token_embedder.CharacterTokenEmbedder.word_to_char[].type_as", "input.view.eq", "input.view.eq", "input.view.eq", "input.view.eq.any", "input.view.eq.any", "input.view.eq.any", "input.view.eq.any", "character_token_embedder.CharacterTokenEmbedder.vocab.pad", "character_token_embedder.CharacterTokenEmbedder.vocab.eos", "character_token_embedder.CharacterTokenEmbedder.vocab.unk", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "input.view.eq.any", "torch.where", "torch.where", "torch.where", "torch.where", "input.view.eq.any", "torch.where", "torch.where", "torch.where", "torch.where", "input.view.eq.unsqueeze", "torch.where.new_zeros", "torch.where.new_zeros", "input.view.eq.unsqueeze", "input.view.eq.unsqueeze", "input.size", "input.view.eq.unsqueeze", "torch.where.new_zeros", "torch.where.new_zeros", "input.view.type_as"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.character_token_embedder.CharacterTokenEmbedder._convolve", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.unk", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "input", ":", "torch", ".", "Tensor", ",", "\n", ")", ":", "\n", "        ", "if", "self", ".", "char_inputs", ":", "\n", "            ", "chars", "=", "input", ".", "view", "(", "-", "1", ",", "self", ".", "max_char_len", ")", "\n", "pads", "=", "chars", "[", ":", ",", "0", "]", ".", "eq", "(", "CHAR_PAD_IDX", ")", "\n", "eos", "=", "chars", "[", ":", ",", "0", "]", ".", "eq", "(", "CHAR_EOS_IDX", ")", "\n", "if", "eos", ".", "any", "(", ")", ":", "\n", "                ", "if", "self", ".", "onnx_trace", ":", "\n", "                    ", "chars", "=", "torch", ".", "where", "(", "eos", ".", "unsqueeze", "(", "1", ")", ",", "chars", ".", "new_zeros", "(", "1", ")", ",", "chars", ")", "\n", "", "else", ":", "\n", "                    ", "chars", "[", "eos", "]", "=", "0", "\n", "\n", "", "", "unk", "=", "None", "\n", "", "else", ":", "\n", "            ", "flat_words", "=", "input", ".", "view", "(", "-", "1", ")", "\n", "chars", "=", "self", ".", "word_to_char", "[", "flat_words", ".", "type_as", "(", "self", ".", "word_to_char", ")", "]", ".", "type_as", "(", "input", ")", "\n", "pads", "=", "flat_words", ".", "eq", "(", "self", ".", "vocab", ".", "pad", "(", ")", ")", "\n", "eos", "=", "flat_words", ".", "eq", "(", "self", ".", "vocab", ".", "eos", "(", ")", ")", "\n", "unk", "=", "flat_words", ".", "eq", "(", "self", ".", "vocab", ".", "unk", "(", ")", ")", "\n", "\n", "", "word_embs", "=", "self", ".", "_convolve", "(", "chars", ")", "\n", "if", "self", ".", "onnx_trace", ":", "\n", "            ", "if", "pads", ".", "any", "(", ")", ":", "\n", "                ", "word_embs", "=", "torch", ".", "where", "(", "pads", ".", "unsqueeze", "(", "1", ")", ",", "word_embs", ".", "new_zeros", "(", "1", ")", ",", "word_embs", ")", "\n", "", "if", "eos", ".", "any", "(", ")", ":", "\n", "                ", "word_embs", "=", "torch", ".", "where", "(", "eos", ".", "unsqueeze", "(", "1", ")", ",", "self", ".", "symbol_embeddings", "[", "self", ".", "eos_idx", "]", ",", "word_embs", ")", "\n", "", "if", "unk", "is", "not", "None", "and", "unk", ".", "any", "(", ")", ":", "\n", "                ", "word_embs", "=", "torch", ".", "where", "(", "unk", ".", "unsqueeze", "(", "1", ")", ",", "self", ".", "symbol_embeddings", "[", "self", ".", "unk_idx", "]", ",", "word_embs", ")", "\n", "", "", "else", ":", "\n", "            ", "if", "pads", ".", "any", "(", ")", ":", "\n", "                ", "word_embs", "[", "pads", "]", "=", "0", "\n", "", "if", "eos", ".", "any", "(", ")", ":", "\n", "                ", "word_embs", "[", "eos", "]", "=", "self", ".", "symbol_embeddings", "[", "self", ".", "eos_idx", "]", "\n", "", "if", "unk", "is", "not", "None", "and", "unk", ".", "any", "(", ")", ":", "\n", "                ", "word_embs", "[", "unk", "]", "=", "self", ".", "symbol_embeddings", "[", "self", ".", "unk_idx", "]", "\n", "\n", "", "", "return", "word_embs", ".", "view", "(", "input", ".", "size", "(", ")", "[", ":", "2", "]", "+", "(", "-", "1", ",", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.character_token_embedder.CharacterTokenEmbedder._convolve": [[137, 159], ["character_token_embedder.CharacterTokenEmbedder.char_embeddings", "char_embs.transpose.transpose.transpose", "enumerate", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "character_token_embedder.CharacterTokenEmbedder.projection", "conv", "torch.max", "torch.max", "torch.max", "torch.max", "torch.relu", "torch.relu", "conv_result.append", "character_token_embedder.CharacterTokenEmbedder.highway"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "_convolve", "(", "\n", "self", ",", "\n", "char_idxs", ":", "torch", ".", "Tensor", ",", "\n", ")", ":", "\n", "        ", "char_embs", "=", "self", ".", "char_embeddings", "(", "char_idxs", ")", "\n", "char_embs", "=", "char_embs", ".", "transpose", "(", "1", ",", "2", ")", "# BTC -> BCT", "\n", "\n", "conv_result", "=", "[", "]", "\n", "\n", "for", "i", ",", "conv", "in", "enumerate", "(", "self", ".", "convolutions", ")", ":", "\n", "            ", "x", "=", "conv", "(", "char_embs", ")", "\n", "x", ",", "_", "=", "torch", ".", "max", "(", "x", ",", "-", "1", ")", "\n", "x", "=", "F", ".", "relu", "(", "x", ")", "\n", "conv_result", ".", "append", "(", "x", ")", "\n", "\n", "", "x", "=", "torch", ".", "cat", "(", "conv_result", ",", "dim", "=", "-", "1", ")", "\n", "\n", "if", "self", ".", "highway", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "highway", "(", "x", ")", "\n", "", "x", "=", "self", ".", "projection", "(", "x", ")", "\n", "\n", "return", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.sparse_multihead_attention.SparseMultiheadAttention.__init__": [[22, 35], ["multihead_attention.MultiheadAttention.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["def", "__init__", "(", "self", ",", "embed_dim", ",", "num_heads", ",", "kdim", "=", "None", ",", "vdim", "=", "None", ",", "dropout", "=", "0.", ",", "bias", "=", "True", ",", "\n", "add_bias_kv", "=", "False", ",", "add_zero_attn", "=", "False", ",", "self_attention", "=", "False", ",", "\n", "encoder_decoder_attention", "=", "False", ",", "stride", "=", "32", ",", "expressivity", "=", "8", ",", "is_bidirectional", "=", "True", ")", ":", "\n", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "embed_dim", ",", "num_heads", ",", "kdim", ",", "vdim", ",", "dropout", ",", "bias", ",", "add_bias_kv", ",", "\n", "add_zero_attn", ",", "self_attention", ",", "encoder_decoder_attention", "\n", ")", "\n", "\n", "self", ".", "is_bidirectional", "=", "is_bidirectional", "\n", "self", ".", "stride", "=", "stride", "\n", "self", ".", "expressivity", "=", "expressivity", "\n", "assert", "(", "self", ".", "stride", ">", "0", "and", "self", ".", "stride", ">=", "self", ".", "expressivity", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.sparse_multihead_attention.SparseMultiheadAttention.compute_checkpoint": [[37, 46], ["math.floor"], "methods", ["None"], ["", "def", "compute_checkpoint", "(", "self", ",", "word_index", ")", ":", "\n", "        ", "if", "word_index", "%", "self", ".", "stride", "==", "0", "and", "word_index", "!=", "0", ":", "\n", "            ", "checkpoint_index", "=", "word_index", "-", "self", ".", "expressivity", "\n", "", "else", ":", "\n", "            ", "checkpoint_index", "=", "(", "\n", "math", ".", "floor", "(", "word_index", "/", "self", ".", "stride", ")", "*", "self", ".", "stride", "\n", "+", "self", ".", "stride", "-", "self", ".", "expressivity", "\n", ")", "\n", "", "return", "checkpoint_index", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.sparse_multihead_attention.SparseMultiheadAttention.compute_subset_summaries": [[48, 58], ["sparse_multihead_attention.SparseMultiheadAttention.compute_checkpoint", "set", "set", "subset_two.union.union.union", "sparse_multihead_attention.SparseMultiheadAttention.compute_checkpoint", "range", "min"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.sparse_multihead_attention.SparseMultiheadAttention.compute_checkpoint", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.sparse_multihead_attention.SparseMultiheadAttention.compute_checkpoint"], ["", "def", "compute_subset_summaries", "(", "self", ",", "absolute_max", ")", ":", "\n", "        ", "checkpoint_index", "=", "self", ".", "compute_checkpoint", "(", "0", ")", "\n", "subset_two", "=", "set", "(", ")", "\n", "while", "checkpoint_index", "<=", "absolute_max", "-", "1", ":", "\n", "            ", "summary", "=", "set", "(", "range", "(", "checkpoint_index", ",", "min", "(", "\n", "checkpoint_index", "+", "self", ".", "expressivity", "+", "1", ",", "absolute_max", ")", "\n", ")", ")", "\n", "subset_two", "=", "subset_two", ".", "union", "(", "summary", ")", "\n", "checkpoint_index", "=", "self", ".", "compute_checkpoint", "(", "checkpoint_index", "+", "self", ".", "stride", ")", "\n", "", "return", "subset_two", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.sparse_multihead_attention.SparseMultiheadAttention.compute_fixed_attention_subset": [[60, 83], ["set", "set.union", "math.floor", "set", "set", "sparse_multihead_attention.SparseMultiheadAttention.compute_subset_summaries", "range", "range", "min", "max", "min"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.sparse_multihead_attention.SparseMultiheadAttention.compute_subset_summaries"], ["", "def", "compute_fixed_attention_subset", "(", "self", ",", "word_index", ",", "tgt_len", ")", ":", "\n", "# +1s account for range function; [min, max) -> [min, max]", "\n", "        ", "if", "not", "self", ".", "is_bidirectional", ":", "\n", "            ", "absolute_max", "=", "word_index", "+", "1", "\n", "", "else", ":", "\n", "            ", "absolute_max", "=", "tgt_len", "\n", "\n", "# Subset 1 - whole window", "\n", "", "rounded_index", "=", "math", ".", "floor", "(", "(", "word_index", "+", "self", ".", "stride", ")", "/", "self", ".", "stride", ")", "*", "self", ".", "stride", "\n", "if", "word_index", "%", "self", ".", "stride", "==", "0", "and", "word_index", "!=", "0", ":", "\n", "            ", "subset_one", "=", "set", "(", "range", "(", "word_index", "-", "self", ".", "stride", ",", "min", "(", "absolute_max", ",", "word_index", "+", "1", ")", ")", ")", "\n", "", "else", ":", "\n", "            ", "subset_one", "=", "set", "(", "range", "(", "max", "(", "0", ",", "rounded_index", "-", "self", ".", "stride", ")", ",", "min", "(", "\n", "absolute_max", ",", "rounded_index", "+", "1", ")", ")", "\n", ")", "\n", "\n", "# Subset 2 - summary per window", "\n", "# If bidirectional, subset 2 is the same for every index", "\n", "", "subset_two", "=", "set", "(", ")", "\n", "if", "not", "self", ".", "is_bidirectional", ":", "\n", "            ", "subset_two", "=", "self", ".", "compute_subset_summaries", "(", "absolute_max", ")", "\n", "\n", "", "return", "subset_one", ".", "union", "(", "subset_two", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.sparse_multihead_attention.SparseMultiheadAttention.buffered_sparse_mask": [[85, 100], ["torch.empty().float().fill_", "set", "range", "torch.empty().float().fill_.type_as", "float", "sparse_multihead_attention.SparseMultiheadAttention.compute_subset_summaries", "sparse_multihead_attention.SparseMultiheadAttention.compute_fixed_attention_subset", "fixed_attention_subset.union.union.union", "torch.LongTensor", "sparse_mask[].index_fill_", "torch.empty().float", "list", "torch.empty"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.sparse_multihead_attention.SparseMultiheadAttention.compute_subset_summaries", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.sparse_multihead_attention.SparseMultiheadAttention.compute_fixed_attention_subset"], ["", "def", "buffered_sparse_mask", "(", "self", ",", "tensor", ",", "tgt_len", ",", "src_len", ")", ":", "\n", "        ", "assert", "(", "tgt_len", ">", "self", ".", "stride", ")", "\n", "sparse_mask", "=", "torch", ".", "empty", "(", "(", "tgt_len", ",", "src_len", ")", ")", ".", "float", "(", ")", ".", "fill_", "(", "float", "(", "'-inf'", ")", ")", "\n", "\n", "# If bidirectional, subset 2 is the same for every index", "\n", "subset_summaries", "=", "set", "(", ")", "\n", "if", "self", ".", "is_bidirectional", ":", "\n", "            ", "subset_summaries", "=", "self", ".", "compute_subset_summaries", "(", "tgt_len", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "tgt_len", ")", ":", "\n", "            ", "fixed_attention_subset", "=", "self", ".", "compute_fixed_attention_subset", "(", "i", ",", "tgt_len", ")", "\n", "fixed_attention_subset", "=", "fixed_attention_subset", ".", "union", "(", "subset_summaries", ")", "\n", "included_word_indices", "=", "torch", ".", "LongTensor", "(", "list", "(", "fixed_attention_subset", ")", ")", "\n", "sparse_mask", "[", "i", "]", ".", "index_fill_", "(", "0", ",", "included_word_indices", ",", "0", ")", "\n", "", "return", "sparse_mask", ".", "type_as", "(", "tensor", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.sparse_multihead_attention.SparseMultiheadAttention.apply_sparse_mask": [[101, 105], ["sparse_multihead_attention.SparseMultiheadAttention.buffered_sparse_mask", "sparse_mask.unsqueeze().expand.unsqueeze().expand.unsqueeze().expand", "sparse_mask.unsqueeze().expand.unsqueeze().expand.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.sparse_multihead_attention.SparseMultiheadAttention.buffered_sparse_mask"], ["", "def", "apply_sparse_mask", "(", "self", ",", "attn_weights", ",", "tgt_len", ",", "src_len", ",", "bsz", ")", ":", "\n", "        ", "sparse_mask", "=", "self", ".", "buffered_sparse_mask", "(", "attn_weights", ",", "tgt_len", ",", "src_len", ")", "\n", "sparse_mask", "=", "sparse_mask", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "bsz", "*", "self", ".", "num_heads", ",", "tgt_len", ",", "src_len", ")", "\n", "attn_weights", "+=", "sparse_mask", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.sparse_transformer_sentence_encoder_layer.SparseTransformerSentenceEncoderLayer.__init__": [[15, 48], ["fairseq.modules.TransformerSentenceEncoderLayer.__init__", "fairseq.modules.sparse_multihead_attention.SparseMultiheadAttention"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "embedding_dim", ":", "float", "=", "768", ",", "\n", "ffn_embedding_dim", ":", "float", "=", "3072", ",", "\n", "num_attention_heads", ":", "float", "=", "8", ",", "\n", "dropout", ":", "float", "=", "0.1", ",", "\n", "attention_dropout", ":", "float", "=", "0.1", ",", "\n", "activation_dropout", ":", "float", "=", "0.1", ",", "\n", "activation_fn", ":", "str", "=", "'relu'", ",", "\n", "add_bias_kv", ":", "bool", "=", "False", ",", "\n", "add_zero_attn", ":", "bool", "=", "False", ",", "\n", "export", ":", "bool", "=", "False", ",", "\n", "is_bidirectional", ":", "bool", "=", "True", ",", "\n", "stride", ":", "int", "=", "32", ",", "\n", "expressivity", ":", "int", "=", "8", ",", "\n", ")", "->", "None", ":", "\n", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "embedding_dim", ",", "ffn_embedding_dim", ",", "num_attention_heads", ",", "dropout", ",", "\n", "attention_dropout", ",", "activation_dropout", ",", "activation_fn", ",", "add_bias_kv", ",", "\n", "add_zero_attn", ",", "export", "\n", ")", "\n", "\n", "self", ".", "self_attn", "=", "SparseMultiheadAttention", "(", "\n", "self", ".", "embedding_dim", ",", "\n", "num_attention_heads", ",", "\n", "dropout", "=", "attention_dropout", ",", "\n", "add_bias_kv", "=", "add_bias_kv", ",", "\n", "add_zero_attn", "=", "add_zero_attn", ",", "\n", "self_attention", "=", "True", ",", "\n", "is_bidirectional", "=", "is_bidirectional", ",", "\n", "stride", "=", "stride", ",", "\n", "expressivity", "=", "expressivity", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.__init__": [[21, 32], ["torch.Module.__init__", "sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.get_embedding", "sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.register_buffer", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.get_embedding"], ["def", "__init__", "(", "self", ",", "embedding_dim", ",", "padding_idx", ",", "init_size", "=", "1024", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "embedding_dim", "=", "embedding_dim", "\n", "self", ".", "padding_idx", "=", "padding_idx", "\n", "self", ".", "weights", "=", "SinusoidalPositionalEmbedding", ".", "get_embedding", "(", "\n", "init_size", ",", "\n", "embedding_dim", ",", "\n", "padding_idx", ",", "\n", ")", "\n", "self", ".", "onnx_trace", "=", "False", "\n", "self", ".", "register_buffer", "(", "'_float_tensor'", ",", "torch", ".", "FloatTensor", "(", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.prepare_for_onnx_export_": [[33, 35], ["None"], "methods", ["None"], ["", "def", "prepare_for_onnx_export_", "(", "self", ")", ":", "\n", "        ", "self", ".", "onnx_trace", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.get_embedding": [[36, 54], ["torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "torch.cat().view", "math.log", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.cat.unsqueeze", "torch.cat.unsqueeze", "torch.cat.unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.sin", "torch.sin", "torch.sin", "torch.sin", "torch.sin", "torch.sin", "torch.sin", "torch.sin", "torch.sin", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "torch.cos"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.log"], ["", "@", "staticmethod", "\n", "def", "get_embedding", "(", "num_embeddings", ",", "embedding_dim", ",", "padding_idx", "=", "None", ")", ":", "\n", "        ", "\"\"\"Build sinusoidal embeddings.\n\n        This matches the implementation in tensor2tensor, but differs slightly\n        from the description in Section 3.5 of \"Attention Is All You Need\".\n        \"\"\"", "\n", "half_dim", "=", "embedding_dim", "//", "2", "\n", "emb", "=", "math", ".", "log", "(", "10000", ")", "/", "(", "half_dim", "-", "1", ")", "\n", "emb", "=", "torch", ".", "exp", "(", "torch", ".", "arange", "(", "half_dim", ",", "dtype", "=", "torch", ".", "float", ")", "*", "-", "emb", ")", "\n", "emb", "=", "torch", ".", "arange", "(", "num_embeddings", ",", "dtype", "=", "torch", ".", "float", ")", ".", "unsqueeze", "(", "1", ")", "*", "emb", ".", "unsqueeze", "(", "0", ")", "\n", "emb", "=", "torch", ".", "cat", "(", "[", "torch", ".", "sin", "(", "emb", ")", ",", "torch", ".", "cos", "(", "emb", ")", "]", ",", "dim", "=", "1", ")", ".", "view", "(", "num_embeddings", ",", "-", "1", ")", "\n", "if", "embedding_dim", "%", "2", "==", "1", ":", "\n", "# zero pad", "\n", "            ", "emb", "=", "torch", ".", "cat", "(", "[", "emb", ",", "torch", ".", "zeros", "(", "num_embeddings", ",", "1", ")", "]", ",", "dim", "=", "1", ")", "\n", "", "if", "padding_idx", "is", "not", "None", ":", "\n", "            ", "emb", "[", "padding_idx", ",", ":", "]", "=", "0", "\n", "", "return", "emb", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.forward": [[55, 82], ["torch.onnx.operators.shape_as_tensor", "torch.onnx.operators.shape_as_tensor", "torch.onnx.operators.shape_as_tensor", "torch.onnx.operators.shape_as_tensor", "torch.onnx.operators.shape_as_tensor", "torch.onnx.operators.shape_as_tensor", "torch.onnx.operators.shape_as_tensor", "torch.onnx.operators.shape_as_tensor", "torch.onnx.operators.shape_as_tensor", "sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.weights.to", "fairseq.utils.make_positions", "sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.weights.index_select().view().detach", "sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.get_embedding", "sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.weights[].expand", "sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.weights.detach().index_select", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.onnx.operators.reshape_from_tensor_shape", "torch.onnx.operators.reshape_from_tensor_shape", "torch.onnx.operators.reshape_from_tensor_shape", "torch.onnx.operators.reshape_from_tensor_shape", "torch.onnx.operators.reshape_from_tensor_shape", "torch.onnx.operators.reshape_from_tensor_shape", "torch.onnx.operators.reshape_from_tensor_shape", "torch.onnx.operators.reshape_from_tensor_shape", "torch.onnx.operators.reshape_from_tensor_shape", "sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.weights.size", "sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.weights.index_select().unsqueeze().repeat", "fairseq.utils.make_positions.view", "sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.weights.index_select().view", "sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.weights.detach", "bsz.view", "seq_len.view", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "timestep.view", "sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.weights.index_select().unsqueeze", "sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.weights.index_select", "fairseq.utils.make_positions.view", "sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.weights.index_select"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.make_positions", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.get_embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "forward", "(", "self", ",", "input", ",", "incremental_state", "=", "None", ",", "timestep", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"", "\n", "bsz", ",", "seq_len", "=", "torch", ".", "onnx", ".", "operators", ".", "shape_as_tensor", "(", "input", ")", "\n", "max_pos", "=", "self", ".", "padding_idx", "+", "1", "+", "seq_len", "\n", "if", "self", ".", "weights", "is", "None", "or", "max_pos", ">", "self", ".", "weights", ".", "size", "(", "0", ")", ":", "\n", "# recompute/expand embeddings if needed", "\n", "            ", "self", ".", "weights", "=", "SinusoidalPositionalEmbedding", ".", "get_embedding", "(", "\n", "max_pos", ",", "\n", "self", ".", "embedding_dim", ",", "\n", "self", ".", "padding_idx", ",", "\n", ")", "\n", "", "self", ".", "weights", "=", "self", ".", "weights", ".", "to", "(", "self", ".", "_float_tensor", ")", "\n", "\n", "if", "incremental_state", "is", "not", "None", ":", "\n", "# positions is the same for every token when decoding a single step", "\n", "            ", "pos", "=", "timestep", ".", "view", "(", "-", "1", ")", "[", "0", "]", "+", "1", "if", "timestep", "is", "not", "None", "else", "seq_len", "\n", "if", "self", ".", "onnx_trace", ":", "\n", "                ", "return", "self", ".", "weights", ".", "index_select", "(", "index", "=", "self", ".", "padding_idx", "+", "pos", ",", "dim", "=", "0", ")", ".", "unsqueeze", "(", "1", ")", ".", "repeat", "(", "bsz", ",", "1", ",", "1", ")", "\n", "", "return", "self", ".", "weights", "[", "self", ".", "padding_idx", "+", "pos", ",", ":", "]", ".", "expand", "(", "bsz", ",", "1", ",", "-", "1", ")", "\n", "\n", "", "positions", "=", "utils", ".", "make_positions", "(", "input", ",", "self", ".", "padding_idx", ",", "onnx_trace", "=", "self", ".", "onnx_trace", ")", "\n", "if", "self", ".", "onnx_trace", ":", "\n", "            ", "flat_embeddings", "=", "self", ".", "weights", ".", "detach", "(", ")", ".", "index_select", "(", "0", ",", "positions", ".", "view", "(", "-", "1", ")", ")", "\n", "embedding_shape", "=", "torch", ".", "cat", "(", "(", "bsz", ".", "view", "(", "1", ")", ",", "seq_len", ".", "view", "(", "1", ")", ",", "torch", ".", "LongTensor", "(", "[", "-", "1", "]", ")", ")", ")", "\n", "embeddings", "=", "torch", ".", "onnx", ".", "operators", ".", "reshape_from_tensor_shape", "(", "flat_embeddings", ",", "embedding_shape", ")", "\n", "return", "embeddings", "\n", "", "return", "self", ".", "weights", ".", "index_select", "(", "0", ",", "positions", ".", "view", "(", "-", "1", ")", ")", ".", "view", "(", "bsz", ",", "seq_len", ",", "-", "1", ")", ".", "detach", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.sinusoidal_positional_embedding.SinusoidalPositionalEmbedding.max_positions": [[83, 86], ["int"], "methods", ["None"], ["", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum number of supported positions.\"\"\"", "\n", "return", "int", "(", "1e5", ")", "# an arbitrary large number", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.layer_norm.LayerNorm": [[9, 17], ["torch.nn.LayerNorm", "torch.cuda.is_available", "FusedLayerNorm"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.layer_norm.LayerNorm"], ["def", "LayerNorm", "(", "normalized_shape", ",", "eps", "=", "1e-5", ",", "elementwise_affine", "=", "True", ",", "export", "=", "False", ")", ":", "\n", "    ", "if", "not", "export", "and", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "from", "apex", ".", "normalization", "import", "FusedLayerNorm", "\n", "return", "FusedLayerNorm", "(", "normalized_shape", ",", "eps", ",", "elementwise_affine", ")", "\n", "", "except", "ImportError", ":", "\n", "            ", "pass", "\n", "", "", "return", "torch", ".", "nn", ".", "LayerNorm", "(", "normalized_shape", ",", "eps", ",", "elementwise_affine", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.lightweight_convolution.LightweightConv1d.__init__": [[37, 53], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "lightweight_convolution.LightweightConv1d.reset_parameters", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.highway.Highway.reset_parameters"], ["def", "__init__", "(", "self", ",", "input_size", ",", "kernel_size", "=", "1", ",", "padding", "=", "0", ",", "num_heads", "=", "1", ",", "\n", "weight_softmax", "=", "False", ",", "bias", "=", "False", ",", "weight_dropout", "=", "0.", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "input_size", "=", "input_size", "\n", "self", ".", "kernel_size", "=", "kernel_size", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "self", ".", "padding", "=", "padding", "\n", "self", ".", "weight_softmax", "=", "weight_softmax", "\n", "self", ".", "weight", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "num_heads", ",", "1", ",", "kernel_size", ")", ")", "\n", "\n", "if", "bias", ":", "\n", "            ", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "input_size", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "bias", "=", "None", "\n", "", "self", ".", "weight_dropout", "=", "weight_dropout", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.lightweight_convolution.LightweightConv1d.reset_parameters": [[54, 58], ["torch.init.xavier_uniform_", "torch.init.xavier_uniform_", "torch.init.xavier_uniform_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "weight", ")", "\n", "if", "self", ".", "bias", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "constant_", "(", "self", ".", "bias", ",", "0.", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.lightweight_convolution.LightweightConv1d.forward": [[59, 83], ["input.view.view.size", "torch.dropout", "torch.dropout", "torch.dropout", "input.view.view.view", "torch.conv1d", "torch.conv1d", "torch.conv1d", "output.view.view.view", "torch.softmax", "torch.softmax", "torch.softmax", "lightweight_convolution.LightweightConv1d.bias.view"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax"], ["", "", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "'''\n        input size: B x C x T\n        output size: B x C x T\n        '''", "\n", "B", ",", "C", ",", "T", "=", "input", ".", "size", "(", ")", "\n", "H", "=", "self", ".", "num_heads", "\n", "\n", "weight", "=", "self", ".", "weight", "\n", "if", "self", ".", "weight_softmax", ":", "\n", "            ", "weight", "=", "F", ".", "softmax", "(", "weight", ",", "dim", "=", "-", "1", ")", "\n", "\n", "", "weight", "=", "F", ".", "dropout", "(", "weight", ",", "self", ".", "weight_dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "# Merge every C/H entries into the batch dimension (C = self.input_size)", "\n", "# B x C x T -> (B * C/H) x H x T", "\n", "# One can also expand the weight to C x 1 x K by a factor of C/H", "\n", "# and do not reshape the input instead, which is slow though", "\n", "input", "=", "input", ".", "view", "(", "-", "1", ",", "H", ",", "T", ")", "\n", "output", "=", "F", ".", "conv1d", "(", "input", ",", "weight", ",", "padding", "=", "self", ".", "padding", ",", "groups", "=", "self", ".", "num_heads", ")", "\n", "output", "=", "output", ".", "view", "(", "B", ",", "C", ",", "T", ")", "\n", "if", "self", ".", "bias", "is", "not", "None", ":", "\n", "            ", "output", "=", "output", "+", "self", ".", "bias", ".", "view", "(", "1", ",", "-", "1", ",", "1", ")", "\n", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.lightweight_convolution.LightweightConv1dTBC.__init__": [[105, 124], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "lightweight_convolution.LightweightConv1dTBC.reset_parameters", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.highway.Highway.reset_parameters"], ["def", "__init__", "(", "self", ",", "input_size", ",", "kernel_size", "=", "1", ",", "padding_l", "=", "None", ",", "num_heads", "=", "1", ",", "\n", "weight_dropout", "=", "0.", ",", "weight_softmax", "=", "False", ",", "bias", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "input_size", "=", "input_size", "\n", "self", ".", "kernel_size", "=", "kernel_size", "\n", "self", ".", "padding_l", "=", "padding_l", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "self", ".", "weight_dropout", "=", "weight_dropout", "\n", "self", ".", "weight_softmax", "=", "weight_softmax", "\n", "\n", "self", ".", "weight", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "num_heads", ",", "1", ",", "kernel_size", ")", ")", "\n", "if", "bias", ":", "\n", "            ", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "input_size", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "bias", "=", "None", "\n", "\n", "", "self", ".", "reset_parameters", "(", ")", "\n", "\n", "self", ".", "onnx_trace", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.lightweight_convolution.LightweightConv1dTBC.reset_parameters": [[125, 129], ["torch.init.xavier_uniform_", "torch.init.xavier_uniform_", "torch.init.xavier_uniform_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "weight", ")", "\n", "if", "self", ".", "bias", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "constant_", "(", "self", ".", "bias", ",", "0.", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.lightweight_convolution.LightweightConv1dTBC.forward": [[130, 147], ["lightweight_convolution.LightweightConv1dTBC._forward_unfolded", "lightweight_convolution.LightweightConv1dTBC._forward_expanded", "lightweight_convolution.LightweightConv1dTBC.bias.view"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.lightweight_convolution.LightweightConv1dTBC._forward_unfolded", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.lightweight_convolution.LightweightConv1dTBC._forward_expanded"], ["", "", "def", "forward", "(", "self", ",", "x", ",", "incremental_state", "=", "None", ",", "unfold", "=", "False", ")", ":", "\n", "        ", "'''Assuming the input, x, of the shape T x B x C and producing an output in the shape T x B x C\n        args:\n            x: Input of shape T x B x C, i.e. (timesteps, batch_size, input_size)\n            incremental_state: A dict to keep the state\n            unfold: unfold the input or not. If not, we use the matrix trick instead\n        '''", "\n", "unfold", "=", "unfold", "or", "(", "incremental_state", "is", "not", "None", ")", "\n", "\n", "if", "unfold", ":", "\n", "            ", "output", "=", "self", ".", "_forward_unfolded", "(", "x", ",", "incremental_state", ")", "\n", "", "else", ":", "\n", "            ", "output", "=", "self", ".", "_forward_expanded", "(", "x", ",", "incremental_state", ")", "\n", "\n", "", "if", "self", ".", "bias", "is", "not", "None", ":", "\n", "            ", "output", "=", "output", "+", "self", ".", "bias", ".", "view", "(", "1", ",", "1", ",", "-", "1", ")", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.lightweight_convolution.LightweightConv1dTBC.prepare_for_onnx_export_": [[148, 150], ["None"], "methods", ["None"], ["", "def", "prepare_for_onnx_export_", "(", "self", ")", ":", "\n", "        ", "self", ".", "onnx_trace", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.lightweight_convolution.LightweightConv1dTBC._forward_unfolded": [[151, 186], ["x.size", "lightweight_convolution.LightweightConv1dTBC.weight.view", "fairseq.utils.softmax().type_as.view().expand().contiguous().view", "torch.dropout", "torch.dropout", "torch.dropout", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "output.view.view.view", "lightweight_convolution.LightweightConv1dTBC._get_input_buffer", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "x_unfold.view.view.view", "fairseq.modules.unfold.unfold1d", "x_unfold.view.view.view", "fairseq.utils.softmax().type_as", "fairseq.utils.softmax().type_as.size", "x.new", "lightweight_convolution.LightweightConv1dTBC._set_input_buffer", "fairseq.utils.softmax().type_as.view().expand().contiguous", "x.unsqueeze", "fairseq.utils.softmax", "fairseq.utils.softmax().type_as.view().expand", "x_unfold.view.view.size", "fairseq.utils.softmax().type_as.view"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.linearized_convolution.LinearizedConvolution._get_input_buffer", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.unfold.unfold1d", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.linearized_convolution.LinearizedConvolution._set_input_buffer", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "_forward_unfolded", "(", "self", ",", "x", ",", "incremental_state", ")", ":", "\n", "        ", "'''The conventional implementation of convolutions.\n        Unfolding the input by having a window shifting to the right.'''", "\n", "T", ",", "B", ",", "C", "=", "x", ".", "size", "(", ")", "\n", "K", ",", "H", "=", "self", ".", "kernel_size", ",", "self", ".", "num_heads", "\n", "R", "=", "C", "//", "H", "\n", "assert", "R", "*", "H", "==", "C", "==", "self", ".", "input_size", "\n", "\n", "weight", "=", "self", ".", "weight", ".", "view", "(", "H", ",", "K", ")", "\n", "if", "incremental_state", "is", "not", "None", ":", "\n", "            ", "input_buffer", "=", "self", ".", "_get_input_buffer", "(", "incremental_state", ")", "\n", "if", "input_buffer", "is", "None", ":", "\n", "                ", "input_buffer", "=", "x", ".", "new", "(", ")", "\n", "", "x_unfold", "=", "torch", ".", "cat", "(", "[", "input_buffer", ",", "x", ".", "unsqueeze", "(", "3", ")", "]", ",", "dim", "=", "3", ")", "\n", "if", "self", ".", "kernel_size", ">", "1", ":", "\n", "                ", "self", ".", "_set_input_buffer", "(", "incremental_state", ",", "x_unfold", "[", ":", ",", ":", ",", ":", ",", "-", "self", ".", "kernel_size", "+", "1", ":", "]", ")", "\n", "", "x_unfold", "=", "x_unfold", ".", "view", "(", "T", "*", "B", "*", "H", ",", "R", ",", "-", "1", ")", "\n", "", "else", ":", "\n", "# unfold the input: T x B x C --> T' x B x C x K", "\n", "            ", "x_unfold", "=", "unfold1d", "(", "x", ",", "self", ".", "kernel_size", ",", "self", ".", "padding_l", ",", "0", ")", "\n", "x_unfold", "=", "x_unfold", ".", "view", "(", "T", "*", "B", "*", "H", ",", "R", ",", "K", ")", "\n", "\n", "", "if", "self", ".", "weight_softmax", ":", "\n", "            ", "weight", "=", "utils", ".", "softmax", "(", "weight", ",", "dim", "=", "1", ",", "onnx_trace", "=", "self", ".", "onnx_trace", ")", ".", "type_as", "(", "weight", ")", "\n", "\n", "", "if", "incremental_state", "is", "not", "None", ":", "\n", "            ", "weight", "=", "weight", "[", ":", ",", "-", "x_unfold", ".", "size", "(", "2", ")", ":", "]", "\n", "K", "=", "weight", ".", "size", "(", "1", ")", "\n", "\n", "", "weight", "=", "weight", ".", "view", "(", "1", ",", "H", ",", "K", ")", ".", "expand", "(", "T", "*", "B", ",", "H", ",", "K", ")", ".", "contiguous", "(", ")", ".", "view", "(", "T", "*", "B", "*", "H", ",", "K", ",", "1", ")", "\n", "\n", "weight", "=", "F", ".", "dropout", "(", "weight", ",", "self", ".", "weight_dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "output", "=", "torch", ".", "bmm", "(", "x_unfold", ",", "weight", ")", "# T*B*H x R x 1", "\n", "output", "=", "output", ".", "view", "(", "T", ",", "B", ",", "C", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.lightweight_convolution.LightweightConv1dTBC._forward_expanded": [[187, 217], ["x.view().transpose.view().transpose.size", "lightweight_convolution.LightweightConv1dTBC.weight.view", "weight.narrow.narrow.view().expand().contiguous", "weight.narrow.narrow.view().transpose", "x.view().transpose.view().transpose.view().transpose", "weight.narrow.narrow.new_zeros", "torch.dropout.as_strided().copy_", "torch.dropout.narrow", "torch.dropout", "torch.dropout", "torch.dropout", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "output.transpose().contiguous().view.transpose().contiguous().view.transpose().contiguous().view", "fairseq.utils.softmax().type_as", "weight.narrow.narrow.narrow", "weight.narrow.narrow.view().expand", "weight.narrow.narrow.view", "x.view().transpose.view().transpose.view", "torch.dropout.as_strided", "output.transpose().contiguous().view.transpose().contiguous().view.transpose().contiguous", "fairseq.utils.softmax", "weight.narrow.narrow.view", "output.transpose().contiguous().view.transpose().contiguous().view.transpose"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax"], ["", "def", "_forward_expanded", "(", "self", ",", "x", ",", "incremental_state", ")", ":", "\n", "        ", "'''Turn the convolution filters into band matrices and do matrix multiplication.\n        This is faster when the sequence is short, but less memory efficient.\n        This is not used in the decoder during inference.\n        '''", "\n", "T", ",", "B", ",", "C", "=", "x", ".", "size", "(", ")", "\n", "K", ",", "H", "=", "self", ".", "kernel_size", ",", "self", ".", "num_heads", "\n", "R", "=", "C", "//", "H", "\n", "assert", "R", "*", "H", "==", "C", "==", "self", ".", "input_size", "\n", "\n", "weight", "=", "self", ".", "weight", ".", "view", "(", "H", ",", "K", ")", "\n", "if", "self", ".", "weight_softmax", ":", "\n", "            ", "weight", "=", "utils", ".", "softmax", "(", "weight", ",", "dim", "=", "1", ",", "onnx_trace", "=", "self", ".", "onnx_trace", ")", ".", "type_as", "(", "weight", ")", "\n", "", "weight", "=", "weight", ".", "view", "(", "1", ",", "H", ",", "K", ")", ".", "expand", "(", "T", "*", "B", ",", "H", ",", "K", ")", ".", "contiguous", "(", ")", "\n", "weight", "=", "weight", ".", "view", "(", "T", ",", "B", "*", "H", ",", "K", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "x", "=", "x", ".", "view", "(", "T", ",", "B", "*", "H", ",", "R", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "P", "=", "self", ".", "padding_l", "\n", "if", "K", ">", "T", "and", "P", "==", "K", "-", "1", ":", "\n", "            ", "weight", "=", "weight", ".", "narrow", "(", "2", ",", "K", "-", "T", ",", "T", ")", "\n", "K", ",", "P", "=", "T", ",", "T", "-", "1", "\n", "# turn the convolution filters into band matrices", "\n", "", "weight_expanded", "=", "weight", ".", "new_zeros", "(", "B", "*", "H", ",", "T", ",", "T", "+", "K", "-", "1", ",", "requires_grad", "=", "False", ")", "\n", "weight_expanded", ".", "as_strided", "(", "(", "B", "*", "H", ",", "T", ",", "K", ")", ",", "(", "T", "*", "(", "T", "+", "K", "-", "1", ")", ",", "T", "+", "K", ",", "1", ")", ")", ".", "copy_", "(", "weight", ")", "\n", "weight_expanded", "=", "weight_expanded", ".", "narrow", "(", "2", ",", "P", ",", "T", ")", "\n", "weight_expanded", "=", "F", ".", "dropout", "(", "weight_expanded", ",", "self", ".", "weight_dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "\n", "output", "=", "torch", ".", "bmm", "(", "weight_expanded", ",", "x", ")", "\n", "output", "=", "output", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", ".", "view", "(", "T", ",", "B", ",", "C", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.lightweight_convolution.LightweightConv1dTBC.reorder_incremental_state": [[218, 223], ["lightweight_convolution.LightweightConv1dTBC._get_input_buffer", "input_buffer.index_select.index_select.index_select", "lightweight_convolution.LightweightConv1dTBC._set_input_buffer"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.linearized_convolution.LinearizedConvolution._get_input_buffer", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.linearized_convolution.LinearizedConvolution._set_input_buffer"], ["", "def", "reorder_incremental_state", "(", "self", ",", "incremental_state", ",", "new_order", ")", ":", "\n", "        ", "input_buffer", "=", "self", ".", "_get_input_buffer", "(", "incremental_state", ")", "\n", "if", "input_buffer", "is", "not", "None", ":", "\n", "            ", "input_buffer", "=", "input_buffer", ".", "index_select", "(", "1", ",", "new_order", ")", "\n", "self", ".", "_set_input_buffer", "(", "incremental_state", ",", "input_buffer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.lightweight_convolution.LightweightConv1dTBC._get_input_buffer": [[224, 226], ["fairseq.utils.get_incremental_state"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.get_incremental_state"], ["", "", "def", "_get_input_buffer", "(", "self", ",", "incremental_state", ")", ":", "\n", "        ", "return", "utils", ".", "get_incremental_state", "(", "self", ",", "incremental_state", ",", "'input_buffer'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.lightweight_convolution.LightweightConv1dTBC._set_input_buffer": [[227, 229], ["fairseq.utils.set_incremental_state"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.set_incremental_state"], ["", "def", "_set_input_buffer", "(", "self", ",", "incremental_state", ",", "new_buffer", ")", ":", "\n", "        ", "return", "utils", ".", "set_incremental_state", "(", "self", ",", "incremental_state", ",", "'input_buffer'", ",", "new_buffer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.lightweight_convolution.LightweightConv1dTBC.extra_repr": [[230, 238], ["None"], "methods", ["None"], ["", "def", "extra_repr", "(", "self", ")", ":", "\n", "        ", "s", "=", "'{}, kernel_size={}, padding_l={}, num_heads={}, weight_softmax={}, bias={}'", ".", "format", "(", "\n", "self", ".", "input_size", ",", "self", ".", "kernel_size", ",", "self", ".", "padding_l", ",", "\n", "self", ".", "num_heads", ",", "self", ".", "weight_softmax", ",", "self", ".", "bias", "is", "not", "None", "\n", ")", "\n", "if", "self", ".", "weight_dropout", ">", "0.", ":", "\n", "            ", "s", "+=", "', weight_dropout={}'", ".", "format", "(", "self", ".", "weight_dropout", ")", "\n", "", "return", "s", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.logsumexp_moe.LogSumExpMoE.forward": [[16, 21], ["ctx.save_for_backward", "torch.logsumexp"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "logp", ",", "posterior", ",", "dim", "=", "-", "1", ")", ":", "\n", "        ", "ctx", ".", "save_for_backward", "(", "posterior", ")", "\n", "ctx", ".", "dim", "=", "dim", "\n", "return", "torch", ".", "logsumexp", "(", "logp", ",", "dim", "=", "dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.logsumexp_moe.LogSumExpMoE.backward": [[22, 27], ["grad_output.unsqueeze"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "grad_output", ")", ":", "\n", "        ", "posterior", ",", "=", "ctx", ".", "saved_tensors", "\n", "grad_logp", "=", "grad_output", ".", "unsqueeze", "(", "ctx", ".", "dim", ")", "*", "posterior", "\n", "return", "grad_logp", ",", "None", ",", "None", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.highway.Highway.__init__": [[17, 29], ["super().__init__", "torch.nn.ModuleList", "torch.nn.ReLU", "highway.Highway.reset_parameters", "torch.nn.Linear", "range"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.highway.Highway.reset_parameters", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear"], ["def", "__init__", "(", "\n", "self", ",", "\n", "input_dim", ":", "int", ",", "\n", "num_layers", ":", "int", "=", "1", "\n", ")", ":", "\n", "        ", "super", "(", "Highway", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "input_dim", "=", "input_dim", "\n", "self", ".", "layers", "=", "nn", ".", "ModuleList", "(", "[", "nn", ".", "Linear", "(", "input_dim", ",", "input_dim", "*", "2", ")", "\n", "for", "_", "in", "range", "(", "num_layers", ")", "]", ")", "\n", "self", ".", "activation", "=", "nn", ".", "ReLU", "(", ")", "\n", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.highway.Highway.reset_parameters": [[30, 41], ["torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.xavier_normal_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "for", "layer", "in", "self", ".", "layers", ":", "\n", "# As per comment in AllenNLP:", "\n", "# We should bias the highway layer to just carry its input forward.  We do that by", "\n", "# setting the bias on `B(x)` to be positive, because that means `g` will be biased to", "\n", "# be high, so we will carry the input forward.  The bias on `B(x)` is the second half", "\n", "# of the bias vector in each Linear layer.", "\n", "            ", "nn", ".", "init", ".", "constant_", "(", "layer", ".", "bias", "[", "self", ".", "input_dim", ":", "]", ",", "1", ")", "\n", "\n", "nn", ".", "init", ".", "constant_", "(", "layer", ".", "bias", "[", ":", "self", ".", "input_dim", "]", ",", "0", ")", "\n", "nn", ".", "init", ".", "xavier_normal_", "(", "layer", ".", "weight", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.highway.Highway.forward": [[42, 53], ["layer", "layer.chunk", "highway.Highway.activation", "torch.sigmoid", "torch.sigmoid.new_tensor"], "methods", ["None"], ["", "", "def", "forward", "(", "\n", "self", ",", "\n", "x", ":", "torch", ".", "Tensor", "\n", ")", ":", "\n", "        ", "for", "layer", "in", "self", ".", "layers", ":", "\n", "            ", "projection", "=", "layer", "(", "x", ")", "\n", "proj_x", ",", "gate", "=", "projection", ".", "chunk", "(", "2", ",", "dim", "=", "-", "1", ")", "\n", "proj_x", "=", "self", ".", "activation", "(", "proj_x", ")", "\n", "gate", "=", "torch", ".", "sigmoid", "(", "gate", ")", "\n", "x", "=", "gate", "*", "x", "+", "(", "gate", ".", "new_tensor", "(", "[", "1", "]", ")", "-", "gate", ")", "*", "proj_x", "\n", "", "return", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.mean_pool_gating_network.MeanPoolGatingNetwork.__init__": [[18, 26], ["super().__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear"], ["def", "__init__", "(", "self", ",", "embed_dim", ",", "num_experts", ",", "dropout", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "embed_dim", "=", "embed_dim", "\n", "self", ".", "num_experts", "=", "num_experts", "\n", "\n", "self", ".", "fc1", "=", "torch", ".", "nn", ".", "Linear", "(", "embed_dim", ",", "embed_dim", ")", "\n", "self", ".", "dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "dropout", ")", "if", "dropout", "is", "not", "None", "else", "None", "\n", "self", ".", "fc2", "=", "torch", ".", "nn", ".", "Linear", "(", "embed_dim", ",", "num_experts", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.mean_pool_gating_network.MeanPoolGatingNetwork.forward": [[27, 52], ["encoder_out[].transpose", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "mean_pool_gating_network.MeanPoolGatingNetwork.fc2", "torch.log_softmax().type_as", "torch.log_softmax().type_as", "ValueError", "encoder_out.clone.clone.clone", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "mean_pool_gating_network.MeanPoolGatingNetwork.fc1", "mean_pool_gating_network.MeanPoolGatingNetwork.dropout", "isinstance", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum.type_as", "torch.sum.type_as", "torch.log_softmax", "torch.log_softmax", "encoder_out[].size"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.mean", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.mean", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.mean", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.mean", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.log_softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.log_softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "forward", "(", "self", ",", "encoder_out", ")", ":", "\n", "        ", "if", "not", "(", "\n", "isinstance", "(", "encoder_out", ",", "dict", ")", "\n", "and", "'encoder_out'", "in", "encoder_out", "\n", "and", "'encoder_padding_mask'", "in", "encoder_out", "\n", "and", "encoder_out", "[", "'encoder_out'", "]", ".", "size", "(", "2", ")", "==", "self", ".", "embed_dim", "\n", ")", ":", "\n", "            ", "raise", "ValueError", "(", "'Unexpected format for encoder_out'", ")", "\n", "\n", "# mean pooling over time", "\n", "", "encoder_padding_mask", "=", "encoder_out", "[", "'encoder_padding_mask'", "]", "# B x T", "\n", "encoder_out", "=", "encoder_out", "[", "'encoder_out'", "]", ".", "transpose", "(", "0", ",", "1", ")", "# B x T x C", "\n", "if", "encoder_padding_mask", "is", "not", "None", ":", "\n", "            ", "encoder_out", "=", "encoder_out", ".", "clone", "(", ")", "# required because of transpose above", "\n", "encoder_out", "[", "encoder_padding_mask", "]", "=", "0", "\n", "ntokens", "=", "torch", ".", "sum", "(", "~", "encoder_padding_mask", ",", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "\n", "x", "=", "torch", ".", "sum", "(", "encoder_out", ",", "dim", "=", "1", ")", "/", "ntokens", ".", "type_as", "(", "encoder_out", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "torch", ".", "mean", "(", "encoder_out", ",", "dim", "=", "1", ")", "\n", "\n", "", "x", "=", "torch", ".", "tanh", "(", "self", ".", "fc1", "(", "x", ")", ")", "\n", "if", "self", ".", "dropout", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "dropout", "(", "x", ")", "\n", "", "x", "=", "self", ".", "fc2", "(", "x", ")", "\n", "return", "F", ".", "log_softmax", "(", "x", ",", "dim", "=", "-", "1", ",", "dtype", "=", "torch", ".", "float32", ")", ".", "type_as", "(", "x", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.scalar_bias.ScalarBias.forward": [[16, 24], ["list", "input.new().fill_", "input.new().fill_.narrow().copy_", "input.size", "input.new", "input.new().fill_.narrow"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "input", ",", "dim", ",", "bias_init", ")", ":", "\n", "        ", "size", "=", "list", "(", "input", ".", "size", "(", ")", ")", "\n", "size", "[", "dim", "]", "+=", "1", "\n", "output", "=", "input", ".", "new", "(", "*", "size", ")", ".", "fill_", "(", "bias_init", ")", "\n", "output", ".", "narrow", "(", "dim", ",", "1", ",", "size", "[", "dim", "]", "-", "1", ")", ".", "copy_", "(", "input", ")", "\n", "ctx", ".", "dim", "=", "dim", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.scalar_bias.ScalarBias.backward": [[25, 28], ["grad.narrow", "grad.size"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "grad", ")", ":", "\n", "        ", "return", "grad", ".", "narrow", "(", "ctx", ".", "dim", ",", "1", ",", "grad", ".", "size", "(", "ctx", ".", "dim", ")", "-", "1", ")", ",", "None", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.scalar_bias.scalar_bias": [[30, 32], ["ScalarBias.apply"], "function", ["None"], ["", "", "def", "scalar_bias", "(", "input", ",", "dim", ",", "bias_init", "=", "0", ")", ":", "\n", "    ", "return", "ScalarBias", ".", "apply", "(", "input", ",", "dim", ",", "bias_init", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.linearized_convolution.LinearizedConvolution.__init__": [[23, 27], ["conv_tbc.ConvTBC.__init__", "linearized_convolution.LinearizedConvolution.register_backward_hook"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["def", "__init__", "(", "self", ",", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "in_channels", ",", "out_channels", ",", "kernel_size", ",", "**", "kwargs", ")", "\n", "self", ".", "_linearized_weight", "=", "None", "\n", "self", ".", "register_backward_hook", "(", "self", ".", "_clear_linearized_weight", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.linearized_convolution.LinearizedConvolution.forward": [[28, 65], ["linearized_convolution.LinearizedConvolution._get_linearized_weight", "input.size", "torch.linear.view", "super().forward", "linearized_convolution.LinearizedConvolution._get_input_buffer", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.linear", "torch.linear", "input.new().zero_", "linearized_convolution.LinearizedConvolution._set_input_buffer", "input_buffer[].clone", "input.view", "input.new", "input.size"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.linearized_convolution.LinearizedConvolution._get_linearized_weight", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.model.AMRModel.forward", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.linearized_convolution.LinearizedConvolution._get_input_buffer", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.linearized_convolution.LinearizedConvolution._set_input_buffer", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "forward", "(", "self", ",", "input", ",", "incremental_state", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            incremental_state: Used to buffer signal; if not None, then input is\n                expected to contain a single frame. If the input order changes\n                between time steps, call reorder_incremental_state.\n        Input:\n            Time x Batch x Channel during training\n            Batch x Time x Channel during inference\n        \"\"\"", "\n", "if", "incremental_state", "is", "None", ":", "\n", "            ", "output", "=", "super", "(", ")", ".", "forward", "(", "input", ")", "\n", "if", "self", ".", "kernel_size", "[", "0", "]", ">", "1", "and", "self", ".", "padding", "[", "0", "]", ">", "0", ":", "\n", "# remove future timesteps added by padding", "\n", "                ", "output", "=", "output", "[", ":", "-", "self", ".", "padding", "[", "0", "]", ",", ":", ",", ":", "]", "\n", "", "return", "output", "\n", "\n", "# reshape weight", "\n", "", "weight", "=", "self", ".", "_get_linearized_weight", "(", ")", "\n", "kw", "=", "self", ".", "kernel_size", "[", "0", "]", "\n", "\n", "bsz", "=", "input", ".", "size", "(", "0", ")", "# input: bsz x len x dim", "\n", "if", "kw", ">", "1", ":", "\n", "            ", "input", "=", "input", ".", "data", "\n", "input_buffer", "=", "self", ".", "_get_input_buffer", "(", "incremental_state", ")", "\n", "if", "input_buffer", "is", "None", ":", "\n", "                ", "input_buffer", "=", "input", ".", "new", "(", "bsz", ",", "kw", ",", "input", ".", "size", "(", "2", ")", ")", ".", "zero_", "(", ")", "\n", "self", ".", "_set_input_buffer", "(", "incremental_state", ",", "input_buffer", ")", "\n", "", "else", ":", "\n", "# shift buffer", "\n", "                ", "input_buffer", "[", ":", ",", ":", "-", "1", ",", ":", "]", "=", "input_buffer", "[", ":", ",", "1", ":", ",", ":", "]", ".", "clone", "(", ")", "\n", "# append next input", "\n", "", "input_buffer", "[", ":", ",", "-", "1", ",", ":", "]", "=", "input", "[", ":", ",", "-", "1", ",", ":", "]", "\n", "input", "=", "input_buffer", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "output", "=", "F", ".", "linear", "(", "input", ".", "view", "(", "bsz", ",", "-", "1", ")", ",", "weight", ",", "self", ".", "bias", ")", "\n", "", "return", "output", ".", "view", "(", "bsz", ",", "1", ",", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.linearized_convolution.LinearizedConvolution.reorder_incremental_state": [[66, 71], ["linearized_convolution.LinearizedConvolution._get_input_buffer", "input_buffer.index_select.index_select.index_select", "linearized_convolution.LinearizedConvolution._set_input_buffer"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.linearized_convolution.LinearizedConvolution._get_input_buffer", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.linearized_convolution.LinearizedConvolution._set_input_buffer"], ["", "def", "reorder_incremental_state", "(", "self", ",", "incremental_state", ",", "new_order", ")", ":", "\n", "        ", "input_buffer", "=", "self", ".", "_get_input_buffer", "(", "incremental_state", ")", "\n", "if", "input_buffer", "is", "not", "None", ":", "\n", "            ", "input_buffer", "=", "input_buffer", ".", "index_select", "(", "0", ",", "new_order", ")", "\n", "self", ".", "_set_input_buffer", "(", "incremental_state", ",", "input_buffer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.linearized_convolution.LinearizedConvolution._get_input_buffer": [[72, 74], ["fairseq.utils.get_incremental_state"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.get_incremental_state"], ["", "", "def", "_get_input_buffer", "(", "self", ",", "incremental_state", ")", ":", "\n", "        ", "return", "utils", ".", "get_incremental_state", "(", "self", ",", "incremental_state", ",", "'input_buffer'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.linearized_convolution.LinearizedConvolution._set_input_buffer": [[75, 77], ["fairseq.utils.set_incremental_state"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.set_incremental_state"], ["", "def", "_set_input_buffer", "(", "self", ",", "incremental_state", ",", "new_buffer", ")", ":", "\n", "        ", "return", "utils", ".", "set_incremental_state", "(", "self", ",", "incremental_state", ",", "'input_buffer'", ",", "new_buffer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.linearized_convolution.LinearizedConvolution._get_linearized_weight": [[78, 85], ["linearized_convolution.LinearizedConvolution.weight.transpose().transpose().contiguous", "linearized_convolution.LinearizedConvolution.view", "linearized_convolution.LinearizedConvolution.size", "linearized_convolution.LinearizedConvolution.weight.transpose().transpose", "linearized_convolution.LinearizedConvolution.weight.transpose"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "_get_linearized_weight", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_linearized_weight", "is", "None", ":", "\n", "            ", "kw", "=", "self", ".", "kernel_size", "[", "0", "]", "\n", "weight", "=", "self", ".", "weight", ".", "transpose", "(", "2", ",", "1", ")", ".", "transpose", "(", "1", ",", "0", ")", ".", "contiguous", "(", ")", "\n", "assert", "weight", ".", "size", "(", ")", "==", "(", "self", ".", "out_channels", ",", "kw", ",", "self", ".", "in_channels", ")", "\n", "self", ".", "_linearized_weight", "=", "weight", ".", "view", "(", "self", ".", "out_channels", ",", "-", "1", ")", "\n", "", "return", "self", ".", "_linearized_weight", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.linearized_convolution.LinearizedConvolution._clear_linearized_weight": [[86, 88], ["None"], "methods", ["None"], ["", "def", "_clear_linearized_weight", "(", "self", ",", "*", "args", ")", ":", "\n", "        ", "self", ".", "_linearized_weight", "=", "None", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.vggblock.VGGBlock.__init__": [[60, 110], ["super().__init__", "vggblock._pair", "vggblock._pair", "vggblock._pair", "torch.ModuleList", "torch.ModuleList", "range", "torch.MaxPool2d", "torch.MaxPool2d", "vggblock.VGGBlock.layers.append", "vggblock.infer_conv_output_dim", "tuple", "vggblock._pair", "torch.Conv2d", "torch.Conv2d", "vggblock.VGGBlock.layers.append", "vggblock.VGGBlock.layers.append", "vggblock.infer_conv_output_dim", "vggblock.VGGBlock.layers.append", "torch.ReLU", "torch.ReLU", "torch.LayerNorm", "torch.LayerNorm"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.vggblock._pair", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.vggblock._pair", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.vggblock._pair", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.vggblock.infer_conv_output_dim", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.vggblock._pair", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.vggblock.infer_conv_output_dim", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.layer_norm.LayerNorm", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.layer_norm.LayerNorm"], ["def", "__init__", "(", "\n", "self", ",", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n", "conv_kernel_size", ",", "\n", "pooling_kernel_size", ",", "\n", "num_conv_layers", ",", "\n", "input_dim", ",", "\n", "conv_stride", "=", "1", ",", "\n", "padding", "=", "None", ",", "\n", "layer_norm", "=", "False", ",", "\n", ")", ":", "\n", "        ", "assert", "(", "\n", "input_dim", "is", "not", "None", "\n", ")", ",", "\"Need input_dim for LayerNorm and infer_conv_output_dim\"", "\n", "super", "(", "VGGBlock", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "in_channels", "=", "in_channels", "\n", "self", ".", "out_channels", "=", "out_channels", "\n", "self", ".", "conv_kernel_size", "=", "_pair", "(", "conv_kernel_size", ")", "\n", "self", ".", "pooling_kernel_size", "=", "_pair", "(", "pooling_kernel_size", ")", "\n", "self", ".", "num_conv_layers", "=", "num_conv_layers", "\n", "self", ".", "padding", "=", "(", "\n", "tuple", "(", "e", "//", "2", "for", "e", "in", "self", ".", "conv_kernel_size", ")", "\n", "if", "padding", "is", "None", "\n", "else", "_pair", "(", "padding", ")", "\n", ")", "\n", "self", ".", "conv_stride", "=", "_pair", "(", "conv_stride", ")", "\n", "\n", "self", ".", "layers", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "layer", "in", "range", "(", "num_conv_layers", ")", ":", "\n", "            ", "conv_op", "=", "nn", ".", "Conv2d", "(", "\n", "in_channels", "if", "layer", "==", "0", "else", "out_channels", ",", "\n", "out_channels", ",", "\n", "self", ".", "conv_kernel_size", ",", "\n", "stride", "=", "self", ".", "conv_stride", ",", "\n", "padding", "=", "self", ".", "padding", ",", "\n", ")", "\n", "self", ".", "layers", ".", "append", "(", "conv_op", ")", "\n", "if", "layer_norm", ":", "\n", "                ", "conv_output_dim", ",", "per_channel_dim", "=", "infer_conv_output_dim", "(", "\n", "conv_op", ",", "input_dim", ",", "in_channels", "if", "layer", "==", "0", "else", "out_channels", "\n", ")", "\n", "self", ".", "layers", ".", "append", "(", "nn", ".", "LayerNorm", "(", "per_channel_dim", ")", ")", "\n", "input_dim", "=", "per_channel_dim", "\n", "", "self", ".", "layers", ".", "append", "(", "nn", ".", "ReLU", "(", ")", ")", "\n", "\n", "", "pool_op", "=", "nn", ".", "MaxPool2d", "(", "kernel_size", "=", "pooling_kernel_size", ",", "ceil_mode", "=", "True", ")", "\n", "self", ".", "layers", ".", "append", "(", "pool_op", ")", "\n", "self", ".", "total_output_dim", ",", "self", ".", "output_dim", "=", "infer_conv_output_dim", "(", "\n", "pool_op", ",", "input_dim", ",", "out_channels", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.vggblock.VGGBlock.forward": [[112, 116], ["enumerate"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "for", "i", ",", "_", "in", "enumerate", "(", "self", ".", "layers", ")", ":", "\n", "            ", "x", "=", "self", ".", "layers", "[", "i", "]", "(", "x", ")", "\n", "", "return", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.vggblock._pair": [[15, 20], ["isinstance", "tuple", "itertools.repeat", "len"], "function", ["None"], ["def", "_pair", "(", "v", ")", ":", "\n", "    ", "if", "isinstance", "(", "v", ",", "Iterable", ")", ":", "\n", "        ", "assert", "len", "(", "v", ")", "==", "2", ",", "\"len(v) != 2\"", "\n", "return", "v", "\n", "", "return", "tuple", "(", "repeat", "(", "v", ",", "2", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.vggblock.infer_conv_output_dim": [[22, 36], ["torch.randn", "torch.randn", "conv_op", "x.transpose.transpose", "x.transpose.size", "x.transpose.size", "x.transpose.contiguous().view().size", "x.transpose.contiguous().view", "x.transpose.contiguous"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "infer_conv_output_dim", "(", "conv_op", ",", "input_dim", ",", "sample_inchannel", ")", ":", "\n", "    ", "sample_seq_len", "=", "200", "\n", "sample_bsz", "=", "10", "\n", "x", "=", "torch", ".", "randn", "(", "sample_bsz", ",", "sample_inchannel", ",", "sample_seq_len", ",", "input_dim", ")", "\n", "# N x C x H x W", "\n", "# N: sample_bsz, C: sample_inchannel, H: sample_seq_len, W: input_dim", "\n", "x", "=", "conv_op", "(", "x", ")", "\n", "# N x C x H x W", "\n", "x", "=", "x", ".", "transpose", "(", "1", ",", "2", ")", "\n", "# N x H x C x W", "\n", "bsz", ",", "seq", "=", "x", ".", "size", "(", ")", "[", ":", "2", "]", "\n", "per_channel_dim", "=", "x", ".", "size", "(", ")", "[", "3", "]", "\n", "# bsz: N, seq: H, CxW the rest", "\n", "return", "x", ".", "contiguous", "(", ")", ".", "view", "(", "bsz", ",", "seq", ",", "-", "1", ")", ".", "size", "(", "-", "1", ")", ",", "per_channel_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_sentence_encoder.TransformerSentenceEncoder.__init__": [[68, 168], ["torch.Module.__init__", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "range", "torch.Embedding", "torch.Embedding", "torch.Embedding", "fairseq.modules.PositionalEmbedding", "fairseq.modules.LayerNorm", "transformer_sentence_encoder.TransformerSentenceEncoder.apply", "transformer_sentence_encoder.TransformerSentenceEncoder.__init__.freeze_module_params"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.Embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.Embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.Embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.Embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.Embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.Embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.positional_embedding.PositionalEmbedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.layer_norm.LayerNorm"], ["def", "__init__", "(", "\n", "self", ",", "\n", "padding_idx", ":", "int", ",", "\n", "vocab_size", ":", "int", ",", "\n", "num_encoder_layers", ":", "int", "=", "6", ",", "\n", "embedding_dim", ":", "int", "=", "768", ",", "\n", "ffn_embedding_dim", ":", "int", "=", "3072", ",", "\n", "num_attention_heads", ":", "int", "=", "8", ",", "\n", "dropout", ":", "float", "=", "0.1", ",", "\n", "attention_dropout", ":", "float", "=", "0.1", ",", "\n", "activation_dropout", ":", "float", "=", "0.1", ",", "\n", "max_seq_len", ":", "int", "=", "256", ",", "\n", "num_segments", ":", "int", "=", "2", ",", "\n", "use_position_embeddings", ":", "bool", "=", "True", ",", "\n", "offset_positions_by_padding", ":", "bool", "=", "True", ",", "\n", "encoder_normalize_before", ":", "bool", "=", "False", ",", "\n", "apply_bert_init", ":", "bool", "=", "False", ",", "\n", "activation_fn", ":", "str", "=", "\"relu\"", ",", "\n", "learned_pos_embedding", ":", "bool", "=", "True", ",", "\n", "add_bias_kv", ":", "bool", "=", "False", ",", "\n", "add_zero_attn", ":", "bool", "=", "False", ",", "\n", "embed_scale", ":", "float", "=", "None", ",", "\n", "freeze_embeddings", ":", "bool", "=", "False", ",", "\n", "n_trans_layers_to_freeze", ":", "int", "=", "0", ",", "\n", "export", ":", "bool", "=", "False", ",", "\n", ")", "->", "None", ":", "\n", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "padding_idx", "=", "padding_idx", "\n", "self", ".", "vocab_size", "=", "vocab_size", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "max_seq_len", "=", "max_seq_len", "\n", "self", ".", "embedding_dim", "=", "embedding_dim", "\n", "self", ".", "num_segments", "=", "num_segments", "\n", "self", ".", "use_position_embeddings", "=", "use_position_embeddings", "\n", "self", ".", "apply_bert_init", "=", "apply_bert_init", "\n", "self", ".", "learned_pos_embedding", "=", "learned_pos_embedding", "\n", "\n", "self", ".", "embed_tokens", "=", "nn", ".", "Embedding", "(", "\n", "self", ".", "vocab_size", ",", "self", ".", "embedding_dim", ",", "self", ".", "padding_idx", "\n", ")", "\n", "self", ".", "embed_scale", "=", "embed_scale", "\n", "\n", "self", ".", "segment_embeddings", "=", "(", "\n", "nn", ".", "Embedding", "(", "self", ".", "num_segments", ",", "self", ".", "embedding_dim", ",", "padding_idx", "=", "None", ")", "\n", "if", "self", ".", "num_segments", ">", "0", "\n", "else", "None", "\n", ")", "\n", "\n", "self", ".", "embed_positions", "=", "(", "\n", "PositionalEmbedding", "(", "\n", "self", ".", "max_seq_len", ",", "\n", "self", ".", "embedding_dim", ",", "\n", "padding_idx", "=", "(", "self", ".", "padding_idx", "if", "offset_positions_by_padding", "else", "None", ")", ",", "\n", "learned", "=", "self", ".", "learned_pos_embedding", ",", "\n", ")", "\n", "if", "self", ".", "use_position_embeddings", "\n", "else", "None", "\n", ")", "\n", "\n", "self", ".", "layers", "=", "nn", ".", "ModuleList", "(", "\n", "[", "\n", "TransformerSentenceEncoderLayer", "(", "\n", "embedding_dim", "=", "self", ".", "embedding_dim", ",", "\n", "ffn_embedding_dim", "=", "ffn_embedding_dim", ",", "\n", "num_attention_heads", "=", "num_attention_heads", ",", "\n", "dropout", "=", "self", ".", "dropout", ",", "\n", "attention_dropout", "=", "attention_dropout", ",", "\n", "activation_dropout", "=", "activation_dropout", ",", "\n", "activation_fn", "=", "activation_fn", ",", "\n", "add_bias_kv", "=", "add_bias_kv", ",", "\n", "add_zero_attn", "=", "add_zero_attn", ",", "\n", "export", "=", "export", ",", "\n", ")", "\n", "for", "_", "in", "range", "(", "num_encoder_layers", ")", "\n", "]", "\n", ")", "\n", "\n", "if", "encoder_normalize_before", ":", "\n", "            ", "self", ".", "emb_layer_norm", "=", "LayerNorm", "(", "self", ".", "embedding_dim", ",", "export", "=", "export", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "emb_layer_norm", "=", "None", "\n", "\n", "# Apply initialization of model params after building the model", "\n", "", "if", "self", ".", "apply_bert_init", ":", "\n", "            ", "self", ".", "apply", "(", "init_bert_params", ")", "\n", "\n", "", "def", "freeze_module_params", "(", "m", ")", ":", "\n", "            ", "if", "m", "is", "not", "None", ":", "\n", "                ", "for", "p", "in", "m", ".", "parameters", "(", ")", ":", "\n", "                    ", "p", ".", "requires_grad", "=", "False", "\n", "\n", "", "", "", "if", "freeze_embeddings", ":", "\n", "            ", "freeze_module_params", "(", "self", ".", "embed_tokens", ")", "\n", "freeze_module_params", "(", "self", ".", "segment_embeddings", ")", "\n", "freeze_module_params", "(", "self", ".", "embed_positions", ")", "\n", "freeze_module_params", "(", "self", ".", "emb_layer_norm", ")", "\n", "\n", "", "for", "layer", "in", "range", "(", "n_trans_layers_to_freeze", ")", ":", "\n", "            ", "freeze_module_params", "(", "self", ".", "layers", "[", "layer", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_sentence_encoder.TransformerSentenceEncoder.forward": [[169, 223], ["tokens.eq", "transformer_sentence_encoder.TransformerSentenceEncoder.embed_tokens", "torch.dropout", "torch.dropout", "torch.dropout", "transformer_sentence_encoder.TransformerSentenceEncoder.transpose", "transformer_sentence_encoder.TransformerSentenceEncoder.transpose", "tokens.eq.any", "transformer_sentence_encoder.TransformerSentenceEncoder.embed_positions", "transformer_sentence_encoder.TransformerSentenceEncoder.segment_embeddings", "transformer_sentence_encoder.TransformerSentenceEncoder.emb_layer_norm", "inner_states.append", "layer", "tokens.eq.unsqueeze().type_as", "inner_states.append", "tokens.eq.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "", "def", "forward", "(", "\n", "self", ",", "\n", "tokens", ":", "torch", ".", "Tensor", ",", "\n", "segment_labels", ":", "torch", ".", "Tensor", "=", "None", ",", "\n", "last_state_only", ":", "bool", "=", "False", ",", "\n", "positions", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "\n", "# compute padding mask. This is needed for multi-head attention", "\n", "        ", "padding_mask", "=", "tokens", ".", "eq", "(", "self", ".", "padding_idx", ")", "\n", "if", "not", "padding_mask", ".", "any", "(", ")", ":", "\n", "            ", "padding_mask", "=", "None", "\n", "\n", "", "x", "=", "self", ".", "embed_tokens", "(", "tokens", ")", "\n", "\n", "if", "self", ".", "embed_scale", "is", "not", "None", ":", "\n", "            ", "x", "*=", "self", ".", "embed_scale", "\n", "\n", "", "if", "self", ".", "embed_positions", "is", "not", "None", ":", "\n", "            ", "x", "+=", "self", ".", "embed_positions", "(", "tokens", ",", "positions", "=", "positions", ")", "\n", "\n", "", "if", "self", ".", "segment_embeddings", "is", "not", "None", "and", "segment_labels", "is", "not", "None", ":", "\n", "            ", "x", "+=", "self", ".", "segment_embeddings", "(", "segment_labels", ")", "\n", "\n", "", "if", "self", ".", "emb_layer_norm", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "emb_layer_norm", "(", "x", ")", "\n", "\n", "", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "\n", "# account for padding while computing the representation", "\n", "if", "padding_mask", "is", "not", "None", ":", "\n", "            ", "x", "*=", "1", "-", "padding_mask", ".", "unsqueeze", "(", "-", "1", ")", ".", "type_as", "(", "x", ")", "\n", "\n", "# B x T x C -> T x B x C", "\n", "", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "inner_states", "=", "[", "]", "\n", "if", "not", "last_state_only", ":", "\n", "            ", "inner_states", ".", "append", "(", "x", ")", "\n", "\n", "", "for", "layer", "in", "self", ".", "layers", ":", "\n", "            ", "x", ",", "_", "=", "layer", "(", "x", ",", "self_attn_padding_mask", "=", "padding_mask", ")", "\n", "if", "not", "last_state_only", ":", "\n", "                ", "inner_states", ".", "append", "(", "x", ")", "\n", "\n", "# T x B x C -> B x T x C", "\n", "", "", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "sentence_rep", "=", "x", "[", ":", ",", "0", ",", ":", "]", "\n", "\n", "if", "last_state_only", ":", "\n", "            ", "inner_states", "=", "[", "x", "]", "\n", "\n", "", "return", "inner_states", ",", "sentence_rep", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_sentence_encoder.init_bert_params": [[19, 42], ["isinstance", "isinstance", "isinstance", "module.weight.data.normal_", "module.weight.data.normal_", "module.weight.data[].zero_", "module.in_proj_weight.data.normal_", "module.bias.data.zero_"], "function", ["None"], ["def", "init_bert_params", "(", "module", ")", ":", "\n", "    ", "\"\"\"\n    Initialize the weights specific to the BERT Model.\n    This overrides the default initializations depending on the specified arguments.\n        1. If normal_init_linear_weights is set then weights of linear\n           layer will be initialized using the normal distribution and\n           bais will be set to the specified value.\n        2. If normal_init_embed_weights is set then weights of embedding\n           layer will be initialized using the normal distribution.\n        3. If normal_init_proj_weights is set then weights of\n           in_project_weight for MultiHeadAttention initialized using\n           the normal distribution (to be validated).\n    \"\"\"", "\n", "\n", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", ":", "\n", "        ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "0.02", ")", "\n", "if", "module", ".", "bias", "is", "not", "None", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "", "", "if", "isinstance", "(", "module", ",", "nn", ".", "Embedding", ")", ":", "\n", "        ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "0.02", ")", "\n", "module", ".", "weight", ".", "data", "[", "module", ".", "padding_idx", "]", ".", "zero_", "(", ")", "\n", "", "if", "isinstance", "(", "module", ",", "MultiheadAttention", ")", ":", "\n", "        ", "module", ".", "in_proj_weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "0.02", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.downsampled_multihead_attention.SingleHeadAttention.__init__": [[19, 61], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "k_layers.append", "v_layers.append", "k_layers.append", "downsampled_multihead_attention.GatedLinear", "v_layers.append", "k_layers.append", "downsampled_multihead_attention.Linear", "v_layers.append", "downsampled_multihead_attention.Linear", "downsampled_multihead_attention.Linear", "downsampled_multihead_attention.Downsample", "downsampled_multihead_attention.Downsample", "downsampled_multihead_attention.GatedLinear", "downsampled_multihead_attention.GatedLinear", "downsampled_multihead_attention.Linear", "downsampled_multihead_attention.Linear"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.downsampled_multihead_attention.GatedLinear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.downsampled_multihead_attention.GatedLinear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.downsampled_multihead_attention.GatedLinear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear"], ["def", "__init__", "(", "\n", "self", ",", "out_channels", ",", "embed_dim", ",", "head_dim", ",", "head_index", ",", "dropout", "=", "0.", ",", "\n", "bias", "=", "True", ",", "project_input", "=", "True", ",", "gated", "=", "False", ",", "downsample", "=", "False", ",", "\n", "num_heads", "=", "1", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "embed_dim", "=", "embed_dim", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "head_index", "=", "head_index", "\n", "self", ".", "head_dim", "=", "head_dim", "\n", "self", ".", "project_input", "=", "project_input", "\n", "self", ".", "gated", "=", "gated", "\n", "self", ".", "downsample", "=", "downsample", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "self", ".", "projection", "=", "None", "\n", "\n", "k_layers", "=", "[", "]", "\n", "v_layers", "=", "[", "]", "\n", "if", "self", ".", "downsample", ":", "\n", "            ", "k_layers", ".", "append", "(", "Downsample", "(", "self", ".", "head_index", ")", ")", "\n", "v_layers", ".", "append", "(", "Downsample", "(", "self", ".", "head_index", ")", ")", "\n", "out_proj_size", "=", "self", ".", "head_dim", "\n", "", "else", ":", "\n", "            ", "out_proj_size", "=", "self", ".", "head_dim", "*", "self", ".", "num_heads", "\n", "", "if", "self", ".", "gated", ":", "\n", "            ", "k_layers", ".", "append", "(", "GatedLinear", "(", "self", ".", "embed_dim", ",", "out_proj_size", ",", "bias", "=", "bias", ")", ")", "\n", "self", ".", "in_proj_q", "=", "GatedLinear", "(", "self", ".", "embed_dim", ",", "out_proj_size", ",", "bias", "=", "bias", ")", "\n", "v_layers", ".", "append", "(", "GatedLinear", "(", "self", ".", "embed_dim", ",", "out_proj_size", ",", "bias", "=", "bias", ")", ")", "\n", "", "else", ":", "\n", "            ", "k_layers", ".", "append", "(", "Linear", "(", "self", ".", "embed_dim", ",", "out_proj_size", ",", "bias", "=", "bias", ")", ")", "\n", "self", ".", "in_proj_q", "=", "Linear", "(", "self", ".", "embed_dim", ",", "out_proj_size", ",", "bias", "=", "bias", ")", "\n", "v_layers", ".", "append", "(", "Linear", "(", "self", ".", "embed_dim", ",", "out_proj_size", ",", "bias", "=", "bias", ")", ")", "\n", "\n", "", "self", ".", "in_proj_k", "=", "nn", ".", "Sequential", "(", "*", "k_layers", ")", "\n", "self", ".", "in_proj_v", "=", "nn", ".", "Sequential", "(", "*", "v_layers", ")", "\n", "\n", "if", "self", ".", "downsample", ":", "\n", "            ", "self", ".", "out_proj", "=", "Linear", "(", "out_proj_size", ",", "self", ".", "head_dim", ",", "bias", "=", "bias", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "out_proj", "=", "Linear", "(", "out_proj_size", ",", "out_channels", ",", "bias", "=", "bias", ")", "\n", "\n", "", "self", ".", "scaling", "=", "self", ".", "head_dim", "**", "-", "0.5", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.downsampled_multihead_attention.SingleHeadAttention.forward": [[62, 148], ["key.size", "query.size", "q.view.view.transpose", "k.view.view.transpose", "fairseq.modules.scalar_bias.scalar_bias.transpose", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.softmax", "torch.softmax", "torch.softmax", "torch.dropout", "torch.dropout", "torch.dropout", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "downsampled_multihead_attention.SingleHeadAttention.out_proj", "list", "key.size", "value.size", "downsampled_multihead_attention.SingleHeadAttention.in_proj_q", "downsampled_multihead_attention.SingleHeadAttention.in_proj_k", "downsampled_multihead_attention.SingleHeadAttention.in_proj_v", "q.view.view.view", "k.view.view.view", "fairseq.modules.scalar_bias.scalar_bias.view", "k.view.view.transpose", "[].unsqueeze", "[].unsqueeze", "fairseq.modules.scalar_bias.scalar_bias", "fairseq.modules.scalar_bias.scalar_bias", "attn.transpose().contiguous().view.transpose().contiguous().view.transpose().contiguous().view", "attn.transpose().contiguous().view.transpose().contiguous().view.transpose().contiguous().view", "query.size", "key_padding_mask.size", "key_padding_mask.size", "k.view.view.size", "query.size", "key.size", "key_padding_mask.max", "attn_weights.view.view.masked_fill", "attn_weights.view.view.view", "attn_weights.view.view.view", "attn_weights.view.view.view", "key_padding_mask.unsqueeze().unsqueeze", "attn.transpose().contiguous().view.transpose().contiguous().view.transpose().contiguous", "attn.transpose().contiguous().view.transpose().contiguous().view.transpose().contiguous", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.tril", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "attn_weights.view.view.data.new().expand().clone", "attn_weights.view.view.data.new().expand().clone", "key_padding_mask.unsqueeze", "attn.transpose().contiguous().view.transpose().contiguous().view.transpose", "attn.transpose().contiguous().view.transpose().contiguous().view.transpose", "attn_weights.view.view.data.new().expand", "attn_weights.view.view.data.new().expand", "attn_weights.view.view.data.new", "attn_weights.view.view.data.new"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.multihead_attention.MultiheadAttention.in_proj_q", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.multihead_attention.MultiheadAttention.in_proj_k", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.multihead_attention.MultiheadAttention.in_proj_v", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.scalar_bias.scalar_bias", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.scalar_bias.scalar_bias", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "forward", "(", "\n", "self", ",", "query", ",", "key", ",", "value", ",", "mask_future_timesteps", "=", "False", ",", "\n", "key_padding_mask", "=", "None", ",", "use_scalar_bias", "=", "False", ",", "\n", ")", ":", "\n", "        ", "\"\"\"Input shape: Time x Batch x Channel\n        Self-attention can be implemented by passing in the same arguments for\n        query, key and value. Future timesteps can be masked with the\n        `mask_future_timesteps` argument. Padding elements can be excluded from\n        the key by passing a binary ByteTensor (`key_padding_mask`) with shape:\n        batch x src_len, where padding elements are indicated by 1s.\n        \"\"\"", "\n", "src_len", ",", "bsz", ",", "out_channels", "=", "key", ".", "size", "(", ")", "\n", "tgt_len", "=", "query", ".", "size", "(", "0", ")", "\n", "assert", "list", "(", "query", ".", "size", "(", ")", ")", "==", "[", "tgt_len", ",", "bsz", ",", "out_channels", "]", "\n", "assert", "key", ".", "size", "(", ")", "==", "value", ".", "size", "(", ")", "\n", "\n", "if", "key_padding_mask", "is", "not", "None", ":", "\n", "            ", "assert", "key_padding_mask", ".", "size", "(", "0", ")", "==", "bsz", "\n", "assert", "key_padding_mask", ".", "size", "(", "1", ")", "==", "src_len", "\n", "\n", "", "if", "self", ".", "downsample", ":", "\n", "            ", "size", "=", "bsz", "\n", "", "else", ":", "\n", "            ", "size", "=", "bsz", "*", "self", ".", "num_heads", "\n", "\n", "", "k", "=", "key", "\n", "v", "=", "value", "\n", "q", "=", "query", "\n", "if", "self", ".", "project_input", ":", "\n", "            ", "q", "=", "self", ".", "in_proj_q", "(", "q", ")", "\n", "k", "=", "self", ".", "in_proj_k", "(", "k", ")", "\n", "v", "=", "self", ".", "in_proj_v", "(", "v", ")", "\n", "src_len", "=", "k", ".", "size", "(", ")", "[", "0", "]", "\n", "", "q", "*=", "self", ".", "scaling", "\n", "\n", "if", "not", "self", ".", "downsample", ":", "\n", "            ", "q", "=", "q", ".", "view", "(", "tgt_len", ",", "size", ",", "self", ".", "head_dim", ")", "\n", "k", "=", "k", ".", "view", "(", "src_len", ",", "size", ",", "self", ".", "head_dim", ")", "\n", "v", "=", "v", ".", "view", "(", "src_len", ",", "size", ",", "self", ".", "head_dim", ")", "\n", "\n", "", "q", "=", "q", ".", "transpose", "(", "0", ",", "1", ")", "\n", "k", "=", "k", ".", "transpose", "(", "0", ",", "1", ")", "\n", "v", "=", "v", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "attn_weights", "=", "torch", ".", "bmm", "(", "q", ",", "k", ".", "transpose", "(", "1", ",", "2", ")", ")", "\n", "if", "mask_future_timesteps", ":", "\n", "            ", "assert", "query", ".", "size", "(", ")", "==", "key", ".", "size", "(", ")", ",", "'mask_future_timesteps only applies to self-attention'", "\n", "attn_weights", "*=", "torch", ".", "tril", "(", "\n", "attn_weights", ".", "data", ".", "new", "(", "[", "1", "]", ")", ".", "expand", "(", "tgt_len", ",", "tgt_len", ")", ".", "clone", "(", ")", ",", "\n", "diagonal", "=", "-", "1", ",", "\n", ")", "[", ":", ",", ":", ":", "self", ".", "head_index", "+", "1", "if", "self", ".", "downsample", "else", "1", "]", ".", "unsqueeze", "(", "0", ")", "\n", "attn_weights", "+=", "torch", ".", "triu", "(", "\n", "attn_weights", ".", "data", ".", "new", "(", "[", "-", "math", ".", "inf", "]", ")", ".", "expand", "(", "tgt_len", ",", "tgt_len", ")", ".", "clone", "(", ")", ",", "\n", "diagonal", "=", "0", "\n", ")", "[", ":", ",", ":", ":", "self", ".", "head_index", "+", "1", "if", "self", ".", "downsample", "else", "1", "]", ".", "unsqueeze", "(", "0", ")", "\n", "", "tgt_size", "=", "tgt_len", "\n", "if", "use_scalar_bias", ":", "\n", "            ", "attn_weights", "=", "scalar_bias", "(", "attn_weights", ",", "2", ")", "\n", "v", "=", "scalar_bias", "(", "v", ",", "1", ")", "\n", "tgt_size", "+=", "1", "\n", "\n", "", "if", "key_padding_mask", "is", "not", "None", ":", "\n", "# don't attend to padding symbols", "\n", "            ", "if", "key_padding_mask", ".", "max", "(", ")", ">", "0", ":", "\n", "                ", "if", "self", ".", "downsample", ":", "\n", "                    ", "attn_weights", "=", "attn_weights", ".", "view", "(", "bsz", ",", "1", ",", "tgt_len", ",", "src_len", ")", "\n", "", "else", ":", "\n", "                    ", "attn_weights", "=", "attn_weights", ".", "view", "(", "size", ",", "self", ".", "num_heads", ",", "tgt_len", ",", "src_len", ")", "\n", "", "attn_weights", "=", "attn_weights", ".", "masked_fill", "(", "\n", "key_padding_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", ",", "\n", "-", "math", ".", "inf", ",", "\n", ")", "\n", "attn_weights", "=", "attn_weights", ".", "view", "(", "size", ",", "tgt_len", ",", "src_len", ")", "\n", "", "", "attn_weights", "=", "F", ".", "softmax", "(", "attn_weights", ",", "dim", "=", "-", "1", ")", "\n", "attn_weights", "=", "F", ".", "dropout", "(", "attn_weights", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "\n", "attn", "=", "torch", ".", "bmm", "(", "attn_weights", ",", "v", ")", "\n", "if", "self", ".", "downsample", ":", "\n", "            ", "attn", "=", "attn", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", ".", "view", "(", "tgt_len", ",", "bsz", ",", "self", ".", "head_dim", ")", "\n", "", "else", ":", "\n", "            ", "attn", "=", "attn", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", ".", "view", "(", "tgt_len", ",", "bsz", ",", "self", ".", "embed_dim", ")", "\n", "\n", "", "attn", "=", "self", ".", "out_proj", "(", "attn", ")", "\n", "\n", "return", "attn", ",", "attn_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.downsampled_multihead_attention.DownsampledMultiHeadAttention.__init__": [[154, 186], ["range", "torch.ModuleList.__init__", "downsampled_multihead_attention.Linear", "torch.ModuleList.__init__", "downsampled_multihead_attention.SingleHeadAttention", "attention_heads.append", "downsampled_multihead_attention.SingleHeadAttention"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["def", "__init__", "(", "\n", "self", ",", "out_channels", ",", "embed_dim", ",", "num_heads", ",", "dropout", "=", "0.", ",", "bias", "=", "True", ",", "\n", "project_input", "=", "True", ",", "gated", "=", "False", ",", "downsample", "=", "False", ",", "\n", ")", ":", "\n", "        ", "self", ".", "embed_dim", "=", "embed_dim", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "head_dim", "=", "embed_dim", "//", "num_heads", "\n", "self", ".", "downsample", "=", "downsample", "\n", "self", ".", "gated", "=", "gated", "\n", "self", ".", "project_input", "=", "project_input", "\n", "assert", "self", ".", "head_dim", "*", "num_heads", "==", "embed_dim", "\n", "\n", "if", "self", ".", "downsample", ":", "\n", "            ", "attention_heads", "=", "[", "]", "\n", "for", "index", "in", "range", "(", "self", ".", "num_heads", ")", ":", "\n", "                ", "attention_heads", ".", "append", "(", "\n", "SingleHeadAttention", "(", "\n", "out_channels", ",", "self", ".", "embed_dim", ",", "self", ".", "head_dim", ",", "index", ",", "\n", "self", ".", "dropout", ",", "bias", ",", "self", ".", "project_input", ",", "self", ".", "gated", ",", "\n", "self", ".", "downsample", ",", "self", ".", "num_heads", ",", "\n", ")", "\n", ")", "\n", "", "super", "(", ")", ".", "__init__", "(", "modules", "=", "attention_heads", ")", "\n", "self", ".", "out_proj", "=", "Linear", "(", "embed_dim", ",", "out_channels", ",", "bias", "=", "bias", ")", "\n", "", "else", ":", "\n", "# either we have a list of attention heads, or just one attention head", "\n", "# if not being downsampled, we can do the heads with one linear layer instead of separate ones", "\n", "            ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "attention_module", "=", "SingleHeadAttention", "(", "\n", "out_channels", ",", "self", ".", "embed_dim", ",", "self", ".", "head_dim", ",", "1", ",", "self", ".", "dropout", ",", "\n", "bias", ",", "self", ".", "project_input", ",", "self", ".", "gated", ",", "self", ".", "downsample", ",", "self", ".", "num_heads", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.downsampled_multihead_attention.DownsampledMultiHeadAttention.forward": [[188, 226], ["key.size", "query.size", "list", "key.size", "value.size", "range", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "downsampled_multihead_attention.DownsampledMultiHeadAttention.out_proj", "downsampled_multihead_attention.DownsampledMultiHeadAttention.attention_module", "attn.append", "attn_weights.append", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "full_attn_weights.view.view.view", "query.size", "attn.append", "attn_weights.append", "attn_weights[].clone", "full_attn_weights.view.view.sum"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "", "def", "forward", "(", "\n", "self", ",", "query", ",", "key", ",", "value", ",", "mask_future_timesteps", "=", "False", ",", "\n", "key_padding_mask", "=", "None", ",", "use_scalar_bias", "=", "False", ",", "\n", ")", ":", "\n", "        ", "src_len", ",", "bsz", ",", "embed_dim", "=", "key", ".", "size", "(", ")", "\n", "tgt_len", "=", "query", ".", "size", "(", "0", ")", "\n", "assert", "embed_dim", "==", "self", ".", "embed_dim", "\n", "assert", "list", "(", "query", ".", "size", "(", ")", ")", "==", "[", "tgt_len", ",", "bsz", ",", "embed_dim", "]", "\n", "assert", "key", ".", "size", "(", ")", "==", "value", ".", "size", "(", ")", "\n", "\n", "tgt_size", "=", "tgt_len", "\n", "if", "use_scalar_bias", ":", "\n", "            ", "tgt_size", "+=", "1", "\n", "\n", "", "attn", "=", "[", "]", "\n", "attn_weights", "=", "[", "]", "\n", "if", "self", ".", "downsample", ":", "\n", "            ", "for", "attention_head_number", "in", "range", "(", "self", ".", "num_heads", ")", ":", "\n", "# call the forward of each attention head", "\n", "                ", "_attn", ",", "_attn_weight", "=", "self", "[", "attention_head_number", "]", "(", "\n", "query", ",", "key", ",", "value", ",", "mask_future_timesteps", ",", "key_padding_mask", ",", "use_scalar_bias", ",", "\n", ")", "\n", "attn", ".", "append", "(", "_attn", ")", "\n", "attn_weights", ".", "append", "(", "_attn_weight", ")", "\n", "", "full_attn", "=", "torch", ".", "cat", "(", "attn", ",", "dim", "=", "2", ")", "\n", "full_attn", "=", "self", ".", "out_proj", "(", "full_attn", ")", "\n", "return", "full_attn", ",", "attn_weights", "[", "0", "]", ".", "clone", "(", ")", "\n", "", "else", ":", "\n", "            ", "_attn", ",", "_attn_weight", "=", "self", ".", "attention_module", "(", "\n", "query", ",", "key", ",", "value", ",", "mask_future_timesteps", ",", "key_padding_mask", ",", "use_scalar_bias", ",", "\n", ")", "\n", "attn", ".", "append", "(", "_attn", ")", "\n", "attn_weights", ".", "append", "(", "_attn_weight", ")", "\n", "full_attn", "=", "torch", ".", "cat", "(", "attn", ",", "dim", "=", "2", ")", "\n", "full_attn_weights", "=", "torch", ".", "cat", "(", "attn_weights", ")", "\n", "full_attn_weights", "=", "full_attn_weights", ".", "view", "(", "bsz", ",", "self", ".", "num_heads", ",", "tgt_size", ",", "src_len", ")", "\n", "full_attn_weights", "=", "full_attn_weights", ".", "sum", "(", "dim", "=", "1", ")", "/", "self", ".", "num_heads", "\n", "return", "full_attn", ",", "full_attn_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.downsampled_multihead_attention.Downsample.__init__": [[232, 235], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["def", "__init__", "(", "self", ",", "index", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "index", "=", "index", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.downsampled_multihead_attention.Downsample.forward": [[236, 238], ["None"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "x", "[", ":", ":", "self", ".", "index", "+", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.downsampled_multihead_attention.Linear": [[240, 246], ["torch.Linear", "nn.Linear.weight.data.normal_", "nn.Linear.bias.data.zero_", "torch.utils.weight_norm", "math.sqrt"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear"], ["", "", "def", "Linear", "(", "in_features", ",", "out_features", ",", "dropout", "=", "0.", ",", "bias", "=", "True", ")", ":", "\n", "    ", "\"\"\"Weight-normalized Linear layer (input: B x T x C)\"\"\"", "\n", "m", "=", "nn", ".", "Linear", "(", "in_features", ",", "out_features", ",", "bias", "=", "bias", ")", "\n", "m", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0", ",", "std", "=", "math", ".", "sqrt", "(", "(", "1", "-", "dropout", ")", "/", "in_features", ")", ")", "\n", "m", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "return", "nn", ".", "utils", ".", "weight_norm", "(", "m", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.downsampled_multihead_attention.GatedLinear": [[248, 256], ["torch.Sequential", "downsampled_multihead_attention.Linear", "torch.GLU", "downsampled_multihead_attention.Linear", "torch.GLU", "downsampled_multihead_attention.Linear"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear"], ["", "def", "GatedLinear", "(", "in_features", ",", "out_features", ",", "dropout", "=", "0.", ",", "bias", "=", "True", ")", ":", "\n", "    ", "\"\"\"Weight-normalized Linear layer (input: B x T x C) with interspersed GLU units\"\"\"", "\n", "return", "nn", ".", "Sequential", "(", "\n", "Linear", "(", "in_features", ",", "out_features", "*", "4", ",", "dropout", ",", "bias", ")", ",", "\n", "nn", ".", "GLU", "(", ")", ",", "\n", "Linear", "(", "out_features", "*", "2", ",", "out_features", "*", "2", ",", "dropout", ",", "bias", ")", ",", "\n", "nn", ".", "GLU", "(", ")", ",", "\n", "Linear", "(", "out_features", ",", "out_features", ",", "dropout", ",", "bias", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.TransformerEncoderLayer.__init__": [[27, 47], ["torch.Module.__init__", "fairseq.modules.MultiheadAttention", "fairseq.modules.LayerNorm", "fairseq.utils.get_activation_fn", "getattr", "transformer_layer.Linear", "transformer_layer.Linear", "fairseq.modules.LayerNorm", "getattr", "getattr"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.layer_norm.LayerNorm", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.get_activation_fn", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.layer_norm.LayerNorm"], ["def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "embed_dim", "=", "args", ".", "encoder_embed_dim", "\n", "self", ".", "self_attn", "=", "MultiheadAttention", "(", "\n", "self", ".", "embed_dim", ",", "args", ".", "encoder_attention_heads", ",", "\n", "dropout", "=", "args", ".", "attention_dropout", ",", "self_attention", "=", "True", "\n", ")", "\n", "self", ".", "self_attn_layer_norm", "=", "LayerNorm", "(", "self", ".", "embed_dim", ")", "\n", "self", ".", "dropout", "=", "args", ".", "dropout", "\n", "self", ".", "activation_fn", "=", "utils", ".", "get_activation_fn", "(", "\n", "activation", "=", "getattr", "(", "args", ",", "'activation_fn'", ",", "'relu'", ")", "\n", ")", "\n", "self", ".", "activation_dropout", "=", "getattr", "(", "args", ",", "'activation_dropout'", ",", "0", ")", "\n", "if", "self", ".", "activation_dropout", "==", "0", ":", "\n", "# for backwards compatibility with models that use args.relu_dropout", "\n", "            ", "self", ".", "activation_dropout", "=", "getattr", "(", "args", ",", "'relu_dropout'", ",", "0", ")", "\n", "", "self", ".", "normalize_before", "=", "args", ".", "encoder_normalize_before", "\n", "self", ".", "fc1", "=", "Linear", "(", "self", ".", "embed_dim", ",", "args", ".", "encoder_ffn_embed_dim", ")", "\n", "self", ".", "fc2", "=", "Linear", "(", "args", ".", "encoder_ffn_embed_dim", ",", "self", ".", "embed_dim", ")", "\n", "self", ".", "final_layer_norm", "=", "LayerNorm", "(", "self", ".", "embed_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.TransformerEncoderLayer.upgrade_state_dict_named": [[48, 66], ["layer_norm_map.items"], "methods", ["None"], ["", "def", "upgrade_state_dict_named", "(", "self", ",", "state_dict", ",", "name", ")", ":", "\n", "        ", "\"\"\"\n        Rename layer norm states from `...layer_norms.0.weight` to\n        `...self_attn_layer_norm.weight` and `...layer_norms.1.weight` to\n        `...final_layer_norm.weight`\n        \"\"\"", "\n", "layer_norm_map", "=", "{", "\n", "'0'", ":", "'self_attn_layer_norm'", ",", "\n", "'1'", ":", "'final_layer_norm'", "\n", "}", "\n", "for", "old", ",", "new", "in", "layer_norm_map", ".", "items", "(", ")", ":", "\n", "            ", "for", "m", "in", "(", "'weight'", ",", "'bias'", ")", ":", "\n", "                ", "k", "=", "'{}.layer_norms.{}.{}'", ".", "format", "(", "name", ",", "old", ",", "m", ")", "\n", "if", "k", "in", "state_dict", ":", "\n", "                    ", "state_dict", "[", "\n", "'{}.{}.{}'", ".", "format", "(", "name", ",", "new", ",", "m", ")", "\n", "]", "=", "state_dict", "[", "k", "]", "\n", "del", "state_dict", "[", "k", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.TransformerEncoderLayer.forward": [[67, 108], ["transformer_layer.TransformerEncoderLayer.maybe_layer_norm", "transformer_layer.TransformerEncoderLayer.self_attn", "torch.dropout", "torch.dropout", "transformer_layer.TransformerEncoderLayer.maybe_layer_norm", "transformer_layer.TransformerEncoderLayer.maybe_layer_norm", "transformer_layer.TransformerEncoderLayer.activation_fn", "torch.dropout", "torch.dropout", "transformer_layer.TransformerEncoderLayer.fc2", "torch.dropout", "torch.dropout", "transformer_layer.TransformerEncoderLayer.maybe_layer_norm", "attn_mask.masked_fill.masked_fill.masked_fill", "transformer_layer.TransformerEncoderLayer.fc1", "attn_mask.masked_fill.masked_fill.byte"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.TransformerDecoderLayer.maybe_layer_norm", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.TransformerDecoderLayer.maybe_layer_norm", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.TransformerDecoderLayer.maybe_layer_norm", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.TransformerDecoderLayer.maybe_layer_norm"], ["", "", "", "", "def", "forward", "(", "self", ",", "x", ",", "encoder_padding_mask", ",", "attn_mask", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`\n            encoder_padding_mask (ByteTensor): binary ByteTensor of shape\n                `(batch, src_len)` where padding elements are indicated by ``1``.\n            attn_mask (ByteTensor): binary tensor of shape (T_tgt, T_src), where\n            T_tgt is the length of query, while T_src is the length of key,\n            though here both query and key is x here,\n            attn_mask[t_tgt, t_src] = 1 means when calculating embedding\n            for t_tgt, t_src is excluded (or masked out), =0 means it is\n            included in attention\n\n        Returns:\n            encoded output of shape `(seq_len, batch, embed_dim)`\n        \"\"\"", "\n", "residual", "=", "x", "\n", "x", "=", "self", ".", "maybe_layer_norm", "(", "self", ".", "self_attn_layer_norm", ",", "x", ",", "before", "=", "True", ")", "\n", "if", "attn_mask", "is", "not", "None", ":", "\n", "            ", "attn_mask", "=", "attn_mask", ".", "masked_fill", "(", "attn_mask", ".", "byte", "(", ")", ",", "-", "1e8", ")", "\n", "# anything in original attn_mask = 1, becomes -1e8", "\n", "# anything in original attn_mask = 0, becomes 0", "\n", "# Note that we cannot use -inf here, because at some edge cases,", "\n", "# the attention weight (before softmax) for some padded element in query", "\n", "# will become -inf, which results in NaN in model parameters", "\n", "# TODO: to formally solve this problem, we need to change fairseq's", "\n", "# MultiheadAttention. We will do this later on.", "\n", "", "x", ",", "_", "=", "self", ".", "self_attn", "(", "query", "=", "x", ",", "key", "=", "x", ",", "value", "=", "x", ",", "key_padding_mask", "=", "encoder_padding_mask", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "residual", "+", "x", "\n", "x", "=", "self", ".", "maybe_layer_norm", "(", "self", ".", "self_attn_layer_norm", ",", "x", ",", "after", "=", "True", ")", "\n", "\n", "residual", "=", "x", "\n", "x", "=", "self", ".", "maybe_layer_norm", "(", "self", ".", "final_layer_norm", ",", "x", ",", "before", "=", "True", ")", "\n", "x", "=", "self", ".", "activation_fn", "(", "self", ".", "fc1", "(", "x", ")", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "activation_dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "self", ".", "fc2", "(", "x", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "residual", "+", "x", "\n", "x", "=", "self", ".", "maybe_layer_norm", "(", "self", ".", "final_layer_norm", ",", "x", ",", "after", "=", "True", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.TransformerEncoderLayer.maybe_layer_norm": [[109, 115], ["layer_norm"], "methods", ["None"], ["", "def", "maybe_layer_norm", "(", "self", ",", "layer_norm", ",", "x", ",", "before", "=", "False", ",", "after", "=", "False", ")", ":", "\n", "        ", "assert", "before", "^", "after", "\n", "if", "after", "^", "self", ".", "normalize_before", ":", "\n", "            ", "return", "layer_norm", "(", "x", ")", "\n", "", "else", ":", "\n", "            ", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.TransformerDecoderLayer.__init__": [[134, 182], ["torch.Module.__init__", "fairseq.modules.MultiheadAttention", "fairseq.utils.get_activation_fn", "getattr", "getattr", "fairseq.modules.LayerNorm", "transformer_layer.Linear", "transformer_layer.Linear", "fairseq.modules.LayerNorm", "getattr", "fairseq.modules.MultiheadAttention", "fairseq.modules.LayerNorm", "getattr", "getattr", "getattr"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.get_activation_fn", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.layer_norm.LayerNorm", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.layer_norm.LayerNorm", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.layer_norm.LayerNorm"], ["def", "__init__", "(", "self", ",", "args", ",", "no_encoder_attn", "=", "False", ",", "add_bias_kv", "=", "False", ",", "add_zero_attn", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "embed_dim", "=", "args", ".", "decoder_embed_dim", "\n", "self", ".", "self_attn", "=", "MultiheadAttention", "(", "\n", "embed_dim", "=", "self", ".", "embed_dim", ",", "\n", "num_heads", "=", "args", ".", "decoder_attention_heads", ",", "\n", "dropout", "=", "args", ".", "attention_dropout", ",", "\n", "add_bias_kv", "=", "add_bias_kv", ",", "\n", "add_zero_attn", "=", "add_zero_attn", ",", "\n", "self_attention", "=", "True", "\n", ")", "\n", "self", ".", "dropout", "=", "args", ".", "dropout", "\n", "self", ".", "activation_fn", "=", "utils", ".", "get_activation_fn", "(", "\n", "activation", "=", "getattr", "(", "args", ",", "'activation_fn'", ",", "'relu'", ")", "\n", ")", "\n", "self", ".", "activation_dropout", "=", "getattr", "(", "args", ",", "'activation_dropout'", ",", "0", ")", "\n", "if", "self", ".", "activation_dropout", "==", "0", ":", "\n", "# for backwards compatibility with models that use args.relu_dropout", "\n", "            ", "self", ".", "activation_dropout", "=", "getattr", "(", "args", ",", "'relu_dropout'", ",", "0", ")", "\n", "", "self", ".", "normalize_before", "=", "args", ".", "decoder_normalize_before", "\n", "\n", "# use layerNorm rather than FusedLayerNorm for exporting.", "\n", "# char_inputs can be used to determint this.", "\n", "# TODO  remove this once we update apex with the fix", "\n", "export", "=", "getattr", "(", "args", ",", "'char_inputs'", ",", "False", ")", "\n", "self", ".", "self_attn_layer_norm", "=", "LayerNorm", "(", "self", ".", "embed_dim", ",", "export", "=", "export", ")", "\n", "\n", "if", "no_encoder_attn", ":", "\n", "            ", "self", ".", "encoder_attn", "=", "None", "\n", "self", ".", "encoder_attn_layer_norm", "=", "None", "\n", "", "else", ":", "\n", "            ", "self", ".", "encoder_attn", "=", "MultiheadAttention", "(", "\n", "self", ".", "embed_dim", ",", "\n", "args", ".", "decoder_attention_heads", ",", "\n", "kdim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "None", ")", ",", "\n", "vdim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "None", ")", ",", "\n", "dropout", "=", "args", ".", "attention_dropout", ",", "\n", "encoder_decoder_attention", "=", "True", ",", "\n", ")", "\n", "self", ".", "encoder_attn_layer_norm", "=", "LayerNorm", "(", "self", ".", "embed_dim", ",", "export", "=", "export", ")", "\n", "\n", "", "self", ".", "fc1", "=", "Linear", "(", "self", ".", "embed_dim", ",", "args", ".", "decoder_ffn_embed_dim", ")", "\n", "self", ".", "fc2", "=", "Linear", "(", "args", ".", "decoder_ffn_embed_dim", ",", "self", ".", "embed_dim", ")", "\n", "\n", "self", ".", "final_layer_norm", "=", "LayerNorm", "(", "self", ".", "embed_dim", ",", "export", "=", "export", ")", "\n", "self", ".", "need_attn", "=", "True", "\n", "\n", "self", ".", "onnx_trace", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.TransformerDecoderLayer.prepare_for_onnx_export_": [[183, 185], ["None"], "methods", ["None"], ["", "def", "prepare_for_onnx_export_", "(", "self", ")", ":", "\n", "        ", "self", ".", "onnx_trace", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.TransformerDecoderLayer.forward": [[186, 267], ["transformer_layer.TransformerDecoderLayer.maybe_layer_norm", "transformer_layer.TransformerDecoderLayer.self_attn", "torch.dropout", "torch.dropout", "transformer_layer.TransformerDecoderLayer.maybe_layer_norm", "transformer_layer.TransformerDecoderLayer.maybe_layer_norm", "transformer_layer.TransformerDecoderLayer.activation_fn", "torch.dropout", "torch.dropout", "transformer_layer.TransformerDecoderLayer.fc2", "torch.dropout", "torch.dropout", "transformer_layer.TransformerDecoderLayer.maybe_layer_norm", "transformer_layer.TransformerDecoderLayer.self_attn._set_input_buffer", "transformer_layer.TransformerDecoderLayer.maybe_layer_norm", "transformer_layer.TransformerDecoderLayer.encoder_attn", "torch.dropout", "torch.dropout", "transformer_layer.TransformerDecoderLayer.maybe_layer_norm", "transformer_layer.TransformerDecoderLayer.fc1", "transformer_layer.TransformerDecoderLayer.self_attn._get_input_buffer", "transformer_layer.TransformerDecoderLayer.encoder_attn._set_input_buffer"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.TransformerDecoderLayer.maybe_layer_norm", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.TransformerDecoderLayer.maybe_layer_norm", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.TransformerDecoderLayer.maybe_layer_norm", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.TransformerDecoderLayer.maybe_layer_norm", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.linearized_convolution.LinearizedConvolution._set_input_buffer", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.TransformerDecoderLayer.maybe_layer_norm", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.TransformerDecoderLayer.maybe_layer_norm", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.linearized_convolution.LinearizedConvolution._get_input_buffer", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.linearized_convolution.LinearizedConvolution._set_input_buffer"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "x", ",", "\n", "encoder_out", "=", "None", ",", "\n", "encoder_padding_mask", "=", "None", ",", "\n", "incremental_state", "=", "None", ",", "\n", "prev_self_attn_state", "=", "None", ",", "\n", "prev_attn_state", "=", "None", ",", "\n", "self_attn_mask", "=", "None", ",", "\n", "self_attn_padding_mask", "=", "None", ",", "\n", "head_attention_masks", "=", "None", ",", "\n", "head_positions", "=", "None", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`\n            encoder_padding_mask (ByteTensor): binary ByteTensor of shape\n                `(batch, src_len)` where padding elements are indicated by ``1``.\n\n        Returns:\n            encoded output of shape `(seq_len, batch, embed_dim)`\n        \"\"\"", "\n", "residual", "=", "x", "\n", "x", "=", "self", ".", "maybe_layer_norm", "(", "self", ".", "self_attn_layer_norm", ",", "x", ",", "before", "=", "True", ")", "\n", "if", "prev_self_attn_state", "is", "not", "None", ":", "\n", "            ", "if", "incremental_state", "is", "None", ":", "\n", "                ", "incremental_state", "=", "{", "}", "\n", "", "prev_key", ",", "prev_value", "=", "prev_self_attn_state", "\n", "saved_state", "=", "{", "\"prev_key\"", ":", "prev_key", ",", "\"prev_value\"", ":", "prev_value", "}", "\n", "self", ".", "self_attn", ".", "_set_input_buffer", "(", "incremental_state", ",", "saved_state", ")", "\n", "", "x", ",", "attn", "=", "self", ".", "self_attn", "(", "\n", "query", "=", "x", ",", "\n", "key", "=", "x", ",", "\n", "value", "=", "x", ",", "\n", "key_padding_mask", "=", "self_attn_padding_mask", ",", "\n", "incremental_state", "=", "incremental_state", ",", "\n", "need_weights", "=", "False", ",", "\n", "attn_mask", "=", "self_attn_mask", ",", "\n", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "residual", "+", "x", "\n", "x", "=", "self", ".", "maybe_layer_norm", "(", "self", ".", "self_attn_layer_norm", ",", "x", ",", "after", "=", "True", ")", "\n", "\n", "if", "self", ".", "encoder_attn", "is", "not", "None", ":", "\n", "            ", "residual", "=", "x", "\n", "x", "=", "self", ".", "maybe_layer_norm", "(", "self", ".", "encoder_attn_layer_norm", ",", "x", ",", "before", "=", "True", ")", "\n", "if", "prev_attn_state", "is", "not", "None", ":", "\n", "# Load key and value from a previous layer if existing", "\n", "                ", "if", "incremental_state", "is", "None", ":", "\n", "                    ", "incremental_state", "=", "{", "}", "\n", "", "prev_key", ",", "prev_value", "=", "prev_attn_state", "\n", "saved_state", "=", "{", "\"prev_key\"", ":", "prev_key", ",", "\"prev_value\"", ":", "prev_value", "}", "\n", "self", ".", "encoder_attn", ".", "_set_input_buffer", "(", "incremental_state", ",", "saved_state", ")", "\n", "", "x", ",", "attn", "=", "self", ".", "encoder_attn", "(", "\n", "query", "=", "x", ",", "\n", "key", "=", "encoder_out", ",", "\n", "value", "=", "encoder_out", ",", "\n", "key_padding_mask", "=", "encoder_padding_mask", ",", "\n", "incremental_state", "=", "incremental_state", ",", "\n", "static_kv", "=", "True", ",", "\n", "need_weights", "=", "(", "not", "self", ".", "training", "and", "self", ".", "need_attn", ")", ",", "\n", "head_attention_masks", "=", "head_attention_masks", ",", "\n", "head_positions", "=", "head_positions", "\n", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "residual", "+", "x", "\n", "x", "=", "self", ".", "maybe_layer_norm", "(", "self", ".", "encoder_attn_layer_norm", ",", "x", ",", "after", "=", "True", ")", "\n", "\n", "", "residual", "=", "x", "\n", "x", "=", "self", ".", "maybe_layer_norm", "(", "self", ".", "final_layer_norm", ",", "x", ",", "before", "=", "True", ")", "\n", "x", "=", "self", ".", "activation_fn", "(", "self", ".", "fc1", "(", "x", ")", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "activation_dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "self", ".", "fc2", "(", "x", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "residual", "+", "x", "\n", "x", "=", "self", ".", "maybe_layer_norm", "(", "self", ".", "final_layer_norm", ",", "x", ",", "after", "=", "True", ")", "\n", "if", "self", ".", "onnx_trace", "and", "incremental_state", "is", "not", "None", ":", "\n", "            ", "saved_state", "=", "self", ".", "self_attn", ".", "_get_input_buffer", "(", "incremental_state", ")", "\n", "self_attn_state", "=", "saved_state", "[", "\"prev_key\"", "]", ",", "saved_state", "[", "\"prev_value\"", "]", "\n", "return", "x", ",", "attn", ",", "self_attn_state", "\n", "", "return", "x", ",", "attn", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.TransformerDecoderLayer.maybe_layer_norm": [[268, 274], ["layer_norm"], "methods", ["None"], ["", "def", "maybe_layer_norm", "(", "self", ",", "layer_norm", ",", "x", ",", "before", "=", "False", ",", "after", "=", "False", ")", ":", "\n", "        ", "assert", "before", "^", "after", "\n", "if", "after", "^", "self", ".", "normalize_before", ":", "\n", "            ", "return", "layer_norm", "(", "x", ")", "\n", "", "else", ":", "\n", "            ", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.TransformerDecoderLayer.make_generation_fast_": [[275, 277], ["None"], "methods", ["None"], ["", "", "def", "make_generation_fast_", "(", "self", ",", "need_attn", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "need_attn", "=", "need_attn", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear": [[279, 285], ["torch.Linear", "torch.init.xavier_uniform_", "torch.init.constant_"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear"], ["", "", "def", "Linear", "(", "in_features", ",", "out_features", ",", "bias", "=", "True", ")", ":", "\n", "    ", "m", "=", "nn", ".", "Linear", "(", "in_features", ",", "out_features", ",", "bias", ")", "\n", "nn", ".", "init", ".", "xavier_uniform_", "(", "m", ".", "weight", ")", "\n", "if", "bias", ":", "\n", "        ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0.", ")", "\n", "", "return", "m", "\n", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.gelu.gelu_accurate": [[15, 19], ["hasattr", "math.sqrt", "torch.tanh", "torch.pow"], "function", ["None"], ["def", "gelu_accurate", "(", "x", ")", ":", "\n", "    ", "if", "not", "hasattr", "(", "gelu_accurate", ",", "\"_a\"", ")", ":", "\n", "        ", "gelu_accurate", ".", "_a", "=", "math", ".", "sqrt", "(", "2", "/", "math", ".", "pi", ")", "\n", "", "return", "0.5", "*", "x", "*", "(", "1", "+", "torch", ".", "tanh", "(", "gelu_accurate", ".", "_a", "*", "(", "x", "+", "0.044715", "*", "torch", ".", "pow", "(", "x", ",", "3", ")", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.gelu.gelu": [[21, 26], ["hasattr", "torch.nn.functional.gelu().type_as", "torch.nn.functional.gelu", "torch.erf", "x.float", "math.sqrt"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.gelu.gelu"], ["", "def", "gelu", "(", "x", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "    ", "if", "hasattr", "(", "torch", ".", "nn", ".", "functional", ",", "'gelu'", ")", ":", "\n", "        ", "return", "torch", ".", "nn", ".", "functional", ".", "gelu", "(", "x", ".", "float", "(", ")", ")", ".", "type_as", "(", "x", ")", "\n", "", "else", ":", "\n", "        ", "return", "x", "*", "0.5", "*", "(", "1.0", "+", "torch", ".", "erf", "(", "x", "/", "math", ".", "sqrt", "(", "2.0", ")", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.unfold.unfold1d": [[9, 18], ["x.unsqueeze.size", "torch.pad", "x.unsqueeze.as_strided", "x.unsqueeze.unsqueeze"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad"], ["def", "unfold1d", "(", "x", ",", "kernel_size", ",", "padding_l", ",", "pad_value", "=", "0", ")", ":", "\n", "    ", "'''unfold T x B x C to T x B x C x K'''", "\n", "if", "kernel_size", ">", "1", ":", "\n", "        ", "T", ",", "B", ",", "C", "=", "x", ".", "size", "(", ")", "\n", "x", "=", "F", ".", "pad", "(", "x", ",", "(", "0", ",", "0", ",", "0", ",", "0", ",", "padding_l", ",", "kernel_size", "-", "1", "-", "padding_l", ")", ",", "value", "=", "pad_value", ")", "\n", "x", "=", "x", ".", "as_strided", "(", "(", "T", ",", "B", ",", "C", ",", "kernel_size", ")", ",", "(", "B", "*", "C", ",", "C", ",", "1", ",", "B", "*", "C", ")", ")", "\n", "", "else", ":", "\n", "        ", "x", "=", "x", ".", "unsqueeze", "(", "3", ")", "\n", "", "return", "x", "\n", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.sparse_transformer_sentence_encoder.SparseTransformerSentenceEncoder.__init__": [[17, 84], ["fairseq.modules.TransformerSentenceEncoder.__init__", "torch.ModuleList", "range", "sparse_transformer_sentence_encoder.SparseTransformerSentenceEncoder.__init__.freeze_module_params"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "padding_idx", ":", "int", ",", "\n", "vocab_size", ":", "int", ",", "\n", "num_encoder_layers", ":", "int", "=", "6", ",", "\n", "embedding_dim", ":", "int", "=", "768", ",", "\n", "ffn_embedding_dim", ":", "int", "=", "3072", ",", "\n", "num_attention_heads", ":", "int", "=", "8", ",", "\n", "dropout", ":", "float", "=", "0.1", ",", "\n", "attention_dropout", ":", "float", "=", "0.1", ",", "\n", "activation_dropout", ":", "float", "=", "0.1", ",", "\n", "max_seq_len", ":", "int", "=", "256", ",", "\n", "num_segments", ":", "int", "=", "2", ",", "\n", "use_position_embeddings", ":", "bool", "=", "True", ",", "\n", "offset_positions_by_padding", ":", "bool", "=", "True", ",", "\n", "encoder_normalize_before", ":", "bool", "=", "False", ",", "\n", "apply_bert_init", ":", "bool", "=", "False", ",", "\n", "activation_fn", ":", "str", "=", "\"relu\"", ",", "\n", "learned_pos_embedding", ":", "bool", "=", "True", ",", "\n", "add_bias_kv", ":", "bool", "=", "False", ",", "\n", "add_zero_attn", ":", "bool", "=", "False", ",", "\n", "embed_scale", ":", "float", "=", "None", ",", "\n", "freeze_embeddings", ":", "bool", "=", "False", ",", "\n", "n_trans_layers_to_freeze", ":", "int", "=", "0", ",", "\n", "export", ":", "bool", "=", "False", ",", "\n", "is_bidirectional", ":", "bool", "=", "True", ",", "\n", "stride", ":", "int", "=", "32", ",", "\n", "expressivity", ":", "int", "=", "8", ",", "\n", ")", "->", "None", ":", "\n", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "padding_idx", ",", "vocab_size", ",", "num_encoder_layers", ",", "embedding_dim", ",", "\n", "ffn_embedding_dim", ",", "num_attention_heads", ",", "dropout", ",", "attention_dropout", ",", "\n", "activation_dropout", ",", "max_seq_len", ",", "num_segments", ",", "use_position_embeddings", ",", "\n", "offset_positions_by_padding", ",", "encoder_normalize_before", ",", "apply_bert_init", ",", "\n", "activation_fn", ",", "learned_pos_embedding", ",", "add_bias_kv", ",", "add_zero_attn", ",", "\n", "embed_scale", ",", "freeze_embeddings", ",", "n_trans_layers_to_freeze", ",", "export", "\n", ")", "\n", "\n", "self", ".", "layers", "=", "nn", ".", "ModuleList", "(", "\n", "[", "\n", "SparseTransformerSentenceEncoderLayer", "(", "\n", "embedding_dim", "=", "self", ".", "embedding_dim", ",", "\n", "ffn_embedding_dim", "=", "ffn_embedding_dim", ",", "\n", "num_attention_heads", "=", "num_attention_heads", ",", "\n", "dropout", "=", "self", ".", "dropout", ",", "\n", "attention_dropout", "=", "attention_dropout", ",", "\n", "activation_dropout", "=", "activation_dropout", ",", "\n", "activation_fn", "=", "activation_fn", ",", "\n", "add_bias_kv", "=", "add_bias_kv", ",", "\n", "add_zero_attn", "=", "add_zero_attn", ",", "\n", "export", "=", "export", ",", "\n", "is_bidirectional", "=", "is_bidirectional", ",", "\n", "stride", "=", "stride", ",", "\n", "expressivity", "=", "expressivity", ",", "\n", ")", "\n", "for", "_", "in", "range", "(", "num_encoder_layers", ")", "\n", "]", "\n", ")", "\n", "\n", "def", "freeze_module_params", "(", "m", ")", ":", "\n", "            ", "if", "m", "is", "not", "None", ":", "\n", "                ", "for", "p", "in", "m", ".", "parameters", "(", ")", ":", "\n", "                    ", "p", ".", "requires_grad", "=", "False", "\n", "\n", "", "", "", "for", "layer", "in", "range", "(", "n_trans_layers_to_freeze", ")", ":", "\n", "            ", "freeze_module_params", "(", "self", ".", "layers", "[", "layer", "]", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.masked_lm_dictionary.MaskedLMDictionary.__init__": [[14, 25], ["fairseq.data.Dictionary.__init__", "masked_lm_dictionary.MaskedLMDictionary.add_symbol", "len"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol"], ["def", "__init__", "(", "\n", "self", ",", "\n", "pad", "=", "'<pad>'", ",", "\n", "eos", "=", "'</s>'", ",", "\n", "unk", "=", "'<unk>'", ",", "\n", "mask", "=", "'<mask>'", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "pad", ",", "eos", ",", "unk", ")", "\n", "self", ".", "mask_word", "=", "mask", "\n", "self", ".", "mask_index", "=", "self", ".", "add_symbol", "(", "mask", ")", "\n", "self", ".", "nspecial", "=", "len", "(", "self", ".", "symbols", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.masked_lm_dictionary.MaskedLMDictionary.mask": [[26, 29], ["None"], "methods", ["None"], ["", "def", "mask", "(", "self", ")", ":", "\n", "        ", "\"\"\"Helper to get index of mask symbol\"\"\"", "\n", "return", "self", ".", "mask_index", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.masked_lm_dictionary.BertDictionary.__init__": [[36, 51], ["masked_lm_dictionary.MaskedLMDictionary.__init__", "masked_lm_dictionary.BertDictionary.add_symbol", "masked_lm_dictionary.BertDictionary.add_symbol", "len"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.add_symbol"], ["def", "__init__", "(", "\n", "self", ",", "\n", "pad", "=", "'<pad>'", ",", "\n", "eos", "=", "'</s>'", ",", "\n", "unk", "=", "'<unk>'", ",", "\n", "mask", "=", "'<mask>'", ",", "\n", "cls", "=", "'<cls>'", ",", "\n", "sep", "=", "'<sep>'", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "pad", ",", "eos", ",", "unk", ",", "mask", ")", "\n", "self", ".", "cls_word", "=", "cls", "\n", "self", ".", "sep_word", "=", "sep", "\n", "self", ".", "cls_index", "=", "self", ".", "add_symbol", "(", "cls", ")", "\n", "self", ".", "sep_index", "=", "self", ".", "add_symbol", "(", "sep", ")", "\n", "self", ".", "nspecial", "=", "len", "(", "self", ".", "symbols", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.masked_lm_dictionary.BertDictionary.cls": [[52, 55], ["None"], "methods", ["None"], ["", "def", "cls", "(", "self", ")", ":", "\n", "        ", "\"\"\"Helper to get index of cls symbol\"\"\"", "\n", "return", "self", ".", "cls_index", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.masked_lm_dictionary.BertDictionary.sep": [[56, 59], ["None"], "methods", ["None"], ["", "def", "sep", "(", "self", ")", ":", "\n", "        ", "\"\"\"Helper to get index of sep symbol\"\"\"", "\n", "return", "self", ".", "sep_index", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.masked_lm_dataset.MaskedLMDataset.__init__": [[57, 101], ["numpy.array", "isinstance", "isinstance", "isinstance"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "\n", "dataset", ":", "FairseqDataset", ",", "\n", "sizes", ":", "np", ".", "ndarray", ",", "\n", "vocab", ":", "Dictionary", ",", "\n", "pad_idx", ":", "int", ",", "\n", "mask_idx", ":", "int", ",", "\n", "classif_token_idx", ":", "int", ",", "\n", "sep_token_idx", ":", "int", ",", "\n", "seed", ":", "int", "=", "1", ",", "\n", "shuffle", ":", "bool", "=", "True", ",", "\n", "has_pairs", ":", "bool", "=", "True", ",", "\n", "segment_id", ":", "int", "=", "0", ",", "\n", "masking_ratio", ":", "float", "=", "0.15", ",", "\n", "masking_prob", ":", "float", "=", "0.8", ",", "\n", "random_token_prob", ":", "float", "=", "0.1", "\n", ")", ":", "\n", "# Make sure the input datasets are the ones supported", "\n", "        ", "assert", "(", "\n", "isinstance", "(", "dataset", ",", "TokenBlockDataset", ")", "or", "\n", "isinstance", "(", "dataset", ",", "BlockPairDataset", ")", "or", "\n", "isinstance", "(", "dataset", ",", "ConcatDataset", ")", "\n", ")", ",", "\"MaskedLMDataset only wraps TokenBlockDataset or BlockPairDataset or \"", "\"ConcatDataset\"", "\n", "\n", "self", ".", "dataset", "=", "dataset", "\n", "self", ".", "sizes", "=", "np", ".", "array", "(", "sizes", ")", "\n", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "pad_idx", "=", "pad_idx", "\n", "self", ".", "mask_idx", "=", "mask_idx", "\n", "self", ".", "classif_token_idx", "=", "classif_token_idx", "\n", "self", ".", "sep_token_idx", "=", "sep_token_idx", "\n", "self", ".", "shuffle", "=", "shuffle", "\n", "self", ".", "seed", "=", "seed", "\n", "self", ".", "has_pairs", "=", "has_pairs", "\n", "self", ".", "segment_id", "=", "segment_id", "\n", "self", ".", "masking_ratio", "=", "masking_ratio", "\n", "self", ".", "masking_prob", "=", "masking_prob", "\n", "self", ".", "random_token_prob", "=", "random_token_prob", "\n", "\n", "# If we have only one block then sizes needs to be updated to include", "\n", "# the classification token", "\n", "if", "not", "has_pairs", ":", "\n", "            ", "self", ".", "sizes", "=", "self", ".", "sizes", "+", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.masked_lm_dataset.MaskedLMDataset.__getitem__": [[102, 117], ["None"], "methods", ["None"], ["", "", "def", "__getitem__", "(", "\n", "self", ",", "\n", "index", ":", "int", "\n", ")", ":", "\n", "# if has_pairs, then expect 2 blocks and a sentence target", "\n", "        ", "if", "self", ".", "has_pairs", ":", "\n", "            ", "(", "block_one", ",", "block_two", ",", "sentence_target", ")", "=", "self", ".", "dataset", "[", "index", "]", "\n", "", "else", ":", "\n", "            ", "block_one", "=", "self", ".", "dataset", "[", "index", "]", "\n", "\n", "", "return", "{", "\n", "\"id\"", ":", "index", ",", "\n", "\"block_one\"", ":", "block_one", ",", "\n", "\"block_two\"", ":", "block_two", "if", "self", ".", "has_pairs", "else", "None", ",", "\n", "\"sentence_target\"", ":", "sentence_target", "if", "self", ".", "has_pairs", "else", "None", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.masked_lm_dataset.MaskedLMDataset.__len__": [[119, 121], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "dataset", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.masked_lm_dataset.MaskedLMDataset._mask_block": [[122, 178], ["numpy.copy", "len", "math.ceil", "numpy.random.choice", "numpy.copy", "range", "numpy.random.random", "numpy.random.randint"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.copy", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.copy"], ["", "def", "_mask_block", "(", "\n", "self", ",", "\n", "sentence", ":", "np", ".", "ndarray", ",", "\n", "mask_idx", ":", "int", ",", "\n", "pad_idx", ":", "int", ",", "\n", "dictionary_token_range", ":", "Tuple", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Mask tokens for Masked Language Model training\n        Samples mask_ratio tokens that will be predicted by LM.\n\n        Note:This function may not be efficient enough since we had multiple\n        conversions between np and torch, we can replace them with torch\n        operators later.\n\n        Args:\n            sentence: 1d tensor to be masked\n            mask_idx: index to use for masking the sentence\n            pad_idx: index to use for masking the target for tokens we aren't\n                predicting\n            dictionary_token_range: range of indices in dictionary which can\n                be used for random word replacement\n                (e.g. without special characters)\n        Return:\n            masked_sent: masked sentence\n            target: target with words which we are not predicting replaced\n                by pad_idx\n        \"\"\"", "\n", "masked_sent", "=", "np", ".", "copy", "(", "sentence", ")", "\n", "sent_length", "=", "len", "(", "sentence", ")", "\n", "mask_num", "=", "math", ".", "ceil", "(", "sent_length", "*", "self", ".", "masking_ratio", ")", "\n", "mask", "=", "np", ".", "random", ".", "choice", "(", "sent_length", ",", "mask_num", ",", "replace", "=", "False", ")", "\n", "target", "=", "np", ".", "copy", "(", "sentence", ")", "\n", "\n", "for", "i", "in", "range", "(", "sent_length", ")", ":", "\n", "            ", "if", "i", "in", "mask", ":", "\n", "                ", "rand", "=", "np", ".", "random", ".", "random", "(", ")", "\n", "\n", "# replace with mask if probability is less than masking_prob", "\n", "# (Eg: 0.8)", "\n", "if", "rand", "<", "self", ".", "masking_prob", ":", "\n", "                    ", "masked_sent", "[", "i", "]", "=", "mask_idx", "\n", "\n", "# replace with random token if probability is less than", "\n", "# masking_prob + random_token_prob (Eg: 0.9)", "\n", "", "elif", "rand", "<", "(", "self", ".", "masking_prob", "+", "self", ".", "random_token_prob", ")", ":", "\n", "# sample random token from dictionary", "\n", "                    ", "masked_sent", "[", "i", "]", "=", "(", "\n", "np", ".", "random", ".", "randint", "(", "\n", "dictionary_token_range", "[", "0", "]", ",", "dictionary_token_range", "[", "1", "]", "\n", ")", "\n", ")", "\n", "", "", "else", ":", "\n", "                ", "target", "[", "i", "]", "=", "pad_idx", "\n", "\n", "", "", "return", "masked_sent", ",", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.masked_lm_dataset.MaskedLMDataset._collate": [[179, 269], ["len", "fairseq.data.data_utils.numpy_seed", "fairseq.data.data_utils.collate_tokens", "torch.LongTensor", "sum", "masked_lm_dataset.MaskedLMDataset._collate.merge"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.data_utils.numpy_seed", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.data_utils.collate_tokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.translation_moe.score.merge"], ["", "def", "_collate", "(", "\n", "self", ",", "\n", "samples", ":", "List", "[", "Dict", "]", ",", "\n", "pad_idx", ":", "int", ",", "\n", "eos_idx", ":", "int", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Does the heavy lifting for creating a batch from the input list of\n        examples. The logic is as follows:\n            1. Mask the input blocks. In case has_pair is True then we have 2\n               blocks to mask.\n            2. Prepend the first masked block tensor with the special token\n               used as sentence embedding. Eg: CLS in BERT. This happens\n               irrespective of the value of has_pair.\n            3. If has_pair is True, then append the first masked block with the\n               special separator token (eg: SEP for BERT) and compute segment\n               label accordingly. In this case, also append the second masked\n               block with this special separator token and compute its segment\n               label.\n            4. For the targets tensor, prepend and append with padding index\n               accordingly.\n            5. Concatenate all tensors.\n        \"\"\"", "\n", "if", "len", "(", "samples", ")", "==", "0", ":", "\n", "            ", "return", "{", "}", "\n", "# To ensure determinism, we reset the state of the PRNG after every", "\n", "# batch based on the seed and the first id of the batch. This ensures", "\n", "# that across epochs we get the same mask for the same example. This", "\n", "# is needed for reproducibility and is how BERT does masking", "\n", "# TODO: Can we add deteminism without this constraint?", "\n", "", "with", "data_utils", ".", "numpy_seed", "(", "self", ".", "seed", "+", "samples", "[", "0", "]", "[", "\"id\"", "]", ")", ":", "\n", "            ", "for", "s", "in", "samples", ":", "\n", "\n", "# token range is needed for replacing with random token during", "\n", "# masking", "\n", "                ", "token_range", "=", "(", "self", ".", "vocab", ".", "nspecial", ",", "len", "(", "self", ".", "vocab", ")", ")", "\n", "\n", "# mask according to specified probabilities.", "\n", "masked_blk_one", ",", "masked_tgt_one", "=", "self", ".", "_mask_block", "(", "\n", "s", "[", "\"block_one\"", "]", ",", "self", ".", "mask_idx", ",", "self", ".", "pad_idx", ",", "token_range", ",", "\n", ")", "\n", "\n", "tokens", "=", "np", ".", "concatenate", "(", "[", "\n", "[", "self", ".", "classif_token_idx", "]", ",", "masked_blk_one", "\n", "]", ")", "\n", "targets", "=", "np", ".", "concatenate", "(", "[", "[", "self", ".", "pad_idx", "]", ",", "masked_tgt_one", "]", ")", "\n", "segments", "=", "np", ".", "ones", "(", "len", "(", "tokens", ")", ")", "*", "self", ".", "segment_id", "\n", "\n", "# if has_pairs is True then we need to add the SEP token to both", "\n", "# the blocks after masking and re-compute segments based on the new", "\n", "# lengths.", "\n", "if", "self", ".", "has_pairs", ":", "\n", "                    ", "tokens_one", "=", "np", ".", "concatenate", "(", "[", "tokens", ",", "[", "self", ".", "sep_token_idx", "]", "]", ")", "\n", "targets_one", "=", "np", ".", "concatenate", "(", "[", "targets", ",", "[", "self", ".", "pad_idx", "]", "]", ")", "\n", "\n", "masked_blk_two", ",", "masked_tgt_two", "=", "self", ".", "_mask_block", "(", "\n", "s", "[", "\"block_two\"", "]", ",", "self", ".", "mask_idx", ",", "self", ".", "pad_idx", ",", "token_range", ")", "\n", "tokens_two", "=", "np", ".", "concatenate", "(", "\n", "[", "masked_blk_two", ",", "[", "self", ".", "sep_token_idx", "]", "]", ")", "\n", "targets_two", "=", "np", ".", "concatenate", "(", "[", "masked_tgt_two", ",", "[", "self", ".", "pad_idx", "]", "]", ")", "\n", "\n", "# block + 1 sep + 1 special (CLS)", "\n", "segments_one", "=", "np", ".", "zeros", "(", "len", "(", "tokens_one", ")", ")", "\n", "# block + 1 sep", "\n", "segments_two", "=", "np", ".", "ones", "(", "len", "(", "tokens_two", ")", ")", "\n", "\n", "tokens", "=", "np", ".", "concatenate", "(", "[", "tokens_one", ",", "tokens_two", "]", ")", "\n", "targets", "=", "np", ".", "concatenate", "(", "[", "targets_one", ",", "targets_two", "]", ")", "\n", "segments", "=", "np", ".", "concatenate", "(", "[", "segments_one", ",", "segments_two", "]", ")", "\n", "\n", "", "s", "[", "\"source\"", "]", "=", "torch", ".", "LongTensor", "(", "tokens", ")", "\n", "s", "[", "\"segment_labels\"", "]", "=", "torch", ".", "LongTensor", "(", "segments", ")", "\n", "s", "[", "\"lm_target\"", "]", "=", "torch", ".", "LongTensor", "(", "targets", ")", "\n", "\n", "", "", "def", "merge", "(", "key", ")", ":", "\n", "            ", "return", "data_utils", ".", "collate_tokens", "(", "\n", "[", "s", "[", "key", "]", "for", "s", "in", "samples", "]", ",", "pad_idx", ",", "eos_idx", ",", "left_pad", "=", "False", "\n", ")", "\n", "", "return", "{", "\n", "\"id\"", ":", "torch", ".", "LongTensor", "(", "[", "s", "[", "\"id\"", "]", "for", "s", "in", "samples", "]", ")", ",", "\n", "\"ntokens\"", ":", "sum", "(", "len", "(", "s", "[", "\"source\"", "]", ")", "for", "s", "in", "samples", ")", ",", "\n", "\"net_input\"", ":", "{", "\n", "\"src_tokens\"", ":", "merge", "(", "\"source\"", ")", ",", "\n", "\"segment_labels\"", ":", "merge", "(", "\"segment_labels\"", ")", ",", "\n", "}", ",", "\n", "\"lm_target\"", ":", "merge", "(", "\"lm_target\"", ")", ",", "\n", "\"sentence_target\"", ":", "torch", ".", "LongTensor", "(", "\n", "[", "s", "[", "\"sentence_target\"", "]", "for", "s", "in", "samples", "]", "\n", ")", "if", "self", ".", "has_pairs", "else", "None", ",", "\n", "\"nsentences\"", ":", "len", "(", "samples", ")", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.masked_lm_dataset.MaskedLMDataset.collater": [[271, 284], ["masked_lm_dataset.MaskedLMDataset._collate", "masked_lm_dataset.MaskedLMDataset.vocab.pad", "masked_lm_dataset.MaskedLMDataset.vocab.eos"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.masked_lm_dataset.MaskedLMDataset._collate", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos"], ["", "def", "collater", "(", "\n", "self", ",", "\n", "samples", ":", "List", "[", "Dict", "]", "\n", ")", ":", "\n", "        ", "\"\"\"Merge a list of samples to form a mini-batch.\n\n        Args:\n            samples (List[dict]): samples to collate\n\n        Returns:\n            dict: a mini-batch of data\n        \"\"\"", "\n", "return", "self", ".", "_collate", "(", "samples", ",", "self", ".", "vocab", ".", "pad", "(", ")", ",", "self", ".", "vocab", ".", "eos", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.masked_lm_dataset.MaskedLMDataset.num_tokens": [[285, 294], ["None"], "methods", ["None"], ["", "def", "num_tokens", "(", "\n", "self", ",", "\n", "index", ":", "int", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Return the number of tokens in a sample. This value is used to\n        enforce max-tokens during batching.\n        \"\"\"", "\n", "return", "self", ".", "sizes", "[", "index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.masked_lm_dataset.MaskedLMDataset.size": [[295, 304], ["None"], "methods", ["None"], ["", "def", "size", "(", "\n", "self", ",", "\n", "index", ":", "int", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Return an example's size as a float or tuple. This value is used when\n        filtering a dataset with max-positions.\n        \"\"\"", "\n", "return", "self", ".", "sizes", "[", "index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.masked_lm_dataset.MaskedLMDataset.ordered_indices": [[305, 316], ["numpy.random.permutation", "order.append", "numpy.lexsort", "len", "numpy.arange", "len"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "ordered_indices", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Return an ordered list of indices. Batches will be constructed based\n        on this order.\n        \"\"\"", "\n", "if", "self", ".", "shuffle", ":", "\n", "            ", "return", "np", ".", "random", ".", "permutation", "(", "len", "(", "self", ")", ")", "\n", "", "else", ":", "\n", "            ", "order", "=", "[", "np", ".", "arange", "(", "len", "(", "self", ")", ")", "]", "\n", "order", ".", "append", "(", "self", ".", "sizes", ")", "\n", "return", "np", ".", "lexsort", "(", "order", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.masked_lm_dataset.MaskedLMDataset.supports_prefetch": [[317, 320], ["getattr"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "supports_prefetch", "(", "self", ")", ":", "\n", "        ", "return", "getattr", "(", "self", ".", "dataset", ",", "\"supports_prefetch\"", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.masked_lm_dataset.MaskedLMDataset.prefetch": [[321, 323], ["masked_lm_dataset.MaskedLMDataset.dataset.prefetch"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.block_pair_dataset.BlockPairDataset.prefetch"], ["", "def", "prefetch", "(", "self", ",", "indices", ")", ":", "\n", "        ", "self", ".", "dataset", ".", "prefetch", "(", "indices", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.block_pair_dataset.BlockPairDataset.__init__": [[37, 100], ["fairseq.data.FairseqDataset.__init__", "dictionary.pad", "dictionary.eos", "dictionary.cls", "dictionary.mask", "dictionary.sep", "len", "len", "enumerate", "enumerate", "block_pair_dataset.BlockPairDataset._generate_sentence_pair", "sum", "math.ceil", "numpy.array", "numpy.array", "block_pair_dataset.BlockPairDataset._sent_to_dataset_index", "block_pair_dataset.BlockPairDataset._pair_sentences", "ValueError", "block_pair_dataset.BlockPairDataset.block_indices.append", "cur_doc.append", "min", "len", "block_pair_dataset.BlockPairDataset.__init__.block_at"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.masked_lm_dictionary.BertDictionary.cls", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.masked_lm_dictionary.MaskedLMDictionary.mask", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.masked_lm_dictionary.BertDictionary.sep", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.block_pair_dataset.BlockPairDataset._generate_sentence_pair", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.block_pair_dataset.BlockPairDataset._sent_to_dataset_index", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.block_pair_dataset.BlockPairDataset._pair_sentences", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["def", "__init__", "(", "\n", "self", ",", "\n", "dataset", ",", "\n", "dictionary", ",", "\n", "sizes", ",", "\n", "block_size", ",", "\n", "break_mode", "=", "\"doc\"", ",", "\n", "short_seq_prob", "=", "0.1", ",", "\n", "doc_break_size", "=", "1", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dataset", "=", "dataset", "\n", "self", ".", "pad", "=", "dictionary", ".", "pad", "(", ")", "\n", "self", ".", "eos", "=", "dictionary", ".", "eos", "(", ")", "\n", "self", ".", "cls", "=", "dictionary", ".", "cls", "(", ")", "\n", "self", ".", "mask", "=", "dictionary", ".", "mask", "(", ")", "\n", "self", ".", "sep", "=", "dictionary", ".", "sep", "(", ")", "\n", "self", ".", "break_mode", "=", "break_mode", "\n", "self", ".", "dictionary", "=", "dictionary", "\n", "self", ".", "short_seq_prob", "=", "short_seq_prob", "\n", "self", ".", "block_indices", "=", "[", "]", "\n", "\n", "assert", "len", "(", "dataset", ")", "==", "len", "(", "sizes", ")", "\n", "\n", "if", "break_mode", "==", "\"doc\"", ":", "\n", "            ", "cur_doc", "=", "[", "]", "\n", "for", "sent_id", ",", "sz", "in", "enumerate", "(", "sizes", ")", ":", "\n", "                ", "assert", "doc_break_size", "==", "0", "or", "sz", "!=", "0", ",", "(", "\n", "\"when doc_break_size is non-zero, we expect documents to be\"", "\n", "\"separated by a blank line with a single eos.\"", "\n", ")", "\n", "# empty line as document separator", "\n", "if", "sz", "==", "doc_break_size", ":", "\n", "                    ", "if", "len", "(", "cur_doc", ")", "==", "0", ":", "\n", "                        ", "continue", "\n", "", "self", ".", "block_indices", ".", "append", "(", "cur_doc", ")", "\n", "cur_doc", "=", "[", "]", "\n", "", "else", ":", "\n", "                    ", "cur_doc", ".", "append", "(", "sent_id", ")", "\n", "", "", "max_num_tokens", "=", "block_size", "-", "3", "# Account for [CLS], [SEP], [SEP]", "\n", "self", ".", "sent_pairs", "=", "[", "]", "\n", "self", ".", "sizes", "=", "[", "]", "\n", "for", "doc_id", ",", "doc", "in", "enumerate", "(", "self", ".", "block_indices", ")", ":", "\n", "                ", "self", ".", "_generate_sentence_pair", "(", "doc", ",", "doc_id", ",", "max_num_tokens", ",", "sizes", ")", "\n", "", "", "elif", "break_mode", "is", "None", "or", "break_mode", "==", "\"none\"", ":", "\n", "# each block should have half of the block size since we are constructing block pair", "\n", "            ", "sent_length", "=", "(", "block_size", "-", "3", ")", "//", "2", "\n", "total_len", "=", "sum", "(", "dataset", ".", "sizes", ")", "\n", "length", "=", "math", ".", "ceil", "(", "total_len", "/", "sent_length", ")", "\n", "\n", "def", "block_at", "(", "i", ")", ":", "\n", "                ", "start", "=", "i", "*", "sent_length", "\n", "end", "=", "min", "(", "start", "+", "sent_length", ",", "total_len", ")", "\n", "return", "(", "start", ",", "end", ")", "\n", "\n", "", "sent_indices", "=", "np", ".", "array", "(", "[", "block_at", "(", "i", ")", "for", "i", "in", "range", "(", "length", ")", "]", ")", "\n", "sent_sizes", "=", "np", ".", "array", "(", "[", "e", "-", "s", "for", "s", ",", "e", "in", "sent_indices", "]", ")", "\n", "dataset_index", "=", "self", ".", "_sent_to_dataset_index", "(", "sent_sizes", ")", "\n", "\n", "# pair sentences", "\n", "self", ".", "_pair_sentences", "(", "dataset_index", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid break_mode: \"", "+", "break_mode", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.block_pair_dataset.BlockPairDataset._pair_sentences": [[101, 123], ["enumerate", "block_pair_dataset.BlockPairDataset.sent_pairs.append", "block_pair_dataset.BlockPairDataset.sizes.append", "numpy.random.rand", "block_pair_dataset.BlockPairDataset._skip_sampling", "len", "len"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.block_pair_dataset.BlockPairDataset._skip_sampling"], ["", "", "def", "_pair_sentences", "(", "self", ",", "dataset_index", ")", ":", "\n", "        ", "\"\"\"\n        Give a list of evenly cut blocks/sentences, pair these sentences with 50%\n        consecutive sentences and 50% random sentences.\n        This is used for none break mode\n        \"\"\"", "\n", "# pair sentences", "\n", "for", "sent_id", ",", "sent", "in", "enumerate", "(", "dataset_index", ")", ":", "\n", "            ", "next_sent_label", "=", "(", "\n", "1", "if", "np", ".", "random", ".", "rand", "(", ")", ">", "0.5", "and", "sent_id", "!=", "len", "(", "dataset_index", ")", "-", "1", "else", "0", "\n", ")", "\n", "if", "next_sent_label", ":", "\n", "                ", "next_sent", "=", "dataset_index", "[", "sent_id", "+", "1", "]", "\n", "", "else", ":", "\n", "                ", "next_sent", "=", "dataset_index", "[", "\n", "self", ".", "_skip_sampling", "(", "len", "(", "dataset_index", ")", ",", "[", "sent_id", ",", "sent_id", "+", "1", "]", ")", "\n", "]", "\n", "", "self", ".", "sent_pairs", ".", "append", "(", "(", "sent", ",", "next_sent", ",", "next_sent_label", ")", ")", "\n", "\n", "# The current blocks don't include the special tokens but the", "\n", "# sizes already account for this", "\n", "self", ".", "sizes", ".", "append", "(", "3", "+", "sent", "[", "3", "]", "+", "next_sent", "[", "3", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.block_pair_dataset.BlockPairDataset._sent_to_dataset_index": [[124, 153], ["dataset_index.append", "len"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "", "def", "_sent_to_dataset_index", "(", "self", ",", "sent_sizes", ")", ":", "\n", "        ", "\"\"\"\n        Build index mapping block indices to the underlying dataset indices\n        \"\"\"", "\n", "dataset_index", "=", "[", "]", "\n", "ds_idx", ",", "ds_remaining", "=", "-", "1", ",", "0", "\n", "for", "to_consume", "in", "sent_sizes", ":", "\n", "            ", "sent_size", "=", "to_consume", "\n", "if", "ds_remaining", "==", "0", ":", "\n", "                ", "ds_idx", "+=", "1", "\n", "ds_remaining", "=", "sent_sizes", "[", "ds_idx", "]", "\n", "", "start_ds_idx", "=", "ds_idx", "\n", "start_offset", "=", "sent_sizes", "[", "ds_idx", "]", "-", "ds_remaining", "\n", "while", "to_consume", ">", "ds_remaining", ":", "\n", "                ", "to_consume", "-=", "ds_remaining", "\n", "ds_idx", "+=", "1", "\n", "ds_remaining", "=", "sent_sizes", "[", "ds_idx", "]", "\n", "", "ds_remaining", "-=", "to_consume", "\n", "dataset_index", ".", "append", "(", "\n", "(", "\n", "start_ds_idx", ",", "# starting index in dataset", "\n", "start_offset", ",", "# starting offset within starting index", "\n", "ds_idx", ",", "# ending index in dataset", "\n", "sent_size", ",", "# sentence length", "\n", ")", "\n", ")", "\n", "", "assert", "ds_remaining", "==", "0", "\n", "assert", "ds_idx", "==", "len", "(", "self", ".", "dataset", ")", "-", "1", "\n", "return", "dataset_index", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.block_pair_dataset.BlockPairDataset._generate_sentence_pair": [[154, 215], ["numpy.random.random", "numpy.random.randint", "len", "current_chunk.append", "sum", "sum", "block_pair_dataset.BlockPairDataset._truncate_sentences", "block_pair_dataset.BlockPairDataset.sent_pairs.append", "block_pair_dataset.BlockPairDataset.sizes.append", "len", "numpy.random.randint", "block_pair_dataset.BlockPairDataset._skip_sampling", "numpy.random.randint", "range", "sum", "len", "len", "len", "len", "sent_b.append", "sum", "len", "len", "numpy.random.rand", "len"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.block_pair_dataset.BlockPairDataset._truncate_sentences", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.block_pair_dataset.BlockPairDataset._skip_sampling", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "_generate_sentence_pair", "(", "self", ",", "doc", ",", "doc_id", ",", "max_num_tokens", ",", "sizes", ")", ":", "\n", "        ", "\"\"\"\n        Go through a single document and genrate sentence paris from it\n        \"\"\"", "\n", "current_chunk", "=", "[", "]", "\n", "current_length", "=", "0", "\n", "curr", "=", "0", "\n", "# To provide more randomness, we decrease target seq length for parts of", "\n", "# samples (10% by default). Note that max_num_tokens is the hard threshold", "\n", "# for batching and will never be changed.", "\n", "target_seq_length", "=", "max_num_tokens", "\n", "if", "np", ".", "random", ".", "random", "(", ")", "<", "self", ".", "short_seq_prob", ":", "\n", "            ", "target_seq_length", "=", "np", ".", "random", ".", "randint", "(", "2", ",", "max_num_tokens", ")", "\n", "# loop through all sentences in document", "\n", "", "while", "curr", "<", "len", "(", "doc", ")", ":", "\n", "            ", "sent_id", "=", "doc", "[", "curr", "]", "\n", "current_chunk", ".", "append", "(", "sent_id", ")", "\n", "current_length", "=", "sum", "(", "sizes", "[", "current_chunk", "]", ")", "\n", "# split chunk and generate pair when exceed target_seq_length or", "\n", "# finish the loop", "\n", "if", "curr", "==", "len", "(", "doc", ")", "-", "1", "or", "current_length", ">=", "target_seq_length", ":", "\n", "# split the chunk into 2 parts", "\n", "                ", "a_end", "=", "1", "\n", "if", "len", "(", "current_chunk", ")", ">", "2", ":", "\n", "                    ", "a_end", "=", "np", ".", "random", ".", "randint", "(", "1", ",", "len", "(", "current_chunk", ")", "-", "1", ")", "\n", "", "sent_a", "=", "current_chunk", "[", ":", "a_end", "]", "\n", "len_a", "=", "sum", "(", "sizes", "[", "sent_a", "]", ")", "\n", "# generate next sentence label, note that if there is only 1 sentence", "\n", "# in current chunk, label is always 0", "\n", "next_sent_label", "=", "(", "\n", "1", "if", "np", ".", "random", ".", "rand", "(", ")", ">", "0.5", "and", "len", "(", "current_chunk", ")", "!=", "1", "else", "0", "\n", ")", "\n", "if", "not", "next_sent_label", ":", "\n", "# if next sentence label is 0, sample sent_b from a random doc", "\n", "                    ", "target_b_length", "=", "target_seq_length", "-", "len_a", "\n", "rand_doc_id", "=", "self", ".", "_skip_sampling", "(", "len", "(", "self", ".", "block_indices", ")", ",", "[", "doc_id", "]", ")", "\n", "random_doc", "=", "self", ".", "block_indices", "[", "rand_doc_id", "]", "\n", "random_start", "=", "np", ".", "random", ".", "randint", "(", "0", ",", "len", "(", "random_doc", ")", ")", "\n", "sent_b", "=", "[", "]", "\n", "len_b", "=", "0", "\n", "for", "j", "in", "range", "(", "random_start", ",", "len", "(", "random_doc", ")", ")", ":", "\n", "                        ", "sent_b", ".", "append", "(", "random_doc", "[", "j", "]", ")", "\n", "len_b", "=", "sum", "(", "sizes", "[", "sent_b", "]", ")", "\n", "if", "len_b", ">=", "target_b_length", ":", "\n", "                            ", "break", "\n", "# return the second part of the chunk since it's not used", "\n", "", "", "num_unused_segments", "=", "len", "(", "current_chunk", ")", "-", "a_end", "\n", "curr", "-=", "num_unused_segments", "\n", "", "else", ":", "\n", "# if next sentence label is 1, use the second part of chunk as sent_B", "\n", "                    ", "sent_b", "=", "current_chunk", "[", "a_end", ":", "]", "\n", "len_b", "=", "sum", "(", "sizes", "[", "sent_b", "]", ")", "\n", "# currently sent_a and sent_B may be longer than max_num_tokens,", "\n", "# truncate them and return block idx and offsets for them", "\n", "", "sent_a", ",", "sent_b", "=", "self", ".", "_truncate_sentences", "(", "\n", "sent_a", ",", "sent_b", ",", "max_num_tokens", "\n", ")", "\n", "self", ".", "sent_pairs", ".", "append", "(", "(", "sent_a", ",", "sent_b", ",", "next_sent_label", ")", ")", "\n", "self", ".", "sizes", ".", "append", "(", "3", "+", "sent_a", "[", "3", "]", "+", "sent_b", "[", "3", "]", ")", "\n", "current_chunk", "=", "[", "]", "\n", "", "curr", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.block_pair_dataset.BlockPairDataset._skip_sampling": [[216, 223], ["numpy.random.randint", "len", "min", "len"], "methods", ["None"], ["", "", "def", "_skip_sampling", "(", "self", ",", "total", ",", "skip_ids", ")", ":", "\n", "        ", "\"\"\"\n        Generate a random integer which is not in skip_ids. Sample range is [0, total)\n        TODO: ids in skip_ids should be consecutive, we can extend it to more generic version later\n        \"\"\"", "\n", "rand_id", "=", "np", ".", "random", ".", "randint", "(", "total", "-", "len", "(", "skip_ids", ")", ")", "\n", "return", "rand_id", "if", "rand_id", "<", "min", "(", "skip_ids", ")", "else", "rand_id", "+", "len", "(", "skip_ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.block_pair_dataset.BlockPairDataset._truncate_sentences": [[224, 258], ["block_pair_dataset.BlockPairDataset._cut_sentence", "block_pair_dataset.BlockPairDataset._cut_sentence", "sum", "sum", "numpy.random.rand", "numpy.random.rand"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.block_pair_dataset.BlockPairDataset._cut_sentence", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.block_pair_dataset.BlockPairDataset._cut_sentence"], ["", "def", "_truncate_sentences", "(", "self", ",", "sent_a", ",", "sent_b", ",", "max_num_tokens", ")", ":", "\n", "        ", "\"\"\"\n        Trancate a pair of sentence to limit total length under max_num_tokens\n        Logics:\n            1. Truncate longer sentence\n            2. Tokens to be truncated could be at the beginning or the end of the sentnce\n        Returns:\n            Truncated sentences represented by dataset idx\n        \"\"\"", "\n", "len_a", ",", "len_b", "=", "sum", "(", "self", ".", "dataset", ".", "sizes", "[", "sent_a", "]", ")", ",", "sum", "(", "self", ".", "dataset", ".", "sizes", "[", "sent_b", "]", ")", "\n", "front_cut_a", "=", "front_cut_b", "=", "end_cut_a", "=", "end_cut_b", "=", "0", "\n", "\n", "while", "True", ":", "\n", "            ", "total_length", "=", "(", "\n", "len_a", "+", "len_b", "-", "front_cut_a", "-", "front_cut_b", "-", "end_cut_a", "-", "end_cut_b", "\n", ")", "\n", "if", "total_length", "<=", "max_num_tokens", ":", "\n", "                ", "break", "\n", "\n", "", "if", "len_a", "-", "front_cut_a", "-", "end_cut_a", ">", "len_b", "-", "front_cut_b", "-", "end_cut_b", ":", "\n", "                ", "if", "np", ".", "random", ".", "rand", "(", ")", "<", "0.5", ":", "\n", "                    ", "front_cut_a", "+=", "1", "\n", "", "else", ":", "\n", "                    ", "end_cut_a", "+=", "1", "\n", "", "", "else", ":", "\n", "                ", "if", "np", ".", "random", ".", "rand", "(", ")", "<", "0.5", ":", "\n", "                    ", "front_cut_b", "+=", "1", "\n", "", "else", ":", "\n", "                    ", "end_cut_b", "+=", "1", "\n", "\n", "# calculate ds indices as well as offsets and return", "\n", "", "", "", "truncated_sent_a", "=", "self", ".", "_cut_sentence", "(", "sent_a", ",", "front_cut_a", ",", "end_cut_a", ")", "\n", "truncated_sent_b", "=", "self", ".", "_cut_sentence", "(", "sent_b", ",", "front_cut_b", ",", "end_cut_b", ")", "\n", "return", "truncated_sent_a", ",", "truncated_sent_b", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.block_pair_dataset.BlockPairDataset._cut_sentence": [[259, 280], ["sum"], "methods", ["None"], ["", "def", "_cut_sentence", "(", "self", ",", "sent", ",", "front_cut", ",", "end_cut", ")", ":", "\n", "        ", "\"\"\"\n        Cut a sentence based on the numbers of tokens to be cut from beginning and end\n        Represent the sentence as dataset idx and return\n        \"\"\"", "\n", "start_ds_idx", ",", "end_ds_idx", ",", "offset", "=", "sent", "[", "0", "]", ",", "sent", "[", "-", "1", "]", ",", "0", "\n", "target_len", "=", "sum", "(", "self", ".", "dataset", ".", "sizes", "[", "sent", "]", ")", "-", "front_cut", "-", "end_cut", "\n", "while", "front_cut", ">", "0", ":", "\n", "            ", "if", "self", ".", "dataset", ".", "sizes", "[", "start_ds_idx", "]", ">", "front_cut", ":", "\n", "                ", "offset", "+=", "front_cut", "\n", "break", "\n", "", "else", ":", "\n", "                ", "front_cut", "-=", "self", ".", "dataset", ".", "sizes", "[", "start_ds_idx", "]", "\n", "start_ds_idx", "+=", "1", "\n", "", "", "while", "end_cut", ">", "0", ":", "\n", "            ", "if", "self", ".", "dataset", ".", "sizes", "[", "end_ds_idx", "]", ">", "end_cut", ":", "\n", "                ", "break", "\n", "", "else", ":", "\n", "                ", "end_cut", "-=", "self", ".", "dataset", ".", "sizes", "[", "end_ds_idx", "]", "\n", "end_ds_idx", "-=", "1", "\n", "", "", "return", "start_ds_idx", ",", "offset", ",", "end_ds_idx", ",", "target_len", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.block_pair_dataset.BlockPairDataset._fetch_block": [[281, 290], ["torch.cat", "range"], "methods", ["None"], ["", "def", "_fetch_block", "(", "self", ",", "start_ds_idx", ",", "offset", ",", "end_ds_idx", ",", "length", ")", ":", "\n", "        ", "\"\"\"\n        Fetch a block of tokens based on its dataset idx\n        \"\"\"", "\n", "buffer", "=", "torch", ".", "cat", "(", "\n", "[", "self", ".", "dataset", "[", "idx", "]", "for", "idx", "in", "range", "(", "start_ds_idx", ",", "end_ds_idx", "+", "1", ")", "]", "\n", ")", "\n", "s", ",", "e", "=", "offset", ",", "offset", "+", "length", "\n", "return", "buffer", "[", "s", ":", "e", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.block_pair_dataset.BlockPairDataset.__getitem__": [[291, 296], ["block_pair_dataset.BlockPairDataset._fetch_block", "block_pair_dataset.BlockPairDataset._fetch_block"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.block_pair_dataset.BlockPairDataset._fetch_block", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.block_pair_dataset.BlockPairDataset._fetch_block"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "block1", ",", "block2", ",", "next_sent_label", "=", "self", ".", "sent_pairs", "[", "index", "]", "\n", "block1", "=", "self", ".", "_fetch_block", "(", "*", "block1", ")", "\n", "block2", "=", "self", ".", "_fetch_block", "(", "*", "block2", ")", "\n", "return", "block1", ",", "block2", ",", "next_sent_label", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.block_pair_dataset.BlockPairDataset.__len__": [[297, 299], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "sizes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.block_pair_dataset.BlockPairDataset.supports_prefetch": [[300, 303], ["getattr"], "methods", ["None"], ["", "@", "property", "\n", "def", "supports_prefetch", "(", "self", ")", ":", "\n", "        ", "return", "getattr", "(", "self", ".", "dataset", ",", "\"supports_prefetch\"", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.block_pair_dataset.BlockPairDataset.prefetch": [[304, 313], ["set", "block_pair_dataset.BlockPairDataset.dataset.prefetch", "range", "range", "set.add", "set.add"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.legacy.block_pair_dataset.BlockPairDataset.prefetch", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.add", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.add"], ["", "def", "prefetch", "(", "self", ",", "indices", ")", ":", "\n", "        ", "prefetch_idx", "=", "set", "(", ")", "\n", "for", "index", "in", "indices", ":", "\n", "            ", "for", "block1", ",", "block2", ",", "_", "in", "[", "self", ".", "sent_pairs", "[", "index", "]", "]", ":", "\n", "                ", "for", "ds_idx", "in", "range", "(", "block1", "[", "0", "]", ",", "block1", "[", "2", "]", "+", "1", ")", ":", "\n", "                    ", "prefetch_idx", ".", "add", "(", "ds_idx", ")", "\n", "", "for", "ds_idx", "in", "range", "(", "block2", "[", "0", "]", ",", "block2", "[", "2", "]", "+", "1", ")", ":", "\n", "                    ", "prefetch_idx", ".", "add", "(", "ds_idx", ")", "\n", "", "", "", "self", ".", "dataset", ".", "prefetch", "(", "prefetch_idx", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.__init__": [[18, 36], ["FairseqDataset.__init__", "open", "f.readline().strip", "line.strip().split", "raw_audio_dataset.RawAudioDataset.fnames.append", "raw_audio_dataset.RawAudioDataset.sizes.append", "f.readline", "len", "int", "line.strip"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["    ", "def", "__init__", "(", "self", ",", "manifest_path", ",", "sample_rate", ",", "max_sample_size", "=", "None", ",", "min_sample_size", "=", "None", ",", "\n", "shuffle", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "sample_rate", "=", "sample_rate", "\n", "self", ".", "fnames", "=", "[", "]", "\n", "self", ".", "sizes", "=", "[", "]", "\n", "self", ".", "max_sample_size", "=", "max_sample_size", "if", "max_sample_size", "is", "not", "None", "else", "sys", ".", "maxsize", "\n", "self", ".", "min_sample_size", "=", "min_sample_size", "if", "min_sample_size", "is", "not", "None", "else", "self", ".", "max_sample_size", "\n", "\n", "with", "open", "(", "manifest_path", ",", "'r'", ")", "as", "f", ":", "\n", "            ", "self", ".", "root_dir", "=", "f", ".", "readline", "(", ")", ".", "strip", "(", ")", "\n", "for", "line", "in", "f", ":", "\n", "                ", "items", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "assert", "len", "(", "items", ")", "==", "2", ",", "line", "\n", "self", ".", "fnames", ".", "append", "(", "items", "[", "0", "]", ")", "\n", "self", ".", "sizes", ".", "append", "(", "int", "(", "items", "[", "1", "]", ")", ")", "\n", "", "", "self", ".", "shuffle", "=", "shuffle", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.__getitem__": [[37, 56], ["os.path.join", "sf.read", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "raw_audio_dataset.RawAudioDataset.dim", "raw_audio_dataset.RawAudioDataset.dim", "raw_audio_dataset.RawAudioDataset.mean", "raw_audio_dataset.RawAudioDataset.resample", "raw_audio_dataset.RawAudioDataset.dim", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.mean", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.resample"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "fname", "=", "os", ".", "path", ".", "join", "(", "self", ".", "root_dir", ",", "self", ".", "fnames", "[", "index", "]", ")", "\n", "import", "soundfile", "as", "sf", "\n", "\n", "wav", ",", "curr_sample_rate", "=", "sf", ".", "read", "(", "fname", ")", "\n", "feats", "=", "torch", ".", "from_numpy", "(", "wav", ")", ".", "float", "(", ")", "\n", "\n", "if", "feats", ".", "dim", "(", ")", "==", "2", ":", "\n", "            ", "feats", "=", "feats", ".", "mean", "(", "-", "1", ")", "\n", "\n", "", "if", "curr_sample_rate", "!=", "self", ".", "sample_rate", ":", "\n", "            ", "factor", "=", "self", ".", "sample_rate", "/", "curr_sample_rate", "\n", "feats", "=", "self", ".", "resample", "(", "feats", ",", "factor", ")", "\n", "\n", "", "assert", "feats", ".", "dim", "(", ")", "==", "1", ",", "feats", ".", "dim", "(", ")", "\n", "\n", "return", "{", "\n", "'id'", ":", "index", ",", "\n", "'source'", ":", "feats", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.resample": [[58, 60], ["torch.interpolate().squeeze", "torch.interpolate().squeeze", "torch.interpolate", "torch.interpolate", "x.view"], "methods", ["None"], ["", "def", "resample", "(", "self", ",", "x", ",", "factor", ")", ":", "\n", "        ", "return", "F", ".", "interpolate", "(", "x", ".", "view", "(", "1", ",", "1", ",", "-", "1", ")", ",", "scale_factor", "=", "factor", ")", ".", "squeeze", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.__len__": [[61, 63], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "fnames", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.collater": [[64, 90], ["min", "sources[].new", "enumerate", "len", "len", "min", "numpy.random.randint", "len", "zip", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "numpy.random.randint"], "methods", ["None"], ["", "def", "collater", "(", "self", ",", "samples", ")", ":", "\n", "        ", "if", "len", "(", "samples", ")", "==", "0", ":", "\n", "            ", "return", "{", "}", "\n", "\n", "", "sources", "=", "[", "s", "[", "'source'", "]", "for", "s", "in", "samples", "]", "\n", "sizes", "=", "[", "len", "(", "s", ")", "for", "s", "in", "sources", "]", "\n", "target_size", "=", "min", "(", "min", "(", "sizes", ")", ",", "self", ".", "max_sample_size", ")", "\n", "\n", "if", "self", ".", "min_sample_size", "<", "target_size", ":", "\n", "            ", "target_size", "=", "np", ".", "random", ".", "randint", "(", "self", ".", "min_sample_size", ",", "target_size", "+", "1", ")", "\n", "\n", "", "collated_sources", "=", "sources", "[", "0", "]", ".", "new", "(", "len", "(", "sources", ")", ",", "target_size", ")", "\n", "for", "i", ",", "(", "source", ",", "size", ")", "in", "enumerate", "(", "zip", "(", "sources", ",", "sizes", ")", ")", ":", "\n", "            ", "diff", "=", "size", "-", "target_size", "\n", "assert", "diff", ">=", "0", "\n", "if", "diff", "==", "0", ":", "\n", "                ", "collated_sources", "[", "i", "]", "=", "source", "\n", "", "else", ":", "\n", "                ", "start", "=", "np", ".", "random", ".", "randint", "(", "0", ",", "diff", "+", "1", ")", "\n", "end", "=", "size", "-", "diff", "+", "start", "\n", "collated_sources", "[", "i", "]", "=", "source", "[", "start", ":", "end", "]", "\n", "\n", "", "", "return", "{", "\n", "'id'", ":", "torch", ".", "LongTensor", "(", "[", "s", "[", "'id'", "]", "for", "s", "in", "samples", "]", ")", ",", "\n", "'net_input'", ":", "{", "\n", "'source'", ":", "collated_sources", ",", "\n", "}", ",", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.get_dummy_batch": [[93, 106], ["raw_audio_dataset.RawAudioDataset.collater", "isinstance", "isinstance", "min", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "range"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.collater"], ["", "def", "get_dummy_batch", "(", "\n", "self", ",", "num_tokens", ",", "max_positions", ",", "src_len", "=", "2048", ",", "tgt_len", "=", "128", ",", "\n", ")", ":", "\n", "        ", "\"\"\"Return a dummy batch with a given number of tokens.\"\"\"", "\n", "if", "isinstance", "(", "max_positions", ",", "float", ")", "or", "isinstance", "(", "max_positions", ",", "int", ")", ":", "\n", "            ", "src_len", "=", "min", "(", "src_len", ",", "max_positions", ")", "\n", "", "bsz", "=", "num_tokens", "//", "src_len", "\n", "return", "self", ".", "collater", "(", "[", "\n", "{", "\n", "'id'", ":", "i", ",", "\n", "'source'", ":", "torch", ".", "rand", "(", "src_len", ")", ",", "\n", "}", "\n", "for", "i", "in", "range", "(", "bsz", ")", "\n", "]", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.num_tokens": [[108, 110], ["raw_audio_dataset.RawAudioDataset.size"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "num_tokens", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "size", "(", "index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size": [[111, 115], ["min"], "methods", ["None"], ["", "def", "size", "(", "self", ",", "index", ")", ":", "\n", "        ", "\"\"\"Return an example's size as a float or tuple. This value is used when\n        filtering a dataset with ``--max-positions``.\"\"\"", "\n", "return", "min", "(", "self", ".", "sizes", "[", "index", "]", ",", "self", ".", "max_sample_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.ordered_indices": [[116, 127], ["order.append", "numpy.lexsort", "numpy.random.permutation", "numpy.arange", "len", "len"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "ordered_indices", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return an ordered list of indices. Batches will be constructed based\n        on this order.\"\"\"", "\n", "\n", "if", "self", ".", "shuffle", ":", "\n", "            ", "order", "=", "[", "np", ".", "random", ".", "permutation", "(", "len", "(", "self", ")", ")", "]", "\n", "", "else", ":", "\n", "            ", "order", "=", "[", "np", ".", "arange", "(", "len", "(", "self", ")", ")", "]", "\n", "\n", "", "order", ".", "append", "(", "self", ".", "sizes", ")", "\n", "return", "np", ".", "lexsort", "(", "order", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.nltk_tokenizer.NLTKTokenizer.__init__": [[12, 18], ["ImportError"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "source_lang", "=", "None", ",", "target_lang", "=", "None", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "from", "nltk", ".", "tokenize", "import", "word_tokenize", "\n", "self", ".", "word_tokenize", "=", "word_tokenize", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "'Please install nltk with: pip install nltk'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.nltk_tokenizer.NLTKTokenizer.encode": [[19, 21], ["nltk_tokenizer.NLTKTokenizer.word_tokenize"], "methods", ["None"], ["", "", "def", "encode", "(", "self", ",", "x", ":", "str", ")", "->", "str", ":", "\n", "        ", "return", "' '", ".", "join", "(", "self", ".", "word_tokenize", "(", "x", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.nltk_tokenizer.NLTKTokenizer.decode": [[22, 24], ["None"], "methods", ["None"], ["", "def", "decode", "(", "self", ",", "x", ":", "str", ")", "->", "str", ":", "\n", "        ", "return", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.gpt2_bpe.GPT2BPE.add_args": [[19, 28], ["parser.add_argument", "parser.add_argument"], "methods", ["None"], ["    ", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "# fmt: off", "\n", "        ", "parser", ".", "add_argument", "(", "'--gpt2-encoder-json'", ",", "type", "=", "str", ",", "\n", "default", "=", "DEFAULT_ENCODER_JSON", ",", "\n", "help", "=", "'path to encoder.json'", ")", "\n", "parser", ".", "add_argument", "(", "'--gpt2-vocab-bpe'", ",", "type", "=", "str", ",", "\n", "default", "=", "DEFAULT_VOCAB_BPE", ",", "\n", "help", "=", "'path to vocab.bpe'", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.gpt2_bpe.GPT2BPE.__init__": [[30, 38], ["fairseq.file_utils.cached_path", "fairseq.file_utils.cached_path", "gpt2_bpe_utils.get_encoder", "getattr", "getattr"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.file_utils.cached_path", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.file_utils.cached_path", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.gpt2_bpe_utils.get_encoder"], ["", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "encoder_json", "=", "file_utils", ".", "cached_path", "(", "\n", "getattr", "(", "args", ",", "'gpt2_encoder_json'", ",", "DEFAULT_ENCODER_JSON", ")", "\n", ")", "\n", "vocab_bpe", "=", "file_utils", ".", "cached_path", "(", "\n", "getattr", "(", "args", ",", "'gpt2_vocab_bpe'", ",", "DEFAULT_VOCAB_BPE", ")", "\n", ")", "\n", "self", ".", "bpe", "=", "get_encoder", "(", "encoder_json", ",", "vocab_bpe", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.gpt2_bpe.GPT2BPE.encode": [[39, 41], ["map", "gpt2_bpe.GPT2BPE.bpe.encode"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.encode"], ["", "def", "encode", "(", "self", ",", "x", ":", "str", ")", "->", "str", ":", "\n", "        ", "return", "' '", ".", "join", "(", "map", "(", "str", ",", "self", ".", "bpe", ".", "encode", "(", "x", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.gpt2_bpe.GPT2BPE.decode": [[42, 44], ["gpt2_bpe.GPT2BPE.bpe.decode", "map", "x.split"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.decode", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["", "def", "decode", "(", "self", ",", "x", ":", "str", ")", "->", "str", ":", "\n", "        ", "return", "self", ".", "bpe", ".", "decode", "(", "map", "(", "int", ",", "x", ".", "split", "(", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.gpt2_bpe.GPT2BPE.is_beginning_of_word": [[45, 47], ["gpt2_bpe.GPT2BPE.decode().startswith", "gpt2_bpe.GPT2BPE.decode"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.decode"], ["", "def", "is_beginning_of_word", "(", "self", ",", "x", ":", "str", ")", "->", "bool", ":", "\n", "        ", "return", "self", ".", "decode", "(", "x", ")", ".", "startswith", "(", "' '", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.subword_nmt_bpe.SubwordNMTBPE.add_args": [[13, 20], ["parser.add_argument", "parser.add_argument"], "methods", ["None"], ["    ", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "# fmt: off", "\n", "        ", "parser", ".", "add_argument", "(", "'--bpe-codes'", ",", "type", "=", "str", ",", "\n", "help", "=", "'path to subword NMT BPE'", ")", "\n", "parser", ".", "add_argument", "(", "'--bpe-separator'", ",", "default", "=", "'@@'", ",", "\n", "help", "=", "'BPE separator'", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.subword_nmt_bpe.SubwordNMTBPE.__init__": [[22, 43], ["fairseq.file_utils.cached_path", "ValueError", "apply_bpe.create_parser", "apply_bpe.create_parser.parse_args", "apply_bpe.BPE", "ImportError"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.file_utils.cached_path"], ["", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "if", "args", ".", "bpe_codes", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "'--bpe-codes is required for --bpe=subword_nmt'", ")", "\n", "", "codes", "=", "file_utils", ".", "cached_path", "(", "args", ".", "bpe_codes", ")", "\n", "try", ":", "\n", "            ", "from", "subword_nmt", "import", "apply_bpe", "\n", "bpe_parser", "=", "apply_bpe", ".", "create_parser", "(", ")", "\n", "bpe_args", "=", "bpe_parser", ".", "parse_args", "(", "[", "\n", "'--codes'", ",", "codes", ",", "\n", "'--separator'", ",", "args", ".", "bpe_separator", ",", "\n", "]", ")", "\n", "self", ".", "bpe", "=", "apply_bpe", ".", "BPE", "(", "\n", "bpe_args", ".", "codes", ",", "\n", "bpe_args", ".", "merges", ",", "\n", "bpe_args", ".", "separator", ",", "\n", "None", ",", "\n", "bpe_args", ".", "glossaries", ",", "\n", ")", "\n", "self", ".", "bpe_symbol", "=", "bpe_args", ".", "separator", "+", "' '", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "'Please install subword_nmt with: pip install subword-nmt'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.subword_nmt_bpe.SubwordNMTBPE.encode": [[44, 46], ["subword_nmt_bpe.SubwordNMTBPE.bpe.process_line"], "methods", ["None"], ["", "", "def", "encode", "(", "self", ",", "x", ":", "str", ")", "->", "str", ":", "\n", "        ", "return", "self", ".", "bpe", ".", "process_line", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.subword_nmt_bpe.SubwordNMTBPE.decode": [[47, 49], ["None"], "methods", ["None"], ["", "def", "decode", "(", "self", ",", "x", ":", "str", ")", "->", "str", ":", "\n", "        ", "return", "(", "x", "+", "' '", ")", ".", "replace", "(", "self", ".", "bpe_symbol", ",", "''", ")", ".", "rstrip", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.gpt2_bpe_utils.Encoder.__init__": [[47, 64], ["gpt2_bpe_utils.bytes_to_unicode", "dict", "gpt2_bpe_utils.Encoder.re.compile", "zip", "gpt2_bpe_utils.Encoder.encoder.items", "gpt2_bpe_utils.Encoder.byte_encoder.items", "range", "ImportError", "len"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.gpt2_bpe_utils.bytes_to_unicode"], ["    ", "def", "__init__", "(", "self", ",", "encoder", ",", "bpe_merges", ",", "errors", "=", "'replace'", ")", ":", "\n", "        ", "self", ".", "encoder", "=", "encoder", "\n", "self", ".", "decoder", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "self", ".", "encoder", ".", "items", "(", ")", "}", "\n", "self", ".", "errors", "=", "errors", "# how to handle errors in decoding", "\n", "self", ".", "byte_encoder", "=", "bytes_to_unicode", "(", ")", "\n", "self", ".", "byte_decoder", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "self", ".", "byte_encoder", ".", "items", "(", ")", "}", "\n", "self", ".", "bpe_ranks", "=", "dict", "(", "zip", "(", "bpe_merges", ",", "range", "(", "len", "(", "bpe_merges", ")", ")", ")", ")", "\n", "self", ".", "cache", "=", "{", "}", "\n", "\n", "try", ":", "\n", "            ", "import", "regex", "as", "re", "\n", "self", ".", "re", "=", "re", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "'Please install regex with: pip install regex'", ")", "\n", "\n", "# Should haved added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions", "\n", "", "self", ".", "pat", "=", "self", ".", "re", ".", "compile", "(", "r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.gpt2_bpe_utils.Encoder.bpe": [[65, 105], ["tuple", "gpt2_bpe_utils.get_pairs", "min", "tuple", "len", "len", "gpt2_bpe_utils.get_pairs", "tuple.index", "tuple.extend", "tuple.append", "tuple.append", "gpt2_bpe_utils.Encoder.bpe_ranks.get", "tuple.extend", "float", "len"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.gpt2_bpe_utils.get_pairs", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.gpt2_bpe_utils.get_pairs", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.index", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "bpe", "(", "self", ",", "token", ")", ":", "\n", "        ", "if", "token", "in", "self", ".", "cache", ":", "\n", "            ", "return", "self", ".", "cache", "[", "token", "]", "\n", "", "word", "=", "tuple", "(", "token", ")", "\n", "pairs", "=", "get_pairs", "(", "word", ")", "\n", "\n", "if", "not", "pairs", ":", "\n", "            ", "return", "token", "\n", "\n", "", "while", "True", ":", "\n", "            ", "bigram", "=", "min", "(", "pairs", ",", "key", "=", "lambda", "pair", ":", "self", ".", "bpe_ranks", ".", "get", "(", "pair", ",", "float", "(", "'inf'", ")", ")", ")", "\n", "if", "bigram", "not", "in", "self", ".", "bpe_ranks", ":", "\n", "                ", "break", "\n", "", "first", ",", "second", "=", "bigram", "\n", "new_word", "=", "[", "]", "\n", "i", "=", "0", "\n", "while", "i", "<", "len", "(", "word", ")", ":", "\n", "                ", "try", ":", "\n", "                    ", "j", "=", "word", ".", "index", "(", "first", ",", "i", ")", "\n", "new_word", ".", "extend", "(", "word", "[", "i", ":", "j", "]", ")", "\n", "i", "=", "j", "\n", "", "except", ":", "\n", "                    ", "new_word", ".", "extend", "(", "word", "[", "i", ":", "]", ")", "\n", "break", "\n", "\n", "", "if", "word", "[", "i", "]", "==", "first", "and", "i", "<", "len", "(", "word", ")", "-", "1", "and", "word", "[", "i", "+", "1", "]", "==", "second", ":", "\n", "                    ", "new_word", ".", "append", "(", "first", "+", "second", ")", "\n", "i", "+=", "2", "\n", "", "else", ":", "\n", "                    ", "new_word", ".", "append", "(", "word", "[", "i", "]", ")", "\n", "i", "+=", "1", "\n", "", "", "new_word", "=", "tuple", "(", "new_word", ")", "\n", "word", "=", "new_word", "\n", "if", "len", "(", "word", ")", "==", "1", ":", "\n", "                ", "break", "\n", "", "else", ":", "\n", "                ", "pairs", "=", "get_pairs", "(", "word", ")", "\n", "", "", "word", "=", "' '", ".", "join", "(", "word", ")", "\n", "self", ".", "cache", "[", "token", "]", "=", "word", "\n", "return", "word", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.gpt2_bpe_utils.Encoder.encode": [[106, 112], ["gpt2_bpe_utils.Encoder.re.findall", "bpe_tokens.extend", "token.encode", "gpt2_bpe_utils.Encoder.bpe().split", "gpt2_bpe_utils.Encoder.bpe"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.encode", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.gpt2_bpe_utils.Encoder.bpe"], ["", "def", "encode", "(", "self", ",", "text", ")", ":", "\n", "        ", "bpe_tokens", "=", "[", "]", "\n", "for", "token", "in", "self", ".", "re", ".", "findall", "(", "self", ".", "pat", ",", "text", ")", ":", "\n", "            ", "token", "=", "''", ".", "join", "(", "self", ".", "byte_encoder", "[", "b", "]", "for", "b", "in", "token", ".", "encode", "(", "'utf-8'", ")", ")", "\n", "bpe_tokens", ".", "extend", "(", "self", ".", "encoder", "[", "bpe_token", "]", "for", "bpe_token", "in", "self", ".", "bpe", "(", "token", ")", ".", "split", "(", "' '", ")", ")", "\n", "", "return", "bpe_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.gpt2_bpe_utils.Encoder.decode": [[113, 117], ["bytearray().decode", "bytearray"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.decode"], ["", "def", "decode", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "text", "=", "''", ".", "join", "(", "[", "self", ".", "decoder", "[", "token", "]", "for", "token", "in", "tokens", "]", ")", "\n", "text", "=", "bytearray", "(", "[", "self", ".", "byte_decoder", "[", "c", "]", "for", "c", "in", "text", "]", ")", ".", "decode", "(", "'utf-8'", ",", "errors", "=", "self", ".", "errors", ")", "\n", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.gpt2_bpe_utils.bytes_to_unicode": [[12, 33], ["functools.lru_cache", "range", "dict", "list", "chr", "zip", "list", "list", "range", "bs.append", "cs.append", "range", "range", "ord", "ord", "ord", "ord", "ord", "ord"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["@", "lru_cache", "(", ")", "\n", "def", "bytes_to_unicode", "(", ")", ":", "\n", "    ", "\"\"\"\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a signficant percentage of your normal, say, 32K bpe vocab.\n    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n    And avoids mapping to whitespace/control characters the bpe code barfs on.\n    \"\"\"", "\n", "bs", "=", "list", "(", "range", "(", "ord", "(", "\"!\"", ")", ",", "ord", "(", "\"~\"", ")", "+", "1", ")", ")", "+", "list", "(", "range", "(", "ord", "(", "\"\u00a1\"", ")", ",", "ord", "(", "\"\u00ac\"", ")", "+", "1", ")", ")", "+", "list", "(", "range", "(", "ord", "(", "\"\u00ae\"", ")", ",", "ord", "(", "\"\u00ff\"", ")", "+", "1", ")", ")", "\n", "cs", "=", "bs", "[", ":", "]", "\n", "n", "=", "0", "\n", "for", "b", "in", "range", "(", "2", "**", "8", ")", ":", "\n", "        ", "if", "b", "not", "in", "bs", ":", "\n", "            ", "bs", ".", "append", "(", "b", ")", "\n", "cs", ".", "append", "(", "2", "**", "8", "+", "n", ")", "\n", "n", "+=", "1", "\n", "", "", "cs", "=", "[", "chr", "(", "n", ")", "for", "n", "in", "cs", "]", "\n", "return", "dict", "(", "zip", "(", "bs", ",", "cs", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.gpt2_bpe_utils.get_pairs": [[34, 44], ["set", "set.add"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.add"], ["", "def", "get_pairs", "(", "word", ")", ":", "\n", "    ", "\"\"\"Return set of symbol pairs in a word.\n    Word is represented as tuple of symbols (symbols being variable-length strings).\n    \"\"\"", "\n", "pairs", "=", "set", "(", ")", "\n", "prev_char", "=", "word", "[", "0", "]", "\n", "for", "char", "in", "word", "[", "1", ":", "]", ":", "\n", "        ", "pairs", ".", "add", "(", "(", "prev_char", ",", "char", ")", ")", "\n", "prev_char", "=", "char", "\n", "", "return", "pairs", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.gpt2_bpe_utils.get_encoder": [[118, 127], ["gpt2_bpe_utils.Encoder", "open", "json.load", "open", "f.read", "tuple", "merge_str.split", "f.read.split"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["", "", "def", "get_encoder", "(", "encoder_json_path", ",", "vocab_bpe_path", ")", ":", "\n", "    ", "with", "open", "(", "encoder_json_path", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "encoder", "=", "json", ".", "load", "(", "f", ")", "\n", "", "with", "open", "(", "vocab_bpe_path", ",", "'r'", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "        ", "bpe_data", "=", "f", ".", "read", "(", ")", "\n", "", "bpe_merges", "=", "[", "tuple", "(", "merge_str", ".", "split", "(", ")", ")", "for", "merge_str", "in", "bpe_data", ".", "split", "(", "'\\n'", ")", "[", "1", ":", "-", "1", "]", "]", "\n", "return", "Encoder", "(", "\n", "encoder", "=", "encoder", ",", "\n", "bpe_merges", "=", "bpe_merges", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.moses_tokenizer.MosesTokenizer.add_args": [[12, 23], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["None"], ["    ", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "# fmt: off", "\n", "        ", "parser", ".", "add_argument", "(", "'--moses-source-lang'", ",", "metavar", "=", "'SRC'", ",", "\n", "help", "=", "'source language'", ")", "\n", "parser", ".", "add_argument", "(", "'--moses-target-lang'", ",", "metavar", "=", "'TARGET'", ",", "\n", "help", "=", "'target language'", ")", "\n", "parser", ".", "add_argument", "(", "'--moses-no-dash-splits'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "\n", "help", "=", "'don\\'t apply dash split rules'", ")", "\n", "parser", ".", "add_argument", "(", "'--moses-no-escape'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "\n", "help", "=", "'don\\'t perform HTML escaping on apostrophy, quotes, etc.'", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.moses_tokenizer.MosesTokenizer.__init__": [[25, 39], ["getattr", "getattr", "getattr", "getattr", "moses_tokenizer.MosesTokenizer", "MosesDetokenizer", "ImportError"], "methods", ["None"], ["", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "self", ".", "args", "=", "args", "\n", "\n", "if", "getattr", "(", "args", ",", "'moses_source_lang'", ",", "None", ")", "is", "None", ":", "\n", "            ", "args", ".", "moses_source_lang", "=", "getattr", "(", "args", ",", "'source_lang'", ",", "'en'", ")", "\n", "", "if", "getattr", "(", "args", ",", "'moses_target_lang'", ",", "None", ")", "is", "None", ":", "\n", "            ", "args", ".", "moses_target_lang", "=", "getattr", "(", "args", ",", "'target_lang'", ",", "'en'", ")", "\n", "\n", "", "try", ":", "\n", "            ", "from", "sacremoses", "import", "MosesTokenizer", ",", "MosesDetokenizer", "\n", "self", ".", "tok", "=", "MosesTokenizer", "(", "args", ".", "moses_source_lang", ")", "\n", "self", ".", "detok", "=", "MosesDetokenizer", "(", "args", ".", "moses_target_lang", ")", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "'Please install Moses tokenizer with: pip install sacremoses'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.moses_tokenizer.MosesTokenizer.encode": [[40, 46], ["moses_tokenizer.MosesTokenizer.tok.tokenize"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.hub_utils.GeneratorHubInterface.tokenize"], ["", "", "def", "encode", "(", "self", ",", "x", ":", "str", ")", "->", "str", ":", "\n", "        ", "return", "self", ".", "tok", ".", "tokenize", "(", "\n", "x", ",", "\n", "aggressive_dash_splits", "=", "(", "not", "self", ".", "args", ".", "moses_no_dash_splits", ")", ",", "\n", "return_str", "=", "True", ",", "\n", "escape", "=", "(", "not", "self", ".", "args", ".", "moses_no_escape", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.moses_tokenizer.MosesTokenizer.decode": [[48, 50], ["moses_tokenizer.MosesTokenizer.detok.detokenize", "x.split"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.hub_utils.GeneratorHubInterface.detokenize", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["", "def", "decode", "(", "self", ",", "x", ":", "str", ")", "->", "str", ":", "\n", "        ", "return", "self", ".", "detok", ".", "detokenize", "(", "x", ".", "split", "(", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.space_tokenizer.SpaceTokenizer.__init__": [[14, 16], ["re.compile"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "source_lang", "=", "None", ",", "target_lang", "=", "None", ")", ":", "\n", "        ", "self", ".", "space_tok", "=", "re", ".", "compile", "(", "r\"\\s+\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.space_tokenizer.SpaceTokenizer.encode": [[17, 19], ["space_tokenizer.SpaceTokenizer.space_tok.sub"], "methods", ["None"], ["", "def", "encode", "(", "self", ",", "x", ":", "str", ")", "->", "str", ":", "\n", "        ", "return", "self", ".", "space_tok", ".", "sub", "(", "' '", ",", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.space_tokenizer.SpaceTokenizer.decode": [[20, 22], ["None"], "methods", ["None"], ["", "def", "decode", "(", "self", ",", "x", ":", "str", ")", "->", "str", ":", "\n", "        ", "return", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.fastbpe.fastBPE.add_args": [[13, 18], ["parser.add_argument"], "methods", ["None"], ["    ", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "# fmt: off", "\n", "        ", "parser", ".", "add_argument", "(", "'--bpe-codes'", ",", "type", "=", "str", ",", "\n", "help", "=", "'path to fastBPE BPE'", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.fastbpe.fastBPE.__init__": [[20, 30], ["fairseq.file_utils.cached_path", "ValueError", "fastBPE.fastBPE", "ImportError"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.file_utils.cached_path"], ["", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "if", "args", ".", "bpe_codes", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "'--bpe-codes is required for --bpe=subword_nmt'", ")", "\n", "", "codes", "=", "file_utils", ".", "cached_path", "(", "args", ".", "bpe_codes", ")", "\n", "try", ":", "\n", "            ", "import", "fastBPE", "\n", "self", ".", "bpe", "=", "fastBPE", ".", "fastBPE", "(", "codes", ")", "\n", "self", ".", "bpe_symbol", "=", "\"@@ \"", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "'Please install fastBPE with: pip install fastBPE'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.fastbpe.fastBPE.encode": [[31, 33], ["fastbpe.fastBPE.bpe.apply"], "methods", ["None"], ["", "", "def", "encode", "(", "self", ",", "x", ":", "str", ")", "->", "str", ":", "\n", "        ", "return", "self", ".", "bpe", ".", "apply", "(", "[", "x", "]", ")", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.fastbpe.fastBPE.decode": [[34, 36], ["None"], "methods", ["None"], ["", "def", "decode", "(", "self", ",", "x", ":", "str", ")", "->", "str", ":", "\n", "        ", "return", "(", "x", "+", "' '", ")", ".", "replace", "(", "self", ".", "bpe_symbol", ",", "''", ")", ".", "rstrip", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.add_args": [[13, 18], ["parser.add_argument"], "methods", ["None"], ["    ", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "# fmt: off", "\n", "        ", "parser", ".", "add_argument", "(", "'--sentencepiece-vocab'", ",", "type", "=", "str", ",", "\n", "help", "=", "'path to sentencepiece vocab'", ")", "\n", "# fmt: on", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.__init__": [[20, 28], ["fairseq.file_utils.cached_path", "spm.SentencePieceProcessor", "sentencepiece_bpe.SentencepieceBPE.sp.Load", "ImportError"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.file_utils.cached_path"], ["", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "vocab", "=", "file_utils", ".", "cached_path", "(", "args", ".", "sentencepiece_vocab", ")", "\n", "try", ":", "\n", "            ", "import", "sentencepiece", "as", "spm", "\n", "self", ".", "sp", "=", "spm", ".", "SentencePieceProcessor", "(", ")", "\n", "self", ".", "sp", ".", "Load", "(", "vocab", ")", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "'Please install sentencepiece with: pip install sentencepiece'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.encode": [[29, 31], ["sentencepiece_bpe.SentencepieceBPE.sp.EncodeAsPieces"], "methods", ["None"], ["", "", "def", "encode", "(", "self", ",", "x", ":", "str", ")", "->", "str", ":", "\n", "        ", "return", "' '", ".", "join", "(", "self", ".", "sp", ".", "EncodeAsPieces", "(", "x", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.decode": [[32, 34], ["x.replace().replace().strip", "x.replace().replace", "x.replace"], "methods", ["None"], ["", "def", "decode", "(", "self", ",", "x", ":", "str", ")", "->", "str", ":", "\n", "        ", "return", "x", ".", "replace", "(", "' '", ",", "''", ")", ".", "replace", "(", "'\\u2581'", ",", "' '", ")", ".", "strip", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.preprocess.merge_files.merge_dir": [[4, 32], ["sorted", "amrs.replace.replace", "amrs.replace.replace", "amrs.replace.replace", "amrs.replace.replace", "os.listdir", "open", "f.write", "print", "filename.startswith", "open", "print", "enumerate", "amrs.replace.append", "amrs.replace.count", "os.path.join", "line.startswith", "amrs.replace.append"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["def", "merge_dir", "(", "dir", ",", "outfile", ")", ":", "\n", "\n", "# collect amrs", "\n", "    ", "amrs", "=", "[", "]", "\n", "for", "filename", "in", "sorted", "(", "os", ".", "listdir", "(", "dir", ")", ")", ":", "\n", "        ", "if", "not", "filename", ".", "startswith", "(", "\"amr\"", ")", ":", "\n", "            ", "continue", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "dir", ",", "filename", ")", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "            ", "print", "(", "filename", ")", "\n", "for", "i", ",", "line", "in", "enumerate", "(", "f", ")", ":", "\n", "                ", "if", "i", "in", "[", "0", ",", "1", "]", ":", "\n", "                    ", "continue", "\n", "", "if", "line", ".", "startswith", "(", "'# ::align'", ")", ":", "\n", "                    ", "continue", "\n", "", "amrs", ".", "append", "(", "line", ")", "\n", "", "amrs", ".", "append", "(", "'\\n'", ")", "\n", "\n", "# normalization", "\n", "", "", "amrs", "=", "''", ".", "join", "(", "amrs", ")", "\n", "amrs", "=", "amrs", ".", "replace", "(", "'\\r'", ",", "''", ")", "\n", "amrs", "=", "amrs", ".", "replace", "(", "'\\n\\n\\n'", ",", "'\\n\\n'", ")", "\n", "amrs", "=", "amrs", ".", "replace", "(", "'\\u0092'", ",", "\"'\"", ")", "\n", "amrs", "=", "amrs", ".", "replace", "(", "'\\u0085'", ",", "\" \"", ")", "\n", "\n", "# write data", "\n", "with", "open", "(", "outfile", ",", "'w+'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "amrs", ")", "\n", "print", "(", "amrs", ".", "count", "(", "'# ::snt'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.preprocess.jamr_2_kevin.merge_dir_kevin": [[6, 47], ["amrs.replace.replace", "amrs.replace.replace", "amrs.replace.replace", "re.sub", "amrs.replace.replace", "zip", "print", "print", "print", "print", "open", "enumerate", "amrs.replace.append", "open", "open", "f.write", "open", "f.write", "file.replace", "file.replace", "file.replace", "amrs.replace.count", "len", "line.startswith", "line.startswith", "amrs.replace.append", "amrs.replace.split", "amr.strip", "tok.startswith", "bad_indices.append", "file.replace", "f.write", "file.replace", "file.replace", "toks.append", "tok.strip", "enumerate", "enumerate", "len", "toks[].strip", "str"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["def", "merge_dir_kevin", "(", "file", ")", ":", "\n", "    ", "amrs", "=", "[", "]", "\n", "toks", "=", "[", "]", "\n", "with", "open", "(", "file", ",", "'r'", ",", "encoding", "=", "'utf8'", ")", "as", "f", ":", "\n", "        ", "for", "i", ",", "line", "in", "enumerate", "(", "f", ")", ":", "\n", "            ", "if", "i", "in", "[", "0", ",", "1", "]", ":", "\n", "                ", "continue", "\n", "", "if", "line", ".", "startswith", "(", "'# ::tok'", ")", ":", "\n", "                ", "toks", ".", "append", "(", "line", "[", "len", "(", "'# ::tok '", ")", ":", "]", ")", "\n", "", "if", "line", ".", "startswith", "(", "'#'", ")", ":", "\n", "                ", "continue", "\n", "", "amrs", ".", "append", "(", "line", ")", "\n", "", "amrs", ".", "append", "(", "'\\n'", ")", "\n", "", "amrs", "=", "''", ".", "join", "(", "amrs", ")", "\n", "amrs", "=", "amrs", ".", "replace", "(", "'\\r'", ",", "''", ")", "\n", "amrs", "=", "amrs", ".", "replace", "(", "'\\n\\n\\n'", ",", "'\\n\\n'", ")", "\n", "amrs", "=", "amrs", ".", "replace", "(", "'\\n\\n'", ",", "'\\n'", ")", "\n", "amrs", "=", "re", ".", "sub", "(", "'\\n[ ]+'", ",", "' '", ",", "amrs", ")", "\n", "amrs", "=", "amrs", ".", "replace", "(", "'  '", ",", "' '", ")", "\n", "amrs", "=", "[", "amr", "+", "'\\n'", "for", "amr", "in", "amrs", ".", "split", "(", "'\\n'", ")", "if", "amr", ".", "strip", "(", ")", "]", "\n", "\n", "# filter out links", "\n", "sent_idx", "=", "0", "\n", "bad_indices", "=", "[", "]", "\n", "for", "amr", ",", "tok", "in", "zip", "(", "amrs", ",", "toks", ")", ":", "\n", "        ", "if", "tok", ".", "startswith", "(", "'< a href ='", ")", "or", "tok", ".", "strip", "(", ")", "==", "'.'", ":", "\n", "            ", "bad_indices", ".", "append", "(", "sent_idx", ")", "\n", "", "sent_idx", "+=", "1", "\n", "", "with", "open", "(", "file", ".", "replace", "(", "'.jamr'", ",", "'.bad_amrs'", ")", ",", "'w+'", ",", "encoding", "=", "'utf8'", ")", "as", "f", ":", "\n", "        ", "for", "sent_idx", "in", "bad_indices", ":", "\n", "            ", "f", ".", "write", "(", "str", "(", "sent_idx", ")", "+", "'\\t'", "+", "toks", "[", "sent_idx", "]", ".", "strip", "(", ")", "+", "'\\t'", "+", "amrs", "[", "sent_idx", "]", ")", "\n", "", "", "amrs", "=", "''", ".", "join", "(", "amr", "for", "i", ",", "amr", "in", "enumerate", "(", "amrs", ")", "if", "i", "not", "in", "bad_indices", ")", "\n", "toks", "=", "''", ".", "join", "(", "tok", "for", "i", ",", "tok", "in", "enumerate", "(", "toks", ")", "if", "i", "not", "in", "bad_indices", ")", "\n", "with", "open", "(", "file", ".", "replace", "(", "'.jamr'", ",", "'.amrs'", ")", ",", "'w+'", ",", "encoding", "=", "'utf8'", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "amrs", ")", "\n", "", "with", "open", "(", "file", ".", "replace", "(", "'.jamr'", ",", "'.sents'", ")", ",", "'w+'", ",", "encoding", "=", "'utf8'", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "toks", ")", "\n", "", "print", "(", "file", ".", "replace", "(", "'.jamr'", ",", "'.amrs'", ")", ")", "\n", "print", "(", "file", ".", "replace", "(", "'.jamr'", ",", "'.sents'", ")", ")", "\n", "print", "(", "file", ".", "replace", "(", "'.jamr'", ",", "'.bad_amrs'", ")", ")", "\n", "print", "(", "amrs", ".", "count", "(", "'\\n'", ")", ",", "'+'", ",", "len", "(", "bad_indices", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.merge_scripts.align_the_rest.align_the_unaligned": [[3, 20], ["range", "len", "abs", "str", "str"], "function", ["None"], ["def", "align_the_unaligned", "(", "unalnd_nodes", ",", "words", ",", "is_alnd_words", ")", ":", "\n", "    ", "ret_alns", "=", "{", "}", "\n", "for", "(", "node", ",", "prev", ")", "in", "unalnd_nodes", ":", "\n", "        ", "aln", "=", "-", "1", "\n", "bestscr", "=", "10000", "\n", "for", "i", "in", "range", "(", "len", "(", "words", ")", ")", ":", "\n", "            ", "if", "not", "is_alnd_words", "[", "i", "]", ":", "\n", "                ", "scr", "=", "abs", "(", "prev", "-", "i", ")", "\n", "if", "scr", "<", "bestscr", ":", "\n", "                    ", "bestscr", "=", "scr", "\n", "aln", "=", "i", "\n", "", "", "", "if", "aln", "!=", "-", "1", ":", "\n", "            ", "ret_alns", "[", "node", "]", "=", "str", "(", "aln", ")", "+", "\"-\"", "+", "str", "(", "aln", "+", "1", ")", "\n", "is_alnd_words", "[", "aln", "]", "=", "True", "\n", "#print(node+\"\\t\"+words[aln])", "\n", "\n", "", "", "return", "ret_alns", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.NoTokenizer.__init__": [[173, 175], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "vocab", ")", ":", "\n", "        ", "self", ".", "vocab", "=", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.NoTokenizer.__call__": [[176, 179], ["spacy.tokens.doc.Doc", "len"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "spaces", "=", "[", "True", "]", "*", "len", "(", "tokens", ")", "\n", "return", "Doc", "(", "self", ".", "vocab", ",", "words", "=", "tokens", ",", "spaces", "=", "spaces", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.__init__": [[237, 308], ["tokens.copy", "list", "list", "set", "state_machine.AMRStateMachine.is_confirmed.add", "range", "state_machine.AMRStateMachine.tokens.append", "reversed", "reversed", "transition_amr_parser.amr.AMR", "enumerate", "len", "print", "print", "state_machine.AMRStateMachine.tokens.append", "state_machine.AMRStateMachine.printStackBuffer", "enumerate", "enumerate"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.copy", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.add", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.printStackBuffer"], ["    ", "def", "__init__", "(", "self", ",", "tokens", ",", "verbose", "=", "False", ",", "add_unaligned", "=", "0", ",", "\n", "actions_by_stack_rules", "=", "None", ",", "amr_graph", "=", "True", ",", "\n", "post_process", "=", "True", ",", "spacy_lemmatizer", "=", "None", ",", "entity_rules", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        TODO: action_list containing list of allowed actions should be\n        mandatory\n        \"\"\"", "\n", "\n", "self", ".", "entity_rules_path", "=", "entity_rules", "\n", "\n", "# word tokens of sentence", "\n", "self", ".", "tokens", "=", "tokens", ".", "copy", "(", ")", "\n", "\n", "# build and store amr graph (needed e.g. for oracle and PENMAN)", "\n", "self", ".", "amr_graph", "=", "amr_graph", "\n", "self", ".", "post_process", "=", "post_process", "\n", "\n", "# spacy lemmatizer", "\n", "self", ".", "spacy_lemmatizer", "=", "spacy_lemmatizer", "\n", "self", ".", "lemmas", "=", "None", "\n", "\n", "# add unaligned", "\n", "if", "add_unaligned", "and", "'<unaligned>'", "not", "in", "self", ".", "tokens", ":", "\n", "            ", "for", "i", "in", "range", "(", "add_unaligned", ")", ":", "\n", "                ", "self", ".", "tokens", ".", "append", "(", "'<unaligned>'", ")", "\n", "# add root", "\n", "", "", "if", "'<ROOT>'", "not", "in", "self", ".", "tokens", ":", "\n", "            ", "self", ".", "tokens", ".", "append", "(", "\"<ROOT>\"", ")", "\n", "# machine is active", "\n", "", "self", ".", "time_step", "=", "0", "\n", "self", ".", "is_closed", "=", "False", "\n", "# init stack, buffer", "\n", "self", ".", "stack", "=", "[", "]", "\n", "self", ".", "buffer", "=", "list", "(", "reversed", "(", "[", "\n", "i", "+", "1", "for", "i", ",", "tok", "in", "enumerate", "(", "self", ".", "tokens", ")", "if", "tok", "!=", "'<unaligned>'", "\n", "]", ")", ")", "\n", "# add root", "\n", "self", ".", "buffer", "[", "0", "]", "=", "-", "1", "\n", "self", ".", "latent", "=", "list", "(", "reversed", "(", "[", "\n", "i", "+", "1", "for", "i", ",", "tok", "in", "enumerate", "(", "self", ".", "tokens", ")", "if", "tok", "==", "'<unaligned>'", "\n", "]", ")", ")", "\n", "\n", "# init amr", "\n", "if", "self", ".", "amr_graph", ":", "\n", "            ", "self", ".", "amr", "=", "AMR", "(", "tokens", "=", "self", ".", "tokens", ")", "\n", "for", "i", ",", "tok", "in", "enumerate", "(", "self", ".", "tokens", ")", ":", "\n", "                ", "if", "tok", "!=", "\"<ROOT>\"", ":", "\n", "                    ", "self", ".", "amr", ".", "nodes", "[", "i", "+", "1", "]", "=", "tok", "\n", "", "", "self", ".", "amr", ".", "nodes", "[", "-", "1", "]", "=", "\"<ROOT>\"", "\n", "\n", "", "self", ".", "new_id", "=", "len", "(", "self", ".", "tokens", ")", "+", "1", "\n", "self", ".", "verbose", "=", "verbose", "\n", "# parser target output", "\n", "self", ".", "actions", "=", "[", "]", "\n", "self", ".", "labels", "=", "[", "]", "\n", "self", ".", "labelsA", "=", "[", "]", "\n", "self", ".", "predicates", "=", "[", "]", "\n", "self", ".", "alignments", "=", "{", "}", "\n", "\n", "# information for oracle", "\n", "self", ".", "merged_tokens", "=", "{", "}", "\n", "self", ".", "entities", "=", "[", "]", "\n", "self", ".", "is_confirmed", "=", "set", "(", ")", "\n", "self", ".", "is_confirmed", ".", "add", "(", "-", "1", ")", "\n", "self", ".", "swapped_words", "=", "{", "}", "\n", "\n", "self", ".", "actions_by_stack_rules", "=", "actions_by_stack_rules", "\n", "\n", "if", "self", ".", "verbose", ":", "\n", "            ", "print", "(", "'INIT'", ")", "\n", "print", "(", "self", ".", "printStackBuffer", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.__deepcopy__": [[309, 328], ["cls.__new__", "state_machine.AMRStateMachine.__dict__.items", "id", "setattr", "setattr", "copy.deepcopy"], "methods", ["None"], ["", "", "def", "__deepcopy__", "(", "self", ",", "memo", ")", ":", "\n", "        ", "\"\"\"\n        Manual deep copy of the machine\n\n        avoid deep copying spacy lemmatizer\n        \"\"\"", "\n", "cls", "=", "self", ".", "__class__", "\n", "result", "=", "cls", ".", "__new__", "(", "cls", ")", "\n", "# DEBUG: usew this to detect very heavy constants that can be refered", "\n", "# import time", "\n", "memo", "[", "id", "(", "self", ")", "]", "=", "result", "\n", "for", "k", ",", "v", "in", "self", ".", "__dict__", ".", "items", "(", ")", ":", "\n", "# start = time.time()", "\n", "            ", "if", "k", "in", "[", "'spacy_lemmatizer'", ",", "'actions_by_stack_rules'", "]", ":", "\n", "                ", "setattr", "(", "result", ",", "k", ",", "v", ")", "\n", "", "else", ":", "\n", "                ", "setattr", "(", "result", ",", "k", ",", "deepcopy", "(", "v", ",", "memo", ")", ")", "\n", "# print(k, time.time() - start)", "\n", "", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.get_buffer_stack_copy": [[329, 332], ["list", "list"], "methods", ["None"], ["", "def", "get_buffer_stack_copy", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return copy of buffer and stack\"\"\"", "\n", "return", "list", "(", "self", ".", "buffer", ")", ",", "list", "(", "self", ".", "stack", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.__str__": [[333, 416], ["range", "len", "str", "len", "mask_view.append", "state_machine.AMRStateMachine.alignments.items", "state_machine.green_font", "state_machine.get_graph_str", "state_machine.green_font", "reversed", "stack_str.append", "stack_str.append", "state_machine.AMRStateMachine.merged_tokens.values", "pointer_view.append", "state_machine.green_font", "isinstance", "node_items.append", "len", "stack_idx.index", "pointer_view.append", "pointer_view.append", "state_machine.green_font", "state_machine.stack_style", "state_machine.stack_style", "len", "stack_idx.index", "state_machine.reduced_style", "len"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.green_font", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.get_graph_str", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.green_font", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.green_font", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.index", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.green_font", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.stack_state_machine.stack_style", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.stack_state_machine.stack_style", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.index", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.reduced_style"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "\"\"\"Command line styling\"\"\"", "\n", "\n", "display_str", "=", "\"\"", "\n", "\n", "# Actions", "\n", "action_str", "=", "' '", ".", "join", "(", "[", "a", "for", "a", "in", "self", ".", "actions", "]", ")", "\n", "# update display str", "\n", "display_str", "+=", "\"%s\\n%s\\n\\n\"", "%", "(", "green_font", "(", "\"# Actions:\"", ")", ",", "action_str", ")", "\n", "\n", "# Buffer", "\n", "buffer_idx", "=", "[", "\n", "i", "-", "1", "if", "i", "!=", "-", "1", "else", "len", "(", "self", ".", "tokens", ")", "-", "1", "\n", "for", "i", "in", "reversed", "(", "self", ".", "buffer", ")", "\n", "]", "\n", "\n", "# Stack", "\n", "stack_idx", "=", "[", "i", "-", "1", "for", "i", "in", "self", ".", "stack", "]", "\n", "stack_str", "=", "[", "]", "\n", "for", "i", "in", "stack_idx", ":", "\n", "            ", "if", "i", "in", "self", ".", "merged_tokens", ":", "\n", "# Take into account merged tokens", "\n", "                ", "stack_str", ".", "append", "(", "\"(\"", "+", "\" \"", ".", "join", "(", "[", "\n", "self", ".", "tokens", "[", "j", "-", "1", "]", "for", "j", "in", "self", ".", "merged_tokens", "[", "i", "]", "\n", "]", ")", "+", "\")\"", ")", "\n", "", "else", ":", "\n", "                ", "stack_str", ".", "append", "(", "self", ".", "tokens", "[", "i", "]", ")", "\n", "", "", "stack_str", "=", "\" \"", ".", "join", "(", "stack_str", ")", "\n", "\n", "merged_pos", "=", "[", "y", "-", "1", "for", "x", "in", "self", ".", "merged_tokens", ".", "values", "(", ")", "for", "y", "in", "x", "]", "\n", "\n", "# mask view", "\n", "mask_view", "=", "[", "]", "\n", "pointer_view", "=", "[", "]", "\n", "for", "position", "in", "range", "(", "len", "(", "self", ".", "tokens", ")", ")", ":", "\n", "# token", "\n", "            ", "token", "=", "str", "(", "self", ".", "tokens", "[", "position", "]", ")", "\n", "len_token", "=", "len", "(", "token", ")", "\n", "# color depending on position", "\n", "if", "position", "in", "buffer_idx", ":", "\n", "                ", "token", "=", "token", "+", "' '", "\n", "", "elif", "position", "in", "stack_idx", ":", "\n", "                ", "token", "=", "stack_style", "(", "token", ",", "position", "+", "1", "in", "self", ".", "is_confirmed", ")", "+", "' '", "\n", "", "elif", "position", "in", "merged_pos", ":", "\n", "                ", "token", "=", "stack_style", "(", "token", "+", "' '", ",", "position", "+", "1", "in", "self", ".", "is_confirmed", ")", "\n", "", "else", ":", "\n", "                ", "token", "=", "reduced_style", "(", "token", ")", "+", "' '", "\n", "# position cursor", "\n", "", "if", "position", "in", "stack_idx", "and", "stack_idx", ".", "index", "(", "position", ")", "==", "len", "(", "stack_idx", ")", "-", "1", ":", "\n", "                ", "pointer_view", ".", "append", "(", "'_'", "*", "len_token", "+", "' '", ")", "\n", "", "elif", "position", "in", "stack_idx", "and", "stack_idx", ".", "index", "(", "position", ")", "==", "len", "(", "stack_idx", ")", "-", "2", ":", "\n", "                ", "pointer_view", ".", "append", "(", "'-'", "*", "len_token", "+", "' '", ")", "\n", "", "else", ":", "\n", "                ", "pointer_view", ".", "append", "(", "' '", "*", "len_token", "+", "' '", ")", "\n", "", "mask_view", ".", "append", "(", "token", ")", "\n", "\n", "", "mask_view_str", "=", "\"\"", ".", "join", "(", "mask_view", ")", "\n", "pointer_view_str", "=", "\"\"", ".", "join", "(", "pointer_view", ")", "\n", "# update display str", "\n", "display_str", "+=", "\"%s\\n%s\\n%s\\n\\n\"", "%", "(", "green_font", "(", "\"# Buffer/Stack/Reduced:\"", ")", ",", "pointer_view_str", ",", "mask_view_str", ")", "\n", "\n", "# nodes (requires on the fly AMR computation)", "\n", "if", "self", ".", "amr_graph", ":", "\n", "\n", "            ", "node_items", "=", "[", "]", "\n", "for", "stack0", ",", "token_pos", "in", "self", ".", "alignments", ".", "items", "(", ")", ":", "\n", "                ", "if", "stack0", "not", "in", "self", ".", "amr", ".", "nodes", ":", "\n", "                    ", "continue", "\n", "", "node", "=", "self", ".", "amr", ".", "nodes", "[", "stack0", "]", "\n", "if", "isinstance", "(", "token_pos", ",", "tuple", ")", ":", "\n", "                    ", "tokens", "=", "\" \"", ".", "join", "(", "self", ".", "tokens", "[", "p", "]", "for", "p", "in", "token_pos", ")", "\n", "", "else", ":", "\n", "                    ", "tokens", "=", "self", ".", "tokens", "[", "token_pos", "]", "\n", "", "node_items", ".", "append", "(", "f'{tokens}--{node}'", ")", "\n", "", "nodes_str", "=", "\"  \"", ".", "join", "(", "node_items", ")", "\n", "# update display str", "\n", "display_str", "+=", "\"%s\\n%s\\n\\n\"", "%", "(", "green_font", "(", "\"# Alignments:\"", ")", ",", "nodes_str", ")", "\n", "\n", "# Graph ", "\n", "display_str", "+=", "green_font", "(", "\"# Graph:\\n\"", ")", "\n", "display_str", "+=", "get_graph_str", "(", "self", ".", "amr", ",", "self", ".", "alignments", ")", "\n", "\n", "", "return", "display_str", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.readAction": [[417, 440], ["action.split", "re.findall"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["", "@", "classmethod", "\n", "def", "readAction", "(", "cls", ",", "action", ")", ":", "\n", "        ", "\"\"\"Read action format\"\"\"", "\n", "if", "'('", "not", "in", "action", ":", "\n", "            ", "return", "action", ",", "None", "\n", "", "elif", "action", "==", "'LA(root)'", ":", "\n", "# To keep original name to keep learner happy", "\n", "            ", "return", "action", ",", "[", "'root'", "]", "\n", "", "else", ":", "\n", "            ", "items", "=", "action", ".", "split", "(", "'('", ")", "\n", "action_label", "=", "items", "[", "0", "]", "\n", "arg_string", "=", "items", "[", "1", "]", "[", ":", "-", "1", "]", "\n", "if", "action_label", "not", "in", "[", "'PRED'", ",", "'CONFIRM'", "]", ":", "\n", "# split by comma respecting quotes", "\n", "                ", "props", "=", "re", ".", "findall", "(", "r'(?:[^\\s,\"]|\"(?:\\\\.|[^\"])*\")+'", ",", "arg_string", ")", "\n", "", "else", ":", "\n", "                ", "props", "=", "[", "arg_string", "]", "\n", "\n", "# To keep original name to keep learner happy", "\n", "", "if", "action_label", "==", "'DEPENDENT'", ":", "\n", "                ", "action_label", "=", "action", "\n", "\n", "", "return", "action_label", ",", "props", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.get_top_of_stack": [[441, 479], ["len", "str", "str", "state_machine.AMRStateMachine.spacy_lemmatizer"], "methods", ["None"], ["", "", "def", "get_top_of_stack", "(", "self", ",", "positions", "=", "False", ",", "lemma", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Returns surface symbols on top of the stack, inclucing merged\n\n        positions=True  returns the positions (unique ids within sentence)\n        \"\"\"", "\n", "# to get the lemma, we will need the positions", "\n", "if", "lemma", ":", "\n", "# Compute lemmas for this sentence and cache it", "\n", "            ", "if", "self", ".", "lemmas", "is", "None", ":", "\n", "                ", "assert", "self", ".", "spacy_lemmatizer", ",", "\"No spacy_lemmatizer provided\"", "\n", "self", ".", "lemmas", "=", "[", "\n", "x", ".", "lemma_", "for", "x", "in", "self", ".", "spacy_lemmatizer", "(", "self", ".", "tokens", "[", ":", "-", "1", "]", ")", "\n", "]", "+", "[", "'ROOT'", "]", "\n", "", "positions", "=", "True", "\n", "", "token", "=", "None", "\n", "merged_tokens", "=", "None", "\n", "if", "len", "(", "self", ".", "stack", ")", ":", "\n", "            ", "stack0", "=", "self", ".", "stack", "[", "-", "1", "]", "\n", "if", "positions", ":", "\n", "                ", "token", "=", "stack0", "-", "1", "\n", "", "else", ":", "\n", "                ", "token", "=", "str", "(", "self", ".", "tokens", "[", "stack0", "-", "1", "]", ")", "\n", "# store merged tokens by separate", "\n", "", "if", "stack0", "in", "self", ".", "merged_tokens", ":", "\n", "                ", "if", "positions", ":", "\n", "                    ", "merged_tokens", "=", "[", "i", "-", "1", "for", "i", "in", "self", ".", "merged_tokens", "[", "stack0", "]", "]", "\n", "", "else", ":", "\n", "                    ", "merged_tokens", "=", "[", "\n", "str", "(", "self", ".", "tokens", "[", "i", "-", "1", "]", ")", "\n", "for", "i", "in", "self", ".", "merged_tokens", "[", "stack0", "]", "\n", "]", "\n", "\n", "", "", "", "if", "lemma", ":", "\n", "            ", "token", "=", "self", ".", "lemmas", "[", "token", "]", "\n", "merged_tokens", "=", "None", "\n", "\n", "", "return", "token", ",", "merged_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.applyAction": [[480, 538], ["state_machine.AMRStateMachine.readAction", "action_label.startswith", "state_machine.AMRStateMachine.SHIFT", "state_machine.AMRStateMachine.CLOSE", "state_machine.AMRStateMachine.REDUCE", "state_machine.AMRStateMachine.LA", "len", "state_machine.AMRStateMachine.RA", "len", "state_machine.AMRStateMachine.CONFIRM", "len", "state_machine.AMRStateMachine.COPY_LEMMA", "state_machine.AMRStateMachine.COPY_SENSE01", "state_machine.AMRStateMachine.SWAP", "state_machine.AMRStateMachine.DUPLICATE", "state_machine.AMRStateMachine.INTRODUCE", "action_label.startswith", "state_machine.AMRStateMachine.DEPENDENT", "state_machine.AMRStateMachine.ENTITY", "state_machine.AMRStateMachine.MERGE", "state_machine.AMRStateMachine.CLOSE", "Exception"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.readAction", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.SHIFT", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.CLOSE", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.REDUCE", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.LA", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.RA", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.CONFIRM", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.COPY_LEMMA", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.COPY_SENSE01", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.SWAP", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.INTRODUCE", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.DEPENDENT", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.ENTITY", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.MERGE", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.CLOSE"], ["", "def", "applyAction", "(", "self", ",", "act", ")", ":", "\n", "\n", "        ", "action_label", ",", "properties", "=", "self", ".", "readAction", "(", "act", ")", "\n", "if", "action_label", ".", "startswith", "(", "'SHIFT'", ")", ":", "\n", "            ", "if", "self", ".", "buffer", ":", "\n", "                ", "self", ".", "SHIFT", "(", "properties", "[", "0", "]", "if", "properties", "else", "None", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "CLOSE", "(", ")", "\n", "return", "True", "\n", "", "", "elif", "action_label", "in", "[", "'REDUCE'", ",", "'REDUCE1'", "]", ":", "\n", "            ", "self", ".", "REDUCE", "(", ")", "\n", "", "elif", "action_label", "in", "[", "'LA(root)'", ",", "'LA'", ",", "'LA1'", "]", ":", "\n", "            ", "assert", "':'", "not", "in", "properties", ",", "\"edge format has no :\"", "\n", "assert", "len", "(", "properties", ")", "==", "1", "\n", "self", ".", "LA", "(", "properties", "[", "0", "]", ")", "\n", "# Also close if LA(root)", "\n", "# FIXME: This breaks stack-LSTM (IndexError: pop from empty list)", "\n", "#             if (", "\n", "#                 properties[0] == 'root' and ", "\n", "#                 self.tokens[self.stack[-1]] == '<ROOT>'", "\n", "#             ):", "\n", "#                 self.CLOSE()", "\n", "", "elif", "action_label", "in", "[", "'RA'", ",", "'RA1'", "]", ":", "\n", "            ", "assert", "':'", "not", "in", "properties", ",", "\"edge format has no :\"", "\n", "assert", "len", "(", "properties", ")", "==", "1", "\n", "self", ".", "RA", "(", "properties", "[", "0", "]", ")", "\n", "", "elif", "action_label", "in", "[", "'PRED'", ",", "'CONFIRM'", "]", ":", "\n", "            ", "assert", "len", "(", "properties", ")", "==", "1", "\n", "self", ".", "CONFIRM", "(", "properties", "[", "0", "]", ")", "\n", "", "elif", "action_label", "in", "[", "'COPY_LEMMA'", "]", ":", "\n", "            ", "self", ".", "COPY_LEMMA", "(", ")", "\n", "", "elif", "action_label", "in", "[", "'COPY_SENSE01'", "]", ":", "\n", "            ", "self", ".", "COPY_SENSE01", "(", ")", "\n", "# TODO: Why multiple keywords for the same action?", "\n", "", "elif", "action_label", "in", "[", "'SWAP'", ",", "'UNSHIFT'", ",", "'UNSHIFT1'", "]", ":", "\n", "            ", "self", ".", "SWAP", "(", ")", "\n", "", "elif", "action_label", "in", "[", "'DUPLICATE'", "]", ":", "\n", "            ", "self", ".", "DUPLICATE", "(", ")", "\n", "", "elif", "action_label", "in", "[", "'INTRODUCE'", "]", ":", "\n", "            ", "self", ".", "INTRODUCE", "(", ")", "\n", "", "elif", "action_label", ".", "startswith", "(", "'DEPENDENT'", ")", ":", "\n", "            ", "self", ".", "DEPENDENT", "(", "*", "properties", ")", "\n", "", "elif", "action_label", "in", "[", "'ADDNODE'", ",", "'ENTITY'", "]", ":", "\n", "# preprocessing", "\n", "            ", "self", ".", "ENTITY", "(", "\",\"", ".", "join", "(", "properties", ")", ")", "\n", "", "elif", "action_label", "in", "[", "'MERGE'", "]", ":", "\n", "            ", "self", ".", "MERGE", "(", ")", "\n", "", "elif", "action_label", "in", "[", "'CLOSE'", "]", ":", "\n", "            ", "self", ".", "CLOSE", "(", ")", "\n", "return", "True", "\n", "", "elif", "act", "==", "'</s>'", ":", "\n", "# Do nothing action. Wait until other machines in the batch finish", "\n", "            ", "pass", "\n", "", "else", ":", "\n", "            ", "raise", "Exception", "(", "f'Unrecognized action: {act}'", ")", "\n", "\n", "# Increase time step", "\n", "", "self", ".", "time_step", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.applyActions": [[539, 545], ["state_machine.AMRStateMachine.CLOSE", "state_machine.AMRStateMachine.applyAction"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.CLOSE", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.DepParsingStateMachine.applyAction"], ["", "def", "applyActions", "(", "self", ",", "actions", ")", ":", "\n", "        ", "for", "action", "in", "actions", ":", "\n", "            ", "is_closed", "=", "self", ".", "applyAction", "(", "action", ")", "\n", "if", "is_closed", ":", "\n", "                ", "return", "\n", "", "", "self", ".", "CLOSE", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.get_pred_by_stack_rules": [[546, 569], ["state_machine.AMRStateMachine.get_top_of_stack", "sorted", "state_machine.AMRStateMachine.actions_by_stack_rules[].items"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.get_top_of_stack"], ["", "def", "get_pred_by_stack_rules", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return valid actions given the stack rules\"\"\"", "\n", "\n", "# rule input", "\n", "token", ",", "merged_tokens", "=", "self", ".", "get_top_of_stack", "(", ")", "\n", "if", "merged_tokens", ":", "\n", "            ", "token", "=", "\",\"", ".", "join", "(", "merged_tokens", ")", "\n", "# merged_token = \",\".join(merged_tokens)", "\n", "# if merged_token in self.actions_by_stack_rules:", "\n", "#    token = merged_token", "\n", "\n", "# rule decision", "\n", "", "if", "token", "not", "in", "self", ".", "actions_by_stack_rules", ":", "\n", "            ", "valid_pred_actions", "=", "[", "]", "\n", "", "else", ":", "\n", "# return nodes ordered by most common", "\n", "            ", "node_counts", "=", "sorted", "(", "\n", "self", ".", "actions_by_stack_rules", "[", "token", "]", ".", "items", "(", ")", ",", "\n", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "\n", "reverse", "=", "True", "\n", ")", "\n", "valid_pred_actions", "=", "[", "f'PRED({nc[0]})'", "for", "nc", "in", "node_counts", "]", "\n", "", "return", "valid_pred_actions", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.get_valid_actions": [[570, 653], ["invalid_actions.extend", "valid_actions.append", "len", "valid_actions.extend", "len", "valid_actions.append", "state_machine.get_forbidden_actions", "valid_actions.extend", "valid_actions.extend", "len", "valid_actions.append", "valid_actions.extend", "valid_actions.extend", "state_machine.AMRStateMachine.get_pred_by_stack_rules", "valid_actions.append", "valid_actions.extend", "valid_actions.extend", "state_machine.AMRStateMachine.swapped_words.get", "state_machine.AMRStateMachine.swapped_words.get", "state_machine.AMRStateMachine.get_top_of_stack"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.get_forbidden_actions", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.get_pred_by_stack_rules", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.get_top_of_stack"], ["", "def", "get_valid_actions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return valid actions for this state at test time\"\"\"", "\n", "\n", "# Quick exit for a closed machine", "\n", "if", "self", ".", "is_closed", ":", "\n", "            ", "return", "[", "'</s>'", "]", ",", "[", "]", "\n", "\n", "# NOTE: Example: valid_actions = ['LA'] invalid_actions = ['LA(:mod)']", "\n", "", "valid_actions", "=", "[", "]", "\n", "invalid_actions", "=", "[", "]", "\n", "\n", "# Buffer not empty", "\n", "if", "True", ":", "# len(self.buffer):", "\n", "# Its admits a SHIFT for empty buffer interpreted as a close", "\n", "            ", "valid_actions", ".", "append", "(", "'SHIFT'", ")", "\n", "# FIXME: reduce also accepted here if node_id != None and something", "\n", "# aligned to it (see tryReduce)", "\n", "\n", "# One or more tokens in stack", "\n", "", "if", "len", "(", "self", ".", "stack", ")", ">", "0", ":", "\n", "\n", "            ", "stack0", "=", "self", ".", "stack", "[", "-", "1", "]", "\n", "\n", "# If not confirmed yet, it can be confirmed", "\n", "if", "stack0", "not", "in", "self", ".", "is_confirmed", ":", "\n", "                ", "valid_actions", ".", "extend", "(", "[", "'COPY_LEMMA'", ",", "'COPY_SENSE01'", "]", ")", "\n", "if", "self", ".", "actions_by_stack_rules", ":", "\n", "                    ", "pred_stack_rules", "=", "self", ".", "get_pred_by_stack_rules", "(", ")", "\n", "if", "pred_stack_rules", ":", "\n", "                        ", "valid_actions", ".", "extend", "(", "pred_stack_rules", ")", "\n", "", "", "else", ":", "\n", "                    ", "valid_actions", ".", "append", "(", "'PRED'", ")", "\n", "\n", "", "", "valid_actions", ".", "extend", "(", "[", "'REDUCE'", ",", "'DEPENDENT'", "]", ")", "\n", "\n", "# Forbid entitity if top token already an entity", "\n", "if", "stack0", "not", "in", "self", ".", "entities", ":", "\n", "# FIXME: Any rules involving MERGE here?", "\n", "# FIXME: Double naming to be rmoevd. This is a source of bugs.", "\n", "                ", "valid_actions", ".", "extend", "(", "[", "'ENTITY'", ",", "'ADDNODE'", "]", ")", "\n", "\n", "# Forbid introduce if no latent", "\n", "", "if", "len", "(", "self", ".", "latent", ")", ">", "0", ":", "\n", "                ", "valid_actions", ".", "append", "(", "'INTRODUCE'", ")", "\n", "\n", "# two or more tokens in stack", "\n", "", "", "if", "len", "(", "self", ".", "stack", ")", ">", "1", ":", "\n", "            ", "stack0", "=", "self", ".", "stack", "[", "-", "1", "]", "\n", "stack1", "=", "self", ".", "stack", "[", "-", "2", "]", "\n", "\n", "# Forbid merging if two words are identical", "\n", "# FIXME: ?? this rule does not make any sense, indices will never", "\n", "# be equal", "\n", "# if stack0 != stack1:", "\n", "valid_actions", ".", "append", "(", "'MERGE'", ")", "\n", "\n", "# Forbid SWAP if both words have been swapped already", "\n", "if", "(", "\n", "(", "\n", "stack0", "not", "in", "self", ".", "swapped_words", "or", "\n", "stack1", "not", "in", "self", ".", "swapped_words", ".", "get", "(", "stack0", ")", "\n", ")", "and", "\n", "(", "\n", "stack1", "not", "in", "self", ".", "swapped_words", "or", "\n", "stack0", "not", "in", "self", ".", "swapped_words", ".", "get", "(", "stack1", ")", "\n", ")", "\n", ")", ":", "\n", "                ", "valid_actions", ".", "extend", "(", "[", "'SWAP'", ",", "'UNSHIFT'", "]", ")", "\n", "\n", "# confirmed nodes can be drawn edges between as long as they are", "\n", "# not repeated", "\n", "", "if", "(", "stack0", "in", "self", ".", "is_confirmed", "and", "stack1", "in", "self", ".", "is_confirmed", ")", ":", "\n", "                ", "valid_actions", ".", "extend", "(", "[", "'LA'", ",", "'RA'", "]", ")", "\n", "\n", "# FIXME: special rule to account for oracle errors", "\n", "", "elif", "self", ".", "get_top_of_stack", "(", ")", "[", "0", "]", "==", "'me'", ":", "\n", "                ", "valid_actions", ".", "extend", "(", "[", "'RA(mode)'", "]", ")", "\n", "\n", "# Forbid actions given graph. Right now avoid some edge duplicates by", "\n", "# forbidding LA, RA and DEPENDENT", "\n", "", "", "invalid_actions", ".", "extend", "(", "get_forbidden_actions", "(", "self", ".", "stack", ",", "self", ".", "amr", ")", ")", "\n", "\n", "return", "valid_actions", ",", "invalid_actions", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.update": [[655, 657], ["state_machine.AMRStateMachine.applyAction"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.DepParsingStateMachine.applyAction"], ["", "def", "update", "(", "self", ",", "act", ")", ":", "\n", "        ", "self", ".", "applyAction", "(", "act", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.get_annotations": [[658, 661], ["state_machine.AMRStateMachine.amr.toJAMRString"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.toJAMRString"], ["", "def", "get_annotations", "(", "self", ")", ":", "\n", "        ", "assert", "self", ".", "amr_graph", ",", "\".toJAMRString() requires amr_graph = True\"", "\n", "return", "self", ".", "amr", ".", "toJAMRString", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.SHIFT": [[662, 680], ["state_machine.AMRStateMachine.buffer.pop", "state_machine.AMRStateMachine.stack.append", "state_machine.AMRStateMachine.labels.append", "state_machine.AMRStateMachine.labelsA.append", "state_machine.AMRStateMachine.predicates.append", "state_machine.AMRStateMachine.CLOSE", "state_machine.AMRStateMachine.actions.append", "state_machine.AMRStateMachine.actions.append", "print", "print", "state_machine.AMRStateMachine.printStackBuffer"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.CLOSE", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.printStackBuffer"], ["", "def", "SHIFT", "(", "self", ",", "shift_label", "=", "None", ")", ":", "\n", "        ", "\"\"\"SHIFT : move buffer[-1] to stack[-1]\"\"\"", "\n", "\n", "# FIXME: No nested actions. This can be handled at try time", "\n", "if", "not", "self", ".", "buffer", ":", "\n", "            ", "self", ".", "CLOSE", "(", ")", "\n", "", "tok", "=", "self", ".", "buffer", ".", "pop", "(", ")", "\n", "self", ".", "stack", ".", "append", "(", "tok", ")", "\n", "if", "shift_label", ":", "\n", "            ", "self", ".", "actions", ".", "append", "(", "f'SHIFT({shift_label})'", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "actions", ".", "append", "(", "'SHIFT'", ")", "\n", "", "self", ".", "labels", ".", "append", "(", "'_'", ")", "\n", "self", ".", "labelsA", ".", "append", "(", "'_'", ")", "\n", "self", ".", "predicates", ".", "append", "(", "'_'", ")", "\n", "if", "self", ".", "verbose", ":", "\n", "            ", "print", "(", "'SHIFT'", ")", "\n", "print", "(", "self", ".", "printStackBuffer", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.REDUCE": [[681, 697], ["state_machine.AMRStateMachine.stack.pop", "state_machine.AMRStateMachine.actions.append", "state_machine.AMRStateMachine.labels.append", "state_machine.AMRStateMachine.labelsA.append", "state_machine.AMRStateMachine.predicates.append", "print", "print", "len", "state_machine.AMRStateMachine.printStackBuffer"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.printStackBuffer"], ["", "", "def", "REDUCE", "(", "self", ")", ":", "\n", "        ", "\"\"\"REDUCE : delete token\"\"\"", "\n", "\n", "stack0", "=", "self", ".", "stack", ".", "pop", "(", ")", "\n", "# if stack0 has no edges, delete it from the amr", "\n", "if", "self", ".", "amr_graph", "and", "stack0", "!=", "-", "1", "and", "stack0", "not", "in", "self", ".", "entities", ":", "\n", "            ", "if", "len", "(", "[", "e", "for", "e", "in", "self", ".", "amr", ".", "edges", "if", "stack0", "in", "e", "]", ")", "==", "0", ":", "\n", "                ", "if", "stack0", "in", "self", ".", "amr", ".", "nodes", ":", "\n", "                    ", "del", "self", ".", "amr", ".", "nodes", "[", "stack0", "]", "\n", "", "", "", "self", ".", "actions", ".", "append", "(", "'REDUCE'", ")", "\n", "self", ".", "labels", ".", "append", "(", "'_'", ")", "\n", "self", ".", "labelsA", ".", "append", "(", "'_'", ")", "\n", "self", ".", "predicates", ".", "append", "(", "'_'", ")", "\n", "if", "self", ".", "verbose", ":", "\n", "            ", "print", "(", "'REDUCE'", ")", "\n", "print", "(", "self", ".", "printStackBuffer", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.CONFIRM": [[698, 718], ["state_machine.AMRStateMachine.actions.append", "state_machine.AMRStateMachine.labels.append", "state_machine.AMRStateMachine.labelsA.append", "state_machine.AMRStateMachine.predicates.append", "state_machine.AMRStateMachine.is_confirmed.add", "state_machine.AMRStateMachine.get_top_of_stack", "tuple", "print", "print", "state_machine.AMRStateMachine.printStackBuffer"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.add", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.get_top_of_stack", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.printStackBuffer"], ["", "", "def", "CONFIRM", "(", "self", ",", "node_label", ")", ":", "\n", "        ", "\"\"\"CONFIRM : assign a propbank label\"\"\"", "\n", "stack0", "=", "self", ".", "stack", "[", "-", "1", "]", "\n", "if", "self", ".", "amr_graph", ":", "\n", "            ", "self", ".", "amr", ".", "nodes", "[", "stack0", "]", "=", "node_label", "\n", "", "self", ".", "actions", ".", "append", "(", "f'PRED({node_label})'", ")", "\n", "self", ".", "labels", ".", "append", "(", "'_'", ")", "\n", "self", ".", "labelsA", ".", "append", "(", "'_'", ")", "\n", "self", ".", "predicates", ".", "append", "(", "node_label", ")", "\n", "self", ".", "is_confirmed", ".", "add", "(", "stack0", ")", "\n", "# keep alignments", "\n", "token", ",", "merged_tokens", "=", "self", ".", "get_top_of_stack", "(", "positions", "=", "True", ")", "\n", "if", "merged_tokens", ":", "\n", "            ", "self", ".", "alignments", "[", "stack0", "]", "=", "tuple", "(", "merged_tokens", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "alignments", "[", "stack0", "]", "=", "token", "\n", "\n", "", "if", "self", ".", "verbose", ":", "\n", "            ", "print", "(", "f'PRED({node_label})'", ")", "\n", "print", "(", "self", ".", "printStackBuffer", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.COPY_LEMMA": [[719, 743], ["state_machine.AMRStateMachine.get_top_of_stack", "state_machine.AMRStateMachine.actions.append", "state_machine.AMRStateMachine.labels.append", "state_machine.AMRStateMachine.labelsA.append", "state_machine.AMRStateMachine.predicates.append", "state_machine.AMRStateMachine.is_confirmed.add", "state_machine.AMRStateMachine.get_top_of_stack", "tuple", "print", "print", "state_machine.AMRStateMachine.printStackBuffer"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.get_top_of_stack", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.add", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.get_top_of_stack", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.printStackBuffer"], ["", "", "def", "COPY_LEMMA", "(", "self", ")", ":", "\n", "        ", "\"\"\"COPY_LEMMA: Same as CONFIRM but use lowercased top-of-stack\"\"\"", "\n", "# get top of stack and lemma", "\n", "stack0", "=", "self", ".", "stack", "[", "-", "1", "]", "\n", "node_label", ",", "_", "=", "self", ".", "get_top_of_stack", "(", "lemma", "=", "True", ")", "\n", "# update AMR graph", "\n", "if", "self", ".", "amr_graph", ":", "\n", "            ", "self", ".", "amr", ".", "nodes", "[", "stack0", "]", "=", "node_label", "\n", "# update statistics", "\n", "", "self", ".", "actions", ".", "append", "(", "f'COPY_LEMMA'", ")", "\n", "self", ".", "labels", ".", "append", "(", "'_'", ")", "\n", "self", ".", "labelsA", ".", "append", "(", "'_'", ")", "\n", "self", ".", "predicates", ".", "append", "(", "node_label", ")", "\n", "self", ".", "is_confirmed", ".", "add", "(", "stack0", ")", "\n", "# keep alignments", "\n", "token", ",", "merged_tokens", "=", "self", ".", "get_top_of_stack", "(", "positions", "=", "True", ")", "\n", "if", "merged_tokens", ":", "\n", "            ", "self", ".", "alignments", "[", "stack0", "]", "=", "tuple", "(", "merged_tokens", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "alignments", "[", "stack0", "]", "=", "token", "\n", "\n", "", "if", "self", ".", "verbose", ":", "\n", "            ", "print", "(", "f'COPY_LEMMA'", ")", "\n", "print", "(", "self", ".", "printStackBuffer", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.COPY_SENSE01": [[744, 769], ["state_machine.AMRStateMachine.get_top_of_stack", "state_machine.AMRStateMachine.actions.append", "state_machine.AMRStateMachine.labels.append", "state_machine.AMRStateMachine.labelsA.append", "state_machine.AMRStateMachine.predicates.append", "state_machine.AMRStateMachine.is_confirmed.add", "state_machine.AMRStateMachine.get_top_of_stack", "tuple", "print", "print", "state_machine.AMRStateMachine.printStackBuffer"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.get_top_of_stack", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.add", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.get_top_of_stack", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.printStackBuffer"], ["", "", "def", "COPY_SENSE01", "(", "self", ")", ":", "\n", "        ", "\"\"\"COPY_SENSE01: Same as CONFIRM but use lowercased top-of-stack\"\"\"", "\n", "# get top of stack and lemma", "\n", "stack0", "=", "self", ".", "stack", "[", "-", "1", "]", "\n", "lemma", ",", "_", "=", "self", ".", "get_top_of_stack", "(", "lemma", "=", "True", ")", "\n", "node_label", "=", "f'{lemma}-01'", "\n", "# update AMR graph", "\n", "if", "self", ".", "amr_graph", ":", "\n", "            ", "self", ".", "amr", ".", "nodes", "[", "stack0", "]", "=", "node_label", "\n", "# update statistics", "\n", "", "self", ".", "actions", ".", "append", "(", "f'COPY_SENSE01'", ")", "\n", "self", ".", "labels", ".", "append", "(", "'_'", ")", "\n", "self", ".", "labelsA", ".", "append", "(", "'_'", ")", "\n", "self", ".", "predicates", ".", "append", "(", "node_label", ")", "\n", "self", ".", "is_confirmed", ".", "add", "(", "stack0", ")", "\n", "# keep alignments", "\n", "token", ",", "merged_tokens", "=", "self", ".", "get_top_of_stack", "(", "positions", "=", "True", ")", "\n", "if", "merged_tokens", ":", "\n", "            ", "self", ".", "alignments", "[", "stack0", "]", "=", "tuple", "(", "merged_tokens", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "alignments", "[", "stack0", "]", "=", "token", "\n", "\n", "", "if", "self", ".", "verbose", ":", "\n", "            ", "print", "(", "f'COPY_SENSE01'", ")", "\n", "print", "(", "self", ".", "printStackBuffer", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.LA": [[770, 791], ["state_machine.AMRStateMachine.actions.append", "state_machine.AMRStateMachine.labelsA.append", "state_machine.AMRStateMachine.predicates.append", "state_machine.AMRStateMachine.amr.edges.append", "state_machine.AMRStateMachine.labels.append", "state_machine.AMRStateMachine.labels.append", "print", "print", "state_machine.AMRStateMachine.printStackBuffer"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.printStackBuffer"], ["", "", "def", "LA", "(", "self", ",", "edge_label", ")", ":", "\n", "        ", "\"\"\"LA : add an edge from stack[-1] to stack[-2]\"\"\"", "\n", "\n", "# Add edge to graph", "\n", "if", "self", ".", "amr_graph", ":", "\n", "            ", "self", ".", "amr", ".", "edges", ".", "append", "(", "(", "\n", "self", ".", "stack", "[", "-", "1", "]", ",", "\n", "f':{edge_label}'", "if", "edge_label", "!=", "'root'", "else", "'root'", ",", "\n", "self", ".", "stack", "[", "-", "2", "]", ",", "\n", ")", ")", "\n", "# keep track of other vars", "\n", "", "self", ".", "actions", ".", "append", "(", "f'LA({edge_label})'", ")", "\n", "if", "edge_label", "!=", "'root'", ":", "\n", "            ", "self", ".", "labels", ".", "append", "(", "edge_label", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "labels", ".", "append", "(", "'_'", ")", "\n", "", "self", ".", "labelsA", ".", "append", "(", "'_'", ")", "\n", "self", ".", "predicates", ".", "append", "(", "'_'", ")", "\n", "if", "self", ".", "verbose", ":", "\n", "            ", "print", "(", "f'LA({edge_label})'", ")", "\n", "print", "(", "self", ".", "printStackBuffer", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.RA": [[792, 812], ["state_machine.AMRStateMachine.actions.append", "state_machine.AMRStateMachine.labelsA.append", "state_machine.AMRStateMachine.predicates.append", "state_machine.AMRStateMachine.amr.edges.append", "state_machine.AMRStateMachine.labels.append", "state_machine.AMRStateMachine.labels.append", "print", "print", "state_machine.AMRStateMachine.printStackBuffer"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.printStackBuffer"], ["", "", "def", "RA", "(", "self", ",", "edge_label", ")", ":", "\n", "        ", "\"\"\"RA : add an edge from stack[-2] to stack[-1]\"\"\"", "\n", "# Add edge to graph", "\n", "if", "self", ".", "amr_graph", ":", "\n", "            ", "self", ".", "amr", ".", "edges", ".", "append", "(", "(", "\n", "self", ".", "stack", "[", "-", "2", "]", ",", "\n", "f':{edge_label}'", "if", "edge_label", "!=", "'root'", "else", "'root'", ",", "\n", "self", ".", "stack", "[", "-", "1", "]", "\n", ")", ")", "\n", "# keep track of other vars", "\n", "", "self", ".", "actions", ".", "append", "(", "f'RA({edge_label})'", ")", "\n", "if", "edge_label", "!=", "'root'", ":", "\n", "            ", "self", ".", "labels", ".", "append", "(", "edge_label", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "labels", ".", "append", "(", "'_'", ")", "\n", "", "self", ".", "labelsA", ".", "append", "(", "'_'", ")", "\n", "self", ".", "predicates", ".", "append", "(", "'_'", ")", "\n", "if", "self", ".", "verbose", ":", "\n", "            ", "print", "(", "f'RA({edge_label})'", ")", "\n", "print", "(", "self", ".", "printStackBuffer", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.MERGE": [[813, 880], ["state_machine.AMRStateMachine.stack.pop", "state_machine.AMRStateMachine.stack.pop", "state_machine.AMRStateMachine.stack.append", "state_machine.AMRStateMachine.actions.append", "state_machine.AMRStateMachine.labels.append", "state_machine.AMRStateMachine.labelsA.append", "state_machine.AMRStateMachine.predicates.append", "state_machine.AMRStateMachine.merged_tokens[].insert", "enumerate", "any", "print", "print", "state_machine.AMRStateMachine.tokens[].replace", "state_machine.AMRStateMachine.entities.remove", "state_machine.AMRStateMachine.is_confirmed.add", "state_machine.AMRStateMachine.printStackBuffer", "state_machine.AMRStateMachine.amr.edges.remove"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.add", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.printStackBuffer"], ["", "", "def", "MERGE", "(", "self", ")", ":", "\n", "        ", "\"\"\"MERGE : merge two tokens to be the same node\"\"\"", "\n", "\n", "lead", "=", "self", ".", "stack", ".", "pop", "(", ")", "\n", "sec", "=", "self", ".", "stack", ".", "pop", "(", ")", "\n", "self", ".", "stack", ".", "append", "(", "lead", ")", "\n", "\n", "# maintain merged tokens dict", "\n", "if", "lead", "not", "in", "self", ".", "merged_tokens", ":", "\n", "            ", "self", ".", "merged_tokens", "[", "lead", "]", "=", "[", "lead", "]", "\n", "", "if", "sec", "in", "self", ".", "merged_tokens", ":", "\n", "            ", "self", ".", "merged_tokens", "[", "lead", "]", "=", "self", ".", "merged_tokens", "[", "sec", "]", "+", "self", ".", "merged_tokens", "[", "lead", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "merged_tokens", "[", "lead", "]", ".", "insert", "(", "0", ",", "sec", ")", "\n", "", "merged", "=", "','", ".", "join", "(", "self", ".", "tokens", "[", "x", "-", "1", "]", ".", "replace", "(", "','", ",", "'-COMMA-'", ")", "for", "x", "in", "self", ".", "merged_tokens", "[", "lead", "]", ")", "\n", "\n", "if", "self", ".", "amr_graph", ":", "\n", "\n", "            ", "for", "i", ",", "e", "in", "enumerate", "(", "self", ".", "amr", ".", "edges", ")", ":", "\n", "                ", "if", "e", "[", "1", "]", "==", "'entity'", ":", "\n", "                    ", "continue", "\n", "", "if", "sec", "==", "e", "[", "0", "]", ":", "\n", "                    ", "self", ".", "amr", ".", "edges", "[", "i", "]", "=", "(", "lead", ",", "e", "[", "1", "]", ",", "e", "[", "2", "]", ")", "\n", "", "if", "sec", "==", "e", "[", "2", "]", ":", "\n", "                    ", "self", ".", "amr", ".", "edges", "[", "i", "]", "=", "(", "e", "[", "0", "]", ",", "e", "[", "1", "]", ",", "lead", ")", "\n", "\n", "# Just in case you merge entities. This shouldn't happen but might.", "\n", "# FIXME: Now this code only active if self.amr_graph = True", "\n", "", "", "if", "lead", "in", "self", ".", "entities", ":", "\n", "                ", "entity_edges", "=", "[", "e", "for", "e", "in", "self", ".", "amr", ".", "edges", "if", "e", "[", "0", "]", "==", "lead", "and", "e", "[", "1", "]", "==", "'entity'", "]", "\n", "lead", "=", "[", "t", "for", "s", ",", "r", ",", "t", "in", "entity_edges", "]", "[", "0", "]", "\n", "", "if", "sec", "in", "self", ".", "entities", ":", "\n", "                ", "entity_edges", "=", "[", "e", "for", "e", "in", "self", ".", "amr", ".", "edges", "if", "e", "[", "0", "]", "==", "sec", "and", "e", "[", "1", "]", "==", "'entity'", "]", "\n", "child", "=", "[", "t", "for", "s", ",", "r", ",", "t", "in", "entity_edges", "]", "[", "0", "]", "\n", "del", "self", ".", "amr", ".", "nodes", "[", "sec", "]", "\n", "for", "e", "in", "entity_edges", ":", "\n", "                    ", "self", ".", "amr", ".", "edges", ".", "remove", "(", "e", ")", "\n", "", "self", ".", "entities", ".", "remove", "(", "sec", ")", "\n", "sec", "=", "child", "\n", "\n", "# make tokens into a single node", "\n", "", "del", "self", ".", "amr", ".", "nodes", "[", "sec", "]", "\n", "self", ".", "amr", ".", "nodes", "[", "lead", "]", "=", "merged", "\n", "\n", "", "elif", "lead", "in", "self", ".", "entities", "or", "sec", "in", "self", ".", "entities", ":", "\n", "\n", "# see FIXME above", "\n", "            ", "pass", "\n", "\n", "# if any token in this merged group is promoted, promote the rest", "\n", "# FIXME: sometimes lead is not in self.merged_tokens. Unclear why", "\n", "", "if", "(", "\n", "lead", "in", "self", ".", "merged_tokens", "and", "\n", "any", "(", "n", "in", "self", ".", "is_confirmed", "for", "n", "in", "self", ".", "merged_tokens", "[", "lead", "]", ")", "\n", ")", ":", "\n", "            ", "for", "n", "in", "self", ".", "merged_tokens", "[", "lead", "]", ":", "\n", "                ", "self", ".", "is_confirmed", ".", "add", "(", "n", ")", "\n", "\n", "# update states", "\n", "", "", "self", ".", "actions", ".", "append", "(", "f'MERGE'", ")", "\n", "self", ".", "labels", ".", "append", "(", "'_'", ")", "\n", "self", ".", "labelsA", ".", "append", "(", "'_'", ")", "\n", "self", ".", "predicates", ".", "append", "(", "'_'", ")", "\n", "\n", "if", "self", ".", "verbose", ":", "\n", "            ", "print", "(", "f'MERGE({self.amr.nodes[lead]})'", ")", "\n", "print", "(", "self", ".", "printStackBuffer", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.ENTITY": [[881, 913], ["state_machine.AMRStateMachine.entities.append", "state_machine.AMRStateMachine.actions.append", "state_machine.AMRStateMachine.labels.append", "state_machine.AMRStateMachine.labelsA.append", "state_machine.AMRStateMachine.predicates.append", "state_machine.AMRStateMachine.is_confirmed.add", "state_machine.AMRStateMachine.get_top_of_stack", "enumerate", "state_machine.AMRStateMachine.amr.edges.append", "tuple", "print", "print", "state_machine.AMRStateMachine.printStackBuffer"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.add", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.get_top_of_stack", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.printStackBuffer"], ["", "", "def", "ENTITY", "(", "self", ",", "entity_type", ")", ":", "\n", "        ", "\"\"\"ENTITY : create a named entity\"\"\"", "\n", "head", "=", "self", ".", "stack", "[", "-", "1", "]", "\n", "child_id", "=", "self", ".", "new_id", "\n", "self", ".", "new_id", "+=", "1", "\n", "\n", "if", "self", ".", "amr_graph", ":", "\n", "# Fixes :rel", "\n", "            ", "for", "(", "i", ",", "(", "s", ",", "l", ",", "t", ")", ")", "in", "enumerate", "(", "self", ".", "amr", ".", "edges", ")", ":", "\n", "                ", "if", "s", "==", "head", ":", "\n", "                    ", "self", ".", "amr", ".", "edges", "[", "i", "]", "=", "(", "child_id", ",", "l", ",", "t", ")", "\n", "", "", "self", ".", "amr", ".", "nodes", "[", "child_id", "]", "=", "self", ".", "amr", ".", "nodes", "[", "head", "]", "\n", "self", ".", "amr", ".", "nodes", "[", "head", "]", "=", "f'({entity_type})'", "\n", "self", ".", "amr", ".", "edges", ".", "append", "(", "(", "head", ",", "'entity'", ",", "child_id", ")", ")", "\n", "", "self", ".", "entities", ".", "append", "(", "head", ")", "\n", "\n", "self", ".", "actions", ".", "append", "(", "f'ADDNODE({entity_type})'", ")", "\n", "self", ".", "labels", ".", "append", "(", "'_'", ")", "\n", "self", ".", "labelsA", ".", "append", "(", "f'{entity_type}'", ")", "\n", "self", ".", "predicates", ".", "append", "(", "'_'", ")", "\n", "self", ".", "is_confirmed", ".", "add", "(", "head", ")", "\n", "\n", "# keep alignments", "\n", "token", ",", "merged_tokens", "=", "self", ".", "get_top_of_stack", "(", "positions", "=", "True", ")", "\n", "if", "merged_tokens", ":", "\n", "            ", "self", ".", "alignments", "[", "head", "]", "=", "tuple", "(", "merged_tokens", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "alignments", "[", "head", "]", "=", "token", "\n", "\n", "", "if", "self", ".", "verbose", ":", "\n", "            ", "print", "(", "f'ADDNODE({entity_type})'", ")", "\n", "print", "(", "self", ".", "printStackBuffer", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.DEPENDENT": [[914, 936], ["state_machine.AMRStateMachine.actions.append", "state_machine.AMRStateMachine.labels.append", "state_machine.AMRStateMachine.labelsA.append", "state_machine.AMRStateMachine.predicates.append", "edge_label.startswith", "state_machine.AMRStateMachine.amr.edges.append", "print", "print", "state_machine.AMRStateMachine.printStackBuffer", "edge_label.replace"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.printStackBuffer"], ["", "", "def", "DEPENDENT", "(", "self", ",", "node_label", ",", "edge_label", ",", "node_id", "=", "None", ")", ":", "\n", "        ", "\"\"\"DEPENDENT : add a single edge and node\"\"\"", "\n", "\n", "head", "=", "self", ".", "stack", "[", "-", "1", "]", "\n", "new_id", "=", "self", ".", "new_id", "\n", "\n", "edge_label", "=", "edge_label", "if", "edge_label", ".", "startswith", "(", "':'", ")", "else", "':'", "+", "edge_label", "\n", "\n", "if", "self", ".", "amr_graph", ":", "\n", "            ", "if", "node_id", ":", "\n", "                ", "new_id", "=", "node_id", "\n", "", "else", ":", "\n", "                ", "self", ".", "amr", ".", "nodes", "[", "new_id", "]", "=", "node_label", "\n", "", "self", ".", "amr", ".", "edges", ".", "append", "(", "(", "head", ",", "edge_label", ",", "new_id", ")", ")", "\n", "", "self", ".", "new_id", "+=", "1", "\n", "self", ".", "actions", ".", "append", "(", "f'DEPENDENT({node_label},{edge_label.replace(\":\",\"\")})'", ")", "\n", "self", ".", "labels", ".", "append", "(", "'_'", ")", "\n", "self", ".", "labelsA", ".", "append", "(", "'_'", ")", "\n", "self", ".", "predicates", ".", "append", "(", "'_'", ")", "\n", "if", "self", ".", "verbose", ":", "\n", "            ", "print", "(", "f'DEPENDENT({edge_label},{node_label})'", ")", "\n", "print", "(", "self", ".", "printStackBuffer", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.SWAP": [[937, 954], ["state_machine.AMRStateMachine.stack.pop", "state_machine.AMRStateMachine.stack.pop", "state_machine.AMRStateMachine.buffer.append", "state_machine.AMRStateMachine.stack.append", "state_machine.AMRStateMachine.actions.append", "state_machine.AMRStateMachine.labels.append", "state_machine.AMRStateMachine.labelsA.append", "state_machine.AMRStateMachine.predicates.append", "state_machine.AMRStateMachine.swapped_words[].append", "print", "print", "state_machine.AMRStateMachine.printStackBuffer"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.printStackBuffer"], ["", "", "def", "SWAP", "(", "self", ")", ":", "\n", "        ", "\"\"\"SWAP : move stack[1] to buffer\"\"\"", "\n", "\n", "stack0", "=", "self", ".", "stack", ".", "pop", "(", ")", "\n", "stack1", "=", "self", ".", "stack", ".", "pop", "(", ")", "\n", "self", ".", "buffer", ".", "append", "(", "stack1", ")", "\n", "self", ".", "stack", ".", "append", "(", "stack0", ")", "\n", "self", ".", "actions", ".", "append", "(", "'UNSHIFT'", ")", "\n", "self", ".", "labels", ".", "append", "(", "'_'", ")", "\n", "self", ".", "labelsA", ".", "append", "(", "'_'", ")", "\n", "self", ".", "predicates", ".", "append", "(", "'_'", ")", "\n", "if", "stack1", "not", "in", "self", ".", "swapped_words", ":", "\n", "            ", "self", ".", "swapped_words", "[", "stack1", "]", "=", "[", "]", "\n", "", "self", ".", "swapped_words", "[", "stack1", "]", ".", "append", "(", "stack0", ")", "\n", "if", "self", ".", "verbose", ":", "\n", "            ", "print", "(", "'UNSHIFT'", ")", "\n", "print", "(", "self", ".", "printStackBuffer", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.INTRODUCE": [[955, 967], ["state_machine.AMRStateMachine.latent.pop", "state_machine.AMRStateMachine.stack.append", "state_machine.AMRStateMachine.actions.append", "state_machine.AMRStateMachine.labels.append", "state_machine.AMRStateMachine.labelsA.append", "state_machine.AMRStateMachine.predicates.append", "print", "print", "state_machine.AMRStateMachine.printStackBuffer"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.printStackBuffer"], ["", "", "def", "INTRODUCE", "(", "self", ")", ":", "\n", "        ", "\"\"\"INTRODUCE : move latent[-1] to stack\"\"\"", "\n", "\n", "latent0", "=", "self", ".", "latent", ".", "pop", "(", ")", "\n", "self", ".", "stack", ".", "append", "(", "latent0", ")", "\n", "self", ".", "actions", ".", "append", "(", "'INTRODUCE'", ")", "\n", "self", ".", "labels", ".", "append", "(", "'_'", ")", "\n", "self", ".", "labelsA", ".", "append", "(", "'_'", ")", "\n", "self", ".", "predicates", ".", "append", "(", "'_'", ")", "\n", "if", "self", ".", "verbose", ":", "\n", "            ", "print", "(", "'INTRODUCE'", ")", "\n", "print", "(", "self", ".", "printStackBuffer", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.CLOSE": [[968, 1045], ["state_machine.AMRStateMachine.actions.append", "state_machine.AMRStateMachine.labels.append", "state_machine.AMRStateMachine.labelsA.append", "state_machine.AMRStateMachine.predicates.append", "enumerate", "state_machine.AMRStateMachine.connectGraph", "state_machine.AMRStateMachine.convert_state_machine_alignments_to_amr_alignments", "print", "state_machine.AMRStateMachine.postprocessing_training", "state_machine.AMRStateMachine.postprocessing", "print", "print", "to_del.append", "state_machine.AMRStateMachine.amr.nodes[].startswith", "state_machine.AMRStateMachine.amr.nodes[].endswith", "r.startswith", "state_machine.AMRStateMachine.printStackBuffer", "state_machine.AMRStateMachine.amr.toJAMRString", "any", "state_machine.AMRStateMachine.amr.nodes[].startswith", "state_machine.AMRStateMachine.amr.nodes[].endswith", "[].isalpha", "[].isdigit", "state_machine.AMRStateMachine.amr.nodes[].replace", "state_machine.AMRStateMachine.amr.nodes[].replace", "state_machine.AMRStateMachine.amr.nodes[].replace", "state_machine.AMRStateMachine.amr.nodes[].replace"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.connectGraph", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.convert_state_machine_alignments_to_amr_alignments", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.postprocessing_training", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.postprocessing", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.printStackBuffer", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.toJAMRString"], ["", "", "def", "CLOSE", "(", "self", ",", "training", "=", "False", ",", "gold_amr", "=", "None", ",", "use_addnonde_rules", "=", "False", ")", ":", "\n", "        ", "\"\"\"CLOSE : finish parsing\"\"\"", "\n", "\n", "self", ".", "buffer", "=", "[", "]", "\n", "self", ".", "stack", "=", "[", "]", "\n", "if", "self", ".", "amr_graph", "and", "self", ".", "post_process", ":", "\n", "            ", "if", "training", "and", "not", "use_addnonde_rules", ":", "\n", "                ", "self", ".", "postprocessing_training", "(", "gold_amr", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "postprocessing", "(", "gold_amr", ")", "\n", "\n", "", "for", "item", "in", "self", ".", "latent", ":", "\n", "                ", "if", "item", "in", "self", ".", "amr", ".", "nodes", "and", "not", "any", "(", "item", "==", "s", "or", "item", "==", "t", "for", "s", ",", "r", ",", "t", "in", "self", ".", "amr", ".", "edges", ")", ":", "\n", "                    ", "del", "self", ".", "amr", ".", "nodes", "[", "item", "]", "\n", "", "", "self", ".", "latent", "=", "[", "]", "\n", "\n", "# delete (reduce) the nodes that were never confirmed or attached", "\n", "to_del", "=", "[", "]", "\n", "for", "n", "in", "self", ".", "amr", ".", "nodes", ":", "\n", "                ", "found", "=", "False", "\n", "if", "n", "in", "self", ".", "is_confirmed", ":", "\n", "                    ", "found", "=", "True", "\n", "", "else", ":", "\n", "                    ", "for", "s", ",", "r", ",", "t", "in", "self", ".", "amr", ".", "edges", ":", "\n", "                        ", "if", "n", "==", "s", "or", "n", "==", "t", ":", "\n", "                            ", "found", "=", "True", "\n", "", "", "", "if", "not", "found", ":", "\n", "                    ", "to_del", ".", "append", "(", "n", ")", "\n", "", "", "for", "n", "in", "to_del", ":", "\n", "                ", "del", "self", ".", "amr", ".", "nodes", "[", "n", "]", "\n", "\n", "# clean concepts", "\n", "", "for", "n", "in", "self", ".", "amr", ".", "nodes", ":", "\n", "                ", "if", "self", ".", "amr", ".", "nodes", "[", "n", "]", "in", "[", "'.'", ",", "'?'", ",", "'!'", ",", "','", ",", "';'", ",", "'\"'", ",", "\"'\"", "]", ":", "\n", "                    ", "self", ".", "amr", ".", "nodes", "[", "n", "]", "=", "'PUNCT'", "\n", "", "if", "self", ".", "amr", ".", "nodes", "[", "n", "]", ".", "startswith", "(", "'\"'", ")", "and", "self", ".", "amr", ".", "nodes", "[", "n", "]", ".", "endswith", "(", "'\"'", ")", ":", "\n", "                    ", "self", ".", "amr", ".", "nodes", "[", "n", "]", "=", "'\"'", "+", "self", ".", "amr", ".", "nodes", "[", "n", "]", ".", "replace", "(", "'\"'", ",", "''", ")", "+", "'\"'", "\n", "", "if", "not", "(", "self", ".", "amr", ".", "nodes", "[", "n", "]", ".", "startswith", "(", "'\"'", ")", "and", "self", ".", "amr", ".", "nodes", "[", "n", "]", ".", "endswith", "(", "'\"'", ")", ")", ":", "\n", "                    ", "for", "ch", "in", "[", "'/'", ",", "':'", ",", "'('", ",", "')'", ",", "'\\\\'", "]", ":", "\n", "                        ", "if", "ch", "in", "self", ".", "amr", ".", "nodes", "[", "n", "]", ":", "\n", "                            ", "self", ".", "amr", ".", "nodes", "[", "n", "]", "=", "self", ".", "amr", ".", "nodes", "[", "n", "]", ".", "replace", "(", "ch", ",", "'-'", ")", "\n", "", "", "", "if", "not", "self", ".", "amr", ".", "nodes", "[", "n", "]", ":", "\n", "                    ", "self", ".", "amr", ".", "nodes", "[", "n", "]", "=", "'None'", "\n", "", "if", "','", "in", "self", ".", "amr", ".", "nodes", "[", "n", "]", ":", "\n", "                    ", "self", ".", "amr", ".", "nodes", "[", "n", "]", "=", "'\"'", "+", "self", ".", "amr", ".", "nodes", "[", "n", "]", ".", "replace", "(", "'\"'", ",", "''", ")", "+", "'\"'", "\n", "", "if", "not", "self", ".", "amr", ".", "nodes", "[", "n", "]", "[", "0", "]", ".", "isalpha", "(", ")", "and", "not", "self", ".", "amr", ".", "nodes", "[", "n", "]", "[", "0", "]", ".", "isdigit", "(", ")", "and", "not", "self", ".", "amr", ".", "nodes", "[", "n", "]", "[", "0", "]", "in", "[", "'-'", ",", "'+'", "]", ":", "\n", "                    ", "self", ".", "amr", ".", "nodes", "[", "n", "]", "=", "'\"'", "+", "self", ".", "amr", ".", "nodes", "[", "n", "]", ".", "replace", "(", "'\"'", ",", "''", ")", "+", "'\"'", "\n", "# clean edges", "\n", "", "", "for", "j", ",", "e", "in", "enumerate", "(", "self", ".", "amr", ".", "edges", ")", ":", "\n", "                ", "s", ",", "r", ",", "t", "=", "e", "\n", "if", "not", "r", ".", "startswith", "(", "':'", ")", ":", "\n", "                    ", "r", "=", "':'", "+", "r", "\n", "", "e", "=", "(", "s", ",", "r", ",", "t", ")", "\n", "self", ".", "amr", ".", "edges", "[", "j", "]", "=", "e", "\n", "# handle missing nodes (this shouldn't happen but a bad sequence of actions can produce it)", "\n", "", "for", "s", ",", "r", ",", "t", "in", "self", ".", "amr", ".", "edges", ":", "\n", "                ", "if", "s", "not", "in", "self", ".", "amr", ".", "nodes", ":", "\n", "                    ", "self", ".", "amr", ".", "nodes", "[", "s", "]", "=", "'NA'", "\n", "", "if", "t", "not", "in", "self", ".", "amr", ".", "nodes", ":", "\n", "                    ", "self", ".", "amr", ".", "nodes", "[", "t", "]", "=", "'NA'", "\n", "", "", "self", ".", "connectGraph", "(", ")", "\n", "\n", "", "self", ".", "actions", ".", "append", "(", "'SHIFT'", ")", "\n", "self", ".", "labels", ".", "append", "(", "'_'", ")", "\n", "self", ".", "labelsA", ".", "append", "(", "'_'", ")", "\n", "self", ".", "predicates", ".", "append", "(", "'_'", ")", "\n", "# FIXME: Make sure that this is not needed when amr_graph = False", "\n", "if", "self", ".", "amr_graph", ":", "\n", "           ", "self", ".", "convert_state_machine_alignments_to_amr_alignments", "(", ")", "\n", "", "if", "self", ".", "verbose", ":", "\n", "            ", "print", "(", "'CLOSE'", ")", "\n", "if", "self", ".", "amr_graph", ":", "\n", "                ", "print", "(", "self", ".", "printStackBuffer", "(", ")", ")", "\n", "print", "(", "self", ".", "amr", ".", "toJAMRString", "(", ")", ")", "\n", "\n", "# Close the machine", "\n", "", "", "self", ".", "is_closed", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.convert_state_machine_alignments_to_amr_alignments": [[1046, 1059], ["type", "list", "copy.deepcopy", "list.append", "type"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "convert_state_machine_alignments_to_amr_alignments", "(", "self", ")", ":", "\n", "# In the state machine, we get the alignments with index 0", "\n", "# However, in the AMR, alignments are stored with index 1, since that is the way the oracle expects it", "\n", "\n", "        ", "for", "node", "in", "self", ".", "alignments", ":", "\n", "            ", "if", "type", "(", "self", ".", "alignments", "[", "node", "]", ")", "==", "int", ":", "\n", "                ", "self", ".", "amr", ".", "alignments", "[", "node", "]", "=", "self", ".", "alignments", "[", "node", "]", "+", "1", "\n", "", "else", ":", "\n", "                ", "new_list", "=", "list", "(", ")", "\n", "for", "alignment", "in", "self", ".", "alignments", "[", "node", "]", ":", "\n", "                    ", "assert", "type", "(", "alignment", ")", "==", "int", "\n", "new_list", ".", "append", "(", "alignment", "+", "1", ")", "\n", "", "self", ".", "amr", ".", "alignments", "[", "node", "]", "=", "deepcopy", "(", "new_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.printStackBuffer": [[1060, 1066], ["reversed", "reversed"], "methods", ["None"], ["", "", "", "def", "printStackBuffer", "(", "self", ")", ":", "\n", "        ", "s", "=", "'STACK ['", "+", "' '", ".", "join", "(", "self", ".", "amr", ".", "nodes", "[", "x", "]", "if", "x", "in", "self", ".", "amr", ".", "nodes", "else", "'None'", "for", "x", "in", "self", ".", "stack", ")", "+", "'] '", "\n", "s", "+=", "'BUFFER ['", "+", "' '", ".", "join", "(", "self", ".", "amr", ".", "nodes", "[", "x", "]", "if", "x", "in", "self", ".", "amr", ".", "nodes", "else", "'None'", "for", "x", "in", "reversed", "(", "self", ".", "buffer", ")", ")", "+", "']\\n'", "\n", "if", "self", ".", "latent", ":", "\n", "            ", "s", "+=", "'LATENT ['", "+", "' '", ".", "join", "(", "self", ".", "amr", ".", "nodes", "[", "x", "]", "if", "x", "in", "self", ".", "amr", ".", "nodes", "else", "'None'", "for", "x", "in", "reversed", "(", "self", ".", "latent", ")", ")", "+", "']\\n'", "\n", "", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.connectGraph": [[1067, 1114], ["potential_roots.copy", "potential_roots.copy", "state_machine.AMRStateMachine.amr.edges.remove", "descendents[].update", "potential_roots.copy.remove", "max", "len", "root_edges.append", "potential_roots.remove", "len", "potential_roots.remove", "state_machine.AMRStateMachine.amr.nodes.keys", "descendents[].update", "state_machine.AMRStateMachine.amr.edges.append", "len", "len"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.copy", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.copy", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "connectGraph", "(", "self", ")", ":", "\n", "        ", "assigned_root", "=", "None", "\n", "root_edges", "=", "[", "]", "\n", "if", "-", "1", "in", "self", ".", "amr", ".", "nodes", ":", "\n", "            ", "del", "self", ".", "amr", ".", "nodes", "[", "-", "1", "]", "\n", "", "for", "s", ",", "r", ",", "t", "in", "self", ".", "amr", ".", "edges", ":", "\n", "            ", "if", "s", "==", "-", "1", "and", "r", "==", "\"root\"", ":", "\n", "                ", "assigned_root", "=", "t", "\n", "", "if", "s", "==", "-", "1", "or", "t", "==", "-", "1", ":", "\n", "                ", "root_edges", ".", "append", "(", "(", "s", ",", "r", ",", "t", ")", ")", "\n", "", "", "for", "e", "in", "root_edges", ":", "\n", "            ", "self", ".", "amr", ".", "edges", ".", "remove", "(", "e", ")", "\n", "\n", "", "if", "not", "self", ".", "amr", ".", "nodes", ":", "\n", "            ", "return", "\n", "\n", "", "descendents", "=", "{", "n", ":", "{", "n", "}", "for", "n", "in", "self", ".", "amr", ".", "nodes", "}", "\n", "potential_roots", "=", "[", "n", "for", "n", "in", "self", ".", "amr", ".", "nodes", "]", "\n", "for", "x", ",", "r", ",", "y", "in", "self", ".", "amr", ".", "edges", ":", "\n", "            ", "if", "y", "in", "potential_roots", "and", "x", "not", "in", "descendents", "[", "y", "]", ":", "\n", "                ", "potential_roots", ".", "remove", "(", "y", ")", "\n", "", "descendents", "[", "x", "]", ".", "update", "(", "descendents", "[", "y", "]", ")", "\n", "for", "n", "in", "descendents", ":", "\n", "                ", "if", "x", "in", "descendents", "[", "n", "]", ":", "\n", "                    ", "descendents", "[", "n", "]", ".", "update", "(", "descendents", "[", "x", "]", ")", "\n", "\n", "", "", "", "disconnected", "=", "potential_roots", ".", "copy", "(", ")", "\n", "for", "n", "in", "potential_roots", ".", "copy", "(", ")", ":", "\n", "            ", "if", "len", "(", "[", "e", "for", "e", "in", "self", ".", "amr", ".", "edges", "if", "e", "[", "0", "]", "==", "n", "]", ")", "==", "0", ":", "\n", "                ", "potential_roots", ".", "remove", "(", "n", ")", "\n", "\n", "# assign root", "\n", "", "", "if", "potential_roots", ":", "\n", "            ", "self", ".", "amr", ".", "root", "=", "potential_roots", "[", "0", "]", "\n", "for", "n", "in", "potential_roots", ":", "\n", "                ", "if", "self", ".", "amr", ".", "nodes", "[", "n", "]", "==", "'multi-sentence'", "or", "n", "==", "assigned_root", ":", "\n", "                    ", "self", ".", "amr", ".", "root", "=", "n", "\n", "", "", "disconnected", ".", "remove", "(", "self", ".", "amr", ".", "root", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "amr", ".", "root", "=", "max", "(", "self", ".", "amr", ".", "nodes", ".", "keys", "(", ")", ",", "\n", "key", "=", "lambda", "x", ":", "len", "(", "[", "e", "for", "e", "in", "self", ".", "amr", ".", "edges", "if", "e", "[", "0", "]", "==", "x", "]", ")", "\n", "-", "len", "(", "[", "e", "for", "e", "in", "self", ".", "amr", ".", "edges", "if", "e", "[", "2", "]", "==", "x", "]", ")", ")", "\n", "# connect graph", "\n", "", "if", "len", "(", "disconnected", ")", ">", "0", ":", "\n", "            ", "for", "n", "in", "disconnected", ":", "\n", "                ", "if", "n", "not", "in", "descendents", "[", "self", ".", "amr", ".", "root", "]", ":", "\n", "                    ", "self", ".", "amr", ".", "edges", ".", "append", "(", "(", "self", ".", "amr", ".", "root", ",", "default_rel", ",", "n", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.postprocessing_training": [[1115, 1145], ["gold_amr.alignmentsToken2Node", "gold_amr.findSubGraph", "enumerate", "state_machine.AMRStateMachine.amr.edges.remove", "state_machine.AMRStateMachine.amr.edges.append", "new_node_ids.append", "new_node_ids.append", "gold_amr.alignmentsToken2Node.index", "gold_amr.alignmentsToken2Node.index"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.alignmentsToken2Node", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.findSubGraph", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.index", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.index"], ["", "", "", "", "def", "postprocessing_training", "(", "self", ",", "gold_amr", ")", ":", "\n", "\n", "        ", "for", "entity_id", "in", "self", ".", "entities", ":", "\n", "\n", "            ", "entity_edges", "=", "[", "e", "for", "e", "in", "self", ".", "amr", ".", "edges", "if", "e", "[", "0", "]", "==", "entity_id", "and", "e", "[", "1", "]", "==", "'entity'", "]", "\n", "\n", "for", "e", "in", "entity_edges", ":", "\n", "                ", "self", ".", "amr", ".", "edges", ".", "remove", "(", "e", ")", "\n", "\n", "", "child_id", "=", "[", "t", "for", "s", ",", "r", ",", "t", "in", "entity_edges", "]", "[", "0", "]", "\n", "del", "self", ".", "amr", ".", "nodes", "[", "child_id", "]", "\n", "\n", "new_node_ids", "=", "[", "]", "\n", "\n", "entity_alignment", "=", "gold_amr", ".", "alignmentsToken2Node", "(", "entity_id", ")", "\n", "gold_entity_subgraph", "=", "gold_amr", ".", "findSubGraph", "(", "entity_alignment", ")", "\n", "\n", "for", "i", ",", "n", "in", "enumerate", "(", "entity_alignment", ")", ":", "\n", "                ", "if", "i", "==", "0", ":", "\n", "                    ", "self", ".", "amr", ".", "nodes", "[", "entity_id", "]", "=", "gold_amr", ".", "nodes", "[", "n", "]", "\n", "new_node_ids", ".", "append", "(", "entity_id", ")", "\n", "", "else", ":", "\n", "                    ", "self", ".", "amr", ".", "nodes", "[", "self", ".", "new_id", "]", "=", "gold_amr", ".", "nodes", "[", "n", "]", "\n", "new_node_ids", ".", "append", "(", "self", ".", "new_id", ")", "\n", "self", ".", "new_id", "+=", "1", "\n", "\n", "", "", "for", "s", ",", "r", ",", "t", "in", "gold_entity_subgraph", ".", "edges", ":", "\n", "                ", "new_s", "=", "new_node_ids", "[", "entity_alignment", ".", "index", "(", "s", ")", "]", "\n", "new_t", "=", "new_node_ids", "[", "entity_alignment", ".", "index", "(", "t", ")", "]", "\n", "self", ".", "amr", ".", "edges", ".", "append", "(", "(", "new_s", ",", "r", ",", "new_t", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.postprocessing": [[1146, 1553], ["entity_type.startswith", "state_machine.AMRStateMachine.amr.nodes[].split", "entity_type.split", "enumerate", "open", "json.load", "gold_amr.alignmentsToken2Node", "gold_amr.findSubGraph", "state_machine.AMRStateMachine.amr.edges.remove", "enumerate", "new_concepts.append", "zip", "entity_tokens[].endswith", "enumerate", "str", "enumerate", "enumerate", "entity_type.endswith", "tok.replace.replace.replace", "tok.replace.replace.lower", "state_machine.AMRStateMachine.amr.edges.append", "new_concepts.append", "len", "state_machine.AMRStateMachine.normalize_token", "state_machine.AMRStateMachine.isdigit", "tok.replace.replace.replace", "tok.replace.replace.isdigit", "state_machine.AMRStateMachine.amr.edges.append", "new_concepts.append", "len", "int", "new_concepts.append", "state_machine.AMRStateMachine.amr.edges.append", "srcs.append", "enumerate", "len", "state_machine.AMRStateMachine.normalize_token", "int", "new_concepts.append", "state_machine.AMRStateMachine.amr.edges.append", "srcs.append", "enumerate", "enumerate", "entity_type.split", "entity_type.split.remove", "enumerate", "tok.replace.replace.replace", "state_machine.AMRStateMachine.amr.edges.append", "new_concepts.append", "enumerate", "state_machine.AMRStateMachine.amr.edges.append", "new_concepts.append", "new_concepts.append", "set", "set", "gold_concepts.append", "date.isdigit", "tok.replace.replace.lower", "date_entity_rules.get", "date_entity_rules.get", "tok.replace.replace.lower", "date_entity_rules.get", "tok.replace.replace.lower", "date_entity_rules.get", "enumerate", "tok.replace.replace.lower", "date_entity_rules.get", "tok.replace.replace.startswith", "tok[].isdigit", "tok.replace.replace.lower", "str", "tok.replace.replace.endswith", "tok[].isdigit", "tok.replace.replace.lower", "tok.replace.replace.isdigit", "state_machine.AMRStateMachine.normalize_token", "tok.replace.replace.lower", "set", "set", "new_concepts.index", "set", "set", "tok.replace.replace.lower", "new_concepts.index", "set", "set", "int", "new_concepts.append", "state_machine.AMRStateMachine.amr.edges.append", "srcs.append", "enumerate", "len", "new_concepts.append", "set", "set", "tok.replace.replace.lower", "gold_concepts.append", "len", "date.isdigit", "tok.replace.replace.lower", "date_entity_rules.get", "tok.replace.replace.lower", "date_entity_rules.get", "tok.replace.replace.lower", "tok.replace.replace.lower().endswith", "assigned_edges.index", "entity_tokens[].isdigit", "len", "len", "len", "int", "tok.replace.replace.startswith", "state_machine.AMRStateMachine.startswith", "node_label.isdigit", "new_concepts.index", "len", "new_concepts.append", "state_machine.AMRStateMachine.amr.edges.append", "new_concepts.append", "state_machine.AMRStateMachine.amr.edges.append", "new_concepts.append", "len", "date.replace().isdigit", "date.split", "tok.replace.replace.lower", "entity_tokens[].lower", "tok.replace.replace.replace", "date_entity_rules.get", "len", "len", "entity_tokens[].lower", "int", "int", "int", "len", "len", "tok[].upper", "date.replace().isdigit", "date.split", "tok.replace.replace.lower", "tok.replace.replace.lower", "tok.replace.replace.lower", "tok.replace.replace.lower", "tok.replace.replace.endswith", "tok.replace.replace.endswith", "tok.replace.replace.endswith", "tok.replace.replace.endswith", "tok.replace.replace.lower", "int", "date.replace", "date.lower", "date.replace", "date[].isdigit", "range", "date.endswith", "date.endswith", "date.endswith", "date.endswith", "len", "date[].isalpha"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.alignmentsToken2Node", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.findSubGraph", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.normalize_token", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.normalize_token", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.normalize_token", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.index", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.index", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.index", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.index", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["", "", "", "def", "postprocessing", "(", "self", ",", "gold_amr", ")", ":", "\n", "\n", "# FIXME: All of this below. ", "\n", "\n", "        ", "global", "entity_rules_json", ",", "entity_rule_stats", ",", "entity_rule_totals", ",", "entity_rule_fails", "\n", "assert", "self", ".", "entity_rules_path", ",", "\"you need to provide entity_rules\"", "\n", "if", "not", "entity_rules_json", ":", "\n", "            ", "with", "open", "(", "self", ".", "entity_rules_path", ",", "'r'", ",", "encoding", "=", "'utf8'", ")", "as", "f", ":", "\n", "                ", "entity_rules_json", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "", "", "for", "entity_id", "in", "self", ".", "entities", ":", "\n", "\n", "            ", "if", "entity_id", "not", "in", "self", ".", "amr", ".", "nodes", ":", "\n", "                ", "continue", "\n", "# Test postprocessing ----------------------------", "\n", "", "gold_concepts", "=", "[", "]", "\n", "if", "gold_amr", ":", "\n", "                ", "entity_alignment", "=", "gold_amr", ".", "alignmentsToken2Node", "(", "entity_id", ")", "\n", "gold_entity_subgraph", "=", "gold_amr", ".", "findSubGraph", "(", "entity_alignment", ")", "\n", "for", "n", "in", "gold_entity_subgraph", ".", "nodes", ":", "\n", "                    ", "node", "=", "gold_entity_subgraph", ".", "nodes", "[", "n", "]", "\n", "if", "n", "==", "gold_entity_subgraph", ".", "root", ":", "\n", "                        ", "gold_concepts", ".", "append", "(", "node", ")", "\n", "", "for", "s", ",", "r", ",", "t", "in", "gold_entity_subgraph", ".", "edges", ":", "\n", "                        ", "if", "t", "==", "n", ":", "\n", "                            ", "edge", "=", "r", "\n", "gold_concepts", ".", "append", "(", "edge", "+", "' '", "+", "node", ")", "\n", "# -------------------------------------------", "\n", "\n", "", "", "", "", "new_concepts", "=", "[", "]", "\n", "\n", "entity_type", "=", "self", ".", "amr", ".", "nodes", "[", "entity_id", "]", "\n", "model_entity_alignments", "=", "None", "\n", "if", "entity_id", "in", "self", ".", "alignments", ":", "\n", "                ", "model_entity_alignments", "=", "self", ".", "alignments", "[", "entity_id", "]", "\n", "", "if", "entity_type", ".", "startswith", "(", "'('", ")", ":", "\n", "                ", "entity_type", "=", "entity_type", "[", "1", ":", "-", "1", "]", "\n", "", "entity_edges", "=", "[", "e", "for", "e", "in", "self", ".", "amr", ".", "edges", "if", "e", "[", "0", "]", "==", "entity_id", "and", "e", "[", "1", "]", "==", "'entity'", "]", "\n", "if", "not", "entity_edges", ":", "\n", "                ", "continue", "\n", "\n", "", "child_id", "=", "[", "t", "for", "s", ",", "r", ",", "t", "in", "entity_edges", "]", "[", "0", "]", "\n", "entity_tokens", "=", "self", ".", "amr", ".", "nodes", "[", "child_id", "]", ".", "split", "(", "','", ")", "\n", "\n", "for", "e", "in", "entity_edges", ":", "\n", "                ", "self", ".", "amr", ".", "edges", ".", "remove", "(", "e", ")", "\n", "", "del", "self", ".", "amr", ".", "nodes", "[", "child_id", "]", "\n", "\n", "# date-entity special rules", "\n", "if", "entity_type", "==", "'date-entity'", ":", "\n", "                ", "date_entity_rules", "=", "entity_rules_json", "[", "'date-entity'", "]", "\n", "assigned_edges", "=", "[", "''", "for", "_", "in", "entity_tokens", "]", "\n", "if", "len", "(", "entity_tokens", ")", "==", "1", ":", "\n", "                    ", "date", "=", "entity_tokens", "[", "0", "]", "\n", "if", "date", ".", "isdigit", "(", ")", "and", "len", "(", "date", ")", "==", "8", ":", "\n", "# format yyyymmdd", "\n", "                        ", "entity_tokens", "=", "[", "date", "[", ":", "4", "]", ",", "date", "[", "4", ":", "6", "]", ",", "date", "[", "6", ":", "]", "]", "\n", "assigned_edges", "=", "[", "':year'", ",", "':month'", ",", "':day'", "]", "\n", "", "elif", "date", ".", "isdigit", "(", ")", "and", "len", "(", "date", ")", "==", "6", ":", "\n", "# format yymmdd", "\n", "                        ", "entity_tokens", "=", "[", "date", "[", ":", "2", "]", ",", "date", "[", "2", ":", "4", "]", ",", "date", "[", "4", ":", "]", "]", "\n", "assigned_edges", "=", "[", "':year'", ",", "':month'", ",", "':day'", "]", "\n", "", "elif", "'/'", "in", "date", "and", "date", ".", "replace", "(", "'/'", ",", "''", ")", ".", "isdigit", "(", ")", ":", "\n", "# format mm-dd-yyyy", "\n", "                        ", "entity_tokens", "=", "date", ".", "split", "(", "'/'", ")", "\n", "assigned_edges", "=", "[", "''", "for", "_", "in", "entity_tokens", "]", "\n", "", "elif", "'-'", "in", "date", "and", "date", ".", "replace", "(", "'-'", ",", "''", ")", ".", "isdigit", "(", ")", ":", "\n", "# format mm-dd-yyyy", "\n", "                        ", "entity_tokens", "=", "date", ".", "split", "(", "'-'", ")", "\n", "assigned_edges", "=", "[", "''", "for", "_", "in", "entity_tokens", "]", "\n", "", "elif", "date", ".", "lower", "(", ")", "==", "'tonight'", ":", "\n", "                        ", "entity_tokens", "=", "[", "'night'", ",", "'today'", "]", "\n", "assigned_edges", "=", "[", "':dayperiod'", ",", "':mod'", "]", "\n", "", "elif", "date", "[", "0", "]", ".", "isdigit", "(", ")", "and", "(", "date", ".", "endswith", "(", "'BC'", ")", "or", "date", ".", "endswith", "(", "'AD'", ")", "or", "date", ".", "endswith", "(", "'BCE'", ")", "or", "date", ".", "endswith", "(", "'CE'", ")", ")", ":", "\n", "# 10,000BC", "\n", "                        ", "idx", "=", "0", "\n", "for", "i", "in", "range", "(", "len", "(", "date", ")", ")", ":", "\n", "                            ", "if", "date", "[", "i", "]", ".", "isalpha", "(", ")", ":", "\n", "                                ", "idx", "=", "i", "\n", "", "", "entity_tokens", "=", "[", "date", "[", ":", "idx", "]", ",", "date", "[", "idx", ":", "]", "]", "\n", "assigned_edges", "=", "[", "':year'", ",", "':era'", "]", "\n", "", "", "for", "j", ",", "tok", "in", "enumerate", "(", "entity_tokens", ")", ":", "\n", "                    ", "if", "assigned_edges", "[", "j", "]", ":", "\n", "                        ", "continue", "\n", "", "if", "tok", ".", "lower", "(", ")", "in", "date_entity_rules", ".", "get", "(", "':weekday'", ",", "[", "]", ")", ":", "\n", "                        ", "assigned_edges", "[", "j", "]", "=", "':weekday'", "\n", "continue", "\n", "", "if", "tok", "in", "date_entity_rules", ".", "get", "(", "':timezone'", ",", "[", "]", ")", ":", "\n", "                        ", "assigned_edges", "[", "j", "]", "=", "':timezone'", "\n", "continue", "\n", "", "if", "tok", ".", "lower", "(", ")", "in", "date_entity_rules", ".", "get", "(", "':calendar'", ",", "[", "]", ")", ":", "\n", "                        ", "assigned_edges", "[", "j", "]", "=", "':calendar'", "\n", "if", "tok", ".", "lower", "(", ")", "==", "'lunar'", ":", "\n", "                            ", "entity_tokens", "[", "j", "]", "=", "'moon'", "\n", "", "continue", "\n", "", "if", "tok", ".", "lower", "(", ")", "in", "date_entity_rules", ".", "get", "(", "':dayperiod'", ",", "[", "]", ")", ":", "\n", "                        ", "assigned_edges", "[", "j", "]", "=", "':dayperiod'", "\n", "for", "idx", ",", "tok", "in", "enumerate", "(", "entity_tokens", ")", ":", "\n", "                            ", "if", "tok", ".", "lower", "(", ")", "==", "'this'", ":", "\n", "                                ", "entity_tokens", "[", "idx", "]", "=", "'today'", "\n", "", "elif", "tok", ".", "lower", "(", ")", "==", "'last'", ":", "\n", "                                ", "entity_tokens", "[", "idx", "]", "=", "'yesterday'", "\n", "", "", "idx", "=", "j", "-", "1", "\n", "if", "idx", ">=", "0", "and", "entity_tokens", "[", "idx", "]", ".", "lower", "(", ")", "==", "'one'", ":", "\n", "                            ", "assigned_edges", "[", "idx", "]", "=", "':quant'", "\n", "", "continue", "\n", "", "if", "tok", "in", "date_entity_rules", ".", "get", "(", "':era'", ",", "[", "]", ")", "or", "tok", ".", "lower", "(", ")", "in", "date_entity_rules", ".", "get", "(", "':era'", ",", "[", "]", ")", "or", "(", "'\"'", "in", "tok", "and", "tok", ".", "replace", "(", "'\"'", ",", "''", ")", "in", "date_entity_rules", ".", "get", "(", "':era'", ",", "[", "]", ")", ")", ":", "\n", "                        ", "assigned_edges", "[", "j", "]", "=", "':era'", "\n", "continue", "\n", "", "if", "tok", ".", "lower", "(", ")", "in", "date_entity_rules", ".", "get", "(", "':season'", ",", "[", "]", ")", ":", "\n", "                        ", "assigned_edges", "[", "j", "]", "=", "':season'", "\n", "continue", "\n", "\n", "", "months", "=", "entity_rules_json", "[", "'normalize'", "]", "[", "'months'", "]", "\n", "if", "tok", ".", "lower", "(", ")", "in", "months", "or", "len", "(", "tok", ".", "lower", "(", ")", ")", "==", "4", "and", "tok", ".", "lower", "(", ")", ".", "endswith", "(", "'.'", ")", "and", "tok", ".", "lower", "(", ")", "[", ":", "3", "]", "in", "months", ":", "\n", "                        ", "if", "':month'", "in", "assigned_edges", ":", "\n", "                            ", "idx", "=", "assigned_edges", ".", "index", "(", "':month'", ")", "\n", "if", "entity_tokens", "[", "idx", "]", ".", "isdigit", "(", ")", ":", "\n", "                                ", "assigned_edges", "[", "idx", "]", "=", "':day'", "\n", "", "", "assigned_edges", "[", "j", "]", "=", "':month'", "\n", "continue", "\n", "", "ntok", "=", "self", ".", "normalize_token", "(", "tok", ")", "\n", "if", "ntok", ".", "isdigit", "(", ")", ":", "\n", "                        ", "if", "j", "+", "1", "<", "len", "(", "entity_tokens", ")", "and", "entity_tokens", "[", "j", "+", "1", "]", ".", "lower", "(", ")", "==", "'century'", ":", "\n", "                            ", "assigned_edges", "[", "j", "]", "=", "':century'", "\n", "continue", "\n", "", "if", "1", "<=", "int", "(", "ntok", ")", "<=", "12", "and", "':month'", "not", "in", "assigned_edges", ":", "\n", "                            ", "if", "not", "(", "tok", ".", "endswith", "(", "'th'", ")", "or", "tok", ".", "endswith", "(", "'st'", ")", "or", "tok", ".", "endswith", "(", "'nd'", ")", "or", "tok", ".", "endswith", "(", "'nd'", ")", ")", ":", "\n", "                                ", "assigned_edges", "[", "j", "]", "=", "':month'", "\n", "continue", "\n", "", "", "if", "1", "<=", "int", "(", "ntok", ")", "<=", "31", "and", "':day'", "not", "in", "assigned_edges", ":", "\n", "                            ", "assigned_edges", "[", "j", "]", "=", "':day'", "\n", "continue", "\n", "", "if", "1", "<=", "int", "(", "ntok", ")", "<=", "10001", "and", "':year'", "not", "in", "assigned_edges", ":", "\n", "                            ", "assigned_edges", "[", "j", "]", "=", "':year'", "\n", "continue", "\n", "", "", "if", "tok", ".", "startswith", "(", "\"'\"", ")", "and", "len", "(", "tok", ")", "==", "3", "and", "tok", "[", "1", ":", "]", ".", "isdigit", "(", ")", ":", "\n", "# 'yy", "\n", "                        ", "assigned_edges", "[", "j", "]", "=", "':year'", "\n", "entity_tokens", "[", "j", "]", "=", "tok", "[", "1", ":", "]", "\n", "continue", "\n", "", "decades", "=", "entity_rules_json", "[", "'normalize'", "]", "[", "'decades'", "]", "\n", "if", "tok", ".", "lower", "(", ")", "in", "decades", ":", "\n", "                        ", "assigned_edges", "[", "j", "]", "=", "':decade'", "\n", "entity_tokens", "[", "j", "]", "=", "str", "(", "decades", "[", "tok", ".", "lower", "(", ")", "]", ")", "\n", "continue", "\n", "", "if", "tok", ".", "endswith", "(", "'s'", ")", "and", "len", "(", "tok", ")", ">", "2", "and", "tok", "[", ":", "2", "]", ".", "isdigit", "(", ")", ":", "\n", "                        ", "assigned_edges", "[", "j", "]", "=", "':decade'", "\n", "entity_tokens", "[", "j", "]", "=", "tok", "[", ":", "-", "1", "]", "\n", "continue", "\n", "", "assigned_edges", "[", "j", "]", "=", "':mod'", "\n", "\n", "", "self", ".", "amr", ".", "nodes", "[", "entity_id", "]", "=", "'date-entity'", "\n", "new_concepts", ".", "append", "(", "'date-entity'", ")", "\n", "for", "tok", ",", "rel", "in", "zip", "(", "entity_tokens", ",", "assigned_edges", ")", ":", "\n", "                    ", "if", "tok", ".", "lower", "(", ")", "in", "[", "'-comma-'", ",", "'of'", ",", "'the'", ",", "'in'", ",", "'at'", ",", "'on'", ",", "'century'", ",", "'-'", ",", "'/'", ",", "''", ",", "'('", ",", "')'", ",", "'\"'", "]", ":", "\n", "                        ", "continue", "\n", "", "tok", "=", "tok", ".", "replace", "(", "'\"'", ",", "''", ")", "\n", "if", "rel", "in", "[", "':year'", ",", "':decade'", "]", ":", "\n", "                        ", "year", "=", "tok", "\n", "if", "len", "(", "year", ")", "==", "2", ":", "\n", "                            ", "tok", "=", "'20'", "+", "year", "if", "(", "0", "<=", "int", "(", "year", ")", "<=", "30", ")", "else", "'19'", "+", "year", "\n", "", "", "if", "rel", "in", "[", "':month'", ",", "':day'", "]", "and", "tok", ".", "isdigit", "(", ")", "and", "int", "(", "tok", ")", "==", "0", ":", "\n", "                        ", "continue", "\n", "", "if", "tok", ".", "isdigit", "(", ")", ":", "\n", "                        ", "while", "tok", ".", "startswith", "(", "'0'", ")", "and", "len", "(", "tok", ")", ">", "1", ":", "\n", "                            ", "tok", "=", "tok", "[", "1", ":", "]", "\n", "", "", "if", "rel", "in", "[", "':day'", ",", "':month'", ",", "':year'", ",", "':era'", ",", "':calendar'", ",", "':century'", ",", "':quant'", ",", "':timezone'", "]", ":", "\n", "                        ", "self", ".", "amr", ".", "nodes", "[", "self", ".", "new_id", "]", "=", "self", ".", "normalize_token", "(", "tok", ")", "\n", "", "else", ":", "\n", "                        ", "self", ".", "amr", ".", "nodes", "[", "self", ".", "new_id", "]", "=", "tok", ".", "lower", "(", ")", "\n", "", "self", ".", "amr", ".", "edges", ".", "append", "(", "(", "entity_id", ",", "rel", ",", "self", ".", "new_id", ")", ")", "\n", "self", ".", "alignments", "[", "self", ".", "new_id", "]", "=", "model_entity_alignments", "\n", "new_concepts", ".", "append", "(", "rel", "+", "' '", "+", "self", ".", "amr", ".", "nodes", "[", "self", ".", "new_id", "]", ")", "\n", "self", ".", "new_id", "+=", "1", "\n", "", "if", "gold_amr", "and", "set", "(", "gold_concepts", ")", "==", "set", "(", "new_concepts", ")", ":", "\n", "                    ", "entity_rule_stats", "[", "'date-entity'", "]", "+=", "1", "\n", "", "entity_rule_totals", "[", "'date-entity'", "]", "+=", "1", "\n", "continue", "\n", "\n", "", "rule", "=", "entity_type", "+", "'\\t'", "+", "','", ".", "join", "(", "entity_tokens", ")", ".", "lower", "(", ")", "\n", "# check if singular is in fixed rules", "\n", "if", "rule", "not", "in", "entity_rules_json", "[", "'fixed'", "]", "and", "len", "(", "entity_tokens", ")", "==", "1", "and", "entity_tokens", "[", "0", "]", ".", "endswith", "(", "'s'", ")", ":", "\n", "                ", "rule", "=", "entity_type", "+", "'\\t'", "+", "entity_tokens", "[", "0", "]", "[", ":", "-", "1", "]", "\n", "\n", "# fixed rules", "\n", "", "if", "rule", "in", "entity_rules_json", "[", "'fixed'", "]", ":", "\n", "                ", "edges", "=", "entity_rules_json", "[", "'fixed'", "]", "[", "rule", "]", "[", "'edges'", "]", "\n", "nodes", "=", "entity_rules_json", "[", "'fixed'", "]", "[", "rule", "]", "[", "'nodes'", "]", "\n", "root", "=", "entity_rules_json", "[", "'fixed'", "]", "[", "rule", "]", "[", "'root'", "]", "\n", "id_map", "=", "{", "}", "\n", "for", "j", ",", "n", "in", "enumerate", "(", "nodes", ")", ":", "\n", "                    ", "node_label", "=", "nodes", "[", "n", "]", "\n", "n", "=", "int", "(", "n", ")", "\n", "\n", "id_map", "[", "n", "]", "=", "entity_id", "if", "n", "==", "root", "else", "self", ".", "new_id", "\n", "self", ".", "new_id", "+=", "1", "\n", "self", ".", "amr", ".", "nodes", "[", "id_map", "[", "n", "]", "]", "=", "node_label", "\n", "self", ".", "alignments", "[", "id_map", "[", "n", "]", "]", "=", "model_entity_alignments", "\n", "new_concepts", ".", "append", "(", "node_label", ")", "\n", "", "for", "s", ",", "r", ",", "t", "in", "edges", ":", "\n", "                    ", "self", ".", "amr", ".", "edges", ".", "append", "(", "(", "id_map", "[", "s", "]", ",", "r", ",", "id_map", "[", "t", "]", ")", ")", "\n", "concept", "=", "self", ".", "amr", ".", "nodes", "[", "id_map", "[", "t", "]", "]", "\n", "if", "concept", "in", "new_concepts", ":", "\n", "                        ", "idx", "=", "new_concepts", ".", "index", "(", "concept", ")", "\n", "new_concepts", "[", "idx", "]", "=", "r", "+", "' '", "+", "new_concepts", "[", "idx", "]", "\n", "\n", "# Fixes :rel", "\n", "", "", "leaf", "=", "None", "\n", "srcs", "=", "[", "None", "]", "\n", "for", "s", ",", "r", ",", "t", "in", "edges", ":", "\n", "                    ", "srcs", ".", "append", "(", "s", ")", "\n", "if", "leaf", "in", "srcs", "and", "t", "not", "in", "srcs", ":", "\n", "                        ", "leaf", "=", "t", "\n", "", "", "if", "leaf", "!=", "None", ":", "\n", "                    ", "for", "(", "i", ",", "e", ")", "in", "enumerate", "(", "self", ".", "amr", ".", "edges", ")", ":", "\n", "                        ", "if", "e", "[", "0", "]", "==", "child_id", ":", "\n", "                            ", "self", ".", "amr", ".", "edges", "[", "i", "]", "=", "(", "id_map", "[", "leaf", "]", ",", "e", "[", "1", "]", ",", "e", "[", "2", "]", ")", "\n", "\n", "", "", "", "if", "gold_amr", "and", "set", "(", "gold_concepts", ")", "==", "set", "(", "new_concepts", ")", ":", "\n", "                    ", "entity_rule_stats", "[", "'fixed'", "]", "+=", "1", "\n", "", "else", ":", "\n", "                    ", "entity_rule_fails", "[", "entity_type", "]", "+=", "1", "\n", "", "entity_rule_totals", "[", "'fixed'", "]", "+=", "1", "\n", "continue", "\n", "\n", "", "rule", "=", "entity_type", "+", "'\\t'", "+", "str", "(", "len", "(", "entity_tokens", ")", ")", "\n", "\n", "# variable rules", "\n", "if", "rule", "in", "entity_rules_json", "[", "'var'", "]", ":", "\n", "                ", "edges", "=", "entity_rules_json", "[", "'var'", "]", "[", "rule", "]", "[", "'edges'", "]", "\n", "nodes", "=", "entity_rules_json", "[", "'var'", "]", "[", "rule", "]", "[", "'nodes'", "]", "\n", "root", "=", "entity_rules_json", "[", "'var'", "]", "[", "rule", "]", "[", "'root'", "]", "\n", "node_map", "=", "{", "}", "\n", "ntok", "=", "None", "\n", "for", "i", ",", "tok", "in", "enumerate", "(", "entity_tokens", ")", ":", "\n", "                    ", "ntok", "=", "self", ".", "normalize_token", "(", "tok", ")", "\n", "node_map", "[", "f'X{i}'", "]", "=", "ntok", "if", "not", "ntok", ".", "startswith", "(", "'\"'", ")", "else", "tok", ".", "lower", "(", ")", "\n", "", "id_map", "=", "{", "}", "\n", "for", "j", ",", "n", "in", "enumerate", "(", "nodes", ")", ":", "\n", "                    ", "node_label", "=", "nodes", "[", "n", "]", "\n", "n", "=", "int", "(", "n", ")", "\n", "\n", "id_map", "[", "n", "]", "=", "entity_id", "if", "n", "==", "root", "else", "self", ".", "new_id", "\n", "self", ".", "new_id", "+=", "1", "\n", "self", ".", "amr", ".", "nodes", "[", "id_map", "[", "n", "]", "]", "=", "node_map", "[", "node_label", "]", "if", "node_label", "in", "node_map", "else", "node_label", "\n", "self", ".", "alignments", "[", "id_map", "[", "n", "]", "]", "=", "model_entity_alignments", "\n", "new_concepts", ".", "append", "(", "self", ".", "amr", ".", "nodes", "[", "id_map", "[", "n", "]", "]", ")", "\n", "\n", "", "for", "s", ",", "r", ",", "t", "in", "edges", ":", "\n", "                    ", "node_label", "=", "self", ".", "amr", ".", "nodes", "[", "id_map", "[", "t", "]", "]", "\n", "if", "'date-entity'", "not", "in", "entity_type", "and", "(", "node_label", ".", "isdigit", "(", ")", "or", "node_label", "in", "[", "'many'", ",", "'few'", ",", "'some'", ",", "'multiple'", ",", "'none'", "]", ")", ":", "\n", "                        ", "r", "=", "':quant'", "\n", "", "self", ".", "amr", ".", "edges", ".", "append", "(", "(", "id_map", "[", "s", "]", ",", "r", ",", "id_map", "[", "t", "]", ")", ")", "\n", "concept", "=", "self", ".", "amr", ".", "nodes", "[", "id_map", "[", "t", "]", "]", "\n", "if", "concept", "in", "new_concepts", ":", "\n", "                        ", "idx", "=", "new_concepts", ".", "index", "(", "concept", ")", "\n", "new_concepts", "[", "idx", "]", "=", "r", "+", "' '", "+", "new_concepts", "[", "idx", "]", "\n", "\n", "# Fixes :rel", "\n", "", "", "leaf", "=", "None", "\n", "srcs", "=", "[", "None", "]", "\n", "for", "s", ",", "r", ",", "t", "in", "edges", ":", "\n", "                    ", "srcs", ".", "append", "(", "s", ")", "\n", "if", "leaf", "in", "srcs", "and", "t", "not", "in", "srcs", ":", "\n", "                        ", "leaf", "=", "t", "\n", "", "", "if", "leaf", "!=", "None", ":", "\n", "                    ", "for", "(", "i", ",", "e", ")", "in", "enumerate", "(", "self", ".", "amr", ".", "edges", ")", ":", "\n", "                        ", "if", "e", "[", "0", "]", "==", "child_id", ":", "\n", "                            ", "self", ".", "amr", ".", "edges", "[", "i", "]", "=", "(", "id_map", "[", "leaf", "]", ",", "e", "[", "1", "]", ",", "e", "[", "2", "]", ")", "\n", "\n", "", "", "", "if", "gold_amr", "and", "set", "(", "gold_concepts", ")", "==", "set", "(", "new_concepts", ")", ":", "\n", "                    ", "entity_rule_stats", "[", "'var'", "]", "+=", "1", "\n", "", "else", ":", "\n", "                    ", "entity_rule_fails", "[", "entity_type", "]", "+=", "1", "\n", "", "entity_rule_totals", "[", "'var'", "]", "+=", "1", "\n", "continue", "\n", "\n", "", "rule", "=", "entity_type", "\n", "\n", "# named entities rules", "\n", "if", "entity_type", ".", "endswith", "(", "',name'", ")", "or", "entity_type", "==", "'name'", ":", "\n", "                ", "name_id", "=", "None", "\n", "if", "rule", "in", "entity_rules_json", "[", "'names'", "]", ":", "\n", "                    ", "edges", "=", "entity_rules_json", "[", "'names'", "]", "[", "rule", "]", "[", "'edges'", "]", "\n", "nodes", "=", "entity_rules_json", "[", "'names'", "]", "[", "rule", "]", "[", "'nodes'", "]", "\n", "root", "=", "entity_rules_json", "[", "'names'", "]", "[", "rule", "]", "[", "'root'", "]", "\n", "id_map", "=", "{", "}", "\n", "for", "j", ",", "n", "in", "enumerate", "(", "nodes", ")", ":", "\n", "                        ", "node_label", "=", "nodes", "[", "n", "]", "\n", "n", "=", "int", "(", "n", ")", "\n", "\n", "id_map", "[", "n", "]", "=", "entity_id", "if", "n", "==", "root", "else", "self", ".", "new_id", "\n", "if", "node_label", "==", "'name'", ":", "\n", "                            ", "name_id", "=", "id_map", "[", "n", "]", "\n", "", "self", ".", "new_id", "+=", "1", "\n", "self", ".", "amr", ".", "nodes", "[", "id_map", "[", "n", "]", "]", "=", "node_label", "\n", "self", ".", "alignments", "[", "id_map", "[", "n", "]", "]", "=", "model_entity_alignments", "\n", "new_concepts", ".", "append", "(", "node_label", ")", "\n", "", "for", "s", ",", "r", ",", "t", "in", "edges", ":", "\n", "                        ", "self", ".", "amr", ".", "edges", ".", "append", "(", "(", "id_map", "[", "s", "]", ",", "r", ",", "id_map", "[", "t", "]", ")", ")", "\n", "concept", "=", "self", ".", "amr", ".", "nodes", "[", "id_map", "[", "t", "]", "]", "\n", "if", "concept", "in", "new_concepts", ":", "\n", "                            ", "idx", "=", "new_concepts", ".", "index", "(", "concept", ")", "\n", "new_concepts", "[", "idx", "]", "=", "r", "+", "' '", "+", "new_concepts", "[", "idx", "]", "\n", "\n", "# Fixes rel:", "\n", "", "", "leaf", "=", "None", "\n", "srcs", "=", "[", "None", "]", "\n", "for", "s", ",", "r", ",", "t", "in", "edges", ":", "\n", "                        ", "srcs", ".", "append", "(", "s", ")", "\n", "if", "leaf", "in", "srcs", "and", "t", "not", "in", "srcs", ":", "\n", "                            ", "leaf", "=", "t", "\n", "", "", "if", "leaf", "!=", "None", ":", "\n", "                        ", "for", "(", "i", ",", "e", ")", "in", "enumerate", "(", "self", ".", "amr", ".", "edges", ")", ":", "\n", "                            ", "if", "e", "[", "0", "]", "==", "child_id", ":", "\n", "                                ", "self", ".", "amr", ".", "edges", "[", "i", "]", "=", "(", "id_map", "[", "leaf", "]", ",", "e", "[", "1", "]", ",", "e", "[", "2", "]", ")", "\n", "\n", "", "", "", "", "else", ":", "\n", "                    ", "nodes", "=", "entity_type", ".", "split", "(", "','", ")", "\n", "nodes", ".", "remove", "(", "'name'", ")", "\n", "name_id", "=", "entity_id", "if", "len", "(", "nodes", ")", "==", "0", "else", "self", ".", "new_id", "\n", "self", ".", "amr", ".", "nodes", "[", "name_id", "]", "=", "'name'", "\n", "self", ".", "alignments", "[", "name_id", "]", "=", "model_entity_alignments", "\n", "self", ".", "new_id", "+=", "1", "\n", "if", "len", "(", "nodes", ")", "==", "0", ":", "\n", "                        ", "new_concepts", ".", "append", "(", "'name'", ")", "\n", "", "for", "j", ",", "node", "in", "enumerate", "(", "nodes", ")", ":", "\n", "                        ", "new_id", "=", "entity_id", "if", "j", "==", "0", "else", "self", ".", "new_id", "\n", "self", ".", "amr", ".", "nodes", "[", "new_id", "]", "=", "node", "\n", "self", ".", "alignments", "[", "new_id", "]", "=", "model_entity_alignments", "\n", "if", "j", "==", "0", ":", "\n", "                            ", "new_concepts", ".", "append", "(", "node", ")", "\n", "", "self", ".", "new_id", "+=", "1", "\n", "if", "j", "==", "len", "(", "nodes", ")", "-", "1", ":", "\n", "                            ", "rel", "=", "':name'", "\n", "self", ".", "amr", ".", "edges", ".", "append", "(", "(", "new_id", ",", "rel", ",", "name_id", ")", ")", "\n", "new_concepts", ".", "append", "(", "':name '", "+", "'name'", ")", "\n", "", "else", ":", "\n", "                            ", "rel", "=", "default_rel", "\n", "self", ".", "amr", ".", "edges", ".", "append", "(", "(", "new_id", ",", "rel", ",", "self", ".", "new_id", ")", ")", "\n", "new_concepts", ".", "append", "(", "default_rel", "+", "' '", "+", "self", ".", "amr", ".", "nodes", "[", "new_id", "]", ")", "\n", "\n", "", "", "", "op_idx", "=", "1", "\n", "for", "tok", "in", "entity_tokens", ":", "\n", "                    ", "tok", "=", "tok", ".", "replace", "(", "'\"'", ",", "''", ")", "\n", "if", "tok", "in", "[", "'('", ",", "')'", ",", "''", "]", ":", "\n", "                        ", "continue", "\n", "", "new_tok", "=", "'\"'", "+", "tok", "[", "0", "]", ".", "upper", "(", ")", "+", "tok", "[", "1", ":", "]", "+", "'\"'", "\n", "self", ".", "amr", ".", "nodes", "[", "self", ".", "new_id", "]", "=", "new_tok", "\n", "self", ".", "alignments", "[", "self", ".", "new_id", "]", "=", "model_entity_alignments", "\n", "rel", "=", "f':op{op_idx}'", "\n", "self", ".", "amr", ".", "edges", ".", "append", "(", "(", "name_id", ",", "rel", ",", "self", ".", "new_id", ")", ")", "\n", "new_concepts", ".", "append", "(", "rel", "+", "' '", "+", "new_tok", ")", "\n", "self", ".", "new_id", "+=", "1", "\n", "op_idx", "+=", "1", "\n", "", "if", "gold_amr", "and", "set", "(", "gold_concepts", ")", "==", "set", "(", "new_concepts", ")", ":", "\n", "                    ", "entity_rule_stats", "[", "'names'", "]", "+=", "1", "\n", "", "entity_rule_totals", "[", "'names'", "]", "+=", "1", "\n", "continue", "\n", "\n", "# unknown entity types", "\n", "", "nodes", "=", "entity_type", ".", "split", "(", "','", ")", "\n", "idx", "=", "0", "\n", "prev_id", "=", "None", "\n", "for", "node", "in", "nodes", ":", "\n", "                ", "if", "node", "in", "[", "'('", ",", "')'", ",", "'\"'", ",", "''", "]", ":", "\n", "                    ", "continue", "\n", "", "new_id", "=", "entity_id", "if", "idx", "==", "0", "else", "self", ".", "new_id", "\n", "self", ".", "amr", ".", "nodes", "[", "new_id", "]", "=", "node", "\n", "self", ".", "alignments", "[", "new_id", "]", "=", "model_entity_alignments", "\n", "self", ".", "new_id", "+=", "1", "\n", "if", "idx", ">", "0", ":", "\n", "# Fixes rel:", "\n", "                    ", "for", "(", "i", ",", "e", ")", "in", "enumerate", "(", "self", ".", "amr", ".", "edges", ")", ":", "\n", "                        ", "if", "e", "[", "0", "]", "==", "prev_id", ":", "\n", "                            ", "self", ".", "amr", ".", "edges", "[", "i", "]", "=", "(", "new_id", ",", "e", "[", "1", "]", ",", "e", "[", "2", "]", ")", "\n", "\n", "", "", "self", ".", "amr", ".", "edges", ".", "append", "(", "(", "prev_id", ",", "default_rel", ",", "new_id", ")", ")", "\n", "new_concepts", ".", "append", "(", "default_rel", "+", "' '", "+", "node", ")", "\n", "", "else", ":", "\n", "                    ", "new_concepts", ".", "append", "(", "node", ")", "\n", "", "idx", "+=", "1", "\n", "prev_id", "=", "new_id", "\n", "\n", "", "for", "(", "i", ",", "e", ")", "in", "enumerate", "(", "self", ".", "amr", ".", "edges", ")", ":", "\n", "                ", "if", "e", "[", "0", "]", "==", "child_id", ":", "\n", "                    ", "if", "(", "prev_id", ",", "e", "[", "1", "]", ",", "e", "[", "2", "]", ")", "not", "in", "self", ".", "amr", ".", "edges", ":", "\n", "                        ", "self", ".", "amr", ".", "edges", "[", "i", "]", "=", "(", "prev_id", ",", "e", "[", "1", "]", ",", "e", "[", "2", "]", ")", "\n", "", "else", ":", "\n", "                        ", "del", "self", ".", "amr", ".", "edges", "[", "i", "]", "\n", "\n", "", "", "", "for", "tok", "in", "entity_tokens", ":", "\n", "                ", "tok", "=", "tok", ".", "replace", "(", "'\"'", ",", "''", ")", "\n", "if", "tok", "in", "[", "'('", ",", "')'", ",", "''", "]", ":", "\n", "                    ", "continue", "\n", "", "self", ".", "amr", ".", "nodes", "[", "self", ".", "new_id", "]", "=", "tok", ".", "lower", "(", ")", "\n", "self", ".", "alignments", "[", "new_id", "]", "=", "model_entity_alignments", "\n", "self", ".", "amr", ".", "edges", ".", "append", "(", "(", "prev_id", ",", "default_rel", ",", "self", ".", "new_id", ")", ")", "\n", "new_concepts", ".", "append", "(", "default_rel", "+", "' '", "+", "tok", ".", "lower", "(", ")", ")", "\n", "self", ".", "new_id", "+=", "1", "\n", "", "if", "gold_amr", "and", "set", "(", "gold_concepts", ")", "==", "set", "(", "new_concepts", ")", ":", "\n", "                ", "entity_rule_stats", "[", "'unknown'", "]", "+=", "1", "\n", "", "else", ":", "\n", "                ", "entity_rule_fails", "[", "entity_type", "]", "+=", "1", "\n", "", "entity_rule_totals", "[", "'unknown'", "]", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.normalize_token": [[1554, 1596], ["string.lower", "NUM_RE.match", "string.lower.replace().replace().replace().replace().replace", "str", "string.lower.endswith", "str", "str", "str", "str", "string.lower.endswith", "str", "units.values", "string.endswith", "open", "json.load", "len", "units.values", "string.lower.replace().replace().replace().replace", "string.lower.replace().replace().replace", "string.lower.replace().replace", "string.lower.replace"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load"], ["", "", "def", "normalize_token", "(", "self", ",", "string", ")", ":", "\n", "        ", "global", "entity_rules_json", "\n", "\n", "if", "not", "entity_rules_json", ":", "\n", "            ", "with", "open", "(", "entities_path", ",", "'r'", ",", "encoding", "=", "'utf8'", ")", "as", "f", ":", "\n", "                ", "entity_rules_json", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "", "", "lstring", "=", "string", ".", "lower", "(", ")", "\n", "months", "=", "entity_rules_json", "[", "'normalize'", "]", "[", "'months'", "]", "\n", "units", "=", "entity_rules_json", "[", "'normalize'", "]", "[", "'units'", "]", "\n", "cardinals", "=", "entity_rules_json", "[", "'normalize'", "]", "[", "'cardinals'", "]", "\n", "ordinals", "=", "entity_rules_json", "[", "'normalize'", "]", "[", "'ordinals'", "]", "\n", "\n", "# number or ordinal", "\n", "if", "NUM_RE", ".", "match", "(", "lstring", ")", ":", "\n", "            ", "return", "lstring", ".", "replace", "(", "','", ",", "''", ")", ".", "replace", "(", "'st'", ",", "''", ")", ".", "replace", "(", "'nd'", ",", "''", ")", ".", "replace", "(", "'rd'", ",", "''", ")", ".", "replace", "(", "'th'", ",", "''", ")", "\n", "\n", "# months", "\n", "", "if", "lstring", "in", "months", ":", "\n", "            ", "return", "str", "(", "months", "[", "lstring", "]", ")", "\n", "", "if", "len", "(", "lstring", ")", "==", "4", "and", "lstring", ".", "endswith", "(", "'.'", ")", "and", "lstring", "[", ":", "3", "]", "in", "months", ":", "\n", "            ", "return", "str", "(", "months", "[", "lstring", "[", ":", "3", "]", "]", ")", "\n", "\n", "# cardinal numbers", "\n", "", "if", "lstring", "in", "cardinals", ":", "\n", "            ", "return", "str", "(", "cardinals", "[", "lstring", "]", ")", "\n", "\n", "# ordinal numbers", "\n", "", "if", "lstring", "in", "ordinals", ":", "\n", "            ", "return", "str", "(", "ordinals", "[", "lstring", "]", ")", "\n", "\n", "# unit abbreviations", "\n", "", "if", "lstring", "in", "units", ":", "\n", "            ", "return", "str", "(", "units", "[", "lstring", "]", ")", "\n", "", "if", "lstring", ".", "endswith", "(", "'s'", ")", "and", "lstring", "[", ":", "-", "1", "]", "in", "units", ":", "\n", "            ", "return", "str", "(", "units", "[", "lstring", "[", ":", "-", "1", "]", "]", ")", "\n", "", "if", "lstring", "in", "units", ".", "values", "(", ")", ":", "\n", "            ", "return", "lstring", "\n", "", "if", "string", ".", "endswith", "(", "'s'", ")", "and", "lstring", "[", ":", "-", "1", "]", "in", "units", ".", "values", "(", ")", ":", "\n", "            ", "return", "lstring", "[", ":", "-", "1", "]", "\n", "\n", "", "return", "'\"'", "+", "string", "+", "'\"'", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.DepParsingStateMachine.__init__": [[1600, 1640], ["tuple", "state_machine.DepParsingStateMachine.swaps_left.append", "len", "enumerate"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["    ", "def", "__init__", "(", "self", ",", "tokens", ")", ":", "\n", "\n", "# tokenized sentence", "\n", "#assert tokens[-1] == \"ROOT\"", "\n", "#tokens.pop()", "\n", "        ", "self", ".", "tokens", "=", "tuple", "(", "tokens", ")", "\n", "\n", "# machine state", "\n", "# keep initial positions as token ids (since token type repeat", "\n", "# theselves)", "\n", "self", ".", "stack", "=", "[", "]", "\n", "self", ".", "buffer", "=", "[", "i", "+", "1", "for", "i", ",", "tok", "in", "enumerate", "(", "self", ".", "tokens", ")", "]", "[", ":", ":", "-", "1", "]", "\n", "\n", "#print(self.tokens, len(tokens))", "\n", "#print(tokens)", "\n", "#print(tokens[::-1])", "\n", "\n", "#print(self.buffer)", "\n", "#exit(0)", "\n", "#No incluimos el root", "\n", "#self.buffer[0] = -1", "\n", "\n", "# extra state", "\n", "# store action history", "\n", "self", ".", "actions", "=", "[", "]", "\n", "# FIXME: This has to be yet set", "\n", "self", ".", "is_closed", "=", "False", "\n", "# update counter", "\n", "self", ".", "time_step", "=", "0", "\n", "self", ".", "open_brackets", "=", "0", "\n", "self", ".", "previous_action", "=", "''", "\n", "self", ".", "unary", "=", "0", "\n", "\n", "self", ".", "nts", "=", "[", "]", "\n", "self", ".", "reduced_nodes", "=", "[", "]", "\n", "self", ".", "num_words", "=", "0", "\n", "\n", "self", ".", "swaps_left", "=", "[", "]", "\n", "for", "i", "in", "self", ".", "buffer", ":", "\n", "            ", "self", ".", "swaps_left", ".", "append", "(", "len", "(", "self", ".", "buffer", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.DepParsingStateMachine.__str__": [[1644, 1697], ["state_machine.green_font", "enumerate", "state_machine.green_font", "len", "mask_view.append", "pointer_view.append", "enumerate", "state_machine.DepParsingStateMachine.stack.index", "pointer_view.append", "pointer_view.append", "state_machine.stack_style", "state_machine.reduced_style", "len", "state_machine.DepParsingStateMachine.stack.index", "len"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.green_font", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.green_font", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.index", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.stack_state_machine.stack_style", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.reduced_style", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.index"], ["", "", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "\"\"\"Command line styling\"\"\"", "\n", "\n", "display_str", "=", "\"\"", "\n", "\n", "# Actions", "\n", "action_title", "=", "green_font", "(", "\"# Actions:\"", ")", "\n", "action_str", "=", "' '", ".", "join", "(", "[", "a", "for", "a", "in", "self", ".", "actions", "]", ")", "\n", "display_str", "+=", "f'{action_title}\\n{action_str}\\n\\n'", "\n", "\n", "# mask view", "\n", "# legacy positioning system", "\n", "token_positions", "=", "[", "i", "+", "1", "for", "i", ",", "tok", "in", "enumerate", "(", "self", ".", "tokens", ")", "]", "[", ":", ":", "-", "1", "]", "\n", "token_positions", "[", "0", "]", "=", "-", "1", "\n", "\n", "mask_view", "=", "[", "]", "\n", "pointer_view", "=", "[", "]", "\n", "for", "idx", ",", "position", "in", "enumerate", "(", "token_positions", ")", ":", "\n", "\n", "# token", "\n", "            ", "token", "=", "self", ".", "tokens", "[", "-", "(", "idx", "+", "1", ")", "]", "\n", "len_token", "=", "len", "(", "token", ")", "\n", "\n", "# color depending on position", "\n", "if", "position", "in", "self", ".", "buffer", ":", "\n", "                ", "token", "=", "token", "+", "' '", "\n", "", "elif", "position", "in", "self", ".", "stack", ":", "\n", "                ", "token", "=", "stack_style", "(", "token", ",", "False", ")", "+", "' '", "\n", "", "else", ":", "\n", "                ", "token", "=", "reduced_style", "(", "token", ")", "+", "' '", "\n", "\n", "# position cursor", "\n", "", "if", "(", "\n", "position", "in", "self", ".", "stack", "and", "\n", "self", ".", "stack", ".", "index", "(", "position", ")", "==", "len", "(", "self", ".", "stack", ")", "-", "1", "\n", ")", ":", "\n", "                ", "pointer_view", ".", "append", "(", "'_'", "*", "len_token", "+", "' '", ")", "\n", "", "elif", "(", "\n", "position", "in", "self", ".", "stack", "and", "\n", "self", ".", "stack", ".", "index", "(", "position", ")", "==", "len", "(", "self", ".", "stack", ")", "-", "2", "\n", ")", ":", "\n", "                ", "pointer_view", ".", "append", "(", "'-'", "*", "len_token", "+", "' '", ")", "\n", "", "else", ":", "\n", "                ", "pointer_view", ".", "append", "(", "' '", "*", "len_token", "+", "' '", ")", "\n", "", "mask_view", ".", "append", "(", "token", ")", "\n", "\n", "# update display str", "\n", "", "title", "=", "green_font", "(", "\"# Buffer/Stack/Reduced:\"", ")", "\n", "pointer_view_str", "=", "\"\"", ".", "join", "(", "pointer_view", "[", ":", ":", "-", "1", "]", ")", "\n", "mask_view_str", "=", "\"\"", ".", "join", "(", "mask_view", "[", ":", ":", "-", "1", "]", ")", "\n", "display_str", "+=", "f'{title}\\n{pointer_view_str}\\n{mask_view_str}\\n\\n'", "\n", "\n", "return", "display_str", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.DepParsingStateMachine.get_buffer_stack_copy": [[1698, 1701], ["list", "list"], "methods", ["None"], ["", "def", "get_buffer_stack_copy", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return copy of buffer and stack\"\"\"", "\n", "return", "list", "(", "self", ".", "buffer", ")", ",", "list", "(", "self", ".", "stack", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.DepParsingStateMachine.applyAction": [[1702, 1811], ["state_machine.DepParsingStateMachine.actions.append", "action.split", "state_machine.DepParsingStateMachine.stack.append", "state_machine.DepParsingStateMachine.buffer.pop", "state_machine.DepParsingStateMachine.nts.append", "state_machine.DepParsingStateMachine.buffer.append", "action.split", "state_machine.DepParsingStateMachine.stack.pop", "state_machine.DepParsingStateMachine.nts.pop", "action.split", "state_machine.DepParsingStateMachine.stack.pop", "int", "state_machine.DepParsingStateMachine.reduced_nodes.append", "action.split", "print", "Exception", "print", "exit"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["", "def", "applyAction", "(", "self", ",", "action", ")", ":", "\n", "        ", "\"\"\"alias for compatibility\"\"\"", "\n", "\n", "base_action", "=", "action", ".", "split", "(", "'('", ")", "[", "0", "]", "\n", "\n", "\"\"\"\n        if base_action == 'SHIFT':\n\n            if self.buffer == []:\n                # shift on empty buffer closes machine\n                self.is_closed = True\n                action = \"SHIFT\" \n            else:    \n                # move one elements from stack to buffer\n                self.stack.append(self.buffer.pop())\n                # if shifted_pos is not None: #?\n                # action = \"%s(%s)\" % (action, shifted_pos)\n        \"\"\"", "\n", "\n", "\"\"\"\n        print(self.previous_action)\n        print('STACK',self.stack)\n        print('BUFFER',self.buffer)\n        print('NTS',self.nts)\n        print('REDUCED NODES',self.reduced_nodes)\n        print(self.time_step)\n        #print('UNARY',self.unary)\n        print('OPEN',self.open_brackets)\n        print(self.actions)\n        print(self.num_words)\n        \"\"\"", "\n", "\n", "if", "base_action", "==", "'SHIFT'", ":", "\n", "            ", "self", ".", "stack", ".", "append", "(", "self", ".", "buffer", ".", "pop", "(", ")", ")", "\n", "self", ".", "previous_action", "=", "'SHIFT'", "\n", "self", ".", "unary", "=", "0", "\n", "self", ".", "num_words", "+=", "1", "\n", "\n", "", "elif", "base_action", "==", "'NT'", ":", "\n", "            ", "self", ".", "nts", ".", "append", "(", "self", ".", "stack", "[", "-", "1", "]", ")", "\n", "self", ".", "previous_action", "=", "'NT'", "\n", "self", ".", "open_brackets", "+=", "1", "\n", "self", ".", "num_words", "=", "0", "\n", "\n", "", "elif", "action", ".", "split", "(", "'('", ")", "[", "0", "]", "==", "'SWAP'", ":", "\n", "            ", "self", ".", "previous_action", "=", "'SWAP'", "\n", "self", ".", "num_words", "-=", "1", "\n", "self", ".", "buffer", ".", "append", "(", "self", ".", "stack", ".", "pop", "(", "-", "2", ")", ")", "\n", "self", ".", "swaps_left", "[", "int", "(", "self", ".", "buffer", "[", "-", "1", "]", ")", "-", "1", "]", "-=", "1", "\n", "\n", "#elif base_action == 'RE': #'LEFT-ARC':", "\n", "# remove second element in stack from the top", "\n", "# remove first element in stack from the top", "\n", "#dependent = self.stack.pop(-2)", "\n", "# close machine if LA(root)", "\n", "#if action == 'LEFT-ARC(root)':", "\n", "#    self.is_closed = True", "\n", "\n", "", "elif", "action", ".", "split", "(", "'('", ")", "[", "0", "]", "==", "'RE'", ":", "\n", "            ", "self", ".", "open_brackets", "-=", "1", "\n", "if", "self", ".", "previous_action", "==", "'RE'", ":", "\n", "                ", "self", ".", "unary", "=", "0", "\n", "", "if", "self", ".", "previous_action", "==", "'NT'", ":", "\n", "                ", "self", ".", "unary", "+=", "1", "\n", "", "self", ".", "previous_action", "=", "'RE'", "\n", "# remove first element in stack from the top", "\n", "last_nt", "=", "self", ".", "nts", ".", "pop", "(", ")", "\n", "self", ".", "num_words", "=", "0", "\n", "while", "True", ":", "\n", "#dependent = self.stack.pop()", "\n", "#if stack[-1].split('(')[0] == 'NT':", "\n", "#print('AA',self.stack[-1],last_nt)", "\n", "                ", "if", "self", ".", "stack", "[", "-", "1", "]", "==", "last_nt", ":", "\n", "                    ", "dependent", "=", "self", ".", "stack", "[", "-", "1", "]", "\n", "self", ".", "reduced_nodes", ".", "append", "(", "self", ".", "stack", "[", "-", "1", "]", ")", "\n", "#self.num_words-=1", "\n", "#nt = self.stack.pop()", "\n", "#dependent = self.stack.pop()", "\n", "#self.stack.append(nt)", "\n", "break", "\n", "", "dependent", "=", "self", ".", "stack", ".", "pop", "(", ")", "\n", "#self.num_words-=1", "\n", "\n", "#elif action.split('(')[0] == 'SWAP':", "\n", "# set element 1 of the stack to 0 of the buffer", "\n", "#    self.buffer.append(self.stack.pop(1))", "\n", "\n", "", "", "elif", "action", ".", "split", "(", "'('", ")", "[", "0", "]", "==", "'TERM'", ":", "\n", "            ", "self", ".", "previous_action", "=", "'TERM'", "\n", "self", ".", "is_closed", "=", "True", "\n", "#exit(0)", "\n", "", "elif", "action", "==", "'</s>'", ":", "\n", "            ", "if", "self", ".", "is_closed", "and", "self", ".", "previous_action", "!=", "'TERM'", ":", "\n", "                ", "print", "(", "'CLOSED WITHOUT TERM'", ")", "\n", "exit", "(", "0", ")", "\n", "# FIXME: Why is this needed now? It was not before", "\n", "", "pass", "\n", "", "else", ":", "\n", "            ", "print", "(", "action", ",", "base_action", ")", "\n", "raise", "Exception", "(", "\"Invalid action %s\"", "%", "action", ")", "\n", "\n", "\n", "# store action history", "\n", "", "self", ".", "actions", ".", "append", "(", "action", ")", "\n", "\n", "# update counter", "\n", "self", ".", "time_step", "+=", "1", "\n", "\n", "return", "action", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.DepParsingStateMachine.get_valid_actions": [[1812, 1858], ["valid_actions.append", "valid_actions.append", "valid_actions.append", "valid_actions.append", "valid_actions.append", "valid_actions.append", "len", "print", "exit", "len", "len", "len", "len", "len", "len", "len", "len", "len", "int"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["", "def", "get_valid_actions", "(", "self", ")", ":", "\n", "\n", "\n", "#if self.is_closed and self.previous_action!='TERM': exit(0)", "\n", "\n", "# Quick exit for a closed machine", "\n", "        ", "if", "self", ".", "is_closed", ":", "\n", "            ", "return", "[", "'</s>'", "]", ",", "[", "]", "\n", "\n", "# if top of the stack contains <ROOT> only LA(root allowed)", "\n", "#if self.stack and self.stack[-1] == -1:", "\n", "#    return ['LEFT-ARC(root)'], []", "\n", "\n", "\n", "", "if", "len", "(", "self", ".", "stack", ")", "==", "0", "and", "len", "(", "self", ".", "buffer", ")", ">", "0", ":", "return", "[", "'SHIFT'", "]", ",", "[", "]", "\n", "\n", "# multiple actions possible", "\n", "valid_actions", "=", "[", "]", "\n", "if", "len", "(", "self", ".", "buffer", ")", ">", "0", "and", "self", ".", "open_brackets", ">", "0", ":", "\n", "            ", "valid_actions", ".", "append", "(", "'SHIFT'", ")", "\n", "\n", "", "if", "len", "(", "self", ".", "buffer", ")", ">", "0", "and", "self", ".", "previous_action", "!=", "'NT'", ":", "\n", "            ", "valid_actions", ".", "append", "(", "'NT'", ")", "\n", "\n", "", "if", "len", "(", "self", ".", "buffer", ")", "==", "0", "and", "(", "self", ".", "unary", "<", "3", "and", "self", ".", "previous_action", "!=", "'NT'", ")", ":", "\n", "            ", "valid_actions", ".", "append", "(", "'NT'", ")", "\n", "\n", "", "if", "len", "(", "self", ".", "stack", ")", ">=", "1", "and", "self", ".", "unary", "<", "4", "and", "self", ".", "open_brackets", ">", "0", ":", "\n", "            ", "valid_actions", ".", "append", "(", "'RE'", ")", "\n", "\n", "\n", "", "if", "len", "(", "self", ".", "stack", ")", ">", "1", "and", "self", ".", "num_words", ">", "1", "and", "self", ".", "stack", "[", "-", "1", "]", "not", "in", "self", ".", "reduced_nodes", "and", "self", ".", "stack", "[", "-", "1", "]", "not", "in", "self", ".", "nts", "and", "self", ".", "stack", "[", "-", "2", "]", "not", "in", "self", ".", "reduced_nodes", "and", "self", ".", "stack", "[", "-", "2", "]", "not", "in", "self", ".", "nts", "and", "self", ".", "swaps_left", "[", "int", "(", "self", ".", "stack", "[", "-", "2", "]", ")", "-", "1", "]", ">", "0", ":", "\n", "#print(self.swaps_left)", "\n", "            ", "valid_actions", ".", "append", "(", "'SWAP'", ")", "\n", "\n", "\n", "", "if", "len", "(", "self", ".", "stack", ")", "==", "1", "and", "len", "(", "self", ".", "buffer", ")", "==", "0", "and", "self", ".", "previous_action", "==", "'RE'", ":", "\n", "            ", "valid_actions", ".", "append", "(", "'TERM'", ")", "\n", "\n", "#print('VALID',valid_actions)", "\n", "", "if", "len", "(", "valid_actions", ")", "==", "0", ":", "\n", "            ", "print", "(", "'THERE ARE NO VALID ACTIONS'", ")", "\n", "exit", "(", "0", ")", "\n", "\n", "", "return", "valid_actions", ",", "[", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.white_background": [[40, 42], ["None"], "function", ["None"], ["def", "white_background", "(", "string", ")", ":", "\n", "    ", "return", "\"\\033[107m%s\\033[0m\"", "%", "string", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.red_background": [[44, 46], ["None"], "function", ["None"], ["", "def", "red_background", "(", "string", ")", ":", "\n", "    ", "return", "\"\\033[101m%s\\033[0m\"", "%", "string", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.green_background": [[48, 50], ["None"], "function", ["None"], ["", "def", "green_background", "(", "string", ")", ":", "\n", "    ", "return", "\"\\033[102m%s\\033[0m\"", "%", "string", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.black_font": [[52, 54], ["None"], "function", ["None"], ["", "def", "black_font", "(", "string", ")", ":", "\n", "    ", "return", "\"\\033[30m%s\\033[0m\"", "%", "string", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.blue_font": [[56, 58], ["None"], "function", ["None"], ["", "def", "blue_font", "(", "string", ")", ":", "\n", "    ", "return", "\"\\033[94m%s\\033[0m\"", "%", "string", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.green_font": [[60, 62], ["None"], "function", ["None"], ["", "def", "green_font", "(", "string", ")", ":", "\n", "    ", "return", "\"\\033[92m%s\\033[0m\"", "%", "string", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.yellow_font": [[64, 66], ["None"], "function", ["None"], ["", "def", "yellow_font", "(", "string", ")", ":", "\n", "    ", "return", "\"\\033[93m%s\\033[0m\"", "%", "string", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.stack_style": [[68, 73], ["state_machine.black_font", "state_machine.black_font", "state_machine.green_background", "state_machine.white_background"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.stack_state_machine.black_font", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.stack_state_machine.black_font", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.green_background", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.stack_state_machine.white_background"], ["", "def", "stack_style", "(", "string", ",", "confirmed", "=", "False", ")", ":", "\n", "    ", "if", "confirmed", ":", "\n", "        ", "return", "black_font", "(", "green_background", "(", "string", ")", ")", "\n", "", "else", ":", "\n", "        ", "return", "black_font", "(", "white_background", "(", "string", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.reduced_style": [[75, 77], ["state_machine.black_font", "state_machine.red_background"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.stack_state_machine.black_font", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.stack_state_machine.red_background"], ["", "", "def", "reduced_style", "(", "string", ")", ":", "\n", "    ", "return", "black_font", "(", "red_background", "(", "string", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.get_forbidden_actions": [[79, 170], ["re.compile", "re.compile", "re.compile", "len", "state_machine.yellow_font", "print", "invalid_actions.append", "len", "re.compile.match", "invalid_actions.append", "len", "invalid_actions.append", "re.compile.match", "invalid_actions.append", "invalid_actions.append", "len", "invalid_actions.append", "invalid_actions.append", "re.compile.match", "invalid_actions.append", "invalid_actions.append", "len", "invalid_actions.append", "invalid_actions.append", "edge.split", "invalid_actions.append", "invalid_actions.append", "edge.split"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.yellow_font", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["", "def", "get_forbidden_actions", "(", "stack", ",", "amr", ")", ":", "\n", "    ", "'''\n    Return actions that create already existing edges (to forbid them)\n    '''", "\n", "\n", "if", "len", "(", "stack", ")", "==", "0", ":", "\n", "        ", "return", "[", "]", "\n", "\n", "# Regex for ARGs", "\n", "", "unique_re", "=", "re", ".", "compile", "(", "r'^(snt|op)([0-9]+)$'", ")", "\n", "arg_re", "=", "re", ".", "compile", "(", "r'^ARG([0-9]+)$'", ")", "\n", "argof_re", "=", "re", ".", "compile", "(", "r'^ARG([0-9]+-of)$'", ")", "\n", "\n", "invalid_actions", "=", "[", "]", "\n", "for", "t", "in", "amr", ".", "edges", ":", "\n", "\n", "# if we find an edge in the list of non repeatable edges, check if", "\n", "# this parent has already had one such edge and forbid repetition in", "\n", "# that case. ", "\n", "\n", "        ", "head_id", "=", "t", "[", "0", "]", "\n", "edge", "=", "t", "[", "1", "]", "[", "1", ":", "]", "\n", "child_id", "=", "t", "[", "2", "]", "\n", "\n", "# FIXME: There is some bug somewhere by which edges are pointing to", "\n", "# unexisting nodes. We skip those cases", "\n", "if", "head_id", "not", "in", "amr", ".", "nodes", "or", "child_id", "not", "in", "amr", ".", "nodes", ":", "\n", "            ", "warning", "=", "yellow_font", "(", "'WARNING'", ")", "\n", "print", "(", "f'{warning}: Edge node id missing from amr.nodes'", ")", "\n", "continue", "\n", "\n", "# info about this edge", "\n", "", "head", "=", "amr", ".", "nodes", "[", "head_id", "]", "\n", "child", "=", "amr", ".", "nodes", "[", "child_id", "]", "\n", "\n", "# unique constants", "\n", "# this edge label can not be repeated with same child name", "\n", "# note that DEPENDENT can be used as well as RA/LA", "\n", "if", "edge", "in", "[", "'polarity'", ",", "'mode'", "]", ":", "\n", "            ", "if", "head_id", "==", "stack", "[", "-", "1", "]", ":", "\n", "# DEPENDENT", "\n", "                ", "invalid_actions", ".", "append", "(", "f'DEPENDENT({child},{edge})'", ")", "\n", "", "if", "len", "(", "stack", ")", ">", "1", ":", "\n", "                ", "if", "head_id", "==", "stack", "[", "-", "1", "]", "and", "(", "child", "==", "amr", ".", "nodes", "[", "stack", "[", "-", "2", "]", "]", ")", ":", "\n", "# LA (stack1 <-- stack0)", "\n", "                    ", "invalid_actions", ".", "append", "(", "f'LA({edge})'", ")", "\n", "", "elif", "head_id", "==", "stack", "[", "-", "2", "]", "and", "(", "child", "==", "amr", ".", "nodes", "[", "stack", "[", "-", "1", "]", "]", ")", ":", "\n", "# RA (stack1 --> stack0)            ", "\n", "                    ", "invalid_actions", ".", "append", "(", "f'RA({edge})'", ")", "\n", "\n", "# snt[0-9] op[0-9] ", "\n", "# this edge label can not be repeated regardless of child label", "\n", "", "", "", "elif", "unique_re", ".", "match", "(", "edge", ")", "and", "len", "(", "stack", ")", ">", "1", ":", "\n", "            ", "if", "head_id", "==", "stack", "[", "-", "1", "]", ":", "\n", "# Left Arcs (stack1 <-- stack0)", "\n", "                ", "invalid_actions", ".", "append", "(", "f'LA({edge})'", ")", "\n", "", "elif", "head_id", "==", "stack", "[", "-", "2", "]", ":", "\n", "# Right Arcs (stack1 --> stack0)            ", "\n", "                ", "invalid_actions", ".", "append", "(", "f'RA({edge})'", ")", "\n", "\n", "# ARG[0-9]", "\n", "# this edge label can not be repeated regardless of child label", "\n", "# watch for reverse ARG-of arcs", "\n", "", "", "elif", "arg_re", ".", "match", "(", "edge", ")", "and", "len", "(", "stack", ")", ">", "1", ":", "\n", "            ", "if", "head_id", "==", "stack", "[", "-", "1", "]", ":", "\n", "# Left Arcs (stack1 <-- stack0)", "\n", "                ", "invalid_actions", ".", "append", "(", "f'LA({edge})'", ")", "\n", "invalid_actions", ".", "append", "(", "f'RA({edge}-of)'", ")", "\n", "", "elif", "head_id", "==", "stack", "[", "-", "2", "]", ":", "\n", "# Right Arcs (stack1 --> stack0)            ", "\n", "                ", "invalid_actions", ".", "append", "(", "f'RA({edge})'", ")", "\n", "invalid_actions", ".", "append", "(", "f'LA({edge}-of)'", ")", "\n", "\n", "# ARG[0-9]-of ", "\n", "# this edge label can not be repeated regardless of parent label", "\n", "# watch for direct ARG arcs", "\n", "# NOTE: father and child roles reverse wrt ARG[0-9]", "\n", "", "", "elif", "argof_re", ".", "match", "(", "edge", ")", "and", "len", "(", "stack", ")", ">", "1", ":", "\n", "            ", "if", "child_id", "==", "stack", "[", "-", "1", "]", ":", "\n", "# Left Arcs (stack1 <-- stack0)", "\n", "                ", "argn", "=", "edge", ".", "split", "(", "'-'", ")", "[", "0", "]", "\n", "invalid_actions", ".", "append", "(", "f'LA({argn})'", ")", "\n", "invalid_actions", ".", "append", "(", "f'RA({edge})'", ")", "\n", "\n", "", "elif", "child_id", "==", "stack", "[", "-", "2", "]", ":", "\n", "# Right Arcs (stack1 --> stack0)            ", "\n", "                ", "argn", "=", "edge", ".", "split", "(", "'-'", ")", "[", "0", "]", "\n", "invalid_actions", ".", "append", "(", "f'RA({argn})'", ")", "\n", "invalid_actions", ".", "append", "(", "f'LA({edge})'", ")", "\n", "\n", "", "", "", "return", "invalid_actions", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.get_spacy_lemmatizer": [[181, 193], ["state_machine.NoTokenizer", "spacy.load", "download", "spacy.load"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load"], ["", "", "def", "get_spacy_lemmatizer", "(", ")", ":", "\n", "# TODO: Unclear why this configuration", "\n", "# from spacy.cli.download import download", "\n", "    ", "try", ":", "\n", "        ", "lemmatizer", "=", "spacy", ".", "load", "(", "'en'", ",", "disable", "=", "[", "'parser'", ",", "'ner'", "]", ")", "\n", "", "except", "OSError", ":", "\n", "# Assume the problem was the spacy models were not downloaded", "\n", "        ", "from", "spacy", ".", "cli", ".", "download", "import", "download", "\n", "download", "(", "'en'", ")", "\n", "lemmatizer", "=", "spacy", ".", "load", "(", "'en'", ",", "disable", "=", "[", "'parser'", ",", "'ner'", "]", ")", "\n", "", "lemmatizer", ".", "tokenizer", "=", "NoTokenizer", "(", "lemmatizer", ".", "vocab", ")", "\n", "return", "lemmatizer", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.get_graph_str": [[195, 233], ["alignments.items", "collections.defaultdict", "collections.defaultdict", "child2parents[].append", "parent2child[].append", "nodes.append", "len", "parent2child[].pop", "len", "state_machine.blue_font", "path.append", "path.pop"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.blue_font", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop"], ["", "def", "get_graph_str", "(", "amr", ",", "alignments", ")", ":", "\n", "\n", "# nodes", "\n", "    ", "graph_str", "=", "''", "\n", "nodes", "=", "[", "]", "\n", "for", "stack0", ",", "token_pos", "in", "alignments", ".", "items", "(", ")", ":", "\n", "        ", "if", "stack0", "in", "amr", ".", "nodes", ":", "\n", "            ", "nodes", ".", "append", "(", "stack0", ")", "\n", "\n", "# directed edges", "\n", "", "", "child2parents", "=", "defaultdict", "(", "list", ")", "\n", "parent2child", "=", "defaultdict", "(", "list", ")", "\n", "edge_labels", "=", "{", "}", "\n", "for", "parent", ",", "label", ",", "child", "in", "amr", ".", "edges", ":", "\n", "        ", "child2parents", "[", "child", "]", ".", "append", "(", "parent", ")", "\n", "parent2child", "[", "parent", "]", ".", "append", "(", "child", ")", "\n", "edge_labels", "[", "(", "parent", ",", "child", ")", "]", "=", "label", "\n", "\n", "# root nodes", "\n", "", "root_nodes", "=", "[", "node", "for", "node", "in", "nodes", "if", "node", "not", "in", "child2parents", "]", "\n", "\n", "# transverse depth first and print graph", "\n", "pad", "=", "'    '", "\n", "for", "node", "in", "root_nodes", ":", "\n", "        ", "graph_str", "+=", "f'{amr.nodes[node]}\\n'", "\n", "path", "=", "[", "node", "]", "\n", "while", "path", ":", "\n", "            ", "if", "len", "(", "parent2child", "[", "path", "[", "-", "1", "]", "]", ")", ":", "\n", "                ", "new_node", "=", "parent2child", "[", "path", "[", "-", "1", "]", "]", ".", "pop", "(", ")", "\n", "depth", "=", "len", "(", "path", ")", "\n", "edge", "=", "blue_font", "(", "edge_labels", "[", "(", "path", "[", "-", "1", "]", ",", "new_node", ")", "]", ")", "\n", "graph_str", "+=", "f'{pad*depth} {edge} {amr.nodes[new_node]}\\n'", "\n", "path", ".", "append", "(", "new_node", ")", "\n", "", "else", ":", "\n", "# leaf found", "\n", "                ", "path", ".", "pop", "(", ")", "\n", "\n", "", "", "", "return", "graph_str", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.learn.argument_parser": [[32, 110], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "print", "range", "print"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["def", "argument_parser", "(", ")", ":", "\n", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'AMR parser.'", ")", "\n", "parser", ".", "add_argument", "(", "\"-A\"", ",", "\"--amr_training_data\"", ",", "required", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "\"-a\"", ",", "\"--amr_dev_data\"", ",", "required", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "\"-s\"", ",", "\"--oracle_stats\"", ",", "required", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'--epoch'", ",", "type", "=", "int", ",", "default", "=", "300", ",", "help", "=", "'maximum epoch number'", ")", "\n", "parser", ".", "add_argument", "(", "'--report'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "'after how many epochs should the model evaluate on dev data?'", ")", "\n", "parser", ".", "add_argument", "(", "'--max_train_size'", ",", "type", "=", "int", ",", "help", "=", "'number of sentences to train on (for debugging purposes)'", ")", "\n", "parser", ".", "add_argument", "(", "'--max_dev_size'", ",", "type", "=", "int", ",", "help", "=", "'number of sentences to evaluate on (for debugging purposes)'", ")", "\n", "parser", ".", "add_argument", "(", "'--name'", ",", "help", "=", "'name of experiment to associate with all output (default: process id)'", ")", "\n", "parser", ".", "add_argument", "(", "'--desc'", ",", "help", "=", "'description of experiment; to be printed at the top of smatch file'", ")", "\n", "parser", ".", "add_argument", "(", "'--start_epoch'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "help", "=", "'start epoch idx'", ")", "\n", "parser", ".", "add_argument", "(", "'--write_actions'", ",", "action", "=", "'store_true'", ",", "help", "=", "'whether to output predicted actions'", ")", "\n", "parser", ".", "add_argument", "(", "'--write_gold_actions'", ",", "help", "=", "'file to output gold actions'", ")", "\n", "parser", ".", "add_argument", "(", "'--write_rule_stats'", ",", "help", "=", "'file to output rule stats'", ")", "\n", "parser", ".", "add_argument", "(", "'--read_gold_actions'", ",", "help", "=", "'use gold actions from this file'", ")", "\n", "parser", ".", "add_argument", "(", "'--load_model'", ",", "help", "=", "'use parameters file to initialize modal'", ")", "\n", "parser", ".", "add_argument", "(", "'--confusion'", ",", "action", "=", "'store_true'", ",", "help", "=", "'write confusion matrix'", ")", "\n", "parser", ".", "add_argument", "(", "'--gpu'", ",", "action", "=", "'store_true'", ",", "help", "=", "'use GPU processor'", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "type", "=", "int", ",", "help", "=", "'random seed'", ")", "\n", "parser", ".", "add_argument", "(", "'--test_mode'", ",", "action", "=", "'store_true'", ",", "help", "=", "'do not train'", ")", "\n", "parser", ".", "add_argument", "(", "'--save_model'", ",", "help", "=", "'store model and metrics here'", ")", "\n", "# hyperparams", "\n", "parser", ".", "add_argument", "(", "'--dim_emb'", ",", "type", "=", "int", ",", "default", "=", "100", ",", "help", "=", "'embedding dim'", ")", "\n", "parser", ".", "add_argument", "(", "'--dim_action'", ",", "type", "=", "int", ",", "default", "=", "100", ",", "help", "=", "'action embedding dim'", ")", "\n", "parser", ".", "add_argument", "(", "'--dim_char_emb'", ",", "type", "=", "int", ",", "default", "=", "50", ",", "help", "=", "'char embedding dim'", ")", "\n", "parser", ".", "add_argument", "(", "'--dim_hidden'", ",", "type", "=", "int", ",", "default", "=", "100", ",", "help", "=", "'hidden dim'", ")", "\n", "parser", ".", "add_argument", "(", "'--dim_char_hidden'", ",", "type", "=", "int", ",", "default", "=", "50", ",", "help", "=", "'char hidden dim'", ")", "\n", "parser", ".", "add_argument", "(", "'--layers'", ",", "type", "=", "int", ",", "default", "=", "2", ",", "help", "=", "'number of RNN layers'", ")", "\n", "parser", ".", "add_argument", "(", "'--dropout'", ",", "type", "=", "float", ",", "default", "=", "0.2", ",", "help", "=", "'dropout rate'", ")", "\n", "parser", ".", "add_argument", "(", "'--clip_grad'", ",", "type", "=", "float", ",", "default", "=", "5.0", ",", "help", "=", "'grad clip at'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr'", ",", "type", "=", "float", ",", "default", "=", "0.01", ",", "help", "=", "'initial learning rate'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr_decay'", ",", "type", "=", "float", ",", "default", "=", "0.8", ",", "help", "=", "'decay ratio of learning rate'", ")", "\n", "parser", ".", "add_argument", "(", "'--min_lr'", ",", "type", "=", "float", ",", "default", "=", "0.0005", ",", "help", "=", "'minimum learning rate to be reach'", ")", "\n", "parser", ".", "add_argument", "(", "'--patience'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "help", "=", "'number of epochs to wait before reducing learning rate'", ")", "\n", "parser", ".", "add_argument", "(", "'--momentum'", ",", "type", "=", "float", ",", "default", "=", "0.9", ",", "help", "=", "'momentum for sgd'", ")", "\n", "parser", ".", "add_argument", "(", "'--replace_unk'", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "help", "=", "'rate to replace training tokens with unk'", ")", "\n", "# ablations", "\n", "parser", ".", "add_argument", "(", "'--no_chars'", ",", "action", "=", "'store_true'", ",", "help", "=", "'do not use character embeddings as input'", ")", "\n", "parser", ".", "add_argument", "(", "'--no_attention'", ",", "action", "=", "'store_true'", ",", "help", "=", "'do not use attention over input sentence'", ")", "\n", "parser", ".", "add_argument", "(", "'--no_bert'", ",", "action", "=", "'store_true'", ",", "help", "=", "'do not use bert embeddings'", ")", "\n", "# experiments", "\n", "parser", ".", "add_argument", "(", "'--obj'", ",", "default", "=", "'ML'", ",", "help", "=", "'set RL or ML objective, default is ML'", ")", "\n", "parser", ".", "add_argument", "(", "'--adam'", ",", "action", "=", "'store_true'", ",", "help", "=", "'use Adam instead of SGD'", ")", "\n", "parser", ".", "add_argument", "(", "'--adadelta'", ",", "action", "=", "'store_true'", ",", "help", "=", "'use Adadelta instead of SGD'", ")", "\n", "parser", ".", "add_argument", "(", "'--function_words'", ",", "action", "=", "'store_true'", ",", "help", "=", "'use function words'", ")", "\n", "parser", ".", "add_argument", "(", "'--function_words_rels'", ",", "action", "=", "'store_true'", ",", "help", "=", "'use function words (relations only)'", ")", "\n", "parser", ".", "add_argument", "(", "'--parse_unaligned'", ",", "action", "=", "'store_true'", ",", "help", "=", "'parse unaligned nodes'", ")", "\n", "parser", ".", "add_argument", "(", "'--weight_inputs'", ",", "action", "=", "'store_true'", ",", "help", "=", "'softmax weight inputs'", ")", "\n", "parser", ".", "add_argument", "(", "'--attend_inputs'", ",", "action", "=", "'store_true'", ",", "help", "=", "'attention weight inputs as a function of the stack'", ")", "\n", "# pretrained bert embeddings", "\n", "parser", ".", "add_argument", "(", "'--pretrained_dim'", ",", "type", "=", "int", ",", "default", "=", "1024", ",", "help", "=", "'pretrained bert dim'", ")", "\n", "parser", ".", "add_argument", "(", "\"-B\"", ",", "\"--bert_training\"", ",", "required", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "\"-b\"", ",", "\"--bert_test\"", ",", "required", "=", "False", ")", "\n", "# tests", "\n", "parser", ".", "add_argument", "(", "'--unit_tests'", ",", "action", "=", "'store_true'", ",", "help", "=", "'test parser'", ")", "\n", "# multiprocess", "\n", "parser", ".", "add_argument", "(", "'--cores'", ",", "type", "=", "int", ",", "default", "=", "10", ",", "help", "=", "'number of cores'", ")", "\n", "parser", ".", "add_argument", "(", "'--batch'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "'number of sentences per batch'", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "if", "args", ".", "unit_tests", ":", "\n", "        ", "print", "(", "'[run tests] Testing parser with default parameters and a small dataset'", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "for", "_", "in", "range", "(", "5", ")", ":", "\n", "            ", "print", "(", "'>'", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "", "args", ".", "seed", "=", "0", "\n", "args", ".", "max_train_size", "=", "100", "\n", "args", ".", "max_dev_size", "=", "100", "\n", "args", ".", "epoch", "=", "1", "\n", "args", ".", "name", "=", "'unit_tests'", "\n", "\n", "", "if", "args", ".", "test_mode", ":", "\n", "        ", "args", ".", "epoch", "=", "1", "\n", "args", ".", "max_train_size", "=", "0", "\n", "\n", "", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.learn.main": [[112, 281], ["learn.argument_parser", "transition_amr_parser.amr.JAMR_CorpusReader", "transition_amr_parser.amr.JAMR_CorpusReader.load_amrs", "transition_amr_parser.amr.JAMR_CorpusReader.load_amrs", "transition_amr_parser.data_oracle.AMR_Oracle", "transition_amr_parser.model.AMRModel", "transition_amr_parser.construct_dataset_train", "learn.setup_multiprocess", "len", "range", "transition_amr_parser.utils.print_log", "enumerate", "transition_amr_parser.utils.print_log", "str().split", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "transition_amr_parser.set_seed", "numpy.random.seed", "os.makedirs", "transition_amr_parser.data_oracle.AMR_Oracle.read_actions", "transition_amr_parser.data_oracle.AMR_Oracle.runOracle", "learn.setup_bert", "transition_amr_parser.utils.print_log", "json.load", "transition_amr_parser.model.AMRModel.load_state_dict", "torch.Adam", "transition_amr_parser.utils.print_log", "min", "open", "f.write", "time.time", "time.time", "transition_amr_parser.utils.print_log", "open", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "transition_amr_parser.model.AMRModel.parameters", "torch.Adadelta", "torch.SGD", "torch.optim.lr_scheduler.ReduceLROnPlateau", "torch.optim.lr_scheduler.ReduceLROnPlateau", "torch.optim.lr_scheduler.ReduceLROnPlateau", "torch.optim.lr_scheduler.ReduceLROnPlateau", "torch.optim.lr_scheduler.ReduceLROnPlateau", "torch.optim.lr_scheduler.ReduceLROnPlateau", "learn.get_bert_embeddings", "transition_amr_parser.utils.print_log", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "time.time", "learn.eval_parser", "time.time", "transition_amr_parser.utils.print_log", "str", "os.getpid", "transition_amr_parser.model.AMRModel.parameters", "transition_amr_parser.model.AMRModel.parameters", "enumerate", "torch.spawn", "learn.train_worker", "transition_amr_parser.model.AMRModel.state_dict", "torch.optim.lr_scheduler.ReduceLROnPlateau.step", "datetime.datetime.now", "datetime.timedelta", "datetime.timedelta", "tuple", "int", "int"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.argument_parser", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.JAMR_CorpusReader.load_amrs", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.JAMR_CorpusReader.load_amrs", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.construct_dataset_train", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.learn.setup_multiprocess", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.set_seed", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.AMR_Oracle.read_actions", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.AMR_Oracle.runOracle", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.learn.setup_bert", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.load_state_dict", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.learn.get_bert_embeddings", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.save", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.save", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.save", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.save", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.save", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.save", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.learn.eval_parser", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.learn.train_worker", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.state_dict", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.polynomial_decay_schedule.PolynomialDecaySchedule.step"], ["", "def", "main", "(", ")", ":", "\n", "\n", "# store time of start", "\n", "    ", "start_timestamp", "=", "str", "(", "datetime", ".", "now", "(", ")", ")", ".", "split", "(", "'.'", ")", "[", "0", "]", "\n", "\n", "# Argument handling", "\n", "args", "=", "argument_parser", "(", ")", "\n", "\n", "# Initialization", "\n", "if", "args", ".", "seed", "is", "not", "None", ":", "\n", "        ", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "utils", ".", "set_seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "", "if", "args", ".", "save_model", ":", "\n", "        ", "os", ".", "makedirs", "(", "args", ".", "save_model", ",", "exist_ok", "=", "True", ")", "\n", "", "exp_name", "=", "args", ".", "name", "if", "args", ".", "name", "else", "f'{os.getpid()}'", "\n", "\n", "# TODO: Provide option to precompute outside of this code.", "\n", "# 1. Store word/action dictionaries in the rule_stats json that the command line", "\n", "# 2. possiblePredicates is already store there", "\n", "# This should eliminate the train_amrs and # oracle.transitions", "\n", "# dependencies (with the esception of RL mode, which is optional)", "\n", "\n", "# Oracle computation", "\n", "cr", "=", "JAMR_CorpusReader", "(", ")", "\n", "cr", ".", "load_amrs", "(", "args", ".", "amr_training_data", ",", "training", "=", "True", ")", "\n", "cr", ".", "load_amrs", "(", "args", ".", "amr_dev_data", ",", "training", "=", "False", ")", "\n", "\n", "oracle", "=", "AMR_Oracle", "(", "verbose", "=", "False", ")", "\n", "\n", "add_unaligned", "=", "10", "if", "args", ".", "parse_unaligned", "else", "0", "\n", "if", "args", ".", "read_gold_actions", ":", "\n", "        ", "oracle", ".", "read_actions", "(", "args", ".", "read_gold_actions", ")", "\n", "", "else", ":", "\n", "        ", "oracle", ".", "runOracle", "(", "\n", "cr", ".", "amrs", ",", "\n", "add_unaligned", "=", "add_unaligned", ",", "\n", "out_actions", "=", "args", ".", "write_gold_actions", ",", "\n", "out_rule_stats", "=", "args", ".", "write_rule_stats", "\n", ")", "\n", "\n", "", "train_amrs", "=", "oracle", ".", "gold_amrs", "\n", "dev_sentences", "=", "[", "\n", "amr", ".", "tokens", "+", "(", "[", "'<unaligned>'", "]", "*", "add_unaligned", "+", "[", "'<ROOT>'", "]", ")", "\n", "for", "amr", "in", "cr", ".", "amrs_dev", "]", "\n", "\n", "if", "args", ".", "max_dev_size", "is", "not", "None", ":", "\n", "        ", "dev_sentences", "=", "dev_sentences", "[", ":", "args", ".", "max_dev_size", "]", "\n", "\n", "# BERT embeddings", "\n", "", "use_bert", "=", "not", "args", ".", "no_bert", "\n", "h5py_train", ",", "h5py_test", "=", "None", ",", "None", "\n", "if", "use_bert", ":", "\n", "        ", "h5py_train", ",", "h5py_test", "=", "setup_bert", "(", "args", ".", "bert_training", ",", "args", ".", "bert_test", ")", "\n", "\n", "", "if", "args", ".", "oracle_stats", ":", "\n", "        ", "print_log", "(", "'parser'", ",", "'Using pre-computed stats'", ")", "\n", "oracle_stats", "=", "json", ".", "load", "(", "open", "(", "args", ".", "oracle_stats", ")", ")", "\n", "", "else", ":", "\n", "        ", "oracle_stats", "=", "oracle", ".", "stats", "\n", "\n", "", "model", "=", "AMRModel", "(", "amrs", "=", "train_amrs", ",", "\n", "oracle_stats", "=", "oracle_stats", ",", "\n", "embedding_dim", "=", "args", ".", "dim_emb", ",", "\n", "action_embedding_dim", "=", "args", ".", "dim_action", ",", "\n", "char_embedding_dim", "=", "args", ".", "dim_char_emb", ",", "\n", "hidden_dim", "=", "args", ".", "dim_hidden", ",", "\n", "char_hidden_dim", "=", "args", ".", "dim_char_hidden", ",", "\n", "rnn_layers", "=", "args", ".", "layers", ",", "\n", "dropout_ratio", "=", "args", ".", "dropout", ",", "\n", "pretrained_dim", "=", "args", ".", "pretrained_dim", ",", "\n", "use_bert", "=", "use_bert", ",", "\n", "use_gpu", "=", "args", ".", "gpu", ",", "\n", "use_chars", "=", "not", "args", ".", "no_chars", ",", "\n", "use_attention", "=", "not", "args", ".", "no_attention", ",", "\n", "use_function_words", "=", "args", ".", "function_words", ",", "\n", "use_function_words_rels", "=", "args", ".", "function_words_rels", ",", "\n", "parse_unaligned", "=", "args", ".", "parse_unaligned", ",", "\n", "weight_inputs", "=", "args", ".", "weight_inputs", ",", "\n", "attend_inputs", "=", "args", ".", "attend_inputs", "\n", ")", "\n", "if", "args", ".", "load_model", ":", "\n", "        ", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "args", ".", "load_model", ")", ")", "\n", "\n", "", "scheduler", "=", "None", "\n", "if", "args", ".", "adam", ":", "\n", "        ", "optimizer", "=", "optim", ".", "Adam", "(", "model", ".", "parameters", "(", ")", ")", "\n", "", "elif", "args", ".", "adadelta", ":", "\n", "        ", "optimizer", "=", "optim", ".", "Adadelta", "(", "model", ".", "parameters", "(", ")", ")", "\n", "", "else", ":", "\n", "        ", "optimizer", "=", "optim", ".", "SGD", "(", "model", ".", "parameters", "(", ")", ",", "lr", "=", "args", ".", "lr", ",", "momentum", "=", "args", ".", "momentum", ",", "nesterov", "=", "True", ")", "\n", "scheduler", "=", "torch", ".", "optim", ".", "lr_scheduler", ".", "ReduceLROnPlateau", "(", "\n", "optimizer", ",", "\n", "'max'", ",", "\n", "factor", "=", "args", ".", "lr_decay", ",", "\n", "patience", "=", "args", ".", "patience", ",", "\n", "min_lr", "=", "args", ".", "min_lr", ",", "\n", "verbose", "=", "True", "\n", ")", "\n", "\n", "", "data", ",", "data_tokens", "=", "utils", ".", "construct_dataset_train", "(", "model", ",", "oracle", ".", "transitions", ",", "gpu", "=", "args", ".", "gpu", ")", "\n", "\n", "# multiprocess", "\n", "master_addr", ",", "master_port", "=", "setup_multiprocess", "(", "args", ")", "\n", "if", "use_bert", ":", "\n", "        ", "print_log", "(", "'parser'", ",", "'Loading BERT'", ")", "\n", "bert_embeddings_train", "=", "[", "get_bert_embeddings", "(", "h5py_train", ",", "id", ",", "tok", ")", "for", "id", ",", "tok", "in", "enumerate", "(", "data_tokens", ")", "]", "\n", "", "else", ":", "\n", "        ", "bert_embeddings_train", "=", "None", "\n", "\n", "", "tot_length", "=", "len", "(", "data", ")", "\n", "if", "args", ".", "max_train_size", ":", "\n", "        ", "tot_length", "=", "min", "(", "args", ".", "max_train_size", ",", "tot_length", ")", "\n", "\n", "# Start log of epoch by epoch improvements, include timestamp", "\n", "", "if", "args", ".", "save_model", ":", "\n", "        ", "smatch_file", "=", "f'{args.save_model}/{exp_name}_smatch.txt'", "\n", "", "else", ":", "\n", "        ", "smatch_file", "=", "f'{exp_name}_smatch.txt'", "\n", "", "with", "open", "(", "smatch_file", ",", "'w+'", ")", "as", "f", ":", "\n", "        ", "desc", "=", "args", ".", "desc", "if", "args", ".", "desc", "else", "''", "\n", "f", ".", "write", "(", "f'{start_timestamp}\\t{desc}\\n'", ")", "\n", "\n", "", "training", "=", "not", "args", ".", "test_mode", "\n", "epoch_list", "=", "range", "(", "args", ".", "start_epoch", ",", "args", ".", "start_epoch", "+", "args", ".", "epoch", ")", "\n", "print_log", "(", "'parser'", ",", "'Start Parsing'", ")", "\n", "for", "epoch_idx", ",", "_", "in", "enumerate", "(", "epoch_list", ")", ":", "\n", "        ", "start", "=", "time", ".", "time", "(", ")", "\n", "\n", "if", "training", ":", "\n", "            ", "if", "epoch_idx", "<", "10", ":", "\n", "                ", "model", ".", "warm_up", "=", "True", "\n", "", "arguments", "=", "(", "\n", "model", ",", "args", ",", "data", ",", "data_tokens", ",", "bert_embeddings_train", ",", "\n", "optimizer", ",", "master_addr", ",", "master_port", ",", "args", ".", "cores", ",", "\n", "exp_name", ",", "epoch_idx", "\n", ")", "\n", "if", "args", ".", "cores", ">", "1", ":", "\n", "                ", "mp", ".", "spawn", "(", "train_worker", ",", "nprocs", "=", "args", ".", "cores", ",", "args", "=", "arguments", ")", "\n", "", "else", ":", "\n", "# call code for single thread, add dummy rank 0", "\n", "                ", "train_worker", "(", "*", "(", "tuple", "(", "[", "0", "]", ")", "+", "arguments", ")", ")", "\n", "", "model", ".", "warm_up", "=", "False", "\n", "\n", "", "end", "=", "time", ".", "time", "(", ")", "\n", "print_log", "(", "'parser'", ",", "f'{timedelta(seconds=int(end-start))}'", ")", "\n", "# create output", "\n", "if", "(", "epoch_idx", "+", "1", "-", "args", ".", "start_epoch", ")", "%", "args", ".", "report", "==", "0", ":", "\n", "\n", "# save model", "\n", "            ", "if", "args", ".", "save_model", ":", "\n", "                ", "parameters_file", "=", "f'{args.save_model}/{exp_name}.epoch{epoch_idx}.params'", "\n", "", "else", ":", "\n", "                ", "parameters_file", "=", "f'{exp_name}.epoch{epoch_idx}.params'", "\n", "", "print_log", "(", "'parser'", ",", "f'Saving model to: {parameters_file}'", ")", "\n", "torch", ".", "save", "(", "model", ".", "state_dict", "(", ")", ",", "parameters_file", ")", "\n", "\n", "start", "=", "time", ".", "time", "(", ")", "\n", "smatch_score", "=", "eval_parser", "(", "exp_name", ",", "model", ",", "args", ",", "dev_sentences", ",", "\n", "h5py_test", ",", "epoch_idx", ",", "smatch_file", ",", "\n", "args", ".", "save_model", ",", "optimizer", ".", "param_groups", ")", "\n", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "print_log", "(", "'evaluation'", ",", "f'{timedelta(seconds=int(end-start))}'", ")", "\n", "\n", "if", "scheduler", "is", "not", "None", "and", "smatch_score", "is", "not", "None", ":", "\n", "                ", "scheduler", ".", "step", "(", "smatch_score", ")", "\n", "\n", "", "", "", "print_log", "(", "'parser'", ",", "'Done'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.learn.train_worker": [[283, 378], ["model.train", "model.reset_stats", "transition_amr_parser.utils.print_log", "tqdm.tqdm", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "model.action_acc.data_as_tensor", "model.label_acc.data_as_tensor", "model.labelA_acc.data_as_tensor", "model.pred_acc.data_as_tensor", "str", "torch.init_process_group", "torch.nn.parallel.DistributedDataParallel", "torch.nn.parallel.DistributedDataParallel", "torch.nn.parallel.DistributedDataParallel", "torch.nn.parallel.DistributedDataParallel", "torch.nn.parallel.DistributedDataParallel", "torch.nn.parallel.DistributedDataParallel", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "len", "min", "transition_amr_parser.pad_batch_tokens", "transition_amr_parser.pad_batch", "transition_amr_parser.make_efficient", "torch.nn.parallel.DistributedDataParallel.zero_grad", "torch.nn.parallel.DistributedDataParallel.forward", "dist_model.forward.backward", "dist_model.forward.sum().item", "torch.utils.clip_grad_norm_", "optimizer.step", "model.action_confusion_matrix.data_as_tensor", "model.label_confusion_matrix.data_as_tensor", "torch.reduce", "torch.reduce", "torch.reduce", "torch.reduce", "torch.reduce", "model.action_acc.reset_from_tensor", "model.label_acc.reset_from_tensor", "model.labelA_acc.reset_from_tensor", "model.pred_acc.reset_from_tensor", "learn.print_epoch_report", "sent_idx.size", "torch.nn.parallel.DistributedDataParallel.parameters", "torch.reduce", "torch.reduce", "losses[].item", "losses[].item", "losses[].item", "losses[].item", "losses[].item", "model.action_confusion_matrix.reset_from_tensor", "model.label_confusion_matrix.reset_from_tensor", "torch.utils.data.distributed.DistributedSampler", "transition_amr_parser.vectorize_words", "dist_model.forward.sum", "range", "range", "range", "sent_idx[].item", "sent_idx[].item"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq-stack-transformer.train.train", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.model.AMRModel.reset_stats", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.data_as_tensor", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.data_as_tensor", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.data_as_tensor", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.data_as_tensor", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.pad_batch_tokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.pad_batch", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.make_efficient", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.optim.fp16_optimizer.MemoryEfficientFP16Optimizer.zero_grad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.model.AMRModel.forward", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.scalar_bias.ScalarBias.backward", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.clip_grad_norm_", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.polynomial_decay_schedule.PolynomialDecaySchedule.step", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.data_as_tensor", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.data_as_tensor", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.reset_from_tensor", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.reset_from_tensor", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.reset_from_tensor", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.reset_from_tensor", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.learn.print_epoch_report", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.reset_from_tensor", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.reset_from_tensor", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.vectorize_words", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item"], ["", "def", "train_worker", "(", "rank", ",", "model", ",", "args", ",", "data", ",", "data_tokens", ",", "bert_embeddings_train", ",", "\n", "optimizer", ",", "master_addr", ",", "master_port", ",", "cores", ",", "exp_name", ",", "epoch_idx", ")", ":", "\n", "    ", "model", ".", "train", "(", ")", "\n", "model", ".", "reset_stats", "(", ")", "\n", "\n", "print_log", "(", "'train'", ",", "f'starting process {rank}'", ")", "\n", "if", "cores", ">", "1", ":", "\n", "        ", "os", ".", "environ", "[", "'MASTER_ADDR'", "]", "=", "master_addr", "\n", "os", ".", "environ", "[", "'MASTER_PORT'", "]", "=", "str", "(", "master_port", ")", "\n", "dist", ".", "init_process_group", "(", "world_size", "=", "cores", ",", "backend", "=", "'gloo'", ",", "rank", "=", "rank", ")", "\n", "dist_model", "=", "torch", ".", "nn", ".", "parallel", ".", "DistributedDataParallel", "(", "model", ",", "find_unused_parameters", "=", "True", ")", "\n", "# Get only slice of data for this core", "\n", "dataset_loader", "=", "DataLoader", "(", "\n", "data", ",", "\n", "args", ".", "batch", ",", "\n", "drop_last", "=", "False", ",", "\n", "sampler", "=", "DistributedSampler", "(", "data", ",", "cores", ",", "rank", ")", "\n", ")", "\n", "", "else", ":", "\n", "        ", "dist_model", "=", "model", "\n", "# Get all data", "\n", "dataset_loader", "=", "DataLoader", "(", "data", ",", "args", ".", "batch", ",", "drop_last", "=", "False", ")", "\n", "\n", "", "total_len", "=", "len", "(", "data", ")", "//", "cores", "\n", "if", "args", ".", "max_train_size", ":", "\n", "        ", "total_len", "=", "min", "(", "total_len", ",", "args", ".", "max_train_size", "//", "cores", ")", "\n", "\n", "# train on sentence", "\n", "", "i", "=", "0", "\n", "for", "batch", "in", "tqdm", "(", "dataset_loader", ",", "desc", "=", "f'[train] epoch {epoch_idx}'", ")", ":", "\n", "        ", "sent_idx", ",", "labelO", ",", "labelA", ",", "action", ",", "pred", "=", "batch", "[", "'sent_idx'", "]", ",", "batch", "[", "'labels'", "]", ",", "batch", "[", "'labelsA'", "]", ",", "batch", "[", "'actions'", "]", ",", "batch", "[", "'preds'", "]", "\n", "\n", "batch_size", "=", "sent_idx", ".", "size", "(", ")", "[", "0", "]", "\n", "tokens", "=", "utils", ".", "pad_batch_tokens", "(", "[", "data_tokens", "[", "sent_idx", "[", "i", "]", ".", "item", "(", ")", "]", "for", "i", "in", "range", "(", "batch_size", ")", "]", ")", "\n", "\n", "sent_emb", "=", "utils", ".", "pad_batch", "(", "\n", "[", "utils", ".", "vectorize_words", "(", "model", ",", "tokens", "[", "i", "]", ",", "training", "=", "True", ",", "random_replace", "=", "args", ".", "replace_unk", ",", "gpu", "=", "args", ".", "gpu", ")", "for", "i", "in", "range", "(", "batch_size", ")", "]", ")", "\n", "bert_emb", "=", "[", "bert_embeddings_train", "[", "sent_idx", "[", "i", "]", ".", "item", "(", ")", "]", "for", "i", "in", "range", "(", "batch_size", ")", "]", "if", "not", "args", ".", "no_bert", "else", "None", "\n", "\n", "labelO", ",", "labelA", ",", "action", ",", "pred", "=", "utils", ".", "make_efficient", "(", "args", ".", "gpu", ",", "labelO", ",", "labelA", ",", "action", ",", "pred", ")", "\n", "\n", "dist_model", ".", "zero_grad", "(", ")", "\n", "\n", "loss", "=", "dist_model", ".", "forward", "(", "sent_idx", ",", "sent_emb", ",", "labelO", ",", "labelA", ",", "action", ",", "pred", ",", "args", ".", "obj", ",", "tokens", "=", "tokens", ",", "bert_embedding", "=", "bert_emb", ")", "\n", "\n", "loss", ".", "backward", "(", ")", "\n", "\n", "model", ".", "epoch_loss", "+=", "loss", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "dist_model", ".", "parameters", "(", ")", ",", "args", ".", "clip_grad", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "i", "+=", "args", ".", "batch", "\n", "if", "args", ".", "max_train_size", "and", "i", ">", "total_len", ":", "\n", "            ", "break", "\n", "\n", "# consolidate stats between different processes", "\n", "# (This is a trick to send info using pytorch's", "\n", "# interface for sending tensors between multiple processes)", "\n", "", "", "losses", "=", "torch", ".", "FloatTensor", "(", "[", "model", ".", "epoch_loss", ",", "model", ".", "action_loss", ",", "\n", "model", ".", "label_loss", ",", "model", ".", "labelA_loss", ",", "\n", "model", ".", "pred_loss", "]", ")", "\n", "\n", "# Get scores", "\n", "action_acc", "=", "model", ".", "action_acc", ".", "data_as_tensor", "(", ")", "\n", "label_acc", "=", "model", ".", "label_acc", ".", "data_as_tensor", "(", ")", "\n", "labelA_acc", "=", "model", ".", "labelA_acc", ".", "data_as_tensor", "(", ")", "\n", "pred_acc", "=", "model", ".", "pred_acc", ".", "data_as_tensor", "(", ")", "\n", "if", "args", ".", "confusion", ":", "\n", "        ", "act_conf", "=", "model", ".", "action_confusion_matrix", ".", "data_as_tensor", "(", ")", "\n", "label_conf", "=", "model", ".", "label_confusion_matrix", ".", "data_as_tensor", "(", ")", "\n", "\n", "", "if", "cores", ">", "1", ":", "\n", "# Sync data across threads", "\n", "        ", "dist", ".", "reduce", "(", "losses", ",", "dst", "=", "0", ")", "\n", "dist", ".", "reduce", "(", "action_acc", ",", "dst", "=", "0", ")", "\n", "dist", ".", "reduce", "(", "label_acc", ",", "dst", "=", "0", ")", "\n", "dist", ".", "reduce", "(", "labelA_acc", ",", "dst", "=", "0", ")", "\n", "dist", ".", "reduce", "(", "pred_acc", ",", "dst", "=", "0", ")", "\n", "if", "args", ".", "confusion", ":", "\n", "            ", "dist", ".", "reduce", "(", "act_conf", ",", "dst", "=", "0", ")", "\n", "dist", ".", "reduce", "(", "label_conf", ",", "dst", "=", "0", ")", "\n", "\n", "", "", "if", "rank", "==", "0", ":", "\n", "        ", "model", ".", "epoch_loss", ",", "model", ".", "action_loss", ",", "model", ".", "label_loss", ",", "model", ".", "labelA_loss", ",", "model", ".", "pred_loss", "=", "losses", "[", "0", "]", ".", "item", "(", ")", ",", "losses", "[", "1", "]", ".", "item", "(", ")", ",", "losses", "[", "2", "]", ".", "item", "(", ")", ",", "losses", "[", "3", "]", ".", "item", "(", ")", ",", "losses", "[", "4", "]", ".", "item", "(", ")", "\n", "model", ".", "action_acc", ".", "reset_from_tensor", "(", "action_acc", ")", "\n", "model", ".", "label_acc", ".", "reset_from_tensor", "(", "label_acc", ")", "\n", "model", ".", "labelA_acc", ".", "reset_from_tensor", "(", "labelA_acc", ")", "\n", "model", ".", "pred_acc", ".", "reset_from_tensor", "(", "pred_acc", ")", "\n", "if", "args", ".", "confusion", ":", "\n", "            ", "model", ".", "action_confusion_matrix", ".", "reset_from_tensor", "(", "act_conf", ")", "\n", "model", ".", "label_confusion_matrix", ".", "reset_from_tensor", "(", "label_conf", ")", "\n", "\n", "# Inform user", "\n", "", "print_epoch_report", "(", "model", ",", "exp_name", ",", "epoch_idx", ",", "args", ".", "weight_inputs", ",", "args", ".", "attend_inputs", ",", "args", ".", "parse_unaligned", ",", "args", ".", "confusion", ",", "args", ".", "save_model", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.learn.eval_parser": [[380, 477], ["model.eval", "transition_amr_parser.utils.print_log", "transition_amr_parser.utils.print_log", "tqdm.tqdm", "transition_amr_parser.utils.print_log", "transition_amr_parser.utils.smatch_wrapper", "transition_amr_parser.utils.print_log", "transition_amr_parser.utils.print_log", "open", "f.write", "transition_amr_parser.utils.print_log", "transition_amr_parser.vectorize_words", "model.forward_single", "zip", "transition_amr_parser.state_machine.AMRStateMachine", "transition_amr_parser.state_machine.AMRStateMachine.applyActions", "str().split", "open", "fid.write", "print", "print", "str", "open", "f.write", "learn.get_bert_embeddings", "act.startswith", "sum", "sum", "sum", "sum", "open", "f.write", "apply_actions.append", "open", "f.write", "f.write", "transition_amr_parser.state_machine.AMRStateMachine.amr.toJAMRString", "str", "act.startswith", "apply_actions.append", "act.startswith", "datetime.datetime.now", "act.startswith", "apply_actions.append", "apply_actions.append", "act.endswith"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.smatch_wrapper", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.vectorize_words", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.model.AMRModel.forward_single", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.applyActions", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.learn.get_bert_embeddings", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.toJAMRString", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "", "def", "eval_parser", "(", "exp_name", ",", "model", ",", "args", ",", "dev_sentences", ",", "h5py_test", ",", "epoch_idx", ",", "smatch_file", ",", "save_model", ",", "param_groups", ")", ":", "\n", "\n", "# save also current learning rate", "\n", "    ", "learning_rate", "=", "\",\"", ".", "join", "(", "[", "str", "(", "x", "[", "'lr'", "]", ")", "for", "x", "in", "param_groups", "]", ")", "\n", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "# evaluate on dev", "\n", "print_log", "(", "'eval'", ",", "f'Evaluating on: {args.amr_dev_data}'", ")", "\n", "if", "save_model", ":", "\n", "        ", "predicted_amr_file", "=", "f'{save_model}/{exp_name}_amrs.epoch{epoch_idx}.dev.txt'", "\n", "", "else", ":", "\n", "        ", "predicted_amr_file", "=", "f'{exp_name}_amrs.epoch{epoch_idx}.dev.txt'", "\n", "", "with", "open", "(", "predicted_amr_file", ",", "'w+'", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "''", ")", "\n", "", "print_log", "(", "'eval'", ",", "f'Writing amr graphs to: {predicted_amr_file}'", ")", "\n", "if", "save_model", ":", "\n", "        ", "actions_file", "=", "f'{save_model}/{exp_name}_actions.epoch{epoch_idx}.dev.txt'", "\n", "", "else", ":", "\n", "        ", "actions_file", "=", "f'{exp_name}_actions.epoch{epoch_idx}.dev.txt'", "\n", "", "if", "args", ".", "write_actions", ":", "\n", "        ", "print_log", "(", "'eval'", ",", "f'Writing actions to: {actions_file}'", ")", "\n", "with", "open", "(", "actions_file", ",", "'w+'", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "''", ")", "\n", "\n", "", "", "sent_idx", "=", "0", "\n", "dev_hash", "=", "0", "\n", "for", "tokens", "in", "tqdm", "(", "dev_sentences", ")", ":", "\n", "\n", "        ", "sent_rep", "=", "utils", ".", "vectorize_words", "(", "model", ",", "tokens", ",", "training", "=", "False", ",", "gpu", "=", "args", ".", "gpu", ")", "\n", "dev_b_emb", "=", "get_bert_embeddings", "(", "h5py_test", ",", "sent_idx", ",", "tokens", ")", "if", "not", "args", ".", "no_bert", "else", "None", "\n", "\n", "_", ",", "actions", ",", "labels", ",", "labelsA", ",", "predicates", "=", "model", ".", "forward_single", "(", "\n", "sent_rep", ",", "\n", "mode", "=", "'predict'", ",", "\n", "tokens", "=", "tokens", ",", "\n", "bert_embedding", "=", "dev_b_emb", "\n", ")", "\n", "\n", "# write amr graphs", "\n", "apply_actions", "=", "[", "]", "\n", "for", "act", ",", "label", ",", "labelA", ",", "predicate", "in", "zip", "(", "actions", ",", "labels", ",", "labelsA", ",", "predicates", ")", ":", "\n", "# print(act, label, labelA, predicate)", "\n", "            ", "if", "act", ".", "startswith", "(", "'PR'", ")", ":", "\n", "                ", "apply_actions", ".", "append", "(", "act", "+", "f'({predicate})'", ")", "\n", "", "elif", "act", ".", "startswith", "(", "'RA'", ")", "or", "act", ".", "startswith", "(", "'LA'", ")", "and", "not", "act", ".", "endswith", "(", "'(root)'", ")", ":", "\n", "                ", "apply_actions", ".", "append", "(", "act", "+", "f'({label})'", ")", "\n", "", "elif", "act", ".", "startswith", "(", "'AD'", ")", ":", "\n", "                ", "apply_actions", ".", "append", "(", "act", "+", "f'({labelA})'", ")", "\n", "", "else", ":", "\n", "                ", "apply_actions", ".", "append", "(", "act", ")", "\n", "", "", "if", "args", ".", "unit_tests", ":", "\n", "            ", "dev_hash", "+=", "sum", "(", "model", ".", "action2idx", "[", "a", "]", "for", "a", "in", "actions", ")", "\n", "dev_hash", "+=", "sum", "(", "model", ".", "labelsO2idx", "[", "l", "]", "for", "l", "in", "labels", "if", "l", ")", "\n", "dev_hash", "+=", "sum", "(", "model", ".", "labelsA2idx", "[", "l", "]", "for", "l", "in", "labelsA", "if", "l", ")", "\n", "dev_hash", "+=", "sum", "(", "model", ".", "pred2idx", "[", "p", "]", "if", "p", "in", "model", ".", "pred2idx", "else", "0", "for", "p", "in", "predicates", "if", "p", ")", "\n", "\n", "# print('[eval]',apply_actions)", "\n", "", "if", "args", ".", "write_actions", ":", "\n", "            ", "with", "open", "(", "actions_file", ",", "'a'", ")", "as", "f", ":", "\n", "                ", "f", ".", "write", "(", "'\\t'", ".", "join", "(", "tokens", ")", "+", "'\\n'", ")", "\n", "f", ".", "write", "(", "'\\t'", ".", "join", "(", "apply_actions", ")", "+", "'\\n\\n'", ")", "\n", "", "", "tr", "=", "AMRStateMachine", "(", "tokens", ",", "verbose", "=", "False", ")", "\n", "tr", ".", "applyActions", "(", "apply_actions", ")", "\n", "with", "open", "(", "predicted_amr_file", ",", "'a'", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "tr", ".", "amr", ".", "toJAMRString", "(", ")", ")", "\n", "", "sent_idx", "+=", "1", "\n", "# run smatch", "\n", "", "print_log", "(", "'eval'", ",", "f'Computing SMATCH'", ")", "\n", "smatch_score", "=", "smatch_wrapper", "(", "\n", "args", ".", "amr_dev_data", ",", "\n", "predicted_amr_file", ",", "\n", "significant", "=", "3", "\n", ")", "\n", "print_log", "(", "'eval'", ",", "f'SMATCH: {smatch_score}'", ")", "\n", "timestamp", "=", "str", "(", "datetime", ".", "now", "(", ")", ")", ".", "split", "(", "'.'", ")", "[", "0", "]", "\n", "\n", "# store all information in file", "\n", "print_log", "(", "'eval'", ",", "f'Writing SMATCH and other info to: {smatch_file}'", ")", "\n", "with", "open", "(", "smatch_file", ",", "'a'", ")", "as", "fid", ":", "\n", "        ", "fid", ".", "write", "(", "\"\\t\"", ".", "join", "(", "[", "\n", "f'epoch {epoch_idx}'", ",", "\n", "f'learning_rate {learning_rate}'", ",", "\n", "f'time {timestamp}'", ",", "\n", "f'F-score {smatch_score}\\n'", "\n", "]", ")", ")", "\n", "\n", "", "if", "args", ".", "unit_tests", ":", "\n", "        ", "test1", "=", "(", "model", ".", "epoch_loss", "==", "3360.1150283813477", ")", "\n", "test2", "=", "(", "dev_hash", "==", "6038", ")", "\n", "print", "(", "f'[run tests] epoch_loss==3360.1150283813477 (got {model.epoch_loss}) {\"pass\" if test1 else \"fail\"}'", ",", "\n", "file", "=", "sys", ".", "stderr", ")", "\n", "print", "(", "f'[run tests] dev hash==6038 (got {dev_hash}) {\"pass\" if test2 else \"fail\"}'", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "assert", "(", "test1", ")", "\n", "assert", "(", "test2", ")", "\n", "\n", "", "return", "smatch_score", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.learn.setup_bert": [[479, 487], ["transition_amr_parser.utils.print_log", "h5py.File", "transition_amr_parser.utils.print_log", "h5py.File", "Exception"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log"], ["", "def", "setup_bert", "(", "bert_train_file", ",", "bert_test_file", ")", ":", "\n", "    ", "if", "not", "bert_train_file", "or", "not", "bert_test_file", ":", "\n", "        ", "raise", "Exception", "(", "'Bert training and test embeddings should be provided (otherwise use --no_bert)'", ")", "\n", "", "print_log", "(", "'parser'", ",", "f'BERT train: {bert_train_file}'", ")", "\n", "h5py_train", "=", "h5py", ".", "File", "(", "bert_train_file", ",", "'r'", ")", "\n", "print_log", "(", "'parser'", ",", "f'BERT test: {bert_test_file}'", ")", "\n", "h5py_test", "=", "h5py", ".", "File", "(", "bert_test_file", ",", "'r'", ")", "\n", "return", "h5py_train", ",", "h5py_test", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.learn.get_bert_embeddings": [[489, 499], ["enumerate", "h5py.get", "str", "Exception", "b_emb.append"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "get_bert_embeddings", "(", "h5py", ",", "sent_idx", ",", "tokens", ")", ":", "\n", "    ", "b_emb", "=", "[", "]", "\n", "for", "i", ",", "word", "in", "enumerate", "(", "tokens", ")", ":", "\n", "        ", "line", "=", "h5py", ".", "get", "(", "str", "(", "sent_idx", ")", ")", "\n", "if", "line", "is", "None", ":", "\n", "            ", "raise", "Exception", "(", "f'BERT failed to find embedding: {sent_idx}, {tokens}'", ")", "\n", "", "if", "word", "not", "in", "[", "'<ROOT>'", ",", "'<unaligned>'", ",", "'<eof>'", "]", ":", "\n", "            ", "embedding", "=", "line", "[", "i", "]", "\n", "b_emb", ".", "append", "(", "embedding", ")", "\n", "", "", "return", "b_emb", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.learn.setup_multiprocess": [[501, 511], ["socket.gethostname", "transition_amr_parser.utils.print_log", "transition_amr_parser.utils.print_log", "torch.is_available", "transition_amr_parser.utils.print_log"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log"], ["", "def", "setup_multiprocess", "(", "args", ")", ":", "\n", "    ", "if", "not", "(", "dist", ".", "is_available", "(", ")", ")", ":", "\n", "        ", "print_log", "(", "'parser'", ",", "f'Warning: Distributed processing unavailable. Defaulting to single process.'", ")", "\n", "args", ".", "cores", "=", "1", "\n", "return", "''", ",", "''", "\n", "", "master_addr", "=", "socket", ".", "gethostname", "(", ")", "\n", "master_port", "=", "'64646'", "\n", "print_log", "(", "'parser'", ",", "f'multiprocessing with {args.cores} processes'", ")", "\n", "print_log", "(", "'parser'", ",", "f'multiprocessing ADDR: {master_addr} PORT: {master_port}'", ")", "\n", "return", "master_addr", ",", "master_port", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.learn.print_epoch_report": [[513, 547], ["transition_amr_parser.utils.print_log", "transition_amr_parser.utils.print_log", "transition_amr_parser.utils.print_log", "transition_amr_parser.utils.print_log", "transition_amr_parser.utils.print_log", "transition_amr_parser.utils.print_log", "transition_amr_parser.utils.print_log", "transition_amr_parser.utils.print_log", "transition_amr_parser.utils.print_log", "transition_amr_parser.utils.print_log", "transition_amr_parser.utils.print_log", "transition_amr_parser.utils.print_log", "transition_amr_parser.utils.print_log", "transition_amr_parser.utils.print_log", "transition_amr_parser.utils.print_log", "transition_amr_parser.utils.print_log", "open", "f.write", "f.write", "f.write", "f.write", "torch.softmax().tolist", "torch.softmax().tolist", "torch.softmax().tolist", "torch.softmax().tolist", "torch.softmax().tolist", "torch.softmax().tolist", "torch.softmax().tolist", "torch.softmax().tolist", "str", "str", "torch.softmax().tolist", "torch.softmax().tolist", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax"], ["", "def", "print_epoch_report", "(", "model", ",", "exp_name", ",", "epoch_idx", ",", "weight_inputs", ",", "attend_inputs", ",", "parse_unaligned", ",", "confusion", ",", "save_model", ")", ":", "\n", "\n", "    ", "print_log", "(", "'train'", ",", "f'Loss {model.epoch_loss}'", ")", "\n", "print_log", "(", "'train'", ",", "f'actions: {model.action_loss} labels: {model.label_loss} labelsA: {model.labelA_loss} predicates: {model.pred_loss}'", ")", "\n", "print_log", "(", "'train'", ",", "f'Accuracy actions: {model.action_acc} labels: {model.label_acc} labelsA: {model.labelA_acc} predicates: {model.pred_acc}'", ")", "\n", "\n", "if", "weight_inputs", ":", "\n", "        ", "print_log", "(", "'train'", ",", "f'(Attention) Stack   Buffer  Actions Attention   [other]'", ")", "\n", "print_log", "(", "'train'", ",", "f'action attention: {F.softmax(model.action_attention, dim=0).tolist()}'", ")", "\n", "print_log", "(", "'train'", ",", "f'label attention: {F.softmax(model.label_attention, dim=0).tolist()}'", ")", "\n", "print_log", "(", "'train'", ",", "f'labelA attention: {F.softmax(model.labelA_attention, dim=0).tolist()}'", ")", "\n", "print_log", "(", "'train'", ",", "f'pred attention: {F.softmax(model.pred_attention, dim=0).tolist()}'", ")", "\n", "if", "parse_unaligned", ":", "\n", "            ", "print_log", "(", "'train'", ",", "f'pred unaligned attention: {F.softmax(model.pred_attention_unaligned, dim=0).tolist()}'", ")", "\n", "", "", "if", "attend_inputs", ":", "\n", "        ", "print_log", "(", "'train'", ",", "f'(Attention bias) Stack   Buffer  Actions Attention   [other]'", ")", "\n", "print_log", "(", "'train'", ",", "f'action attention: {F.softmax(model.action_attention.bias, dim=0).tolist()}'", ")", "\n", "print_log", "(", "'train'", ",", "f'label attention: {F.softmax(model.label_attention.bias, dim=0).tolist()}'", ")", "\n", "print_log", "(", "'train'", ",", "f'labelA attention: {F.softmax(model.labelA_attention.bias, dim=0).tolist()}'", ")", "\n", "print_log", "(", "'train'", ",", "f'pred attention: {F.softmax(model.pred_attention.bias, dim=0).tolist()}'", ")", "\n", "if", "parse_unaligned", ":", "\n", "            ", "print_log", "(", "'train'", ",", "f'pred unaligned attention: {F.softmax(model.pred_attention_unaligned.bias, dim=0).tolist()}'", ")", "\n", "\n", "", "", "if", "confusion", ":", "\n", "        ", "if", "save_model", ":", "\n", "            ", "confusion_file", "=", "f'{save_model}/{exp_name}_confusion_matrix.epoch{epoch_idx}.train.txt'", "\n", "", "else", ":", "\n", "            ", "confusion_file", "=", "f'{exp_name}_confusion_matrix.epoch{epoch_idx}.train.txt'", "\n", "", "print_log", "(", "'train'", ",", "f'Writing confusion matrix to: {confusion_file}'", ")", "\n", "with", "open", "(", "confusion_file", ",", "'w+'", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "'Actions:\\n'", ")", "\n", "f", ".", "write", "(", "str", "(", "model", ".", "action_confusion_matrix", ")", "+", "'\\n'", ")", "\n", "f", ".", "write", "(", "'Labels:\\n'", ")", "\n", "f", ".", "write", "(", "str", "(", "model", ".", "label_confusion_matrix", ")", "+", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.io.read_frame": [[9, 63], ["xml.parse().getroot", "ET.parse().getroot.findall", "predicate.findall", "xml.parse", "roleset_data.findall", "roleset_data.findall", "roleset_data.findall", "examples.findall", "examples.findall", "[].append", "[].append", "len", "args.append", "args[].update", "examples.findall"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update"], ["def", "read_frame", "(", "xml_file", ")", ":", "\n", "    ", "'''\n    Read probpank XML\n    '''", "\n", "\n", "root", "=", "ET", ".", "parse", "(", "xml_file", ")", ".", "getroot", "(", ")", "\n", "propbank", "=", "{", "}", "\n", "for", "predicate", "in", "root", ".", "findall", "(", "'predicate'", ")", ":", "\n", "        ", "lemma", "=", "predicate", ".", "attrib", "[", "'lemma'", "]", "\n", "for", "roleset_data", "in", "predicate", ".", "findall", "(", "'roleset'", ")", ":", "\n", "\n", "# ID of the role e.g. run.01", "\n", "            ", "pred_id", "=", "roleset_data", ".", "attrib", "[", "'id'", "]", "\n", "\n", "# basic meta-data", "\n", "propbank", "[", "pred_id", "]", "=", "{", "\n", "'lemma'", ":", "lemma", ",", "\n", "'description'", ":", "roleset_data", ".", "attrib", "[", "'name'", "]", "\n", "}", "\n", "\n", "# alias", "\n", "propbank", "[", "pred_id", "]", "[", "'aliases'", "]", "=", "[", "]", "\n", "for", "aliases", "in", "roleset_data", ".", "findall", "(", "'aliases'", ")", ":", "\n", "                ", "for", "alias", "in", "aliases", ":", "\n", "                    ", "propbank", "[", "pred_id", "]", "[", "'aliases'", "]", ".", "append", "(", "alias", ".", "text", ")", "\n", "\n", "# roles", "\n", "", "", "propbank", "[", "pred_id", "]", "[", "'roles'", "]", "=", "{", "}", "\n", "for", "roles", "in", "roleset_data", ".", "findall", "(", "'roles'", ")", ":", "\n", "                ", "for", "role", "in", "roles", ":", "\n", "                    ", "if", "role", ".", "tag", "==", "'note'", ":", "\n", "                        ", "continue", "\n", "", "number", "=", "role", ".", "attrib", "[", "'n'", "]", "\n", "propbank", "[", "pred_id", "]", "[", "'roles'", "]", "[", "f'ARG{number}'", "]", "=", "role", ".", "attrib", "\n", "\n", "# examples", "\n", "", "", "propbank", "[", "pred_id", "]", "[", "'examples'", "]", "=", "[", "]", "\n", "for", "examples", "in", "roleset_data", ".", "findall", "(", "'example'", ")", ":", "\n", "                ", "sentence", "=", "examples", ".", "findall", "(", "'text'", ")", "\n", "assert", "len", "(", "sentence", ")", "==", "1", "\n", "sentence", "=", "sentence", "[", "0", "]", ".", "text", "\n", "tokens", "=", "[", "x", ".", "text", "for", "x", "in", "examples", ".", "findall", "(", "'rel'", ")", "]", "\n", "args", "=", "[", "]", "\n", "for", "x", "in", "examples", ".", "findall", "(", "'arg'", ")", ":", "\n", "                    ", "args", ".", "append", "(", "x", ".", "attrib", ")", "\n", "args", "[", "-", "1", "]", ".", "update", "(", "{", "'text'", ":", "x", ".", "text", "}", ")", "\n", "", "propbank", "[", "pred_id", "]", "[", "'examples'", "]", ".", "append", "(", "{", "\n", "'sentence'", ":", "sentence", ",", "\n", "'tokens'", ":", "tokens", ",", "\n", "'args'", ":", "args", "\n", "}", ")", "\n", "\n", "\n", "", "", "", "return", "propbank", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.io.read_action_scores": [[65, 96], ["open", "line.strip.strip", "list", "list.append", "list.append", "list.append", "action_scores.append", "map", "float", "int", "list.append", "list.append", "ast.literal_eval", "line.strip.split", "line.strip.split", "line.strip.split", "line.strip.split", "line.strip.split", "line.strip.split", "line.strip.split", "line.strip.split"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["", "def", "read_action_scores", "(", "file_path", ")", ":", "\n", "    ", "\"\"\"\n    Reads scores to judge the optimality of an action set, comprise\n\n    sentence id (position in the original corpus)       1 int\n    unormalized scores                                  3 int\n    sequence normalized score e.g. smatch               1 float \n    action sequence length                              1 int\n    saved because of {score, length, None (original)}   1 str\n    action sequence (tab separated)                     1 str (tab separated)\n\n    TODO: Probability\n    \"\"\"", "\n", "action_scores", "=", "[", "]", "\n", "with", "open", "(", "file_path", ")", "as", "fid", ":", "\n", "        ", "for", "line", "in", "fid", ":", "\n", "            ", "line", "=", "line", ".", "strip", "(", ")", "\n", "items", "=", "list", "(", "map", "(", "int", ",", "line", ".", "split", "(", ")", "[", ":", "4", "]", ")", ")", "\n", "items", ".", "append", "(", "float", "(", "line", ".", "split", "(", ")", "[", "4", "]", ")", ")", "\n", "items", ".", "append", "(", "int", "(", "line", ".", "split", "(", ")", "[", "5", "]", ")", ")", "\n", "items", ".", "append", "(", "\n", "None", "if", "line", ".", "split", "(", ")", "[", "6", "]", "==", "'None'", "else", "line", ".", "split", "(", ")", "[", "6", "]", "\n", ")", "\n", "if", "line", ".", "split", "(", ")", "[", "7", "]", "[", "0", "]", "==", "'['", ":", "\n", "# backwards compatibility fix", "\n", "                ", "items", ".", "append", "(", "ast", ".", "literal_eval", "(", "\" \"", ".", "join", "(", "line", ".", "split", "(", ")", "[", "7", ":", "]", ")", ")", ")", "\n", "", "else", ":", "\n", "                ", "items", ".", "append", "(", "line", ".", "split", "(", ")", "[", "7", ":", "]", ")", "\n", "", "action_scores", ".", "append", "(", "items", ")", "\n", "\n", "", "", "return", "action_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.io.write_action_scores": [[98, 124], ["open", "fid.write"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write"], ["", "def", "write_action_scores", "(", "file_path", ",", "action_scores", ")", ":", "\n", "    ", "\"\"\"\n    Writes scores to judge the optimality of an action set, comprise\n\n    sentence id (position in the original corpus)       1 int\n    unormalized scores                                  3 int\n    sequence normalized score e.g. smatch               1 float \n    action sequence length                              1 int\n    saved because of {score, length, None (original)}   1 str\n    action sequence (tab separated)                     1 str (tab separated)\n\n    TODO: Probability\n    \"\"\"", "\n", "\n", "with", "open", "(", "file_path", ",", "'w'", ")", "as", "fid", ":", "\n", "        ", "for", "items", "in", "action_scores", ":", "\n", "            ", "sid", "=", "items", "[", "0", "]", "\n", "score", "=", "items", "[", "1", ":", "4", "]", "\n", "smatch", "=", "items", "[", "4", "]", "\n", "length", "=", "items", "[", "5", "]", "\n", "reason", "=", "items", "[", "6", "]", "\n", "actions", "=", "items", "[", "7", "]", "\n", "if", "actions", "is", "not", "None", ":", "\n", "                ", "actions", "=", "'\\t'", ".", "join", "(", "actions", ")", "\n", "", "fid", ".", "write", "(", "\n", "f'{sid} {score[0]} {score[1]} {score[2]} {smatch} {length} {reason} {actions}\\n'", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.io.read_amr": [[127, 156], ["transition_amr_parser.amr.JAMR_CorpusReader", "transition_amr_parser.amr.JAMR_CorpusReader.load_amrs", "enumerate", "new_tokens.append", "token.replace.replace", "replacement_rules.keys"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.JAMR_CorpusReader.load_amrs", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "", "", "def", "read_amr", "(", "in_amr", ",", "unicode_fixes", "=", "False", ")", ":", "\n", "\n", "    ", "corpus", "=", "JAMR_CorpusReader", "(", ")", "\n", "corpus", ".", "load_amrs", "(", "in_amr", ")", "\n", "\n", "if", "unicode_fixes", ":", "\n", "\n", "# Replacement rules for unicode chartacters", "\n", "        ", "replacement_rules", "=", "{", "\n", "'\u02c8t\u0283\u00e6r\u026ati'", ":", "'charity'", ",", "\n", "'\\x96'", ":", "'_'", ",", "\n", "'\u2299'", ":", "'O'", "\n", "}", "\n", "\n", "# FIXME: normalization shold be more robust. Right now use the tokens", "\n", "# of the amr inside the oracle. This is why we need to normalize them.", "\n", "for", "idx", ",", "amr", "in", "enumerate", "(", "corpus", ".", "amrs", ")", ":", "\n", "            ", "new_tokens", "=", "[", "]", "\n", "for", "token", "in", "amr", ".", "tokens", ":", "\n", "                ", "forbidden", "=", "[", "x", "for", "x", "in", "replacement_rules", ".", "keys", "(", ")", "if", "x", "in", "token", "]", "\n", "if", "forbidden", ":", "\n", "                    ", "token", "=", "token", ".", "replace", "(", "\n", "forbidden", "[", "0", "]", ",", "\n", "replacement_rules", "[", "forbidden", "[", "0", "]", "]", "\n", ")", "\n", "", "new_tokens", ".", "append", "(", "token", ")", "\n", "", "amr", ".", "tokens", "=", "new_tokens", "\n", "\n", "", "", "return", "corpus", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.io.read_rule_stats": [[158, 165], ["collections.Counter", "collections.Counter", "open", "json.loads", "fid.read"], "function", ["None"], ["", "def", "read_rule_stats", "(", "rule_stats_json", ")", ":", "\n", "    ", "with", "open", "(", "rule_stats_json", ")", "as", "fid", ":", "\n", "        ", "rule_stats", "=", "json", ".", "loads", "(", "fid", ".", "read", "(", ")", ")", "\n", "# convert to counters", "\n", "", "rule_stats", "[", "'possible_predicates'", "]", "=", "Counter", "(", "rule_stats", "[", "'possible_predicates'", "]", ")", "\n", "rule_stats", "[", "'action_vocabulary'", "]", "=", "Counter", "(", "rule_stats", "[", "'action_vocabulary'", "]", ")", "\n", "return", "rule_stats", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.io.write_rule_stats": [[167, 170], ["open", "fid.write", "json.dumps"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write"], ["", "def", "write_rule_stats", "(", "rule_stats_json", ",", "content", ")", ":", "\n", "    ", "with", "open", "(", "rule_stats_json", ",", "'w'", ")", "as", "fid", ":", "\n", "        ", "fid", ".", "write", "(", "json", ".", "dumps", "(", "content", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.io.read_propbank": [[172, 187], ["open", "line.rstrip.rstrip", "line.rstrip.split", "re.match().groups", "re.match", "line.rstrip.split", "re.match"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["", "", "def", "read_propbank", "(", "propbank_file", ")", ":", "\n", "\n", "# Read frame argument description", "\n", "    ", "arguments_by_sense", "=", "{", "}", "\n", "with", "open", "(", "propbank_file", ")", "as", "fid", ":", "\n", "        ", "for", "line", "in", "fid", ":", "\n", "            ", "line", "=", "line", ".", "rstrip", "(", ")", "\n", "sense", "=", "line", ".", "split", "(", ")", "[", "0", "]", "\n", "arguments", "=", "[", "\n", "re", ".", "match", "(", "'^(ARG.+):$'", ",", "x", ")", ".", "groups", "(", ")", "[", "0", "]", "\n", "for", "x", "in", "line", ".", "split", "(", ")", "[", "1", ":", "]", "if", "re", ".", "match", "(", "'^(ARG.+):$'", ",", "x", ")", "\n", "]", "\n", "arguments_by_sense", "[", "sense", "]", "=", "arguments", "\n", "\n", "", "", "return", "arguments_by_sense", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.io.writer": [[189, 217], ["open", "open.close", "open", "open.close", "open.write", "open.write"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.GoldMiner.close", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.GoldMiner.close", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write"], ["", "def", "writer", "(", "file_path", ",", "add_return", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Returns a writer that writes to file_path if it is not None, does nothing\n    otherwise\n\n    calling the writed without arguments will close the file\n    \"\"\"", "\n", "if", "file_path", ":", "\n", "# Erase file", "\n", "        ", "fid", "=", "open", "(", "file_path", ",", "'w+'", ")", "\n", "fid", ".", "close", "(", ")", "\n", "# open for appending", "\n", "fid", "=", "open", "(", "file_path", ",", "'a+'", ",", "encoding", "=", "'utf8'", ")", "\n", "", "else", ":", "\n", "        ", "fid", "=", "None", "\n", "\n", "", "def", "append_data", "(", "content", "=", "None", ")", ":", "\n", "        ", "\"\"\"writes to open file\"\"\"", "\n", "if", "fid", ":", "\n", "            ", "if", "content", "is", "None", ":", "\n", "                ", "fid", ".", "close", "(", ")", "\n", "", "else", ":", "\n", "                ", "if", "add_return", ":", "\n", "                    ", "fid", ".", "write", "(", "content", "+", "'\\n'", ")", "\n", "", "else", ":", "\n", "                    ", "fid", ".", "write", "(", "content", ")", "\n", "\n", "", "", "", "", "return", "append_data", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.io.tokenized_sentences_egenerator": [[219, 223], ["open", "line.rstrip().split", "line.rstrip"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["", "def", "tokenized_sentences_egenerator", "(", "file_path", ")", ":", "\n", "    ", "with", "open", "(", "file_path", ")", "as", "fid", ":", "\n", "        ", "for", "line", "in", "fid", ":", "\n", "            ", "yield", "line", ".", "rstrip", "(", ")", ".", "split", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.io.read_tokenized_sentences": [[225, 231], ["open", "sentences.append", "line.rstrip().split", "line.rstrip"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["", "", "", "def", "read_tokenized_sentences", "(", "file_path", ",", "separator", "=", "' '", ")", ":", "\n", "    ", "sentences", "=", "[", "]", "\n", "with", "open", "(", "file_path", ")", "as", "fid", ":", "\n", "        ", "for", "line", "in", "fid", ":", "\n", "            ", "sentences", ".", "append", "(", "line", ".", "rstrip", "(", ")", ".", "split", "(", "separator", ")", ")", "\n", "", "", "return", "sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.io.write_tokenized_sentences": [[233, 238], ["open", "fid.write", "str", "separator.join"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write"], ["", "def", "write_tokenized_sentences", "(", "file_path", ",", "content", ",", "separator", "=", "' '", ")", ":", "\n", "    ", "with", "open", "(", "file_path", ",", "'w'", ")", "as", "fid", ":", "\n", "        ", "for", "line", "in", "content", ":", "\n", "            ", "line", "=", "[", "str", "(", "x", ")", "for", "x", "in", "line", "]", "\n", "fid", ".", "write", "(", "f'{separator.join(line)}\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.io.read_sentences": [[240, 249], ["open", "line.rstrip.rstrip", "sentences.append"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "", "", "def", "read_sentences", "(", "file_path", ",", "add_root_token", "=", "False", ")", ":", "\n", "    ", "sentences", "=", "[", "]", "\n", "with", "open", "(", "file_path", ")", "as", "fid", ":", "\n", "        ", "for", "line", "in", "fid", ":", "\n", "            ", "line", "=", "line", ".", "rstrip", "(", ")", "\n", "if", "add_root_token", ":", "\n", "                ", "line", "=", "line", "+", "\" <ROOT>\"", "\n", "", "sentences", ".", "append", "(", "line", ")", "\n", "", "", "return", "sentences", "\n", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.GoldMiner.__init__": [[60, 74], ["gold_miner.read_amr", "set", "open"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.read_amr"], ["def", "__init__", "(", "self", ",", "ref_amr_path", ",", "out_actions_path", ",", "entity_rules", "=", "None", ")", ":", "\n", "\n", "# set entity_rules to be used", "\n", "        ", "self", ".", "entity_rules", "=", "entity_rules", "\n", "# read reference sentences ", "\n", "self", ".", "ref_amr_lines", "=", "read_amr", "(", "ref_amr_path", ")", "\n", "self", ".", "oracle_smatch_counts_cache", "=", "{", "}", "\n", "self", ".", "restart_num", "=", "10", "\n", "self", ".", "mined_actions", "=", "{", "}", "\n", "self", ".", "original_actions", "=", "{", "}", "\n", "self", ".", "saved_actions", "=", "set", "(", ")", "\n", "# start new file ", "\n", "self", ".", "out_actions_path", "=", "out_actions_path", "\n", "self", ".", "fid", "=", "open", "(", "self", ".", "out_actions_path", ",", "'w'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.GoldMiner.close": [[75, 77], ["gold_miner.GoldMiner.fid.close"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.GoldMiner.close"], ["", "def", "close", "(", "self", ")", ":", "\n", "        ", "self", ".", "fid", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.GoldMiner.update": [[78, 117], ["gold_miner.get_smatch_counts", "gold_miner.get_smatch_counts", "gold_miner.get_amr", "smatch.compute_f", "gold_miner.get_amr", "smatch.compute_f", "smatch.compute_f"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.get_smatch_counts", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.get_smatch_counts", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.get_amr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.get_amr"], ["", "def", "update", "(", "self", ",", "sample_id", ",", "tokens", ",", "oracle_actions", ",", "actions", ")", ":", "\n", "\n", "# compute smatch for the rule oracle or retrieve from cache", "\n", "        ", "if", "sample_id", "not", "in", "self", ".", "oracle_smatch_counts_cache", ":", "\n", "\n", "# compute smatch", "\n", "            ", "oracle_smatch_counts", "=", "get_smatch_counts", "(", "\n", "self", ".", "ref_amr_lines", "[", "sample_id", "]", ",", "\n", "get_amr", "(", "tokens", ",", "oracle_actions", ",", "self", ".", "entity_rules", ")", ",", "\n", "self", ".", "restart_num", "\n", ")", "\n", "oracle_smatch", "=", "smatch", ".", "compute_f", "(", "*", "oracle_smatch_counts", ")", "[", "2", "]", "\n", "\n", "# store actions", "\n", "self", ".", "original_actions", "[", "sample_id", "]", "=", "(", "oracle_actions", ",", "oracle_smatch_counts", ")", "\n", "\n", "# cache", "\n", "self", ".", "oracle_smatch_counts_cache", "[", "sample_id", "]", "=", "oracle_smatch_counts", "\n", "\n", "", "else", ":", "\n", "\n", "# If we outperform oracle keep the amr", "\n", "            ", "oracle_smatch_counts", "=", "self", ".", "oracle_smatch_counts_cache", "[", "sample_id", "]", "\n", "oracle_smatch", "=", "smatch", ".", "compute_f", "(", "*", "oracle_smatch_counts", ")", "[", "2", "]", "\n", "\n", "# compute smatch for hypothesis", "\n", "", "hypo_counts", "=", "get_smatch_counts", "(", "\n", "self", ".", "ref_amr_lines", "[", "sample_id", "]", ",", "\n", "get_amr", "(", "tokens", ",", "actions", ")", ",", "\n", "self", ".", "restart_num", "\n", ")", "\n", "hypo_smatch", "=", "smatch", ".", "compute_f", "(", "*", "hypo_counts", ")", "[", "2", "]", "\n", "\n", "# if hypothesis outperforms oracle, keep it", "\n", "if", "oracle_smatch", "<", "hypo_smatch", ":", "\n", "            ", "actions", "=", "[", "a", "for", "a", "in", "actions", "if", "a", "!=", "'</s>'", "]", "\n", "self", ".", "mined_actions", "[", "sample_id", "]", "=", "(", "actions", ",", "hypo_counts", ")", "\n", "\n", "", "return", "hypo_smatch", ",", "oracle_smatch", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.GoldMiner.save_actions": [[118, 145], ["set", "sorted", "gold_miner.GoldMiner.fid.close", "open", "gold_miner.GoldMiner.print_evaluation", "gold_miner.GoldMiner.original_actions.keys", "gold_miner.GoldMiner.saved_actions.add", "set", "gold_miner.GoldMiner.fid.write", "gold_miner.GoldMiner.fid.write"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.GoldMiner.close", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.GoldMiner.print_evaluation", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.add", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write"], ["", "def", "save_actions", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Save all what is collected until now\n        \"\"\"", "\n", "\n", "# index that are still not saved", "\n", "indices_seen", "=", "set", "(", "self", ".", "original_actions", ".", "keys", "(", ")", ")", "\n", "indices_to_dump", "=", "sorted", "(", "set", "(", "indices_seen", ")", "-", "self", ".", "saved_actions", ")", "\n", "\n", "# save new indices, if the id is in the mined actions, save that instead", "\n", "for", "did", "in", "indices_to_dump", ":", "\n", "# Skip saved actions", "\n", "            ", "if", "did", "in", "self", ".", "saved_actions", ":", "\n", "                ", "continue", "\n", "", "if", "did", "in", "self", ".", "mined_actions", ":", "\n", "                ", "self", ".", "fid", ".", "write", "(", "f'{did} '", "+", "'\\t'", ".", "join", "(", "self", ".", "mined_actions", "[", "did", "]", "[", "0", "]", ")", "+", "'\\n'", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "fid", ".", "write", "(", "f'{did} '", "+", "'\\t'", ".", "join", "(", "self", ".", "original_actions", "[", "did", "]", "[", "0", "]", ")", "+", "'\\n'", ")", "\n", "# note this sentence index down", "\n", "", "self", ".", "saved_actions", ".", "add", "(", "did", ")", "\n", "\n", "# save progress", "\n", "", "self", ".", "fid", ".", "close", "(", ")", "\n", "self", ".", "fid", "=", "open", "(", "self", ".", "out_actions_path", ",", "'a+'", ")", "\n", "\n", "# print progress", "\n", "self", ".", "print_evaluation", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.GoldMiner.print_evaluation": [[146, 164], ["gold_miner.GoldMiner.original_actions.keys", "gold_miner.smatch_from_counts", "gold_miner.smatch_from_counts", "print", "print", "original_counts.append", "new_counts.append", "new_counts.append"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.smatch_from_counts", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.smatch_from_counts", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "print_evaluation", "(", "self", ")", ":", "\n", "\n", "# compute overall smatch until this point", "\n", "        ", "original_counts", "=", "[", "]", "\n", "new_counts", "=", "[", "]", "\n", "for", "did", "in", "self", ".", "original_actions", ".", "keys", "(", ")", ":", "\n", "            ", "original_counts", ".", "append", "(", "self", ".", "original_actions", "[", "did", "]", "[", "1", "]", ")", "\n", "if", "did", "in", "self", ".", "mined_actions", ":", "\n", "                ", "new_counts", ".", "append", "(", "self", ".", "mined_actions", "[", "did", "]", "[", "1", "]", ")", "\n", "", "else", ":", "\n", "                ", "new_counts", ".", "append", "(", "self", ".", "original_actions", "[", "did", "]", "[", "1", "]", ")", "\n", "\n", "# compute smatch ", "\n", "", "", "original_smatch", "=", "smatch_from_counts", "(", "original_counts", ")", "\n", "new_smatch", "=", "smatch_from_counts", "(", "new_counts", ")", "\n", "\n", "print", "(", "'{:1.3f}'", ".", "format", "(", "original_smatch", ")", ")", "\n", "print", "(", "'{:1.3f}'", ".", "format", "(", "new_smatch", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.read_amr": [[9, 18], ["open", "amr.AMR.get_amr_line", "ref_amr_lines.append", "amr.AMR.get_amr_line"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["def", "read_amr", "(", "file_path", ")", ":", "\n", "# read all lines", "\n", "    ", "ref_amr_lines", "=", "[", "]", "\n", "with", "open", "(", "file_path", ",", "encoding", "=", "'utf8'", ")", "as", "fid", ":", "\n", "        ", "line", "=", "AMR", ".", "get_amr_line", "(", "fid", ")", "\n", "while", "line", ":", "\n", "            ", "ref_amr_lines", ".", "append", "(", "line", ")", "\n", "line", "=", "AMR", ".", "get_amr_line", "(", "fid", ")", "\n", "", "", "return", "ref_amr_lines", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.get_amr": [[20, 40], ["transition_amr_parser.state_machine.AMRStateMachine", "transition_amr_parser.state_machine.AMRStateMachine.amr.toJAMRString", "amr.AMR.get_amr_line", "transition_amr_parser.state_machine.AMRStateMachine.applyAction", "transition_amr_parser.utils.yellow_font", "print", "transition_amr_parser.state_machine.AMRStateMachine.CLOSE", "state_machine.amr.toJAMRString.split", "[].lower", "transition_amr_parser.state_machine.AMRStateMachine.get_top_of_stack"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.toJAMRString", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.DepParsingStateMachine.applyAction", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.yellow_font", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.CLOSE", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.get_top_of_stack"], ["", "def", "get_amr", "(", "tokens", ",", "actions", ",", "entity_rules", ")", ":", "\n", "\n", "# play state machine to get AMR", "\n", "    ", "state_machine", "=", "AMRStateMachine", "(", "tokens", ",", "entity_rules", "=", "entity_rules", ")", "\n", "for", "action", "in", "actions", ":", "\n", "# FIXME: It is unclear that this will be allways the right option", "\n", "# manual exploration of dev yielded 4 cases and it works for the 4", "\n", "        ", "if", "action", "==", "\"<unk>\"", ":", "\n", "            ", "action", "=", "f'PRED({state_machine.get_top_of_stack()[0].lower()})'", "\n", "", "state_machine", ".", "applyAction", "(", "action", ")", "\n", "\n", "# sanity check: foce close", "\n", "", "if", "not", "state_machine", ".", "is_closed", ":", "\n", "        ", "alert_str", "=", "yellow_font", "(", "'Machine not closed!'", ")", "\n", "print", "(", "alert_str", ")", "\n", "state_machine", ".", "CLOSE", "(", ")", "\n", "\n", "# TODO: Probably waisting ressources here", "\n", "", "amr_str", "=", "state_machine", ".", "amr", ".", "toJAMRString", "(", ")", "\n", "return", "AMR", ".", "get_amr_line", "(", "amr_str", ".", "split", "(", "'\\n'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.get_smatch_counts": [[42, 44], ["gold_miner.score_amr_pair"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.score_amr_pair"], ["", "def", "get_smatch_counts", "(", "ref_amr", ",", "rec_amr", ",", "restart_num", ",", "counts", "=", "False", ")", ":", "\n", "    ", "return", "score_amr_pair", "(", "ref_amr", ",", "rec_amr", ",", "restart_num", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.get_writer": [[46, 53], ["open", "fid.write"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write"], ["", "def", "get_writer", "(", "file_path", ")", ":", "\n", "    ", "with", "open", "(", "file_path", ",", "'w'", ")", "as", "fid", ":", "\n", "        ", "def", "writer", "(", "lines", ")", ":", "\n", "            ", "nonlocal", "fid", "\n", "for", "line", "in", "lines", ":", "\n", "                ", "fid", ".", "write", "(", "f'{line}\\n'", ")", "\n", "", "", "", "return", "writer", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.smatch_from_counts": [[166, 173], ["sum", "sum", "sum", "smatch.compute_f"], "function", ["None"], ["", "", "def", "smatch_from_counts", "(", "counts", ")", ":", "\n", "    ", "total_counts", "=", "[", "\n", "sum", "(", "[", "x", "[", "0", "]", "for", "x", "in", "counts", "]", ")", ",", "\n", "sum", "(", "[", "x", "[", "1", "]", "for", "x", "in", "counts", "]", ")", ",", "\n", "sum", "(", "[", "x", "[", "2", "]", "for", "x", "in", "counts", "]", ")", "\n", "]", "\n", "return", "smatch", ".", "compute_f", "(", "*", "total_counts", ")", "[", "2", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.get_smatch_reward": [[175, 235], ["gold_miner.read_amr", "enumerate", "sample[].tolist", "hypo_smatch_counts.append", "gold_miner.get_smatch_counts", "ipdb.set_trace", "print", "sample[].tolist", "gold_miner.get_amr", "gold_miner.get_smatch_counts", "gold_miner.get_amr"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.read_amr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.get_smatch_counts", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.pdb.set_trace", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.get_amr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.get_smatch_counts", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.get_amr"], ["", "def", "get_smatch_reward", "(", "ref_amr_path", ",", "task", ",", "restart_num", "=", "10", ")", ":", "\n", "    ", "\"\"\"\n    Closure returning a function that computes smatch given sample batch and\n    corresponding hypothesis\n\n    It also computes oracle score for reference, which is cached for speed\n    \"\"\"", "\n", "\n", "# read entire data first", "\n", "ref_amr_lines", "=", "read_amr", "(", "ref_amr_path", ")", "\n", "\n", "# cache for smatch of the rule-based oracle (which is fixed)", "\n", "oracle_smatch_counts_cache", "=", "{", "}", "\n", "\n", "def", "smatch_reward", "(", "sample", ",", "hypos", ")", ":", "\n", "\n", "        ", "nonlocal", "oracle_smatch_counts_cache", "\n", "nonlocal", "task", "\n", "nonlocal", "restart_num", "\n", "\n", "# Loop over batch sentences:", "\n", "hypo_smatch_counts", "=", "[", "]", "\n", "for", "i", ",", "sample_id", "in", "enumerate", "(", "sample", "[", "'id'", "]", ".", "tolist", "(", ")", ")", ":", "\n", "\n", "            ", "src_tokens", "=", "sample", "[", "'net_input'", "]", "[", "'src_tokens'", "]", "[", "i", ",", ":", "]", "\n", "\n", "# compute smatch for the rule oracle or retrieve from cache", "\n", "if", "sample_id", "not", "in", "oracle_smatch_counts_cache", ":", "\n", "\n", "\n", "# compute smatch", "\n", "                ", "oracle_smatch_counts_cache", "[", "sample_id", "]", "=", "get_smatch_counts", "(", "\n", "ref_amr_lines", "[", "sample_id", "]", ",", "\n", "get_amr", "(", "src_tokens", ",", "sample", "[", "'target'", "]", "[", "i", ",", ":", "]", ",", "task", ")", ",", "\n", "restart_num", "\n", ")", "\n", "\n", "# compute smatch for hypothesis", "\n", "# get amr", "\n", "", "hypo_smatch_counts", ".", "append", "(", "[", "\n", "get_smatch_counts", "(", "\n", "ref_amr_lines", "[", "sample_id", "]", ",", "\n", "get_amr", "(", "src_tokens", ",", "None", ",", "task", ",", "hypo", "=", "hypo", ")", ",", "\n", "restart_num", "\n", ")", "\n", "for", "hypo", "in", "hypos", "[", "i", "]", "\n", "]", ")", "\n", "\n", "if", "oracle_smatch_counts_cache", "[", "sample_id", "]", "<", "hypo_smatch_counts", "[", "-", "1", "]", "[", "0", "]", ":", "\n", "                ", "import", "ipdb", ";", "ipdb", ".", "set_trace", "(", "context", "=", "30", ")", "\n", "print", "(", ")", "\n", "\n", "", "", "oracle_smatch", "=", "[", "\n", "oracle_smatch_counts_cache", "[", "sample_id", "]", "\n", "for", "sample_id", "in", "sample", "[", "'id'", "]", ".", "tolist", "(", ")", "\n", "]", "\n", "\n", "return", "hypo_smatch_counts", ",", "oracle_smatch", "\n", "\n", "", "return", "smatch_reward", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.score_amr_pair": [[237, 291], ["amr.AMR.parse_AMR_line", "amr.AMR.parse_AMR_line", "AMR.parse_AMR_line.rename_node", "AMR.parse_AMR_line.rename_node", "AMR.parse_AMR_line.get_triples", "AMR.parse_AMR_line.get_triples", "smatch.get_best_match", "len", "len", "len", "len", "len", "len", "len", "AMR.parse_AMR_line.get_triples", "len", "len", "len", "len", "len", "len"], "function", ["None"], ["", "def", "score_amr_pair", "(", "ref_amr_line", ",", "rec_amr_line", ",", "restart_num", ",", "justinstance", "=", "False", ",", "\n", "justattribute", "=", "False", ",", "justrelation", "=", "False", ")", ":", "\n", "\n", "# parse lines", "\n", "    ", "amr1", "=", "AMR", ".", "parse_AMR_line", "(", "ref_amr_line", ")", "\n", "amr2", "=", "AMR", ".", "parse_AMR_line", "(", "rec_amr_line", ")", "\n", "\n", "if", "amr2", "is", "None", ":", "\n", "        ", "return", "0", ",", "0", ",", "len", "(", "amr1", ".", "get_triples", "(", ")", "[", "0", "]", ")", "\n", "\n", "# Fix prefix", "\n", "", "prefix1", "=", "\"a\"", "\n", "prefix2", "=", "\"b\"", "\n", "# Rename node to \"a1\", \"a2\", .etc", "\n", "amr1", ".", "rename_node", "(", "prefix1", ")", "\n", "# Renaming node to \"b1\", \"b2\", .etc", "\n", "amr2", ".", "rename_node", "(", "prefix2", ")", "\n", "\n", "# get triples", "\n", "(", "instance1", ",", "attributes1", ",", "relation1", ")", "=", "amr1", ".", "get_triples", "(", ")", "\n", "(", "instance2", ",", "attributes2", ",", "relation2", ")", "=", "amr2", ".", "get_triples", "(", ")", "\n", "\n", "# optionally turn off some of the node comparison", "\n", "doinstance", "=", "doattribute", "=", "dorelation", "=", "True", "\n", "if", "justinstance", ":", "\n", "        ", "doattribute", "=", "dorelation", "=", "False", "\n", "", "if", "justattribute", ":", "\n", "        ", "doinstance", "=", "dorelation", "=", "False", "\n", "", "if", "justrelation", ":", "\n", "        ", "doinstance", "=", "doattribute", "=", "False", "\n", "\n", "", "(", "best_mapping", ",", "best_match_num", ")", "=", "smatch", ".", "get_best_match", "(", "\n", "instance1", ",", "attributes1", ",", "relation1", ",", "\n", "instance2", ",", "attributes2", ",", "relation2", ",", "\n", "prefix1", ",", "prefix2", ",", "\n", "restart_num", ",", "\n", "doinstance", "=", "doinstance", ",", "\n", "doattribute", "=", "doattribute", ",", "\n", "dorelation", "=", "dorelation", "\n", ")", "\n", "\n", "if", "justinstance", ":", "\n", "        ", "test_triple_num", "=", "len", "(", "instance1", ")", "\n", "gold_triple_num", "=", "len", "(", "instance2", ")", "\n", "", "elif", "justattribute", ":", "\n", "        ", "test_triple_num", "=", "len", "(", "attributes1", ")", "\n", "gold_triple_num", "=", "len", "(", "attributes2", ")", "\n", "", "elif", "justrelation", ":", "\n", "        ", "test_triple_num", "=", "len", "(", "relation1", ")", "\n", "gold_triple_num", "=", "len", "(", "relation2", ")", "\n", "", "else", ":", "\n", "        ", "test_triple_num", "=", "len", "(", "instance1", ")", "+", "len", "(", "attributes1", ")", "+", "len", "(", "relation1", ")", "\n", "gold_triple_num", "=", "len", "(", "instance2", ")", "+", "len", "(", "attributes2", ")", "+", "len", "(", "relation2", ")", "\n", "", "return", "best_match_num", ",", "test_triple_num", ",", "gold_triple_num", "\n", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.parse.argument_parsing": [[12, 71], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "bool", "bool"], "function", ["None"], ["    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "description", "=", "'Call parser from the command line'", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'-i'", ",", "'--in-tokenized-sentences'", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "'File with one __tokenized__ sentence per line'", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'-c'", ",", "'--in-checkpoint'", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "'one fairseq model checkpoint (or various, separated by :)'", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'-o'", ",", "'--out-amr'", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "'File to store AMR in PENNMAN format'", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--roberta-batch-size'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "10", ",", "\n", "help", "=", "'Batch size for roberta computation (watch for OOM)'", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--batch-size'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "128", ",", "\n", "help", "=", "'Batch size for decoding (excluding roberta)'", "\n", ")", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n", "\n", "", "def", "main", "(", ")", ":", "\n", "\n", "# argument handling", "\n", "    ", "args", "=", "argument_parsing", "(", ")", "\n", "\n", "# read tokenized sentences", "\n", "sentences", "=", "read_sentences", "(", "args", ".", "in_tokenized_sentences", ")", "\n", "split_sentences", "=", "[", "]", "\n", "for", "sentence", "in", "sentences", ":", "\n", "        ", "split_sentences", ".", "append", "(", "tokenize_line", "(", "sentence", ")", ")", "\n", "", "print", "(", "len", "(", "split_sentences", ")", ")", "\n", "\n", "# load parser", "\n", "start", "=", "time", ".", "time", "(", ")", "\n", "parser", "=", "AMRParser", ".", "from_checkpoint", "(", "args", ".", "in_checkpoint", ")", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "time_secs", "=", "timedelta", "(", "seconds", "=", "float", "(", "end", "-", "start", ")", ")", "\n", "print", "(", "f'Total time taken to load parser: {time_secs}'", ")", "\n", "\n", "# TODO: max batch sizes could be computed from max sentence length", "\n", "\n", "# parse", "\n", "start", "=", "time", ".", "time", "(", ")", "\n", "result", "=", "parser", ".", "parse_sentences", "(", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.parse.ordered_exit": [[73, 76], ["print", "exit"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["batch_size", "=", "args", ".", "batch_size", ",", "\n", "roberta_batch_size", "=", "args", ".", "roberta_batch_size", ",", "\n", ")", "\n", "end", "=", "time", ".", "time", "(", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.parse.parse_sentences": [[78, 105], ["transition_amr_parser.io.read_sentences", "print", "time.time", "parser.parse_sentences", "time.time", "print", "datetime.timedelta", "print", "split_sentences.append", "len", "len", "fairseq.tokenizer.tokenize_line", "float", "open", "range", "len", "fid.write"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.io.read_sentences", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_transformer_amr_parser.AMRParser.parse_sentences", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.tokenizer.tokenize_line", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write"], ["time_secs", "=", "timedelta", "(", "seconds", "=", "float", "(", "end", "-", "start", ")", ")", "\n", "print", "(", "f'Total time taken to parse sentences: {time_secs}'", ")", "\n", "\n", "# write annotations", "\n", "with", "open", "(", "args", ".", "out_amr", ",", "'w'", ")", "as", "fid", ":", "\n", "        ", "for", "i", "in", "range", "(", "0", ",", "len", "(", "sentences", ")", ")", ":", "\n", "            ", "fid", ".", "write", "(", "result", "[", "i", "]", ")", "\n", "\n", "\n", "", "", "", "if", "__name__", "==", "'__main__'", ":", "\n", "    ", "main", "(", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.parse.simple_inspector": [[107, 115], ["os.system", "print", "input"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.parse.breakpoint_inspector": [[117, 125], ["os.system", "print", "pdb.set_trace"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.pdb.set_trace"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.parse.main": [[127, 173], ["parse.argument_parsing", "time.time", "transition_amr_parser.stack_transformer_amr_parser.AMRParser.from_checkpoint", "time.time", "datetime.timedelta", "print", "signal.signal", "signal.signal", "parse.parse_sentences", "float", "input", "os.system", "AMRParser.from_checkpoint.parse_sentences", "os.system", "print", "print", "input.strip", "input.split"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.parse.argument_parsing", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_transformer_amr_parser.AMRParser.from_checkpoint", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_transformer_amr_parser.AMRParser.parse_sentences", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_transformer_amr_parser.AMRParser.parse_sentences", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr_parser.AMRParser.__init__": [[14, 28], ["amr_parser.AMRParser.load_model", "amr_parser.AMRParser.load_roberta", "os.path.dirname", "os.path.join", "os.path.isfile", "os.path.dirname", "os.path.join", "os.path.isfile"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr_parser.AMRParser.load_model", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_transformer_amr_parser.load_roberta"], ["    ", "def", "__init__", "(", "self", ",", "model_path", ",", "roberta_cache_path", "=", "None", ",", "oracle_stats_path", "=", "None", ",", "config_path", "=", "None", ",", "model_use_gpu", "=", "False", ",", "roberta_use_gpu", "=", "False", ",", "verbose", "=", "False", ",", "logger", "=", "None", ")", ":", "\n", "        ", "if", "not", "oracle_stats_path", ":", "\n", "            ", "model_folder", "=", "os", ".", "path", ".", "dirname", "(", "model_path", ")", "\n", "oracle_stats_path", "=", "os", ".", "path", ".", "join", "(", "model_folder", ",", "\"train.rules.json\"", ")", "\n", "assert", "os", ".", "path", ".", "isfile", "(", "oracle_stats_path", ")", ",", "f'Expected train.rules.json in {model_folder}'", "\n", "", "if", "not", "config_path", ":", "\n", "            ", "model_folder", "=", "os", ".", "path", ".", "dirname", "(", "model_path", ")", "\n", "config_path", "=", "os", ".", "path", ".", "join", "(", "model_folder", ",", "\"config.json\"", ")", "\n", "assert", "os", ".", "path", ".", "isfile", "(", "config_path", ")", ",", "f'Expected config.json in {model_folder}'", "\n", "", "self", ".", "model", "=", "self", ".", "load_model", "(", "model_path", ",", "oracle_stats_path", ",", "config_path", ",", "model_use_gpu", ")", "\n", "self", ".", "roberta", "=", "self", ".", "load_roberta", "(", "roberta_use_gpu", ",", "roberta_cache_path", ")", "\n", "self", ".", "logger", "=", "logger", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr_parser.AMRParser.load_roberta": [[29, 40], ["fairseq.models.roberta.RobertaModel.from_pretrained.eval", "torch.hub.load", "fairseq.models.roberta.RobertaModel.from_pretrained", "fairseq.models.roberta.RobertaModel.from_pretrained.cuda"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.model_gottbert.GottbertModel.from_pretrained"], ["", "def", "load_roberta", "(", "self", ",", "roberta_use_gpu", ",", "roberta_cache_path", "=", "None", ")", ":", "\n", "\n", "        ", "if", "not", "roberta_cache_path", ":", "\n", "# Load the Roberta Model from torch hub", "\n", "            ", "roberta", "=", "torch", ".", "hub", ".", "load", "(", "'pytorch/fairseq'", ",", "'roberta.large'", ")", "\n", "", "else", ":", "\n", "            ", "roberta", "=", "RobertaModel", ".", "from_pretrained", "(", "roberta_cache_path", ",", "checkpoint_file", "=", "'model.pt'", ")", "\n", "", "roberta", ".", "eval", "(", ")", "\n", "if", "roberta_use_gpu", ":", "\n", "            ", "roberta", ".", "cuda", "(", ")", "\n", "", "return", "roberta", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr_parser.AMRParser.load_model": [[41, 69], ["json.load", "json.load", "transition_amr_parser.model.AMRModel", "transition_amr_parser.model.AMRModel.load_state_dict", "transition_amr_parser.model.AMRModel.eval", "open", "open", "torch.load"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.lr_scheduler.fairseq_lr_scheduler.FairseqLRScheduler.load_state_dict", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load"], ["", "def", "load_model", "(", "self", ",", "model_path", ",", "oracle_stats_path", ",", "config_path", ",", "model_use_gpu", ")", ":", "\n", "\n", "        ", "oracle_stats", "=", "json", ".", "load", "(", "open", "(", "oracle_stats_path", ")", ")", "\n", "config", "=", "json", ".", "load", "(", "open", "(", "config_path", ")", ")", "\n", "model", "=", "AMRModel", "(", "\n", "oracle_stats", "=", "oracle_stats", ",", "\n", "embedding_dim", "=", "config", "[", "\"embedding_dim\"", "]", ",", "\n", "action_embedding_dim", "=", "config", "[", "\"action_embedding_dim\"", "]", ",", "\n", "char_embedding_dim", "=", "config", "[", "\"char_embedding_dim\"", "]", ",", "\n", "hidden_dim", "=", "config", "[", "\"hidden_dim\"", "]", ",", "\n", "char_hidden_dim", "=", "config", "[", "\"char_hidden_dim\"", "]", ",", "\n", "rnn_layers", "=", "config", "[", "\"rnn_layers\"", "]", ",", "\n", "dropout_ratio", "=", "config", "[", "\"dropout_ratio\"", "]", ",", "\n", "pretrained_dim", "=", "config", "[", "\"pretrained_dim\"", "]", ",", "\n", "use_bert", "=", "config", "[", "\"use_bert\"", "]", ",", "\n", "use_gpu", "=", "model_use_gpu", ",", "\n", "use_chars", "=", "config", "[", "\"use_chars\"", "]", ",", "\n", "use_attention", "=", "config", "[", "\"use_attention\"", "]", ",", "\n", "use_function_words", "=", "config", "[", "\"use_function_words\"", "]", ",", "\n", "use_function_words_rels", "=", "config", "[", "\"use_function_words_rels\"", "]", ",", "\n", "parse_unaligned", "=", "config", "[", "\"parse_unaligned\"", "]", ",", "\n", "weight_inputs", "=", "config", "[", "\"weight_inputs\"", "]", ",", "\n", "attend_inputs", "=", "config", "[", "\"attend_inputs\"", "]", "\n", ")", "\n", "\n", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "model_path", ")", ")", "\n", "model", ".", "eval", "(", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr_parser.AMRParser.get_embeddings": [[70, 78], ["transition_amr_parser.roberta_utils.extract_features_aligned_to_words", "torch.stack().detach().cpu().numpy", "str", "torch.stack().detach().cpu().numpy.append", "torch.stack().detach().cpu", "torch.stack().detach", "torch.stack"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.roberta_utils.extract_features_aligned_to_words", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "get_embeddings", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "features", "=", "extract_features_aligned_to_words", "(", "self", ".", "roberta", ",", "tokens", "=", "tokens", ",", "use_all_layers", "=", "True", ",", "return_all_hiddens", "=", "True", ")", "\n", "embeddings", "=", "[", "]", "\n", "for", "tok", "in", "features", ":", "\n", "            ", "if", "str", "(", "tok", ")", "not", "in", "[", "'<s>'", ",", "'</s>'", "]", ":", "\n", "                ", "embeddings", ".", "append", "(", "tok", ".", "vector", ")", "\n", "", "", "embeddings", "=", "torch", ".", "stack", "(", "embeddings", ")", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "return", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr_parser.AMRParser.parse_sentence": [[79, 87], ["transition_amr_parser.vectorize_words", "amr_parser.AMRParser.get_embeddings", "amr_parser.AMRParser.model.parse_sentence", "tokens.append"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.vectorize_words", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr_parser.AMRParser.get_embeddings", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.fake_parse.FakeAMRParser.parse_sentence", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "parse_sentence", "(", "self", ",", "tokens", ")", ":", "\n", "# The model expects <ROOT> token at the end of the input sentence", "\n", "        ", "if", "tokens", "[", "-", "1", "]", "!=", "\"<ROOT>\"", ":", "\n", "            ", "tokens", ".", "append", "(", "\"<ROOT>\"", ")", "\n", "", "sent_rep", "=", "utils", ".", "vectorize_words", "(", "self", ".", "model", ",", "tokens", ",", "training", "=", "False", ",", "gpu", "=", "self", ".", "model", ".", "use_gpu", ")", "\n", "bert_emb", "=", "self", ".", "get_embeddings", "(", "tokens", ")", "\n", "amr", "=", "self", ".", "model", ".", "parse_sentence", "(", "tokens", ",", "sent_rep", ",", "bert_emb", ")", "\n", "return", "amr", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.__init__": [[40, 58], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "tokens", "=", "None", ",", "root", "=", "''", ",", "nodes", "=", "None", ",", "edges", "=", "None", ",", "alignments", "=", "None", ",", "score", "=", "0.0", ")", ":", "\n", "\n", "        ", "if", "alignments", "is", "None", ":", "\n", "            ", "alignments", "=", "{", "}", "\n", "", "if", "edges", "is", "None", ":", "\n", "            ", "edges", "=", "[", "]", "\n", "", "if", "nodes", "is", "None", ":", "\n", "            ", "nodes", "=", "{", "}", "\n", "", "if", "tokens", "is", "None", ":", "\n", "            ", "tokens", "=", "[", "]", "\n", "", "self", ".", "tokens", "=", "tokens", "\n", "self", ".", "root", "=", "root", "\n", "self", ".", "nodes", "=", "nodes", "\n", "self", ".", "edges", "=", "edges", "\n", "self", ".", "alignments", "=", "alignments", "\n", "self", ".", "score", "=", "score", "\n", "\n", "self", ".", "token2node_memo", "=", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.__str__": [[59, 92], ["r.replace.replace.replace", "type", "sorted", "list"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "output", "=", "''", "\n", "# tokens", "\n", "output", "+=", "'# ::tok '", "+", "(", "' '", ".", "join", "(", "self", ".", "tokens", ")", ")", "+", "'\\n'", "\n", "# score", "\n", "if", "self", ".", "score", ":", "\n", "            ", "output", "+=", "f'# ::scr\\t{self.score}\\n'", "\n", "# nodes", "\n", "", "for", "n", "in", "self", ".", "nodes", ":", "\n", "            ", "alignment", "=", "''", "\n", "if", "n", "in", "self", ".", "alignments", "and", "self", ".", "alignments", "[", "n", "]", ":", "\n", "                ", "if", "type", "(", "self", ".", "alignments", "[", "n", "]", ")", "==", "int", ":", "\n", "                    ", "alignment", "=", "f'\\t{self.alignments[n]-1}-{self.alignments[n]}'", "\n", "", "else", ":", "\n", "                    ", "alignments_in_order", "=", "sorted", "(", "list", "(", "self", ".", "alignments", "[", "n", "]", ")", ")", "\n", "alignment", "=", "f'\\t{alignments_in_order[0]-1}-{alignments_in_order[-1]}'", "\n", "", "", "output", "+=", "f'# ::node\\t{n}\\t{self.nodes[n] if n in self.nodes else \"None\"}'", "+", "alignment", "+", "'\\n'", "\n", "# root", "\n", "", "root", "=", "self", ".", "root", "\n", "alignment", "=", "''", "\n", "# if root in self.alignments and self.alignments[root]:", "\n", "#     if type(self.alignments[root]) == int:", "\n", "#         alignment = f'\\t{self.alignments[root]-1}-{self.alignments[root]}'", "\n", "#     else:", "\n", "#         alignments_in_order = sorted(list(self.alignments[root]))", "\n", "#         alignment = f'\\t{alignments_in_order[0]-1}-{alignments_in_order[-1]}'", "\n", "if", "self", ".", "root", ":", "\n", "            ", "output", "+=", "f'# ::root\\t{root}\\t{self.nodes[root] if root in self.nodes else \"None\"}'", "+", "alignment", "+", "'\\n'", "\n", "# edges", "\n", "", "for", "s", ",", "r", ",", "t", "in", "self", ".", "edges", ":", "\n", "            ", "r", "=", "r", ".", "replace", "(", "':'", ",", "''", ")", "\n", "output", "+=", "f'# ::edge\\t{self.nodes[s] if s in self.nodes else \"None\"}\\t{r}\\t{self.nodes[t] if t in self.nodes else \"None\"}\\t{s}\\t{t}\\t\\n'", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split": [[93, 118], ["amr.find_subgraph_edges", "amr.AMR.subgraph_from_edges", "amr.AMR.subgraph_from_edges", "father_subgraph_edges.append"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.find_subgraph_edges", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.subgraph_from_edges", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.subgraph_from_edges", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "split", "(", "self", ",", "split_node_id", ")", ":", "\n", "        ", "'''\n        Return two AMR graphs resulting from splitting a graph by a node.\n        '''", "\n", "assert", "split_node_id", "in", "self", ".", "nodes", "\n", "\n", "# Child subgraph edges", "\n", "child_subgraph_edges", "=", "find_subgraph_edges", "(", "self", ".", "edges", ",", "split_node_id", ")", "\n", "# Parent subgraph edges", "\n", "father_subgraph_edges", "=", "[", "]", "\n", "for", "edge", "in", "self", ".", "edges", ":", "\n", "            ", "if", "edge", "not", "in", "child_subgraph_edges", ":", "\n", "                ", "father_subgraph_edges", ".", "append", "(", "edge", ")", "\n", "\n", "# Instantiate clases", "\n", "", "", "child_amr", "=", "self", ".", "subgraph_from_edges", "(", "\n", "split_node_id", ",", "\n", "child_subgraph_edges", "\n", ")", "\n", "parent_amr", "=", "self", ".", "subgraph_from_edges", "(", "\n", "self", ".", "root", ",", "\n", "father_subgraph_edges", "\n", ")", "\n", "\n", "return", "parent_amr", ",", "child_amr", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.subgraph_from_edges": [[120, 151], ["set", "amr.AMR", "set", "set", "amr.AMR.nodes.items", "amr.AMR.alignments.items"], "methods", ["None"], ["", "def", "subgraph_from_edges", "(", "self", ",", "root_id", ",", "subgraph_edges", ")", ":", "\n", "\n", "# Get node ids", "\n", "        ", "subgraph_node_ids", "=", "set", "(", ")", "\n", "for", "e", "in", "subgraph_edges", ":", "\n", "            ", "subgraph_node_ids", "|=", "set", "(", "[", "e", "[", "0", "]", "]", ")", "\n", "subgraph_node_ids", "|=", "set", "(", "[", "e", "[", "2", "]", "]", ")", "\n", "\n", "# get nodes", "\n", "", "subgraph_nodes", "=", "{", "\n", "node_id", ":", "node", "\n", "for", "node_id", ",", "node", "in", "self", ".", "nodes", ".", "items", "(", ")", "\n", "if", "node_id", "in", "subgraph_node_ids", "\n", "}", "\n", "\n", "# get alignments", "\n", "subgraph_alignments", "=", "{", "\n", "node_id", ":", "alignments", "\n", "for", "node_id", ",", "alignments", "in", "self", ".", "alignments", ".", "items", "(", ")", "\n", "if", "node_id", "in", "subgraph_node_ids", "\n", "}", "\n", "\n", "# we use all the tokens as we do not know where the boundaries are.", "\n", "# Also alignments refer to absolute positions.", "\n", "\n", "return", "AMR", "(", "\n", "root", "=", "root_id", ",", "\n", "tokens", "=", "self", ".", "tokens", ",", "\n", "edges", "=", "subgraph_edges", ",", "\n", "nodes", "=", "subgraph_nodes", ",", "\n", "alignments", "=", "subgraph_alignments", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.get_entity_nodes": [[153, 162], ["entity_ids.append"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "get_entity_nodes", "(", "self", ")", ":", "\n", "        ", "entity_ids", "=", "[", "]", "\n", "for", "t", "in", "self", ".", "edges", ":", "\n", "            ", "if", "t", "[", "1", "]", "==", "':name'", "and", "self", ".", "nodes", "[", "t", "[", "2", "]", "]", "==", "'name'", ":", "\n", "                ", "root_node", "=", "t", "[", "0", "]", "\n", "name_node", "=", "t", "[", "2", "]", "\n", "name_nodes", "=", "[", "t", "[", "2", "]", "for", "t", "in", "self", ".", "edges", "if", "t", "[", "0", "]", "==", "name_node", "]", "\n", "entity_ids", ".", "append", "(", "[", "root_node", ",", "name_nodes", "]", ")", "\n", "", "", "return", "entity_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.alignmentsToken2Node": [[163, 167], ["sorted"], "methods", ["None"], ["", "def", "alignmentsToken2Node", "(", "self", ",", "token_id", ")", ":", "\n", "        ", "if", "token_id", "not", "in", "self", ".", "token2node_memo", ":", "\n", "            ", "self", ".", "token2node_memo", "[", "token_id", "]", "=", "sorted", "(", "[", "node_id", "for", "node_id", "in", "self", ".", "alignments", "if", "token_id", "in", "self", ".", "alignments", "[", "node_id", "]", "]", ")", "\n", "", "return", "self", ".", "token2node_memo", "[", "token_id", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.copy": [[168, 170], ["amr.AMR", "amr.AMR.tokens.copy", "amr.AMR.nodes.copy", "amr.AMR.edges.copy", "amr.AMR.alignments.copy"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.copy", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.copy", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.copy", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.copy"], ["", "def", "copy", "(", "self", ")", ":", "\n", "        ", "return", "AMR", "(", "self", ".", "tokens", ".", "copy", "(", ")", ",", "self", ".", "root", ",", "self", ".", "nodes", ".", "copy", "(", ")", ",", "self", ".", "edges", ".", "copy", "(", ")", ",", "self", ".", "alignments", ".", "copy", "(", ")", ",", "self", ".", "score", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.findSubGraph": [[175, 189], ["node_ids.copy", "amr.AMR", "amr.AMR", "sg_edges.append", "len", "node_ids.copy.remove"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.copy", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["def", "findSubGraph", "(", "self", ",", "node_ids", ")", ":", "\n", "        ", "if", "not", "node_ids", ":", "\n", "            ", "return", "AMR", "(", ")", "\n", "", "potential_root", "=", "node_ids", ".", "copy", "(", ")", "\n", "sg_edges", "=", "[", "]", "\n", "for", "x", ",", "r", ",", "y", "in", "self", ".", "edges", ":", "\n", "            ", "if", "x", "in", "node_ids", "and", "y", "in", "node_ids", ":", "\n", "                ", "sg_edges", ".", "append", "(", "(", "x", ",", "r", ",", "y", ")", ")", "\n", "if", "y", "in", "potential_root", ":", "\n", "                    ", "potential_root", ".", "remove", "(", "y", ")", "\n", "", "", "", "root", "=", "potential_root", "[", "0", "]", "if", "len", "(", "potential_root", ")", ">", "0", "else", "node_ids", "[", "0", "]", "\n", "return", "AMR", "(", "root", "=", "root", ",", "\n", "edges", "=", "sg_edges", ",", "\n", "nodes", "=", "{", "n", ":", "self", ".", "nodes", "[", "n", "]", "for", "n", "in", "node_ids", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.toJAMRString": [[190, 260], ["str", "set", "nodes.copy", "new_id.isalpha", "new_id.islower", "sorted", "set", "amr_string.replace.replace.replace", "nodes.remove", "nodes.update", "len", "len", "amr.InvalidAMRError", "amr_string.replace.replace.startswith", "amr_string[].isdigit", "amr_string.replace.replace.startswith", "len", "new_ids.values", "new_ids.values", "set.add", "str", "new_ids.values", "amr_string.replace.replace.replace", "amr_string.replace.replace.replace", "str.split", "concept[].isalpha"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.copy", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.add", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["", "def", "toJAMRString", "(", "self", ",", "only_penman", "=", "False", ",", "allow_incomplete", "=", "False", ")", ":", "\n", "        ", "output", "=", "str", "(", "self", ")", "\n", "\n", "# amr string", "\n", "amr_string", "=", "f'[[{self.root}]]'", "\n", "new_ids", "=", "{", "}", "\n", "for", "n", "in", "self", ".", "nodes", ":", "\n", "            ", "new_id", "=", "self", ".", "nodes", "[", "n", "]", "[", "0", "]", "if", "self", ".", "nodes", "[", "n", "]", "else", "'x'", "\n", "if", "new_id", ".", "isalpha", "(", ")", "and", "new_id", ".", "islower", "(", ")", ":", "\n", "                ", "if", "new_id", "in", "new_ids", ".", "values", "(", ")", ":", "\n", "                    ", "j", "=", "2", "\n", "while", "f'{new_id}{j}'", "in", "new_ids", ".", "values", "(", ")", ":", "\n", "                        ", "j", "+=", "1", "\n", "", "new_id", "=", "f'{new_id}{j}'", "\n", "", "", "else", ":", "\n", "                ", "j", "=", "0", "\n", "while", "f'x{j}'", "in", "new_ids", ".", "values", "(", ")", ":", "\n", "                    ", "j", "+=", "1", "\n", "", "new_id", "=", "f'x{j}'", "\n", "", "new_ids", "[", "n", "]", "=", "new_id", "\n", "", "depth", "=", "1", "\n", "nodes", "=", "{", "self", ".", "root", "}", "\n", "completed", "=", "set", "(", ")", "\n", "while", "'[['", "in", "amr_string", ":", "\n", "            ", "tab", "=", "'      '", "*", "depth", "\n", "for", "n", "in", "nodes", ".", "copy", "(", ")", ":", "\n", "                ", "id", "=", "new_ids", "[", "n", "]", "if", "n", "in", "new_ids", "else", "'r91'", "\n", "concept", "=", "self", ".", "nodes", "[", "n", "]", "if", "n", "in", "new_ids", "and", "self", ".", "nodes", "[", "n", "]", "else", "'None'", "\n", "edges", "=", "sorted", "(", "[", "e", "for", "e", "in", "self", ".", "edges", "if", "e", "[", "0", "]", "==", "n", "]", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ")", "\n", "targets", "=", "set", "(", "t", "for", "s", ",", "r", ",", "t", "in", "edges", ")", "\n", "edges", "=", "[", "f'{r} [[{t}]]'", "for", "s", ",", "r", ",", "t", "in", "edges", "]", "\n", "children", "=", "f'\\n{tab}'", ".", "join", "(", "edges", ")", "\n", "if", "children", ":", "\n", "                    ", "children", "=", "f'\\n{tab}'", "+", "children", "\n", "", "if", "n", "not", "in", "completed", ":", "\n", "                    ", "if", "(", "concept", "[", "0", "]", ".", "isalpha", "(", ")", "and", "concept", "not", "in", "[", "'imperative'", ",", "'expressive'", ",", "'interrogative'", "]", ")", "or", "targets", ":", "\n", "                        ", "amr_string", "=", "amr_string", ".", "replace", "(", "f'[[{n}]]'", ",", "f'({id} / {concept}{children})'", ",", "1", ")", "\n", "", "else", ":", "\n", "                        ", "amr_string", "=", "amr_string", ".", "replace", "(", "f'[[{n}]]'", ",", "f'{concept}'", ")", "\n", "", "completed", ".", "add", "(", "n", ")", "\n", "", "amr_string", "=", "amr_string", ".", "replace", "(", "f'[[{n}]]'", ",", "f'{id}'", ")", "\n", "nodes", ".", "remove", "(", "n", ")", "\n", "nodes", ".", "update", "(", "targets", ")", "\n", "", "depth", "+=", "1", "\n", "\n", "", "if", "allow_incomplete", ":", "\n", "            ", "pass", "\n", "\n", "", "else", ":", "\n", "            ", "if", "len", "(", "completed", ")", "<", "len", "(", "self", ".", "nodes", ")", ":", "\n", "                ", "raise", "InvalidAMRError", "(", "\"Tried to print an uncompleted AMR\"", ")", "\n", "", "if", "amr_string", ".", "startswith", "(", "'\"'", ")", "or", "amr_string", "[", "0", "]", ".", "isdigit", "(", ")", "or", "amr_string", "[", "0", "]", "==", "'-'", ":", "\n", "                ", "amr_string", "=", "'(x / '", "+", "amr_string", "+", "')'", "\n", "", "if", "not", "amr_string", ".", "startswith", "(", "'('", ")", ":", "\n", "                ", "amr_string", "=", "'('", "+", "amr_string", "+", "')'", "\n", "", "if", "len", "(", "self", ".", "nodes", ")", "==", "0", ":", "\n", "                ", "amr_string", "=", "'(a / amr-empty)'", "\n", "\n", "# ::short attribute from Revanth                                                    \\", "\n", "\n", "", "output", "+=", "f'# ::short\\t{str(new_ids)}\\t\\n'", "\n", "output", "+=", "amr_string", "+", "'\\n\\n'", "\n", "\n", "", "if", "only_penman", ":", "\n", "            ", "output", "=", "'\\n'", ".", "join", "(", "[", "\n", "line", "\n", "for", "line", "in", "output", ".", "split", "(", "'\\n'", ")", "if", "line", "and", "line", "[", "0", "]", "!=", "'#'", "\n", "]", ")", "\n", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.JAMR_CorpusReader.__init__": [[266, 275], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "amrs", "=", "[", "]", "\n", "self", ".", "amrs_dev", "=", "[", "]", "\n", "\n", "# index dictionaries", "\n", "self", ".", "nodes2Ints", "=", "{", "}", "\n", "self", ".", "words2Ints", "=", "{", "}", "\n", "self", ".", "chars2Ints", "=", "{", "}", "\n", "self", ".", "labels2Ints", "=", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.JAMR_CorpusReader.load_amrs": [[289, 376], ["amrs.append", "open", "amr.AMR", "len", "amrs.pop", "len", "amrs.append", "line.startswith", "line.strip", "print", "amr.AMR", "tokens.split.split.split", "amrs[].tokens.extend", "line.startswith", "float", "line.startswith", "len", "amr.JAMR_CorpusReader.words2Ints.setdefault", "line.strip", "enumerate", "line.startswith", "len", "amr.JAMR_CorpusReader.chars2Ints.setdefault", "len", "line.split", "enumerate", "amrs[].edges.append", "line.startswith", "len", "tab.strip", "line.split", "tab.startswith", "tab.endswith", "tuple", "line.split", "root.strip.strip.strip", "tab.strip", "amr.JAMR_CorpusReader.nodes2Ints.setdefault", "tab.strip().split", "int", "int", "list", "tab.strip", "amr.JAMR_CorpusReader.labels2Ints.setdefault", "tab.strip", "len", "range", "len", "tab.strip", "tab.strip"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["def", "load_amrs", "(", "self", ",", "amr_file_name", ",", "training", "=", "True", ",", "verbose", "=", "False", ")", ":", "\n", "\n", "        ", "amrs", "=", "self", ".", "amrs", "if", "training", "else", "self", ".", "amrs_dev", "\n", "\n", "amrs", ".", "append", "(", "AMR", "(", ")", ")", "\n", "\n", "fp", "=", "open", "(", "amr_file_name", ",", "encoding", "=", "'utf8'", ")", "\n", "for", "line", "in", "fp", ":", "\n", "# empty line, prepare to read next amr in dataset", "\n", "            ", "if", "len", "(", "line", ".", "strip", "(", ")", ")", "==", "0", ":", "\n", "                ", "if", "verbose", ":", "\n", "                    ", "print", "(", "amrs", "[", "-", "1", "]", ")", "\n", "", "amrs", ".", "append", "(", "AMR", "(", ")", ")", "\n", "# amr tokens", "\n", "", "elif", "line", ".", "startswith", "(", "\"# ::tok\"", ")", ":", "\n", "                ", "tokens", "=", "line", "[", "len", "(", "'# ::tok '", ")", ":", "]", "\n", "tokens", "=", "tokens", ".", "split", "(", ")", "\n", "amrs", "[", "-", "1", "]", ".", "tokens", ".", "extend", "(", "tokens", ")", "\n", "for", "tok", "in", "tokens", ":", "\n", "# TODO: update dictionaries after entire AMR is read", "\n", "                    ", "if", "training", ":", "\n", "                        ", "self", ".", "words2Ints", ".", "setdefault", "(", "tok", ",", "len", "(", "self", ".", "words2Ints", ")", ")", "\n", "for", "char", "in", "tok", ":", "\n", "                            ", "self", ".", "chars2Ints", ".", "setdefault", "(", "char", ",", "len", "(", "self", ".", "chars2Ints", ")", ")", "\n", "# amr score", "\n", "", "", "", "", "elif", "line", ".", "startswith", "(", "\"# ::scr\"", ")", ":", "\n", "                ", "score", "=", "line", ".", "strip", "(", ")", "[", "len", "(", "'# ::scr '", ")", ":", "]", "\n", "score", "=", "float", "(", "score", ")", "\n", "amrs", "[", "-", "1", "]", ".", "score", "=", "score", "\n", "# an amr node", "\n", "", "elif", "line", ".", "startswith", "(", "\"# ::node\"", ")", ":", "\n", "                ", "node_id", "=", "''", "\n", "for", "col", ",", "tab", "in", "enumerate", "(", "line", ".", "split", "(", "\"\\t\"", ")", ")", ":", "\n", "# node id", "\n", "                    ", "if", "col", "==", "1", ":", "\n", "                        ", "node_id", "=", "tab", ".", "strip", "(", ")", "\n", "# node label", "\n", "", "elif", "col", "==", "2", ":", "\n", "                        ", "node", "=", "tab", ".", "strip", "(", ")", "\n", "amrs", "[", "-", "1", "]", ".", "nodes", "[", "node_id", "]", "=", "node", "\n", "# TODO: update dictionaries after entire AMR is read", "\n", "if", "training", ":", "\n", "                            ", "self", ".", "nodes2Ints", ".", "setdefault", "(", "node", ",", "len", "(", "self", ".", "nodes2Ints", ")", ")", "\n", "# alignment", "\n", "", "", "elif", "col", "==", "3", ":", "\n", "                        ", "if", "'-'", "not", "in", "tab", ":", "\n", "                            ", "continue", "\n", "", "start_end", "=", "tab", ".", "strip", "(", ")", ".", "split", "(", "\"-\"", ")", "\n", "start", "=", "int", "(", "start_end", "[", "0", "]", ")", "# inclusive", "\n", "end", "=", "int", "(", "start_end", "[", "1", "]", ")", "# exclusive", "\n", "word_idxs", "=", "list", "(", "range", "(", "start", "+", "1", ",", "end", "+", "1", ")", ")", "# off by one (we start at index 1)", "\n", "amrs", "[", "-", "1", "]", ".", "alignments", "[", "node_id", "]", "=", "word_idxs", "\n", "\n", "# an amr edge", "\n", "", "", "", "elif", "line", ".", "startswith", "(", "\"# ::edge\"", ")", ":", "\n", "                ", "edge", "=", "[", "''", ",", "''", ",", "''", "]", "\n", "in_quotes", "=", "False", "\n", "quote_offset", "=", "0", "\n", "for", "col", ",", "tab", "in", "enumerate", "(", "line", ".", "split", "(", "\"\\t\"", ")", ")", ":", "\n", "                    ", "if", "tab", ".", "startswith", "(", "'\"'", ")", ":", "\n", "                        ", "in_quotes", "=", "True", "\n", "", "if", "tab", ".", "endswith", "(", "'\"'", ")", ":", "\n", "                        ", "in_quotes", "=", "False", "\n", "# edge label", "\n", "", "if", "col", "==", "2", "+", "(", "quote_offset", ")", ":", "\n", "                        ", "edge", "[", "1", "]", "=", "':'", "+", "tab", ".", "strip", "(", ")", "\n", "# TODO: update dictionaries after entire AMR is read", "\n", "if", "training", ":", "\n", "                            ", "self", ".", "labels2Ints", ".", "setdefault", "(", "tab", ",", "len", "(", "self", ".", "labels2Ints", ")", ")", "\n", "# edge source id", "\n", "", "", "elif", "col", "==", "4", "+", "(", "quote_offset", ")", ":", "\n", "                        ", "edge", "[", "0", "]", "=", "tab", ".", "strip", "(", ")", "\n", "# edge target id", "\n", "", "elif", "col", "==", "5", "+", "(", "quote_offset", ")", ":", "\n", "                        ", "edge", "[", "2", "]", "=", "tab", ".", "strip", "(", ")", "\n", "", "if", "in_quotes", ":", "\n", "                        ", "quote_offset", "+=", "1", "\n", "", "", "amrs", "[", "-", "1", "]", ".", "edges", ".", "append", "(", "tuple", "(", "edge", ")", ")", "\n", "# amr root", "\n", "", "elif", "line", ".", "startswith", "(", "\"# ::root\"", ")", ":", "\n", "                ", "splinetabs", "=", "line", ".", "split", "(", "\"\\t\"", ")", "\n", "root", "=", "splinetabs", "[", "1", "]", "\n", "root", "=", "root", ".", "strip", "(", ")", "\n", "amrs", "[", "-", "1", "]", ".", "root", "=", "root", "\n", "\n", "", "", "if", "len", "(", "amrs", "[", "-", "1", "]", ".", "nodes", ")", "==", "0", ":", "\n", "            ", "amrs", ".", "pop", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.find_subgraph_edges": [[8, 33], ["collections.defaultdict", "children[].append", "nodes.pop", "nodes.append", "subgraph_edges.append", "nodes.pop"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop"], ["def", "find_subgraph_edges", "(", "total_edges", ",", "split_node_id", ")", ":", "\n", "\n", "    ", "children", "=", "defaultdict", "(", "list", ")", "\n", "for", "t", "in", "total_edges", ":", "\n", "        ", "children", "[", "t", "[", "0", "]", "]", ".", "append", "(", "t", ")", "\n", "\n", "# Find subgraph first", "\n", "", "subgraph_edges", "=", "[", "]", "\n", "nodes", "=", "[", "split_node_id", "]", "\n", "while", "nodes", ":", "\n", "        ", "if", "nodes", "[", "-", "1", "]", "not", "in", "children", ":", "\n", "# backtrack to previous node", "\n", "            ", "nodes", ".", "pop", "(", ")", "\n", "# get edges from current node to children", "\n", "", "edges", "=", "[", "e", "for", "e", "in", "children", "[", "nodes", "[", "-", "1", "]", "]", "if", "e", "not", "in", "subgraph_edges", "]", "\n", "if", "edges", ":", "\n", "            ", "edge", "=", "edges", "[", "0", "]", "\n", "node_id", "=", "edge", "[", "2", "]", "\n", "nodes", ".", "append", "(", "node_id", ")", "\n", "subgraph_edges", ".", "append", "(", "edge", ")", "\n", "", "else", ":", "\n", "# backtrack to previous node", "\n", "            ", "nodes", ".", "pop", "(", ")", "\n", "\n", "", "", "return", "subgraph_edges", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.get_duplicate_edges": [[378, 403], ["re.compile", "re.compile", "re.compile", "collections.Counter", "collections.Counter.update", "re.compile.match", "collections.Counter.items", "re.compile.match", "re.compile.match", "edge.split"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["", "", "", "def", "get_duplicate_edges", "(", "amr", ")", ":", "\n", "\n", "# Regex for ARGs", "\n", "    ", "arg_re", "=", "re", ".", "compile", "(", "r'^(ARG)([0-9]+)$'", ")", "\n", "unique_re", "=", "re", ".", "compile", "(", "r'^(snt|op)([0-9]+)$'", ")", "\n", "argof_re", "=", "re", ".", "compile", "(", "r'^ARG([0-9]+)-of$'", ")", "\n", "\n", "# count duplicate edges", "\n", "edge_child_count", "=", "Counter", "(", ")", "\n", "for", "t", "in", "amr", ".", "edges", ":", "\n", "        ", "edge", "=", "t", "[", "1", "]", "[", "1", ":", "]", "\n", "if", "edge", "in", "[", "'polarity'", ",", "'mode'", "]", ":", "\n", "            ", "keys", "=", "[", "(", "t", "[", "0", "]", ",", "edge", ",", "amr", ".", "nodes", "[", "t", "[", "2", "]", "]", ")", "]", "\n", "", "elif", "unique_re", ".", "match", "(", "edge", ")", ":", "\n", "            ", "keys", "=", "[", "(", "t", "[", "0", "]", ",", "edge", ")", "]", "\n", "", "elif", "arg_re", ".", "match", "(", "edge", ")", ":", "\n", "            ", "keys", "=", "[", "(", "t", "[", "0", "]", ",", "edge", ")", "]", "\n", "", "elif", "argof_re", ".", "match", "(", "edge", ")", ":", "\n", "# normalize ARG0-of --> to ARG0 <--", "\n", "            ", "keys", "=", "[", "(", "t", "[", "2", "]", ",", "edge", ".", "split", "(", "'-'", ")", "[", "0", "]", ")", "]", "\n", "", "else", ":", "\n", "            ", "continue", "\n", "", "edge_child_count", ".", "update", "(", "keys", ")", "\n", "\n", "", "return", "[", "(", "t", ",", "c", ")", "for", "t", ",", "c", "in", "edge_child_count", ".", "items", "(", ")", "if", "c", ">", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.main": [[405, 410], ["amr.JAMR_CorpusReader", "JAMR_CorpusReader.load_amrs", "len"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.JAMR_CorpusReader.load_amrs"], ["", "def", "main", "(", ")", ":", "\n", "    ", "file", "=", "sys", ".", "argv", "[", "1", "]", "if", "len", "(", "sys", ".", "argv", ")", ">", "1", "else", "\"our_aln_2016.txt\"", "\n", "\n", "cr", "=", "JAMR_CorpusReader", "(", ")", "\n", "cr", ".", "load_amrs", "(", "file", ",", "verbose", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_transformer_amr_parser.Model.__init__": [[107, 112], ["stack_transformer_amr_parser.Model.reset"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.reset"], ["def", "__init__", "(", "self", ",", "models", ",", "target_dictionary", ")", ":", "\n", "        ", "self", ".", "temperature", "=", "1.", "\n", "self", ".", "target_dictionary", "=", "target_dictionary", "\n", "self", ".", "models", "=", "models", "\n", "self", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_transformer_amr_parser.Model.reset": [[113, 120], ["fairseq.sequence_generator.EnsembleModel", "stack_transformer_amr_parser.Model.model.eval"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval"], ["", "def", "reset", "(", "self", ")", ":", "\n", "# This is to clear the cache of key values, there may be more efficient", "\n", "# ways", "\n", "        ", "self", ".", "model", "=", "EnsembleModel", "(", "self", ".", "models", ")", "\n", "# reset cache for encoder", "\n", "self", ".", "encoder_outs", "=", "None", "\n", "self", ".", "model", ".", "eval", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_transformer_amr_parser.Model.precompute_encoder": [[121, 126], ["stack_transformer_amr_parser.extract_encoder", "stack_transformer_amr_parser.Model.model.forward_encoder"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_transformer_amr_parser.extract_encoder", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.sequence_generator.EnsembleModel.forward_encoder"], ["", "def", "precompute_encoder", "(", "self", ",", "sample", ")", ":", "\n", "        ", "\"\"\"Encoder of the encoder-decoder is fixed and can be precomputed\"\"\"", "\n", "encoder_input", "=", "extract_encoder", "(", "sample", ")", "\n", "encoder_outs", "=", "self", ".", "model", ".", "forward_encoder", "(", "encoder_input", ")", "\n", "return", "encoder_outs", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_transformer_amr_parser.Model.get_action": [[127, 155], ["stack_transformer_amr_parser.Model.model.forward_decoder", "stack_transformer_amr_parser.Model.precompute_encoder", "lprobs.argmax().tolist", "torch.squeeze().tolist", "lprobs.argmax", "torch.squeeze", "lprobs.exp().multinomial", "lprobs.exp"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.sequence_generator.EnsembleModel.forward_decoder", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_transformer_amr_parser.Model.precompute_encoder", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.argmax"], ["", "def", "get_action", "(", "self", ",", "sample", ",", "parser_state", ",", "prev_actions", ")", ":", "\n", "\n", "# Compute part of the model that does not depend on episode steps", "\n", "# (encoder). Cache it for future use", "\n", "# precompute encoder for speed", "\n", "        ", "if", "self", ".", "encoder_outs", "is", "None", ":", "\n", "            ", "self", ".", "encoder_outs", "=", "self", ".", "precompute_encoder", "(", "sample", ")", "\n", "\n", "# call model with pre-computed encoder, previous generated actions", "\n", "# (tokens) and state machine status", "\n", "", "lprobs", ",", "avg_attn_scores", "=", "self", ".", "model", ".", "forward_decoder", "(", "\n", "prev_actions", ",", "\n", "self", ".", "encoder_outs", ",", "\n", "parser_state", ",", "\n", "temperature", "=", "self", ".", "temperature", "\n", ")", "\n", "\n", "# Get most probable action", "\n", "if", "True", ":", "\n", "            ", "best_action_indices", "=", "lprobs", ".", "argmax", "(", "dim", "=", "1", ")", ".", "tolist", "(", ")", "\n", "", "else", ":", "\n", "# sampling", "\n", "            ", "best_action_indices", "=", "torch", ".", "squeeze", "(", "\n", "lprobs", ".", "exp", "(", ")", ".", "multinomial", "(", "1", ")", ",", "1", "\n", ")", ".", "tolist", "(", ")", "\n", "", "actions", "=", "[", "self", ".", "target_dictionary", "[", "i", "]", "for", "i", "in", "best_action_indices", "]", "\n", "actions_lprob", "=", "[", "lprobs", "[", "0", ",", "i", "]", "for", "i", "in", "best_action_indices", "]", "\n", "return", "actions", ",", "actions_lprob", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_transformer_amr_parser.AMRParser.__init__": [[159, 195], ["os.path.dirname", "os.path.exists", "transition_amr_parser.stack_transformer.amr_state_machine.StateMachineBatch"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDataset.exists"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "model", ",", "# Pytorch model", "\n", "machine_rules", ",", "# path to train.rules.json ", "\n", "machine_type", ",", "# AMR, NER, etc", "\n", "src_dict", ",", "# fairseq dict", "\n", "tgt_dict", ",", "# fairseq dict", "\n", "use_cuda", ",", "#", "\n", "embeddings", "=", "None", ",", "# pytorch RoBERTa model (if dealing with token input)", "\n", "inspector", "=", "None", "# function to call after each step", "\n", ")", ":", "\n", "\n", "# member variables", "\n", "        ", "self", ".", "src_dict", "=", "src_dict", "\n", "self", ".", "tgt_dict", "=", "tgt_dict", "\n", "self", ".", "embeddings", "=", "embeddings", "\n", "self", ".", "inspector", "=", "inspector", "\n", "self", ".", "model", "=", "model", "\n", "self", ".", "use_cuda", "=", "use_cuda", "\n", "\n", "# automatically find entity rules", "\n", "# TODO: replace machine rules by machine folder", "\n", "model_folder", "=", "os", ".", "path", ".", "dirname", "(", "machine_rules", ")", "\n", "entity_rules", "=", "f'{model_folder}/entity_rules.json'", "\n", "assert", "os", ".", "path", ".", "exists", "(", "entity_rules", ")", ",", "\"Missing {entity_rules}\"", "\n", "\n", "# uninitialized batch of state machines", "\n", "self", ".", "state_machine_batch", "=", "StateMachineBatch", "(", "\n", "src_dict", ",", "\n", "tgt_dict", ",", "\n", "machine_type", ",", "\n", "machine_rules", "=", "machine_rules", ",", "\n", "entity_rules", "=", "entity_rules", ",", "\n", "# note that on standalone, we do need to post-process to get AMR", "\n", "# annotations", "\n", "post_process", "=", "True", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_transformer_amr_parser.AMRParser.from_checkpoint": [[197, 269], ["fairseq.options.get_interactive_generation_parser", "fairseq.options.add_optimization_args", "fairseq.options.parse_args_and_arch", "os.path.dirname", "os.path.isfile", "bool", "os.path.isfile", "os.path.isfile", "fairseq.data.Dictionary.load", "fairseq.data.Dictionary.load", "fairseq.tasks.translation.TranslationTask", "stack_transformer_amr_parser.load_models", "transition_amr_parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings", "print", "os.path.isfile", "stack_transformer_amr_parser.AMRParser.", "open", "json.loads", "torch.cuda.is_available", "checkpoint.split", "fid.read", "stack_transformer_amr_parser.load_roberta", "int"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.get_interactive_generation_parser", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.add_optimization_args", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.options.parse_args_and_arch", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_transformer_amr_parser.load_models", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_transformer_amr_parser.load_roberta"], ["", "@", "classmethod", "\n", "def", "from_checkpoint", "(", "self", ",", "checkpoint", ",", "roberta_cache_path", "=", "None", ",", "\n", "inspector", "=", "None", ")", ":", "\n", "        ", "'''\n        Initialize model from checkpoint\n        '''", "\n", "\n", "# load fairseq task", "\n", "parser", "=", "options", ".", "get_interactive_generation_parser", "(", ")", "\n", "options", ".", "add_optimization_args", "(", "parser", ")", "\n", "args", "=", "options", ".", "parse_args_and_arch", "(", "parser", ",", "input_args", "=", "[", "'--data dummy'", "]", ")", "\n", "\n", "# Read extra arguments", "\n", "model_folder", "=", "os", ".", "path", ".", "dirname", "(", "checkpoint", ".", "split", "(", "':'", ")", "[", "0", "]", ")", "\n", "# config with fairseq-preprocess and fairseq-train args", "\n", "config_json", "=", "f'{model_folder}/config.json'", "\n", "assert", "os", ".", "path", ".", "isfile", "(", "config_json", ")", ",", "\"Model trained with v0.3.0 or above?\"", "\n", "with", "open", "(", "config_json", ")", "as", "fid", ":", "\n", "            ", "extra_args", "=", "json", ".", "loads", "(", "fid", ".", "read", "(", ")", ")", "\n", "", "prepro_args", "=", "extra_args", "[", "'fairseq_preprocess_args'", "]", "\n", "train_args", "=", "extra_args", "[", "'fairseq_train_args'", "]", "\n", "# extra args by hand", "\n", "args", ".", "source_lang", "=", "'en'", "\n", "args", ".", "target_lang", "=", "'actions'", "\n", "args", ".", "path", "=", "checkpoint", "\n", "args", ".", "roberta_cache_path", "=", "roberta_cache_path", "\n", "dim", "=", "train_args", "[", "'--pretrained-embed-dim'", "]", "[", "0", "]", "\n", "args", ".", "model_overrides", "=", "\"{'pretrained_embed_dim':%s, 'task': 'translation'}\"", "%", "dim", "\n", "assert", "bool", "(", "args", ".", "left_pad_source", ")", ",", "\"Only left pad supported\"", "\n", "\n", "# dictionaries", "\n", "src_dict_path", "=", "f'{model_folder}/dict.{args.source_lang}.txt'", "\n", "tgt_dict_path", "=", "f'{model_folder}/dict.{args.target_lang}.txt'", "\n", "assert", "os", ".", "path", ".", "isfile", "(", "src_dict_path", ")", ",", "f\"Missing {src_dict_path}.\\nModel trained with v0.3.0 or above?\"", "\"\\ncheck scripts/stack-transformer/update_model_to_v0.3.0.sh\"", "\n", "assert", "os", ".", "path", ".", "isfile", "(", "tgt_dict_path", ")", ",", "f\"Missing {tgt_dict_path}.\\nModel trained with v0.3.0 or above?\"", "\"\\ncheck scripts/stack-transformer/update_model_to_v0.3.0.sh\"", "\n", "src_dict", "=", "Dictionary", ".", "load", "(", "src_dict_path", ")", "\n", "tgt_dict", "=", "Dictionary", ".", "load", "(", "tgt_dict_path", ")", "\n", "\n", "use_cuda", "=", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "cpu", "\n", "\n", "# Override task to ensure compatibility with old models and overide", "\n", "# TODO: Task may not be even needed", "\n", "task", "=", "TranslationTask", "(", "args", ",", "src_dict", ",", "tgt_dict", ")", "\n", "model", "=", "load_models", "(", "args", ",", "task", ",", "use_cuda", ")", "\n", "\n", "# Load RoBERTa", "\n", "embeddings", "=", "PretrainedEmbeddings", "(", "\n", "name", "=", "prepro_args", "[", "'--pretrained-embed'", "]", "[", "0", "]", ",", "\n", "bert_layers", "=", "[", "int", "(", "x", ")", "for", "x", "in", "prepro_args", "[", "'--bert-layers'", "]", "]", "\n", "if", "'--bert-layers'", "in", "prepro_args", "else", "None", ",", "\n", "model", "=", "load_roberta", "(", "\n", "name", "=", "prepro_args", "[", "'--pretrained-embed'", "]", "[", "0", "]", ",", "\n", "roberta_cache_path", "=", "args", ".", "roberta_cache_path", ",", "\n", "roberta_use_gpu", "=", "use_cuda", "\n", ")", "\n", ")", "\n", "\n", "print", "(", "\"Finished loading models\"", ")", "\n", "\n", "# State machine variables", "\n", "machine_rules", "=", "f'{model_folder}/train.rules.json'", "\n", "assert", "os", ".", "path", ".", "isfile", "(", "machine_rules", ")", ",", "f\"Missing {machine_rules}\"", "\n", "machine_type", "=", "prepro_args", "[", "'--machine-type'", "]", "[", "0", "]", "\n", "\n", "return", "self", "(", "model", ",", "machine_rules", ",", "machine_type", ",", "src_dict", ",", "tgt_dict", ",", "\n", "use_cuda", ",", "embeddings", "=", "embeddings", ",", "inspector", "=", "inspector", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_transformer_amr_parser.AMRParser.get_bert_features_batched": [[270, 287], ["math.ceil", "tqdm.tqdm.tqdm", "print", "range", "stack_transformer_amr_parser.AMRParser.embeddings.extract_batch", "range", "len", "len", "len", "len", "len", "bert_data.append", "copy.deepcopy", "copy.deepcopy", "copy.deepcopy"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.extract_batch", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "get_bert_features_batched", "(", "self", ",", "sentences", ",", "batch_size", ")", ":", "\n", "        ", "bert_data", "=", "[", "]", "\n", "num_batches", "=", "math", ".", "ceil", "(", "len", "(", "sentences", ")", "/", "batch_size", ")", "\n", "for", "i", "in", "tqdm", "(", "range", "(", "0", ",", "num_batches", ")", ",", "desc", "=", "'roberta'", ")", ":", "\n", "            ", "batch", "=", "sentences", "[", "i", "*", "batch_size", ":", "i", "*", "batch_size", "+", "batch_size", "]", "\n", "batch_data", "=", "self", ".", "embeddings", ".", "extract_batch", "(", "batch", ")", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "batch", ")", ")", ":", "\n", "                ", "bert_data", ".", "append", "(", "(", "\n", "copy", ".", "deepcopy", "(", "batch_data", "[", "\"word_features\"", "]", "[", "i", "]", ")", ",", "\n", "copy", ".", "deepcopy", "(", "batch_data", "[", "\"wordpieces_roberta\"", "]", "[", "i", "]", ")", ",", "\n", "copy", ".", "deepcopy", "(", "\n", "batch_data", "[", "\"word2piece_scattered_indices\"", "]", "[", "i", "]", "\n", ")", "\n", ")", ")", "\n", "", "", "print", "(", "len", "(", "bert_data", ")", ")", "\n", "assert", "len", "(", "bert_data", ")", "==", "len", "(", "sentences", ")", "\n", "return", "bert_data", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_transformer_amr_parser.AMRParser.get_token_ids": [[288, 295], ["stack_transformer_amr_parser.AMRParser.src_dict.encode_line"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.encode_line"], ["", "def", "get_token_ids", "(", "self", ",", "sentence", ")", ":", "\n", "        ", "return", "self", ".", "src_dict", ".", "encode_line", "(", "\n", "line", "=", "sentence", ",", "\n", "line_tokenizer", "=", "tokenize_line", ",", "\n", "add_if_not_exist", "=", "False", ",", "\n", "append_eos", "=", "False", ",", "\n", "reverse_order", "=", "False", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_transformer_amr_parser.AMRParser.convert_sentences_to_data": [[297, 319], ["stack_transformer_amr_parser.AMRParser.get_bert_features_batched", "enumerate", "stack_transformer_amr_parser.AMRParser.get_token_ids", "data.append", "fairseq.tokenizer.tokenize_line"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_transformer_amr_parser.AMRParser.get_bert_features_batched", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_transformer_amr_parser.AMRParser.get_token_ids", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.tokenizer.tokenize_line"], ["", "def", "convert_sentences_to_data", "(", "self", ",", "sentences", ",", "batch_size", ",", "\n", "roberta_batch_size", ")", ":", "\n", "\n", "# extract RoBERTa features", "\n", "        ", "roberta_features", "=", "self", ".", "get_bert_features_batched", "(", "sentences", ",", "roberta_batch_size", ")", "\n", "\n", "# organize data into a fairseq batch", "\n", "data", "=", "[", "]", "\n", "for", "index", ",", "sentence", "in", "enumerate", "(", "sentences", ")", ":", "\n", "            ", "ids", "=", "self", ".", "get_token_ids", "(", "sentence", ")", "\n", "word_features", ",", "wordpieces_roberta", ",", "word2piece_scattered_indices", "=", "roberta_features", "[", "index", "]", "\n", "data", ".", "append", "(", "{", "\n", "'id'", ":", "index", ",", "\n", "'source'", ":", "ids", ",", "\n", "'source_fix_emb'", ":", "word_features", ",", "\n", "'src_wordpieces'", ":", "wordpieces_roberta", ",", "\n", "'src_wp2w'", ":", "word2piece_scattered_indices", ",", "\n", "'orig_tokens'", ":", "tokenize_line", "(", "sentence", ")", "\n", "}", ")", "\n", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_transformer_amr_parser.AMRParser.get_iterator": [[320, 332], ["range", "math.ceil", "fairseq.data.language_pair_dataset.collate", "batches.append", "len", "stack_transformer_amr_parser.AMRParser.src_dict.pad", "stack_transformer_amr_parser.AMRParser.src_dict.eos"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.language_pair_dataset.collate", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos"], ["", "def", "get_iterator", "(", "self", ",", "samples", ",", "batch_size", ")", ":", "\n", "        ", "batches", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "0", ",", "math", ".", "ceil", "(", "len", "(", "samples", ")", "/", "batch_size", ")", ")", ":", "\n", "            ", "sample", "=", "samples", "[", "i", "*", "batch_size", ":", "i", "*", "batch_size", "+", "batch_size", "]", "\n", "batch", "=", "collate", "(", "\n", "sample", ",", "pad_idx", "=", "self", ".", "src_dict", ".", "pad", "(", ")", ",", "\n", "eos_idx", "=", "self", ".", "src_dict", ".", "eos", "(", ")", ",", "\n", "left_pad_source", "=", "True", ",", "\n", "state_machine", "=", "False", "\n", ")", "\n", "batches", ".", "append", "(", "batch", ")", "\n", "", "return", "batches", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_transformer_amr_parser.AMRParser.parse_feature_batch": [[333, 411], ["stack_transformer_amr_parser.get_batch_tensors", "stack_transformer_amr_parser.AMRParser.state_machine_batch.reset", "stack_transformer_amr_parser.AMRParser.model.reset", "any", "stack_transformer_amr_parser.AMRParser.state_machine_batch.get_active_logits", "stack_transformer_amr_parser.AMRParser.model.get_action", "enumerate", "stack_transformer_amr_parser.AMRParser.state_machine_batch.update_masks", "stack_transformer_amr_parser.AMRParser.inspector", "stack_transformer_amr_parser.AMRParser.state_machine_batch.memory[].clone", "stack_transformer_amr_parser.AMRParser.state_machine_batch.memory_pos[].clone", "machine.update", "stack_transformer_amr_parser.AMRParser.tgt_dict.index", "print", "transition_amr_parser.utils.yellow_font"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_transformer_amr_parser.get_batch_tensors", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.reset", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.reset", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.get_active_logits", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_transformer_amr_parser.Model.get_action", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.stack_state_machine.StackStateMachine.update_masks", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.index", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.yellow_font"], ["", "def", "parse_feature_batch", "(", "self", ",", "sample", ")", ":", "\n", "\n", "# Get input tensors and preallocate space for output tensors", "\n", "# TODO: This could be inside StateMachineBatch but we need to be", "\n", "# coherent with fairseq/generate.py", "\n", "        ", "src_tokens", ",", "src_lengths", ",", "target_actions", "=", "get_batch_tensors", "(", "\n", "sample", ",", "\n", "self", ".", "src_dict", ",", "\n", "self", ".", "state_machine_batch", ".", "machine_type", ",", "\n", ")", "\n", "# Initialize state machine and get first states", "\n", "# get rules from model folder", "\n", "self", ".", "state_machine_batch", ".", "reset", "(", "\n", "src_tokens", ",", "\n", "src_lengths", ",", "\n", "target_actions", ".", "shape", "[", "1", "]", ",", "\n", "orig_tokens", "=", "sample", "[", "'net_input'", "]", "[", "'orig_tokens'", "]", "\n", ")", "\n", "\n", "# Reset model. This is to clean up the key/value cache in the decoder", "\n", "# and the encoder cache. There may be more efficient ways.", "\n", "self", ".", "model", ".", "reset", "(", ")", "\n", "\n", "# Loop over actions until all machines finish", "\n", "time_step", "=", "0", "\n", "while", "any", "(", "not", "m", ".", "is_closed", "for", "m", "in", "self", ".", "state_machine_batch", ".", "machines", ")", ":", "\n", "\n", "# inspect parsing on the fly", "\n", "            ", "if", "self", ".", "inspector", ":", "\n", "                ", "self", ".", "inspector", "(", "self", ".", "state_machine_batch", ")", "\n", "\n", "# Get target masks from machine state", "\n", "", "logits_indices", ",", "logits_mask", "=", "self", ".", "state_machine_batch", ".", "get_active_logits", "(", ")", "\n", "parser_state", "=", "(", "\n", "self", ".", "state_machine_batch", ".", "memory", "[", ":", ",", ":", ",", ":", "time_step", "+", "1", "]", ".", "clone", "(", ")", ",", "\n", "self", ".", "state_machine_batch", ".", "memory_pos", "[", ":", ",", ":", ",", ":", "time_step", "+", "1", "]", "\n", ".", "clone", "(", ")", ",", "\n", "logits_mask", ",", "\n", "logits_indices", "\n", ")", "\n", "\n", "# Get most probably action from the model for each sentence", "\n", "# given input data, previous actions and state machine state", "\n", "actions", ",", "log_probs", "=", "self", ".", "model", ".", "get_action", "(", "\n", "sample", ",", "\n", "parser_state", ",", "\n", "target_actions", "[", ":", ",", ":", "time_step", "+", "1", "]", ".", "data", ",", "\n", ")", "\n", "\n", "# act on the state machine batch", "\n", "# Loop over paralele machines in the batch", "\n", "for", "m_idx", ",", "machine", "in", "enumerate", "(", "\n", "self", ".", "state_machine_batch", ".", "machines", "\n", ")", ":", "\n", "\n", "# Emergency stop. If we reach maximum action number, force", "\n", "# stop the machine", "\n", "                ", "if", "(", "\n", "time_step", "+", "2", "==", "target_actions", ".", "shape", "[", "1", "]", "and", "\n", "not", "machine", ".", "is_closed", "\n", ")", ":", "\n", "                    ", "msg", "=", "f'machine {m_idx} not closed at step {time_step}'", "\n", "print", "(", "yellow_font", "(", "msg", ")", ")", "\n", "action", "=", "'CLOSE'", "\n", "", "else", ":", "\n", "                    ", "action", "=", "actions", "[", "m_idx", "]", "\n", "\n", "# update state machine", "\n", "", "machine", ".", "update", "(", "action", ")", "\n", "# update list of previous actions", "\n", "action_idx", "=", "self", ".", "tgt_dict", ".", "index", "(", "action", ")", "\n", "target_actions", "[", "m_idx", ",", "time_step", "+", "1", "]", "=", "action_idx", "\n", "\n", "# update counters and recompute masks", "\n", "", "time_step", "+=", "1", "\n", "self", ".", "state_machine_batch", ".", "step_index", "+=", "1", "\n", "self", ".", "state_machine_batch", ".", "update_masks", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_transformer_amr_parser.AMRParser.parse_sentences": [[412, 464], ["print", "stack_transformer_amr_parser.AMRParser.convert_sentences_to_data", "stack_transformer_amr_parser.AMRParser.get_iterator", "tqdm.tqdm.tqdm", "range", "len", "len", "sentences.append", "stack_transformer_amr_parser.AMRParser.parse_feature_batch", "sample[].detach().cpu().tolist", "enumerate", "len", "result.append", "str", "tokens.append", "fairseq.utils.move_to_cuda", "transition_amr_parser.amr.get_duplicate_edges", "any", "sample[].detach().cpu", "machine.get_annotations", "transition_amr_parser.utils.yellow_font", "print", "print", "print", "print", "dict", "sample[].detach"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_transformer_amr_parser.AMRParser.convert_sentences_to_data", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_transformer_amr_parser.AMRParser.get_iterator", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_transformer_amr_parser.AMRParser.parse_feature_batch", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.move_to_cuda", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.get_duplicate_edges", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.get_annotations", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.yellow_font", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["", "", "def", "parse_sentences", "(", "self", ",", "batch", ",", "batch_size", "=", "128", ",", "roberta_batch_size", "=", "10", ")", ":", "\n", "\n", "# max batch_size", "\n", "        ", "if", "len", "(", "batch", ")", "<", "batch_size", ":", "\n", "            ", "batch_size", "=", "len", "(", "batch", ")", "\n", "", "print", "(", "\"Running on batch size: \"", "+", "str", "(", "batch_size", ")", ")", "\n", "\n", "sentences", "=", "[", "]", "\n", "# The model expects <ROOT> token at the end of the input sentence", "\n", "for", "tokens", "in", "batch", ":", "\n", "            ", "if", "tokens", "[", "-", "1", "]", "!=", "\"<ROOT>\"", ":", "\n", "                ", "tokens", ".", "append", "(", "\"<ROOT>\"", ")", "\n", "", "sentences", ".", "append", "(", "\" \"", ".", "join", "(", "tokens", ")", ")", "\n", "\n", "", "data", "=", "self", ".", "convert_sentences_to_data", "(", "sentences", ",", "batch_size", ",", "\n", "roberta_batch_size", ")", "\n", "data_iterator", "=", "self", ".", "get_iterator", "(", "data", ",", "batch_size", ")", "\n", "\n", "# Loop over batches of sentences", "\n", "amr_annotations", "=", "{", "}", "\n", "for", "sample", "in", "tqdm", "(", "data_iterator", ",", "desc", "=", "'decoding'", ")", ":", "\n", "\n", "# step, sample, state_machine_batch, tokens = state", "\n", "            ", "sample", "=", "utils", ".", "move_to_cuda", "(", "sample", ")", "if", "self", ".", "use_cuda", "else", "sample", "\n", "\n", "# parse for this batch", "\n", "self", ".", "parse_feature_batch", "(", "sample", ")", "\n", "\n", "# collect all annotations", "\n", "ids", "=", "sample", "[", "\"id\"", "]", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "tolist", "(", ")", "\n", "for", "index", ",", "id", "in", "enumerate", "(", "ids", ")", ":", "\n", "                ", "machine", "=", "self", ".", "state_machine_batch", ".", "machines", "[", "index", "]", "\n", "try", ":", "\n", "                    ", "amr_annotations", "[", "id", "]", "=", "machine", ".", "get_annotations", "(", ")", "\n", "", "except", "InvalidAMRError", "as", "exception", ":", "\n", "                    ", "print", "(", "f'\\nFailed at sentence {id}\\n'", ")", "\n", "raise", "exception", "\n", "\n", "# sanity check annotations", "\n", "", "dupes", "=", "get_duplicate_edges", "(", "machine", ".", "amr", ")", "\n", "if", "any", "(", "dupes", ")", ":", "\n", "                    ", "msg", "=", "yellow_font", "(", "'WARNING:'", ")", "\n", "print", "(", "f'{msg} duplicated edges in sent {id}'", ",", "end", "=", "' '", ")", "\n", "print", "(", "dict", "(", "dupes", ")", ")", "\n", "print", "(", "' '", ".", "join", "(", "machine", ".", "tokens", ")", ")", "\n", "\n", "# return the AMRs in order", "\n", "", "", "", "result", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "batch", ")", ")", ":", "\n", "            ", "result", ".", "append", "(", "amr_annotations", "[", "i", "]", ")", "\n", "\n", "", "return", "result", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_transformer_amr_parser.load_models": [[29, 42], ["fairseq.checkpoint_utils.load_model_ensemble", "stack_transformer_amr_parser.Model", "args.path.split", "print", "print", "eval", "m.cuda"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.checkpoint_utils.load_model_ensemble", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval"], ["def", "load_models", "(", "args", ",", "task", ",", "use_cuda", ")", ":", "\n", "    ", "models", ",", "_", "=", "checkpoint_utils", ".", "load_model_ensemble", "(", "\n", "args", ".", "path", ".", "split", "(", "':'", ")", ",", "\n", "arg_overrides", "=", "eval", "(", "args", ".", "model_overrides", ")", ",", "\n", "task", "=", "task", ",", "\n", ")", "\n", "if", "use_cuda", ":", "\n", "        ", "print", "(", "\"using GPU for models\"", ")", "\n", "[", "m", ".", "cuda", "(", ")", "for", "m", "in", "models", "]", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"using CPU for models\"", ")", "\n", "", "model", "=", "Model", "(", "models", ",", "task", ".", "target_dictionary", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_transformer_amr_parser.load_roberta": [[44, 57], ["RobertaModel.from_pretrained.eval", "torch.hub.load", "fairseq.models.roberta.RobertaModel.from_pretrained", "RobertaModel.from_pretrained.cuda"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.model_gottbert.GottbertModel.from_pretrained"], ["", "def", "load_roberta", "(", "name", "=", "None", ",", "roberta_cache_path", "=", "None", ",", "roberta_use_gpu", "=", "False", ")", ":", "\n", "    ", "if", "not", "roberta_cache_path", ":", "\n", "# Load the Roberta Model from torch hub", "\n", "        ", "roberta", "=", "torch", ".", "hub", ".", "load", "(", "'pytorch/fairseq'", ",", "name", ")", "\n", "", "else", ":", "\n", "        ", "roberta", "=", "RobertaModel", ".", "from_pretrained", "(", "\n", "roberta_cache_path", ",", "\n", "checkpoint_file", "=", "'model.pt'", "\n", ")", "\n", "", "roberta", ".", "eval", "(", ")", "\n", "if", "roberta_use_gpu", ":", "\n", "        ", "roberta", ".", "cuda", "(", ")", "\n", "", "return", "roberta", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_transformer_amr_parser.extract_encoder": [[59, 68], ["sample[].items"], "function", ["None"], ["", "def", "extract_encoder", "(", "sample", ")", ":", "\n", "    ", "encoder_input", "=", "{", "\n", "k", ":", "v", "for", "k", ",", "v", "in", "sample", "[", "'net_input'", "]", ".", "items", "(", ")", "\n", "if", "k", "not", "in", "[", "'prev_output_tokens'", "]", "\n", "}", "\n", "# Add dummy memory and memory pos", "\n", "encoder_input", "[", "'memory'", "]", "=", "None", "\n", "encoder_input", "[", "'memory_pos'", "]", "=", "None", "\n", "return", "encoder_input", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_transformer_amr_parser.get_batch_tensors": [[70, 101], ["stack_transformer_amr_parser.extract_encoder", "src_tokens.size", "src_tokens.new().long().fill_", "int", "source_dictionary.pad", "source_dictionary.eos", "src_tokens.clone().detach", "src_lengths.clone().detach", "math.ceil", "int", "src_tokens.new().long", "math.ceil", "src_tokens.clone", "src_lengths.clone", "src_lengths.max().item", "src_tokens.new", "src_tokens.ne", "src_tokens.ne", "src_lengths.max().item", "source_dictionary.eos", "source_dictionary.pad", "src_lengths.max", "src_lengths.max"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_transformer_amr_parser.extract_encoder", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.eos", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad"], ["", "def", "get_batch_tensors", "(", "sample", ",", "source_dictionary", ",", "machine_type", ")", ":", "\n", "\n", "# auxiliary variables", "\n", "    ", "encoder_input", "=", "extract_encoder", "(", "sample", ")", "\n", "src_tokens", "=", "encoder_input", "[", "'src_tokens'", "]", "\n", "src_lengths", "=", "(", "\n", "src_tokens", ".", "ne", "(", "source_dictionary", ".", "eos", "(", ")", ")", "&", "\n", "src_tokens", ".", "ne", "(", "source_dictionary", ".", "pad", "(", ")", ")", "\n", ")", ".", "long", "(", ")", ".", "sum", "(", "dim", "=", "1", ")", "\n", "input_size", "=", "src_tokens", ".", "size", "(", ")", "\n", "bsz", "=", "input_size", "[", "0", "]", "\n", "\n", "# max number of steps in episode", "\n", "if", "machine_type", "==", "'NER'", ":", "\n", "        ", "max_len", "=", "int", "(", "math", ".", "ceil", "(", "src_lengths", ".", "max", "(", ")", ".", "item", "(", ")", "*", "3", ")", ")", "\n", "", "elif", "machine_type", "==", "'AMR'", ":", "\n", "        ", "max_len", "=", "int", "(", "math", ".", "ceil", "(", "src_lengths", ".", "max", "(", ")", ".", "item", "(", ")", "*", "10", ")", ")", "\n", "\n", "# more aux vars", "\n", "", "bos_token", "=", "None", "\n", "beam_size", "=", "1", "\n", "target_actions", "=", "src_tokens", ".", "new", "(", "\n", "bsz", "*", "beam_size", ",", "max_len", "+", "2", "\n", ")", ".", "long", "(", ")", ".", "fill_", "(", "source_dictionary", ".", "pad", "(", ")", ")", "\n", "target_actions", "[", ":", ",", "0", "]", "=", "source_dictionary", ".", "eos", "(", ")", "if", "bos_token", "is", "None", "else", "bos_token", "\n", "\n", "return", "(", "\n", "src_tokens", ".", "clone", "(", ")", ".", "detach", "(", ")", ",", "\n", "src_lengths", ".", "clone", "(", ")", ".", "detach", "(", ")", ",", "\n", "target_actions", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.roberta_utils.get_tokens": [[9, 11], ["roberta.task.source_dictionary.encode_line", "roberta.bpe.encode"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.encode_line", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.encode"], ["def", "get_tokens", "(", "roberta", ",", "word", ")", ":", "\n", "    ", "return", "roberta", ".", "task", ".", "source_dictionary", ".", "encode_line", "(", "roberta", ".", "bpe", ".", "encode", "(", "word", ")", ",", "append_eos", "=", "False", ",", "add_if_not_exist", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.roberta_utils.get_alignments_and_tokens": [[13, 30], ["roberta_utils.get_tokens", "bpe_tokens.extend", "alignments.append", "len", "roberta_utils.get_tokens", "bpe_tokens.extend", "alignments.append", "torch.LongTensor", "len", "roberta.task.source_dictionary.index", "range", "roberta.task.source_dictionary.index", "len", "range", "len"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.roberta_utils.get_tokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.roberta_utils.get_tokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.index", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.index"], ["", "def", "get_alignments_and_tokens", "(", "roberta", ",", "words", ")", ":", "\n", "    ", "bpe_tokens", "=", "[", "]", "\n", "alignment_position", "=", "1", "\n", "alignments", "=", "[", "]", "\n", "first_word_tokens", "=", "get_tokens", "(", "roberta", ",", "words", "[", "0", "]", ")", "\n", "bpe_tokens", ".", "extend", "(", "first_word_tokens", ")", "\n", "alignments", ".", "append", "(", "[", "(", "alignment_position", "+", "i", ")", "for", "i", "in", "range", "(", "0", ",", "len", "(", "first_word_tokens", ")", ")", "]", ")", "\n", "alignment_position", "=", "alignment_position", "+", "len", "(", "first_word_tokens", ")", "\n", "\n", "for", "word", "in", "words", "[", "1", ":", "]", ":", "\n", "        ", "tokens", "=", "get_tokens", "(", "roberta", ",", "\" \"", "+", "word", ")", "\n", "bpe_tokens", ".", "extend", "(", "tokens", ")", "\n", "alignments", ".", "append", "(", "[", "(", "alignment_position", "+", "i", ")", "for", "i", "in", "range", "(", "0", ",", "len", "(", "tokens", ")", ")", "]", ")", "\n", "alignment_position", "=", "alignment_position", "+", "len", "(", "tokens", ")", "\n", "\n", "", "final_bpe_tokens", "=", "[", "roberta", ".", "task", ".", "source_dictionary", ".", "index", "(", "'<s>'", ")", "]", "+", "bpe_tokens", "+", "[", "roberta", ".", "task", ".", "source_dictionary", ".", "index", "(", "'</s>'", ")", "]", "\n", "return", "alignments", ",", "torch", ".", "LongTensor", "(", "final_bpe_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.roberta_utils.align_features_to_words": [[32, 57], ["collections.Counter", "features.new", "range", "torch.stack", "features.dim", "features.new.unsqueeze", "torch.stack.append", "max", "len", "torch.stack.append", "collections.Counter.get", "weighted_features[].sum", "range", "len"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "align_features_to_words", "(", "roberta", ",", "features", ",", "alignment", ")", ":", "\n", "    ", "\"\"\"\n    Align given features to words.\n\n    Args:\n        roberta (RobertaHubInterface): RoBERTa instance\n        features (torch.Tensor): features to align of shape `(T_bpe x C)`\n        alignment: alignment between BPE tokens and words returned by\n            func:`align_bpe_to_words`.\n    \"\"\"", "\n", "assert", "features", ".", "dim", "(", ")", "==", "2", "\n", "\n", "bpe_counts", "=", "Counter", "(", "j", "for", "bpe_indices", "in", "alignment", "for", "j", "in", "bpe_indices", ")", "\n", "assert", "bpe_counts", "[", "0", "]", "==", "0", "# <s> shouldn't be aligned", "\n", "denom", "=", "features", ".", "new", "(", "[", "bpe_counts", ".", "get", "(", "j", ",", "1", ")", "for", "j", "in", "range", "(", "len", "(", "features", ")", ")", "]", ")", "\n", "weighted_features", "=", "features", "/", "denom", ".", "unsqueeze", "(", "-", "1", ")", "\n", "output", "=", "[", "weighted_features", "[", "0", "]", "]", "\n", "largest_j", "=", "-", "1", "\n", "for", "bpe_indices", "in", "alignment", ":", "\n", "        ", "output", ".", "append", "(", "weighted_features", "[", "bpe_indices", "]", ".", "sum", "(", "dim", "=", "0", ")", ")", "\n", "largest_j", "=", "max", "(", "largest_j", ",", "*", "bpe_indices", ")", "\n", "", "for", "j", "in", "range", "(", "largest_j", "+", "1", ",", "len", "(", "features", ")", ")", ":", "\n", "        ", "output", ".", "append", "(", "weighted_features", "[", "j", "]", ")", "\n", "", "output", "=", "torch", ".", "stack", "(", "output", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.roberta_utils.extract_features_aligned_to_words_batched": [[59, 87], ["fairseq.models.roberta.alignment_utils.spacy_nlp", "fairseq.data.data_utils.collate_tokens", "model.extract_features", "zip", "sentence.split", "roberta_utils.get_alignments_and_tokens", "bpe_toks.append", "alignments.append", "spacy_tokens.append", "sum", "roberta_utils.align_features_to_words", "spacy.tokens.Doc", "results.append", "len", "copy.copy"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.alignment_utils.spacy_nlp", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.data_utils.collate_tokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.extract_features", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.roberta_utils.get_alignments_and_tokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.roberta_utils.align_features_to_words", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.copy"], ["", "def", "extract_features_aligned_to_words_batched", "(", "model", ",", "sentences", ":", "list", ",", "use_all_layers", ":", "bool", "=", "True", ",", "return_all_hiddens", ":", "bool", "=", "False", ")", "->", "torch", ".", "Tensor", ":", "\n", "    ", "nlp", "=", "spacy_nlp", "(", ")", "\n", "bpe_toks", "=", "[", "]", "\n", "alignments", "=", "[", "]", "\n", "spacy_tokens", "=", "[", "]", "\n", "for", "sentence", "in", "sentences", ":", "\n", "        ", "toks", "=", "sentence", ".", "split", "(", ")", "\n", "alignment", ",", "bpe_tok", "=", "get_alignments_and_tokens", "(", "model", ",", "toks", ")", "\n", "bpe_toks", ".", "append", "(", "bpe_tok", ")", "\n", "alignments", ".", "append", "(", "alignment", ")", "\n", "spacy_tokens", ".", "append", "(", "toks", ")", "\n", "\n", "", "bpe_toks_collated", "=", "collate_tokens", "(", "bpe_toks", ",", "pad_idx", "=", "1", ")", "\n", "\n", "features", "=", "model", ".", "extract_features", "(", "bpe_toks_collated", ",", "return_all_hiddens", "=", "return_all_hiddens", ")", "\n", "final_features", "=", "sum", "(", "features", "[", "1", ":", "]", ")", "/", "(", "len", "(", "features", ")", "-", "1", ")", "\n", "\n", "results", "=", "[", "]", "\n", "for", "bpe_tok", ",", "final_feature", ",", "alignment", ",", "toks", "in", "zip", "(", "bpe_toks", ",", "final_features", ",", "alignments", ",", "spacy_tokens", ")", ":", "\n", "        ", "aligned_feats", "=", "align_features_to_words", "(", "model", ",", "final_feature", "[", "0", ":", "bpe_tok", ".", "shape", "[", "0", "]", "]", ",", "alignment", ")", "\n", "doc", "=", "Doc", "(", "\n", "nlp", ".", "vocab", ",", "\n", "words", "=", "[", "'<s>'", "]", "+", "[", "x", "for", "x", "in", "toks", "]", "+", "[", "'</s>'", "]", ",", "\n", ")", "\n", "doc", ".", "user_token_hooks", "[", "'vector'", "]", "=", "lambda", "token", ":", "aligned_feats", "[", "token", ".", "i", "]", "\n", "results", ".", "append", "(", "copy", ".", "copy", "(", "doc", ")", ")", "\n", "\n", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.roberta_utils.extract_features_aligned_to_words": [[89, 102], ["fairseq.models.roberta.alignment_utils.spacy_nlp", "roberta_utils.get_alignments_and_tokens", "model.extract_features", "final_features.squeeze.squeeze", "roberta_utils.align_features_to_words", "spacy.tokens.Doc", "sum", "len"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.roberta.alignment_utils.spacy_nlp", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.roberta_utils.get_alignments_and_tokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.extract_features", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.roberta_utils.align_features_to_words"], ["", "def", "extract_features_aligned_to_words", "(", "model", ",", "tokens", ":", "list", ",", "use_all_layers", ":", "bool", "=", "True", ",", "return_all_hiddens", ":", "bool", "=", "False", ")", "->", "torch", ".", "Tensor", ":", "\n", "    ", "nlp", "=", "spacy_nlp", "(", ")", "\n", "alignment", ",", "bpe_tok", "=", "get_alignments_and_tokens", "(", "model", ",", "tokens", ")", "\n", "features", "=", "model", ".", "extract_features", "(", "bpe_tok", ",", "return_all_hiddens", "=", "return_all_hiddens", ")", "\n", "final_features", "=", "sum", "(", "features", "[", "1", ":", "]", ")", "/", "(", "len", "(", "features", ")", "-", "1", ")", "\n", "final_features", "=", "final_features", ".", "squeeze", "(", "0", ")", "\n", "aligned_feats", "=", "align_features_to_words", "(", "model", ",", "final_features", ",", "alignment", ")", "\n", "doc", "=", "Doc", "(", "\n", "nlp", ".", "vocab", ",", "\n", "words", "=", "[", "'<s>'", "]", "+", "[", "x", "for", "x", "in", "tokens", "]", "+", "[", "'</s>'", "]", "\n", ")", "\n", "doc", ".", "user_token_hooks", "[", "'vector'", "]", "=", "lambda", "token", ":", "aligned_feats", "[", "token", ".", "i", "]", "\n", "return", "doc", "\n", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.model.AMRModel.__init__": [[16, 221], ["super().__init__", "transition_amr_parser.state_machine.get_spacy_lemmatizer", "transition_amr_parser.state_machine.get_spacy_lemmatizer", "model.AMRModel.singletons.discard", "model.AMRModel.singletons.discard", "model.AMRModel.singletons.discard", "model.AMRModel.singletons.discard", "len", "len", "len", "len", "len", "model.AMRModel.labelsA2idx.items", "model.AMRModel.labelsO2idx.items", "model.AMRModel.pred2idx.items", "transition_amr_parser.print_log", "transition_amr_parser.print_log", "transition_amr_parser.print_log", "transition_amr_parser.print_log", "transition_amr_parser.print_log", "transition_amr_parser.print_log", "transition_amr_parser.print_log", "transition_amr_parser.print_log", "transition_amr_parser.print_log", "transition_amr_parser.print_log", "transition_amr_parser.print_log", "transition_amr_parser.print_log", "transition_amr_parser.print_log", "transition_amr_parser.print_log", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.LSTMCell", "torch.LSTMCell", "torch.LSTMCell", "torch.LSTMCell", "torch.LSTMCell", "torch.LSTMCell", "torch.LSTMCell", "torch.LSTMCell", "torch.LSTMCell", "transition_amr_parser.xavier_init", "transition_amr_parser.xavier_init", "transition_amr_parser.xavier_init", "transition_amr_parser.xavier_init", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "transition_amr_parser.Accuracy", "transition_amr_parser.Accuracy", "transition_amr_parser.Accuracy", "transition_amr_parser.Accuracy", "transition_amr_parser.Accuracy", "transition_amr_parser.Accuracy", "transition_amr_parser.Accuracy", "transition_amr_parser.Accuracy", "transition_amr_parser.ConfusionMatrix", "transition_amr_parser.ConfusionMatrix", "transition_amr_parser.ConfusionMatrix", "transition_amr_parser.ConfusionMatrix", "model.AMRModel.rand_init", "model.AMRModel.labelsA.append", "model.AMRModel.labelsO.append", "model.AMRModel.preds.append", "print", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.LSTMCell", "torch.LSTMCell", "torch.LSTMCell", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "model.AMRModel.action2idx.keys", "model.AMRModel.labelsO2idx.keys", "model.AMRModel.modules", "model.AMRModel.labelsO2idx.items", "model.AMRModel.labelsA2idx.items", "model.AMRModel.node2idx.items", "model.AMRModel.pred2idx.items", "model.AMRModel.action2idx.items", "model.AMRModel.word2idx.items", "model.AMRModel.char2idx.items", "len", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "len", "len", "len", "len", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "m.cuda", "len", "len", "len", "len", "len", "len", "len", "len", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.get_spacy_lemmatizer", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.get_spacy_lemmatizer", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.Embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.Embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.Embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.Embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.Embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.Embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.Embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.Embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.Embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.Embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.Embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.Embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.Embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.Embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.Embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lstm.LSTMCell", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lstm.LSTMCell", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lstm.LSTMCell", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lstm.LSTMCell", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lstm.LSTMCell", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lstm.LSTMCell", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lstm.LSTMCell", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lstm.LSTMCell", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lstm.LSTMCell", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.xavier_init", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.xavier_init", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.xavier_init", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.xavier_init", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.model.AMRModel.rand_init", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.Embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.Embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.transformer.Embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lstm.LSTM", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lstm.LSTM", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lstm.LSTM", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lstm.LSTM", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lstm.LSTM", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lstm.LSTM", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lstm.LSTM", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lstm.LSTM", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lstm.LSTM", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lstm.LSTM", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lstm.LSTM", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lstm.LSTM", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lstm.LSTMCell", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lstm.LSTMCell", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.models.lstm.LSTMCell", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.modules.transformer_layer.Linear"], ["FairseqLanguageModel", ",", "\n", "register_model", ",", "\n", "register_model_architecture", ",", "\n", ")", "\n", "from", "fairseq", ".", "modules", "import", "(", "\n", "LayerNorm", ",", "\n", "TransformerSentenceEncoder", ",", "\n", ")", "\n", "from", "fairseq", ".", "modules", ".", "transformer_sentence_encoder", "import", "init_bert_params", "\n", "\n", "from", ".", "hub_interface", "import", "RobertaHubInterface", "\n", "\n", "\n", "@", "register_model", "(", "'roberta'", ")", "\n", "class", "RobertaModel", "(", "FairseqLanguageModel", ")", ":", "\n", "\n", "    ", "@", "classmethod", "\n", "def", "hub_models", "(", "cls", ")", ":", "\n", "        ", "return", "{", "\n", "'roberta.base'", ":", "'http://dl.fbaipublicfiles.com/fairseq/models/roberta.base.tar.gz'", ",", "\n", "'roberta.large'", ":", "'http://dl.fbaipublicfiles.com/fairseq/models/roberta.large.tar.gz'", ",", "\n", "'roberta.large.mnli'", ":", "'http://dl.fbaipublicfiles.com/fairseq/models/roberta.large.mnli.tar.gz'", ",", "\n", "'roberta.large.wsc'", ":", "'http://dl.fbaipublicfiles.com/fairseq/models/roberta.large.wsc.tar.gz'", ",", "\n", "}", "\n", "\n", "", "def", "__init__", "(", "self", ",", "args", ",", "encoder", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "encoder", ")", "\n", "self", ".", "args", "=", "args", "\n", "\n", "# We follow BERT's random weight initialization", "\n", "self", ".", "apply", "(", "init_bert_params", ")", "\n", "\n", "self", ".", "classification_heads", "=", "nn", ".", "ModuleDict", "(", ")", "\n", "\n", "", "@", "staticmethod", "\n", "def", "add_args", "(", "parser", ")", ":", "\n", "        ", "\"\"\"Add model-specific arguments to the parser.\"\"\"", "\n", "parser", ".", "add_argument", "(", "'--encoder-layers'", ",", "type", "=", "int", ",", "metavar", "=", "'L'", ",", "\n", "help", "=", "'num encoder layers'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'H'", ",", "\n", "help", "=", "'encoder embedding dimension'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-ffn-embed-dim'", ",", "type", "=", "int", ",", "metavar", "=", "'F'", ",", "\n", "help", "=", "'encoder embedding dimension for FFN'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-attention-heads'", ",", "type", "=", "int", ",", "metavar", "=", "'A'", ",", "\n", "help", "=", "'num encoder attention heads'", ")", "\n", "parser", ".", "add_argument", "(", "'--activation-fn'", ",", "\n", "choices", "=", "utils", ".", "get_available_activation_fns", "(", ")", ",", "\n", "help", "=", "'activation function to use'", ")", "\n", "parser", ".", "add_argument", "(", "'--pooler-activation-fn'", ",", "\n", "choices", "=", "utils", ".", "get_available_activation_fns", "(", ")", ",", "\n", "help", "=", "'activation function to use for pooler layer'", ")", "\n", "parser", ".", "add_argument", "(", "'--encoder-normalize-before'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'apply layernorm before each encoder block'", ")", "\n", "parser", ".", "add_argument", "(", "'--dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability'", ")", "\n", "parser", ".", "add_argument", "(", "'--attention-dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability for attention weights'", ")", "\n", "parser", ".", "add_argument", "(", "'--activation-dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability after activation in FFN'", ")", "\n", "parser", ".", "add_argument", "(", "'--pooler-dropout'", ",", "type", "=", "float", ",", "metavar", "=", "'D'", ",", "\n", "help", "=", "'dropout probability in the masked_lm pooler layers'", ")", "\n", "parser", ".", "add_argument", "(", "'--max-positions'", ",", "type", "=", "int", ",", "\n", "help", "=", "'number of positional embeddings to learn'", ")", "\n", "parser", ".", "add_argument", "(", "'--load-checkpoint-heads'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'(re-)register and load heads when loading checkpoints'", ")", "\n", "\n", "", "@", "classmethod", "\n", "def", "build_model", "(", "cls", ",", "args", ",", "task", ")", ":", "\n", "        ", "\"\"\"Build a new model instance.\"\"\"", "\n", "\n", "# make sure all arguments are present", "\n", "base_architecture", "(", "args", ")", "\n", "\n", "if", "not", "hasattr", "(", "args", ",", "'max_positions'", ")", ":", "\n", "            ", "args", ".", "max_positions", "=", "args", ".", "tokens_per_sample", "\n", "\n", "", "encoder", "=", "RobertaEncoder", "(", "args", ",", "task", ".", "source_dictionary", ")", "\n", "return", "cls", "(", "args", ",", "encoder", ")", "\n", "\n", "", "def", "forward", "(", "self", ",", "src_tokens", ",", "features_only", "=", "False", ",", "return_all_hiddens", "=", "False", ",", "classification_head_name", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "if", "classification_head_name", "is", "not", "None", ":", "\n", "            ", "features_only", "=", "True", "\n", "\n", "", "x", ",", "extra", "=", "self", ".", "decoder", "(", "src_tokens", ",", "features_only", ",", "return_all_hiddens", ",", "**", "kwargs", ")", "\n", "\n", "if", "classification_head_name", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "classification_heads", "[", "classification_head_name", "]", "(", "x", ")", "\n", "", "return", "x", ",", "extra", "\n", "\n", "", "def", "register_classification_head", "(", "self", ",", "name", ",", "num_classes", "=", "None", ",", "inner_dim", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Register a classification head.\"\"\"", "\n", "if", "name", "in", "self", ".", "classification_heads", ":", "\n", "            ", "prev_num_classes", "=", "self", ".", "classification_heads", "[", "name", "]", ".", "out_proj", ".", "out_features", "\n", "prev_inner_dim", "=", "self", ".", "classification_heads", "[", "name", "]", ".", "dense", ".", "out_features", "\n", "if", "num_classes", "!=", "prev_num_classes", "or", "inner_dim", "!=", "prev_inner_dim", ":", "\n", "                ", "print", "(", "\n", "'WARNING: re-registering head \"{}\" with num_classes {} (prev: {}) '", "\n", "'and inner_dim {} (prev: {})'", ".", "format", "(", "\n", "name", ",", "num_classes", ",", "prev_num_classes", ",", "inner_dim", ",", "prev_inner_dim", "\n", ")", "\n", ")", "\n", "", "", "self", ".", "classification_heads", "[", "name", "]", "=", "RobertaClassificationHead", "(", "\n", "self", ".", "args", ".", "encoder_embed_dim", ",", "\n", "inner_dim", "or", "self", ".", "args", ".", "encoder_embed_dim", ",", "\n", "num_classes", ",", "\n", "self", ".", "args", ".", "pooler_activation_fn", ",", "\n", "self", ".", "args", ".", "pooler_dropout", ",", "\n", ")", "\n", "\n", "", "@", "property", "\n", "def", "supported_targets", "(", "self", ")", ":", "\n", "        ", "return", "{", "'self'", "}", "\n", "\n", "", "@", "classmethod", "\n", "def", "from_pretrained", "(", "cls", ",", "model_name_or_path", ",", "checkpoint_file", "=", "'model.pt'", ",", "data_name_or_path", "=", "'.'", ",", "**", "kwargs", ")", ":", "\n", "        ", "from", "fairseq", "import", "hub_utils", "\n", "x", "=", "hub_utils", ".", "from_pretrained", "(", "\n", "model_name_or_path", ",", "\n", "checkpoint_file", ",", "\n", "data_name_or_path", ",", "\n", "archive_map", "=", "cls", ".", "hub_models", "(", ")", ",", "\n", "bpe", "=", "'gpt2'", ",", "\n", "load_checkpoint_heads", "=", "True", ",", "\n", "**", "kwargs", ",", "\n", ")", "\n", "return", "RobertaHubInterface", "(", "x", "[", "'args'", "]", ",", "x", "[", "'task'", "]", ",", "x", "[", "'models'", "]", "[", "0", "]", ")", "\n", "\n", "", "def", "upgrade_state_dict_named", "(", "self", ",", "state_dict", ",", "name", ")", ":", "\n", "        ", "prefix", "=", "name", "+", "'.'", "if", "name", "!=", "''", "else", "''", "\n", "current_head_names", "=", "[", "]", "if", "not", "hasattr", "(", "self", ",", "'classification_heads'", ")", "else", "self", ".", "classification_heads", ".", "keys", "(", ")", "\n", "\n", "# Handle new classification heads present in the state dict.", "\n", "keys_to_delete", "=", "[", "]", "\n", "for", "k", "in", "state_dict", ".", "keys", "(", ")", ":", "\n", "            ", "if", "not", "k", ".", "startswith", "(", "prefix", "+", "'classification_heads.'", ")", ":", "\n", "                ", "continue", "\n", "\n", "", "head_name", "=", "k", "[", "len", "(", "prefix", "+", "'classification_heads.'", ")", ":", "]", ".", "split", "(", "'.'", ")", "[", "0", "]", "\n", "num_classes", "=", "state_dict", "[", "prefix", "+", "'classification_heads.'", "+", "head_name", "+", "'.out_proj.weight'", "]", ".", "size", "(", "0", ")", "\n", "inner_dim", "=", "state_dict", "[", "prefix", "+", "'classification_heads.'", "+", "head_name", "+", "'.dense.weight'", "]", ".", "size", "(", "0", ")", "\n", "\n", "if", "getattr", "(", "self", ".", "args", ",", "'load_checkpoint_heads'", ",", "False", ")", ":", "\n", "                ", "if", "head_name", "not", "in", "current_head_names", ":", "\n", "                    ", "self", ".", "register_classification_head", "(", "head_name", ",", "num_classes", ",", "inner_dim", ")", "\n", "", "", "else", ":", "\n", "                ", "if", "head_name", "not", "in", "current_head_names", ":", "\n", "                    ", "print", "(", "\n", "'WARNING: deleting classification head ({}) from checkpoint '", "\n", "'not present in current model: {}'", ".", "format", "(", "head_name", ",", "k", ")", "\n", ")", "\n", "keys_to_delete", ".", "append", "(", "k", ")", "\n", "", "elif", "(", "\n", "num_classes", "!=", "self", ".", "classification_heads", "[", "head_name", "]", ".", "out_proj", ".", "out_features", "\n", "or", "inner_dim", "!=", "self", ".", "classification_heads", "[", "head_name", "]", ".", "dense", ".", "out_features", "\n", ")", ":", "\n", "                    ", "print", "(", "\n", "'WARNING: deleting classification head ({}) from checkpoint '", "\n", "'with different dimensions than current model: {}'", ".", "format", "(", "head_name", ",", "k", ")", "\n", ")", "\n", "keys_to_delete", ".", "append", "(", "k", ")", "\n", "", "", "", "for", "k", "in", "keys_to_delete", ":", "\n", "            ", "del", "state_dict", "[", "k", "]", "\n", "\n", "# Copy any newly-added classification heads into the state dict", "\n", "# with their current weights.", "\n", "", "if", "hasattr", "(", "self", ",", "'classification_heads'", ")", ":", "\n", "            ", "cur_state", "=", "self", ".", "classification_heads", ".", "state_dict", "(", ")", "\n", "for", "k", ",", "v", "in", "cur_state", ".", "items", "(", ")", ":", "\n", "                ", "if", "prefix", "+", "'classification_heads.'", "+", "k", "not", "in", "state_dict", ":", "\n", "                    ", "print", "(", "'Overwriting'", ",", "prefix", "+", "'classification_heads.'", "+", "k", ")", "\n", "state_dict", "[", "prefix", "+", "'classification_heads.'", "+", "k", "]", "=", "v", "\n", "\n", "\n", "", "", "", "", "", "class", "RobertaLMHead", "(", "nn", ".", "Module", ")", ":", "\n", "    ", "\"\"\"Head for masked language modeling.\"\"\"", "\n", "\n", "def", "__init__", "(", "self", ",", "embed_dim", ",", "output_dim", ",", "activation_fn", ",", "weight", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "embed_dim", ")", "\n", "self", ".", "activation_fn", "=", "utils", ".", "get_activation_fn", "(", "activation_fn", ")", "\n", "self", ".", "layer_norm", "=", "LayerNorm", "(", "embed_dim", ")", "\n", "\n", "if", "weight", "is", "None", ":", "\n", "            ", "weight", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "output_dim", ",", "bias", "=", "False", ")", ".", "weight", "\n", "", "self", ".", "weight", "=", "weight", "\n", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "output_dim", ")", ")", "\n", "\n", "", "def", "forward", "(", "self", ",", "features", ",", "**", "kwargs", ")", ":", "\n", "        ", "x", "=", "self", ".", "dense", "(", "features", ")", "\n", "x", "=", "self", ".", "activation_fn", "(", "x", ")", "\n", "x", "=", "self", ".", "layer_norm", "(", "x", ")", "\n", "\n", "# project back to size of vocabulary with bias", "\n", "x", "=", "F", ".", "linear", "(", "x", ",", "self", ".", "weight", ")", "+", "self", ".", "bias", "\n", "\n", "return", "x", "\n", "\n", "\n", "", "", "class", "RobertaClassificationHead", "(", "nn", ".", "Module", ")", ":", "\n", "    ", "\"\"\"Head for sentence-level classification tasks.\"\"\"", "\n", "\n", "def", "__init__", "(", "self", ",", "input_dim", ",", "inner_dim", ",", "num_classes", ",", "activation_fn", ",", "pooler_dropout", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "input_dim", ",", "inner_dim", ")", "\n", "self", ".", "activation_fn", "=", "utils", ".", "get_activation_fn", "(", "activation_fn", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.model.AMRModel.reset_stats": [[222, 236], ["model.AMRModel.action_acc.reset", "model.AMRModel.label_acc.reset", "model.AMRModel.labelA_acc.reset", "model.AMRModel.pred_acc.reset", "model.AMRModel.action_confusion_matrix.reset", "model.AMRModel.label_confusion_matrix.reset"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.reset", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.reset", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.reset", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.reset", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.reset", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.reset"], ["self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "p", "=", "pooler_dropout", ")", "\n", "self", ".", "out_proj", "=", "nn", ".", "Linear", "(", "inner_dim", ",", "num_classes", ")", "\n", "\n", "", "def", "forward", "(", "self", ",", "features", ",", "**", "kwargs", ")", ":", "\n", "        ", "x", "=", "features", "[", ":", ",", "0", ",", ":", "]", "# take <s> token (equiv. to [CLS])", "\n", "x", "=", "self", ".", "dropout", "(", "x", ")", "\n", "x", "=", "self", ".", "dense", "(", "x", ")", "\n", "x", "=", "self", ".", "activation_fn", "(", "x", ")", "\n", "x", "=", "self", ".", "dropout", "(", "x", ")", "\n", "x", "=", "self", ".", "out_proj", "(", "x", ")", "\n", "return", "x", "\n", "\n", "\n", "", "", "class", "RobertaEncoder", "(", "FairseqDecoder", ")", ":", "\n", "    "]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.model.AMRModel.rand_init": [[237, 289], ["transition_amr_parser.initialize_embedding", "transition_amr_parser.initialize_embedding", "transition_amr_parser.initialize_embedding", "transition_amr_parser.initialize_embedding", "transition_amr_parser.initialize_embedding", "transition_amr_parser.initialize_embedding", "transition_amr_parser.initialize_embedding", "transition_amr_parser.initialize_embedding", "transition_amr_parser.initialize_embedding", "transition_amr_parser.initialize_embedding", "transition_amr_parser.initialize_lstm_cell", "transition_amr_parser.initialize_lstm_cell", "transition_amr_parser.initialize_lstm_cell", "transition_amr_parser.initialize_lstm_cell", "transition_amr_parser.initialize_lstm_cell", "transition_amr_parser.initialize_lstm_cell", "transition_amr_parser.initialize_linear", "transition_amr_parser.initialize_linear", "transition_amr_parser.initialize_linear", "transition_amr_parser.initialize_linear", "transition_amr_parser.initialize_linear", "transition_amr_parser.initialize_linear", "transition_amr_parser.initialize_linear", "transition_amr_parser.initialize_linear", "transition_amr_parser.initialize_linear", "transition_amr_parser.initialize_linear", "transition_amr_parser.initialize_linear", "transition_amr_parser.initialize_linear", "transition_amr_parser.initialize_linear", "transition_amr_parser.initialize_linear", "transition_amr_parser.initialize_linear", "transition_amr_parser.initialize_linear", "transition_amr_parser.initialize_linear", "transition_amr_parser.initialize_linear", "transition_amr_parser.initialize_linear", "transition_amr_parser.initialize_linear", "transition_amr_parser.initialize_linear", "transition_amr_parser.initialize_linear", "transition_amr_parser.initialize_linear", "transition_amr_parser.initialize_linear", "transition_amr_parser.initialize_linear", "transition_amr_parser.initialize_linear", "transition_amr_parser.initialize_embedding", "transition_amr_parser.initialize_embedding", "transition_amr_parser.initialize_lstm", "transition_amr_parser.initialize_lstm", "transition_amr_parser.initialize_lstm", "transition_amr_parser.initialize_lstm", "transition_amr_parser.initialize_linear", "transition_amr_parser.initialize_linear", "transition_amr_parser.initialize_linear", "transition_amr_parser.initialize_linear", "transition_amr_parser.initialize_lstm", "transition_amr_parser.initialize_lstm", "transition_amr_parser.initialize_lstm", "transition_amr_parser.initialize_lstm", "transition_amr_parser.initialize_linear", "transition_amr_parser.initialize_linear", "transition_amr_parser.initialize_lstm_cell", "transition_amr_parser.initialize_lstm_cell", "transition_amr_parser.initialize_linear", "transition_amr_parser.initialize_linear", "transition_amr_parser.initialize_linear", "transition_amr_parser.initialize_linear", "transition_amr_parser.initialize_zero", "transition_amr_parser.initialize_zero", "transition_amr_parser.initialize_zero", "transition_amr_parser.initialize_zero", "transition_amr_parser.initialize_zero", "transition_amr_parser.initialize_zero", "transition_amr_parser.initialize_zero", "transition_amr_parser.initialize_zero", "transition_amr_parser.initialize_linear", "transition_amr_parser.initialize_linear", "transition_amr_parser.initialize_zero", "transition_amr_parser.initialize_zero"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_lstm_cell", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_lstm_cell", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_lstm_cell", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_lstm_cell", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_lstm_cell", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_lstm_cell", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_embedding", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_lstm", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_lstm", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_lstm", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_lstm", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_lstm", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_lstm", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_lstm", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_lstm", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_lstm_cell", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_lstm_cell", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_zero", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_zero", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_zero", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_zero", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_zero", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_zero", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_zero", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_zero", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_linear", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_zero", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_zero"], ["\n", "\n", "def", "__init__", "(", "self", ",", "args", ",", "dictionary", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ")", "\n", "self", ".", "args", "=", "args", "\n", "self", ".", "sentence_encoder", "=", "TransformerSentenceEncoder", "(", "\n", "padding_idx", "=", "dictionary", ".", "pad", "(", ")", ",", "\n", "vocab_size", "=", "len", "(", "dictionary", ")", ",", "\n", "num_encoder_layers", "=", "args", ".", "encoder_layers", ",", "\n", "embedding_dim", "=", "args", ".", "encoder_embed_dim", ",", "\n", "ffn_embedding_dim", "=", "args", ".", "encoder_ffn_embed_dim", ",", "\n", "num_attention_heads", "=", "args", ".", "encoder_attention_heads", ",", "\n", "dropout", "=", "args", ".", "dropout", ",", "\n", "attention_dropout", "=", "args", ".", "attention_dropout", ",", "\n", "activation_dropout", "=", "args", ".", "activation_dropout", ",", "\n", "max_seq_len", "=", "args", ".", "max_positions", ",", "\n", "num_segments", "=", "0", ",", "\n", "encoder_normalize_before", "=", "True", ",", "\n", "apply_bert_init", "=", "True", ",", "\n", "activation_fn", "=", "args", ".", "activation_fn", ",", "\n", ")", "\n", "self", ".", "lm_head", "=", "RobertaLMHead", "(", "\n", "embed_dim", "=", "args", ".", "encoder_embed_dim", ",", "\n", "output_dim", "=", "len", "(", "dictionary", ")", ",", "\n", "activation_fn", "=", "args", ".", "activation_fn", ",", "\n", "weight", "=", "self", ".", "sentence_encoder", ".", "embed_tokens", ".", "weight", ",", "\n", ")", "\n", "\n", "", "def", "forward", "(", "self", ",", "src_tokens", ",", "features_only", "=", "False", ",", "return_all_hiddens", "=", "False", ",", "**", "unused", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            src_tokens (LongTensor): input tokens of shape `(batch, src_len)`\n            features_only (bool, optional): skip LM head and just return\n                features. If True, the output will be of shape\n                `(batch, src_len, embed_dim)`.\n            return_all_hiddens (bool, optional): also return all of the\n                intermediate hidden states (default: False).\n\n        Returns:\n            tuple:\n                - the LM output of shape `(batch, src_len, vocab)`\n                - a dictionary of additional data, where 'inner_states'\n                  is a list of hidden states.\n        \"\"\"", "\n", "x", ",", "extra", "=", "self", ".", "extract_features", "(", "src_tokens", ",", "return_all_hiddens", ")", "\n", "if", "not", "features_only", ":", "\n", "            ", "x", "=", "self", ".", "output_layer", "(", "x", ")", "\n", "", "return", "x", ",", "extra", "\n", "\n", "", "def", "extract_features", "(", "self", ",", "src_tokens", ",", "return_all_hiddens", "=", "False", ",", "**", "unused", ")", ":", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.model.AMRModel.forward": [[290, 374], ["range", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat.sum", "torch.cat.sum", "torch.cat.sum", "sentence_tensor.size", "torch.cat.append", "torch.cat.append", "torch.cat.append", "model.AMRModel.forward_single", "model.AMRModel.forward_single", "model.AMRModel.build_amr", "model.AMRModel.forward_single", "model.AMRModel.build_amr", "str", "open", "open", "open", "open.write", "open.write", "open.write", "open.close", "open.close", "open.close", "os.system", "os.system", "open", "open.readline().strip().split", "open", "open.readline().strip().split", "open.close", "open.close", "os.remove", "os.remove", "os.remove", "os.remove", "os.remove", "loss.reshape", "sentence_tensor[].unsqueeze", "labelsO[].unsqueeze", "labelsA[].unsqueeze", "actions[].unsqueeze", "preds[].unsqueeze", "sentence_tensor[].unsqueeze", "labelsO[].unsqueeze", "labelsA[].unsqueeze", "actions[].unsqueeze", "preds[].unsqueeze", "sentence_tensor[].unsqueeze", "labelsO[].unsqueeze", "labelsA[].unsqueeze", "actions[].unsqueeze", "preds[].unsqueeze", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "model.AMRModel.toJAMRString", "model.AMRModel.toJAMRString", "gold_amr.toJAMRString", "float", "float", "sent_idx[].item", "os.getpid", "open.readline().strip", "len", "open.readline().strip", "len", "open.readline", "open.readline"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.model.AMRModel.forward_single", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.model.AMRModel.forward_single", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.model.AMRModel.build_amr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.model.AMRModel.forward_single", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.model.AMRModel.build_amr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.GoldMiner.close", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.GoldMiner.close", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.GoldMiner.close", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.GoldMiner.close", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.GoldMiner.close", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.distributed_utils.get_rank", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.distributed_utils.get_rank", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.distributed_utils.get_rank", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.distributed_utils.get_rank", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.distributed_utils.get_rank", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.distributed_utils.get_rank", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.distributed_utils.get_rank", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.distributed_utils.get_rank", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.distributed_utils.get_rank", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.toJAMRString", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.toJAMRString", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.toJAMRString", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item"], ["        ", "inner_states", ",", "_", "=", "self", ".", "sentence_encoder", "(", "\n", "src_tokens", ",", "last_state_only", "=", "not", "return_all_hiddens", ",", "\n", ")", "\n", "features", "=", "inner_states", "[", "-", "1", "]", "\n", "return", "features", ",", "{", "'inner_states'", ":", "inner_states", "if", "return_all_hiddens", "else", "None", "}", "\n", "\n", "", "def", "output_layer", "(", "self", ",", "features", ",", "**", "unused", ")", ":", "\n", "        ", "return", "self", ".", "lm_head", "(", "features", ")", "\n", "\n", "", "def", "max_positions", "(", "self", ")", ":", "\n", "        ", "\"\"\"Maximum output length supported by the encoder.\"\"\"", "\n", "return", "self", ".", "args", ".", "max_positions", "\n", "\n", "\n", "", "", "@", "register_model_architecture", "(", "'roberta'", ",", "'roberta'", ")", "\n", "def", "base_architecture", "(", "args", ")", ":", "\n", "    ", "args", ".", "encoder_layers", "=", "getattr", "(", "args", ",", "'encoder_layers'", ",", "12", ")", "\n", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "768", ")", "\n", "args", ".", "encoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_ffn_embed_dim'", ",", "3072", ")", "\n", "args", ".", "encoder_attention_heads", "=", "getattr", "(", "args", ",", "'encoder_attention_heads'", ",", "12", ")", "\n", "\n", "args", ".", "activation_fn", "=", "getattr", "(", "args", ",", "'activation_fn'", ",", "'gelu'", ")", "\n", "args", ".", "pooler_activation_fn", "=", "getattr", "(", "args", ",", "'pooler_activation_fn'", ",", "'tanh'", ")", "\n", "\n", "args", ".", "dropout", "=", "getattr", "(", "args", ",", "'dropout'", ",", "0.1", ")", "\n", "args", ".", "attention_dropout", "=", "getattr", "(", "args", ",", "'attention_dropout'", ",", "0.1", ")", "\n", "args", ".", "activation_dropout", "=", "getattr", "(", "args", ",", "'activation_dropout'", ",", "0.0", ")", "\n", "args", ".", "pooler_dropout", "=", "getattr", "(", "args", ",", "'pooler_dropout'", ",", "0.0", ")", "\n", "\n", "\n", "", "@", "register_model_architecture", "(", "'roberta'", ",", "'roberta_base'", ")", "\n", "def", "roberta_base_architecture", "(", "args", ")", ":", "\n", "    ", "base_architecture", "(", "args", ")", "\n", "\n", "\n", "", "@", "register_model_architecture", "(", "'roberta'", ",", "'roberta_large'", ")", "\n", "def", "roberta_large_architecture", "(", "args", ")", ":", "\n", "    ", "args", ".", "encoder_layers", "=", "getattr", "(", "args", ",", "'encoder_layers'", ",", "24", ")", "\n", "args", ".", "encoder_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_embed_dim'", ",", "1024", ")", "\n", "args", ".", "encoder_ffn_embed_dim", "=", "getattr", "(", "args", ",", "'encoder_ffn_embed_dim'", ",", "4096", ")", "\n", "args", ".", "encoder_attention_heads", "=", "getattr", "(", "args", ",", "'encoder_attention_heads'", ",", "16", ")", "\n", "base_architecture", "(", "args", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.model.AMRModel.forward_single": [[375, 783], ["model.AMRModel.dropout_emb", "word_embeds.squeeze.squeeze.squeeze", "sentence_tensor.squeeze.squeeze.squeeze", "transition_amr_parser.StackRNN", "transition_amr_parser.StackRNN", "transition_amr_parser.StackRNN", "transition_amr_parser.StackRNN", "transition_amr_parser.StackRNN", "transition_amr_parser.StackRNN", "sentence_tensor.squeeze.squeeze.tolist", "list", "enumerate", "zip", "set", "set", "set", "set", "set.discard", "enumerate", "model.AMRModel.word_embeds", "model.AMRModel.dropout_emb", "action_embeds.squeeze.squeeze.squeeze", "actions.squeeze.squeeze.squeeze", "model.AMRModel.dropout_emb", "pred_embeds.squeeze.squeeze.squeeze", "preds.squeeze.squeeze.squeeze", "model.AMRModel.dropout_emb", "labelA_embeds.squeeze.squeeze.squeeze", "labelsA.squeeze.squeeze.squeeze", "model.AMRModel.dropout_emb", "labelO_embeds.squeeze.squeeze.squeeze", "labelsO.squeeze.squeeze.squeeze", "transition_amr_parser.StackRNN", "transition_amr_parser.StackRNN", "len", "reversed", "reversed", "model.AMRModel.forward_lstm", "model.AMRModel.backward_lstm", "transition_amr_parser.reverse_sequence", "transition_amr_parser.reverse_sequence", "model.AMRModel.predict_with_softmax", "transition_amr_parser.StackRNN.push", "real_action.startswith", "predict_actions.append", "predict_labels.append", "predict_labelsA.append", "predict_predicates.append", "total_losses.extend", "len", "model.AMRModel.action_embeds", "model.AMRModel.pred_embeds", "model.AMRModel.labelA_embeds", "model.AMRModel.labelO_embeds", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "word_embeds[].unsqueeze", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "model.AMRModel.pretrained_2_embed", "token_embedding[].unsqueeze", "torch.cat.unsqueeze", "torch.cat.unsqueeze", "torch.cat.unsqueeze", "transition_amr_parser.reverse_sequence().unsqueeze", "transition_amr_parser.reverse_sequence().unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat().squeeze", "torch.cat().squeeze", "torch.cat().squeeze", "torch.cat().squeeze", "torch.cat().squeeze", "torch.cat().squeeze", "torch.cat().squeeze", "torch.cat().squeeze", "torch.cat().squeeze", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.matmul().unsqueeze", "torch.matmul().unsqueeze", "torch.matmul().unsqueeze", "torch.matmul().unsqueeze", "torch.matmul().unsqueeze", "torch.matmul().unsqueeze", "torch.matmul().unsqueeze", "torch.matmul().unsqueeze", "torch.matmul().unsqueeze", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "list", "model.AMRModel.predict_with_softmax", "model.AMRModel.get_possible_actions", "model.AMRModel.weight_vectors", "model.AMRModel.action_acc.add", "transition_amr_parser.StackRNN.pop", "bufferint.pop", "stackint.append", "transition_amr_parser.StackRNN.push", "real_action.startswith", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "len", "losses_per_component.append", "losses_per_component.append", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "model.AMRModel.tok_2_embed", "bert_embed.cuda.cuda.cuda", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "latent.append", "latentint.append", "transition_amr_parser.StackRNN.push", "bufferint.append", "len", "len", "torch.cat.squeeze", "torch.cat.squeeze", "torch.cat.squeeze", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "model.AMRModel.attention_ff1_1", "transition_amr_parser.StackRNN.output", "transition_amr_parser.StackRNN.output", "transition_amr_parser.StackRNN.output", "transition_amr_parser.StackRNN.output", "model.AMRModel.prevent_overfitting", "model.AMRModel.action2idx.values", "transition_amr_parser.StackRNN.output", "model.AMRModel.weight_vectors", "model.AMRModel.weight_vectors", "predict_actions.append", "predict_labels.append", "predict_labelsA.append", "predict_predicates.append", "stackint.pop", "transition_amr_parser.StackRNN.pop", "real_action.startswith", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "bert_embed.cuda.cuda.unsqueeze", "transition_amr_parser.reverse_sequence", "transition_amr_parser.reverse_sequence", "transition_amr_parser.StackRNN.output", "transition_amr_parser.StackRNN.output", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "model.AMRModel.dropout", "model.AMRModel.weight_vectors", "model.AMRModel.action_attention().squeeze", "len", "transition_amr_parser.StackRNN.push", "stackint.pop", "stackint.pop", "bufferint.append", "stackint.append", "transition_amr_parser.StackRNN.pop", "transition_amr_parser.StackRNN.pop", "transition_amr_parser.StackRNN.push", "transition_amr_parser.StackRNN.push", "swapped_words[].append", "real_action.startswith", "actions[].item", "torch.sum().item", "torch.sum().item", "torch.sum().item", "torch.sum().item", "torch.sum().item", "torch.sum().item", "torch.sum().item", "torch.sum().item", "torch.sum().item", "word_embeds[].unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "model.AMRModel.char_embeds", "model.AMRModel.char_lstm_forward", "model.AMRModel.char_lstm_backward", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat().squeeze.transpose", "torch.cat().squeeze.transpose", "torch.cat().squeeze.transpose", "transition_amr_parser.StackRNN.output", "model.AMRModel.label_attention().squeeze", "stackint.pop", "transition_amr_parser.StackRNN.pop", "transition_amr_parser.StackRNN.pop", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "transition_amr_parser.StackRNN.push", "set.discard", "real_action.startswith", "actions[].item", "len", "word_embeds[].unsqueeze", "chars_tensor.cuda.cuda.cuda", "chars_tensor.cuda.cuda.unsqueeze", "model.AMRModel.transpose", "transition_amr_parser.reverse_sequence", "transition_amr_parser.reverse_sequence", "model.AMRModel.action_attention", "model.AMRModel.merge_composition", "transition_amr_parser.StackRNN.pop", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "transition_amr_parser.StackRNN.push", "set.discard", "real_action.startswith", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "word_embeds[].unsqueeze", "model.AMRModel.transpose", "word_embeds[].unsqueeze", "model.AMRModel.label_attention", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "model.AMRModel.dep_composition", "transition_amr_parser.StackRNN.pop", "transition_amr_parser.StackRNN.pop", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "transition_amr_parser.StackRNN.push", "transition_amr_parser.StackRNN.push", "set.add", "set.discard", "set.discard", "real_action.startswith", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "model.AMRModel.predict_with_softmax", "model.AMRModel.label_acc.add", "model.AMRModel.arc_composition_head", "label.startswith", "set.add", "transition_amr_parser.StackRNN.pop", "transition_amr_parser.StackRNN.pop", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "transition_amr_parser.StackRNN.push", "transition_amr_parser.StackRNN.push", "set.add", "set.discard", "set.discard", "real_action.startswith", "transition_amr_parser.StackRNN.output", "model.AMRModel.weight_vectors", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "label.endswith", "model.AMRModel.predict_with_softmax", "model.AMRModel.label_acc.add", "model.AMRModel.arc_composition_head", "label.startswith", "set.add", "transition_amr_parser.StackRNN.pop", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "transition_amr_parser.StackRNN.push", "set.add", "set.discard", "real_action.startswith", "model.AMRModel.weight_vectors", "transition_amr_parser.StackRNN.output", "model.AMRModel.weight_vectors", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "label.endswith", "transition_amr_parser.StackRNN.last", "model.AMRModel.weight_vectors", "model.AMRModel.predict_with_softmax", "model.AMRModel.pred_acc.add", "model.AMRModel.pred_composition", "model.AMRModel.predict_with_softmax", "transition_amr_parser.StackRNN.pop", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "transition_amr_parser.StackRNN.push", "set.add", "set.discard", "real_action.startswith", "model.AMRModel.label_attention().squeeze", "model.AMRModel.weight_vectors", "model.AMRModel.weight_vectors", "preds[].item", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "model.AMRModel.weight_vectors", "model.AMRModel.labelA_acc.add", "model.AMRModel.addnode_composition", "latent.pop", "transition_amr_parser.StackRNN.push", "latentint.pop", "stackint.append", "model.AMRModel.label_attention().squeeze", "pred_attention().squeeze", "possible_predicates.append", "model.AMRModel.lemmatizer", "model.AMRModel.weight_vectors", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "model.AMRModel.label_attention", "model.AMRModel.labelA_attention().squeeze", "model.AMRModel.label_attention", "pred_attention", "tokens.index", "model.AMRModel.labelA_attention"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.reverse_sequence", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.reverse_sequence", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.model.AMRModel.predict_with_softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.push", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.model.AMRModel.predict_with_softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.model.AMRModel.get_possible_actions", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.model.AMRModel.weight_vectors", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.add", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.push", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.push", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.output", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.output", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.output", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.output", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.output", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.model.AMRModel.weight_vectors", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.model.AMRModel.weight_vectors", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.reverse_sequence", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.reverse_sequence", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.output", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.output", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.model.AMRModel.weight_vectors", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.push", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.push", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.push", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.output", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.push", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.reverse_sequence", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.reverse_sequence", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.push", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.push", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.push", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.add", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.model.AMRModel.predict_with_softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.add", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.add", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.push", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.push", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.add", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.output", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.model.AMRModel.weight_vectors", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.model.AMRModel.predict_with_softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.add", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.add", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.push", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.add", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.model.AMRModel.weight_vectors", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.output", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.model.AMRModel.weight_vectors", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.last", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.model.AMRModel.weight_vectors", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.model.AMRModel.predict_with_softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.add", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.model.AMRModel.predict_with_softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.push", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.add", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.model.AMRModel.weight_vectors", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.model.AMRModel.weight_vectors", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.model.AMRModel.weight_vectors", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.add", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.push", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.model.AMRModel.weight_vectors", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.index"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.model.AMRModel.predict_with_softmax": [[785, 827], ["torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.nn.functional.log_softmax.argmax().item", "torch.nn.functional.log_softmax.argmax().item", "torch.nn.functional.log_softmax.argmax().item", "torch.distributions.categorical.Categorical", "torch.distributions.categorical.Categorical", "torch.distributions.categorical.Categorical", "torch.distributions.categorical.Categorical", "torch.distributions.categorical.Categorical", "torch.distributions.categorical.Categorical", "torch.distributions.categorical.Categorical", "torch.distributions.categorical.Categorical", "torch.distributions.categorical.Categorical", "torch.distributions.categorical.Categorical.sample", "torch.distributions.categorical.Categorical.sample", "torch.distributions.categorical.Categorical.sample", "losses.append", "embeds[].unsqueeze", "softmax1", "softmax1", "enumerate", "random.randint", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "losses.append", "confusion_matrix.add", "model.AMRModel.dropout_emb", "embeddings[].unsqueeze", "model.AMRModel.dropout", "model.AMRModel.dropout", "softmax2", "torch.LongTensor().cuda", "torch.LongTensor().cuda", "torch.LongTensor().cuda", "torch.LongTensor().cuda", "torch.LongTensor().cuda", "torch.LongTensor().cuda", "torch.LongTensor().cuda", "torch.LongTensor().cuda", "torch.LongTensor().cuda", "softmax2", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.nn.functional.log_softmax.argmax", "torch.nn.functional.log_softmax.argmax", "torch.nn.functional.log_softmax.argmax", "tensor[].item", "torch.from_numpy().cuda", "torch.from_numpy().cuda", "torch.from_numpy().cuda", "torch.from_numpy().cuda", "torch.from_numpy().cuda", "torch.from_numpy().cuda", "torch.from_numpy().cuda", "torch.from_numpy().cuda", "torch.from_numpy().cuda", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "embeds", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "numpy.array", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "numpy.array", "tensor[].item"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.log_softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.log_softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.log_softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.log_softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.log_softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.log_softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.log_softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.log_softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.log_softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.hub_utils.GeneratorHubInterface.sample", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.hub_utils.GeneratorHubInterface.sample", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.hub_utils.GeneratorHubInterface.sample", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.add", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.argmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.argmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.argmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.model.AMRModel.weight_vectors": [[828, 834], ["torch.cat().sum().unsqueeze", "torch.cat().sum().unsqueeze", "torch.cat().sum().unsqueeze", "torch.cat().sum().unsqueeze", "torch.cat().sum().unsqueeze", "torch.cat().sum().unsqueeze", "torch.cat().sum().unsqueeze", "torch.cat().sum().unsqueeze", "torch.cat().sum().unsqueeze", "enumerate", "torch.softmax", "torch.softmax", "torch.softmax", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.cat().sum", "torch.cat().sum", "torch.cat().sum", "torch.cat().sum", "torch.cat().sum", "torch.cat().sum", "torch.cat().sum", "torch.cat().sum", "torch.cat().sum", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.softmax"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.model.AMRModel.get_possible_actions": [[835, 892], ["model.AMRModel.action2idx.items", "k.startswith", "k.startswith", "valid_actions.append", "k.startswith", "k.startswith", "len", "len", "len", "valid_actions.append", "k.startswith", "valid_actions.append", "len", "len", "valid_actions.append", "k.startswith", "k.startswith", "k.startswith", "k.endswith", "len", "valid_actions.append", "k.startswith", "k.startswith", "k.startswith", "k.startswith", "k.startswith", "len", "valid_actions.append", "k.startswith", "k.endswith", "k.endswith", "valid_actions.append", "k.startswith", "len", "valid_actions.append", "Exception", "len", "valid_actions.append", "len"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.model.AMRModel.build_amr": [[893, 909], ["zip", "transition_amr_parser.state_machine.AMRStateMachine", "transition_amr_parser.state_machine.AMRStateMachine", "transition_amr_parser.state_machine.AMRStateMachine.applyActions", "transition_amr_parser.state_machine.AMRStateMachine.applyActions", "act.startswith", "apply_actions.append", "act.startswith", "apply_actions.append", "act.startswith", "act.startswith", "apply_actions.append", "apply_actions.append", "act.endswith"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.applyActions", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.applyActions", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.model.AMRModel.parse_sentence": [[910, 918], ["model.AMRModel.forward_single", "model.AMRModel.build_amr"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.model.AMRModel.forward_single", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.model.AMRModel.build_amr"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.edit.yellow_font": [[20, 22], ["None"], "function", ["None"], ["def", "yellow_font", "(", "string", ")", ":", "\n", "    ", "return", "\"\\033[93m%s\\033[0m\"", "%", "string", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.edit.argument_parser": [[23, 84], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["", "def", "argument_parser", "(", ")", ":", "\n", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Tool to handle AMR'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--in-amr\"", ",", "\n", "help", "=", "\"input AMR files in pennman notation\"", ",", "\n", "type", "=", "str", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--out-amr\"", ",", "\n", "help", "=", "\"output AMR files in pennman notation\"", ",", "\n", "type", "=", "str", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--in-tokens\"", ",", "\n", "help", "=", "\"tab separated tokens one sentence per line\"", ",", "\n", "type", "=", "str", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--in-scored-actions\"", ",", "\n", "help", "=", "\"actions and action features pre-appended\"", ",", "\n", "type", "=", "str", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--in-actions\"", ",", "\n", "help", "=", "\"tab separated actions one sentence per line\"", ",", "\n", "type", "=", "str", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--out-actions\"", ",", "\n", "help", "=", "\"tab separated actions one sentence per line\"", ",", "\n", "type", "=", "str", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--merge-mined\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"--out-actions will contain merge of --in-actions and --in-scored-actions\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--fix-actions\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"fix actions split by whitespace arguments\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--in-rule-stats\"", ",", "\n", "help", "=", "\"Input rule stats for statistics building\"", ",", "\n", "type", "=", "str", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--out-rule-stats\"", ",", "\n", "help", "=", "\"Output rule stats from mined actions\"", ",", "\n", "type", "=", "str", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--entity-rules\"", ",", "\n", "help", "=", "\"entity rules\"", ",", "\n", "type", "=", "str", "\n", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.edit.print_score_action_stats": [[86, 100], ["collections.Counter", "print", "collections.Counter.update", "range", "smatch.compute_f"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update"], ["", "def", "print_score_action_stats", "(", "scored_actions", ")", ":", "\n", "    ", "scores", "=", "[", "0", ",", "0", ",", "0", "]", "\n", "action_count", "=", "Counter", "(", ")", "\n", "for", "sa", "in", "scored_actions", ":", "\n", "        ", "action_count", ".", "update", "(", "[", "sa", "[", "6", "]", "]", ")", "\n", "for", "i", "in", "range", "(", "3", ")", ":", "\n", "            ", "scores", "[", "i", "]", "+=", "sa", "[", "i", "+", "1", "]", "\n", "", "", "smatch", "=", "compute_f", "(", "*", "scores", ")", "[", "2", "]", "\n", "display_str", "=", "'Smatch {:.3f} scored mined {:d} length mined {:d}'", ".", "format", "(", "\n", "smatch", ",", "\n", "action_count", "[", "'score'", "]", ",", "\n", "action_count", "[", "'length'", "]", "\n", ")", "\n", "print", "(", "display_str", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.edit.fix_actions_split_by_spaces": [[102, 135], ["new_actions.append", "print", "len", "edit.yellow_font", "len", "new_sent_actions.append", "new_sent_actions.append", "sent_actions[].split", "len", "sent_actions[].split"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.yellow_font", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["", "def", "fix_actions_split_by_spaces", "(", "actions", ")", ":", "\n", "\n", "# Fix actions split by spaces", "\n", "    ", "new_actions", "=", "[", "]", "\n", "num_fixed", "=", "0", "\n", "for", "sent_actions", "in", "actions", ":", "\n", "        ", "new_sent_actions", "=", "[", "]", "\n", "index", "=", "0", "\n", "while", "index", "<", "len", "(", "sent_actions", ")", ":", "\n", "# There can be no actions with a single quote. If one found look", "\n", "# until we find another one and assume the sapn covered is a split", "\n", "# action", "\n", "            ", "if", "len", "(", "sent_actions", "[", "index", "]", ".", "split", "(", "'\"'", ")", ")", "==", "2", ":", "\n", "                ", "start_index", "=", "index", "\n", "index", "+=", "1", "\n", "while", "len", "(", "sent_actions", "[", "index", "]", ".", "split", "(", "'\"'", ")", ")", "!=", "2", ":", "\n", "                    ", "index", "+=", "1", "\n", "", "new_sent_actions", ".", "append", "(", "\n", "\" \"", ".", "join", "(", "sent_actions", "[", "start_index", ":", "index", "+", "1", "]", ")", "\n", ")", "\n", "num_fixed", "+=", "1", "\n", "", "else", ":", "\n", "                ", "new_sent_actions", ".", "append", "(", "sent_actions", "[", "index", "]", ")", "\n", "# increase index    ", "\n", "", "index", "+=", "1", "\n", "", "new_actions", ".", "append", "(", "new_sent_actions", ")", "\n", "\n", "", "if", "num_fixed", ":", "\n", "        ", "message_str", "=", "(", "f'WARNING: {num_fixed} actions had to be fixed for '", "\n", "'whitespace split'", ")", "\n", "print", "(", "yellow_font", "(", "message_str", ")", ")", "\n", "\n", "", "return", "new_actions", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.edit.merge_actions": [[137, 145], ["enumerate", "created_actions.append", "created_actions.append"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "merge_actions", "(", "actions", ",", "scored_actions", ")", ":", "\n", "    ", "created_actions", "=", "[", "]", "\n", "for", "index", ",", "actions", "in", "enumerate", "(", "actions", ")", ":", "\n", "        ", "if", "scored_actions", "[", "index", "]", "[", "6", "]", "is", "not", "None", ":", "\n", "            ", "created_actions", ".", "append", "(", "scored_actions", "[", "index", "]", "[", "7", "]", ")", "\n", "", "else", ":", "\n", "            ", "created_actions", ".", "append", "(", "actions", ")", "\n", "", "", "return", "created_actions", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.edit.merge_rules": [[147, 206], ["rule_stats[].items", "transition_amr_parser.state_machine.get_spacy_lemmatizer", "collections.defaultdict", "tqdm.tqdm", "edit.merge_both_rules", "collections.Counter", "enumerate", "transition_amr_parser.state_machine.AMRStateMachine", "collections.Counter", "transition_amr_parser.state_machine.AMRStateMachine.get_top_of_stack", "action.startswith", "transition_amr_parser.state_machine.AMRStateMachine.applyAction", "possible_predicates[].update", "possible_predicates[].update", "transition_amr_parser.state_machine.AMRStateMachine.get_top_of_stack", "possible_predicates[].update", "possible_predicates[].update", "transition_amr_parser.state_machine.AMRStateMachine.get_top_of_stack", "possible_predicates[].update", "possible_predicates[].update"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.get_spacy_lemmatizer", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.edit.merge_both_rules", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.get_top_of_stack", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.DepParsingStateMachine.applyAction", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.get_top_of_stack", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.get_top_of_stack", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update"], ["", "def", "merge_rules", "(", "sentences", ",", "actions", ",", "rule_stats", ",", "entity_rules", "=", "None", ")", ":", "\n", "\n", "# generate rules to restrict action space by stack content", "\n", "    ", "actions_by_stack_rules", "=", "rule_stats", "[", "'possible_predicates'", "]", "\n", "for", "token", ",", "counter", "in", "rule_stats", "[", "'possible_predicates'", "]", ".", "items", "(", ")", ":", "\n", "        ", "actions_by_stack_rules", "[", "token", "]", "=", "Counter", "(", "counter", ")", "\n", "\n", "", "spacy_lemmatizer", "=", "get_spacy_lemmatizer", "(", ")", "\n", "\n", "possible_predicates", "=", "defaultdict", "(", "lambda", ":", "Counter", "(", ")", ")", "\n", "for", "index", ",", "sentence_actions", "in", "tqdm", "(", "enumerate", "(", "actions", ")", ",", "desc", "=", "'merge rules'", ")", ":", "\n", "\n", "        ", "tokens", "=", "sentences", "[", "index", "]", "\n", "\n", "# Initialize machine", "\n", "state_machine", "=", "AMRStateMachine", "(", "\n", "tokens", ",", "\n", "actions_by_stack_rules", "=", "actions_by_stack_rules", ",", "\n", "spacy_lemmatizer", "=", "spacy_lemmatizer", ",", "\n", "entity_rules", "=", "entity_rules", "\n", ")", "\n", "\n", "for", "action", "in", "sentence_actions", ":", "\n", "# NOTE: At the oracle, possible predicates are collected before", "\n", "# PRED/COPY decision (tryConfirm action) we have to take all of", "\n", "# them into account", "\n", "            ", "position", ",", "mpositions", "=", "state_machine", ".", "get_top_of_stack", "(", "positions", "=", "True", ")", "\n", "if", "action", ".", "startswith", "(", "'PRED'", ")", ":", "\n", "                ", "node", "=", "action", "[", "5", ":", "-", "1", "]", "\n", "possible_predicates", "[", "tokens", "[", "position", "]", "]", ".", "update", "(", "[", "node", "]", ")", "\n", "if", "mpositions", ":", "\n", "                    ", "mtokens", "=", "','", ".", "join", "(", "[", "tokens", "[", "p", "]", "for", "p", "in", "mpositions", "]", ")", "\n", "possible_predicates", "[", "mtokens", "]", ".", "update", "(", "[", "node", "]", ")", "\n", "\n", "", "", "elif", "action", "==", "'COPY_LEMMA'", ":", "\n", "                ", "lemma", ",", "_", "=", "state_machine", ".", "get_top_of_stack", "(", "lemma", "=", "True", ")", "\n", "node", "=", "lemma", "\n", "possible_predicates", "[", "tokens", "[", "position", "]", "]", ".", "update", "(", "[", "node", "]", ")", "\n", "if", "mpositions", ":", "\n", "                    ", "mtokens", "=", "','", ".", "join", "(", "[", "tokens", "[", "p", "]", "for", "p", "in", "mpositions", "]", ")", "\n", "possible_predicates", "[", "mtokens", "]", ".", "update", "(", "[", "node", "]", ")", "\n", "\n", "", "", "elif", "action", "==", "'COPY_SENSE01'", ":", "\n", "                ", "lemma", ",", "_", "=", "state_machine", ".", "get_top_of_stack", "(", "lemma", "=", "True", ")", "\n", "node", "=", "f'{lemma}-01'", "\n", "possible_predicates", "[", "tokens", "[", "position", "]", "]", ".", "update", "(", "[", "node", "]", ")", "\n", "if", "mpositions", ":", "\n", "                    ", "mtokens", "=", "','", ".", "join", "(", "[", "tokens", "[", "p", "]", "for", "p", "in", "mpositions", "]", ")", "\n", "possible_predicates", "[", "mtokens", "]", ".", "update", "(", "[", "node", "]", ")", "\n", "\n", "# execute action", "\n", "", "", "state_machine", ".", "applyAction", "(", "action", ")", "\n", "\n", "", "", "out_rule_stats", "=", "rule_stats", "\n", "new_possible_predicates", "=", "merge_both_rules", "(", "possible_predicates", ",", "actions_by_stack_rules", ")", "\n", "out_rule_stats", "[", "'possible_predicates'", "]", "=", "new_possible_predicates", "\n", "\n", "return", "out_rule_stats", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.edit.merge_both_rules": [[207, 232], ["list", "list", "list", "dict", "copy.deepcopy", "old_action_rules.keys", "new_action_rules.keys", "set", "set", "copy.deepcopy", "copy.deepcopy", "list", "list", "copy.deepcopy", "copy.deepcopy", "new_action_rules[].keys", "new_action_rules[].keys", "old_action_rules[].keys"], "function", ["None"], ["", "def", "merge_both_rules", "(", "new_action_rules", ",", "old_action_rules", ")", ":", "\n", "    ", "keys_old_rules", "=", "list", "(", "old_action_rules", ".", "keys", "(", ")", ")", "\n", "keys_new_rules", "=", "list", "(", "new_action_rules", ".", "keys", "(", ")", ")", "\n", "\n", "keys_common", "=", "list", "(", "set", "(", "keys_old_rules", ")", "&", "set", "(", "keys_new_rules", ")", ")", "\n", "merged_action_rules", "=", "dict", "(", ")", "\n", "for", "key", "in", "keys_old_rules", ":", "\n", "        ", "if", "key", "not", "in", "keys_common", ":", "\n", "            ", "merged_action_rules", "[", "key", "]", "=", "deepcopy", "(", "old_action_rules", "[", "key", "]", ")", "\n", "\n", "", "", "for", "key", "in", "keys_new_rules", ":", "\n", "        ", "if", "key", "not", "in", "keys_common", ":", "\n", "            ", "merged_action_rules", "[", "key", "]", "=", "deepcopy", "(", "new_action_rules", "[", "key", "]", ")", "\n", "\n", "", "", "for", "key", "in", "keys_common", ":", "\n", "        ", "if", "list", "(", "new_action_rules", "[", "key", "]", ".", "keys", "(", ")", ")", "==", "list", "(", "old_action_rules", "[", "key", "]", ".", "keys", "(", ")", ")", ":", "\n", "            ", "merged_action_rules", "[", "key", "]", "=", "deepcopy", "(", "old_action_rules", "[", "key", "]", ")", "\n", "", "else", ":", "\n", "            ", "merged_action_rules", "[", "key", "]", "=", "deepcopy", "(", "old_action_rules", "[", "key", "]", ")", "\n", "for", "predicate", "in", "new_action_rules", "[", "key", "]", ".", "keys", "(", ")", ":", "\n", "                ", "if", "predicate", "not", "in", "merged_action_rules", "[", "key", "]", ":", "\n", "                    ", "merged_action_rules", "[", "key", "]", "[", "predicate", "]", "=", "new_action_rules", "[", "key", "]", "[", "predicate", "]", "\n", "\n", "\n", "", "", "", "", "return", "deepcopy", "(", "merged_action_rules", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.edit.main": [[233, 305], ["edit.argument_parser", "transition_amr_parser.io.read_amr", "transition_amr_parser.io.read_tokenized_sentences", "transition_amr_parser.io.read_tokenized_sentences", "transition_amr_parser.io.read_action_scores", "edit.print_score_action_stats", "transition_amr_parser.io.read_rule_stats", "print", "edit.merge_actions", "edit.fix_actions_split_by_spaces", "edit.merge_rules", "print", "os.path.dirname", "transition_amr_parser.io.write_tokenized_sentences", "print", "transition_amr_parser.io.write_rule_stats", "print", "os.makedirs", "open", "len", "len", "fid.write", "amr.toJAMRString"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.argument_parser", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.read_amr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.io.read_tokenized_sentences", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.io.read_tokenized_sentences", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.io.read_action_scores", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.edit.print_score_action_stats", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.io.read_rule_stats", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.edit.merge_actions", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.edit.fix_actions_split_by_spaces", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.edit.merge_rules", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.io.write_tokenized_sentences", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.io.write_rule_stats", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.toJAMRString"], ["", "def", "main", "(", ")", ":", "\n", "\n", "# Argument handling", "\n", "    ", "args", "=", "argument_parser", "(", ")", "\n", "\n", "# Read", "\n", "# Load AMR (replace some unicode characters)", "\n", "if", "args", ".", "in_amr", ":", "\n", "        ", "corpus", "=", "read_amr", "(", "args", ".", "in_amr", ",", "unicode_fixes", "=", "True", ")", "\n", "amrs", "=", "corpus", ".", "amrs", "\n", "# Load tokens    ", "\n", "", "if", "args", ".", "in_tokens", ":", "\n", "        ", "sentences", "=", "read_tokenized_sentences", "(", "args", ".", "in_tokens", ",", "separator", "=", "'\\t'", ")", "\n", "# Load actions i.e. oracle", "\n", "", "if", "args", ".", "in_actions", ":", "\n", "        ", "actions", "=", "read_tokenized_sentences", "(", "args", ".", "in_actions", ",", "separator", "=", "'\\t'", ")", "\n", "# Load scored actions i.e. mined oracle     ", "\n", "", "if", "args", ".", "in_scored_actions", ":", "\n", "        ", "scored_actions", "=", "read_action_scores", "(", "args", ".", "in_scored_actions", ")", "\n", "# measure performance", "\n", "print_score_action_stats", "(", "scored_actions", ")", "\n", "# Load rule stats", "\n", "", "if", "args", ".", "in_rule_stats", ":", "\n", "        ", "rule_stats", "=", "read_rule_stats", "(", "args", ".", "in_rule_stats", ")", "\n", "\n", "# Modify", "\n", "# merge --in-actions and --in-scored-actions and store in --out-actions", "\n", "", "if", "args", ".", "merge_mined", ":", "\n", "# sanity checks", "\n", "        ", "assert", "args", ".", "in_tokens", ",", "\"--merge-mined requires --in-tokens\"", "\n", "assert", "args", ".", "in_actions", ",", "\"--merge-mined requires --in-actions\"", "\n", "assert", "args", ".", "in_rule_stats", ",", "\"--merge-mined requires --in-rule-stats\"", "\n", "assert", "args", ".", "out_rule_stats", ",", "\"--merge-mined requires --out-rule-stats\"", "\n", "if", "args", ".", "in_actions", ":", "\n", "            ", "assert", "len", "(", "actions", ")", "==", "len", "(", "scored_actions", ")", "\n", "", "print", "(", "f'Merging {args.out_actions} and {args.in_scored_actions}'", ")", "\n", "\n", "# actions", "\n", "actions", "=", "merge_actions", "(", "actions", ",", "scored_actions", ")", "\n", "\n", "# fix actions split by whitespace arguments ", "\n", "", "if", "args", ".", "fix_actions", ":", "\n", "        ", "actions", "=", "fix_actions_split_by_spaces", "(", "actions", ")", "\n", "\n", "# merge rules", "\n", "", "if", "args", ".", "merge_mined", ":", "\n", "        ", "out_rule_stats", "=", "merge_rules", "(", "sentences", ",", "actions", ",", "rule_stats", ",", "entity_rules", "=", "args", ".", "entity_rules", ")", "\n", "print", "(", "f'Merging {args.out_rule_stats} and {args.in_rule_stats}'", ")", "\n", "\n", "# Write", "\n", "# actions", "\n", "", "if", "args", ".", "out_actions", ":", "\n", "        ", "dirname", "=", "os", ".", "path", ".", "dirname", "(", "args", ".", "out_actions", ")", "\n", "if", "dirname", ":", "\n", "            ", "os", ".", "makedirs", "(", "dirname", ",", "exist_ok", "=", "True", ")", "\n", "", "write_tokenized_sentences", "(", "\n", "args", ".", "out_actions", ",", "\n", "actions", ",", "\n", "separator", "=", "'\\t'", "\n", ")", "\n", "print", "(", "f'Wrote {args.out_actions}'", ")", "\n", "\n", "# rule stats", "\n", "", "if", "args", ".", "out_rule_stats", ":", "\n", "        ", "write_rule_stats", "(", "args", ".", "out_rule_stats", ",", "out_rule_stats", ")", "\n", "print", "(", "f'Wrote {args.out_rule_stats}'", ")", "\n", "\n", "# AMR", "\n", "", "if", "args", ".", "out_amr", ":", "\n", "        ", "with", "open", "(", "args", ".", "out_amr", ",", "'w'", ")", "as", "fid", ":", "\n", "            ", "for", "amr", "in", "amrs", ":", "\n", "                ", "fid", ".", "write", "(", "amr", ".", "toJAMRString", "(", ")", ")", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.__init__": [[2, 12], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "cell", ",", "initial_state", ",", "dropout", ",", "activation", ",", "empty_embedding", "=", "None", ")", ":", "\n", "        ", "self", ".", "cell", "=", "cell", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "states", "=", "[", "initial_state", "]", "\n", "self", ".", "embeddings", "=", "[", "None", "]", "\n", "self", ".", "strings", "=", "[", "None", "]", "\n", "self", ".", "empty", "=", "None", "\n", "self", ".", "activation", "=", "activation", "\n", "if", "empty_embedding", "is", "not", "None", ":", "\n", "            ", "self", ".", "empty", "=", "empty_embedding", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.push": [[13, 19], ["stack_lstm.StackRNN.cell", "stack_lstm.StackRNN.states.append", "stack_lstm.StackRNN.embeddings.append", "stack_lstm.StackRNN.strings.append", "stack_lstm.StackRNN.dropout"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "", "def", "push", "(", "self", ",", "embedding", ",", "string", ")", ":", "\n", "        ", "h", ",", "c", "=", "self", ".", "states", "[", "-", "1", "]", "\n", "output", "=", "self", ".", "cell", "(", "embedding", ",", "(", "self", ".", "dropout", "(", "h", ")", ",", "c", ")", ")", "\n", "self", ".", "states", ".", "append", "(", "output", ")", "\n", "self", ".", "embeddings", ".", "append", "(", "embedding", ")", "\n", "self", ".", "strings", ".", "append", "(", "string", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop": [[20, 23], ["stack_lstm.StackRNN.states.pop", "stack_lstm.StackRNN.embeddings.pop", "stack_lstm.StackRNN.strings.pop"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop"], ["", "def", "pop", "(", "self", ")", ":", "\n", "        ", "self", ".", "states", ".", "pop", "(", ")", "\n", "return", "(", "self", ".", "embeddings", ".", "pop", "(", ")", ",", "self", ".", "strings", ".", "pop", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.last": [[24, 26], ["None"], "methods", ["None"], ["", "def", "last", "(", "self", ")", ":", "\n", "        ", "return", "(", "self", ".", "embeddings", "[", "-", "1", "]", ",", "self", ".", "strings", "[", "-", "1", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.output": [[27, 29], ["stack_lstm.StackRNN.activation", "len"], "methods", ["None"], ["", "def", "output", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "activation", "(", "self", ".", "states", "[", "-", "1", "]", "[", "0", "]", ")", "if", "len", "(", "self", ".", "states", ")", ">", "1", "else", "self", ".", "empty", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.clear": [[30, 35], ["len", "stack_lstm.StackRNN.states.pop", "stack_lstm.StackRNN.embeddings.pop", "stack_lstm.StackRNN.strings.pop"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop"], ["", "def", "clear", "(", "self", ")", ":", "\n", "        ", "while", "len", "(", "self", ".", "states", ")", ">", "1", ":", "\n", "            ", "self", ".", "states", ".", "pop", "(", ")", "\n", "self", ".", "embeddings", ".", "pop", "(", ")", "\n", "self", ".", "strings", ".", "pop", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.__len__": [[36, 38], ["len"], "methods", ["None"], ["", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "states", ")", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.__str__": [[39, 41], ["str"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "'<StackRNN>: '", "+", "' '", ".", "join", "(", "str", "(", "t", ")", "for", "t", "in", "self", ".", "strings", "[", "1", ":", "]", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.AMR_Oracle.__init__": [[354, 373], ["collections.Counter"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "entity_rules", ",", "verbose", "=", "False", ")", ":", "\n", "        ", "self", ".", "amrs", "=", "[", "]", "\n", "self", ".", "gold_amrs", "=", "[", "]", "\n", "self", ".", "transitions", "=", "[", "]", "\n", "self", ".", "verbose", "=", "verbose", "\n", "\n", "# predicates", "\n", "self", ".", "preds2Ints", "=", "{", "}", "\n", "self", ".", "possiblePredicates", "=", "{", "}", "\n", "\n", "self", ".", "new_edge", "=", "''", "\n", "self", ".", "new_node", "=", "''", "\n", "self", ".", "entity_type", "=", "''", "\n", "self", ".", "dep_id", "=", "None", "\n", "\n", "self", ".", "swapped_words", "=", "{", "}", "\n", "\n", "self", ".", "possibleEntityTypes", "=", "Counter", "(", ")", "\n", "self", ".", "entity_rules", "=", "entity_rules", "\n", "# DEBUG", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.AMR_Oracle.read_actions": [[376, 393], ["f.read.replace", "f.read.split", "open", "f.read", "sent.split", "s[].split", "s[].split", "transitions.append", "transitions[].applyActions", "sent.strip", "len", "IOError", "transition_amr_parser.state_machine.AMRStateMachine"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.applyActions"], ["", "def", "read_actions", "(", "self", ",", "actions_file", ")", ":", "\n", "        ", "transitions", "=", "[", "]", "\n", "with", "open", "(", "actions_file", ",", "'r'", ",", "encoding", "=", "'utf8'", ")", "as", "f", ":", "\n", "            ", "sentences", "=", "f", ".", "read", "(", ")", "\n", "", "sentences", "=", "sentences", ".", "replace", "(", "'\\r'", ",", "''", ")", "\n", "sentences", "=", "sentences", ".", "split", "(", "'\\n\\n'", ")", "\n", "for", "sent", "in", "sentences", ":", "\n", "            ", "if", "not", "sent", ".", "strip", "(", ")", ":", "\n", "                ", "continue", "\n", "", "s", "=", "sent", ".", "split", "(", "'\\n'", ")", "\n", "if", "len", "(", "s", ")", "<", "2", ":", "\n", "                ", "raise", "IOError", "(", "f'Action file formatted incorrectly: {sent}'", ")", "\n", "", "tokens", "=", "s", "[", "0", "]", ".", "split", "(", "'\\t'", ")", "\n", "actions", "=", "s", "[", "1", "]", ".", "split", "(", "'\\t'", ")", "\n", "transitions", ".", "append", "(", "AMRStateMachine", "(", "tokens", ",", "entity_rules", "=", "self", ".", "entity_rules", ")", ")", "\n", "transitions", "[", "-", "1", "]", ".", "applyActions", "(", "actions", ")", "\n", "", "self", ".", "transitions", "=", "transitions", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.AMR_Oracle.runOracle": [[394, 674], ["transition_amr_parser.utils.print_log", "transition_amr_parser.io.writer", "transition_amr_parser.io.writer", "transition_amr_parser.io.writer", "transition_amr_parser.io.writer", "tqdm.tqdm.tqdm", "transition_amr_parser.utils.print_log", "transition_amr_parser.io.writer.", "transition_amr_parser.io.writer.", "transition_amr_parser.io.writer.", "transition_amr_parser.io.writer.", "collections.Counter", "preprocess_amr.copy", "collections.Counter", "collections.Counter", "collections.Counter", "transition_amr_parser.state_machine.get_spacy_lemmatizer", "enumerate", "data_oracle.preprocess_amr", "transition_amr_parser.state_machine.AMRStateMachine", "data_oracle.AMR_Oracle.transitions.append", "data_oracle.AMR_Oracle.amrs.append", "transition_amr_parser.state_machine.AMRStateMachine.CLOSE", "transition_amr_parser.io.writer.", "data_oracle.AMR_Oracle.stats[].update", "data_oracle.AMR_Oracle.stats[].update", "sep.join", "sep.join", "transition_amr_parser.io.writer.", "transition_amr_parser.io.writer.", "len", "len", "print", "collections.Counter", "print", "print", "collections.Counter", "print", "print", "len", "print", "len", "print", "data_oracle.AMR_Oracle.tryMerge", "transition_amr_parser.state_machine.AMRStateMachine.get_top_of_stack", "transition_amr_parser.state_machine.AMRStateMachine.get_valid_actions", "transition_amr_parser.state_machine.AMRStateMachine.applyAction", "transition_amr_parser.io.writer.", "transition_amr_parser.state_machine.AMRStateMachine.amr.toJAMRString", "data_oracle.AMR_Oracle.action2idx.setdefault", "data_oracle.AMR_Oracle.pred2idx.setdefault", "data_oracle.AMR_Oracle.labelsO2idx.setdefault", "data_oracle.AMR_Oracle.labelsA2idx.setdefault", "data_oracle.AMR_Oracle.word2idx.setdefault", "data_oracle.AMR_Oracle.node2idx.setdefault", "open", "fid.write", "data_oracle.yellow_font", "data_oracle.yellow_font", "data_oracle.yellow_font", "data_oracle.AMR_Oracle.tryEntity", "action.replace.replace.split", "actions_not_in_whitelist.append", "actions_in_blacklist.append", "data_oracle.label_shift", "dangling_nodes.append", "str", "a.startswith", "transition_amr_parser.state_machine.AMRStateMachine.readAction", "len", "len", "len", "len", "len", "data_oracle.AMR_Oracle.char2idx.setdefault", "len", "data_oracle.AMR_Oracle.stats[].items", "json.dumps", "data_oracle.AMR_Oracle.tryDependent", "action.replace.replace.replace", "len", "a[].split", "a[].split", "str", "data_oracle.AMR_Oracle.tryConfirm", "data_oracle.AMR_Oracle.new_edge.startswith", "data_oracle.AMR_Oracle.tryIntroduce", "transition_amr_parser.state_machine.AMRStateMachine.get_top_of_stack", "data_oracle.AMR_Oracle.tryLA", "data_oracle.AMR_Oracle.tryRA", "data_oracle.AMR_Oracle.tryReduce", "data_oracle.AMR_Oracle.trySWAP"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.io.writer", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.io.writer", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.io.writer", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.io.writer", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.copy", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.get_spacy_lemmatizer", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.preprocess_amr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.CLOSE", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.AMR_Oracle.tryMerge", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.get_top_of_stack", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.get_valid_actions", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.DepParsingStateMachine.applyAction", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.toJAMRString", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.yellow_font", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.yellow_font", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.yellow_font", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.AMR_Oracle.tryEntity", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.label_shift", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.readAction", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.AMR_Oracle.tryDependent", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.AMR_Oracle.tryConfirm", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.AMR_Oracle.tryIntroduce", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.get_top_of_stack", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.AMR_Oracle.tryLA", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.AMR_Oracle.tryRA", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.AMR_Oracle.tryReduce", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.AMR_Oracle.trySWAP"], ["", "def", "runOracle", "(", "self", ",", "gold_amrs", ",", "propbank_args", "=", "None", ",", "out_oracle", "=", "None", ",", "\n", "out_amr", "=", "None", ",", "out_sentences", "=", "None", ",", "out_actions", "=", "None", ",", "\n", "out_rule_stats", "=", "None", ",", "add_unaligned", "=", "0", ",", "\n", "no_whitespace_in_actions", "=", "False", ",", "multitask_words", "=", "None", ",", "\n", "copy_lemma_action", "=", "False", ",", "addnode_count_cutoff", "=", "None", ")", ":", "\n", "\n", "        ", "print_log", "(", "\"oracle\"", ",", "\"Parsing data\"", ")", "\n", "# deep copy of gold AMRs", "\n", "self", ".", "gold_amrs", "=", "[", "gold_amr", ".", "copy", "(", ")", "for", "gold_amr", "in", "gold_amrs", "]", "\n", "\n", "\n", "# open all files (if paths provided) and get writers to them", "\n", "oracle_write", "=", "writer", "(", "out_oracle", ")", "\n", "amr_write", "=", "writer", "(", "out_amr", ")", "\n", "sentence_write", "=", "writer", "(", "out_sentences", ",", "add_return", "=", "True", ")", "\n", "actions_write", "=", "writer", "(", "out_actions", ",", "add_return", "=", "True", ")", "\n", "\n", "# This will store overall stats", "\n", "self", ".", "stats", "=", "{", "\n", "'possible_predicates'", ":", "Counter", "(", ")", ",", "\n", "'action_vocabulary'", ":", "Counter", "(", ")", ",", "\n", "'addnode_counts'", ":", "Counter", "(", ")", "\n", "}", "\n", "\n", "# unaligned tokens", "\n", "included_unaligned", "=", "[", "\n", "'-'", ",", "'and'", ",", "'multi-sentence'", ",", "'person'", ",", "'cause-01'", ",", "'you'", ",", "'more'", ",", "\n", "'imperative'", ",", "'1'", ",", "'thing'", ",", "\n", "]", "\n", "\n", "# initialize spacy lemmatizer out of the sentence loop for speed", "\n", "spacy_lemmatizer", "=", "None", "\n", "if", "copy_lemma_action", ":", "\n", "            ", "spacy_lemmatizer", "=", "get_spacy_lemmatizer", "(", ")", "\n", "\n", "# Store invalid actions", "\n", "", "actions_not_in_whitelist", "=", "[", "]", "\n", "actions_in_blacklist", "=", "[", "]", "\n", "dangling_nodes", "=", "[", "]", "\n", "\n", "# Loop over golf AMRs", "\n", "for", "sent_idx", ",", "gold_amr", "in", "tqdm", "(", "\n", "enumerate", "(", "self", ".", "gold_amrs", ")", ",", "\n", "desc", "=", "f'computing oracle'", ",", "\n", "total", "=", "len", "(", "self", ".", "gold_amrs", ")", "\n", ")", ":", "\n", "\n", "            ", "if", "self", ".", "verbose", ":", "\n", "                ", "print", "(", "\"New Sentence \"", "+", "str", "(", "sent_idx", ")", "+", "\"\\n\\n\\n\"", ")", "\n", "\n", "# TODO: Describe what is this pre-processing", "\n", "", "gold_amr", "=", "preprocess_amr", "(", "gold_amr", ",", "add_unaligned", ",", "included_unaligned", ")", "\n", "\n", "# Initialize state machine", "\n", "tr", "=", "AMRStateMachine", "(", "\n", "gold_amr", ".", "tokens", ",", "\n", "verbose", "=", "self", ".", "verbose", ",", "\n", "add_unaligned", "=", "add_unaligned", ",", "\n", "spacy_lemmatizer", "=", "spacy_lemmatizer", ",", "\n", "entity_rules", "=", "self", ".", "entity_rules", "\n", ")", "\n", "self", ".", "transitions", ".", "append", "(", "tr", ")", "\n", "self", ".", "amrs", ".", "append", "(", "tr", ".", "amr", ")", "\n", "\n", "# Loop over potential actions", "\n", "while", "tr", ".", "buffer", "or", "tr", ".", "stack", ":", "\n", "\n", "                ", "if", "self", ".", "tryMerge", "(", "tr", ",", "tr", ".", "amr", ",", "gold_amr", ")", ":", "\n", "                    ", "action", "=", "'MERGE'", "\n", "\n", "", "elif", "self", ".", "tryEntity", "(", "tr", ",", "tr", ".", "amr", ",", "gold_amr", ")", ":", "\n", "                    ", "action", "=", "f'ADDNODE({self.entity_type})'", "\n", "\n", "", "elif", "self", ".", "tryDependent", "(", "tr", ",", "tr", ".", "amr", ",", "gold_amr", ")", ":", "\n", "                    ", "edge", "=", "self", ".", "new_edge", "[", "1", ":", "]", "if", "self", ".", "new_edge", ".", "startswith", "(", "':'", ")", "else", "self", ".", "new_edge", "\n", "action", "=", "f'DEPENDENT({self.new_node},{edge})'", "\n", "self", ".", "dep_id", "=", "None", "\n", "\n", "", "elif", "self", ".", "tryConfirm", "(", "tr", ",", "tr", ".", "amr", ",", "gold_amr", ")", ":", "\n", "# if --copy-lemma-action check if lemma or first sense", "\n", "# equal node name. Use corresponding action", "\n", "                    ", "if", "copy_lemma_action", ":", "\n", "                        ", "lemma", ",", "_", "=", "tr", ".", "get_top_of_stack", "(", "lemma", "=", "True", ")", "\n", "if", "copy_lemma_action", "and", "lemma", "==", "self", ".", "new_node", ":", "\n", "                            ", "action", "=", "'COPY_LEMMA'", "\n", "", "elif", "copy_lemma_action", "and", "f'{lemma}-01'", "==", "self", ".", "new_node", ":", "\n", "                            ", "action", "=", "'COPY_SENSE01'", "\n", "", "else", ":", "\n", "                            ", "action", "=", "f'PRED({self.new_node})'", "\n", "", "", "else", ":", "\n", "                        ", "action", "=", "f'PRED({self.new_node})'", "\n", "\n", "", "", "elif", "self", ".", "tryIntroduce", "(", "tr", ",", "tr", ".", "amr", ",", "gold_amr", ")", ":", "\n", "                    ", "action", "=", "'INTRODUCE'", "\n", "\n", "", "elif", "self", ".", "tryLA", "(", "tr", ",", "tr", ".", "amr", ",", "gold_amr", ")", ":", "\n", "                    ", "if", "self", ".", "new_edge", "==", "'root'", ":", "\n", "                        ", "action", "=", "f'LA({self.new_edge})'", "\n", "", "else", ":", "\n", "                        ", "action", "=", "f'LA({self.new_edge[1:]})'", "\n", "\n", "", "", "elif", "self", ".", "tryRA", "(", "tr", ",", "tr", ".", "amr", ",", "gold_amr", ")", ":", "\n", "                    ", "if", "self", ".", "new_edge", "==", "'root'", ":", "\n", "                        ", "action", "=", "f'RA({self.new_edge})'", "\n", "", "else", ":", "\n", "                        ", "action", "=", "f'RA({self.new_edge[1:]})'", "\n", "\n", "", "", "elif", "self", ".", "tryReduce", "(", "tr", ",", "tr", ".", "amr", ",", "gold_amr", ")", ":", "\n", "                    ", "action", "=", "'REDUCE'", "\n", "\n", "", "elif", "self", ".", "trySWAP", "(", "tr", ",", "tr", ".", "amr", ",", "gold_amr", ")", ":", "\n", "                    ", "action", "=", "'UNSHIFT'", "\n", "\n", "", "elif", "tr", ".", "buffer", ":", "\n", "                    ", "action", "=", "'SHIFT'", "\n", "\n", "", "else", ":", "\n", "                    ", "tr", ".", "stack", "=", "[", "]", "\n", "tr", ".", "buffer", "=", "[", "]", "\n", "break", "\n", "\n", "# Store stats", "\n", "# get token(s) at the top of the stack", "\n", "", "token", ",", "merged_tokens", "=", "tr", ".", "get_top_of_stack", "(", ")", "\n", "action_label", "=", "action", ".", "split", "(", "'('", ")", "[", "0", "]", "\n", "\n", "# invalid actions", "\n", "valid", ",", "not_valid", "=", "tr", ".", "get_valid_actions", "(", ")", "\n", "if", "action", "not", "in", "valid", "and", "action_label", "not", "in", "valid", ":", "\n", "                    ", "actions_not_in_whitelist", ".", "append", "(", "(", "sent_idx", ",", "token", ",", "action", ")", ")", "\n", "", "if", "action", "in", "not_valid", ":", "\n", "                    ", "actions_in_blacklist", ".", "append", "(", "(", "sent_idx", ",", "token", ",", "action", ")", ")", "\n", "\n", "# check action has not invalid chars and normalize", "\n", "# TODO: --no-whitespace-in-actions being deprecated", "\n", "", "if", "no_whitespace_in_actions", "and", "action_label", "==", "'PRED'", ":", "\n", "                    ", "assert", "'_'", "not", "in", "action", ",", "\"--no-whitespace-in-actions prohibits use of _ in actions\"", "\n", "if", "' '", "in", "action_label", ":", "\n", "                        ", "action", "=", "action", ".", "replace", "(", "' '", ",", "'_'", ")", "\n", "\n", "# Add prediction ot top of the buffer", "\n", "", "", "if", "action", "==", "'SHIFT'", "and", "multitask_words", "is", "not", "None", ":", "\n", "                    ", "action", "=", "label_shift", "(", "tr", ",", "multitask_words", ")", "\n", "\n", "# APPLY ACTION", "\n", "", "tr", ".", "applyAction", "(", "action", ")", "\n", "\n", "# Close machine", "\n", "", "tr", ".", "CLOSE", "(", "\n", "training", "=", "True", ",", "\n", "gold_amr", "=", "gold_amr", ",", "\n", "use_addnonde_rules", "=", "use_addnode_rules", "\n", ")", "\n", "\n", "# store dangling nodes", "\n", "for", "e", "in", "tr", ".", "amr", ".", "edges", ":", "\n", "                ", "if", "e", "[", "1", "]", "==", "':rel'", ":", "\n", "                    ", "de", "=", "f'{e[1]} {tr.amr.nodes[e[2]]}'", "\n", "dangling_nodes", ".", "append", "(", "(", "sent_idx", ",", "de", ")", ")", "\n", "\n", "# update files", "\n", "", "", "if", "out_oracle", ":", "\n", "# to avoid printing", "\n", "                ", "oracle_write", "(", "str", "(", "tr", ")", ")", "\n", "# JAMR format AMR", "\n", "", "amr_write", "(", "tr", ".", "amr", ".", "toJAMRString", "(", ")", ")", "\n", "# Tokens and actions", "\n", "# extra tag to be reduced at start ", "\n", "tokens", "=", "tr", ".", "amr", ".", "tokens", "\n", "actions", "=", "tr", ".", "actions", "\n", "\n", "# Update action count", "\n", "self", ".", "stats", "[", "'action_vocabulary'", "]", ".", "update", "(", "actions", ")", "\n", "del", "gold_amr", ".", "nodes", "[", "-", "1", "]", "\n", "addnode_actions", "=", "[", "a", "for", "a", "in", "actions", "if", "a", ".", "startswith", "(", "'ADDNODE'", ")", "]", "\n", "self", ".", "stats", "[", "'addnode_counts'", "]", ".", "update", "(", "addnode_actions", ")", "\n", "\n", "# separator", "\n", "if", "no_whitespace_in_actions", ":", "\n", "                ", "sep", "=", "\" \"", "\n", "", "else", ":", "\n", "                ", "sep", "=", "\"\\t\"", "\n", "", "tokens", "=", "sep", ".", "join", "(", "tokens", ")", "\n", "actions", "=", "sep", ".", "join", "(", "actions", ")", "\n", "# Write", "\n", "sentence_write", "(", "tokens", ")", "\n", "actions_write", "(", "actions", ")", "\n", "\n", "", "print_log", "(", "\"oracle\"", ",", "\"Done\"", ")", "\n", "\n", "# close files if open", "\n", "oracle_write", "(", ")", "\n", "amr_write", "(", ")", "\n", "sentence_write", "(", ")", "\n", "actions_write", "(", ")", "\n", "\n", "self", ".", "labelsO2idx", "=", "{", "'<pad>'", ":", "0", "}", "\n", "self", ".", "labelsA2idx", "=", "{", "'<pad>'", ":", "0", "}", "\n", "self", ".", "pred2idx", "=", "{", "'<pad>'", ":", "0", "}", "\n", "self", ".", "action2idx", "=", "{", "'<pad>'", ":", "0", "}", "\n", "\n", "for", "tr", "in", "self", ".", "transitions", ":", "\n", "            ", "for", "a", "in", "tr", ".", "actions", ":", "\n", "                ", "a", "=", "AMRStateMachine", ".", "readAction", "(", "a", ")", "[", "0", "]", "\n", "self", ".", "action2idx", ".", "setdefault", "(", "a", ",", "len", "(", "self", ".", "action2idx", ")", ")", "\n", "", "for", "p", "in", "tr", ".", "predicates", ":", "\n", "                ", "self", ".", "pred2idx", ".", "setdefault", "(", "p", ",", "len", "(", "self", ".", "pred2idx", ")", ")", "\n", "", "for", "l", "in", "tr", ".", "labels", ":", "\n", "                ", "self", ".", "labelsO2idx", ".", "setdefault", "(", "l", ",", "len", "(", "self", ".", "labelsO2idx", ")", ")", "\n", "", "for", "l", "in", "tr", ".", "labelsA", ":", "\n", "                ", "self", ".", "labelsA2idx", ".", "setdefault", "(", "l", ",", "len", "(", "self", ".", "labelsA2idx", ")", ")", "\n", "\n", "", "", "self", ".", "stats", "[", "\"action2idx\"", "]", "=", "self", ".", "action2idx", "\n", "self", ".", "stats", "[", "\"pred2idx\"", "]", "=", "self", ".", "pred2idx", "\n", "self", ".", "stats", "[", "\"labelsO2idx\"", "]", "=", "self", ".", "labelsO2idx", "\n", "self", ".", "stats", "[", "\"labelsA2idx\"", "]", "=", "self", ".", "labelsA2idx", "\n", "\n", "# Compute the word dictionary", "\n", "\n", "self", ".", "char2idx", "=", "{", "'<unk>'", ":", "0", "}", "\n", "self", ".", "word2idx", "=", "{", "'<unk>'", ":", "0", ",", "'<eof>'", ":", "1", ",", "'<ROOT>'", ":", "2", ",", "'<unaligned>'", ":", "3", "}", "\n", "self", ".", "node2idx", "=", "{", "}", "\n", "self", ".", "word_counter", "=", "Counter", "(", ")", "\n", "\n", "for", "amr", "in", "self", ".", "gold_amrs", ":", "\n", "            ", "for", "tok", "in", "amr", ".", "tokens", ":", "\n", "                ", "self", ".", "word_counter", "[", "tok", "]", "+=", "1", "\n", "self", ".", "word2idx", ".", "setdefault", "(", "tok", ",", "len", "(", "self", ".", "word2idx", ")", ")", "\n", "for", "ch", "in", "tok", ":", "\n", "                    ", "self", ".", "char2idx", ".", "setdefault", "(", "ch", ",", "len", "(", "self", ".", "char2idx", ")", ")", "\n", "", "", "for", "n", "in", "amr", ".", "nodes", ":", "\n", "                ", "self", ".", "node2idx", ".", "setdefault", "(", "amr", ".", "nodes", "[", "n", "]", ",", "len", "(", "self", ".", "node2idx", ")", ")", "\n", "\n", "", "", "self", ".", "stats", "[", "\"char2idx\"", "]", "=", "self", ".", "char2idx", "\n", "self", ".", "stats", "[", "\"word2idx\"", "]", "=", "self", ".", "word2idx", "\n", "self", ".", "stats", "[", "\"node2idx\"", "]", "=", "self", ".", "node2idx", "\n", "self", ".", "stats", "[", "\"word_counter\"", "]", "=", "self", ".", "word_counter", "\n", "\n", "self", ".", "stats", "[", "'possible_predicates'", "]", "=", "self", ".", "possiblePredicates", "\n", "\n", "if", "addnode_count_cutoff", ":", "\n", "            ", "self", ".", "stats", "[", "'addnode_blacklist'", "]", "=", "[", "\n", "a", "\n", "for", "a", ",", "c", "in", "self", ".", "stats", "[", "'addnode_counts'", "]", ".", "items", "(", ")", "\n", "if", "c", "<=", "addnode_count_cutoff", "\n", "]", "\n", "num_addnode_blackl", "=", "len", "(", "self", ".", "stats", "[", "'addnode_blacklist'", "]", ")", "\n", "num_addnode", "=", "len", "(", "self", ".", "stats", "[", "'addnode_counts'", "]", ")", "\n", "print", "(", "f'{num_addnode_blackl}/{num_addnode} blacklisted ADDNODES'", ")", "\n", "del", "self", ".", "stats", "[", "'addnode_counts'", "]", "\n", "\n", "# State machine stats for this senetnce", "\n", "", "if", "out_rule_stats", ":", "\n", "            ", "with", "open", "(", "out_rule_stats", ",", "'w'", ")", "as", "fid", ":", "\n", "                ", "fid", ".", "write", "(", "json", ".", "dumps", "(", "self", ".", "stats", ")", ")", "\n", "\n", "# Inform about invalid actions", "\n", "", "", "if", "actions_not_in_whitelist", ":", "\n", "            ", "fa_count", "=", "Counter", "(", "\n", "[", "a", "[", "2", "]", ".", "split", "(", "'('", ")", "[", "0", "]", "for", "a", "in", "actions_not_in_whitelist", "]", "\n", ")", "\n", "print", "(", "yellow_font", "(", "\n", "\"Not whitelisted actions used e.g. arcs for unconfirmed words\"", "\n", ")", ")", "\n", "print", "(", "fa_count", ")", "\n", "", "if", "actions_in_blacklist", ":", "\n", "            ", "fa_count", "=", "Counter", "(", "\n", "[", "a", "[", "2", "]", ".", "split", "(", "'('", ")", "[", "0", "]", "for", "a", "in", "actions_in_blacklist", "]", "\n", ")", "\n", "msg", "=", "\"Blacklisted actions used e.g. duplicated edges\"", "\n", "print", "(", "yellow_font", "(", "msg", ")", ")", "\n", "print", "(", "fa_count", ")", "\n", "\n", "# Inform about disconnected nodes ", "\n", "", "if", "dangling_nodes", ":", "\n", "            ", "num_nodes", "=", "len", "(", "dangling_nodes", ")", "\n", "message", "=", "f'There were {num_nodes} disconnected nodes (:rel)'", "\n", "print", "(", "yellow_font", "(", "message", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.AMR_Oracle.tryConfirm": [[675, 723], ["gold_amr.alignmentsToken2Node", "len", "len", "any", "len", "collections.Counter", "data_oracle.AMR_Oracle.preds2Ints.setdefault", "gold_amr.findSubGraph", "len"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.alignmentsToken2Node", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.findSubGraph"], ["", "", "def", "tryConfirm", "(", "self", ",", "transitions", ",", "amr", ",", "gold_amr", ")", ":", "\n", "        ", "\"\"\"\n        Check if the next action is CONFIRM\n\n        If the gold node label is different from the assigned label,\n        return the gold label.\n        \"\"\"", "\n", "if", "not", "transitions", ".", "stack", ":", "\n", "            ", "return", "False", "\n", "\n", "# Rules that use oracle info", "\n", "\n", "", "stack0", "=", "transitions", ".", "stack", "[", "-", "1", "]", "\n", "tok_alignment", "=", "gold_amr", ".", "alignmentsToken2Node", "(", "stack0", ")", "\n", "\n", "# TODO: What is the logic here?", "\n", "if", "len", "(", "tok_alignment", ")", "==", "0", ":", "\n", "            ", "return", "False", "\n", "", "if", "len", "(", "tok_alignment", ")", ">", "1", ":", "\n", "            ", "if", "any", "(", "gold_amr", ".", "nodes", "[", "n", "]", "==", "'name'", "for", "n", "in", "tok_alignment", ")", ":", "\n", "                ", "return", "False", "\n", "\n", "# TODO: What is the logic here?", "\n", "", "", "if", "stack0", "in", "transitions", ".", "entities", ":", "\n", "            ", "return", "False", "\n", "\n", "", "if", "len", "(", "tok_alignment", ")", "==", "1", ":", "\n", "            ", "gold_id", "=", "tok_alignment", "[", "0", "]", "\n", "", "elif", "'DEPENDENT'", "in", "transitions", ".", "actions", "[", "-", "1", "]", ":", "\n", "            ", "gold_id", "=", "gold_amr", ".", "findSubGraph", "(", "tok_alignment", ")", ".", "root", "#for DPENDENT, pred is on the root", "\n", "", "else", ":", "\n", "            ", "gold_id", "=", "tok_alignment", "[", "-", "1", "]", "#ADDNODE can have pred on leaf, assuming [-1] is the leaf", "\n", "\n", "", "isPred", "=", "stack0", "not", "in", "transitions", ".", "is_confirmed", "\n", "\n", "if", "isPred", ":", "\n", "\n", "# FIXME: state altering code should be outside of tryACTION", "\n", "            ", "new_node", "=", "gold_amr", ".", "nodes", "[", "gold_id", "]", "\n", "old_node", "=", "amr", ".", "nodes", "[", "stack0", "]", "\n", "\n", "if", "old_node", "not", "in", "self", ".", "possiblePredicates", ":", "\n", "                ", "self", ".", "possiblePredicates", "[", "old_node", "]", "=", "Counter", "(", ")", "\n", "", "if", "new_node", "not", "in", "self", ".", "preds2Ints", ":", "\n", "                ", "self", ".", "preds2Ints", ".", "setdefault", "(", "new_node", ",", "len", "(", "self", ".", "preds2Ints", ")", ")", "\n", "", "self", ".", "possiblePredicates", "[", "old_node", "]", "[", "new_node", "]", "+=", "1", "\n", "self", ".", "new_node", "=", "new_node", "\n", "", "return", "isPred", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.AMR_Oracle.tryLA": [[724, 752], ["data_oracle.AMR_Oracle.isHead", "len", "len", "data_oracle.AMR_Oracle.tryMerge"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.AMR_Oracle.isHead", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.AMR_Oracle.tryMerge"], ["", "def", "tryLA", "(", "self", ",", "transitions", ",", "amr", ",", "gold_amr", ")", ":", "\n", "        ", "\"\"\"\n        Check if the next action is LA (left arc)\n\n        If there is an unpredicted edge from stack[-1] to stack[-2]\n        return the edge label.\n        \"\"\"", "\n", "\n", "if", "len", "(", "transitions", ".", "stack", ")", "<", "2", ":", "\n", "            ", "return", "False", "\n", "\n", "# Rules that use oracle info", "\n", "\n", "# check if we should MERGE instead", "\n", "", "if", "len", "(", "transitions", ".", "buffer", ")", ">", "0", ":", "\n", "            ", "buffer0", "=", "transitions", ".", "buffer", "[", "-", "1", "]", "\n", "stack0", "=", "transitions", ".", "stack", "[", "-", "1", "]", "\n", "if", "self", ".", "tryMerge", "(", "transitions", ",", "amr", ",", "gold_amr", ",", "first", "=", "stack0", ",", "second", "=", "buffer0", ")", ":", "\n", "                ", "return", "False", "\n", "\n", "", "", "head", "=", "transitions", ".", "stack", "[", "-", "1", "]", "\n", "dependent", "=", "transitions", ".", "stack", "[", "-", "2", "]", "\n", "isLeftHead", ",", "labelL", "=", "self", ".", "isHead", "(", "amr", ",", "gold_amr", ",", "head", ",", "dependent", ")", "\n", "\n", "# FIXME: state altering code should be outside of tryACTION", "\n", "if", "isLeftHead", ":", "\n", "            ", "self", ".", "new_edge", "=", "labelL", "\n", "", "return", "isLeftHead", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.AMR_Oracle.tryRA": [[753, 781], ["data_oracle.AMR_Oracle.isHead", "len", "len", "data_oracle.AMR_Oracle.tryMerge"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.AMR_Oracle.isHead", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.AMR_Oracle.tryMerge"], ["", "def", "tryRA", "(", "self", ",", "transitions", ",", "amr", ",", "gold_amr", ")", ":", "\n", "        ", "\"\"\"\n        Check if the next action is RA (right arc)\n\n        If there is an unpredicted edge from stack[-2] to stack[-1]\n        return the edge label.\n        \"\"\"", "\n", "\n", "if", "len", "(", "transitions", ".", "stack", ")", "<", "2", ":", "\n", "            ", "return", "False", "\n", "\n", "# Rules that use oracle info", "\n", "\n", "# check if we should MERGE instead", "\n", "", "if", "len", "(", "transitions", ".", "buffer", ")", ">", "0", ":", "\n", "            ", "buffer0", "=", "transitions", ".", "buffer", "[", "-", "1", "]", "\n", "stack0", "=", "transitions", ".", "stack", "[", "-", "1", "]", "\n", "if", "self", ".", "tryMerge", "(", "transitions", ",", "amr", ",", "gold_amr", ",", "first", "=", "stack0", ",", "second", "=", "buffer0", ")", ":", "\n", "                ", "return", "False", "\n", "\n", "", "", "head", "=", "transitions", ".", "stack", "[", "-", "2", "]", "\n", "dependent", "=", "transitions", ".", "stack", "[", "-", "1", "]", "\n", "isRightHead", ",", "labelR", "=", "self", ".", "isHead", "(", "amr", ",", "gold_amr", ",", "head", ",", "dependent", ")", "\n", "\n", "# FIXME: state altering code should be outside of tryACTION", "\n", "if", "isRightHead", ":", "\n", "            ", "self", ".", "new_edge", "=", "labelR", "\n", "", "return", "isRightHead", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.AMR_Oracle.tryReduce": [[782, 845], ["gold_amr.alignmentsToken2Node", "len", "gold_amr.alignmentsToken2Node", "len", "gold_amr.findSubGraph"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.alignmentsToken2Node", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.alignmentsToken2Node", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.findSubGraph"], ["", "def", "tryReduce", "(", "self", ",", "transitions", ",", "amr", ",", "gold_amr", ",", "node_id", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Check if the next action is REDUCE\n\n        If\n        1) there is nothing aligned to a token, or\n        2) all gold edges are already predicted for the token,\n        then return True.\n        \"\"\"", "\n", "\n", "if", "not", "transitions", ".", "stack", "and", "not", "node_id", ":", "\n", "            ", "return", "False", "\n", "\n", "# Rules that use oracle info", "\n", "\n", "", "stack0", "=", "transitions", ".", "stack", "[", "-", "1", "]", "\n", "# FIXME: where is id defined?", "\n", "node_id", "=", "stack0", "if", "not", "node_id", "else", "id", "\n", "\n", "tok_alignment", "=", "gold_amr", ".", "alignmentsToken2Node", "(", "node_id", ")", "\n", "if", "len", "(", "tok_alignment", ")", "==", "0", ":", "\n", "            ", "return", "True", "\n", "\n", "# if we should merge, i.e. the alignment is the same as the next token,", "\n", "# do not reduce", "\n", "", "if", "transitions", ".", "buffer", ":", "\n", "            ", "buffer0", "=", "transitions", ".", "buffer", "[", "-", "1", "]", "\n", "buffer0_alignment", "=", "gold_amr", ".", "alignmentsToken2Node", "(", "buffer0", ")", "\n", "if", "buffer0_alignment", "==", "tok_alignment", ":", "\n", "                ", "return", "False", "\n", "\n", "", "", "if", "len", "(", "tok_alignment", ")", "==", "1", ":", "\n", "            ", "gold_id", "=", "tok_alignment", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "gold_id", "=", "gold_amr", ".", "findSubGraph", "(", "tok_alignment", ")", ".", "root", "\n", "\n", "# check if all edges are already predicted", "\n", "\n", "", "countSource", "=", "0", "\n", "countTarget", "=", "0", "\n", "countSourceGold", "=", "0", "\n", "countTargetGold", "=", "0", "\n", "for", "s", ",", "r", ",", "t", "in", "amr", ".", "edges", ":", "\n", "            ", "if", "r", "==", "'entity'", ":", "\n", "                ", "continue", "\n", "", "if", "s", "==", "node_id", ":", "\n", "                ", "countSource", "+=", "1", "\n", "", "if", "t", "==", "node_id", ":", "\n", "                ", "countTarget", "+=", "1", "\n", "", "", "for", "s", ",", "r", ",", "t", "in", "gold_amr", ".", "edges", ":", "\n", "            ", "if", "s", "==", "gold_id", ":", "\n", "                ", "countSourceGold", "+=", "1", "\n", "", "if", "t", "==", "gold_id", ":", "\n", "                ", "countTargetGold", "+=", "1", "\n", "", "", "if", "node_id", "in", "transitions", ".", "entities", ":", "\n", "            ", "for", "s", ",", "r", ",", "t", "in", "gold_amr", ".", "edges", ":", "\n", "                ", "if", "s", "==", "gold_id", "and", "t", "in", "tok_alignment", ":", "\n", "                    ", "countSource", "+=", "1", "\n", "", "if", "t", "==", "gold_id", "and", "s", "in", "tok_alignment", ":", "\n", "                    ", "countTarget", "+=", "1", "\n", "", "", "", "if", "countSourceGold", "==", "countSource", "and", "countTargetGold", "==", "countTarget", ":", "\n", "            ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.AMR_Oracle.tryMerge": [[846, 875], ["gold_amr.alignmentsToken2Node", "gold_amr.alignmentsToken2Node", "set().intersection", "set", "len", "set"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.alignmentsToken2Node", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.alignmentsToken2Node"], ["", "def", "tryMerge", "(", "self", ",", "transitions", ",", "amr", ",", "gold_amr", ",", "first", "=", "None", ",", "second", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Check if the next action is MERGE\n\n        Merge if two tokens have the same alignment.\n        \"\"\"", "\n", "\n", "# conditions", "\n", "if", "not", "first", "or", "not", "second", ":", "\n", "            ", "if", "len", "(", "transitions", ".", "stack", ")", "<", "2", ":", "\n", "                ", "return", "False", "\n", "", "first", "=", "transitions", ".", "stack", "[", "-", "1", "]", "\n", "second", "=", "transitions", ".", "stack", "[", "-", "2", "]", "\n", "", "if", "first", "==", "second", ":", "\n", "            ", "return", "False", "\n", "\n", "# Rules that use oracle info", "\n", "\n", "", "first_alignment", "=", "gold_amr", ".", "alignmentsToken2Node", "(", "first", ")", "\n", "second_alignment", "=", "gold_amr", ".", "alignmentsToken2Node", "(", "second", ")", "\n", "if", "not", "first_alignment", "or", "not", "second_alignment", ":", "\n", "            ", "return", "False", "\n", "\n", "# If both tokens aremapped to same node or overlap", "\n", "", "if", "first_alignment", "==", "second_alignment", ":", "\n", "            ", "return", "True", "\n", "", "if", "set", "(", "first_alignment", ")", ".", "intersection", "(", "set", "(", "second_alignment", ")", ")", ":", "\n", "            ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.AMR_Oracle.trySWAP": [[876, 921], ["gold_amr.alignmentsToken2Node", "len", "len", "data_oracle.AMR_Oracle.tryMerge", "data_oracle.AMR_Oracle.isHead", "data_oracle.AMR_Oracle.isHead", "gold_amr.alignmentsToken2Node", "transitions.swapped_words.get", "transitions.swapped_words.get"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.alignmentsToken2Node", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.AMR_Oracle.tryMerge", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.AMR_Oracle.isHead", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.AMR_Oracle.isHead", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.alignmentsToken2Node"], ["", "def", "trySWAP", "(", "self", ",", "transitions", ",", "amr", ",", "gold_amr", ")", ":", "\n", "        ", "\"\"\"\n        Check if the next action is SWAP\n\n        SWAP if there is an unpredicted gold edge between stack[-1]\n        and some other node in the stack (blocked by stack[-2])\n        or if stack1 can be reduced.\n        \"\"\"", "\n", "if", "len", "(", "transitions", ".", "stack", ")", "<", "2", ":", "\n", "            ", "return", "False", "\n", "\n", "", "stack0", "=", "transitions", ".", "stack", "[", "-", "1", "]", "\n", "stack1", "=", "transitions", ".", "stack", "[", "-", "2", "]", "\n", "\n", "# Forbid if both words have been swapped already", "\n", "if", "stack0", "in", "transitions", ".", "swapped_words", "and", "stack1", "in", "transitions", ".", "swapped_words", ".", "get", "(", "stack0", ")", ":", "\n", "            ", "return", "False", "\n", "", "if", "stack1", "in", "transitions", ".", "swapped_words", "and", "stack0", "in", "transitions", ".", "swapped_words", ".", "get", "(", "stack1", ")", ":", "\n", "            ", "return", "False", "\n", "\n", "# Rules that use oracle info", "\n", "\n", "# check if we should MERGE instead", "\n", "", "if", "len", "(", "transitions", ".", "buffer", ")", ">", "0", ":", "\n", "            ", "buffer0", "=", "transitions", ".", "buffer", "[", "-", "1", "]", "\n", "if", "self", ".", "tryMerge", "(", "transitions", ",", "amr", ",", "gold_amr", ",", "first", "=", "stack0", ",", "second", "=", "buffer0", ")", ":", "\n", "                ", "return", "False", "\n", "\n", "# Look for tokens other than stack-top-two that can be head or child", "\n", "# of stack-top", "\n", "", "", "tok_alignment", "=", "gold_amr", ".", "alignmentsToken2Node", "(", "stack0", ")", "\n", "for", "tok", "in", "transitions", ".", "stack", ":", "\n", "            ", "if", "tok", "==", "stack1", "or", "tok", "==", "stack0", ":", "\n", "                ", "continue", "\n", "", "isHead", ",", "labelL", "=", "self", ".", "isHead", "(", "amr", ",", "gold_amr", ",", "stack0", ",", "tok", ")", "\n", "if", "isHead", ":", "\n", "                ", "return", "True", "\n", "", "isHead", ",", "labelR", "=", "self", ".", "isHead", "(", "amr", ",", "gold_amr", ",", "tok", ",", "stack0", ")", "\n", "if", "isHead", ":", "\n", "                ", "return", "True", "\n", "# check if we need to merge two tokens separated by stack1", "\n", "", "k_alignment", "=", "gold_amr", ".", "alignmentsToken2Node", "(", "tok", ")", "\n", "if", "k_alignment", "==", "tok_alignment", ":", "\n", "                ", "return", "True", "\n", "", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.AMR_Oracle.tryDependent": [[922, 960], ["gold_amr.alignmentsToken2Node", "len", "gold_amr.findSubGraph"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.alignmentsToken2Node", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.findSubGraph"], ["", "def", "tryDependent", "(", "self", ",", "transitions", ",", "amr", ",", "gold_amr", ")", ":", "\n", "        ", "\"\"\"\n        Check if the next action is DEPENDENT\n\n\n        Only for :polarity and :mode, if an edge and node is aligned\n        to this token in the gold amr but does not exist in the predicted amr,\n        the oracle adds it using the DEPENDENT action.\n        \"\"\"", "\n", "\n", "if", "not", "transitions", ".", "stack", ":", "\n", "            ", "return", "False", "\n", "\n", "# Rules that use oracle info", "\n", "\n", "", "stack0", "=", "transitions", ".", "stack", "[", "-", "1", "]", "\n", "tok_alignment", "=", "gold_amr", ".", "alignmentsToken2Node", "(", "stack0", ")", "\n", "\n", "if", "not", "tok_alignment", ":", "\n", "            ", "return", "False", "\n", "\n", "", "if", "len", "(", "tok_alignment", ")", "==", "1", ":", "\n", "            ", "source", "=", "tok_alignment", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "source", "=", "gold_amr", ".", "findSubGraph", "(", "tok_alignment", ")", ".", "root", "\n", "\n", "", "for", "s", ",", "r", ",", "t", "in", "gold_amr", ".", "edges", ":", "\n", "            ", "if", "s", "==", "source", "and", "r", "in", "[", "\":polarity\"", ",", "\":mode\"", "]", ":", "\n", "# FIXME: state altering code should be outside of tryACTION", "\n", "# in this case we need to recompute ...", "\n", "                ", "if", "(", "stack0", ",", "r", ")", "in", "[", "(", "e", "[", "0", "]", ",", "e", "[", "1", "]", ")", "for", "e", "in", "amr", ".", "edges", "]", ":", "\n", "                    ", "continue", "\n", "", "if", "t", "not", "in", "tok_alignment", "and", "(", "t", "in", "gold_amr", ".", "alignments", "and", "gold_amr", ".", "alignments", "[", "t", "]", ")", ":", "\n", "                    ", "continue", "\n", "", "self", ".", "new_edge", "=", "r", "\n", "self", ".", "new_node", "=", "gold_amr", ".", "nodes", "[", "t", "]", "\n", "return", "True", "\n", "", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.AMR_Oracle.tryEntity": [[961, 1021], ["gold_amr.alignmentsToken2Node", "reversed", "len", "len", "data_oracle.AMR_Oracle.tryMerge", "data_oracle.AMR_Oracle.tryMerge", "gold_amr.findSubGraph", "len", "len", "any"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.alignmentsToken2Node", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.AMR_Oracle.tryMerge", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.AMR_Oracle.tryMerge", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.findSubGraph"], ["", "def", "tryEntity", "(", "self", ",", "transitions", ",", "amr", ",", "gold_amr", ")", ":", "\n", "        ", "\"\"\"\n        Check if the next action is ENTITY\n        \"\"\"", "\n", "\n", "if", "not", "transitions", ".", "stack", ":", "\n", "            ", "return", "False", "\n", "\n", "", "stack0", "=", "transitions", ".", "stack", "[", "-", "1", "]", "\n", "\n", "# check if already an entity", "\n", "if", "stack0", "in", "transitions", ".", "entities", ":", "\n", "            ", "return", "False", "\n", "\n", "# Rules that use oracle info", "\n", "\n", "", "tok_alignment", "=", "gold_amr", ".", "alignmentsToken2Node", "(", "stack0", ")", "\n", "\n", "# check if alignment empty (or singleton)", "\n", "if", "len", "(", "tok_alignment", ")", "<=", "1", ":", "\n", "            ", "return", "False", "\n", "\n", "# check if we should MERGE instead", "\n", "", "if", "len", "(", "transitions", ".", "stack", ")", ">", "1", ":", "\n", "            ", "id", "=", "transitions", ".", "stack", "[", "-", "2", "]", "\n", "if", "self", ".", "tryMerge", "(", "transitions", ",", "amr", ",", "gold_amr", ",", "first", "=", "stack0", ",", "second", "=", "id", ")", ":", "\n", "                ", "return", "False", "\n", "", "", "for", "id", "in", "reversed", "(", "transitions", ".", "buffer", ")", ":", "\n", "            ", "if", "self", ".", "tryMerge", "(", "transitions", ",", "amr", ",", "gold_amr", ",", "first", "=", "stack0", ",", "second", "=", "id", ")", ":", "\n", "                ", "return", "False", "\n", "\n", "", "", "edges", "=", "gold_amr", ".", "findSubGraph", "(", "tok_alignment", ")", ".", "edges", "\n", "if", "not", "edges", ":", "\n", "            ", "return", "False", "\n", "\n", "# check if we should use DEPENDENT instead", "\n", "", "if", "len", "(", "tok_alignment", ")", "==", "2", ":", "\n", "            ", "if", "len", "(", "edges", ")", "==", "1", "and", "edges", "[", "0", "]", "[", "1", "]", "in", "[", "':mode'", ",", "':polarity'", "]", ":", "\n", "                ", "return", "False", "\n", "\n", "# FIXME: state altering code should be outside of tryACTION", "\n", "", "", "final_nodes", "=", "[", "n", "for", "n", "in", "tok_alignment", "if", "not", "any", "(", "s", "==", "n", "for", "s", ",", "r", ",", "t", "in", "edges", ")", "]", "\n", "# Fixes :rel", "\n", "gold_src", "=", "0", "\n", "for", "s", ",", "r", ",", "t", "in", "gold_amr", ".", "edges", ":", "\n", "            ", "if", "s", "in", "final_nodes", ":", "\n", "                ", "gold_src", "+=", "1", "\n", "", "", "pred_src", "=", "0", "\n", "for", "s", ",", "r", ",", "t", "in", "amr", ".", "edges", ":", "\n", "            ", "if", "s", "==", "stack0", ":", "\n", "                ", "pred_src", "+=", "1", "\n", "\n", "", "", "if", "pred_src", "<", "gold_src", ":", "\n", "            ", "return", "False", "\n", "\n", "", "new_nodes", "=", "[", "gold_amr", ".", "nodes", "[", "n", "]", "for", "n", "in", "tok_alignment", "if", "n", "not", "in", "final_nodes", "]", "\n", "self", ".", "entity_type", "=", "','", ".", "join", "(", "new_nodes", ")", "\n", "self", ".", "possibleEntityTypes", "[", "self", ".", "entity_type", "]", "+=", "1", "\n", "\n", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.AMR_Oracle.isHead": [[1022, 1059], ["gold_amr.alignmentsToken2Node", "gold_amr.alignmentsToken2Node", "len", "len", "len", "gold_amr.findSubGraph", "gold_amr.findSubGraph", "sources.remove"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.alignmentsToken2Node", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.alignmentsToken2Node", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.findSubGraph", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.findSubGraph"], ["", "def", "isHead", "(", "self", ",", "amr", ",", "gold_amr", ",", "x", ",", "y", ")", ":", "\n", "        ", "\"\"\"\n        Check if the x is the head of y in the gold AMR graph\n\n        If (the root of) x has an edge to (the root of) y in the gold AMR\n        which is not in the predicted AMR, return True.\n        \"\"\"", "\n", "\n", "x_alignment", "=", "gold_amr", ".", "alignmentsToken2Node", "(", "x", ")", "\n", "y_alignment", "=", "gold_amr", ".", "alignmentsToken2Node", "(", "y", ")", "\n", "\n", "if", "not", "y_alignment", "or", "not", "x_alignment", ":", "\n", "            ", "return", "False", ",", "''", "\n", "# get root of subgraph aligned to x", "\n", "", "if", "len", "(", "x_alignment", ")", ">", "1", ":", "\n", "            ", "source", "=", "gold_amr", ".", "findSubGraph", "(", "x_alignment", ")", ".", "root", "\n", "", "else", ":", "\n", "            ", "source", "=", "x_alignment", "[", "0", "]", "\n", "# get root of subgraph aligned to y", "\n", "", "if", "len", "(", "y_alignment", ")", ">", "1", ":", "\n", "            ", "target", "=", "gold_amr", ".", "findSubGraph", "(", "y_alignment", ")", ".", "root", "\n", "", "else", ":", "\n", "            ", "target", "=", "y_alignment", "[", "0", "]", "\n", "\n", "", "sources", "=", "[", "nid", "for", "nid", "in", "x_alignment", "]", "\n", "if", "len", "(", "x_alignment", ")", ">", "1", ":", "\n", "            ", "if", "x", "in", "self", ".", "transitions", "[", "-", "1", "]", ".", "entities", ":", "\n", "                ", "sources", "=", "[", "source", "]", "\n", "", "else", ":", "\n", "                ", "sources", ".", "remove", "(", "source", ")", "\n", "\n", "", "", "for", "s", ",", "r", ",", "t", "in", "gold_amr", ".", "edges", ":", "\n", "            ", "if", "s", "in", "sources", "and", "target", "==", "t", ":", "\n", "# check if already assigned", "\n", "                ", "if", "(", "x", ",", "r", ",", "y", ")", "not", "in", "amr", ".", "edges", ":", "\n", "                    ", "return", "True", ",", "r", "\n", "", "", "", "return", "False", ",", "''", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.AMR_Oracle.tryIntroduce": [[1060, 1093], ["reversed", "len", "data_oracle.AMR_Oracle.tryMerge", "len", "data_oracle.AMR_Oracle.isHead", "data_oracle.AMR_Oracle.isHead", "transitions.latent.append", "transitions.latent.append", "transitions.latent.pop", "transitions.latent.pop"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.AMR_Oracle.tryMerge", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.AMR_Oracle.isHead", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.AMR_Oracle.isHead", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop"], ["", "def", "tryIntroduce", "(", "self", ",", "transitions", ",", "amr", ",", "gold_amr", ")", ":", "\n", "        ", "\"\"\"\n        TODO:\n        \"\"\"", "\n", "if", "not", "transitions", ".", "stack", "or", "not", "transitions", ".", "latent", ":", "\n", "            ", "return", "False", "\n", "", "stack0", "=", "transitions", ".", "stack", "[", "-", "1", "]", "\n", "\n", "# Rules that use oracle info", "\n", "\n", "# check if we should MERGE instead", "\n", "if", "len", "(", "transitions", ".", "buffer", ")", ">", "0", ":", "\n", "            ", "buffer0", "=", "transitions", ".", "buffer", "[", "-", "1", "]", "\n", "stack0", "=", "transitions", ".", "stack", "[", "-", "1", "]", "\n", "if", "self", ".", "tryMerge", "(", "transitions", ",", "amr", ",", "gold_amr", ",", "first", "=", "stack0", ",", "second", "=", "buffer0", ")", ":", "\n", "                ", "return", "False", "\n", "\n", "", "", "idx", "=", "len", "(", "transitions", ".", "latent", ")", "-", "1", "\n", "for", "latentk", "in", "reversed", "(", "transitions", ".", "latent", ")", ":", "\n", "            ", "isHead", ",", "label", "=", "self", ".", "isHead", "(", "amr", ",", "gold_amr", ",", "stack0", ",", "latentk", ")", "\n", "\n", "if", "isHead", ":", "\n", "# rearrange latent if necessary", "\n", "                ", "transitions", ".", "latent", ".", "append", "(", "transitions", ".", "latent", ".", "pop", "(", "idx", ")", ")", "\n", "return", "True", "\n", "", "isHead", ",", "label", "=", "self", ".", "isHead", "(", "amr", ",", "gold_amr", ",", "latentk", ",", "stack0", ")", "\n", "\n", "if", "isHead", ":", "\n", "# rearrange latent if necessary", "\n", "                ", "transitions", ".", "latent", ".", "append", "(", "transitions", ".", "latent", ".", "pop", "(", "idx", ")", ")", "\n", "return", "True", "\n", "", "idx", "-=", "1", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.yellow_font": [[39, 41], ["None"], "function", ["None"], ["def", "yellow_font", "(", "string", ")", ":", "\n", "    ", "return", "\"\\033[93m%s\\033[0m\"", "%", "string", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.argument_parser": [[43, 145], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["", "def", "argument_parser", "(", ")", ":", "\n", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'AMR parser oracle'", ")", "\n", "# Single input parameters", "\n", "parser", ".", "add_argument", "(", "\n", "\"--in-amr\"", ",", "\n", "help", "=", "\"AMR notation in LDC format\"", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--in-propbank-args\"", ",", "\n", "help", "=", "\"Propbank argument data\"", ",", "\n", "type", "=", "str", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--out-oracle\"", ",", "\n", "help", "=", "\"tokens, AMR notation and actions given by oracle\"", ",", "\n", "type", "=", "str", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--out-sentences\"", ",", "\n", "help", "=", "\"tokenized sentences from --in-amr\"", ",", "\n", "type", "=", "str", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--out-actions\"", ",", "\n", "help", "=", "\"actions given by oracle\"", ",", "\n", "type", "=", "str", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--out-action-stats\"", ",", "\n", "help", "=", "\"statistics about actions\"", ",", "\n", "type", "=", "str", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--out-rule-stats\"", ",", "\n", "help", "=", "\"statistics about alignments\"", ",", "\n", "type", "=", "str", "\n", ")", "\n", "# Multiple input parameters", "\n", "parser", ".", "add_argument", "(", "\n", "\"--out-amr\"", ",", "\n", "help", "=", "\"corresponding AMR\"", ",", "\n", "type", "=", "str", "\n", ")", "\n", "#", "\n", "parser", ".", "add_argument", "(", "\n", "\"--verbose\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"verbose processing\"", "\n", ")", "\n", "#", "\n", "parser", ".", "add_argument", "(", "\n", "\"--multitask-max-words\"", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"number of woprds to use for multi-task\"", "\n", ")", "\n", "# Labeled shift args", "\n", "parser", ".", "add_argument", "(", "\n", "\"--out-multitask-words\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"where to store top-k words for multi-task\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--in-multitask-words\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"where to read top-k words for multi-task\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--no-whitespace-in-actions\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"avoid tab separation in actions and sentences by removing whitespaces\"", "\n", ")", "\n", "# copy lemma action", "\n", "parser", ".", "add_argument", "(", "\n", "\"--copy-lemma-action\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Use copy action from Spacy lemmas\"", "\n", ")", "\n", "# skip empty amrs", "\n", "parser", ".", "add_argument", "(", "\n", "\"--skip-empty\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Skip empty AMRs (otherwise it will raise and error)\"", "\n", ")", "\n", "# copy lemma action", "\n", "parser", ".", "add_argument", "(", "\n", "\"--addnode-count-cutoff\"", ",", "\n", "help", "=", "\"forbid all addnode actions appearing less times than count\"", ",", "\n", "type", "=", "int", "\n", ")", "\n", "# path to entity rules generated from the train file", "\n", "parser", ".", "add_argument", "(", "\n", "\"--entity-rules\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"entity rules\"", "\n", ")", "\n", "#", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.preprocess_amr": [[147, 186], ["enumerate", "gold_amr.tokens.append", "gold_amr.edges.append", "gold_amr.alignmentsToken2Node", "range", "len", "gold_amr.tokens.append", "gold_amr.alignments[].remove", "gold_amr.nodes[].startswith", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.alignmentsToken2Node", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "preprocess_amr", "(", "gold_amr", ",", "add_unaligned", ",", "included_unaligned", ")", ":", "\n", "\n", "# clean alignments", "\n", "    ", "for", "i", ",", "tok", "in", "enumerate", "(", "gold_amr", ".", "tokens", ")", ":", "\n", "        ", "align", "=", "gold_amr", ".", "alignmentsToken2Node", "(", "i", "+", "1", ")", "\n", "if", "len", "(", "align", ")", "==", "2", ":", "\n", "            ", "edges", "=", "[", "\n", "(", "s", ",", "r", ",", "t", ")", "\n", "for", "s", ",", "r", ",", "t", "in", "gold_amr", ".", "edges", "\n", "if", "s", "in", "align", "and", "t", "in", "align", "\n", "]", "\n", "if", "not", "edges", ":", "\n", "                ", "remove", "=", "1", "\n", "if", "(", "\n", "gold_amr", ".", "nodes", "[", "align", "[", "1", "]", "]", ".", "startswith", "(", "tok", "[", ":", "2", "]", ")", "or", "\n", "len", "(", "gold_amr", ".", "alignments", "[", "align", "[", "0", "]", "]", ")", ">", "\n", "len", "(", "gold_amr", ".", "alignments", "[", "align", "[", "1", "]", "]", ")", "\n", ")", ":", "\n", "                    ", "remove", "=", "0", "\n", "", "gold_amr", ".", "alignments", "[", "align", "[", "remove", "]", "]", ".", "remove", "(", "i", "+", "1", ")", "\n", "gold_amr", ".", "token2node_memo", "=", "{", "}", "\n", "\n", "# TODO: describe this", "\n", "", "", "", "if", "add_unaligned", ":", "\n", "        ", "for", "i", "in", "range", "(", "add_unaligned", ")", ":", "\n", "            ", "gold_amr", ".", "tokens", ".", "append", "(", "\"<unaligned>\"", ")", "\n", "for", "n", "in", "gold_amr", ".", "nodes", ":", "\n", "                ", "if", "n", "not", "in", "gold_amr", ".", "alignments", "or", "not", "gold_amr", ".", "alignments", "[", "n", "]", ":", "\n", "                    ", "if", "gold_amr", ".", "nodes", "[", "n", "]", "in", "included_unaligned", ":", "\n", "                        ", "gold_amr", ".", "alignments", "[", "n", "]", "=", "[", "len", "(", "gold_amr", ".", "tokens", ")", "]", "\n", "break", "\n", "\n", "# add root node", "\n", "", "", "", "", "", "gold_amr", ".", "tokens", ".", "append", "(", "\"<ROOT>\"", ")", "\n", "gold_amr", ".", "nodes", "[", "-", "1", "]", "=", "\"<ROOT>\"", "\n", "gold_amr", ".", "edges", ".", "append", "(", "(", "-", "1", ",", "\"root\"", ",", "gold_amr", ".", "root", ")", ")", "\n", "gold_amr", ".", "alignments", "[", "-", "1", "]", "=", "[", "-", "1", "]", "\n", "\n", "return", "gold_amr", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.get_node_alignment_counts": [[188, 216], ["collections.defaultdict", "collections.defaultdict", "range", "collections.defaultdict.items", "collections.Counter", "len", "train_amr.alignmentsToken2Node", "node_by_token[].update", "alignments[].append", "len"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.alignmentsToken2Node", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "get_node_alignment_counts", "(", "gold_amrs_train", ")", ":", "\n", "    ", "\"\"\"Get statistics of alignments between nodes and surface words\"\"\"", "\n", "\n", "node_by_token", "=", "defaultdict", "(", "lambda", ":", "Counter", "(", ")", ")", "\n", "for", "train_amr", "in", "gold_amrs_train", ":", "\n", "\n", "# Get alignments", "\n", "        ", "alignments", "=", "defaultdict", "(", "list", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "train_amr", ".", "tokens", ")", ")", ":", "\n", "            ", "for", "al_node", "in", "train_amr", ".", "alignmentsToken2Node", "(", "i", "+", "1", ")", ":", "\n", "                ", "alignments", "[", "al_node", "]", ".", "append", "(", "\n", "train_amr", ".", "tokens", "[", "i", "]", "\n", ")", "\n", "\n", "", "", "for", "node_id", ",", "aligned_tokens", "in", "alignments", ".", "items", "(", ")", ":", "\n", "# join multiple words into one single expression", "\n", "            ", "if", "len", "(", "aligned_tokens", ")", ">", "1", ":", "\n", "                ", "token_str", "=", "\" \"", ".", "join", "(", "aligned_tokens", ")", "\n", "", "else", ":", "\n", "                ", "token_str", "=", "aligned_tokens", "[", "0", "]", "\n", "\n", "", "node", "=", "train_amr", ".", "nodes", "[", "node_id", "]", "\n", "\n", "# count number of time a node is aligned to a token, indexed by", "\n", "# token", "\n", "node_by_token", "[", "token_str", "]", ".", "update", "(", "[", "node", "]", ")", "\n", "\n", "", "", "return", "node_by_token", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.is_most_common": [[218, 232], ["len", "len", "node_counts.most_common", "node_counts.most_common", "node_counts.most_common", "node_counts.most_common"], "function", ["None"], ["", "def", "is_most_common", "(", "node_counts", ",", "node", ",", "rank", "=", "0", ")", ":", "\n", "\n", "    ", "return", "(", "\n", "(", "\n", "# as many results as the rank and node in that rank matches", "\n", "len", "(", "node_counts", ")", "==", "rank", "+", "1", "and", "\n", "node_counts", ".", "most_common", "(", "rank", "+", "1", ")", "[", "-", "1", "]", "[", "0", "]", "==", "node", "\n", ")", "or", "(", "\n", "# more results than the rank, node in that rank matches, and rank", "\n", "# results is more probable than rank + 1", "\n", "len", "(", "node_counts", ")", ">", "rank", "+", "1", "and", "\n", "node_counts", ".", "most_common", "(", "rank", "+", "1", ")", "[", "-", "1", "]", "[", "0", "]", "==", "node", "and", "\n", "node_counts", ".", "most_common", "(", "rank", "+", "1", ")", "[", "-", "1", "]", "[", "1", "]", ">", "\n", "node_counts", ".", "most_common", "(", "rank", "+", "2", ")", "[", "-", "1", "]", "[", "1", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.alert_inconsistencies": [[236, 311], ["len", "collections.Counter", "collections.defaultdict", "collections.defaultdict", "len", "collections.Counter.items", "transition_amr_parser.amr.get_duplicate_edges", "collections.Counter.update", "amr.toJAMRString", "amr_counts_by_sentence[].update", "len", "print", "print", "collections.defaultdict", "sum", "print", "print", "collections.Counter", "collect_duplicates.extend", "len", "max", "data_oracle.yellow_font", "data_oracle.yellow_font", "re.match", "collections.defaultdict.values", "data_oracle.yellow_font", "dict", "collections.defaultdict.values", "counter.values"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.get_duplicate_edges", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.toJAMRString", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.yellow_font", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.yellow_font", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.yellow_font"], ["", "def", "alert_inconsistencies", "(", "gold_amrs", ")", ":", "\n", "    ", "num_sentences", "=", "len", "(", "gold_amrs", ")", "\n", "sentence_count", "=", "Counter", "(", ")", "\n", "amr_by_amrkey_by_sentence", "=", "defaultdict", "(", "dict", ")", "\n", "amr_counts_by_sentence", "=", "defaultdict", "(", "lambda", ":", "Counter", "(", ")", ")", "\n", "collect_duplicates", "=", "[", "]", "\n", "for", "amr", "in", "gold_amrs", ":", "\n", "\n", "# collect duplicate arcs ", "\n", "        ", "dupes", "=", "get_duplicate_edges", "(", "amr", ")", "\n", "if", "dupes", ":", "\n", "            ", "collect_duplicates", ".", "extend", "(", "dupes", ")", "\n", "\n", "# hash of sentence", "\n", "", "skey", "=", "\" \"", ".", "join", "(", "amr", ".", "tokens", ")", "\n", "\n", "# count number of time sentence repeated", "\n", "sentence_count", ".", "update", "(", "[", "skey", "]", ")", "\n", "\n", "# hash of AMR labeling", "\n", "akey", "=", "amr", ".", "toJAMRString", "(", ")", "\n", "\n", "# store different amr labels for same sent, keep has map", "\n", "if", "akey", "not", "in", "amr_by_amrkey_by_sentence", "[", "skey", "]", ":", "\n", "            ", "amr_by_amrkey_by_sentence", "[", "skey", "]", "[", "akey", "]", "=", "amr", "\n", "\n", "# count how many time each hash appears", "\n", "", "amr_counts_by_sentence", "[", "skey", "]", ".", "update", "(", "[", "akey", "]", ")", "\n", "\n", "", "num_unique_sents", "=", "len", "(", "sentence_count", ")", "\n", "\n", "num_labelings", "=", "0", "\n", "for", "skey", ",", "sent_count", "in", "sentence_count", ".", "items", "(", ")", ":", "\n", "        ", "num_labelings", "+=", "len", "(", "amr_counts_by_sentence", "[", "skey", "]", ")", "\n", "if", "len", "(", "amr_counts_by_sentence", "[", "skey", "]", ")", ">", "1", ":", "\n", "            ", "pass", "\n", "# There is more than one labeling for this sentence", "\n", "# amrs = list(amr_by_amrkey_by_sentence[skey].values())", "\n", "\n", "# inform user", "\n", "", "", "if", "num_sentences", ">", "num_unique_sents", ":", "\n", "        ", "num_repeated", "=", "num_sentences", "-", "num_unique_sents", "\n", "perc", "=", "num_repeated", "/", "num_sentences", "\n", "alert_str", "=", "'{:d}/{:d} {:2.1f} % repeated sents (max {:d} times)'", ".", "format", "(", "\n", "num_repeated", ",", "\n", "num_sentences", ",", "\n", "100", "*", "perc", ",", "\n", "max", "(", "\n", "count", "\n", "for", "counter", "in", "amr_counts_by_sentence", ".", "values", "(", ")", "\n", "for", "count", "in", "counter", ".", "values", "(", ")", "\n", ")", "\n", ")", "\n", "print", "(", "yellow_font", "(", "alert_str", ")", ")", "\n", "\n", "", "if", "num_labelings", ">", "num_unique_sents", ":", "\n", "        ", "num_inconsistent", "=", "num_labelings", "-", "num_unique_sents", "\n", "perc", "=", "num_inconsistent", "/", "num_sentences", "\n", "alert_str", "=", "'{:d}/{:d} {:2.4f} % inconsistent labelings from repeated sents'", ".", "format", "(", "\n", "num_inconsistent", ",", "\n", "num_sentences", ",", "\n", "perc", "\n", ")", "\n", "print", "(", "yellow_font", "(", "alert_str", ")", ")", "\n", "\n", "", "if", "collect_duplicates", ":", "\n", "        ", "dupes_by_arc", "=", "defaultdict", "(", "int", ")", "\n", "for", "item", "in", "collect_duplicates", ":", "\n", "            ", "if", "re", ".", "match", "(", "'^[0-9\\.]+$'", ",", "item", "[", "0", "]", "[", "0", "]", ")", ":", "\n", "                ", "dupes_by_arc", "[", "item", "[", "0", "]", "[", "1", "]", "]", "+=", "item", "[", "1", "]", "\n", "", "else", ":", "\n", "                ", "dupes_by_arc", "[", "item", "[", "0", "]", "[", "0", "]", "]", "+=", "item", "[", "1", "]", "\n", "", "", "num_dup", "=", "sum", "(", "dupes_by_arc", ".", "values", "(", ")", ")", "\n", "print", "(", "yellow_font", "(", "f'AMR contains {num_dup} duplicate edges'", ")", ")", "\n", "print", "(", "dict", "(", "dupes_by_arc", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.read_multitask_words": [[313, 321], ["open", "line.strip().split", "len", "multitask_words.append", "line.strip"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "", "def", "read_multitask_words", "(", "multitask_list", ")", ":", "\n", "    ", "multitask_words", "=", "[", "]", "\n", "with", "open", "(", "multitask_list", ")", "as", "fid", ":", "\n", "        ", "for", "line", "in", "fid", ":", "\n", "            ", "items", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "if", "len", "(", "items", ")", ">", "2", ":", "\n", "                ", "multitask_words", ".", "append", "(", "items", "[", "1", "]", ")", "\n", "", "", "", "return", "multitask_words", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.label_shift": [[323, 331], ["state_machine.get_buffer_stack_copy"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.DepParsingStateMachine.get_buffer_stack_copy"], ["", "def", "label_shift", "(", "state_machine", ",", "multitask_words", ")", ":", "\n", "# TODO: Legacy numbering", "\n", "    ", "buffer", ",", "_", "=", "state_machine", ".", "get_buffer_stack_copy", "(", ")", "\n", "top_of_buffer", "=", "state_machine", ".", "tokens", "[", "buffer", "[", "-", "1", "]", "-", "1", "]", "\n", "if", "top_of_buffer", "in", "multitask_words", ":", "\n", "        ", "return", "f'SHIFT({top_of_buffer})'", "\n", "", "else", ":", "\n", "        ", "return", "'SHIFT'", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.get_multitask_actions": [[333, 350], ["collections.Counter", "dict", "collections.Counter.update", "dict.update", "list", "sorted", "collections.Counter.items"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update"], ["", "", "def", "get_multitask_actions", "(", "max_symbols", ",", "tokenized_corpus", ",", "add_root", "=", "False", ")", ":", "\n", "\n", "    ", "word_count", "=", "Counter", "(", ")", "\n", "for", "sentence", "in", "tokenized_corpus", ":", "\n", "        ", "word_count", ".", "update", "(", "[", "x", "for", "x", "in", "sentence", "]", ")", "\n", "\n", "# Restrict to top-k words", "\n", "", "allowed_words", "=", "dict", "(", "list", "(", "sorted", "(", "\n", "word_count", ".", "items", "(", ")", ",", "\n", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ")", "\n", ")", "[", "-", "max_symbols", ":", "]", ")", "\n", "\n", "if", "add_root", ":", "\n", "# Add root regardless", "\n", "        ", "allowed_words", ".", "update", "(", "{", "'ROOT'", ":", "word_count", "[", "'ROOT'", "]", "}", ")", "\n", "\n", "", "return", "allowed_words", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.process_multitask_words": [[1095, 1123], ["data_oracle.get_multitask_actions", "open", "get_multitask_actions.keys", "fid.write", "open", "line.strip", "fid.readlines"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.get_multitask_actions", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.readlines"], ["", "", "def", "process_multitask_words", "(", "tokenized_corpus", ",", "multitask_max_words", ",", "\n", "in_multitask_words", ",", "out_multitask_words", ",", "\n", "add_root", "=", "False", ")", ":", "\n", "\n", "# Load/Save words for multi-task", "\n", "    ", "if", "multitask_max_words", ":", "\n", "        ", "assert", "multitask_max_words", "\n", "assert", "out_multitask_words", "\n", "# get top words", "\n", "multitask_words", "=", "get_multitask_actions", "(", "\n", "multitask_max_words", ",", "\n", "tokenized_corpus", ",", "\n", "add_root", "=", "add_root", "\n", ")", "\n", "# store in file", "\n", "with", "open", "(", "out_multitask_words", ",", "'w'", ")", "as", "fid", ":", "\n", "            ", "for", "word", "in", "multitask_words", ".", "keys", "(", ")", ":", "\n", "                ", "fid", ".", "write", "(", "f'{word}\\n'", ")", "\n", "", "", "", "elif", "in_multitask_words", ":", "\n", "        ", "assert", "not", "multitask_max_words", "\n", "assert", "not", "out_multitask_words", "\n", "# store in file", "\n", "with", "open", "(", "in_multitask_words", ")", "as", "fid", ":", "\n", "            ", "multitask_words", "=", "[", "line", ".", "strip", "(", ")", "for", "line", "in", "fid", ".", "readlines", "(", ")", "]", "\n", "", "", "else", ":", "\n", "        ", "multitask_words", "=", "None", "\n", "\n", "", "return", "multitask_words", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.print_corpus_info": [[1125, 1140], ["print", "collections.Counter", "sum", "print", "collections.Counter", "sum", "print", "collections.Counter", "sum", "print", "collections.Counter.values", "collections.Counter.values", "collections.Counter.values", "len", "amr.nodes.values", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["", "def", "print_corpus_info", "(", "amrs", ")", ":", "\n", "\n", "# print some info", "\n", "    ", "print", "(", "f'{len(amrs)} sentences'", ")", "\n", "node_label_count", "=", "Counter", "(", "[", "\n", "n", "for", "amr", "in", "amrs", "for", "n", "in", "amr", ".", "nodes", ".", "values", "(", ")", "\n", "]", ")", "\n", "node_tokens", "=", "sum", "(", "node_label_count", ".", "values", "(", ")", ")", "\n", "print", "(", "f'{len(node_label_count)}/{node_tokens} node types/tokens'", ")", "\n", "edge_label_count", "=", "Counter", "(", "[", "t", "[", "1", "]", "for", "amr", "in", "amrs", "for", "t", "in", "amr", ".", "edges", "]", ")", "\n", "edge_tokens", "=", "sum", "(", "edge_label_count", ".", "values", "(", ")", ")", "\n", "print", "(", "f'{len(edge_label_count)}/{edge_tokens} edge types/tokens'", ")", "\n", "word_label_count", "=", "Counter", "(", "[", "w", "for", "amr", "in", "amrs", "for", "w", "in", "amr", ".", "tokens", "]", ")", "\n", "word_tokens", "=", "sum", "(", "word_label_count", ".", "values", "(", ")", ")", "\n", "print", "(", "f'{len(word_label_count)}/{word_tokens} word types/tokens'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.main": [[1141, 1213], ["data_oracle.argument_parser", "print", "transition_amr_parser.io.read_amr", "data_oracle.print_corpus_info", "data_oracle.alert_inconsistencies", "data_oracle.process_multitask_words", "transition_amr_parser.utils.print_log", "data_oracle.AMR_Oracle", "data_oracle.AMR_Oracle.runOracle", "transition_amr_parser.io.read_propbank", "transition_amr_parser.utils.print_log", "transition_amr_parser.utils.print_log", "list", "transition_amr_parser.utils.print_log", "transition_amr_parser.utils.print_log", "transition_amr_parser.utils.print_log", "open", "fid.write", "sum", "sum", "AMR_Oracle.stats[].most_common", "json.dumps", "transition_amr_parser.utils.print_log", "transition_amr_parser.state_machine.entity_rule_stats.values", "transition_amr_parser.state_machine.entity_rule_totals.values"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.argument_parser", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.read_amr", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.print_corpus_info", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.alert_inconsistencies", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.process_multitask_words", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.data_oracle.AMR_Oracle.runOracle", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.io.read_propbank", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log"], ["", "def", "main", "(", ")", ":", "\n", "\n", "# Argument handling", "\n", "    ", "args", "=", "argument_parser", "(", ")", "\n", "\n", "# Load AMR (replace some unicode characters)", "\n", "print", "(", "f'Read {args.in_amr}'", ")", "\n", "corpus", "=", "read_amr", "(", "args", ".", "in_amr", ",", "unicode_fixes", "=", "True", ")", "\n", "amrs", "=", "corpus", ".", "amrs", "\n", "# Remove empty AMRS", "\n", "if", "args", ".", "skip_empty", ":", "\n", "        ", "amrs", "=", "[", "amr", "for", "amr", "in", "amrs", "if", "amr", ".", "tokens", "]", "\n", "# print general info and about inconsistencies in AMR annotations", "\n", "", "print_corpus_info", "(", "amrs", ")", "\n", "alert_inconsistencies", "(", "amrs", ")", "\n", "\n", "# Load propbank (if provided)", "\n", "propbank_args", "=", "None", "\n", "if", "args", ".", "in_propbank_args", ":", "\n", "        ", "propbank_args", "=", "read_propbank", "(", "args", ".", "in_propbank_args", ")", "\n", "\n", "# read/write multi-task (labeled shift) action ", "\n", "", "multitask_words", "=", "process_multitask_words", "(", "\n", "[", "list", "(", "amr", ".", "tokens", ")", "for", "amr", "in", "amrs", "]", ",", "\n", "args", ".", "multitask_max_words", ",", "\n", "args", ".", "in_multitask_words", ",", "\n", "args", ".", "out_multitask_words", ",", "\n", "add_root", "=", "True", "\n", ")", "\n", "\n", "# TODO: At the end, an oracle is just a parser with oracle info. This could", "\n", "# be turner into a loop similar to parser.py (or directly use that and a", "\n", "# AMROracleParser())", "\n", "print_log", "(", "\"amr\"", ",", "\"Processing oracle\"", ")", "\n", "oracle", "=", "AMR_Oracle", "(", "args", ".", "entity_rules", ",", "verbose", "=", "args", ".", "verbose", ")", "\n", "oracle", ".", "runOracle", "(", "\n", "amrs", ",", "\n", "propbank_args", ",", "\n", "out_oracle", "=", "args", ".", "out_oracle", ",", "\n", "out_amr", "=", "args", ".", "out_amr", ",", "\n", "out_sentences", "=", "args", ".", "out_sentences", ",", "\n", "out_actions", "=", "args", ".", "out_actions", ",", "\n", "out_rule_stats", "=", "args", ".", "out_rule_stats", ",", "\n", "add_unaligned", "=", "0", ",", "\n", "no_whitespace_in_actions", "=", "args", ".", "no_whitespace_in_actions", ",", "\n", "multitask_words", "=", "multitask_words", ",", "\n", "copy_lemma_action", "=", "args", ".", "copy_lemma_action", ",", "\n", "addnode_count_cutoff", "=", "args", ".", "addnode_count_cutoff", "\n", ")", "\n", "\n", "# inform user", "\n", "for", "stat", "in", "oracle", ".", "stats", ":", "\n", "        ", "if", "args", ".", "verbose", ":", "\n", "            ", "print_log", "(", "\"amr\"", ",", "stat", ")", "\n", "print_log", "(", "\"amr\"", ",", "oracle", ".", "stats", "[", "stat", "]", ".", "most_common", "(", "100", ")", ")", "\n", "print_log", "(", "\"amr\"", ",", "\"\"", ")", "\n", "\n", "", "", "if", "args", ".", "out_action_stats", ":", "\n", "# Store rule statistics", "\n", "        ", "with", "open", "(", "args", ".", "out_action_stats", ",", "'w'", ")", "as", "fid", ":", "\n", "            ", "fid", ".", "write", "(", "json", ".", "dumps", "(", "oracle", ".", "stats", ")", ")", "\n", "\n", "", "", "if", "use_addnode_rules", ":", "\n", "        ", "for", "x", "in", "entity_rule_totals", ":", "\n", "            ", "perc", "=", "entity_rule_stats", "[", "x", "]", "/", "entity_rule_totals", "[", "x", "]", "\n", "if", "args", ".", "verbose", ":", "\n", "                ", "print_log", "(", "x", ",", "entity_rule_stats", "[", "x", "]", ",", "'/'", ",", "\n", "entity_rule_totals", "[", "x", "]", ",", "'='", ",", "f'{perc:.2f}'", ")", "\n", "", "", "perc", "=", "sum", "(", "entity_rule_stats", ".", "values", "(", ")", ")", "/", "sum", "(", "entity_rule_totals", ".", "values", "(", ")", ")", "\n", "print_log", "(", "'Totals:'", ",", "f'{perc:.2f}'", ")", "\n", "print_log", "(", "'Totals:'", ",", "'Failed Entity Predictions:'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.fake_parse.FakeAMRParser.__init__": [[230, 254], ["transition_amr_parser.state_machine.get_spacy_lemmatizer", "collections.Counter", "collections.Counter"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.get_spacy_lemmatizer"], ["def", "__init__", "(", "self", ",", "logger", "=", "None", ",", "machine_type", "=", "'AMR'", ",", "\n", "from_sent_act_pairs", "=", "None", ",", "actions_by_stack_rules", "=", "None", ",", "\n", "no_whitespace_in_actions", "=", "False", ",", "entity_rules", "=", "None", ")", ":", "\n", "\n", "        ", "assert", "not", "no_whitespace_in_actions", ",", "'--no-whitespace-in-actions deprected'", "\n", "\n", "# Dummy mode: simulate parser from pre-computed pairs of sentences", "\n", "# and actions", "\n", "self", ".", "actions_by_sentence", "=", "{", "\n", "\" \"", ".", "join", "(", "sent", ")", ":", "actions", "for", "sent", ",", "actions", "in", "from_sent_act_pairs", "\n", "}", "\n", "self", ".", "logger", "=", "logger", "\n", "self", ".", "sent_idx", "=", "0", "\n", "self", ".", "actions_by_stack_rules", "=", "actions_by_stack_rules", "\n", "self", ".", "no_whitespace_in_actions", "=", "no_whitespace_in_actions", "\n", "self", ".", "machine_type", "=", "machine_type", "\n", "self", ".", "entity_rules", "=", "entity_rules", "\n", "# initialize here for speed", "\n", "self", ".", "spacy_lemmatizer", "=", "get_spacy_lemmatizer", "(", ")", "\n", "\n", "# counters", "\n", "self", ".", "pred_counts", "=", "Counter", "(", ")", "\n", "self", ".", "rule_violation", "=", "Counter", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.fake_parse.FakeAMRParser.parse_sentence": [[255, 322], ["sentence_str.split", "fake_parse.get_bio_tags", "transition_amr_parser.state_machine.AMRStateMachine", "fake_parse.FakeAMRParser.logger.update", "transition_amr_parser.state_machine.DepParsingStateMachine.applyAction", "transition_amr_parser.state_machine.DepParsingStateMachine", "len", "print", "fake_parse.restrict_action", "bio_alignments.update", "transition_amr_parser.utils.yellow_font", "fake_parse.get_bio_from_machine"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.fake_parse.get_bio_tags", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.DepParsingStateMachine.applyAction", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.fake_parse.restrict_action", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.yellow_font", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.fake_parse.get_bio_from_machine"], ["", "def", "parse_sentence", "(", "self", ",", "sentence_str", ")", ":", "\n", "        ", "\"\"\"\n        sentence_str is a string with whitespace separated tokens\n        \"\"\"", "\n", "\n", "# simulated actions given by a parsing model", "\n", "key", "=", "\" \"", ".", "join", "(", "sentence_str", ")", "\n", "assert", "sentence_str", "in", "self", ".", "actions_by_sentence", ",", "\"Fake parser has no actions for sentence: %s\"", "%", "sentence_str", "\n", "actions", "=", "self", ".", "actions_by_sentence", "[", "sentence_str", "]", "\n", "tokens", "=", "sentence_str", ".", "split", "(", ")", "\n", "\n", "# Initialize state machine", "\n", "if", "self", ".", "machine_type", "==", "'AMR'", ":", "\n", "            ", "state_machine", "=", "AMRStateMachine", "(", "\n", "tokens", ",", "\n", "actions_by_stack_rules", "=", "self", ".", "actions_by_stack_rules", ",", "\n", "spacy_lemmatizer", "=", "self", ".", "spacy_lemmatizer", ",", "\n", "entity_rules", "=", "self", ".", "entity_rules", "\n", ")", "\n", "", "elif", "self", ".", "machine_type", "==", "'dep-parsing'", ":", "\n", "            ", "state_machine", "=", "DepParsingStateMachine", "(", "tokens", ")", "\n", "\n", "# this will store AMR parsing as BIO tag (PRED, ADDNODE)", "\n", "", "bio_alignments", "=", "{", "}", "\n", "\n", "# execute parsing model", "\n", "while", "not", "state_machine", ".", "is_closed", ":", "\n", "\n", "# Print state (pause if solicited)", "\n", "            ", "self", ".", "logger", ".", "update", "(", "self", ".", "sent_idx", ",", "state_machine", ")", "\n", "\n", "if", "len", "(", "actions", ")", "<=", "state_machine", ".", "time_step", ":", "\n", "# if machine is not propperly closed hard exit", "\n", "                ", "print", "(", "yellow_font", "(", "\n", "f'machine not closed at step {state_machine.time_step}'", "\n", ")", ")", "\n", "raw_action", "=", "'CLOSE'", "\n", "", "else", ":", "\n", "# get action from model", "\n", "                ", "raw_action", "=", "actions", "[", "state_machine", ".", "time_step", "]", "\n", "\n", "# restrict action space according to machine restrictions and", "\n", "# statistics", "\n", "", "if", "self", ".", "machine_type", "==", "'AMR'", ":", "\n", "                ", "raw_action", "=", "restrict_action", "(", "\n", "state_machine", ",", "\n", "raw_action", ",", "\n", "self", ".", "pred_counts", ",", "\n", "self", ".", "rule_violation", "\n", ")", "\n", "\n", "# update bio tags from AMR", "\n", "bio_alignments", ".", "update", "(", "\n", "get_bio_from_machine", "(", "state_machine", ",", "raw_action", ")", "\n", ")", "\n", "\n", "# Update state machine", "\n", "", "state_machine", ".", "applyAction", "(", "raw_action", ")", "\n", "\n", "# build bio tags", "\n", "", "bio_tags", "=", "get_bio_tags", "(", "state_machine", ",", "bio_alignments", ")", "\n", "\n", "# count one sentence more", "\n", "self", ".", "sent_idx", "+=", "1", "\n", "\n", "return", "state_machine", ",", "bio_tags", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.fake_parse.Logger.__init__": [[326, 345], ["signal.signal", "signal.signal", "print", "exit"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["    ", "def", "__init__", "(", "self", ",", "step_by_step", "=", "None", ",", "clear_print", "=", "None", ",", "pause_time", "=", "None", ",", "\n", "verbose", "=", "False", ")", ":", "\n", "\n", "        ", "self", ".", "step_by_step", "=", "step_by_step", "\n", "self", ".", "clear_print", "=", "clear_print", "\n", "self", ".", "pause_time", "=", "pause_time", "\n", "self", ".", "verbose", "=", "verbose", "or", "self", ".", "step_by_step", "\n", "\n", "if", "step_by_step", ":", "\n", "\n", "# Set traps for system signals to die graceful when Ctrl-C used", "\n", "\n", "            ", "def", "ordered_exit", "(", "signum", ",", "frame", ")", ":", "\n", "                ", "\"\"\"Mesage user when killing by signal\"\"\"", "\n", "print", "(", "\"\\nStopped by user\\n\"", ")", "\n", "exit", "(", "0", ")", "\n", "\n", "", "signal", ".", "signal", "(", "signal", ".", "SIGINT", ",", "ordered_exit", ")", "\n", "signal", ".", "signal", "(", "signal", ".", "SIGTERM", ",", "ordered_exit", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.fake_parse.Logger.update": [[346, 360], ["print", "print", "os.system", "time.sleep", "input"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["", "", "def", "update", "(", "self", ",", "sent_idx", ",", "state_machine", ")", ":", "\n", "\n", "        ", "if", "self", ".", "verbose", ":", "\n", "            ", "if", "self", ".", "clear_print", ":", "\n", "# clean screen each time", "\n", "                ", "os", ".", "system", "(", "'clear'", ")", "\n", "", "print", "(", "f'sentence {sent_idx}\\n'", ")", "\n", "print", "(", "state_machine", ")", "\n", "# step by step mode", "\n", "if", "self", ".", "step_by_step", ":", "\n", "                ", "if", "self", ".", "pause_time", ":", "\n", "                    ", "time", ".", "sleep", "(", "self", ".", "pause_time", ")", "\n", "", "else", ":", "\n", "                    ", "input", "(", "'Press any key to continue'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.fake_parse.argument_parser": [[25, 128], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "numpy.random.randint", "bool"], "function", ["None"], ["def", "argument_parser", "(", ")", ":", "\n", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'AMR parser'", ")", "\n", "# Multiple input parameters", "\n", "parser", ".", "add_argument", "(", "\n", "\"--in-sentences\"", ",", "\n", "help", "=", "\"file space with carriare return separated sentences\"", ",", "\n", "type", "=", "str", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--in-actions\"", ",", "\n", "help", "=", "\"file space with carriage return separated sentences\"", ",", "\n", "type", "=", "str", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--entity-rules\"", ",", "\n", "help", "=", "\"entity rules\"", ",", "\n", "type", "=", "str", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--out-amr\"", ",", "\n", "help", "=", "\"parsing model\"", ",", "\n", "type", "=", "str", "\n", ")", "\n", "# state machine rules", "\n", "parser", ".", "add_argument", "(", "\n", "\"--action-rules-from-stats\"", ",", "\n", "help", "=", "\"Use oracle statistics to restrict possible actions\"", ",", "\n", "type", "=", "str", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--verbose\"", ",", "\n", "help", "=", "\"verbose mode\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "default", "=", "False", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--machine-type\"", ",", "\n", "choices", "=", "[", "'AMR'", ",", "'dep-parsing'", "]", ",", "\n", "default", "=", "'AMR'", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--separator\"", ",", "\n", "default", "=", "'\\t'", "\n", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "\"--step-by-step\"", ",", "\n", "help", "=", "\"pause after each action\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "default", "=", "False", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--pause-time\"", ",", "\n", "help", "=", "\"time waited after each step, default is manual\"", ",", "\n", "type", "=", "int", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--clear-print\"", ",", "\n", "help", "=", "\"clear command line before each print\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "default", "=", "False", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--offset\"", ",", "\n", "help", "=", "\"start at given sentence number (starts at zero)\"", ",", "\n", "type", "=", "int", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--random-up-to\"", ",", "\n", "help", "=", "\"sample randomly from a max number\"", ",", "\n", "type", "=", "int", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--no-whitespace-in-actions\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Assume whitespaces normalized to _ in PRED\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--sanity-check\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Sanity check produced AMRs\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--out-bio-tags\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"Output AMR info as BIO tags (PRED and ADDNODE actions)\"", "\n", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "# Argument pre-processing", "\n", "if", "args", ".", "random_up_to", ":", "\n", "        ", "args", ".", "offset", "=", "np", ".", "random", ".", "randint", "(", "args", ".", "random_up_to", ")", "\n", "\n", "# force verbose", "\n", "", "if", "not", "args", ".", "verbose", ":", "\n", "        ", "args", ".", "verbose", "=", "bool", "(", "args", ".", "step_by_step", ")", "\n", "\n", "# Sanity checks", "\n", "", "assert", "args", ".", "in_sentences", "\n", "assert", "args", ".", "in_actions", "\n", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.fake_parse.reduce_counter": [[130, 140], ["collections.Counter", "counts.items", "reducer"], "function", ["None"], ["", "def", "reduce_counter", "(", "counts", ",", "reducer", ")", ":", "\n", "    ", "\"\"\"\n    Returns a new counter from an existing one where keys have been mapped\n    to in  many-to-one fashion and counts added\n    \"\"\"", "\n", "new_counts", "=", "Counter", "(", ")", "\n", "for", "key", ",", "count", "in", "counts", ".", "items", "(", ")", ":", "\n", "        ", "new_key", "=", "reducer", "(", "key", ")", "\n", "new_counts", "[", "new_key", "]", "+=", "count", "\n", "", "return", "new_counts", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.fake_parse.restrict_action": [[142, 182], ["state_machine.get_valid_actions", "state_machine.get_top_of_stack", "rule_violation.update", "state_machine.get_top_of_stack", "pred_counts.update", "pred_counts.update", "pred_counts.update", "token.lower", "raw_action.split", "raw_action.split"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.get_valid_actions", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.get_top_of_stack", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.get_top_of_stack", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["", "def", "restrict_action", "(", "state_machine", ",", "raw_action", ",", "pred_counts", ",", "rule_violation", ")", ":", "\n", "\n", "# Get valid actions", "\n", "    ", "valid_actions", ",", "invalid_actions", "=", "state_machine", ".", "get_valid_actions", "(", ")", "\n", "\n", "# Fallback for constrained PRED actions", "\n", "if", "'PRED'", "in", "raw_action", ":", "\n", "        ", "if", "'PRED'", "not", "in", "valid_actions", ":", "\n", "# apply restrictions to predict actions", "\n", "# get valid predict actions", "\n", "            ", "valid_pred_actions", "=", "[", "\n", "a", "for", "a", "in", "valid_actions", "if", "'PRED'", "in", "a", "\n", "]", "\n", "if", "valid_pred_actions", "==", "[", "]", ":", "\n", "# no rule found for this token, try copy", "\n", "                ", "token", ",", "tokens", "=", "state_machine", ".", "get_top_of_stack", "(", ")", "\n", "if", "tokens", ":", "\n", "                    ", "token", "=", "\",\"", ".", "join", "(", "tokens", ")", "\n", "# reasign raw action", "\n", "", "raw_action", "=", "f'PRED({token.lower()})'", "\n", "pred_counts", ".", "update", "(", "[", "'token OOV'", "]", ")", "\n", "", "elif", "raw_action", "not", "in", "valid_pred_actions", ":", "\n", "# not found, get most common match", "\n", "# reasign raw action", "\n", "                ", "raw_action", "=", "valid_pred_actions", "[", "0", "]", "\n", "pred_counts", ".", "update", "(", "[", "'alignment OOV'", "]", ")", "\n", "", "else", ":", "\n", "                ", "pred_counts", ".", "update", "(", "[", "'matches'", "]", ")", "\n", "", "", "", "elif", "(", "\n", "(", "\n", "raw_action", "not", "in", "valid_actions", "and", "\n", "raw_action", ".", "split", "(", "'('", ")", "[", "0", "]", "not", "in", "valid_actions", "\n", ")", "\n", "or", "raw_action", "in", "invalid_actions", "\n", ")", ":", "\n", "# note-down rule violation", "\n", "        ", "token", ",", "_", "=", "state_machine", ".", "get_top_of_stack", "(", ")", "\n", "rule_violation", ".", "update", "(", "[", "raw_action", ".", "split", "(", "'('", ")", "[", "0", "]", "]", ")", "\n", "\n", "", "return", "raw_action", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.fake_parse.get_bio_from_machine": [[184, 204], ["raw_action.startswith", "raw_action.startswith", "state_machine.get_top_of_stack", "state_machine.get_top_of_stack", "tuple", "state_machine.get_top_of_stack", "Exception"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.get_top_of_stack", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.get_top_of_stack", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.get_top_of_stack"], ["", "def", "get_bio_from_machine", "(", "state_machine", ",", "raw_action", ")", ":", "\n", "    ", "annotations", "=", "{", "}", "\n", "if", "(", "\n", "raw_action", ".", "startswith", "(", "'PRED'", ")", "or", "\n", "raw_action", ".", "startswith", "(", "'ADDNODE'", ")", "or", "\n", "raw_action", "in", "[", "'COPY_SENSE01'", ",", "'COPY_LEMMA'", "]", "\n", ")", ":", "\n", "        ", "if", "raw_action", "==", "'COPY_SENSE01'", ":", "\n", "            ", "lemma", ",", "_", "=", "state_machine", ".", "get_top_of_stack", "(", "lemma", "=", "True", ")", "\n", "raw_action", "=", "f'PRED({lemma}-01)'", "\n", "", "elif", "raw_action", "==", "'COPY_LEMMA'", ":", "\n", "            ", "lemma", ",", "_", "=", "state_machine", ".", "get_top_of_stack", "(", "lemma", "=", "True", ")", "\n", "raw_action", "=", "f'PRED({lemma})'", "\n", "", "token", ",", "tokens", "=", "state_machine", ".", "get_top_of_stack", "(", "positions", "=", "True", ")", "\n", "tokens", "=", "tuple", "(", "tokens", ")", "if", "tokens", "else", "[", "token", "]", "\n", "for", "token", "in", "tokens", ":", "\n", "            ", "if", "token", "in", "annotations", ":", "\n", "                ", "raise", "Exception", "(", "'Overlapping annotations'", ")", "\n", "", "annotations", "[", "token", "]", "=", "raw_action", "\n", "", "", "return", "annotations", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.fake_parse.get_bio_tags": [[206, 222], ["enumerate", "bio_tags.append"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "get_bio_tags", "(", "state_machine", ",", "bio_alignments", ")", ":", "\n", "    ", "bio_tags", "=", "[", "]", "\n", "prev_label", "=", "None", "\n", "for", "idx", ",", "token", "in", "enumerate", "(", "state_machine", ".", "tokens", "[", ":", "-", "1", "]", ")", ":", "\n", "        ", "if", "idx", "in", "bio_alignments", ":", "\n", "            ", "if", "prev_label", "==", "bio_alignments", "[", "idx", "]", ":", "\n", "                ", "tag", "=", "f'I-{bio_alignments[idx]}'", "\n", "", "else", ":", "\n", "                ", "tag", "=", "f'B-{bio_alignments[idx]}'", "\n", "", "prev_label", "=", "bio_alignments", "[", "idx", "]", "\n", "", "else", ":", "\n", "            ", "prev_label", "=", "None", "\n", "tag", "=", "'O'", "\n", "", "bio_tags", ".", "append", "(", "(", "token", ",", "tag", ")", ")", "\n", "\n", "", "return", "bio_tags", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.fake_parse.main": [[362, 458], ["fake_parse.argument_parser", "transition_amr_parser.io.read_tokenized_sentences", "fake_parse.Logger", "transition_amr_parser.io.read_tokenized_sentences", "fake_parse.FakeAMRParser", "tqdm.tqdm", "transition_amr_parser.io.read_rule_stats", "rule_stats[].items", "len", "len", "transition_amr_parser.io.writer", "transition_amr_parser.io.writer", "enumerate", "fake_parse.FakeAMRParser.parse_sentence", "transition_amr_parser.amr.get_duplicate_edges", "getattr", "print", "print", "print", "print", "transition_amr_parser.io.writer.", "transition_amr_parser.io.writer.", "collections.Counter", "zip", "any", "transition_amr_parser.utils.yellow_font", "print", "print", "print", "transition_amr_parser.io.writer.", "transition_amr_parser.utils.yellow_font", "dict", "transition_amr_parser.io.writer.", "machine.amr.toJAMRString", "print"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.argument_parser", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.io.read_tokenized_sentences", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.io.read_tokenized_sentences", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.io.read_rule_stats", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.io.writer", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.io.writer", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.fake_parse.FakeAMRParser.parse_sentence", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.get_duplicate_edges", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.yellow_font", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.yellow_font", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.toJAMRString", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["", "", "", "", "", "def", "main", "(", ")", ":", "\n", "\n", "# Argument handling", "\n", "    ", "args", "=", "argument_parser", "(", ")", "\n", "\n", "# Get data", "\n", "sentences", "=", "read_tokenized_sentences", "(", "args", ".", "in_sentences", ",", "separator", "=", "args", ".", "separator", ")", "\n", "\n", "# Initialize logger/printer", "\n", "logger", "=", "Logger", "(", "\n", "step_by_step", "=", "args", ".", "step_by_step", ",", "\n", "clear_print", "=", "args", ".", "clear_print", ",", "\n", "pause_time", "=", "args", ".", "pause_time", ",", "\n", "verbose", "=", "args", ".", "verbose", "\n", ")", "\n", "\n", "# generate rules to restrict action space by stack content", "\n", "if", "args", ".", "action_rules_from_stats", ":", "\n", "        ", "rule_stats", "=", "read_rule_stats", "(", "args", ".", "action_rules_from_stats", ")", "\n", "actions_by_stack_rules", "=", "rule_stats", "[", "'possible_predicates'", "]", "\n", "for", "token", ",", "counter", "in", "rule_stats", "[", "'possible_predicates'", "]", ".", "items", "(", ")", ":", "\n", "            ", "actions_by_stack_rules", "[", "token", "]", "=", "Counter", "(", "counter", ")", "\n", "\n", "", "", "else", ":", "\n", "        ", "actions_by_stack_rules", "=", "None", "\n", "\n", "# Fake parser built from actions", "\n", "", "actions", "=", "read_tokenized_sentences", "(", "\n", "args", ".", "in_actions", ",", "\n", "separator", "=", "args", ".", "separator", "\n", ")", "\n", "assert", "len", "(", "sentences", ")", "==", "len", "(", "actions", ")", "\n", "parsing_model", "=", "FakeAMRParser", "(", "\n", "from_sent_act_pairs", "=", "zip", "(", "sentences", ",", "actions", ")", ",", "\n", "machine_type", "=", "args", ".", "machine_type", ",", "\n", "logger", "=", "logger", ",", "\n", "actions_by_stack_rules", "=", "actions_by_stack_rules", ",", "\n", "no_whitespace_in_actions", "=", "args", ".", "no_whitespace_in_actions", ",", "\n", "entity_rules", "=", "args", ".", "entity_rules", "\n", ")", "\n", "\n", "# Get output AMR writer", "\n", "if", "args", ".", "out_amr", ":", "\n", "        ", "amr_write", "=", "writer", "(", "args", ".", "out_amr", ")", "\n", "", "if", "args", ".", "out_bio_tags", ":", "\n", "        ", "bio_write", "=", "writer", "(", "args", ".", "out_bio_tags", ")", "\n", "\n", "# Loop over sentences", "\n", "", "for", "sent_idx", ",", "tokens", "in", "tqdm", "(", "enumerate", "(", "sentences", ")", ",", "desc", "=", "'parsing'", ")", ":", "\n", "\n", "# fast-forward until desired sentence number", "\n", "        ", "if", "args", ".", "offset", "and", "sent_idx", "<", "args", ".", "offset", ":", "\n", "            ", "continue", "\n", "\n", "# parse", "\n", "# NOTE: To simulate the real endpoint, input provided as a string of", "\n", "# whitespace separated tokens", "\n", "", "machine", ",", "bio_tags", "=", "parsing_model", ".", "parse_sentence", "(", "\" \"", ".", "join", "(", "tokens", ")", ")", "\n", "\n", "# sanity check annotations", "\n", "dupes", "=", "get_duplicate_edges", "(", "machine", ".", "amr", ")", "\n", "if", "args", ".", "sanity_check", "and", "any", "(", "dupes", ")", ":", "\n", "            ", "msg", "=", "yellow_font", "(", "'WARNING:'", ")", "\n", "print", "(", "f'{msg} duplicated edges in sent {sent_idx}'", ",", "end", "=", "' '", ")", "\n", "print", "(", "dict", "(", "dupes", ")", ")", "\n", "print", "(", "' '", ".", "join", "(", "machine", ".", "tokens", ")", ")", "\n", "\n", "# store output AMR", "\n", "", "if", "args", ".", "out_bio_tags", ":", "\n", "            ", "tag_str", "=", "'\\n'", ".", "join", "(", "[", "f'{to} {ta}'", "for", "to", ",", "ta", "in", "bio_tags", "]", ")", "\n", "tag_str", "+=", "'\\n\\n'", "\n", "bio_write", "(", "tag_str", ")", "\n", "", "if", "args", ".", "out_amr", ":", "\n", "            ", "try", ":", "\n", "                ", "amr_write", "(", "machine", ".", "amr", ".", "toJAMRString", "(", ")", ")", "\n", "", "except", "InvalidAMRError", "as", "exception", ":", "\n", "                ", "print", "(", "f'\\nFailed at sentence {sent_idx}\\n'", ")", "\n", "raise", "exception", "\n", "\n", "\n", "", "", "", "if", "(", "\n", "getattr", "(", "parsing_model", ",", "\"rule_violation\"", ")", "and", "\n", "parsing_model", ".", "rule_violation", "\n", ")", ":", "\n", "        ", "print", "(", "yellow_font", "(", "\"There were one or more action rule violations\"", ")", ")", "\n", "print", "(", "parsing_model", ".", "rule_violation", ")", "\n", "\n", "", "if", "args", ".", "action_rules_from_stats", ":", "\n", "        ", "print", "(", "\"Predict rules had following statistics\"", ")", "\n", "print", "(", "parsing_model", ".", "pred_counts", ")", "\n", "\n", "# close output writers", "\n", "", "if", "args", ".", "out_amr", ":", "\n", "        ", "amr_write", "(", ")", "\n", "", "if", "args", ".", "out_bio_tags", ":", "\n", "        ", "bio_write", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.AMRDataset.__init__": [[16, 23], ["isinstance", "utils.pad_batch", "isinstance", "utils.pad_batch", "isinstance", "utils.pad_batch", "isinstance", "utils.pad_batch"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.pad_batch", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.pad_batch", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.pad_batch", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.pad_batch"], [")", "\n", "from", "fairseq", ".", "tasks", "import", "FairseqTask", "\n", "\n", "\n", "def", "dummy_dictionary", "(", "vocab_size", ",", "prefix", "=", "'token_'", ")", ":", "\n", "    ", "d", "=", "Dictionary", "(", ")", "\n", "for", "i", "in", "range", "(", "vocab_size", ")", ":", "\n", "        ", "token", "=", "prefix", "+", "str", "(", "i", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.AMRDataset.__getitem__": [[24, 30], ["None"], "methods", ["None"], ["d", ".", "add_symbol", "(", "token", ")", "\n", "", "d", ".", "finalize", "(", "padding_factor", "=", "1", ")", "# don't add extra padding symbols", "\n", "return", "d", "\n", "\n", "\n", "", "def", "dummy_dataloader", "(", "\n", "samples", ",", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.AMRDataset.__len__": [[31, 33], ["len"], "methods", ["None"], ["padding_idx", "=", "1", ",", "\n", "eos_idx", "=", "2", ",", "\n", "batch_size", "=", "None", ",", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.Accuracy.__init__": [[235, 238], ["None"], "methods", ["None"], ["", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.Accuracy.add": [[239, 243], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.Accuracy.val": [[244, 249], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.Accuracy.__str__": [[250, 252], ["utils.Accuracy.val"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.Accuracy.val"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.Accuracy.reset": [[253, 256], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.Accuracy.data_as_tensor": [[257, 261], ["torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "numpy.array"], "methods", ["None"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.Accuracy.reset_from_tensor": [[262, 266], ["utils.Accuracy.reset", "tensor[].item", "tensor[].item"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.reset", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.__init__": [[270, 273], ["list", "sorted", "collections.Counter"], "methods", ["None"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.reset": [[274, 276], ["collections.Counter"], "methods", ["None"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.add": [[277, 279], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.data_as_tensor": [[280, 284], ["torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "numpy.array"], "methods", ["None"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.reset_from_tensor": [[285, 290], ["utils.ConfusionMatrix.reset", "enumerate", "enumerate", "tensor[].item"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.reset", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.__str__": [[291, 316], ["collections.Counter", "sum", "sorted", "collections.Counter.values", "sum", "utils.ConfusionMatrix.confusion_matrix[].values", "float", "float"], "methods", ["None"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.NoTokenizer.__init__": [[341, 343], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.NoTokenizer.__call__": [[344, 347], ["spacy.tokens.doc.Doc", "len"], "methods", ["None"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.to_scalar": [[38, 41], ["var.view().data.tolist", "var.view"], "function", ["None"], ["# add any missing data to samples", "\n", "", "for", "i", ",", "sample", "in", "enumerate", "(", "samples", ")", ":", "\n", "        ", "if", "'id'", "not", "in", "sample", ":", "\n", "            ", "sample", "[", "'id'", "]", "=", "i", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.argmax": [[43, 47], ["torch.max", "torch.max", "utils.to_scalar"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.to_scalar"], ["# create dataloader", "\n", "", "", "dataset", "=", "TestDataset", "(", "samples", ")", "\n", "dataloader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "dataset", ",", "\n", "batch_size", "=", "batch_size", ",", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.vectorize_words": [[49, 59], ["utils.vectorize_random_replace", "torch.LongTensor().cuda", "torch.LongTensor().cuda", "torch.LongTensor", "torch.LongTensor", "utils.vectorize_safe", "utils.vectorize_safe", "torch.LongTensor", "torch.LongTensor"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.vectorize_random_replace", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.vectorize_safe", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.vectorize_safe"], [")", "\n", "return", "iter", "(", "dataloader", ")", "\n", "\n", "\n", "", "def", "sequence_generator_setup", "(", ")", ":", "\n", "# construct dummy dictionary", "\n", "    ", "d", "=", "dummy_dictionary", "(", "vocab_size", "=", "2", ")", "\n", "\n", "eos", "=", "d", ".", "eos", "(", ")", "\n", "w1", "=", "4", "\n", "w2", "=", "5", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.vectorize": [[61, 64], ["None"], "function", ["None"], ["# construct source data", "\n", "src_tokens", "=", "torch", ".", "LongTensor", "(", "[", "[", "w1", ",", "w2", ",", "eos", "]", ",", "[", "w1", ",", "w2", ",", "eos", "]", "]", ")", "\n", "src_lengths", "=", "torch", ".", "LongTensor", "(", "[", "2", ",", "2", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.vectorize_safe": [[66, 69], ["word_dict.get"], "function", ["None"], ["unk", "=", "0.", "\n", "args", ".", "beam_probs", "=", "[", "\n", "# step 0:", "\n", "torch", ".", "FloatTensor", "(", "[", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.vectorize_random_replace": [[71, 81], ["enumerate", "enumerate", "word_dict.get", "random.random"], "function", ["None"], ["# sentence 1:", "\n", "[", "0.0", ",", "unk", ",", "0.9", ",", "0.1", "]", ",", "# beam 1", "\n", "[", "0.0", ",", "unk", ",", "0.9", ",", "0.1", "]", ",", "# beam 2", "\n", "# sentence 2:", "\n", "[", "0.0", ",", "unk", ",", "0.7", ",", "0.3", "]", ",", "\n", "[", "0.0", ",", "unk", ",", "0.7", ",", "0.3", "]", ",", "\n", "]", ")", ",", "\n", "# step 1:", "\n", "torch", ".", "FloatTensor", "(", "[", "\n", "# eos      w1   w2       prefix", "\n", "# sentence 1:", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.construct_dataset_train": [[83, 115], ["list", "list", "list", "list", "list", "list", "enumerate", "utils.vectorize_safe", "utils.vectorize_safe", "utils.vectorize_safe", "utils.vectorize_safe", "utils.AMRDataset", "tr.amr.tokens.copy", "list.append", "list.append", "list.append", "list.append", "list.append", "list.append", "tr.readAction", "torch.LongTensor().cuda", "torch.LongTensor().cuda", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor().cuda", "torch.LongTensor().cuda", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor().cuda", "torch.LongTensor().cuda", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor().cuda", "torch.LongTensor().cuda", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.vectorize_safe", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.vectorize_safe", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.vectorize_safe", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.vectorize_safe", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.copy", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.readAction"], ["[", "0.0", ",", "unk", ",", "0.9", ",", "0.1", "]", ",", "# w2: 0.1", "\n", "# sentence 2:", "\n", "[", "0.25", ",", "unk", ",", "0.35", ",", "0.4", "]", ",", "# w1: 0.7  (don't emit: w1 <eos>: 0.7*0.25)", "\n", "[", "0.00", ",", "unk", ",", "0.10", ",", "0.9", "]", ",", "# w2: 0.3", "\n", "]", ")", ",", "\n", "# step 2:", "\n", "torch", ".", "FloatTensor", "(", "[", "\n", "# eos      w1   w2       prefix", "\n", "# sentence 1:", "\n", "[", "0.0", ",", "unk", ",", "0.1", ",", "0.9", "]", ",", "# w2 w1: 0.1*0.9", "\n", "[", "0.6", ",", "unk", ",", "0.2", ",", "0.2", "]", ",", "# w2 w2: 0.1*0.1  (emit: w2 w2 <eos>: 0.1*0.1*0.6)", "\n", "# sentence 2:", "\n", "[", "0.60", ",", "unk", ",", "0.4", ",", "0.00", "]", ",", "# w1 w2: 0.7*0.4  (emit: w1 w2 <eos>: 0.7*0.4*0.6)", "\n", "[", "0.01", ",", "unk", ",", "0.0", ",", "0.99", "]", ",", "# w2 w2: 0.3*0.9", "\n", "]", ")", ",", "\n", "# step 3:", "\n", "torch", ".", "FloatTensor", "(", "[", "\n", "# eos      w1   w2       prefix", "\n", "# sentence 1:", "\n", "[", "1.0", ",", "unk", ",", "0.0", ",", "0.0", "]", ",", "# w2 w1 w2: 0.1*0.9*0.9  (emit: w2 w1 w2 <eos>: 0.1*0.9*0.9*1.0)", "\n", "[", "1.0", ",", "unk", ",", "0.0", ",", "0.0", "]", ",", "# w2 w1 w1: 0.1*0.9*0.1  (emit: w2 w1 w1 <eos>: 0.1*0.9*0.1*1.0)", "\n", "# sentence 2:", "\n", "[", "0.1", ",", "unk", ",", "0.5", ",", "0.4", "]", ",", "# w2 w2 w2: 0.3*0.9*0.99  (emit: w2 w2 w2 <eos>: 0.3*0.9*0.99*0.1)", "\n", "[", "1.0", ",", "unk", ",", "0.0", ",", "0.0", "]", ",", "# w1 w2 w1: 0.7*0.4*0.4  (emit: w1 w2 w1 <eos>: 0.7*0.4*0.4*1.0)", "\n", "]", ")", ",", "\n", "]", "\n", "\n", "task", "=", "TestTranslationTask", ".", "setup_task", "(", "args", ",", "d", ",", "d", ")", "\n", "model", "=", "task", ".", "build_model", "(", "args", ")", "\n", "tgt_dict", "=", "task", ".", "target_dictionary", "\n", "\n", "return", "tgt_dict", ",", "w1", ",", "w2", ",", "src_tokens", ",", "src_lengths", ",", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.xavier_init": [[117, 122], ["torch.FloatTensor", "torch.FloatTensor", "torch.init.xavier_normal_", "t.cuda.cuda"], "function", ["None"], ["", "class", "TestDataset", "(", "torch", ".", "utils", ".", "data", ".", "Dataset", ")", ":", "\n", "\n", "    ", "def", "__init__", "(", "self", ",", "data", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "data", "=", "data", "\n", "self", ".", "sizes", "=", "None", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_embedding": [[124, 128], ["numpy.sqrt", "torch.init.uniform_", "input_embedding.size"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "data", "[", "index", "]", "\n", "\n", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "data", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_zero": [[130, 134], ["torch.init.zeros_", "input_linear.bias.data.zero_"], "function", ["None"], ["\n", "", "", "class", "TestTranslationTask", "(", "FairseqTask", ")", ":", "\n", "\n", "    ", "def", "__init__", "(", "self", ",", "args", ",", "src_dict", ",", "tgt_dict", ",", "model", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "args", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_linear": [[136, 145], ["torch.init.orthogonal_", "torch.init.xavier_normal_", "torch.init.zeros_"], "function", ["None"], ["self", ".", "tgt_dict", "=", "tgt_dict", "\n", "self", ".", "model", "=", "model", "\n", "\n", "", "@", "classmethod", "\n", "def", "setup_task", "(", "cls", ",", "args", ",", "src_dict", "=", "None", ",", "tgt_dict", "=", "None", ",", "model", "=", "None", ")", ":", "\n", "        ", "return", "cls", "(", "args", ",", "src_dict", ",", "tgt_dict", ",", "model", ")", "\n", "\n", "", "def", "build_model", "(", "self", ",", "args", ")", ":", "\n", "        ", "return", "TestModel", ".", "build_model", "(", "args", ",", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_lstm": [[147, 164], ["range", "eval", "torch.init.orthogonal_", "eval", "torch.init.orthogonal_", "range", "eval", "torch.init.zeros_", "eval", "torch.init.zeros_", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval"], ["def", "source_dictionary", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "src_dict", "\n", "\n", "", "@", "property", "\n", "def", "target_dictionary", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "tgt_dict", "\n", "\n", "\n", "", "", "class", "TestModel", "(", "FairseqEncoderDecoderModel", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "encoder", ",", "decoder", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "encoder", ",", "decoder", ")", "\n", "\n", "", "@", "classmethod", "\n", "def", "build_model", "(", "cls", ",", "args", ",", "task", ")", ":", "\n", "        ", "encoder", "=", "TestEncoder", "(", "args", ",", "task", ".", "source_dictionary", ")", "\n", "decoder", "=", "TestIncrementalDecoder", "(", "args", ",", "task", ".", "target_dictionary", ")", "\n", "return", "cls", "(", "encoder", ",", "decoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.initialize_lstm_cell": [[166, 183], ["eval", "numpy.sqrt", "torch.init.uniform_", "eval", "numpy.sqrt", "torch.init.uniform_", "eval", "torch.init.zeros_", "eval", "torch.init.zeros_", "eval.size", "eval.size", "eval.size", "eval.size"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "", "class", "TestEncoder", "(", "FairseqEncoder", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "args", ",", "dictionary", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ")", "\n", "self", ".", "args", "=", "args", "\n", "\n", "", "def", "forward", "(", "self", ",", "src_tokens", ",", "src_lengths", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "src_tokens", "\n", "\n", "", "def", "reorder_encoder_out", "(", "self", ",", "encoder_out", ",", "new_order", ")", ":", "\n", "        ", "return", "encoder_out", ".", "index_select", "(", "0", ",", "new_order", ")", "\n", "\n", "\n", "", "", "class", "TestIncrementalDecoder", "(", "FairseqIncrementalDecoder", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "args", ",", "dictionary", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "dictionary", ")", "\n", "assert", "hasattr", "(", "args", ",", "'beam_probs'", ")", "or", "hasattr", "(", "args", ",", "'probs'", ")", "\n", "args", ".", "max_decoder_positions", "=", "getattr", "(", "args", ",", "'max_decoder_positions'", ",", "100", ")", "\n", "self", ".", "args", "=", "args", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.make_efficient": [[185, 197], ["labelO.contiguous.contiguous().cuda", "labelA.contiguous.contiguous().cuda", "action.contiguous.contiguous().cuda", "pred.contiguous.contiguous().cuda", "labelO.contiguous.contiguous", "labelA.contiguous.contiguous", "action.contiguous.contiguous", "pred.contiguous.contiguous", "labelO.contiguous.contiguous", "labelA.contiguous.contiguous", "action.contiguous.contiguous", "pred.contiguous.contiguous"], "function", ["None"], ["", "def", "forward", "(", "self", ",", "prev_output_tokens", ",", "encoder_out", "=", "None", ",", "incremental_state", "=", "None", ")", ":", "\n", "        ", "if", "incremental_state", "is", "not", "None", ":", "\n", "            ", "prev_output_tokens", "=", "prev_output_tokens", "[", ":", ",", "-", "1", ":", "]", "\n", "", "bbsz", "=", "prev_output_tokens", ".", "size", "(", "0", ")", "\n", "vocab", "=", "len", "(", "self", ".", "dictionary", ")", "\n", "src_len", "=", "encoder_out", ".", "size", "(", "1", ")", "\n", "tgt_len", "=", "prev_output_tokens", ".", "size", "(", "1", ")", "\n", "\n", "# determine number of steps", "\n", "if", "incremental_state", "is", "not", "None", ":", "\n", "# cache step number", "\n", "            ", "step", "=", "utils", ".", "get_incremental_state", "(", "self", ",", "incremental_state", ",", "'step'", ")", "\n", "if", "step", "is", "None", ":", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.reverse_sequence": [[199, 204], ["torch.LongTensor", "torch.LongTensor", "seq.index_select", "reversed_idx.cuda.cuda", "reversed", "range", "seq.size"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["", "utils", ".", "set_incremental_state", "(", "self", ",", "incremental_state", ",", "'step'", ",", "step", "+", "1", ")", "\n", "steps", "=", "[", "step", "]", "\n", "", "else", ":", "\n", "            ", "steps", "=", "list", "(", "range", "(", "tgt_len", ")", ")", "\n", "\n", "# define output in terms of raw probs", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.pad_batch": [[206, 216], ["max", "enumerate", "torch.cat", "torch.cat", "torch.nn.functional.pad", "len", "batch_list[].unsqueeze", "sent.size", "ex.size", "batch_list[].size"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], ["            ", "assert", "self", ".", "args", ".", "probs", ".", "dim", "(", ")", "==", "3", ",", "'expected probs to have size bsz*steps*vocab'", "\n", "probs", "=", "self", ".", "args", ".", "probs", ".", "index_select", "(", "1", ",", "torch", ".", "LongTensor", "(", "steps", ")", ")", "\n", "", "else", ":", "\n", "            ", "probs", "=", "torch", ".", "FloatTensor", "(", "bbsz", ",", "len", "(", "steps", ")", ",", "vocab", ")", ".", "zero_", "(", ")", "\n", "for", "i", ",", "step", "in", "enumerate", "(", "steps", ")", ":", "\n", "# args.beam_probs gives the probability for every vocab element,", "\n", "# starting with eos, then unknown, and then the rest of the vocab", "\n", "                ", "if", "step", "<", "len", "(", "self", ".", "args", ".", "beam_probs", ")", ":", "\n", "                    ", "probs", "[", ":", ",", "i", ",", "self", ".", "dictionary", ".", "eos", "(", ")", ":", "]", "=", "self", ".", "args", ".", "beam_probs", "[", "step", "]", "\n", "", "else", ":", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.pad_batch_tokens": [[218, 227], ["max", "enumerate", "len", "len", "range", "batch_list[].append"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["\n", "# random attention", "\n", "", "", "", "attn", "=", "torch", ".", "rand", "(", "bbsz", ",", "tgt_len", ",", "src_len", ")", "\n", "\n", "dev", "=", "prev_output_tokens", ".", "device", "\n", "return", "probs", ".", "to", "(", "dev", ")", ",", "attn", ".", "to", "(", "dev", ")", "\n", "\n", "", "def", "get_normalized_probs", "(", "self", ",", "net_output", ",", "log_probs", ",", "_", ")", ":", "\n", "# the decoder returns probabilities directly", "\n", "        ", "probs", "=", "net_output", "[", "0", "]", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.set_seed": [[229, 231], ["random.seed"], "function", ["None"], ["            ", "return", "probs", ".", "log", "(", ")", "\n", "", "else", ":", "\n", "            ", "return", "probs", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.yellow_font": [[318, 320], ["None"], "function", ["None"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.print_log": [[322, 326], ["print", "str().split", "str", "datetime.datetime.now"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.smatch_wrapper": [[328, 338], ["subprocess.Popen", "smatch_stdout[].split", "line.decode"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.decode"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.__init__": [[415, 442], ["amr_state_machine.get_action_indexer", "os.path.isfile", "transition_amr_parser.io.read_rule_stats", "amr_state_machine.machine_generator", "amr_state_machine.machine_generator"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.get_action_indexer", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.io.read_rule_stats", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.machine_generator", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.machine_generator"], ["def", "__init__", "(", "self", ",", "src_dict", ",", "tgt_dict", ",", "machine_type", ",", "machine_rules", "=", "None", ",", "\n", "entity_rules", "=", "None", ",", "post_process", "=", "False", ")", ":", "\n", "\n", "# Get all actions indexed by prefix", "\n", "# TODO: This code can be inline here now that we have __init__/reset", "\n", "        ", "self", ".", "action_indexer", "=", "get_action_indexer", "(", "tgt_dict", ".", "symbols", ")", "\n", "\n", "# Load rule stats if provided", "\n", "if", "machine_rules", "is", "not", "None", ":", "\n", "            ", "assert", "os", ".", "path", ".", "isfile", "(", "machine_rules", ")", "\n", "rule_stats", "=", "read_rule_stats", "(", "machine_rules", ")", "\n", "# self.state_machine = StateMachine(folder)", "\n", "self", ".", "get_new_state_machine", "=", "machine_generator", "(", "\n", "rule_stats", "[", "'possible_predicates'", "]", ",", "\n", "entity_rules", "=", "entity_rules", ",", "\n", "post_process", "=", "post_process", "\n", ")", "\n", "", "else", ":", "\n", "            ", "assert", "machine_type", "!=", "'AMR'", ",", "\"AMR machine expects --machine-rules\"", "\n", "rule_stats", "=", "None", "\n", "self", ".", "get_new_state_machine", "=", "machine_generator", "(", "None", ")", "\n", "\n", "# store some variables", "\n", "", "self", ".", "src_dict", "=", "src_dict", "\n", "self", ".", "tgt_dict", "=", "tgt_dict", "\n", "self", ".", "machine_type", "=", "machine_type", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.reset": [[443, 484], ["range", "src_tokens.unsqueeze().repeat", "amr_state_machine.StateMachineBatch.update_masks", "amr_state_machine.StateMachineBatch.machines.append", "amr_state_machine.StateMachineBatch.left_pad.append", "src_tokens[].cpu().numpy", "amr_state_machine.StateMachineBatch.get_new_state_machine", "src_tokens.unsqueeze", "len", "torch.ones_like", "amr_state_machine.StateMachineBatch.tgt_dict.pad", "torch.ones_like", "amr_state_machine.StateMachineBatch.tgt_dict.pad", "src_tokens[].cpu"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.stack_state_machine.StackStateMachine.update_masks", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad"], ["", "def", "reset", "(", "self", ",", "src_tokens", ",", "src_lengths", ",", "max_tgt_len", ",", "orig_tokens", "=", "None", ")", ":", "\n", "        ", "'''\n        Reset state of state machine and start with new sentence\n        '''", "\n", "\n", "batch_size", ",", "max_src_len", "=", "src_tokens", ".", "shape", "\n", "\n", "# time step counter", "\n", "self", ".", "step_index", "=", "0", "\n", "\n", "# Watch out, these two variables need to be reorderd in reorder_state!", "\n", "self", ".", "left_pad", "=", "[", "]", "\n", "self", ".", "machines", "=", "[", "]", "\n", "for", "batch_idx", "in", "range", "(", "batch_size", ")", ":", "\n", "\n", "# Get tokens and sentence length", "\n", "            ", "sent_len", "=", "src_lengths", "[", "batch_idx", "]", "\n", "if", "orig_tokens", "is", "None", ":", "\n", "                ", "word_idx", "=", "src_tokens", "[", "batch_idx", ",", "-", "sent_len", ":", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "tokens", "=", "[", "self", ".", "src_dict", "[", "x", "]", "for", "x", "in", "word_idx", "]", "\n", "", "else", ":", "\n", "                ", "tokens", "=", "orig_tokens", "[", "batch_idx", "]", "\n", "assert", "len", "(", "tokens", ")", "==", "sent_len", "\n", "\n", "# intialize state machine batch for size 1", "\n", "", "self", ".", "machines", ".", "append", "(", "self", ".", "get_new_state_machine", "(", "\n", "tokens", ",", "\n", "machine_type", "=", "self", ".", "machine_type", "\n", ")", ")", "\n", "\n", "# store left pad size to be used in mask creation", "\n", "self", ".", "left_pad", ".", "append", "(", "max_src_len", "-", "sent_len", ")", "\n", "\n", "# these have the same info as buffer and stack but in stack-transformer", "\n", "# form (batch_size * beam_size, src_len, tgt_len)", "\n", "", "dummy", "=", "src_tokens", ".", "unsqueeze", "(", "2", ")", ".", "repeat", "(", "1", ",", "1", ",", "max_tgt_len", ")", "\n", "self", ".", "memory", "=", "(", "torch", ".", "ones_like", "(", "dummy", ")", "*", "self", ".", "tgt_dict", ".", "pad", "(", ")", ")", ".", "float", "(", ")", "\n", "self", ".", "memory_pos", "=", "(", "\n", "torch", ".", "ones_like", "(", "dummy", ")", "*", "self", ".", "tgt_dict", ".", "pad", "(", ")", "\n", ")", ".", "float", "(", ")", "\n", "self", ".", "update_masks", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.get_active_logits": [[485, 519], ["set", "torch.zeros", "range", "list", "len", "len", "len", "set", "amr_state_machine.StateMachineBatch.machines[].get_valid_actions", "enumerate", "amr_state_machine.StateMachineBatch.action_indexer", "amr_state_machine.StateMachineBatch.action_indexer", "list"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.get_valid_actions"], ["", "def", "get_active_logits", "(", "self", ")", ":", "\n", "\n", "# Collect active indices for the entire batch", "\n", "        ", "batch_active_logits", "=", "set", "(", ")", "\n", "shape", "=", "(", "len", "(", "self", ".", "machines", ")", ",", "1", ",", "len", "(", "self", ".", "tgt_dict", ".", "symbols", ")", ")", "\n", "logits_mask", "=", "torch", ".", "zeros", "(", "shape", ",", "dtype", "=", "torch", ".", "int16", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "machines", ")", ")", ":", "\n", "            ", "if", "self", ".", "machines", "[", "i", "]", ".", "is_closed", ":", "\n", "# TODO: Change this to <pad> (will mess with decoder)", "\n", "                ", "expanded_valid_indices", "=", "set", "(", "[", "self", ".", "tgt_dict", ".", "indices", "[", "'</s>'", "]", "]", ")", "\n", "", "else", ":", "\n", "                ", "valid_actions", ",", "invalid_actions", "=", "self", ".", "machines", "[", "i", "]", ".", "get_valid_actions", "(", ")", "\n", "expanded_valid_indices", "=", "(", "\n", "self", ".", "action_indexer", "(", "valid_actions", ")", "\n", "-", "self", ".", "action_indexer", "(", "invalid_actions", ")", "\n", ")", "\n", "", "batch_active_logits", "|=", "expanded_valid_indices", "\n", "logits_mask", "[", "i", ",", "0", ",", "list", "(", "expanded_valid_indices", ")", "]", "=", "1", "\n", "", "batch_active_logits", "=", "list", "(", "batch_active_logits", ")", "\n", "\n", "# FIXME: Entropic fix to avoid <unk>. Fix at oracle/fairseq level", "\n", "# needed", "\n", "batch_active_logits", "=", "[", "\n", "idx", "\n", "for", "idx", "in", "batch_active_logits", "\n", "if", "idx", "!=", "self", ".", "tgt_dict", ".", "indices", "[", "'<unk>'", "]", "\n", "]", "\n", "\n", "# store as indices mapping", "\n", "logits_indices", "=", "{", "\n", "key", ":", "idx", "\n", "for", "idx", ",", "key", "in", "enumerate", "(", "batch_active_logits", ")", "\n", "}", "\n", "return", "logits_indices", ",", "logits_mask", "[", ":", ",", ":", ",", "batch_active_logits", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update": [[520, 543], ["len", "range", "amr_state_machine.StateMachineBatch.update_masks", "len", "amr_state_machine.StateMachineBatch.machines[].applyAction"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.stack_state_machine.StackStateMachine.update_masks", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.DepParsingStateMachine.applyAction"], ["", "def", "update", "(", "self", ",", "action_batch", ")", ":", "\n", "\n", "# sanity check", "\n", "        ", "batch_size", "=", "len", "(", "action_batch", ")", "\n", "assert", "batch_size", "==", "len", "(", "self", ".", "machines", ")", "\n", "# batch_size, num_actions = log_probabilities.shape", "\n", "# assert batch_size == len(self.machines)", "\n", "# FIXME: Decode adds extra symbols? This seem to be appended but this", "\n", "# is a dangerous behaviour", "\n", "# if num_actions != len(self.tgt_dict.symbols):", "\n", "#    import ipdb; ipdb.set_trace(context=30)", "\n", "#    pass", "\n", "\n", "# execute most probable valid action and return masked probabilities", "\n", "# for each sentence in the batch", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "self", ".", "machines", "[", "i", "]", ".", "applyAction", "(", "action_batch", "[", "i", "]", ")", "\n", "\n", "# increase action counter", "\n", "", "self", ".", "step_index", "+=", "1", "\n", "\n", "# update state expressed as masks", "\n", "self", ".", "update_masks", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update_masks": [[544, 606], ["enumerate", "NotImplementedError", "machine.get_buffer_stack_copy", "amr_state_machine.StateMachineBatch.left_pad[].item", "numpy.arange", "torch.tensor().float", "numpy.arange", "torch.tensor().float", "len", "len", "numpy.array", "len", "len", "torch.tensor", "numpy.array", "len", "len", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.DepParsingStateMachine.get_buffer_stack_copy", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item"], ["", "def", "update_masks", "(", "self", ",", "add_padding", "=", "0", ")", ":", "\n", "\n", "# basis is all padded", "\n", "        ", "if", "add_padding", ":", "\n", "            ", "raise", "NotImplementedError", "(", ")", "\n", "# Need to concatenate extra space", "\n", "\n", "", "for", "sent_index", ",", "machine", "in", "enumerate", "(", "self", ".", "machines", ")", ":", "\n", "\n", "# if machine is closed stop here", "\n", "            ", "if", "machine", ".", "is_closed", ":", "\n", "                ", "continue", "\n", "\n", "# Get machines buffer and stack compatible with AMR machine", "\n", "# get legacy indexing of buffer and stack from function", "\n", "", "machine_buffer", ",", "machine_stack", "=", "machine", ".", "get_buffer_stack_copy", "(", ")", "\n", "\n", "# Reset mask to all non pad elements as being in deleted state", "\n", "pad", "=", "self", ".", "left_pad", "[", "sent_index", "]", ".", "item", "(", ")", "\n", "self", ".", "memory", "[", "sent_index", ",", "pad", ":", ",", "self", ".", "step_index", "]", "=", "5", "\n", "self", ".", "memory_pos", "[", "sent_index", ",", "pad", ":", ",", "self", ".", "step_index", "]", "=", "0", "\n", "\n", "# Set buffer elements taking into account padding", "\n", "if", "machine_buffer", ":", "\n", "\n", "                ", "indices", "=", "np", ".", "array", "(", "machine_buffer", ")", "-", "1", "+", "pad", "\n", "indices", "[", "indices", "==", "-", "1", "-", "1", "+", "pad", "]", "=", "len", "(", "machine", ".", "tokens", ")", "-", "1", "+", "pad", "\n", "\n", "# update masks", "\n", "buffer_pos", "=", "np", ".", "arange", "(", "len", "(", "machine_buffer", ")", ")", "\n", "positions", "=", "len", "(", "machine_buffer", ")", "-", "buffer_pos", "-", "1", "\n", "self", ".", "memory", "[", "sent_index", ",", "indices", ",", "self", ".", "step_index", "]", "=", "3", "\n", "self", ".", "memory_pos", "[", "sent_index", ",", "indices", ",", "self", ".", "step_index", "]", "=", "torch", ".", "tensor", "(", "positions", ",", "device", "=", "self", ".", "memory", ".", "device", ")", ".", "float", "(", ")", "\n", "\n", "# Set stack elements taking into account padding", "\n", "", "if", "machine_stack", ":", "\n", "\n", "                ", "indices", "=", "np", ".", "array", "(", "machine_stack", ")", "-", "1", "+", "pad", "\n", "# index of root in stack, if there is", "\n", "root_in_stack_idx", "=", "(", "indices", "==", "-", "1", "-", "1", "+", "pad", ")", ".", "nonzero", "(", ")", "[", "0", "]", "\n", "indices", "[", "root_in_stack_idx", "]", "=", "len", "(", "machine", ".", "tokens", ")", "-", "1", "+", "pad", "\n", "\n", "# update masks", "\n", "stack_pos", "=", "np", ".", "arange", "(", "len", "(", "machine_stack", ")", ")", "\n", "positions", "=", "len", "(", "machine_stack", ")", "-", "stack_pos", "-", "1", "\n", "self", ".", "memory", "[", "sent_index", ",", "indices", ",", "self", ".", "step_index", "]", "=", "4", "\n", "# FIXME: This is a BUG in preprocessing by which", "\n", "# shifted ROOT is considered deleted", "\n", "# update masks", "\n", "#print(sent_index,machine_stack)", "\n", "\n", "self", ".", "memory", "[", "\n", "sent_index", ",", "\n", "indices", "[", "root_in_stack_idx", "]", ",", "\n", "self", ".", "step_index", "\n", "]", "=", "5", "\n", "\n", "\n", "self", ".", "memory_pos", "[", "sent_index", ",", "indices", ",", "self", ".", "step_index", "]", "=", "torch", ".", "tensor", "(", "positions", ",", "device", "=", "self", ".", "memory", ".", "device", ")", ".", "float", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.reoder_machine": [[607, 627], ["set", "reorder_state.cpu().tolist", "reorder_state.cpu", "new_machines.append", "new_left_pad.append", "new_machines.append", "new_left_pad.append", "set.add", "copy.deepcopy"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.add"], ["", "", "", "def", "reoder_machine", "(", "self", ",", "reorder_state", ")", ":", "\n", "        ", "\"\"\"Reorder/eliminate machines during decoding\"\"\"", "\n", "\n", "new_machines", "=", "[", "]", "\n", "new_left_pad", "=", "[", "]", "\n", "used_indices", "=", "set", "(", ")", "\n", "for", "i", "in", "reorder_state", ".", "cpu", "(", ")", ".", "tolist", "(", ")", ":", "\n", "            ", "if", "i", "in", "used_indices", ":", "\n", "# If a machine is duplicated we need to deep copy", "\n", "                ", "new_machines", ".", "append", "(", "deepcopy", "(", "self", ".", "machines", "[", "i", "]", ")", ")", "\n", "new_left_pad", ".", "append", "(", "self", ".", "left_pad", "[", "i", "]", ")", "\n", "", "else", ":", "\n", "                ", "new_machines", ".", "append", "(", "self", ".", "machines", "[", "i", "]", ")", "\n", "new_left_pad", ".", "append", "(", "self", ".", "left_pad", "[", "i", "]", ")", "\n", "used_indices", ".", "add", "(", "i", ")", "\n", "", "", "self", ".", "machines", "=", "new_machines", "\n", "self", ".", "left_pad", "=", "new_left_pad", "\n", "\n", "self", ".", "memory", "=", "self", ".", "memory", "[", "reorder_state", ",", ":", ",", ":", "]", "\n", "self", ".", "memory_pos", "=", "self", ".", "memory_pos", "[", "reorder_state", ",", ":", ",", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.white_background": [[39, 41], ["None"], "function", ["None"], ["def", "white_background", "(", "string", ")", ":", "\n", "    ", "return", "\"\\033[107m%s\\033[0m\"", "%", "string", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.red_background": [[43, 45], ["None"], "function", ["None"], ["", "def", "red_background", "(", "string", ")", ":", "\n", "    ", "return", "\"\\033[101m%s\\033[0m\"", "%", "string", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.black_font": [[47, 49], ["None"], "function", ["None"], ["", "def", "black_font", "(", "string", ")", ":", "\n", "    ", "return", "\"\\033[30m%s\\033[0m\"", "%", "string", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.yellow_font": [[51, 53], ["None"], "function", ["None"], ["", "def", "yellow_font", "(", "string", ")", ":", "\n", "    ", "return", "\"\\033[93m%s\\033[0m\"", "%", "string", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.stack_style": [[55, 57], ["amr_state_machine.black_font", "amr_state_machine.white_background"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.stack_state_machine.black_font", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.stack_state_machine.white_background"], ["", "def", "stack_style", "(", "string", ")", ":", "\n", "    ", "return", "black_font", "(", "white_background", "(", "string", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.ordered_exit": [[59, 62], ["print", "exit"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["", "def", "ordered_exit", "(", "signum", ",", "frame", ")", ":", "\n", "    ", "print", "(", "\"\\nStopped by user\\n\"", ")", "\n", "exit", "(", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.get_word_states": [[64, 90], ["amr_state_machine.get_buffer_stack_copy", "range", "len", "reversed", "memory.append", "memory_position.append", "len", "buffer.index", "memory.append", "memory_position.append", "memory.append", "memory_position.append", "len", "stack.index"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.DepParsingStateMachine.get_buffer_stack_copy", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.index", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.index"], ["", "def", "get_word_states", "(", "amr_state_machine", ",", "sent_tokens", ",", "indices", "=", "[", "3", ",", "4", ",", "5", "]", ")", ":", "\n", "\n", "# get legacy indexing of buffer and stack from function", "\n", "    ", "buffer", ",", "stack", "=", "amr_state_machine", ".", "get_buffer_stack_copy", "(", ")", "\n", "# translate to sane indexing", "\n", "buffer", "=", "[", "\n", "i", "-", "1", "if", "i", "!=", "-", "1", "else", "len", "(", "sent_tokens", ")", "-", "1", "\n", "for", "i", "in", "reversed", "(", "buffer", ")", "\n", "]", "\n", "stack", "=", "[", "i", "-", "1", "for", "i", "in", "stack", "]", "\n", "\n", "# translate to word states", "\n", "memory", "=", "[", "]", "\n", "memory_position", "=", "[", "]", "\n", "for", "idx", "in", "range", "(", "len", "(", "sent_tokens", ")", ")", ":", "\n", "        ", "if", "idx", "in", "buffer", ":", "\n", "            ", "memory", ".", "append", "(", "indices", "[", "0", "]", ")", "\n", "memory_position", ".", "append", "(", "buffer", ".", "index", "(", "idx", ")", ")", "\n", "", "elif", "idx", "in", "stack", ":", "\n", "            ", "memory", ".", "append", "(", "indices", "[", "1", "]", ")", "\n", "memory_position", ".", "append", "(", "len", "(", "stack", ")", "-", "stack", ".", "index", "(", "idx", ")", "-", "1", ")", "\n", "", "else", ":", "\n", "            ", "memory", ".", "append", "(", "indices", "[", "2", "]", ")", "\n", "memory_position", ".", "append", "(", "0", ")", "\n", "\n", "", "", "return", "memory", ",", "memory_position", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.argument_parser": [[96, 155], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["def", "argument_parser", "(", ")", ":", "\n", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'AMR parser'", ")", "\n", "# Multiple input parameters", "\n", "parser", ".", "add_argument", "(", "\n", "\"--in-sentences\"", ",", "\n", "help", "=", "\"file space with carriare return separated sentences\"", ",", "\n", "type", "=", "str", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--in-actions\"", ",", "\n", "help", "=", "\"file space with carriare return separated sentences\"", ",", "\n", "type", "=", "str", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--in-rule-stats\"", ",", "\n", "help", "=", "\"rule statistics computed by the state machine\"", ",", "\n", "type", "=", "str", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--out-word-states\"", ",", "\n", "help", "=", "\"stack-transformer word states\"", ",", "\n", "type", "=", "str", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--out-valid_actions\"", ",", "\n", "help", "=", "\"stack-transformer word states\"", ",", "\n", "type", "=", "str", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--offset\"", ",", "\n", "help", "=", "\"start at given sentence number (starts at zero)\"", ",", "\n", "type", "=", "int", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--verbose\"", ",", "\n", "help", "=", "\"plot information\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "default", "=", "False", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--step-by-step\"", ",", "\n", "help", "=", "\"pause after each action\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "default", "=", "False", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--pause-time\"", ",", "\n", "help", "=", "\"pause time after each action\"", ",", "\n", "default", "=", "0", ",", "\n", "type", "=", "float", "\n", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "if", "args", ".", "step_by_step", "or", "args", ".", "pause_time", ":", "\n", "# It is assumed that we want verbosity in this case", "\n", "        ", "args", ".", "verbose", "=", "True", "\n", "\n", "", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.h5_writer": [[157, 187], ["h5py.File", "h5py.File.create_group", "enumerate", "h5py.File.close", "fid.create_group.create_dataset", "str"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.gold_miner.GoldMiner.close"], ["", "def", "h5_writer", "(", "file_path", ")", ":", "\n", "\n", "    ", "import", "h5py", "\n", "fid", "=", "h5py", ".", "File", "(", "file_path", ",", "'w'", ",", "libver", "=", "'latest'", ")", "\n", "sent_idx", "=", "0", "\n", "\n", "def", "append_data", "(", "content", "=", "None", ")", ":", "\n", "        ", "\"\"\"writes to open file\"\"\"", "\n", "\n", "# close file", "\n", "if", "content", "is", "None", ":", "\n", "            ", "fid", ".", "close", "(", ")", "\n", "return", "\n", "\n", "# add content for a new sentence", "\n", "", "nonlocal", "sent_idx", "\n", "sent_group", "=", "fid", ".", "create_group", "(", "f'sentence-{sent_idx}'", ")", "\n", "for", "idx", ",", "arr", "in", "enumerate", "(", "content", ")", ":", "\n", "            ", "sent_group", ".", "create_dataset", "(", "\n", "str", "(", "idx", ")", ",", "\n", "data", "=", "arr", ",", "\n", "shape", "=", "arr", ".", "shape", ",", "\n", "chunks", "=", "arr", ".", "shape", ",", "\n", "compression", "=", "'gzip'", ",", "\n", "compression_opts", "=", "9", "\n", ")", "\n", "\n", "", "sent_idx", "=", "sent_idx", "+", "1", "\n", "\n", "", "return", "append_data", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.get_valid_actions": [[189, 234], ["amr_state_machine.get_valid_actions", "stats[].update", "numpy.array", "amr_state_machine.get_top_of_stack", "gold_action.startswith", "stats[].update", "valid_actions.append", "len", "stats[].update", "valid_actions.append", "valid_actions.extend", "len", "action_list.index"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.get_valid_actions", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.AMRStateMachine.get_top_of_stack", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.index"], ["", "def", "get_valid_actions", "(", "action_list", ",", "amr_state_machine", ",", "train_rule_stats", ",", "\n", "action_by_basic", ",", "gold_action", ",", "stats", ")", ":", "\n", "# Get basic actions", "\n", "    ", "valid_basic_actions", ",", "invalid_actions", "=", "amr_state_machine", ".", "get_valid_actions", "(", ")", "\n", "\n", "# Expand non-pred actions", "\n", "valid_actions", "=", "[", "\n", "b", "\n", "for", "a", "in", "valid_basic_actions", "if", "a", "!=", "'PRED'", "\n", "for", "b", "in", "action_by_basic", "[", "a", "]", "\n", "]", "\n", "\n", "# constrain PRED actions by train stats", "\n", "if", "'PRED'", "in", "valid_basic_actions", ":", "\n", "\n", "# Get tokens at top of the stack", "\n", "        ", "token", ",", "merged_tokens", "=", "amr_state_machine", ".", "get_top_of_stack", "(", ")", "\n", "if", "merged_tokens", ":", "\n", "# if merged tokens ket present, use it", "\n", "            ", "merged_tokens", "=", "\",\"", ".", "join", "(", "merged_tokens", ")", "\n", "if", "merged_tokens", "in", "train_rule_stats", "[", "'possible_predicates'", "]", ":", "\n", "                ", "token", "=", "merged_tokens", "\n", "\n", "# add rules from possible predicates", "\n", "", "", "if", "token", "in", "train_rule_stats", "[", "'possible_predicates'", "]", ":", "\n", "            ", "nodes", "=", "train_rule_stats", "[", "'possible_predicates'", "]", "[", "token", "]", "\n", "possible_predicates", "=", "[", "f'PRED({node})'", "for", "node", "in", "nodes", "]", "\n", "# add to the total number of actions", "\n", "valid_actions", ".", "extend", "(", "possible_predicates", ")", "\n", "\n", "# ensure gold action is among the choices if it is a PRED", "\n", "", "", "if", "(", "\n", "gold_action", ".", "startswith", "(", "'PRED'", ")", "and", "\n", "gold_action", "not", "in", "train_rule_stats", "[", "'possible_predicates'", "]", "\n", ")", ":", "\n", "        ", "stats", "[", "'missing_pred_count'", "]", ".", "update", "(", "[", "gold_action", "]", ")", "\n", "valid_actions", ".", "append", "(", "gold_action", ")", "\n", "\n", "", "stats", "[", "'fan_out_count'", "]", ".", "update", "(", "[", "len", "(", "valid_actions", ")", "]", ")", "\n", "\n", "if", "len", "(", "valid_actions", ")", "==", "0", ":", "\n", "        ", "stats", "[", "'missing_action_count'", "]", ".", "update", "(", "[", "gold_action", "]", ")", "\n", "valid_actions", ".", "append", "(", "gold_action", ")", "\n", "\n", "", "return", "np", ".", "array", "(", "[", "action_list", ".", "index", "(", "act", ")", "for", "act", "in", "valid_actions", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.main": [[236, 335], ["amr_state_machine.argument_parser", "transition_amr_parser.io.read_rule_stats", "list", "collections.defaultdict", "train_rule_stats[].keys", "amr_state_machine.h5_writer", "amr_state_machine.h5_writer", "amr_state_machine.readlines", "amr_state_machine.readlines", "transition_amr_parser.state_machine.get_spacy_lemmatizer", "tqdm.tqdm", "max", "print", "h5_writer.", "h5_writer.", "sorted", "action_by_basic[].append", "len", "len", "collections.Counter", "collections.Counter", "collections.Counter", "zip", "transition_amr_parser.state_machine.AMRStateMachine", "h5_writer.", "h5_writer.", "len", "stats[].keys", "train_rule_stats[].keys", "action.split", "len", "amr_state_machine.get_word_states", "amr_state_machine.get_valid_actions", "word_states_sent.append", "valid_actions_sent.append", "transition_amr_parser.state_machine.AMRStateMachine.applyAction", "print", "stats[].most_common", "amr_state_machine.yellow_font"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.argument_parser", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.io.read_rule_stats", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.h5_writer", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.h5_writer", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.readlines", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.readlines", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.get_spacy_lemmatizer", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.get_word_states", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.get_valid_actions", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.DepParsingStateMachine.applyAction", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.yellow_font"], ["", "def", "main", "(", ")", ":", "\n", "\n", "# Argument handling", "\n", "    ", "args", "=", "argument_parser", "(", ")", "\n", "\n", "# read rules", "\n", "train_rule_stats", "=", "read_rule_stats", "(", "args", ".", "in_rule_stats", ")", "\n", "assert", "'action_vocabulary'", "in", "train_rule_stats", "\n", "assert", "'possible_predicates'", "in", "train_rule_stats", "\n", "action_list", "=", "list", "(", "sorted", "(", "train_rule_stats", "[", "'action_vocabulary'", "]", ".", "keys", "(", ")", ")", ")", "\n", "\n", "# get all actions indexec by action root", "\n", "action_by_basic", "=", "defaultdict", "(", "list", ")", "\n", "for", "action", "in", "train_rule_stats", "[", "'action_vocabulary'", "]", ".", "keys", "(", ")", ":", "\n", "        ", "key", "=", "action", ".", "split", "(", "'('", ")", "[", "0", "]", "\n", "action_by_basic", "[", "key", "]", ".", "append", "(", "action", ")", "\n", "\n", "# open file for reading if provided", "\n", "", "write_out_states", "=", "h5_writer", "(", "args", ".", "out_word_states", ")", "\n", "write_out_valid_actions", "=", "h5_writer", "(", "args", ".", "out_valid_actions", ")", "\n", "\n", "# Read content", "\n", "# TODO: Point to LDC data", "\n", "sentences", "=", "readlines", "(", "args", ".", "in_sentences", ")", "\n", "actions", "=", "readlines", "(", "args", ".", "in_actions", ")", "\n", "assert", "len", "(", "sentences", ")", "==", "len", "(", "actions", ")", "\n", "\n", "# initialize spacy lemmatizer out of the sentence loop for speed", "\n", "spacy_lemmatizer", "=", "get_spacy_lemmatizer", "(", ")", "\n", "\n", "sent_idx", "=", "-", "1", "\n", "stats", "=", "{", "\n", "'missing_pred_count'", ":", "Counter", "(", ")", ",", "\n", "'missing_action_count'", ":", "Counter", "(", ")", ",", "\n", "'fan_out_count'", ":", "Counter", "(", ")", "\n", "}", "\n", "for", "sent_tokens", ",", "sent_actions", "in", "tqdm", "(", "\n", "zip", "(", "sentences", ",", "actions", ")", ",", "\n", "desc", "=", "'extracting oracle masks'", ",", "\n", "total", "=", "len", "(", "actions", ")", "\n", ")", ":", "\n", "\n", "# keep count of sentence index", "\n", "        ", "sent_idx", "+=", "1", "\n", "if", "args", ".", "offset", "and", "sent_idx", "<", "args", ".", "offset", ":", "\n", "            ", "continue", "\n", "\n", "# Initialize state machine", "\n", "", "amr_state_machine", "=", "AMRStateMachine", "(", "\n", "sent_tokens", ",", "\n", "spacy_lemmatizer", "=", "spacy_lemmatizer", "\n", ")", "\n", "\n", "# process each action", "\n", "word_states_sent", "=", "[", "]", "\n", "valid_actions_sent", "=", "[", "]", "\n", "for", "raw_action", "in", "sent_actions", ":", "\n", "\n", "# Store states BEFORE ACTION", "\n", "# state of each word (buffer B, stack S, reduced X)", "\n", "            ", "word_states", "=", "get_word_states", "(", "amr_state_machine", ",", "sent_tokens", ")", "\n", "\n", "# Get actions valid for this state", "\n", "valid_actions", "=", "get_valid_actions", "(", "\n", "action_list", ",", "\n", "amr_state_machine", ",", "\n", "train_rule_stats", ",", "\n", "action_by_basic", ",", "\n", "raw_action", ",", "\n", "stats", "\n", ")", "\n", "\n", "# update info", "\n", "word_states_sent", ".", "append", "(", "word_states", ")", "\n", "valid_actions_sent", ".", "append", "(", "valid_actions", ")", "\n", "\n", "# Update machine", "\n", "amr_state_machine", ".", "applyAction", "(", "raw_action", ")", "\n", "\n", "# Write states for this sentence", "\n", "", "write_out_states", "(", "word_states_sent", ")", "\n", "write_out_valid_actions", "(", "valid_actions_sent", ")", "\n", "\n", "# inform usre about missing predicates", "\n", "", "for", "miss", "in", "[", "'missing_pred_count'", ",", "'missing_action_count'", "]", ":", "\n", "        ", "num_missing", "=", "len", "(", "stats", "[", "miss", "]", ")", "\n", "if", "num_missing", ":", "\n", "            ", "alert_str", "=", "f'{num_missing} {miss} rule_stats'", "\n", "print", "(", "yellow_font", "(", "alert_str", ")", ")", "\n", "\n", "# inform user about fan-out stats", "\n", "", "", "mode_fan_out", "=", "stats", "[", "'fan_out_count'", "]", ".", "most_common", "(", "1", ")", "[", "0", "]", "[", "0", "]", "\n", "max_fan_out", "=", "max", "(", "stats", "[", "'fan_out_count'", "]", ".", "keys", "(", ")", ")", "\n", "alert_str", "=", "f'num_actions mode: {mode_fan_out} max: {max_fan_out}'", "\n", "print", "(", "alert_str", ")", "\n", "\n", "# Close file", "\n", "write_out_states", "(", ")", "\n", "write_out_valid_actions", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.readlines": [[337, 342], ["open", "line.rstrip().split", "line.rstrip"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["", "def", "readlines", "(", "file_path", ")", ":", "\n", "    ", "with", "open", "(", "file_path", ")", "as", "fid", ":", "\n", "# FIXME: using tab should be an option", "\n", "        ", "lines", "=", "[", "line", ".", "rstrip", "(", ")", ".", "split", "(", "'\\t'", ")", "for", "line", "in", "fid", "]", "\n", "", "return", "lines", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.print_state_machine": [[344, 373], ["zip", "os.system", "print", "print", "print", "print", "print", "print", "display_items.append", "display_pos.append", "amr_state_machine.white_background", "amr_state_machine.red_background", "display_pos.append", "display_pos.append", "amr_state_machine.black_font", "amr_state_machine.black_font", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.stack_state_machine.white_background", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.stack_state_machine.red_background", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.stack_state_machine.black_font", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.stack_state_machine.black_font"], ["", "def", "print_state_machine", "(", "word_states", ",", "sent_tokens", ",", "sent_idx", ",", "amr_state_machine", ")", ":", "\n", "\n", "# mask display strings", "\n", "    ", "display_items", "=", "[", "]", "\n", "display_pos", "=", "[", "]", "\n", "for", "state", ",", "token", "in", "zip", "(", "word_states", ",", "sent_tokens", ")", ":", "\n", "        ", "if", "state", "[", "1", "]", "==", "'B'", ":", "\n", "            ", "styled_token", "=", "\" %s\"", "%", "token", "\n", "", "elif", "state", "[", "1", "]", "==", "'S'", ":", "\n", "            ", "styled_token", "=", "white_background", "(", "\" %s\"", "%", "black_font", "(", "token", ")", ")", "\n", "", "else", ":", "\n", "            ", "styled_token", "=", "red_background", "(", "\" %s\"", "%", "black_font", "(", "token", ")", ")", "\n", "", "display_items", ".", "append", "(", "styled_token", ")", "\n", "if", "state", "==", "(", "0", ",", "'S'", ")", ":", "\n", "# top of stack", "\n", "            ", "display_pos", ".", "append", "(", "' '", "+", "'_'", "*", "len", "(", "token", ")", ")", "\n", "", "elif", "state", "==", "(", "1", ",", "'S'", ")", ":", "\n", "# second of stack", "\n", "            ", "display_pos", ".", "append", "(", "' '", "+", "'-'", "*", "len", "(", "token", ")", ")", "\n", "", "else", ":", "\n", "            ", "display_pos", ".", "append", "(", "' '", "+", "' '", "*", "len", "(", "token", ")", ")", "\n", "\n", "", "", "os", ".", "system", "(", "'clear'", ")", "\n", "print", "(", "\"\"", ")", "\n", "print", "(", "\"sentence %d\\n\"", "%", "sent_idx", ")", "\n", "print", "(", "\"\"", ".", "join", "(", "display_items", ")", ")", "\n", "print", "(", "\"\"", ".", "join", "(", "display_pos", ")", ")", "\n", "print", "(", "\"\"", ")", "\n", "print", "(", "amr_state_machine", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.get_action_indexer": [[375, 408], ["collections.defaultdict", "symbols.index", "action_list_by_prefix[].append", "isinstance", "set", "action.split", "set", "set.add", "set.add", "symbols.index", "symbols.index"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.index", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.add", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.add", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.index", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.index"], ["", "def", "get_action_indexer", "(", "symbols", ")", ":", "\n", "\n", "    ", "action_list_by_prefix", "=", "defaultdict", "(", "list", ")", "\n", "for", "action", "in", "symbols", ":", "\n", "        ", "prefix", "=", "action", ".", "split", "(", "'('", ")", "[", "0", "]", "\n", "index", "=", "symbols", ".", "index", "(", "action", ")", "\n", "action_list_by_prefix", "[", "prefix", "]", ".", "append", "(", "index", ")", "\n", "\n", "", "def", "action_indexer", "(", "actions", ")", ":", "\n", "        ", "\"\"\"\n        Create a map of each actions prefix to each action in the dictionary\n        \"\"\"", "\n", "\n", "assert", "isinstance", "(", "actions", ",", "list", ")", "\n", "\n", "nonlocal", "symbols", "\n", "nonlocal", "action_list_by_prefix", "\n", "\n", "idx", "=", "set", "(", ")", "\n", "for", "action", "in", "actions", ":", "\n", "            ", "if", "'('", "in", "action", ":", "\n", "# specific action, just index", "\n", "                ", "if", "action", "not", "in", "symbols", ":", "\n", "# FIXME: Brittle", "\n", "                    ", "idx", ".", "add", "(", "symbols", ".", "index", "(", "'<unk>'", ")", ")", "\n", "", "else", ":", "\n", "                    ", "idx", ".", "add", "(", "symbols", ".", "index", "(", "action", ")", ")", "\n", "", "", "else", ":", "\n", "# base action, expand", "\n", "                ", "idx", "|=", "set", "(", "action_list_by_prefix", "[", "action", "]", ")", "\n", "", "", "return", "idx", "\n", "\n", "", "return", "action_indexer", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.update_machine": [[629, 683], ["enumerate", "state_machine.update_masks", "tokens[].tolist", "machine.get_valid_actions", "machine.applyAction", "float", "Exception", "machine.applyAction", "float", "machine.applyAction", "action.split"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.stack_state_machine.StackStateMachine.update_masks", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.get_valid_actions", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.DepParsingStateMachine.applyAction", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.DepParsingStateMachine.applyAction", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.DepParsingStateMachine.applyAction", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["", "", "def", "update_machine", "(", "step", ",", "tokens", ",", "scores", ",", "state_machine", ")", ":", "\n", "\n", "# Update action by action", "\n", "    ", "for", "index", ",", "action_index", "in", "enumerate", "(", "tokens", "[", ":", ",", "step", "+", "1", "]", ".", "tolist", "(", ")", ")", ":", "\n", "\n", "# machine of the batch", "\n", "        ", "machine", "=", "state_machine", ".", "machines", "[", "index", "]", "\n", "\n", "valid_actions", ",", "invalid_actions", "=", "machine", ".", "get_valid_actions", "(", ")", "\n", "\n", "# some action may be invalid due to beam > 1. They should", "\n", "# have -Inf score so that they are pruned on the next iteration", "\n", "# so we do not execute those actions", "\n", "action", "=", "state_machine", ".", "tgt_dict", ".", "symbols", "[", "action_index", "]", "\n", "if", "machine", ".", "is_closed", ":", "\n", "\n", "# machine is closed", "\n", "            ", "if", "scores", "[", "index", ",", "step", "]", "!=", "float", "(", "\"-inf\"", ")", ":", "\n", "                ", "raise", "Exception", "(", "\"Machine closed at score not -Inf!\"", ")", "\n", "", "machine", ".", "applyAction", "(", "'</s>'", ")", "\n", "\n", "", "elif", "(", "\n", "(", "\n", "action", "not", "in", "valid_actions", "and", "\n", "action", ".", "split", "(", "'('", ")", "[", "0", "]", "not", "in", "valid_actions", "\n", ")", "\n", "or", "action", "in", "invalid_actions", "\n", ")", ":", "\n", "\n", "#print('IS HAPPENING')", "\n", "#print(valid_actions)", "\n", "#print(action)", "\n", "#exit(0)", "\n", "# FIXME: This should not happen", "\n", "            ", "machine", ".", "applyAction", "(", "'</s>'", ")", "\n", "scores", "[", "index", ",", "step", "]", "=", "float", "(", "\"-inf\"", ")", "\n", "#             print(machine)", "\n", "#             print(action)", "\n", "#             print()", "\n", "#             import ipdb; ipdb.set_trace()", "\n", "#             print(scores[index, step].item())", "\n", "#", "\n", "#             # if the scores are -Inf it will be pruned the next step", "\n", "#             if scores[index, step] != float(\"-inf\"):", "\n", "#                 import ipdb; ipdb.set_trace(context=30)", "\n", "#             # TODO: Close machine?", "\n", "#             # machine.applyAction('</s>')", "\n", "\n", "", "else", ":", "\n", "            ", "machine", ".", "applyAction", "(", "action", ")", "\n", "\n", "# increase counter and update masks", "\n", "", "", "state_machine", ".", "step_index", "+=", "1", "\n", "state_machine", ".", "update_masks", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.machine_generator": [[685, 731], ["transition_amr_parser.state_machine.get_spacy_lemmatizer", "transition_amr_parser.state_machine.AMRStateMachine", "Exception", "transition_amr_parser.state_machine.DepParsingStateMachine", "BIOStateMachine", "Exception"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.get_spacy_lemmatizer"], ["", "def", "machine_generator", "(", "actions_by_stack_rules", ",", "spacy_lemmatizer", "=", "None", ",", "entity_rules", "=", "None", ",", "post_process", "=", "False", ")", ":", "\n", "    ", "\"\"\"Return function that itself returns initialized state machines\"\"\"", "\n", "\n", "# initialize spacy lemmatizer", "\n", "if", "spacy_lemmatizer", "is", "None", ":", "\n", "        ", "spacy_lemmatizer", "=", "get_spacy_lemmatizer", "(", ")", "\n", "\n", "", "def", "get_new_state_machine", "(", "sent_tokens", ",", "machine_type", "=", "None", ")", ":", "\n", "\n", "        ", "nonlocal", "actions_by_stack_rules", "\n", "nonlocal", "spacy_lemmatizer", "\n", "nonlocal", "entity_rules", "\n", "\n", "# automatic determination of machine if no flag provided", "\n", "if", "sent_tokens", "[", "0", "]", "in", "[", "'<NER>'", ",", "'<AMR>'", ",", "'SRL'", "]", ":", "\n", "            ", "assert", "machine_type", "is", "None", ",", "\"specify --machine-type OR pre-append <machine-type token>\"", "\n", "machine_type", "=", "sent_tokens", "[", "0", "]", "[", "1", ":", "-", "1", "]", "\n", "", "elif", "machine_type", "is", "None", ":", "\n", "            ", "Exception", "(", "\n", "\"needs either --machine-type or appending <machine-type token>\"", "\n", ")", "\n", "\n", "# select machine", "\n", "", "if", "machine_type", "==", "'AMR'", ":", "\n", "            ", "return", "AMRStateMachine", "(", "\n", "sent_tokens", ",", "\n", "actions_by_stack_rules", "=", "actions_by_stack_rules", ",", "\n", "spacy_lemmatizer", "=", "spacy_lemmatizer", ",", "\n", "entity_rules", "=", "entity_rules", ",", "\n", "# this is only needed to generate the AMR", "\n", "post_process", "=", "post_process", "\n", ")", "\n", "", "elif", "machine_type", "==", "'dep-parsing'", ":", "\n", "#assert sent_tokens[-1] == 'ROOT'", "\n", "# sent_tokens.pop()", "\n", "# sent_tokens.append('<ROOT>')", "\n", "            ", "return", "DepParsingStateMachine", "(", "sent_tokens", ")", "\n", "\n", "", "elif", "machine_type", "in", "[", "'NER'", ",", "'SRL'", "]", ":", "\n", "            ", "from", "bio_tags", ".", "machine", "import", "BIOStateMachine", "\n", "return", "BIOStateMachine", "(", "sent_tokens", ")", "\n", "", "else", ":", "\n", "            ", "raise", "Exception", "(", "f'Unknown machine {machine_type}'", ")", "\n", "\n", "", "", "return", "get_new_state_machine", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.fix_shift_multi_task": [[733, 773], ["enumerate", "lprobs.argmax().tolist", "state_machine_batch.machines[].get_buffer_stack_copy", "action.startswith", "len", "lprobs[].item", "list", "lprobs.argmax", "state_machine_batch.action_indexer", "tgt_dict.symbols.index", "tgt_dict.symbols.index", "tgt_dict.symbols.index"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.DepParsingStateMachine.get_buffer_stack_copy", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.argmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.index", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.index", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.index"], ["", "def", "fix_shift_multi_task", "(", "lprobs", ",", "state_machine_batch", ",", "tgt_dict", ",", "\n", "logits_indices", ")", ":", "\n", "\n", "# fix for multi-task mode: If we guess right SHIFT, rename it to", "\n", "# SHIFT({top_of_buffer}), if action exists", "\n", "    ", "for", "machine_idx", ",", "i", "in", "enumerate", "(", "lprobs", ".", "argmax", "(", "dim", "=", "1", ")", ".", "tolist", "(", ")", ")", ":", "\n", "\n", "# Get copy of buffer", "\n", "        ", "machine_buffer", ",", "_", "=", "state_machine_batch", ".", "machines", "[", "machine_idx", "]", ".", "get_buffer_stack_copy", "(", ")", "\n", "\n", "action", "=", "tgt_dict", ".", "symbols", "[", "i", "]", "\n", "machine", "=", "state_machine_batch", ".", "machines", "[", "machine_idx", "]", "\n", "if", "action", ".", "startswith", "(", "'SHIFT'", ")", "and", "len", "(", "machine_buffer", ")", ":", "\n", "\n", "# get probability of chosen SHIFT (highest shift probability)", "\n", "            ", "best_shift_logprob", "=", "lprobs", "[", "machine_idx", ",", "i", "]", ".", "item", "(", ")", "\n", "shift_idx", "=", "list", "(", "state_machine_batch", ".", "action_indexer", "(", "[", "'SHIFT'", "]", ")", ")", "\n", "\n", "# get labeled SHIFT action", "\n", "top_of_buffer", "=", "machine", ".", "tokens", "[", "machine_buffer", "[", "-", "1", "]", "-", "1", "]", "\n", "tob_shift_action", "=", "f'SHIFT({top_of_buffer})'", "\n", "\n", "# check if its registered", "\n", "if", "tob_shift_action", "in", "tgt_dict", ".", "symbols", ":", "\n", "                ", "if", "tgt_dict", ".", "symbols", ".", "index", "(", "tob_shift_action", ")", "==", "i", ":", "\n", "# we selected the correct SHIFT already", "\n", "                    ", "new_action_index", "=", "i", "\n", "", "else", ":", "\n", "# select correct labeled SHIFT", "\n", "                    ", "new_action_index", "=", "tgt_dict", ".", "symbols", ".", "index", "(", "tob_shift_action", ")", "\n", "", "", "else", ":", "\n", "# remove label from SHIFT (as it is not correct)", "\n", "                ", "new_action_index", "=", "tgt_dict", ".", "symbols", ".", "index", "(", "'SHIFT'", ")", "\n", "\n", "# Keep most probable shift as its probability, set rest to zero", "\n", "", "lprobs", "[", "machine_idx", ",", "shift_idx", "]", "=", "-", "math", ".", "inf", "\n", "lprobs", "[", "machine_idx", ",", "new_action_index", "]", "=", "best_shift_logprob", "\n", "\n", "", "", "return", "lprobs", "\n", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.__init__": [[198, 205], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append": [[206, 208], ["data_utils.Examples.examples.append"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.save": [[209, 248], ["sorted", "collections.defaultdict", "all", "os.path.dirname", "range", "set", "list", "sorted", "results[].append", "open", "range", "set", "data_utils.Examples.path.split", "open", "fid.write", "len", "collections.defaultdict.values", "fid.write", "max"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.scripts.wav2vec_featurize.H5Writer.write"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.collate_embeddings": [[6, 37], ["max", "list", "values[].new().fill_", "enumerate", "set", "len", "data_utils.collate_embeddings.copy_tensor"], "function", ["None"], ["import", "torch", "\n", "\n", "\n", "def", "calc_mean_invstddev", "(", "feature", ")", ":", "\n", "    ", "if", "len", "(", "feature", ".", "size", "(", ")", ")", "!=", "2", ":", "\n", "        ", "raise", "ValueError", "(", "\"We expect the input feature to be 2-D tensor\"", ")", "\n", "", "mean", "=", "feature", ".", "mean", "(", "0", ")", "\n", "var", "=", "feature", ".", "var", "(", "0", ")", "\n", "# avoid division by ~zero", "\n", "eps", "=", "1e-8", "\n", "if", "(", "var", "<", "eps", ")", ".", "any", "(", ")", ":", "\n", "        ", "return", "mean", ",", "1.0", "/", "(", "torch", ".", "sqrt", "(", "var", ")", "+", "eps", ")", "\n", "", "return", "mean", ",", "1.0", "/", "torch", ".", "sqrt", "(", "var", ")", "\n", "\n", "\n", "", "def", "apply_mv_norm", "(", "features", ")", ":", "\n", "    ", "mean", ",", "invstddev", "=", "calc_mean_invstddev", "(", "features", ")", "\n", "res", "=", "(", "features", "-", "mean", ")", "*", "invstddev", "\n", "return", "res", "\n", "\n", "\n", "", "def", "lengths_to_encoder_padding_mask", "(", "lengths", ",", "batch_first", "=", "False", ")", ":", "\n", "    "]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.collate_target_masks": [[39, 119], ["set", "max", "min", "[].new().fill_", "range", "target_mask_sentence.append", "logit_indices.type.type", "logit_indices_sentence.append", "set", "max", "len", "len", "target_mask.view", "logit_indices.type.tolist", "[].new", "enumerate", "list"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["\n", "max_lengths", "=", "torch", ".", "max", "(", "lengths", ")", ".", "item", "(", ")", "\n", "bsz", "=", "lengths", ".", "size", "(", "0", ")", "\n", "encoder_padding_mask", "=", "torch", ".", "arange", "(", "\n", "max_lengths", "\n", ")", ".", "to", "(", "# a (T, ) tensor with [0, ..., T-1]", "\n", "lengths", ".", "device", "\n", ")", ".", "view", "(", "# move to the right device", "\n", "1", ",", "max_lengths", "\n", ")", ".", "expand", "(", "# reshape to (1, T)-shaped tensor", "\n", "bsz", ",", "-", "1", "\n", ")", ">=", "lengths", ".", "view", "(", "# expand to (B, T)-shaped tensor", "\n", "bsz", ",", "1", "\n", ")", ".", "expand", "(", "\n", "-", "1", ",", "max_lengths", "\n", ")", "\n", "if", "not", "batch_first", ":", "\n", "        ", "return", "encoder_padding_mask", ".", "t", "(", ")", ",", "max_lengths", "\n", "", "else", ":", "\n", "        ", "return", "encoder_padding_mask", ",", "max_lengths", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.collate_wp_idx": [[121, 171], ["max", "values[].new().fill_", "enumerate", "v.size", "values[].new", "dst.numel", "src.numel", "dst.copy_", "data_utils.collate_embeddings.copy_tensor"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.collate_masks": [[173, 194], ["max", "max", "values[].new().fill_", "enumerate", "dst.copy_", "v.size", "v.size", "values[].new", "dst.numel", "src.numel", "data_utils.collate_embeddings.copy_tensor"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.audio.raw_audio_dataset.RawAudioDataset.size"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.model_gottbert.GottbertModel.hub_models": [[18, 22], ["None"], "methods", ["None"], ["    ", "@", "classmethod", "\n", "def", "hub_models", "(", "cls", ")", ":", "\n", "        ", "return", "{", "\n", "'gottbert-base'", ":", "'https://dl.gottbert.de/fairseq/models/gottbert-base.tar.gz'", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.model_gottbert.GottbertModel.from_pretrained": [[24, 50], ["hub_utils.from_pretrained", "hub_interface.RobertaHubInterface", "cls.hub_models"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.model_gottbert.GottbertModel.from_pretrained", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.model_gottbert.GottbertModel.hub_models"], ["", "@", "classmethod", "\n", "def", "from_pretrained", "(", "cls", ",", "\n", "model_name_or_path", ",", "\n", "checkpoint_file", "=", "'model.pt'", ",", "\n", "data_name_or_path", "=", "'.'", ",", "\n", "bpe", "=", "'hf_byte_bpe'", ",", "\n", "bpe_vocab", "=", "'vocab.json'", ",", "\n", "bpe_merges", "=", "'merges.txt'", ",", "\n", "bpe_add_prefix_space", "=", "False", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "from", "fairseq", "import", "hub_utils", "\n", "\n", "x", "=", "hub_utils", ".", "from_pretrained", "(", "\n", "model_name_or_path", ",", "\n", "checkpoint_file", ",", "\n", "data_name_or_path", ",", "\n", "archive_map", "=", "cls", ".", "hub_models", "(", ")", ",", "\n", "bpe", "=", "bpe", ",", "\n", "load_checkpoint_heads", "=", "True", ",", "\n", "bpe_vocab", "=", "bpe_vocab", ",", "\n", "bpe_merges", "=", "bpe_merges", ",", "\n", "bpe_add_prefix_space", "=", "bpe_add_prefix_space", ",", "\n", "**", "kwargs", ",", "\n", ")", "\n", "return", "RobertaHubInterface", "(", "x", "[", "'args'", "]", ",", "x", "[", "'task'", "]", ",", "x", "[", "'models'", "]", "[", "0", "]", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.stack_state_machine.StackStateMachine.__init__": [[48, 130], ["max", "range", "all", "src_tokens.unsqueeze().repeat", "stack_state_machine.StackStateMachine.update_masks", "len", "stack_state_machine.StackStateMachine.batch.append", "torch.ones_like", "tgt_dict.pad", "torch.ones_like", "tgt_dict.pad", "src_tokens[].cpu().numpy", "src_tokens[].cpu().numpy", "src_tokens.unsqueeze", "enumerate", "list", "src_tokens[].cpu", "src_tokens[].cpu", "range", "a.split"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.stack_state_machine.StackStateMachine.update_masks", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.pad", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["def", "__init__", "(", "self", ",", "src_tokens", ",", "src_lengths", ",", "src_dict", ",", "tgt_dict", ",", "\n", "max_tgt_len", ",", "beam_size", ")", ":", "\n", "\n", "# FIXME: This assumes left padding of source. This is for consistency", "\n", "# with fairseq", "\n", "        ", "self", ".", "left_pad_source", "=", "True", "\n", "assert", "self", ".", "left_pad_source", "\n", "\n", "# FIXME: current code may give problems with POS multi-task, see this", "\n", "# flag", "\n", "self", ".", "use_word_multitask", "=", "True", "\n", "\n", "# Initialize memory states and positions in stack-transformer", "\n", "# Note that input has batch and beam dimensions flattened into one ", "\n", "self", ".", "beam_size", "=", "beam_size", "\n", "\n", "# fairseqs dictionary to convert indices to symbols", "\n", "self", ".", "tgt_dict", "=", "tgt_dict", "\n", "\n", "# store per sentence information", "\n", "self", ".", "batch", "=", "[", "]", "\n", "max_len", "=", "max", "(", "src_lengths", ")", "\n", "for", "sent_index", "in", "range", "(", "len", "(", "src_lengths", ")", ")", ":", "\n", "\n", "# Dictionary indices for this sentece", "\n", "# we will also need the left padding for each batch", "\n", "            ", "src_len", "=", "src_lengths", "[", "sent_index", "]", "\n", "if", "self", ".", "left_pad_source", ":", "\n", "                ", "left_pad", "=", "max_len", "-", "src_len", "\n", "indices_tensor", "=", "src_tokens", "[", "sent_index", ",", "-", "src_len", ":", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "", "else", ":", "\n", "                ", "left_pad", "=", "0", "\n", "indices_tensor", "=", "src_tokens", "[", "sent_index", ",", ":", "src_len", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "# Add sentence state variables", "\n", "", "self", ".", "batch", ".", "append", "(", "{", "\n", "'sentence'", ":", "[", "src_dict", ".", "symbols", "[", "i", "]", "for", "i", "in", "indices_tensor", "]", ",", "\n", "'stack'", ":", "[", "]", ",", "\n", "'buffer'", ":", "list", "(", "range", "(", "src_len", ")", ")", ",", "\n", "'is_finished'", ":", "False", ",", "\n", "'left_pad'", ":", "left_pad", ",", "\n", "'log_likelihood'", ":", "0", ",", "\n", "'action_history'", ":", "[", "]", ",", "\n", "# original index. Due to pruning and reordering this can differ", "\n", "# from the current postion", "\n", "'sent_index'", ":", "sent_index", ",", "\n", "# position of parento", "\n", "'heads'", ":", "[", "None", "for", "_", "in", "indices_tensor", "]", ",", "\n", "# label of arc to parent", "\n", "'labels'", ":", "[", "None", "for", "_", "in", "indices_tensor", "]", "\n", "}", ")", "\n", "\n", "# sanity check: all sentences end in root", "\n", "", "assert", "all", "(", "item", "[", "'sentence'", "]", "[", "-", "1", "]", "==", "'ROOT'", "for", "item", "in", "self", ".", "batch", ")", ",", "\"Error in padding?\"", "\n", "\n", "# keep count of time step for stopping and action history", "\n", "self", ".", "max_tgt_len", "=", "max_tgt_len", "\n", "self", ".", "step_index", "=", "0", "\n", "\n", "# these have the same info as buffer and stack but in stack-transformer", "\n", "# form (batch_size * beam_size, src_len, tgt_len)", "\n", "dummy", "=", "src_tokens", ".", "unsqueeze", "(", "2", ")", ".", "repeat", "(", "1", ",", "1", ",", "max_tgt_len", ")", "\n", "self", ".", "memory", "=", "torch", ".", "ones_like", "(", "dummy", ")", "*", "tgt_dict", ".", "pad", "(", ")", "\n", "self", ".", "memory_pos", "=", "torch", ".", "ones_like", "(", "dummy", ")", "*", "tgt_dict", ".", "pad", "(", ")", "\n", "self", ".", "update_masks", "(", ")", "\n", "\n", "# Store valid action indices", "\n", "self", ".", "valid_indices", "=", "{", "}", "\n", "for", "action", "in", "[", "'SHIFT'", ",", "'LEFT-ARC'", ",", "'RIGHT-ARC'", ",", "'SWAP'", "]", ":", "\n", "            ", "self", ".", "valid_indices", "[", "action", "]", "=", "[", "\n", "self", ".", "tgt_dict", ".", "indices", "[", "a", "]", "\n", "for", "a", "in", "self", ".", "tgt_dict", ".", "symbols", "\n", "if", "action", "in", "a", ".", "split", "(", "','", ")", "[", "0", "]", "\n", "]", "\n", "\n", "# Store valid shifts", "\n", "# FIXME: Specific for auto-encoding multi-task", "\n", "", "self", ".", "shift_by_word", "=", "{", "\n", "word", ":", "self", ".", "tgt_dict", ".", "indices", "[", "'SHIFT(%s)'", "%", "word", "]", "\n", "for", "idx", ",", "word", "in", "enumerate", "(", "src_dict", ".", "symbols", ")", "\n", "if", "(", "'SHIFT(%s)'", "%", "word", ")", "in", "self", ".", "tgt_dict", ".", "symbols", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.stack_state_machine.StackStateMachine.__str__": [[132, 164], ["stack_state_machine.black_font", "stack_state_machine.white_background"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.stack_state_machine.black_font", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.stack_state_machine.white_background"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "# prints first sentence of each beam for the entire batch", "\n", "        ", "max_num_sentences", "=", "3", "\n", "str_rep", "=", "\"\"", "\n", "\n", "# pick first two and last two sentences for display", "\n", "sample_sent_indices", "=", "[", "0", ",", "1", "]", "\n", "#            0, ", "\n", "#            1 * self.beam_size, ", "\n", "#            len(self.sentences) - 2 * self.beam_size, ", "\n", "#            len(self.sentences) - 1 * self.beam_size", "\n", "#        ]", "\n", "# display those", "\n", "for", "state", "in", "self", ".", "batch", ":", "\n", "            ", "if", "state", "[", "'action_history'", "]", ":", "\n", "                ", "str_rep", "+=", "(", "\n", "\"%s %3.8f\"", "%", "\n", "(", "\n", "state", "[", "'action_history'", "]", "[", "-", "1", "]", "[", "0", "]", ",", "\n", "state", "[", "'action_history'", "]", "[", "-", "1", "]", "[", "1", "]", "\n", ")", "\n", ")", "\n", "str_rep", "+=", "\"\\n\"", "\n", "", "str_rep", "+=", "\" \"", ".", "join", "(", "[", "\n", "state", "[", "'sentence'", "]", "[", "idx", "]", "for", "idx", "in", "state", "[", "'buffer'", "]", "\n", "]", ")", "\n", "str_rep", "+=", "\"\\n\"", "\n", "str_rep", "+=", "black_font", "(", "white_background", "(", "\" \"", ".", "join", "(", "[", "\n", "state", "[", "'sentence'", "]", "[", "idx", "]", "for", "idx", "in", "state", "[", "'stack'", "]", "\n", "]", ")", ")", ")", "\n", "str_rep", "+=", "\"\\n\\n\"", "\n", "", "return", "str_rep", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.stack_state_machine.StackStateMachine.get_valid_action_indices": [[165, 208], ["len", "len", "valid_actions.extend", "len", "valid_actions.extend", "valid_actions.extend", "len", "valid_actions.extend", "len", "len"], "methods", ["None"], ["", "def", "get_valid_action_indices", "(", "self", ",", "state", ")", ":", "\n", "\n", "        ", "valid_actions", "=", "[", "]", "\n", "if", "state", "[", "'is_finished'", "]", ":", "\n", "# machine had been stopped", "\n", "# FIXME: Its EOS or PAD?", "\n", "            ", "valid_actions", "=", "[", "self", ".", "tgt_dict", ".", "indices", "[", "'</s>'", "]", "]", "\n", "\n", "", "elif", "(", "len", "(", "state", "[", "'stack'", "]", ")", "==", "2", "and", "len", "(", "state", "[", "'buffer'", "]", ")", "==", "0", ")", ":", "\n", "# stop machine", "\n", "            ", "valid_actions", "=", "[", "self", ".", "tgt_dict", ".", "indices", "[", "'LEFT-ARC(root)'", "]", "]", "\n", "\n", "", "else", ":", "\n", "            ", "if", "(", "\n", "len", "(", "state", "[", "'buffer'", "]", ")", ">", "0", "and", "\n", "not", "(", "len", "(", "state", "[", "'buffer'", "]", ")", "==", "1", "and", "len", "(", "state", "[", "'stack'", "]", ")", ">", "1", ")", "\n", ")", ":", "\n", "\n", "# If SHIFT(top-of-buffer) is a valid action constrain to that", "\n", "# FIXME: This is not robust (may find POS-looking word as top", "\n", "# of buffer)", "\n", "                ", "buffer_top_word", "=", "state", "[", "'sentence'", "]", "[", "state", "[", "'buffer'", "]", "[", "0", "]", "]", "\n", "shift_tob", "=", "'SHIFT(%s)'", "%", "buffer_top_word", "\n", "if", "(", "\n", "self", ".", "use_word_multitask", "and", "\n", "shift_tob", "in", "self", ".", "tgt_dict", ".", "indices", "\n", ")", ":", "\n", "                    ", "valid_indices", "=", "[", "self", ".", "tgt_dict", ".", "indices", "[", "shift_tob", "]", "]", "\n", "", "else", ":", "\n", "                    ", "valid_indices", "=", "self", ".", "valid_indices", "[", "'SHIFT'", "]", "\n", "\n", "# can shift", "\n", "", "valid_actions", ".", "extend", "(", "valid_indices", ")", "\n", "\n", "", "if", "len", "(", "state", "[", "'stack'", "]", ")", ">=", "2", ":", "\n", "# can draw arcs", "\n", "                ", "valid_actions", ".", "extend", "(", "self", ".", "valid_indices", "[", "'LEFT-ARC'", "]", ")", "\n", "valid_actions", ".", "extend", "(", "self", ".", "valid_indices", "[", "'RIGHT-ARC'", "]", ")", "\n", "if", "state", "[", "'stack'", "]", "[", "-", "1", "]", ">", "state", "[", "'stack'", "]", "[", "-", "2", "]", ":", "\n", "# can swap", "\n", "                    ", "valid_actions", ".", "extend", "(", "self", ".", "valid_indices", "[", "'SWAP'", "]", ")", "\n", "\n", "", "", "", "return", "valid_actions", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.stack_state_machine.StackStateMachine.act": [[209, 259], ["action.split", "state[].append", "action.split", "state[].pop", "state[].pop", "action.split", "len", "[].split", "state[].pop", "action.split", "len", "[].split", "state[].insert", "len", "state[].pop", "Exception", "action.split", "action.split", "action.split", "action.split"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.stack_lstm.StackRNN.pop", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["", "def", "act", "(", "self", ",", "action", ",", "state", ",", "step_index", ")", ":", "\n", "\n", "# if action n-gram, keep only the first", "\n", "        ", "action", "=", "action", ".", "split", "(", "','", ")", "[", "0", "]", "\n", "\n", "if", "action", ".", "split", "(", "'('", ")", "[", "0", "]", "==", "'SHIFT'", ":", "\n", "# move one elements from stack to buffer", "\n", "            ", "assert", "state", "[", "'buffer'", "]", ",", "\"Can not SHIFT empty buffer\"", "\n", "state", "[", "'stack'", "]", ".", "append", "(", "state", "[", "'buffer'", "]", ".", "pop", "(", "0", ")", ")", "\n", "\n", "", "elif", "action", ".", "split", "(", "'('", ")", "[", "0", "]", "==", "'LEFT-ARC'", ":", "\n", "# remove second element in stack from the top", "\n", "            ", "assert", "len", "(", "state", "[", "'stack'", "]", ")", ">=", "2", ",", "\"Need at least size 2 stack\"", "\n", "assert", "state", "[", "'sentence'", "]", "[", "state", "[", "'stack'", "]", "[", "-", "2", "]", "]", "!=", "'ROOT'", ",", "\"Dependent can not be ROOT\"", "\n", "# remove first element in stack from the top", "\n", "dependent", "=", "state", "[", "'stack'", "]", ".", "pop", "(", "-", "2", ")", "\n", "# store head and label           ", "\n", "state", "[", "'heads'", "]", "[", "dependent", "]", "=", "state", "[", "'stack'", "]", "[", "-", "1", "]", "\n", "state", "[", "'labels'", "]", "[", "dependent", "]", "=", "action", ".", "split", "(", "'('", ")", "[", "1", "]", ".", "split", "(", "')'", ")", "[", "0", "]", "\n", "\n", "# finishing action", "\n", "if", "action", "==", "'LEFT-ARC(root)'", ":", "\n", "# check no orphans", "\n", "# if any(x is None for x in state['heads'][:-1]):", "\n", "#    import ipdb; ipdb.set_trace(context=30)", "\n", "#    print(\"\")", "\n", "                ", "state", "[", "'is_finished'", "]", "=", "True", "\n", "\n", "", "", "elif", "action", ".", "split", "(", "'('", ")", "[", "0", "]", "==", "'RIGHT-ARC'", ":", "\n", "            ", "assert", "len", "(", "state", "[", "'stack'", "]", ")", ">=", "2", ",", "\"Need at least size 2 stack\"", "\n", "assert", "state", "[", "'sentence'", "]", "[", "state", "[", "'stack'", "]", "[", "-", "1", "]", "]", "!=", "'ROOT'", ",", "\"Dependent can not be ROOT\"", "\n", "# remove first element in stack from the top", "\n", "dependent", "=", "state", "[", "'stack'", "]", ".", "pop", "(", "-", "1", ")", "\n", "# store head and label           ", "\n", "state", "[", "'heads'", "]", "[", "dependent", "]", "=", "state", "[", "'stack'", "]", "[", "-", "1", "]", "\n", "state", "[", "'labels'", "]", "[", "dependent", "]", "=", "action", ".", "split", "(", "'('", ")", "[", "1", "]", ".", "split", "(", "')'", ")", "[", "0", "]", "\n", "\n", "", "elif", "(", "action", ".", "split", "(", "'('", ")", "[", "0", "]", "==", "'SWAP'", "and", "state", "[", "'stack'", "]", "[", "-", "1", "]", ">=", "state", "[", "'stack'", "]", "[", "-", "2", "]", ")", ":", "\n", "            ", "assert", "len", "(", "state", "[", "'stack'", "]", ")", ">=", "2", ",", "\"Need at least size 2 stack\"", "\n", "# set element 1 of the stack to 0 of the buffer", "\n", "state", "[", "'buffer'", "]", ".", "insert", "(", "0", ",", "state", "[", "'stack'", "]", ".", "pop", "(", "-", "2", ")", ")", "\n", "\n", "", "elif", "action", ".", "split", "(", "'('", ")", "[", "0", "]", "==", "'</s>'", "and", "state", "[", "'is_finished'", "]", ":", "\n", "# If machine is finished we should receive EOS", "\n", "            ", "pass", "\n", "\n", "", "else", ":", "\n", "            ", "raise", "Exception", "(", "\"Invalid action %s\"", "%", "action", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.stack_state_machine.StackStateMachine.update_state": [[260, 337], ["enumerate", "stack_state_machine.StackStateMachine.update_masks", "torch.ones_like", "float", "torch.LongTensor", "stack_state_machine.StackStateMachine.to", "log_p.argmax", "float", "stack_state_machine.StackStateMachine.act", "state[].append", "stack_state_machine.StackStateMachine.get_valid_action_indices", "len", "max", "log_p.max().cpu", "action_logp[].max", "numpy.exp", "log_p.max", "float"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.stack_state_machine.StackStateMachine.update_masks", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.argmax", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.stack_state_machine.StackStateMachine.act", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.stack_state_machine.StackStateMachine.get_valid_action_indices"], ["", "", "def", "update_state", "(", "self", ",", "action_logp", ",", "RUEDITAS", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        action_logp (batch_size, src_len, tgt_len) \n\n        RUEDITAS is gold actions!!\n        \"\"\"", "\n", "\n", "# This wills store log-probabilities for actions adding state machine", "\n", "# constraints ", "\n", "const_action_logp", "=", "torch", ".", "ones_like", "(", "action_logp", ")", "*", "float", "(", "'-inf'", ")", "\n", "\n", "# loop over log probabilities output by the model. Note that lats batch ", "\n", "# may have less than len(self.buffers) ", "\n", "\n", "# Due to reordering/pruning we have curr_index and sent_index", "\n", "# (original)", "\n", "for", "curr_index", ",", "state", "in", "enumerate", "(", "self", ".", "batch", ")", ":", "\n", "\n", "# Constrain only to valid actions. Set all other log probabilities", "\n", "# to zero constrain valid actions to gold actions if provided", "\n", "            ", "if", "RUEDITAS", "is", "None", ":", "\n", "                ", "valid_indices", "=", "self", ".", "get_valid_action_indices", "(", "state", ")", "\n", "", "else", ":", "\n", "                ", "valid_indices", "=", "RUEDITAS", "[", "curr_index", ",", "self", ".", "step_index", "]", "\n", "\n", "", "if", "len", "(", "state", "[", "'buffer'", "]", ")", "and", "self", ".", "use_word_multitask", ":", "\n", "# FIXME: This is specific for word multi-task", "\n", "# If restricted to valid buffer shift prediction, set valid", "\n", "# shoft top most probable shift action regardless of the word", "\n", "# shifted", "\n", "                ", "buffer_top_word", "=", "state", "[", "'sentence'", "]", "[", "state", "[", "'buffer'", "]", "[", "0", "]", "]", "\n", "shift_tob", "=", "'SHIFT(%s)'", "%", "buffer_top_word", "\n", "if", "shift_tob", "in", "self", ".", "tgt_dict", ".", "indices", ":", "\n", "                    ", "valid_shift", "=", "self", ".", "tgt_dict", ".", "indices", "[", "shift_tob", "]", "\n", "all_shifts", "=", "self", ".", "valid_indices", "[", "'SHIFT'", "]", "\n", "action_logp", "[", "curr_index", ",", "valid_shift", "]", "=", "action_logp", "[", "curr_index", ",", "all_shifts", "]", ".", "max", "(", ")", "\n", "\n", "# This should not happen but somehown it does: extra check for", "\n", "# index out of bounds", "\n", "", "", "assert", "max", "(", "valid_indices", ")", "<", "action_logp", ".", "shape", "[", "1", "]", ",", "\"Out of Bounds in output vocabulary!\"", "\n", "# valid_indices = [i for i in valid_indices if i < action_logp.shape[1]]", "\n", "# cast to GPU/CPU tesor    ", "\n", "valid_indices", "=", "torch", ".", "LongTensor", "(", "valid_indices", ")", "\n", "valid_indices", "=", "valid_indices", ".", "to", "(", "action_logp", ".", "device", ")", "\n", "# constrain log probabilities to valid actions", "\n", "const_action_logp", "[", "curr_index", ",", "valid_indices", "]", "=", "action_logp", "[", "curr_index", ",", "valid_indices", "]", "\n", "\n", "# get most probable action", "\n", "log_p", "=", "const_action_logp", "[", "curr_index", ",", ":", "]", "\n", "# check for no valid actions", "\n", "assert", "not", "(", "log_p", "==", "float", "(", "\"-inf\"", ")", ")", ".", "all", "(", ")", ",", "\"No valid action found!\"", "\n", "best_action_index", "=", "log_p", ".", "argmax", "(", ")", "\n", "best_action", "=", "self", ".", "tgt_dict", ".", "symbols", "[", "best_action_index", "]", "\n", "best_action_logp", "=", "float", "(", "log_p", ".", "max", "(", ")", ".", "cpu", "(", ")", ")", "\n", "\n", "# carry on action", "\n", "self", ".", "act", "(", "best_action", ",", "state", ",", "self", ".", "step_index", ")", "\n", "\n", "# update accumulated (unnormalized) log likelihood", "\n", "state", "[", "'log_likelihood'", "]", "+=", "best_action_logp", "\n", "\n", "# update action history", "\n", "state", "[", "'action_history'", "]", ".", "append", "(", "(", "\n", "best_action", ",", "\n", "np", ".", "exp", "(", "best_action_logp", ")", "\n", ")", ")", "\n", "\n", "# increase action counter", "\n", "", "self", ".", "step_index", "+=", "1", "\n", "\n", "# update state expressed as masks", "\n", "self", ".", "update_masks", "(", ")", "\n", "return", "const_action_logp", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.stack_state_machine.StackStateMachine.update_masks": [[338, 365], ["enumerate", "ipdb.set_trace", "print", "torch.arange().to", "torch.LongTensor().to", "torch.LongTensor", "torch.LongTensor", "torch.arange", "torch.LongTensor", "range"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.pdb.set_trace", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["", "def", "update_masks", "(", "self", ")", ":", "\n", "\n", "# Get masks from states", "\n", "# buffer words    ", "\n", "        ", "device", "=", "self", ".", "memory", ".", "device", "\n", "# reset", "\n", "for", "curr_index", ",", "state", "in", "enumerate", "(", "self", ".", "batch", ")", ":", "\n", "\n", "            ", "if", "state", "[", "'buffer'", "]", ":", "\n", "                ", "padded_indices", "=", "torch", ".", "LongTensor", "(", "state", "[", "'buffer'", "]", ")", "+", "state", "[", "'left_pad'", "]", "\n", "self", ".", "memory", "[", "curr_index", ",", "padded_indices", ",", "self", ".", "step_index", "]", "=", "3", "\n", "self", ".", "memory_pos", "[", "curr_index", ",", "padded_indices", ",", "self", ".", "step_index", "]", "=", "torch", ".", "arange", "(", "padded_indices", ".", "shape", "[", "0", "]", ")", ".", "to", "(", "device", ")", "\n", "\n", "", "if", "state", "[", "'stack'", "]", ":", "\n", "                ", "padded_indices", "=", "torch", ".", "LongTensor", "(", "state", "[", "'stack'", "]", ")", "+", "state", "[", "'left_pad'", "]", "\n", "self", ".", "memory", "[", "curr_index", ",", "padded_indices", ",", "self", ".", "step_index", "]", "=", "4", "\n", "self", ".", "memory_pos", "[", "curr_index", ",", "padded_indices", ",", "self", ".", "step_index", "]", "=", "torch", ".", "LongTensor", "(", "range", "(", "padded_indices", ".", "shape", "[", "0", "]", ")", "[", ":", ":", "-", "1", "]", ")", ".", "to", "(", "device", ")", "\n", "\n", "", "", "if", "self", ".", "step_index", ">=", "self", ".", "max_tgt_len", ":", "\n", "            ", "not_finished_idices", "=", "[", "\n", "idx", "for", "state", "in", "self", ".", "batch", "\n", "if", "not", "state", "[", "'is_finished'", "]", "\n", "]", "\n", "import", "ipdb", ";", "ipdb", ".", "set_trace", "(", "context", "=", "30", ")", "\n", "print", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.stack_state_machine.StackStateMachine.reoder_machine": [[366, 375], ["stack_state_machine.StackStateMachine.update_masks", "copy.deepcopy", "reorder_state.cpu().tolist", "reorder_state.cpu"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.stack_state_machine.StackStateMachine.update_masks"], ["", "", "def", "reoder_machine", "(", "self", ",", "reorder_state", ")", ":", "\n", "        ", "\"\"\"Reorder/eliminate machines during decoding\"\"\"", "\n", "# We need to deep copy", "\n", "self", ".", "batch", "=", "[", "\n", "deepcopy", "(", "self", ".", "batch", "[", "i", "]", ")", "for", "i", "in", "reorder_state", ".", "cpu", "(", ")", ".", "tolist", "(", ")", "\n", "]", "\n", "self", ".", "memory", "=", "self", ".", "memory", "[", "reorder_state", ",", ":", ",", ":", "]", "\n", "self", ".", "memory_pos", "=", "self", ".", "memory_pos", "[", "reorder_state", ",", ":", ",", ":", "]", "\n", "self", ".", "update_masks", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.stack_state_machine.white_background": [[31, 33], ["None"], "function", ["None"], ["def", "white_background", "(", "string", ")", ":", "\n", "    ", "return", "\"\\033[107m%s\\033[0m\"", "%", "string", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.stack_state_machine.red_background": [[34, 36], ["None"], "function", ["None"], ["", "def", "red_background", "(", "string", ")", ":", "\n", "    ", "return", "\"\\033[41m%s\\033[0m\"", "%", "string", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.stack_state_machine.black_font": [[37, 39], ["None"], "function", ["None"], ["", "def", "black_font", "(", "string", ")", ":", "\n", "    ", "return", "\"\\033[30m%s\\033[0m\"", "%", "string", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.stack_state_machine.stack_style": [[40, 42], ["stack_state_machine.black_font", "stack_state_machine.white_background"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.stack_state_machine.black_font", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.stack_state_machine.white_background"], ["", "def", "stack_style", "(", "string", ")", ":", "\n", "    ", "return", "black_font", "(", "white_background", "(", "string", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.stack_state_machine.sanity_check_masks": [[377, 406], ["pdb.set_trace", "torch.all", "ipdb.set_trace", "print", "torch.all", "ipdb.set_trace", "print", "ipdb.set_trace", "print", "pre_mask.sum"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.pdb.set_trace", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.pdb.set_trace", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.pdb.set_trace", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.pdb.set_trace", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["", "", "def", "sanity_check_masks", "(", "pre_mask", ",", "post_mask", ",", "encoder_padding_mask", ")", ":", "\n", "\n", "    ", "batch_size", ",", "target_size", ",", "source_size", "=", "pre_mask", ".", "shape", "\n", "\n", "# TODO:", "\n", "# no degenerate (all masked) softmax over source", "\n", "# no degenerate (all masked) softmax over target logits ", "\n", "\n", "import", "pdb", ";", "pdb", ".", "set_trace", "(", ")", "\n", "# buffer check: starts full and ends up empty", "\n", "for", "i", "in", "[", "0", ",", "1", "]", ":", "\n", "        ", "if", "not", "torch", ".", "all", "(", "pre_mask", "[", "i", ",", "0", ",", ":", "]", "==", "1", ")", ":", "\n", "            ", "import", "ipdb", ";", "ipdb", ".", "set_trace", "(", "context", "=", "30", ")", "\n", "print", "(", "\"\"", ")", "\n", "#if not torch.all(pre_mask[i, -1, :] == 0):", "\n", "#    import ipdb; ipdb.set_trace(context=30)", "\n", "#    print(\"\")", "\n", "# stack check: starts empty and ends up with one word", "\n", "", "", "for", "i", "in", "[", "2", ",", "3", "]", ":", "\n", "        ", "if", "not", "torch", ".", "all", "(", "pre_mask", "[", "i", ",", "0", ",", ":", "]", "==", "0", ")", ":", "\n", "            ", "import", "ipdb", ";", "ipdb", ".", "set_trace", "(", "context", "=", "30", ")", "\n", "print", "(", "\"\"", ")", "\n", "#if not pre_mask[i, -1, :].sum() == 1:", "\n", "#    import ipdb; ipdb.set_trace(context=30)", "\n", "#    print(\"\")", "\n", "\n", "", "if", "(", "pre_mask", ".", "sum", "(", "2", ")", "==", "0", ")", ".", "any", "(", ")", ":", "\n", "            ", "import", "ipdb", ";", "ipdb", ".", "set_trace", "(", "context", "=", "30", ")", "\n", "print", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.stack_state_machine.get_heads_stack_representation": [[408, 515], ["memory.repeat.repeat", "torch.ones_like", "pre_mask.transpose().contiguous().view().transpose.transpose().contiguous().view().transpose", "pre_mask.transpose().contiguous().view().transpose.new_ones", "pre_mask.transpose().contiguous().view().transpose.fill_", "float", "embed_stack_positions", "head_positions.view.view", "pre_mask.transpose().contiguous().view().transpose.transpose().contiguous().view", "memory_pos.view().long", "pre_mask.transpose().contiguous().view().transpose.sum", "memory.repeat.transpose().contiguous().view().transpose", "pre_mask.transpose().contiguous().view().transpose.transpose().contiguous", "memory_pos.view", "memory.repeat.transpose().contiguous().view", "pre_mask.transpose().contiguous().view().transpose.transpose", "pre_mask.transpose().contiguous().view().transpose.sum", "memory.repeat.transpose().contiguous", "memory.repeat.transpose"], "function", ["None"], ["", "", "", "def", "get_heads_stack_representation", "(", "memory", ",", "memory_pos", ",", "num_heads", ",", "\n", "embed_stack_positions", ",", "do_stack", "=", "True", ",", "\n", "do_buffer", "=", "True", ",", "do_top", "=", "False", ",", "\n", "do_positions", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    memory (batch_size, src_len, tgt_len) \n    \"\"\"", "\n", "\n", "# FIXME: more memory efficient code possible. We can apply the masks", "\n", "# positions only to the heads affected. No need to instantiate tensors of", "\n", "# whole size", "\n", "\n", "# FIXME: memory values hard coded in preprocessing.py", "\n", "# memory2int = {'B': 3, 'S' : 4, 'X': 5}", "\n", "\n", "# we will need the positions before reshaping memory", "\n", "if", "do_positions", ":", "\n", "        ", "pos_indices", "=", "memory", "==", "4", "\n", "\n", "# pre-mask", "\n", "# applied to the logits of the attention mechanisms", "\n", "# clone token memory states for each head", "\n", "", "memory", "=", "memory", ".", "repeat", "(", "num_heads", ",", "1", ",", "1", ",", "1", ")", "\n", "# TODO: Right now hand coded", "\n", "# 1 head buffer, 1 head stack, rest free", "\n", "assert", "num_heads", "in", "[", "6", ",", "4", "]", "\n", "pre_mask", "=", "torch", ".", "ones_like", "(", "memory", ")", "\n", "# (num_heads, batch_size, src_len, tgt_len)", "\n", "if", "do_buffer", ":", "\n", "# only use the buffer", "\n", "        ", "pre_mask", "[", "0", ",", ":", ",", ":", ",", ":", "]", "=", "memory", "[", "0", ",", ":", ",", ":", ",", ":", "]", "==", "3", "\n", "# mask everything that is not top two positions", "\n", "if", "do_top", ":", "\n", "            ", "pre_mask", "[", "0", ",", ":", ",", ":", ",", ":", "]", "[", "memory_pos", ">", "1", "]", "=", "0", "\n", "", "", "if", "do_stack", ":", "\n", "# only use the stack", "\n", "        ", "pre_mask", "[", "1", ",", ":", ",", ":", ",", ":", "]", "=", "memory", "[", "1", ",", ":", ",", ":", ",", ":", "]", "==", "4", "\n", "# mask everything that is not top two positions", "\n", "if", "do_top", ":", "\n", "            ", "pre_mask", "[", "1", ",", ":", ",", ":", ",", ":", "]", "[", "memory_pos", ">", "1", "]", "=", "0", "\n", "\n", "# flatten head and batch into same dimension. Heads for each batch element", "\n", "# must be contiguous", "\n", "", "", "_", ",", "batch_size", ",", "src_len", ",", "tgt_len", "=", "pre_mask", ".", "shape", "\n", "shape_pre_mask", "=", "batch_size", "*", "num_heads", ",", "src_len", ",", "tgt_len", "\n", "# (num_heads * batch_size, tgt_len, src_len)", "\n", "pre_mask", "=", "pre_mask", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", ".", "view", "(", "*", "shape_pre_mask", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "\n", "# To ispect i-th batch element heads", "\n", "# pre_mask[num_heads*i:num_heads*i+num_heads, :, :]", "\n", "\n", "# post-mask", "\n", "# applied to the attended values (all heads)", "\n", "# sets head to zero if all words of pre-mask were masked ", "\n", "# (empty buffer or stack)", "\n", "# FIXME: There should be a command for this", "\n", "shape", "=", "(", "batch_size", "*", "num_heads", ",", "tgt_len", ",", "1", ")", "\n", "# (num_heads * batch_size, tgt_len, 1)", "\n", "post_mask", "=", "pre_mask", ".", "new_ones", "(", "shape", ")", "\n", "post_mask", "[", "pre_mask", ".", "sum", "(", "2", ")", "==", "0", "]", "=", "0.0", "\n", "\n", "# pre-mask extra changes", "\n", "# what is filtered by the post mask can be here set to one", "\n", "pre_mask", "[", "(", "pre_mask", ".", "sum", "(", "2", ")", "==", "0", ")", ".", "unsqueeze", "(", "2", ")", ".", "repeat", "(", "1", ",", "1", ",", "src_len", ")", "]", "=", "1", "\n", "# We need to avoid full masking of pad on target side as this leads to nans", "\n", "# on the final prediction softmax. However, if we do not mask pad on source", "\n", "# here, the post_mask will not detect some cases leading to nans in", "\n", "# attention softmax. We thus remove masking pads after computing post_mask", "\n", "pre_mask", "[", "memory", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", ".", "view", "(", "*", "shape_pre_mask", ")", ".", "transpose", "(", "1", ",", "2", ")", "==", "1", "]", "=", "1", "\n", "#", "\n", "indices", "=", "pre_mask", "==", "1", "\n", "# store pre_mask in log domain", "\n", "pre_mask", "=", "pre_mask", "\n", "\n", "pre_mask", ".", "fill_", "(", "float", "(", "\"-Inf\"", ")", ")", "\n", "pre_mask", "[", "indices", "]", "=", "0.", "\n", "\n", "# sanity checks: ", "\n", "# sanity_check_masks(pre_mask, post_mask, encoder_padding_mask)", "\n", "\n", "# Buffer/Stack positions", "\n", "# if not do_stack_top:", "\n", "if", "do_positions", ":", "\n", "\n", "# get position weights", "\n", "        ", "max_positions", ",", "emb_size", "=", "embed_stack_positions", ".", "weights", ".", "shape", "\n", "\n", "# use half the positions for stack and half for buffer. Need to ceil", "\n", "# positions at that value", "\n", "# separate position indices for stack and buffer (half range for each)", "\n", "max_positions", "=", "max_positions", "//", "2", "\n", "memory_pos", "[", "memory_pos", ">=", "max_positions", "]", "=", "max_positions", "-", "1", "\n", "memory_pos", "[", "pos_indices", "]", "+=", "max_positions", "\n", "\n", "# (batch_size, src_len, tgt_len)", "\n", "# flatten batch and source dimenstion to call position embedder", "\n", "shape", "=", "(", "memory_pos", ".", "shape", "[", "0", "]", "*", "memory_pos", ".", "shape", "[", "1", "]", ",", "memory_pos", ".", "shape", "[", "2", "]", ")", "\n", "head_positions", "=", "embed_stack_positions", "(", "memory_pos", ".", "view", "(", "*", "shape", ")", ".", "long", "(", ")", ")", "\n", "# -> (batch_size, src_len, tgt_len, emb_size)", "\n", "shape", "=", "(", "batch_size", ",", "src_len", ",", "tgt_len", ",", "emb_size", ")", "\n", "head_positions", "=", "head_positions", ".", "view", "(", "*", "shape", ")", "\n", "\n", "", "else", ":", "\n", "\n", "        ", "head_positions", "=", "None", "\n", "\n", "", "return", "(", "pre_mask", ",", "post_mask", ")", ",", "head_positions", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.stack_state_machine.state_machine_encoder": [[517, 571], ["encode_state_machine.startswith", "encode_state_machine.startswith", "stack_state_machine.get_heads_stack_representation", "encode_state_machine.split", "encode_state_machine.split", "Exception"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.stack_state_machine.get_heads_stack_representation", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["", "def", "state_machine_encoder", "(", "encode_state_machine", ",", "memory", ",", "memory_pos", ",", "num_heads", ",", "\n", "embed_stack_positions", ",", "layer_index", ",", "encoder_padding_mask", ")", ":", "\n", "\n", "    ", "if", "(", "\n", "(", "encode_state_machine", "==", "'layer0'", "and", "layer_index", "==", "0", ")", "or", "\n", "encode_state_machine", "==", "'layer0_nopos'", "and", "layer_index", "==", "0", "or", "\n", "encode_state_machine", "==", "'all-layers'", "or", "\n", "encode_state_machine", "==", "'all-layers_nopos'", "or", "\n", "encode_state_machine", "==", "'all-layers_top_nopos'", "or", "\n", "encode_state_machine", "==", "'only_stack_nopos'", "or", "\n", "encode_state_machine", "==", "'only_buffer_nopos'", "or", "\n", "encode_state_machine", "==", "'only_stack_top_nopos'", "or", "\n", "encode_state_machine", "==", "'only_buffer_top_nopos'", "\n", ")", ":", "\n", "\n", "# default options", "\n", "        ", "do_stack", "=", "True", "\n", "do_buffer", "=", "True", "\n", "do_top", "=", "False", "\n", "\n", "if", "encode_state_machine", ".", "startswith", "(", "'only_buffer'", ")", ":", "\n", "            ", "do_stack", "=", "False", "\n", "", "if", "encode_state_machine", ".", "startswith", "(", "'only_stack'", ")", ":", "\n", "            ", "do_buffer", "=", "False", "\n", "\n", "# TODO: Change by _top", "\n", "", "if", "'top'", "in", "encode_state_machine", ".", "split", "(", "'_'", ")", ":", "\n", "            ", "do_top", "=", "True", "\n", "\n", "", "do_positions", "=", "'nopos'", "not", "in", "encode_state_machine", ".", "split", "(", "'_'", ")", "\n", "\n", "# stack/buffer state masks ", "\n", "head_attention_masks", ",", "head_positions", "=", "get_heads_stack_representation", "(", "\n", "memory", ",", "\n", "memory_pos", ",", "\n", "num_heads", ",", "\n", "embed_stack_positions", ",", "\n", "do_stack", "=", "do_stack", ",", "\n", "do_buffer", "=", "do_buffer", ",", "\n", "do_top", "=", "do_top", ",", "\n", "do_positions", "=", "do_positions", "\n", ")", "\n", "\n", "", "elif", "encode_state_machine", "in", "[", "'layer0'", ",", "'layer0_nopos'", "]", "and", "layer_index", "!=", "0", ":", "\n", "        ", "head_attention_masks", "=", "None", "\n", "head_positions", "=", "None", "\n", "\n", "", "else", ":", "\n", "        ", "raise", "Exception", "(", "\n", "\"Unknown encode_state_machine %s\"", "%", "\n", "encode_state_machine", "\n", ")", "\n", "\n", "", "return", "head_attention_masks", ",", "head_positions", "\n", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.preprocess.dataset_dest_prefix": [[20, 26], ["None"], "function", ["None"], ["\n", "\n", "\n", "from", "fairseq", ".", "tokenizer", "import", "tokenize_line", ",", "tab_tokenize", "\n", "from", "transition_amr_parser", ".", "stack_transformer", ".", "preprocess", "import", "make_state_machine", "\n", "\n", "def", "main", "(", "args", ")", ":", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.preprocess.dataset_dest_file": [[28, 31], ["preprocess.dataset_dest_prefix"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.preprocess.dataset_dest_prefix"], ["\n", "print", "(", "args", ")", "\n", "\n", "os", ".", "makedirs", "(", "args", ".", "destdir", ",", "exist_ok", "=", "True", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.preprocess.make_binary_stack": [[33, 205], ["transition_amr_parser.stack_transformer.amr_state_machine.get_action_indexer", "transition_amr_parser.stack_transformer.amr_state_machine.machine_generator", "collections.Counter", "transition_amr_parser.io.read_rule_stats", "fairseq.data.indexed_dataset.make_builder", "preprocess.dataset_dest_file", "fairseq.data.indexed_dataset.make_builder", "preprocess.dataset_dest_file", "fairseq.data.indexed_dataset.make_builder", "open", "open", "tqdm.tqdm", "print", "preprocess.dataset_dest_file", "indexed_data[].finalize", "preprocess.dataset_dest_file", "indexed_dataset.make_builder.finalize", "preprocess.dataset_dest_file", "indexed_dataset.make_builder.finalize", "print", "print", "os.path.isfile", "preprocess.dataset_dest_file", "tokenize", "tokenize", "transition_amr_parser.stack_transformer.amr_state_machine.machine_generator.", "numpy.zeros", "set", "enumerate", "transition_amr_parser.stack_transformer.amr_state_machine.yellow_font", "fid_act.readline", "len", "len", "transition_amr_parser.stack_transformer.amr_state_machine.get_word_states", "sent_data[].append", "sent_data[].append", "get_new_state_machine.applyAction", "indexed_data[].add_item", "list", "indexed_dataset.make_builder.add_item", "indexed_dataset.make_builder.add_item", "print", "get_new_state_machine.get_valid_actions", "torch.Tensor", "torch.Tensor", "torch.stack().view", "torch.Tensor().view", "torch.Tensor", "transition_amr_parser.stack_transformer.amr_state_machine.get_action_indexer.", "transition_amr_parser.stack_transformer.amr_state_machine.get_action_indexer.", "target_vocab.symbols.index", "target_vocab.symbols.index", "valid_action_idx.add", "collections.Counter.update", "len", "valid_action_idx.add", "torch.stack", "torch.Tensor", "list"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.get_action_indexer", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.machine_generator", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.io.read_rule_stats", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.make_builder", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.preprocess.dataset_dest_file", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.make_builder", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.preprocess.dataset_dest_file", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.make_builder", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.preprocess.dataset_dest_file", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDatasetBuilder.finalize", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.preprocess.dataset_dest_file", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDatasetBuilder.finalize", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.preprocess.dataset_dest_file", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDatasetBuilder.finalize", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.preprocess.dataset_dest_file", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.hub_utils.GeneratorHubInterface.tokenize", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.hub_utils.GeneratorHubInterface.tokenize", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.yellow_font", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.get_word_states", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.state_machine.DepParsingStateMachine.applyAction", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDatasetBuilder.add_item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDatasetBuilder.add_item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDatasetBuilder.add_item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.get_valid_actions", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.index", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.index", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.add", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.StateMachineBatch.update", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.utils.ConfusionMatrix.add"], ["\n", "task", "=", "tasks", ".", "get_task", "(", "args", ".", "task", ")", "\n", "\n", "\n", "\n", "# Determine tokenizer", "\n", "if", "args", ".", "tokenize_by_whitespace", ":", "\n", "# for the rest normal tokenization", "\n", "        ", "tokenize", "=", "tokenize_line", "\n", "", "else", ":", "\n", "# for AMR we expect tokenization by tab", "\n", "        ", "tokenize", "=", "tab_tokenize", "\n", "\n", "", "def", "train_path", "(", "lang", ")", ":", "\n", "        ", "return", "\"{}{}\"", ".", "format", "(", "args", ".", "trainpref", ",", "(", "\".\"", "+", "lang", ")", "if", "lang", "else", "\"\"", ")", "\n", "\n", "", "def", "valid_path", "(", "lang", ")", ":", "\n", "        ", "return", "\"{}{}\"", ".", "format", "(", "args", ".", "validpref", ",", "(", "\".\"", "+", "lang", ")", "if", "lang", "else", "\"\"", ")", "\n", "\n", "", "def", "file_name", "(", "prefix", ",", "lang", ")", ":", "\n", "        ", "fname", "=", "prefix", "\n", "if", "lang", "is", "not", "None", ":", "\n", "            ", "fname", "+=", "\".{lang}\"", ".", "format", "(", "lang", "=", "lang", ")", "\n", "", "return", "fname", "\n", "\n", "", "def", "dest_path", "(", "prefix", ",", "lang", ")", ":", "\n", "        ", "return", "os", ".", "path", ".", "join", "(", "args", ".", "destdir", ",", "file_name", "(", "prefix", ",", "lang", ")", ")", "\n", "\n", "", "def", "dict_path", "(", "lang", ")", ":", "\n", "        ", "return", "dest_path", "(", "\"dict\"", ",", "lang", ")", "+", "\".txt\"", "\n", "\n", "", "def", "build_dictionary", "(", "filenames", ",", "src", "=", "False", ",", "tgt", "=", "False", ",", "tokenize", "=", "tokenize", ")", ":", "\n", "        ", "assert", "src", "^", "tgt", "\n", "return", "task", ".", "build_dictionary", "(", "\n", "filenames", ",", "\n", "workers", "=", "args", ".", "workers", ",", "\n", "threshold", "=", "args", ".", "thresholdsrc", "if", "src", "else", "args", ".", "thresholdtgt", ",", "\n", "nwords", "=", "args", ".", "nwordssrc", "if", "src", "else", "args", ".", "nwordstgt", ",", "\n", "padding_factor", "=", "args", ".", "padding_factor", ",", "\n", "tokenize", "=", "tokenize", "\n", ")", "\n", "\n", "", "if", "not", "args", ".", "srcdict", "and", "os", ".", "path", ".", "exists", "(", "dict_path", "(", "args", ".", "source_lang", ")", ")", ":", "\n", "        ", "raise", "FileExistsError", "(", "dict_path", "(", "args", ".", "source_lang", ")", ")", "\n", "", "if", "target", "and", "not", "args", ".", "tgtdict", "and", "os", ".", "path", ".", "exists", "(", "dict_path", "(", "args", ".", "target_lang", ")", ")", ":", "\n", "        ", "raise", "FileExistsError", "(", "dict_path", "(", "args", ".", "target_lang", ")", ")", "\n", "\n", "", "if", "args", ".", "joined_dictionary", ":", "\n", "        ", "assert", "not", "args", ".", "srcdict", "or", "not", "args", ".", "tgtdict", ",", "\"cannot use both --srcdict and --tgtdict with --joined-dictionary\"", "\n", "\n", "if", "args", ".", "srcdict", ":", "\n", "            ", "src_dict", "=", "task", ".", "load_dictionary", "(", "args", ".", "srcdict", ")", "\n", "", "elif", "args", ".", "tgtdict", ":", "\n", "            ", "src_dict", "=", "task", ".", "load_dictionary", "(", "args", ".", "tgtdict", ")", "\n", "", "else", ":", "\n", "            ", "assert", "args", ".", "trainpref", ",", "\"--trainpref must be set if --srcdict is not specified\"", "\n", "src_dict", "=", "build_dictionary", "(", "\n", "{", "train_path", "(", "lang", ")", "for", "lang", "in", "[", "args", ".", "source_lang", ",", "args", ".", "target_lang", "]", "}", ",", "src", "=", "True", ",", "tokenize", "=", "tokenize", "\n", ")", "\n", "", "tgt_dict", "=", "src_dict", "\n", "", "else", ":", "\n", "        ", "if", "args", ".", "srcdict", ":", "\n", "            ", "src_dict", "=", "task", ".", "load_dictionary", "(", "args", ".", "srcdict", ")", "\n", "", "else", ":", "\n", "            ", "assert", "args", ".", "trainpref", ",", "\"--trainpref must be set if --srcdict is not specified\"", "\n", "# FIXME: Hacky way to avoid <unks>, add dev", "\n", "# As long as we use only BERT for input this should be fine", "\n", "dict_paths", "=", "[", "train_path", "(", "args", ".", "source_lang", ")", ",", "valid_path", "(", "args", ".", "source_lang", ")", "]", "\n", "src_dict", "=", "build_dictionary", "(", "dict_paths", ",", "src", "=", "True", ",", "tokenize", "=", "tokenize", ")", "\n", "\n", "", "if", "target", ":", "\n", "            ", "if", "args", ".", "tgtdict", ":", "\n", "                ", "tgt_dict", "=", "task", ".", "load_dictionary", "(", "args", ".", "tgtdict", ")", "\n", "", "else", ":", "\n", "                ", "assert", "args", ".", "trainpref", ",", "\"--trainpref must be set if --tgtdict is not specified\"", "\n", "# Hacky way to avoid <unks>, add dev", "\n", "# dict_paths = [train_path(args.target_lang), valid_path(args.target_lang)]", "\n", "dict_paths", "=", "[", "train_path", "(", "args", ".", "target_lang", ")", "]", "\n", "tgt_dict", "=", "build_dictionary", "(", "dict_paths", ",", "tgt", "=", "True", ",", "tokenize", "=", "tokenize", ")", "\n", "", "", "else", ":", "\n", "            ", "tgt_dict", "=", "None", "\n", "\n", "", "", "src_dict", ".", "save", "(", "dict_path", "(", "args", ".", "source_lang", ")", ")", "\n", "if", "target", "and", "tgt_dict", "is", "not", "None", ":", "\n", "        ", "tgt_dict", ".", "save", "(", "dict_path", "(", "args", ".", "target_lang", ")", ")", "\n", "\n", "", "def", "make_binary_dataset", "(", "vocab", ",", "input_prefix", ",", "output_prefix", ",", "lang", ",", "num_workers", ")", ":", "\n", "        ", "print", "(", "\"| [{}] Dictionary: {} types\"", ".", "format", "(", "lang", ",", "len", "(", "vocab", ")", "-", "1", ")", ")", "\n", "n_seq_tok", "=", "[", "0", ",", "0", "]", "\n", "replaced", "=", "Counter", "(", ")", "\n", "\n", "def", "merge_result", "(", "worker_result", ")", ":", "\n", "            ", "replaced", ".", "update", "(", "worker_result", "[", "\"replaced\"", "]", ")", "\n", "n_seq_tok", "[", "0", "]", "+=", "worker_result", "[", "\"nseq\"", "]", "\n", "n_seq_tok", "[", "1", "]", "+=", "worker_result", "[", "\"ntok\"", "]", "\n", "\n", "", "input_file", "=", "\"{}{}\"", ".", "format", "(", "\n", "input_prefix", ",", "(", "\".\"", "+", "lang", ")", "if", "lang", "is", "not", "None", "else", "\"\"", "\n", ")", "\n", "offsets", "=", "Binarizer", ".", "find_offsets", "(", "input_file", ",", "num_workers", ")", "\n", "pool", "=", "None", "\n", "if", "num_workers", ">", "1", ":", "\n", "            ", "raise", "Exception", "(", "\"to set tokenize, we can not allow num_workers > 1\"", ")", "\n", "pool", "=", "Pool", "(", "processes", "=", "num_workers", "-", "1", ")", "\n", "for", "worker_id", "in", "range", "(", "1", ",", "num_workers", ")", ":", "\n", "                ", "prefix", "=", "\"{}{}\"", ".", "format", "(", "output_prefix", ",", "worker_id", ")", "\n", "pool", ".", "apply_async", "(", "\n", "binarize", ",", "\n", "(", "\n", "args", ",", "\n", "input_file", ",", "\n", "vocab", ",", "\n", "prefix", ",", "\n", "lang", ",", "\n", "offsets", "[", "worker_id", "]", ",", "\n", "offsets", "[", "worker_id", "+", "1", "]", ",", "\n", "tokenize", "# this may fail", "\n", ")", ",", "\n", "callback", "=", "merge_result", "\n", ")", "\n", "", "pool", ".", "close", "(", ")", "\n", "\n", "", "ds", "=", "indexed_dataset", ".", "make_builder", "(", "dataset_dest_file", "(", "args", ",", "output_prefix", ",", "lang", ",", "\"bin\"", ")", ",", "\n", "impl", "=", "args", ".", "dataset_impl", ",", "vocab_size", "=", "len", "(", "vocab", ")", ")", "\n", "merge_result", "(", "\n", "Binarizer", ".", "binarize", "(", "\n", "input_file", ",", "vocab", ",", "lambda", "t", ":", "ds", ".", "add_item", "(", "t", ")", ",", "\n", "offset", "=", "0", ",", "end", "=", "offsets", "[", "1", "]", ",", "\n", "append_eos", "=", "False", ",", "\n", "tokenize", "=", "tokenize", "\n", ")", "\n", ")", "\n", "if", "num_workers", ">", "1", ":", "\n", "            ", "pool", ".", "join", "(", ")", "\n", "for", "worker_id", "in", "range", "(", "1", ",", "num_workers", ")", ":", "\n", "                ", "prefix", "=", "\"{}{}\"", ".", "format", "(", "output_prefix", ",", "worker_id", ")", "\n", "temp_file_path", "=", "dataset_dest_prefix", "(", "args", ",", "prefix", ",", "lang", ")", "\n", "ds", ".", "merge_file_", "(", "temp_file_path", ")", "\n", "os", ".", "remove", "(", "indexed_dataset", ".", "data_file_path", "(", "temp_file_path", ")", ")", "\n", "os", ".", "remove", "(", "indexed_dataset", ".", "index_file_path", "(", "temp_file_path", ")", ")", "\n", "\n", "", "", "ds", ".", "finalize", "(", "dataset_dest_file", "(", "args", ",", "output_prefix", ",", "lang", ",", "\"idx\"", ")", ")", "\n", "\n", "print", "(", "\n", "\"| [{}] {}: {} sents, {} tokens, {:.3}% replaced by {}\"", ".", "format", "(", "\n", "lang", ",", "\n", "input_file", ",", "\n", "n_seq_tok", "[", "0", "]", ",", "\n", "n_seq_tok", "[", "1", "]", ",", "\n", "100", "*", "sum", "(", "replaced", ".", "values", "(", ")", ")", "/", "n_seq_tok", "[", "1", "]", ",", "\n", "vocab", ".", "unk_word", ",", "\n", ")", "\n", ")", "\n", "\n", "", "def", "make_dataset", "(", "vocab", ",", "input_prefix", ",", "output_prefix", ",", "lang", ",", "num_workers", "=", "1", ")", ":", "\n", "        ", "if", "args", ".", "dataset_impl", "==", "\"raw\"", ":", "\n", "# Copy original text file to destination folder", "\n", "            ", "output_text_file", "=", "dest_path", "(", "\n", "output_prefix", "+", "\".{}-{}\"", ".", "format", "(", "args", ".", "source_lang", ",", "args", ".", "target_lang", ")", ",", "\n", "lang", ",", "\n", ")", "\n", "shutil", ".", "copyfile", "(", "file_name", "(", "input_prefix", ",", "lang", ")", ",", "output_text_file", ")", "\n", "", "else", ":", "\n", "            ", "make_binary_dataset", "(", "vocab", ",", "input_prefix", ",", "output_prefix", ",", "lang", ",", "num_workers", ")", "\n", "\n", "", "", "def", "make_all", "(", "lang", ",", "vocab", ")", ":", "\n", "        ", "if", "args", ".", "trainpref", ":", "\n", "            ", "make_dataset", "(", "vocab", ",", "args", ".", "trainpref", ",", "\"train\"", ",", "lang", ",", "num_workers", "=", "args", ".", "workers", ")", "\n", "", "if", "args", ".", "validpref", ":", "\n", "            ", "for", "k", ",", "validpref", "in", "enumerate", "(", "args", ".", "validpref", ".", "split", "(", "\",\"", ")", ")", ":", "\n", "                ", "outprefix", "=", "\"valid{}\"", ".", "format", "(", "k", ")", "if", "k", ">", "0", "else", "\"valid\"", "\n", "make_dataset", "(", "vocab", ",", "validpref", ",", "outprefix", ",", "lang", ",", "num_workers", "=", "args", ".", "workers", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.preprocess.make_binary_bert_features": [[207, 283], ["transition_amr_parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings", "fairseq.data.indexed_dataset.make_builder", "fairseq.data.indexed_dataset.make_builder", "fairseq.data.indexed_dataset.make_builder", "indexed_dataset.make_builder.finalize", "indexed_dataset.make_builder.finalize", "indexed_dataset.make_builder.finalize", "preprocess.dataset_dest_file", "preprocess.dataset_dest_file", "preprocess.dataset_dest_file", "open", "print", "preprocess.dataset_dest_file", "preprocess.dataset_dest_file", "preprocess.dataset_dest_file", "transition_amr_parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.extract", "indexed_dataset.make_builder.add_item", "indexed_dataset.make_builder.add_item", "indexed_dataset.make_builder.add_item", "tokenize", "len", "word_features.cpu().view", "preprocess.get_scatter_indices", "print", "str().rstrip", "sentence.split", "word_features.cpu", "str"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.make_builder", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.make_builder", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.make_builder", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDatasetBuilder.finalize", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDatasetBuilder.finalize", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDatasetBuilder.finalize", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.preprocess.dataset_dest_file", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.preprocess.dataset_dest_file", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.preprocess.dataset_dest_file", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.preprocess.dataset_dest_file", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.preprocess.dataset_dest_file", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.preprocess.dataset_dest_file", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.extract", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDatasetBuilder.add_item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDatasetBuilder.add_item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.indexed_dataset.MMapIndexedDatasetBuilder.add_item", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.hub_utils.GeneratorHubInterface.tokenize", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.get_scatter_indices", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split"], ["            ", "for", "k", ",", "testpref", "in", "enumerate", "(", "args", ".", "testpref", ".", "split", "(", "\",\"", ")", ")", ":", "\n", "                ", "outprefix", "=", "\"test{}\"", ".", "format", "(", "k", ")", "if", "k", ">", "0", "else", "\"test\"", "\n", "make_dataset", "(", "vocab", ",", "testpref", ",", "outprefix", ",", "lang", ",", "num_workers", "=", "args", ".", "workers", ")", "\n", "\n", "", "", "", "make_all", "(", "args", ".", "source_lang", ",", "src_dict", ")", "\n", "if", "target", ":", "\n", "        ", "make_all", "(", "args", ".", "target_lang", ",", "tgt_dict", ")", "\n", "\n", "# Make preprocessing data for the state machine", "\n", "", "make_state_machine", "(", "args", ",", "src_dict", ",", "tgt_dict", ",", "tokenize", "=", "tokenize", ")", "\n", "\n", "print", "(", "\"| Wrote preprocessed data to {}\"", ".", "format", "(", "args", ".", "destdir", ")", ")", "\n", "\n", "if", "args", ".", "alignfile", ":", "\n", "        ", "assert", "args", ".", "trainpref", ",", "\"--trainpref must be set if --alignfile is specified\"", "\n", "src_file_name", "=", "train_path", "(", "args", ".", "source_lang", ")", "\n", "tgt_file_name", "=", "train_path", "(", "args", ".", "target_lang", ")", "\n", "freq_map", "=", "{", "}", "\n", "with", "open", "(", "args", ".", "alignfile", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "align_file", ":", "\n", "            ", "with", "open", "(", "src_file_name", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "src_file", ":", "\n", "                ", "with", "open", "(", "tgt_file_name", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "tgt_file", ":", "\n", "                    ", "for", "a", ",", "s", ",", "t", "in", "zip_longest", "(", "align_file", ",", "src_file", ",", "tgt_file", ")", ":", "\n", "                        ", "si", "=", "src_dict", ".", "encode_line", "(", "s", ",", "add_if_not_exist", "=", "False", ")", "\n", "ti", "=", "tgt_dict", ".", "encode_line", "(", "t", ",", "add_if_not_exist", "=", "False", ")", "\n", "ai", "=", "list", "(", "map", "(", "lambda", "x", ":", "tuple", "(", "x", ".", "split", "(", "\"-\"", ")", ")", ",", "a", ".", "split", "(", ")", ")", ")", "\n", "for", "sai", ",", "tai", "in", "ai", ":", "\n", "                            ", "srcidx", "=", "si", "[", "int", "(", "sai", ")", "]", "\n", "tgtidx", "=", "ti", "[", "int", "(", "tai", ")", "]", "\n", "if", "srcidx", "!=", "src_dict", ".", "unk", "(", ")", "and", "tgtidx", "!=", "tgt_dict", ".", "unk", "(", ")", ":", "\n", "                                ", "assert", "srcidx", "!=", "src_dict", ".", "pad", "(", ")", "\n", "assert", "srcidx", "!=", "src_dict", ".", "eos", "(", ")", "\n", "assert", "tgtidx", "!=", "tgt_dict", ".", "pad", "(", ")", "\n", "assert", "tgtidx", "!=", "tgt_dict", ".", "eos", "(", ")", "\n", "\n", "if", "srcidx", "not", "in", "freq_map", ":", "\n", "                                    ", "freq_map", "[", "srcidx", "]", "=", "{", "}", "\n", "", "if", "tgtidx", "not", "in", "freq_map", "[", "srcidx", "]", ":", "\n", "                                    ", "freq_map", "[", "srcidx", "]", "[", "tgtidx", "]", "=", "1", "\n", "", "else", ":", "\n", "                                    ", "freq_map", "[", "srcidx", "]", "[", "tgtidx", "]", "+=", "1", "\n", "\n", "", "", "", "", "", "", "", "align_dict", "=", "{", "}", "\n", "for", "srcidx", "in", "freq_map", ".", "keys", "(", ")", ":", "\n", "            ", "align_dict", "[", "srcidx", "]", "=", "max", "(", "freq_map", "[", "srcidx", "]", ",", "key", "=", "freq_map", "[", "srcidx", "]", ".", "get", ")", "\n", "\n", "", "with", "open", "(", "\n", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "destdir", ",", "\n", "\"alignment.{}-{}.txt\"", ".", "format", "(", "args", ".", "source_lang", ",", "args", ".", "target_lang", ")", ",", "\n", ")", ",", "\n", "\"w\"", ",", "encoding", "=", "'utf-8'", "\n", ")", "as", "f", ":", "\n", "            ", "for", "k", ",", "v", "in", "align_dict", ".", "items", "(", ")", ":", "\n", "                ", "print", "(", "\"{} {}\"", ".", "format", "(", "src_dict", "[", "k", "]", ",", "tgt_dict", "[", "v", "]", ")", ",", "file", "=", "f", ")", "\n", "\n", "\n", "# FIXME: This was False pre BERT merge", "\n", "# def binarize(args, filename, vocab, output_prefix, lang, offset, end, append_eos=False):", "\n", "\n", "", "", "", "", "def", "binarize", "(", "args", ",", "filename", ",", "vocab", ",", "output_prefix", ",", "lang", ",", "offset", ",", "end", ",", "append_eos", "=", "True", ",", "tokenize", "=", "tokenize_line", ")", ":", "\n", "    ", "ds", "=", "indexed_dataset", ".", "make_builder", "(", "dataset_dest_file", "(", "args", ",", "output_prefix", ",", "lang", ",", "\"bin\"", ")", ",", "\n", "impl", "=", "args", ".", "dataset_impl", ",", "vocab_size", "=", "len", "(", "vocab", ")", ")", "\n", "\n", "def", "consumer", "(", "tensor", ")", ":", "\n", "        ", "ds", ".", "add_item", "(", "tensor", ")", "\n", "\n", "", "res", "=", "Binarizer", ".", "binarize", "(", "filename", ",", "vocab", ",", "consumer", ",", "append_eos", "=", "append_eos", ",", "\n", "offset", "=", "offset", ",", "end", "=", "end", ",", "tokenize", "=", "tokenize", ")", "\n", "ds", ".", "finalize", "(", "dataset_dest_file", "(", "args", ",", "output_prefix", ",", "lang", ",", "\"idx\"", ")", ")", "\n", "return", "res", "\n", "\n", "\n", "", "def", "dataset_dest_prefix", "(", "args", ",", "output_prefix", ",", "lang", ")", ":", "\n", "    ", "base", "=", "\"{}/{}\"", ".", "format", "(", "args", ".", "destdir", ",", "output_prefix", ")", "\n", "lang_part", "=", "(", "\n", "\".{}-{}.{}\"", ".", "format", "(", "args", ".", "source_lang", ",", "args", ".", "target_lang", ",", "lang", ")", "if", "lang", "is", "not", "None", "else", "\"\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.preprocess.make_masks": [[286, 292], ["NotImplementedError", "preprocess.make_binary_stack"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.preprocess.make_binary_stack"], ["\n", "", "def", "dataset_dest_file", "(", "args", ",", "output_prefix", ",", "lang", ",", "extension", ")", ":", "\n", "    ", "base", "=", "dataset_dest_prefix", "(", "args", ",", "output_prefix", ",", "lang", ")", "\n", "return", "\"{}.{}\"", ".", "format", "(", "base", ",", "extension", ")", "\n", "\n", "\n", "", "def", "get_offsets", "(", "input_file", ",", "num_workers", ")", ":", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.preprocess.make_state_machine": [[294, 316], ["preprocess.make_binary_bert_features", "preprocess.make_masks", "enumerate", "enumerate", "args.validpref.split", "preprocess.make_binary_bert_features", "preprocess.make_masks", "args.testpref.split", "preprocess.make_binary_bert_features", "preprocess.make_masks"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.preprocess.make_binary_bert_features", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.preprocess.make_masks", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.preprocess.make_binary_bert_features", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.preprocess.make_masks", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.preprocess.make_binary_bert_features", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.preprocess.make_masks"], ["\n", "\n", "", "def", "cli_main", "(", ")", ":", "\n", "    ", "parser", "=", "options", ".", "get_preprocessing_parser", "(", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "main", "(", "args", ")", "\n", "\n", "\n", "", "if", "__name__", "==", "\"__main__\"", ":", "\n", "    ", "cli_main", "(", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.preprocess.get_scatter_indices": [[318, 330], ["torch.tensor", "range", "range", "len", "zip", "len", "isinstance", "len"], "function", ["None"], []], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.__init__": [[171, 223], ["torch.hub.load", "pretrained_embeddings.PretrainedEmbeddings.roberta.eval", "torch.cuda.is_available", "pretrained_embeddings.PretrainedEmbeddings.roberta.cuda", "print", "print", "fairseq.models.roberta.RobertaModel.from_pretrained", "pretrained_embeddings.PretrainedEmbeddings.roberta.eval", "torch.cuda.is_available", "pretrained_embeddings.PretrainedEmbeddings.roberta.cuda", "print", "print", "fairseq.models.roberta.model_gottbert.GottbertModel.from_pretrained", "pretrained_embeddings.PretrainedEmbeddings.roberta.eval", "torch.cuda.is_available", "Exception", "pretrained_embeddings.PretrainedEmbeddings.roberta.cuda", "print", "print"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.dictionary.Dictionary.load", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.model_gottbert.GottbertModel.from_pretrained", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.model_gottbert.GottbertModel.from_pretrained", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.utils.eval", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print"], ["    ", "def", "__init__", "(", "self", ",", "name", ",", "bert_layers", ",", "model", "=", "None", ")", ":", "\n", "\n", "# embedding type name", "\n", "        ", "self", ".", "name", "=", "name", "\n", "# select some layers for averaging", "\n", "self", ".", "bert_layers", "=", "bert_layers", "\n", "\n", "if", "model", "is", "None", ":", "\n", "            ", "if", "name", "in", "[", "'roberta.large'", "]", ":", "\n", "\n", "# Extract", "\n", "                ", "self", ".", "roberta", "=", "torch", ".", "hub", ".", "load", "(", "'pytorch/fairseq'", ",", "name", ")", "\n", "#self.roberta = RobertaModel.from_pretrained('roberta.large', 'model.pt', './roberta.large')", "\n", "\n", "\n", "self", ".", "roberta", ".", "eval", "(", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "                    ", "self", ".", "roberta", ".", "cuda", "(", ")", "\n", "print", "(", "f'Using {name} extraction in GPU'", ")", "\n", "", "else", ":", "\n", "                    ", "print", "(", "f'Using {name} extraction in cpu (slow, wont OOM)'", ")", "\n", "\n", "", "", "elif", "name", "in", "[", "'roberta.base'", "]", ":", "\n", "\n", "\n", "                ", "self", ".", "roberta", "=", "RobertaModel", ".", "from_pretrained", "(", "'roberta.base'", ",", "'model.pt'", ",", "'./roberta.base'", ")", "\n", "\n", "\n", "self", ".", "roberta", ".", "eval", "(", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "                    ", "self", ".", "roberta", ".", "cuda", "(", ")", "\n", "print", "(", "f'Using {name} extraction in GPU'", ")", "\n", "", "else", ":", "\n", "                    ", "print", "(", "f'Using {name} extraction in cpu (slow, wont OOM)'", ")", "\n", "\n", "", "", "elif", "name", "in", "[", "'gottbert-base'", "]", ":", "\n", "\n", "                ", "self", ".", "roberta", "=", "GottbertModel", ".", "from_pretrained", "(", "'./gottbert-base'", ")", "\n", "\n", "\n", "self", ".", "roberta", ".", "eval", "(", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "                    ", "self", ".", "roberta", ".", "cuda", "(", ")", "\n", "print", "(", "f'Using {name} extraction in GPU'", ")", "\n", "", "else", ":", "\n", "                    ", "print", "(", "f'Using {name} extraction in cpu (slow, wont OOM)'", ")", "\n", "", "", "else", ":", "\n", "                ", "raise", "Exception", "(", "\n", "f'Unknown --pretrained-embed {name}'", "\n", ")", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "roberta", "=", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.extract_features": [[224, 242], ["pretrained_embeddings.PretrainedEmbeddings.roberta.extract_features", "pretrained_embeddings.PretrainedEmbeddings.roberta.extract_features", "sum", "torch.div", "sum.append", "len"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.extract_features", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.extract_features", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "", "def", "extract_features", "(", "self", ",", "worpieces", ")", ":", "\n", "        ", "\"\"\"Extract features from wordpieces\"\"\"", "\n", "\n", "if", "self", ".", "bert_layers", "is", "None", ":", "\n", "# normal RoBERTa", "\n", "            ", "return", "self", ".", "roberta", ".", "extract_features", "(", "worpieces", ")", "\n", "", "else", ":", "\n", "# layer average RoBERTa", "\n", "            ", "features", "=", "self", ".", "roberta", ".", "extract_features", "(", "\n", "worpieces", ",", "\n", "return_all_hiddens", "=", "True", "\n", ")", "\n", "# sum layers", "\n", "feature_layers", "=", "[", "]", "\n", "for", "layer_index", "in", "self", ".", "bert_layers", ":", "\n", "                ", "feature_layers", ".", "append", "(", "features", "[", "layer_index", "]", ")", "\n", "", "feature_layers", "=", "sum", "(", "feature_layers", ")", "\n", "return", "torch", ".", "div", "(", "feature_layers", ",", "len", "(", "self", ".", "bert_layers", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.extract": [[243, 312], ["pretrained_embeddings.get_wordpiece_to_word_map", "pretrained_embeddings.PretrainedEmbeddings.roberta.encode", "pretrained_embeddings.PretrainedEmbeddings.detach", "pretrained_embeddings.get_average_embeddings", "pretrained_embeddings.PretrainedEmbeddings.extract_features", "pretrained_embeddings.PretrainedEmbeddings.extract_features", "torch.cat", "print", "pretrained_embeddings.PretrainedEmbeddings.extract_features", "transition_amr_parser.stack_transformer.amr_state_machine.yellow_font", "pretrained_embeddings.PretrainedEmbeddings.to", "pretrained_embeddings.PretrainedEmbeddings.to", "pretrained_embeddings.PretrainedEmbeddings.to"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.get_wordpiece_to_word_map", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.encode", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.get_average_embeddings", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.extract_features", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.extract_features", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.extract_features", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.amr_state_machine.yellow_font"], ["", "", "def", "extract", "(", "self", ",", "sentence_string", ")", ":", "\n", "        ", "\"\"\"\n        sentence_string (not tokenized)\n        \"\"\"", "\n", "\n", "# get words, wordpieces and mapping", "\n", "# FIXME: PTB oracle already tokenized", "\n", "word2piece", "=", "get_wordpiece_to_word_map", "(", "\n", "sentence_string", ",", "\n", "self", ".", "roberta", ".", "bpe", "\n", ")", "\n", "\n", "# NOTE: We need to re-extract BPE inside roberta. Token indices", "\n", "# will also be different. BOS/EOS added", "\n", "worpieces_roberta", "=", "self", ".", "roberta", ".", "encode", "(", "sentence_string", ")", "\n", "\n", "# Extract roberta, remove BOS/EOS", "\n", "# Hotfix for sequences above 512", "\n", "if", "worpieces_roberta", ".", "shape", "[", "0", "]", ">", "512", ":", "\n", "            ", "excess", "=", "worpieces_roberta", ".", "shape", "[", "0", "]", "-", "512", "\n", "# first 512 tokens", "\n", "last_layer", "=", "self", ".", "extract_features", "(", "\n", "worpieces_roberta", ".", "to", "(", "self", ".", "roberta", ".", "device", ")", "[", ":", "512", "]", "\n", ")", "\n", "# last 512 tokens", "\n", "last_layer2", "=", "self", ".", "extract_features", "(", "\n", "worpieces_roberta", ".", "to", "(", "self", ".", "roberta", ".", "device", ")", "[", "excess", ":", "]", "\n", ")", "\n", "# concatenate", "\n", "shape", "=", "(", "last_layer", ",", "last_layer2", "[", ":", ",", "-", "excess", ":", ",", ":", "]", ")", "\n", "last_layer", "=", "torch", ".", "cat", "(", "shape", ",", "1", ")", "\n", "\n", "assert", "worpieces_roberta", ".", "shape", "[", "0", "]", "==", "last_layer", ".", "shape", "[", "1", "]", "\n", "\n", "# warn user about this", "\n", "string", "=", "'\\nMAX_POS overflow!! {worpieces_roberta.shape[0]}'", "\n", "print", "(", "yellow_font", "(", "string", ")", ")", "\n", "\n", "", "else", ":", "\n", "\n", "# Normal extraction", "\n", "            ", "last_layer", "=", "self", ".", "extract_features", "(", "\n", "worpieces_roberta", ".", "to", "(", "self", ".", "roberta", ".", "device", ")", "\n", ")", "\n", "\n", "# FIXME: this should not bee needed using roberta.eval()", "\n", "", "last_layer", "=", "last_layer", ".", "detach", "(", ")", "\n", "\n", "# Ignore start and end symbols", "\n", "last_layer", "=", "last_layer", "[", "0", ":", "1", ",", "1", ":", "-", "1", ",", ":", "]", "\n", "\n", "# average over wordpieces of same word", "\n", "word_features", "=", "get_average_embeddings", "(", "\n", "last_layer", ",", "\n", "word2piece", "\n", ")", "\n", "\n", "#        # sanity check differentiable and non differentiable averaging", "\n", "#        match", "\n", "#        from torch_scatter import scatter_mean", "\n", "#        word_features2 = scatter_mean(", "\n", "#            last_layer[0, :, :],", "\n", "#            get_scatter_indices(word2piece).to(roberta.device),", "\n", "#            dim=0", "\n", "#        )", "\n", "#        # This works", "\n", "#        assert np.allclose(word_features.cpu(), word_features2.cpu())", "\n", "\n", "return", "word_features", ",", "worpieces_roberta", ",", "word2piece", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.extract_batch": [[313, 340], ["fairseq.data.data_utils.collate_tokens", "pretrained_embeddings.PretrainedEmbeddings.extract_features", "roberta_batch_features.detach().cpu.detach().cpu.detach().cpu", "enumerate", "pretrained_embeddings.get_wordpiece_to_word_map", "pretrained_embeddings.PretrainedEmbeddings.roberta.encode", "src_wordpieces.append", "src_word2piece.append", "zip", "pretrained_embeddings.get_average_embeddings", "pretrained_embeddings.get_scatter_indices", "bert_data[].append", "bert_data[].append", "bert_data[].append", "copy.deepcopy", "copy.deepcopy", "roberta_batch_features.detach().cpu.detach().cpu.detach", "roberta_features.unsqueeze", "len"], "methods", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.data.data_utils.collate_tokens", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.PretrainedEmbeddings.extract_features", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.get_wordpiece_to_word_map", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.encode", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.get_average_embeddings", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.get_scatter_indices", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "extract_batch", "(", "self", ",", "sentence_string_batch", ")", ":", "\n", "        ", "bert_data", "=", "{", "}", "\n", "bert_data", "[", "\"word_features\"", "]", "=", "[", "]", "\n", "bert_data", "[", "\"wordpieces_roberta\"", "]", "=", "[", "]", "\n", "bert_data", "[", "\"word2piece_scattered_indices\"", "]", "=", "[", "]", "\n", "src_wordpieces", "=", "[", "]", "\n", "src_word2piece", "=", "[", "]", "\n", "for", "sentence", "in", "sentence_string_batch", ":", "\n", "            ", "word2piece", "=", "get_wordpiece_to_word_map", "(", "sentence", ",", "self", ".", "roberta", ".", "bpe", ")", "\n", "wordpieces_roberta", "=", "self", ".", "roberta", ".", "encode", "(", "sentence", ")", "\n", "wordpieces_roberta", "=", "wordpieces_roberta", "[", ":", "512", "]", "\n", "src_wordpieces", ".", "append", "(", "copy", ".", "deepcopy", "(", "wordpieces_roberta", ")", ")", "\n", "src_word2piece", ".", "append", "(", "copy", ".", "deepcopy", "(", "word2piece", ")", ")", "\n", "\n", "", "src_wordpieces_collated", "=", "collate_tokens", "(", "src_wordpieces", ",", "pad_idx", "=", "1", ")", "\n", "roberta_batch_features", "=", "self", ".", "extract_features", "(", "src_wordpieces_collated", ")", "\n", "roberta_batch_features", "=", "roberta_batch_features", ".", "detach", "(", ")", ".", "cpu", "(", ")", "\n", "for", "index", ",", "(", "word2piece", ",", "wordpieces_roberta", ")", "in", "enumerate", "(", "zip", "(", "src_word2piece", ",", "src_wordpieces", ")", ")", ":", "\n", "            ", "roberta_features", "=", "roberta_batch_features", "[", "index", "]", "\n", "roberta_features", "=", "roberta_features", "[", "1", ":", "len", "(", "wordpieces_roberta", ")", "-", "1", "]", "\n", "word_features", "=", "get_average_embeddings", "(", "roberta_features", ".", "unsqueeze", "(", "0", ")", ",", "word2piece", ")", "\n", "word2piece_scattered_indices", "=", "get_scatter_indices", "(", "word2piece", ",", "reverse", "=", "True", ")", "\n", "bert_data", "[", "\"word_features\"", "]", ".", "append", "(", "word_features", "[", "0", "]", ")", "\n", "bert_data", "[", "\"wordpieces_roberta\"", "]", ".", "append", "(", "wordpieces_roberta", ")", "\n", "bert_data", "[", "\"word2piece_scattered_indices\"", "]", ".", "append", "(", "word2piece_scattered_indices", ")", "\n", "\n", "", "return", "bert_data", "\n", "", "", ""]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.get_average_embeddings": [[11, 31], ["len", "torch.zeros().to", "enumerate", "isinstance", "torch.zeros", "column.mean.mean"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack-transformer.rank_model.mean"], ["def", "get_average_embeddings", "(", "final_layer", ",", "word2piece", ")", ":", "\n", "\n", "# Average worpiece representations to get word representations", "\n", "    ", "num_words", "=", "len", "(", "word2piece", ")", "\n", "batch_dim", ",", "num_wordpieces", ",", "hidden_size", "=", "final_layer", ".", "shape", "\n", "assert", "batch_dim", "==", "1", ",", "\"batch_size must be 1\"", "\n", "if", "num_words", "<", "num_wordpieces", ":", "\n", "        ", "word_features", "=", "torch", ".", "zeros", "(", "\n", "(", "1", ",", "num_words", ",", "hidden_size", ")", "\n", ")", ".", "to", "(", "final_layer", ".", "device", ")", "\n", "for", "word_idx", ",", "wordpiece_idx", "in", "enumerate", "(", "word2piece", ")", ":", "\n", "# column of features for all involved worpieces", "\n", "            ", "column", "=", "final_layer", "[", "0", ":", "1", ",", "wordpiece_idx", ",", ":", "]", "\n", "if", "isinstance", "(", "wordpiece_idx", ",", "list", ")", ":", "\n", "                ", "column", "=", "column", ".", "mean", "(", "1", ",", "keepdim", "=", "True", ")", "\n", "", "word_features", "[", "0", ":", "1", ",", "word_idx", ",", ":", "]", "=", "column", "\n", "", "", "else", ":", "\n", "        ", "word_features", "=", "final_layer", "\n", "\n", "", "return", "word_features", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.get_wordpiece_to_word_map": [[33, 153], ["sentence.split", "isinstance", "isinstance", "range", "roberta_bpe.decode", "len", "len", "len", "enumerate", "ord", "roberta_bpe.encode().split", "range", "len", "len", "print", "print", "print", "print", "len", "len", "len", "print", "print", "print", "print", "print", "len", "word_to_wordpiece.append", "subword_sequence.append", "len", "len", "ord", "wptok.append", "print", "range", "len", "len", "len", "len", "roberta_bpe.encode", "word_to_wordpiece.append", "ord", "len", "word_to_wordpiece.append", "subword_sequence.append", "wordpiece_tokens[].lstrip", "word_to_wordpiece.append", "ord", "wptok.append", "wptok.append", "word_to_wordpiece.append", "wptok[].lstrip", "len", "len", "word_to_wordpiece.append"], "function", ["home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.decode", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.transition_amr_parser.amr.AMR.split", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.fairseq.progress_bar.tensorboard_log_wrapper.print", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.encoders.sentencepiece_bpe.SentencepieceBPE.encode", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append", "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.data_utils.Examples.append"], ["", "def", "get_wordpiece_to_word_map", "(", "sentence", ",", "roberta_bpe", ")", ":", "\n", "# replace all instances of 3-4 byte characters by '@'", "\n", "    ", "converted_sentence", "=", "''", "\n", "for", "char", "in", "sentence", ":", "\n", "        ", "if", "ord", "(", "char", ")", "<", "2304", ":", "\n", "            ", "converted_sentence", "+=", "char", "\n", "", "else", ":", "\n", "            ", "converted_sentence", "+=", "\"@\"", "\n", "", "if", "sentence", "!=", "converted_sentence", ":", "\n", "            ", "sentence", "=", "converted_sentence", "\n", "# 3-4 byte character conversion ends here", "\n", "\n", "# Get word and worpiece tokens according to RoBERTa", "\n", "# sentence = sentence.replace(u'\\x91', u' ')", "\n", "# sentence = sentence.replace(u'\\x98', u' ')", "\n", "\n", "", "", "word_tokens", "=", "sentence", ".", "split", "(", ")", "\n", "wordpiece_tokens", "=", "[", "\n", "roberta_bpe", ".", "decode", "(", "wordpiece", ")", "\n", "for", "wordpiece", "in", "roberta_bpe", ".", "encode", "(", "sentence", ")", ".", "split", "(", ")", "\n", "]", "\n", "\n", "assert", "len", "(", "word_tokens", ")", "<=", "len", "(", "wordpiece_tokens", ")", "\n", "assert", "isinstance", "(", "word_tokens", ",", "list", ")", "\n", "assert", "isinstance", "(", "wordpiece_tokens", ",", "list", ")", "\n", "\n", "w_index", "=", "0", "\n", "word_to_wordpiece", "=", "[", "]", "\n", "subword_sequence", "=", "[", "]", "\n", "bad_unicode_flag", "=", "0", "\n", "overrun_sentence_flag", "=", "0", "\n", "for", "wp_index", "in", "range", "(", "len", "(", "wordpiece_tokens", ")", ")", ":", "\n", "        ", "if", "w_index", "in", "range", "(", "len", "(", "word_tokens", ")", ")", ":", "\n", "            ", "word", "=", "word_tokens", "[", "w_index", "]", "\n", "if", "word", "==", "wordpiece_tokens", "[", "wp_index", "]", ":", "\n", "                ", "word_to_wordpiece", ".", "append", "(", "wp_index", ")", "\n", "w_index", "+=", "1", "\n", "\n", "", "else", ":", "\n", "                ", "subword_sequence", ".", "append", "(", "wp_index", ")", "\n", "word_from_pieces", "=", "\"\"", ".", "join", "(", "[", "\n", "# NOTE: Facebooks BPE signals SOW with whitesplace", "\n", "wordpiece_tokens", "[", "i", "]", ".", "lstrip", "(", ")", "\n", "for", "i", "in", "subword_sequence", "\n", "]", ")", "\n", "if", "word", "==", "word_from_pieces", ":", "\n", "                    ", "word_to_wordpiece", ".", "append", "(", "subword_sequence", ")", "\n", "w_index", "+=", "1", "\n", "subword_sequence", "=", "[", "]", "\n", "", "elif", "word_from_pieces", "not", "in", "word", ":", "\n", "                    ", "word_to_wordpiece", ".", "append", "(", "subword_sequence", ")", "\n", "w_index", "+=", "1", "\n", "subword_sequence", "=", "[", "]", "\n", "bad_unicode_flag", "=", "1", "\n", "# assert word_from_pieces in word, \\", "\n", "#    \"wordpiece must be at least a segment of current word\"", "\n", "", "", "", "", "if", "bad_unicode_flag", "==", "0", ":", "\n", "#assert len(word_tokens) == len(word_to_wordpiece)", "\n", "        ", "if", "len", "(", "word_tokens", ")", "!=", "len", "(", "word_to_wordpiece", ")", ":", "\n", "            ", "print", "(", "\"sentence: \"", ",", "sentence", ")", "\n", "print", "(", "\"wordpiecetokens: \"", ",", "wordpiece_tokens", ")", "\n", "print", "(", "\"word token count: \"", ",", "len", "(", "word_tokens", ")", ")", "\n", "print", "(", "\"word_to_wordpiece count: \"", ",", "len", "(", "word_to_wordpiece", ")", ")", "\n", "", "return", "word_to_wordpiece", "\n", "", "else", ":", "\n", "# remove extra bad token 'fffd' for oov 2-byte characters", "\n", "        ", "wptok", "=", "[", "]", "\n", "i", "=", "0", "\n", "while", "(", "i", "<", "len", "(", "wordpiece_tokens", ")", ")", ":", "\n", "            ", "x", "=", "wordpiece_tokens", "[", "i", "]", "\n", "if", "ord", "(", "x", "[", "-", "1", ":", "]", ")", "!=", "65533", ":", "\n", "                ", "wptok", ".", "append", "(", "x", ")", "\n", "i", "+=", "1", "\n", "", "else", ":", "\n", "                ", "print", "(", "\"X: \"", ",", "x", ")", "\n", "nx", "=", "wordpiece_tokens", "[", "i", "+", "1", "]", "\n", "if", "ord", "(", "x", "[", "-", "1", ":", "]", ")", "==", "65533", ":", "\n", "                    ", "if", "ord", "(", "nx", "[", "-", "1", ":", "]", ")", "==", "65533", ":", "\n", "                        ", "wptok", ".", "append", "(", "x", ")", "\n", "i", "+=", "2", "\n", "", "else", ":", "\n", "                        ", "wptok", ".", "append", "(", "x", ")", "\n", "i", "+=", "1", "\n", "### reimplementation of word_to_wordpiece with the modified roberta wordpieces", "\n", "", "", "", "", "w_index", "=", "0", "\n", "word_to_wordpiece", "=", "[", "]", "\n", "subword_sequence", "=", "[", "]", "\n", "bad_match_flag", "=", "0", "\n", "for", "wp_index", ",", "wp", "in", "enumerate", "(", "wptok", ")", ":", "\n", "            ", "if", "w_index", "in", "range", "(", "len", "(", "word_tokens", ")", ")", ":", "\n", "                ", "word", "=", "word_tokens", "[", "w_index", "]", "\n", "if", "word", "==", "wptok", "[", "wp_index", "]", ":", "\n", "                    ", "word_to_wordpiece", ".", "append", "(", "wp_index", ")", "\n", "w_index", "+=", "1", "\n", "", "else", ":", "\n", "                    ", "subword_sequence", ".", "append", "(", "wp_index", ")", "\n", "word_from_pieces", "=", "\"\"", ".", "join", "(", "[", "\n", "wptok", "[", "i", "]", ".", "lstrip", "(", ")", "\n", "for", "i", "in", "subword_sequence", "\n", "]", ")", "\n", "if", "word", "==", "word_from_pieces", ":", "\n", "                        ", "word_to_wordpiece", ".", "append", "(", "subword_sequence", ")", "\n", "w_index", "+=", "1", "\n", "subword_sequence", "=", "[", "]", "\n", "", "elif", "word_from_pieces", "not", "in", "word", ":", "\n", "# compare the length instead of strings since there are offending", "\n", "# characters in roberta wordpieces", "\n", "                        ", "if", "len", "(", "word", ")", "==", "len", "(", "word_from_pieces", ")", ":", "\n", "                            ", "word_to_wordpiece", ".", "append", "(", "subword_sequence", ")", "\n", "w_index", "+=", "1", "\n", "subword_sequence", "=", "[", "]", "\n", "\n", "# assert len(word_tokens) == len(word_to_wordpiece)", "\n", "", "", "", "", "", "if", "len", "(", "word_tokens", ")", "!=", "len", "(", "word_to_wordpiece", ")", ":", "\n", "            ", "print", "(", "\"SENTENCE: \"", ",", "sentence", ")", "\n", "print", "(", "\"WPTOK: \"", ",", "wptok", ",", "\" \"", ",", "len", "(", "wptok", ")", ")", "\n", "print", "(", "\"WORDPIECE: \"", ",", "wordpiece_tokens", ",", "\" \"", ",", "len", "(", "wordpiece_tokens", ")", ")", "\n", "print", "(", "\"WORD token count: \"", ",", "len", "(", "word_tokens", ")", ")", "\n", "print", "(", "\"WORD_to_wordpiece: \"", ",", "word_to_wordpiece", ",", "\" \"", ",", "len", "(", "word_to_wordpiece", ")", ")", "\n", "", "return", "word_to_wordpiece", "\n", "\n"]], "home.repos.pwc.inspect_result.danifg_disco-seq2seq-parser.stack_transformer.pretrained_embeddings.get_scatter_indices": [[155, 167], ["torch.tensor", "range", "range", "len", "zip", "len", "isinstance", "len"], "function", ["None"], ["", "", "def", "get_scatter_indices", "(", "word2piece", ",", "reverse", "=", "False", ")", ":", "\n", "    ", "if", "reverse", ":", "\n", "        ", "indices", "=", "range", "(", "len", "(", "word2piece", ")", ")", "[", ":", ":", "-", "1", "]", "\n", "", "else", ":", "\n", "        ", "indices", "=", "range", "(", "len", "(", "word2piece", ")", ")", "\n", "# we will need as well the wordpiece to word indices", "\n", "", "wp_indices", "=", "[", "\n", "[", "index", "]", "*", "(", "len", "(", "span", ")", "if", "isinstance", "(", "span", ",", "list", ")", "else", "1", ")", "\n", "for", "index", ",", "span", "in", "zip", "(", "indices", ",", "word2piece", ")", "\n", "]", "\n", "wp_indices", "=", "[", "x", "for", "span", "in", "wp_indices", "for", "x", "in", "span", "]", "\n", "return", "torch", ".", "tensor", "(", "wp_indices", ")", "\n", "\n"]]}