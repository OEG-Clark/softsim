{"home.repos.pwc.inspect_result.beckschen_transmix.None.transmix.Mixup_transmix.__init__": [[33, 49], ["len"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "mixup_alpha", "=", "1.", ",", "cutmix_alpha", "=", "0.", ",", "cutmix_minmax", "=", "None", ",", "prob", "=", "1.0", ",", "switch_prob", "=", "0.5", ",", "\n", "mode", "=", "'batch'", ",", "correct_lam", "=", "True", ",", "label_smoothing", "=", "0.1", ",", "num_classes", "=", "1000", ")", ":", "\n", "        ", "self", ".", "mixup_alpha", "=", "mixup_alpha", "\n", "self", ".", "cutmix_alpha", "=", "cutmix_alpha", "\n", "self", ".", "cutmix_minmax", "=", "cutmix_minmax", "\n", "if", "self", ".", "cutmix_minmax", "is", "not", "None", ":", "\n", "            ", "assert", "len", "(", "self", ".", "cutmix_minmax", ")", "==", "2", "\n", "# force cutmix alpha == 1.0 when minmax active to keep logic simple & safe", "\n", "self", ".", "cutmix_alpha", "=", "1.0", "\n", "", "self", ".", "mix_prob", "=", "prob", "\n", "self", ".", "switch_prob", "=", "switch_prob", "\n", "self", ".", "label_smoothing", "=", "label_smoothing", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "mode", "=", "mode", "\n", "self", ".", "correct_lam", "=", "correct_lam", "# correct lambda based on clipped area for cutmix", "\n", "self", ".", "mixup_enabled", "=", "True", "# set to false to disable mixing (intended tp be set by train loop)", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.None.transmix.Mixup_transmix._mix_batch": [[50, 65], ["transmix.Mixup_transmix._params_per_batch", "timm.data.mixup.cutmix_bbox_and_lam", "x.flip().mul_", "x.mul_().add_", "x.flip", "x.flip", "x.mul_"], "methods", ["None"], ["", "def", "_mix_batch", "(", "self", ",", "x", ")", ":", "\n", "        ", "lam", ",", "use_cutmix", "=", "self", ".", "_params_per_batch", "(", ")", "\n", "\n", "if", "lam", "==", "1.", ":", "\n", "            ", "return", "1.", "\n", "", "if", "use_cutmix", ":", "\n", "            ", "(", "yl", ",", "yh", ",", "xl", ",", "xh", ")", ",", "lam", "=", "cutmix_bbox_and_lam", "(", "\n", "x", ".", "shape", ",", "lam", ",", "ratio_minmax", "=", "self", ".", "cutmix_minmax", ",", "correct_lam", "=", "self", ".", "correct_lam", ")", "\n", "x", "[", ":", ",", ":", ",", "yl", ":", "yh", ",", "xl", ":", "xh", "]", "=", "x", ".", "flip", "(", "0", ")", "[", ":", ",", ":", ",", "yl", ":", "yh", ",", "xl", ":", "xh", "]", "# cutmix for input!", "\n", "return", "lam", ",", "(", "yl", ",", "yh", ",", "xl", ",", "xh", ")", "# return box!", "\n", "", "else", ":", "\n", "            ", "x_flipped", "=", "x", ".", "flip", "(", "0", ")", ".", "mul_", "(", "1.", "-", "lam", ")", "\n", "x", ".", "mul_", "(", "lam", ")", ".", "add_", "(", "x_flipped", ")", "\n", "\n", "", "return", "lam", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.None.transmix.Mixup_transmix.transmix_label": [[67, 92], ["torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "mask.view().repeat.view().repeat.view().repeat", "len", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "mask.view().repeat.view().repeat.view", "lam.unsqueeze", "torch.Upsample", "torch.Upsample", "mask.view().repeat.view().repeat.unsqueeze().unsqueeze", "int", "mask.view().repeat.view().repeat.unsqueeze", "math.sqrt"], "methods", ["None"], ["", "def", "transmix_label", "(", "self", ",", "target", ",", "attn", ",", "input_shape", ",", "ratio", "=", "0.5", ")", ":", "\n", "        ", "\"\"\"use the self information?\n        args:\n            attn (torch.tensor): attention map from the last Transformer with shape (N, hw)\n            target (tuple): (target, y1, y2, use_cutmix, box)\n                target (torch.tensor): mixed target by area-ratio\n                y1 (torch.tensor): one-hot label for image A (background image) (N, k)\n                y2 (torch.tensor): one-hot label for image B (cropped patch)  (N, k)\n                use_cutmix (bool): enable cutmix if True, otherwise enable Mixup\n                box (tuple): (yl, yh, xl, xh)\n        returns:\n            target (torch.tensor): with shape (N, K)\n        \"\"\"", "\n", "# the placeholder _ is the area-based target", "\n", "(", "_", ",", "y1", ",", "y2", ",", "box", ")", "=", "target", "\n", "lam0", "=", "(", "box", "[", "1", "]", "-", "box", "[", "0", "]", ")", "*", "(", "box", "[", "3", "]", "-", "box", "[", "2", "]", ")", "/", "(", "input_shape", "[", "2", "]", "*", "input_shape", "[", "3", "]", ")", "\n", "mask", "=", "torch", ".", "zeros", "(", "(", "input_shape", "[", "2", "]", ",", "input_shape", "[", "3", "]", ")", ")", ".", "cuda", "(", ")", "\n", "mask", "[", "box", "[", "0", "]", ":", "box", "[", "1", "]", ",", "box", "[", "2", "]", ":", "box", "[", "3", "]", "]", "=", "1", "\n", "mask", "=", "nn", ".", "Upsample", "(", "size", "=", "int", "(", "math", ".", "sqrt", "(", "attn", ".", "shape", "[", "1", "]", ")", ")", ")", "(", "mask", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "0", ")", ")", ".", "int", "(", ")", "\n", "mask", "=", "mask", ".", "view", "(", "1", ",", "-", "1", ")", ".", "repeat", "(", "len", "(", "attn", ")", ",", "1", ")", "# (b, hw)", "\n", "w1", ",", "w2", "=", "torch", ".", "sum", "(", "(", "1", "-", "mask", ")", "*", "attn", ",", "dim", "=", "1", ")", ",", "torch", ".", "sum", "(", "mask", "*", "attn", ",", "dim", "=", "1", ")", "\n", "lam1", "=", "w2", "/", "(", "w1", "+", "w2", ")", "# (b, )", "\n", "lam", "=", "(", "lam0", "+", "lam1", ")", "/", "2", "# ()+(b,) ratio=0.5", "\n", "target", "=", "y1", "*", "(", "1.", "-", "lam", ")", ".", "unsqueeze", "(", "1", ")", "+", "y2", "*", "lam", ".", "unsqueeze", "(", "1", ")", "\n", "return", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.None.transmix.Mixup_transmix.__call__": [[93, 108], ["transmix.Mixup_transmix._mix_batch", "isinstance", "transmix.mixup_target", "len"], "methods", ["home.repos.pwc.inspect_result.beckschen_transmix.None.transmix.Mixup_transmix._mix_batch", "home.repos.pwc.inspect_result.beckschen_transmix.None.transmix.mixup_target"], ["", "def", "__call__", "(", "self", ",", "x", ",", "target", ")", ":", "\n", "        ", "assert", "len", "(", "x", ")", "%", "2", "==", "0", ",", "'Batch size should be even when using this'", "\n", "assert", "self", ".", "mode", "==", "'batch'", ",", "'Mixup mode is batch by default'", "\n", "lam", "=", "self", ".", "_mix_batch", "(", "x", ")", "# tuple or value", "\n", "if", "isinstance", "(", "lam", ",", "tuple", ")", ":", "\n", "            ", "lam", ",", "box", "=", "lam", "# lam: (b,)", "\n", "use_cutmix", "=", "True", "\n", "", "else", ":", "# lam is a value", "\n", "            ", "use_cutmix", "=", "False", "\n", "\n", "", "mixed_target", ",", "y1", ",", "y2", "=", "mixup_target", "(", "target", ",", "self", ".", "num_classes", ",", "lam", ",", "self", ".", "label_smoothing", ",", "x", ".", "device", ",", "return_y1y2", "=", "True", ")", "# tuple or tensor", "\n", "if", "use_cutmix", ":", "\n", "            ", "return", "x", ",", "(", "mixed_target", ",", "y1", ",", "y2", ",", "box", ")", "\n", "", "else", ":", "\n", "            ", "return", "x", ",", "mixed_target", "", "", "", "", ""]], "home.repos.pwc.inspect_result.beckschen_transmix.None.transmix.mixup_target": [[6, 15], ["timm.data.mixup.one_hot", "timm.data.mixup.one_hot", "target.flip", "timm.data.mixup.one_hot.clone", "timm.data.mixup.one_hot.clone"], "function", ["None"], ["def", "mixup_target", "(", "target", ",", "num_classes", ",", "lam", "=", "1.", ",", "smoothing", "=", "0.0", ",", "device", "=", "'cuda'", ",", "return_y1y2", "=", "False", ")", ":", "\n", "    ", "off_value", "=", "smoothing", "/", "num_classes", "\n", "on_value", "=", "1.", "-", "smoothing", "+", "off_value", "\n", "y1", "=", "one_hot", "(", "target", ",", "num_classes", ",", "on_value", "=", "on_value", ",", "off_value", "=", "off_value", ",", "device", "=", "device", ")", "\n", "y2", "=", "one_hot", "(", "target", ".", "flip", "(", "0", ")", ",", "num_classes", ",", "on_value", "=", "on_value", ",", "off_value", "=", "off_value", ",", "device", "=", "device", ")", "\n", "if", "return_y1y2", ":", "\n", "        ", "return", "y1", "*", "lam", "+", "y2", "*", "(", "1.", "-", "lam", ")", ",", "y1", ".", "clone", "(", ")", ",", "y2", ".", "clone", "(", ")", "\n", "", "else", ":", "\n", "        ", "return", "y1", "*", "lam", "+", "y2", "*", "(", "1.", "-", "lam", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.None.train._parse_args": [[296, 311], ["config_parser.parse_known_args", "parser.parse_args", "yaml.safe_dump", "open", "yaml.safe_load", "parser.set_defaults"], "function", ["None"], ["def", "_parse_args", "(", ")", ":", "\n", "# Do we have a config file to parse?", "\n", "    ", "args_config", ",", "remaining", "=", "config_parser", ".", "parse_known_args", "(", ")", "\n", "if", "args_config", ".", "config", ":", "\n", "        ", "with", "open", "(", "args_config", ".", "config", ",", "'r'", ")", "as", "f", ":", "\n", "            ", "cfg", "=", "yaml", ".", "safe_load", "(", "f", ")", "\n", "parser", ".", "set_defaults", "(", "**", "cfg", ")", "\n", "\n", "# The main arg parser parses the rest of the args, the usual", "\n", "# defaults will have been overridden if config file specified.", "\n", "", "", "args", "=", "parser", ".", "parse_args", "(", "remaining", ")", "\n", "\n", "# Cache the args as a text string to save them in the output dir later", "\n", "args_text", "=", "yaml", ".", "safe_dump", "(", "args", ".", "__dict__", ",", "default_flow_style", "=", "False", ")", "\n", "return", "args", ",", "args_text", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.None.train.main": [[313, 659], ["setup_default_logging", "train._parse_args", "print", "random_seed", "timm.models.create_model", "timm.data.resolve_data_config", "torch.nn.parallel.DistributedDataParallel.cuda", "timm.optim.create_optimizer_v2", "timm.scheduler.create_scheduler", "timm.data.create_dataset", "timm.data.create_dataset", "timm.data.create_loader", "timm.data.create_loader", "LabelSmoothingCrossEntropy.cuda", "torch.CrossEntropyLoss().cuda", "torch.cuda.set_device", "torch.cuda.set_device", "torch.distributed.init_process_group", "torch.distributed.init_process_group", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.distributed.get_rank", "torch.distributed.get_rank", "_logger.info", "_logger.info", "hasattr", "_logger.info", "vars", "timm.models.convert_splitbn_model", "torch.nn.parallel.DistributedDataParallel.to", "torch.jit.script", "torch.jit.script", "amp.initialize", "timm.utils.ApexScaler", "timm.models.resume_checkpoint", "ModelEmaV2", "lr_scheduler.step", "_logger.info", "dict", "timm.data.AugMixDataset", "JsdCrossEntropy", "get_outdir", "CheckpointSaver", "range", "_logger.info", "wandb.init", "_logger.warning", "int", "max", "convert_syncbn_model", "torch.nn.SyncBatchNorm.convert_sync_batchnorm", "torch.nn.SyncBatchNorm.convert_sync_batchnorm", "_logger.info", "timm.optim.optimizer_kwargs", "_logger.info", "timm.utils.NativeScaler", "timm.models.load_checkpoint", "ApexDDP", "torch.nn.parallel.DistributedDataParallel", "Mixup_transmix", "torch.CrossEntropyLoss", "open", "f.write", "train.train_one_epoch", "train.validate", "_logger.warning", "_logger.info", "_logger.info", "_logger.info", "_logger.info", "timm.data.FastCollateMixup", "timm.data.Mixup", "BinaryCrossEntropy", "SoftTargetCrossEntropy", "torch.CrossEntropyLoss", "os.path.join", "hasattr", "timm.data.create_loader.sampler.set_epoch", "distribute_bn", "train.validate", "lr_scheduler.step", "update_summary", "CheckpointSaver.save_checkpoint", "timm.models.safe_model_name", "sum", "BinaryCrossEntropy", "LabelSmoothingCrossEntropy", "datetime.datetime.now().strftime", "timm.models.safe_model_name", "str", "_logger.info", "distribute_bn", "os.path.join", "m.numel", "datetime.datetime.now", "torch.nn.parallel.DistributedDataParallel.parameters"], "function", ["home.repos.pwc.inspect_result.beckschen_transmix.None.train._parse_args", "home.repos.pwc.inspect_result.beckschen_transmix.None.train.train_one_epoch", "home.repos.pwc.inspect_result.beckschen_transmix.None.train.validate", "home.repos.pwc.inspect_result.beckschen_transmix.None.train.validate"], ["", "def", "main", "(", ")", ":", "\n", "    ", "setup_default_logging", "(", ")", "\n", "args", ",", "args_text", "=", "_parse_args", "(", ")", "\n", "print", "(", "args", ")", "\n", "\n", "if", "args", ".", "log_wandb", ":", "\n", "        ", "if", "has_wandb", ":", "\n", "            ", "wandb", ".", "init", "(", "project", "=", "args", ".", "experiment", ",", "config", "=", "args", ")", "\n", "", "else", ":", "\n", "            ", "_logger", ".", "warning", "(", "\"You've requested to log metrics to wandb but package not found. \"", "\n", "\"Metrics not being logged to wandb, try `pip install wandb`\"", ")", "\n", "\n", "", "", "args", ".", "prefetcher", "=", "not", "args", ".", "no_prefetcher", "\n", "args", ".", "distributed", "=", "False", "\n", "if", "'WORLD_SIZE'", "in", "os", ".", "environ", ":", "\n", "        ", "args", ".", "distributed", "=", "int", "(", "os", ".", "environ", "[", "'WORLD_SIZE'", "]", ")", ">", "1", "\n", "", "args", ".", "device", "=", "'cuda:0'", "\n", "args", ".", "world_size", "=", "1", "\n", "args", ".", "rank", "=", "0", "# global rank", "\n", "if", "args", ".", "distributed", ":", "\n", "        ", "args", ".", "device", "=", "'cuda:%d'", "%", "args", ".", "local_rank", "\n", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "local_rank", ")", "\n", "torch", ".", "distributed", ".", "init_process_group", "(", "backend", "=", "'nccl'", ",", "init_method", "=", "'env://'", ")", "\n", "args", ".", "world_size", "=", "torch", ".", "distributed", ".", "get_world_size", "(", ")", "\n", "args", ".", "rank", "=", "torch", ".", "distributed", ".", "get_rank", "(", ")", "\n", "_logger", ".", "info", "(", "'Training in distributed mode with multiple processes, 1 GPU per process. Process %d, total %d.'", "\n", "%", "(", "args", ".", "rank", ",", "args", ".", "world_size", ")", ")", "\n", "", "else", ":", "\n", "        ", "_logger", ".", "info", "(", "'Training with a single process on 1 GPUs.'", ")", "\n", "", "assert", "args", ".", "rank", ">=", "0", "\n", "\n", "# resolve AMP arguments based on PyTorch / Apex availability", "\n", "use_amp", "=", "None", "\n", "if", "args", ".", "amp", ":", "\n", "# `--amp` chooses native amp before apex (APEX ver not actively maintained)", "\n", "        ", "if", "has_native_amp", ":", "\n", "            ", "args", ".", "native_amp", "=", "True", "\n", "", "elif", "has_apex", ":", "\n", "            ", "args", ".", "apex_amp", "=", "True", "\n", "", "", "if", "args", ".", "apex_amp", "and", "has_apex", ":", "\n", "        ", "use_amp", "=", "'apex'", "\n", "", "elif", "args", ".", "native_amp", "and", "has_native_amp", ":", "\n", "        ", "use_amp", "=", "'native'", "\n", "", "elif", "args", ".", "apex_amp", "or", "args", ".", "native_amp", ":", "\n", "        ", "_logger", ".", "warning", "(", "\"Neither APEX or native Torch AMP is available, using float32. \"", "\n", "\"Install NVIDA apex or upgrade to PyTorch 1.6\"", ")", "\n", "\n", "", "random_seed", "(", "args", ".", "seed", ",", "args", ".", "rank", ")", "\n", "\n", "model", "=", "create_model", "(", "\n", "args", ".", "model", ",", "\n", "pretrained", "=", "args", ".", "pretrained", ",", "\n", "num_classes", "=", "args", ".", "num_classes", ",", "\n", "drop_rate", "=", "args", ".", "drop", ",", "\n", "drop_connect_rate", "=", "args", ".", "drop_connect", ",", "# DEPRECATED, use drop_path", "\n", "drop_path_rate", "=", "args", ".", "drop_path", ",", "\n", "drop_block_rate", "=", "args", ".", "drop_block", ",", "\n", "global_pool", "=", "args", ".", "gp", ",", "\n", "bn_momentum", "=", "args", ".", "bn_momentum", ",", "\n", "bn_eps", "=", "args", ".", "bn_eps", ",", "\n", "scriptable", "=", "args", ".", "torchscript", ",", "\n", "checkpoint_path", "=", "args", ".", "initial_checkpoint", ")", "\n", "if", "args", ".", "num_classes", "is", "None", ":", "\n", "        ", "assert", "hasattr", "(", "model", ",", "'num_classes'", ")", ",", "'Model must have `num_classes` attr if not set on cmd line/config.'", "\n", "args", ".", "num_classes", "=", "model", ".", "num_classes", "# FIXME handle model default vs config num_classes more elegantly", "\n", "\n", "", "if", "args", ".", "local_rank", "==", "0", ":", "\n", "        ", "_logger", ".", "info", "(", "\n", "f'Model {safe_model_name(args.model)} created, param count:{sum([m.numel() for m in model.parameters()])}'", ")", "\n", "\n", "", "data_config", "=", "resolve_data_config", "(", "vars", "(", "args", ")", ",", "model", "=", "model", ",", "verbose", "=", "args", ".", "local_rank", "==", "0", ")", "\n", "\n", "# setup augmentation batch splits for contrastive loss or split bn", "\n", "num_aug_splits", "=", "0", "\n", "if", "args", ".", "aug_splits", ">", "0", ":", "\n", "        ", "assert", "args", ".", "aug_splits", ">", "1", ",", "'A split of 1 makes no sense'", "\n", "num_aug_splits", "=", "args", ".", "aug_splits", "\n", "\n", "# enable split bn (separate bn stats per batch-portion)", "\n", "", "if", "args", ".", "split_bn", ":", "\n", "        ", "assert", "num_aug_splits", ">", "1", "or", "args", ".", "resplit", "\n", "model", "=", "convert_splitbn_model", "(", "model", ",", "max", "(", "num_aug_splits", ",", "2", ")", ")", "\n", "\n", "# move model to GPU, enable channels last layout if set", "\n", "", "model", ".", "cuda", "(", ")", "\n", "if", "args", ".", "channels_last", ":", "\n", "        ", "model", "=", "model", ".", "to", "(", "memory_format", "=", "torch", ".", "channels_last", ")", "\n", "\n", "# setup synchronized BatchNorm for distributed training", "\n", "", "if", "args", ".", "distributed", "and", "args", ".", "sync_bn", ":", "\n", "        ", "assert", "not", "args", ".", "split_bn", "\n", "if", "has_apex", "and", "use_amp", "==", "'apex'", ":", "\n", "# Apex SyncBN preferred unless native amp is activated", "\n", "            ", "model", "=", "convert_syncbn_model", "(", "model", ")", "\n", "", "else", ":", "\n", "            ", "model", "=", "torch", ".", "nn", ".", "SyncBatchNorm", ".", "convert_sync_batchnorm", "(", "model", ")", "\n", "", "if", "args", ".", "local_rank", "==", "0", ":", "\n", "            ", "_logger", ".", "info", "(", "\n", "'Converted model to use Synchronized BatchNorm. WARNING: You may have issues if using '", "\n", "'zero initialized BN layers (enabled by default for ResNets) while sync-bn enabled.'", ")", "\n", "\n", "", "", "if", "args", ".", "torchscript", ":", "\n", "        ", "assert", "not", "use_amp", "==", "'apex'", ",", "'Cannot use APEX AMP with torchscripted model'", "\n", "assert", "not", "args", ".", "sync_bn", ",", "'Cannot use SyncBatchNorm with torchscripted model'", "\n", "model", "=", "torch", ".", "jit", ".", "script", "(", "model", ")", "\n", "\n", "", "optimizer", "=", "create_optimizer_v2", "(", "model", ",", "**", "optimizer_kwargs", "(", "cfg", "=", "args", ")", ")", "\n", "\n", "# setup automatic mixed-precision (AMP) loss scaling and op casting", "\n", "amp_autocast", "=", "suppress", "# do nothing", "\n", "loss_scaler", "=", "None", "\n", "if", "use_amp", "==", "'apex'", ":", "\n", "        ", "model", ",", "optimizer", "=", "amp", ".", "initialize", "(", "model", ",", "optimizer", ",", "opt_level", "=", "'O1'", ")", "\n", "loss_scaler", "=", "ApexScaler", "(", ")", "\n", "if", "args", ".", "local_rank", "==", "0", ":", "\n", "            ", "_logger", ".", "info", "(", "'Using NVIDIA APEX AMP. Training in mixed precision.'", ")", "\n", "", "", "elif", "use_amp", "==", "'native'", ":", "\n", "        ", "amp_autocast", "=", "torch", ".", "cuda", ".", "amp", ".", "autocast", "\n", "loss_scaler", "=", "NativeScaler", "(", ")", "\n", "if", "args", ".", "local_rank", "==", "0", ":", "\n", "            ", "_logger", ".", "info", "(", "'Using native Torch AMP. Training in mixed precision.'", ")", "\n", "", "", "else", ":", "\n", "        ", "if", "args", ".", "local_rank", "==", "0", ":", "\n", "            ", "_logger", ".", "info", "(", "'AMP not enabled. Training in float32.'", ")", "\n", "\n", "# optionally resume from a checkpoint", "\n", "", "", "resume_epoch", "=", "None", "\n", "if", "args", ".", "resume", ":", "\n", "        ", "resume_epoch", "=", "resume_checkpoint", "(", "\n", "model", ",", "args", ".", "resume", ",", "\n", "optimizer", "=", "None", "if", "args", ".", "no_resume_opt", "else", "optimizer", ",", "\n", "loss_scaler", "=", "None", "if", "args", ".", "no_resume_opt", "else", "loss_scaler", ",", "\n", "log_info", "=", "args", ".", "local_rank", "==", "0", ")", "\n", "\n", "# setup exponential moving average of model weights, SWA could be used here too", "\n", "", "model_ema", "=", "None", "\n", "if", "args", ".", "model_ema", ":", "\n", "# Important to create EMA model after cuda(), DP wrapper, and AMP but before SyncBN and DDP wrapper", "\n", "        ", "model_ema", "=", "ModelEmaV2", "(", "\n", "model", ",", "decay", "=", "args", ".", "model_ema_decay", ",", "device", "=", "'cpu'", "if", "args", ".", "model_ema_force_cpu", "else", "None", ")", "\n", "if", "args", ".", "resume", ":", "\n", "            ", "load_checkpoint", "(", "model_ema", ".", "module", ",", "args", ".", "resume", ",", "use_ema", "=", "True", ")", "\n", "\n", "# setup distributed training", "\n", "", "", "if", "args", ".", "distributed", ":", "\n", "        ", "if", "has_apex", "and", "use_amp", "==", "'apex'", ":", "\n", "# Apex DDP preferred unless native amp is activated", "\n", "            ", "if", "args", ".", "local_rank", "==", "0", ":", "\n", "                ", "_logger", ".", "info", "(", "\"Using NVIDIA APEX DistributedDataParallel.\"", ")", "\n", "", "model", "=", "ApexDDP", "(", "model", ",", "delay_allreduce", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "if", "args", ".", "local_rank", "==", "0", ":", "\n", "                ", "_logger", ".", "info", "(", "\"Using native Torch DistributedDataParallel.\"", ")", "\n", "", "model", "=", "NativeDDP", "(", "model", ",", "device_ids", "=", "[", "args", ".", "local_rank", "]", ",", "broadcast_buffers", "=", "not", "args", ".", "no_ddp_bb", ")", "\n", "# NOTE: EMA model does not need to be wrapped by DDP", "\n", "\n", "# setup learning rate schedule and starting epoch", "\n", "", "", "lr_scheduler", ",", "num_epochs", "=", "create_scheduler", "(", "args", ",", "optimizer", ")", "\n", "start_epoch", "=", "0", "\n", "if", "args", ".", "start_epoch", "is", "not", "None", ":", "\n", "# a specified start_epoch will always override the resume epoch", "\n", "        ", "start_epoch", "=", "args", ".", "start_epoch", "\n", "", "elif", "resume_epoch", "is", "not", "None", ":", "\n", "        ", "start_epoch", "=", "resume_epoch", "\n", "", "if", "lr_scheduler", "is", "not", "None", "and", "start_epoch", ">", "0", ":", "\n", "        ", "lr_scheduler", ".", "step", "(", "start_epoch", ")", "\n", "\n", "", "if", "args", ".", "local_rank", "==", "0", ":", "\n", "        ", "_logger", ".", "info", "(", "'Scheduled epochs: {}'", ".", "format", "(", "num_epochs", ")", ")", "\n", "\n", "# create the train and eval datasets", "\n", "", "dataset_train", "=", "create_dataset", "(", "\n", "args", ".", "dataset", ",", "root", "=", "args", ".", "data_dir", ",", "split", "=", "args", ".", "train_split", ",", "is_training", "=", "True", ",", "\n", "class_map", "=", "args", ".", "class_map", ",", "\n", "download", "=", "args", ".", "dataset_download", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "repeats", "=", "args", ".", "epoch_repeats", ")", "\n", "dataset_eval", "=", "create_dataset", "(", "\n", "args", ".", "dataset", ",", "root", "=", "args", ".", "data_dir", ",", "split", "=", "args", ".", "val_split", ",", "is_training", "=", "False", ",", "\n", "class_map", "=", "args", ".", "class_map", ",", "\n", "download", "=", "args", ".", "dataset_download", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ")", "\n", "\n", "# setup mixup / cutmix", "\n", "collate_fn", "=", "None", "\n", "mixup_fn", "=", "None", "\n", "mixup_active", "=", "args", ".", "mixup", ">", "0", "or", "args", ".", "cutmix", ">", "0.", "or", "args", ".", "cutmix_minmax", "is", "not", "None", "\n", "if", "mixup_active", ":", "\n", "        ", "mixup_args", "=", "dict", "(", "\n", "mixup_alpha", "=", "args", ".", "mixup", ",", "cutmix_alpha", "=", "args", ".", "cutmix", ",", "cutmix_minmax", "=", "args", ".", "cutmix_minmax", ",", "\n", "prob", "=", "args", ".", "mixup_prob", ",", "switch_prob", "=", "args", ".", "mixup_switch_prob", ",", "mode", "=", "args", ".", "mixup_mode", ",", "\n", "label_smoothing", "=", "args", ".", "smoothing", ",", "num_classes", "=", "args", ".", "num_classes", ")", "\n", "if", "args", ".", "transmix", ":", "\n", "# wrap mixup_fn with TransMix helper, disable args.prefetcher", "\n", "            ", "from", "transmix", "import", "Mixup_transmix", "\n", "mixup_fn", "=", "Mixup_transmix", "(", "**", "mixup_args", ")", "\n", "", "else", ":", "\n", "            ", "if", "args", ".", "prefetcher", ":", "\n", "                ", "assert", "not", "num_aug_splits", "# collate conflict (need to support deinterleaving in collate mixup)", "\n", "collate_fn", "=", "FastCollateMixup", "(", "**", "mixup_args", ")", "\n", "", "else", ":", "\n", "                ", "mixup_fn", "=", "Mixup", "(", "**", "mixup_args", ")", "\n", "\n", "\n", "# wrap dataset in AugMix helper", "\n", "", "", "", "if", "num_aug_splits", ">", "1", ":", "\n", "        ", "dataset_train", "=", "AugMixDataset", "(", "dataset_train", ",", "num_splits", "=", "num_aug_splits", ")", "\n", "\n", "# create data loaders w/ augmentation pipeiine", "\n", "", "train_interpolation", "=", "args", ".", "train_interpolation", "\n", "if", "args", ".", "no_aug", "or", "not", "train_interpolation", ":", "\n", "        ", "train_interpolation", "=", "data_config", "[", "'interpolation'", "]", "\n", "\n", "", "if", "args", ".", "total_batch_size", ":", "\n", "        ", "args", ".", "batch_size", "=", "args", ".", "total_batch_size", "//", "args", ".", "world_size", "\n", "\n", "", "loader_train", "=", "create_loader", "(", "\n", "dataset_train", ",", "\n", "input_size", "=", "data_config", "[", "'input_size'", "]", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "is_training", "=", "True", ",", "\n", "use_prefetcher", "=", "args", ".", "prefetcher", ",", "\n", "no_aug", "=", "args", ".", "no_aug", ",", "\n", "re_prob", "=", "args", ".", "reprob", ",", "\n", "re_mode", "=", "args", ".", "remode", ",", "\n", "re_count", "=", "args", ".", "recount", ",", "\n", "re_split", "=", "args", ".", "resplit", ",", "\n", "scale", "=", "args", ".", "scale", ",", "\n", "ratio", "=", "args", ".", "ratio", ",", "\n", "hflip", "=", "args", ".", "hflip", ",", "\n", "vflip", "=", "args", ".", "vflip", ",", "\n", "color_jitter", "=", "args", ".", "color_jitter", ",", "\n", "auto_augment", "=", "args", ".", "aa", ",", "\n", "num_aug_repeats", "=", "args", ".", "aug_repeats", ",", "\n", "num_aug_splits", "=", "num_aug_splits", ",", "\n", "interpolation", "=", "train_interpolation", ",", "\n", "mean", "=", "data_config", "[", "'mean'", "]", ",", "\n", "std", "=", "data_config", "[", "'std'", "]", ",", "\n", "num_workers", "=", "args", ".", "workers", ",", "\n", "distributed", "=", "args", ".", "distributed", ",", "\n", "collate_fn", "=", "collate_fn", ",", "\n", "pin_memory", "=", "args", ".", "pin_mem", ",", "\n", "use_multi_epochs_loader", "=", "args", ".", "use_multi_epochs_loader", ",", "\n", "worker_seeding", "=", "args", ".", "worker_seeding", ",", "\n", ")", "\n", "\n", "loader_eval", "=", "create_loader", "(", "\n", "dataset_eval", ",", "\n", "input_size", "=", "data_config", "[", "'input_size'", "]", ",", "\n", "batch_size", "=", "args", ".", "validation_batch_size", "or", "args", ".", "batch_size", ",", "\n", "is_training", "=", "False", ",", "\n", "use_prefetcher", "=", "args", ".", "prefetcher", ",", "\n", "interpolation", "=", "data_config", "[", "'interpolation'", "]", ",", "\n", "mean", "=", "data_config", "[", "'mean'", "]", ",", "\n", "std", "=", "data_config", "[", "'std'", "]", ",", "\n", "num_workers", "=", "args", ".", "workers", ",", "\n", "distributed", "=", "args", ".", "distributed", ",", "\n", "crop_pct", "=", "data_config", "[", "'crop_pct'", "]", ",", "\n", "pin_memory", "=", "args", ".", "pin_mem", ",", "\n", ")", "\n", "\n", "# setup loss function", "\n", "if", "args", ".", "jsd_loss", ":", "\n", "        ", "assert", "num_aug_splits", ">", "1", "# JSD only valid with aug splits set", "\n", "train_loss_fn", "=", "JsdCrossEntropy", "(", "num_splits", "=", "num_aug_splits", ",", "smoothing", "=", "args", ".", "smoothing", ")", "\n", "", "elif", "mixup_active", ":", "\n", "# smoothing is handled with mixup target transform which outputs sparse, soft targets", "\n", "        ", "if", "args", ".", "bce_loss", ":", "\n", "            ", "train_loss_fn", "=", "BinaryCrossEntropy", "(", "target_threshold", "=", "args", ".", "bce_target_thresh", ")", "\n", "", "else", ":", "\n", "            ", "train_loss_fn", "=", "SoftTargetCrossEntropy", "(", ")", "\n", "", "", "elif", "args", ".", "smoothing", ":", "\n", "        ", "if", "args", ".", "bce_loss", ":", "\n", "            ", "train_loss_fn", "=", "BinaryCrossEntropy", "(", "smoothing", "=", "args", ".", "smoothing", ",", "target_threshold", "=", "args", ".", "bce_target_thresh", ")", "\n", "", "else", ":", "\n", "            ", "train_loss_fn", "=", "LabelSmoothingCrossEntropy", "(", "smoothing", "=", "args", ".", "smoothing", ")", "\n", "", "", "else", ":", "\n", "        ", "train_loss_fn", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "", "train_loss_fn", "=", "train_loss_fn", ".", "cuda", "(", ")", "\n", "validate_loss_fn", "=", "nn", ".", "CrossEntropyLoss", "(", ")", ".", "cuda", "(", ")", "\n", "\n", "# setup checkpoint saver and eval metric tracking", "\n", "eval_metric", "=", "args", ".", "eval_metric", "\n", "best_metric", "=", "None", "\n", "best_epoch", "=", "None", "\n", "saver", "=", "None", "\n", "output_dir", "=", "None", "\n", "if", "args", ".", "rank", "==", "0", ":", "\n", "        ", "if", "args", ".", "experiment", ":", "\n", "            ", "exp_name", "=", "args", ".", "experiment", "\n", "", "else", ":", "\n", "            ", "exp_name", "=", "'-'", ".", "join", "(", "[", "\n", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "\"%Y%m%d-%H%M%S\"", ")", ",", "\n", "safe_model_name", "(", "args", ".", "model", ")", ",", "\n", "str", "(", "data_config", "[", "'input_size'", "]", "[", "-", "1", "]", ")", "\n", "]", ")", "\n", "", "output_dir", "=", "get_outdir", "(", "args", ".", "output", "if", "args", ".", "output", "else", "'./output/train'", ",", "exp_name", ")", "\n", "decreasing", "=", "True", "if", "eval_metric", "==", "'loss'", "else", "False", "\n", "saver", "=", "CheckpointSaver", "(", "\n", "model", "=", "model", ",", "optimizer", "=", "optimizer", ",", "args", "=", "args", ",", "model_ema", "=", "model_ema", ",", "amp_scaler", "=", "loss_scaler", ",", "\n", "checkpoint_dir", "=", "output_dir", ",", "recovery_dir", "=", "output_dir", ",", "decreasing", "=", "decreasing", ",", "max_history", "=", "args", ".", "checkpoint_hist", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "'args.yaml'", ")", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "args_text", ")", "\n", "\n", "", "", "try", ":", "\n", "        ", "for", "epoch", "in", "range", "(", "start_epoch", ",", "num_epochs", ")", ":", "\n", "            ", "if", "args", ".", "distributed", "and", "hasattr", "(", "loader_train", ".", "sampler", ",", "'set_epoch'", ")", ":", "\n", "                ", "loader_train", ".", "sampler", ".", "set_epoch", "(", "epoch", ")", "\n", "\n", "", "train_metrics", "=", "train_one_epoch", "(", "\n", "epoch", ",", "model", ",", "loader_train", ",", "optimizer", ",", "train_loss_fn", ",", "args", ",", "\n", "lr_scheduler", "=", "lr_scheduler", ",", "saver", "=", "saver", ",", "output_dir", "=", "output_dir", ",", "\n", "amp_autocast", "=", "amp_autocast", ",", "loss_scaler", "=", "loss_scaler", ",", "model_ema", "=", "model_ema", ",", "mixup_fn", "=", "mixup_fn", ")", "\n", "\n", "if", "args", ".", "distributed", "and", "args", ".", "dist_bn", "in", "(", "'broadcast'", ",", "'reduce'", ")", ":", "\n", "                ", "if", "args", ".", "local_rank", "==", "0", ":", "\n", "                    ", "_logger", ".", "info", "(", "\"Distributing BatchNorm running means and vars\"", ")", "\n", "", "distribute_bn", "(", "model", ",", "args", ".", "world_size", ",", "args", ".", "dist_bn", "==", "'reduce'", ")", "\n", "\n", "", "eval_metrics", "=", "validate", "(", "model", ",", "loader_eval", ",", "validate_loss_fn", ",", "args", ",", "amp_autocast", "=", "amp_autocast", ")", "\n", "\n", "if", "model_ema", "is", "not", "None", "and", "not", "args", ".", "model_ema_force_cpu", ":", "\n", "                ", "if", "args", ".", "distributed", "and", "args", ".", "dist_bn", "in", "(", "'broadcast'", ",", "'reduce'", ")", ":", "\n", "                    ", "distribute_bn", "(", "model_ema", ",", "args", ".", "world_size", ",", "args", ".", "dist_bn", "==", "'reduce'", ")", "\n", "", "ema_eval_metrics", "=", "validate", "(", "\n", "model_ema", ".", "module", ",", "loader_eval", ",", "validate_loss_fn", ",", "args", ",", "amp_autocast", "=", "amp_autocast", ",", "log_suffix", "=", "' (EMA)'", ")", "\n", "eval_metrics", "=", "ema_eval_metrics", "\n", "\n", "", "if", "lr_scheduler", "is", "not", "None", ":", "\n", "# step LR for next epoch", "\n", "                ", "lr_scheduler", ".", "step", "(", "epoch", "+", "1", ",", "eval_metrics", "[", "eval_metric", "]", ")", "\n", "\n", "", "if", "output_dir", "is", "not", "None", ":", "\n", "                ", "update_summary", "(", "\n", "epoch", ",", "train_metrics", ",", "eval_metrics", ",", "os", ".", "path", ".", "join", "(", "output_dir", ",", "'summary.csv'", ")", ",", "\n", "write_header", "=", "best_metric", "is", "None", ",", "log_wandb", "=", "args", ".", "log_wandb", "and", "has_wandb", ")", "\n", "\n", "", "if", "saver", "is", "not", "None", ":", "\n", "# save proper checkpoint with eval metric", "\n", "                ", "save_metric", "=", "eval_metrics", "[", "eval_metric", "]", "\n", "best_metric", ",", "best_epoch", "=", "saver", ".", "save_checkpoint", "(", "epoch", ",", "metric", "=", "save_metric", ")", "\n", "\n", "", "", "", "except", "KeyboardInterrupt", ":", "\n", "        ", "pass", "\n", "", "if", "best_metric", "is", "not", "None", ":", "\n", "        ", "_logger", ".", "info", "(", "'*** Best metric: {0} (epoch {1})'", ".", "format", "(", "best_metric", ",", "best_epoch", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.None.train.train_one_epoch": [[661, 773], ["AverageMeter", "AverageMeter", "AverageMeter", "model.train", "time.time", "enumerate", "hasattr", "collections.OrderedDict", "hasattr", "len", "len", "AverageMeter.update", "optimizer.zero_grad", "torch.cuda.synchronize", "torch.cuda.synchronize", "AverageMeter.update", "time.time", "optimizer.sync_lookahead", "input.contiguous.contiguous", "amp_autocast", "model", "loss_fn", "AverageMeter.update", "loss_scaler", "loss_fn.backward", "optimizer.step", "model_ema.update", "saver.save_recovery", "lr_scheduler.step_update", "time.time", "input.contiguous.cuda", "mixup_fn.transmix_label.cuda", "mixup_fn", "isinstance", "loss_fn.item", "input.contiguous.size", "dispatch_clip_grad", "time.time", "sum", "len", "reduce_tensor", "AverageMeter.update", "_logger.info", "mixup_fn.transmix_label", "timm.models.model_parameters", "timm.models.model_parameters", "reduce_tensor.item", "input.contiguous.size", "torchvision.utils.save_image", "len", "os.path.join", "input.contiguous.size", "input.contiguous.size"], "function", ["home.repos.pwc.inspect_result.beckschen_transmix.None.transmix.Mixup_transmix.transmix_label"], ["", "", "def", "train_one_epoch", "(", "\n", "epoch", ",", "model", ",", "loader", ",", "optimizer", ",", "loss_fn", ",", "args", ",", "\n", "lr_scheduler", "=", "None", ",", "saver", "=", "None", ",", "output_dir", "=", "None", ",", "amp_autocast", "=", "suppress", ",", "\n", "loss_scaler", "=", "None", ",", "model_ema", "=", "None", ",", "mixup_fn", "=", "None", ")", ":", "\n", "\n", "    ", "if", "args", ".", "mixup_off_epoch", "and", "epoch", ">=", "args", ".", "mixup_off_epoch", ":", "\n", "        ", "if", "args", ".", "prefetcher", "and", "loader", ".", "mixup_enabled", ":", "\n", "            ", "loader", ".", "mixup_enabled", "=", "False", "\n", "", "elif", "mixup_fn", "is", "not", "None", ":", "\n", "            ", "mixup_fn", ".", "mixup_enabled", "=", "False", "\n", "\n", "", "", "second_order", "=", "hasattr", "(", "optimizer", ",", "'is_second_order'", ")", "and", "optimizer", ".", "is_second_order", "\n", "batch_time_m", "=", "AverageMeter", "(", ")", "\n", "data_time_m", "=", "AverageMeter", "(", ")", "\n", "losses_m", "=", "AverageMeter", "(", ")", "\n", "\n", "model", ".", "train", "(", ")", "\n", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "last_idx", "=", "len", "(", "loader", ")", "-", "1", "\n", "num_updates", "=", "epoch", "*", "len", "(", "loader", ")", "\n", "for", "batch_idx", ",", "(", "input", ",", "target", ")", "in", "enumerate", "(", "loader", ")", ":", "\n", "        ", "last_batch", "=", "batch_idx", "==", "last_idx", "\n", "data_time_m", ".", "update", "(", "time", ".", "time", "(", ")", "-", "end", ")", "\n", "if", "not", "args", ".", "prefetcher", ":", "\n", "            ", "input", ",", "target", "=", "input", ".", "cuda", "(", ")", ",", "target", ".", "cuda", "(", ")", "\n", "if", "mixup_fn", "is", "not", "None", ":", "\n", "                ", "input", ",", "target", "=", "mixup_fn", "(", "input", ",", "target", ")", "# target (B, K), or target is tuple under transmix", "\n", "\n", "", "", "if", "args", ".", "channels_last", ":", "\n", "            ", "input", "=", "input", ".", "contiguous", "(", "memory_format", "=", "torch", ".", "channels_last", ")", "\n", "\n", "", "with", "amp_autocast", "(", ")", ":", "\n", "            ", "output", "=", "model", "(", "input", ")", "\n", "if", "args", ".", "transmix", ":", "\n", "                ", "(", "output", ",", "attn", ")", "=", "output", "# attention from cls_token to images: (b, hw)", "\n", "if", "isinstance", "(", "target", ",", "tuple", ")", ":", "# target is tuple of (target, y1, y2, lam) when switch to cutmix", "\n", "                    ", "target", "=", "mixup_fn", ".", "transmix_label", "(", "target", ",", "attn", ",", "input", ".", "shape", ")", "\n", "", "", "loss", "=", "loss_fn", "(", "output", ",", "target", ")", "\n", "\n", "\n", "", "if", "not", "args", ".", "distributed", ":", "\n", "            ", "losses_m", ".", "update", "(", "loss", ".", "item", "(", ")", ",", "input", ".", "size", "(", "0", ")", ")", "\n", "\n", "", "optimizer", ".", "zero_grad", "(", ")", "\n", "if", "loss_scaler", "is", "not", "None", ":", "\n", "            ", "loss_scaler", "(", "\n", "loss", ",", "optimizer", ",", "\n", "clip_grad", "=", "args", ".", "clip_grad", ",", "clip_mode", "=", "args", ".", "clip_mode", ",", "\n", "parameters", "=", "model_parameters", "(", "model", ",", "exclude_head", "=", "'agc'", "in", "args", ".", "clip_mode", ")", ",", "\n", "create_graph", "=", "second_order", ")", "\n", "", "else", ":", "\n", "            ", "loss", ".", "backward", "(", "create_graph", "=", "second_order", ")", "\n", "if", "args", ".", "clip_grad", "is", "not", "None", ":", "\n", "                ", "dispatch_clip_grad", "(", "\n", "model_parameters", "(", "model", ",", "exclude_head", "=", "'agc'", "in", "args", ".", "clip_mode", ")", ",", "\n", "value", "=", "args", ".", "clip_grad", ",", "mode", "=", "args", ".", "clip_mode", ")", "\n", "", "optimizer", ".", "step", "(", ")", "\n", "\n", "", "if", "model_ema", "is", "not", "None", ":", "\n", "            ", "model_ema", ".", "update", "(", "model", ")", "\n", "\n", "", "torch", ".", "cuda", ".", "synchronize", "(", ")", "\n", "num_updates", "+=", "1", "\n", "batch_time_m", ".", "update", "(", "time", ".", "time", "(", ")", "-", "end", ")", "\n", "if", "last_batch", "or", "batch_idx", "%", "args", ".", "log_interval", "==", "0", ":", "\n", "            ", "lrl", "=", "[", "param_group", "[", "'lr'", "]", "for", "param_group", "in", "optimizer", ".", "param_groups", "]", "\n", "lr", "=", "sum", "(", "lrl", ")", "/", "len", "(", "lrl", ")", "\n", "\n", "if", "args", ".", "distributed", ":", "\n", "                ", "reduced_loss", "=", "reduce_tensor", "(", "loss", ".", "data", ",", "args", ".", "world_size", ")", "\n", "losses_m", ".", "update", "(", "reduced_loss", ".", "item", "(", ")", ",", "input", ".", "size", "(", "0", ")", ")", "\n", "\n", "", "if", "args", ".", "local_rank", "==", "0", ":", "\n", "                ", "_logger", ".", "info", "(", "\n", "'Train: {} [{:>4d}/{} ({:>3.0f}%)]  '", "\n", "'Loss: {loss.val:#.4g} ({loss.avg:#.3g})  '", "\n", "'Time: {batch_time.val:.3f}s, {rate:>7.2f}/s  '", "\n", "'({batch_time.avg:.3f}s, {rate_avg:>7.2f}/s)  '", "\n", "'LR: {lr:.3e}  '", "\n", "'Data: {data_time.val:.3f} ({data_time.avg:.3f})'", ".", "format", "(", "\n", "epoch", ",", "\n", "batch_idx", ",", "len", "(", "loader", ")", ",", "\n", "100.", "*", "batch_idx", "/", "last_idx", ",", "\n", "loss", "=", "losses_m", ",", "\n", "batch_time", "=", "batch_time_m", ",", "\n", "rate", "=", "input", ".", "size", "(", "0", ")", "*", "args", ".", "world_size", "/", "batch_time_m", ".", "val", ",", "\n", "rate_avg", "=", "input", ".", "size", "(", "0", ")", "*", "args", ".", "world_size", "/", "batch_time_m", ".", "avg", ",", "\n", "lr", "=", "lr", ",", "\n", "data_time", "=", "data_time_m", ")", ")", "\n", "\n", "if", "args", ".", "save_images", "and", "output_dir", ":", "\n", "                    ", "torchvision", ".", "utils", ".", "save_image", "(", "\n", "input", ",", "\n", "os", ".", "path", ".", "join", "(", "output_dir", ",", "'train-batch-%d.jpg'", "%", "batch_idx", ")", ",", "\n", "padding", "=", "0", ",", "\n", "normalize", "=", "True", ")", "\n", "\n", "", "", "", "if", "saver", "is", "not", "None", "and", "args", ".", "recovery_interval", "and", "(", "\n", "last_batch", "or", "(", "batch_idx", "+", "1", ")", "%", "args", ".", "recovery_interval", "==", "0", ")", ":", "\n", "            ", "saver", ".", "save_recovery", "(", "epoch", ",", "batch_idx", "=", "batch_idx", ")", "\n", "\n", "", "if", "lr_scheduler", "is", "not", "None", ":", "\n", "            ", "lr_scheduler", ".", "step_update", "(", "num_updates", "=", "num_updates", ",", "metric", "=", "losses_m", ".", "avg", ")", "\n", "\n", "", "end", "=", "time", ".", "time", "(", ")", "\n", "# end for", "\n", "\n", "", "if", "hasattr", "(", "optimizer", ",", "'sync_lookahead'", ")", ":", "\n", "        ", "optimizer", ".", "sync_lookahead", "(", ")", "\n", "\n", "", "return", "OrderedDict", "(", "[", "(", "'loss'", ",", "losses_m", ".", "avg", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.None.train.validate": [[775, 837], ["AverageMeter", "AverageMeter", "AverageMeter", "AverageMeter", "model.eval", "time.time", "collections.OrderedDict", "len", "torch.no_grad", "torch.no_grad", "enumerate", "isinstance", "loss_fn", "accuracy", "torch.cuda.synchronize", "torch.cuda.synchronize", "AverageMeter.update", "AverageMeter.update", "AverageMeter.update", "AverageMeter.update", "time.time", "input.contiguous.cuda", "target.cuda.cuda", "input.contiguous.contiguous", "amp_autocast", "model", "output.unfold().mean.unfold().mean", "reduce_tensor", "reduce_tensor", "reduce_tensor", "reduce_tensor.item", "input.contiguous.size", "reduce_tensor.item", "output.unfold().mean.size", "reduce_tensor.item", "output.unfold().mean.size", "_logger.info", "time.time", "output.unfold().mean.unfold", "target.cuda.size"], "function", ["None"], ["", "def", "validate", "(", "model", ",", "loader", ",", "loss_fn", ",", "args", ",", "amp_autocast", "=", "suppress", ",", "log_suffix", "=", "''", ")", ":", "\n", "    ", "batch_time_m", "=", "AverageMeter", "(", ")", "\n", "losses_m", "=", "AverageMeter", "(", ")", "\n", "top1_m", "=", "AverageMeter", "(", ")", "\n", "top5_m", "=", "AverageMeter", "(", ")", "\n", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "last_idx", "=", "len", "(", "loader", ")", "-", "1", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "batch_idx", ",", "(", "input", ",", "target", ")", "in", "enumerate", "(", "loader", ")", ":", "\n", "            ", "last_batch", "=", "batch_idx", "==", "last_idx", "\n", "if", "not", "args", ".", "prefetcher", ":", "\n", "                ", "input", "=", "input", ".", "cuda", "(", ")", "\n", "target", "=", "target", ".", "cuda", "(", ")", "\n", "", "if", "args", ".", "channels_last", ":", "\n", "                ", "input", "=", "input", ".", "contiguous", "(", "memory_format", "=", "torch", ".", "channels_last", ")", "\n", "\n", "", "with", "amp_autocast", "(", ")", ":", "\n", "                ", "output", "=", "model", "(", "input", ")", "\n", "", "if", "isinstance", "(", "output", ",", "(", "tuple", ",", "list", ")", ")", ":", "\n", "                ", "output", "=", "output", "[", "0", "]", "\n", "\n", "# augmentation reduction", "\n", "", "reduce_factor", "=", "args", ".", "tta", "\n", "if", "reduce_factor", ">", "1", ":", "\n", "                ", "output", "=", "output", ".", "unfold", "(", "0", ",", "reduce_factor", ",", "reduce_factor", ")", ".", "mean", "(", "dim", "=", "2", ")", "\n", "target", "=", "target", "[", "0", ":", "target", ".", "size", "(", "0", ")", ":", "reduce_factor", "]", "\n", "\n", "", "loss", "=", "loss_fn", "(", "output", ",", "target", ")", "\n", "acc1", ",", "acc5", "=", "accuracy", "(", "output", ",", "target", ",", "topk", "=", "(", "1", ",", "5", ")", ")", "\n", "\n", "if", "args", ".", "distributed", ":", "\n", "                ", "reduced_loss", "=", "reduce_tensor", "(", "loss", ".", "data", ",", "args", ".", "world_size", ")", "\n", "acc1", "=", "reduce_tensor", "(", "acc1", ",", "args", ".", "world_size", ")", "\n", "acc5", "=", "reduce_tensor", "(", "acc5", ",", "args", ".", "world_size", ")", "\n", "", "else", ":", "\n", "                ", "reduced_loss", "=", "loss", ".", "data", "\n", "\n", "", "torch", ".", "cuda", ".", "synchronize", "(", ")", "\n", "\n", "losses_m", ".", "update", "(", "reduced_loss", ".", "item", "(", ")", ",", "input", ".", "size", "(", "0", ")", ")", "\n", "top1_m", ".", "update", "(", "acc1", ".", "item", "(", ")", ",", "output", ".", "size", "(", "0", ")", ")", "\n", "top5_m", ".", "update", "(", "acc5", ".", "item", "(", ")", ",", "output", ".", "size", "(", "0", ")", ")", "\n", "\n", "batch_time_m", ".", "update", "(", "time", ".", "time", "(", ")", "-", "end", ")", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "if", "args", ".", "local_rank", "==", "0", "and", "(", "last_batch", "or", "batch_idx", "%", "args", ".", "log_interval", "==", "0", ")", ":", "\n", "                ", "log_name", "=", "'Test'", "+", "log_suffix", "\n", "_logger", ".", "info", "(", "\n", "'{0}: [{1:>4d}/{2}]  '", "\n", "'Time: {batch_time.val:.3f} ({batch_time.avg:.3f})  '", "\n", "'Loss: {loss.val:>7.4f} ({loss.avg:>6.4f})  '", "\n", "'Acc@1: {top1.val:>7.4f} ({top1.avg:>7.4f})  '", "\n", "'Acc@5: {top5.val:>7.4f} ({top5.avg:>7.4f})'", ".", "format", "(", "\n", "log_name", ",", "batch_idx", ",", "last_idx", ",", "batch_time", "=", "batch_time_m", ",", "\n", "loss", "=", "losses_m", ",", "top1", "=", "top1_m", ",", "top5", "=", "top5_m", ")", ")", "\n", "\n", "", "", "", "metrics", "=", "OrderedDict", "(", "[", "(", "'loss'", ",", "losses_m", ".", "avg", ")", ",", "(", "'top1'", ",", "top1_m", ".", "avg", ")", ",", "(", "'top5'", ",", "top5_m", ".", "avg", ")", "]", ")", "\n", "\n", "return", "metrics", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.Attention.__init__": [[185, 197], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.VisionTransformer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dim", ",", "num_heads", "=", "8", ",", "qkv_bias", "=", "False", ",", "attn_drop", "=", "0.", ",", "proj_drop", "=", "0.", ",", "return_attn", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "head_dim", "=", "dim", "//", "num_heads", "\n", "self", ".", "scale", "=", "head_dim", "**", "-", "0.5", "\n", "\n", "self", ".", "qkv", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", "*", "3", ",", "bias", "=", "qkv_bias", ")", "\n", "self", ".", "attn_drop", "=", "nn", ".", "Dropout", "(", "attn_drop", ")", "\n", "self", ".", "proj", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", ")", "\n", "self", ".", "proj_drop", "=", "nn", ".", "Dropout", "(", "proj_drop", ")", "\n", "\n", "self", ".", "return_attn", "=", "return_attn", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.Attention.forward": [[198, 214], ["vision_transformer.Attention.qkv().reshape().permute", "vision_transformer.Attention.unbind", "vision_transformer.Attention.softmax", "vision_transformer.Attention.detach().clone", "vision_transformer.Attention.attn_drop", "vision_transformer.Attention.proj", "vision_transformer.Attention.proj_drop", "vision_transformer.Attention.qkv().reshape", "k.transpose", "vision_transformer.Attention.detach", "vision_transformer.Attention.qkv"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "B", ",", "N", ",", "C", "=", "x", ".", "shape", "\n", "qkv", "=", "self", ".", "qkv", "(", "x", ")", ".", "reshape", "(", "B", ",", "N", ",", "3", ",", "self", ".", "num_heads", ",", "C", "//", "self", ".", "num_heads", ")", ".", "permute", "(", "2", ",", "0", ",", "3", ",", "1", ",", "4", ")", "\n", "q", ",", "k", ",", "v", "=", "qkv", ".", "unbind", "(", "0", ")", "# make torchscript happy (cannot use tensor as tuple)", "\n", "\n", "attn", "=", "(", "q", "@", "k", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ")", "*", "self", ".", "scale", "\n", "attn", "=", "attn", ".", "softmax", "(", "dim", "=", "-", "1", ")", "\n", "attn_softmax", "=", "attn", ".", "detach", "(", ")", ".", "clone", "(", ")", "\n", "attn", "=", "self", ".", "attn_drop", "(", "attn", ")", "\n", "\n", "x", "=", "(", "attn", "@", "v", ")", ".", "transpose", "(", "1", ",", "2", ")", ".", "reshape", "(", "B", ",", "N", ",", "C", ")", "\n", "x", "=", "self", ".", "proj", "(", "x", ")", "\n", "x", "=", "self", ".", "proj_drop", "(", "x", ")", "\n", "if", "self", ".", "return_attn", ":", "\n", "            ", "return", "x", ",", "attn_softmax", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.Block.__init__": [[218, 229], ["torch.Module.__init__", "norm_layer", "vision_transformer.Attention", "norm_layer", "int", "timm.models.layers.Mlp", "timm.models.layers.DropPath", "torch.Identity", "torch.Identity", "torch.Identity"], "methods", ["home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.VisionTransformer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dim", ",", "num_heads", ",", "mlp_ratio", "=", "4.", ",", "qkv_bias", "=", "False", ",", "drop", "=", "0.", ",", "attn_drop", "=", "0.", ",", "\n", "drop_path", "=", "0.", ",", "act_layer", "=", "nn", ".", "GELU", ",", "norm_layer", "=", "nn", ".", "LayerNorm", ",", "return_attn", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "norm1", "=", "norm_layer", "(", "dim", ")", "\n", "self", ".", "attn", "=", "Attention", "(", "dim", ",", "num_heads", "=", "num_heads", ",", "qkv_bias", "=", "qkv_bias", ",", "attn_drop", "=", "attn_drop", ",", "proj_drop", "=", "drop", ",", "return_attn", "=", "return_attn", ")", "\n", "# NOTE: drop path for stochastic depth, we shall see if this is better than dropout here", "\n", "self", ".", "drop_path", "=", "DropPath", "(", "drop_path", ")", "if", "drop_path", ">", "0.", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "norm2", "=", "norm_layer", "(", "dim", ")", "\n", "mlp_hidden_dim", "=", "int", "(", "dim", "*", "mlp_ratio", ")", "\n", "self", ".", "mlp", "=", "Mlp", "(", "in_features", "=", "dim", ",", "hidden_features", "=", "mlp_hidden_dim", ",", "act_layer", "=", "act_layer", ",", "drop", "=", "drop", ")", "\n", "self", ".", "return_attn", "=", "return_attn", "# modified by jchen", "\n", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.Block.forward": [[229, 240], ["vision_transformer.Block.attn", "vision_transformer.Block.drop_path", "vision_transformer.Block.drop_path", "vision_transformer.Block.norm1", "vision_transformer.Block.drop_path", "vision_transformer.Block.drop_path", "vision_transformer.Block.attn", "vision_transformer.Block.mlp", "vision_transformer.Block.mlp", "vision_transformer.Block.norm1", "vision_transformer.Block.norm2", "vision_transformer.Block.norm2"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "self", ".", "return_attn", ":", "\n", "            ", "res", "=", "x", "\n", "x", ",", "attn", "=", "self", ".", "attn", "(", "self", ".", "norm1", "(", "x", ")", ")", "\n", "x", "=", "res", "+", "self", ".", "drop_path", "(", "x", ")", "\n", "x", "=", "x", "+", "self", ".", "drop_path", "(", "self", ".", "mlp", "(", "self", ".", "norm2", "(", "x", ")", ")", ")", "\n", "return", "x", ",", "attn", "\n", "\n", "", "x", "=", "x", "+", "self", ".", "drop_path", "(", "self", ".", "attn", "(", "self", ".", "norm1", "(", "x", ")", ")", ")", "\n", "x", "=", "x", "+", "self", ".", "drop_path", "(", "self", ".", "mlp", "(", "self", ".", "norm2", "(", "x", ")", ")", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.VisionTransformer.__init__": [[252, 319], ["torch.Module.__init__", "embed_layer", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Sequential", "torch.Sequential", "torch.Sequential", "norm_layer", "vision_transformer.VisionTransformer.init_weights", "functools.partial", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "x.item", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Identity", "torch.Identity", "torch.Identity", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "collections.OrderedDict", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Identity", "torch.Identity", "torch.Identity", "vision_transformer.Block", "range", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Tanh", "torch.Tanh", "torch.Tanh"], "methods", ["home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.VisionTransformer.__init__", "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.VisionTransformer.init_weights"], ["def", "__init__", "(", "self", ",", "img_size", "=", "224", ",", "patch_size", "=", "16", ",", "in_chans", "=", "3", ",", "num_classes", "=", "1000", ",", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "\n", "num_heads", "=", "12", ",", "mlp_ratio", "=", "4.", ",", "qkv_bias", "=", "True", ",", "representation_size", "=", "None", ",", "distilled", "=", "False", ",", "\n", "drop_rate", "=", "0.", ",", "attn_drop_rate", "=", "0.", ",", "drop_path_rate", "=", "0.", ",", "embed_layer", "=", "PatchEmbed", ",", "norm_layer", "=", "None", ",", "\n", "act_layer", "=", "None", ",", "weight_init", "=", "''", ",", "return_attn", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            img_size (int, tuple): input image size\n            patch_size (int, tuple): patch size\n            in_chans (int): number of input channels\n            num_classes (int): number of classes for classification head\n            embed_dim (int): embedding dimension\n            depth (int): depth of transformer\n            num_heads (int): number of attention heads\n            mlp_ratio (int): ratio of mlp hidden dim to embedding dim\n            qkv_bias (bool): enable bias for qkv if True\n            representation_size (Optional[int]): enable and set representation layer (pre-logits) to this value if set\n            distilled (bool): model includes a distillation token and head as in DeiT models\n            drop_rate (float): dropout rate\n            attn_drop_rate (float): attention dropout rate\n            drop_path_rate (float): stochastic depth rate\n            embed_layer (nn.Module): patch embedding layer\n            norm_layer: (nn.Module): normalization layer\n            weight_init: (str): weight init scheme\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "return_attn", "=", "return_attn", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "num_features", "=", "self", ".", "embed_dim", "=", "embed_dim", "# num_features for consistency with other models", "\n", "self", ".", "num_tokens", "=", "2", "if", "distilled", "else", "1", "\n", "norm_layer", "=", "norm_layer", "or", "partial", "(", "nn", ".", "LayerNorm", ",", "eps", "=", "1e-6", ")", "\n", "act_layer", "=", "act_layer", "or", "nn", ".", "GELU", "\n", "\n", "self", ".", "patch_embed", "=", "embed_layer", "(", "\n", "img_size", "=", "img_size", ",", "patch_size", "=", "patch_size", ",", "in_chans", "=", "in_chans", ",", "embed_dim", "=", "embed_dim", ")", "\n", "num_patches", "=", "self", ".", "patch_embed", ".", "num_patches", "\n", "\n", "self", ".", "cls_token", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "1", ",", "1", ",", "embed_dim", ")", ")", "\n", "self", ".", "dist_token", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "1", ",", "1", ",", "embed_dim", ")", ")", "if", "distilled", "else", "None", "\n", "self", ".", "pos_embed", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "1", ",", "num_patches", "+", "self", ".", "num_tokens", ",", "embed_dim", ")", ")", "\n", "self", ".", "pos_drop", "=", "nn", ".", "Dropout", "(", "p", "=", "drop_rate", ")", "\n", "\n", "dpr", "=", "[", "x", ".", "item", "(", ")", "for", "x", "in", "torch", ".", "linspace", "(", "0", ",", "drop_path_rate", ",", "depth", ")", "]", "# stochastic depth decay rule", "\n", "self", ".", "blocks", "=", "nn", ".", "Sequential", "(", "*", "[", "\n", "Block", "(", "\n", "dim", "=", "embed_dim", ",", "num_heads", "=", "num_heads", ",", "mlp_ratio", "=", "mlp_ratio", ",", "qkv_bias", "=", "qkv_bias", ",", "drop", "=", "drop_rate", ",", "\n", "attn_drop", "=", "attn_drop_rate", ",", "drop_path", "=", "dpr", "[", "i", "]", ",", "norm_layer", "=", "norm_layer", ",", "act_layer", "=", "act_layer", ",", "\n", "return_attn", "=", "return_attn", "and", "i", "==", "depth", "-", "1", ")", "\n", "for", "i", "in", "range", "(", "depth", ")", "]", ")", "\n", "self", ".", "norm", "=", "norm_layer", "(", "embed_dim", ")", "\n", "\n", "# Representation layer", "\n", "if", "representation_size", "and", "not", "distilled", ":", "\n", "            ", "self", ".", "num_features", "=", "representation_size", "\n", "self", ".", "pre_logits", "=", "nn", ".", "Sequential", "(", "OrderedDict", "(", "[", "\n", "(", "'fc'", ",", "nn", ".", "Linear", "(", "embed_dim", ",", "representation_size", ")", ")", ",", "\n", "(", "'act'", ",", "nn", ".", "Tanh", "(", ")", ")", "\n", "]", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "pre_logits", "=", "nn", ".", "Identity", "(", ")", "\n", "\n", "# Classifier head(s)", "\n", "", "self", ".", "head", "=", "nn", ".", "Linear", "(", "self", ".", "num_features", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "head_dist", "=", "None", "\n", "if", "distilled", ":", "\n", "            ", "self", ".", "head_dist", "=", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "self", ".", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "", "self", ".", "init_weights", "(", "weight_init", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.VisionTransformer.init_weights": [[320, 332], ["timm.models.layers.trunc_normal_", "mode.startswith", "timm.models.layers.trunc_normal_", "timm.models.helpers.named_apply", "timm.models.layers.trunc_normal_", "vision_transformer.VisionTransformer.apply", "math.log", "functools.partial"], "methods", ["None"], ["", "def", "init_weights", "(", "self", ",", "mode", "=", "''", ")", ":", "\n", "        ", "assert", "mode", "in", "(", "'jax'", ",", "'jax_nlhb'", ",", "'nlhb'", ",", "''", ")", "\n", "head_bias", "=", "-", "math", ".", "log", "(", "self", ".", "num_classes", ")", "if", "'nlhb'", "in", "mode", "else", "0.", "\n", "trunc_normal_", "(", "self", ".", "pos_embed", ",", "std", "=", ".02", ")", "\n", "if", "self", ".", "dist_token", "is", "not", "None", ":", "\n", "            ", "trunc_normal_", "(", "self", ".", "dist_token", ",", "std", "=", ".02", ")", "\n", "", "if", "mode", ".", "startswith", "(", "'jax'", ")", ":", "\n", "# leave cls token as zeros to match jax impl", "\n", "            ", "named_apply", "(", "partial", "(", "_init_vit_weights", ",", "head_bias", "=", "head_bias", ",", "jax_impl", "=", "True", ")", ",", "self", ")", "\n", "", "else", ":", "\n", "            ", "trunc_normal_", "(", "self", ".", "cls_token", ",", "std", "=", ".02", ")", "\n", "self", ".", "apply", "(", "_init_vit_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.VisionTransformer._init_weights": [[333, 336], ["vision_transformer._init_vit_weights"], "methods", ["home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer._init_vit_weights"], ["", "", "def", "_init_weights", "(", "self", ",", "m", ")", ":", "\n", "# this fn left here for compat with downstream users", "\n", "        ", "_init_vit_weights", "(", "m", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.VisionTransformer.load_pretrained": [[337, 340], ["torch.jit.ignore", "torch.jit.ignore", "torch.jit.ignore", "torch.jit.ignore", "torch.jit.ignore", "torch.jit.ignore", "torch.jit.ignore", "torch.jit.ignore", "torch.jit.ignore", "vision_transformer._load_weights"], "methods", ["home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer._load_weights"], ["", "@", "torch", ".", "jit", ".", "ignore", "(", ")", "\n", "def", "load_pretrained", "(", "self", ",", "checkpoint_path", ",", "prefix", "=", "''", ")", ":", "\n", "        ", "_load_weights", "(", "self", ",", "checkpoint_path", ",", "prefix", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.VisionTransformer.no_weight_decay": [[341, 344], ["None"], "methods", ["None"], ["", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "no_weight_decay", "(", "self", ")", ":", "\n", "        ", "return", "{", "'pos_embed'", ",", "'cls_token'", ",", "'dist_token'", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.VisionTransformer.get_classifier": [[345, 350], ["None"], "methods", ["None"], ["", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "dist_token", "is", "None", ":", "\n", "            ", "return", "self", ".", "head", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "head", ",", "self", ".", "head_dist", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.VisionTransformer.reset_classifier": [[351, 356], ["torch.Linear", "torch.Linear", "torch.Linear", "torch.Identity", "torch.Identity", "torch.Identity", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Identity", "torch.Identity", "torch.Identity"], "methods", ["None"], ["", "", "def", "reset_classifier", "(", "self", ",", "num_classes", ",", "global_pool", "=", "''", ")", ":", "\n", "        ", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "head", "=", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "if", "self", ".", "num_tokens", "==", "2", ":", "\n", "            ", "self", ".", "head_dist", "=", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "self", ".", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.VisionTransformer.forward_features": [[357, 380], ["vision_transformer.VisionTransformer.patch_embed", "vision_transformer.VisionTransformer.cls_token.expand", "vision_transformer.VisionTransformer.pos_drop", "vision_transformer.VisionTransformer.blocks", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "vision_transformer.VisionTransformer.norm", "vision_transformer.VisionTransformer.norm", "vision_transformer.VisionTransformer.pre_logits", "vision_transformer.VisionTransformer.dist_token.expand", "vision_transformer.VisionTransformer.pre_logits"], "methods", ["None"], ["", "", "def", "forward_features", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "patch_embed", "(", "x", ")", "\n", "cls_token", "=", "self", ".", "cls_token", ".", "expand", "(", "x", ".", "shape", "[", "0", "]", ",", "-", "1", ",", "-", "1", ")", "# stole cls_tokens impl from Phil Wang, thanks", "\n", "if", "self", ".", "dist_token", "is", "None", ":", "\n", "            ", "x", "=", "torch", ".", "cat", "(", "(", "cls_token", ",", "x", ")", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "torch", ".", "cat", "(", "(", "cls_token", ",", "self", ".", "dist_token", ".", "expand", "(", "x", ".", "shape", "[", "0", "]", ",", "-", "1", ",", "-", "1", ")", ",", "x", ")", ",", "dim", "=", "1", ")", "\n", "", "x", "=", "self", ".", "pos_drop", "(", "x", "+", "self", ".", "pos_embed", ")", "\n", "x", "=", "self", ".", "blocks", "(", "x", ")", "\n", "if", "self", ".", "return_attn", ":", "\n", "            ", "x", ",", "attn", "=", "x", "[", "0", "]", ",", "x", "[", "1", "]", "\n", "attn", "=", "torch", ".", "mean", "(", "attn", "[", ":", ",", ":", ",", "0", ",", "1", ":", "]", ",", "dim", "=", "1", ")", "# attn from cls_token to images", "\n", "x", "=", "self", ".", "norm", "(", "x", ")", "\n", "if", "self", ".", "dist_token", "is", "None", ":", "\n", "                ", "return", "self", ".", "pre_logits", "(", "x", "[", ":", ",", "0", "]", ")", ",", "attn", "\n", "", "else", ":", "\n", "                ", "return", "x", "[", ":", ",", "0", "]", ",", "x", "[", ":", ",", "1", "]", ",", "attn", "\n", "", "", "else", ":", "\n", "            ", "x", "=", "self", ".", "norm", "(", "x", ")", "\n", "if", "self", ".", "dist_token", "is", "None", ":", "\n", "                ", "return", "self", ".", "pre_logits", "(", "x", "[", ":", ",", "0", "]", ")", "\n", "", "else", ":", "\n", "                ", "return", "x", "[", ":", ",", "0", "]", ",", "x", "[", ":", ",", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.VisionTransformer.forward": [[381, 406], ["vision_transformer.VisionTransformer.forward_features", "vision_transformer.VisionTransformer.head", "vision_transformer.VisionTransformer.head", "vision_transformer.VisionTransformer.head", "vision_transformer.VisionTransformer.head_dist", "vision_transformer.VisionTransformer.head", "vision_transformer.VisionTransformer.head_dist", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting", "torch.jit.is_scripting"], "methods", ["home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.VisionTransformer.forward_features"], ["", "", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "if", "self", ".", "return_attn", ":", "\n", "            ", "if", "self", ".", "head_dist", "is", "not", "None", ":", "\n", "                ", "x", ",", "x_dist", ",", "attn", "=", "self", ".", "head", "(", "x", "[", "0", "]", ")", ",", "self", ".", "head_dist", "(", "x", "[", "1", "]", ")", ",", "x", "[", "2", "]", "# x must be a tuple", "\n", "if", "self", ".", "training", "and", "not", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "# during inference, return the average of both classifier predictions", "\n", "                    ", "return", "x", ",", "x_dist", ",", "attn", "\n", "", "else", ":", "\n", "                    ", "return", "(", "x", "+", "x_dist", ")", "/", "2", ",", "attn", "\n", "", "", "else", ":", "\n", "                ", "x", ",", "attn", "=", "x", "[", "0", "]", ",", "x", "[", "1", "]", "\n", "x", "=", "self", ".", "head", "(", "x", ")", "\n", "", "return", "x", ",", "attn", "\n", "", "else", ":", "\n", "            ", "if", "self", ".", "head_dist", "is", "not", "None", ":", "\n", "                ", "x", ",", "x_dist", "=", "self", ".", "head", "(", "x", "[", "0", "]", ")", ",", "self", ".", "head_dist", "(", "x", "[", "1", "]", ")", "# x must be a tuple", "\n", "if", "self", ".", "training", "and", "not", "torch", ".", "jit", ".", "is_scripting", "(", ")", ":", "\n", "# during inference, return the average of both classifier predictions", "\n", "                    ", "return", "x", ",", "x_dist", "\n", "", "else", ":", "\n", "                    ", "return", "(", "x", "+", "x_dist", ")", "/", "2", "\n", "", "", "else", ":", "\n", "                ", "x", "=", "self", ".", "head", "(", "x", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer._cfg": [[43, 51], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "\n", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "'pool_size'", ":", "None", ",", "\n", "'crop_pct'", ":", ".9", ",", "'interpolation'", ":", "'bicubic'", ",", "'fixed_input_size'", ":", "True", ",", "\n", "'mean'", ":", "IMAGENET_INCEPTION_MEAN", ",", "'std'", ":", "IMAGENET_INCEPTION_STD", ",", "\n", "'first_conv'", ":", "'patch_embed.proj'", ",", "'classifier'", ":", "'head'", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer._init_vit_weights": [[408, 441], ["isinstance", "name.startswith", "torch.init.zeros_", "torch.init.constant_", "name.startswith", "isinstance", "timm.models.layers.lecun_normal_", "isinstance", "timm.models.layers.lecun_normal_", "torch.init.zeros_", "torch.init.zeros_", "torch.init.zeros_", "torch.init.ones_", "torch.init.xavier_uniform_", "timm.models.layers.trunc_normal_", "torch.init.zeros_", "torch.init.normal_", "torch.init.zeros_"], "function", ["None"], ["", "", "", "def", "_init_vit_weights", "(", "module", ":", "nn", ".", "Module", ",", "name", ":", "str", "=", "''", ",", "head_bias", ":", "float", "=", "0.", ",", "jax_impl", ":", "bool", "=", "False", ")", ":", "\n", "    ", "\"\"\" ViT weight initialization\n    * When called without n, head_bias, jax_impl args it will behave exactly the same\n      as my original init for compatibility with prev hparam / downstream use cases (ie DeiT).\n    * When called w/ valid n (module name) and jax_impl=True, will (hopefully) match JAX impl\n    \"\"\"", "\n", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", ":", "\n", "        ", "if", "name", ".", "startswith", "(", "'head'", ")", ":", "\n", "            ", "nn", ".", "init", ".", "zeros_", "(", "module", ".", "weight", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "module", ".", "bias", ",", "head_bias", ")", "\n", "", "elif", "name", ".", "startswith", "(", "'pre_logits'", ")", ":", "\n", "            ", "lecun_normal_", "(", "module", ".", "weight", ")", "\n", "nn", ".", "init", ".", "zeros_", "(", "module", ".", "bias", ")", "\n", "", "else", ":", "\n", "            ", "if", "jax_impl", ":", "\n", "                ", "nn", ".", "init", ".", "xavier_uniform_", "(", "module", ".", "weight", ")", "\n", "if", "module", ".", "bias", "is", "not", "None", ":", "\n", "                    ", "if", "'mlp'", "in", "name", ":", "\n", "                        ", "nn", ".", "init", ".", "normal_", "(", "module", ".", "bias", ",", "std", "=", "1e-6", ")", "\n", "", "else", ":", "\n", "                        ", "nn", ".", "init", ".", "zeros_", "(", "module", ".", "bias", ")", "\n", "", "", "", "else", ":", "\n", "                ", "trunc_normal_", "(", "module", ".", "weight", ",", "std", "=", ".02", ")", "\n", "if", "module", ".", "bias", "is", "not", "None", ":", "\n", "                    ", "nn", ".", "init", ".", "zeros_", "(", "module", ".", "bias", ")", "\n", "", "", "", "", "elif", "jax_impl", "and", "isinstance", "(", "module", ",", "nn", ".", "Conv2d", ")", ":", "\n", "# NOTE conv was left to pytorch default in my original init", "\n", "        ", "lecun_normal_", "(", "module", ".", "weight", ")", "\n", "if", "module", ".", "bias", "is", "not", "None", ":", "\n", "            ", "nn", ".", "init", ".", "zeros_", "(", "module", ".", "bias", ")", "\n", "", "", "elif", "isinstance", "(", "module", ",", "(", "nn", ".", "LayerNorm", ",", "nn", ".", "GroupNorm", ",", "nn", ".", "BatchNorm2d", ")", ")", ":", "\n", "        ", "nn", ".", "init", ".", "zeros_", "(", "module", ".", "bias", ")", "\n", "nn", ".", "init", ".", "ones_", "(", "module", ".", "weight", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer._load_weights": [[443, 521], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "np.load", "hasattr", "model.patch_embed.proj.weight.copy_", "model.patch_embed.proj.bias.copy_", "model.cls_token.copy_", "vision_transformer._load_weights._n2p"], "function", ["None"], ["", "", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "_load_weights", "(", "model", ":", "VisionTransformer", ",", "checkpoint_path", ":", "str", ",", "prefix", ":", "str", "=", "''", ")", ":", "\n", "    ", "\"\"\" Load weights from .npz checkpoints for official Google Brain Flax implementation\n    \"\"\"", "\n", "import", "numpy", "as", "np", "\n", "\n", "def", "_n2p", "(", "w", ",", "t", "=", "True", ")", ":", "\n", "        ", "if", "w", ".", "ndim", "==", "4", "and", "w", ".", "shape", "[", "0", "]", "==", "w", ".", "shape", "[", "1", "]", "==", "w", ".", "shape", "[", "2", "]", "==", "1", ":", "\n", "            ", "w", "=", "w", ".", "flatten", "(", ")", "\n", "", "if", "t", ":", "\n", "            ", "if", "w", ".", "ndim", "==", "4", ":", "\n", "                ", "w", "=", "w", ".", "transpose", "(", "[", "3", ",", "2", ",", "0", ",", "1", "]", ")", "\n", "", "elif", "w", ".", "ndim", "==", "3", ":", "\n", "                ", "w", "=", "w", ".", "transpose", "(", "[", "2", ",", "0", ",", "1", "]", ")", "\n", "", "elif", "w", ".", "ndim", "==", "2", ":", "\n", "                ", "w", "=", "w", ".", "transpose", "(", "[", "1", ",", "0", "]", ")", "\n", "", "", "return", "torch", ".", "from_numpy", "(", "w", ")", "\n", "\n", "", "w", "=", "np", ".", "load", "(", "checkpoint_path", ")", "\n", "if", "not", "prefix", "and", "'opt/target/embedding/kernel'", "in", "w", ":", "\n", "        ", "prefix", "=", "'opt/target/'", "\n", "\n", "", "if", "hasattr", "(", "model", ".", "patch_embed", ",", "'backbone'", ")", ":", "\n", "# hybrid", "\n", "        ", "backbone", "=", "model", ".", "patch_embed", ".", "backbone", "\n", "stem_only", "=", "not", "hasattr", "(", "backbone", ",", "'stem'", ")", "\n", "stem", "=", "backbone", "if", "stem_only", "else", "backbone", ".", "stem", "\n", "stem", ".", "conv", ".", "weight", ".", "copy_", "(", "adapt_input_conv", "(", "stem", ".", "conv", ".", "weight", ".", "shape", "[", "1", "]", ",", "_n2p", "(", "w", "[", "f'{prefix}conv_root/kernel'", "]", ")", ")", ")", "\n", "stem", ".", "norm", ".", "weight", ".", "copy_", "(", "_n2p", "(", "w", "[", "f'{prefix}gn_root/scale'", "]", ")", ")", "\n", "stem", ".", "norm", ".", "bias", ".", "copy_", "(", "_n2p", "(", "w", "[", "f'{prefix}gn_root/bias'", "]", ")", ")", "\n", "if", "not", "stem_only", ":", "\n", "            ", "for", "i", ",", "stage", "in", "enumerate", "(", "backbone", ".", "stages", ")", ":", "\n", "                ", "for", "j", ",", "block", "in", "enumerate", "(", "stage", ".", "blocks", ")", ":", "\n", "                    ", "bp", "=", "f'{prefix}block{i + 1}/unit{j + 1}/'", "\n", "for", "r", "in", "range", "(", "3", ")", ":", "\n", "                        ", "getattr", "(", "block", ",", "f'conv{r + 1}'", ")", ".", "weight", ".", "copy_", "(", "_n2p", "(", "w", "[", "f'{bp}conv{r + 1}/kernel'", "]", ")", ")", "\n", "getattr", "(", "block", ",", "f'norm{r + 1}'", ")", ".", "weight", ".", "copy_", "(", "_n2p", "(", "w", "[", "f'{bp}gn{r + 1}/scale'", "]", ")", ")", "\n", "getattr", "(", "block", ",", "f'norm{r + 1}'", ")", ".", "bias", ".", "copy_", "(", "_n2p", "(", "w", "[", "f'{bp}gn{r + 1}/bias'", "]", ")", ")", "\n", "", "if", "block", ".", "downsample", "is", "not", "None", ":", "\n", "                        ", "block", ".", "downsample", ".", "conv", ".", "weight", ".", "copy_", "(", "_n2p", "(", "w", "[", "f'{bp}conv_proj/kernel'", "]", ")", ")", "\n", "block", ".", "downsample", ".", "norm", ".", "weight", ".", "copy_", "(", "_n2p", "(", "w", "[", "f'{bp}gn_proj/scale'", "]", ")", ")", "\n", "block", ".", "downsample", ".", "norm", ".", "bias", ".", "copy_", "(", "_n2p", "(", "w", "[", "f'{bp}gn_proj/bias'", "]", ")", ")", "\n", "", "", "", "", "embed_conv_w", "=", "_n2p", "(", "w", "[", "f'{prefix}embedding/kernel'", "]", ")", "\n", "", "else", ":", "\n", "        ", "embed_conv_w", "=", "adapt_input_conv", "(", "\n", "model", ".", "patch_embed", ".", "proj", ".", "weight", ".", "shape", "[", "1", "]", ",", "_n2p", "(", "w", "[", "f'{prefix}embedding/kernel'", "]", ")", ")", "\n", "", "model", ".", "patch_embed", ".", "proj", ".", "weight", ".", "copy_", "(", "embed_conv_w", ")", "\n", "model", ".", "patch_embed", ".", "proj", ".", "bias", ".", "copy_", "(", "_n2p", "(", "w", "[", "f'{prefix}embedding/bias'", "]", ")", ")", "\n", "model", ".", "cls_token", ".", "copy_", "(", "_n2p", "(", "w", "[", "f'{prefix}cls'", "]", ",", "t", "=", "False", ")", ")", "\n", "pos_embed_w", "=", "_n2p", "(", "w", "[", "f'{prefix}Transformer/posembed_input/pos_embedding'", "]", ",", "t", "=", "False", ")", "\n", "if", "pos_embed_w", ".", "shape", "!=", "model", ".", "pos_embed", ".", "shape", ":", "\n", "        ", "pos_embed_w", "=", "resize_pos_embed", "(", "# resize pos embedding when different size from pretrained weights", "\n", "pos_embed_w", ",", "model", ".", "pos_embed", ",", "getattr", "(", "model", ",", "'num_tokens'", ",", "1", ")", ",", "model", ".", "patch_embed", ".", "grid_size", ")", "\n", "", "model", ".", "pos_embed", ".", "copy_", "(", "pos_embed_w", ")", "\n", "model", ".", "norm", ".", "weight", ".", "copy_", "(", "_n2p", "(", "w", "[", "f'{prefix}Transformer/encoder_norm/scale'", "]", ")", ")", "\n", "model", ".", "norm", ".", "bias", ".", "copy_", "(", "_n2p", "(", "w", "[", "f'{prefix}Transformer/encoder_norm/bias'", "]", ")", ")", "\n", "if", "isinstance", "(", "model", ".", "head", ",", "nn", ".", "Linear", ")", "and", "model", ".", "head", ".", "bias", ".", "shape", "[", "0", "]", "==", "w", "[", "f'{prefix}head/bias'", "]", ".", "shape", "[", "-", "1", "]", ":", "\n", "        ", "model", ".", "head", ".", "weight", ".", "copy_", "(", "_n2p", "(", "w", "[", "f'{prefix}head/kernel'", "]", ")", ")", "\n", "model", ".", "head", ".", "bias", ".", "copy_", "(", "_n2p", "(", "w", "[", "f'{prefix}head/bias'", "]", ")", ")", "\n", "", "if", "isinstance", "(", "getattr", "(", "model", ".", "pre_logits", ",", "'fc'", ",", "None", ")", ",", "nn", ".", "Linear", ")", "and", "f'{prefix}pre_logits/bias'", "in", "w", ":", "\n", "        ", "model", ".", "pre_logits", ".", "fc", ".", "weight", ".", "copy_", "(", "_n2p", "(", "w", "[", "f'{prefix}pre_logits/kernel'", "]", ")", ")", "\n", "model", ".", "pre_logits", ".", "fc", ".", "bias", ".", "copy_", "(", "_n2p", "(", "w", "[", "f'{prefix}pre_logits/bias'", "]", ")", ")", "\n", "", "for", "i", ",", "block", "in", "enumerate", "(", "model", ".", "blocks", ".", "children", "(", ")", ")", ":", "\n", "        ", "block_prefix", "=", "f'{prefix}Transformer/encoderblock_{i}/'", "\n", "mha_prefix", "=", "block_prefix", "+", "'MultiHeadDotProductAttention_1/'", "\n", "block", ".", "norm1", ".", "weight", ".", "copy_", "(", "_n2p", "(", "w", "[", "f'{block_prefix}LayerNorm_0/scale'", "]", ")", ")", "\n", "block", ".", "norm1", ".", "bias", ".", "copy_", "(", "_n2p", "(", "w", "[", "f'{block_prefix}LayerNorm_0/bias'", "]", ")", ")", "\n", "block", ".", "attn", ".", "qkv", ".", "weight", ".", "copy_", "(", "torch", ".", "cat", "(", "[", "\n", "_n2p", "(", "w", "[", "f'{mha_prefix}{n}/kernel'", "]", ",", "t", "=", "False", ")", ".", "flatten", "(", "1", ")", ".", "T", "for", "n", "in", "(", "'query'", ",", "'key'", ",", "'value'", ")", "]", ")", ")", "\n", "block", ".", "attn", ".", "qkv", ".", "bias", ".", "copy_", "(", "torch", ".", "cat", "(", "[", "\n", "_n2p", "(", "w", "[", "f'{mha_prefix}{n}/bias'", "]", ",", "t", "=", "False", ")", ".", "reshape", "(", "-", "1", ")", "for", "n", "in", "(", "'query'", ",", "'key'", ",", "'value'", ")", "]", ")", ")", "\n", "block", ".", "attn", ".", "proj", ".", "weight", ".", "copy_", "(", "_n2p", "(", "w", "[", "f'{mha_prefix}out/kernel'", "]", ")", ".", "flatten", "(", "1", ")", ")", "\n", "block", ".", "attn", ".", "proj", ".", "bias", ".", "copy_", "(", "_n2p", "(", "w", "[", "f'{mha_prefix}out/bias'", "]", ")", ")", "\n", "for", "r", "in", "range", "(", "2", ")", ":", "\n", "            ", "getattr", "(", "block", ".", "mlp", ",", "f'fc{r + 1}'", ")", ".", "weight", ".", "copy_", "(", "_n2p", "(", "w", "[", "f'{block_prefix}MlpBlock_3/Dense_{r}/kernel'", "]", ")", ")", "\n", "getattr", "(", "block", ".", "mlp", ",", "f'fc{r + 1}'", ")", ".", "bias", ".", "copy_", "(", "_n2p", "(", "w", "[", "f'{block_prefix}MlpBlock_3/Dense_{r}/bias'", "]", ")", ")", "\n", "", "block", ".", "norm2", ".", "weight", ".", "copy_", "(", "_n2p", "(", "w", "[", "f'{block_prefix}LayerNorm_2/scale'", "]", ")", ")", "\n", "block", ".", "norm2", ".", "bias", ".", "copy_", "(", "_n2p", "(", "w", "[", "f'{block_prefix}LayerNorm_2/bias'", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.resize_pos_embed": [[523, 543], ["_logger.info", "int", "_logger.info", "posemb_grid.permute().reshape.reshape().permute", "torch.interpolate", "posemb_grid.permute().reshape.permute().reshape", "torch.cat", "torch.cat", "torch.cat", "math.sqrt", "len", "len", "len", "posemb_grid.permute().reshape.reshape", "posemb_grid.permute().reshape.permute", "int", "math.sqrt"], "function", ["None"], ["", "", "def", "resize_pos_embed", "(", "posemb", ",", "posemb_new", ",", "num_tokens", "=", "1", ",", "gs_new", "=", "(", ")", ")", ":", "\n", "# Rescale the grid of position embeddings when loading from state_dict. Adapted from", "\n", "# https://github.com/google-research/vision_transformer/blob/00883dd691c63a6830751563748663526e811cee/vit_jax/checkpoint.py#L224", "\n", "    ", "_logger", ".", "info", "(", "'Resized position embedding: %s to %s'", ",", "posemb", ".", "shape", ",", "posemb_new", ".", "shape", ")", "\n", "ntok_new", "=", "posemb_new", ".", "shape", "[", "1", "]", "\n", "if", "num_tokens", ":", "\n", "        ", "posemb_tok", ",", "posemb_grid", "=", "posemb", "[", ":", ",", ":", "num_tokens", "]", ",", "posemb", "[", "0", ",", "num_tokens", ":", "]", "\n", "ntok_new", "-=", "num_tokens", "\n", "", "else", ":", "\n", "        ", "posemb_tok", ",", "posemb_grid", "=", "posemb", "[", ":", ",", ":", "0", "]", ",", "posemb", "[", "0", "]", "\n", "", "gs_old", "=", "int", "(", "math", ".", "sqrt", "(", "len", "(", "posemb_grid", ")", ")", ")", "\n", "if", "not", "len", "(", "gs_new", ")", ":", "# backwards compatibility", "\n", "        ", "gs_new", "=", "[", "int", "(", "math", ".", "sqrt", "(", "ntok_new", ")", ")", "]", "*", "2", "\n", "", "assert", "len", "(", "gs_new", ")", ">=", "2", "\n", "_logger", ".", "info", "(", "'Position embedding grid-size from %s to %s'", ",", "[", "gs_old", ",", "gs_old", "]", ",", "gs_new", ")", "\n", "posemb_grid", "=", "posemb_grid", ".", "reshape", "(", "1", ",", "gs_old", ",", "gs_old", ",", "-", "1", ")", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", "\n", "posemb_grid", "=", "F", ".", "interpolate", "(", "posemb_grid", ",", "size", "=", "gs_new", ",", "mode", "=", "'bicubic'", ",", "align_corners", "=", "False", ")", "\n", "posemb_grid", "=", "posemb_grid", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", ".", "reshape", "(", "1", ",", "gs_new", "[", "0", "]", "*", "gs_new", "[", "1", "]", ",", "-", "1", ")", "\n", "posemb", "=", "torch", ".", "cat", "(", "[", "posemb_tok", ",", "posemb_grid", "]", ",", "dim", "=", "1", ")", "\n", "return", "posemb", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.checkpoint_filter_fn": [[545, 562], ["state_dict.items", "resize_pos_embed.reshape", "len", "vision_transformer.resize_pos_embed", "getattr"], "function", ["home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.resize_pos_embed"], ["", "def", "checkpoint_filter_fn", "(", "state_dict", ",", "model", ")", ":", "\n", "    ", "\"\"\" convert patch embedding weight from manual patchify + linear proj to conv\"\"\"", "\n", "out_dict", "=", "{", "}", "\n", "if", "'model'", "in", "state_dict", ":", "\n", "# For deit models", "\n", "        ", "state_dict", "=", "state_dict", "[", "'model'", "]", "\n", "", "for", "k", ",", "v", "in", "state_dict", ".", "items", "(", ")", ":", "\n", "        ", "if", "'patch_embed.proj.weight'", "in", "k", "and", "len", "(", "v", ".", "shape", ")", "<", "4", ":", "\n", "# For old models that I trained prior to conv based patchification", "\n", "            ", "O", ",", "I", ",", "H", ",", "W", "=", "model", ".", "patch_embed", ".", "proj", ".", "weight", ".", "shape", "\n", "v", "=", "v", ".", "reshape", "(", "O", ",", "-", "1", ",", "H", ",", "W", ")", "\n", "", "elif", "k", "==", "'pos_embed'", "and", "v", ".", "shape", "!=", "model", ".", "pos_embed", ".", "shape", ":", "\n", "# To resize pos embedding when using model at different size from pretrained weights", "\n", "            ", "v", "=", "resize_pos_embed", "(", "\n", "v", ",", "model", ".", "pos_embed", ",", "getattr", "(", "model", ",", "'num_tokens'", ",", "1", ")", ",", "model", ".", "patch_embed", ".", "grid_size", ")", "\n", "", "out_dict", "[", "k", "]", "=", "v", "\n", "", "return", "out_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer._create_vision_transformer": [[564, 587], ["kwargs.get", "kwargs.get", "kwargs.pop", "timm.models.helpers.build_model_with_cfg", "RuntimeError", "_logger.warning"], "function", ["None"], ["", "def", "_create_vision_transformer", "(", "variant", ",", "pretrained", "=", "False", ",", "default_cfg", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "    ", "default_cfg", "=", "default_cfg", "or", "default_cfgs", "[", "variant", "]", "\n", "if", "kwargs", ".", "get", "(", "'features_only'", ",", "None", ")", ":", "\n", "        ", "raise", "RuntimeError", "(", "'features_only not implemented for Vision Transformer models.'", ")", "\n", "\n", "# NOTE this extra code to support handling of repr size for in21k pretrained models", "\n", "", "default_num_classes", "=", "default_cfg", "[", "'num_classes'", "]", "\n", "num_classes", "=", "kwargs", ".", "get", "(", "'num_classes'", ",", "default_num_classes", ")", "\n", "repr_size", "=", "kwargs", ".", "pop", "(", "'representation_size'", ",", "None", ")", "\n", "if", "repr_size", "is", "not", "None", "and", "num_classes", "!=", "default_num_classes", ":", "\n", "# Remove representation layer if fine-tuning. This may not always be the desired action,", "\n", "# but I feel better than doing nothing by default for fine-tuning. Perhaps a better interface?", "\n", "        ", "_logger", ".", "warning", "(", "\"Removing representation layer for fine-tuning.\"", ")", "\n", "repr_size", "=", "None", "\n", "\n", "", "model", "=", "build_model_with_cfg", "(", "\n", "VisionTransformer", ",", "variant", ",", "pretrained", ",", "\n", "default_cfg", "=", "default_cfg", ",", "\n", "representation_size", "=", "repr_size", ",", "\n", "pretrained_filter_fn", "=", "checkpoint_filter_fn", ",", "\n", "pretrained_custom_load", "=", "'npz'", "in", "default_cfg", "[", "'url'", "]", ",", "\n", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.deit_small_patch16_224_return_attn": [[590, 600], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "deit_small_patch16_224_return_attn", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "\n", "    ", "\"\"\" an extra output for the class attention\n    DeiT-small model @ 224x224 from paper (https://arxiv.org/abs/2012.12877).\n    ImageNet-1k weights from https://github.com/facebookresearch/deit.\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "384", ",", "depth", "=", "12", ",", "num_heads", "=", "6", ",", "return_attn", "=", "True", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'deit_small_patch16_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.vit_tiny_patch16_224": [[601, 608], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_tiny_patch16_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Tiny (Vit-Ti/16)\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "192", ",", "depth", "=", "12", ",", "num_heads", "=", "3", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_tiny_patch16_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.vit_tiny_patch16_384": [[610, 617], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_tiny_patch16_384", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Tiny (Vit-Ti/16) @ 384x384.\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "192", ",", "depth", "=", "12", ",", "num_heads", "=", "3", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_tiny_patch16_384'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.vit_small_patch32_224": [[619, 626], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_small_patch32_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Small (ViT-S/32)\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "32", ",", "embed_dim", "=", "384", ",", "depth", "=", "12", ",", "num_heads", "=", "6", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_small_patch32_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.vit_small_patch32_384": [[628, 635], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_small_patch32_384", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Small (ViT-S/32) at 384x384.\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "32", ",", "embed_dim", "=", "384", ",", "depth", "=", "12", ",", "num_heads", "=", "6", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_small_patch32_384'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.vit_small_patch16_224": [[637, 645], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_small_patch16_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Small (ViT-S/16)\n    NOTE I've replaced my previous 'small' model definition and weights with the small variant from the DeiT paper\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "384", ",", "depth", "=", "12", ",", "num_heads", "=", "6", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_small_patch16_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.vit_small_patch16_384": [[647, 655], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_small_patch16_384", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Small (ViT-S/16)\n    NOTE I've replaced my previous 'small' model definition and weights with the small variant from the DeiT paper\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "384", ",", "depth", "=", "12", ",", "num_heads", "=", "6", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_small_patch16_384'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.vit_base_patch32_224": [[657, 665], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_base_patch32_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Base (ViT-B/32) from original paper (https://arxiv.org/abs/2010.11929).\n    ImageNet-1k weights fine-tuned from in21k, source https://github.com/google-research/vision_transformer.\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "32", ",", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_base_patch32_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.vit_base_patch32_384": [[667, 675], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_base_patch32_384", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Base model (ViT-B/32) from original paper (https://arxiv.org/abs/2010.11929).\n    ImageNet-1k weights fine-tuned from in21k @ 384x384, source https://github.com/google-research/vision_transformer.\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "32", ",", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_base_patch32_384'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.vit_base_patch16_224": [[677, 685], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_base_patch16_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Base (ViT-B/16) from original paper (https://arxiv.org/abs/2010.11929).\n    ImageNet-1k weights fine-tuned from in21k @ 224x224, source https://github.com/google-research/vision_transformer.\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_base_patch16_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.vit_base_patch16_384": [[687, 695], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_base_patch16_384", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Base model (ViT-B/16) from original paper (https://arxiv.org/abs/2010.11929).\n    ImageNet-1k weights fine-tuned from in21k @ 384x384, source https://github.com/google-research/vision_transformer.\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_base_patch16_384'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.vit_base_patch8_224": [[697, 705], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_base_patch8_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Base (ViT-B/8) from original paper (https://arxiv.org/abs/2010.11929).\n    ImageNet-1k weights fine-tuned from in21k @ 224x224, source https://github.com/google-research/vision_transformer.\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "8", ",", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_base_patch8_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.vit_large_patch32_224": [[707, 714], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_large_patch32_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Large model (ViT-L/32) from original paper (https://arxiv.org/abs/2010.11929). No pretrained weights.\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "32", ",", "embed_dim", "=", "1024", ",", "depth", "=", "24", ",", "num_heads", "=", "16", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_large_patch32_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.vit_large_patch32_384": [[716, 724], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_large_patch32_384", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Large model (ViT-L/32) from original paper (https://arxiv.org/abs/2010.11929).\n    ImageNet-1k weights fine-tuned from in21k @ 384x384, source https://github.com/google-research/vision_transformer.\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "32", ",", "embed_dim", "=", "1024", ",", "depth", "=", "24", ",", "num_heads", "=", "16", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_large_patch32_384'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.vit_large_patch16_224": [[726, 734], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_large_patch16_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Large model (ViT-L/32) from original paper (https://arxiv.org/abs/2010.11929).\n    ImageNet-1k weights fine-tuned from in21k @ 224x224, source https://github.com/google-research/vision_transformer.\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "1024", ",", "depth", "=", "24", ",", "num_heads", "=", "16", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_large_patch16_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.vit_large_patch16_384": [[736, 744], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_large_patch16_384", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Large model (ViT-L/16) from original paper (https://arxiv.org/abs/2010.11929).\n    ImageNet-1k weights fine-tuned from in21k @ 384x384, source https://github.com/google-research/vision_transformer.\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "1024", ",", "depth", "=", "24", ",", "num_heads", "=", "16", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_large_patch16_384'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.vit_base_patch16_sam_224": [[746, 754], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_base_patch16_sam_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Base (ViT-B/16) w/ SAM pretrained weights. Paper: https://arxiv.org/abs/2106.01548\n    \"\"\"", "\n", "# NOTE original SAM weights release worked with representation_size=768", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "representation_size", "=", "0", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_base_patch16_sam_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.vit_base_patch32_sam_224": [[756, 764], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_base_patch32_sam_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Base (ViT-B/32) w/ SAM pretrained weights. Paper: https://arxiv.org/abs/2106.01548\n    \"\"\"", "\n", "# NOTE original SAM weights release worked with representation_size=768", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "32", ",", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "representation_size", "=", "0", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_base_patch32_sam_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.vit_tiny_patch16_224_in21k": [[766, 775], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_tiny_patch16_224_in21k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Tiny (Vit-Ti/16).\n    ImageNet-21k weights @ 224x224, source https://github.com/google-research/vision_transformer.\n    NOTE: this model has valid 21k classifier head and no representation (pre-logits) layer\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "192", ",", "depth", "=", "12", ",", "num_heads", "=", "3", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_tiny_patch16_224_in21k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.vit_small_patch32_224_in21k": [[777, 786], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_small_patch32_224_in21k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Small (ViT-S/16)\n    ImageNet-21k weights @ 224x224, source https://github.com/google-research/vision_transformer.\n    NOTE: this model has valid 21k classifier head and no representation (pre-logits) layer\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "32", ",", "embed_dim", "=", "384", ",", "depth", "=", "12", ",", "num_heads", "=", "6", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_small_patch32_224_in21k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.vit_small_patch16_224_in21k": [[788, 797], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_small_patch16_224_in21k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Small (ViT-S/16)\n    ImageNet-21k weights @ 224x224, source https://github.com/google-research/vision_transformer.\n    NOTE: this model has valid 21k classifier head and no representation (pre-logits) layer\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "384", ",", "depth", "=", "12", ",", "num_heads", "=", "6", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_small_patch16_224_in21k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.vit_base_patch32_224_in21k": [[799, 809], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_base_patch32_224_in21k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Base model (ViT-B/32) from original paper (https://arxiv.org/abs/2010.11929).\n    ImageNet-21k weights @ 224x224, source https://github.com/google-research/vision_transformer.\n    NOTE: this model has valid 21k classifier head and no representation (pre-logits) layer\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "32", ",", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_base_patch32_224_in21k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.vit_base_patch16_224_in21k": [[811, 821], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_base_patch16_224_in21k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Base model (ViT-B/16) from original paper (https://arxiv.org/abs/2010.11929).\n    ImageNet-21k weights @ 224x224, source https://github.com/google-research/vision_transformer.\n    NOTE: this model has valid 21k classifier head and no representation (pre-logits) layer\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_base_patch16_224_in21k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.vit_base_patch8_224_in21k": [[823, 833], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_base_patch8_224_in21k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Base model (ViT-B/8) from original paper (https://arxiv.org/abs/2010.11929).\n    ImageNet-21k weights @ 224x224, source https://github.com/google-research/vision_transformer.\n    NOTE: this model has valid 21k classifier head and no representation (pre-logits) layer\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "8", ",", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_base_patch8_224_in21k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.vit_large_patch32_224_in21k": [[835, 845], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_large_patch32_224_in21k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Large model (ViT-L/32) from original paper (https://arxiv.org/abs/2010.11929).\n    ImageNet-21k weights @ 224x224, source https://github.com/google-research/vision_transformer.\n    NOTE: this model has a representation layer but the 21k classifier head is zero'd out in original weights\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "32", ",", "embed_dim", "=", "1024", ",", "depth", "=", "24", ",", "num_heads", "=", "16", ",", "representation_size", "=", "1024", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_large_patch32_224_in21k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.vit_large_patch16_224_in21k": [[847, 857], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_large_patch16_224_in21k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Large model (ViT-L/16) from original paper (https://arxiv.org/abs/2010.11929).\n    ImageNet-21k weights @ 224x224, source https://github.com/google-research/vision_transformer.\n    NOTE: this model has valid 21k classifier head and no representation (pre-logits) layer\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "16", ",", "embed_dim", "=", "1024", ",", "depth", "=", "24", ",", "num_heads", "=", "16", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_large_patch16_224_in21k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.vit_huge_patch14_224_in21k": [[859, 869], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_huge_patch14_224_in21k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Huge model (ViT-H/14) from original paper (https://arxiv.org/abs/2010.11929).\n    ImageNet-21k weights @ 224x224, source https://github.com/google-research/vision_transformer.\n    NOTE: this model has a representation layer but the 21k classifier head is zero'd out in original weights\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "\n", "patch_size", "=", "14", ",", "embed_dim", "=", "1280", ",", "depth", "=", "32", ",", "num_heads", "=", "16", ",", "representation_size", "=", "1280", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_huge_patch14_224_in21k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.deit_tiny_patch16_224": [[871, 879], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "deit_tiny_patch16_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" DeiT-tiny model @ 224x224 from paper (https://arxiv.org/abs/2012.12877).\n    ImageNet-1k weights from https://github.com/facebookresearch/deit.\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "192", ",", "depth", "=", "12", ",", "num_heads", "=", "3", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'deit_tiny_patch16_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.deit_small_patch16_224": [[881, 889], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "deit_small_patch16_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" DeiT-small model @ 224x224 from paper (https://arxiv.org/abs/2012.12877).\n    ImageNet-1k weights from https://github.com/facebookresearch/deit.\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "384", ",", "depth", "=", "12", ",", "num_heads", "=", "6", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'deit_small_patch16_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.deit_base_patch16_224": [[891, 899], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "deit_base_patch16_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" DeiT base model @ 224x224 from paper (https://arxiv.org/abs/2012.12877).\n    ImageNet-1k weights from https://github.com/facebookresearch/deit.\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'deit_base_patch16_224'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.deit_base_patch16_384": [[901, 909], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "deit_base_patch16_384", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" DeiT base model @ 384x384 from paper (https://arxiv.org/abs/2012.12877).\n    ImageNet-1k weights from https://github.com/facebookresearch/deit.\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'deit_base_patch16_384'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.deit_tiny_distilled_patch16_224": [[911, 920], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "deit_tiny_distilled_patch16_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" DeiT-tiny distilled model @ 224x224 from paper (https://arxiv.org/abs/2012.12877).\n    ImageNet-1k weights from https://github.com/facebookresearch/deit.\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "192", ",", "depth", "=", "12", ",", "num_heads", "=", "3", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "\n", "'deit_tiny_distilled_patch16_224'", ",", "pretrained", "=", "pretrained", ",", "distilled", "=", "True", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.deit_small_distilled_patch16_224": [[922, 931], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "deit_small_distilled_patch16_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" DeiT-small distilled model @ 224x224 from paper (https://arxiv.org/abs/2012.12877).\n    ImageNet-1k weights from https://github.com/facebookresearch/deit.\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "384", ",", "depth", "=", "12", ",", "num_heads", "=", "6", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "\n", "'deit_small_distilled_patch16_224'", ",", "pretrained", "=", "pretrained", ",", "distilled", "=", "True", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.deit_base_distilled_patch16_224": [[933, 942], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "deit_base_distilled_patch16_224", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" DeiT-base distilled model @ 224x224 from paper (https://arxiv.org/abs/2012.12877).\n    ImageNet-1k weights from https://github.com/facebookresearch/deit.\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "\n", "'deit_base_distilled_patch16_224'", ",", "pretrained", "=", "pretrained", ",", "distilled", "=", "True", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.deit_base_distilled_patch16_384": [[944, 953], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "deit_base_distilled_patch16_384", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" DeiT-base distilled model @ 384x384 from paper (https://arxiv.org/abs/2012.12877).\n    ImageNet-1k weights from https://github.com/facebookresearch/deit.\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "\n", "'deit_base_distilled_patch16_384'", ",", "pretrained", "=", "pretrained", ",", "distilled", "=", "True", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.vit_base_patch16_224_miil_in21k": [[955, 963], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_base_patch16_224_miil_in21k", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Base (ViT-B/16) from original paper (https://arxiv.org/abs/2010.11929).\n    Weights taken from: https://github.com/Alibaba-MIIL/ImageNet21K\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "qkv_bias", "=", "False", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_base_patch16_224_miil_in21k'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer.vit_base_patch16_224_miil": [[965, 973], ["dict", "vision_transformer._create_vision_transformer"], "function", ["home.repos.pwc.inspect_result.beckschen_transmix.models.vision_transformer._create_vision_transformer"], ["", "@", "register_model", "\n", "def", "vit_base_patch16_224_miil", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\" ViT-Base (ViT-B/16) from original paper (https://arxiv.org/abs/2010.11929).\n    Weights taken from: https://github.com/Alibaba-MIIL/ImageNet21K\n    \"\"\"", "\n", "model_kwargs", "=", "dict", "(", "patch_size", "=", "16", ",", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "qkv_bias", "=", "False", ",", "**", "kwargs", ")", "\n", "model", "=", "_create_vision_transformer", "(", "'vit_base_patch16_224_miil'", ",", "pretrained", "=", "pretrained", ",", "**", "model_kwargs", ")", "\n", "return", "model", "\n", "", ""]]}