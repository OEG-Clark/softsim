{"home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.DQN.DQN.__init__": [[88, 163], ["random.seed", "DQN.DQN.readParameters", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "replayMemory.ReplayMemory", "DQN.DQN.targetNetwork.load_state_dict", "torch.Adam", "torch.Adam", "torch.Adam", "Models.DNN_Atari.DNN_Atari().to", "Models.DNN_Atari.DNN_Atari().to", "DQN.DQN.policyNetwork.state_dict", "DQN.DQN.policyNetwork.parameters", "DQN.DQN.initReporting", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "Models.DNN_MinAtar.DNN_MinAtar().to", "Models.DNN_MinAtar.DNN_MinAtar().to", "Models.FeedforwardDNN.FeedForwardDNN().to", "Models.FeedforwardDNN.FeedForwardDNN().to", "str", "str", "Models.DNN_Atari.DNN_Atari", "Models.DNN_Atari.DNN_Atari", "math.exp", "Models.DNN_MinAtar.DNN_MinAtar", "Models.DNN_MinAtar.DNN_MinAtar", "Models.FeedforwardDNN.FeedForwardDNN", "Models.FeedforwardDNN.FeedForwardDNN"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.DQN.DQN.readParameters", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.DQN.DQN.initReporting"], ["def", "__init__", "(", "self", ",", "observationSpace", ",", "actionSpace", ",", "environment", ",", "\n", "parametersFileName", "=", "''", ",", "reporting", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Initializing the RL agent based on the DQN Deep Reinforcement Learning\n              algorithm, by setting up the algorithm parameters as well as \n              the Deep Neural Networks.\n        \n        INPUTS: - observationSpace: RL observation space.\n                - actionSpace: RL action space.\n                - environment: Name of the RL environment.\n                - parametersFileName: Name of the JSON parameters file.\n                - reporting: Enable the reporting of the results.\n        \n        OUTPUTS: /\n        \"\"\"", "\n", "\n", "# Initialize the random function with a fixed random seed", "\n", "random", ".", "seed", "(", "0", ")", "\n", "\n", "# Setting of the parameters", "\n", "if", "parametersFileName", "==", "''", ":", "\n", "            ", "parametersFileName", "=", "''", ".", "join", "(", "[", "'Parameters/parameters_DQN_'", ",", "str", "(", "environment", ")", ",", "'.json'", "]", ")", "\n", "", "parameters", "=", "self", ".", "readParameters", "(", "parametersFileName", ")", "\n", "\n", "# Set the device for DNN computations (CPU or GPU)", "\n", "self", ".", "device", "=", "torch", ".", "device", "(", "'cuda:'", "+", "str", "(", "parameters", "[", "'GPUNumber'", "]", ")", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "'cpu'", ")", "\n", "\n", "# Set the general parameters of the DQN algorithm", "\n", "self", ".", "gamma", "=", "parameters", "[", "'gamma'", "]", "\n", "self", ".", "learningRate", "=", "parameters", "[", "'learningRate'", "]", "\n", "self", ".", "epsilon", "=", "parameters", "[", "'epsilon'", "]", "\n", "self", ".", "targetUpdatePeriod", "=", "parameters", "[", "'targetUpdatePeriod'", "]", "\n", "self", ".", "learningUpdatePeriod", "=", "parameters", "[", "'learningUpdatePeriod'", "]", "\n", "self", ".", "rewardClipping", "=", "parameters", "[", "'rewardClipping'", "]", "\n", "self", ".", "gradientClipping", "=", "parameters", "[", "'gradientClipping'", "]", "\n", "\n", "# Set the Experience Replay mechanism", "\n", "self", ".", "batchSize", "=", "parameters", "[", "'batchSize'", "]", "\n", "self", ".", "capacity", "=", "parameters", "[", "'capacity'", "]", "\n", "self", ".", "replayMemory", "=", "ReplayMemory", "(", "self", ".", "capacity", ")", "\n", "\n", "# Set both the observation and action spaces", "\n", "self", ".", "observationSpace", "=", "observationSpace", "\n", "self", ".", "actionSpace", "=", "actionSpace", "\n", "\n", "# Set the two Deep Neural Networks of the DQN algorithm (policy and target)", "\n", "self", ".", "atari", "=", "parameters", "[", "'atari'", "]", "\n", "self", ".", "minatar", "=", "parameters", "[", "'minatar'", "]", "\n", "if", "self", ".", "atari", ":", "\n", "            ", "self", ".", "policyNetwork", "=", "DNN_Atari", "(", "observationSpace", ",", "actionSpace", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "targetNetwork", "=", "DNN_Atari", "(", "observationSpace", ",", "actionSpace", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "", "elif", "self", ".", "minatar", ":", "\n", "            ", "self", ".", "policyNetwork", "=", "DNN_MinAtar", "(", "observationSpace", ",", "actionSpace", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "targetNetwork", "=", "DNN_MinAtar", "(", "observationSpace", ",", "actionSpace", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "policyNetwork", "=", "FeedForwardDNN", "(", "observationSpace", ",", "actionSpace", ",", "parameters", "[", "'structureDNN'", "]", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "targetNetwork", "=", "FeedForwardDNN", "(", "observationSpace", ",", "actionSpace", ",", "parameters", "[", "'structureDNN'", "]", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "", "self", ".", "targetNetwork", ".", "load_state_dict", "(", "self", ".", "policyNetwork", ".", "state_dict", "(", ")", ")", "\n", "\n", "# Set the Deep Learning optimizer", "\n", "self", ".", "optimizer", "=", "optim", ".", "Adam", "(", "self", ".", "policyNetwork", ".", "parameters", "(", ")", ",", "lr", "=", "self", ".", "learningRate", ",", "eps", "=", "self", ".", "epsilon", ")", "\n", "\n", "# Set the Epsilon-Greedy exploration technique", "\n", "self", ".", "epsilonStart", "=", "parameters", "[", "'epsilonStart'", "]", "\n", "self", ".", "epsilonEnd", "=", "parameters", "[", "'epsilonEnd'", "]", "\n", "self", ".", "epsilonDecay", "=", "parameters", "[", "'epsilonDecay'", "]", "\n", "self", ".", "epsilonTest", "=", "parameters", "[", "'epsilonTest'", "]", "\n", "self", ".", "epsilonValue", "=", "lambda", "iteration", ":", "self", ".", "epsilonEnd", "+", "(", "self", ".", "epsilonStart", "-", "self", ".", "epsilonEnd", ")", "*", "math", ".", "exp", "(", "-", "1", "*", "iteration", "/", "self", ".", "epsilonDecay", ")", "\n", "\n", "# Initialization of the counter for the number of steps", "\n", "self", ".", "steps", "=", "0", "\n", "\n", "# Initialization of the experiment folder and tensorboard writer", "\n", "if", "reporting", ":", "\n", "            ", "self", ".", "initReporting", "(", "parameters", ",", "'DQN'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.DQN.DQN.readParameters": [[165, 178], ["open", "json.load"], "methods", ["None"], ["", "", "def", "readParameters", "(", "self", ",", "fileName", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Read the appropriate JSON file to load the parameters.\n        \n        INPUTS: - fileName: Name of the JSON file to read.\n        \n        OUTPUTS: - parametersDict: Dictionary containing the parameters.\n        \"\"\"", "\n", "\n", "# Reading of the parameters file, and conversion to Python disctionary", "\n", "with", "open", "(", "fileName", ")", "as", "parametersFile", ":", "\n", "            ", "parametersDict", "=", "json", ".", "load", "(", "parametersFile", ")", "\n", "", "return", "parametersDict", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.DQN.DQN.initReporting": [[180, 202], ["datetime.datetime.now().strftime", "os.mkdir", "torch.utils.tensorboard.SummaryWriter", "torch.utils.tensorboard.SummaryWriter", "torch.utils.tensorboard.SummaryWriter", "open", "json.dump", "datetime.datetime.now"], "methods", ["None"], ["", "def", "initReporting", "(", "self", ",", "parameters", ",", "algorithm", "=", "'DQN'", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Initialize both the experiment folder and the tensorboard\n              writer for reporting (and storing) the results.\n        \n        INPUTS: - parameters: Parameters to ne stored in the experiment folder.\n                - algorithm: Name of the RL algorithm.\n        \n        OUTPUTS: /\n        \"\"\"", "\n", "\n", "while", "True", ":", "\n", "            ", "try", ":", "\n", "                ", "time", "=", "datetime", ".", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "\"%d_%m_%Y-%H:%M:%S\"", ")", "\n", "self", ".", "experimentFolder", "=", "''", ".", "join", "(", "[", "'Experiments/'", ",", "algorithm", ",", "'_'", ",", "time", ",", "'/'", "]", ")", "\n", "os", ".", "mkdir", "(", "self", ".", "experimentFolder", ")", "\n", "with", "open", "(", "''", ".", "join", "(", "[", "self", ".", "experimentFolder", ",", "'Parameters.json'", "]", ")", ",", "\"w\"", ")", "as", "f", ":", "\n", "                    ", "json", ".", "dump", "(", "parameters", ",", "f", ",", "indent", "=", "4", ")", "\n", "", "self", ".", "writer", "=", "SummaryWriter", "(", "''", ".", "join", "(", "[", "'Tensorboard/'", ",", "algorithm", ",", "'_'", ",", "time", "]", ")", ")", "\n", "break", "\n", "", "except", ":", "\n", "                ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.DQN.DQN.processState": [[204, 214], ["None"], "methods", ["None"], ["", "", "", "def", "processState", "(", "self", ",", "state", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Potentially process the RL state returned by the environment.\n        \n        INPUTS: - state: RL state returned by the environment.\n        \n        OUTPUTS: - state: RL state processed.\n        \"\"\"", "\n", "\n", "return", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.DQN.DQN.processReward": [[216, 226], ["numpy.clip"], "methods", ["None"], ["", "def", "processReward", "(", "self", ",", "reward", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Potentially process the RL reward returned by the environment.\n        \n        INPUTS: - reward: RL reward returned by the environment.\n        \n        OUTPUTS: - reward: RL reward processed.\n        \"\"\"", "\n", "\n", "return", "np", ".", "clip", "(", "reward", ",", "-", "self", ".", "rewardClipping", ",", "self", ".", "rewardClipping", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.DQN.DQN.updateTargetNetwork": [[228, 243], ["DQN.DQN.targetNetwork.load_state_dict", "DQN.DQN.policyNetwork.state_dict"], "methods", ["None"], ["", "def", "updateTargetNetwork", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Taking into account the update frequency (parameter), update the\n              target Deep Neural Network by copying the policy Deep Neural Network\n              parameters (weights, bias, etc.).\n        \n        INPUTS: /\n        \n        OUTPUTS: /\n        \"\"\"", "\n", "\n", "# Check if an update is required (update frequency)", "\n", "if", "(", "self", ".", "steps", "%", "self", ".", "targetUpdatePeriod", "==", "0", ")", ":", "\n", "# Transfer the DNN parameters (policy network -> target network)", "\n", "            ", "self", ".", "targetNetwork", ".", "load_state_dict", "(", "self", ".", "policyNetwork", ".", "state_dict", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.DQN.DQN.chooseAction": [[245, 276], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.from_numpy().float().to().unsqueeze", "torch.from_numpy().float().to().unsqueeze", "torch.from_numpy().float().to().unsqueeze", "torch.from_numpy().float().to().unsqueeze", "torch.from_numpy().float().to().unsqueeze", "torch.from_numpy().float().to().unsqueeze", "torch.from_numpy().float().to().unsqueeze", "torch.from_numpy().float().to().unsqueeze", "torch.from_numpy().float().to().unsqueeze", "DQN.DQN.policyNetwork().squeeze", "QValues.cpu().numpy.cpu().numpy.max", "action.item", "matplotlib.pyplot.figure", "matplotlib.pyplot.figure.add_subplot", "QValues.cpu().numpy.cpu().numpy.cpu().numpy", "range", "plt.figure.add_subplot.set_xlabel", "plt.figure.add_subplot.set_ylabel", "plt.figure.add_subplot.legend", "matplotlib.pyplot.show", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "DQN.DQN.policyNetwork", "plt.figure.add_subplot.axvline", "QValues.cpu().numpy.cpu().numpy.cpu", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "str"], "methods", ["None"], ["", "", "def", "chooseAction", "(", "self", ",", "state", ",", "plot", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Choose a valid RL action from the action space according to the\n              RL policy as well as the current RL state observed.\n        \n        INPUTS: - state: RL state returned by the environment.\n                - plot: Enable the plotting of information about the decision.\n        \n        OUTPUTS: - action: RL action chosen from the action space.\n        \"\"\"", "\n", "\n", "# Choose the best action based on the RL policy", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "state", "=", "torch", ".", "from_numpy", "(", "state", ")", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", ".", "unsqueeze", "(", "0", ")", "\n", "QValues", "=", "self", ".", "policyNetwork", "(", "state", ")", ".", "squeeze", "(", "0", ")", "\n", "_", ",", "action", "=", "QValues", ".", "max", "(", "0", ")", "\n", "\n", "# If required, plot the expected return Q associated with each action", "\n", "if", "plot", ":", "\n", "                ", "colors", "=", "[", "'blue'", ",", "'red'", ",", "'orange'", ",", "'green'", ",", "'purple'", ",", "'brown'", "]", "\n", "fig", "=", "plt", ".", "figure", "(", ")", "\n", "ax", "=", "fig", ".", "add_subplot", "(", ")", "\n", "QValues", "=", "QValues", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "for", "a", "in", "range", "(", "self", ".", "actionSpace", ")", ":", "\n", "                    ", "ax", ".", "axvline", "(", "x", "=", "QValues", "[", "a", "]", ",", "linewidth", "=", "5", ",", "label", "=", "''", ".", "join", "(", "[", "'Action '", ",", "str", "(", "a", ")", ",", "' expected return Q'", "]", ")", ",", "color", "=", "colors", "[", "a", "]", ")", "\n", "", "ax", ".", "set_xlabel", "(", "'Expected return Q'", ")", "\n", "ax", ".", "set_ylabel", "(", "''", ")", "\n", "ax", ".", "legend", "(", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n", "", "return", "action", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.DQN.DQN.chooseActionEpsilonGreedy": [[278, 298], ["random.random", "DQN.DQN.chooseAction", "random.randrange"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorldOptimal.StochasticGridWorldOptimal.chooseAction"], ["", "", "def", "chooseActionEpsilonGreedy", "(", "self", ",", "state", ",", "epsilon", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Choose a valid RL action from the action space according to the\n              RL policy as well as the current RL state observed, following the \n              Epsilon Greedy exploration mechanism.\n        \n        INPUTS: - state: RL state returned by the environment.\n                - epsilon: Epsilon value from Epsilon Greedy technique.\n        \n        OUTPUTS: - action: RL action chosen from the action space.\n        \"\"\"", "\n", "\n", "# EXPLOITATION -> RL policy", "\n", "if", "(", "random", ".", "random", "(", ")", ">", "epsilon", ")", ":", "\n", "            ", "action", "=", "self", ".", "chooseAction", "(", "state", ")", "\n", "# EXPLORATION -> Random", "\n", "", "else", ":", "\n", "            ", "action", "=", "random", ".", "randrange", "(", "self", ".", "actionSpace", ")", "\n", "\n", "", "return", "action", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.DQN.DQN.fillReplayMemory": [[300, 333], ["DQN.DQN.replayMemory.__len__", "DQN.DQN.processState", "trainingEnv.reset", "random.randrange", "trainingEnv.step", "DQN.DQN.processReward", "DQN.DQN.processState", "DQN.DQN.replayMemory.push"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.AtariWrapper.LazyFrames.__len__", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorldOptimal.StochasticGridWorldOptimal.processState", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorld.StochasticGridWorld.reset", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorld.StochasticGridWorld.step", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.DQN.DQN.processReward", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorldOptimal.StochasticGridWorldOptimal.processState", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.replayMemory.ReplayMemory.push"], ["", "def", "fillReplayMemory", "(", "self", ",", "trainingEnv", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Fill the experiences replay memory with random experiences before the\n              the training procedure begins.\n        \n        INPUTS: - trainingEnv: Training RL environment.\n                \n        OUTPUTS: /\n        \"\"\"", "\n", "\n", "# Fill the replay memory with random RL experiences", "\n", "while", "self", ".", "replayMemory", ".", "__len__", "(", ")", "<", "self", ".", "capacity", ":", "\n", "\n", "# Set the initial RL variables", "\n", "            ", "state", "=", "self", ".", "processState", "(", "trainingEnv", ".", "reset", "(", ")", ")", "\n", "done", "=", "0", "\n", "\n", "# Interact with the training environment until termination", "\n", "while", "done", "==", "0", ":", "\n", "\n", "# Choose an action according to the RL policy and the current RL state", "\n", "                ", "action", "=", "random", ".", "randrange", "(", "self", ".", "actionSpace", ")", "\n", "\n", "# Interact with the environment with the chosen action", "\n", "nextState", ",", "reward", ",", "done", ",", "info", "=", "trainingEnv", ".", "step", "(", "action", ")", "\n", "\n", "# Process the RL variables retrieved and insert this new experience into the Experience Replay memory", "\n", "reward", "=", "self", ".", "processReward", "(", "reward", ")", "\n", "nextState", "=", "self", ".", "processState", "(", "nextState", ")", "\n", "self", ".", "replayMemory", ".", "push", "(", "state", ",", "action", ",", "reward", ",", "nextState", ",", "done", ")", "\n", "\n", "# Update the RL state", "\n", "state", "=", "nextState", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.DQN.DQN.learning": [[335, 379], ["len", "DQN.DQN.dataLoaderIter.next", "batch[].float().to", "batch[].long().to", "batch[].float().to", "batch[].float().to", "batch[].float().to", "DQN.DQN.policyNetwork().gather().squeeze", "torch.smooth_l1_loss", "torch.smooth_l1_loss", "torch.smooth_l1_loss", "DQN.DQN.optimizer.zero_grad", "torch.smooth_l1_loss.backward", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "DQN.DQN.optimizer.step", "torch.smooth_l1_loss.item", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "DQN.DQN.targetNetwork().gather().squeeze", "DQN.DQN.policyNetwork.parameters", "batch[].float", "batch[].long", "batch[].float", "batch[].float", "batch[].float", "DQN.DQN.policyNetwork().gather", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "batch[].long().to.unsqueeze", "DQN.DQN.policyNetwork", "DQN.DQN.targetNetwork().gather", "DQN.DQN.policyNetwork", "nextActions.unsqueeze", "DQN.DQN.targetNetwork"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorld.StochasticGridWorld.step"], ["", "", "", "def", "learning", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Sample a batch of past experiences and learn from it\n              by updating the Reinforcement Learning policy.\n        \n        INPUTS: /\n        \n        OUTPUTS: - loss: Loss of the learning procedure.\n        \"\"\"", "\n", "\n", "# Check that the replay memory is filled enough", "\n", "if", "(", "len", "(", "self", ".", "replayMemory", ")", ">=", "self", ".", "batchSize", ")", ":", "\n", "\n", "# Sample a batch of experiences from the replay memory", "\n", "            ", "batch", "=", "self", ".", "dataLoaderIter", ".", "next", "(", ")", "\n", "state", "=", "batch", "[", "0", "]", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "action", "=", "batch", "[", "1", "]", ".", "long", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "reward", "=", "batch", "[", "2", "]", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "nextState", "=", "batch", "[", "3", "]", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "done", "=", "batch", "[", "4", "]", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "# Compute the current Q values returned by the policy network", "\n", "currentQValues", "=", "self", ".", "policyNetwork", "(", "state", ")", ".", "gather", "(", "1", ",", "action", ".", "unsqueeze", "(", "1", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "\n", "# Compute the next Q values returned by the target network", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "nextActions", "=", "torch", ".", "max", "(", "self", ".", "policyNetwork", "(", "nextState", ")", ",", "1", ")", "[", "1", "]", "\n", "nextQValues", "=", "self", ".", "targetNetwork", "(", "nextState", ")", ".", "gather", "(", "1", ",", "nextActions", ".", "unsqueeze", "(", "1", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "expectedQValues", "=", "reward", "+", "self", ".", "gamma", "*", "nextQValues", "*", "(", "1", "-", "done", ")", "\n", "\n", "# Compute the loss (typically Huber or MSE loss)", "\n", "", "loss", "=", "F", ".", "smooth_l1_loss", "(", "currentQValues", ",", "expectedQValues", ")", "\n", "\n", "# Computation of the gradients", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "\n", "# Gradient Clipping", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "policyNetwork", ".", "parameters", "(", ")", ",", "self", ".", "gradientClipping", ")", "\n", "\n", "# Perform the Deep Neural Network optimization", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "return", "loss", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.DQN.DQN.training": [[381, 519], ["DQN.DQN.testing", "pandas.DataFrame", "pandas.DataFrame.to_csv", "pandas.DataFrame", "pandas.DataFrame.to_csv", "DQN.DQN.writer.close", "DQN.DQN.fillReplayMemory", "range", "matplotlib.pyplot.figure", "pandas.DataFrame.plot", "matplotlib.pyplot.xlabel", "matplotlib.pyplot.ylabel", "matplotlib.pyplot.savefig", "matplotlib.pyplot.show", "matplotlib.pyplot.figure", "pandas.DataFrame.plot", "matplotlib.pyplot.xlabel", "matplotlib.pyplot.ylabel", "matplotlib.pyplot.savefig", "matplotlib.pyplot.show", "matplotlib.pyplot.figure", "pandas.DataFrame.plot", "matplotlib.pyplot.xlabel", "matplotlib.pyplot.ylabel", "matplotlib.pyplot.savefig", "matplotlib.pyplot.show", "matplotlib.pyplot.figure", "pandas.DataFrame.plot", "matplotlib.pyplot.xlabel", "matplotlib.pyplot.ylabel", "matplotlib.pyplot.savefig", "matplotlib.pyplot.show", "print", "DQN.DQN.processState", "performanceTraining.append", "DQN.DQN.writer.add_scalar", "DQN.DQN.writer.add_scalar", "print", "print", "print", "trainingEnv.reset", "DQN.DQN.chooseActionEpsilonGreedy", "trainingEnv.step", "DQN.DQN.processReward", "DQN.DQN.processState", "DQN.DQN.replayMemory.push", "DQN.DQN.updateTargetNetwork", "DQN.DQN.testing", "performanceTesting.append", "DQN.DQN.writer.add_scalar", "DQN.DQN.writer.add_scalar", "pandas.DataFrame", "pandas.DataFrame.to_csv", "pandas.DataFrame", "pandas.DataFrame.to_csv", "print", "DQN.DQN.epsilonValue", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "iter", "DQN.DQN.learning", "str", "str", "str", "str"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.DQN.DQN.testing", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.DQN.DQN.fillReplayMemory", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorldOptimal.StochasticGridWorldOptimal.processState", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorld.StochasticGridWorld.reset", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.DQN.DQN.chooseActionEpsilonGreedy", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorld.StochasticGridWorld.step", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.DQN.DQN.processReward", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorldOptimal.StochasticGridWorldOptimal.processState", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.replayMemory.ReplayMemory.push", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.DQN.DQN.updateTargetNetwork", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.DQN.DQN.testing", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.FQF.FQF.learning"], ["", "", "def", "training", "(", "self", ",", "trainingEnv", ",", "numberOfEpisodes", ",", "verbose", "=", "True", ",", "rendering", "=", "False", ",", "plotTraining", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Train the RL agent by interacting with the RL environment.\n        \n        INPUTS: - trainingEnv: Training RL environment.\n                - numberOfEpisodes: Number of episodes for the training phase.\n                - verbose: Enable the printing of a training feedback.\n                - rendering: Enable the environment rendering.\n                - plotTraining: Enable the plotting of the training results.\n        \n        OUTPUTS: - trainingEnv: Training RL environment.\n        \"\"\"", "\n", "\n", "# Initialization of several variables for storing the training and testing results", "\n", "performanceTraining", "=", "[", "]", "\n", "performanceTesting", "=", "[", "]", "\n", "\n", "try", ":", "\n", "# If required, print the training progression", "\n", "            ", "if", "verbose", ":", "\n", "                ", "print", "(", "\"Training progression (hardware selected => \"", "+", "str", "(", "self", ".", "device", ")", "+", "\"):\"", ")", "\n", "\n", "# Fill the replay memory with a number of random experiences", "\n", "", "self", ".", "fillReplayMemory", "(", "trainingEnv", ")", "\n", "self", ".", "steps", "=", "0", "\n", "\n", "# Training phase for the number of episodes specified as parameter", "\n", "for", "episode", "in", "range", "(", "numberOfEpisodes", ")", ":", "\n", "\n", "# Set the initial RL variables", "\n", "                ", "state", "=", "self", ".", "processState", "(", "trainingEnv", ".", "reset", "(", ")", ")", "\n", "done", "=", "0", "\n", "\n", "# Set the performance tracking veriables", "\n", "totalReward", "=", "0", "\n", "\n", "# Interact with the training environment until termination", "\n", "while", "done", "==", "0", ":", "\n", "\n", "# Choose an action according to the RL policy and the current RL state", "\n", "                    ", "action", "=", "self", ".", "chooseActionEpsilonGreedy", "(", "state", ",", "self", ".", "epsilonValue", "(", "self", ".", "steps", ")", ")", "\n", "\n", "# Interact with the environment with the chosen action", "\n", "nextState", ",", "reward", ",", "done", ",", "info", "=", "trainingEnv", ".", "step", "(", "action", ")", "\n", "\n", "# Process the RL variables retrieved and insert this new experience into the Experience Replay memory", "\n", "reward", "=", "self", ".", "processReward", "(", "reward", ")", "\n", "nextState", "=", "self", ".", "processState", "(", "nextState", ")", "\n", "self", ".", "replayMemory", ".", "push", "(", "state", ",", "action", ",", "reward", ",", "nextState", ",", "done", ")", "\n", "\n", "# Execute the learning procedure of the RL algorithm", "\n", "if", "self", ".", "steps", "%", "self", ".", "learningUpdatePeriod", "==", "0", ":", "\n", "                        ", "self", ".", "dataLoader", "=", "DataLoader", "(", "dataset", "=", "self", ".", "replayMemory", ",", "batch_size", "=", "self", ".", "batchSize", ",", "shuffle", "=", "True", ")", "\n", "self", ".", "dataLoaderIter", "=", "iter", "(", "self", ".", "dataLoader", ")", "\n", "self", ".", "learning", "(", ")", "\n", "\n", "# If required, update the target deep neural network (update frequency)", "\n", "", "self", ".", "updateTargetNetwork", "(", ")", "\n", "\n", "# Update the RL state", "\n", "state", "=", "nextState", "\n", "\n", "# Continuous tracking of the training performance", "\n", "totalReward", "+=", "reward", "\n", "\n", "# Incrementation of the number of iterations (steps)", "\n", "self", ".", "steps", "+=", "1", "\n", "\n", "# Store and report the performance of the RL policy (training)", "\n", "", "performanceTraining", ".", "append", "(", "[", "episode", ",", "self", ".", "steps", ",", "totalReward", "]", ")", "\n", "self", ".", "writer", ".", "add_scalar", "(", "\"Training score (1)\"", ",", "totalReward", ",", "episode", ")", "\n", "self", ".", "writer", ".", "add_scalar", "(", "\"Training score (2)\"", ",", "totalReward", ",", "self", ".", "steps", ")", "\n", "\n", "# Store and report the performance of the RL policy (testing)", "\n", "if", "episode", "%", "4", "==", "0", ":", "\n", "                    ", "_", ",", "testingScore", "=", "self", ".", "testing", "(", "trainingEnv", ",", "False", ",", "False", ")", "\n", "performanceTesting", ".", "append", "(", "[", "episode", ",", "self", ".", "steps", ",", "testingScore", "]", ")", "\n", "self", ".", "writer", ".", "add_scalar", "(", "\"Testing score (1)\"", ",", "testingScore", ",", "episode", ")", "\n", "self", ".", "writer", ".", "add_scalar", "(", "\"Testing score (2)\"", ",", "testingScore", ",", "self", ".", "steps", ")", "\n", "\n", "# Store the training and testing results in a csv file", "\n", "", "if", "episode", "%", "100", "==", "0", ":", "\n", "                    ", "dataframeTraining", "=", "pd", ".", "DataFrame", "(", "performanceTraining", ",", "columns", "=", "[", "'Episode'", ",", "'Steps'", ",", "'Score'", "]", ")", "\n", "dataframeTraining", ".", "to_csv", "(", "''", ".", "join", "(", "[", "self", ".", "experimentFolder", ",", "'TrainingResults.csv'", "]", ")", ")", "\n", "dataframeTesting", "=", "pd", ".", "DataFrame", "(", "performanceTesting", ",", "columns", "=", "[", "'Episode'", ",", "'Steps'", ",", "'Score'", "]", ")", "\n", "dataframeTesting", ".", "to_csv", "(", "''", ".", "join", "(", "[", "self", ".", "experimentFolder", ",", "'TestingResults.csv'", "]", ")", ")", "\n", "\n", "# If required, print a training feedback", "\n", "", "if", "verbose", ":", "\n", "                    ", "print", "(", "\"\"", ".", "join", "(", "[", "\"Episode \"", ",", "str", "(", "episode", "+", "1", ")", ",", "\"/\"", ",", "str", "(", "numberOfEpisodes", ")", ",", "\": training score = \"", ",", "str", "(", "totalReward", ")", "]", ")", ",", "end", "=", "'\\r'", ",", "flush", "=", "True", ")", "\n", "\n", "", "", "", "except", "KeyboardInterrupt", ":", "\n", "            ", "print", "(", ")", "\n", "print", "(", "\"WARNING: Training prematurely interrupted...\"", ")", "\n", "print", "(", ")", "\n", "\n", "# Assess the algorithm performance on the training environment", "\n", "", "trainingEnv", ",", "testingScore", "=", "self", ".", "testing", "(", "trainingEnv", ",", "verbose", ",", "rendering", ")", "\n", "\n", "# Store the training results into a csv file", "\n", "dataframeTraining", "=", "pd", ".", "DataFrame", "(", "performanceTraining", ",", "columns", "=", "[", "'Episode'", ",", "'Steps'", ",", "'Score'", "]", ")", "\n", "dataframeTraining", ".", "to_csv", "(", "''", ".", "join", "(", "[", "self", ".", "experimentFolder", ",", "'TrainingResults.csv'", "]", ")", ")", "\n", "\n", "# Store the testing results into a csv file", "\n", "dataframeTesting", "=", "pd", ".", "DataFrame", "(", "performanceTesting", ",", "columns", "=", "[", "'Episode'", ",", "'Steps'", ",", "'Score'", "]", ")", "\n", "dataframeTesting", ".", "to_csv", "(", "''", ".", "join", "(", "[", "self", ".", "experimentFolder", ",", "'TestingResults.csv'", "]", ")", ")", "\n", "\n", "# If required, plot the training results", "\n", "if", "plotTraining", ":", "\n", "            ", "plt", ".", "figure", "(", ")", "\n", "dataframeTraining", ".", "plot", "(", "x", "=", "'Episode'", ",", "y", "=", "'Score'", ")", "\n", "plt", ".", "xlabel", "(", "'Episode'", ")", "\n", "plt", ".", "ylabel", "(", "'Score'", ")", "\n", "plt", ".", "savefig", "(", "''", ".", "join", "(", "[", "self", ".", "experimentFolder", ",", "'TrainingScore1.png'", "]", ")", ")", "\n", "plt", ".", "show", "(", ")", "\n", "plt", ".", "figure", "(", ")", "\n", "dataframeTraining", ".", "plot", "(", "x", "=", "'Steps'", ",", "y", "=", "'Score'", ")", "\n", "plt", ".", "xlabel", "(", "'Steps'", ")", "\n", "plt", ".", "ylabel", "(", "'Score'", ")", "\n", "plt", ".", "savefig", "(", "''", ".", "join", "(", "[", "self", ".", "experimentFolder", ",", "'TrainingScore2.png'", "]", ")", ")", "\n", "plt", ".", "show", "(", ")", "\n", "plt", ".", "figure", "(", ")", "\n", "dataframeTesting", ".", "plot", "(", "x", "=", "'Episode'", ",", "y", "=", "'Score'", ")", "\n", "plt", ".", "xlabel", "(", "'Episode'", ")", "\n", "plt", ".", "ylabel", "(", "'Score'", ")", "\n", "plt", ".", "savefig", "(", "''", ".", "join", "(", "[", "self", ".", "experimentFolder", ",", "'TestingScore1.png'", "]", ")", ")", "\n", "plt", ".", "show", "(", ")", "\n", "plt", ".", "figure", "(", ")", "\n", "dataframeTesting", ".", "plot", "(", "x", "=", "'Steps'", ",", "y", "=", "'Score'", ")", "\n", "plt", ".", "xlabel", "(", "'Steps'", ")", "\n", "plt", ".", "ylabel", "(", "'Score'", ")", "\n", "plt", ".", "savefig", "(", "''", ".", "join", "(", "[", "self", ".", "experimentFolder", ",", "'TestingScore2.png'", "]", ")", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n", "# Closing of the tensorboard writer", "\n", "", "self", ".", "writer", ".", "close", "(", ")", "\n", "\n", "return", "trainingEnv", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.DQN.DQN.testing": [[521, 566], ["DQN.DQN.processState", "testingEnv.reset", "DQN.DQN.chooseActionEpsilonGreedy", "testingEnv.step", "DQN.DQN.processState", "DQN.DQN.processReward", "print", "testingEnv.render", "DQN.DQN.chooseAction", "str"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorldOptimal.StochasticGridWorldOptimal.processState", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorld.StochasticGridWorld.reset", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.DQN.DQN.chooseActionEpsilonGreedy", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorld.StochasticGridWorld.step", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorldOptimal.StochasticGridWorldOptimal.processState", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.DQN.DQN.processReward", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorld.StochasticGridWorld.render", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorldOptimal.StochasticGridWorldOptimal.chooseAction"], ["", "def", "testing", "(", "self", ",", "testingEnv", ",", "verbose", "=", "True", ",", "rendering", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Test the RL agent trained on the RL environment provided.\n        \n        INPUTS: - testingEnv: Testing RL environment.\n                - verbose: Enable the printing of the testing performance.\n                - rendering: Enable the rendering of the RL environment.\n        \n        OUTPUTS: - testingEnv: Testing RL environment.\n                 - testingScore : Score associated with the testing phase.\n        \"\"\"", "\n", "\n", "# Initialization of some RL variables", "\n", "state", "=", "self", ".", "processState", "(", "testingEnv", ".", "reset", "(", ")", ")", "\n", "done", "=", "0", "\n", "\n", "# Initialization of some variables tracking the RL agent performance", "\n", "testingScore", "=", "0", "\n", "\n", "# Interact with the environment until the episode termination", "\n", "while", "done", "==", "0", ":", "\n", "\n", "# Choose an action according to the RL policy and the current RL state", "\n", "            ", "action", "=", "self", ".", "chooseActionEpsilonGreedy", "(", "state", ",", "self", ".", "epsilonTest", ")", "\n", "\n", "# If required, show the environment rendering", "\n", "if", "rendering", ":", "\n", "                ", "testingEnv", ".", "render", "(", ")", "\n", "self", ".", "chooseAction", "(", "state", ",", "True", ")", "\n", "\n", "# Interact with the environment with the chosen action", "\n", "", "nextState", ",", "reward", ",", "done", ",", "_", "=", "testingEnv", ".", "step", "(", "action", ")", "\n", "\n", "# Process the RL variables retrieved", "\n", "state", "=", "self", ".", "processState", "(", "nextState", ")", "\n", "reward", "=", "self", ".", "processReward", "(", "reward", ")", "\n", "\n", "# Continuous tracking of the training performance", "\n", "testingScore", "+=", "reward", "\n", "\n", "# If required, print the testing performance", "\n", "", "if", "verbose", ":", "\n", "            ", "print", "(", "\"\"", ".", "join", "(", "[", "\"Test environment: score = \"", ",", "str", "(", "testingScore", ")", "]", ")", ")", "\n", "\n", "", "return", "testingEnv", ",", "testingScore", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.DQN.DQN.plotExpectedPerformance": [[568, 725], ["copy.deepcopy", "numpy.zeros", "numpy.zeros", "print", "numpy.transpose", "numpy.transpose", "range", "numpy.transpose", "numpy.transpose", "range", "numpy.array", "numpy.array", "range", "numpy.array", "numpy.array", "pandas.DataFrame", "pandas.DataFrame.to_csv", "pandas.DataFrame", "pandas.DataFrame.to_csv", "matplotlib.pyplot.figure", "matplotlib.pyplot.figure.add_subplot", "plt.figure.add_subplot.plot", "plt.figure.add_subplot.fill_between", "matplotlib.pyplot.savefig", "matplotlib.pyplot.figure", "matplotlib.pyplot.figure.add_subplot", "plt.figure.add_subplot.plot", "plt.figure.add_subplot.fill_between", "matplotlib.pyplot.savefig", "DQN.DQN.writer.close", "DQN.DQN.policyNetwork.state_dict", "range", "pandas.DataFrame().rolling().mean", "movingAverage[].tolist", "pandas.DataFrame().rolling().mean", "movingAverage[].tolist", "expectedPerformance.append", "stdPerformance.append", "expectedPerformance.append", "stdPerformance.append", "range", "range", "str", "print", "DQN.DQN.fillReplayMemory", "tqdm.tqdm.tqdm", "print", "print", "print", "numpy.mean", "numpy.std", "numpy.mean", "numpy.std", "len", "len", "range", "DQN.DQN.processState", "DQN.DQN.writer.add_scalar", "DQN.DQN.writer.add_scalar", "DQN.DQN.testing", "DQN.DQN.writer.add_scalar", "DQN.DQN.writer.add_scalar", "trainingEnv.reset", "DQN.DQN.policyNetwork.load_state_dict", "DQN.DQN.targetNetwork.load_state_dict", "torch.Adam", "torch.Adam", "torch.Adam", "DQN.DQN.replayMemory.reset", "pandas.DataFrame().rolling", "pandas.DataFrame().rolling", "trainingEnv.reset", "DQN.DQN.chooseActionEpsilonGreedy", "trainingEnv.step", "DQN.DQN.processReward", "DQN.DQN.processState", "DQN.DQN.replayMemory.push", "DQN.DQN.updateTargetNetwork", "DQN.DQN.policyNetwork.parameters", "str", "str", "DQN.DQN.epsilonValue", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "iter", "DQN.DQN.learning", "pandas.DataFrame", "pandas.DataFrame"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.DQN.DQN.fillReplayMemory", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorldOptimal.StochasticGridWorldOptimal.processState", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.DQN.DQN.testing", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorld.StochasticGridWorld.reset", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorld.StochasticGridWorld.reset", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorld.StochasticGridWorld.reset", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.DQN.DQN.chooseActionEpsilonGreedy", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorld.StochasticGridWorld.step", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.DQN.DQN.processReward", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorldOptimal.StochasticGridWorldOptimal.processState", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.replayMemory.ReplayMemory.push", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.DQN.DQN.updateTargetNetwork", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.FQF.FQF.learning"], ["", "def", "plotExpectedPerformance", "(", "self", ",", "trainingEnv", ",", "numberOfEpisodes", ",", "iterations", "=", "10", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Plot the expected performance of the DRL algorithm.\n        \n        INPUTS: - trainingEnv: Training RL environment.\n                - numberOfEpisodes: Number of episodes for the training phase.\n                - iterations: Number of training/testing iterations to compute\n                              the expected performance.\n        \n        OUTPUTS: - trainingEnv: Training RL environment.\n        \"\"\"", "\n", "\n", "# Save the initial Deep Neural Network weights", "\n", "initialWeights", "=", "copy", ".", "deepcopy", "(", "self", ".", "policyNetwork", ".", "state_dict", "(", ")", ")", "\n", "\n", "# Initialization of several variables for monitoring the performance of the RL agent", "\n", "performanceTraining", "=", "np", ".", "zeros", "(", "(", "numberOfEpisodes", ",", "iterations", ")", ")", "\n", "performanceTesting", "=", "np", ".", "zeros", "(", "(", "numberOfEpisodes", ",", "iterations", ")", ")", "\n", "\n", "# Print the hardware selected for the training of the Deep Neural Network (either CPU or GPU)", "\n", "print", "(", "\"Hardware selected for training: \"", "+", "str", "(", "self", ".", "device", ")", ")", "\n", "\n", "try", ":", "\n", "\n", "# Apply the training/testing procedure for the number of iterations specified", "\n", "            ", "for", "iteration", "in", "range", "(", "iterations", ")", ":", "\n", "\n", "# Print the progression", "\n", "                ", "print", "(", "''", ".", "join", "(", "[", "\"Expected performance evaluation progression: \"", ",", "str", "(", "iteration", "+", "1", ")", ",", "\"/\"", ",", "str", "(", "iterations", ")", "]", ")", ")", "\n", "\n", "# Fill the replay memory with a number of random experiences", "\n", "self", ".", "fillReplayMemory", "(", "trainingEnv", ")", "\n", "self", ".", "steps", "=", "0", "\n", "\n", "# Training phase for the number of episodes specified as parameter", "\n", "for", "episode", "in", "tqdm", "(", "range", "(", "numberOfEpisodes", ")", ")", ":", "\n", "\n", "# Set the initial RL variables", "\n", "                    ", "state", "=", "self", ".", "processState", "(", "trainingEnv", ".", "reset", "(", ")", ")", "\n", "done", "=", "0", "\n", "\n", "# Set the performance tracking variables", "\n", "totalReward", "=", "0", "\n", "\n", "# Interact with the training environment until termination", "\n", "while", "done", "==", "0", ":", "\n", "\n", "# Choose an action according to the RL policy and the current RL state", "\n", "                        ", "action", "=", "self", ".", "chooseActionEpsilonGreedy", "(", "state", ",", "self", ".", "epsilonValue", "(", "self", ".", "steps", ")", ")", "\n", "\n", "# Interact with the environment with the chosen action", "\n", "nextState", ",", "reward", ",", "done", ",", "info", "=", "trainingEnv", ".", "step", "(", "action", ")", "\n", "\n", "# Process the RL variables retrieved and insert this new experience into the Experience Replay memory", "\n", "reward", "=", "self", ".", "processReward", "(", "reward", ")", "\n", "nextState", "=", "self", ".", "processState", "(", "nextState", ")", "\n", "self", ".", "replayMemory", ".", "push", "(", "state", ",", "action", ",", "reward", ",", "nextState", ",", "done", ")", "\n", "\n", "# Execute the learning procedure", "\n", "if", "self", ".", "steps", "%", "self", ".", "learningUpdatePeriod", "==", "0", ":", "\n", "                            ", "self", ".", "dataLoader", "=", "DataLoader", "(", "dataset", "=", "self", ".", "replayMemory", ",", "batch_size", "=", "self", ".", "batchSize", ",", "shuffle", "=", "True", ")", "\n", "self", ".", "dataLoaderIter", "=", "iter", "(", "self", ".", "dataLoader", ")", "\n", "self", ".", "learning", "(", ")", "\n", "\n", "# If required, update the target deep neural network (update frequency)", "\n", "", "self", ".", "updateTargetNetwork", "(", ")", "\n", "\n", "# Update the RL state", "\n", "state", "=", "nextState", "\n", "\n", "# Continuous tracking of the training performance", "\n", "totalReward", "+=", "reward", "\n", "\n", "# Incrementation of the number of iterations (steps)", "\n", "self", ".", "steps", "+=", "1", "\n", "\n", "# Store and report the performance of the RL policy (training)", "\n", "", "performanceTraining", "[", "episode", "]", "[", "iteration", "]", "=", "totalReward", "\n", "self", ".", "writer", ".", "add_scalar", "(", "\"Training score (1)\"", ",", "totalReward", ",", "episode", ")", "\n", "self", ".", "writer", ".", "add_scalar", "(", "\"Training score (2)\"", ",", "totalReward", ",", "self", ".", "steps", ")", "\n", "\n", "# Store and report the performance of the RL policy (testing)", "\n", "_", ",", "testingScore", "=", "self", ".", "testing", "(", "trainingEnv", ",", "False", ",", "False", ")", "\n", "performanceTesting", "[", "episode", "]", "[", "iteration", "]", "=", "testingScore", "\n", "self", ".", "writer", ".", "add_scalar", "(", "\"Testing score (1)\"", ",", "testingScore", ",", "episode", ")", "\n", "self", ".", "writer", ".", "add_scalar", "(", "\"Testing score (2)\"", ",", "testingScore", ",", "self", ".", "steps", ")", "\n", "\n", "# Restore the initial state of the RL agent", "\n", "", "if", "iteration", "<", "(", "iterations", "-", "1", ")", ":", "\n", "                    ", "trainingEnv", ".", "reset", "(", ")", "\n", "self", ".", "policyNetwork", ".", "load_state_dict", "(", "initialWeights", ")", "\n", "self", ".", "targetNetwork", ".", "load_state_dict", "(", "initialWeights", ")", "\n", "self", ".", "optimizer", "=", "optim", ".", "Adam", "(", "self", ".", "policyNetwork", ".", "parameters", "(", ")", ",", "lr", "=", "self", ".", "learningRate", ",", "eps", "=", "self", ".", "epsilon", ")", "\n", "self", ".", "replayMemory", ".", "reset", "(", ")", "\n", "self", ".", "steps", "=", "0", "\n", "\n", "", "", "iteration", "+=", "1", "\n", "\n", "", "except", "KeyboardInterrupt", ":", "\n", "            ", "print", "(", ")", "\n", "print", "(", "\"WARNING: Expected performance evaluation prematurely interrupted...\"", ")", "\n", "print", "(", ")", "\n", "\n", "# Smooth the training and testing performances for better readibility (moving average)", "\n", "", "performanceTraining", "=", "np", ".", "transpose", "(", "performanceTraining", ")", "\n", "performanceTesting", "=", "np", ".", "transpose", "(", "performanceTesting", ")", "\n", "for", "i", "in", "range", "(", "iterations", ")", ":", "\n", "            ", "movingAverage", "=", "pd", ".", "DataFrame", "(", "performanceTraining", "[", "i", "]", ",", "columns", "=", "[", "'Score'", "]", ")", ".", "rolling", "(", "100", ",", "min_periods", "=", "1", ")", ".", "mean", "(", ")", "\n", "performanceTraining", "[", "i", "]", "=", "movingAverage", "[", "'Score'", "]", ".", "tolist", "(", ")", "\n", "movingAverage", "=", "pd", ".", "DataFrame", "(", "performanceTesting", "[", "i", "]", ",", "columns", "=", "[", "'Score'", "]", ")", ".", "rolling", "(", "100", ",", "min_periods", "=", "1", ")", ".", "mean", "(", ")", "\n", "performanceTesting", "[", "i", "]", "=", "movingAverage", "[", "'Score'", "]", ".", "tolist", "(", ")", "\n", "", "performanceTraining", "=", "np", ".", "transpose", "(", "performanceTraining", ")", "\n", "performanceTesting", "=", "np", ".", "transpose", "(", "performanceTesting", ")", "\n", "\n", "# Compute the expectation and standard deviation of the training and testing performances", "\n", "expectedPerformance", "=", "[", "]", "\n", "stdPerformance", "=", "[", "]", "\n", "for", "episode", "in", "range", "(", "numberOfEpisodes", ")", ":", "\n", "            ", "expectedPerformance", ".", "append", "(", "np", ".", "mean", "(", "performanceTraining", "[", "episode", "]", "[", ":", "iteration", "]", ")", ")", "\n", "stdPerformance", ".", "append", "(", "np", ".", "std", "(", "performanceTraining", "[", "episode", "]", "[", ":", "iteration", "]", ")", ")", "\n", "", "expectedPerformanceTraining", "=", "np", ".", "array", "(", "expectedPerformance", ")", "\n", "stdPerformanceTraining", "=", "np", ".", "array", "(", "stdPerformance", ")", "\n", "expectedPerformance", "=", "[", "]", "\n", "stdPerformance", "=", "[", "]", "\n", "for", "episode", "in", "range", "(", "numberOfEpisodes", ")", ":", "\n", "            ", "expectedPerformance", ".", "append", "(", "np", ".", "mean", "(", "performanceTesting", "[", "episode", "]", "[", ":", "iteration", "]", ")", ")", "\n", "stdPerformance", ".", "append", "(", "np", ".", "std", "(", "performanceTesting", "[", "episode", "]", "[", ":", "iteration", "]", ")", ")", "\n", "", "expectedPerformanceTesting", "=", "np", ".", "array", "(", "expectedPerformance", ")", "\n", "stdPerformanceTesting", "=", "np", ".", "array", "(", "stdPerformance", ")", "\n", "\n", "# Store the training and testing results into a csv file", "\n", "dataTraining", "=", "{", "'Expectation'", ":", "expectedPerformanceTraining", ",", "\n", "'StandardDeviation'", ":", "stdPerformanceTraining", "}", "\n", "dataframeTraining", "=", "pd", ".", "DataFrame", "(", "dataTraining", ")", "\n", "dataframeTraining", ".", "to_csv", "(", "''", ".", "join", "(", "[", "self", ".", "experimentFolder", ",", "'TrainingResults.csv'", "]", ")", ")", "\n", "\n", "dataTesting", "=", "{", "'Expectation'", ":", "expectedPerformanceTesting", ",", "\n", "'StandardDeviation'", ":", "stdPerformanceTesting", "}", "\n", "dataframeTesting", "=", "pd", ".", "DataFrame", "(", "dataTesting", ")", "\n", "dataframeTesting", ".", "to_csv", "(", "''", ".", "join", "(", "[", "self", ".", "experimentFolder", ",", "'TestingResults.csv'", "]", ")", ")", "\n", "\n", "# Plot the expected performance (training and testing)", "\n", "fig", "=", "plt", ".", "figure", "(", ")", "\n", "ax", "=", "fig", ".", "add_subplot", "(", "111", ",", "ylabel", "=", "'Training score'", ",", "xlabel", "=", "'Episode'", ")", "\n", "ax", ".", "plot", "(", "expectedPerformanceTraining", ")", "\n", "ax", ".", "fill_between", "(", "range", "(", "len", "(", "expectedPerformanceTraining", ")", ")", ",", "expectedPerformanceTraining", "-", "stdPerformanceTraining", ",", "expectedPerformanceTraining", "+", "stdPerformanceTraining", ",", "alpha", "=", "0.25", ")", "\n", "plt", ".", "savefig", "(", "''", ".", "join", "(", "[", "self", ".", "experimentFolder", ",", "'TrainingScores.png'", "]", ")", ")", "\n", "fig", "=", "plt", ".", "figure", "(", ")", "\n", "ax", "=", "fig", ".", "add_subplot", "(", "111", ",", "ylabel", "=", "'Testing score'", ",", "xlabel", "=", "'Episode'", ")", "\n", "ax", ".", "plot", "(", "expectedPerformanceTesting", ")", "\n", "ax", ".", "fill_between", "(", "range", "(", "len", "(", "expectedPerformanceTesting", ")", ")", ",", "expectedPerformanceTesting", "-", "stdPerformanceTesting", ",", "expectedPerformanceTesting", "+", "stdPerformanceTesting", ",", "alpha", "=", "0.25", ")", "\n", "plt", ".", "savefig", "(", "''", ".", "join", "(", "[", "self", ".", "experimentFolder", ",", "'TestingScores.png'", "]", ")", ")", "\n", "\n", "# Closing of the tensorboard writer", "\n", "self", ".", "writer", ".", "close", "(", ")", "\n", "\n", "return", "trainingEnv", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.DQN.DQN.saveModel": [[727, 737], ["torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "DQN.DQN.policyNetwork.state_dict"], "methods", ["None"], ["", "def", "saveModel", "(", "self", ",", "fileName", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Save the RL policy, by saving the policy Deep Neural Network.\n        \n        INPUTS: - fileName: Name of the file.\n        \n        OUTPUTS: /\n        \"\"\"", "\n", "\n", "torch", ".", "save", "(", "self", ".", "policyNetwork", ".", "state_dict", "(", ")", ",", "fileName", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.DQN.DQN.loadModel": [[739, 750], ["DQN.DQN.policyNetwork.load_state_dict", "DQN.DQN.targetNetwork.load_state_dict", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "DQN.DQN.policyNetwork.state_dict"], "methods", ["None"], ["", "def", "loadModel", "(", "self", ",", "fileName", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Load the RL policy, by loading the policy Deep Neural Network.\n        \n        INPUTS: - fileName: Name of the file.\n        \n        OUTPUTS: /\n        \"\"\"", "\n", "\n", "self", ".", "policyNetwork", ".", "load_state_dict", "(", "torch", ".", "load", "(", "fileName", ",", "map_location", "=", "self", ".", "device", ")", ")", "\n", "self", ".", "targetNetwork", ".", "load_state_dict", "(", "self", ".", "policyNetwork", ".", "state_dict", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.DQN.DQN.plotEpsilonAnnealing": [[752, 767], ["matplotlib.pyplot.figure", "matplotlib.pyplot.plot", "matplotlib.pyplot.xlabel", "matplotlib.pyplot.ylabel", "matplotlib.pyplot.show", "DQN.DQN.epsilonValue", "range"], "methods", ["None"], ["", "def", "plotEpsilonAnnealing", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Plot the annealing behaviour of the Epsilon variable\n              (Epsilon-Greedy exploration technique).\n        \n        INPUTS: /\n        \n        OUTPUTS: /\n        \"\"\"", "\n", "\n", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "[", "self", ".", "epsilonValue", "(", "i", ")", "for", "i", "in", "range", "(", "1000000", ")", "]", ")", "\n", "plt", ".", "xlabel", "(", "\"Steps\"", ")", "\n", "plt", ".", "ylabel", "(", "\"Epsilon\"", ")", "\n", "plt", ".", "show", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.UMDQN_W.UMDQN_W.__init__": [[60, 131], ["DQN.DQN.DQN.__init__", "UMDQN_W.UMDQN_W.readParameters", "torch.device", "torch.device", "torch.device", "torch.device", "replayMemory.ReplayMemory", "numpy.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "UMDQN_W.UMDQN_W.supportTorch.repeat().view", "UMDQN_W.UMDQN_W.targetNetwork.load_state_dict", "torch.Adam", "torch.Adam", "UMDQN_W.UMDQN_W.initReporting", "Models.UMDQN_W_Model_Atari.UMDQN_W_Model_Atari().to", "Models.UMDQN_W_Model_Atari.UMDQN_W_Model_Atari().to", "Models.UMDQN_W_Model.UMDQN_W_Model().to", "Models.UMDQN_W_Model.UMDQN_W_Model().to", "UMDQN_W.UMDQN_W.policyNetwork.state_dict", "UMDQN_W.UMDQN_W.policyNetwork.parameters", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "UMDQN_W.UMDQN_W.supportTorch.repeat", "str", "str", "Models.UMDQN_W_Model_Atari.UMDQN_W_Model_Atari", "Models.UMDQN_W_Model_Atari.UMDQN_W_Model_Atari", "Models.UMDQN_W_Model.UMDQN_W_Model", "Models.UMDQN_W_Model.UMDQN_W_Model", "math.exp"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.MonotonicNN.__init__", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.DQN.DQN.readParameters", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.DQN.DQN.initReporting"], ["def", "__init__", "(", "self", ",", "observationSpace", ",", "actionSpace", ",", "environment", ",", "\n", "parametersFileName", "=", "''", ",", "reporting", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Initializing the RL agent based on the UMDQN_W Deep Reinforcement Learning\n              algorithm, by setting up the algorithm parameters as well as \n              the Deep Neural Networks.\n        \n        INPUTS: - observationSpace: RL observation space.\n                - actionSpace: RL action space.\n                - environment: Name of the RL environment.\n                - parametersFileName: Name of the JSON parameters file.\n                - reporting: Enable the reporting of the results.\n        \n        OUTPUTS: /\n        \"\"\"", "\n", "\n", "# Initialization of the DQN parent class", "\n", "DQN", ".", "__init__", "(", "self", ",", "observationSpace", ",", "actionSpace", ",", "environment", ",", "parametersFileName", ",", "False", ")", "\n", "\n", "# Setting of the parameters", "\n", "if", "parametersFileName", "==", "''", ":", "\n", "            ", "parametersFileName", "=", "''", ".", "join", "(", "[", "'Parameters/parameters_UMDQN_W_'", ",", "str", "(", "environment", ")", ",", "'.json'", "]", ")", "\n", "", "parameters", "=", "self", ".", "readParameters", "(", "parametersFileName", ")", "\n", "\n", "# Set the device for DNN computations (CPU or GPU)", "\n", "self", ".", "device", "=", "torch", ".", "device", "(", "'cuda:'", "+", "str", "(", "parameters", "[", "'GPUNumber'", "]", ")", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "'cpu'", ")", "\n", "\n", "# Set the general parameters of the RL algorithm", "\n", "self", ".", "gamma", "=", "parameters", "[", "'gamma'", "]", "\n", "self", ".", "learningRate", "=", "parameters", "[", "'learningRate'", "]", "\n", "self", ".", "epsilon", "=", "parameters", "[", "'epsilon'", "]", "\n", "self", ".", "targetUpdatePeriod", "=", "parameters", "[", "'targetUpdatePeriod'", "]", "\n", "self", ".", "learningUpdatePeriod", "=", "parameters", "[", "'learningUpdatePeriod'", "]", "\n", "self", ".", "rewardClipping", "=", "parameters", "[", "'rewardClipping'", "]", "\n", "self", ".", "gradientClipping", "=", "parameters", "[", "'gradientClipping'", "]", "\n", "\n", "# Set the Experience Replay mechanism", "\n", "self", ".", "batchSize", "=", "parameters", "[", "'batchSize'", "]", "\n", "self", ".", "capacity", "=", "parameters", "[", "'capacity'", "]", "\n", "self", ".", "replayMemory", "=", "ReplayMemory", "(", "self", ".", "capacity", ")", "\n", "\n", "# Set the distribution support (quantile fractions)", "\n", "self", ".", "numberOfSamples", "=", "parameters", "[", "'numberOfSamples'", "]", "\n", "self", ".", "support", "=", "np", ".", "linspace", "(", "0.0", ",", "1.0", ",", "self", ".", "numberOfSamples", ")", "\n", "self", ".", "supportTorch", "=", "torch", ".", "linspace", "(", "0.0", ",", "1.0", ",", "self", ".", "numberOfSamples", ",", "device", "=", "self", ".", "device", ")", "\n", "self", ".", "supportRepeatedBatchSize", "=", "self", ".", "supportTorch", ".", "repeat", "(", "self", ".", "batchSize", ",", "1", ")", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "self", ".", "kappa", "=", "1.0", "\n", "\n", "# Set the two Deep Neural Networks of the RL algorithm (policy and target)", "\n", "self", ".", "atari", "=", "parameters", "[", "'atari'", "]", "\n", "self", ".", "minatar", "=", "parameters", "[", "'minatar'", "]", "\n", "if", "self", ".", "atari", "or", "self", ".", "minatar", ":", "\n", "            ", "self", ".", "policyNetwork", "=", "UMDQN_W_Model_Atari", "(", "observationSpace", ",", "actionSpace", ",", "parameters", "[", "'structureUMNN'", "]", ",", "parameters", "[", "'stateEmbedding'", "]", ",", "parameters", "[", "'numberOfSteps'", "]", ",", "self", ".", "device", ",", "minAtar", "=", "self", ".", "minatar", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "targetNetwork", "=", "UMDQN_W_Model_Atari", "(", "observationSpace", ",", "actionSpace", ",", "parameters", "[", "'structureUMNN'", "]", ",", "parameters", "[", "'stateEmbedding'", "]", ",", "parameters", "[", "'numberOfSteps'", "]", ",", "self", ".", "device", ",", "minAtar", "=", "self", ".", "minatar", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "policyNetwork", "=", "UMDQN_W_Model", "(", "observationSpace", ",", "actionSpace", ",", "parameters", "[", "'structureDNN'", "]", ",", "parameters", "[", "'structureUMNN'", "]", ",", "parameters", "[", "'stateEmbedding'", "]", ",", "parameters", "[", "'numberOfSteps'", "]", ",", "self", ".", "device", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "targetNetwork", "=", "UMDQN_W_Model", "(", "observationSpace", ",", "actionSpace", ",", "parameters", "[", "'structureDNN'", "]", ",", "parameters", "[", "'structureUMNN'", "]", ",", "parameters", "[", "'stateEmbedding'", "]", ",", "parameters", "[", "'numberOfSteps'", "]", ",", "self", ".", "device", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "", "self", ".", "targetNetwork", ".", "load_state_dict", "(", "self", ".", "policyNetwork", ".", "state_dict", "(", ")", ")", "\n", "\n", "# Set the Deep Learning optimizer", "\n", "self", ".", "optimizer", "=", "optim", ".", "Adam", "(", "self", ".", "policyNetwork", ".", "parameters", "(", ")", ",", "lr", "=", "self", ".", "learningRate", ",", "eps", "=", "self", ".", "epsilon", ")", "\n", "\n", "# Set the Epsilon-Greedy exploration technique", "\n", "self", ".", "epsilonStart", "=", "parameters", "[", "'epsilonStart'", "]", "\n", "self", ".", "epsilonEnd", "=", "parameters", "[", "'epsilonEnd'", "]", "\n", "self", ".", "epsilonDecay", "=", "parameters", "[", "'epsilonDecay'", "]", "\n", "self", ".", "epsilonTest", "=", "parameters", "[", "'epsilonTest'", "]", "\n", "self", ".", "epsilonValue", "=", "lambda", "iteration", ":", "self", ".", "epsilonEnd", "+", "(", "self", ".", "epsilonStart", "-", "self", ".", "epsilonEnd", ")", "*", "math", ".", "exp", "(", "-", "1", "*", "iteration", "/", "self", ".", "epsilonDecay", ")", "\n", "\n", "# Initialization of the experiment folder and tensorboard writer", "\n", "self", ".", "initReporting", "(", "parameters", ",", "'UMDQN_W'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.UMDQN_W.UMDQN_W.chooseAction": [[133, 188], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.from_numpy().float().to().unsqueeze", "torch.from_numpy().float().to().unsqueeze", "torch.from_numpy().float().to().unsqueeze", "torch.from_numpy().float().to().unsqueeze", "UMDQN_W.UMDQN_W.policyNetwork", "quantiles.squeeze().cpu().numpy.squeeze().cpu().numpy.mean", "QValues.squeeze().cpu().numpy.squeeze().cpu().numpy.max", "action.item", "UMDQN_W.UMDQN_W.supportTorch.unsqueeze", "matplotlib.pyplot.figure", "matplotlib.pyplot.subplot", "torch.linspace().unsqueeze", "torch.linspace().unsqueeze", "torch.linspace().unsqueeze", "torch.linspace().unsqueeze", "UMDQN_W.UMDQN_W.policyNetwork", "quantiles.squeeze().cpu().numpy.squeeze().cpu().numpy.mean", "taus.cpu().numpy.cpu().numpy.cpu().numpy", "quantiles.squeeze().cpu().numpy.squeeze().cpu().numpy.squeeze().cpu().numpy", "QValues.squeeze().cpu().numpy.squeeze().cpu().numpy.squeeze().cpu().numpy", "range", "matplotlib.pyplot.subplot.set_xlabel", "matplotlib.pyplot.subplot.set_ylabel", "matplotlib.pyplot.subplot.legend", "matplotlib.pyplot.show", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "matplotlib.pyplot.subplot.plot", "matplotlib.pyplot.subplot.axhline", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "taus.cpu().numpy.cpu().numpy.cpu", "quantiles.squeeze().cpu().numpy.squeeze().cpu().numpy.squeeze().cpu", "QValues.squeeze().cpu().numpy.squeeze().cpu().numpy.squeeze().cpu", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "quantiles.squeeze().cpu().numpy.squeeze().cpu().numpy.squeeze", "QValues.squeeze().cpu().numpy.squeeze().cpu().numpy.squeeze", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "str", "str"], "methods", ["None"], ["", "def", "chooseAction", "(", "self", ",", "state", ",", "plot", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Choose a valid RL action from the action space according to the\n              RL policy as well as the current RL state observed.\n        \n        INPUTS: - state: RL state returned by the environment.\n                - plot: Enable the plotting of the random returns distributions.\n        \n        OUTPUTS: - action: RL action chosen from the action space.\n        \"\"\"", "\n", "\n", "# Choose the best action based on the RL policy", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "state", "=", "torch", ".", "from_numpy", "(", "state", ")", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", ".", "unsqueeze", "(", "0", ")", "\n", "quantiles", "=", "self", ".", "policyNetwork", "(", "state", ",", "self", ".", "supportTorch", ".", "unsqueeze", "(", "1", ")", ")", "\n", "QValues", "=", "quantiles", ".", "mean", "(", "1", ")", "\n", "_", ",", "action", "=", "QValues", ".", "max", "(", "0", ")", "\n", "\n", "# If required, plot the return distribution associated with each action", "\n", "if", "plot", ":", "\n", "                ", "colors", "=", "[", "'blue'", ",", "'red'", ",", "'orange'", ",", "'green'", ",", "'purple'", ",", "'brown'", "]", "\n", "plt", ".", "figure", "(", ")", "\n", "ax", "=", "plt", ".", "subplot", "(", "1", ",", "1", ",", "1", ")", "\n", "taus", "=", "torch", ".", "linspace", "(", "0.0", ",", "1.0", ",", "self", ".", "numberOfSamples", "*", "10", ",", "device", "=", "self", ".", "device", ")", ".", "unsqueeze", "(", "1", ")", "\n", "quantiles", "=", "self", ".", "policyNetwork", "(", "state", ",", "taus", ")", "\n", "QValues", "=", "quantiles", ".", "mean", "(", "1", ")", "\n", "taus", "=", "taus", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "quantiles", "=", "quantiles", ".", "squeeze", "(", "0", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "QValues", "=", "QValues", ".", "squeeze", "(", "0", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "for", "a", "in", "range", "(", "self", ".", "actionSpace", ")", ":", "\n", "                    ", "ax", ".", "plot", "(", "taus", ",", "quantiles", "[", "a", "]", ",", "linestyle", "=", "'-'", ",", "label", "=", "''", ".", "join", "(", "[", "'Action '", ",", "str", "(", "a", ")", ",", "' random return Z'", "]", ")", ",", "color", "=", "colors", "[", "a", "]", ")", "\n", "ax", ".", "axhline", "(", "y", "=", "QValues", "[", "a", "]", ",", "linewidth", "=", "2", ",", "linestyle", "=", "'--'", ",", "label", "=", "''", ".", "join", "(", "[", "'Action '", ",", "str", "(", "a", ")", ",", "' expected return Q'", "]", ")", ",", "color", "=", "colors", "[", "a", "]", ")", "\n", "", "ax", ".", "set_xlabel", "(", "'Quantile fraction'", ")", "\n", "ax", ".", "set_ylabel", "(", "'Quantile Function (QF)'", ")", "\n", "ax", ".", "legend", "(", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\"\"\"\n                # Saving of the data into external files\n                taus = np.linspace(0, 1, self.numberOfSamples*10)\n                dataQF = {\n                'Action0_x': taus,\n                'Action0_y': quantiles[0],\n                'Action1_x': taus,\n                'Action1_y': quantiles[1],\n                'Action2_x': taus,\n                'Action2_y': quantiles[2],\n                'Action3_x': taus,\n                'Action3_y': quantiles[3],\n                }\n                dataframeQF = pd.DataFrame(dataQF)\n                dataframeQF.to_csv('Figures/Distributions/UMDQN_W.csv')\n                quit()\n                \"\"\"", "\n", "\n", "", "return", "action", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.UMDQN_W.UMDQN_W.learning": [[190, 250], ["len", "UMDQN_W.UMDQN_W.dataLoaderIter.next", "batch[].float().to", "batch[].long().to", "batch[].float().to", "batch[].float().to", "batch[].float().to", "UMDQN_W.UMDQN_W.policyNetwork", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "quantiles.clamp.clamp.clamp", "targetQuantiles.clamp.clamp.clamp", "difference.abs", "torch.where", "torch.where", "torch.where", "torch.where", "loss.mean().sum().mean.mean().sum().mean.mean().sum().mean", "UMDQN_W.UMDQN_W.optimizer.zero_grad", "loss.mean().sum().mean.mean().sum().mean.backward", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "UMDQN_W.UMDQN_W.optimizer.step", "loss.mean().sum().mean.mean().sum().mean.item", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "UMDQN_W.UMDQN_W.targetNetwork", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "targetQuantiles.clamp.clamp.unsqueeze", "quantiles.clamp.clamp.unsqueeze", "UMDQN_W.UMDQN_W.policyNetwork.parameters", "batch[].float", "batch[].long", "batch[].float", "batch[].float", "batch[].float", "torch.index_select.view().mean().max", "torch.index_select.view().mean().max", "batch[].float().to.unsqueeze", "difference.abs.pow", "loss.mean().sum().mean.mean().sum().mean.mean().sum", "range", "torch.index_select.view().mean", "torch.index_select.view().mean", "range", "batch[].float().to.unsqueeze", "loss.mean().sum().mean.mean().sum().mean.mean", "UMDQN_W.UMDQN_W.supportRepeatedBatchSize.view", "torch.index_select.view", "torch.index_select.view"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorld.StochasticGridWorld.step"], ["", "", "def", "learning", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Sample a batch of past experiences and learn from it\n              by updating the Reinforcement Learning policy.\n        \n        INPUTS: /\n        \n        OUTPUTS: - loss: Loss of the learning procedure.\n        \"\"\"", "\n", "\n", "# Check that the replay memory is filled enough", "\n", "if", "(", "len", "(", "self", ".", "replayMemory", ")", ">=", "self", ".", "batchSize", ")", ":", "\n", "\n", "# Sample a batch of experiences from the replay memory", "\n", "            ", "batch", "=", "self", ".", "dataLoaderIter", ".", "next", "(", ")", "\n", "state", "=", "batch", "[", "0", "]", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "action", "=", "batch", "[", "1", "]", ".", "long", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "reward", "=", "batch", "[", "2", "]", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "nextState", "=", "batch", "[", "3", "]", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "done", "=", "batch", "[", "4", "]", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "# Computation of the current return distribution", "\n", "quantiles", "=", "self", ".", "policyNetwork", "(", "state", ",", "self", ".", "supportRepeatedBatchSize", ")", "\n", "selection", "=", "torch", ".", "tensor", "(", "[", "self", ".", "actionSpace", "*", "i", "+", "action", "[", "i", "]", "for", "i", "in", "range", "(", "self", ".", "batchSize", ")", "]", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "self", ".", "device", ")", "\n", "quantiles", "=", "torch", ".", "index_select", "(", "quantiles", ",", "0", ",", "selection", ")", "\n", "\n", "# Computation of the new distribution to be learnt by the policy DNN", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "nextQuantiles", "=", "self", ".", "targetNetwork", "(", "nextState", ",", "self", ".", "supportRepeatedBatchSize", ")", "\n", "nextAction", "=", "nextQuantiles", ".", "view", "(", "self", ".", "batchSize", ",", "self", ".", "actionSpace", ",", "self", ".", "numberOfSamples", ")", ".", "mean", "(", "2", ")", ".", "max", "(", "1", ")", "[", "1", "]", "\n", "selection", "=", "torch", ".", "tensor", "(", "[", "self", ".", "actionSpace", "*", "i", "+", "nextAction", "[", "i", "]", "for", "i", "in", "range", "(", "self", ".", "batchSize", ")", "]", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "self", ".", "device", ")", "\n", "nextQuantiles", "=", "torch", ".", "index_select", "(", "nextQuantiles", ",", "0", ",", "selection", ")", "\n", "targetQuantiles", "=", "reward", ".", "unsqueeze", "(", "1", ")", "+", "self", ".", "gamma", "*", "nextQuantiles", "*", "(", "1", "-", "done", ".", "unsqueeze", "(", "1", ")", ")", "\n", "\n", "#\"\"\"", "\n", "# Improve stability with the lower and upper bounds of the random return", "\n", "", "minZ", "=", "-", "1", "\n", "maxZ", "=", "10", "\n", "quantiles", "=", "quantiles", ".", "clamp", "(", "min", "=", "minZ", ",", "max", "=", "maxZ", ")", "\n", "targetQuantiles", "=", "targetQuantiles", ".", "clamp", "(", "min", "=", "minZ", ",", "max", "=", "maxZ", ")", "\n", "#\"\"\"", "\n", "\n", "# Computation of the loss", "\n", "difference", "=", "targetQuantiles", ".", "unsqueeze", "(", "1", ")", "-", "quantiles", ".", "unsqueeze", "(", "2", ")", "\n", "error", "=", "difference", ".", "abs", "(", ")", "\n", "loss", "=", "torch", ".", "where", "(", "error", "<=", "self", ".", "kappa", ",", "0.5", "*", "error", ".", "pow", "(", "2", ")", ",", "self", ".", "kappa", "*", "(", "error", "-", "(", "0.5", "*", "self", ".", "kappa", ")", ")", ")", "\n", "loss", "=", "(", "self", ".", "supportRepeatedBatchSize", ".", "view", "(", "self", ".", "batchSize", ",", "self", ".", "numberOfSamples", ",", "1", ")", "-", "(", "difference", "<", "0", ")", ".", "float", "(", ")", ")", ".", "abs", "(", ")", "*", "loss", "/", "self", ".", "kappa", "\n", "loss", "=", "loss", ".", "mean", "(", "1", ")", ".", "sum", "(", "1", ")", ".", "mean", "(", ")", "\n", "\n", "# Computation of the gradients", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "\n", "# Gradient Clipping", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "policyNetwork", ".", "parameters", "(", ")", ",", "self", ".", "gradientClipping", ")", "\n", "\n", "# Perform the Deep Neural Network optimization", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "return", "loss", ".", "item", "(", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.replayMemory.ReplayMemory.__init__": [[34, 48], ["random.seed", "collections.deque"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "capacity", "=", "10000", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Initialization of the replay memory data structure.\n        \n        INPUTS: - capacity: Capacity of the data structure, specifying the\n                            maximum number of experiences to be stored\n                            simultaneously into the data structure.\n        \n        OUTPUTS: /\n        \"\"\"", "\n", "\n", "random", ".", "seed", "(", "0", ")", "\n", "self", ".", "capacity", "=", "capacity", "\n", "self", ".", "memory", "=", "deque", "(", "maxlen", "=", "capacity", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.replayMemory.ReplayMemory.__getitem__": [[50, 61], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Outputing the item associated with the provided index\n              from the replay memory.\n        \n        INPUTS: /\n        \n        OUTPUTS: - item: Selected item of the replay memory.\n        \"\"\"", "\n", "\n", "return", "self", ".", "memory", "[", "index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.replayMemory.ReplayMemory.__len__": [[63, 74], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Return the size of the replay memory, i.e. the number of experiences\n              currently stored into the data structure.\n        \n        INPUTS: /\n        \n        OUTPUTS: - length: Size of the replay memory.\n        \"\"\"", "\n", "\n", "return", "len", "(", "self", ".", "memory", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.replayMemory.ReplayMemory.push": [[76, 93], ["replayMemory.ReplayMemory.memory.append"], "methods", ["None"], ["", "def", "push", "(", "self", ",", "state", ",", "action", ",", "reward", ",", "nextState", ",", "done", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Insert a new experience into the replay memory. An experience\n              is composed of a state, an action, a reward, a next state and\n              a termination signal.\n        \n        INPUTS: - state: RL state of the experience to be stored.\n                - action: RL action of the experience to be stored.\n                - reward: RL reward of the experience to be stored.\n                - nextState: RL next state of the experience to be stored.\n                - done: RL termination signal of the experience to be stored.\n        \n        OUTPUTS: /\n        \"\"\"", "\n", "\n", "# FIFO policy", "\n", "self", ".", "memory", ".", "append", "(", "(", "state", ",", "action", ",", "reward", ",", "nextState", ",", "done", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.replayMemory.ReplayMemory.sample": [[95, 110], ["zip", "random.sample"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.replayMemory.ReplayMemory.sample"], ["", "def", "sample", "(", "self", ",", "batchSize", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Sample a batch of experiences from the replay memory.\n        \n        INPUTS: - batchSize: Size of the batch to sample.\n        \n        OUTPUTS: - state: RL states of the experience batch sampled.\n                 - action: RL actions of the experience batch sampled.\n                 - reward: RL rewards of the experience batch sampled.\n                 - nextState: RL next states of the experience batch sampled.\n                 - done: RL termination signals of the experience batch sampled.\n        \"\"\"", "\n", "\n", "state", ",", "action", ",", "reward", ",", "nextState", ",", "done", "=", "zip", "(", "*", "random", ".", "sample", "(", "self", ".", "memory", ",", "batchSize", ")", ")", "\n", "return", "state", ",", "action", ",", "reward", ",", "nextState", ",", "done", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.replayMemory.ReplayMemory.reset": [[112, 123], ["random.seed", "collections.deque"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Reset (empty) the replay memory.\n        \n        INPUTS: /\n        \n        OUTPUTS: /\n        \"\"\"", "\n", "\n", "random", ".", "seed", "(", "0", ")", "\n", "self", ".", "memory", "=", "deque", "(", "maxlen", "=", "self", ".", "capacity", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.UMDQN_C.UMDQN_C.__init__": [[61, 137], ["DQN.DQN.DQN.__init__", "UMDQN_C.UMDQN_C.readParameters", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "replayMemory.ReplayMemory", "numpy.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "UMDQN_C.UMDQN_C.supportTorch.repeat().view", "UMDQN_C.UMDQN_C.targetNetwork.load_state_dict", "torch.Adam", "torch.Adam", "torch.Adam", "UMDQN_C.UMDQN_C.initReporting", "Models.UMDQN_C_Model_Atari.UMDQN_C_Model_Atari().to", "Models.UMDQN_C_Model_Atari.UMDQN_C_Model_Atari().to", "Models.UMDQN_C_Model.UMDQN_C_Model().to", "Models.UMDQN_C_Model.UMDQN_C_Model().to", "UMDQN_C.UMDQN_C.policyNetwork.state_dict", "UMDQN_C.UMDQN_C.policyNetwork.parameters", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "UMDQN_C.UMDQN_C.supportTorch.repeat", "str", "str", "Models.UMDQN_C_Model_Atari.UMDQN_C_Model_Atari", "Models.UMDQN_C_Model_Atari.UMDQN_C_Model_Atari", "Models.UMDQN_C_Model.UMDQN_C_Model", "Models.UMDQN_C_Model.UMDQN_C_Model", "math.exp"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.MonotonicNN.__init__", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.DQN.DQN.readParameters", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.DQN.DQN.initReporting"], ["def", "__init__", "(", "self", ",", "observationSpace", ",", "actionSpace", ",", "environment", ",", "\n", "parametersFileName", "=", "''", ",", "reporting", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Initializing the RL agent based on the UMDQN_C Deep Reinforcement Learning\n              algorithm, by setting up the algorithm parameters as well as \n              the Deep Neural Networks.\n        \n        INPUTS: - observationSpace: RL observation space.\n                - actionSpace: RL action space.\n                - environment: Name of the RL environment.\n                - parametersFileName: Name of the JSON parameters file.\n                - reporting: Enable the reporting of the results.\n        \n        OUTPUTS: /\n        \"\"\"", "\n", "\n", "# Initialization of the DQN parent class", "\n", "DQN", ".", "__init__", "(", "self", ",", "observationSpace", ",", "actionSpace", ",", "environment", ",", "parametersFileName", ",", "False", ")", "\n", "\n", "# Setting of the parameters", "\n", "if", "parametersFileName", "==", "''", ":", "\n", "            ", "parametersFileName", "=", "''", ".", "join", "(", "[", "'Parameters/parameters_UMDQN_C_'", ",", "str", "(", "environment", ")", ",", "'.json'", "]", ")", "\n", "", "parameters", "=", "self", ".", "readParameters", "(", "parametersFileName", ")", "\n", "\n", "# Set the device for DNN computations (CPU or GPU)", "\n", "self", ".", "device", "=", "torch", ".", "device", "(", "'cuda:'", "+", "str", "(", "parameters", "[", "'GPUNumber'", "]", ")", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "'cpu'", ")", "\n", "\n", "# Set the general parameters of the RL algorithm", "\n", "self", ".", "gamma", "=", "parameters", "[", "'gamma'", "]", "\n", "self", ".", "learningRate", "=", "parameters", "[", "'learningRate'", "]", "\n", "self", ".", "epsilon", "=", "parameters", "[", "'epsilon'", "]", "\n", "self", ".", "targetUpdatePeriod", "=", "parameters", "[", "'targetUpdatePeriod'", "]", "\n", "self", ".", "learningUpdatePeriod", "=", "parameters", "[", "'learningUpdatePeriod'", "]", "\n", "self", ".", "rewardClipping", "=", "parameters", "[", "'rewardClipping'", "]", "\n", "self", ".", "gradientClipping", "=", "parameters", "[", "'gradientClipping'", "]", "\n", "\n", "# Set the Experience Replay mechanism", "\n", "self", ".", "batchSize", "=", "parameters", "[", "'batchSize'", "]", "\n", "self", ".", "capacity", "=", "parameters", "[", "'capacity'", "]", "\n", "self", ".", "replayMemory", "=", "ReplayMemory", "(", "self", ".", "capacity", ")", "\n", "\n", "# Set the distribution support", "\n", "self", ".", "numberOfSamples", "=", "parameters", "[", "'numberOfSamples'", "]", "\n", "self", ".", "minReturn", "=", "parameters", "[", "'minReturn'", "]", "\n", "self", ".", "maxReturn", "=", "parameters", "[", "'maxReturn'", "]", "\n", "self", ".", "support", "=", "np", ".", "linspace", "(", "self", ".", "minReturn", ",", "self", ".", "maxReturn", ",", "self", ".", "numberOfSamples", ")", "\n", "self", ".", "supportTorch", "=", "torch", ".", "linspace", "(", "self", ".", "minReturn", ",", "self", ".", "maxReturn", ",", "self", ".", "numberOfSamples", ",", "device", "=", "self", ".", "device", ")", "\n", "self", ".", "supportRepeatedBatchSize", "=", "self", ".", "supportTorch", ".", "repeat", "(", "self", ".", "batchSize", ",", "1", ")", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "self", ".", "uniformProba", "=", "1", "/", "(", "self", ".", "maxReturn", "-", "self", ".", "minReturn", ")", "\n", "\n", "# Enable the faster but potentially less accurate estimation of the expectation", "\n", "self", ".", "fasterExpectation", "=", "parameters", "[", "'fasterExpectation'", "]", "\n", "\n", "# Set the two Deep Neural Networks of the RL algorithm (policy and target)", "\n", "self", ".", "atari", "=", "parameters", "[", "'atari'", "]", "\n", "self", ".", "minatar", "=", "parameters", "[", "'minatar'", "]", "\n", "if", "self", ".", "atari", "or", "self", ".", "minatar", ":", "\n", "            ", "self", ".", "policyNetwork", "=", "UMDQN_C_Model_Atari", "(", "observationSpace", ",", "actionSpace", ",", "parameters", "[", "'structureUMNN'", "]", ",", "parameters", "[", "'stateEmbedding'", "]", ",", "parameters", "[", "'numberOfSteps'", "]", ",", "self", ".", "device", ",", "minAtar", "=", "self", ".", "minatar", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "targetNetwork", "=", "UMDQN_C_Model_Atari", "(", "observationSpace", ",", "actionSpace", ",", "parameters", "[", "'structureUMNN'", "]", ",", "parameters", "[", "'stateEmbedding'", "]", ",", "parameters", "[", "'numberOfSteps'", "]", ",", "self", ".", "device", ",", "minAtar", "=", "self", ".", "minatar", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "policyNetwork", "=", "UMDQN_C_Model", "(", "observationSpace", ",", "actionSpace", ",", "parameters", "[", "'structureDNN'", "]", ",", "parameters", "[", "'structureUMNN'", "]", ",", "parameters", "[", "'stateEmbedding'", "]", ",", "parameters", "[", "'numberOfSteps'", "]", ",", "self", ".", "device", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "targetNetwork", "=", "UMDQN_C_Model", "(", "observationSpace", ",", "actionSpace", ",", "parameters", "[", "'structureDNN'", "]", ",", "parameters", "[", "'structureUMNN'", "]", ",", "parameters", "[", "'stateEmbedding'", "]", ",", "parameters", "[", "'numberOfSteps'", "]", ",", "self", ".", "device", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "", "self", ".", "targetNetwork", ".", "load_state_dict", "(", "self", ".", "policyNetwork", ".", "state_dict", "(", ")", ")", "\n", "\n", "# Set the Deep Learning optimizer", "\n", "self", ".", "optimizer", "=", "optim", ".", "Adam", "(", "self", ".", "policyNetwork", ".", "parameters", "(", ")", ",", "lr", "=", "self", ".", "learningRate", ",", "eps", "=", "self", ".", "epsilon", ")", "\n", "\n", "# Set the Epsilon-Greedy exploration technique", "\n", "self", ".", "epsilonStart", "=", "parameters", "[", "'epsilonStart'", "]", "\n", "self", ".", "epsilonEnd", "=", "parameters", "[", "'epsilonEnd'", "]", "\n", "self", ".", "epsilonDecay", "=", "parameters", "[", "'epsilonDecay'", "]", "\n", "self", ".", "epsilonTest", "=", "parameters", "[", "'epsilonTest'", "]", "\n", "self", ".", "epsilonValue", "=", "lambda", "iteration", ":", "self", ".", "epsilonEnd", "+", "(", "self", ".", "epsilonStart", "-", "self", ".", "epsilonEnd", ")", "*", "math", ".", "exp", "(", "-", "1", "*", "iteration", "/", "self", ".", "epsilonDecay", ")", "\n", "\n", "# Initialization of the experiment folder and tensorboard writer", "\n", "self", ".", "initReporting", "(", "parameters", ",", "'UMDQN_C'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.UMDQN_C.UMDQN_C.chooseAction": [[139, 203], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.from_numpy().float().to().unsqueeze", "torch.from_numpy().float().to().unsqueeze", "torch.from_numpy().float().to().unsqueeze", "torch.from_numpy().float().to().unsqueeze", "torch.from_numpy().float().to().unsqueeze", "torch.from_numpy().float().to().unsqueeze", "torch.from_numpy().float().to().unsqueeze", "torch.from_numpy().float().to().unsqueeze", "torch.from_numpy().float().to().unsqueeze", "UMDQN_C.UMDQN_C.max", "action.item", "UMDQN_C.UMDQN_C.policyNetwork.getExpectation().squeeze", "UMDQN_C.UMDQN_C.policyNetwork.getDerivative", "matplotlib.pyplot.figure", "matplotlib.pyplot.subplot", "matplotlib.pyplot.subplot", "range", "matplotlib.pyplot.subplot.set_xlabel", "matplotlib.pyplot.subplot.set_ylabel", "matplotlib.pyplot.subplot.set_xlabel", "matplotlib.pyplot.subplot.set_ylabel", "matplotlib.pyplot.subplot.legend", "matplotlib.pyplot.subplot.legend", "matplotlib.pyplot.show", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "UMDQN_C.UMDQN_C.supportTorch.unsqueeze", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "numpy.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "UMDQN_C.UMDQN_C.policyNetwork", "UMDQN_C.UMDQN_C.policyNetwork.getDerivative", "matplotlib.pyplot.subplot.plot", "matplotlib.pyplot.subplot.plot", "matplotlib.pyplot.subplot.fill_between", "matplotlib.pyplot.subplot.axvline", "matplotlib.pyplot.subplot.axvline", "UMDQN_C.UMDQN_C.policyNetwork.getExpectation", "torch.linspace.unsqueeze", "torch.linspace.unsqueeze", "torch.linspace.unsqueeze", "torch.linspace.unsqueeze", "torch.linspace.unsqueeze", "torch.linspace.unsqueeze", "cdfs[].cpu", "pdfs[].cpu", "pdfs[].cpu", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "str", "str", "str", "str"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.UMDQN_C_Model_Atari.UMDQN_C_Model_Atari.getDerivative", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.UMDQN_C_Model_Atari.UMDQN_C_Model_Atari.getDerivative", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.UMDQN_C_Model_Atari.UMDQN_C_Model_Atari.getExpectation"], ["", "def", "chooseAction", "(", "self", ",", "state", ",", "plot", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Choose a valid RL action from the action space according to the\n              RL policy as well as the current RL state observed.\n        \n        INPUTS: - state: RL state returned by the environment.\n                - plot: Enable the plotting of the random returns distributions.\n        \n        OUTPUTS: - action: RL action chosen from the action space.\n        \"\"\"", "\n", "\n", "# Choose the best action based on the RL policy", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "state", "=", "torch", ".", "from_numpy", "(", "state", ")", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", ".", "unsqueeze", "(", "0", ")", "\n", "if", "self", ".", "fasterExpectation", ":", "\n", "                ", "QValues", "=", "self", ".", "policyNetwork", ".", "getExpectation", "(", "state", ",", "self", ".", "minReturn", ",", "self", ".", "maxReturn", ",", "10", "*", "self", ".", "numberOfSamples", ")", ".", "squeeze", "(", "0", ")", "\n", "", "else", ":", "\n", "                ", "pdfs", "=", "self", ".", "policyNetwork", ".", "getDerivative", "(", "state", ",", "self", ".", "supportTorch", ".", "unsqueeze", "(", "1", ")", ")", "\n", "QValues", "=", "(", "pdfs", "*", "self", ".", "supportTorch", ")", ".", "sum", "(", "1", ")", "/", "(", "self", ".", "numberOfSamples", "*", "self", ".", "uniformProba", ")", "\n", "", "_", ",", "action", "=", "QValues", ".", "max", "(", "0", ")", "\n", "\n", "# If required, plot the return distribution associated with each action", "\n", "if", "plot", ":", "\n", "                ", "colors", "=", "[", "'blue'", ",", "'red'", ",", "'orange'", ",", "'green'", ",", "'purple'", ",", "'brown'", "]", "\n", "plt", ".", "figure", "(", ")", "\n", "ax1", "=", "plt", ".", "subplot", "(", "2", ",", "1", ",", "1", ")", "\n", "ax2", "=", "plt", ".", "subplot", "(", "2", ",", "1", ",", "2", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                    ", "accurateSupport", "=", "np", ".", "linspace", "(", "self", ".", "minReturn", ",", "self", ".", "maxReturn", ",", "self", ".", "numberOfSamples", "*", "10", ")", "\n", "accurateSupportTorch", "=", "torch", ".", "linspace", "(", "self", ".", "minReturn", ",", "self", ".", "maxReturn", ",", "self", ".", "numberOfSamples", "*", "10", ",", "device", "=", "self", ".", "device", ")", "\n", "cdfs", "=", "self", ".", "policyNetwork", "(", "state", ",", "accurateSupportTorch", ".", "unsqueeze", "(", "1", ")", ")", "\n", "pdfs", "=", "self", ".", "policyNetwork", ".", "getDerivative", "(", "state", ",", "accurateSupportTorch", ".", "unsqueeze", "(", "1", ")", ")", "\n", "QValues", "=", "(", "(", "pdfs", "*", "accurateSupportTorch", ")", ".", "sum", "(", "1", ")", ")", "/", "(", "self", ".", "numberOfSamples", "*", "10", "*", "self", ".", "uniformProba", ")", "\n", "", "for", "a", "in", "range", "(", "self", ".", "actionSpace", ")", ":", "\n", "                    ", "ax1", ".", "plot", "(", "accurateSupport", ",", "cdfs", "[", "a", "]", ".", "cpu", "(", ")", ",", "linestyle", "=", "'-'", ",", "label", "=", "''", ".", "join", "(", "[", "'Action '", ",", "str", "(", "a", ")", ",", "' random return Z'", "]", ")", ",", "color", "=", "colors", "[", "a", "]", ")", "\n", "ax2", ".", "plot", "(", "accurateSupport", ",", "pdfs", "[", "a", "]", ".", "cpu", "(", ")", ",", "linestyle", "=", "'-'", ",", "label", "=", "''", ".", "join", "(", "[", "'Action '", ",", "str", "(", "a", ")", ",", "' random return Z'", "]", ")", ",", "color", "=", "colors", "[", "a", "]", ")", "\n", "ax2", ".", "fill_between", "(", "accurateSupport", ",", "accurateSupport", "*", "0", ",", "pdfs", "[", "a", "]", ".", "cpu", "(", ")", ",", "alpha", "=", "0.25", ",", "color", "=", "colors", "[", "a", "]", ")", "\n", "ax1", ".", "axvline", "(", "x", "=", "QValues", "[", "a", "]", ",", "linewidth", "=", "2", ",", "linestyle", "=", "'--'", ",", "label", "=", "''", ".", "join", "(", "[", "'Action '", ",", "str", "(", "a", ")", ",", "' expected return Q'", "]", ")", ",", "color", "=", "colors", "[", "a", "]", ")", "\n", "ax2", ".", "axvline", "(", "x", "=", "QValues", "[", "a", "]", ",", "linewidth", "=", "2", ",", "linestyle", "=", "'--'", ",", "label", "=", "''", ".", "join", "(", "[", "'Action '", ",", "str", "(", "a", ")", ",", "' expected return Q'", "]", ")", ",", "color", "=", "colors", "[", "a", "]", ")", "\n", "", "ax1", ".", "set_xlabel", "(", "'Random return'", ")", "\n", "ax1", ".", "set_ylabel", "(", "'Cumulative Density Function (CDF)'", ")", "\n", "ax2", ".", "set_xlabel", "(", "'Random return'", ")", "\n", "ax2", ".", "set_ylabel", "(", "'Probability Density Function (PDF)'", ")", "\n", "ax1", ".", "legend", "(", ")", "\n", "ax2", ".", "legend", "(", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\"\"\"\n                # Saving of the data into external files\n                dataCDF = {\n                'Action0_x': accurateSupport,\n                'Action0_y': cdfs[0].cpu(),\n                'Action1_x': accurateSupport,\n                'Action1_y': cdfs[1].cpu(),\n                'Action2_x': accurateSupport,\n                'Action2_y': cdfs[2].cpu(),\n                'Action3_x': accurateSupport,\n                'Action3_y': cdfs[3].cpu(),\n                }\n                dataframeCDF = pd.DataFrame(dataCDF)\n                dataframeCDF.to_csv('Figures/Distributions/UMDQN_C.csv')\n                quit()\n                \"\"\"", "\n", "\n", "", "return", "action", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.UMDQN_C.UMDQN_C.learning": [[205, 265], ["len", "UMDQN_C.UMDQN_C.dataLoaderIter.next", "batch[].float().to", "batch[].float().to", "batch[].float().to", "batch[].float().to", "batch[].float().to", "UMDQN_C.UMDQN_C.policyNetwork", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.index_select().view", "torch.index_select().view", "torch.index_select().view", "torch.index_select().view", "torch.index_select().view", "torch.index_select().view", "torch.index_select().view", "torch.index_select().view", "torch.index_select().view", "UMDQN_C.UMDQN_C.max", "torch.mse_loss", "torch.mse_loss", "torch.mse_loss", "UMDQN_C.UMDQN_C.optimizer.zero_grad", "torch.mse_loss.backward", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "UMDQN_C.UMDQN_C.optimizer.step", "torch.mse_loss.item", "UMDQN_C.UMDQN_C.targetNetwork.getExpectation", "UMDQN_C.UMDQN_C.targetNetwork.getDerivative", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "batch[].float().to.view().repeat().view", "UMDQN_C.UMDQN_C.targetNetwork", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "range", "targetCdfs.view.view.view", "UMDQN_C.UMDQN_C.policyNetwork.parameters", "batch[].float", "batch[].float", "batch[].float", "batch[].float", "batch[].float", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "range", "batch[].float().to.view().repeat", "range", "batch[].float().to.view"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorld.StochasticGridWorld.step", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.UMDQN_C_Model_Atari.UMDQN_C_Model_Atari.getExpectation", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.UMDQN_C_Model_Atari.UMDQN_C_Model_Atari.getDerivative"], ["", "", "def", "learning", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Sample a batch of past experiences and learn from it\n              by updating the Reinforcement Learning policy.\n        \n        INPUTS: /\n        \n        OUTPUTS: - loss: Loss of the learning procedure.\n        \"\"\"", "\n", "\n", "# Check that the replay memory is filled enough", "\n", "if", "(", "len", "(", "self", ".", "replayMemory", ")", ">=", "self", ".", "batchSize", ")", ":", "\n", "\n", "# Sample a batch of experiences from the replay memory", "\n", "            ", "batch", "=", "self", ".", "dataLoaderIter", ".", "next", "(", ")", "\n", "state", "=", "batch", "[", "0", "]", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "action", "=", "batch", "[", "1", "]", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "reward", "=", "batch", "[", "2", "]", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "nextState", "=", "batch", "[", "3", "]", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "done", "=", "batch", "[", "4", "]", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "# Computation of the current return distribution, according to the policy DNN", "\n", "cdfs", "=", "self", ".", "policyNetwork", "(", "state", ",", "self", ".", "supportRepeatedBatchSize", ")", "\n", "selection", "=", "torch", ".", "tensor", "(", "[", "self", ".", "actionSpace", "*", "i", "+", "action", "[", "i", "]", "for", "i", "in", "range", "(", "self", ".", "batchSize", ")", "]", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "self", ".", "device", ")", "\n", "cdfs", "=", "torch", ".", "index_select", "(", "cdfs", ",", "0", ",", "selection", ")", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "\n", "# Computation of the next action, according to the policy DNN", "\n", "if", "self", ".", "fasterExpectation", ":", "\n", "                ", "expectedReturns", "=", "self", ".", "targetNetwork", ".", "getExpectation", "(", "nextState", ",", "self", ".", "minReturn", ",", "self", ".", "maxReturn", ",", "10", "*", "self", ".", "numberOfSamples", ")", "\n", "", "else", ":", "\n", "                ", "pdfs", "=", "self", ".", "targetNetwork", ".", "getDerivative", "(", "nextState", ",", "self", ".", "supportRepeatedBatchSize", ")", "\n", "expectedReturns", "=", "(", "(", "(", "pdfs", "*", "self", ".", "supportTorch", ")", ".", "sum", "(", "1", ")", ")", "/", "(", "self", ".", "numberOfSamples", "*", "self", ".", "uniformProba", ")", ")", ".", "view", "(", "-", "1", ",", "self", ".", "actionSpace", ")", "\n", "", "_", ",", "nextAction", "=", "expectedReturns", ".", "max", "(", "1", ")", "\n", "\n", "# Computation of the new distribution to be learnt by the policy DNN", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "r", "=", "reward", ".", "view", "(", "self", ".", "batchSize", ",", "1", ")", ".", "repeat", "(", "1", ",", "self", ".", "numberOfSamples", ")", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "support", "=", "(", "self", ".", "supportRepeatedBatchSize", "-", "r", ")", "/", "self", ".", "gamma", "\n", "targetCdfs", "=", "self", ".", "targetNetwork", "(", "nextState", ",", "support", ")", "\n", "selection", "=", "torch", ".", "tensor", "(", "[", "self", ".", "actionSpace", "*", "i", "+", "nextAction", "[", "i", "]", "for", "i", "in", "range", "(", "self", ".", "batchSize", ")", "]", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "self", ".", "device", ")", "\n", "targetCdfs", "=", "torch", ".", "index_select", "(", "targetCdfs", ",", "0", ",", "selection", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "batchSize", ")", ":", "\n", "                    ", "if", "done", "[", "i", "]", "==", "1", ":", "\n", "                        ", "targetCdfs", "[", "i", "]", "=", "(", "self", ".", "supportTorch", ">", "reward", "[", "i", "]", ")", ".", "float", "(", ")", "\n", "", "", "targetCdfs", "=", "targetCdfs", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "\n", "# Compute the loss", "\n", "", "loss", "=", "F", ".", "mse_loss", "(", "cdfs", ",", "targetCdfs", ")", "\n", "\n", "# Computation of the gradients", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "\n", "# Gradient Clipping", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "policyNetwork", ".", "parameters", "(", ")", ",", "self", ".", "gradientClipping", ")", "\n", "\n", "# Perform the Deep Neural Network optimization", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "return", "loss", ".", "item", "(", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.AtariWrapper.NoopWrapper.__init__": [[35, 49], ["gym.Wrapper.__init__"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.MonotonicNN.__init__"], ["def", "__init__", "(", "self", ",", "env", ",", "maxNoop", "=", "30", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Initialization of the wrapped environment.\n        \n        INPUTS: - env: Environment to wrap.\n                - maxNoop: Maximum number of Noop actions to execute at reset.\n        \n        OUTPUTS: /\n        \"\"\"", "\n", "\n", "gym", ".", "Wrapper", ".", "__init__", "(", "self", ",", "env", ")", "\n", "\n", "# Initialization of important variables", "\n", "self", ".", "maxNoop", "=", "maxNoop", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.AtariWrapper.NoopWrapper.reset": [[51, 71], ["AtariWrapper.NoopWrapper.env.reset", "numpy.random.randint", "range", "AtariWrapper.NoopWrapper.env.step", "AtariWrapper.NoopWrapper.env.reset"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorld.StochasticGridWorld.reset", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorld.StochasticGridWorld.step", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorld.StochasticGridWorld.reset"], ["", "def", "reset", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Reset the environment, with the execution of Noop actions.\n        \n        INPUTS: - kwargs: Parameters for the resetting of the wrapped environment.\n        \n        OUTPUTS: - state: RL state.\n        \"\"\"", "\n", "\n", "# Resetting of the wrapped environment", "\n", "self", ".", "env", ".", "reset", "(", "**", "kwargs", ")", "\n", "\n", "# Execution of a random number of Noop actions", "\n", "numberNoop", "=", "np", ".", "random", ".", "randint", "(", "1", ",", "self", ".", "maxNoop", "+", "1", ")", "\n", "for", "_", "in", "range", "(", "numberNoop", ")", ":", "\n", "            ", "state", ",", "_", ",", "done", ",", "_", "=", "self", ".", "env", ".", "step", "(", "0", ")", "\n", "if", "done", ":", "\n", "                ", "state", "=", "self", ".", "env", ".", "reset", "(", "**", "kwargs", ")", "\n", "\n", "", "", "return", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.AtariWrapper.NoopWrapper.step": [[73, 86], ["AtariWrapper.NoopWrapper.env.step"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorld.StochasticGridWorld.step"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Execute a specified action in the wrapped environment.\n        \n        INPUTS: - action: Action to be executed.\n        \n        OUTPUTS: - state: RL state.\n                 - reward: RL reward.\n                 - done: RL episode termination signal.\n                 - info: Additional information (optional).\n        \"\"\"", "\n", "\n", "return", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.AtariWrapper.SkipWrapper.__init__": [[105, 123], ["gym.Wrapper.__init__", "numpy.zeros"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.MonotonicNN.__init__"], ["def", "__init__", "(", "self", ",", "env", ",", "skip", "=", "4", ",", "stickyActionsProba", "=", "0", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Initialization of the wrapped environment.\n        \n        INPUTS: - env: Environment to wrap.\n                - skip: Number of frames to skip.\n                - stickyActionsProba: Probability associated with sticky actions.\n        \n        OUTPUTS: /\n        \"\"\"", "\n", "\n", "gym", ".", "Wrapper", ".", "__init__", "(", "self", ",", "env", ")", "\n", "\n", "# Initialization of important variables", "\n", "self", ".", "skip", "=", "skip", "\n", "self", ".", "buffer", "=", "np", ".", "zeros", "(", "(", "2", ",", ")", "+", "env", ".", "observation_space", ".", "shape", ",", "dtype", "=", "env", ".", "observation_space", ".", "dtype", ")", "\n", "self", ".", "stickyActionsProba", "=", "stickyActionsProba", "\n", "self", ".", "previousAction", "=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.AtariWrapper.SkipWrapper.step": [[125, 166], ["range", "AtariWrapper.SkipWrapper.buffer.max", "random.random", "AtariWrapper.SkipWrapper.env.step", "AtariWrapper.SkipWrapper.env.step"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorld.StochasticGridWorld.step", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorld.StochasticGridWorld.step"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Execute a specified action in the wrapped environment, taking into\n              account the skipped frames and the sticky actions.\n        \n        INPUTS: - action: Action to be executed.\n        \n        OUTPUTS: - state: RL state.\n                 - reward: RL reward.\n                 - done: RL episode termination signal.\n                 - info: Additional information (optional).\n        \"\"\"", "\n", "\n", "# Initialization of variables", "\n", "totalReward", "=", "0.0", "\n", "\n", "# Loop through the frames to skip", "\n", "for", "i", "in", "range", "(", "self", ".", "skip", ")", ":", "\n", "\n", "# Sticky actions", "\n", "            ", "if", "random", ".", "random", "(", ")", ">", "self", ".", "stickyActionsProba", ":", "\n", "                ", "state", ",", "reward", ",", "done", ",", "info", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "self", ".", "previousAction", "=", "action", "\n", "", "else", ":", "\n", "                ", "state", ",", "reward", ",", "done", ",", "info", "=", "self", ".", "env", ".", "step", "(", "self", ".", "previousAction", ")", "\n", "\n", "# Storing of the two last frames into the buffer", "\n", "", "if", "i", "==", "self", ".", "skip", "-", "2", ":", "self", ".", "buffer", "[", "0", "]", "=", "state", "\n", "if", "i", "==", "self", ".", "skip", "-", "1", ":", "self", ".", "buffer", "[", "1", "]", "=", "state", "\n", "\n", "# Aggregation of the RL variables", "\n", "totalReward", "+=", "reward", "\n", "\n", "# Early breaking if termination signal", "\n", "if", "done", ":", "\n", "                ", "break", "\n", "\n", "# Pixel-wise maximum of the last two frames", "\n", "", "", "newState", "=", "self", ".", "buffer", ".", "max", "(", "axis", "=", "0", ")", "\n", "\n", "return", "newState", ",", "totalReward", ",", "done", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.AtariWrapper.SkipWrapper.reset": [[168, 183], ["AtariWrapper.SkipWrapper.env.reset"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorld.StochasticGridWorld.reset"], ["", "def", "reset", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Reset the environment.\n        \n        INPUTS: - kwargs: Parameters for the resetting of the wrapped environment.\n        \n        OUTPUTS: - state: RL state.\n        \"\"\"", "\n", "# Resetting of the wrapped environment", "\n", "state", "=", "self", ".", "env", ".", "reset", "(", "**", "kwargs", ")", "\n", "\n", "# Resetting of the previous action (sticky actions technique)", "\n", "self", ".", "previousAction", "=", "1", "\n", "\n", "return", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.AtariWrapper.FireResetWrapper.__init__": [[202, 212], ["gym.Wrapper.__init__"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.MonotonicNN.__init__"], ["def", "__init__", "(", "self", ",", "env", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Initialization of the wrapped environment.\n        \n        INPUTS: - env: Environment to wrap.\n        \n        OUTPUTS: /\n        \"\"\"", "\n", "\n", "gym", ".", "Wrapper", ".", "__init__", "(", "self", ",", "env", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.AtariWrapper.FireResetWrapper.reset": [[214, 232], ["AtariWrapper.FireResetWrapper.env.reset", "AtariWrapper.FireResetWrapper.env.step", "AtariWrapper.FireResetWrapper.env.reset"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorld.StochasticGridWorld.reset", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorld.StochasticGridWorld.step", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorld.StochasticGridWorld.reset"], ["", "def", "reset", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Reset the environment, with the execution of \"Fire\" action.\n        \n        INPUTS: - kwargs: Parameters for the resetting of the wrapped environment.\n        \n        OUTPUTS: - state: RL state.\n        \"\"\"", "\n", "\n", "# Resetting of the wrapped environment", "\n", "self", ".", "env", ".", "reset", "(", "**", "kwargs", ")", "\n", "\n", "# Execution of the \"Fire\" action", "\n", "state", ",", "_", ",", "done", ",", "_", "=", "self", ".", "env", ".", "step", "(", "1", ")", "\n", "if", "done", ":", "\n", "            ", "state", "=", "self", ".", "env", ".", "reset", "(", "**", "kwargs", ")", "\n", "\n", "", "return", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.AtariWrapper.FireResetWrapper.step": [[234, 257], ["AtariWrapper.FireResetWrapper.env.step", "range", "AtariWrapper.FireResetWrapper.env.step"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorld.StochasticGridWorld.step", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorld.StochasticGridWorld.step"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Execute a specified action in the wrapped environment.\n        \n        INPUTS: - action: Action to be executed.\n        \n        OUTPUTS: - state: RL state.\n                 - reward: RL reward.\n                 - done: RL episode termination signal.\n                 - info: Additional information (optional).\n        \"\"\"", "\n", "\n", "# Normal step function with the chosen action", "\n", "state", ",", "reward", ",", "done", ",", "info", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "\n", "# Execution of the \"FIRE\" action if a loss of life happens", "\n", "if", "not", "done", "and", "info", "[", "'lossOfLife'", "]", "==", "True", ":", "\n", "            ", "for", "_", "in", "range", "(", "3", ")", ":", "# For countering sticky actions", "\n", "                ", "state", ",", "_", ",", "done", ",", "_", "=", "self", ".", "env", ".", "step", "(", "1", ")", "\n", "if", "done", ":", "\n", "                    ", "return", "state", ",", "reward", ",", "done", ",", "info", "\n", "\n", "", "", "", "return", "state", ",", "reward", ",", "done", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.AtariWrapper.LossOfLifeWrapper.__init__": [[276, 289], ["gym.Wrapper.__init__"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.MonotonicNN.__init__"], ["def", "__init__", "(", "self", ",", "env", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Initialization of the wrapped environment.\n        \n        INPUTS: - env: Environment to wrap.\n        \n        OUTPUTS: /\n        \"\"\"", "\n", "\n", "gym", ".", "Wrapper", ".", "__init__", "(", "self", ",", "env", ")", "\n", "\n", "# Initialization of important variables", "\n", "self", ".", "numberOfLives", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.AtariWrapper.LossOfLifeWrapper.reset": [[291, 307], ["AtariWrapper.LossOfLifeWrapper.env.reset", "AtariWrapper.LossOfLifeWrapper.env.unwrapped.ale.lives"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorld.StochasticGridWorld.reset"], ["", "def", "reset", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Reset the environment, with the execution of Noop actions.\n        \n        INPUTS: - kwargs: Parameters for the resetting of the wrapped environment.\n        \n        OUTPUTS: - state: RL state.\n        \"\"\"", "\n", "\n", "# Reset of the wrapped environment", "\n", "state", "=", "self", ".", "env", ".", "reset", "(", "**", "kwargs", ")", "\n", "\n", "# Update of the number of lives for the agent", "\n", "self", ".", "numberOfLives", "=", "self", ".", "env", ".", "unwrapped", ".", "ale", ".", "lives", "(", ")", "\n", "\n", "return", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.AtariWrapper.LossOfLifeWrapper.step": [[309, 337], ["AtariWrapper.LossOfLifeWrapper.env.step", "AtariWrapper.LossOfLifeWrapper.env.unwrapped.ale.lives"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorld.StochasticGridWorld.step"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Execute a specified action in the wrapped environment.\n        \n        INPUTS: - action: Action to be executed.\n        \n        OUTPUTS: - state: RL state.\n                 - reward: RL reward.\n                 - done: RL episode termination signal.\n                 - info: Additional information (optional).\n        \"\"\"", "\n", "\n", "# Step for the wrapped environment", "\n", "state", ",", "reward", ",", "done", ",", "info", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "\n", "# Check for the loss of life", "\n", "lives", "=", "self", ".", "env", ".", "unwrapped", ".", "ale", ".", "lives", "(", ")", "\n", "lossOfLife", "=", "False", "\n", "if", "lives", "<", "self", ".", "numberOfLives", ":", "\n", "            ", "lossOfLife", "=", "True", "\n", "\n", "# Update of the number of lives for the agent", "\n", "", "self", ".", "numberOfLives", "=", "lives", "\n", "\n", "# Include the \"loss of life\" information into the info variable", "\n", "info", "[", "'lossOfLife'", "]", "=", "lossOfLife", "\n", "\n", "return", "state", ",", "reward", ",", "done", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.AtariWrapper.ClipRewardWrapper.__init__": [[354, 364], ["gym.RewardWrapper.__init__"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.MonotonicNN.__init__"], ["def", "__init__", "(", "self", ",", "env", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Initialization of the wrapped environment.\n        \n        INPUTS: - env: Environment to wrap.\n        \n        OUTPUTS: /\n        \"\"\"", "\n", "\n", "gym", ".", "RewardWrapper", ".", "__init__", "(", "self", ",", "env", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.AtariWrapper.ClipRewardWrapper.reward": [[366, 376], ["numpy.sign"], "methods", ["None"], ["", "def", "reward", "(", "self", ",", "reward", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Process the RL reward.\n        \n        INPUTS: - reward: RL reward to be processed.\n        \n        OUTPUTS: - reward: RL reward processed.\n        \"\"\"", "\n", "\n", "return", "np", ".", "sign", "(", "reward", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.AtariWrapper.FrameWrapper.__init__": [[393, 406], ["gym.ObservationWrapper.__init__", "gym.spaces.Box"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.MonotonicNN.__init__"], ["def", "__init__", "(", "self", ",", "env", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Initialization of the wrapped environment.\n        \n        INPUTS: - env: Environment to wrap.\n        \n        OUTPUTS: /\n        \"\"\"", "\n", "\n", "gym", ".", "ObservationWrapper", ".", "__init__", "(", "self", ",", "env", ")", "\n", "\n", "# Initialization of the new observation space (state space)", "\n", "self", ".", "observation_space", "=", "gym", ".", "spaces", ".", "Box", "(", "low", "=", "0", ",", "high", "=", "255", ",", "shape", "=", "(", "84", ",", "84", ",", "1", ")", ",", "dtype", "=", "env", ".", "observation_space", ".", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.AtariWrapper.FrameWrapper.observation": [[408, 420], ["cv2.cvtColor", "cv2.resize"], "methods", ["None"], ["", "def", "observation", "(", "self", ",", "frame", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Process the frame.\n        \n        INPUTS: - frame: Frame to be processed.\n        \n        OUTPUTS: - state: RL state after processing.\n        \"\"\"", "\n", "\n", "frame", "=", "cv2", ".", "cvtColor", "(", "frame", ",", "cv2", ".", "COLOR_RGB2GRAY", ")", "\n", "frame", "=", "cv2", ".", "resize", "(", "frame", ",", "(", "84", ",", "84", ")", ",", "interpolation", "=", "cv2", ".", "INTER_AREA", ")", "\n", "return", "frame", "[", ":", ",", ":", ",", "None", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.AtariWrapper.NormalizationWrapper.__init__": [[437, 450], ["gym.ObservationWrapper.__init__", "gym.spaces.Box"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.MonotonicNN.__init__"], ["def", "__init__", "(", "self", ",", "env", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Initialization of the wrapped environment.\n        \n        INPUTS: - env: Environment to wrap.\n        \n        OUTPUTS: /\n        \"\"\"", "\n", "\n", "gym", ".", "ObservationWrapper", ".", "__init__", "(", "self", ",", "env", ")", "\n", "\n", "# Initialization of the new observation space (state space)", "\n", "self", ".", "observation_space", "=", "gym", ".", "spaces", ".", "Box", "(", "low", "=", "0.0", ",", "high", "=", "1.0", ",", "shape", "=", "env", ".", "observation_space", ".", "shape", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.AtariWrapper.NormalizationWrapper.observation": [[452, 462], ["numpy.array().astype", "numpy.array"], "methods", ["None"], ["", "def", "observation", "(", "self", ",", "frame", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Process the frame.\n        \n        INPUTS: - frame: Frame to be processed.\n        \n        OUTPUTS: - state: RL state after processing.\n        \"\"\"", "\n", "\n", "return", "np", ".", "array", "(", "frame", ")", ".", "astype", "(", "np", ".", "float32", ")", "/", "255.0", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.AtariWrapper.StackingWrapper.__init__": [[482, 501], ["gym.Wrapper.__init__", "collections.deque", "gym.spaces.Box"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.MonotonicNN.__init__"], ["def", "__init__", "(", "self", ",", "env", ",", "numberOfFrames", "=", "4", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Initialization of the wrapped environment.\n        \n        INPUTS: - env: Environment to wrap.\n                - numberOfFrames: Number of frames to be stacked.\n        \n        OUTPUTS: /\n        \"\"\"", "\n", "\n", "gym", ".", "Wrapper", ".", "__init__", "(", "self", ",", "env", ")", "\n", "\n", "# Initialization of important variables", "\n", "self", ".", "numberOfFrames", "=", "numberOfFrames", "\n", "self", ".", "frames", "=", "deque", "(", "[", "]", ",", "maxlen", "=", "numberOfFrames", ")", "\n", "\n", "# Setting of the new observation space (state space)", "\n", "space", "=", "env", ".", "observation_space", ".", "shape", "\n", "self", ".", "observation_space", "=", "gym", ".", "spaces", ".", "Box", "(", "low", "=", "0", ",", "high", "=", "255", ",", "shape", "=", "(", "space", "[", "0", "]", ",", "space", "[", "1", "]", ",", "space", "[", "2", "]", "*", "numberOfFrames", ")", ",", "dtype", "=", "env", ".", "observation_space", ".", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.AtariWrapper.StackingWrapper.reset": [[503, 520], ["AtariWrapper.StackingWrapper.env.reset", "range", "AtariWrapper.LazyFrames", "AtariWrapper.StackingWrapper.frames.append", "list"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorld.StochasticGridWorld.reset"], ["", "def", "reset", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Reset the wrapped environment.\n        \n        INPUTS: - kwargs: Parameters for the resetting of the wrapped environment.\n        \n        OUTPUTS: - state: RL state.\n        \"\"\"", "\n", "\n", "# Resetting of the wrapped environment", "\n", "state", "=", "self", ".", "env", ".", "reset", "(", "**", "kwargs", ")", "\n", "\n", "# Storing of the first frame n times", "\n", "for", "_", "in", "range", "(", "self", ".", "numberOfFrames", ")", ":", "\n", "            ", "self", ".", "frames", ".", "append", "(", "state", ")", "\n", "\n", "", "return", "LazyFrames", "(", "list", "(", "self", ".", "frames", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.AtariWrapper.StackingWrapper.step": [[522, 541], ["AtariWrapper.StackingWrapper.env.step", "AtariWrapper.StackingWrapper.frames.append", "AtariWrapper.LazyFrames", "list"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorld.StochasticGridWorld.step"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Execute a specified action in the wrapped environment.\n        \n        INPUTS: - action: Action to be executed.\n        \n        OUTPUTS: - state: RL state.\n                 - reward: RL reward.\n                 - done: RL episode termination signal.\n                 - info: Additional information (optional).\n        \"\"\"", "\n", "\n", "# Execution of the action specified", "\n", "state", ",", "reward", ",", "done", ",", "info", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "\n", "# Appending of the new frame resulting from this action", "\n", "self", ".", "frames", ".", "append", "(", "state", ")", "\n", "\n", "return", "LazyFrames", "(", "list", "(", "self", ".", "frames", ")", ")", ",", "reward", ",", "done", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.AtariWrapper.LazyFrames.__init__": [[555, 559], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "frames", ")", ":", "\n", "\n", "        ", "self", ".", "_frames", "=", "frames", "\n", "self", ".", "_out", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.AtariWrapper.LazyFrames._force": [[561, 566], ["numpy.concatenate"], "methods", ["None"], ["", "def", "_force", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_out", "is", "None", ":", "\n", "            ", "self", ".", "_out", "=", "np", ".", "concatenate", "(", "self", ".", "_frames", ",", "axis", "=", "2", ")", "\n", "self", ".", "_frames", "=", "None", "\n", "", "return", "self", ".", "_out", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.AtariWrapper.LazyFrames.__array__": [[568, 573], ["AtariWrapper.LazyFrames._force", "out.astype.astype.astype"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.AtariWrapper.LazyFrames._force"], ["", "def", "__array__", "(", "self", ",", "dtype", "=", "None", ")", ":", "\n", "        ", "out", "=", "self", ".", "_force", "(", ")", "\n", "if", "dtype", "is", "not", "None", ":", "\n", "            ", "out", "=", "out", ".", "astype", "(", "dtype", ")", "\n", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.AtariWrapper.LazyFrames.__len__": [[575, 577], ["len", "AtariWrapper.LazyFrames._force"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.AtariWrapper.LazyFrames._force"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "_force", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.AtariWrapper.LazyFrames.__getitem__": [[579, 581], ["AtariWrapper.LazyFrames._force"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.AtariWrapper.LazyFrames._force"], ["", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "return", "self", ".", "_force", "(", ")", "[", "i", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.AtariWrapper.PytorchWrapper.__init__": [[599, 613], ["gym.ObservationWrapper.__init__", "gym.spaces.Box"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.MonotonicNN.__init__"], ["def", "__init__", "(", "self", ",", "env", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Initialization of the wrapped environment.\n        \n        INPUTS: - env: Environment to wrap.\n        \n        OUTPUTS: /\n        \"\"\"", "\n", "\n", "gym", ".", "ObservationWrapper", ".", "__init__", "(", "self", ",", "env", ")", "\n", "\n", "# Initialization of the new observation space (state space)", "\n", "space", "=", "self", ".", "observation_space", ".", "shape", "\n", "self", ".", "observation_space", "=", "gym", ".", "spaces", ".", "Box", "(", "low", "=", "0.0", ",", "high", "=", "1.0", ",", "shape", "=", "(", "space", "[", "-", "1", "]", ",", "space", "[", "0", "]", ",", "space", "[", "1", "]", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.AtariWrapper.PytorchWrapper.observation": [[615, 625], ["numpy.moveaxis"], "methods", ["None"], ["", "def", "observation", "(", "self", ",", "observation", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Process the observation.\n        \n        INPUTS: - observation: Observation to be processed.\n        \n        OUTPUTS: - state: RL state after processing.\n        \"\"\"", "\n", "\n", "return", "np", ".", "moveaxis", "(", "observation", ",", "2", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.AtariWrapper.AtariWrapper.wrapper": [[642, 669], ["gym.make", "AtariWrapper.NoopWrapper", "AtariWrapper.SkipWrapper", "AtariWrapper.LossOfLifeWrapper", "AtariWrapper.FireResetWrapper", "AtariWrapper.ClipRewardWrapper", "AtariWrapper.FrameWrapper", "AtariWrapper.NormalizationWrapper", "AtariWrapper.StackingWrapper", "AtariWrapper.PytorchWrapper"], "methods", ["None"], ["def", "wrapper", "(", "self", ",", "environment", ",", "maxNoop", "=", "30", ",", "skip", "=", "4", ",", "numberOfFramesStacked", "=", "4", ",", "stickyActionsProba", "=", "0", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Wrapping of the OpenAI gym {}NoFramekip-v4 environment.\n        \n        INPUTS: - environment: Name of the environment ({}NoFramekip-v4).\n                - maxNoop: Maximum number of Noop actions to execute at reset.\n                - skip: Number of frames to skip.\n                - numberOfFramesStacked: Number of frames to be stacked.\n                - stickyActionsProba: Probability associated with sticky actions.\n        \n        OUTPUTS: env: Wrapped environment.\n        \"\"\"", "\n", "\n", "# Creation of the OpenAI gym environment.", "\n", "env", "=", "gym", ".", "make", "(", "environment", ")", "\n", "\n", "# Application of the necessary wrappers.", "\n", "env", "=", "NoopWrapper", "(", "env", ",", "maxNoop", "=", "maxNoop", ")", "\n", "env", "=", "SkipWrapper", "(", "env", ",", "skip", "=", "skip", ",", "stickyActionsProba", "=", "stickyActionsProba", ")", "\n", "env", "=", "LossOfLifeWrapper", "(", "env", ")", "\n", "env", "=", "FireResetWrapper", "(", "env", ")", "\n", "env", "=", "ClipRewardWrapper", "(", "env", ")", "\n", "env", "=", "FrameWrapper", "(", "env", ")", "\n", "env", "=", "NormalizationWrapper", "(", "env", ")", "\n", "env", "=", "StackingWrapper", "(", "env", ",", "numberOfFrames", "=", "numberOfFramesStacked", ")", "\n", "env", "=", "PytorchWrapper", "(", "env", ")", "\n", "return", "env", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.AtariWrapper.MinAtarWrapper.wrapper": [[685, 700], ["gym.make", "AtariWrapper.PytorchWrapper"], "methods", ["None"], ["def", "wrapper", "(", "self", ",", "environment", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Wrapping of the OpenAI gym {}NoFramekip-v4 environment.\n        \n        INPUTS: - environment: Name of the environment (MinAtar).\n        \n        OUTPUTS: env: Wrapped environment.\n        \"\"\"", "\n", "\n", "# Creation of the OpenAI gym environment.", "\n", "env", "=", "gym", ".", "make", "(", "environment", ")", "\n", "\n", "# Wrapping of the environment", "\n", "env", "=", "PytorchWrapper", "(", "env", ")", "\n", "return", "env", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.MonteCarloDistributions.MonteCarloDistributions.__init__": [[46, 61], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "environment", ",", "policy", ",", "gamma", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Perform the initialization of the class.\n        \n        INPUTS: - environment: Environment analysed.\n                - policy: Policy analysed.\n                - gamma: Discount factor.\n        \n        OUTPUTS: /\n        \"\"\"", "\n", "\n", "# Initialization of important variables", "\n", "self", ".", "environment", "=", "environment", "\n", "self", ".", "policy", "=", "policy", "\n", "self", ".", "gamma", "=", "gamma", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.MonteCarloDistributions.MonteCarloDistributions.samplingMonteCarlo": [[63, 114], ["range", "MonteCarloDistributions.MonteCarloDistributions.environment.reset", "MonteCarloDistributions.MonteCarloDistributions.environment.setState", "MonteCarloDistributions.MonteCarloDistributions.environment.step", "samples.append", "MonteCarloDistributions.MonteCarloDistributions.policy.processState", "MonteCarloDistributions.MonteCarloDistributions.policy.chooseAction", "MonteCarloDistributions.MonteCarloDistributions.environment.step"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorld.StochasticGridWorld.reset", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorld.StochasticGridWorld.setState", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorld.StochasticGridWorld.step", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorldOptimal.StochasticGridWorldOptimal.processState", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorldOptimal.StochasticGridWorldOptimal.chooseAction", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorld.StochasticGridWorld.step"], ["", "def", "samplingMonteCarlo", "(", "self", ",", "initialState", ",", "initialAction", ",", "numberOfSamples", "=", "numberOfSamples", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Collect Monte Carlo samples of the expected return associated\n              with the state and action specified.\n        \n        INPUTS: - initialState: RL state to start from.\n                - initialAction: RL action to start from.\n                \n                - numberOfSamples: Number of Monte Carlo samples to collect.\n        \n        OUTPUTS: - samples: Monte Carlo samples collected.\n        \"\"\"", "\n", "\n", "# Initialization of the memory storing the MC samples", "\n", "samples", "=", "[", "]", "\n", "\n", "# Generation of the MC samples", "\n", "for", "_i", "in", "range", "(", "numberOfSamples", ")", ":", "\n", "\n", "# Initialization of some variables", "\n", "            ", "expectedReturn", "=", "0", "\n", "step", "=", "0", "\n", "\n", "# Reset of the environment and initialization to the desired state", "\n", "self", ".", "environment", ".", "reset", "(", ")", "\n", "state", "=", "self", ".", "environment", ".", "setState", "(", "initialState", ")", "\n", "\n", "# Execution of the action specified", "\n", "nextState", ",", "reward", ",", "done", ",", "info", "=", "self", ".", "environment", ".", "step", "(", "initialAction", ")", "\n", "\n", "# Update of the expected return", "\n", "expectedReturn", "+=", "(", "reward", "*", "(", "self", ".", "gamma", "**", "step", ")", ")", "\n", "step", "+=", "1", "\n", "\n", "# Loop until episode termination", "\n", "while", "done", "==", "0", ":", "\n", "\n", "# Execute the next ation according to the policy selected", "\n", "                ", "state", "=", "self", ".", "policy", ".", "processState", "(", "nextState", ")", "\n", "policyAction", "=", "self", ".", "policy", ".", "chooseAction", "(", "state", ",", "plot", "=", "False", ")", "\n", "nextState", ",", "reward", ",", "done", ",", "info", "=", "self", ".", "environment", ".", "step", "(", "policyAction", ")", "\n", "\n", "# Update of the expected return", "\n", "expectedReturn", "+=", "(", "reward", "*", "(", "self", ".", "gamma", "**", "step", ")", ")", "\n", "step", "+=", "1", "\n", "\n", "# Add the MC sample to the memory", "\n", "", "samples", ".", "append", "(", "expectedReturn", ")", "\n", "\n", "# Output the MC samples collected", "\n", "", "return", "samples", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.MonteCarloDistributions.MonteCarloDistributions.plotDistributions": [[116, 252], ["range", "matplotlib.pyplot.figure", "matplotlib.pyplot.subplot", "range", "matplotlib.pyplot.subplot.set_xlabel", "matplotlib.pyplot.subplot.set_ylabel", "matplotlib.pyplot.subplot.set", "matplotlib.pyplot.subplot", "range", "matplotlib.pyplot.subplot.set_xlabel", "matplotlib.pyplot.subplot.set_ylabel", "matplotlib.pyplot.subplot.set", "matplotlib.pyplot.subplot", "matplotlib.pyplot.hist", "matplotlib.pyplot.hist", "matplotlib.pyplot.hist", "matplotlib.pyplot.hist", "matplotlib.pyplot.subplot.clear", "matplotlib.pyplot.subplot.plot", "matplotlib.pyplot.subplot.plot", "matplotlib.pyplot.subplot.plot", "matplotlib.pyplot.subplot.plot", "matplotlib.pyplot.subplot.set_xlabel", "matplotlib.pyplot.subplot.set_ylabel", "matplotlib.pyplot.subplot.set", "matplotlib.pyplot.subplot.legend", "matplotlib.pyplot.savefig", "matplotlib.pyplot.figure", "matplotlib.pyplot.subplot", "range", "matplotlib.pyplot.subplot.set_xlabel", "matplotlib.pyplot.subplot.set_ylabel", "matplotlib.pyplot.subplot.set", "matplotlib.pyplot.savefig", "matplotlib.pyplot.figure", "matplotlib.pyplot.subplot", "range", "matplotlib.pyplot.subplot.set_xlabel", "matplotlib.pyplot.subplot.set_ylabel", "matplotlib.pyplot.subplot.set", "matplotlib.pyplot.savefig", "matplotlib.pyplot.figure", "matplotlib.pyplot.subplot", "matplotlib.pyplot.hist", "matplotlib.pyplot.hist", "matplotlib.pyplot.hist", "matplotlib.pyplot.hist", "matplotlib.pyplot.subplot.clear", "matplotlib.pyplot.subplot.plot", "matplotlib.pyplot.subplot.plot", "matplotlib.pyplot.subplot.plot", "matplotlib.pyplot.subplot.plot", "matplotlib.pyplot.subplot.set_xlabel", "matplotlib.pyplot.subplot.set_ylabel", "matplotlib.pyplot.subplot.set", "matplotlib.pyplot.savefig", "matplotlib.pyplot.hist", "matplotlib.pyplot.hist", "matplotlib.pyplot.hist", "matplotlib.pyplot.hist", "matplotlib.pyplot.hist", "matplotlib.pyplot.hist", "matplotlib.pyplot.hist", "matplotlib.pyplot.hist", "pandas.DataFrame", "pandas.DataFrame", "pandas.DataFrame", "pandas.DataFrame.to_csv", "pandas.DataFrame.to_csv", "pandas.DataFrame.to_csv", "samples.append", "matplotlib.pyplot.hist", "matplotlib.pyplot.hist", "matplotlib.pyplot.hist", "matplotlib.pyplot.hist", "MonteCarloDistributions.MonteCarloDistributions.samplingMonteCarlo"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.MonteCarloDistributions.MonteCarloDistributions.samplingMonteCarlo"], ["", "def", "plotDistributions", "(", "self", ",", "state", ",", "numberOfSamples", "=", "numberOfSamples", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Collect Monte Carlo samples of the expected return associated\n              with the state and action specified.\n        \n        INPUTS: - state: RL state to start from.\n                - numberOfSamples: Number of Monte Carlo samples to collect.\n        \n        OUTPUTS: /\n        \"\"\"", "\n", "\n", "# Generation of the Monte Carlo samples", "\n", "samples", "=", "[", "]", "\n", "actions", "=", "4", "\n", "for", "action", "in", "range", "(", "actions", ")", ":", "\n", "            ", "samples", ".", "append", "(", "self", ".", "samplingMonteCarlo", "(", "state", ",", "action", ",", "numberOfSamples", ")", ")", "\n", "\n", "# Initialization of the figure", "\n", "", "colors", "=", "[", "'blue'", ",", "'red'", ",", "'orange'", ",", "'green'", ",", "'purple'", ",", "'brown'", "]", "\n", "fig", "=", "plt", ".", "figure", "(", ")", "\n", "\n", "# Plotting of the PDF of the random return", "\n", "ax1", "=", "plt", ".", "subplot", "(", "3", ",", "1", ",", "1", ")", "\n", "for", "action", "in", "range", "(", "actions", ")", ":", "\n", "            ", "plt", ".", "hist", "(", "samples", "[", "action", "]", ",", "bins", "=", "bins", ",", "density", "=", "density", ",", "range", "=", "plotRange", ",", "histtype", "=", "histtype", ",", "color", "=", "colors", "[", "action", "]", ")", "\n", "", "ax1", ".", "set_xlabel", "(", "'Random return'", ")", "\n", "ax1", ".", "set_ylabel", "(", "'PDF'", ")", "\n", "ax1", ".", "set", "(", "xlim", "=", "(", "-", "2", ",", "2", ")", ")", "\n", "\n", "# Plotting of the CDF of the random return", "\n", "ax2", "=", "plt", ".", "subplot", "(", "3", ",", "1", ",", "2", ")", "\n", "for", "action", "in", "range", "(", "actions", ")", ":", "\n", "            ", "plt", ".", "hist", "(", "samples", "[", "action", "]", ",", "bins", "=", "bins", ",", "density", "=", "density", ",", "range", "=", "plotRange", ",", "histtype", "=", "histtype", ",", "cumulative", "=", "True", ",", "color", "=", "colors", "[", "action", "]", ")", "\n", "", "ax2", ".", "set_xlabel", "(", "'Random return'", ")", "\n", "ax2", ".", "set_ylabel", "(", "'CDF'", ")", "\n", "ax2", ".", "set", "(", "xlim", "=", "(", "-", "2", ",", "2", ")", ")", "\n", "\n", "# Plotting of the QF of the random return", "\n", "ax3", "=", "plt", ".", "subplot", "(", "3", ",", "1", ",", "3", ")", "\n", "CDF0", "=", "plt", ".", "hist", "(", "samples", "[", "0", "]", ",", "bins", "=", "bins", ",", "density", "=", "density", ",", "range", "=", "plotRange", ",", "histtype", "=", "histtype", ",", "cumulative", "=", "True", ",", "color", "=", "'white'", ")", "\n", "CDF1", "=", "plt", ".", "hist", "(", "samples", "[", "1", "]", ",", "bins", "=", "bins", ",", "density", "=", "density", ",", "range", "=", "plotRange", ",", "histtype", "=", "histtype", ",", "cumulative", "=", "True", ",", "color", "=", "'white'", ")", "\n", "CDF2", "=", "plt", ".", "hist", "(", "samples", "[", "2", "]", ",", "bins", "=", "bins", ",", "density", "=", "density", ",", "range", "=", "plotRange", ",", "histtype", "=", "histtype", ",", "cumulative", "=", "True", ",", "color", "=", "'white'", ")", "\n", "CDF3", "=", "plt", ".", "hist", "(", "samples", "[", "3", "]", ",", "bins", "=", "bins", ",", "density", "=", "density", ",", "range", "=", "plotRange", ",", "histtype", "=", "histtype", ",", "cumulative", "=", "True", ",", "color", "=", "'white'", ")", "\n", "ax3", ".", "clear", "(", ")", "\n", "ax3", ".", "plot", "(", "CDF0", "[", "0", "]", ",", "CDF0", "[", "1", "]", "[", "1", ":", "]", ",", "color", "=", "colors", "[", "0", "]", ")", "\n", "ax3", ".", "plot", "(", "CDF1", "[", "0", "]", ",", "CDF1", "[", "1", "]", "[", "1", ":", "]", ",", "color", "=", "colors", "[", "1", "]", ")", "\n", "ax3", ".", "plot", "(", "CDF2", "[", "0", "]", ",", "CDF2", "[", "1", "]", "[", "1", ":", "]", ",", "color", "=", "colors", "[", "2", "]", ")", "\n", "ax3", ".", "plot", "(", "CDF3", "[", "0", "]", ",", "CDF3", "[", "1", "]", "[", "1", ":", "]", ",", "color", "=", "colors", "[", "3", "]", ")", "\n", "ax3", ".", "set_xlabel", "(", "'Quantile fraction'", ")", "\n", "ax3", ".", "set_ylabel", "(", "'QF'", ")", "\n", "ax3", ".", "set", "(", "xlim", "=", "(", "0", ",", "1", ")", ")", "\n", "ax3", ".", "legend", "(", "[", "'Move right'", ",", "'Move down'", ",", "'Move left'", ",", "'Move up'", "]", ")", "\n", "\n", "# Saving of the figure generated", "\n", "plt", ".", "savefig", "(", "\"Figures/Distributions/MonteCarloDistributions.pdf\"", ",", "format", "=", "'pdf'", ")", "\n", "\n", "# Generation of the figure for the PDF of the random return", "\n", "fig", "=", "plt", ".", "figure", "(", "figsize", "=", "(", "10", ",", "4", ")", ")", "\n", "ax1", "=", "plt", ".", "subplot", "(", "1", ",", "1", ",", "1", ")", "\n", "for", "action", "in", "range", "(", "actions", ")", ":", "\n", "            ", "plt", ".", "hist", "(", "samples", "[", "action", "]", ",", "bins", "=", "bins", ",", "density", "=", "density", ",", "range", "=", "plotRange", ",", "histtype", "=", "histtype", ",", "color", "=", "colors", "[", "action", "]", ")", "\n", "", "ax1", ".", "set_xlabel", "(", "'Random return'", ")", "\n", "ax1", ".", "set_ylabel", "(", "'PDF'", ")", "\n", "ax1", ".", "set", "(", "xlim", "=", "(", "-", "0.5", ",", "1.5", ")", ",", "ylim", "=", "(", "0", ",", "3.5", ")", ")", "\n", "plt", ".", "savefig", "(", "\"Figures/Distributions/MonteCarloDistributionsPDF.pdf\"", ",", "format", "=", "'pdf'", ")", "\n", "# Generation of the figure for the CDF of the random return", "\n", "fig", "=", "plt", ".", "figure", "(", "figsize", "=", "(", "10", ",", "4", ")", ")", "\n", "ax2", "=", "plt", ".", "subplot", "(", "1", ",", "1", ",", "1", ")", "\n", "for", "action", "in", "range", "(", "actions", ")", ":", "\n", "            ", "plt", ".", "hist", "(", "samples", "[", "action", "]", ",", "bins", "=", "bins", ",", "density", "=", "density", ",", "range", "=", "plotRange", ",", "histtype", "=", "histtype", ",", "cumulative", "=", "True", ",", "color", "=", "colors", "[", "action", "]", ")", "\n", "", "ax2", ".", "set_xlabel", "(", "'Random return'", ")", "\n", "ax2", ".", "set_ylabel", "(", "'CDF'", ")", "\n", "ax2", ".", "set", "(", "xlim", "=", "(", "-", "0.5", ",", "1.5", ")", ",", "ylim", "=", "(", "-", "0.1", ",", "1.1", ")", ")", "\n", "plt", ".", "savefig", "(", "\"Figures/Distributions/MonteCarloDistributionsCDF.pdf\"", ",", "format", "=", "'pdf'", ")", "\n", "# Generation of the figure for the QF of the random return", "\n", "fig", "=", "plt", ".", "figure", "(", "figsize", "=", "(", "10", ",", "4", ")", ")", "\n", "ax3", "=", "plt", ".", "subplot", "(", "1", ",", "1", ",", "1", ")", "\n", "CDF0", "=", "plt", ".", "hist", "(", "samples", "[", "0", "]", ",", "bins", "=", "bins", ",", "density", "=", "density", ",", "range", "=", "plotRange", ",", "histtype", "=", "histtype", ",", "cumulative", "=", "True", ",", "color", "=", "'white'", ")", "\n", "CDF1", "=", "plt", ".", "hist", "(", "samples", "[", "1", "]", ",", "bins", "=", "bins", ",", "density", "=", "density", ",", "range", "=", "plotRange", ",", "histtype", "=", "histtype", ",", "cumulative", "=", "True", ",", "color", "=", "'white'", ")", "\n", "CDF2", "=", "plt", ".", "hist", "(", "samples", "[", "2", "]", ",", "bins", "=", "bins", ",", "density", "=", "density", ",", "range", "=", "plotRange", ",", "histtype", "=", "histtype", ",", "cumulative", "=", "True", ",", "color", "=", "'white'", ")", "\n", "CDF3", "=", "plt", ".", "hist", "(", "samples", "[", "3", "]", ",", "bins", "=", "bins", ",", "density", "=", "density", ",", "range", "=", "plotRange", ",", "histtype", "=", "histtype", ",", "cumulative", "=", "True", ",", "color", "=", "'white'", ")", "\n", "ax3", ".", "clear", "(", ")", "\n", "ax3", ".", "plot", "(", "CDF0", "[", "0", "]", ",", "CDF0", "[", "1", "]", "[", "1", ":", "]", ",", "color", "=", "colors", "[", "0", "]", ")", "\n", "ax3", ".", "plot", "(", "CDF1", "[", "0", "]", ",", "CDF1", "[", "1", "]", "[", "1", ":", "]", ",", "color", "=", "colors", "[", "1", "]", ")", "\n", "ax3", ".", "plot", "(", "CDF2", "[", "0", "]", ",", "CDF2", "[", "1", "]", "[", "1", ":", "]", ",", "color", "=", "colors", "[", "2", "]", ")", "\n", "ax3", ".", "plot", "(", "CDF3", "[", "0", "]", ",", "CDF3", "[", "1", "]", "[", "1", ":", "]", ",", "color", "=", "colors", "[", "3", "]", ")", "\n", "ax3", ".", "set_xlabel", "(", "'Quantile fraction'", ")", "\n", "ax3", ".", "set_ylabel", "(", "'QF'", ")", "\n", "ax3", ".", "set", "(", "xlim", "=", "(", "0", ",", "1", ")", ",", "ylim", "=", "(", "-", "0.5", ",", "1.5", ")", ")", "\n", "plt", ".", "savefig", "(", "\"Figures/Distributions/MonteCarloDistributionsQF.pdf\"", ",", "format", "=", "'pdf'", ")", "\n", "\n", "# Saving of the data into external files", "\n", "PDF0", "=", "plt", ".", "hist", "(", "samples", "[", "0", "]", ",", "bins", "=", "bins", ",", "density", "=", "density", ",", "range", "=", "plotRange", ",", "histtype", "=", "histtype", ",", "color", "=", "'white'", ")", "\n", "PDF1", "=", "plt", ".", "hist", "(", "samples", "[", "1", "]", ",", "bins", "=", "bins", ",", "density", "=", "density", ",", "range", "=", "plotRange", ",", "histtype", "=", "histtype", ",", "color", "=", "'white'", ")", "\n", "PDF2", "=", "plt", ".", "hist", "(", "samples", "[", "2", "]", ",", "bins", "=", "bins", ",", "density", "=", "density", ",", "range", "=", "plotRange", ",", "histtype", "=", "histtype", ",", "color", "=", "'white'", ")", "\n", "PDF3", "=", "plt", ".", "hist", "(", "samples", "[", "3", "]", ",", "bins", "=", "bins", ",", "density", "=", "density", ",", "range", "=", "plotRange", ",", "histtype", "=", "histtype", ",", "color", "=", "'white'", ")", "\n", "CDF0", "=", "plt", ".", "hist", "(", "samples", "[", "0", "]", ",", "bins", "=", "bins", ",", "density", "=", "density", ",", "range", "=", "plotRange", ",", "histtype", "=", "histtype", ",", "cumulative", "=", "True", ",", "color", "=", "'white'", ")", "\n", "CDF1", "=", "plt", ".", "hist", "(", "samples", "[", "1", "]", ",", "bins", "=", "bins", ",", "density", "=", "density", ",", "range", "=", "plotRange", ",", "histtype", "=", "histtype", ",", "cumulative", "=", "True", ",", "color", "=", "'white'", ")", "\n", "CDF2", "=", "plt", ".", "hist", "(", "samples", "[", "2", "]", ",", "bins", "=", "bins", ",", "density", "=", "density", ",", "range", "=", "plotRange", ",", "histtype", "=", "histtype", ",", "cumulative", "=", "True", ",", "color", "=", "'white'", ")", "\n", "CDF3", "=", "plt", ".", "hist", "(", "samples", "[", "3", "]", ",", "bins", "=", "bins", ",", "density", "=", "density", ",", "range", "=", "plotRange", ",", "histtype", "=", "histtype", ",", "cumulative", "=", "True", ",", "color", "=", "'white'", ")", "\n", "dataPDF", "=", "{", "\n", "'Action0_x'", ":", "PDF0", "[", "1", "]", "[", "1", ":", "]", ",", "\n", "'Action0_y'", ":", "PDF0", "[", "0", "]", ",", "\n", "'Action1_x'", ":", "PDF1", "[", "1", "]", "[", "1", ":", "]", ",", "\n", "'Action1_y'", ":", "PDF1", "[", "0", "]", ",", "\n", "'Action2_x'", ":", "PDF2", "[", "1", "]", "[", "1", ":", "]", ",", "\n", "'Action2_y'", ":", "PDF2", "[", "0", "]", ",", "\n", "'Action3_x'", ":", "PDF3", "[", "1", "]", "[", "1", ":", "]", ",", "\n", "'Action3_y'", ":", "PDF3", "[", "0", "]", ",", "\n", "}", "\n", "dataCDF", "=", "{", "\n", "'Action0_x'", ":", "CDF0", "[", "1", "]", "[", "1", ":", "]", ",", "\n", "'Action0_y'", ":", "CDF0", "[", "0", "]", ",", "\n", "'Action1_x'", ":", "CDF1", "[", "1", "]", "[", "1", ":", "]", ",", "\n", "'Action1_y'", ":", "CDF1", "[", "0", "]", ",", "\n", "'Action2_x'", ":", "CDF2", "[", "1", "]", "[", "1", ":", "]", ",", "\n", "'Action2_y'", ":", "CDF2", "[", "0", "]", ",", "\n", "'Action3_x'", ":", "CDF3", "[", "1", "]", "[", "1", ":", "]", ",", "\n", "'Action3_y'", ":", "CDF3", "[", "0", "]", ",", "\n", "}", "\n", "dataQF", "=", "{", "\n", "'Action0_y'", ":", "CDF0", "[", "1", "]", "[", "1", ":", "]", ",", "\n", "'Action0_x'", ":", "CDF0", "[", "0", "]", ",", "\n", "'Action1_y'", ":", "CDF1", "[", "1", "]", "[", "1", ":", "]", ",", "\n", "'Action1_x'", ":", "CDF1", "[", "0", "]", ",", "\n", "'Action2_y'", ":", "CDF2", "[", "1", "]", "[", "1", ":", "]", ",", "\n", "'Action2_x'", ":", "CDF2", "[", "0", "]", ",", "\n", "'Action3_y'", ":", "CDF3", "[", "1", "]", "[", "1", ":", "]", ",", "\n", "'Action3_x'", ":", "CDF3", "[", "0", "]", ",", "\n", "}", "\n", "dataframePDF", "=", "pd", ".", "DataFrame", "(", "dataPDF", ")", "\n", "dataframeCDF", "=", "pd", ".", "DataFrame", "(", "dataCDF", ")", "\n", "dataframeQF", "=", "pd", ".", "DataFrame", "(", "dataQF", ")", "\n", "dataframePDF", ".", "to_csv", "(", "'Figures/Distributions/MonteCarloPDF.csv'", ")", "\n", "dataframeCDF", ".", "to_csv", "(", "'Figures/Distributions/MonteCarloCDF.csv'", ")", "\n", "dataframeQF", ".", "to_csv", "(", "'Figures/Distributions/MonteCarloQF.csv'", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.CDQN.CDQN.__init__": [[59, 130], ["DQN.DQN.DQN.__init__", "CDQN.CDQN.readParameters", "torch.device", "torch.device", "torch.device", "torch.device", "replayMemory.ReplayMemory", "numpy.linspace", "torch.linspace().to", "torch.linspace().to", "torch.linspace().to", "torch.linspace().to", "CDQN.CDQN.targetNetwork.load_state_dict", "torch.Adam", "torch.Adam", "CDQN.CDQN.initReporting", "Models.CDQN_Model_Atari.CDQN_Model_Atari().to", "Models.CDQN_Model_Atari.CDQN_Model_Atari().to", "Models.CDQN_Model.CDQN_Model().to", "Models.CDQN_Model.CDQN_Model().to", "CDQN.CDQN.policyNetwork.state_dict", "CDQN.CDQN.policyNetwork.parameters", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "str", "str", "Models.CDQN_Model_Atari.CDQN_Model_Atari", "Models.CDQN_Model_Atari.CDQN_Model_Atari", "Models.CDQN_Model.CDQN_Model", "Models.CDQN_Model.CDQN_Model", "math.exp"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.MonotonicNN.__init__", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.DQN.DQN.readParameters", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.DQN.DQN.initReporting"], ["def", "__init__", "(", "self", ",", "observationSpace", ",", "actionSpace", ",", "environment", ",", "\n", "parametersFileName", "=", "''", ",", "reporting", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Initializing the RL agent based on the CDQN Deep Reinforcement Learning\n              algorithm, by setting up the algorithm parameters as well as \n              the Deep Neural Networks.\n        \n        INPUTS: - observationSpace: RL observation space.\n                - actionSpace: RL action space.\n                - environment: Name of the RL environment.\n                - parametersFileName: Name of the JSON parameters file.\n                - reporting: Enable the reporting of the results.\n        \n        OUTPUTS: /\n        \"\"\"", "\n", "\n", "# Initialization of the DQN parent class", "\n", "DQN", ".", "__init__", "(", "self", ",", "observationSpace", ",", "actionSpace", ",", "environment", ",", "parametersFileName", ",", "False", ")", "\n", "\n", "# Setting of the parameters", "\n", "if", "parametersFileName", "==", "''", ":", "\n", "            ", "parametersFileName", "=", "''", ".", "join", "(", "[", "'Parameters/parameters_CDQN_'", ",", "str", "(", "environment", ")", ",", "'.json'", "]", ")", "\n", "", "parameters", "=", "self", ".", "readParameters", "(", "parametersFileName", ")", "\n", "\n", "# Set the device for DNN computations (CPU or GPU)", "\n", "self", ".", "device", "=", "torch", ".", "device", "(", "'cuda:'", "+", "str", "(", "parameters", "[", "'GPUNumber'", "]", ")", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "'cpu'", ")", "\n", "\n", "# Set the general parameters of the RL algorithm", "\n", "self", ".", "gamma", "=", "parameters", "[", "'gamma'", "]", "\n", "self", ".", "learningRate", "=", "parameters", "[", "'learningRate'", "]", "\n", "self", ".", "epsilon", "=", "parameters", "[", "'epsilon'", "]", "\n", "self", ".", "targetUpdatePeriod", "=", "parameters", "[", "'targetUpdatePeriod'", "]", "\n", "self", ".", "learningUpdatePeriod", "=", "parameters", "[", "'learningUpdatePeriod'", "]", "\n", "self", ".", "rewardClipping", "=", "parameters", "[", "'rewardClipping'", "]", "\n", "self", ".", "gradientClipping", "=", "parameters", "[", "'gradientClipping'", "]", "\n", "\n", "# Set the Experience Replay mechanism", "\n", "self", ".", "batchSize", "=", "parameters", "[", "'batchSize'", "]", "\n", "self", ".", "capacity", "=", "parameters", "[", "'capacity'", "]", "\n", "self", ".", "replayMemory", "=", "ReplayMemory", "(", "self", ".", "capacity", ")", "\n", "\n", "# Set the distribution support", "\n", "self", ".", "numberOfAtoms", "=", "parameters", "[", "'numberOfAtoms'", "]", "\n", "self", ".", "minReturn", "=", "parameters", "[", "'minReturn'", "]", "\n", "self", ".", "maxReturn", "=", "parameters", "[", "'maxReturn'", "]", "\n", "self", ".", "support", "=", "np", ".", "linspace", "(", "self", ".", "minReturn", ",", "self", ".", "maxReturn", ",", "self", ".", "numberOfAtoms", ")", "\n", "self", ".", "supportTorch", "=", "torch", ".", "linspace", "(", "self", ".", "minReturn", ",", "self", ".", "maxReturn", ",", "self", ".", "numberOfAtoms", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "# Set the two Deep Neural Networks of the RL algorithm (policy and target)", "\n", "self", ".", "atari", "=", "parameters", "[", "'atari'", "]", "\n", "self", ".", "minatar", "=", "parameters", "[", "'minatar'", "]", "\n", "if", "self", ".", "atari", "or", "self", ".", "minatar", ":", "\n", "            ", "self", ".", "policyNetwork", "=", "CDQN_Model_Atari", "(", "observationSpace", ",", "actionSpace", "*", "self", ".", "numberOfAtoms", ",", "self", ".", "numberOfAtoms", ",", "minAtar", "=", "self", ".", "minatar", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "targetNetwork", "=", "CDQN_Model_Atari", "(", "observationSpace", ",", "actionSpace", "*", "self", ".", "numberOfAtoms", ",", "self", ".", "numberOfAtoms", ",", "minAtar", "=", "self", ".", "minatar", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "policyNetwork", "=", "CDQN_Model", "(", "observationSpace", ",", "actionSpace", "*", "self", ".", "numberOfAtoms", ",", "parameters", "[", "'structureDNN'", "]", ",", "self", ".", "numberOfAtoms", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "targetNetwork", "=", "CDQN_Model", "(", "observationSpace", ",", "actionSpace", "*", "self", ".", "numberOfAtoms", ",", "parameters", "[", "'structureDNN'", "]", ",", "self", ".", "numberOfAtoms", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "", "self", ".", "targetNetwork", ".", "load_state_dict", "(", "self", ".", "policyNetwork", ".", "state_dict", "(", ")", ")", "\n", "\n", "# Set the Deep Learning optimizer", "\n", "self", ".", "optimizer", "=", "optim", ".", "Adam", "(", "self", ".", "policyNetwork", ".", "parameters", "(", ")", ",", "lr", "=", "self", ".", "learningRate", ",", "eps", "=", "self", ".", "epsilon", ")", "\n", "\n", "# Set the Epsilon-Greedy exploration technique", "\n", "self", ".", "epsilonStart", "=", "parameters", "[", "'epsilonStart'", "]", "\n", "self", ".", "epsilonEnd", "=", "parameters", "[", "'epsilonEnd'", "]", "\n", "self", ".", "epsilonDecay", "=", "parameters", "[", "'epsilonDecay'", "]", "\n", "self", ".", "epsilonTest", "=", "parameters", "[", "'epsilonTest'", "]", "\n", "self", ".", "epsilonValue", "=", "lambda", "iteration", ":", "self", ".", "epsilonEnd", "+", "(", "self", ".", "epsilonStart", "-", "self", ".", "epsilonEnd", ")", "*", "math", ".", "exp", "(", "-", "1", "*", "iteration", "/", "self", ".", "epsilonDecay", ")", "\n", "\n", "# Initialization of the experiment folder and tensorboard writer", "\n", "self", ".", "initReporting", "(", "parameters", ",", "'CDQN'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.CDQN.CDQN.chooseAction": [[132, 167], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.from_numpy().float().to().unsqueeze", "torch.from_numpy().float().to().unsqueeze", "torch.from_numpy().float().to().unsqueeze", "torch.from_numpy().float().to().unsqueeze", "CDQN.CDQN.policyNetwork().squeeze", "distributionReturn.sum", "QValues.cpu().numpy.cpu().numpy.max", "action.item", "matplotlib.pyplot.figure", "matplotlib.pyplot.figure.add_subplot", "QValues.cpu().numpy.cpu().numpy.cpu().numpy", "range", "plt.figure.add_subplot.set_xlabel", "plt.figure.add_subplot.set_ylabel", "plt.figure.add_subplot.legend", "matplotlib.pyplot.show", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "CDQN.CDQN.policyNetwork", "distribution[].cpu().numpy", "plt.figure.add_subplot.bar", "plt.figure.add_subplot.axvline", "QValues.cpu().numpy.cpu().numpy.cpu", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "distribution[].cpu", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "str", "str"], "methods", ["None"], ["", "def", "chooseAction", "(", "self", ",", "state", ",", "plot", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Choose a valid RL action from the action space according to the\n              RL policy as well as the current RL state observed.\n        \n        INPUTS: - state: RL state returned by the environment.\n                - plot: Enable the plotting of the random returns distributions.\n        \n        OUTPUTS: - action: RL action chosen from the action space.\n        \"\"\"", "\n", "\n", "# Choose the best action based on the RL policy", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "state", "=", "torch", ".", "from_numpy", "(", "state", ")", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", ".", "unsqueeze", "(", "0", ")", "\n", "distribution", "=", "self", ".", "policyNetwork", "(", "state", ")", ".", "squeeze", "(", "0", ")", "\n", "distributionReturn", "=", "distribution", "*", "self", ".", "supportTorch", "\n", "QValues", "=", "distributionReturn", ".", "sum", "(", "1", ")", "\n", "_", ",", "action", "=", "QValues", ".", "max", "(", "0", ")", "\n", "\n", "# If required, plot the return distribution associated with each action", "\n", "if", "plot", ":", "\n", "                ", "colors", "=", "[", "'blue'", ",", "'red'", ",", "'orange'", ",", "'green'", ",", "'purple'", ",", "'brown'", "]", "\n", "fig", "=", "plt", ".", "figure", "(", ")", "\n", "ax", "=", "fig", ".", "add_subplot", "(", ")", "\n", "QValues", "=", "QValues", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "for", "a", "in", "range", "(", "self", ".", "actionSpace", ")", ":", "\n", "                    ", "dist", "=", "distribution", "[", "a", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "ax", ".", "bar", "(", "self", ".", "support", ",", "dist", ",", "label", "=", "''", ".", "join", "(", "[", "'Action '", ",", "str", "(", "a", ")", ",", "' random return Z'", "]", ")", ",", "width", "=", "(", "self", ".", "maxReturn", "-", "self", ".", "minReturn", ")", "/", "self", ".", "numberOfAtoms", ",", "edgecolor", "=", "'black'", ",", "alpha", "=", "0.5", ",", "color", "=", "colors", "[", "a", "]", ")", "\n", "ax", ".", "axvline", "(", "x", "=", "QValues", "[", "a", "]", ",", "linewidth", "=", "2", ",", "linestyle", "=", "'--'", ",", "label", "=", "''", ".", "join", "(", "[", "'Action '", ",", "str", "(", "a", ")", ",", "' expected return Q'", "]", ")", ",", "color", "=", "colors", "[", "a", "]", ")", "\n", "", "ax", ".", "set_xlabel", "(", "'Random return'", ")", "\n", "ax", ".", "set_ylabel", "(", "'Probability Density Function (PDF)'", ")", "\n", "ax", ".", "legend", "(", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n", "", "return", "action", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.CDQN.CDQN.learning": [[169, 225], ["len", "CDQN.CDQN.dataLoaderIter.next", "batch[].float().to", "batch[].long().to", "batch[].float().to", "batch[].float().to", "batch[].float().to", "CDQN.CDQN.policyNetwork", "action.unsqueeze().unsqueeze().expand.unsqueeze().unsqueeze().expand.unsqueeze().unsqueeze().expand", "distribution.gather().squeeze.gather().squeeze.gather().squeeze", "CDQN.CDQN.optimizer.zero_grad", "loss.backward", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "CDQN.CDQN.optimizer.step", "loss.item", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "CDQN.CDQN.targetNetwork", "[].unsqueeze().unsqueeze().expand", "nextDistribution.gather().squeeze.gather().squeeze.gather().squeeze", "tz.clamp.clamp.clamp", "b.floor().long", "b.ceil().long", "torch.linspace().long().unsqueeze().expand().to", "torch.linspace().long().unsqueeze().expand().to", "torch.linspace().long().unsqueeze().expand().to", "torch.linspace().long().unsqueeze().expand().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to.view().index_add_", "torch.zeros().to.view().index_add_", "torch.zeros().to.view().index_add_", "torch.zeros().to.view().index_add_", "CDQN.CDQN.policyNetwork.parameters", "batch[].float", "batch[].long", "batch[].float", "batch[].float", "batch[].float", "action.unsqueeze().unsqueeze().expand.unsqueeze().unsqueeze().expand.unsqueeze().unsqueeze", "distribution.gather().squeeze.gather().squeeze.gather", "float", "batch[].float().to.view", "[].unsqueeze().unsqueeze", "nextDistribution.gather().squeeze.gather().squeeze.gather", "b.floor", "b.ceil", "torch.linspace().long().unsqueeze().expand", "torch.linspace().long().unsqueeze().expand", "torch.linspace().long().unsqueeze().expand", "torch.linspace().long().unsqueeze().expand", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros().to.view", "torch.zeros().to.view", "torch.zeros().to.view", "torch.zeros().to.view", "action.unsqueeze().unsqueeze().expand.unsqueeze().unsqueeze().expand.unsqueeze", "nextDistribution.gather().squeeze.gather().squeeze.size", "[].unsqueeze", "batch[].float().to.view", "torch.linspace().long().unsqueeze", "torch.linspace().long().unsqueeze", "torch.linspace().long().unsqueeze", "torch.linspace().long().unsqueeze", "b.ceil().long.float", "b.floor().long.float", "distribution.gather().squeeze.gather().squeeze.log", "torch.linspace().long", "torch.linspace().long", "torch.linspace().long", "torch.linspace().long", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorld.StochasticGridWorld.step"], ["", "", "def", "learning", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Sample a batch of past experiences and learn from it\n              by updating the Reinforcement Learning policy.\n        \n        INPUTS: /\n        \n        OUTPUTS: - loss: Loss of the learning procedure.\n        \"\"\"", "\n", "\n", "# Check that the replay memory is filled enough", "\n", "if", "(", "len", "(", "self", ".", "replayMemory", ")", ">=", "self", ".", "batchSize", ")", ":", "\n", "\n", "# Sample a batch of experiences from the replay memory", "\n", "            ", "batch", "=", "self", ".", "dataLoaderIter", ".", "next", "(", ")", "\n", "state", "=", "batch", "[", "0", "]", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "action", "=", "batch", "[", "1", "]", ".", "long", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "reward", "=", "batch", "[", "2", "]", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "nextState", "=", "batch", "[", "3", "]", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "done", "=", "batch", "[", "4", "]", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "# Computation of the current return distribution", "\n", "distribution", "=", "self", ".", "policyNetwork", "(", "state", ")", "\n", "action", "=", "action", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "self", ".", "batchSize", ",", "1", ",", "self", ".", "numberOfAtoms", ")", "\n", "distribution", "=", "distribution", ".", "gather", "(", "1", ",", "action", ")", ".", "squeeze", "(", "1", ")", "\n", "\n", "# Computation of the new distribution to be learnt by the policy DNN", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "nextDistribution", "=", "self", ".", "targetNetwork", "(", "nextState", ")", "\n", "nextAction", "=", "(", "nextDistribution", "*", "self", ".", "supportTorch", ")", ".", "sum", "(", "2", ")", ".", "max", "(", "1", ")", "[", "1", "]", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "self", ".", "batchSize", ",", "1", ",", "self", ".", "numberOfAtoms", ")", "\n", "nextDistribution", "=", "nextDistribution", ".", "gather", "(", "1", ",", "nextAction", ")", ".", "squeeze", "(", "1", ")", "\n", "deltaZ", "=", "float", "(", "self", ".", "maxReturn", "-", "self", ".", "minReturn", ")", "/", "(", "self", ".", "numberOfAtoms", "-", "1", ")", "\n", "tz", "=", "reward", ".", "view", "(", "-", "1", ",", "1", ")", "+", "(", "1", "-", "done", ".", "view", "(", "-", "1", ",", "1", ")", ")", "*", "self", ".", "gamma", "*", "self", ".", "supportTorch", "\n", "tz", "=", "tz", ".", "clamp", "(", "min", "=", "self", ".", "minReturn", ",", "max", "=", "self", ".", "maxReturn", ")", "\n", "b", "=", "(", "(", "tz", "-", "self", ".", "minReturn", ")", "/", "deltaZ", ")", "\n", "l", "=", "b", ".", "floor", "(", ")", ".", "long", "(", ")", "\n", "u", "=", "b", ".", "ceil", "(", ")", ".", "long", "(", ")", "\n", "offset", "=", "torch", ".", "linspace", "(", "0", ",", "(", "self", ".", "batchSize", "-", "1", ")", "*", "self", ".", "numberOfAtoms", ",", "self", ".", "batchSize", ")", ".", "long", "(", ")", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "self", ".", "batchSize", ",", "self", ".", "numberOfAtoms", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "projectedDistribution", "=", "torch", ".", "zeros", "(", "nextDistribution", ".", "size", "(", ")", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "projectedDistribution", ".", "view", "(", "-", "1", ")", ".", "index_add_", "(", "0", ",", "(", "l", "+", "offset", ")", ".", "view", "(", "-", "1", ")", ",", "(", "nextDistribution", "*", "(", "u", ".", "float", "(", ")", "-", "b", ")", ")", ".", "view", "(", "-", "1", ")", ")", "\n", "projectedDistribution", ".", "view", "(", "-", "1", ")", ".", "index_add_", "(", "0", ",", "(", "u", "+", "offset", ")", ".", "view", "(", "-", "1", ")", ",", "(", "nextDistribution", "*", "(", "b", "-", "l", ".", "float", "(", ")", ")", ")", ".", "view", "(", "-", "1", ")", ")", "\n", "\n", "# Computation of the loss", "\n", "", "loss", "=", "-", "(", "projectedDistribution", "*", "distribution", ".", "log", "(", ")", ")", ".", "sum", "(", "1", ")", ".", "mean", "(", ")", "\n", "\n", "# Computation of the gradients", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "\n", "# Gradient Clipping", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "policyNetwork", ".", "parameters", "(", ")", ",", "self", ".", "gradientClipping", ")", "\n", "\n", "# Perform the Deep Neural Network optimization", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "return", "loss", ".", "item", "(", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.QR_DQN.QR_DQN.__init__": [[57, 127], ["DQN.DQN.DQN.__init__", "QR_DQN.QR_DQN.readParameters", "torch.device", "torch.device", "torch.device", "torch.device", "replayMemory.ReplayMemory", "QR_DQN.QR_DQN.targetNetwork.load_state_dict", "torch.Adam", "torch.Adam", "QR_DQN.QR_DQN.initReporting", "Models.QR_DQN_Model_Atari.QR_DQN_Model_Atari().to", "Models.QR_DQN_Model_Atari.QR_DQN_Model_Atari().to", "Models.QR_DQN_Model.QR_DQN_Model().to", "Models.QR_DQN_Model.QR_DQN_Model().to", "QR_DQN.QR_DQN.policyNetwork.state_dict", "QR_DQN.QR_DQN.policyNetwork.parameters", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "str", "str", "Models.QR_DQN_Model_Atari.QR_DQN_Model_Atari", "Models.QR_DQN_Model_Atari.QR_DQN_Model_Atari", "Models.QR_DQN_Model.QR_DQN_Model", "Models.QR_DQN_Model.QR_DQN_Model", "math.exp", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.MonotonicNN.__init__", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.DQN.DQN.readParameters", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.DQN.DQN.initReporting"], ["def", "__init__", "(", "self", ",", "observationSpace", ",", "actionSpace", ",", "environment", ",", "\n", "parametersFileName", "=", "''", ",", "reporting", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Initializing the RL agent based on the QR-DQN Deep Reinforcement Learning\n              algorithm, by setting up the algorithm parameters as well as \n              the Deep Neural Networks.\n        \n        INPUTS: - observationSpace: RL observation space.\n                - actionSpace: RL action space.\n                - environment: Name of the RL environment.\n                - parametersFileName: Name of the JSON parameters file.\n                - reporting: Enable the reporting of the results.\n        \n        OUTPUTS: /\n        \"\"\"", "\n", "\n", "# Initialization of the DQN parent class", "\n", "DQN", ".", "__init__", "(", "self", ",", "observationSpace", ",", "actionSpace", ",", "environment", ",", "parametersFileName", ",", "False", ")", "\n", "\n", "# Setting of the parameters", "\n", "if", "parametersFileName", "==", "''", ":", "\n", "            ", "parametersFileName", "=", "''", ".", "join", "(", "[", "'Parameters/parameters_QR_DQN_'", ",", "str", "(", "environment", ")", ",", "'.json'", "]", ")", "\n", "", "parameters", "=", "self", ".", "readParameters", "(", "parametersFileName", ")", "\n", "\n", "# Set the device for DNN computations (CPU or GPU)", "\n", "self", ".", "device", "=", "torch", ".", "device", "(", "'cuda:'", "+", "str", "(", "parameters", "[", "'GPUNumber'", "]", ")", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "'cpu'", ")", "\n", "\n", "# Set the general parameters of the RL algorithm", "\n", "self", ".", "gamma", "=", "parameters", "[", "'gamma'", "]", "\n", "self", ".", "learningRate", "=", "parameters", "[", "'learningRate'", "]", "\n", "self", ".", "epsilon", "=", "parameters", "[", "'epsilon'", "]", "\n", "self", ".", "targetUpdatePeriod", "=", "parameters", "[", "'targetUpdatePeriod'", "]", "\n", "self", ".", "learningUpdatePeriod", "=", "parameters", "[", "'learningUpdatePeriod'", "]", "\n", "self", ".", "rewardClipping", "=", "parameters", "[", "'rewardClipping'", "]", "\n", "self", ".", "gradientClipping", "=", "parameters", "[", "'gradientClipping'", "]", "\n", "\n", "# Set the Experience Replay mechanism", "\n", "self", ".", "batchSize", "=", "parameters", "[", "'batchSize'", "]", "\n", "self", ".", "capacity", "=", "parameters", "[", "'capacity'", "]", "\n", "self", ".", "replayMemory", "=", "ReplayMemory", "(", "self", ".", "capacity", ")", "\n", "\n", "# Set the distribution support", "\n", "self", ".", "numberOfQuantiles", "=", "parameters", "[", "'numberOfQuantiles'", "]", "\n", "self", ".", "quantileProbability", "=", "1.", "/", "self", ".", "numberOfQuantiles", "\n", "self", ".", "tau", "=", "(", "(", "torch", ".", "linspace", "(", "0.0", ",", "1.0", ",", "self", ".", "numberOfQuantiles", "+", "1", ")", "[", ":", "-", "1", "]", "+", "torch", ".", "linspace", "(", "0.0", ",", "1.0", ",", "self", ".", "numberOfQuantiles", "+", "1", ")", "[", "1", ":", "]", ")", "/", "2", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "kappa", "=", "1.0", "\n", "\n", "# Set the two Deep Neural Networks of the RL algorithm (policy and target)", "\n", "self", ".", "atari", "=", "parameters", "[", "'atari'", "]", "\n", "self", ".", "minatar", "=", "parameters", "[", "'minatar'", "]", "\n", "if", "self", ".", "atari", "or", "self", ".", "minatar", ":", "\n", "            ", "self", ".", "policyNetwork", "=", "QR_DQN_Model_Atari", "(", "observationSpace", ",", "actionSpace", "*", "self", ".", "numberOfQuantiles", ",", "self", ".", "numberOfQuantiles", ",", "minAtar", "=", "self", ".", "minatar", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "targetNetwork", "=", "QR_DQN_Model_Atari", "(", "observationSpace", ",", "actionSpace", "*", "self", ".", "numberOfQuantiles", ",", "self", ".", "numberOfQuantiles", ",", "minAtar", "=", "self", ".", "minatar", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "policyNetwork", "=", "QR_DQN_Model", "(", "observationSpace", ",", "actionSpace", "*", "self", ".", "numberOfQuantiles", ",", "parameters", "[", "'structureDNN'", "]", ",", "self", ".", "numberOfQuantiles", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "targetNetwork", "=", "QR_DQN_Model", "(", "observationSpace", ",", "actionSpace", "*", "self", ".", "numberOfQuantiles", ",", "parameters", "[", "'structureDNN'", "]", ",", "self", ".", "numberOfQuantiles", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "", "self", ".", "targetNetwork", ".", "load_state_dict", "(", "self", ".", "policyNetwork", ".", "state_dict", "(", ")", ")", "\n", "\n", "# Set the Deep Learning optimizer", "\n", "self", ".", "optimizer", "=", "optim", ".", "Adam", "(", "self", ".", "policyNetwork", ".", "parameters", "(", ")", ",", "lr", "=", "self", ".", "learningRate", ",", "eps", "=", "self", ".", "epsilon", ")", "\n", "\n", "# Set the Epsilon-Greedy exploration technique", "\n", "self", ".", "epsilonStart", "=", "parameters", "[", "'epsilonStart'", "]", "\n", "self", ".", "epsilonEnd", "=", "parameters", "[", "'epsilonEnd'", "]", "\n", "self", ".", "epsilonDecay", "=", "parameters", "[", "'epsilonDecay'", "]", "\n", "self", ".", "epsilonTest", "=", "parameters", "[", "'epsilonTest'", "]", "\n", "self", ".", "epsilonValue", "=", "lambda", "iteration", ":", "self", ".", "epsilonEnd", "+", "(", "self", ".", "epsilonStart", "-", "self", ".", "epsilonEnd", ")", "*", "math", ".", "exp", "(", "-", "1", "*", "iteration", "/", "self", ".", "epsilonDecay", ")", "\n", "\n", "# Initialization of the experiment folder and tensorboard writer", "\n", "self", ".", "initReporting", "(", "parameters", ",", "'QR_DQN'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.QR_DQN.QR_DQN.chooseAction": [[129, 164], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.from_numpy().float().to().unsqueeze", "torch.from_numpy().float().to().unsqueeze", "torch.from_numpy().float().to().unsqueeze", "torch.from_numpy().float().to().unsqueeze", "QR_DQN.QR_DQN.policyNetwork().squeeze", "quantiles.cpu().numpy.cpu().numpy.mean", "QValues.cpu().numpy.cpu().numpy.max", "action.item", "matplotlib.pyplot.figure", "matplotlib.pyplot.figure.add_subplot", "QR_DQN.QR_DQN.tau.cpu().numpy", "quantiles.cpu().numpy.cpu().numpy.cpu().numpy", "QValues.cpu().numpy.cpu().numpy.cpu().numpy", "range", "plt.figure.add_subplot.set_xlabel", "plt.figure.add_subplot.set_ylabel", "plt.figure.add_subplot.legend", "matplotlib.pyplot.show", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "QR_DQN.QR_DQN.policyNetwork", "plt.figure.add_subplot.plot", "plt.figure.add_subplot.axhline", "QR_DQN.QR_DQN.tau.cpu", "quantiles.cpu().numpy.cpu().numpy.cpu", "QValues.cpu().numpy.cpu().numpy.cpu", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "str", "str"], "methods", ["None"], ["", "def", "chooseAction", "(", "self", ",", "state", ",", "plot", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Choose a valid RL action from the action space according to the\n              RL policy as well as the current RL state observed.\n        \n        INPUTS: - state: RL state returned by the environment.\n                - plot: Enable the plotting of the random returns distributions.\n        \n        OUTPUTS: - action: RL action chosen from the action space.\n        \"\"\"", "\n", "\n", "# Choose the best action based on the RL policy", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "state", "=", "torch", ".", "from_numpy", "(", "state", ")", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", ".", "unsqueeze", "(", "0", ")", "\n", "quantiles", "=", "self", ".", "policyNetwork", "(", "state", ")", ".", "squeeze", "(", "0", ")", "\n", "QValues", "=", "quantiles", ".", "mean", "(", "1", ")", "\n", "_", ",", "action", "=", "QValues", ".", "max", "(", "0", ")", "\n", "\n", "# If required, plot the return distribution associated with each action", "\n", "if", "plot", ":", "\n", "                ", "colors", "=", "[", "'blue'", ",", "'red'", ",", "'orange'", ",", "'green'", ",", "'purple'", ",", "'brown'", "]", "\n", "fig", "=", "plt", ".", "figure", "(", ")", "\n", "ax", "=", "fig", ".", "add_subplot", "(", ")", "\n", "tau", "=", "self", ".", "tau", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "quantiles", "=", "quantiles", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "QValues", "=", "QValues", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "for", "a", "in", "range", "(", "self", ".", "actionSpace", ")", ":", "\n", "                    ", "ax", ".", "plot", "(", "tau", ",", "quantiles", "[", "a", "]", ",", "linestyle", "=", "'-'", ",", "label", "=", "''", ".", "join", "(", "[", "'Action '", ",", "str", "(", "a", ")", ",", "' random return Z'", "]", ")", ",", "color", "=", "colors", "[", "a", "]", ")", "\n", "ax", ".", "axhline", "(", "y", "=", "QValues", "[", "a", "]", ",", "linewidth", "=", "2", ",", "linestyle", "=", "'--'", ",", "label", "=", "''", ".", "join", "(", "[", "'Action '", ",", "str", "(", "a", ")", ",", "' expected return Q'", "]", ")", ",", "color", "=", "colors", "[", "a", "]", ")", "\n", "", "ax", ".", "set_xlabel", "(", "'Quantile fraction'", ")", "\n", "ax", ".", "set_ylabel", "(", "'Quantile Function (QF)'", ")", "\n", "ax", ".", "legend", "(", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n", "", "return", "action", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.QR_DQN.QR_DQN.learning": [[166, 217], ["len", "QR_DQN.QR_DQN.dataLoaderIter.next", "batch[].float().to", "batch[].long().to", "batch[].float().to", "batch[].float().to", "batch[].float().to", "QR_DQN.QR_DQN.policyNetwork", "action.view().expand.view().expand.view().expand", "quantiles.gather().squeeze.gather().squeeze.gather().squeeze", "difference.abs", "torch.where", "torch.where", "torch.where", "torch.where", "loss.mean().sum().mean.mean().sum().mean.mean().sum().mean", "QR_DQN.QR_DQN.optimizer.zero_grad", "loss.mean().sum().mean.mean().sum().mean.backward", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "QR_DQN.QR_DQN.optimizer.step", "loss.mean().sum().mean.mean().sum().mean.item", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "QR_DQN.QR_DQN.targetNetwork", "[].view().expand", "nextQuantiles.gather().squeeze.gather().squeeze.gather().squeeze", "targetQuantiles.unsqueeze", "quantiles.gather().squeeze.gather().squeeze.unsqueeze", "QR_DQN.QR_DQN.policyNetwork.parameters", "batch[].float", "batch[].long", "batch[].float", "batch[].float", "batch[].float", "action.view().expand.view().expand.view", "quantiles.gather().squeeze.gather().squeeze.gather", "batch[].float().to.unsqueeze", "difference.abs.pow", "loss.mean().sum().mean.mean().sum().mean.mean().sum", "[].view", "nextQuantiles.gather().squeeze.gather().squeeze.gather", "batch[].float().to.unsqueeze", "loss.mean().sum().mean.mean().sum().mean.mean", "nextQuantiles.gather().squeeze.gather().squeeze.mean().max", "nextQuantiles.gather().squeeze.gather().squeeze.mean"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorld.StochasticGridWorld.step"], ["", "", "def", "learning", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Sample a batch of past experiences and learn from it\n              by updating the Reinforcement Learning policy.\n        \n        INPUTS: /\n        \n        OUTPUTS: - loss: Loss of the learning procedure.\n        \"\"\"", "\n", "\n", "# Check that the replay memory is filled enough", "\n", "if", "(", "len", "(", "self", ".", "replayMemory", ")", ">=", "self", ".", "batchSize", ")", ":", "\n", "\n", "# Sample a batch of experiences from the replay memory", "\n", "            ", "batch", "=", "self", ".", "dataLoaderIter", ".", "next", "(", ")", "\n", "state", "=", "batch", "[", "0", "]", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "action", "=", "batch", "[", "1", "]", ".", "long", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "reward", "=", "batch", "[", "2", "]", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "nextState", "=", "batch", "[", "3", "]", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "done", "=", "batch", "[", "4", "]", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "# Computation of the current return distribution", "\n", "quantiles", "=", "self", ".", "policyNetwork", "(", "state", ")", "\n", "action", "=", "action", ".", "view", "(", "self", ".", "batchSize", ",", "1", ",", "1", ")", ".", "expand", "(", "self", ".", "batchSize", ",", "1", ",", "self", ".", "numberOfQuantiles", ")", "\n", "quantiles", "=", "quantiles", ".", "gather", "(", "1", ",", "action", ")", ".", "squeeze", "(", "1", ")", "\n", "\n", "# Computation of the new distribution to be learnt by the policy DNN", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "nextQuantiles", "=", "self", ".", "targetNetwork", "(", "nextState", ")", "\n", "nextAction", "=", "nextQuantiles", ".", "mean", "(", "2", ")", ".", "max", "(", "1", ")", "[", "1", "]", ".", "view", "(", "self", ".", "batchSize", ",", "1", ",", "1", ")", ".", "expand", "(", "self", ".", "batchSize", ",", "1", ",", "self", ".", "numberOfQuantiles", ")", "\n", "nextQuantiles", "=", "nextQuantiles", ".", "gather", "(", "1", ",", "nextAction", ")", ".", "squeeze", "(", "1", ")", "\n", "targetQuantiles", "=", "reward", ".", "unsqueeze", "(", "1", ")", "+", "self", ".", "gamma", "*", "nextQuantiles", "*", "(", "1", "-", "done", ".", "unsqueeze", "(", "1", ")", ")", "\n", "\n", "# Computation of the loss", "\n", "", "difference", "=", "targetQuantiles", ".", "unsqueeze", "(", "1", ")", "-", "quantiles", ".", "unsqueeze", "(", "2", ")", "\n", "error", "=", "difference", ".", "abs", "(", ")", "\n", "loss", "=", "torch", ".", "where", "(", "error", "<=", "self", ".", "kappa", ",", "0.5", "*", "error", ".", "pow", "(", "2", ")", ",", "self", ".", "kappa", "*", "(", "error", "-", "(", "0.5", "*", "self", ".", "kappa", ")", ")", ")", "\n", "loss", "=", "(", "self", ".", "tau", "-", "(", "difference", "<", "0", ")", ".", "float", "(", ")", ")", ".", "abs", "(", ")", "*", "loss", "/", "self", ".", "kappa", "\n", "loss", "=", "loss", ".", "mean", "(", "1", ")", ".", "sum", "(", "1", ")", ".", "mean", "(", ")", "\n", "\n", "# Computation of the gradients", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "\n", "# Gradient Clipping", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "policyNetwork", ".", "parameters", "(", ")", ",", "self", ".", "gradientClipping", ")", "\n", "\n", "# Perform the Deep Neural Network optimization", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "return", "loss", ".", "item", "(", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.UMDQN_KL.UMDQN_KL.__init__": [[61, 138], ["DQN.DQN.DQN.__init__", "UMDQN_KL.UMDQN_KL.readParameters", "torch.device", "torch.device", "torch.device", "torch.device", "replayMemory.ReplayMemory", "numpy.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "UMDQN_KL.UMDQN_KL.supportTorch.repeat().view", "UMDQN_KL.UMDQN_KL.targetNetwork.load_state_dict", "torch.Adam", "torch.Adam", "UMDQN_KL.UMDQN_KL.initReporting", "Models.UMDQN_KL_Model_Atari.UMDQN_KL_Model_Atari().to", "Models.UMDQN_KL_Model_Atari.UMDQN_KL_Model_Atari().to", "Models.UMDQN_KL_Model.UMDQN_KL_Model().to", "Models.UMDQN_KL_Model.UMDQN_KL_Model().to", "UMDQN_KL.UMDQN_KL.policyNetwork.state_dict", "UMDQN_KL.UMDQN_KL.policyNetwork.parameters", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "UMDQN_KL.UMDQN_KL.supportTorch.repeat", "str", "str", "Models.UMDQN_KL_Model_Atari.UMDQN_KL_Model_Atari", "Models.UMDQN_KL_Model_Atari.UMDQN_KL_Model_Atari", "Models.UMDQN_KL_Model.UMDQN_KL_Model", "Models.UMDQN_KL_Model.UMDQN_KL_Model", "math.exp"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.MonotonicNN.__init__", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.DQN.DQN.readParameters", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.DQN.DQN.initReporting"], ["def", "__init__", "(", "self", ",", "observationSpace", ",", "actionSpace", ",", "environment", ",", "\n", "parametersFileName", "=", "''", ",", "reporting", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Initializing the RL agent based on the UMDQN_KL Deep Reinforcement Learning\n              algorithm, by setting up the algorithm parameters as well as \n              the Deep Neural Networks.\n        \n        INPUTS: - observationSpace: RL observation space.\n                - actionSpace: RL action space.\n                - environment: Name of the RL environment.\n                - parametersFileName: Name of the JSON parameters file.\n                - reporting: Enable the reporting of the results.\n        \n        OUTPUTS: /\n        \"\"\"", "\n", "\n", "# Initialization of the DQN parent class", "\n", "DQN", ".", "__init__", "(", "self", ",", "observationSpace", ",", "actionSpace", ",", "environment", ",", "parametersFileName", ",", "False", ")", "\n", "\n", "# Setting of the parameters", "\n", "if", "parametersFileName", "==", "''", ":", "\n", "            ", "parametersFileName", "=", "''", ".", "join", "(", "[", "'Parameters/parameters_UMDQN_KL_'", ",", "str", "(", "environment", ")", ",", "'.json'", "]", ")", "\n", "", "parameters", "=", "self", ".", "readParameters", "(", "parametersFileName", ")", "\n", "\n", "# Set the device for DNN computations (CPU or GPU)", "\n", "self", ".", "device", "=", "torch", ".", "device", "(", "'cuda:'", "+", "str", "(", "parameters", "[", "'GPUNumber'", "]", ")", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "'cpu'", ")", "\n", "\n", "# Set the general parameters of the RL algorithm", "\n", "self", ".", "gamma", "=", "parameters", "[", "'gamma'", "]", "\n", "self", ".", "learningRate", "=", "parameters", "[", "'learningRate'", "]", "\n", "self", ".", "epsilon", "=", "parameters", "[", "'epsilon'", "]", "\n", "self", ".", "targetUpdatePeriod", "=", "parameters", "[", "'targetUpdatePeriod'", "]", "\n", "self", ".", "learningUpdatePeriod", "=", "parameters", "[", "'learningUpdatePeriod'", "]", "\n", "self", ".", "rewardClipping", "=", "parameters", "[", "'rewardClipping'", "]", "\n", "self", ".", "gradientClipping", "=", "parameters", "[", "'gradientClipping'", "]", "\n", "\n", "# Set the Experience Replay mechanism", "\n", "self", ".", "batchSize", "=", "parameters", "[", "'batchSize'", "]", "\n", "self", ".", "capacity", "=", "parameters", "[", "'capacity'", "]", "\n", "self", ".", "replayMemory", "=", "ReplayMemory", "(", "self", ".", "capacity", ")", "\n", "\n", "# Set the distribution support", "\n", "self", ".", "numberOfSamples", "=", "parameters", "[", "'numberOfSamples'", "]", "\n", "self", ".", "minReturn", "=", "parameters", "[", "'minReturn'", "]", "\n", "self", ".", "maxReturn", "=", "parameters", "[", "'maxReturn'", "]", "\n", "self", ".", "support", "=", "np", ".", "linspace", "(", "self", ".", "minReturn", ",", "self", ".", "maxReturn", ",", "self", ".", "numberOfSamples", ")", "\n", "self", ".", "supportTorch", "=", "torch", ".", "linspace", "(", "self", ".", "minReturn", ",", "self", ".", "maxReturn", ",", "self", ".", "numberOfSamples", ",", "device", "=", "self", ".", "device", ")", "\n", "self", ".", "supportRepeatedBatchSize", "=", "self", ".", "supportTorch", ".", "repeat", "(", "self", ".", "batchSize", ",", "1", ")", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "self", ".", "uniformProba", "=", "1", "/", "(", "self", ".", "maxReturn", "-", "self", ".", "minReturn", ")", "\n", "self", ".", "deltaSupport", "=", "self", ".", "support", "[", "1", "]", "-", "self", ".", "support", "[", "0", "]", "\n", "\n", "# Enable the faster but potentially less accurate estimation of the expectation", "\n", "self", ".", "fasterExpectation", "=", "parameters", "[", "'fasterExpectation'", "]", "\n", "\n", "# Set the two Deep Neural Networks of the RL algorithm (policy and target)", "\n", "self", ".", "atari", "=", "parameters", "[", "'atari'", "]", "\n", "self", ".", "minatar", "=", "parameters", "[", "'minatar'", "]", "\n", "if", "self", ".", "atari", "or", "self", ".", "minatar", ":", "\n", "            ", "self", ".", "policyNetwork", "=", "UMDQN_KL_Model_Atari", "(", "observationSpace", ",", "actionSpace", ",", "parameters", "[", "'structureUMNN'", "]", ",", "parameters", "[", "'stateEmbedding'", "]", ",", "parameters", "[", "'numberOfSteps'", "]", ",", "self", ".", "device", ",", "minAtar", "=", "self", ".", "minatar", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "targetNetwork", "=", "UMDQN_KL_Model_Atari", "(", "observationSpace", ",", "actionSpace", ",", "parameters", "[", "'structureUMNN'", "]", ",", "parameters", "[", "'stateEmbedding'", "]", ",", "parameters", "[", "'numberOfSteps'", "]", ",", "self", ".", "device", ",", "minAtar", "=", "self", ".", "minatar", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "policyNetwork", "=", "UMDQN_KL_Model", "(", "observationSpace", ",", "actionSpace", ",", "parameters", "[", "'structureDNN'", "]", ",", "parameters", "[", "'structureUMNN'", "]", ",", "parameters", "[", "'stateEmbedding'", "]", ",", "parameters", "[", "'numberOfSteps'", "]", ",", "self", ".", "device", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "targetNetwork", "=", "UMDQN_KL_Model", "(", "observationSpace", ",", "actionSpace", ",", "parameters", "[", "'structureDNN'", "]", ",", "parameters", "[", "'structureUMNN'", "]", ",", "parameters", "[", "'stateEmbedding'", "]", ",", "parameters", "[", "'numberOfSteps'", "]", ",", "self", ".", "device", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "", "self", ".", "targetNetwork", ".", "load_state_dict", "(", "self", ".", "policyNetwork", ".", "state_dict", "(", ")", ")", "\n", "\n", "# Set the Deep Learning optimizer", "\n", "self", ".", "optimizer", "=", "optim", ".", "Adam", "(", "self", ".", "policyNetwork", ".", "parameters", "(", ")", ",", "lr", "=", "self", ".", "learningRate", ",", "eps", "=", "self", ".", "epsilon", ")", "\n", "\n", "# Set the Epsilon-Greedy exploration technique", "\n", "self", ".", "epsilonStart", "=", "parameters", "[", "'epsilonStart'", "]", "\n", "self", ".", "epsilonEnd", "=", "parameters", "[", "'epsilonEnd'", "]", "\n", "self", ".", "epsilonDecay", "=", "parameters", "[", "'epsilonDecay'", "]", "\n", "self", ".", "epsilonTest", "=", "parameters", "[", "'epsilonTest'", "]", "\n", "self", ".", "epsilonValue", "=", "lambda", "iteration", ":", "self", ".", "epsilonEnd", "+", "(", "self", ".", "epsilonStart", "-", "self", ".", "epsilonEnd", ")", "*", "math", ".", "exp", "(", "-", "1", "*", "iteration", "/", "self", ".", "epsilonDecay", ")", "\n", "\n", "# Initialization of the experiment folder and tensorboard writer", "\n", "self", ".", "initReporting", "(", "parameters", ",", "'UMDQN_KL'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.UMDQN_KL.UMDQN_KL.chooseAction": [[140, 197], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.from_numpy().float().to().unsqueeze", "torch.from_numpy().float().to().unsqueeze", "torch.from_numpy().float().to().unsqueeze", "torch.from_numpy().float().to().unsqueeze", "UMDQN_KL.UMDQN_KL.max", "action.item", "UMDQN_KL.UMDQN_KL.policyNetwork.getExpectation().squeeze", "UMDQN_KL.UMDQN_KL.policyNetwork", "matplotlib.pyplot.figure", "matplotlib.pyplot.subplot", "range", "matplotlib.pyplot.subplot.set_xlabel", "matplotlib.pyplot.subplot.set_ylabel", "matplotlib.pyplot.subplot.legend", "matplotlib.pyplot.show", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "UMDQN_KL.UMDQN_KL.supportTorch.unsqueeze", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "numpy.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "UMDQN_KL.UMDQN_KL.policyNetwork", "matplotlib.pyplot.subplot.plot", "matplotlib.pyplot.subplot.fill_between", "matplotlib.pyplot.subplot.axvline", "UMDQN_KL.UMDQN_KL.policyNetwork.getExpectation", "torch.linspace.unsqueeze", "torch.linspace.unsqueeze", "pdfs[].cpu", "pdfs[].cpu", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "str", "str"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.UMDQN_C_Model_Atari.UMDQN_C_Model_Atari.getExpectation"], ["", "def", "chooseAction", "(", "self", ",", "state", ",", "plot", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Choose a valid RL action from the action space according to the\n              RL policy as well as the current RL state observed.\n        \n        INPUTS: - state: RL state returned by the environment.\n                - plot: Enable the plotting of the random returns distributions.\n        \n        OUTPUTS: - action: RL action chosen from the action space.\n        \"\"\"", "\n", "\n", "# Choose the best action based on the RL policy", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "state", "=", "torch", ".", "from_numpy", "(", "state", ")", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", ".", "unsqueeze", "(", "0", ")", "\n", "if", "self", ".", "fasterExpectation", ":", "\n", "                ", "QValues", "=", "self", ".", "policyNetwork", ".", "getExpectation", "(", "state", ",", "self", ".", "minReturn", ",", "self", ".", "maxReturn", ",", "10", "*", "self", ".", "numberOfSamples", ")", ".", "squeeze", "(", "0", ")", "\n", "", "else", ":", "\n", "                ", "pdfs", "=", "self", ".", "policyNetwork", "(", "state", ",", "self", ".", "supportTorch", ".", "unsqueeze", "(", "1", ")", ")", "\n", "QValues", "=", "(", "pdfs", "*", "self", ".", "supportTorch", ")", ".", "sum", "(", "1", ")", "/", "(", "self", ".", "numberOfSamples", "*", "self", ".", "uniformProba", ")", "\n", "", "_", ",", "action", "=", "QValues", ".", "max", "(", "0", ")", "\n", "\n", "# If required, plot the return distribution associated with each action", "\n", "if", "plot", ":", "\n", "                ", "colors", "=", "[", "'blue'", ",", "'red'", ",", "'orange'", ",", "'green'", ",", "'purple'", ",", "'brown'", "]", "\n", "plt", ".", "figure", "(", ")", "\n", "ax", "=", "plt", ".", "subplot", "(", "1", ",", "1", ",", "1", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                    ", "accurateSupport", "=", "np", ".", "linspace", "(", "self", ".", "minReturn", ",", "self", ".", "maxReturn", ",", "self", ".", "numberOfSamples", "*", "10", ")", "\n", "accurateSupportTorch", "=", "torch", ".", "linspace", "(", "self", ".", "minReturn", ",", "self", ".", "maxReturn", ",", "self", ".", "numberOfSamples", "*", "10", ",", "device", "=", "self", ".", "device", ")", "\n", "pdfs", "=", "self", ".", "policyNetwork", "(", "state", ",", "accurateSupportTorch", ".", "unsqueeze", "(", "1", ")", ")", "\n", "QValues", "=", "(", "(", "pdfs", "*", "accurateSupportTorch", ")", ".", "sum", "(", "1", ")", ")", "/", "(", "self", ".", "numberOfSamples", "*", "10", "*", "self", ".", "uniformProba", ")", "\n", "", "for", "a", "in", "range", "(", "self", ".", "actionSpace", ")", ":", "\n", "                    ", "ax", ".", "plot", "(", "accurateSupport", ",", "pdfs", "[", "a", "]", ".", "cpu", "(", ")", ",", "linestyle", "=", "'-'", ",", "label", "=", "''", ".", "join", "(", "[", "'Action '", ",", "str", "(", "a", ")", ",", "' random return Z'", "]", ")", ",", "color", "=", "colors", "[", "a", "]", ")", "\n", "ax", ".", "fill_between", "(", "accurateSupport", ",", "accurateSupport", "*", "0", ",", "pdfs", "[", "a", "]", ".", "cpu", "(", ")", ",", "alpha", "=", "0.25", ",", "color", "=", "colors", "[", "a", "]", ")", "\n", "ax", ".", "axvline", "(", "x", "=", "QValues", "[", "a", "]", ",", "linewidth", "=", "2", ",", "linestyle", "=", "'--'", ",", "label", "=", "''", ".", "join", "(", "[", "'Action '", ",", "str", "(", "a", ")", ",", "' expected return Q'", "]", ")", ",", "color", "=", "colors", "[", "a", "]", ")", "\n", "", "ax", ".", "set_xlabel", "(", "'Random return'", ")", "\n", "ax", ".", "set_ylabel", "(", "'Probability Density Function (PDF)'", ")", "\n", "ax", ".", "legend", "(", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\"\"\"\n                # Saving of the data into external files\n                dataPDF = {\n                'Action0_x': accurateSupport,\n                'Action0_y': pdfs[0].cpu(),\n                'Action1_x': accurateSupport,\n                'Action1_y': pdfs[1].cpu(),\n                'Action2_x': accurateSupport,\n                'Action2_y': pdfs[2].cpu(),\n                'Action3_x': accurateSupport,\n                'Action3_y': pdfs[3].cpu(),\n                }\n                dataframePDF = pd.DataFrame(dataPDF)\n                dataframePDF.to_csv('Figures/Distributions/UMDQN_KL.csv')\n                quit()\n                \"\"\"", "\n", "\n", "", "return", "action", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.UMDQN_KL.UMDQN_KL.learning": [[199, 262], ["len", "UMDQN_KL.UMDQN_KL.dataLoaderIter.next", "batch[].float().to", "batch[].float().to", "batch[].float().to", "batch[].float().to", "batch[].float().to", "UMDQN_KL.UMDQN_KL.policyNetwork", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.index_select().view", "torch.index_select().view", "torch.index_select().view", "torch.index_select().view", "UMDQN_KL.UMDQN_KL.optimizer.zero_grad", "loss.backward", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "UMDQN_KL.UMDQN_KL.optimizer.step", "loss.item", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "UMDQN_KL.UMDQN_KL.max", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "batch[].float().to.view().repeat().view", "UMDQN_KL.UMDQN_KL.targetNetwork", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "range", "targetPdfs.view.view.clamp", "targetPdfs.view.view.view", "UMDQN_KL.UMDQN_KL.policyNetwork.parameters", "batch[].float", "batch[].float", "batch[].float", "batch[].float", "batch[].float", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "UMDQN_KL.UMDQN_KL.targetNetwork.getExpectation", "UMDQN_KL.UMDQN_KL.targetNetwork", "range", "batch[].float().to.view().repeat", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "range", "targetPdfs.view.view.log", "torch.index_select().view.log", "torch.index_select().view.log", "batch[].float().to.view", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "scipy.norm.pdf", "reward[].item"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorld.StochasticGridWorld.step", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.UMDQN_C_Model_Atari.UMDQN_C_Model_Atari.getExpectation"], ["", "", "def", "learning", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Sample a batch of past experiences and learn from it\n              by updating the Reinforcement Learning policy.\n        \n        INPUTS: /\n        \n        OUTPUTS: - loss: Loss of the learning procedure.\n        \"\"\"", "\n", "\n", "# Check that the replay memory is filled enough", "\n", "if", "(", "len", "(", "self", ".", "replayMemory", ")", ">=", "self", ".", "batchSize", ")", ":", "\n", "\n", "# Sample a batch of experiences from the replay memory", "\n", "            ", "batch", "=", "self", ".", "dataLoaderIter", ".", "next", "(", ")", "\n", "state", "=", "batch", "[", "0", "]", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "action", "=", "batch", "[", "1", "]", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "reward", "=", "batch", "[", "2", "]", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "nextState", "=", "batch", "[", "3", "]", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "done", "=", "batch", "[", "4", "]", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "# Computation of the current return distribution, according to the policy DNN", "\n", "pdfs", "=", "self", ".", "policyNetwork", "(", "state", ",", "self", ".", "supportRepeatedBatchSize", ")", "\n", "selection", "=", "torch", ".", "tensor", "(", "[", "self", ".", "actionSpace", "*", "i", "+", "action", "[", "i", "]", "for", "i", "in", "range", "(", "self", ".", "batchSize", ")", "]", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "self", ".", "device", ")", "\n", "currentPdfs", "=", "torch", ".", "index_select", "(", "pdfs", ",", "0", ",", "selection", ")", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "\n", "# Computation of the next action, according to the policy DNN", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "if", "self", ".", "fasterExpectation", ":", "\n", "                    ", "expectedReturns", "=", "self", ".", "targetNetwork", ".", "getExpectation", "(", "nextState", ",", "self", ".", "minReturn", ",", "self", ".", "maxReturn", ",", "10", "*", "self", ".", "numberOfSamples", ")", "\n", "", "else", ":", "\n", "                    ", "pdfs", "=", "self", ".", "targetNetwork", "(", "nextState", ",", "self", ".", "supportRepeatedBatchSize", ")", "\n", "expectedReturns", "=", "(", "(", "(", "pdfs", "*", "self", ".", "supportTorch", ")", ".", "sum", "(", "1", ")", ")", "/", "(", "self", ".", "numberOfSamples", "*", "self", ".", "uniformProba", ")", ")", ".", "view", "(", "-", "1", ",", "self", ".", "actionSpace", ")", "\n", "", "_", ",", "nextAction", "=", "expectedReturns", ".", "max", "(", "1", ")", "\n", "\n", "# Computation of the new distribution to be learnt by the policy DNN", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "r", "=", "reward", ".", "view", "(", "self", ".", "batchSize", ",", "1", ")", ".", "repeat", "(", "1", ",", "self", ".", "numberOfSamples", ")", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "support", "=", "(", "self", ".", "supportRepeatedBatchSize", "-", "r", ")", "/", "self", ".", "gamma", "\n", "targetPdfs", "=", "self", ".", "targetNetwork", "(", "nextState", ",", "support", ")", "\n", "selection", "=", "torch", ".", "tensor", "(", "[", "self", ".", "actionSpace", "*", "i", "+", "nextAction", "[", "i", "]", "for", "i", "in", "range", "(", "self", ".", "batchSize", ")", "]", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "self", ".", "device", ")", "\n", "targetPdfs", "=", "torch", ".", "index_select", "(", "targetPdfs", ",", "0", ",", "selection", ")", "\n", "targetPdfs", "=", "targetPdfs", "/", "self", ".", "gamma", "\n", "for", "i", "in", "range", "(", "self", ".", "batchSize", ")", ":", "\n", "                    ", "if", "done", "[", "i", "]", "==", "1", ":", "\n", "                        ", "targetPdfs", "[", "i", "]", "=", "torch", ".", "tensor", "(", "stats", ".", "norm", ".", "pdf", "(", "self", ".", "support", ",", "reward", "[", "i", "]", ".", "item", "(", ")", ",", "self", ".", "deltaSupport", ")", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "", "", "targetPdfs", "=", "targetPdfs", ".", "clamp", "(", "min", "=", "1e-6", ")", "\n", "targetPdfs", "=", "targetPdfs", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "\n", "# Compute the loss", "\n", "", "loss", "=", "(", "targetPdfs", "*", "(", "targetPdfs", ".", "log", "(", ")", "-", "currentPdfs", ".", "log", "(", ")", ")", ")", ".", "sum", "(", ")", "\n", "\n", "# Computation of the gradients", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "\n", "# Gradient Clipping", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "policyNetwork", ".", "parameters", "(", ")", ",", "self", ".", "gradientClipping", ")", "\n", "\n", "# Perform the Deep Neural Network optimization", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "return", "loss", ".", "item", "(", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.IQN.IQN.__init__": [[60, 130], ["DQN.DQN.DQN.__init__", "IQN.IQN.readParameters", "torch.device", "torch.device", "torch.device", "torch.device", "replayMemory.ReplayMemory", "IQN.IQN.targetNetwork.load_state_dict", "torch.Adam", "torch.Adam", "IQN.IQN.initReporting", "Models.IQN_Model_Atari.IQN_Model_Atari().to", "Models.IQN_Model_Atari.IQN_Model_Atari().to", "Models.IQN_Model.IQN_Model().to", "Models.IQN_Model.IQN_Model().to", "IQN.IQN.policyNetwork.state_dict", "IQN.IQN.policyNetwork.parameters", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "str", "str", "Models.IQN_Model_Atari.IQN_Model_Atari", "Models.IQN_Model_Atari.IQN_Model_Atari", "Models.IQN_Model.IQN_Model", "Models.IQN_Model.IQN_Model", "math.exp"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.MonotonicNN.__init__", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.DQN.DQN.readParameters", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.DQN.DQN.initReporting"], ["def", "__init__", "(", "self", ",", "observationSpace", ",", "actionSpace", ",", "environment", ",", "\n", "parametersFileName", "=", "''", ",", "reporting", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Initializing the RL agent based on the IQN Deep Reinforcement Learning\n              algorithm, by setting up the algorithm parameters as well as \n              the Deep Neural Networks.\n        \n        INPUTS: - observationSpace: RL observation space.\n                - actionSpace: RL action space.\n                - environment: Name of the RL environment.\n                - parametersFileName: Name of the JSON parameters file.\n                - reporting: Enable the reporting of the results.\n        \n        OUTPUTS: /\n        \"\"\"", "\n", "\n", "# Initialization of the DQN parent class", "\n", "DQN", ".", "__init__", "(", "self", ",", "observationSpace", ",", "actionSpace", ",", "environment", ",", "parametersFileName", ",", "False", ")", "\n", "\n", "# Setting of the parameters", "\n", "if", "parametersFileName", "==", "''", ":", "\n", "            ", "parametersFileName", "=", "''", ".", "join", "(", "[", "'Parameters/parameters_IQN_'", ",", "str", "(", "environment", ")", ",", "'.json'", "]", ")", "\n", "", "parameters", "=", "self", ".", "readParameters", "(", "parametersFileName", ")", "\n", "\n", "# Set the device for DNN computations (CPU or GPU)", "\n", "self", ".", "device", "=", "torch", ".", "device", "(", "'cuda:'", "+", "str", "(", "parameters", "[", "'GPUNumber'", "]", ")", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "'cpu'", ")", "\n", "\n", "# Set the general parameters of the RL algorithm", "\n", "self", ".", "gamma", "=", "parameters", "[", "'gamma'", "]", "\n", "self", ".", "learningRate", "=", "parameters", "[", "'learningRate'", "]", "\n", "self", ".", "epsilon", "=", "parameters", "[", "'epsilon'", "]", "\n", "self", ".", "targetUpdatePeriod", "=", "parameters", "[", "'targetUpdatePeriod'", "]", "\n", "self", ".", "learningUpdatePeriod", "=", "parameters", "[", "'learningUpdatePeriod'", "]", "\n", "self", ".", "rewardClipping", "=", "parameters", "[", "'rewardClipping'", "]", "\n", "self", ".", "gradientClipping", "=", "parameters", "[", "'gradientClipping'", "]", "\n", "\n", "# Set the Experience Replay mechanism", "\n", "self", ".", "batchSize", "=", "parameters", "[", "'batchSize'", "]", "\n", "self", ".", "capacity", "=", "parameters", "[", "'capacity'", "]", "\n", "self", ".", "replayMemory", "=", "ReplayMemory", "(", "self", ".", "capacity", ")", "\n", "\n", "# Set the distribution support", "\n", "self", ".", "N", "=", "parameters", "[", "'N'", "]", "\n", "self", ".", "K", "=", "parameters", "[", "'K'", "]", "\n", "self", ".", "NCos", "=", "parameters", "[", "'NCos'", "]", "\n", "self", ".", "kappa", "=", "1.0", "\n", "\n", "# Set the two Deep Neural Networks of the RL algorithm (policy and target)", "\n", "self", ".", "atari", "=", "parameters", "[", "'atari'", "]", "\n", "self", ".", "minatar", "=", "parameters", "[", "'minatar'", "]", "\n", "if", "self", ".", "atari", "or", "self", ".", "minatar", ":", "\n", "            ", "self", ".", "policyNetwork", "=", "IQN_Model_Atari", "(", "observationSpace", ",", "actionSpace", ",", "self", ".", "NCos", ",", "self", ".", "device", ",", "minAtar", "=", "self", ".", "minatar", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "targetNetwork", "=", "IQN_Model_Atari", "(", "observationSpace", ",", "actionSpace", ",", "self", ".", "NCos", ",", "self", ".", "device", ",", "minAtar", "=", "self", ".", "minatar", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "policyNetwork", "=", "IQN_Model", "(", "observationSpace", ",", "actionSpace", ",", "parameters", "[", "'structureDNN'", "]", ",", "parameters", "[", "'stateEmbedding'", "]", ",", "self", ".", "NCos", ",", "self", ".", "device", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "targetNetwork", "=", "IQN_Model", "(", "observationSpace", ",", "actionSpace", ",", "parameters", "[", "'structureDNN'", "]", ",", "parameters", "[", "'stateEmbedding'", "]", ",", "self", ".", "NCos", ",", "self", ".", "device", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "", "self", ".", "targetNetwork", ".", "load_state_dict", "(", "self", ".", "policyNetwork", ".", "state_dict", "(", ")", ")", "\n", "\n", "# Set the Deep Learning optimizer", "\n", "self", ".", "optimizer", "=", "optim", ".", "Adam", "(", "self", ".", "policyNetwork", ".", "parameters", "(", ")", ",", "lr", "=", "self", ".", "learningRate", ",", "eps", "=", "self", ".", "epsilon", ")", "\n", "\n", "# Set the Epsilon-Greedy exploration technique", "\n", "self", ".", "epsilonStart", "=", "parameters", "[", "'epsilonStart'", "]", "\n", "self", ".", "epsilonEnd", "=", "parameters", "[", "'epsilonEnd'", "]", "\n", "self", ".", "epsilonDecay", "=", "parameters", "[", "'epsilonDecay'", "]", "\n", "self", ".", "epsilonTest", "=", "parameters", "[", "'epsilonTest'", "]", "\n", "self", ".", "epsilonValue", "=", "lambda", "iteration", ":", "self", ".", "epsilonEnd", "+", "(", "self", ".", "epsilonStart", "-", "self", ".", "epsilonEnd", ")", "*", "math", ".", "exp", "(", "-", "1", "*", "iteration", "/", "self", ".", "epsilonDecay", ")", "\n", "\n", "# Initialization of the experiment folder and tensorboard writer", "\n", "self", ".", "initReporting", "(", "parameters", ",", "'IQN'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.IQN.IQN.chooseAction": [[132, 168], ["action.item", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.from_numpy().float().to().unsqueeze", "torch.from_numpy().float().to().unsqueeze", "torch.from_numpy().float().to().unsqueeze", "torch.from_numpy().float().to().unsqueeze", "IQN.IQN.policyNetwork", "quantiles.squeeze().cpu().numpy.squeeze().cpu().numpy.mean", "QValues.squeeze().cpu().numpy.squeeze().cpu().numpy.max", "matplotlib.pyplot.figure", "matplotlib.pyplot.figure.add_subplot", "IQN.IQN.policyNetwork", "taus[].squeeze().cpu().numpy", "quantiles.squeeze().cpu().numpy.squeeze().cpu().numpy.squeeze().cpu().numpy", "QValues.squeeze().cpu().numpy.squeeze().cpu().numpy.squeeze().cpu().numpy", "range", "plt.figure.add_subplot.set_xlabel", "plt.figure.add_subplot.set_ylabel", "plt.figure.add_subplot.legend", "matplotlib.pyplot.show", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "plt.figure.add_subplot.plot", "plt.figure.add_subplot.axhline", "taus[].squeeze().cpu", "quantiles.squeeze().cpu().numpy.squeeze().cpu().numpy.squeeze().cpu", "QValues.squeeze().cpu().numpy.squeeze().cpu().numpy.squeeze().cpu", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "taus[].squeeze", "quantiles.squeeze().cpu().numpy.squeeze().cpu().numpy.squeeze", "QValues.squeeze().cpu().numpy.squeeze().cpu().numpy.squeeze", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "str", "str"], "methods", ["None"], ["", "def", "chooseAction", "(", "self", ",", "state", ",", "plot", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Choose a valid RL action from the action space according to the\n              RL policy as well as the current RL state observed.\n        \n        INPUTS: - state: RL state returned by the environment.\n                - plot: Enable the plotting of the random returns distributions.\n        \n        OUTPUTS: - action: RL action chosen from the action space.\n        \"\"\"", "\n", "\n", "# Choose the best action based on the RL policy", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "state", "=", "torch", ".", "from_numpy", "(", "state", ")", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", ".", "unsqueeze", "(", "0", ")", "\n", "quantiles", ",", "_", "=", "self", ".", "policyNetwork", "(", "state", ",", "self", ".", "K", ")", "\n", "QValues", "=", "quantiles", ".", "mean", "(", "2", ")", "\n", "_", ",", "action", "=", "QValues", ".", "max", "(", "1", ")", "\n", "\n", "# If required, plot the return distribution associated with each action", "\n", "if", "plot", ":", "\n", "                ", "colors", "=", "[", "'blue'", ",", "'red'", ",", "'orange'", ",", "'green'", ",", "'purple'", ",", "'brown'", "]", "\n", "fig", "=", "plt", ".", "figure", "(", ")", "\n", "ax", "=", "fig", ".", "add_subplot", "(", ")", "\n", "quantiles", ",", "taus", "=", "self", ".", "policyNetwork", "(", "state", ",", "10000", ",", "False", ")", "\n", "taus", "=", "taus", "[", "0", "]", ".", "squeeze", "(", "1", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "quantiles", "=", "quantiles", ".", "squeeze", "(", "0", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "QValues", "=", "QValues", ".", "squeeze", "(", "0", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "for", "a", "in", "range", "(", "self", ".", "actionSpace", ")", ":", "\n", "                    ", "ax", ".", "plot", "(", "taus", ",", "quantiles", "[", "a", "]", ",", "linestyle", "=", "'-'", ",", "label", "=", "''", ".", "join", "(", "[", "'Action '", ",", "str", "(", "a", ")", ",", "' random return Z'", "]", ")", ",", "color", "=", "colors", "[", "a", "]", ")", "\n", "ax", ".", "axhline", "(", "y", "=", "QValues", "[", "a", "]", ",", "linewidth", "=", "2", ",", "linestyle", "=", "'--'", ",", "label", "=", "''", ".", "join", "(", "[", "'Action '", ",", "str", "(", "a", ")", ",", "' expected return Q'", "]", ")", ",", "color", "=", "colors", "[", "a", "]", ")", "\n", "", "ax", ".", "set_xlabel", "(", "'Quantile fraction'", ")", "\n", "ax", ".", "set_ylabel", "(", "'Quantile Function (QF)'", ")", "\n", "ax", ".", "legend", "(", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n", "", "", "return", "action", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.IQN.IQN.learning": [[170, 228], ["len", "IQN.IQN.dataLoaderIter.next", "batch[].float().to", "batch[].long().to", "batch[].float().to", "batch[].float().to", "batch[].float().to", "IQN.IQN.policyNetwork", "action.view().expand.view().expand.view().expand", "quantiles.gather().squeeze.gather().squeeze.gather().squeeze", "difference.abs", "torch.where", "torch.where", "torch.where", "torch.where", "difference.pow.mean().sum.mean().sum().mean", "IQN.IQN.optimizer.zero_grad", "difference.pow.mean().sum.backward", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "IQN.IQN.optimizer.step", "difference.pow.mean().sum.item", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "IQN.IQN.targetNetwork", "[].view().expand", "nextQuantiles.gather().squeeze.gather().squeeze.gather().squeeze", "targetQuantiles.unsqueeze", "quantiles.gather().squeeze.gather().squeeze.unsqueeze", "difference.pow", "difference.pow.mean().sum", "IQN.IQN.policyNetwork.parameters", "batch[].float", "batch[].long", "batch[].float", "batch[].float", "batch[].float", "action.view().expand.view().expand.view", "quantiles.gather().squeeze.gather().squeeze.gather", "batch[].float().to.unsqueeze", "difference.pow.pow", "difference.pow.mean().sum.mean().sum", "[].view", "nextQuantiles.gather().squeeze.gather().squeeze.gather", "difference.pow.mean", "batch[].float().to.unsqueeze", "difference.pow.mean().sum.mean", "nextQuantiles.gather().squeeze.gather().squeeze.mean().max", "nextQuantiles.gather().squeeze.gather().squeeze.mean"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorld.StochasticGridWorld.step"], ["", "def", "learning", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Sample a batch of past experiences and learn from it\n              by updating the Reinforcement Learning policy.\n        \n        INPUTS: /\n        \n        OUTPUTS: - loss: Loss of the learning procedure.\n        \"\"\"", "\n", "\n", "# Check that the replay memory is filled enough", "\n", "if", "(", "len", "(", "self", ".", "replayMemory", ")", ">=", "self", ".", "batchSize", ")", ":", "\n", "\n", "# Sample a batch of experiences from the replay memory", "\n", "            ", "batch", "=", "self", ".", "dataLoaderIter", ".", "next", "(", ")", "\n", "state", "=", "batch", "[", "0", "]", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "action", "=", "batch", "[", "1", "]", ".", "long", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "reward", "=", "batch", "[", "2", "]", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "nextState", "=", "batch", "[", "3", "]", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "done", "=", "batch", "[", "4", "]", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "# Computation of the current return distribution", "\n", "quantiles", ",", "taus", "=", "self", ".", "policyNetwork", "(", "state", ",", "self", ".", "N", ")", "\n", "action", "=", "action", ".", "view", "(", "self", ".", "batchSize", ",", "1", ",", "1", ")", ".", "expand", "(", "self", ".", "batchSize", ",", "1", ",", "self", ".", "N", ")", "\n", "quantiles", "=", "quantiles", ".", "gather", "(", "1", ",", "action", ")", ".", "squeeze", "(", "1", ")", "\n", "\n", "# Computation of the new distribution to be learnt by the policy DNN", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "nextQuantiles", ",", "_", "=", "self", ".", "targetNetwork", "(", "nextState", ",", "self", ".", "N", ")", "\n", "nextAction", "=", "nextQuantiles", ".", "mean", "(", "2", ")", ".", "max", "(", "1", ")", "[", "1", "]", ".", "view", "(", "self", ".", "batchSize", ",", "1", ",", "1", ")", ".", "expand", "(", "self", ".", "batchSize", ",", "1", ",", "self", ".", "N", ")", "\n", "nextQuantiles", "=", "nextQuantiles", ".", "gather", "(", "1", ",", "nextAction", ")", ".", "squeeze", "(", "1", ")", "\n", "targetQuantiles", "=", "reward", ".", "unsqueeze", "(", "1", ")", "+", "self", ".", "gamma", "*", "nextQuantiles", "*", "(", "1", "-", "done", ".", "unsqueeze", "(", "1", ")", ")", "\n", "\n", "# Computation of the loss", "\n", "", "difference", "=", "targetQuantiles", ".", "unsqueeze", "(", "1", ")", "-", "quantiles", ".", "unsqueeze", "(", "2", ")", "\n", "error", "=", "difference", ".", "abs", "(", ")", "\n", "loss", "=", "torch", ".", "where", "(", "error", "<=", "self", ".", "kappa", ",", "0.5", "*", "error", ".", "pow", "(", "2", ")", ",", "self", ".", "kappa", "*", "(", "error", "-", "(", "0.5", "*", "self", ".", "kappa", ")", ")", ")", "\n", "loss", "=", "(", "taus", "-", "(", "difference", "<", "0", ")", ".", "float", "(", ")", ")", ".", "abs", "(", ")", "*", "loss", "/", "self", ".", "kappa", "\n", "loss", "=", "loss", ".", "mean", "(", "1", ")", ".", "sum", "(", "1", ")", ".", "mean", "(", ")", "\n", "\n", "# Without Huber loss (to be tested)", "\n", "lossMSE", "=", "False", "\n", "if", "lossMSE", ":", "\n", "                ", "difference", "=", "targetQuantiles", "-", "quantiles", "\n", "error", "=", "difference", ".", "pow", "(", "2", ")", "\n", "loss", "=", "error", ".", "mean", "(", "1", ")", ".", "sum", "(", ")", "\n", "\n", "# Computation of the gradients", "\n", "", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "\n", "# Gradient Clipping", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "policyNetwork", ".", "parameters", "(", ")", ",", "self", ".", "gradientClipping", ")", "\n", "\n", "# Perform the Deep Neural Network optimization", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "return", "loss", ".", "item", "(", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.FQF.FQF.__init__": [[58, 135], ["DQN.DQN.DQN.__init__", "FQF.FQF.readParameters", "torch.device", "torch.device", "torch.device", "torch.device", "replayMemory.ReplayMemory", "FQF.FQF.targetNetwork.load_state_dict", "torch.Adam", "torch.Adam", "Models.FQF_Model_Bis.FQF_Model_Bis().to", "torch.RMSprop", "torch.RMSprop", "FQF.FQF.initReporting", "Models.FQF_Model_Atari.FQF_Model_Atari().to", "Models.FQF_Model_Atari.FQF_Model_Atari().to", "FQF.FQF.policyNetwork.getEmbeddingSize", "Models.FQF_Model.FQF_Model().to", "Models.FQF_Model.FQF_Model().to", "FQF.FQF.policyNetwork.state_dict", "FQF.FQF.policyNetwork.parameters", "FQF.FQF.fractionProposalNetwork.parameters", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "Models.FQF_Model_Bis.FQF_Model_Bis", "str", "str", "Models.FQF_Model_Atari.FQF_Model_Atari", "Models.FQF_Model_Atari.FQF_Model_Atari", "Models.FQF_Model.FQF_Model", "Models.FQF_Model.FQF_Model", "math.exp"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.MonotonicNN.__init__", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.DQN.DQN.readParameters", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.DQN.DQN.initReporting", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.FQF_Model_Atari.FQF_Model_Atari.getEmbeddingSize"], ["def", "__init__", "(", "self", ",", "observationSpace", ",", "actionSpace", ",", "environment", ",", "\n", "parametersFileName", "=", "''", ",", "reporting", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Initializing the RL agent based on the FQF Deep Reinforcement Learning\n              algorithm, by setting up the algorithm parameters as well as \n              the Deep Neural Networks.\n        \n        INPUTS: - observationSpace: RL observation space.\n                - actionSpace: RL action space.\n                - environment: Name of the RL environment.\n                - parametersFileName: Name of the JSON parameters file.\n                - reporting: Enable the reporting of the results.\n        \n        OUTPUTS: /\n        \"\"\"", "\n", "\n", "# Initialization of the DQN parent class", "\n", "DQN", ".", "__init__", "(", "self", ",", "observationSpace", ",", "actionSpace", ",", "environment", ",", "parametersFileName", ",", "False", ")", "\n", "\n", "# Setting of the parameters", "\n", "if", "parametersFileName", "==", "''", ":", "\n", "            ", "parametersFileName", "=", "''", ".", "join", "(", "[", "'Parameters/parameters_FQF_'", ",", "str", "(", "environment", ")", ",", "'.json'", "]", ")", "\n", "", "parameters", "=", "self", ".", "readParameters", "(", "parametersFileName", ")", "\n", "\n", "# Set the device for DNN computations (CPU or GPU)", "\n", "self", ".", "device", "=", "torch", ".", "device", "(", "'cuda:'", "+", "str", "(", "parameters", "[", "'GPUNumber'", "]", ")", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "'cpu'", ")", "\n", "\n", "# Set the general parameters of the RL algorithm", "\n", "self", ".", "gamma", "=", "parameters", "[", "'gamma'", "]", "\n", "self", ".", "learningRate", "=", "parameters", "[", "'learningRate'", "]", "\n", "self", ".", "epsilon", "=", "parameters", "[", "'epsilon'", "]", "\n", "self", ".", "targetUpdatePeriod", "=", "parameters", "[", "'targetUpdatePeriod'", "]", "\n", "self", ".", "learningUpdatePeriod", "=", "parameters", "[", "'learningUpdatePeriod'", "]", "\n", "self", ".", "rewardClipping", "=", "parameters", "[", "'rewardClipping'", "]", "\n", "self", ".", "gradientClipping", "=", "parameters", "[", "'gradientClipping'", "]", "\n", "\n", "# Set the Experience Replay mechanism", "\n", "self", ".", "batchSize", "=", "parameters", "[", "'batchSize'", "]", "\n", "self", ".", "capacity", "=", "parameters", "[", "'capacity'", "]", "\n", "self", ".", "replayMemory", "=", "ReplayMemory", "(", "self", ".", "capacity", ")", "\n", "\n", "# Set the distribution support", "\n", "self", ".", "N", "=", "parameters", "[", "'N'", "]", "\n", "self", ".", "K", "=", "parameters", "[", "'K'", "]", "\n", "self", ".", "NCos", "=", "parameters", "[", "'NCos'", "]", "\n", "self", ".", "kappa", "=", "1.0", "\n", "\n", "# Set the two Deep Neural Networks of the RL algorithm (policy and target)", "\n", "self", ".", "atari", "=", "parameters", "[", "'atari'", "]", "\n", "self", ".", "minatar", "=", "parameters", "[", "'minatar'", "]", "\n", "if", "self", ".", "atari", "or", "self", ".", "minatar", ":", "\n", "            ", "self", ".", "policyNetwork", "=", "FQF_Model_Atari", "(", "observationSpace", ",", "actionSpace", ",", "self", ".", "NCos", ",", "self", ".", "device", ",", "minAtar", "=", "self", ".", "minatar", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "targetNetwork", "=", "FQF_Model_Atari", "(", "observationSpace", ",", "actionSpace", ",", "self", ".", "NCos", ",", "self", ".", "device", ",", "minAtar", "=", "self", ".", "minatar", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "stateEmbedding", "=", "self", ".", "policyNetwork", ".", "getEmbeddingSize", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "policyNetwork", "=", "FQF_Model", "(", "observationSpace", ",", "actionSpace", ",", "parameters", "[", "'structureDNN'", "]", ",", "parameters", "[", "'stateEmbedding'", "]", ",", "self", ".", "NCos", ",", "self", ".", "device", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "targetNetwork", "=", "FQF_Model", "(", "observationSpace", ",", "actionSpace", ",", "parameters", "[", "'structureDNN'", "]", ",", "parameters", "[", "'stateEmbedding'", "]", ",", "self", ".", "NCos", ",", "self", ".", "device", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "stateEmbedding", "=", "parameters", "[", "'stateEmbedding'", "]", "\n", "", "self", ".", "targetNetwork", ".", "load_state_dict", "(", "self", ".", "policyNetwork", ".", "state_dict", "(", ")", ")", "\n", "\n", "# Set the Deep Learning optimizer", "\n", "self", ".", "optimizer", "=", "optim", ".", "Adam", "(", "self", ".", "policyNetwork", ".", "parameters", "(", ")", ",", "lr", "=", "self", ".", "learningRate", ",", "eps", "=", "self", ".", "epsilon", ")", "\n", "\n", "# Set the Fraction Proposal Network of the FQF algorithm + associated parameters", "\n", "self", ".", "fractionProposalNetwork", "=", "FQF_Model_Bis", "(", "stateEmbedding", ",", "self", ".", "N", ",", "self", ".", "device", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "optimizerFPN", "=", "optim", ".", "RMSprop", "(", "self", ".", "fractionProposalNetwork", ".", "parameters", "(", ")", ",", "lr", "=", "0.000000001", ",", "alpha", "=", "0.95", ",", "eps", "=", "0.00001", ")", "\n", "self", ".", "entropyCoefficient", "=", "0.001", "\n", "\n", "# Set the Epsilon-Greedy exploration technique", "\n", "self", ".", "epsilonStart", "=", "parameters", "[", "'epsilonStart'", "]", "\n", "self", ".", "epsilonEnd", "=", "parameters", "[", "'epsilonEnd'", "]", "\n", "self", ".", "epsilonDecay", "=", "parameters", "[", "'epsilonDecay'", "]", "\n", "self", ".", "epsilonTest", "=", "parameters", "[", "'epsilonTest'", "]", "\n", "self", ".", "epsilonValue", "=", "lambda", "iteration", ":", "self", ".", "epsilonEnd", "+", "(", "self", ".", "epsilonStart", "-", "self", ".", "epsilonEnd", ")", "*", "math", ".", "exp", "(", "-", "1", "*", "iteration", "/", "self", ".", "epsilonDecay", ")", "\n", "\n", "# Initialization of the experiment folder and tensorboard writer", "\n", "self", ".", "initReporting", "(", "parameters", ",", "'FQF'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.FQF.FQF.chooseAction": [[137, 177], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.from_numpy().float().to().unsqueeze", "torch.from_numpy().float().to().unsqueeze", "torch.from_numpy().float().to().unsqueeze", "torch.from_numpy().float().to().unsqueeze", "FQF.FQF.policyNetwork.embedding", "FQF.FQF.fractionProposalNetwork", "FQF.FQF.policyNetwork", "quantiles.squeeze().cpu().numpy.squeeze().cpu().numpy.mean", "QValues.squeeze().cpu().numpy.squeeze().cpu().numpy.max", "action.item", "matplotlib.pyplot.figure", "matplotlib.pyplot.figure.add_subplot", "torch.linspace().to", "torch.linspace().to", "torch.linspace().to", "torch.linspace().to", "FQF.FQF.policyNetwork", "quantiles.squeeze().cpu().numpy.squeeze().cpu().numpy.mean", "taus.cpu().numpy.cpu().numpy.cpu().numpy", "quantiles.squeeze().cpu().numpy.squeeze().cpu().numpy.squeeze().cpu().numpy", "QValues.squeeze().cpu().numpy.squeeze().cpu().numpy.squeeze().cpu().numpy", "range", "plt.figure.add_subplot.set_xlabel", "plt.figure.add_subplot.set_ylabel", "plt.figure.add_subplot.legend", "matplotlib.pyplot.show", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "torch.from_numpy().float().to", "taus.cpu().numpy.cpu().numpy.unsqueeze", "plt.figure.add_subplot.plot", "plt.figure.add_subplot.axhline", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "taus.cpu().numpy.cpu().numpy.cpu", "quantiles.squeeze().cpu().numpy.squeeze().cpu().numpy.squeeze().cpu", "QValues.squeeze().cpu().numpy.squeeze().cpu().numpy.squeeze().cpu", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "quantiles.squeeze().cpu().numpy.squeeze().cpu().numpy.squeeze", "QValues.squeeze().cpu().numpy.squeeze().cpu().numpy.squeeze", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "str", "str"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.FQF_Model_Atari.FQF_Model_Atari.embedding"], ["", "def", "chooseAction", "(", "self", ",", "state", ",", "plot", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Choose a valid RL action from the action space according to the\n              RL policy as well as the current RL state observed.\n        \n        INPUTS: - state: RL state returned by the environment.\n                - plot: Enable the plotting of the random returns distributions.\n        \n        OUTPUTS: - action: RL action chosen from the action space.\n        \"\"\"", "\n", "\n", "# Choose the best action based on the RL policy", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "state", "=", "torch", ".", "from_numpy", "(", "state", ")", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", ".", "unsqueeze", "(", "0", ")", "\n", "stateEmbedding", "=", "self", ".", "policyNetwork", ".", "embedding", "(", "state", ")", "\n", "_", ",", "tausBis", ",", "_", "=", "self", ".", "fractionProposalNetwork", "(", "stateEmbedding", ")", "\n", "quantiles", "=", "self", ".", "policyNetwork", "(", "state", ",", "tausBis", ",", "stateEmbedding", ")", "\n", "QValues", "=", "quantiles", ".", "mean", "(", "2", ")", "\n", "_", ",", "action", "=", "QValues", ".", "max", "(", "1", ")", "\n", "\n", "# If required, plot the return distribution associated with each action", "\n", "if", "plot", ":", "\n", "                ", "colors", "=", "[", "'blue'", ",", "'red'", ",", "'orange'", ",", "'green'", ",", "'purple'", ",", "'brown'", "]", "\n", "fig", "=", "plt", ".", "figure", "(", ")", "\n", "ax", "=", "fig", ".", "add_subplot", "(", ")", "\n", "taus", "=", "torch", ".", "linspace", "(", "0.0", ",", "1.0", ",", "10000", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "quantiles", "=", "self", ".", "policyNetwork", "(", "state", ",", "taus", ".", "unsqueeze", "(", "0", ")", ",", "stateEmbedding", ")", "\n", "QValues", "=", "quantiles", ".", "mean", "(", "2", ")", "\n", "taus", "=", "taus", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "quantiles", "=", "quantiles", ".", "squeeze", "(", "0", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "QValues", "=", "QValues", ".", "squeeze", "(", "0", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "for", "a", "in", "range", "(", "self", ".", "actionSpace", ")", ":", "\n", "                    ", "ax", ".", "plot", "(", "taus", ",", "quantiles", "[", "a", "]", ",", "linestyle", "=", "'-'", ",", "label", "=", "''", ".", "join", "(", "[", "'Action '", ",", "str", "(", "a", ")", ",", "' random return Z'", "]", ")", ",", "color", "=", "colors", "[", "a", "]", ")", "\n", "ax", ".", "axhline", "(", "y", "=", "QValues", "[", "a", "]", ",", "linewidth", "=", "2", ",", "linestyle", "=", "'--'", ",", "label", "=", "''", ".", "join", "(", "[", "'Action '", ",", "str", "(", "a", ")", ",", "' expected return Q'", "]", ")", ",", "color", "=", "colors", "[", "a", "]", ")", "\n", "", "ax", ".", "set_xlabel", "(", "'Quantile fraction'", ")", "\n", "ax", ".", "set_ylabel", "(", "'Quantile Function (QF)'", ")", "\n", "ax", ".", "legend", "(", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n", "", "return", "action", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.None.FQF.FQF.learning": [[179, 251], ["len", "FQF.FQF.dataLoaderIter.next", "batch[].float().to", "batch[].long().to", "batch[].float().to", "batch[].float().to", "batch[].float().to", "FQF.FQF.policyNetwork.embedding", "FQF.FQF.fractionProposalNetwork", "FQF.FQF.policyNetwork", "batch[].long().to.view().expand", "quantiles.gather().squeeze.gather().squeeze.gather().squeeze", "difference.abs", "torch.where", "torch.where", "torch.where", "torch.where", "loss.mean().sum().mean.mean().sum().mean.mean().sum().mean", "FQF.FQF.optimizerFPN.zero_grad", "fractionalLoss.backward", "FQF.FQF.optimizerFPN.step", "FQF.FQF.optimizer.zero_grad", "loss.mean().sum().mean.mean().sum().mean.backward", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "FQF.FQF.optimizer.step", "loss.mean().sum().mean.mean().sum().mean.item", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "FQF.FQF.policyNetwork", "batch[].long().to.view().expand", "quantilesBis.gather().squeeze.gather().squeeze.gather().squeeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "entropy.mean", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "FQF.FQF.targetNetwork.embedding", "FQF.FQF.targetNetwork", "[].view().expand", "nextQuantiles.gather().squeeze.gather().squeeze.gather().squeeze", "targetQuantiles.unsqueeze", "quantiles.gather().squeeze.gather().squeeze.unsqueeze", "FQF.FQF.policyNetwork.parameters", "batch[].float", "batch[].long", "batch[].float", "batch[].float", "batch[].float", "batch[].long().to.view", "quantiles.gather().squeeze.gather().squeeze.gather", "batch[].float().to.unsqueeze", "difference.abs.pow", "loss.mean().sum().mean.mean().sum().mean.mean().sum", "batch[].long().to.view", "quantilesBis.gather().squeeze.gather().squeeze.gather", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "[].view", "nextQuantiles.gather().squeeze.gather().squeeze.gather", "batch[].float().to.unsqueeze", "loss.mean().sum().mean.mean().sum().mean.mean", "tausBis.unsqueeze", "nextQuantiles.gather().squeeze.gather().squeeze.mean().max", "nextQuantiles.gather().squeeze.gather().squeeze.mean"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.FQF_Model_Atari.FQF_Model_Atari.embedding", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorld.StochasticGridWorld.step", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorld.StochasticGridWorld.step", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.FQF_Model_Atari.FQF_Model_Atari.embedding"], ["", "", "def", "learning", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Sample a batch of past experiences and learn from it\n              by updating the Reinforcement Learning policy.\n        \n        INPUTS: /\n        \n        OUTPUTS: - loss: Loss of the learning procedure.\n        \"\"\"", "\n", "\n", "# Check that the replay memory is filled enough", "\n", "if", "(", "len", "(", "self", ".", "replayMemory", ")", ">=", "self", ".", "batchSize", ")", ":", "\n", "\n", "# Sample a batch of experiences from the replay memory", "\n", "            ", "batch", "=", "self", ".", "dataLoaderIter", ".", "next", "(", ")", "\n", "state", "=", "batch", "[", "0", "]", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "action", "=", "batch", "[", "1", "]", ".", "long", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "reward", "=", "batch", "[", "2", "]", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "nextState", "=", "batch", "[", "3", "]", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "done", "=", "batch", "[", "4", "]", ".", "float", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "# Computation of the current return distribution", "\n", "stateEmbedding", "=", "self", ".", "policyNetwork", ".", "embedding", "(", "state", ")", "\n", "taus", ",", "tausBis", ",", "entropy", "=", "self", ".", "fractionProposalNetwork", "(", "stateEmbedding", ")", "\n", "quantiles", "=", "self", ".", "policyNetwork", "(", "state", ",", "tausBis", ",", "stateEmbedding", ")", "\n", "actionBis", "=", "action", ".", "view", "(", "self", ".", "batchSize", ",", "1", ",", "1", ")", ".", "expand", "(", "self", ".", "batchSize", ",", "1", ",", "self", ".", "N", ")", "\n", "quantiles", "=", "quantiles", ".", "gather", "(", "1", ",", "actionBis", ")", ".", "squeeze", "(", "1", ")", "\n", "\n", "# Computation of the Fractional loss for the FPN", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "quantilesBis", "=", "self", ".", "policyNetwork", "(", "state", ",", "taus", "[", ":", ",", "1", ":", "-", "1", "]", ",", "stateEmbedding", ")", "\n", "actionBis", "=", "action", ".", "view", "(", "self", ".", "batchSize", ",", "1", ",", "1", ")", ".", "expand", "(", "self", ".", "batchSize", ",", "1", ",", "self", ".", "N", "-", "1", ")", "\n", "quantilesBis", "=", "quantilesBis", ".", "gather", "(", "1", ",", "actionBis", ")", ".", "squeeze", "(", "1", ")", "\n", "", "gradients1", "=", "quantilesBis", "-", "quantiles", "[", ":", ",", ":", "-", "1", "]", "\n", "gradients2", "=", "quantilesBis", "-", "quantiles", "[", ":", ",", "1", ":", "]", "\n", "flag1", "=", "quantilesBis", ">", "torch", ".", "cat", "(", "[", "quantiles", "[", ":", ",", ":", "1", "]", ",", "quantilesBis", "[", ":", ",", ":", "-", "1", "]", "]", ",", "dim", "=", "1", ")", "\n", "flag2", "=", "quantilesBis", "<", "torch", ".", "cat", "(", "[", "quantilesBis", "[", ":", ",", "1", ":", "]", ",", "quantiles", "[", ":", ",", "-", "1", ":", "]", "]", ",", "dim", "=", "1", ")", "\n", "gradients", "=", "(", "torch", ".", "where", "(", "flag1", ",", "gradients1", ",", "-", "gradients1", ")", "+", "torch", ".", "where", "(", "flag2", ",", "gradients2", ",", "-", "gradients2", ")", ")", ".", "view", "(", "self", ".", "batchSize", ",", "self", ".", "N", "-", "1", ")", "\n", "fractionalLoss", "=", "(", "gradients", "*", "taus", "[", ":", ",", "1", ":", "-", "1", "]", ")", ".", "sum", "(", "dim", "=", "1", ")", ".", "mean", "(", ")", "\n", "fractionalLoss", "+=", "self", ".", "entropyCoefficient", "*", "entropy", ".", "mean", "(", ")", "\n", "\n", "# Computation of the new distribution to be learnt by the policy DNN", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "nextStateEmbedding", "=", "self", ".", "targetNetwork", ".", "embedding", "(", "nextState", ")", "\n", "nextQuantiles", "=", "self", ".", "targetNetwork", "(", "nextState", ",", "tausBis", ",", "nextStateEmbedding", ")", "\n", "nextAction", "=", "nextQuantiles", ".", "mean", "(", "2", ")", ".", "max", "(", "1", ")", "[", "1", "]", ".", "view", "(", "self", ".", "batchSize", ",", "1", ",", "1", ")", ".", "expand", "(", "self", ".", "batchSize", ",", "1", ",", "self", ".", "N", ")", "\n", "nextQuantiles", "=", "nextQuantiles", ".", "gather", "(", "1", ",", "nextAction", ")", ".", "squeeze", "(", "1", ")", "\n", "targetQuantiles", "=", "reward", ".", "unsqueeze", "(", "1", ")", "+", "self", ".", "gamma", "*", "nextQuantiles", "*", "(", "1", "-", "done", ".", "unsqueeze", "(", "1", ")", ")", "\n", "\n", "# Computation of the quantile huber loss", "\n", "", "difference", "=", "targetQuantiles", ".", "unsqueeze", "(", "1", ")", "-", "quantiles", ".", "unsqueeze", "(", "2", ")", "\n", "error", "=", "difference", ".", "abs", "(", ")", "\n", "loss", "=", "torch", ".", "where", "(", "error", "<=", "self", ".", "kappa", ",", "0.5", "*", "error", ".", "pow", "(", "2", ")", ",", "self", ".", "kappa", "*", "(", "error", "-", "(", "0.5", "*", "self", ".", "kappa", ")", ")", ")", "\n", "loss", "=", "(", "tausBis", ".", "unsqueeze", "(", "2", ")", "-", "(", "difference", "<", "0", ")", ".", "float", "(", ")", ")", ".", "abs", "(", ")", "*", "loss", "/", "self", ".", "kappa", "\n", "loss", "=", "loss", ".", "mean", "(", "1", ")", ".", "sum", "(", "1", ")", ".", "mean", "(", ")", "\n", "\n", "# Update of the Fraction Proposal Network parameters", "\n", "self", ".", "optimizerFPN", ".", "zero_grad", "(", ")", "\n", "fractionalLoss", ".", "backward", "(", "retain_graph", "=", "True", ")", "\n", "self", ".", "optimizerFPN", ".", "step", "(", ")", "\n", "\n", "# Computation of the gradients", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "\n", "# Gradient Clipping", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "policyNetwork", ".", "parameters", "(", ")", ",", "self", ".", "gradientClipping", ")", "\n", "\n", "# Perform the Deep Neural Network optimization", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "return", "loss", ".", "item", "(", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorldOptimal.StochasticGridWorldOptimal.__init__": [[17, 32], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "environment", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Perform the initialization of the class.\n        \n        INPUTS: - environment: Stochastic grid world environment considered.\n        \n        OUTPUTS: - processState: Preprocessing of the RL state.\n                 - chooseAction: Choose the optimal RL action.\n        \"\"\"", "\n", "\n", "# Initialization of important variables", "\n", "self", ".", "environment", "=", "environment", "\n", "self", ".", "size", "=", "self", ".", "environment", ".", "size", "\n", "self", ".", "trapPosition", "=", "self", ".", "environment", ".", "trapPosition", "\n", "self", ".", "targetPosition", "=", "self", ".", "environment", ".", "targetPosition", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorldOptimal.StochasticGridWorldOptimal.processState": [[34, 44], ["None"], "methods", ["None"], ["", "def", "processState", "(", "self", ",", "state", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Potentially process the RL state returned by the environment.\n        \n        INPUTS: - state: RL state returned by the environment.\n        \n        OUTPUTS: - state: RL state processed.\n        \"\"\"", "\n", "\n", "return", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorldOptimal.StochasticGridWorldOptimal.chooseAction": [[46, 80], ["None"], "methods", ["None"], ["", "def", "chooseAction", "(", "self", ",", "state", ",", "plot", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Choose the optimal RL action.\n        \n        INPUTS: - state: RL state returned by the environment.\n                - plot: False, because not supported.\n        \n        OUTPUTS: - action: RL action selected.\n        \"\"\"", "\n", "\n", "# Retrieve the coordinates of the agent", "\n", "x", "=", "state", "[", "0", "]", "\n", "y", "=", "state", "[", "1", "]", "\n", "\n", "# Implementation of the optimal policy", "\n", "if", "x", "==", "self", ".", "targetPosition", "[", "0", "]", "and", "y", "<", "self", ".", "trapPosition", "[", "1", "]", ":", "\n", "            ", "action", "=", "0", "\n", "", "elif", "x", "==", "self", ".", "targetPosition", "[", "0", "]", "and", "y", ">", "self", ".", "trapPosition", "[", "1", "]", ":", "\n", "            ", "action", "=", "3", "\n", "", "elif", "y", "==", "self", ".", "targetPosition", "[", "1", "]", "and", "x", "<", "self", ".", "targetPosition", "[", "0", "]", ":", "\n", "            ", "action", "=", "0", "\n", "", "elif", "y", "==", "self", ".", "targetPosition", "[", "1", "]", "and", "x", ">", "self", ".", "targetPosition", "[", "0", "]", ":", "\n", "            ", "action", "=", "2", "\n", "", "elif", "(", "x", "<", "self", ".", "targetPosition", "[", "0", "]", "or", "x", ">", "self", ".", "targetPosition", "[", "0", "]", ")", "and", "y", "<", "(", "self", ".", "targetPosition", "[", "1", "]", "-", "1", ")", ":", "\n", "            ", "action", "=", "3", "\n", "", "elif", "y", "==", "(", "self", ".", "targetPosition", "[", "1", "]", "-", "1", ")", "and", "y", ">", "self", ".", "trapPosition", "[", "1", "]", "and", "x", "<", "self", ".", "targetPosition", "[", "0", "]", ":", "\n", "            ", "action", "=", "0", "\n", "", "elif", "y", "==", "(", "self", ".", "targetPosition", "[", "1", "]", "-", "1", ")", "and", "y", ">", "self", ".", "trapPosition", "[", "1", "]", "and", "x", ">", "self", ".", "targetPosition", "[", "0", "]", ":", "\n", "            ", "action", "=", "2", "\n", "", "else", ":", "\n", "            ", "action", "=", "3", "\n", "\n", "# Return of the RL action selected", "\n", "", "return", "action", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorld.StochasticGridWorld.__init__": [[60, 100], ["gym.Env.__init__", "random.seed", "gym.spaces.Box", "gym.spaces.Discrete", "int", "int", "numpy.array", "time.time", "int", "int", "int", "int", "int", "random.random", "random.random", "random.random", "random.random"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.MonotonicNN.__init__"], ["def", "__init__", "(", "self", ",", "size", "=", "size", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Perform the initialization of the RL environment.\n        \n        INPUTS: - size: Size of the square grid world.\n        \n        OUTPUTS: /\n        \"\"\"", "\n", "\n", "super", "(", "StochasticGridWorld", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# Initialize the random function with a fixed random seed", "\n", "random", ".", "seed", "(", "time", ".", "time", "(", ")", ")", "\n", "\n", "# Definition of the observation/state and action spaces", "\n", "self", ".", "observation_space", "=", "spaces", ".", "Box", "(", "low", "=", "0", ",", "high", "=", "size", "-", "1", ",", "shape", "=", "(", "2", ",", "1", ")", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "self", ".", "action_space", "=", "spaces", ".", "Discrete", "(", "4", ")", "\n", "self", ".", "size", "=", "size", "\n", "\n", "# Initialization of the traps and target positions", "\n", "self", ".", "trapPosition", "=", "[", "int", "(", "self", ".", "size", "/", "2", ")", ",", "int", "(", "self", ".", "size", "/", "2", ")", "]", "\n", "self", ".", "targetPosition", "=", "[", "int", "(", "self", ".", "size", "/", "2", ")", ",", "self", ".", "size", "-", "1", "]", "\n", "\n", "# Initialization of the player position", "\n", "x", "=", "int", "(", "random", ".", "random", "(", ")", "*", "(", "self", ".", "size", "-", "1", ")", ")", "\n", "y", "=", "int", "(", "random", ".", "random", "(", ")", "*", "(", "self", ".", "size", "-", "1", ")", ")", "\n", "self", ".", "playerPosition", "=", "[", "x", ",", "y", "]", "\n", "while", "self", ".", "playerPosition", "==", "self", ".", "targetPosition", "or", "self", ".", "playerPosition", "==", "self", ".", "trapPosition", ":", "\n", "            ", "x", "=", "int", "(", "random", ".", "random", "(", ")", "*", "(", "self", ".", "size", "-", "1", ")", ")", "\n", "y", "=", "int", "(", "random", ".", "random", "(", ")", "*", "(", "self", ".", "size", "-", "1", ")", ")", "\n", "self", ".", "playerPosition", "=", "[", "x", ",", "y", "]", "\n", "\n", "# Initialization of the time elapsed", "\n", "", "self", ".", "timeElapsed", "=", "0", "\n", "\n", "# Initialization of the RL variables", "\n", "self", ".", "state", "=", "np", ".", "array", "(", "[", "self", ".", "playerPosition", "[", "0", "]", ",", "self", ".", "playerPosition", "[", "1", "]", "]", ")", "\n", "self", ".", "reward", "=", "0.", "\n", "self", ".", "done", "=", "0", "\n", "self", ".", "info", "=", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorld.StochasticGridWorld.reset": [[102, 128], ["int", "int", "numpy.array", "int", "int", "random.random", "random.random", "random.random", "random.random"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Perform a reset of the RL environment.\n        \n        INPUTS: /\n        \n        OUTPUTS: - state: RL state or observation.\n        \"\"\"", "\n", "\n", "# Reset of the player position and time elapsed", "\n", "x", "=", "int", "(", "random", ".", "random", "(", ")", "*", "(", "self", ".", "size", "-", "1", ")", ")", "\n", "y", "=", "int", "(", "random", ".", "random", "(", ")", "*", "(", "self", ".", "size", "-", "1", ")", ")", "\n", "self", ".", "playerPosition", "=", "[", "x", ",", "y", "]", "\n", "while", "self", ".", "playerPosition", "==", "self", ".", "targetPosition", "or", "self", ".", "playerPosition", "==", "self", ".", "trapPosition", ":", "\n", "            ", "x", "=", "int", "(", "random", ".", "random", "(", ")", "*", "(", "self", ".", "size", "-", "1", ")", ")", "\n", "y", "=", "int", "(", "random", ".", "random", "(", ")", "*", "(", "self", ".", "size", "-", "1", ")", ")", "\n", "self", ".", "playerPosition", "=", "[", "x", ",", "y", "]", "\n", "", "self", ".", "timeElapsed", "=", "0", "\n", "\n", "# Reset of the RL variables", "\n", "self", ".", "state", "=", "np", ".", "array", "(", "[", "self", ".", "playerPosition", "[", "0", "]", ",", "self", ".", "playerPosition", "[", "1", "]", "]", ")", "\n", "self", ".", "reward", "=", "0.", "\n", "self", ".", "done", "=", "0", "\n", "self", ".", "info", "=", "{", "}", "\n", "\n", "return", "self", ".", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorld.StochasticGridWorld.step": [[130, 195], ["random.random", "numpy.array", "min", "numpy.random.normal", "max", "numpy.random.normal", "max", "numpy.random.normal", "min", "print"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Update the RL environment according to the agent's action.\n        \n        INPUTS: - action: RL action outputted by the agent.\n        \n        OUTPUTS: - state: RL state or observation.\n                 - reward: RL reward signal.\n                 - done: RL termination signal.\n                 - info: Additional RL information.\n        \"\"\"", "\n", "\n", "# Stochasticity associated with the next move of the agent", "\n", "rand", "=", "random", ".", "random", "(", ")", "\n", "if", "rand", ">", "doubleProbability", ":", "\n", "            ", "moveRange", "=", "1", "\n", "", "else", ":", "\n", "            ", "moveRange", "=", "2", "\n", "\n", "# Go right", "\n", "", "if", "action", "==", "0", ":", "\n", "            ", "self", ".", "playerPosition", "[", "0", "]", "=", "min", "(", "self", ".", "playerPosition", "[", "0", "]", "+", "moveRange", ",", "self", ".", "size", "-", "1", ")", "\n", "# Go down", "\n", "", "elif", "action", "==", "1", ":", "\n", "            ", "self", ".", "playerPosition", "[", "1", "]", "=", "max", "(", "self", ".", "playerPosition", "[", "1", "]", "-", "moveRange", ",", "0", ")", "\n", "# Go left", "\n", "", "elif", "action", "==", "2", ":", "\n", "            ", "self", ".", "playerPosition", "[", "0", "]", "=", "max", "(", "self", ".", "playerPosition", "[", "0", "]", "-", "moveRange", ",", "0", ")", "\n", "# Go up", "\n", "", "elif", "action", "==", "3", ":", "\n", "            ", "self", ".", "playerPosition", "[", "1", "]", "=", "min", "(", "self", ".", "playerPosition", "[", "1", "]", "+", "moveRange", ",", "self", ".", "size", "-", "1", ")", "\n", "# Invalid action", "\n", "", "else", ":", "\n", "            ", "print", "(", "\"Error: invalid action...\"", ")", "\n", "\n", "# Incrementation of the time elapsed", "\n", "", "self", ".", "timeElapsed", "+=", "1", "\n", "\n", "# Assign the appropriate RL reward", "\n", "if", "stochasticRewards", ":", "\n", "            ", "self", ".", "reward", "=", "np", ".", "random", ".", "normal", "(", "loc", "=", "0.0", ",", "scale", "=", "0.1", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "reward", "=", "0.0", "\n", "", "if", "self", ".", "playerPosition", "==", "self", ".", "targetPosition", ":", "\n", "            ", "if", "stochasticRewards", ":", "\n", "                ", "self", ".", "reward", "=", "np", ".", "random", ".", "normal", "(", "loc", "=", "1.0", ",", "scale", "=", "0.1", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "reward", "=", "1.0", "\n", "", "self", ".", "done", "=", "1", "\n", "", "elif", "self", ".", "playerPosition", "==", "self", ".", "trapPosition", ":", "\n", "            ", "if", "stochasticRewards", ":", "\n", "                ", "self", ".", "reward", "=", "np", ".", "random", ".", "normal", "(", "loc", "=", "-", "1.0", ",", "scale", "=", "0.1", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "reward", "=", "-", "1.0", "\n", "", "self", ".", "done", "=", "1", "\n", "\n", "# Check if the time elapsed reaches the time limit", "\n", "", "if", "self", ".", "timeElapsed", ">=", "timeOut", ":", "\n", "            ", "self", ".", "done", "=", "1", "\n", "\n", "# Update of the RL state", "\n", "", "self", ".", "state", "=", "np", ".", "array", "(", "[", "self", ".", "playerPosition", "[", "0", "]", ",", "self", ".", "playerPosition", "[", "1", "]", "]", ")", "\n", "\n", "# Return of the RL variables", "\n", "return", "self", ".", "state", ",", "self", ".", "reward", ",", "self", ".", "done", ",", "self", ".", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorld.StochasticGridWorld.render": [[197, 218], ["matplotlib.pyplot.figure", "matplotlib.pyplot.figure.gca", "plt.figure.gca.set_xticks", "plt.figure.gca.set_yticks", "plt.figure.gca.set", "matplotlib.pyplot.scatter", "matplotlib.pyplot.scatter", "matplotlib.pyplot.scatter", "matplotlib.pyplot.grid", "matplotlib.pyplot.text", "matplotlib.pyplot.show", "numpy.arange", "numpy.arange", "str"], "methods", ["None"], ["", "def", "render", "(", "self", ",", "mode", "=", "'human'", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Render graphically the current state of the RL environment.\n        \n        INPUTS: /\n        \n        OUTPUTS: /\n        \"\"\"", "\n", "\n", "fig", "=", "plt", ".", "figure", "(", "figsize", "=", "(", "8", ",", "8", ")", ")", "\n", "ax", "=", "fig", ".", "gca", "(", ")", "\n", "ax", ".", "set_xticks", "(", "np", ".", "arange", "(", "0", ",", "self", ".", "size", "+", "1", ",", "1", ")", ")", "\n", "ax", ".", "set_yticks", "(", "np", ".", "arange", "(", "0", ",", "self", ".", "size", "+", "1", ",", "1", ")", ")", "\n", "ax", ".", "set", "(", "xlim", "=", "(", "0", ",", "self", ".", "size", ")", ",", "ylim", "=", "(", "0", ",", "self", ".", "size", ")", ")", "\n", "plt", ".", "scatter", "(", "self", ".", "playerPosition", "[", "0", "]", "+", "0.5", ",", "self", ".", "playerPosition", "[", "1", "]", "+", "0.5", ",", "s", "=", "100", ",", "color", "=", "'blue'", ")", "\n", "plt", ".", "scatter", "(", "self", ".", "targetPosition", "[", "0", "]", "+", "0.5", ",", "self", ".", "targetPosition", "[", "1", "]", "+", "0.5", ",", "s", "=", "100", ",", "color", "=", "'green'", ")", "\n", "plt", ".", "scatter", "(", "self", ".", "trapPosition", "[", "0", "]", "+", "0.5", ",", "self", ".", "trapPosition", "[", "1", "]", "+", "0.5", ",", "s", "=", "100", ",", "color", "=", "'red'", ")", "\n", "plt", ".", "grid", "(", ")", "\n", "text", "=", "''", ".", "join", "(", "[", "'Time elapsed: '", ",", "str", "(", "self", ".", "timeElapsed", ")", "]", ")", "\n", "plt", ".", "text", "(", "0", ",", "self", ".", "size", "+", "0.2", ",", "text", ",", "fontsize", "=", "12", ")", "\n", "plt", ".", "show", "(", ")", "\n", "#plt.savefig(\"Figures/Distributions/StochasticGridWorldState.pdf\", format=\"pdf\")", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorld.StochasticGridWorld.setState": [[221, 239], ["stochasticGridWorld.StochasticGridWorld.reset", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.CustomEnvironments.stochasticGridWorld.StochasticGridWorld.reset"], ["", "def", "setState", "(", "self", ",", "state", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Reset the RL environment and set a specific initial state.\n        \n        INPUTS: - state: Information about the state to set.\n        \n        OUTPUTS: - state: RL state of the environment.\n        \"\"\"", "\n", "\n", "# Reset of the environment", "\n", "self", ".", "reset", "(", ")", "\n", "\n", "# Set the initial state as specified", "\n", "self", ".", "timeElapsed", "=", "0", "\n", "self", ".", "playerPosition", "=", "[", "state", "[", "0", "]", ",", "state", "[", "1", "]", "]", "\n", "self", ".", "state", "=", "np", ".", "array", "(", "[", "self", ".", "playerPosition", "[", "0", "]", ",", "self", ".", "playerPosition", "[", "1", "]", "]", ")", "\n", "\n", "return", "self", ".", "state", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.FQF_Model_Bis.FQF_Model_Bis.__init__": [[29, 51], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Linear", "torch.Linear", "torch.LogSoftmax", "torch.LogSoftmax"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.MonotonicNN.__init__"], ["def", "__init__", "(", "self", ",", "numberOfInputs", ",", "numberOfOutputs", ",", "device", "=", "'cpu'", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Defining and initializing the Deep Neural Network.\n        \n        INPUTS: - numberOfInputs: Input shape (state embedding).\n                - numberOfOutputs: Output shape (number of quantile fractions).\n                - device: Running device (hardware acceleration).\n        \n        OUTPUTS: /\n        \"\"\"", "\n", "\n", "# Call the constructor of the parent class (Pytorch torch.nn.Module)", "\n", "super", "(", "FQF_Model_Bis", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# Initialization of useful variables", "\n", "self", ".", "device", "=", "device", "\n", "self", ".", "N", "=", "numberOfOutputs", "\n", "\n", "# Initialization of the Deep Neural Network.", "\n", "self", ".", "network", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "numberOfInputs", ",", "numberOfOutputs", ")", ",", "\n", "nn", ".", "LogSoftmax", "(", "dim", "=", "1", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.FQF_Model_Bis.FQF_Model_Bis.forward": [[54, 75], ["FQF_Model_Bis.FQF_Model_Bis.network", "torch.cumsum", "torch.cumsum", "torch.cumsum", "torch.cumsum", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "FQF_Model_Bis.FQF_Model_Bis.exp", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "FQF_Model_Bis.FQF_Model_Bis.exp"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Implementing the forward pass of the Deep Neural Network.\n        \n        INPUTS: - x: Input of the Deep Neural Network. (state embedding).\n        \n        OUTPUTS: - taus: Quantile fractions generated.\n                 - tausBis: Quantile fractions generated.\n                 - entropy: Entropy associated with the DNN output.\n        \"\"\"", "\n", "\n", "# Generation of quantile fractions", "\n", "out", "=", "self", ".", "network", "(", "x", ")", "\n", "taus", "=", "torch", ".", "cumsum", "(", "out", ".", "exp", "(", ")", ",", "dim", "=", "1", ")", "\n", "taus", "=", "torch", ".", "cat", "(", "(", "torch", ".", "zeros", "(", "(", "out", ".", "shape", "[", "0", "]", ",", "1", ")", ")", ".", "to", "(", "self", ".", "device", ")", ",", "taus", ")", ",", "dim", "=", "1", ")", "\n", "tausBis", "=", "(", "taus", "[", ":", ",", ":", "-", "1", "]", "+", "taus", "[", ":", ",", "1", ":", "]", ")", ".", "detach", "(", ")", "/", "2.", "\n", "\n", "# Computation of the associated entropy", "\n", "entropy", "=", "-", "(", "out", "*", "out", ".", "exp", "(", ")", ")", ".", "sum", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "\n", "return", "taus", ",", "tausBis", ",", "entropy", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.UMDQN_C_Model.UMDQN_C_Model.__init__": [[34, 57], ["torch.Module.__init__", "Models.FeedforwardDNN.FeedForwardDNN", "Models.MonotonicNN.MonotonicNN"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.MonotonicNN.__init__"], ["def", "__init__", "(", "self", ",", "numberOfInputs", ",", "numberOfOutputs", ",", "\n", "structureDNN", ",", "structureUMNN", ",", "stateEmbedding", ",", "\n", "numberOfSteps", ",", "device", "=", "'cpu'", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Defining and initializing the Deep Neural Network.\n        \n        INPUTS: - numberOfInputs: Number of inputs of the Deep Neural Network.\n                - numberOfOutputs: Number of outputs of the Deep Neural Network.\n                - structureDNN: Structure of the feedforward DNN for state embedding.\n                - structureUMNN: Structure of the UMNN for distribution representation.\n                - stateEmbedding: Dimension of the state embedding.\n                - numberOfSteps: Number of integration steps for the UMNN.\n                - device: Hardware device (CPU or GPU).\n        \n        OUTPUTS: /\n        \"\"\"", "\n", "\n", "# Call the constructor of the parent class (Pytorch torch.nn.Module)", "\n", "super", "(", "UMDQN_C_Model", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# Initialization of the Deep Neural Network", "\n", "self", ".", "stateEmbeddingDNN", "=", "FeedForwardDNN", "(", "numberOfInputs", ",", "stateEmbedding", ",", "structureDNN", ")", "\n", "self", ".", "UMNN", "=", "MonotonicNN", "(", "stateEmbedding", "+", "1", ",", "structureUMNN", ",", "numberOfSteps", ",", "numberOfOutputs", ",", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.UMDQN_C_Model.UMDQN_C_Model.forward": [[59, 80], ["state.size", "UMDQN_C_Model.UMDQN_C_Model.stateEmbeddingDNN", "torch.sigmoid.repeat().view", "torch.sigmoid.repeat().view", "UMDQN_C_Model.UMDQN_C_Model.UMNN", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.sigmoid.size", "torch.sigmoid.size", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "torch.sigmoid.repeat", "torch.sigmoid.repeat", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "int", "len", "len"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "state", ",", "q", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Implementing the forward pass of the Deep Neural Network.\n        \n        INPUTS: - state: RL state.\n                - q: Samples of potential returns.\n        \n        OUTPUTS: - output: Output of the Deep Neural Network.\n        \"\"\"", "\n", "\n", "# State embedding part of the Deep Neural Network", "\n", "batchSize", "=", "state", ".", "size", "(", "0", ")", "\n", "x", "=", "self", ".", "stateEmbeddingDNN", "(", "state", ")", "\n", "x", "=", "x", ".", "repeat", "(", "1", ",", "int", "(", "len", "(", "q", ")", "/", "len", "(", "state", ")", ")", ")", ".", "view", "(", "-", "1", ",", "x", ".", "size", "(", "1", ")", ")", "\n", "\n", "# UMNNN part of the Deep Neural Network", "\n", "x", "=", "self", ".", "UMNN", "(", "q", ",", "x", ")", "\n", "\n", "# Sigmoid activation function + appropriate format", "\n", "x", "=", "torch", ".", "sigmoid", "(", "x", ")", "\n", "return", "torch", ".", "cat", "(", "torch", ".", "chunk", "(", "torch", ".", "transpose", "(", "x", ",", "0", ",", "1", ")", ",", "batchSize", ",", "dim", "=", "1", ")", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.UMDQN_C_Model.UMDQN_C_Model.getDerivative": [[82, 105], ["state.size", "UMDQN_C_Model.UMDQN_C_Model.stateEmbeddingDNN", "torch.sigmoid.repeat().view", "torch.sigmoid.repeat().view", "UMDQN_C_Model.UMDQN_C_Model.UMNN", "UMDQN_C_Model.UMDQN_C_Model.UMNN", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.sigmoid.size", "torch.sigmoid.size", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "torch.sigmoid.repeat", "torch.sigmoid.repeat", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "int", "len", "len"], "methods", ["None"], ["", "def", "getDerivative", "(", "self", ",", "state", ",", "q", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Get the derivative internally computed by the UMNN.\n        \n        INPUTS: - state: RL state.\n                - q: Samples of potential returns.\n        \n        OUTPUTS: - output: Derivative internally computed by the UMNN.\n        \"\"\"", "\n", "\n", "# State embedding part of the Deep Neural Network", "\n", "batchSize", "=", "state", ".", "size", "(", "0", ")", "\n", "x", "=", "self", ".", "stateEmbeddingDNN", "(", "state", ")", "\n", "x", "=", "x", ".", "repeat", "(", "1", ",", "int", "(", "len", "(", "q", ")", "/", "len", "(", "state", ")", ")", ")", ".", "view", "(", "-", "1", ",", "x", ".", "size", "(", "1", ")", ")", "\n", "\n", "# Computation of both PDF and CDF", "\n", "pdf", "=", "self", ".", "UMNN", "(", "q", ",", "x", ",", "only_derivative", "=", "True", ")", "\n", "cdf", "=", "self", ".", "UMNN", "(", "q", ",", "x", ",", "only_derivative", "=", "False", ")", "\n", "\n", "# Correction of the sigmoid + appropriate format", "\n", "x", "=", "torch", ".", "sigmoid", "(", "cdf", ")", "\n", "x", "=", "x", "*", "(", "1", "-", "x", ")", "*", "pdf", "\n", "return", "torch", ".", "cat", "(", "torch", ".", "chunk", "(", "torch", ".", "transpose", "(", "x", ",", "0", ",", "1", ")", ",", "batchSize", ",", "dim", "=", "1", ")", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.UMDQN_C_Model.UMDQN_C_Model.getExpectation": [[107, 125], ["UMDQN_C_Model.UMDQN_C_Model.stateEmbeddingDNN", "UMDQN_C_Model.UMDQN_C_Model.UMNN.expectation", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.MonotonicNN.expectation"], ["", "def", "getExpectation", "(", "self", ",", "state", ",", "minReturn", ",", "maxReturn", ",", "numberOfPoints", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Get the expectation of the PDF internally computed by the UMNN.\n        \n        INPUTS: - state: RL state.\n                - minReturn: Minimum return.\n                - maxReturn: Maximum return.\n                - numberOfPoints: Number of points for the computations (accuracy).\n        \n        OUTPUTS: - expectation: Expectation computed.\n        \"\"\"", "\n", "\n", "# State embedding part of the Deep Neural Network", "\n", "state", "=", "self", ".", "stateEmbeddingDNN", "(", "state", ")", "\n", "\n", "# Computation of the expectation of the PDF internally computed by the UMNN", "\n", "expectation", "=", "self", ".", "UMNN", ".", "expectation", "(", "state", ",", "lambda", "x", ":", "x", ",", "lambda", "x", ":", "torch", ".", "sigmoid", "(", "x", ")", "*", "(", "1", "-", "torch", ".", "sigmoid", "(", "x", ")", ")", ",", "minReturn", ",", "maxReturn", ",", "numberOfPoints", ")", "\n", "return", "expectation", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.CDQN_Model.CDQN_Model.__init__": [[30, 51], ["torch.Module.__init__", "int", "Models.FeedforwardDNN.FeedForwardDNN"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.MonotonicNN.__init__"], ["def", "__init__", "(", "self", ",", "numberOfInputs", ",", "numberOfOutputs", ",", "structure", ",", "numberOfAtoms", "=", "51", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Defining and initializing the Deep Neural Network.\n        \n        INPUTS: - numberOfInputs: Number of inputs of the Deep Neural Network.\n                - numberOfOutputs: Number of outputs of the Deep Neural Network.\n                - structure: Structure of the Deep Neural Network (hidden layers).\n                - numberOfAtoms: Number of atoms for the support (see C51 algorithm).\n        \n        OUTPUTS: /\n        \"\"\"", "\n", "\n", "# Call the constructor of the parent class (Pytorch torch.nn.Module)", "\n", "super", "(", "CDQN_Model", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# Initialization of useful variables", "\n", "self", ".", "numberOfAtoms", "=", "numberOfAtoms", "\n", "self", ".", "numberOfActions", "=", "int", "(", "numberOfOutputs", "/", "numberOfAtoms", ")", "\n", "\n", "# Initialization of the Deep Neural Network", "\n", "self", ".", "network", "=", "FeedForwardDNN", "(", "numberOfInputs", ",", "numberOfOutputs", ",", "structure", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.CDQN_Model.CDQN_Model.forward": [[53, 65], ["CDQN_Model.CDQN_Model.network", "torch.softmax", "torch.softmax", "torch.softmax.clamp", "CDQN_Model.CDQN_Model.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Implementing the forward pass of the Deep Neural Network.\n        \n        INPUTS: - x: Input of the Deep Neural Network.\n        \n        OUTPUTS: - y: Output of the Deep Neural Network.\n        \"\"\"", "\n", "\n", "x", "=", "self", ".", "network", "(", "x", ")", "\n", "y", "=", "F", ".", "softmax", "(", "x", ".", "view", "(", "-", "1", ",", "self", ".", "numberOfActions", ",", "self", ".", "numberOfAtoms", ")", ",", "dim", "=", "-", "1", ")", "\n", "return", "y", ".", "clamp", "(", "min", "=", "1e-6", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.QR_DQN_Model.QR_DQN_Model.__init__": [[29, 50], ["torch.Module.__init__", "int", "Models.FeedforwardDNN.FeedForwardDNN"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.MonotonicNN.__init__"], ["def", "__init__", "(", "self", ",", "numberOfInputs", ",", "numberOfOutputs", ",", "structure", ",", "numberOfQuantiles", "=", "200", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Defining and initializing the Deep Neural Network.\n        \n        INPUTS: - numberOfInputs: Number of inputs of the Deep Neural Network.\n                - numberOfOutputs: Number of outputs of the Deep Neural Network.\n                - structure: Structure of the Deep Neural Network (hidden layers).\n                - numberOfQuantiles: Number of quantiles for approximating the distribution.\n        \n        OUTPUTS: /\n        \"\"\"", "\n", "\n", "# Call the constructor of the parent class (Pytorch torch.nn.Module)", "\n", "super", "(", "QR_DQN_Model", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# Initialization of useful variables", "\n", "self", ".", "numberOfQuantiles", "=", "numberOfQuantiles", "\n", "self", ".", "numberOfActions", "=", "int", "(", "numberOfOutputs", "/", "numberOfQuantiles", ")", "\n", "\n", "# Initialization of the Deep Neural Network.", "\n", "self", ".", "network", "=", "FeedForwardDNN", "(", "numberOfInputs", ",", "numberOfOutputs", ",", "structure", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.QR_DQN_Model.QR_DQN_Model.forward": [[52, 63], ["QR_DQN_Model.QR_DQN_Model.network", "QR_DQN_Model.QR_DQN_Model.view", "QR_DQN_Model.QR_DQN_Model.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Implementing the forward pass of the Deep Neural Network.\n        \n        INPUTS: - x: Input of the Deep Neural Network.\n        \n        OUTPUTS: - y: Output of the Deep Neural Network.\n        \"\"\"", "\n", "\n", "x", "=", "self", ".", "network", "(", "x", ")", "\n", "return", "x", ".", "view", "(", "x", ".", "size", "(", "0", ")", ",", "self", ".", "numberOfActions", ",", "self", ".", "numberOfQuantiles", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.UMDQN_W_Model.UMDQN_W_Model.__init__": [[32, 55], ["torch.Module.__init__", "Models.FeedforwardDNN.FeedForwardDNN", "Models.MonotonicNN.MonotonicNN"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.MonotonicNN.__init__"], ["def", "__init__", "(", "self", ",", "numberOfInputs", ",", "numberOfOutputs", ",", "\n", "structureDNN", ",", "structureUMNN", ",", "stateEmbedding", ",", "\n", "numberOfSteps", ",", "device", "=", "'cpu'", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Defining and initializing the Deep Neural Network.\n        \n        INPUTS: - numberOfInputs: Number of inputs of the Deep Neural Network.\n                - numberOfOutputs: Number of outputs of the Deep Neural Network.\n                - structureDNN: Structure of the feedforward DNN for state embedding.\n                - structureUMNN: Structure of the UMNN for distribution representation.\n                - stateEmbedding: Dimension of the state embedding.\n                - numberOfSteps: Number of integration steps for the UMNN.\n                - device: Hardware device (CPU or GPU).\n        \n        OUTPUTS: /\n        \"\"\"", "\n", "\n", "# Call the constructor of the parent class (Pytorch torch.nn.Module)", "\n", "super", "(", "UMDQN_W_Model", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# Initialization of the Deep Neural Network", "\n", "self", ".", "stateEmbeddingDNN", "=", "FeedForwardDNN", "(", "numberOfInputs", ",", "stateEmbedding", ",", "structureDNN", ")", "\n", "self", ".", "UMNN", "=", "MonotonicNN", "(", "stateEmbedding", "+", "1", ",", "structureUMNN", ",", "numberOfSteps", ",", "numberOfOutputs", ",", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.UMDQN_W_Model.UMDQN_W_Model.forward": [[57, 77], ["state.size", "UMDQN_W_Model.UMDQN_W_Model.stateEmbeddingDNN", "UMDQN_W_Model.UMDQN_W_Model.repeat().view", "UMDQN_W_Model.UMDQN_W_Model.UMNN", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "UMDQN_W_Model.UMDQN_W_Model.size", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "UMDQN_W_Model.UMDQN_W_Model.repeat", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "int", "len", "len"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "state", ",", "taus", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Implementing the forward pass of the Deep Neural Network.\n        \n        INPUTS: - state: RL state.\n                - taus: Samples of taus.\n        \n        OUTPUTS: - output: Output of the Deep Neural Network.\n        \"\"\"", "\n", "\n", "# State embedding part of the Deep Neural Network", "\n", "batchSize", "=", "state", ".", "size", "(", "0", ")", "\n", "x", "=", "self", ".", "stateEmbeddingDNN", "(", "state", ")", "\n", "x", "=", "x", ".", "repeat", "(", "1", ",", "int", "(", "len", "(", "taus", ")", "/", "len", "(", "state", ")", ")", ")", ".", "view", "(", "-", "1", ",", "x", ".", "size", "(", "1", ")", ")", "\n", "\n", "# UMNNN part of the Deep Neural Network", "\n", "x", "=", "self", ".", "UMNN", "(", "taus", ",", "x", ")", "\n", "\n", "# Appropriate format", "\n", "return", "torch", ".", "cat", "(", "torch", ".", "chunk", "(", "torch", ".", "transpose", "(", "x", ",", "0", ",", "1", ")", ",", "batchSize", ",", "dim", "=", "1", ")", ",", "0", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.CNN_MinAtar.CNN_MinAtar.__init__": [[29, 52], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.MonotonicNN.__init__"], ["def", "__init__", "(", "self", ",", "numberOfInputs", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Defining and initializing the Convolutional Neural Network.\n        \n        INPUTS: - numberOfInputs: Number of inputs of the Convolutional Neural Network.\n        \n        OUTPUTS: /\n        \"\"\"", "\n", "\n", "# Call the constructor of the parent class (Pytorch torch.nn.Module)", "\n", "super", "(", "CNN_MinAtar", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# Initialization of some variables", "\n", "self", ".", "channels", "=", "numberOfInputs", "\n", "self", ".", "size", "=", "10", "\n", "self", ".", "filters", "=", "16", "\n", "self", ".", "kernel", "=", "3", "\n", "self", ".", "stride", "=", "1", "\n", "\n", "# Initialization of the Convolutional Neural Network", "\n", "self", ".", "network", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "self", ".", "channels", ",", "self", ".", "filters", ",", "self", ".", "kernel", ",", "self", ".", "stride", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.CNN_MinAtar.CNN_MinAtar.getOutputSize": [[55, 66], ["int"], "methods", ["None"], ["", "def", "getOutputSize", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Get the size of the Convolutional Neural Network output.\n        \n        INPUTS: /\n        \n        OUTPUTS: - size: Size of the Convolutional Neural Network. output.\n        \"\"\"", "\n", "\n", "newSize", "=", "(", "(", "self", ".", "size", "-", "self", ".", "kernel", ")", "/", "self", ".", "stride", ")", "+", "1", "\n", "return", "int", "(", "newSize", "*", "newSize", "*", "self", ".", "filters", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.CNN_MinAtar.CNN_MinAtar.forward": [[68, 79], ["CNN_MinAtar.CNN_MinAtar.network", "CNN_MinAtar.CNN_MinAtar.view", "CNN_MinAtar.CNN_MinAtar.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Implementing the forward pass of the Convolutional Neural Network.\n        \n        INPUTS: - x: Input of the Convolutional Neural Network.\n        \n        OUTPUTS: - y: Output of the Convolutional Neural Network.\n        \"\"\"", "\n", "\n", "x", "=", "self", ".", "network", "(", "x", ")", "\n", "return", "x", ".", "view", "(", "x", ".", "size", "(", "0", ")", ",", "-", "1", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.IQN_Model.IQN_Model.__init__": [[31, 56], ["torch.Module.__init__", "torch.tensor().view().to", "torch.tensor().view().to", "torch.tensor().view().to", "torch.tensor().view().to", "Models.FeedforwardDNN.FeedForwardDNN", "torch.Sequential", "torch.Sequential", "Models.FeedforwardDNN.FeedForwardDNN", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.tensor().view", "torch.tensor().view", "torch.tensor().view", "torch.tensor().view", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "range"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.MonotonicNN.__init__"], ["def", "__init__", "(", "self", ",", "numberOfInputs", ",", "numberOfOutputs", ",", "structure", ",", "stateEmbedding", ",", "NCos", "=", "64", ",", "device", "=", "'cpu'", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Defining and initializing the Deep Neural Network.\n        \n        INPUTS: - numberOfInputs: Number of inputs of the Deep Neural Network.\n                - numberOfOutputs: Number of outputs of the Deep Neural Network.\n                - structure: Structure of the state embedding Deep Neural Network (hidden layers).\n                - stateEmbedding: Number of values to represent the state.\n                - Ncos: Number of elements in cosine function.\n        \n        OUTPUTS: /\n        \"\"\"", "\n", "\n", "# Call the constructor of the parent class (Pytorch torch.nn.Module)", "\n", "super", "(", "IQN_Model", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# Initialization of useful variables", "\n", "self", ".", "device", "=", "device", "\n", "self", ".", "NCos", "=", "NCos", "\n", "self", ".", "piMultiples", "=", "torch", ".", "tensor", "(", "[", "np", ".", "pi", "*", "i", "for", "i", "in", "range", "(", "self", ".", "NCos", ")", "]", ",", "dtype", "=", "torch", ".", "float", ")", ".", "view", "(", "1", ",", "1", ",", "self", ".", "NCos", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "# Initialization of the Deep Neural Network", "\n", "self", ".", "stateEmbedding", "=", "FeedForwardDNN", "(", "numberOfInputs", ",", "stateEmbedding", ",", "structure", ")", "\n", "self", ".", "cosEmbedding", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "NCos", ",", "stateEmbedding", ")", ",", "nn", ".", "ReLU", "(", ")", ")", "\n", "self", ".", "feedForwardDNN", "=", "FeedForwardDNN", "(", "stateEmbedding", ",", "numberOfOutputs", ",", "[", "256", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.IQN_Model.IQN_Model.forward": [[58, 91], ["IQN_Model.IQN_Model.size", "IQN_Model.IQN_Model.stateEmbedding().unsqueeze", "torch.cos().view", "torch.cos().view", "torch.cos().view", "torch.cos().view", "IQN_Model.IQN_Model.cosEmbedding().view", "IQN_Model.IQN_Model.feedForwardDNN", "torch.rand().to().unsqueeze", "torch.rand().to().unsqueeze", "torch.rand().to().unsqueeze", "torch.rand().to().unsqueeze", "torch.linspace().to", "torch.linspace().to", "torch.linspace().to", "torch.linspace().to", "taus.repeat().unsqueeze.repeat().unsqueeze.repeat().unsqueeze", "IQN_Model.IQN_Model.transpose", "IQN_Model.IQN_Model.stateEmbedding", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "IQN_Model.IQN_Model.cosEmbedding", "torch.rand().to", "torch.rand().to", "torch.rand().to", "torch.rand().to", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "taus.repeat().unsqueeze.repeat().unsqueeze.repeat", "torch.rand", "torch.rand", "torch.rand", "torch.rand"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "N", ",", "randomSampling", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Implementing the forward pass of the Deep Neural Network.\n        \n        INPUTS: - x: Input of the Deep Neural Network.\n                - N: Number of quantiles to generate.\n                - randomSampling: Boolean specifying whether the quantiles are\n                                  sampled randomly or not (default: True).\n        \n        OUTPUTS: - y: Output of the Deep Neural Network.\n        \"\"\"", "\n", "\n", "# State embedding part of the Deep Neural Network", "\n", "batchSize", "=", "x", ".", "size", "(", "0", ")", "\n", "x", "=", "self", ".", "stateEmbedding", "(", "x", ")", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "# Generate a number of quantiles (randomly or not)", "\n", "if", "randomSampling", ":", "\n", "            ", "taus", "=", "torch", ".", "rand", "(", "batchSize", ",", "N", ")", ".", "to", "(", "self", ".", "device", ")", ".", "unsqueeze", "(", "2", ")", "\n", "", "else", ":", "\n", "            ", "taus", "=", "torch", ".", "linspace", "(", "0.0", ",", "1.0", ",", "N", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "taus", "=", "taus", ".", "repeat", "(", "batchSize", ",", "1", ")", ".", "unsqueeze", "(", "2", ")", "\n", "\n", "# Quantile embedding part of the Deep Neural Network", "\n", "", "cos", "=", "torch", ".", "cos", "(", "taus", "*", "self", ".", "piMultiples", ")", ".", "view", "(", "batchSize", "*", "N", ",", "self", ".", "NCos", ")", "\n", "cos", "=", "self", ".", "cosEmbedding", "(", "cos", ")", ".", "view", "(", "batchSize", ",", "N", ",", "-", "1", ")", "\n", "\n", "# Multiplication of both state and cos embeddings outputs (combination)", "\n", "x", "=", "(", "x", "*", "cos", ")", ".", "view", "(", "batchSize", ",", "N", ",", "-", "1", ")", "\n", "\n", "# Distribution part of the Deep Neural Network", "\n", "x", "=", "self", ".", "feedForwardDNN", "(", "x", ")", "\n", "return", "x", ".", "transpose", "(", "1", ",", "2", ")", ",", "taus", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.FQF_Model.FQF_Model.__init__": [[32, 57], ["torch.Module.__init__", "torch.tensor().view().to", "torch.tensor().view().to", "torch.tensor().view().to", "torch.tensor().view().to", "Models.FeedforwardDNN.FeedForwardDNN", "torch.Sequential", "torch.Sequential", "Models.FeedforwardDNN.FeedForwardDNN", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.tensor().view", "torch.tensor().view", "torch.tensor().view", "torch.tensor().view", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "range"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.MonotonicNN.__init__"], ["def", "__init__", "(", "self", ",", "numberOfInputs", ",", "numberOfOutputs", ",", "structure", ",", "stateEmbedding", ",", "NCos", "=", "64", ",", "device", "=", "'cpu'", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Defining and initializing the Deep Neural Network.\n        \n        INPUTS: - numberOfInputs: Number of inputs of the Deep Neural Network.\n                - numberOfOutputs: Number of outputs of the Deep Neural Network.\n                - structure: Structure of the state embedding Deep Neural Network (hidden layers).\n                - stateEmbedding: Number of values to represent the state.\n                - Ncos: Number of elements in cosine function.\n        \n        OUTPUTS: /\n        \"\"\"", "\n", "\n", "# Call the constructor of the parent class (Pytorch torch.nn.Module)", "\n", "super", "(", "FQF_Model", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# Initialization of useful variables", "\n", "self", ".", "device", "=", "device", "\n", "self", ".", "NCos", "=", "NCos", "\n", "self", ".", "piMultiples", "=", "torch", ".", "tensor", "(", "[", "np", ".", "pi", "*", "i", "for", "i", "in", "range", "(", "self", ".", "NCos", ")", "]", ",", "dtype", "=", "torch", ".", "float", ")", ".", "view", "(", "1", ",", "1", ",", "self", ".", "NCos", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "# Initialization of the Deep Neural Network.", "\n", "self", ".", "stateEmbedding", "=", "FeedForwardDNN", "(", "numberOfInputs", ",", "stateEmbedding", ",", "structure", ")", "\n", "self", ".", "cosEmbedding", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "NCos", ",", "stateEmbedding", ")", ",", "nn", ".", "ReLU", "(", ")", ")", "\n", "self", ".", "feedForwardDNN", "=", "FeedForwardDNN", "(", "stateEmbedding", ",", "numberOfOutputs", ",", "[", "256", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.FQF_Model.FQF_Model.embedding": [[59, 69], ["FQF_Model.FQF_Model.stateEmbedding"], "methods", ["None"], ["", "def", "embedding", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Implementing the embedding part of the Deep Neural Network.\n        \n        INPUTS: - x: Input of the Deep Neural Network.\n        \n        OUTPUTS: - y: Embedded input of the Deep Neural Network.\n        \"\"\"", "\n", "\n", "return", "self", ".", "stateEmbedding", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.FQF_Model.FQF_Model.forward": [[71, 100], ["embedding.unsqueeze.size", "taus.size", "torch.cos().view", "torch.cos().view", "torch.cos().view", "torch.cos().view", "FQF_Model.FQF_Model.cosEmbedding().view", "FQF_Model.FQF_Model.feedForwardDNN", "embedding.unsqueeze.transpose", "FQF_Model.FQF_Model.stateEmbedding().unsqueeze", "embedding.unsqueeze", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "FQF_Model.FQF_Model.cosEmbedding", "FQF_Model.FQF_Model.stateEmbedding", "taus.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "taus", ",", "embedding", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Implementing the forward pass of the Deep Neural Network.\n        \n        INPUTS: - x: Input of the Deep Neural Network.\n                - taus: Quantiles (generated by the FPN).\n                - embedding: Embedding of the Deep Neural Network input (state).\n        \n        OUTPUTS: - y: Output of the Deep Neural Network.\n        \"\"\"", "\n", "\n", "# State embedding part of the Deep Neural Network", "\n", "batchSize", "=", "x", ".", "size", "(", "0", ")", "\n", "if", "embedding", "==", "None", ":", "\n", "            ", "x", "=", "self", ".", "stateEmbedding", "(", "x", ")", ".", "unsqueeze", "(", "1", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "embedding", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "# Quantile embedding part of the Deep Neural Network", "\n", "", "N", "=", "taus", ".", "size", "(", "1", ")", "\n", "cos", "=", "torch", ".", "cos", "(", "taus", ".", "unsqueeze", "(", "2", ")", "*", "self", ".", "piMultiples", ")", ".", "view", "(", "batchSize", "*", "N", ",", "self", ".", "NCos", ")", "\n", "cos", "=", "self", ".", "cosEmbedding", "(", "cos", ")", ".", "view", "(", "batchSize", ",", "N", ",", "-", "1", ")", "\n", "\n", "# Multiplication of both state and cos embeddings outputs (combination)", "\n", "x", "=", "(", "x", "*", "cos", ")", ".", "view", "(", "batchSize", ",", "N", ",", "-", "1", ")", "\n", "\n", "# Distribution part of the Deep Neural Network", "\n", "x", "=", "self", ".", "feedForwardDNN", "(", "x", ")", "\n", "return", "x", ".", "transpose", "(", "1", ",", "2", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.DNN_MinAtar.DNN_MinAtar.__init__": [[31, 49], ["torch.Module.__init__", "Models.CNN_MinAtar.CNN_MinAtar().getOutputSize", "torch.Sequential", "Models.CNN_MinAtar.CNN_MinAtar", "Models.FeedforwardDNN.FeedForwardDNN", "Models.CNN_MinAtar.CNN_MinAtar"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.MonotonicNN.__init__", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.CNN_Atari.CNN_Atari.getOutputSize"], ["def", "__init__", "(", "self", ",", "numberOfInputs", ",", "numberOfOutputs", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Defining and initializing the Deep Neural Network.\n        \n        INPUTS: - numberOfInputs: Number of inputs of the Deep Neural Network.\n                - numberOfOutputs: Number of outputs of the Deep Neural Network.\n        \n        OUTPUTS: /\n        \"\"\"", "\n", "\n", "# Call the constructor of the parent class (Pytorch torch.nn.Module)", "\n", "super", "(", "DNN_MinAtar", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# Initialization of the Deep Neural Network.", "\n", "CNNOutputSize", "=", "CNN_MinAtar", "(", "numberOfInputs", ")", ".", "getOutputSize", "(", ")", "\n", "self", ".", "network", "=", "nn", ".", "Sequential", "(", "\n", "CNN_MinAtar", "(", "numberOfInputs", ")", ",", "\n", "FeedForwardDNN", "(", "CNNOutputSize", ",", "numberOfOutputs", ",", "[", "128", "]", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.DNN_MinAtar.DNN_MinAtar.forward": [[52, 62], ["DNN_MinAtar.DNN_MinAtar.network"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Implementing the forward pass of the Deep Neural Network.\n        \n        INPUTS: - x: Input of the Deep Neural Network.\n        \n        OUTPUTS: - y: Output of the Deep Neural Network.\n        \"\"\"", "\n", "\n", "return", "self", ".", "network", "(", "x", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.IQN_Model_Atari.IQN_Model_Atari.__init__": [[33, 65], ["torch.Module.__init__", "torch.tensor().view().to", "torch.tensor().view().to", "torch.tensor().view().to", "torch.tensor().view().to", "Models.CNN_MinAtar.CNN_MinAtar", "Models.CNN_MinAtar.CNN_MinAtar().getOutputSize", "torch.Sequential", "torch.Sequential", "Models.FeedforwardDNN.FeedForwardDNN", "Models.CNN_Atari.CNN_Atari", "Models.CNN_Atari.CNN_Atari().getOutputSize", "torch.Sequential", "torch.Sequential", "Models.FeedforwardDNN.FeedForwardDNN", "torch.tensor().view", "torch.tensor().view", "torch.tensor().view", "torch.tensor().view", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "Models.CNN_MinAtar.CNN_MinAtar", "Models.CNN_Atari.CNN_Atari", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "range"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.MonotonicNN.__init__", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.CNN_Atari.CNN_Atari.getOutputSize", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.CNN_Atari.CNN_Atari.getOutputSize"], ["def", "__init__", "(", "self", ",", "numberOfInputs", ",", "numberOfOutputs", ",", "NCos", "=", "64", ",", "device", "=", "'cpu'", ",", "\n", "minAtar", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Defining and initializing the Deep Neural Network.\n        \n        INPUTS: - numberOfInputs: Number of inputs of the Deep Neural Network.\n                - numberOfOutputs: Number of outputs of the Deep Neural Network.\n                - Ncos: Number of elements in cosine function.\n                - minAtar: Boolean specifying whether the env is \"MinAtar\" or not.\n        \n        OUTPUTS: /\n        \"\"\"", "\n", "\n", "# Call the constructor of the parent class (Pytorch torch.nn.Module)", "\n", "super", "(", "IQN_Model_Atari", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# Initialization of useful variables", "\n", "self", ".", "device", "=", "device", "\n", "self", ".", "NCos", "=", "NCos", "\n", "self", ".", "piMultiples", "=", "torch", ".", "tensor", "(", "[", "np", ".", "pi", "*", "i", "for", "i", "in", "range", "(", "self", ".", "NCos", ")", "]", ",", "dtype", "=", "torch", ".", "float", ")", ".", "view", "(", "1", ",", "1", ",", "self", ".", "NCos", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "# Initialization of the Deep Neural Network", "\n", "if", "minAtar", ":", "\n", "            ", "self", ".", "stateEmbedding", "=", "CNN_MinAtar", "(", "numberOfInputs", ")", "\n", "stateEmbedding", "=", "CNN_MinAtar", "(", "numberOfInputs", ")", ".", "getOutputSize", "(", ")", "\n", "self", ".", "cosEmbedding", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "NCos", ",", "stateEmbedding", ")", ",", "nn", ".", "ReLU", "(", ")", ")", "\n", "self", ".", "feedForwardDNN", "=", "FeedForwardDNN", "(", "stateEmbedding", ",", "numberOfOutputs", ",", "[", "128", "]", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "stateEmbedding", "=", "CNN_Atari", "(", "numberOfInputs", ")", "\n", "stateEmbedding", "=", "CNN_Atari", "(", "numberOfInputs", ")", ".", "getOutputSize", "(", ")", "\n", "self", ".", "cosEmbedding", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "NCos", ",", "stateEmbedding", ")", ",", "nn", ".", "ReLU", "(", ")", ")", "\n", "self", ".", "feedForwardDNN", "=", "FeedForwardDNN", "(", "stateEmbedding", ",", "numberOfOutputs", ",", "[", "512", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.IQN_Model_Atari.IQN_Model_Atari.forward": [[67, 100], ["IQN_Model_Atari.IQN_Model_Atari.size", "IQN_Model_Atari.IQN_Model_Atari.stateEmbedding().unsqueeze", "torch.cos().view", "torch.cos().view", "torch.cos().view", "torch.cos().view", "IQN_Model_Atari.IQN_Model_Atari.cosEmbedding().view", "IQN_Model_Atari.IQN_Model_Atari.feedForwardDNN", "torch.rand().to().unsqueeze", "torch.rand().to().unsqueeze", "torch.rand().to().unsqueeze", "torch.rand().to().unsqueeze", "torch.linspace().to", "torch.linspace().to", "torch.linspace().to", "torch.linspace().to", "taus.repeat().unsqueeze.repeat().unsqueeze.repeat().unsqueeze", "IQN_Model_Atari.IQN_Model_Atari.transpose", "IQN_Model_Atari.IQN_Model_Atari.stateEmbedding", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "IQN_Model_Atari.IQN_Model_Atari.cosEmbedding", "torch.rand().to", "torch.rand().to", "torch.rand().to", "torch.rand().to", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "taus.repeat().unsqueeze.repeat().unsqueeze.repeat", "torch.rand", "torch.rand", "torch.rand", "torch.rand"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ",", "N", ",", "randomSampling", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Implementing the forward pass of the Deep Neural Network.\n        \n        INPUTS: - x: Input of the Deep Neural Network.\n                - N: Number of quantiles to generate.\n                - randomSampling: Boolean specifying whether the quantiles are\n                                  sampled randomly or not (default: True).\n        \n        OUTPUTS: - y: Output of the Deep Neural Network.\n        \"\"\"", "\n", "\n", "# State embedding part of the Deep Neural Network", "\n", "batchSize", "=", "x", ".", "size", "(", "0", ")", "\n", "x", "=", "self", ".", "stateEmbedding", "(", "x", ")", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "# Generate a number of quantiles (randomly or not)", "\n", "if", "randomSampling", ":", "\n", "            ", "taus", "=", "torch", ".", "rand", "(", "batchSize", ",", "N", ")", ".", "to", "(", "self", ".", "device", ")", ".", "unsqueeze", "(", "2", ")", "\n", "", "else", ":", "\n", "            ", "taus", "=", "torch", ".", "linspace", "(", "0.0", ",", "1.0", ",", "N", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "taus", "=", "taus", ".", "repeat", "(", "batchSize", ",", "1", ")", ".", "unsqueeze", "(", "2", ")", "\n", "\n", "# Quantile embedding part of the Deep Neural Network", "\n", "", "cos", "=", "torch", ".", "cos", "(", "taus", "*", "self", ".", "piMultiples", ")", ".", "view", "(", "batchSize", "*", "N", ",", "self", ".", "NCos", ")", "\n", "cos", "=", "self", ".", "cosEmbedding", "(", "cos", ")", ".", "view", "(", "batchSize", ",", "N", ",", "-", "1", ")", "\n", "\n", "# Multiplication of both state and cos embeddings outputs (combination)", "\n", "x", "=", "(", "x", "*", "cos", ")", ".", "view", "(", "batchSize", ",", "N", ",", "-", "1", ")", "\n", "\n", "# Distribution part of the Deep Neural Network", "\n", "x", "=", "self", ".", "feedForwardDNN", "(", "x", ")", "\n", "return", "x", ".", "transpose", "(", "1", ",", "2", ")", ",", "taus", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.QR_DQN_Model_Atari.QR_DQN_Model_Atari.__init__": [[30, 54], ["torch.Module.__init__", "int", "Models.DNN_MinAtar.DNN_MinAtar", "Models.DNN_Atari.DNN_Atari"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.MonotonicNN.__init__"], ["def", "__init__", "(", "self", ",", "numberOfInputs", ",", "numberOfOutputs", ",", "numberOfQuantiles", "=", "200", ",", "minAtar", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Defining and initializing the Deep Neural Network.\n        \n        INPUTS: - numberOfInputs: Number of inputs of the Deep Neural Network.\n                - numberOfOutputs: Number of outputs of the Deep Neural Network.\n                - numberOfQuantiles: Number of quantiles for approximating the distribution.\n                - minAtar: Boolean specifying whether the env is \"MinAtar\" or not.\n        \n        OUTPUTS: /\n        \"\"\"", "\n", "\n", "# Call the constructor of the parent class (Pytorch torch.nn.Module)", "\n", "super", "(", "QR_DQN_Model_Atari", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# Initialization of useful variables", "\n", "self", ".", "numberOfQuantiles", "=", "numberOfQuantiles", "\n", "self", ".", "numberOfActions", "=", "int", "(", "numberOfOutputs", "/", "numberOfQuantiles", ")", "\n", "\n", "# Initialization of the Deep Neural Network.", "\n", "if", "minAtar", ":", "\n", "            ", "self", ".", "network", "=", "DNN_MinAtar", "(", "numberOfInputs", ",", "numberOfOutputs", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "network", "=", "DNN_Atari", "(", "numberOfInputs", ",", "numberOfOutputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.QR_DQN_Model_Atari.QR_DQN_Model_Atari.forward": [[56, 67], ["QR_DQN_Model_Atari.QR_DQN_Model_Atari.network", "QR_DQN_Model_Atari.QR_DQN_Model_Atari.view", "QR_DQN_Model_Atari.QR_DQN_Model_Atari.size"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Implementing the forward pass of the Deep Neural Network.\n        \n        INPUTS: - x: Input of the Deep Neural Network.\n        \n        OUTPUTS: - y: Output of the Deep Neural Network.\n        \"\"\"", "\n", "\n", "x", "=", "self", ".", "network", "(", "x", ")", "\n", "return", "x", ".", "view", "(", "x", ".", "size", "(", "0", ")", ",", "self", ".", "numberOfActions", ",", "self", ".", "numberOfQuantiles", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.CDQN_Model_Atari.CDQN_Model_Atari.__init__": [[31, 55], ["torch.Module.__init__", "int", "Models.DNN_MinAtar.DNN_MinAtar", "Models.DNN_Atari.DNN_Atari"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.MonotonicNN.__init__"], ["def", "__init__", "(", "self", ",", "numberOfInputs", ",", "numberOfOutputs", ",", "numberOfAtoms", ",", "minAtar", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Defining and initializing the Deep Neural Network.\n        \n        INPUTS: - numberOfInputs: Number of inputs of the Deep Neural Network.\n                - numberOfOutputs: Number of outputs of the Deep Neural Network.\n                - numberOfAtoms: Number of atoms for the support (see C51 algorithm).\n                - minAtar: Boolean specifying whether the env is \"MinAtar\" or not.\n        \n        OUTPUTS: /\n        \"\"\"", "\n", "\n", "# Call the constructor of the parent class (Pytorch torch.nn.Module)", "\n", "super", "(", "CDQN_Model_Atari", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# Initialization of useful variables", "\n", "self", ".", "numberOfAtoms", "=", "numberOfAtoms", "\n", "self", ".", "numberOfActions", "=", "int", "(", "numberOfOutputs", "/", "numberOfAtoms", ")", "\n", "\n", "# Initialization of the Deep Neural Network", "\n", "if", "minAtar", ":", "\n", "            ", "self", ".", "network", "=", "DNN_MinAtar", "(", "numberOfInputs", ",", "numberOfOutputs", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "network", "=", "DNN_Atari", "(", "numberOfInputs", ",", "numberOfOutputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.CDQN_Model_Atari.CDQN_Model_Atari.forward": [[57, 69], ["CDQN_Model_Atari.CDQN_Model_Atari.network", "torch.softmax", "torch.softmax", "torch.softmax.clamp", "CDQN_Model_Atari.CDQN_Model_Atari.view"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Implementing the forward pass of the Deep Neural Network.\n        \n        INPUTS: - x: Input of the Deep Neural Network.\n        \n        OUTPUTS: - y: Output of the Deep Neural Network.\n        \"\"\"", "\n", "\n", "x", "=", "self", ".", "network", "(", "x", ")", "\n", "y", "=", "F", ".", "softmax", "(", "x", ".", "view", "(", "-", "1", ",", "self", ".", "numberOfActions", ",", "self", ".", "numberOfAtoms", ")", ",", "dim", "=", "-", "1", ")", "\n", "return", "y", ".", "clamp", "(", "min", "=", "1e-6", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.FeedforwardDNN.FeedForwardDNN.__init__": [[27, 51], ["torch.Module.__init__", "zip", "FeedforwardDNN.FeedForwardDNN.network.pop", "torch.Sequential", "FeedforwardDNN.FeedForwardDNN.network.extend", "torch.Linear", "torch.ReLU"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.MonotonicNN.__init__"], ["def", "__init__", "(", "self", ",", "numberOfInputs", ",", "numberOfOutputs", ",", "structure", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Defining and initializing the feedforward DNN.\n        \n        INPUTS: - numberOfInputs: Number of inputs of the Deep Neural Network.\n                - numberOfOutputs: Number of outputs of the Deep Neural Network.\n                - structure: Structure of the feedforward DNN (hidden layers).\n        \n        OUTPUTS: /\n        \"\"\"", "\n", "\n", "# Call the constructor of the parent class (Pytorch torch.nn.Module)", "\n", "super", "(", "FeedForwardDNN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# Initialization of the FeedForward DNN", "\n", "self", ".", "network", "=", "[", "]", "\n", "structure", "=", "[", "numberOfInputs", "]", "+", "structure", "+", "[", "numberOfOutputs", "]", "\n", "for", "inFeature", ",", "outFeature", "in", "zip", "(", "structure", ",", "structure", "[", "1", ":", "]", ")", ":", "\n", "            ", "self", ".", "network", ".", "extend", "(", "[", "\n", "nn", ".", "Linear", "(", "inFeature", ",", "outFeature", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "]", ")", "\n", "", "self", ".", "network", ".", "pop", "(", ")", "\n", "self", ".", "network", "=", "nn", ".", "Sequential", "(", "*", "self", ".", "network", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.FeedforwardDNN.FeedForwardDNN.forward": [[53, 63], ["FeedforwardDNN.FeedForwardDNN.network"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Implementing the forward pass of the feedforward DNN.\n        \n        INPUTS: - x: Input of the feedforward DNN.\n        \n        OUTPUTS: - y: Output of the feedforward DNN.\n        \"\"\"", "\n", "\n", "return", "self", ".", "network", "(", "x", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.UMDQN_KL_Model.UMDQN_KL_Model.__init__": [[33, 56], ["torch.Module.__init__", "Models.FeedforwardDNN.FeedForwardDNN", "Models.MonotonicNN.OneDimensionnalNF"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.MonotonicNN.__init__"], ["def", "__init__", "(", "self", ",", "numberOfInputs", ",", "numberOfOutputs", ",", "\n", "structureDNN", ",", "structureUMNN", ",", "stateEmbedding", ",", "\n", "numberOfSteps", ",", "device", "=", "'cpu'", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Defining and initializing the Deep Neural Network.\n        \n        INPUTS: - numberOfInputs: Number of inputs of the Deep Neural Network.\n                - numberOfOutputs: Number of outputs of the Deep Neural Network.\n                - structureDNN: Structure of the feedforward DNN for state embedding.\n                - structureUMNN: Structure of the UMNN for distribution representation.\n                - stateEmbedding: Dimension of the state embedding.\n                - numberOfSteps: Number of integration steps for the UMNN.\n                - device: Hardware device (CPU or GPU).\n        \n        OUTPUTS: /\n        \"\"\"", "\n", "\n", "# Call the constructor of the parent class (Pytorch torch.nn.Module)", "\n", "super", "(", "UMDQN_KL_Model", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# Initialization of the Deep Neural Network", "\n", "self", ".", "stateEmbeddingDNN", "=", "FeedForwardDNN", "(", "numberOfInputs", ",", "stateEmbedding", ",", "structureDNN", ")", "\n", "self", ".", "UMNN", "=", "OneDimensionnalNF", "(", "stateEmbedding", "+", "1", ",", "structureUMNN", ",", "numberOfSteps", ",", "numberOfOutputs", ",", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.UMDQN_KL_Model.UMDQN_KL_Model.forward": [[58, 82], ["state.size", "UMDQN_KL_Model.UMDQN_KL_Model.stateEmbeddingDNN", "x.clamp.clamp.repeat().view", "UMDQN_KL_Model.UMDQN_KL_Model.UMNN", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "x.clamp.clamp.clamp", "x.clamp.clamp.size", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "x.clamp.clamp.repeat", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "int", "len", "len"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "state", ",", "q", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Implementing the forward pass of the Deep Neural Network.\n        \n        INPUTS: - state: RL state.\n                - q: Samples of potential returns.\n        \n        OUTPUTS: - output: Output of the Deep Neural Network.\n        \"\"\"", "\n", "\n", "# State embedding part of the Deep Neural Network", "\n", "batchSize", "=", "state", ".", "size", "(", "0", ")", "\n", "x", "=", "self", ".", "stateEmbeddingDNN", "(", "state", ")", "\n", "x", "=", "x", ".", "repeat", "(", "1", ",", "int", "(", "len", "(", "q", ")", "/", "len", "(", "state", ")", ")", ")", ".", "view", "(", "-", "1", ",", "x", ".", "size", "(", "1", ")", ")", "\n", "\n", "# UMNN part of the Deep Neural Network", "\n", "x", "=", "self", ".", "UMNN", "(", "q", ",", "x", ")", "\n", "\n", "# Formatting of the output and post processing operations", "\n", "x", "=", "torch", ".", "cat", "(", "torch", ".", "chunk", "(", "torch", ".", "transpose", "(", "x", ",", "0", ",", "1", ")", ",", "batchSize", ",", "dim", "=", "1", ")", ",", "0", ")", "\n", "x", "=", "torch", ".", "exp", "(", "x", ")", "\n", "x", "=", "x", ".", "clamp", "(", "min", "=", "1e-6", ")", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.UMDQN_KL_Model.UMDQN_KL_Model.getExpectation": [[84, 102], ["UMDQN_KL_Model.UMDQN_KL_Model.stateEmbeddingDNN", "UMDQN_KL_Model.UMDQN_KL_Model.UMNN.expectation"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.MonotonicNN.expectation"], ["", "def", "getExpectation", "(", "self", ",", "state", ",", "minReturn", ",", "maxReturn", ",", "numberOfPoints", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Get the expectation of the PDF internally computed by the UMNN.\n        \n        INPUTS: - state: RL state.\n                - minReturn: Minimum return.\n                - maxReturn: Maximum return.\n                - numberOfPoints: Number of points for the computations (accuracy).\n        \n        OUTPUTS: - expectation: Expectation computed.\n        \"\"\"", "\n", "\n", "# State embedding part of the Deep Neural Network", "\n", "state", "=", "self", ".", "stateEmbeddingDNN", "(", "state", ")", "\n", "\n", "# Computation of the expectation of the PDF internally computed by the UMNN", "\n", "expectation", "=", "self", ".", "UMNN", ".", "expectation", "(", "state", ",", "lambda", "x", ":", "x", ",", "minReturn", ",", "maxReturn", ",", "numberOfPoints", ")", "\n", "return", "expectation", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.UMDQN_W_Model_Atari.UMDQN_W_Model_Atari.__init__": [[33, 59], ["torch.Module.__init__", "Models.MonotonicNN.MonotonicNN", "Models.DNN_MinAtar.DNN_MinAtar", "Models.DNN_Atari.DNN_Atari"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.MonotonicNN.__init__"], ["def", "__init__", "(", "self", ",", "numberOfInputs", ",", "numberOfOutputs", ",", "\n", "structureUMNN", ",", "stateEmbedding", ",", "\n", "numberOfSteps", ",", "device", "=", "'cpu'", ",", "minAtar", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Defining and initializing the Deep Neural Network.\n        \n        INPUTS: - numberOfInputs: Number of inputs of the Deep Neural Network.\n                - numberOfOutputs: Number of outputs of the Deep Neural Network.\n                - structureUMNN: Structure of the UMNN for distribution representation.\n                - stateEmbedding: Dimension of the state embedding.\n                - numberOfSteps: Number of integration steps for the UMNN.\n                - device: Hardware device (CPU or GPU).\n                - minAtar: Boolean specifying whether the env is \"MinAtar\" or not.\n        \n        OUTPUTS: /\n        \"\"\"", "\n", "\n", "# Call the constructor of the parent class (Pytorch torch.nn.Module)", "\n", "super", "(", "UMDQN_W_Model_Atari", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# Initialization of the Deep Neural Network", "\n", "if", "minAtar", ":", "\n", "            ", "self", ".", "stateEmbeddingDNN", "=", "DNN_MinAtar", "(", "numberOfInputs", ",", "stateEmbedding", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "stateEmbeddingDNN", "=", "DNN_Atari", "(", "numberOfInputs", ",", "stateEmbedding", ")", "\n", "", "self", ".", "UMNN", "=", "MonotonicNN", "(", "stateEmbedding", "+", "1", ",", "structureUMNN", ",", "numberOfSteps", ",", "numberOfOutputs", ",", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.UMDQN_W_Model_Atari.UMDQN_W_Model_Atari.forward": [[61, 81], ["state.size", "UMDQN_W_Model_Atari.UMDQN_W_Model_Atari.stateEmbeddingDNN", "UMDQN_W_Model_Atari.UMDQN_W_Model_Atari.repeat().view", "UMDQN_W_Model_Atari.UMDQN_W_Model_Atari.UMNN", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "UMDQN_W_Model_Atari.UMDQN_W_Model_Atari.size", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "UMDQN_W_Model_Atari.UMDQN_W_Model_Atari.repeat", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "int", "len", "len"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "state", ",", "taus", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Implementing the forward pass of the Deep Neural Network.\n        \n        INPUTS: - state: RL state.\n                - taus: Samples of taus.\n        \n        OUTPUTS: - output: Output of the Deep Neural Network.\n        \"\"\"", "\n", "\n", "# State embedding part of the Deep Neural Network", "\n", "batchSize", "=", "state", ".", "size", "(", "0", ")", "\n", "x", "=", "self", ".", "stateEmbeddingDNN", "(", "state", ")", "\n", "x", "=", "x", ".", "repeat", "(", "1", ",", "int", "(", "len", "(", "taus", ")", "/", "len", "(", "state", ")", ")", ")", ".", "view", "(", "-", "1", ",", "x", ".", "size", "(", "1", ")", ")", "\n", "\n", "# UMNNN part of the Deep Neural Network", "\n", "x", "=", "self", ".", "UMNN", "(", "taus", ",", "x", ")", "\n", "\n", "# Appropriate format", "\n", "return", "torch", ".", "cat", "(", "torch", ".", "chunk", "(", "torch", ".", "transpose", "(", "x", ",", "0", ",", "1", ")", ",", "batchSize", ",", "dim", "=", "1", ")", ",", "0", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.UMDQN_KL_Model_Atari.UMDQN_KL_Model_Atari.__init__": [[34, 60], ["torch.Module.__init__", "Models.MonotonicNN.OneDimensionnalNF", "Models.DNN_MinAtar.DNN_MinAtar", "Models.DNN_Atari.DNN_Atari"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.MonotonicNN.__init__"], ["def", "__init__", "(", "self", ",", "numberOfInputs", ",", "numberOfOutputs", ",", "\n", "structureUMNN", ",", "stateEmbedding", ",", "numberOfSteps", ",", "\n", "device", "=", "'cpu'", ",", "minAtar", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Defining and initializing the Deep Neural Network.\n        \n        INPUTS: - numberOfInputs: Number of inputs of the Deep Neural Network.\n                - numberOfOutputs: Number of outputs of the Deep Neural Network.\n                - structureUMNN: Structure of the UMNN for distribution representation.\n                - stateEmbedding: Dimension of the state embedding.\n                - numberOfSteps: Number of integration steps for the UMNN.\n                - device: Hardware device (CPU or GPU).\n                - minAtar: Boolean specifying whether the env is \"MinAtar\" or not.\n        \n        OUTPUTS: /\n        \"\"\"", "\n", "\n", "# Call the constructor of the parent class (Pytorch torch.nn.Module)", "\n", "super", "(", "UMDQN_KL_Model_Atari", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# Initialization of the Deep Neural Network", "\n", "if", "minAtar", ":", "\n", "            ", "self", ".", "stateEmbeddingDNN", "=", "DNN_MinAtar", "(", "numberOfInputs", ",", "stateEmbedding", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "stateEmbeddingDNN", "=", "DNN_Atari", "(", "numberOfInputs", ",", "stateEmbedding", ")", "\n", "", "self", ".", "UMNN", "=", "OneDimensionnalNF", "(", "stateEmbedding", "+", "1", ",", "structureUMNN", ",", "numberOfSteps", ",", "numberOfOutputs", ",", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.UMDQN_KL_Model_Atari.UMDQN_KL_Model_Atari.forward": [[62, 86], ["state.size", "UMDQN_KL_Model_Atari.UMDQN_KL_Model_Atari.stateEmbeddingDNN", "x.clamp.clamp.repeat().view", "UMDQN_KL_Model_Atari.UMDQN_KL_Model_Atari.UMNN", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "x.clamp.clamp.clamp", "x.clamp.clamp.size", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "x.clamp.clamp.repeat", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "int", "len", "len"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "state", ",", "q", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Implementing the forward pass of the Deep Neural Network.\n        \n        INPUTS: - state: RL state.\n                - q: Samples of potential returns.\n        \n        OUTPUTS: - output: Output of the Deep Neural Network.\n        \"\"\"", "\n", "\n", "# State embedding part of the Deep Neural Network", "\n", "batchSize", "=", "state", ".", "size", "(", "0", ")", "\n", "x", "=", "self", ".", "stateEmbeddingDNN", "(", "state", ")", "\n", "x", "=", "x", ".", "repeat", "(", "1", ",", "int", "(", "len", "(", "q", ")", "/", "len", "(", "state", ")", ")", ")", ".", "view", "(", "-", "1", ",", "x", ".", "size", "(", "1", ")", ")", "\n", "\n", "# UMNN part of the Deep Neural Network", "\n", "x", "=", "self", ".", "UMNN", "(", "q", ",", "x", ")", "\n", "\n", "# Formatting of the output and post processing operations", "\n", "x", "=", "torch", ".", "cat", "(", "torch", ".", "chunk", "(", "torch", ".", "transpose", "(", "x", ",", "0", ",", "1", ")", ",", "batchSize", ",", "dim", "=", "1", ")", ",", "0", ")", "\n", "x", "=", "torch", ".", "exp", "(", "x", ")", "\n", "x", "=", "x", ".", "clamp", "(", "min", "=", "1e-6", ")", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.UMDQN_KL_Model_Atari.UMDQN_KL_Model_Atari.getExpectation": [[88, 106], ["UMDQN_KL_Model_Atari.UMDQN_KL_Model_Atari.stateEmbeddingDNN", "UMDQN_KL_Model_Atari.UMDQN_KL_Model_Atari.UMNN.expectation"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.MonotonicNN.expectation"], ["", "def", "getExpectation", "(", "self", ",", "state", ",", "minReturn", ",", "maxReturn", ",", "numberOfPoints", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Get the expectation of the PDF internally computed by the UMNN.\n        \n        INPUTS: - state: RL state.\n                - minReturn: Minimum return.\n                - maxReturn: Maximum return.\n                - numberOfPoints: Number of points for the computations (accuracy).\n        \n        OUTPUTS: - expectation: Expectation computed.\n        \"\"\"", "\n", "\n", "# State embedding part of the Deep Neural Network", "\n", "state", "=", "self", ".", "stateEmbeddingDNN", "(", "state", ")", "\n", "\n", "# Computation of the expectation of the PDF internally computed by the UMNN", "\n", "expectation", "=", "self", ".", "UMNN", ".", "expectation", "(", "state", ",", "lambda", "x", ":", "x", ",", "minReturn", ",", "maxReturn", ",", "numberOfPoints", ")", "\n", "return", "expectation", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.DNN_Atari.DNN_Atari.__init__": [[31, 49], ["torch.Module.__init__", "Models.CNN_Atari.CNN_Atari().getOutputSize", "torch.Sequential", "Models.CNN_Atari.CNN_Atari", "Models.FeedforwardDNN.FeedForwardDNN", "Models.CNN_Atari.CNN_Atari"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.MonotonicNN.__init__", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.CNN_Atari.CNN_Atari.getOutputSize"], ["def", "__init__", "(", "self", ",", "numberOfInputs", ",", "numberOfOutputs", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Defining and initializing the Deep Neural Network.\n        \n        INPUTS: - numberOfInputs: Number of inputs of the Deep Neural Network.\n                - numberOfOutputs: Number of outputs of the Deep Neural Network.\n        \n        OUTPUTS: /\n        \"\"\"", "\n", "\n", "# Call the constructor of the parent class (Pytorch torch.nn.Module)", "\n", "super", "(", "DNN_Atari", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# Initialization of the Deep Neural Network.", "\n", "CNNOutputSize", "=", "CNN_Atari", "(", "numberOfInputs", ")", ".", "getOutputSize", "(", ")", "\n", "self", ".", "network", "=", "nn", ".", "Sequential", "(", "\n", "CNN_Atari", "(", "numberOfInputs", ")", ",", "\n", "FeedForwardDNN", "(", "CNNOutputSize", ",", "numberOfOutputs", ",", "[", "512", "]", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.DNN_Atari.DNN_Atari.forward": [[52, 62], ["DNN_Atari.DNN_Atari.network"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Implementing the forward pass of the Deep Neural Network.\n        \n        INPUTS: - x: Input of the Deep Neural Network.\n        \n        OUTPUTS: - y: Output of the Deep Neural Network.\n        \"\"\"", "\n", "\n", "return", "self", ".", "network", "(", "x", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.CNN_Atari.CNN_Atari.__init__": [[29, 49], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.MonotonicNN.__init__"], ["def", "__init__", "(", "self", ",", "numberOfInputs", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Defining and initializing the Convolutional Neural Network.\n        \n        INPUTS: - numberOfInputs: Number of inputs of the Convolutional Neural Network.\n        \n        OUTPUTS: /\n        \"\"\"", "\n", "\n", "# Call the constructor of the parent class (Pytorch torch.nn.Module)", "\n", "super", "(", "CNN_Atari", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# Initialization of the Convolutional Neural Network", "\n", "self", ".", "network", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "numberOfInputs", ",", "32", ",", "kernel_size", "=", "8", ",", "stride", "=", "4", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Conv2d", "(", "32", ",", "64", ",", "kernel_size", "=", "4", ",", "stride", "=", "2", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Conv2d", "(", "64", ",", "64", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.CNN_Atari.CNN_Atari.getOutputSize": [[52, 62], ["CNN_Atari.CNN_Atari.network().view().size", "CNN_Atari.CNN_Atari.network().view", "CNN_Atari.CNN_Atari.network", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "def", "getOutputSize", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Get the size of the Convolutional Neural Network output.\n        \n        INPUTS: /\n        \n        OUTPUTS: - size: Size of the Convolutional Neural Network. output.\n        \"\"\"", "\n", "\n", "return", "self", ".", "network", "(", "torch", ".", "zeros", "(", "1", ",", "*", "(", "4", ",", "84", ",", "84", ")", ")", ")", ".", "view", "(", "1", ",", "-", "1", ")", ".", "size", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.CNN_Atari.CNN_Atari.forward": [[64, 75], ["CNN_Atari.CNN_Atari.network", "CNN_Atari.CNN_Atari.view", "CNN_Atari.CNN_Atari.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Implementing the forward pass of the Convolutional Neural Network.\n        \n        INPUTS: - x: Input of the Convolutional Neural Network.\n        \n        OUTPUTS: - y: Output of the Convolutional Neural Network.\n        \"\"\"", "\n", "\n", "x", "=", "self", ".", "network", "(", "x", ")", "\n", "return", "x", ".", "view", "(", "x", ".", "size", "(", "0", ")", ",", "-", "1", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.UMDQN_C_Model_Atari.UMDQN_C_Model_Atari.__init__": [[35, 61], ["torch.Module.__init__", "Models.MonotonicNN.MonotonicNN", "Models.DNN_MinAtar.DNN_MinAtar", "Models.DNN_Atari.DNN_Atari"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.MonotonicNN.__init__"], ["def", "__init__", "(", "self", ",", "numberOfInputs", ",", "numberOfOutputs", ",", "\n", "structureUMNN", ",", "stateEmbedding", ",", "numberOfSteps", ",", "\n", "device", "=", "'cpu'", ",", "minAtar", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Defining and initializing the Deep Neural Network.\n        \n        INPUTS: - numberOfInputs: Number of inputs of the Deep Neural Network.\n                - numberOfOutputs: Number of outputs of the Deep Neural Network.\n                - structureUMNN: Structure of the UMNN for distribution representation.\n                - stateEmbedding: Dimension of the state embedding.\n                - numberOfSteps: Number of integration steps for the UMNN.\n                - device: Hardware device (CPU or GPU).\n                - minAtar: Boolean specifying whether the env is \"MinAtar\" or not.\n        \n        OUTPUTS: /\n        \"\"\"", "\n", "\n", "# Call the constructor of the parent class (Pytorch torch.nn.Module)", "\n", "super", "(", "UMDQN_C_Model_Atari", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# Initialization of the Deep Neural Network", "\n", "if", "minAtar", ":", "\n", "            ", "self", ".", "stateEmbeddingDNN", "=", "DNN_MinAtar", "(", "numberOfInputs", ",", "stateEmbedding", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "stateEmbeddingDNN", "=", "DNN_Atari", "(", "numberOfInputs", ",", "stateEmbedding", ")", "\n", "", "self", ".", "UMNN", "=", "MonotonicNN", "(", "stateEmbedding", "+", "1", ",", "structureUMNN", ",", "numberOfSteps", ",", "numberOfOutputs", ",", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.UMDQN_C_Model_Atari.UMDQN_C_Model_Atari.forward": [[63, 84], ["state.size", "UMDQN_C_Model_Atari.UMDQN_C_Model_Atari.stateEmbeddingDNN", "torch.sigmoid.repeat().view", "torch.sigmoid.repeat().view", "UMDQN_C_Model_Atari.UMDQN_C_Model_Atari.UMNN", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.sigmoid.size", "torch.sigmoid.size", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "torch.sigmoid.repeat", "torch.sigmoid.repeat", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "int", "len", "len"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "state", ",", "q", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Implementing the forward pass of the Deep Neural Network.\n        \n        INPUTS: - state: RL state.\n                - q: Samples of potential returns.\n        \n        OUTPUTS: - output: Output of the Deep Neural Network.\n        \"\"\"", "\n", "\n", "# State embedding part of the Deep Neural Network", "\n", "batchSize", "=", "state", ".", "size", "(", "0", ")", "\n", "x", "=", "self", ".", "stateEmbeddingDNN", "(", "state", ")", "\n", "x", "=", "x", ".", "repeat", "(", "1", ",", "int", "(", "len", "(", "q", ")", "/", "len", "(", "state", ")", ")", ")", ".", "view", "(", "-", "1", ",", "x", ".", "size", "(", "1", ")", ")", "\n", "\n", "# UMNNN part of the Deep Neural Network", "\n", "x", "=", "self", ".", "UMNN", "(", "q", ",", "x", ")", "\n", "\n", "# Sigmoid activation function + appropriate format", "\n", "x", "=", "torch", ".", "sigmoid", "(", "x", ")", "\n", "return", "torch", ".", "cat", "(", "torch", ".", "chunk", "(", "torch", ".", "transpose", "(", "x", ",", "0", ",", "1", ")", ",", "batchSize", ",", "dim", "=", "1", ")", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.UMDQN_C_Model_Atari.UMDQN_C_Model_Atari.getDerivative": [[86, 109], ["state.size", "UMDQN_C_Model_Atari.UMDQN_C_Model_Atari.stateEmbeddingDNN", "torch.sigmoid.repeat().view", "torch.sigmoid.repeat().view", "UMDQN_C_Model_Atari.UMDQN_C_Model_Atari.UMNN", "UMDQN_C_Model_Atari.UMDQN_C_Model_Atari.UMNN", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.sigmoid.size", "torch.sigmoid.size", "torch.chunk", "torch.chunk", "torch.chunk", "torch.chunk", "torch.sigmoid.repeat", "torch.sigmoid.repeat", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "int", "len", "len"], "methods", ["None"], ["", "def", "getDerivative", "(", "self", ",", "state", ",", "q", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Get the derivative internally computed by the UMNN.\n        \n        INPUTS: - state: RL state.\n                - q: Samples of potential returns.\n        \n        OUTPUTS: - output: Derivative internally computed by the UMNN.\n        \"\"\"", "\n", "\n", "# State embedding part of the Deep Neural Network", "\n", "batchSize", "=", "state", ".", "size", "(", "0", ")", "\n", "x", "=", "self", ".", "stateEmbeddingDNN", "(", "state", ")", "\n", "x", "=", "x", ".", "repeat", "(", "1", ",", "int", "(", "len", "(", "q", ")", "/", "len", "(", "state", ")", ")", ")", ".", "view", "(", "-", "1", ",", "x", ".", "size", "(", "1", ")", ")", "\n", "\n", "# Computation of both PDF and CDF", "\n", "pdf", "=", "self", ".", "UMNN", "(", "q", ",", "x", ",", "only_derivative", "=", "True", ")", "\n", "cdf", "=", "self", ".", "UMNN", "(", "q", ",", "x", ",", "only_derivative", "=", "False", ")", "\n", "\n", "# Correction of the sigmoid + appropriate format", "\n", "x", "=", "torch", ".", "sigmoid", "(", "cdf", ")", "\n", "x", "=", "x", "*", "(", "1", "-", "x", ")", "*", "pdf", "\n", "return", "torch", ".", "cat", "(", "torch", ".", "chunk", "(", "torch", ".", "transpose", "(", "x", ",", "0", ",", "1", ")", ",", "batchSize", ",", "dim", "=", "1", ")", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.UMDQN_C_Model_Atari.UMDQN_C_Model_Atari.getExpectation": [[111, 129], ["UMDQN_C_Model_Atari.UMDQN_C_Model_Atari.stateEmbeddingDNN", "UMDQN_C_Model_Atari.UMDQN_C_Model_Atari.UMNN.expectation", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.MonotonicNN.expectation"], ["", "def", "getExpectation", "(", "self", ",", "state", ",", "minReturn", ",", "maxReturn", ",", "numberOfPoints", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Get the expectation of the PDF internally computed by the UMNN.\n        \n        INPUTS: - state: RL state.\n                - minReturn: Minimum return.\n                - maxReturn: Maximum return.\n                - numberOfPoints: Number of points for the computations (accuracy).\n        \n        OUTPUTS: - expectation: Expectation computed.\n        \"\"\"", "\n", "\n", "# State embedding part of the Deep Neural Network", "\n", "state", "=", "self", ".", "stateEmbeddingDNN", "(", "state", ")", "\n", "\n", "# Computation of the expectation of the PDF internally computed by the UMNN", "\n", "expectation", "=", "self", ".", "UMNN", ".", "expectation", "(", "state", ",", "lambda", "x", ":", "x", ",", "lambda", "x", ":", "torch", ".", "sigmoid", "(", "x", ")", "*", "(", "1", "-", "torch", ".", "sigmoid", "(", "x", ")", ")", ",", "minReturn", ",", "maxReturn", ",", "numberOfPoints", ")", "\n", "return", "expectation", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.FQF_Model_Atari.FQF_Model_Atari.__init__": [[34, 66], ["torch.Module.__init__", "torch.tensor().view().to", "torch.tensor().view().to", "torch.tensor().view().to", "torch.tensor().view().to", "Models.CNN_MinAtar.CNN_MinAtar", "Models.CNN_MinAtar.CNN_MinAtar().getOutputSize", "torch.Sequential", "torch.Sequential", "Models.FeedforwardDNN.FeedForwardDNN", "Models.CNN_Atari.CNN_Atari", "Models.CNN_Atari.CNN_Atari().getOutputSize", "torch.Sequential", "torch.Sequential", "Models.FeedforwardDNN.FeedForwardDNN", "torch.tensor().view", "torch.tensor().view", "torch.tensor().view", "torch.tensor().view", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "Models.CNN_MinAtar.CNN_MinAtar", "Models.CNN_Atari.CNN_Atari", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "range"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.MonotonicNN.__init__", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.CNN_Atari.CNN_Atari.getOutputSize", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.CNN_Atari.CNN_Atari.getOutputSize"], ["def", "__init__", "(", "self", ",", "numberOfInputs", ",", "numberOfOutputs", ",", "NCos", "=", "64", ",", "device", "=", "'cpu'", ",", "\n", "minAtar", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Defining and initializing the Deep Neural Network.\n        \n        INPUTS: - numberOfInputs: Number of inputs of the Deep Neural Network.\n                - numberOfOutputs: Number of outputs of the Deep Neural Network.\n                - Ncos: Number of elements in cosine function.\n                - minAtar: Boolean specifying whether the env is \"MinAtar\" or not.\n        \n        OUTPUTS: /\n        \"\"\"", "\n", "\n", "# Call the constructor of the parent class (Pytorch torch.nn.Module)", "\n", "super", "(", "FQF_Model_Atari", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# Initialization of useful variables", "\n", "self", ".", "device", "=", "device", "\n", "self", ".", "NCos", "=", "NCos", "\n", "self", ".", "piMultiples", "=", "torch", ".", "tensor", "(", "[", "np", ".", "pi", "*", "i", "for", "i", "in", "range", "(", "self", ".", "NCos", ")", "]", ",", "dtype", "=", "torch", ".", "float", ")", ".", "view", "(", "1", ",", "1", ",", "self", ".", "NCos", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "# Initialization of the Deep Neural Network.", "\n", "if", "minAtar", ":", "\n", "            ", "self", ".", "stateEmbedding", "=", "CNN_MinAtar", "(", "numberOfInputs", ")", "\n", "self", ".", "stateEmbeddingSize", "=", "CNN_MinAtar", "(", "numberOfInputs", ")", ".", "getOutputSize", "(", ")", "\n", "self", ".", "cosEmbedding", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "NCos", ",", "self", ".", "stateEmbeddingSize", ")", ",", "nn", ".", "ReLU", "(", ")", ")", "\n", "self", ".", "feedForwardDNN", "=", "FeedForwardDNN", "(", "self", ".", "stateEmbeddingSize", ",", "numberOfOutputs", ",", "[", "128", "]", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "stateEmbedding", "=", "CNN_Atari", "(", "numberOfInputs", ")", "\n", "self", ".", "stateEmbeddingSize", "=", "CNN_Atari", "(", "numberOfInputs", ")", ".", "getOutputSize", "(", ")", "\n", "self", ".", "cosEmbedding", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "NCos", ",", "self", ".", "stateEmbeddingSize", ")", ",", "nn", ".", "ReLU", "(", ")", ")", "\n", "self", ".", "feedForwardDNN", "=", "FeedForwardDNN", "(", "self", ".", "stateEmbeddingSize", ",", "numberOfOutputs", ",", "[", "512", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.FQF_Model_Atari.FQF_Model_Atari.embedding": [[68, 78], ["FQF_Model_Atari.FQF_Model_Atari.stateEmbedding"], "methods", ["None"], ["", "", "def", "embedding", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Implementing the embedding part of the Deep Neural Network.\n        \n        INPUTS: - x: Input of the Deep Neural Network.\n        \n        OUTPUTS: - y: Embedded input of the Deep Neural Network.\n        \"\"\"", "\n", "\n", "return", "self", ".", "stateEmbedding", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.FQF_Model_Atari.FQF_Model_Atari.getEmbeddingSize": [[80, 90], ["None"], "methods", ["None"], ["", "def", "getEmbeddingSize", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Return the size of the state embedding.\n        \n        INPUTS: /\n        \n        OUTPUTS: - stateEmbeddingSize: Size of the state embedding.\n        \"\"\"", "\n", "\n", "return", "self", ".", "stateEmbeddingSize", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.FQF_Model_Atari.FQF_Model_Atari.forward": [[92, 121], ["embedding.unsqueeze.size", "taus.size", "torch.cos().view", "torch.cos().view", "torch.cos().view", "torch.cos().view", "FQF_Model_Atari.FQF_Model_Atari.cosEmbedding().view", "FQF_Model_Atari.FQF_Model_Atari.feedForwardDNN", "embedding.unsqueeze.transpose", "FQF_Model_Atari.FQF_Model_Atari.stateEmbedding().unsqueeze", "embedding.unsqueeze", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "FQF_Model_Atari.FQF_Model_Atari.cosEmbedding", "FQF_Model_Atari.FQF_Model_Atari.stateEmbedding", "taus.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "taus", ",", "embedding", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        GOAL: Implementing the forward pass of the Deep Neural Network.\n        \n        INPUTS: - x: Input of the Deep Neural Network.\n                - taus: Quantiles (generated by the FPN).\n                - embedding: Embedding of the Deep Neural Network input (state)\n        \n        OUTPUTS: - y: Output of the Deep Neural Network.\n        \"\"\"", "\n", "\n", "# State embedding part of the Deep Neural Network", "\n", "batchSize", "=", "x", ".", "size", "(", "0", ")", "\n", "if", "embedding", "==", "None", ":", "\n", "            ", "x", "=", "self", ".", "stateEmbedding", "(", "x", ")", ".", "unsqueeze", "(", "1", ")", "\n", "", "else", ":", "\n", "            ", "x", "=", "embedding", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "# Quantile embedding part of the Deep Neural Network", "\n", "", "N", "=", "taus", ".", "size", "(", "1", ")", "\n", "cos", "=", "torch", ".", "cos", "(", "taus", ".", "unsqueeze", "(", "2", ")", "*", "self", ".", "piMultiples", ")", ".", "view", "(", "batchSize", "*", "N", ",", "self", ".", "NCos", ")", "\n", "cos", "=", "self", ".", "cosEmbedding", "(", "cos", ")", ".", "view", "(", "batchSize", ",", "N", ",", "-", "1", ")", "\n", "\n", "# Multiplication of both state and cos embeddings outputs (combination)", "\n", "x", "=", "(", "x", "*", "cos", ")", ".", "view", "(", "batchSize", ",", "N", ",", "-", "1", ")", "\n", "\n", "# Distribution part of the Deep Neural Network", "\n", "x", "=", "self", ".", "feedForwardDNN", "(", "x", ")", "\n", "return", "x", ".", "transpose", "(", "1", ",", "2", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.IntegrandNN.__init__": [[17, 29], ["torch.Module.__init__", "zip", "MonotonicNN.IntegrandNN.net.pop", "MonotonicNN.IntegrandNN.net.append", "torch.Sequential", "torch.Sequential", "MonotonicNN.IntegrandNN.net.extend", "torch.ELU", "torch.ELU", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.MonotonicNN.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_d", ",", "hidden_layers", ",", "n_out", "=", "1", ")", ":", "\n", "        ", "super", "(", "IntegrandNN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "net", "=", "[", "]", "\n", "hs", "=", "[", "in_d", "]", "+", "hidden_layers", "+", "[", "n_out", "]", "\n", "for", "h0", ",", "h1", "in", "zip", "(", "hs", ",", "hs", "[", "1", ":", "]", ")", ":", "\n", "            ", "self", ".", "net", ".", "extend", "(", "[", "\n", "nn", ".", "Linear", "(", "h0", ",", "h1", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "]", ")", "\n", "", "self", ".", "net", ".", "pop", "(", ")", "\n", "self", ".", "net", ".", "append", "(", "nn", ".", "ELU", "(", ")", ")", "\n", "self", ".", "net", "=", "nn", ".", "Sequential", "(", "*", "self", ".", "net", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.IntegrandNN.forward": [[30, 32], ["MonotonicNN.IntegrandNN.net", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "h", ")", ":", "\n", "        ", "return", "self", ".", "net", "(", "torch", ".", "cat", "(", "(", "x", ",", "h", ")", ",", "1", ")", ")", "+", "1.", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.OneDimensionnalNF.__init__": [[35, 42], ["torch.Module.__init__", "MonotonicNN.MonotonicNN", "MonotonicNN.OneDimensionnalNF.register_buffer", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.MonotonicNN.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_d", ",", "hidden_layers", ",", "nb_steps", "=", "200", ",", "n_out", "=", "1", ",", "dev", "=", "\"cpu\"", ")", ":", "\n", "        ", "super", "(", "OneDimensionnalNF", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "device", "=", "dev", "\n", "self", ".", "nb_steps", "=", "nb_steps", "\n", "self", ".", "n_out", "=", "n_out", "\n", "self", ".", "net", "=", "MonotonicNN", "(", "in_d", ",", "hidden_layers", ",", "nb_steps", "=", "nb_steps", ",", "n_out", "=", "n_out", ",", "dev", "=", "dev", ")", "\n", "self", ".", "register_buffer", "(", "\"pi\"", ",", "torch", ".", "tensor", "(", "math", ".", "pi", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.OneDimensionnalNF.forward": [[47, 58], ["torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "MonotonicNN.OneDimensionnalNF.net.net", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "z.clamp_", "MonotonicNN.OneDimensionnalNF.net.integrand", "torch.log", "torch.log", "torch.log", "torch.log", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "UMNN.ParallelNeuralIntegral.apply", "torch.log", "torch.log", "torch.log", "torch.log", "MonotonicNN._flatten", "MonotonicNN.OneDimensionnalNF.net.integrand.parameters"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN._flatten"], ["def", "forward", "(", "self", ",", "x", ",", "h", ")", ":", "\n", "        ", "x0", "=", "torch", ".", "zeros", "(", "x", ".", "shape", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "out", "=", "self", ".", "net", ".", "net", "(", "h", ")", "\n", "offset", "=", "out", "[", ":", ",", ":", "self", ".", "n_out", "]", "\n", "scaling", "=", "torch", ".", "exp", "(", "out", "[", ":", ",", "self", ".", "n_out", ":", "]", ")", "\n", "jac", "=", "scaling", "*", "self", ".", "net", ".", "integrand", "(", "x", ",", "h", ")", "\n", "z", "=", "scaling", "*", "ParallelNeuralIntegral", ".", "apply", "(", "x0", ",", "x", ",", "self", ".", "net", ".", "integrand", ",", "_flatten", "(", "self", ".", "net", ".", "integrand", ".", "parameters", "(", ")", ")", ",", "h", ",", "self", ".", "nb_steps", ")", "+", "offset", "\n", "z", ".", "clamp_", "(", "-", "10.", ",", "10.", ")", "\n", "log_prob_gauss", "=", "-", ".5", "*", "(", "torch", ".", "log", "(", "self", ".", "pi", "*", "2", ")", "+", "z", "**", "2", ")", "\n", "ll", "=", "log_prob_gauss", "+", "torch", ".", "log", "(", "jac", "+", "1e-10", ")", "\n", "return", "ll", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.OneDimensionnalNF.expectation": [[59, 89], ["torch.arange().to", "torch.arange().to", "torch.arange().to", "torch.arange().to", "torch.argmin().item", "torch.argmin().item", "torch.argmin().item", "torch.argmin().item", "MonotonicNN.OneDimensionnalNF.net.net", "out[].unsqueeze().expand", "torch.exp().unsqueeze().expand", "torch.exp().unsqueeze().expand", "torch.exp().unsqueeze().expand", "torch.exp().unsqueeze().expand", "h.unsqueeze().expand().reshape", "torch.arange().to.unsqueeze().expand().reshape", "torch.arange().to.unsqueeze().expand().reshape", "MonotonicNN.OneDimensionnalNF.net.integrand", "MonotonicNN.OneDimensionnalNF.reshape", "MonotonicNN.OneDimensionnalNF.cumsum", "torch.log", "torch.log", "torch.log", "torch.log", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.argmin", "torch.argmin", "torch.argmin", "torch.argmin", "out[].unsqueeze", "torch.exp().unsqueeze", "torch.exp().unsqueeze", "torch.exp().unsqueeze", "torch.exp().unsqueeze", "h.unsqueeze().expand", "torch.arange().to.unsqueeze().expand", "torch.arange().to.unsqueeze().expand", "z[].expand", "torch.log", "torch.log", "torch.log", "torch.log", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "h.unsqueeze", "torch.arange().to.unsqueeze", "torch.arange().to.unsqueeze", "x_func().unsqueeze().unsqueeze().expand", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "x_func().unsqueeze().unsqueeze", "x_func().unsqueeze", "x_func"], "methods", ["None"], ["", "def", "expectation", "(", "self", ",", "h", ",", "x_func", ",", "min", "=", "-", "10", ",", "max", "=", "10", ",", "npts", "=", "1000", ")", ":", "\n", "# Using first order Euler method .", "\n", "        ", "b_size", "=", "h", ".", "shape", "[", "0", "]", "\n", "n_out", "=", "self", ".", "n_out", "\n", "dx", "=", "(", "max", "-", "min", ")", "/", "(", "npts", "-", "1", ")", "\n", "emb_size", "=", "h", ".", "shape", "[", "1", "]", "\n", "\n", "x", "=", "torch", ".", "arange", "(", "min", ",", "max", "+", "(", "max", "-", "min", ")", "/", "(", "npts", "-", "1", ")", ",", "dx", ")", ".", "to", "(", "h", ".", "device", ")", "\n", "npts", "=", "x", ".", "shape", "[", "0", "]", "\n", "zero_idx", "=", "torch", ".", "argmin", "(", "x", "**", "2", ")", ".", "item", "(", ")", "\n", "\n", "out", "=", "self", ".", "net", ".", "net", "(", "h", ")", "\n", "offset", "=", "out", "[", ":", ",", ":", "self", ".", "n_out", "]", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "b_size", ",", "npts", ",", "n_out", ")", "\n", "scaling", "=", "torch", ".", "exp", "(", "out", "[", ":", ",", "self", ".", "n_out", ":", "]", ")", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "b_size", ",", "npts", ",", "n_out", ")", "\n", "\n", "h_values", "=", "h", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "b_size", ",", "npts", ",", "emb_size", ")", ".", "reshape", "(", "-", "1", ",", "emb_size", ")", "\n", "x_values", "=", "x", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "b_size", ",", "npts", ")", ".", "reshape", "(", "-", "1", ",", "1", ")", "\n", "\n", "f_values", "=", "self", ".", "net", ".", "integrand", "(", "x_values", ",", "h_values", ")", "\n", "f_values", "=", "f_values", ".", "reshape", "(", "b_size", ",", "npts", ",", "n_out", ")", "*", "scaling", "\n", "\n", "z", "=", "(", "dx", "*", "f_values", ".", "cumsum", "(", "1", ")", ")", "\n", "z", "=", "(", "z", "-", "z", "[", ":", ",", "[", "zero_idx", "]", ",", ":", "]", ".", "expand", "(", "-", "1", ",", "npts", ",", "-", "1", ")", ")", "+", "offset", "\n", "log_prob_gauss", "=", "-", ".5", "*", "(", "torch", ".", "log", "(", "self", ".", "pi", "*", "2", ")", "+", "z", "**", "2", ")", "\n", "ll", "=", "log_prob_gauss", "+", "torch", ".", "log", "(", "f_values", "+", "1e-10", ")", "\n", "\n", "\n", "expectations", "=", "(", "x_func", "(", "x", ")", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "b_size", ",", "npts", ",", "n_out", ")", "*", "torch", ".", "exp", "(", "ll", ")", ")", ".", "sum", "(", "1", ")", "*", "dx", "\n", "\n", "return", "expectations", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.MonotonicNN.__init__": [[97, 112], ["torch.Module.__init__", "MonotonicNN.IntegrandNN", "zip", "MonotonicNN.MonotonicNN.net.pop", "torch.Sequential", "torch.Sequential", "MonotonicNN.MonotonicNN.net.extend", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.MonotonicNN.__init__"], ["def", "__init__", "(", "self", ",", "in_d", ",", "hidden_layers", ",", "nb_steps", "=", "200", ",", "n_out", "=", "1", ",", "dev", "=", "\"cpu\"", ")", ":", "\n", "        ", "super", "(", "MonotonicNN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "integrand", "=", "IntegrandNN", "(", "in_d", ",", "hidden_layers", ",", "n_out", ")", "\n", "self", ".", "net", "=", "[", "]", "\n", "hs", "=", "[", "in_d", "-", "1", "]", "+", "hidden_layers", "+", "[", "2", "*", "n_out", "]", "\n", "for", "h0", ",", "h1", "in", "zip", "(", "hs", ",", "hs", "[", "1", ":", "]", ")", ":", "\n", "            ", "self", ".", "net", ".", "extend", "(", "[", "\n", "nn", ".", "Linear", "(", "h0", ",", "h1", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "]", ")", "\n", "", "self", ".", "net", ".", "pop", "(", ")", "\n", "self", ".", "net", "=", "nn", ".", "Sequential", "(", "*", "self", ".", "net", ")", "\n", "self", ".", "device", "=", "dev", "\n", "self", ".", "nb_steps", "=", "nb_steps", "\n", "self", ".", "n_out", "=", "n_out", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.MonotonicNN.forward": [[116, 124], ["torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "MonotonicNN.MonotonicNN.net", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "MonotonicNN.MonotonicNN.integrand", "UMNN.ParallelNeuralIntegral.apply", "MonotonicNN._flatten", "MonotonicNN.MonotonicNN.integrand.parameters"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN._flatten"], ["def", "forward", "(", "self", ",", "x", ",", "h", ",", "only_derivative", "=", "False", ")", ":", "\n", "        ", "x0", "=", "torch", ".", "zeros", "(", "x", ".", "shape", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "out", "=", "self", ".", "net", "(", "h", ")", "\n", "offset", "=", "out", "[", ":", ",", ":", "self", ".", "n_out", "]", "\n", "scaling", "=", "torch", ".", "exp", "(", "out", "[", ":", ",", "self", ".", "n_out", ":", "]", ")", "\n", "if", "only_derivative", ":", "\n", "            ", "return", "scaling", "*", "self", ".", "integrand", "(", "x", ",", "h", ")", "\n", "", "return", "scaling", "*", "ParallelNeuralIntegral", ".", "apply", "(", "x0", ",", "x", ",", "self", ".", "integrand", ",", "_flatten", "(", "self", ".", "integrand", ".", "parameters", "(", ")", ")", ",", "h", ",", "self", ".", "nb_steps", ")", "+", "offset", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.MonotonicNN.inverse": [[133, 153], ["h.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view", "[].view", "[].view", "range", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "[].view", "h.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.arange().view", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "MonotonicNN.MonotonicNN.forward().view", "MonotonicNN.MonotonicNN.forward().view", "h.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze().expand", "MonotonicNN.MonotonicNN.forward().view", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "MonotonicNN.MonotonicNN.forward", "MonotonicNN.MonotonicNN.forward", "h.unsqueeze().expand().contiguous().view.unsqueeze().expand().contiguous().view.unsqueeze", "x_max.view", "x_min.view", "MonotonicNN.MonotonicNN.forward", "x_middle.view"], "methods", ["home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.MonotonicNN.forward", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.MonotonicNN.forward", "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.MonotonicNN.forward"], ["def", "inverse", "(", "self", ",", "y", ",", "h", ",", "min", "=", "-", "10", ",", "max", "=", "10", ",", "nb_iter", "=", "10", ")", ":", "\n", "        ", "idx", "=", "(", "torch", ".", "arange", "(", "0", ",", "self", ".", "n_out", "**", "2", ",", "self", ".", "n_out", "+", "1", ")", ".", "view", "(", "1", ",", "-", "1", ")", "+", "torch", ".", "arange", "(", "0", ",", "(", "self", ".", "n_out", "**", "2", ")", "*", "y", ".", "shape", "[", "0", "]", ",", "self", ".", "n_out", "**", "2", ")", ".", "view", "(", "-", "1", ",", "1", ")", ")", ".", "view", "(", "-", "1", ")", "\n", "h", "=", "h", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "-", "1", ",", "self", ".", "n_out", ",", "-", "1", ")", ".", "contiguous", "(", ")", ".", "view", "(", "y", ".", "shape", "[", "0", "]", "*", "self", ".", "n_out", ",", "-", "1", ")", "\n", "\n", "# Old inversion by binary search", "\n", "x_max", "=", "torch", ".", "ones", "(", "y", ".", "shape", "[", "0", "]", ",", "self", ".", "n_out", ")", ".", "to", "(", "y", ".", "device", ")", "*", "max", "\n", "x_min", "=", "torch", ".", "ones", "(", "y", ".", "shape", "[", "0", "]", ",", "self", ".", "n_out", ")", ".", "to", "(", "y", ".", "device", ")", "*", "min", "\n", "y_max", "=", "self", ".", "forward", "(", "x_max", ".", "view", "(", "-", "1", ",", "1", ")", ",", "h", ")", ".", "view", "(", "-", "1", ")", "[", "idx", "]", ".", "view", "(", "-", "1", ",", "self", ".", "n_out", ")", "\n", "y_min", "=", "self", ".", "forward", "(", "x_min", ".", "view", "(", "-", "1", ",", "1", ")", ",", "h", ")", ".", "view", "(", "-", "1", ")", "[", "idx", "]", ".", "view", "(", "-", "1", ",", "self", ".", "n_out", ")", "\n", "\n", "for", "i", "in", "range", "(", "nb_iter", ")", ":", "\n", "            ", "x_middle", "=", "(", "x_max", "+", "x_min", ")", "/", "2", "\n", "y_middle", "=", "self", ".", "forward", "(", "x_middle", ".", "view", "(", "-", "1", ",", "1", ")", ",", "h", ")", ".", "view", "(", "-", "1", ")", "[", "idx", "]", ".", "view", "(", "-", "1", ",", "self", ".", "n_out", ")", "\n", "left", "=", "(", "y_middle", ">", "y", ")", ".", "float", "(", ")", "\n", "right", "=", "1", "-", "left", "\n", "x_max", "=", "left", "*", "x_middle", "+", "right", "*", "x_max", "\n", "x_min", "=", "right", "*", "x_middle", "+", "left", "*", "x_min", "\n", "y_max", "=", "left", "*", "y_middle", "+", "right", "*", "y_max", "\n", "y_min", "=", "right", "*", "y_middle", "+", "left", "*", "y_min", "\n", "", "return", "(", "x_max", "+", "x_min", ")", "/", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN.MonotonicNN.expectation": [[154, 182], ["torch.arange().to", "torch.arange().to", "torch.arange().to", "torch.arange().to", "torch.argmin().item", "torch.argmin().item", "torch.argmin().item", "torch.argmin().item", "MonotonicNN.MonotonicNN.net", "out[].unsqueeze().expand", "torch.exp().unsqueeze().expand", "torch.exp().unsqueeze().expand", "torch.exp().unsqueeze().expand", "torch.exp().unsqueeze().expand", "h.unsqueeze().expand().reshape", "torch.arange().to.unsqueeze().expand().reshape", "torch.arange().to.unsqueeze().expand().reshape", "MonotonicNN.MonotonicNN.integrand", "out_deriv", "MonotonicNN.MonotonicNN.reshape", "MonotonicNN.MonotonicNN.cumsum", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.argmin", "torch.argmin", "torch.argmin", "torch.argmin", "out[].unsqueeze", "torch.exp().unsqueeze", "torch.exp().unsqueeze", "torch.exp().unsqueeze", "torch.exp().unsqueeze", "h.unsqueeze().expand", "torch.arange().to.unsqueeze().expand", "torch.arange().to.unsqueeze().expand", "F_values[].expand", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "h.unsqueeze", "torch.arange().to.unsqueeze", "torch.arange().to.unsqueeze", "x_func().unsqueeze().unsqueeze().expand", "x_func().unsqueeze().unsqueeze", "x_func().unsqueeze", "x_func"], "methods", ["None"], ["", "def", "expectation", "(", "self", ",", "h", ",", "x_func", ",", "out_deriv", ",", "min", "=", "-", "10", ",", "max", "=", "10", ",", "npts", "=", "1000", ")", ":", "\n", "# Using first order Euler method .", "\n", "        ", "b_size", "=", "h", ".", "shape", "[", "0", "]", "\n", "n_out", "=", "self", ".", "n_out", "\n", "dx", "=", "(", "max", "-", "min", ")", "/", "(", "npts", "-", "1", ")", "\n", "emb_size", "=", "h", ".", "shape", "[", "1", "]", "\n", "\n", "x", "=", "torch", ".", "arange", "(", "min", ",", "max", "+", "(", "max", "-", "min", ")", "/", "(", "npts", "-", "1", ")", ",", "dx", ")", ".", "to", "(", "h", ".", "device", ")", "\n", "npts", "=", "x", ".", "shape", "[", "0", "]", "\n", "zero_idx", "=", "torch", ".", "argmin", "(", "x", "**", "2", ")", ".", "item", "(", ")", "\n", "\n", "out", "=", "self", ".", "net", "(", "h", ")", "\n", "offset", "=", "out", "[", ":", ",", ":", "self", ".", "n_out", "]", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "b_size", ",", "npts", ",", "n_out", ")", "\n", "scaling", "=", "torch", ".", "exp", "(", "out", "[", ":", ",", "self", ".", "n_out", ":", "]", ")", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "b_size", ",", "npts", ",", "n_out", ")", "\n", "\n", "h_values", "=", "h", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "b_size", ",", "npts", ",", "emb_size", ")", ".", "reshape", "(", "-", "1", ",", "emb_size", ")", "\n", "x_values", "=", "x", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "b_size", ",", "npts", ")", ".", "reshape", "(", "-", "1", ",", "1", ")", "\n", "\n", "f_values", "=", "self", ".", "integrand", "(", "x_values", ",", "h_values", ")", "\n", "f_values", "=", "f_values", ".", "reshape", "(", "b_size", ",", "npts", ",", "n_out", ")", "*", "scaling", "\n", "\n", "F_values", "=", "(", "dx", "*", "f_values", ".", "cumsum", "(", "1", ")", ")", "\n", "F_values", "=", "(", "F_values", "-", "F_values", "[", ":", ",", "[", "zero_idx", "]", ",", ":", "]", ".", "expand", "(", "-", "1", ",", "npts", ",", "-", "1", ")", ")", "+", "offset", "\n", "corrected_F_values", "=", "out_deriv", "(", "F_values", ")", "\n", "\n", "expectations", "=", "(", "x_func", "(", "x", ")", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "b_size", ",", "npts", ",", "n_out", ")", "*", "f_values", "*", "corrected_F_values", ")", ".", "sum", "(", "1", ")", "*", "dx", "\n", "\n", "return", "expectations", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ThibautTheate_Unconstrained-Monotonic-Deep-Q-Network-algorithm.Models.MonotonicNN._flatten": [[11, 14], ["p.contiguous().view", "torch.cat", "torch.cat", "torch.tensor", "torch.tensor", "len", "p.contiguous"], "function", ["None"], ["def", "_flatten", "(", "sequence", ")", ":", "\n", "    ", "flat", "=", "[", "p", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ")", "for", "p", "in", "sequence", "]", "\n", "return", "torch", ".", "cat", "(", "flat", ")", "if", "len", "(", "flat", ")", ">", "0", "else", "torch", ".", "tensor", "(", "[", "]", ")", "\n", "\n"]]}